[
  {
    "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models",
    "link": "https://arxiv.org/pdf/2307.06949.pdf",
    "upvote": "49",
    "text": "HyperDreamBooth: HyperNetworks for Fast\nPersonalization of Text-to-Image Models\nNataniel Ruiz\nYuanzhen Li\nVarun Jampani\nWei Wei\nTingbo Hou\nYael Pritch\nNeal Wadhwa\nMichael Rubinstein\nKfir Aberman\nGoogle Research\nFigure 1: Using only a single input image, HyperDreamBooth is able to personalize a text-to-image diffusion model\n25x faster than DreamBooth [25], by using (1) a HyperNetwork to generate an initial prediction of a subset of network\nweights that are then (2) refined using fast finetuning for high fidelity to subject detail. Our method both conserves\nmodel integrity and style diversity while closely approximating the subject\u2019s essence and details.\nAbstract\nPersonalization has emerged as a prominent aspect within the field of generative AI,\nenabling the synthesis of individuals in diverse contexts and styles, while retaining high-\nfidelity to their identities. However, the process of personalization presents inherent\nchallenges in terms of time and memory requirements. Fine-tuning each personalized\nmodel needs considerable GPU time investment, and storing a personalized model per\nsubject can be demanding in terms of storage capacity. To overcome these challenges,\nwe propose HyperDreamBooth\u2014a hypernetwork capable of efficiently generating a\nsmall set of personalized weights from a single image of a person. By composing\nthese weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth\ncan generate a person\u2019s face in various contexts and styles, with high subject details\nwhile also preserving the model\u2019s crucial knowledge of diverse styles and semantic\nmodifications. Our method achieves personalization on faces in roughly 20 seconds,\n25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as\none reference image, with the same quality and style diversity as DreamBooth. Also\nour method yields a model that is 10000x smaller than a normal DreamBooth model.\nProject page: https://hyperdreambooth.github.io\nPreprint. Under review.\narXiv:2307.06949v1  [cs.CV]  13 Jul 2023\n1\nIntroduction\nRecent work on text-to-image (T2I) personalization [25] has opened the door for a new class of creative\napplications. Specifically, for face personalization, it allows generation of new images of a specific face\nor person in different styles. The impressive diversity of styles is owed to the strong prior of pre-trained\ndiffusion model, and one of the key properties of works such as DreamBooth [25], is the ability to implant\na new subject into the model without damaging the model\u2019s prior. Another key feature of this type of\nmethod is that subject\u2019s essence and details are conserved even when applying vastly different styles. For\nexample, when training on photographs of a person\u2019s face, one is able to generate new images of that\nperson in animated cartoon styles, where a part of that person\u2019s essence is preserved and represented in\nthe animated cartoon figure - suggesting some amount of visual semantic understanding in the diffusion\nmodel. These are two core characteristics of DreamBooth and related methods, that we would like to leave\nuntouched. Nevertheless, DreamBooth has some shortcomings: size and speed. For size, the original\nDreamBooth paper finetunes all of the weights of the UNet and Text Encoder of the diffusion model,\nwhich amount to more than 1GB for Stable Diffusion. In terms of speed, notwithstanding inference speed\nissues of diffusion models, training a DreamBooth model takes about 5 minutes for Stable Diffusion\n(1,000 iterations of training). This limits the potential impact of the work. In this work, we want to address\nthese shortcomings, without altering the impressive key properties of DreamBooth, namely style diversity\nand subject fidelity, as depctied in Figure 1. Specifically, we want to conserve model integrity and closely\napproximate subject essence in a fast manner with a small model.\nOur work proposes to tackle the problems of size and speed of DreamBooth, while preserving model\nintegrity, editability and subject fidelity. We propose the following contributions:\n\u2022 Lighweight DreamBooth (LiDB) - a personalized text-to-image model, where the customized\npart is roughly 100KB of size. This is achieved by training a DreamBooth model in a low-\ndimensional weight-space generated by a random orthogonal incomplete basis inside of a\nlow-rank adaptation [16] weight space.\n\u2022 New HyperNetwork architecture that leverages the Lightweight DreamBooth configuration and\ngenerates the customized part of the weights for a given subject in a text-to-image diffusion\nmodel. These provide a strong directional initialization that allows us to further finetune the\nmodel in order to achieve strong subject fidelity within a few iteration. Our method is 25x faster\nthan DreamBooth while achieving similar performances.\n\u2022 We propose the technique of rank-relaxed finetuning, where the rank of a LoRA DreamBooth\nmodel is relaxed during optimization in order to achieve higher subject fidelity, allowing us to\ninitialize the personalized model with an initial approximation using our HyperNetwork, and\nthen approximate the high-level subject details using rank-relaxed finetuning.\nOne key aspect that leads us to investigate a HyperNetwork approach is the realization that in order to\nbe able to synthesize specific subjects with high fidelity, using a given generative model, we have to\n\u201cmodify\" its output domain, and insert knowledge about the subject into the model, namely by modifying\nthe network weights.\n2\nRelated Work\nText-to-Image Models\nSeveral recent models such as Imagen [26], DALL-E2 [22], Stable Diffusion\n(SD) [24], Muse [8], Parti [33] etc. demonstrate excellent image generation capabilities given a text\nprompt. Some Text-to-Image (T2I) models such as Stable Diffusion and Muse also allows conditioning\nthe generation with a given image via an encoder network. Techniques such as ControlNet [35] propose\nways to incorporate new input conditioning such as depth. Test text and image based conditioning in these\nmodels do not capture sufficient subject details. Given the relatively small size of SD, for the ease of\nexperimentation, we demonstrate our HyperDreamBooth on SD model. But the proposed technique is\ngeneric and can be applicable to any T2I model.\nPersonalization of Generative Models\nGiven one or few subject images, the aim of personalized\ngeneration is to generate images of that particular subject in various contexts. Earlier works in this space\nuse GANs to edit a given subject image into new contexts. Pivotal tuning [23] proposes to finetune a\nGAN with an inverted latent code. The work of [21] proposes to finetune StyleGAN using around 100\nface images to obtain a personalized generative prior. Casanova et al. [7] proposes to condition a GAN\n2\nFigure 2: HyperDreamBooth Training and Fast Fine-Tuning. Phase-1: Training a hypernetwork to predict network\nweights from a face image, such that a text-to-image diffusion network outputs the person\u2019s face from the sentence\n\"a [v] face\" if the predicted weights are applied to it. We use pre-computed personalized weights for supervision,\nusing an L2 loss, as well as the vanilla diffusion reconstruction loss. Phase-2: Given a face image, our hypernetwork\npredicts an initial guess for the network weights, which are then fine-tuned using the reconstruction loss to enhance\nfidelity.\nusing an input image to generate variations of that input image. All these GAN based techniques suffer\nfrom either poor subject fidelity or a lack of context diversity in the generated images.\nHyperNetworks were introduced as an idea of using an auxiliary neural network to predict network\nweights in order to change the functioning of a specific neural network [13]. Since then, they have been\nused for tasks in image generation that are close to personalization, such as inversion for StyleGAN [4],\nsimilar to work that seeks to invert the latent code of an image in order to edit that image in the GAN\nlatent space [3].\nT2I Personalization via Finetuning\nMore recently, several works propose techniques for personalizing\nT2I models resulting in higher subject fidelity and versatile text based recontextualization of a given subject.\nTextual Inversion [11] proposes to optimize an input text embedding on the few subject images and use\nthat optimized text embedding to generate subject images. [30] propose a richer textual inversion space\ncapturing more subject details. DreamBooth [25] proposes to optimize the entire T2I network weights to\nadapt to a given subject resulting in higher subject fidelity in output images. Several works propose ways\nto optimize compact weight spaces instead of the entire network as in DreamBooth. CustomDiffusion [19]\nproposes to only optimize cross-attention layers. SVDiff [14] proposes to optimize singular values of\nweights. LoRa [2, 16] proposes to optimize low-rank approximations of weight residuals. StyleDrop [28]\n3\nproposes to use adapter tuning [15] and finetunes a small set of adapter weights for style personalization.\nDreamArtist [10] proposes a one-shot personalization techniques by employing a positive-negative prompt\ntuning strategy. Most of these finetuning techniques, despite generating high-quality subject-driven\ngenerations, are slow and can take several minutes for every subject.\nFast T2I Personalization\nSeveral concurrent works propose ways for faster personalization of T2I\nmodels. The works of [12] and [31] propose to learn encoders that predicts initial text embeddings\nfollowing by complete network finetuning for better subject fidelity. In contrast, our hypernetwork directly\npredicts low-rank network residuals. SuTI [9] proposes to first create a large paired dataset of input\nimages and the corresponding recontexualized images generated using standard DreamBooth. It then uses\nthis dataset to train a separate network that can perform personalized image generation in a feed-forward\nmanner. Despite mitigating the need for finetuning, the inference model in SuTI does not conserve the\noriginal T2I model\u2019s integrity and also suffers from a lack of high subject fidelity. InstantBooth [27] and\nTaming Encoder [17] create a new conditioning branch for the diffusion model, which can be conditioned\nusing a small set of images, or a single image, in order to generate personalized outputs in different styles.\nBoth methods need to train the diffusion model, or the conditioning branch, to achieve this task. These\nmethods are trained on large datasets of images (InstantBooth 1.3M samples of bodies from a proprietary\ndataset, Taming Encoder on CelebA [20] and Getty [1]). FastComposer [32] proposes to use image\nencoder to predict subject-specific embeddings and focus on the problem of identity blending in multi-\nsubject generation. The work of [5] propose to guide the diffusion process using face recognition loss to\ngenerate specific subject images. In such guidance techniques, it is usually difficult to balance diversity in\nrecontextualizations and subject fidelity while also keeping the generations within the image distribution.\nFace0 [29] proposes to condition a T2I model on face embeddings so that one can generate subject-specific\nimages in a feedforward manner without any test-time optimization. Celeb-basis [34] proposes to learn\nPCA basis of celebrity name embeddings which are then used for efficient personalization of T2I models.\nIn contrast to these existing techniques, we propose a novel hypernetwork based approach to directly\npredict low-rank network residuals for a given subject.\n3\nPreliminaries\nLatent Diffusion Models (LDM). Text-to-Image (T2I) diffusion models D\u03b8(\u03f5, c) iteratively denoises\na given noise map \u03f5 \u2208 Rh\u00d7w into an image I following the description of a text prompt T, which is\nconverted into an input text embedding c = \u0398(T) using a text encoder \u0398. In this work, we use Stable\nDiffusion [24], a specific instatiation of LDM [24]. Briefly, LDM consists of 3 main components: An\nimage encoder that encodes a given image into latent code; a decoder that decodes the latent code back to\nimage pixels; and a U-Net denoising network D that iteratively denoises a noisy latent code. See [24] for\nmore details.\nDreamBooth [25] provides a network fine-tuning strategy to adapt a given T2I denoising network D\u03b8 to\ngenerate images of a specific subject. At a high-level, DreamBooth optimizes all the diffusion network\nweights \u03b8 on a few given subject images while also retaining the generalization ability of the original\nmodel with class-specific prior preservation loss [25]. In the case of Stable Diffusion [24], this amounts\nto finetuning the entire denoising UNet has over 1GB of parameters. In addition, DreamBooth on a single\nsubject takes about 5 minutes with 1K training iterations.\nLow Rank Adaptation (LoRA) [16, 2] provides a memory-efficient and faster technique for DreamBooth.\nSpecifically, LoRa proposes to finetune the network weight residuals instead of the entire weights. That\nis, for a layer l with weight matrix W \u2208 Rn\u00d7m, LoRa proposes to finetune the residuals \u2206W. For\ndiffusion models, LoRa is usually applied for the cross and self-attention layers of the network [2]. A key\naspect of LoRa is the decomposition of \u2206W matrix into low-rank matrices A \u2208 Rn\u00d7r and B \u2208 Rr\u00d7m:\n\u2206W = AB. The key idea here is that r << n and the combined number of weights in both A and\nB is much lower than the number of parameters in the original residual \u2206W. Priors work show that\nthis low-rank residual finetuning is an effective technique that preserves several favorable properties of\nthe original DreamBooth while also being memory-efficient as well as fast, remarkably even when we\nset r = 1. For stable diffusion 1.5 model, LoRA-DreamBooth with r = 1 has approximately 386K\nparameters corresponding to only about 1.6MB in size.\n4\nMethod\nOur approach consists of 3 core elements which we explain in this section. We begin by introducing the\nconcept of the Lightweight DreamBooth (LiDB) and demonstrate how the Low-Rank decomposition\n4\nFigure 3: Lightweight DreamBooth: we propose a new low-dimensional weight-space for model personalization\ngenerated by a random orthogonal incomplete basis inside LoRA weight-space. This achieves models of roughly\n100KB of size (0.01% of original DreamBooth and 7.5% of LoRA DreamBooth size) and, surprisingly, is sufficient\nto achieve strong personalization results with solid editability.\n(LoRa) of the weights can be further decomposed to effectively minimize the number of personalized\nweights within the model. Next, we discuss the HyperNetwork training and the architecture the model\nentails, which enables us to predict the LiDB weights from a single image. Lastly, we present the concept\nof rank-relaxed fast fine-tuning, a technique that enables us to significantly amplify the fidelity of the\noutput subject within a few seconds. Fig. 2 shows the overview of hypernetwork training followed by fast\nfine-tuning strategy in our HyperDreamBooth technique.\n4.1\nLightweight DreamBooth (LiDB)\nGiven our objective of generating the personalized subset of weights directly using a HyperNetwork, it\nwould be beneficial to reduce their number to a minimum while maintaining strong results for subject\nfidelity, editability and style diversity. To this end, we propose a new low-dimensional weight space for\nmodel personalization which allows for personalized diffusion models that are 10,000 times smaller than\na DreamBooth model and more than 10 times smaller than a LoRA DreamBooth model. Our final version\nhas only 30K variables and takes up only 120 KB of storage space.\nThe core idea behind Lightweight DreamBooth (LiDB) is to further decompose the weight-space of a\nrank-1 LoRa residuals. Specifically, we do this using a random orthogonal incomplete basis within the\nrank-1 LoRA weight-space. We illustrate the idea in Figure 3. The approach can also be understood as\nfurther decomposing the Down (A) and Up (B) matrices of LoRA into two matrices each: A = AauxAtrain\nwith Aaux \u2208 Rn\u00d7a and Atrain \u2208 Ra\u00d7r and B = BtrainBaux with Btrain \u2208 Rr\u00d7b and Baux \u2208 Rb\u00d7m. where\nthe aux layers are randomly initialized with row-wise orthogonal vectors and are frozen; and the train\nlayers are learned. Two new hyperparameters are introduced: a and b, which we set experimentally. Thus\nthe weight-residual in a LiDB linear layer is represented as:\n\u2206Wx = AauxAtrainBtrainBaux,\n(1)\nwhere r << min(n, m), a < n and b < m. Aaux and Baux are randomly initialized with orthogonal row\nvectors with constant magnitude - and frozen, and Btrain and Atrain are learnable. Surprisingly, we find that\nwith a = 100 and b = 50, which yields models that have only 30K trainable variables and are 120 KB in\nsize, personalization results are strong and maintain subject fidelity, editability and style diversity. We\nshow results for personalization using LiDB in the experiments section.\n5\nFigure 4: HyperNetwork Architecture: Our hypernetwork consists of a Visual Transformer (ViT) encoder that\ntranslates face images into latent face features that are then concatenated to latent layer weight features that are\ninitiated by zeros. A Transformer Decoder receives the sequence of the concatenated features and predicts the values\nof the weight features in an iterative manner by refining the initial weights with delta predictions. The final layer\nweight deltas that will be added to the diffusion network are obtained by passing the decoder outputs through learnable\nlinear layers.\n4.2\nHyperNetwork for Fast Personalization of Text-to-Image Models\nWe propose a HyperNetwork for fast personalization of a pre-trained T2I model. Let \u02dc\u03b8 denote the set of all\nLiDB residual matrices: Atrain and Btrain for each of the cross-attention and self-attention layers of the T2I\nmodel. In essence, the HyperNetwork H\u03b7 with \u03b7 parameters takes the given image x as input and predicts\nthe LiDB low-rank residuals \u02c6\u03b8 = H\u03b7(x). The HyperNetwork is trained on a dataset of domain-specific\nimages with a vanilla diffusion denoising loss and a weight-space loss:\nL(x) = \u03b1||D\u02c6\u03b8(x + \u03f5, c) \u2212 x||2\n2 + \u03b2||\u02c6\u03b8 \u2212 \u03b8||2\n2,\n(2)\nwhere x is the reference image, \u03b8 are the pre-optimized weight parameters of the personalized model\nfor image x, D\u03b8 is the diffusion model (with weights \u03b8) conditioned on the noisy image x + \u03f5 and the\nsupervisory text-prompt c, and finally \u03b1 and \u03b2 are hyperparameters that control for the relative weight of\neach loss. Fig. 2 (top) illustrates the hypernetwork training.\nSupervisory Text Prompt\nWe propose to eschew any type of learned token embedding for this task,\nand our hypernetwork acts solely to predict the LiDB weights of the diffusion model. We simply propose\nto condition the learning process \u201ca [V] face\u201d for all samples, where [V] is a rare identifier described\nin [25]. At inference time variations of this prompt can be used, to insert semantic modifications, for\nexample \u201ca [V] face in impressionist style\u201d.\nHyperNetwork Architecture\nConcretely, as illustrated in Fig. 4, we separate the HyperNetwork\narchitecture into two parts: a ViT image encoder and a transformer decoder. We use a ViT-H for\nthe encoder architecture and a 2-hidden layer transformer decoder for the decoder architecture. The\ntransformer decoder is a strong fit for this type of weight prediction task, since the output of a diffusion\nUNet or Text Encoder is sequentially dependent on the weights of the layers, thus in order to personalize\na model there is interdependence of the weights from different layers. In previous work [13, 4], this\ndependency is not rigorously modeled in the HyperNetwork, whereas with a transformer decoder with a\npositional embedding, this positional dependency is modeled - similar to dependencies between words in\na language model transformer. To the best of our knowledge this is the first use of a transformer decoder\nas a HyperNetwork.\nIterative Prediction\nWe find that the HyperNetwork achieves better and more confident predictions\ngiven an iterative learning and prediction scenario [4], where intermediate weight predictions are fed to\nthe HyperNetwork and the network\u2019s task is to improve that initial prediction. We only perform the image\nencoding once, and these extracted features f are then used for all rounds of iterative prediction for the\nHyperNetwork decoding transformer T . This speeds up training and inference, and we find that it does\nnot affect the quality of results. Specifically, the forward pass of T becomes:\n\u02c6\u03b8k = T (f, \u02c6\u03b8k\u22121),\n(3)\nwhere k is the current iteration of weight prediction, and terminates once k = s, where s is a hyperparam-\neter controlling the maximum amount of iterations. Weights \u03b8 are initialized to zero for k = 0. Trainable\nlinear layers are used to convert the decoder outputs into the final layer weights. We use the CelebAHQ\n6\nFigure 5: HyperNetwork + Fast Finetuning achieves strong results. Here we show, for each reference (row), outputs\nfrom the initial hypernetwork prediction (HyperNetwork Prediction column), as well as results after HyperNetwork\nprediction and fast finetuning (HyperNetwork + Fast Finetuning). We also show generated results without the\nHyperNetwork prediction component, demonstrating its importance.\ndataset [18] for training the HyperNetwork, and find that we only need 15K identities to achieve strong\nresults, much less data than other concurrent methods.\n4.3\nRank-Relaxed Fast Finetuning\nWe find that the initial HyperNetwork prediction is in great measure directionally correct and generates\nfaces with similar semantic attributes (gender, facial hair, hair color, skin color, etc.) as the target face\nconsistently. Nevertheless, fine details are not sufficiently captured. We propose a final fast finetuning\nstep in order to capture such details, which is magnitudes faster than DreamBooth, but achieves virtually\nidentical results with strong subject fidelity, editability and style diversity. Specifically, we first predict\npersonalized diffusion model weights \u02c6\u03b8 = H(x) and then subsequently finetune the weights using the\ndiffusion denoising loss L(x) = ||D\u02c6\u03b8(x + \u03f5, c) \u2212 x||2\n2. A key contribution of our work is the idea of\nrank-relaxed finetuning, where we relax the rank of the LoRA model from r = 1 to r > 1 before fast\nfinetuning. Specifically, we add the predicted HyperNetwork weights to the overall weights of the model,\nand then perform LoRA finetuning with a new higher rank. This expands the capability of our method of\napproximating high-frequency details of the subject, giving higher subject fidelity than methods that are\nlocked to lower ranks of weight updates. To the best of our knowledge we are the first to propose such\nrank-relaxed LoRA models.\nWe use the same supervision text prompt \u201ca [V] face\u201d this fast finetuning step. We find that given\nthe HyperNetwork initialization, fast finetuning can be done in 40 iterations, which is 25x faster than\nDreamBooth [25] and LoRA DreamBooth [2]. We show an example of initial, intermediate and final\nresults in Figure 5.\n5\nExperiments\nWe implement our HyperDreamBooth on the Stable Diffusion v1.5 diffusion model and we predict the\nLoRa weights for all cross and self-attention layers of the diffusion UNet as well as the CLIP text encoder.\nFor privacy reasons, all face images used for visuals are synthetic, from the SFHQ dataset [6]. For training,\nwe use 15K images from CelebA-HQ [18].\n5.1\nSubject Personalization Results\nOur method achieves strong personalization results for widely diverse faces, with performance that is\nidentically or surpasses that of the state-of-the art optimization driven methods [25, 11]. Moreover, we\nachieve very strong editability, with semantic transformations of face identities into highly different\ndomains such as figurines and animated characters, and we conserve the strong style prior of the model\nwhich allows for a wide variety of style generations. We show results in Figure 6.\n7\nFigure 6: Results Gallery: Our method can generate novel artistic and stylized results of diverse subjects (depicted\nin an input image, left) with considerable editability while maintaining the integrity to the subject\u2019s key facial\ncharacteristics. The output images were generated with the following captions (top-left to bottom-right): \u201cAn\nInstagram selfie of a [V] face\", \u201cA Pixar character of a [V] face\", \u201cA [V] face with bark skin\", \u201cA [V] face as a rock\nstar\". Rightmost: \u201cA professional shot of a [V] face\".\nGiven the statistical nature of HyperNetwork prediction, some samples that are OOD for the HyperNetwork\ndue to lighting, pose, or other reasons, can yield subotpimal results. Specifically, we identity three types of\nerrors that can occur. There can be (1) a semantic directional error in the HyperNetwork\u2019s initial prediction\nwhich can yield erroneous semantic information of a subject (wrong eye color, wrong hair type, wrong\ngender, etc.) (2) incorrect subject detail capture during the fast finetuning phase, which yields samples\nthat are close to the reference identity but not similar enough and (3) underfitting of both HyperNetwork\nand fast finetuning, which can yield low editability with respect to some styles.\n8\nFigure 7: Qualitative Comparison: We compare random generated samples for our method (HyperDreamBooth),\nDreamBooth and Textual Inversion for two different identities and five different stylistic prompts. We observe that our\nmethod generally achieves very strong editability while preserving identity, generally surpassing competing methods\nin the single-reference regime.\nTable 1: Comparisons. We compare our method for face identity preservation (Face Rec.), subject fidelity (DINO,\nCLIP-I) and prompt fidelity (CLIP-T) to DreamBooth and Textual Inversion. We find that our method preserves\nidentity and subject fidelity more closely, while also achieving a higher score in prompt fidelity.\nMethod\nFace Rec. \u2191\nDINO \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nOurs\n0.655\n0.473\n0.577\n0.286\nDreamBooth\n0.618\n0.441\n0.546\n0.282\nTextual Inversion\n0.623\n0.289\n0.472\n0.277\n5.2\nComparisons\nQualitative Comparisons\nWe compare our method to both Textual Inversion [11] and DreamBooth [25]\nusing the parameters proposed in both works, with the exception that we increase the number of iterations\nof DreamBooth to 1,200 in order to achieve improved personalization and facial details. Results are\nshown in Figure 7. We observe that our method outperforms both Textual Inversion and DreamBooth\ngenerally, in the one-input-image regime.\nQuantitative Comparisons and Ablations\nWe compare our method to Textual Inversion and Dream-\nBooth using a face recognition metric (\u201cFace Rec.\u201d using an Inception ResNet, trained on VGGFace2),\nand the DINO, CLIP-I and CLIP-T metrics proposed in [25]. We use 100 identities from CelebAHQ [18],\nand 30 prompts, including both simple and complex style-modification and recontextualization prompts\nfor a total of 30,000 samples. We show in Table 1 that our approach obtains the highest scores for all\nmetrics. One thing to note is that face recognition metrics are relatively weak in this specific scenario,\ngiven that face recognition networks are only trained on real images and are not trained to recognize the\n9\nTable 2: Comparisons with DreamBooth. We compare our method to DreamBooth with differently tuned hy-\nperparameters to close the optimization time gap. We find that by increasing the learning rate and decreasing the\nnumber of iterations there is degradation of results, and DreamBooth does not achieve results similar to our method.\nDreamBooth-Agg-1 uses 400 iterations and DreamBooth-Agg-2 uses 40 iterations instead of the normal 1200 for our\nvanilla DreamBooth.\nMethod\nFace Rec. \u2191\nDINO \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nOurs\n0.655\n0.473\n0.577\n0.286\nDreamBooth\n0.618\n0.441\n0.546\n0.282\nDreamBooth-Agg-1\n0.615\n0.323\n0.431\n0.313\nDreamBooth-Agg-2\n0.616\n0.360\n0.467\n0.302\nTable 3: HyperNetwork Ablation. We ablate several components of our approach, including not using the\nhypernetwork component at test-time (No Hyper), only using the hypernetwork prediction without fast finetuning\n(Only Hyper) and using our full method without iterative prediction (k=1). We show that our full method performs\nbest for all fidelity metrics, although No Hyper achieves slightly better prompt following.\nMethod\nFace Rec. \u2191\nDINO \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nOurs\n0.655\n0.473\n0.577\n0.286\nNo Hyper\n0.647\n0.392\n0.498\n0.299\nOnly Hyper\n0.631\n0.414\n0.501\n0.298\nOurs (k=1)\n0.648\n0.464\n0.570\n0.288\nsame person in different styles. In order to compensate for this, we conduct a user study described further\nbelow.\nWe also conduct comparisons to more aggressive DreamBooth training, with lower number of iterations\nand higher learning rate. Specifically, we use 400 iterations for DreamBooth-Agg-1 and 40 iterations for\nDreamBooth-Agg-2 instead of 1200 for DreamBooth. We increase the learning rate and tune the weight\ndecay to compensate for the change in number of iterations. Note that DreamBooth-Agg-2 is roughly\nequivalent to only doing fast finetuning without the hypernetwork component of our work. We show\nin Table 2 that more aggressive training of DreamBooth generally degrades results when not using our\nmethod, which includes a HyperNetwork initialization of the diffusion model weights.\nFinally, we show an ablation study of our method. We remove the HyperNetwork (No Hyper), only\nuse the HyperNetwork without finetuning (Only Hyper) and also use our full setup without iterative\nHyperNetwork predictions (k=1). We show results in Table 3 and find that our full setup with iterative\nprediction achieves best subject fidelity, with a slightly lower prompt following metric.\nUser Study\nWe conduct a user study for face identity preservation of outputs and compare our method\nto DreamBooth and Textual Inversion. Specifically, we present the reference face image and two random\ngenerations using the same prompt from our method and the baseline, and ask the user to rate which one\nhas most similar face identity to the reference face image. We test a total of 25 identities, and query 5\nusers per question, with a total of 1,000 sample pairs evaluated. We take the majority vote for each pair.\nWe present our results in Table 4, where we show a strong preference for face identity preservation of our\nmethod.\nTable 4: User Study. Since face recognition networks are not trained to recognize the same face with different styles\nand can sometimes fail catastrophically, we conduct a user study for identity fidelity in our stylized generations and\ncompare one-to-one against DreamBooth and Textual Inversion. Users generally prefer images generated by our\napproach.\nMethod\nIdentity Fidelity \u2191\nOurs\n0.648\nDreamBooth\n0.233\nUndecided\n0.119\nOurs\n0.706\nTextual Inversion\n0.216\nUndecided\n0.078\n10\n6\nSocietal Impact\nThis work aims to empower users with a tool for augmenting their creativity and ability to express\nthemselves through creations in an intuitive manner. However, advanced methods for image generation\ncan affect society in complex ways [26]. Our proposed method inherits many possible concerns that affect\nthis class of image generation, including altering sensitive personal characteristics such as skin color, age\nand gender, as well as reproducing unfair bias that can already be found in pre-trained model\u2019s training\ndata. The underlying open source pre-trained model used in our work, Stable Diffusion, exhibits some of\nthese concerns. All concerns related to our work have been present in the litany of recent personalization\nwork, and the only augmented risk is that our method is more efficient and faster than previous work. In\nparticular, we haven\u2019t found in our experiments any difference with respect to previous work on bias, or\nharmful content, and we have qualitatively found that our method works equally well across different\nethnicities, ages, and other important personal characteristics. Nevertheless, future research in generative\nmodeling and model personalization must continue investigating and revalidating these concerns.\n7\nConclusion\nIn this work, we have presented HyperDreamBooth a novel method for fast and lightweight subject-driven\npersonalization of text-to-image diffusion models. Our method leverages a HyperNetwork to generate\nLightweight DreamBooth (LiDB) parameters for a diffusion model with a subsequent fast rank-relaxed\nfinetuning that achieves a significant reduction in size and speed compared to DreamBooth and other\noptimization-based personalization work. We have demonstrated that our method can produce high-quality\nand diverse images of faces in different styles and with different semantic modifications, while preserving\nsubject details and model integrity.\nReferences\n[1] Getty images. https://gettyimages.com/. Accessed: 2023-07-13. 4\n[2] Low-rank adaptation for fast text-to-image diffusion fine-tuning. https://github.com/cloneofsimo/lora,\n2022. 3, 4, 7\n[3] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle: A residual-based stylegan encoder via iterative\nrefinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6711\u20136720,\n2021. 3\n[4] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. Hyperstyle: Stylegan inversion with\nhypernetworks for real image editing. In Proceedings of the IEEE/CVF conference on computer Vision and\npattern recognition, pages 18511\u201318521, 2022. 3, 6\n[5] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and\nTom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 843\u2013852, 2023. 4\n[6] David Beniaguev.\nSynthetic faces high quality (sfhq) dataset.\nhttps://github.com/SelfishGene/\nSFHQ-dataset, 2022. 7\n[7] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano. Instance-\nconditioned gan. Advances in Neural Information Processing Systems, 34:27517\u201327529, 2021. 2\n[8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin\nMurphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative\ntransformers. arXiv preprint arXiv:2301.00704, 2023. 2\n[9] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen.\nSubject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186, 2023. 4\n[10] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation\nvia contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 4\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nAn image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint\narXiv:2208.01618, 2022. 3, 7, 9\n[12] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Designing an\nencoder for fast personalization of text-to-image models. arXiv preprint arXiv:2302.12228, 2023. 4\n[13] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016. 3, 6\n[14] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact\nparameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305, 2023. 3\n11\n[15] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. arXiv preprint\narXiv:1902.00751, 2019. 4\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 3, 4\n[17] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang,\nand Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion\nmodels. arXiv preprint arXiv:2304.02642, 2023. 4\n[18] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,\nstability, and variation. arXiv preprint arXiv:1710.10196, 2017. 7, 9\n[19] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1931\u20131941, 2023. 3\n[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), December 2015. 4\n[21] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael\nPritch, and Daniel Cohen-Or. Mystyle: A personalized generative prior. ACM Transactions on Graphics (TOG),\n41(6):1\u201310, 2022. 2\n[22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2\n[23] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of\nreal images. ACM Transactions on graphics (TOG), 42(1):1\u201313, 2022. 2\n[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 10684\u201310695, 2022. 2, 4\n[25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. 2022. 1, 2, 3, 4, 6, 7, 9\n[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494,\n2022. 2, 11\n[27] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without\ntest-time finetuning. arXiv preprint arXiv:2304.03411, 2023. 4\n[28] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,\nLu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan. Styledrop:\nText-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 3\n[29] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning a\ntext-to-image model on a face. arXiv preprint arXiv:2306.06638, 2023. 4\n[30] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in\ntext-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 3\n[31] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual\nconcepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848,\n2023. 4\n[32] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00e9do Durand, and Song Han. Fastcomposer: Tuning-free\nmulti-subject image generation with localized attention. arXiv preprint arXiv:2305.10431, 2023. 4\n[33] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image\ngeneration. arXiv preprint arXiv:2206.10789, 2022. 2\n[34] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, and Huicheng\nZheng. Inserting anybody in diffusion models via celeb basis. arXiv preprint arXiv:2306.00926, 2023. 4\n[35] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 2\n12\n"
  },
  {
    "title": "In-context Autoencoder for Context Compression in a Large Language Model",
    "link": "https://arxiv.org/pdf/2307.06945.pdf",
    "upvote": "24",
    "text": "Published as a conference paper at ICLR 2024\nIN-CONTEXT AUTOENCODER FOR CONTEXT\nCOMPRESSION IN A LARGE LANGUAGE MODEL\nTao Ge\u2217\nJing Hu\u2020\nLei Wang\u2020\nXun Wang\nSi-Qing Chen\nFuru Wei\nMicrosoft Corporation\n{tage,v-hjing,v-leiwang7,xunwang,sqchen,fuwei}@microsoft.com\nABSTRACT\nWe propose the In-context Autoencoder (ICAE), leveraging the power of a large\nlanguage models (LLM) to compress a long context into short compact memory\nslots that can be directly conditioned on by the LLM for various purposes. ICAE\nis first pretrained using both autoencoding and language modeling objectives on\nmassive text data, enabling it to generate memory slots that accurately and compre-\nhensively represent the original context; Then, it is fine-tuned on instruction data\nfor producing desirable responses to various prompts. Experiments demonstrate\nthat our lightweight ICAE, introducing about 1% additional parameters, effectively\nachieves 4\u00d7 context compression based on Llama, offering advantages in both\nimproved latency and GPU memory cost during inference, and showing an interest-\ning insight in memorization as well as potential for scalability. These promising\nresults imply a novel perspective on the connection between working memory in\ncognitive science and representation learning in LLMs, revealing ICAE\u2019s signifi-\ncant implications in addressing the long context problem and suggesting further\nresearch in LLM context management. Our data, code and models are available at\nhttps://github.com/getao/icae.\nAs ar&\ufb01cial intelligence becomes an increasingly powerful force, some of the world\u2019s biggest \ncompanies are worrying about how the technology will be used ethically, and how the public will \nperceive its spread. To combat these problems (among others), \ufb01ve tech companies \u2014 Google, \nAmazon, MicrosoD, Facebook, and IBM \u2014 set up a research group called the Partnership on AI. \n\u2026 \nAI taking white collar jobs, eroding trust in public media, becoming embedded in public ins&tu&ons \nlike the courts and hospitals: these are the sorts of problems facing the industry in the future.\nLong context\n\u02dc\nm1\n\u2026\nMemory Slots \n\u02dc\nmk\n\u02dc\nm1 \u02dc\nm2 \u2026\u02dc\nmk\nContext:\nPrompt: List the companies that \nset up Partnership on AI research \ngroup\nLLM Response: Google, Amazon, \nMicrosoD, Facebook and IBM.\n\u02dc\nm1 \u02dc\nm2 \u2026\u02dc\nmk\nContext:\nPrompt: What are poten6al \nchallenges the AI industry might \nface in the future?\nLLM Response: AI taking white \ncollar jobs, eroding trust in \npublic media, and becoming \nembedded in public ins&tu&ons.\n\u02dc\nm2\nIn-context \nautoencoding\nFigure 1: Compressing a long context into a short span of memory slots. The memory slots can be\nconditioned on by the target LLM on behalf of the original context to respond to various prompts.\n1\nINTRODUCTION\nLong context modeling is a fundamental challenge for Transformer-based (Vaswani et al., 2017)\nLLMs due to their inherent self-attention mechanism. Much previous research (Child et al., 2019;\nBeltagy et al., 2020; Rae et al., 2019; Choromanski et al., 2020; Bulatov et al., 2022; Zheng et al.,\n2022; Wu et al., 2022; Bulatov et al., 2023; Ding et al., 2023) attempts to tackle the long context issue\nthrough architectural innovations of an LLM. While they approach long context with a significant\nreduction in computation and memory complexity, they often struggle to overcome the notable decline\n\u2217Correspondence to Tao Ge (sggetao@gmail.com)\n\u2020Internship at Microsoft Research\n1\narXiv:2307.06945v3  [cs.CL]  18 Mar 2024\nPublished as a conference paper at ICLR 2024\nin performance on long contexts, as highlighted by Liu et al. (2023). In contrast to these efforts, we\napproach the long context problem from a novel angle \u2013 context compression.\nA s _ a r t i f i a l _ i n t e l l i g e n c e _ ... _ i n _ t h e _ f u t u r e .\nCharacter\nAs artificial intelligence becomes ... in the future.\nWord\nMemory\nList the companies that set up\nPartnership on AI research group\nPrompt\nLarge Language Model (LLM)\nGoogle, Amazon, Microsoft, Facebook and IBM.\nResponse\nLength = 2572\nLength = 512\nLength = 128\nFigure 2: Various context lengths (e.g., 2572 chars, 512 words, 128 memory slots) serve the same\nfunction when conditioned on by an LLM for responding to the given prompt.\nContext compression is motivated by that a text can be represented in different lengths in an LLM\nwhile conveying the same information. As shown in Figure 2, if we use characters to represent the\ntext, it will have a length of 2,572; if we represent it using (sub-)words, we only need a context length\nof 512 without affecting the response accuracy. So, is there a more compact representation allowing\nus to achieve the same goal with a shorter context?\nWe explore this problem and propose the ICAE which leverages the power of an LLM to achieve high\ncompression of contexts. The ICAE consists of 2 modules: a learnable encoder adapted from the\nLLM with LoRA (Hu et al., 2021) for encoding a long context into a small number of memory slots,\nand a fixed decoder, which is the LLM itself where the memory slots representing the original context\nare conditioned on to interact with prompts to accomplish various goals, as illustrated in Figure 1.\nWe first pretrain the ICAE using both autoencoding (AE) and language modeling (LM) objectives\nso that it can learn to generate memory slots from which the decoder (i.e., the LLM) can recover the\noriginal context or perform continuation. The pretraining with massive text data enables the ICAE\nto be well generalized, allowing the resulting memory slots to represent the original context more\naccurately and comprehensively. Then, we fine-tune the pretrained ICAE on instruction data for\npractical scenarios by enhancing its generated memory slots\u2019 interaction with various prompts. We\nshow the ICAE (based on Llama) learned with our pretraining and fine-tuning method can effectively\nproduce memory slots with 4\u00d7 context compression. We highlight our contributions as follows:\n\u2022 We propose In-context Autoencoder (ICAE) \u2013 a novel approach to context compression by lever-\naging the power of an LLM. The ICAE either enables an LLM to express more information with\nthe same context length or allows it to represent the same content with a shorter context, thereby\nenhancing the model\u2019s ability to handle long contexts with improved latency and memory cost\nduring inference. Its promising results and its scalability may suggest further research efforts in\ncontext management for an LLM, which is orthogonal to other long context modeling studies and\ncan be combined with them to further improve the handling of long contexts in an LLM.\n\u2022 In addition to context compression, ICAE provides an access to probe how an LLM performs\nmemorization. We observe that extensive self-supervised learning in the pretraining phase is\nvery helpful to enhance the ICAE\u2019s capability to encode the original context into compressed\nmemory slots. This pretraining process may share some analogies with humans enhancing their\nmemory capacity through extensive memory training, which improves the brain\u2019s memory encoding\ncapabilities (Ericsson et al., 1980; Engle et al., 1999; Maguire et al., 2003). We also show that\nan LLM\u2019s memorization pattern is highly similar to humans (see Table 2 and Table 3). All\nthese results imply a novel perspective on the connection between working memory in cognitive\nscience (Baddeley, 1992) and representation learning in LLMs (i.e., context window).\n2\nPublished as a conference paper at ICLR 2024\n2\nIN-CONTEXT AUTOENCODER\n2.1\nMODEL ARCHITECTURE\nLike a typical autoencoder (Kramer, 1991), ICAE consists of an encoder and a decoder. Similar to the\ndesign of Gisting (Mu et al., 2023) and AutoCompressor (Chevalier et al., 2023), the ICAE performs\nboth the encoding and decoding processes in an in-context manner, as illustrated in Figure 3.\ne(w1) e(w2)\ne(wL) em(m1)\nem(mk)\nLLM\n\u02dc\nm1\n\u02dc\nmk\n\u2026\nOriginal Context\nMemory tokens\ne(wi)\n\u2026\n\u2026\n\u2026\nMemory Slots\nLLM\ne(w1) e(w2)\ne(wL\u22121)\ne(wi)\n\u2026\n\u2026\nTeacher-forcing\nw1\nw2\nwi+1\nw3\n\u2026\n\u2026\nwL\nLoRA\nEncoder\nDecoder\nOriginal Context\n[AE]\nFigure 3: The encoder of the ICAE is a LoRA-adapted LLM, which is used for encoding the original\ncontext c = (w1, w2, . . . , wL) into a few memory slots ( f\nm1, . . . , f\nmk). The decoder of the ICAE is\nthe target LLM itself that can condition on the memory slots produced by the encoder for various\npurposes (e.g., the autoencoding task as in this figure). e(\u00b7) denotes the word embedding lookup in\nthe target LLM and em(\u00b7) denotes the learnable embedding lookup of memory tokens that are used\nfor producing memory slots.\u201c[AE]\u201d is a special token to indicate the autoencoding pretraining task.\nGiven the intuition, we propose to use a LoRA-adapted LLM as the encoder of the ICAE, as\nillustrated in Figure 3. When encoding a context c = (w1, . . . , wL) with the length L, we first append\nk (k << L) memory tokens (m1, . . . , mk) to the context c to obtain their outputs ( f\nm1, . . . , f\nmk) as\nthe memory slots for the context c. Therefore, the ICAE encoder is very lightweight \u2013 it only adds a\nLoRA adapter and an embedding lookup for memory tokens compared with the target LLM.\nAs introduced above, we expect the memory slots ( f\nm1, . . . , f\nmk) to be conditioned on by the target\nLLM on behalf of the original context c. Therefore, we use the untouched target LLM as the decoder\nof the ICAE to ensure the compatibility of memory slots within the target LLM.\n2.2\nPRETRAINING\n2.2.1\nAUTOENCODING\nAs a typical autoencoder, one of the ICAE\u2019s pretraining objectives is to restore the original input text\nc of the length L from its produced memory slots ( f\nm1, . . . , f\nmk) of the length k:\nLAE =\nmax\ng\nm1,...,g\nmk\nP(c| f\nm1, . . . , f\nmk; \u0398LLM) =\nmax\n\u0398LoRA,em P(c|m1 . . . mk; \u0398LLM, \u0398LoRA, em)\nTo indicate the autoencoding task, we append a special token \u201c[AE]\u201d to ( f\nm1, . . . , f\nmk) in the decoder,\nas Figure 3 shows. As this pretraining objective does not need any extra annotation, we can use\nmassive text data to train the In-context Autoencoder.\n2.2.2\nTEXT CONTINUATION\nWhile autoencoding pretraining offers a straightforward learning objective to encode a context, its\ninherent simplicity and exclusive focus on the single objective may lead to suboptimal generalization.\nTo address this issue, we incorporate an additional objective during the pretraining phase: text contin-\nuation, as illustrated in Figure 7 in Appendix A. This self-supervised task is widely acknowledged to\n3\nPublished as a conference paper at ICLR 2024\nfacilitate the learning of more generalizable representations in language models:\nLLM =\nmax\ng\nm1,...,g\nmk\nP(o| f\nm1, . . . , f\nmk; \u0398LLM) =\nmax\n\u0398LoRA,em P(o|m1 . . . mk; \u0398LLM, \u0398LoRA, em)\nwhere o = (wL+1, . . . , wL+N) denotes the continuation of context c. This objective helps improve\ngeneralization and circumvent excessive reliance on, and overfitting to, the autoencoding task.\n2.3\nINSTRUCTION FINE-TUNING\nAfter pretraining, the generated memory slots produced by the pretrained ICAE are expected to\nrepresent the original context. However, for LLMs, the purpose of providing a context extends\nbeyond rote memorization or continuation; instead, the more common use scenario is using the\nprovided context as a basis for accurately and appropriately responding to various prompts, ultimately\naccomplishing the tasks we want it to perform (Wei et al., 2021; Ouyang et al., 2022).\nTo enhance the interaction of memory slots produced by the ICAE with diverse prompts, we further\nfine-tune the ICAE with the PWC dataset (Prompt-with-Context) \u2013 a dataset1 introduced in this paper,\nwhich consists of thousands of (context, prompt, response) samples (as shown in Figure 1).\nFormally, the ICAE is fine-tuned for learning to encode the context into the memory slots based on\nwhich the decoder (i.e., the target LLM) can produce a desirable response r1 . . . rn according to a\ngiven prompt p1 . . . pm, as shown in Figure 8 in Appendix A:\nLFT = max\ng\nm1...g\nmk\nP(r1 . . . rn| f\nm1 . . . f\nmk, p1 . . . pm; \u0398LLM)\n=\nmax\n\u0398LoRA,em P(r1 . . . rn|m1 . . . mk, p1 . . . pm; \u0398LLM, \u0398LoRA, em)\n3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETTING\nData\nWe pretrain the ICAE with the Pile (Gao et al., 2020). For instruction fine-tuning, we use the\nPWC dataset, as introduced in Section 2.3, which contains 240k (context, prompt, response) samples\nfor training and 18k samples for testing. The context length distribution of test samples is shown in\nFigure 10. By default, the maximal token length (excluding memory slots) we set during training is\n512 in both the ICAE\u2019s encoder and decoder in our experiments.\nModel Configuration\nWe use the LlaMa (Touvron et al., 2023a;b) as the target LLM to test the\nICAE\u2019s performance in context compression. For the encoder of the ICAE, LoRA is applied to the\nquery and value projections of the LLM\u2019s multi-head attention. In our default setting, the memory\nslot length k is set to 128, and the LoRA rank r is set to 128 unless otherwise specified. The resulting\nICAE only adds about 1% learnable parameters on top of the target LLM.\nDue to space limitations, more training details are introduced in Table 8 in Appendix A.\n3.2\nRESULTS\n3.2.1\nPRETRAINED ICAE\nWe first evaluate the autoencoding performance of the pretrained ICAE (without instruction fine-\ntuning) using the following three metrics to understand how well it restores the original context from\nits produced memory slots: BLEU (Papineni et al., 2002), Exact-Match (EM)2 and cross entropy loss.\nFigure 4 presents the autoencoding results of the ICAE based on the Llama-7b. The ICAE demon-\nstrates a very low overall loss, below 0.05, indicating that the produced memory slots retain almost\nall the information of the original context. When the context length is within 300, the ICAE can\nalmost perfectly reconstruct the original context, achieving nearly 100% BLEU and EM scores. As\nthe context length increases beyond 400, both BLEU and EM scores start to decline, indicating\ninsufficient capacity of the 128-length memory slots. However, even at a context length of 500, the\n1Despite some (prompt, response) datasets such as Self-Instruct (Wang et al., 2022), most of their samples\neither have no context or very short contexts, which are not suitable for evaluation in our setting. Therefore, we\nestablish the PWC dataset with the help of the GPT-4 (OpenAI, 2023). We include the details in Appendix C.\n2EM denotes the proportion of the exact matching prefix length to the total length. For a context of 512\ntokens, if its first 256 tokens are perfectly restored but its 257th token is not, the EM score is 256/512 = 0.5.\n4\nPublished as a conference paper at ICLR 2024\n100\n200\n300\n400\n500\nlength\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00\nBLEU-4\nBLEU-4\n100\n200\n300\n400\n500\nlength\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEM\nExact Match (EM)\n100\n200\n300\n400\n500\nlength\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nLoss\nLoss\nFigure 4: Autoencoding results of the ICAE based on the Llama-7b with memory length k = 128.\nThe horizontal axis represents the original context length of test examples. For example, the horizontal\naxis value of 100 refers to the test examples with context lengths ranging from 95 to 105.\n100\n200\n300\n400\n500\nlength\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBLEU-4\nk=32\nk=64\nk=128\n100\n200\n300\n400\n500\nlength\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nloss\nk=32\nk=64\nk=128\nFigure 5: BLEU and loss at different memory slot lengths k.\nmedian BLEU remains over 0.98, and the median EM approaches 0.6 (e.g., perfectly reconstructing\nabout the first 300 words of a 512-token context), showing remarkable performance of ICAE.\nWe then analyze the effect of the memory size k on the result. According to Figure 5, as the memory\nslot length k decreases, the ICAE\u2019s ability to memorize longer samples significantly deteriorates.\nCompared to k = 128 where the BLEU score can still reach over 95% at a context length of 500,\nthe BLEU scores become much less satisfactory for k values of 64 and 32, indicating an inability to\nlosslessly retain the original context. This observation is also evident from the loss curve, suggesting\nthat achieving over 4\u00d7 compression is rather challenging.\nTable 1: Text continuation evaluation for the pretrained ICAE. Similar to the autoencoding evaluation,\na higher compression ratio tends to result in more pronounced losses in language modeling.\nContext length\nText Continuation\nPPL (w/ original context)\nPPL (w/ 128 memory slots)\n\u2206\n128\u2192128 (1\u00d7)\n9.99\n10.15\n+0.16\n256\u2192128 (2\u00d7)\n9.45\n9.77\n+0.32\n512\u2192128 (4\u00d7)\n9.01\n9.50\n+0.49\nSimilarly, the text continuation evaluation presented in Table 1 also illustrates that a higher compres-\nsion ratio tends to result in more pronounced losses in language modeling.\nTable 2 presents 1 specific example of the ICAE performing text restoration, demonstrating an\ninteresting behavior: \u201clarge pretrained language model\u201d is restored as \u201clarge pretrained model\u201d\nand \u201cThe results prove\u201d is restored as \u201cThe experimental evidence proves\u201d. These restoration errors\nresemble mistakes humans would make when memorizing the same text. This suggests that, like\nhumans, the model selectively emphasizes or neglects certain parts of the information during the\nmemorization based on its own understanding. It is also consistent with Peng et al. (2023): the\nstronger the LLM, the fewer it needs to memorize, and thus the smaller the memorization effort. This\nis similar to human learning: knowledgeable individuals tend to learn more effortlessly, while those\nwith limited knowledge often rely on rote memorization to acquire new information.\nTo further look into the memorization insight, we test restoration performance for different types of\n512-token texts with 128 memory slots produced by ICAE to investigate whether its memorization\ncapability is consistent across different content types. According to Table 3, in contrast to compressing\n5\nPublished as a conference paper at ICLR 2024\nTable 2: 1 example showing how the pretrained ICAE (k = 128) restores the original context.\nOrigin Context\nRestoration\nLarge pretrained language models have shown surprising In-\nContext Learning (ICL) ability. With a few demonstration\ninput-label pairs, they can predict the label for an unseen in-\nput without additional parameter updates. Despite the great\nsuccess in performance, the working mechanism of ICL still\nremains an open problem. In order to better understand how\nICL works, this paper explains language models as meta-\noptimizers and understands ICL as a kind of implicit finetun-\ning. Theoretically, we figure out that the Transformer attention\nhas a dual form of gradient descent based optimization. On\ntop of it, we understand ICL as follows: GPT first produces\nmetagradients according to the demonstration examples, and\nthen these meta-gradients are applied to the original GPT to\nbuild an ICL model. Experimentally, we comprehensively\ncompare the behavior of ICL and explicit finetuning based\non real tasks to provide empirical evidence that supports our\nunderstanding. The results prove that ICL behaves similarly\nto explicit finetuning at the prediction level, the representation\nlevel, and the attention behavior level. Further, inspired by our\nunderstanding of meta-optimization, we design a momentum-\nbased attention by analogy with the momentum-based gradient\ndescent algorithm. Its consistently better performance over\nvanilla attention supports our understanding again from an-\nother aspect, and more importantly, it shows the potential to\nutilize our understanding for future model designing.\nLarge pretrained models have shown surprising In-Context\nLearning (ICL) ability. With a few demonstration input-label\npairs, they can predict the label for an unseen input without\nadditional parameter updates. Despite the great success in per-\nformance, the working mechanism of ICL still remains an open\nproblem. In order to better understand how ICL works, this\npaper explains how language models as meta-optimizers and\nunderstands ICL as a kind of implicit finetuning. Theoretically,\nwe figure out that the Transformer attention has a dual form\nof gradient descent based on optimization. On top of it, we\nunderstand ICL as follows: GPT first produces metagradients\naccording to the demonstration examples, and then these meta-\ngradients are applied to the original GPT to build an ICL model.\nExperimentally, we comprehensively compare the behavior of\nICL and explicit finetuning based on real tasks to provide em-\npirical evidence that supports our findings. The experimental\nevidence proves that ICL behaves like us to the same extent.\nPrediction at the explicit finetuning level, the representation\nlevel, and the attention behavior level. Further, inspired by our\nunderstanding of meta-optimization, we design a momentum-\nbased attention by analogy with the gradient descent-based\nmomentum gradient algorithm. Its consistently better perfor-\nmance against vanilla attention supports us again from another\naspect, and more importantly, it shows the potential to use our\nunderstanding for future modeling tasks.\nTable 3: Restoration performance for different types of 512-token content with 128 memory slots.\nPatterned random text is obtained by adding 1 to each token_id in a normal text.\nContent type\nLoss\nBLEU\nNormal text\n0.01\n99.3\nPatterned random text\n1.63\n3.5\nCompletely random text\n4.55\n0.2\nnormal texts which can be well restored, compressing and restoring less common texts (i.e., random\ntexts) becomes very challenging, reflected by much worse loss and BLEU scores. All these results\nstrongly support our intuition that an LLM\u2019s memorization pattern is highly similar to humans.\nBased on this intuition, it is very likely that a more powerful LLM may support a higher compression\nratio without significant forgetting. We will discuss it in Section 3.3.1.\n3.2.2\nFINE-TUNED ICAE\nIn order to evaluate the fine-tuned ICAE\u2019s performance, we evaluate on the PWC test set. We use\nthe GPT-4 to compare the outputs of the two systems to determine which one performs better or if\nthey are on par with each other, following Mu et al. (2023). Table 4 shows the comparison of results\nof the LLMs conditioned on memory slots and original contexts. For Llama-7b (fine-tuned ICAE),\nwe compare with Alpaca and StableLM-tuned-alpha-7b since there is no official instruction-tuned\nLlama-1 model. The Llama-7b (ICAE) conditioned on 128 memory slots largely outperforms both\nAlpaca and StableLM which can access original contexts (\u223c512 tokens), with a win rate of 56.7%\nand 74.1% respectively and a win+tie rate of 73%\u223c81%. However, when compared to the GPT-4\n(we regard it as the gold standard), there is still a significant gap, with around 70% of the cases\nunderperforming the GPT-4\u2019s results, and a win+tie ratio of about only 30%.\nWhen we switch the base model to Llama-2-chat, we observe ICAE\u2019s performance becomes much\nbetter than its counterpart based on Llama-1: when k = 128, its win+tie rate can reach around 75%\nagaint the GPT-4 although it still lags behind its counterpart conditioning on the original context as\nthe compression is lossy. As k increases, the win+tie rate further improves while the compression rate\ndecreases. We perform the same comparative studies on Llama-2-13b-chat and observe better results\nof ICAE, supporting our assumption in Section 3.2.1 that the ICAE can benefit more on larger LLMs.\nWe investigate the impact of memory length on results. Table 5 shows pairwise comparisons between\nICAE models with varying memory slot lengths. A higher compression ratio makes it harder to ensure\n6\nPublished as a conference paper at ICLR 2024\nTable 4: Memory slots VS Original contexts (\u223c512 tokens) on the PWC test set\nSystem 1\n(k memory slots)\nSystem 2\n(original context)\nJudgement (%)\nwin\nlose\ntie\non par (win+tie)\nLlama-7b (ICAE, k=128)\nAlpaca\n56.7\n26.9\n16.4\n73.1\nStableLM-7b\n74.1\n18.8\n7.2\n81.3\nGPT-4 (gold)\n3.4\n69.4\n27.2\n30.6\nLlama-2-7b-chat (ICAE, k=64)\nLlama-2-7b-chat\n13.6\n51.6\n34.8\n48.4\nGPT-4 (gold)\n1.9\n44.7\n53.4\n55.3\nLlama-2-7b-chat (ICAE, k=128)\nLlama-2-7b-chat\n19.6\n45.4\n35.0\n54.6\nGPT-4 (gold)\n2.8\n25.8\n71.4\n74.2\nLlama-2-7b-chat (ICAE, k=256)\nLlama-2-7b-chat\n22.0\n22.2\n55.8\n77.8\nGPT-4 (gold)\n3.8\n20.5\n75.7\n79.5\nLlama-2-13b-chat (ICAE, k=256)\nLlama-2-13b-chat\n21.9\n20.8\n57.3\n79.2\nGPT-4 (gold)\n4.0\n19.2\n76.8\n80.8\nTable 5: ICAE with different memory slot lengths and different pretraining setups. The last row is the\ncomparison between 128-length ICAE\u2019s memory and 128-token summary produced by the GPT-4.\nICAE (Llama-2-7b-chat)\nJudgement\nwin (%)\nlose (%)\ntie (%)\nwin/lose\nk = 128 (pretrained) VS k = 64 (pretrained)\n57.6\n19.5\n22.9\n3.0\nk = 64 (pretrained) VS k = 32 (pretrained)\n44.7\n21.8\n33.5\n2.1\nk = 64 (pretrained) VS k = 128 (no pretraining)\n33.1\n28.0\n38.9\n1.2\nk = 128 (pretrained) VS k = 128 (no pretraining)\n60.4\n9.5\n30.1\n6.4\nk = 128 (pretrained) VS k = 128 (pretrained only with AE)\n36.4\n28.5\n35.1\n1.3\nk = 128 (pretrained) VS k = 128 (pretrained only with LM)\n35.1\n24.9\n40.0\n1.4\nk = 128 (pretrained) VS 128-token summary (by GPT-4)\n34.1\n17.6\n48.3\n1.9\nresponse quality, but a larger ratio doesn\u2019t always lead to worse performance. Table 5 highlights\nthat a pretrained ICAE with 8\u00d7 compression (k=64) can match a non-pretrained ICAE with 4\u00d7\ncompression (k=128). Under the same ratio, the pretrained ICAE performs much better than its\nnon-pretrained counterpart, emphasizing the importance of pretraining. By comparing the outputs\ngenerated via the pretrained and non-pretrained ICAE, we find the pretrained ICAE suffers less from\nhallucination than the non-pretrained counterpart (see the examples in Table 9 in Appendix D). We\nassume the pretraining of ICAE improves the LLM\u2019s working memory as it shares some analogies\nwith humans enhancing their memory capacity via extensive memory training which improves the\nbrain\u2019s memory encoding capabilities. We also examine pretraining objectives and find combining3\nAE and LM yields better results than using AE or LM individually (the 4th row in Table 5).\nThe last row of Table 5 compares ICAE\u2019s 128-length memory slots with a summary4 within 128\ntokens (\u223c100 words). Memory slots significantly outperform summaries under the same context\nlength, with \u223c2\u00d7 win/lose ratio, proving to be more compact and informative than natural language.\n3.3\nANALYSIS\n3.3.1\nSCALABILITY\nAs discussed above, ICAE should achieve better compression performance with a more powerful\ntarget LLM. To verify this assumption, we compare the ICAE\u2019s performance on three target LLMs:\nLlama-7b, Llama-2-7b and Llama-2-13b in Table 6, which align well with our expectations \u2013 a more\npotent target LLM can achieve a superior compression ratio of contexts.\n3.3.2\nLATENCY\nWe conducted an empirical test to evaluate the impact of ICAE\u2019s 4\u00d7 context compression on inference\nefficiency. For this efficiency test, we fix the context (i.e., input) length to either 512 or 2048 and the\n3Lpretrain = \u03bbLAE + (1 \u2212 \u03bb)LLM. We find \u03bb = 0.4 \u223c 0.6 leads to the best result.\n4Produced by the GPT-4. The specific prompt text is presented in Appendix D.\n7\nPublished as a conference paper at ICLR 2024\nTable 6: The results of pretrained ICAE (512\u2192128) based on different target LLMs\nTarget LLM\nAE\nText Continuation\nBLEU(%)\nLoss\nPPL (original context)\nPPL (memory slot)\n\u2206\nLlama-7b\n99.1\n0.017\n9.01\n9.50\n+0.49\nLlama-2-7b\n99.5\n0.009\n8.81\n9.18\n+0.37\nLlama-2-13b\n99.8\n0.004\n8.15\n8.45\n+0.30\nTable 7: Latency comparison of LLM (generation) and LLM+ICAE (compression then generation)\nInput\n(Batch\u00d7Length)\nMethod\nCompression Time\nDecoding\nTime\nTotal\nTime\n(Cachable)\n8*2048\nLLM\n-\n24.0\n24.0\nLLM+ICAE\n3.4\n3.9\n7.3 (3.3\u00d7)\n8*512\nLLM\n-\n9.3\n9.3\nLLM+ICAE\n0.6\n3.7\n4.3 (2.2\u00d7)\n32*512\nLLM\n-\n24.3\n24.3\nLLM+ICAE\n2.6\n4.2\n6.8 (3.6\u00d7)\ngeneration length to 128. Table 7 shows that context compression by ICAE is helpful to improve\nLLM (i.e., Llama-7b) inference efficiency, achieving over 2\u00d7 speedup. Its acceleration becomes\neven more significant \u2013 around 3.5\u00d7 \u2013 in compute-intensive scenarios (e.g., 8\u00d72048 and 32\u00d7512).\nGiven that the compressed memory slots can be cached in advance (for frequently used texts like\ntextbooks, government reports or articles of law), ICAE may introduce over 7\u00d7 inference speedup in\nthese cases. Details of the profiling are presented in Appendix B.\n3.3.3\nMULTIPLE SPANS OF MEMORY SLOTS\nThus far, we have mainly discussed a single span of memory slots. In this section, we shall discuss\nmultiple spans of memory slots. As illustrated in Figure 6(Left), we can segment a long context\ninto N chunks, compress them individually, and then concatenate them to represent the original\nlong context. However, this did not work initially, because the model had never seen multiple span\nconcatenation patterns during training. Fortunately, we can incorporate a small number of multiple\nspan concatenation samples during training, enabling the model to work with concatenated spans of\nmemory slots, as OpenAI\u2019s work (Bavarian et al., 2022) on introducing the \u201cfill in the middle\u201d ability\nfor the GPT. The results in Table 6(Right) indicate that, using an equivalent length context, ICAE\u2019s\nmemory achieves better performance \u2013 because memory can represent 4\u00d7 the original context length.\nThe ability of ICAE demonstrates great promise to handle long contexts, as it can save a significant\namount of GPU memory when addressing long contexts without touching the existing LLM. As\nillustrated in Figure 6(Right), 2048-length memory slots can perform on par with 4096-token contexts.\nThis means that conditioning on 2048 memory slots instead of the original 4096 context tokens can\nsave about 20GB of GPU memory5 with minimal quality degradation.\n4\nRELATED WORK\nPrompt compression and context distillation (Askell et al., 2021; Snell et al., 2022) are closely related\nareas to this work: Wingate et al. (2022) proposed a method to learn compact soft prompts to simulate\nthe original natural language prompt by optimizing the KL divergence. However, this approach has\na very high computational cost, as it requires performing back-propagation for each new incoming\nprompt to learn and obtain the compressed prompt, which severely limits its application. Qin &\nVan Durme (2023) proposes Neural Agglomerative Embeddings named NUGGET, which encodes\nlanguage into a compact representation for an encoder-decoder model. GIST (Mu et al., 2023)\nachieves prompt compression by fine-tuning a LLM in a similar way to ours. The resulting model can\nproduce a number of gist tokens as the compression of a prompt, which are similar to our memory\n5Llama-7b (fp16) requires 24GB GPU memory for 2048 context tokens and 44GB for 4096 during inference\n(measured without optimization like flash attention).\n8\nPublished as a conference paper at ICLR 2024\nChunk 1\n\u2026\nChunk N\nICAE\nSpan 1\nSpan N\n\u2026\nConcatenation\nL tokens\nk memory slots\n128 256\n512\n1024\n2048\nContext Length\n8.00\n8.25\n8.50\n8.75\n9.00\n9.25\n9.50\n9.75\nPerplexity\nOriginal Context VS Memory Slot Concatenation\noriginal context\nmemory slot\nFigure 6: Left: Individually compress then concatenate multiple spans of memory slots; Right:\nPerplexity comparison with original contexts and 4\u00d7 compressed memory slots \u2013 for example, 1024-\nlength memory slots are obtained by compressing the original context with a length of 4096 tokens.\nslots. Nonetheless, this approach is limited to compressing short prompts6 and thus does not address\nthe real issue of long contexts. Also, this method requires fine-tuning the LLM, and the obtained gist\ntokens also need to be used within the specially tuned LLM (for gist tokens) and seem not compatible\nwith the untouched LLM. Another contemporary study related to ours is AutoCompressors (Chevalier\net al., 2023) for recursively compressing long text into summary vectors. Like Mu et al. (2023), the\nLLM must be tuned to work with generated summary vectors and its training is sophisticated as it\ninvolves recursive compression. In contrast, we propose a very simple, straightforward and scalable\napproach to generating memory slots that can be used in the target LLM with different prompts for\nvarious purposes. Moreover, our approach is much more parameter-efficient (i.e., LoRA) for tuning\non top of the existing LLM. Additionally, there is recent work (Jiang et al., 2023a) that studies how\nto compress prompts into more concise natural language.\nAlso, there is related work studying compressing indescribable concepts into (vector) tokens for later\nuse in other contexts. Representative work includes Gal et al. (2022) which compresses a vision\nobject into a token and Ge et al. (2023) which compresses a text style into a token.\nConsidering related work from a boarder perspective of compression, Jiang et al. (2023b) examines\nkNN-based prediction using general-purpose compressors, such as gzip. Del\u00e9tang et al. (2023)\nextensively investigates the compression abilities of LLMs, uncovering their potential as versatile\npredictors, which also provides insights into recent developments in scaling laws and tokenization.\n5\nCONCLUSION AND FUTURE WORK\nWe propose the In-context Autoencoder (ICAE) to leverage the power of an LLM to highly compress\ncontexts. By generating compact and informative memory slots to represent the original context,\nthe ICAE enables an LLM to acquire more information with the same context length or represent\nthe same content with a shorter context, thereby enhancing the model\u2019s capability to handle long\ncontexts as well as reducing computation and memory overheads for inference in many practical\nscenarios like Retrieval Augmented Generation (Lewis et al., 2020) and advanced prompting methods\n(Wei et al., 2022; Wang et al., 2023). Moreover, ICAE provides insight into how an LLM performs\nmemorization, offering a novel perspective on the connection between the memory of LLMs and\nhumans, and suggesting future research in LLM context management.\nDue to computational limitations, our experiments were conducted on Llama models up to 13 billion\nparameters. As discussed in the paper, ICAE is expected to benefit even more from more powerful\nLLMs, where it should be able to achieve more significant compression ratios. In the future, we\nhope to have sufficient computational resources to validate the effectiveness of ICAE on larger and\nstronger LLMs. In addition, we plan to explore the application of ICAE in multimodal LLMs (as\nthe context length for images, videos, and audio is often much longer and has greater compression\npotential) for helping unify compact representation across modalities in the era of LLM/AGI.\n6Prompts in Mu et al. (2023) refer to task instructions before input texts, so they are usually short.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\nAlan Baddeley. Working memory. Science, 255(5044):556\u2013559, 1992.\nMohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry\nTworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in\nNeural Information Processing Systems, 35:11079\u201311091, 2022.\nAydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and beyond\nwith rmt. arXiv preprint arXiv:2304.11062, 2023.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to\ncompress contexts. arXiv preprint arXiv:2305.14788, 2023.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s\nSarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J.\nColwell, and Adrian Weller. Rethinking attention with performers. ArXiv, abs/2009.14794, 2020.\nGr\u00e9goire Del\u00e9tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christo-\npher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al.\nLanguage modeling is compression. arXiv preprint arXiv:2309.10668, 2023.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.\nLongnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023.\nRandall W Engle, Stephen W Tuholski, James E Laughlin, and Andrew RA Conway. Working\nmemory, short-term memory, and general fluid intelligence: a latent-variable approach. Journal of\nexperimental psychology: General, 128(3):309, 1999.\nK Anders Ericsson, William G Chase, and Steve Faloon. Acquisition of a memory skill. Science, 208\n(4448):1181\u20131182, 1980.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv preprint arXiv:2208.01618, 2022.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nTao Ge, Hu Jing, Li Dong, Shaoguang Mao, Yan Xia, Xun Wang, Si-Qing Chen, and Furu\nWei.\nExtensible prompts for language models on zero-shot language style customization.\nIn A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Ad-\nvances in Neural Information Processing Systems, volume 36, pp. 35576\u201335591. Curran Asso-\nciates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/\n2023/file/6fcbfb3721c1781728b10c6685cc2f6c-Paper-Conference.pdf.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\n10\nPublished as a conference paper at ICLR 2024\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing\nprompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736,\n2023a.\nZhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, and Jimmy Lin. \u201clow-\nresource\u201d text classification: A parameter-free classification method with compressors. In Findings\nof the Association for Computational Linguistics: ACL 2023, pp. 6810\u20136828, 2023b.\nMark A. Kramer. Nonlinear principal component analysis using autoassociative neural networks.\nAiche Journal, 37:233\u2013243, 1991.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\n9459\u20139474, 2020.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts, 2023.\nEleanor A Maguire, Elizabeth R Valentine, John M Wilding, and Narinder Kapur.\nRoutes to\nremembering: the brains behind superior memory. Nature neuroscience, 6(1):90\u201395, 2003.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. arXiv\npreprint arXiv:2304.08467, 2023.\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. 10 2002. doi: 10.3115/1073083.1073135.\nGuangyue Peng, Tao Ge, Si-Qing Chen, Furu Wei, and Houfeng Wang. Semiparametric language\nmodels are scalable continual learners. arXiv preprint arXiv:2303.01421, 2023.\nGuanghui Qin and Benjamin Van Durme. Nugget: Neural agglomerative embeddings of text.\nIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and\nJonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning,\nvolume 202 of Proceedings of Machine Learning Research, pp. 28337\u201328350. PMLR, 23\u201329 Jul\n2023. URL https://proceedings.mlr.press/v202/qin23a.html.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\nCharlie Snell, Dan Klein, and Ruiqi Zhong.\nLearning by distilling context.\narXiv preprint\narXiv:2209.15189, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u2019elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. ArXiv, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\n11\nPublished as a conference paper at ICLR 2024\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the\nemergent cognitive synergy in large language models: A task-solving agent through multi-persona\nself-collaboration. arXiv preprint arXiv:2307.05300, 2023.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824\u201324837, 2022.\nDavid Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive\nconditioning for controllability and toxicity reduction in language models.\narXiv preprint\narXiv:2210.03162, 2022.\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers.\narXiv preprint arXiv:2203.08913, 2022.\nLin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mecha-\nnism. In International Conference on Machine Learning, 2022.\nA\nMODEL TRAINING CONFIGURATION\nWe show how to perform pretraining with the text continuation objective and instruction fine-tuning\nin Figure 7 and 8.\ne(w1) e(w2)\ne(wL) em(m1)\nem(mk)\nLLM\n\u02dc\nm1\n\u02dc\nmk\n\u2026\nOriginal Context\nMemory tokens\ne(wi)\n\u2026\n\u2026\n\u2026\nMemory Slots\nLLM\ne(wL+1)e(wL+2)\ne(wL+N\u22121)\n\u2026\nTeacher-forcing\nwL+1\nwL+2\nwL+N\nwL+3\n\u2026\nLoRA\nEncoder\nDecoder\nConNnuaNon\nFigure 7: Pretraining with the text continuation objective to predict next tokens\nWe train the ICAE on 8 Nvidia A100 GPUs (80GB). The hyperparameters for pretraining and\nfine-tuning ICAE are presented in Table 8. We by default train the ICAE with bf16.\nB\nPROFILING SETUP\nWe test the latency (Section 3.3.2) on 1 Nvidia A100 GPU (80GB). The test machine has the CPU\nof AMD EPYC\u2122 7413 with 24 cores and 216GB RAM. The runtime configuration is python=3.9,\npytorch=2.0.1, cuda=11.7, cudnn=8.5.\n12\nPublished as a conference paper at ICLR 2024\ne(w1) e(w2)\ne(wL) em(m1)\nem(mk)\nLLM\n\u02dc\nm1\n\u02dc\nmk\n\u2026\nOriginal Context\nMemory tokens\ne(wi)\n\u2026\n\u2026\n\u2026\nMemory Slots\nLLM\nr2\n\u2026\nr3\nLoRA\nEncoder\nDecoder\ne(p1) e(p2)\ne(pm)\n\u2026\ne(r1) e(r2)\ne(rn\u22121)\n\u2026\nr1\nrn\nPrompt\nTeacher-forcing\nResponse\nFigure 8: Instruct fine-tuning of the ICAE to make its produced memory slots interact with prompts\nfor accomplishing various purposes in the target LLM. In this figure, (p1, . . . , pm) denotes the prompt\ntokens and (r1, . . . , rn) denotes the response tokens.\nTable 8: Hyperparameters for training\nHyperparameter\nValue\nOptimizer\nAdamW\nlearning rate\n1e-4 (pretrain); 5e-5 (fine-tuning)\nbatch size\n256\nwarmup\n300\n#updates\n200k (pretrain); 30k (fine-tuning)\nclip norm\n2.0\nC\nPROMPT-WITH-CONTEXT DATASET\nWe introduce the PROMPT-WITH-CONTEXT (PWC) dataset where each sample entry is a triple (text,\nprompt, answer), as depicted in Figure 9. To construct this dataset, we first sample 20k texts from the\nPile dataset. Then, for each text, we employ the GPT-4 to provide 15 prompts (10 specific prompts\nand 5 general prompts) about the text and give the corresponding answers. The prompt instructing\nthe GPT-4 is outlined in Listing 1.\nThe dataset is composed of 240k examples for training purposes, with an additional 18k examples for\ntesting. The context length distribution of test samples is presented in Table 10.\nListing 1: Prompt used by GPT4 API to generate the PWC dataset.\nDesign 10 prompts specified to the above text to test understanding of\nthe above text. These prompts should be diverse and cover as many\naspects (e.g., topic, genre, structure, style, polarity, key information\nand details) of the text as possible. The first half of these prompts\nshould be like an instruction, the other should be like a question. In\naddition to the prompts specified to the above text, please also design\n5 general prompts like \"rephrase the above text\", \"summarize the above\ntext\", \"write a title for the above text\", \"extract a few keywords for\nthe above text\" and \"write a paragraph (i.e., continuation) that follows\nthe above text\". Each prompt should be outputted in the following\nformat: [{\"prompt\": your generated prompt, \"answer\": the answer to the\nprompt}]\n13\nPublished as a conference paper at ICLR 2024\nAs ar&\ufb01cial intelligence becomes an increasingly powerful force, some of the world\u2019s biggest \ncompanies are worrying about how the technology will be used ethically, and how the public will \nperceive its spread. To combat these problems (among others), \ufb01ve tech companies \u2014 Google, \nAmazon, MicrosoD, Facebook, and IBM \u2014 set up a research group called the Partnership on AI. \n\u2026 \nNew trustees joining include Dario Amodei of OpenAI \u2014 another industry AI research group \nfounded by Elon Musk and Y Combinator president Sam Altman \u2014 and Carol Rose of the ACLU. \n\u2026 \nAI taking white collar jobs, eroding trust in public media, becoming embedded in public ins&tu&ons \nlike the courts and hospitals: these are the sorts of problems facing the industry in the future.\nContext\nPrompt: List the \ufb01ve tech companies \nthat ini6ally set up the Partnership \non AI.\nAnswer: Google, Amazon, MicrosoD, \nFacebook and IBM.\nPrompt: What are poten6al \nchallenges the AI industry might \nface in the future?\nAnswer: Challenges include AI \ntaking white collar jobs, eroding \ntrust in public media, and \nbecoming embedded in public \nins&tu&ons like courts and \nhospitals..\nPrompt: Which organiza6on \ndid Dario Amodei previously \nwork for?\nAnswer: OpenAI\n\u2026\n\u2026\nFigure 9: Construction of the PWC dataset: we use the GPT-4 to generate a variety of prompt-answer\npairs according to contexts. The resulting dataset is used for instruction fine-tuning (240k for training)\nand evaluation (18k for testing) in this work.\n400\n450\n500\n550\n600\n650\n700\n750\n800\n0\n200\n400\n600\n800\n1000\nFigure 10: The context length distribution of test samples: Most samples are longer than 500 tokens.\nD\nGPT-4 EVALUATION\nAccording to Mu et al. (2023), we formulate an evaluation prompt to be used with the GPT-4 API. The\nprompt, as illustrated in Listing 2, consists of a task description along with three specific examples.\nWe supply GPT-4 with a text, a prompt, and two distinct model-generated responses. The task for\nGPT-4 is to determine the superior answer or recognize a tie. The chosen examples encompass\nscenarios where Assistant A performs better, Assistant B performs better, and when a tie occurs. This\nmethodology enables us to effectively assess the model\u2019s quality. Specially, the orders where the\n14\nPublished as a conference paper at ICLR 2024\nmodel responses are presented to the GPT-4 are swapped randomly to alleviate bias, as Touvron et al.\n(2023b) did.\nListing 2: Prompt for the GPT-4 evaluation. This prompt consists of a description of the task and\nthree specific examples.\nGiven a piece of text, an instruction for this text, and two AI\nassistant answers, your task is to choose the better answer and provide\nreasons. Evaluate the answers holistically, paying special attention to\nwhether the response (1) follows the given instruction and (2) is\ncorrect. If both answers correctly respond to the prompt, you should\njudge it as a tie.\nExample 1:\n\u2018\u2018\u2018\nText: We report the development of GPT-4, a large-scale, multimodal\nmodel which can accept image and text inputs and produce text outputs.\nWhile less capable than humans in many real-world scenarios, GPT-4\nexhibits human-level performance on various professional and academic\nbenchmarks, including passing a simulated bar exam with a score around\nthe top 10% of test takers. GPT-4 is a Transformerbased model\npre-trained to predict the next token in a document. The post-training\nalignment process results in improved performance on measures of\nfactuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that\nbehave predictably across a wide range of scales. This allowed us to\naccurately predict some aspects of GPT-4\u2019s performance based on models\ntrained with no more than 1/1,000th the compute of GPT-4.\nPrompt: What is GPT4?\nAssistant A: GPT4 is a large-scale language-trained transformer-based\nmodel.\nAssistant B: GPT4 can produce outputs.\n\u2018\u2018\u2018\nYour output should be:\n\u2018\u2018\u2018\n{\"reason\": \"The instruction asks what GPT4 is, and from the original\ntext, we know that GPT4 is a multimodal, large-scale model that can\ngenerate text. Therefore, Assistant A is the closer answer, while\nAssistant B did not follow the instruction well in providing a\nresponse.\", \"choice\": \"A\"}\n\u2018\u2018\u2018\nExample 2:\n\u2018\u2018\u2018\nText: Making language models bigger does not inherently make them better\nat following a user\u2019s intent. For example, large language models can\ngenerate outputs that are untruthful, toxic, or simply not helpful to\nthe user. In other words, these models are not aligned with their users.\nIn this paper, we show an avenue for aligning language models with user\nintent on a wide range of tasks by fine-tuning with human feedback.\nStarting with a set of labeler-written prompts and prompts submitted\nthrough the OpenAI API, we collect a dataset of labeler demonstrations\nof the desired model behavior, which we use to fine-tune GPT-3 using\nsupervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs\nfrom the 1.3B parameter InstructGPT model are preferred to outputs from\nthe 175B GPT-3, despite having 100x fewer parameters. Moreover,\nInstructGPT models show improvements in truthfulness and reductions in\ntoxic output generation while having minimal performance regressions on\npublic NLP datasets. Even though InstructGPT still makes simple\nmistakes, our results show that fine-tuning with human feedback is a\npromising direction for aligning language models with human intent.\n15\nPublished as a conference paper at ICLR 2024\nPrompt: Write a title for the above text.\nAssistant A: Improving Fine-Tuning for Language Models: A GPT-3-inspired\nApproach\nAssistant B: Training language models to follow instructions with human\nfeedback\n\u2018\u2018\u2018\nYour output should be:\n\u2018\u2018\u2018\n{\"reason\": \"This text discusses how to make large language models follow\nuser instructions better, and Assistant B\u2019s response is more in line\nwith the meaning of the text.\", \"choice\": \"B\"}\n\u2018\u2018\u2018\nExample 3:\n\u2018\u2018\u2018\nText: Though technology companies announced massive layoffs last year,\n2023 has been much worse. Layoffs have far outpaced last year\u2019s cuts, as\ntech giants including Amazon, Facebook parent company Meta, Microsoft,\nGoogle, IBM, SAP, and Salesforce - as well as many smaller companies -\nannounce sweeping job cuts. The problem: Big Tech went on a hiring binge\nduring the pandemic when lockdowns sparked a tech buying spree to\nsupport remote work and an uptick in e-commerce, and now they face\nrevenue declines. Although global IT spending is forecast to rise in\n2023, with enterprise software and IT services experiencing the greatest\ngrowth, the overall increase is expected to be modest, with data center\nsystems and communications services growing by less than 1%, according\nto market research firm Gartner. Meanwhile hardware sales are forecast\nto decline. Continuing supply chain issues, inflation, and the war in\nUkraine are also having an impact on both business and consumer\nspending, leading to fears of recession. According to data compiled by\nLayoffs.fyi, the online tracker keeping tabs on job losses in the\ntechnology sector, 784 tech companies have laid off about 208,181 staff\nso far this year, compared to 164,411 layoffs last year.\nPrompt: How many job losses have occurred in the technology sector so\nfar in 2023?\nAssistant A: 208,181 job losses have occurred in the technology sector\nso far in 2023.\nAssistant B: 208,181.\n\u2018\u2018\u2018\nYour output should be:\n\u2018\u2018\u2018\n{\"reason\": \"Both answers are acceptable and correct. They should be a\ntie.\", \"choice\": \"Tie\"}\n\u2018\u2018\u2018\nYour response should only be in the JSON format above; THERE SHOULD BE\nNO OTHER CONTENT INCLUDED IN YOUR RESPONSE. Write the \"reason\" key\nbefore writing the \"choice\" key, so that you think step-by-step before\nmaking your decision. KEEP YOUR REASONING BRIEF. Again, don\u2019t favor\neither A or B if they are both acceptable and correct -- judge a tie\ninstead.\nThe prompt that the GPT-4 uses to generate 128-token summary is as follows:\n\u201cWrite a summary for the above text.\nYour summary should not\nexceed 100 words but should include as much information of the\noriginal text as possible.\u201d\nWe show examples of the GPT-4 evaluation on a pretrained and a non-pretrained ICAE in Table 9.\n16\nPublished as a conference paper at ICLR 2024\nTable 9: Examples of outputs by the target LLM (i.e., Llama) conditioning on memory slots (k = 128)\nproduced by the pretrained and non-pretrained ICAE. The highlighted parts are not faithful to the\ncontext.\nPassage 1 (514 tokens):\nFrench senior civil servant arrested on suspicion of spying for North Korea\nNovember 27, 2018 by Joseph Fitsanakis\nA senior civil servant in the upper house of the French parliament has been arrested on suspicion of spying for North Korea,\naccording to prosecutors. The news of the suspected spy\u2019s arrest was first reported on Monday by Quotidien, a daily politics\nand culture show on the Monaco-based television channel TMC. The show cited \u201ca judicial source in Paris\u201d and said that\nFrance\u2019s domestic security and counterintelligence agency, the General Directorate for Internal Security (DGSI), was in charge\nof the espionage case.\n(omitted 233 words)\nThe senior civil servant faces up to 30 years in prison if found guilty of espionage.\nPrompt: What is the maximum prison sentence Quennedey could face if found guilty?\nAssistant A (pretrained ICAE): Quennedey could face up to 30 years in prison if found guilty.\nAssistant B (non-pretrained ICAE): Quennedey could face up to three years in prison if found guilty.\nAnswer (by the GPT-4): Up to 30 years.\nGPT-4 evaluation: Assistant A correctly states the maximum prison sentence from the text, while Assistant B provides an\nincorrect number.\nPassage 2 (506 tokens):\nWith Red Hat, IBM to become the leading hybrid cloud provider Watch Now\nAfter IBM acquired Red Hat, I suggested IBM paid $34 billion for the Linux power so it could become a hybrid-cloud power.\nWith the news that Red Hat will acquire NooBaa, a hybrid-cloud, data-storage company, it\u2019s become clearer than ever that the\nIBM-Red Hat deal is all about the hybrid cloud.\n(omitted 208 words)\nAs Ranga Rangachari, Red Hat\u2019s vice president of Storage and Hyperconverged Infrastructure, said in a statement:\n\"Data portability is a key imperative for organizations building and deploying cloud-native applications across private and\nmultiple clouds. NooBaa\u2019s technologies will augment our portfolio and strengthen our ability to meet the needs of developers\nin today\u2019s hybrid and multicloud world. We are thrilled to welcome a technical team of nine to the Red Hat family as we work\ntogether to further solidify Red Hat as a leading provider of open hybrid-cloud technologies.\nPrompt: Discuss the role of Ranga Rangachari in the context of the text\nAssistant A (pretrained ICAE): Ranga Rangachari is the vice president of Storage and Hyperconverged Infrastructure at Red\nHat. He is responsible for the company\u2019s storage and hyperconverged offerings, and he welcomes the nine-member team of\ntechnical experts from NoaBaa to join Red Hat as a part of the IBM acquisition.\nAssistant B (non-pretrained ICAE): Ranga Rangachari is the CEO of Red Hat, and he is excited about the acquisition of\nNooBaa as it will help Red Hat become a leader in hybrid cloud management. He believes that NooBaa\u2019s technology will\nenable Red Hat to provide a single, comprehensive platform for managing data across multiple clouds, and that this will be a\nsignificant differentiator for Red Hat in the market.\nAnswer (by the GPT-4): Ranga Rangachari is the vice president of Storage and Hyperconverged Infrastructure at Red Hat. He\nmade a statement regarding the acquisition of NooBaa, highlighting the importance of data portability in today\u2019s hybrid and\nmulticloud world and how NooBaa\u2019s technologies would strengthen Red Hat\u2019s ability to meet the needs of developers.\nGPT-4 evaluation: Assistant A correctly identifies Ranga Rangachari\u2019s role as the vice president of Storage and Hypercon-\nverged Infrastructure at Red Hat and accurately describes his statement about the acquisition of NooBaa. Assistant B incorrectly\nstates that Ranga Rangachari is the CEO of Red Hat.\n17\n"
  },
  {
    "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
    "link": "https://arxiv.org/pdf/2307.06942.pdf",
    "upvote": "20",
    "text": "InternVid: A Large-scale Video-Text Dataset for\nMultimodal Understanding and Generation\nYi Wang\u22171, Yinan He\u22171, Yizhuo Li\u22174,1, Kunchang Li6,1, Jiashuo Yu1, Xin Ma3,1, Xinhao Li2,1\nGuo Chen3,1, Xinyuan Chen1, Yaohui Wang1, Conghui He1, Ping Luo4,1, Ziwei Liu5,1\nYali Wang\u20206,1, Limin Wang\u20202,1, Yu Qiao\u20201\n1OpenGVLab, Shanghai AI Laboratory\n2Nanjing University\n3Monash University\n4The University of Hong Kong\n5Nanyang Technological University\n6Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\nhttps://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid\nAbstract\nThis paper introduces InternVid, a large-scale video-centric multimodal dataset\nthat enables learning powerful and transferable video-text representations for mul-\ntimodal understanding and generation. The InternVid dataset contains over 7\nmillion videos lasting nearly 760K hours, yielding 234M video clips accompanied\nby detailed descriptions of total 4.1B words. Our core contribution is to develop\na scalable approach to autonomously build a high-quality video-text dataset with\nlarge language models (LLM), thereby showcasing its efficacy in learning video-\nlanguage representation at scale. Specifically, we utilize a multi-scale approach\nto generate video-related descriptions. Furthermore, we introduce ViCLIP, a\nvideo-text representation learning model based on ViT-L. Learned on InternVid via\ncontrastive learning, this model demonstrates leading zero-shot action recognition\nand competitive video retrieval performance. Beyond basic video understanding\ntasks like recognition and retrieval, our dataset and model have broad applications.\nThey are particularly beneficial for generating interleaved video-text data for learn-\ning a video-centric dialogue system, advancing video-to-text and text-to-video\ngeneration research. These proposed resources provide a tool for researchers and\npractitioners interested in multimodal video understanding and generation.\n1\nIntroduction\nLearning transferable video-text representations is both challenging and essential for video under-\nstanding in various real-world applications such as autonomous driving, intelligent surveillance,\nhuman-computer interaction, and visual searching. While multimodal contrastive learning using\nweb-scale data has been successful in image-text representation, it remains underexplored in the\nvideo-language domain.\nA key reason for this limited exploration is the lack of a high quality video-language dataset for\npretraining at scale. Current research relies on datasets like HowTo100M [2], HD-VILA [3], and YT-\nTemporal [4, 5], whose texts are generated using automatic speech recognition (ASR). Despite their\nlarge scale, these datasets often have low semantic correlations between the videos and corresponding\ntextual descriptions [2\u20135]. Empirical studies demonstrate that improving this correlation (e.g. aligning\nvideos with subtitles to improve their matching) significantly benefits downstream tasks such as video\nretrieval and video question answering [6]. Recent works have utilized WebVid10M [6], a dataset\n* Equal contribution.\n\u2020 Corresponding authors.\narXiv:2307.06942v2  [cs.CV]  4 Jan 2024\nolder women sitting at a table eating a bowl of \nfood with chopsticks.\n\u8001\u7956\u5a46\u5c31\u662f\u8fd9\u70b9\u597d\uff0c\u4e0d\u6311\u98df\u3002\u5565\u5b50\u90fd\u5403\u5f97\u3002\u8001\u7956\u5a46\u5750\u8f66\u54ea\u91cc\u90fd\u80fd\u53bb\u3002\n(The old granny is good in this aspect, she is not picky and can eat \nanything. She can go anywhere by car.)\nwomen dressed in traditional costumes standing\nin a room.\n[\u043c\u0443\u0437\u044b\u043a\u0430] [\u0430\u043f\u043b\u043e\u0434\u0438\u0441\u043c\u0435\u043d\u0442\u044b]\n([Music] [Applause])\na man and a woman brushing their teeth in the \nbathroom.\n\u5206\u304b\u3063\u3066\u308b\u308f\u3088\u5f85\u3063\u3066\u30c6\u30a3\u30c3\u30b7\u30e5\u306f\u3069\u3053\u30b7\u30e3\u30ef\u30fc\u3092\u8cb7\u3063\u3066\u9583\u3044\u305f\n\u307e\u305a\u306f\u3053\u3093\u306a\u98a8\u306b\u5c45\u308b\u306e (I understand, just wait. Where are the \ntissues? I had an idea to buy a shower, but for now I\u2019ll stay like this.)\na woman in a red wagon with a child in it in a \nstore as she walks through the store.\ndrive a car out.\nFigure 1: Examples (we give three frames of each video clip), the corresponding generated captions,\nand ASR transcripts in InternVid. In the captions, we highlight nouns in blue and verbs in green.\nNon-English transcripts are translated to English using LLM [1].\nwith higher-quality alt-texts, to address the low video-text correlation issue. However, its limited\nscale and dynamics hinder its use in current data and model scaling studies. Specifically, only 10M\nvideo-text pairs are provided, and the depicted scenes contain relatively few actions or activities.\nWe propose a large-scale video-centric dataset InternVid to address the challenge of scaling up video-\nlanguage modeling while maintaining high video-text correspondence. Visual examples are given\nin Figure 1. Note the ASR transcripts barely depict visual elements in videos while the generated\ncaptions do. The dataset contains highly-correlated video-text pairs and includes over 7 million\nvideos, totaling 760,000 hours and resulting in 234 million video clips, with various subsets for\ndifferent needs. These videos cover 16 scenarios and around 6,000 motion descriptions. To improve\nvideo-text matching, we generate captions using a multiscale approach. In the coarse scale, we\ncaption the middle frame of each video and use the description as the video caption. In the fine scale,\nwe produce frame-by-frame captions and summarize them with a language model.\nLeveraging InternVid, we scale a video-language transformer (ViT-L) in contrastive learning from a\ndata perspective, and its experiments prove InternVid enables learning scalable video-text models.\nWe introduce video masking to the model to accelerate the whole learning without compromising its\neffectiveness. The video and text encoders are initialized from the CLIP pretrained model with the\nsame scale. With InternVid, we learn a video-text model for several epochs, achieving impressive\nzero-shot performance. Compared with previous Video CLIP variants, our proposed ViCLIP shows\nnotable performance improvement, especially in zero-shot settings.\nIn addition to large-scale video-language contrastive pretraining, we discover its effectiveness in\nproducing interleaved video-text data for learning a video-centric dialogue system like Flamingo\n[7, 8], and advancing video generation. Since the text-annotated clips are extracted from videos, we\nnaturally collect clips and their corresponding text based on the sampling locations. This results\nin approximately 7 million interleaved data pieces, suitable for instruction tuning as multi-turn\nvideo-centric dialogue. For video generation, we filter the core set and obtain 18 million video\nclips. Alongside WebVid-10M, InternVid can significantly improve a stable-diffusion based video\ngeneration model to new heights.\nIn summary, our contributions are threefold.\n\u2022 We introduce a new web-scale video-language dataset InternVid. This dataset, aimed at advancing\nvideo-related multimodal understanding and generation at scale, is created using a multi-scale\nvideo captioning approach powered by LLM, ensuring high-quality video-text data with minimal\nhuman intervention. InternVid has 7 million videos, corresponding to 234 million clips each with\nthe generated captions. Spanning 16 scenes and about 6 thousand actions, the dataset includes\ncomputational features (video-text correlation and visual aesthetics) across the entirely of the\ndataset and gives way to diverse subsets to cater to varying training needs.\n\u2022 We learn a new video-language model, ViCLIP, which is trained on InternVid using ViT-L. It\nincorporates both constrastive learning and mask modeling techniques, allowing for efficient\n2\nlearning of transferrable video-language representation. This model achieves state-of-the-art zero-\nshot action recognition in Kinetics, scoring 75.7, 73.5, and 66.4 on K400, K600, and K700 with\nthe average top1 and top5 accuracies, respectively. It also gets competitive performance on video\nretrieval, setting a new baseline for video-text understanding.\n\u2022 InternVid fosters the development of multimodal dialogue systems and text-to-video generation.\nThe proposed ViCLIP learned on InternVid could serve as a vision backbone of video-centric\ndialogue systems[9\u201311], conducting tasks as action recognition, temporal understanding, reasoning,\nand creativity within an open-ended environment. Furthermore, we provide a subset, InternVid-\nAesthetics, created using specific video-text relation and visual aesthetic filtering. This subset aids\nin generating high-resolution watermark-free videos. Utilizing InternVid-Aesthetics, both visual\nand quantitative outcomes of a simple text-to-video baseline can be noticeably enhanced (FVD:\n705.3 -> 616.5).\n2\nRelated Work\nMultimodal Datasets.\nVision-text data pairs are necessary to enable crossmodal learning. To learn\nvison-language representation effectively, these datasets should be large at scale and high at vision-\ntext correlations. To this end, researches usually leverage existing web images with alt-text [12\u201318]\nand videos with ASR transcriptions [2, 4, 5, 3, 6, 19, 20] for scalable learning. With LAION-5B\u2019s\nintroduction [19], researchers now have access to hundreds or millions or billions of image-text pairs,\nopening up new avenues for research on large-scale image-language pretraining.\nFor video-centric multimodal datasets, HowTo100M [2] collected instructional YouTube videos and\nexploited the corresponding ASR subtitles for learning joint representations. Zellers et al. [4, 5]\nand Xue et al. [3] proposed YT-Temporal and HD-VILA for Audio-Visual-Language joint learning\nand high-resolution video crossmodal learning, respectively. On the other hand, Bain et al. [6]\nfound video-text alignment matters more than their quantities, so they produced WebVid [6] where\n10M videos with the corresponding alt-texts. This is frequently employed in recent video-language\npretraining approaches [21]. Similarly, based on CC3M, Nagrani et al. proposed VideoCC3M [22]\nby transferring captions from image-text datasets to video ones. In this work, we target to present a\nlarge-scale video-language dataset with high-quality descriptions.\nVideo Understanding.\nPretraining large-scale video-text models and fine-tuning them for down-\nstream tasks has become the norm in the video-language field [23\u201325, 21, 26, 25, 15, 27\u201333, 4, 5, 34\u2013\n36]. Early techniques [30, 31] used pretrained visual and language encoders to obtain offline video\nand text features, but recent methods [24, 23, 15, 27, 37, 38] highlight the advantages of end-to-\nend training. Common practices include two or three pretraining tasks, such as masked language\nmodeling [39], video-text matching [40], video-text contrastive learning [25, 32], masked video\nmodeling [37, 38, 32], and video-text masked modeling [41].\nIn the multimodal video context, VIOLET [41] combined masked language and video modeling, while\nAll-in-one [40] proposes a unified pretraining approach with a shared backbone, and LAVENDER [39]\nunified tasks through masked language modeling. Despite their success in multimodal benchmarks,\nthese methods\u2019 reliance on limited video-text data hampers performance in video-only tasks like\naction recognition. Conversely, InternVideo [32] and UMT [21] combined masked modeling with\ncrossmodal contrastive learning, leading to competitve performance in both video-only and video-\nlanguage tasks. MERLOT Reserve [5] exploited 20 million video-text-audio pairs for training joint\nvideo representations using contrastive matching, setting new standards in video recognition and\nvisual commonsense reasoning. VALOR [42] also employed different modality encoders for video,\naudio, and text processing, and introduces video-to-text and audio-to-text pretasks to improve vision-\naudio-language learning. To address modality entanglement in crossmodal learning, mPLUG-2 [43]\nintroduced a shared module across image, video, and text to encourage modality collaboration while\nreserving modality-specific modules for their differences. Similar to [32, 26], VLAB [44] adapted\na CLIP-pretrained ViT to model spatiotemporal variations and blends it with CLIP ViT with cross\nattention for handling both images and videos.\n3\nInternVid: A Video-Centric Multimodal Dataset\nA high-quality video-text dataset at scale is a premise to conduct large-scale video-language learning\nand associated tasks. We identify three crucial factors in constructing this dataset: substantial\n3\nDataset\nCaption\nDomain\n#Videos\n#Clips\nLenClip\nLenCap\nDur(h)\nRes\nMSR-VTT [45]\nManual\nopen\n7.2K\n10K\n15.0\n9.3\n40\n240P\nDideMo [46]\nManual\nFlickr\n10.5K\n27K\n6.9\n8.0\n87\n-\nLSMDC [47]\nManual\nmovie\n200\n118K\n4.8\n7.0\n158\n1080P\nYouCook2 [48]\nManual\ncooking\n2K\n14K\n19.6\n8.8\n176\n-\nHow2 [49]\nManual\ninstruct\n13.2K\n80K\n90.0\n20.0\n2K\n-\nANet Caption [50]\nManual\naction\n20K\n100K\n36.0\n13.5\n849\n-\nVideoCC3M [22]\nTransfer\nopen\n6.3M\n10.3M\n10\n-\n17.5K\n-\nWebVid10M [6]\nAlt-text\nopen\n10.7M\n10.7M\n18.0\n12.0\n52K\n360P\nWTS70M [51]\nMetadata\naction\n70M\n70M\n10\n-\n194K\n-\nHowTo100M [2]\nASR\ninstruct\n1.2M\n136M\n3.6\n4.0\n134.5K\n240P\nHD-VILA-100M [3]\nASR\nopen\n3.3M\n103M\n13.4\n32.5\n371.5K\n720P\nYT-Temporal-180M [4]\nASR\nopen\n6M\n180M\n-\n-\n-\n-\nInternVid (ours)\nGenerated\nopen\n7.1M\n234M\n11.7\n17.6\n760.3K\n720P*\nTable 1: Statistics of InternVid and its comparison with existing video-language datasets. *In\nInternVid, most videos (around 85%) are in 720P and the remaining are in from 360P to 512P.\ntemporal dynamics, rich and diverse semantics, and strong video-text correlations. To ensure high\ntemporal dynamics, we gather videos retrieved using action/activity-based query words. For rich and\nvaried semantics, we not only crawl trending videos across various categories but also deliberately\nincrease the proportion of data consciously collected from various countries and languages. To\nstrengthen video-text correlations, we employ image captioning and language models to generate\nvideo descriptions from frame-specific annotations. Next, we elaborate the dataset construction\nprocess and discuss its statistics and characteristics.\n3.1\nData Curation\nWe collect videos from YouTube considering the diversity and richness of its data, and its support\nfor academic usage. Totally we obtain 7 million public YouTube videos with an average duration of\n6.4 minutes, covering 16 topics. We ensure the uniqueness of our dataset by creating a database of\nYouTube video IDs and excluding any videos already present in publicly available datasets (released\nprior to April 2023). The data curation strategies are two-fold. On one hand, We select popular\nchannels and the corresponding hot or high-rated videos from the categories e.g. news, gaming, etc.,\nresulting in 2 million videos. On the other hand, we create a list of verbs related to actions/activities.\nWith it, we also obtain 5.1 million videos by choosing the top retrieved ones.\nDefining Actions in Kinetics & Motives for Queries.\nWe define around 6.1K action phrases from\nAmerican Time Use Survey (ATUS), public video datasets, and text corpus. Then they are refined\nboth manually and automatically. We employ actions from ATUS from 2017 to 2022 [52], merging\nthem and removing the duplicates. For the referenced public video data, we leverage Kinetics [53],\nSomethingSomething series [54, 55], UCF101 [56], and so on. This provides us with 1103 action\nlabels. Moreover, we access several visual grounding corpus [57\u201359]. A language model [1] is\nemployed to extract actions and their corresponding targets (if exist) to form phrases from the corpus,\nleading to 5001 actions with manual checking. Totally, we collect 6104 action queries for searching\nvideos on YouTube.\nCollection Strategies.\nTo ensure the quality of our dataset, we established specific crawling rules.\nWe only collected videos that were between 10 seconds and 30 minutes in duration and had resolutions\nranging from 360P to 720P. Videos with resolutions below 360P were excluded, and those above\n720P were either downloaded in their 720P version or resized to 720P. In this process, we prioritize\nthe highest available resolution. To provide a comprehensive mutimodal dataset, we gather videos\nalong with their audio, subtitles, titles, and summaries. Captions for the videos were generated\nautomatically using a video captioning pipeline described in Section 3.2.\nIn formation, the collected multimodal data contain videos V, their audios A, metadata (title Wtitle,\nvideo descriptions Wcontent, query words Wquery, tags Wtag, etc), subtitles (user generated contents\nor auto-generated ones), and more. Each video V could be treated as a sequence of clips {Ci}i=1,2,...,\nand we can segment their corresponding audio as {Ai}i=1,2,... and ASR subtitles as {Wasr\ni }i=1,2,....\nFor the metadata, we suppose clips share the same meta when they are sampled from the same video.\nTrimming.\nWe segment videos (lasting an average of 5 minutes) into clips (for around 10 seconds)\nusing scene variance. For starters, videos are cut into shorter ones based on their scene changes.\n4\nthe food is \nprepared and \nready to be eaten\nImage Captioning Model (BLIP2, Tag2Text)\na person cooking \nsome food on a \nstove top\na person cooking \nsome food in a \nfrying pan.\na tab\nwith lo\na person putting \nfood in a basket\nLanguage Model\na person is cooking some food in a frying pan.\nSampled \nFrames\nFrame \nCaptions\nVideo \nCaptions\na table topped \nwith lots of food\nA person cooks various dishes on a stove top and frying pan, prepares the food, sets a table with lots of \nfood, and puts some of it in a basket.\nFigure 2: The proposed multiscale video caption pipeline. The captions in coarse and fine scales are\nmarked in green and dark green, respectively.\nWe directly employ the corresponding filter in PySceneDetect 1 with a threshold as 27. During this\nprocedure, we also filter out clips in still or extreme dynamics (e.g. a browse of a photo gallery).\nAfter the filtering, we get total 234M video clips whose durations range from 2s to more than 30s.\n3.2\nMultiscale Video Captioning\nTo generate video captions that are scalable, rich, and diverse, we employ a multiscale method\nwith two distinct captioning strategies, as depicted in Figure 2. On the finer scale, we simplify the\nvideo captioning process by concentrating on the common objects, actions, and scene descriptions\nwithin the video clip. We deliberately overlook intricate details such as subtle facial expressions\n& movements, and other nuanced elements. On the coarser scale, we adopt the single-frame bias\nassumption from [60] and exclusively caption the central frame of the video. Given our focus on\nbrief clips (around 10 seconds) filtered via scene segmentation, most videos predominantly display\nconsistent objects without substantial appearance alterations. This circumvents the identity-preserving\nissue when dealing with videos from image perspectives. Technically, we employ the lightweight\nimage captioning model Tag2Text [61] for the finer scale, which describes videos at low fps in a\nframe-by-frame manner. These individual image captions are then synthesized into a comprehensive\nvideo description using a pretrained language model [62, 63]. At the coarser scale, we use BLIP2\n[64] to caption the middle frame of the clip.\n3.3\nStatistics and Features\nWe present the key statistics of InternVid with other popular video-language datasets in Table 1. More\ndetailed ones are given below.\nDiversity & Richness.\nWe collected videos from 16 popular categories with varying percentages,\nas illustrated in Figure 3. Unlike prior studies [2\u20134], we ensured diversity by selecting videos from\ncountries with different languages instead of relying on a dominant language environment. The\ncountries we sampled from include the UK, USA, Australia, Japan, Korea, China, Russia, and France,\namong others. In terms of duration, every video lasts 351.9s on average. Almost half (49%) of\nthe videos are five minutes or less, while a quarter (26%) fall between five and ten minutes. Only\n8% of the videos are over 20 minutes long. Among the curated videos, 85% were high-resolution\n(720P), while the remaining 15% had lower resolutions ranging from 360P to 720P. Although the\nlower-resolution videos may not perform as well as the high-resolution ones in content generation\ntasks, they can still be useful in video-language representation learning, provided that they have\nappropriate captions.\nInternVid exhibits diverse clip durations and caption lengths in the segmented clip level. The aesthetic\nscores and clip-caption similarities are distributed uniformly, as shown in Figure 4. The majority of\nclips are 0-10 seconds in length, accounting for 85% of all clips (Figure 4: left). Approximately half\nof the clips have captions with 10-20 words, while one-third of the clip captions have fewer than 10\nwords. About 11% of clips have long captions with more than 20 words.\n1https://github.com/Breakthrough/PySceneDetect\n5\n15%\n5%\n16%\n8%\n18%\n18%\n16%\n4%\njp\nth\nkor\nfra\nen\nzh\nru\nOther 4 languages\n29%\n14%\n10%\n9%\n9%\n5%\n5%\n5%\n14%\nPeople & Blogs\nEducation\nNews & Politics\nHowto & Style\nEntertainment\nGaming\nScience & Technology\nSports\nOthers 11 Categories\n49%\n26%\n17%\n8%\n<5min\n5min~10min\n10min~15min\n15min~20min\n>20min\nVideo Countries\nVideo Durations\nVideo Categories\nFigure 3: Video statistics in InternVid. It encompasses a diverse set of categories, gathered from\nmultiple countries and averaging a duration of five minutes.\n36%\n26%\n23%\n8%\n0.0s~2.0s\n2.0s~4.0s\n4.0s~10.0s\n10.0s~20.0s\n>20s\n7%\nClip  Durations\n34%\n55%\n4% 7%\n0~10words\n10~20words\n20~30words\n>30words\nClip  Caption Length\n0\n0.15\n0.3\n0.45\n0\n0.3\n0.6\n0.9\nUMT-SIM & Aesthetic \nUMT-SIM\nAesthetic Score\nFigure 4: Clip statistics in InternVid. InternVid contains a diverse distribution of clip durations and\ncaption lengths. It also offers aesthetic scores and multimodal similarity scores for each clip.\nWe measured the aesthetic scores of all clips using an open-source model [19]. We uniformly sampled\nfour frames of each clip, calculated their aesthetic scores, and took the maximum score as the video\naesthetic score. For clip-caption similarity computation, we used a video-language model called\nUMT [21]. We computed the cosine similarity between video embeddings and text embeddings,\nagain using a uniform sampling of four frames for each clip. Most clips score around 4-6 in terms\nof aesthetics, accounting for approximately 75% of the data. For UMT-SIM, over 80% of the clips\nscored between 0.3-0.4, with the remaining clips scoring around 0.2-0.3 or 0.4-0.5. Based on these\ncomputed aesthetics and UMT-SIM scores, we can generate different versions of InternVid to meet\nvarious requirements.\nActionness.\nIn terms of actionness, the InternVid dataset contains about ten times more verbs than\nthe WebVid10M dataset. To evaluate this, we used the NLTK toolkit to analyze the number of verbs\nin captions, focusing on extracting and tagging all unique verbs. We found a total of 109,485 verbs\nin the WebVid10M caption dataset, while the InternVid dataset contained 212,155 unique instances\nof verbs. While these counts may not be entirely accurate due to our simple counting method, we\nbelieve they provide a rough indication of the actionness of the two datasets.\n3.4\nInterleaved Video-Text Data Generation\nUtilizing the created video captions, we can develop an integrated video-text dataset for in-context\nvideo learning, allowing video-based sequence models to perform new tasks without additional\ntraining. Previous research, such as Flamingo [7, 8], Kosmos-1 [65], and Multimodal C4 [66],\nconfirms that pretraining on the interleaved image-text sequences results in significant multimodal\nin-context abilities. To the best of our knowledge, a large-scale interleaved video-text dataset has not\nyet been established. Our work represents the initial step in creating and making it publicly available.\nWe create InternVid-ICL, containing 7.1M interleaved video-text data pairs. We propose three distinct\nmethods for organizing clips and their captions:\n\u2022 Arrange clips and their descriptions sequentially based on their temporal order within the same\nvideo, as illustrated in Figure 5 (a).\n\u2022 Enhance diversity in interleaved video-text items by assigning ASR text to a used clip in addition\nto its caption, as demonstrated in Figure 5 (b).\n6\nFigure 5: Interleaved video-text data generation in InternVid with three formats.\nFigure 6: Framework of ViCLIP.\n\u2022 Extend method 1 by concatenating two interleaved multimodal items, creating a video-centric\ndialogue simulating user queries involving multiple videos (Figure 5 (c)).\n[..., \"the inside of a home has a rug and a light on.\", \"\u266a We could\nleave the Christmas lights up til January \u266a\", ..., \"woman with blond\nhair playing guitar\", \"\u266a Have I known you 20 seconds or 20 years?\n\u266a\",\n, \"close-up of a bathroom sink with\nsoap bubbles and other items\", \"a bathroom is seen with a sink and\ntwo lights\", \"a woman swiming inside of a fishbowl with a ladder and\na man\", \"\u266a Can I go wher you go?\n\u266a\",\n, \"devils roll the dice, angels roll their eyes\",\"\u266a And, take me\nout, and take me home \u266a\" ,..., \"the man is standing in a room with\npink carpet\",\"\u266a You\u2019re my, my \u266a\", \"a woman in yellow is dancing\nwith a man in a red room\", \"\u266a My, My lover \u266a\",\n, \"a woman is sitting on a chair, playing\na guitar and a woman holding a balloon\", \"\u266a \u266a \u266a\", \"two men smiling\nwhile holding wine glasses and drinking beer\", \"\u266a We could let our\nfriends crash in the living room \u266a\" ...]\nTable 2: Interleaved video-text data format (b) in InternVid. The caption and ASR transcript of\neach clip is shown in black and gray, respectively. We can achieve interleaved video-text data format\n(a) by abandoning ASR transcripts. To obtain data format (c), we concatenate multiple videos with\ninterleaved video-text data (a).\nOne visual example of these arrangements is provided in Table 9.\n4\nViCLIP: Learning Video-Text Representation at Scale\nBuilt upon CLIP [67], we make a simple video-text pretraining baseline ViCLIP. It consists of a\nvideo encoder (ViT) [68] and a text encoder, as given in Figure 6. Both modules are initialized\n7\nMethod\nTraining Data\nK400\nK600\nK700\ntop-1 (\u2191) AVG (\u2191) top-1 (\u2191) AVG (\u2191) top-1 (\u2191) AVG (\u2191)\nCLIP\nCLIP400M\n58.42\n70.14\n55.11\n67.16\n46.12\n58.38\nCLIP\nDataComp-1B\n56.14\n67.67\n54.15\n65.83\n45.36\n57.01\nEVA-CLIP-L Merged-2B\n-\n65.00\n-\n64.90\n-\n59.10\nEVA-CLIP-E LAION-2B\n-\n69.80\n-\n69.30\n-\n63.40\nViCLIP\n+WebVid10M\n59.88\n71.03\n58.66\n69.84\n50.23\n61.86\nViCLIP\n+InternVid-10M\n56.68\n68.17\n54.67\n66.28\n46.53\n58.73\nViCLIP\n+InternVid-50M\n57.18\n68.93\n55.36\n67.07\n47.00\n59.36\nViCLIP\n+InternVid-200M\n59.80\n71.09\n57.80\n69.34\n49.30\n61.25\nViCLIP\n+InternVid-10M-DIV 63.00\n74.15\n60.68\n72.07\n52.50\n64.59\nViCLIP\n+InternVid-10M-FLT 64.80\n75.70\n62.20\n73.53\n54.30\n66.38\nTable 3: Zero-shot action recognition results on Kinetics 400/600/700.\nfrom the corresponding CLIP components. We update the native attention in the video encoder to\nspatiotemporal attention while maintaining other design elements. For efficient learning, we apply\nmasking to videos in pre-training. The optimization target is the contrastive loss between input video\nand text embeddings.\nVideo & Text Encoders with Masking Learning.\nOur video encoder uses a standard ViT with\nspatiotemporal attention. We apply random patch masking following MAE-based methods [37, 38]\nto the input videos. It significantly alleviates the computational burden. The used text encoder is also\na transformer followed by [67, 19].\nUnmasked Video-Text Pretraining.\nWe feed all visual tokens into the video transformer instead\nof just the masked ones towards the end of the pretraining process. This helps bridge the gap between\npretraining and downstream applications where the full video is used as input. We perform unmasked\ntraining for 0.5 epochs with a learning rate of 4e-6.\nTraining Objectives. Our framework optimizes video-text alignment. It minimizes InfoNCE loss\n[69] using global video and text features, as\nLC = LV\u2192T\nC\n+LT\u2192V\nC\n= \u2212\nN\nX\ni=1\nlog\nexp(sim(f V\ni , f T\ni )/\u03c4)\nPN\nj=1 exp(sim(f V\ni , f T\nj )/\u03c4)\n\u2212\nN\nX\ni=1\nlog\nexp(sim(f T\ni , f V\ni )/\u03c4)\nPN\nj=1 exp(sim(f T\ni , f V\nj )/\u03c4)\n, (1)\nwhere f V and f T denote the learned video and text embeddings, respectively. sim(\u00b7) computes the\ncosine similarity between two features. \u03c4 is the learnable temperature.\nImplementation.\nViCLIP is learned with 64 NVIDIA A100 GPUs for 3 days with 50M video-text\npairs. We introduce DeepSpeed and FlashAttention [70] for training and inference acceleration.\nWe learn ViCLIP on five subsets of InternVid and evaluated its performance on popular video-related\nbenchmarks using full-finetuned and zero-shot settings. We sample subsets InternVid-10M, InternVid-\n50M, and InternVid-200M randomly. For InternVid-10M-DIV, we prioritize to sample clips from\ndifferent videos first, then we sample clips with varying probabilities according to the video length\nwhere they are extracted. The longer their source video is, the lower chance they are sampled. For\nInternVid-10M-FLT, we employ the sampling strategy of InternVid-10M-DIV and select clips with\nUMT-SIM scores ranking among the top 30% to ensure high quality.\n4.1\nTransferable Video Representation Performance\nAction Recognition. In addition to OpenAI\u2019s CLIP-L (CLIP400M [67]) and LAION (DataComp-\n1B [71]), we also include EVA-CLIP-L/14 and EVA-CLIP-E/14 [72] for comparison. More experi-\nmental settings are given in App. E.1.\nZero-Shot. Table 3 shows that when trained on InternVid-10M-FLT, ViCLIP outperforms all other\nmethods, including EVA-CLIP-E. This result validates InternVid\u2019s effectiveness in learning video-\ntext embeddings. Note that ViCLIP with InternVid-10M-FLT sets new records on zero-shot action\nrecognition in Kinetics 400/600/700, demonstrating a significant performance boost compared to\nViCLIP with WebVid10M or other models. Moreover, ViCLIP trained on InternVid-10M-FLT\nexceeds its performance on InternVid-200M. Normally, we would expect the model trained on\nInternVid-200M to perform better than those on -10M-DIV or -FLT, given that the latter two subsets\n8\nMethod\nTraining Data\nK400\nSthSthV2\ntop-1 (\u2191)\ntop-5 (\u2191)\ntop-1 (\u2191)\ntop-5 (\u2191)\nCLIP\nCLIP400M\n86.7\n97.2\n70.1\n92.5\nCLIP\nDataComp-1B\n85.6\n96.8\n68.9\n91.8\nViCLIP\n+WebVid10M\n85.0\n96.8\n68.7\n91.9\nViCLIP\n+InternVid-10M-FLT\n86.8\n97.5\n71.2\n93.2\nViCLIP\n+InternVid-10M-FLT+K710\n88.0\n97.8\n71.8\n93.6\nViCLIP\n+InternVid-200M\n87.9\n97.9\n73.6\n94.9\nViCLIP\n+InternVid-200M+K710\n88.7\n98.2\n74.2\n95.0\nTable 4: Fine-tuned action recognition results on Kinetics 400 and SomethingSomethingV2.\nMethod\nData\nMSR-VTT\nLSMDC\nDiDeMo\nMSVD\nANet\nT2V\nV2T\nT2V\nV2T\nT2V\nV2T\nT2V\nV2T\nT2V\nV2T\nCLIP\nCLIP400M\n29.0\n25.8\n13.9\n15.2\n11.5\n19.1\n37.9\n60.0\n8.3\n12.2\nCLIP\nDataComp-1B\n30.4\n24.2\n13.9\n11.9\n12.7\n18.7\n40.5\n57.2\n9.1\n13.2\nCLIP4Clip [73]\n+HowTo100M\n32.0\n-\n15.1\n-\n-\n-\n38.5\n-\n-\n-\nViCLIP\n+WebVid10M\n35.6\n33.1\n16.5\n13.4\n14.5\n23.3\n45.3\n69.0\n12.4\n19.0\nViCLIP\n+InternVid-10M\n36.4\n37.1\n17.1\n15.0\n16.4\n25.9\n45.2\n69.8\n13.5\n23.4\nViCLIP\n+InternVid-50M\n39.7\n40.7\n18.0\n16.7\n16.7\n26.4\n46.5\n72.2\n13.6\n23.2\nViCLIP\n+InternVid-200M\n39.3\n39.5\n18.3\n16.6\n17.1\n25.5\n47.3\n70.0\n13.7\n21.6\nViCLIP\n+InternVid-10M-DIV\n41.5\n41.6\n18.5\n17.4\n17.7\n26.2\n48.6\n71.9\n14.8\n23.4\nViCLIP\n+InternVid-10M-FLT\n42.4\n41.3\n20.1\n16.9\n18.4\n27.9\n49.1\n75.1\n15.1\n24.0\nTable 5: Results of zero-shot video retrieval on MSR-VTT, LSMDC, DiDeMo, MSVD, and ANet.\nderive from the former. Unless this discrepancy results from improper learning, we conjecture that\nfalse negative samples could severely impede video-text contrastive learning if we don\u2019t purposefully\nreduce the number of clips taken from the same video. Specifically, we hypothesize that clips from\nthe same video share similar representations and captions. Contrastive learning, however, assumes\nthese clips to be different. This situation also undermines the significance of using a large batch size\nin current training since it increases the probability of encountering more false negatives. We believe\nthis assumption is applicable to other video tasks as well and plan to explore this further in the future.\nFine-tuned. In Table 4, note when comparing ViCLIP trained on InternVid with image CLIP models or\nViCLIP trained with WebVid, there is a clear increase in accuracy. Unlike the zero-shot results, when\nViCLIP is pretrained with a larger number (200M) of video-text data pairs, it achieves higher accuracy\nin fine-tuned recognition tasks (87.9% in K400 and 73.6% in SthSthV2) compared to when pretrained\n(86.8% in K400 and 71.2% in SthSthV2) with fewer data (10M). This suggests that InternVid provides\ngreater benefits for fine-tuned action-related tasks. The decrease in performance of ViCLIP with\nWebVid highlights the importance of addressing the distribution gap between WebVid and the action\nvideos used for evaluation, emphasizing the need to collect videos with evident temporal dynamics.\nVideo-Text Retrieval. We evaluate the video retrieval performance of baselines and ViCLIP using\ndifferent pretraining datasets on five popular benchmarks [52, 45, 74, 46, 75], as shown in Table 5\nand 6. We uniformly sample eight frames from the input videos. For the CLIP models from OpenAI\n[67] and LAION [19], we utilize their officially released ViT-L models and extract video embeddings\nby averaging the computed frame-wise image embeddings. Our ViCLIP directly predicts video\nembeddings. For evaluating retrieval performance, we report R@1 scores for both text-to-video (t2v)\nand video-to-text (v2t) tasks in 5 and 6.\nBoth Table 5 and 6 demonstrate that video-language pretraining is crucial for enhancing fine-tuned\nand zero-shot retrieval performance. This point is substantiated by the comparison between CLIP\nand ViCLIP using InternVid-50M. Table 5 exhibits a boost of nearly 4-10 points across different\nbenchmarks in the zero-shot setting. Meanwhile, Table 6 shows an increase of approximately 10\npoints across all R@1 scores in the fine-tuned setting.\nZero-Shot. Table 5 reveals InternVid-10M outperforms WebVid when employing the same method,\nViCLIP, with an average increase of 6.3% in R@1 across nearly all benchmarks. This improvement\ncan be further amplified by diversifying the training clips used, as InternVid-10M-DIV and -FLT\nsurpass WebVid on ViCLIP with gains in R@1 of 14.0% and 17.1%, respectively. These results\nunderline, once again, the effectiveness of the correspondence between our generated video captions\n9\nMethod\nData\nMSR-VTT\nLSMDC\nDiDeMo\nMSVD\nANet\nT2V\nV2T\nT2V\nV2T\nT2V\nV2T\nT2V\nV2T\nT2V\nV2T\nCLIP\nCLIP400M\n38.2\n38.7\n22.5\n22.6\n32.2\n33.9\n67.3\n69.9\n26.1\n26.9\nCLIP\nDataComp-1B\n37.2\n37.5\n18.7\n18.5\n33.5\n34.2\n66.3\n70.2\n24.5\n25.8\nCLIP4Clip [73]\n+HowTo100M\n45.6\n45.9\n24.3\n23.8\n43.0\n43.6\n45.2\n48.4\n40.3\n41.6\nViCLIP\n+WebVid10M\n50.8\n49.3\n27.3\n28.4\n48.1\n48.5\n76.7\n81.2\n44.5\n43.2\nViCLIP\n+InternVid-10M\n51.8\n49.7\n28.5\n29.4\n49.5\n50.6\n77.2\n80.0\n49.7\n48.4\nViCLIP\n+InternVid-50M\n52.8\n52.2\n30.9\n30.9\n49.4\n48.7\n78.1\n80.0\n49.7\n49.0\nViCLIP\n+InternVid-200M\n53.7\n53.4\n29.3\n31.3\n51.1\n50.8\n79.9\n78.4\n52.8\n51.1\nViCLIP\n+InternVid-10M-DIV\n55.0\n53.3\n32.0\n30.0\n51.7\n52.1\n75.8\n77.8\n50.4\n48.9\nViCLIP\n+InternVid-10M-FLT\n52.5\n51.8\n33.0\n32.5\n49.4\n50.2\n77.2\n79.0\n49.8\n48.1\nTable 6: Results of fine-tuned video retrieval on MSR-VTT, LSMDC, DiDeMo, MSVD, and ANet.\nFigure 7: Zero-shot action recognition (top-1 ac-\ncuracy) on Kinetics-400 / -600 / -700.\nFigure 8: Video retrieval average performance\n(text-to-video R@1) across five datasets.\nand their corresponding videos. Comparing CLIP4Clip using HowTo100M with ViCLIP using Web-\nVid10M or InternVid-10M shows that the correlation between video and text influences performance\nmore significantly than their quantity. Moreover, the zero-shot performance demonstrates that the\nvideo-text representation learned using InternVid is transferable. This claim is supported by its\nsuperior performance across multiple video retrieval benchmarks.\nFine-Tuned. Table 6 exhibits a noticeable improvement when transitioning from InternVid-10M to\nWebVid10M while using ViCLIP for both t2v and v2t retrieval across almost all datasets. On average,\nthere is a 3.7% increase in t2v R@1 across all benchmarks, with particularly significant rise observed\nin ActivityNet (an increase of over 11.9%). However, ViCLIP using WebVid10M yields better v2t\nR@1 scores than when using InternVid-10M (81.2 vs. 80.0). We believe this does not alter the overall\ntrend that InternVid-10M generally provides more advantage to ViCLIP than WebVid10M does.\nThe benefits of used video data become even more apparent when comparing InternVid-10M-DIV\nor InternVid-10M-FLT with WebVid10M. Their overall increases are 5.8% and 5.1%, respectively.\nDespite these improvements, issues related to data diversity persist.\nData Scaling and Issues. Figure 7 and 8 illustrate how ViCLIP\u2019s performance changes in zero-\nshot and fine-tuning settings when varying the scale of InternVid. In both scenarios, increasing\nthe data scale results in significant increases in performance. As shown in Figure 7, ViCLIP\u2019s\ndiscriminative ability linearly increases with the increasing volume of training videos used (10M \u2192\n200M). Meanwhile, Figure 8 shows that the retrieval performance increase becomes marginal when\nscaling the training data beyond 50M. It\u2019s vital to note our model is trained using only contrastive loss\nwithout employing popular designs such as matching head and its corresponding loss. Consequently,\nthis retrieval result doesn\u2019t allow for any definitive conclusions about whether there exists a turning\npoint after which scaling up the training videos becomes less beneficial currently. More explorations\nare necessary in these retrieval experiments. However, these findings generally suggest that enhancing\nthe scale of pretraining data can improve the transferability of the learned representation.\n10\nMethod\nTraining Data\nUCF-101\nMSR-VTT\nIS (\u2191)\nFID (\u2193)\nFVD (\u2193)\nCLIPSIM (\u2191)\nVideoCrafter2\nWebVid10M\n18.26\n66.95\n910.87\n0.2875\nVideoFusion 3\nWebVid10M\n17.49\n75.77\n639.90\n0.2795\nt2v baseline\nWebVid10M\n13.97\n98.25\n705.25\n0.2657\nt2v baseline\nWebVid10M+InternVid18M\n21.04+7.07\n60.25\u221238.00\n616.51\u221288.74\n0.2951+0.0294\nTable 7: Zero-shot text-to-video generation performance.\nt2v baseline\n(+InternVid)\nt2v baseline\n\uff08WebVid10M)\nVideoFusion\n\uff08WebVid10M)\nVideoCrafter\n\uff08WebVid10M)\na bald man\nFigure 9: Comparison of samples from t2v baseline to others. The used prompt is: a bald man in\na black t-shirt is playing a guitar.\n4.2\nText-to-Video Generation\nOur InternVid dataset improves existing text-to-video generation models by providing video-text\npairs with high correspondence. To establish a video generation baseline, we extend spatiotemporal\nmodeling on the latent space of an open-source text-to-image diffusion model [76]. We train the video\ngeneration approach with two settings: one using WebVid10M [6], and the other using InternVid-\nAesthetics-18M in addition to WebVid10M [6]. InternVid-Aesthetics-18M is a subset of InternVid\nconsisting of clips with an aesthetic score of at least 4. Quantitative (Table 7) and qualitative (Figure\n18) evaluations demonstrate the effectiveness of InternVid in video generation tasks. To evaluate\nour models quantitatively, we perform zero-shot text-to-video experiments and randomly sample\n2,020 videos from the UCF-101 dataset and 2,990 videos from the MSRVTT dataset. Following the\nprotocols in [77], we report CLIPSIM, IS, FID, and FVD metrics.\nIn Table 7, we observe that our t2v baseline trained on WebVid10M performs poorly in terms\nof IS, FID, and CLIPSIM when compared to other approaches. However, with the addition of\nInternVid-Aesthetics-18M, our t2v baseline demonstrates significant improvements in these metrics\nand outperforms other methods by a considerable margin. In Figure 18, we observe that the text-to-\nvideo (t2v) baseline using both WebVid10M and InternVid-Aesthetics-18M significantly outperforms\nother methods in terms of visual quality and temporal coherence. It is worth noting that the t2v\nbaseline using InternVid does not contain watermarks, which is a data bias in WebVid10M. These\nresults demonstrate the potential of InternVid for high-quality video generation.\n4.3\nVideo-Centric Dialogue System\nInspired by recent vision-centric dialogue systems [10, 78, 79], we integrate our pretrained ViCLIP\n(with InternVid) into VideoChat [10] to show how our data and model can empower multimodal\ndialogue methods with effective video modeling capability. In implementation, we inherit nearly all\n11\nWhere is this video taken from? \nThe video is taken from an aerial view of a skyscraper-filled city,\nwith\nthe\nShanghai\nskyline\nbeing\nthe\nprominent\nfeature\nin\nthe\nbackground.\nThe\nspecific\nlocation\nis\nShanghai,\nChina,\nwith\nseveral\nskyscrapers\nand\na\nriver\nvisible\nin\nthe\nscene.\nAdditionally,\na\nfew\nspecific\nbuildings\nand\nroads\nare\nalso\nseen,\nsuch\nas\nthe\nOriental\nPearl Tower and the Huangpu River.\nHow many buildings in 00:12?\nThere\nare\nseveral\nbuildings\nvisible\nin\nthe\n00:12\nframe,\nincluding\nseveral towers and skyscrapers, as well as a river and a city skyline.\nThe\nexact\nnumber\nof\nbuildings\nis\ndifficult\nto\ncount,\nas\nthey\nare\ninterspersed\nwith\nother\nobjects\nand\nstructures\nin\nthe\nscene,\nbut\nthere are several visible. The buildings include the Oriental Pearl\nTower, the World Financial Center, and the IFC Mall, among others.\nFigure 10: Video Spatial Understanding Task. This figure demonstrates the spatial understanding\nand temporal localization capabilities of VideoChat-ViCLIP trained with our dataset.\nDescribe the activity in the video.\nThe man is playing the guitar and moving around. The video shows\nhim holding the guitar and playing it.\nFigure 11: Video Action Recognition Task. The video comes from Kinetics-400, with the label\n\"playing guitar\".\ndesigns of VideoChat-Embed, just replacing its visual encoder with our ViCLIP (trained on InternVid).\nWe evaluate VideoChat-ViCLIP in spatial understanding (Figure 10), action recognition (Figure 11),\ntemporal understanding (Figure 12), video reasoning (Figure 13), and video creative (Figure 14) tasks.\nOur qualitative evaluations demonstrate its decent video-to-text capabilities, suggesting promising\npotential for improving video captioning further.\nEvaluation Aspect\nCorrectness\nDetail\nContextual\nTemporal\nConsistency\nAvg\nof Information\nOrientation\nUnderstanding\nUnderstanding\nVideoChat (Eva-g)\n2.23\n2.5\n2.53\n1.94\n2.24\n2.29\nLLaMA Adapter\n2.03\n2.32\n2.3\n1.98\n2.15\n2.16\nVideo LLaMA\n1.96\n2.18\n2.16\n1.82\n1.79\n1.98\nVideo-ChatGPT\n2.4\n2.52\n2.62\n1.98\n2.37\n2.38\nVideoChat-ViCLIP\n2.86\n2.52\n3.08\n2.36\n2.4\n2.64\nTable 8: Performance benchmarking of text generation models.\nIn terms of quantitative comparison, as shown in Table 4.3, VideoChat-ViCLIP significantly out-\nperforms the vanilla VideoChat (using Eva-g as the vision encoder) and other systems across all\nevaluation aspects of the quantitative video conversation evaluation framework in [78]. Specifically,\nthe model shows remarkable improvements in the correctness of information (from 2.23 to 2.86),\n12\nStep-by-step describe the video.\nFirst,\na\nwoman\nin\na\ngreen\nshirt\nstands\nin\na\nkitchen\nwith\na\ntransparent bowl on a table, holding a knife. Next, she uses the\nknife to cut a green apple on the table in the kitchen. Then, she\ncontinues to cut the apple, still using the knife. Afterward, she\ncuts\nanother\napple\non\nthe\ntable, holding\nthe knife\nin\nher\nhand.\nFinally, she cuts the apple and puts it in a glass blender on the\ntable. Throughout the video, other apples are visible on the table,\nand\nthe\nwoman\nuses\na\nknife\nto\ncut\nthem\nas\nwell.\nThe\nbackground\nconsists of a wooden table, countertops, and a kitchen. The video\nis in slow motion and has a blurry, green-tinted appearance.\nWhat will she do next?\nBased\non\nthe\ncontext,\nit\nis\nreasonable\nto\nassume\nthat\nshe\nmay\ncontinue to cut apples and prepare them for juicing.\nFigure 12: Temporal Understanding Task. VideoChat-ViCLIP can easily handle temporal under-\nstanding tasks and make predictions based on these temporal sequences due to its training on rich\ntemporal information on InternVid.\nWhat is strange about this video?\nIt shows a man and a lion interacting in a forest. The man is holding\na lion by the ears, and the lion seems to be okay with being petted\nby the man. They seem to be enjoying each other\u2019s company. The video\nshows a rare and unusual interaction between a man and a lion, which\nis very rare and surprising to see in real life.\nFigure 13: Video Reasoning Task. Our dialogue system has the ability to understand common sense\nand make causal inferences based on the contents within the video.\ncontextual understanding (from 2.53 to 3.08), and temporal understanding (from 1.94 to 2.36). The\naverage score also increases from 2.29 to 2.64, showing an overall performance gain.\n5\nConclusion\nOur dataset, InternVid, is designed for multimodal research (both understanding and generation)\nfocused on videos. It consists of over 230 million video clips sourced from 7 million high-resolution\n(720P) YouTube videos. We use existing models with a multiscale approach to generate clip-level\ndescriptions. Our studies confirm the efficacy of captions, and the large volume of video-text data\nenables crossmodal learning and text-to-video generation at scale. By training with our data, we\ndevelop a video-text representation baseline ViCLIP using ViT-L and analyze briefly how the data\nscale affects learned crossmodal embeddings. In addition to perception tasks, we show that InternVid\nimproves text-to-video generation performance when using a subset of clips based on their aesthetic\n13\nWrite a short poem for this video.\nIn blue flowers she walks, wind in hair.\nWhite straw hat, peace in her stare.\nShe breathes, admires the sky.\nTall she stands, with eyes up high.\nFigure 14: Video Creative tasks. This figure shows the performance of VideoChat-ViCLIP on\ncreative and generative tasks within our dataset, like generating poetry based on video content.\nscores. With its data, annotations, metadata, and computed scores, we believe InternVid can fuel a\nvariety of studies and applications.\nFigure 15: The word cloud (Top-200) of the generated captions in the InternVid dataset reveals that\nthe captions predominantly highlight the rich actions of the objects.\nA\nData Availability Statement\nWe are committed to maintaining transparency and compliance in our data collection and sharing\nmethods. In accordance with these principles, please note the following:\nPublicly Available Data: The data utilized in our studies is publicly available. We do not use any\nexclusive or private data sources.\nData Sharing Policy: Our data sharing policy builds upon the precedent set by prior works like\nKinetics, HD-VILA, and others. Instead of providing the original raw data, we only supply the\nYouTube video IDs necessary for downloading the respective content.\nUsage Rights: The data released by us is intended exclusively for research purposes. Any potential\ncommercial usage is not sanctioned under this agreement.\nCompliance with YouTube Policies: Our data collection and release practices are strictly in accord\nwith YouTube\u2019s data privacy policies. We ensure that no user data or privacy rights are violated\nduring the process.\nData Licence: We employ the protocol of CC BY 4.0.\nB\nLimitations & Societal Impact\nAll video data used in our research are downloaded from YouTube using Safe for Work (SFW) queries\nand channels. To ensure appropriate content, we employ a simple NSFW filter: a binary classifier\ndesigned to recognize and exclude non-ethical videos. For privacy considerations and in respect of\ndata sharing practices, we share only the YouTube ID of the videos, similar to previous academic\nworks. This approach aligns with YouTube\u2019s data protocols and ensures no violation of privacy or\n14\nEnglish.\nChinese.\nKorean.\nGerman.\nFigure 16: The word clouds of the ASR transcripts of four different languages (English, Chinese,\nKorean, and German). We collect videos from various countries or regions with 11 different languages.\nHere we list four of them to show how these transcripts are distributed in words.\ndata usage rules. Despite these precautions, our work has some limitations, primarily related to data\ndiversity and representativeness. Although YouTube is an extensive source encompassing a wide\nrange of video categories, certain specific types of footage may be excluded or scarcely collected,\nincluding: public area surveillance, sports competitions, movies, documentaries, etc. The exclusion\nof such categories is often due to copyright restrictions or other limits imposed by the platform.\nTherefore, while our dataset provides a broad view of everyday video content, its coverage does not\nextend to every possible category or type of video. These limitations should be taken into account\nwhen considering the generalizability of our results across all types of video data.\nC\nMore Statistics in InternVid\nActionness.\nInternVid contains way more verbs than the WebVid10M. We used NLTK toolkit to\nanalyze the number of verbs in captions, focusing on tagging all unique verbs. We found a total of\n109,485 verbs in the WebVid10M, while InternVid contained 212,155 ones. While the counts may\nnot be that accurate due to our simple counting, we believe they provide a rough indication of the\nactionness of the two datasets.\nVideo Caption and Transcript Distribution.\nTo analyze the word distribution of our generated\ncaptions and multilingual (ASR) transcripts, we compute their distributions. The resulting word\ndistribution of the captions is presented in Figure 15, which includes objects (tv, car, door, plant, etc.),\nattributes (green, young, large, long, etc.), locations (middle, behind, south, next, etc.), scenes (room,\nstage, kitchen, office, etc.), actions/events (walking, eating, cutting, holding, etc.), and more.\nWe also include four word distributions of different languages in Figure 16, reflecting trends in\ndifferent countries and offering potential data customization along with the provided metadata.\nD\nInternVid-ICL: Interleaved Video-Text for In-Context Video Learning\nVisual Examples.\nAs given in the paper, we provide examples video+text interleaved entries for\nin-cntext learning as Flamingo. Table 9 gives an example about format (a): arrange clips and their\ndescriptions sequentially based on their temporal order within the same video. Note the videos are\nrandomly dropped with a probability (0.3) for constructing richer text context compared with the\noriginal video-text pair combinations in sequential.\n15\n[..., \"the inside of a home has a rug and a light on.\", \"\u266a We could\nleave the Christmas lights up til January \u266a\", ..., \"woman with blond\nhair playing guitar\", \"\u266a Have I known you 20 seconds or 20 years?\n\u266a\",\n, \"close-up of a bathroom sink with\nsoap bubbles and other items\", \"a bathroom is seen with a sink and\ntwo lights\", \"a woman swiming inside of a fishbowl with a ladder and\na man\", \"\u266a Can I go wher you go?\n\u266a\",\n, \"devils roll the dice, angels roll their eyes\",\"\u266a And, take me\nout, and take me home \u266a\" ,..., \"the man is standing in a room with\npink carpet\",\"\u266a You\u2019re my, my \u266a\", \"a woman in yellow is dancing\nwith a man in a red room\", \"\u266a My, My lover \u266a\",\n, \"a woman is sitting on a chair, playing\na guitar and a woman holding a balloon\", \"\u266a \u266a \u266a\", \"two men smiling\nwhile holding wine glasses and drinking beer\", \"\u266a We could let our\nfriends crash in the living room \u266a\" ...]\nTable 9: Interleaved video-text data format (b) in InternVid. The caption and ASR transcript of\neach clip is shown in black and gray, respectively. We can achieve interleaved video-text data format\n(a) by abandoning ASR transcripts. To obtain data format (c), we concatenate multiple videos with\ninterleaved video-text data (a).\nE\nImplementation Details\nE.1\nViCLIP\nAction Recognition.\nIn the zero-shot action recognition, we sample 8 frames in each video.\nFollowing the settings in CLIP and EVA-CLIP, we report the mean of top-1 and top-5 accuracy for\nKinetics-400 / -600 / -700. In Section 4.1, we show ViCLIP learnt on WebVid or InternVid is an\neffective zero-shot action recognition model.\nIn the full fine-tuned setting, we conduct two experiments with two receipts. In Table 4, for the\nexperiments where the training data excluded K710, we followed the common practice of finetuning\nthe pretrained ViCLIP with the training data from the evaluation dataset. On the other hand, for the\nexperiments where the training data included K710, we adopted a training trick inspired by [26]. We\nfirst finetuned the pretrained ViCLIP with K710 [26], and then proceeded with the common supervised\nfinetuning setting. By incorporating the supervised finetuning with K710, ViCLIP demonstrated\nbetter performance in the fine-tuned tasks compared to experiments that did not include K710.\nVideo Retrieval.\nIn the full-finetuning setting, we tune the pretrained ViCLIP with not only video-\ntext contrastive loss but also video-text matching loss on the training data of the evaluated benchmarks.\nDuring both training and testing, we sample 12 frames. Detailed hyper-parameters are given in Table\n10. In the zero-shot setting, we sample only 8 frames for evaluations.\nE.2\nVideo Generation Baseline\nWe used the spatiotemporal modeling approach from [80] and built our text-to-video generation\nbaseline on the work of [76]. Our approach consists of a U-Net with a transformer that models\nits latents, using interleaved spatiotemporal attention (ST-Attn), cross-attention for visual-text, a\nfeed-forward network (FFN), and temporal attention (T-Attn), as illustrated in Figure 17. To adapt\nthe 2D convolutional layers in [76] to 3D, we extended 3 \u00d7 3 kernels into 1 \u00d7 3 \u00d7 3 ones. We also\nextended the original spatial attentions to spatiotemporal ones. We initialized our baseline using\nall text-to-image diffusion model parameters, while the newly added temporal attention layers used\ndefault parameters.\n16\nconfig\nMSRVTT\nDiDeMo\nANet\nLSMDC\nMSVD\noptimizer\nAdamW\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\nweight decay\n0.02\nlearning rate schedule\ncosine decay\nlearning rate\n2e-5\n4e-5\n2e-5\n2e-5\n4e-5\nbatch size\n256\nwarmup epochs\n1\ntotal epochs\n7\n8\n5\n10\n20\ninput frame\n12\nmax text length\n32\n96\n64\n64\n150\ndrop path\n0.3\n0.2\n0.3\n0.3\n0.2\nflip augmentation\nyes\naugmentation\nMultiScaleCrop [0.5, 1]\nTable 10: Video-text retrieval fine-tuning settings.\n\u2026\n\u2026\nST-\nAttn\nCross\n-Attn\nFFN\nT-\nAttn\nReconstruction Loss\n\u201cA cat walking on the piano. \u201d\nDiffusion\nnoise\nConv Block\nTransformer \nBlock\nTraining\nFrozen\nFigure 17: Framework of our text-to-video generation baseline.\nFor the ST-Attn implementation, we used frame embeddings from the U-Net encoder instead of video\nembeddings as in [80]. We concatenated the embeddings of the previous and current frame for values\nand keys in attention, while using the current frame embedding alone as queries. The rest of the\nimplementation remained the same as the original.\nText-to-Video Evaluation.\nTo evaluate our text-to-video model, we conducted zero-shot experi-\nments on the UCF-101 and MSRVTT datasets, following the method from [77]. For UCF-101, we\nused the class names as text prompts and generated 20 samples per class (total of 2,020 videos). For\nMSRVTT, we randomly selected one caption per video from the official test set (total of 2,990 videos).\nTo ensure a fair comparison, we used the official implementation of VideoCrafter and VideoFusion\n[81] to generate the same number of videos with the same text prompts. During video sampling and\nevaluation, we generated 16 frames per video.\nWe assess the overall quality of the synthesized results on UCF-101 using framewise-FID, FVD,\nand Inception Score (IS), and evaluate the text-video semantic similarity on MSRVTT using clip\nsimilarity (CLIPSIM). For framewise-FID and IS, we use the pretrained Inceptionv3 network weights\nas our image encoder. For FVD, we use the pretrained InceptionI3d model and followed the TATS\nmethod [82]. To compute CLIPSIM, we calculate the clip text-image similarity for each frame with\nrespect to the given text prompts and computed the average score. We use the ViT-B-32 clip model as\nthe backbone, consistent with previous work [77].\n17\nCaptioning\nMethod\nRetrieval\nAction Recognition\nZero-Shot\nFine-Tuned\nZero-Shot\nMSR-VTT\nMSR-VTT\nK400\nK600\nK700\nT2V\nV2T\nT2V\nV2T\ntop-1\nAVG\ntop-1\nAVG\ntop-1\nAVG\nVideoChat\n33.9\n32.3\n46.6\n47.1\n54.68\n67.74\n51.70\n64.91\n43.67\n56.51\nOurs\n38.6\n38.5\n49.0\n49.2\n58.52\n71.11\n55.37\n68.27\n47.09\n59.98\nTable 11: Video retrieval and action recognition results of ViCLIP-B trained on InternVid-FLT-10M\nwith the captions generated by VideoChat and our captioning approach.\nt2v baseline\n(+InternVid)\nt2v baseline\n\uff08WebVid10M)\nVideoFusion\n\uff08WebVid10M)\nVideoCrafter\n\uff08WebVid10M)\na bald\nFigure 18: Comparison of samples from t2v baseline to others. We provide zero-shot text-to-video\ngeneration results of different methods trained on both WebVid10M and the additional InternVid-\nAes-18M. The used prompt is: a bald man in a black t-shirt is playing a guitar.\nF\nMore Results\nF.1\nEffectiveness of Our Multiscale Captioning Approach\nTo further validate the effectiveness of our proposed captioning method, we establish a video caption\nbaseline using the video multimodal model VideoChat [10] for comparison. We input the video\nclip into the model with the prompt \"Please describe the content in the given video.\"\nand apply it to InternVid-10M-FLT, resulting in 10 million new captions generated by VideoChat.\nSubsequently, we train two versions of ViCLIP-Base using InternVid-10M-FLT, each version trained\nwith one of the two types of captions.\nTable 11 demonstrates that ViCLIP-B trained using our captions outperforms the version trained using\ncaptions from VideoChat in both video retrieval (MSR-VTT) and action recognition (K400/600/700).\nThese results are particularly noteworthy considering that the only difference in training lies in the\ncaptions generated by the two different approaches. Therefore, these findings further confirm the\nsuperior performance of our proposed captioning method compared to the baseline VideoChat.\n18\nF.2\nText-to-Video Generation\nIn Figure 18, we observe that the t2v baseline using both WebVid10M and InternVid-Aes-18M\nsignificantly outperforms others in visual quality and temporal coherence. Note that the t2v baseline\nusing InternVid does not contain watermarks, which is a data bias in WebVid10M.\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nNeurIPS, 33:1877\u20131901, 2020.\n[2] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV,\npages 2630\u20132640, 2019.\n[3] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining\nGuo. Advancing high-resolution video-language representation with large-scale video transcriptions. In\nCVPR, pages 5036\u20135045, 2022.\n[4] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin\nChoi. Merlot: Multimodal neural script knowledge models. NeurIPS, 34:23634\u201323651, 2021.\n[5] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya\nKusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through\nvision and language and sound. In CVPR, pages 16375\u201316387, 2022.\n[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In ICCV, pages 1728\u20131738, 2021.\n[7] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock,\nAida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\nZisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv,\nabs/2204.14198, 2022.\n[8] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Jenia Jitsev, et al. Openflamingo, 2023.\n[9] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[10] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.\n[11] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang\nYang, Qingyun Li, Jiashuo Yu, et al. Internchat: Solving vision-centric tasks by interacting with chatbots\nbeyond language. arXiv preprint arXiv:2305.05662, 2023.\n[12] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM,\n59(2):64\u201373, 2016.\n[13] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In ACL, pages 2556\u20132565, 2018.\n[14] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In CVPR, pages 3558\u20133568, 2021.\n[15] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nScaling up vision-language pre-training for image captioning. In CVPR, pages 17980\u201317989, 2022.\n[16] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data\ncreated by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\n19\n[17] Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, JiaQi Wang, and Dahua\nLin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. arXiv\npreprint arXiv:2308.10755, 2023.\n[18] Conghui He, Wei Li, Zhenjiang Jin, Wang Wang, Chao Xu, and Dahua Lin. Opendatalab: Empowering\ngeneral artificial intelligence with open datasets. https://opendatalab.com, 2022.\n[19] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n[20] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-\nbased image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval, pages\n2443\u20132449, 2021.\n[21] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher:\nTowards training-efficient video foundation models. arXiv preprint arXiv:2303.16058, 2023.\n[22] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia\nSchmid. Learning audio-video modalities from image captions. In ECCV, pages 407\u2013426. Springer, 2022.\n[23] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman.\nEnd-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.\n[24] Tianhao Li and Limin Wang. Learning spatiotemporal features via video and text pair discrimination.\nCoRR, abs/2001.05691, 2020.\n[25] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke\nZettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text\nunderstanding. arXiv preprint arXiv:2109.14084, 2021.\n[26] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao. Uniformerv2:\nSpatiotemporal learning by arming image vits with video uniformer. arXiv preprint arXiv:2211.09552,\n2022.\n[27] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu,\nPengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-\nlanguage transformers. In CVPR, 2022.\n[28] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and\nKurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383,\n2021.\n[29] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,\nXin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint\narXiv:2111.07783, 2021.\n[30] Chen Sun, Austin Myers, Carl Vondrick, Kevin P. Murphy, and Cordelia Schmid. Videobert: A joint model\nfor video and language representation learning. ICCV, 2019.\n[31] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. CVPR, 2020.\n[32] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu,\nYi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao.\nInternvideo: General video foundation models via generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022.\n[33] Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong\nZheng, Bingkun Huang, et al. Internvideo-ego4d: A pack of champion solutions to ego4d challenges.\narXiv preprint arXiv:2211.09529, 2022.\n[34] Ziyun Zeng, Yuying Ge, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, and Yixiao Ge. Learning transferable\nspatiotemporal representations from natural script knowledge. In CVPR, pages 23079\u201323089, 2023.\n[35] Ziyun Zeng, Yixiao Ge, Zhan Tong, Xihui Liu, Shu-Tao Xia, and Ying Shan. Tvtsv2: Learning out-of-the-\nbox spatiotemporal visual representations at scale. arXiv preprint arXiv:2305.14173, 2023.\n20\n[36] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang,\nYu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models. arXiv preprint\narXiv:2305.13292, 2023.\n[37] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training. In NeurIPS, 2022.\n[38] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.\nVideomae v2: Scaling video masked autoencoders with dual masking. In CVPR, 2023.\n[39] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender:\nUnifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160,\n2022.\n[40] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan,\nXiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv\npreprint arXiv:2203.07303, 2022.\n[41] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-\nto-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681,\n2021.\n[42] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu. Valor:\nVision-audio-language omni-perception pretraining model and dataset. arXiv preprint arXiv:2304.08345,\n2023.\n[43] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian,\nWei Wang, et al. mplug-2: A modularized multi-modal foundation model across text, image and video.\narXiv preprint arXiv:2302.00402, 2023.\n[44] Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, Zikang Liu, Dongmei Fu, Yi Yang, Jing\nLiu, and Jiashi Feng. Vlab: Enhancing video language pre-training by feature adapting and blending.\narXiv preprint arXiv:2305.13167, 2023.\n[45] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video\nand language. In CVPR, pages 5288\u20135296, 2016.\n[46] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.\nLocalizing moments in video with natural language. In ICCV, pages 5803\u20135812, 2017.\n[47] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron\nCourville, and Bernt Schiele. Movie description. IJCV, 123:94\u2013120, 2017.\n[48] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web\ninstructional videos. In AAAI, 2018.\n[49] Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo\u00efc Barrault, Lucia Specia, and\nFlorian Metze. How2: a large-scale dataset for multimodal language understanding. arXiv preprint\narXiv:1811.00347, 2018.\n[50] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events\nin videos. In ICCV, pages 706\u2013715, 2017.\n[51] Jonathan C Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia Schmid, and David A\nRoss. Learning video representations from textual web supervision. arXiv preprint arXiv:2007.14937,\n2020.\n[52] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-\nscale video benchmark for human activity understanding. In 2015 IEEE conference on computer vision\nand pattern recognition (CVPR), pages 961\u2013970. IEEE, 2015.\n[53] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.\nIn CVPR, 2017.\n[54] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something\nsomething\" video database for learning and evaluating visual common sense. In ICCV, pages 5842\u20135850,\n2017.\n21\n[55] Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, David Fleet, and Roland Memisevic. On\nthe effectiveness of task granularity for transfer learning. arXiv preprint arXiv:1804.09235, 2018.\n[56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n[57] Sijie Song, Xudong Lin, Jiaying Liu, Zongming Guo, and Shih-Fu Chang. Co-grounding networks with\nsemantic attention for referring expression comprehension in videos. In CVPR, June 2021.\n[58] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Tubedetr: Spatio-temporal\nvideo grounding with transformers. In CVPR, pages 16442\u201316453, 2022.\n[59] Zhenyang Li, Ran Tao, Efstratios Gavves, Cees G. M. Snoek, and Arnold W. M. Smeulders. Tracking by\nnatural language specification. CVPR, 2017.\n[60] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning.\narXiv preprint arXiv:2206.03428, 2022.\n[61] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong\nGuo, and Lei Zhang. Tag2text: Guiding vision-language model via image tagging. arXiv preprint\narXiv:2303.05657, 2023.\n[62] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJMLR, 21(1):5485\u20135551, 2020.\n[63] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[64] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[65] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045, 2023.\n[66] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of\nimages interleaved with text. arXiv preprint arXiv:2304.06939, 2023.\n[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021.\n[68] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[69] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n[70] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. NeurIPS, 35:16344\u201316359, 2022.\n[71] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,\nRyan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next\ngeneration of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n[72] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques\nfor clip at scale. arXiv preprint arXiv:2303.15389, 2023.\n[73] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An\nempirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 2022.\n[74] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 3202\u20133212, 2015.\n22\n[75] David L Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human\nLanguage Technologies-Volume 1, pages 190\u2013200. Association for Computational Linguistics, 2011.\n[76] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.\n[77] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR,\npages 22563\u201322575, 2023.\n[78] Salman Khan Muhammad Maaz, Hanoona Rasheed and Fahad Khan. Video-chatgpt: Towards detailed\nvideo understanding via large vision and language models. ArXiv 2306.05424, 2023.\n[79] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.\n[80] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video\ngeneration. arXiv preprint arXiv:2212.11565, 2022.\n[81] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren\nZhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In\nCVPR, pages 10209\u201310218, 2023.\n[82] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi\nParikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part\nXVII, pages 102\u2013118. Springer, 2022.\n23\n"
  },
  {
    "title": "Self-consistency for open-ended generations",
    "link": "https://arxiv.org/pdf/2307.06857.pdf",
    "upvote": "9",
    "text": "Lightweight reranking for language model generations\nSiddhartha Jain\u2217\nAWS AI Labs\nsiddjin@amazon.com\nXiaofei Ma\nAWS AI Labs\nxiaofeim@amazon.com\nAnoop Deoras\nAWS AI Labs\nadeoras@amazon.com\nBing Xiang\nAWS AI Labs\nbxiang@amazon.com\nAbstract\nLarge Language Models (LLMs) can exhibit\nconsiderable variation in the quality of their\nsampled outputs. Reranking and selecting\nthe best generation from the sampled set\nis a popular way of obtaining strong gains\nin generation quality.\nIn this paper, we\npresent a novel approach for reranking LLM\ngenerations. Unlike other techniques that\nmight involve additional inferences or train-\ning a specialized reranker, our approach re-\nlies on easy to compute pairwise statistics\nbetween the generations that have minimal\ncompute overhead. We show that our ap-\nproach can be formalized as an extension\nof self-consistency and analyze its perfor-\nmance in that framework, theoretically as\nwell as via simulations. We show strong\nimprovements for selecting the best k gen-\nerations for code generation tasks as well\nas robust improvements for the best gen-\neration for the tasks of autoformalization,\nsummarization, and translation. While our\napproach only assumes black-box access to\nLLMs, we show that additional access to to-\nken probabilities can improve performance\neven further.\n1\nIntroduction\nThe rapid advancement and remarkable achieve-\nments of generative large-scale pre-trained language\nmodels (LLMs) have brought about a revolutionary\ntransformation in the field of natural language pro-\ncessing (NLP). These models have demonstrated\nsignificant enhancements in various NLP applica-\ntions, such as machine translation, summarization,\nand code generation. Individual generations sam-\npled from the models often yield high-quality re-\nsults. However the quality of generated outputs can\nexhibit considerable variability. Multiple output\nsamplings for the same input can produce certain\ngenerations which are of substantially higher qual-\nity than the quality of the average generation from\nthe model.\nSeveral approaches have been proposed to exploit\nthis phenomenon. One strategy involves improving\n\u2217Corresponding author\nthe underlying models themselves to make the qual-\nity of the average generation consistently better.\nThis can be achieved by taking existing model gen-\nerations, ranking them based on a human feedback,\nautomated evaluation metrics like BLEU score, or\nexecution feedback in case of code. The ranked gen-\nerations can then be finetuned on directly or can\nbe used to train a reward model that can be used\nin an RL loop (Hsieh et al., 2023; Ouyang et al.,\n2022; Ho et al., 2022; Polu et al., 2022; Liu and\nLiu, 2021; Ouyang et al., 2022). Another common\napproach is best-of-n sampling or reranking. In this\napproach, the underlying model is not touched \u2013\nwe instead take multiple samples from the model\nand select the best one post-facto using a reranking\nmethod (Ravaut et al., 2022; Jiang et al., 2022b;\nZhang et al., 2022; Chen et al., 2021; Shi et al.,\n2022; Li et al., 2022; Mizumoto and Matsumoto,\n2016; Uesato et al., 2022). While this approach\ncan often given strong improvements, most extant\nreranking techniques involve computationally in-\ntensive or cumbersome methods to compute the\nranking criterion. These include methods like train-\ning an auxiliary model as a reranker, evaluating\nthe probability of the query given the generated an-\nswer (query likelihood) but at the price of doubling\nthe inference cost, etc. In case of code generation\nmodels, another alternative is executing the gener-\nated code on unit tests. While such an approach\nhas been applied in various models such as Alpha-\nCode (Li et al., 2022) which is targeted towards\ncontest coding problems, it becomes much less fea-\nsible as you move past the contest coding setting\ndue to the complexity of setting up the build envi-\nronment for arbitrary code as well as sandboxing it\nappropriately.\nRecently,\na\nsimple\napproach,\ncalled\nself-\nconsistency was proposed for selecting the best an-\nswer from multiple generations (Wang et al., 2022)\nfor tasks where the set of possible answers is small \u2013\nfor example multiple choice questions or math word\nproblems where there is a unique answer consist-\ning of a single or a very limited number of tokens.\nIn that paper, the authors sample multiple chain-\nof-thought generations from the LLM, extract the\npredicted answer at end each generation and select\nthe answer with the most number of votes. The\nmotivation behind this is the observation that you\narXiv:2307.06857v3  [cs.AI]  11 Jan 2024\ncan take different reasoning paths to get to the\nsame answer. Thus the method aims to marginalize\nover multiple different reasoning paths and rank\nthe answers based on their marginal probability\nrather than their probability conditioned on a sin-\ngle reasoning path. While they achieve substantial\nimprovements over existing baselines, it is not im-\nmediately clear how to apply this to open-ended\ngeneration tasks like code generation, summariza-\ntion, or translation - where there is often no chain-\nof-thought or reasoning path to marginalize over,\nnor is there necessarily a unique correct answer.\nWe start off with two key observations \u2013 (1) We\ncan have semantically equivalent or near-equivalent\ngenerations that are nevertheless not exact matches.\nThese are one subset of generations we can marginal-\nize over (2) For open-ended tasks, a generation can\nencompass multiple elements. For summarization,\nthere might be multiple relevant facts in the text\nthat a good summary should mention. For code,\nthere might be multiple branch conditions that\nneed to be present to generate a correct implemen-\ntation. Our generation set could be structured such\nthat while different generations include a different\nsubset of elements (different facts in case of sum-\nmarization or different branch conditions in case\nof code), we have only a single generation that\ncontains all of the relevant elements. In this case,\nsimply marginalizing over semantically equivalent\ngenerations would not be sufficient as there is no\nsemantically equivalent generation for the optimal\ngeneration.\nWe develop these two observations in the next\nsection into a minimal overhead reranking method\nfor such open-ended tasks which does not require\naccess to token probabilities.\nConcretely, our contributions are as follows \u2013\n\u2022 We connect the above two observations with\nthe notion of self-consistency. Based on that\nconnection, we then proceed to design an effec-\ntive minimal overhead reranker which does not\nrequire access to token probabilities. We show\nthat the reranking methods utilized in previous\nworks Shi et al. (2022); Li et al. (2022) can\nalso be understood within the same conceptual\nframework.\n\u2022 We conduct simulations where we demonstrate\nthat our framework is capable of recovering the\nbest or near-best generation in many cases. We\nalso prove some properties of our methodology\nthat provide guarantees on its effectiveness.\n\u2022 We extend our reranker to optionally account\nfor token log probabilities (if they are pro-\nvided) and show that doing so gives a much\nbetter reranker than just mean log probability\nreranking (which also requires access to token\nlog probabilities)\n\u2022 Empirically, while our focus is on code gener-\nation tasks where we demonstrate significant\ngains, we also experiment with the tasks of\nautoformalization, summarization, and trans-\nlation and find that our approach leads to\nnon-trivial though smaller gains there.\n\u2022 As our method is based on pairwise similarity\nbetween generations, we are able to leverage\nthat property to improve ranked best-of-k per-\nformance for different values of k.\n\u2022 We conduct multiple experiments ablations to\nunderstand the effect of various experimental\nsettings.\nThe rest of the paper is organized as follows. In\nSection 2 we present our motivation. In Section 3 we\npresent our method and the similarity function. In\nSection 4, we present and discuss our experimental\nresults. In Section 5, we describe the related work\nand we finally conclude in Section 6.\n2\nMotivation\nConsider the following coding problem from the\nMBPP dataset \u2013\ndef remove_dirty_chars(string, second_string):\n\"\"\"\nWrite a function to remove characters\nfrom the first string which are\npresent in the second string.\n>>> remove_dirty_chars(\"probasscurve\",\n\"pros\")\n\u2019bacuve\u2019\n>>> remove_dirty_chars(\"digitalindia\",\n\"talent\")\n\u2019digiidi\u2019\n>>> remove_dirty_chars(\"exoticmiles\",\n\"toxic\")\n\u2019emles\u2019\n\"\"\"\nA solution to the above problem would be seman-\ntically equivalent to \"iterate over the string skipping\ncharacters in second_string and then convert the\nresult back to a string and return\". Two parts of\nthe semantic meaning of this solution could then be\n(1) the return type should be a string (2) when it-\nerating through the string, any character in second\nstring has to be skipped over. These observations\ncan be converted into predicates for the generations.\nSpecifically, for this prompt, we can define the pred-\nicates (1) p1 = is the return value of the generated\nprogram a string? (2) p2 = in the generated pro-\ngram, are all characters in second_string skipped\nover in the return string? These predicates capture\nproperties of the semantic meaning of the genera-\ntions. We sample three generations for the above\nprompt resulting in the following generations:\n# First generation (Incorrect)\nreturn [char for char in string if char\nnot in second_string]\n# Second generation (Incorrect)\nreturn \u2019\u2019.join([char for char in string])\n# Third generation (Correct)\nreturn \u2019\u2019.join([char for char in string\nif char not in second_string])\nNow if we were able to evaluate the above predi-\ncates at inference time on the generations, we would\nbe able to detect that generation 3 is the only one\nthat satisfies both and is thus an optimal genera-\ntion. However generating the relevant predicates,\nand then generating code to evaluate arbitrary pred-\nicates on code that confirms to the given natural\nlanguage specification with high precision is an un-\nsolved problem.\nIs there a way transform the problem into some-\nthing more tractable? Let us look at the votes each\npredicate gets from the different generations (i.e.\non how many generations the predicate evaluates\nto true). p1 gets 2/3 votes (from the 2nd and 3rd\ngeneration) and thus the majority vote is that it\nshould be true. p2 gets 2/3 votes (from the 1st and\n3rd generation) and thus the majority vote again\nsays it should be true. Generation 3 is the only one\nthat agrees with the majority vote for p1, p2 and is\nthus the consensus choice.\nIn fact, we do not even have to do the step of\nfirst counting votes for p1, p2 to figure out what\ntheir majority vote value is! We can just compute\nhow much a generation agrees with the other 2\ngenerations on the evaluation for p1, p2. To elabo-\nrate, generation 1 agrees with generation 3 on p2\nbut not p1. It does not agree with generation 2 on\nanything. Thus it has a total agreement score of 1.\nSimilarly generation 2 also has an agreement score\nof 1. Generation 3 however agrees with generation\n1 on p2 and with generation 2 on p1 resulting in\nan agreement score of 2. Thus generation 3 has\nthe highest agreement with all other generations\nand is the consensus choice. This transformation is\ndepicted in Figure 1.\nThere are a couple of points to note regarding the\nabove. (A) The answer we want for both predicates\nends up being also what the majority vote predicts.\nThis is the connection to self-consistency that we\nwill formalize shortly. (B) While the above proce-\ndure handles Observation (2) in the mentioned in\nthe introduction, does it also handle Observation\n(1)? Note that if we had a 4th generation\n# Fourth generation (Correct)\nnew_str = \u2019\u2019\nfor c in string:\nif c not in second_string\nnew_str += c\nreturn new_str\nthis would also agree with the 3rd generation on\np1, p2. Thus this generation is semantically equiv-\nalent to the 3rd generation with respect to p1, p2.\nIncluding this generation would lead to a tie be-\ntween the 3rd and 4th generation with respect to\nthe scheme above. Thus the above procedure can\nalso account for generations that are semantically\nequivalent.\nAs mentioned before, at inference time how-\never, we have access neither to such predicates\nnor to what their evaluation on the different gener-\nations might be. However, as the previous example\nshowed, we do not actually even need an evalua-\ntion of the predicates on the generations. We only\nactually need an understanding of how much a pair\nof generations agree on relevant predicates. In the\nnext section we will describe simple similarity func-\ntions to approximate this agreement without any\nknowledge of the possible predicates and later show\ntheir empirical effectiveness. For now, we attempt\nto formalize the above intuition, connect it with\nself-consistency, and give it theoretical grounding.\nLet v be a vector of length k where each element\nrepresents a predicate and the value represents the\nproperty\u2019s value that you want the generation to\nhave. For example, for the example above, v would\nbe of length 2 with a value of 1 in both. In gen-\neral, we do not restrict v to be just a binary vector\nas some predicates can have multiple possibilities\nas an answer.\nLet ui be the vector for the ith\ngeneration representing the predicates\u2019 values for\nthat generation. We now make the self-consistency\nassumption which is that for each individual pred-\nicate, the most frequent response is assumed to\nbe correct. Formally if vl can take on ml values\n1, . . . , ml and without loss of generality, vl = 1,\nthen l = arg maxj\nPn\ni=1 I(ul\ni = j).\nNow as mentioned, at inference time, we may\nnot have access to either the predicates or their\nvalues for different generations. Thus we only as-\nsume access to the agreement between the different\ngenerations on the predicates\u2019 values (later on we\nwill approximate this with similarity functions be-\ntween generations). In particular we assume we\nknow what the pairwise fractional agreement on\nthe predicates is between generations denoted as\na(ui, uj) = 1\nk\nPk\nt=1 I(ut\ni = ut\nj)\u2200i, j \u2208 [1, n] where i\nindexes the generations and t the predicates. We\nthen try to identify a generation i such that the\naverage pairwise fractional agreement for that gen-\neration with all other generations is maximized \u2013\ni.e. a(ui, v) is maximized.\nPlurality approximation\nPairwise agreement \napproximation\nFigure 1: On the left we have the original setup where we have predicates which we know the optimal generation should satisfy\nand which we can evaluate on the generations. In the middle, we drop the assumption that we know whether the optimal generation\nshould satisfy the predicates or not. On the right, we drop the assumption that we need to evaluate the predicates on the different\ngenerations \u2013 only assuming we know on how many predicates a pair of generations agree\nGiven this problem formulation and selection\ncriterion, we can establish the following:\nTheorem 2.1. For k = 1, we always recover the\nbest u. However for k > 1, it is not guaranteed.\nInformally if there is only a single predicate we\ncare about, then we can always recover the optimal\ngeneration. Moreover:\nTheorem 2.2. If there exists ub = v, then b =\narg maxi\n1\nn\u22121\nP\ni\u0338=j a(ui, uj).\nInformally this says that if a generation g exists\nsuch that its predicate vector perfectly aligns with\nthe optimal vector v, selecting the generation with\nthe highest average fractional agreement with other\ngenerations will pick g.\nThe previous theorem only works if the optimal\ngeneration is part of the set of generations. What if\nthat is not the case (as is likely)? The next theorem\ngives upper and lower bounds on the fractional\nagreement we can expect. Now if we assume that\nuj\ni are iid from Bernoulli(pj) and n is the number\nof generations, then we can show that\nTheorem 2.3. E[Pk\nj uj\nb] \u2264 Pk\nj=1 pi +\nq\nk log n\n2\nwhere ub denotes the sequence selected by our\nmethod.\nAll proofs for these theorems are presented in the\nSupplement. While the theorems give some guar-\nantees on the performance, the bounds in Theorem\n2.3 are still not very tight. Furthermore, They are\nonly for the case where the predicates are binary\nvalued. To further substantiate our selection cri-\nterion \u2014 picking the generation with the highest\naverage fractional agreement with all other genera-\ntions \u2014 we conducted a simulation. The setup is\nas follows \u2013 we fix the number of predicates (length\nk of the vector v in the above notation) as well as\nthe number of values the predicate can take. We\nthen simulate the generations predicate evalutions\nby assuming a generation has an equal chance of\nhaving an value the predicate can take. However we\nforce the self-consistency constraint that for every\npredicate, the plurality of generations should have\nthe property that matches the predicate value in\nv. The results are in the Supplement. Our find-\nings show that our method successfully recovers the\nbest generation the majority of the time, signifi-\ncantly outperforming random selection. Moreover,\non average, the generation we recover demonstrates\nnearly 100% agreement with best generation, even\nin cases where we do not select the best generation.\nThe full details are in the Supplement.\n3\nMethod\nAs previously mentioned, we may not have the ca-\npability to compute predicates at inference time,\nthereby rendering the computation of the exact frac-\ntional agreement with v i.e. a(u, v), unattainable.\nHowever as we found out in the last section, choos-\ning the generation that has the maximum average\nfractional similarity with all other generations can\nbe a good approximation. However as we may not\nhave predicates at inference time, we cannot always\ncompute that either. Intuitively however, if two\ngenerations are more similar to each other \u2013 for\nan appropriate definition of similarity \u2013 then they\nwill tend to agree more on any possible predicates.\nSurprisingly, we find that a very simple similarity\nfunction, which we will define shortly, is sufficient\nfor our purposes.\nOnce we have our similarity function,\nwe\ncan define a generalized self-consistency score\nGSCSim(i)\nfor\neach\ngeneration\ni,\ngiven\nby\n1\nM\u22121\nPM\nj=1,j\u0338=i Sim(i, j).\nHere, Sim denotes the\nsimilarity function, and M represents the number\nof generations.\nFor generations with unique answers, if we have:\nSim(i, j) = I(Answer in generation i is an exact\nmatch with Answer in generation j)\nthis is equivalent to the self-consistency criterion.\nTwo other reranking methods - MBR-Exec (Shi\net al., 2022) and AlphaCode (Li et al., 2022) - can\nbe viewed in terms of the same formulation with\nthe difference being that of the similarity function.\nMBR-Exec executes model generated code. It then\ndefines gives a similarity score of 1 if a pair of pro-\ngrams agree on all unit tests and 0 otherwiseFor\neach program, they sum the similarity vs all other\nprograms and pick the program with the highest\nsimilarity. Similarly AlphaCode clusters its gener-\nated programs by executing them on test cases and\nselecting a program from the largest cluster \u2013 with\ntwo programs cluster together if they agree on on\nall test cases. This is conceptually equivalent to\nwhat MBR-Exec does. We give further evidence\nthat this is a useful way to frame self-consistency by\nevaluating another OpenAI Ada embedding based\nsimilarity function (Section H in the Supplement).\nWhile its performance is promising, as the similar-\nity function is a lot more heavyweight requiring a\nseparate embedding model, we chose not to explore\nit further.\nOne straightforward way to encode a generation\nis by using a binary vector that denotes the presence\nor absence of an n-gram. Surprisingly, we find this\nsimple encoding to be sufficient for defining a robust\nsimilarity function. For open-ended generation, we\ndefine our similarity function as follows. For each\ngeneration we define a vector v of size |V | where\nV is set of all possible n-grams for n = 1 to n = K\nwhere K is a hyperparameter. For the experiments\nin this paper, we simply use K = 1. We show in\nSection F, increasing K can be helpful though only\nup to a point. Each element i of v is simply whether\ntoken i is present in the generation or not. We then\ntake the inner product between two such vectors\nas similarity. We call this the Ngram consistency\nscore (NCS) and refer to the K = 1 version as the\nUnigram consistency score (UCS). Figure 2 shows a\nvisualization of v for an example sentence. Formally\nUCS(i, j) =\n1\n|V |vi \u00b7 vj\nwhere\nvj\ni = I(tj \u2208 gi)\nwhere tj is the jth token and gi the ith generation.\nThis definition only requires model generations and\nincurs minimal computational overhead \u2013 we only\nneed to compute the unigram overlap instead of\ntraining an auxiliary model, running generated pro-\ngrams, or performing additional inferences using\nthe same model (which will increase compute cost\nas well as latency). Notably, we don\u2019t normalize\nthe inner product by the norm of the vectors. This\nis a deliberate design choice that encourages more\ndiverse sequences, in response to known issues of\nneural generation models producing degenerate and\nrepetitive sequences (Zhang et al., 2022; Welleck\net al., 2019). We delve into this topic in Section I\nin the Supplement.\nWhen token probabilities are available, we can\nleverage them to improve our approach. Intuitively,\nif a generation has a low token probability for the\ngenerated token, then finding a match for that that\ntoken should count for less. In accordance with this\nintuition, we introduce two further variants. First\nwe modify the definition of v as follows\nvj\ni =\n\uf8f1\n\uf8f2\n\uf8f3\n1\nci\nj\nPci\nj\nk p(ti,k\nj )\nif tj \u2208 gi,\n0\notherwise\nwhere cj\ni is the number of times token tj appears\nin generation i and p(ti,k\nj ) is the token probability\nof the jth token\u2019s kth appearance in generation\ni. We call this the weighted n-gram consistency\nscore (WUCS). Figure 2 has a visualization for an\nexample sentence.\nThe mean log probability of a sequence is an\noft-used ranking method. We can combine it with\nWUCS by further weighting each generation by the\nper token probability as follows \u2013 for a generation\ni, Consensus-WUCS = WUCS \u00b7e(1/|gi|)\u00b7p(gi) where\ngi is the length of generation i.\nFinally, to rank the generations, we employ\narg maxi GSCSim(i) where Sim can take the form\nof UCS, WUCS, or Consensus-UCS.\n3.1\nExtending to ranked pass@k\nA common evaluation metric for code generation\nproblems is ranked pass@k wherein we assess\nwhether any program among the top k selected\nprograms (selected from a larger set) can pass all\nthe given unit tests for that problem. Typically, the\ntop k generations are selected based on a predeter-\nmined ranking. However, with our similarity-based\nmetric, we can apply a more nuanced approach.\nFor a particular problem, if the highest-ranked\ngeneration for a specific prompt is correct, we have\nalready succeeded. We would only need to utilize\nthe remaining generations in our k-budget if the\ntop-ranked generation does not pass some unit test\ncase.\nIn this event, we could consider the top-\nranked generation as a hard negative and select the\nnext generation that exhibits lower similarity to\nthe top-ranked generation.\nMore specifically, if we have selected programs\nSk\u2032 so far (|Sk\u2032| = k\u2032 < k, then we modify the GCS\nfunction to select the k\u2032 + 1th item in the list. In\nparticular, we compute\nGCSranked\nsim\n=\n1\nn \u2212 1(\nX\nj /\u2208Sk\u2032\nsim(i, j)\u2212\nX\nj\u2208Sk\u2032\nsim(i, j))\nNote that for k = 1, GCS and GCSranked are\nequivalent. We demonstrate in Section 4.4 that\nGCSranked\nSim\nperforms significantly better in ranking\nfor pass@k where k > 1 than raw GCS. This ap-\nproach leads to a more efficient utilization of the\nranked generations, improving the overall effective-\nness of the code generation task.\nI ate food\nwhile\nate\nfood\ncat\nhat\n1\n1\n0\n0\n0\n0.9\n0.7\n0.8\n0.5\n0.7\n0.3\n0.8\n0.9\n0.6\nI\nthink\nI\nsaw\nyour\ndog\nchase\nmy\ndog\nI : (0.9 + 0.8)/2 =\n0.7\ndog : (0.6 + 0.3)/2 = 0.45\nwhile =\n0\nthink =\n0.7\nsaw =\n0.5\nused by UCS\nused by Weighted-UCS\n<latexit sha1_base64=\"DGSD5sfvitVvbRtYDjCT97rpYuY=\">A\nB83icbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV7AM6Q8mkmTY0kxmSTKEM/Q03LhRx68+482/MtLPQ1gOBwzn3ck9OkAiujeN8o9LG5tb2T\nnm3srd/cHhUPT7p6DhVlLVpLGLVC4hmgkvWNtwI1ksUI1EgWDeY3Od+d8qU5rF8MrOE+REZSR5ySoyVPC8iZhyE2XQ+4INqzak7C+B14hakBgV\nag+qXN4xpGjFpqCBa910nMX5GlOFUsHnFSzVLCJ2QEetbKknEtJ8tMs/xhVWGOIyVfdLghfp7IyOR1rMosJN5Rr3q5eJ/Xj814a2fcZmkhkm6P\nBSmApsY5wXgIVeMGjGzhFDFbVZMx0QRamxNFVuCu/rldK5qrvX9cZjo9a8K+owxmcwyW4cANeIAWtIFCAs/wCm8oRS/oHX0sR0uo2DmFP0C\nfP38gkf8=</latexit>vi\n<latexit sha1_base64=\"DGSD5sfvitVvbRtYDjCT97rpYuY=\">AB83icbVDLSgMxFL2pr1pfVZdugkVwVWakqMuiG5cV7AM6Q8mkmTY0kx\nmSTKEM/Q03LhRx68+482/MtLPQ1gOBwzn3ck9OkAiujeN8o9LG5tb2Tnm3srd/cHhUPT7p6DhVlLVpLGLVC4hmgkvWNtwI1ksUI1EgWDeY3Od+d8qU5rF8MrOE+REZSR5ySoyVPC8iZhyE2XQ+4INqzak7C+B14hakBgVag+qXN4xpGjFpqCBa910nMX5GlOFUsHnFSzVLCJ2QEetbKknEtJ8tMs\n/xhVWGOIyVfdLghfp7IyOR1rMosJN5Rr3q5eJ/Xj814a2fcZmkhkm6PBSmApsY5wXgIVeMGjGzhFDFbVZMx0QRamxNFVuCu/rldK5qrvX9cZjo9a8K+owxmcwyW4cANeIAWtIFCAs/wCm8oRS/oHX0sR0uo2DmFP0CfP38gkf8=</latexit>vi\nFigure 2: On the left, we depict the vi corresponding to the sentence I ate food used by UCS. On the right we show the vi\ncorresponding to the sentence I think I saw your dog chase my dog used by Weighted-UCS\n4\nResults\nWe conducted experiments utilizing the Codex\nfamily of models, specifically Codex-davinci-001,\nCodex-davinci-002, and Codex-Cushman as well as\nLlama family of models. In addition we also eval-\nuated GPT-J for Xsum, MiniF2F, and WMT14.\nUnfortunately due to the unexpected shutdown\nof the OpenAI API, we were unable to obtain\nresults for Codex-001 and Codex-Cushman on\nthe Xsum, MiniF2F, and WMT14 datasets. We\nevaluated these models on a range of datasets\nfor code generation tasks \u2013 in particular on the\nHumanEval (Chen et al., 2021), MBPP, MBPP-\nsanitized (Austin et al., 2021) datasets for code\ngeneration. For the autoformalization of MiniF2F\nto Isabelle, we used the dataset provided by (Jiang\net al., 2022a). For text summarization, we utilized\nthe Xsum dataset (Narayan et al., 2018). For ma-\nchine translation, we used the WMT14 French to\nEnglish and German to English datasets (Bojar\net al., 2014).\nOur primary evaluation metric for code genera-\ntion is ranked pass@1 where we rerank a sample set\nof generations and assess whether the top-ranked\ngeneration successfully passes all unit tests. We\nalso evaluate with ranked pass@k for k > 1. For\nthe MiniF2F autoformalization task, we measure\nthe quality using the BLEU score, following Wu\net al. (2022). For Xsum we use the Rouge-2 and\nRouge-L scores for evaluation. For all code genera-\ntion datasets, we sample 125 generations from the\nmodels which serves as our dataset for the different\nexperiments For MiniF2F and Xsum, we sample 50\ngenerations from the model. Unless otherwise spec-\nified, for all experiments, we use the Codex-davinci-\n002 model. Following (Shi et al., 2022; Zhang et al.,\n2022), we perform bootstrap sampling 50 times\nwith a sample size of 25 to generate the results.\nOur baselines are Random selection, Ranking\nby mean log probability, Ranking using Medoid\nin our confidence weighted unigram space, and for\ncode generation - ranking using the Coder Reviewer\nRanker method (Zhang et al., 2022). A full de-\nscription of the datasets, experiments, and\nthe baselines is in the Supplement. We also\nhave several additional results in the Supple-\nment.\n4.1\nGSC scores are higher for correct\nanswers\nAs a sanity check, we first evaluate whether the GSC\nscores are indeed higher for the correct generations 1\nThe results are in Table 8 in the Supplement. The\nratios are consistently > 1 for all models except for\nthe UL2-20B model for which they still remain very\nclose to 1.\n4.2\nUCS shows strong improvements for\nCode Generation\nAs shown in Tables 1 and 2 (Supplement), the\napplication of the UCS, WUCS, and Consensus-\nWUCS methods leads to substantial improvements\nin the accuracy as well as mean reciprocal rank of\ncode generation across various models and datasets.\nIn the HumanEval dataset, UCS variants consis-\ntently outperform the traditional methods, namely\nRandom and mean log probability. For instance,\nthe Codex002 model exhibits a substantial accu-\nracy improvement from 0.435 (Random) to 0.568\n(Consensus-WUCS). Even the less performing mod-\nels, such as Llama-13B and Llama-30B, exhibit\nnoticeable accuracy gains when our proposed meth-\nods are employed.\nSimilar trends are observed in the MBPP-S and\nMBPP datasets.\nUCS, WUCS, and Consensus-\nWUCS consistently improve the accuracy across all\nmodels. Specifically, the Consensus-WUCS method\nconsistently dominates Random and mean log prob-\nability ranking in all categories, and almost al-\nways outperforms WUCS as well. Of particular\nnote is the performance of WUCS, which surpasses\nthe mean log probability method in every model\nand dataset combination.\nIn fact it is the best\nmethod for all dataset and model combinations ex-\ncept LLama-13B model for MBBP and MBPP-S.\nUCS, which does not require token probabilities and\n1We used the generations in (Li et al., 2022) provided\nby them as part of their Supplementary Material.\nrelies only on the generations, also demonstrates a\nconsistent superiority over the random reranking.\nConsensus-WUCS and WUCS are also almost al-\nways better than the Medoid based approach with\nConsensus-WUCS outperforming it 13/15 times. A\ndiscussion of the mean reciprocal ranking perfor-\nmance is deferred to the Supplement but the trend\nis similar.\n4.3\nUCS shows consistent improvements\nfor non-coding tasks\nNext, we evaluated the performance of UCS,\nWUCS, and Consensus-WUCS on the non-coding\ntasks. In the case of the MiniF2F dataset, evaluated\nusing the BLEU metric, Consensus-WUCS outper-\nforms all other methods for the Codex002 model\nexcept for Medoid. For the Llama-13B, Llama-30B,\nand GPT-J models, the top performers are closely\nmatched, with Consensus-WUCS, WUCS, and UCS\nall delivering competitive scores.\nTurning to the Xsum dataset, we see a similar\ntrend. For the Rouge-2 metric, Consensus-WUCS\nachieves the highest score for the Codex002 and\nboth LLama models, and ties for the best score\nwith WUCS for the Llama-13B model.\nIn the\nGPT-J model, UCS performs slightly better than\nthe WUCS and Consensus-WUCS. Nonetheless,\nall these methods surpass Random, and Mean-\nlogp reranking methods and almost always surpass\nMedoid.\nWith the Rouge-L metric, UCS variants show\nthe best performance for the all models except\nCodex002. For the Llama-30B model, WUCS and\nConsensus-WUCS share the top spot, while UCS\nachieves the best score for the GPT-J model. Once\nagain, these methods generally outperform Medoid,\nRandom, and Mean-logp reranking methods.\nFor the WMT14 translation dataset, Conensus-\nWUCS is the best for all models and both tasks\nexcept for the German to English Llama-13B model.\nWUCS also shows strong performance, matching\nConsensus-WUCS score on 3 model-task combina-\ntions. UCS is also consistently better than random\nselection.\nIn total, Consensus-WUCS gets the top spot in\n12/20 comparisons, WUCS in 7/20, UCS in 3/20,\nand Medoid in 5/20 primarily due to MiniF2F.\n4.3.1\nCause of smaller performance\nimprovements for non-coding tasks\nWe took the top 3 and bottom 3 generations for\ncoding and non-coding tasks and computed the un-\nigram overlap within each set. The results are in\nTable 6 (Supplement). The ratio of the overlap for\ncoding tasks was a lot higher than that for non-\ncoding tasks giving a hint as to why we see a much\nstronger improvement for coding tasks. This means\nthat if a unigram is not shared between two gen-\nerations, that gives a lot more information about\nwhether two generations are semantically far apart\nfor coding tasks versus non-coding tasks. Thus com-\nputing the unigram overlap statistic is a lot more\ninformative for code generation tasks vs non-code\ngeneration tasks which could be the reason behind\nthe smaller gains for non-coding tasks. However we\nwant to note that while the gains are smaller, they\nare similar to gains that that past published papers\nreport for such metrics and importantly, the gains\nare robust across different tasks and models.\n4.4\nGCSranked comparison\nIn Figure 3 (Supplement), we show how the model\nperformance changes as k for pass@k increases.\nWe compare GCS vs GCSranked. While the perfor-\nmance of GCS declines quickly, GCSranked main-\ntains good performance even at larger values of k\nfor all code generation datasets.\n5\nRelated Work\n5.1\nAuxiliary reranker\nIn Mizumoto and Matsumoto (2016), they use a\nperceptron based reranker to rerank model gener-\nated translations. SummaReranker (Ravaut et al.,\n2022) use mixture of experts training to train their\nreranker to optimize for multiple automated evalua-\ntion metrics (like ROUGE or BLEU score) at once.\nPairReranker (Jiang et al., 2022b) uses automated\nevaluation metrics to rank model generations and\nthen select the top few best and worse and train\na model to classify the better summary between\na pair of summaries. All of the previous rerank-\ning methods however require training an auxiliary\nmodel.\n5.2\nCode generation reranking\nThere have also been multiple reranking propos-\nals for code generation in particular.\nA unique\ncharacteristic of code (as oppposed to text) is that\ncode can be executed. Thus several methods have\ntried to exploit that property for reranking. MBR-\nExec (Shi et al., 2022) and AlphaCode (Li et al.,\n2022) both execute the generated codes on unit\ntests. They rank the different codes according to\nhow many other codes are semantically equivalent\nto them (i.e. have the same results on the given\nunit tests). CodeT (Chen et al., 2022) uses LLMs to\ngenerate both code and candidate unit tests. They\nthen find sets of generated codes such that the prod-\nuct of the size of the set and the size of the unit test\nset the codes agree on is maximized. More recently,\nCoder-Reviewer Ranker (Zhang et al., 2022) applies\nthe well known Maximum Mutual Information ob-\njective (Li et al., 2015) to code generating LLMs\nby using the strong few shot and zero prompting\ncapabilities of LLMs to obtain the query likelihood.\nNo logprobs used\nRandom\nUCS\nHumanEval\nCodex002\n0.435\n0.539\nCodex001\n0.345\n0.402\nCode-Cushman\n0.311\n0.353\nLlama-13B\n0.142\n0.177\nLlama-30B\n0.207\n0.257\nMBPP-S\nCodex002\n0.55\n0.572\nCodex001\n0.494\n0.523\nCode-Cushman\n0.436\n0.457\nLlama-13B\n0.247\n0.261\nLlama-30B\n0.325\n0.253\nMBPP\nCodex002\n0.536\n0.58\nCodex001\n0.475\n0.505\nCode-Cushman\n0.305\n0.386\nLlama-13B\n0.185\n0.183\nLlama-30B\n0.262\n0.276\nlogprobs used\nMedoid\nMean-logp\nWUCS\nConsensus-WUCS\n0.437\n0.539\n0.558\n0.568\n0.354\n0.408\n0.426\n0.445\n0.335\n0.355\n0.373\n0.381\n0.17\n0.17\n0.187\n0.192\n0.225\n0.228\n0.263\n0.267\n0.583\n0.57\n0.580\n0.589\n0.532\n0.515\n0.535\n0.546\n0.467\n0.456\n0.472\n0.488\n0.284\n0.27\n0.266\n0.277\n0.357\n0.348\n0.363\n0.373\n0.563\n0.512\n0.587\n0.594\n0.505\n0.503\n0.520\n0.525\n0.343\n0.319\n0.405\n0.420\n0.202\n0.197\n0.195\n0.199\n0.276\n0.273\n0.287\n0.294\nNo logprobs used\nRandom\nUCS\nMiniF2F\nCodex002\n55.8\n55.6\nLlama-13B\n24.3\n24.6\nLlama-30B\n26\n25.6\nGPT-J\n24.2\n24.7\nXsum Rouge2\nCodex002\n19.7\n21\nLlama-13B\n9.2\n10.4\nLlama-30B\n10.7\n12.1\nGPT-J\n6.5\n7.1\nXsum RougeL\nCodex002\n33.9\n34.8\nLlama-13B\n19.6\n20.9\nLlama-30B\n21.4\n23\nGPT-J\n17.2\n18\nWMT14 French \u2192\nEnglish BLEU\nCodex002\n34.7\n36.2\nLlama-13B\n4.3\n4.4\nLlama-30B\n3.9\n4\nGPT-J\n3.8\n3.9\nWMT14 German \u2192\nEnglish BLEU\nCodex002\n30.7\n31.6\nLlama-13B\n3.4\n3.5\nLlama-30B\n3.7\n3.7\nGPT-J\n3.1\n3.3\nlogprobs used\nMedoid\nMean-logp\nWUCS\nConsensus-WUCS\n58.2\n52.9\n55.8\n56.2\n24.9\n24.2\n24.7\n24.8\n26.4\n25.6\n25.7\n25.7\n24.8\n24\n24.8\n24.8\n21.8\n21.4\n21.5\n21.9\n10.3\n10.3\n10.6\n10.6\n12\n12.2\n12.2\n12.3\n6.9\n6.6\n7\n6.9\n36.3\n35.1\n35.3\n35.6\n20.7\n20.3\n21\n20.9\n22.7\n22.8\n23.1\n23.1\n17.5\n16.6\n17.8\n17.5\n35.9\n36.6\n36.5\n37\n4.2\n4.5\n4.5\n4.6\n4\n4\n4.1\n4.1\n3.8\n3.9\n4\n4\n31.2\n33.2\n32.1\n34\n3.1\n4\n3.5\n3.6\n3.5\n3.9\n3.8\n3.9\n3.2\n3.2\n3.3\n3.3\nTable 1: Accuracy of generated code for HumanEval, MBPP, MBBP-S as well as performance on Xsum, MiniF2F,\nWMT14 French to English, and WMT14 German to English datasets. All scores are out of 100. Best results are\ncolored in first, second. Italics for best in category (logprobs used vs not).\n6\nConclusion\nWe analyze the self-consistency method for prob-\nlems that have fixed answers and develop a frame-\nwork to extend it to open-ended generations. We\nestablish connections between our framework and\nother code generation reranking functions and prove\nthat if the optimal generation is present in our gen-\neration set, we can always recover it as well as prove\nbounds on how close we can get to the optimal gen-\neration under certain settings.\nOur simulated tests reveal our ability to consis-\ntently recover the best or close to best possible gen-\neration in the set. We introduce several lightweight\nsimilarity functions and show that they give strong\nand consistent improvements over state of the art\nbaselines. Notably, our Unigram Consistency Score\n(UCS) function, the most minimal of our similarity\nfunctions, requires only access to raw generations to\neffectively rerank. We show that the UCS variants\nuniformly enhance the performance of code and text\ngeneration and are competitive with strong base-\nlines like Coder Reviewer Reranker despite them\nneeding a lot more compute resources as well as\ntime. For code geneartion, we also leverage the fact\nthat our reranking metric is based on pairwise simi-\nlarity to improve performance for pass@k for k > 1.\nAdditionally, we conduct multiple variations on our\nprimary experiments to ascertain the robustness\nand reliability of our performance.\n7\nBroader Impact and Limitations\nAs a paper that tries to improve the performance of\nLarge Language Models (LLMs), it inherits the risk\nand rewards of LLMs in general. LLMs have shown\nthemselves highly relevant and useful for a number\nof tasks but in particular code generation. Our\nmethod shows particularly strong improvements for\nthat task and thus we hope will have a broad impact.\nNevertheless, we did not evaluate our method on\nwhether it increases its propensity to select biased\nor toxic generations which we leave to future work.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye,\nMaarten Bosma, Henryk Michalewski, David Do-\nhan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc\nLe, et al. 2021. Program synthesis with large lan-\nguage models. arXiv preprint arXiv:2108.07732.\nOnd\u0159ej Bojar, Christian Buck, Christian Feder-\nmann, Barry Haddow, Philipp Koehn, Johannes\nLeveling, Christof Monz, Pavel Pecina, Matt Post,\nHerve Saint-Amand, et al. 2014. Findings of the\n2014 workshop on statistical machine translation.\nIn Proceedings of the ninth workshop on statisti-\ncal machine translation, pages 12\u201358.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang\nZan, Zeqi Lin, Jian-Guang Lou, and Weizhu\nChen. 2022. Codet: Code generation with gener-\nated tests. arXiv preprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, et al. 2021. Evaluating\nlarge language models trained on code. arXiv\npreprint arXiv:2107.03374.\nNamgyu Ho, Laura Schmid, and Se-Young Yun.\n2022. Large language models are reasoning teach-\ners. arXiv preprint arXiv:2212.10071.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes,\nand Yejin Choi. 2019.\nThe curious case\nof neural text degeneration.\narXiv preprint\narXiv:1904.09751.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander Rat-\nner, Ranjay Krishna, Chen-Yu Lee, and Tomas\nPfister. 2023.\nDistilling step-by-step!\noutper-\nforming larger language models with less training\ndata and smaller model sizes. arXiv preprint\narXiv:2305.02301.\nAlbert Q Jiang, Sean Welleck, Jin Peng Zhou,\nWenda Li, Jiacheng Liu, Mateja Jamnik, Timo-\nth\u00e9e Lacroix, Yuhuai Wu, and Guillaume Lample.\n2022a. Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.\narXiv\npreprint arXiv:2210.12283.\nDongfu Jiang, Bill Yuchen Lin, and Xiang Ren.\n2022b.\nPairreranker:\nPairwise reranking for\nnatural language generation.\narXiv preprint\narXiv:2212.10555.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng\nGao, and Bill Dolan. 2015. A diversity-promoting\nobjective function for neural conversation models.\narXiv preprint arXiv:1510.03055.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin\nDal Lago, et al. 2022. Competition-level code gen-\neration with alphacode. Science, 378(6624):1092\u2013\n1097.\nYixin Liu and Pengfei Liu. 2021.\nSimcls:\nA\nsimple framework for contrastive learning of\nabstractive summarization.\narXiv preprint\narXiv:2106.01890.\nTomoya Mizumoto and Yuji Matsumoto. 2016. Dis-\ncriminative reranking for grammatical error cor-\nrection with statistical machine translation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics: Human Language Tech-\nnologies, pages 1133\u20131138.\nShashi Narayan, Shay B Cohen, and Mirella La-\npata. 2018. Don\u2019t give me the details, just the\nsummary! topic-aware convolutional neural net-\nworks for extreme summarization. arXiv preprint\narXiv:1808.08745.\nLong Ouyang,\nJeffrey Wu,\nXu Jiang,\nDiogo\nAlmeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang,\nSandhini Agarwal,\nKatarina\nSlama, Alex Ray, et al. 2022. Training language\nmodels to follow instructions with human feed-\nback. Advances in Neural Information Processing\nSystems, 35:27730\u201327744.\nStanislas Polu, Jesse Michael Han, Kunhao Zheng,\nMantas Baksys, Igor Babuschkin, and Ilya\nSutskever. 2022.\nFormal mathematics state-\nment curriculum learning.\narXiv preprint\narXiv:2202.01344.\nMathieu Ravaut, Shafiq Joty, and Nancy F Chen.\n2022. Summareranker: A multi-task mixture-of-\nexperts re-ranking framework for abstractive sum-\nmarization. arXiv preprint arXiv:2203.06569.\nFreda Shi, Daniel Fried, Marjan Ghazvininejad,\nLuke Zettlemoyer, and Sida I Wang. 2022. Natu-\nral language to code translation with execution.\narXiv preprint arXiv:2204.11454.\nJonathan Uesato, Nate Kushman, Ramana Kumar,\nFrancis Song, Noah Siegel, Lisa Wang, Anto-\nnia Creswell, Geoffrey Irving, and Irina Higgins.\n2022. Solving math word problems with process-\nand outcome-based feedback.\narXiv preprint\narXiv:2211.14275.\nMartin J Wainwright. 2019.\nHigh-dimensional\nstatistics:\nA non-asymptotic viewpoint, vol-\nume 48. Cambridge university press.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, and Denny Zhou. 2022.\nSelf-\nconsistency improves chain of thought rea-\nsoning in language models.\narXiv preprint\narXiv:2203.11171.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily\nDinan, Kyunghyun Cho, and Jason Weston. 2019.\nNeural text generation with unlikelihood training.\narXiv preprint arXiv:1908.04319.\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li,\nMarkus Rabe, Charles Staats, Mateja Jamnik,\nand Christian Szegedy. 2022. Autoformalization\nwith large language models. Advances in Neural\nInformation Processing Systems, 35:32353\u201332368.\nTianyi Zhang, Tao Yu, Tatsunori B Hashimoto,\nMike Lewis, Wen-tau Yih, Daniel Fried, and\nSida I Wang. 2022.\nCoder reviewer rerank-\ning\nfor\ncode\ngeneration.\narXiv\npreprint\narXiv:2211.16490.\nSupplementary Material\nA\nProofs\nA.1\nProof of Theorem 2.1\nProof. This is true by definition for k = 1. For\nk > 1, let us assume that the number of categories\nL = 3. If the best generation g agrees with v on\nonly one of the elements, then wlog, let that be the\n1st one. Then the agreement score is (p1 + p\u2032\n2)/2\nwhere p\u2032\n2 < p2.\nLet the agreement score for a\ngeneration g\u2032 that does not agree at all with v be\n(p\u2032\n1 + p\u2032\u2032\n2)/2. However if for example p1 = 0.34, p\u2032\n1 =\n0.32, p\u2032\n2 = 0.01, p\u2032\u2032\n2 = 0.32, then g\u2032 will be selected\nover g.\nA.2\nProof of Theorem 2.2\nProof. It is true by assumption for k = 1.\nAs-\nsume it is true for k = t.\nThen that means\nthat given the self consistency assumption that\nat(ub, v) is the highest possible where at is the\nagreement until k = t. Then for t + 1, we know\nthat P\ni\u0338=b I(ut+1\nb\n= ut+1\ni\nis the highest (again by\nself-consistency assumption). Thus at+1 is also the\nhighest proving the theorem.\nA.3\nProof of Theorem 2.3\nFormally, let uj\ni\n\u223c Bernoulli(pj).\nLet b =\narg maxi\nPj pj \u00b7 uj\ni + (1 \u2212 pj) \u00b7 (1 \u2212 uj\ni)\n=\narg maxi\nPj uj\ni \u00b7 (2pj \u2212 1) (i.e.\nthe sequence se-\nlected by our method). Then we want a bound on\nE[Pk\nj uj\nb].\nProof. Let qi = P\nj uj\ni.\nAs all are iid, E[qi] =\nP\nj pj. We can upper bound this by upper bound-\ning E[maxi qi]. Note that uj\ni is subgaussian with\nparameter 1/2 as it\u2019s bounded in [0, 1].\nThus\nqi is subgaussian with parameter\n\u221a\nk/2.\nThus\nE[max qi \u2212 E[qj]] \u2264\nq\nk log n\n2\n=\u21d2\nE[max qi] \u2264\nP\ni pi +\nq\nk log n\n2\nwhere n is the number of genera-\ntions (Wainwright, 2019)\nB\nSimulation results\nWe setup our simulation as follows. Let d be the\nnumber of predicates, n the number of generations,\nand l the number of categories. Then for each pred-\nicate, we uniformly at random sample a categorical\ndistribution and then generate ui from that distri-\nbution. We then apply our criterion of picking the\nub that has the highest average fractional agree-\nment with all other ui and measure (1) the % of\ntimes we are able to retrieve the generation that has\nthe best agreement with v (2) the % agreement ub\nhas with the best possible generation out of the set.\nWe vary d, l between 2 and 50, and n between 25\nand 250. All our results are based on 1000 samples.\nThe results are in Figures 4 and 5.\nFor the first metric, we are able to retrieve the\nbest generation a very high fraction of the time\nwhen l is < 5 even when d goes to higher values.\nEven when l is larger, we are still able to retrieve\nthe best generation a non-trivial fraction of times\n\u2013 and notably our performance does not degrade\nmuch as n goes from 25 to 250.\nTurning our attention to the second metric, we\nare able to consistently get a generation close to the\nbest generation. This is especially true for small l\nwhere even when d increases to large values, we are\nable to get close to 100% agreement with the best\ngeneration. Even at high values of l however, we get\nrelatively good agreement with the best generation \u2013\nespecially compared to picking a random generation\n\u2013 a heuristic we consistently beat.\n2\n4\n6\n8\n10\nk\n0.55\n0.60\n0.65\n0.70\n0.75\nAccuracy\nHumanEval pass@k\nGSC\nGSC_ranked\n2\n4\n6\n8\n10\nk\n0.60\n0.65\n0.70\n0.75\nAccuracy\nMBPP pass@k\nGSC\nGSC_ranked\n2\n4\n6\n8\n10\nk\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nAccuracy\nMBPP-S pass@k\nGSC\nGSC_ranked\nFigure 3: pass@k for k > 1 for HumanEval, MBPP,\nMBPP-S\nC\nExperimental baselines\nAs mentioned earlier, we could not obtain Codex-\n001 and Codex-Cushman results on Xsum and\nMiniF2F due to the unexpected API shutdown.\nFor the BLEU and Rouge-2 metrics, we report the\nvalues divided by 100. In terms of our baselines,\nwe have\n1. Random selection - we randomly select a\ngeneration from the set of generations\n2. Ranking by mean log probability - we take\nthe average log probability across the tokens in\nMedoid\nMean-logp\nUCS\nWUCS\nConsensus-WUCS\nHumanEval\nCodex002\n0.515\n0.604\n0.615\n0.630\n0.633\nCodex001\n0.432\n0.484\n0.488\n0.507\n0.517\nCode-\nCushman\n0.4\n0.428\n0.434\n0.451\n0.454\nLlama-13B\n0.231\n0.221\n0.242\n0.248\n0.25\nLlama-30B\n0.29\n0.286\n0.324\n0.327\n0.327\nMBPP-S\nCodex002\n0.64\n0.626\n0.67\n0.643\n0.647\nCodex001\n0.594\n0.575\n0.594\n0.599\n0.605\nCode-\nCushman\n0.527\n0.521\n0.531\n0.541\n0.549\nLlama-13B\n0.355\n0.331\n0.340\n0.344\n0.347\nLlama-30B\n0.425\n0.408\n0.337\n0.436\n0.438\nMBPP\nCodex002\n0.631\n0.549\n0.651\n0.655\n0.659\nCodex001\n0.574\n0.58\n0.587\n0.596\n0.598\nCode-\nCushman\n0.435\n0.29\n0.479\n0.494\n0.503\nLlama-13B\n0.269\n0.3\n0.261\n0.305\n0.304\nLlama-30B\n0.346\n0.332\n0.351\n0.358\n0.359\nTable 2: Mean reciprocal rank of generations for HumanEval, MBPP, MBBP-S. Best results are colored in first,\nsecond.\nthe generation and select the generation with\nthe highest mean log probability\n3. Ranking using Medoid - we take the gen-\neration with the lowest mean distance to all\nother generations in our confidence weighted\nunigram space as used in WUCS.\n4. Coder Reviewer Ranker - This method has\ntwo variants \u2013 Normalized Reviewer (NR), and\nNormalized Coder Reviewer (NCR). NR com-\nputes the mean per token log p(x|y), where y\nis the generation and x is the prompt, and\nthen ranks based on this metric. On the other\nhand, NCR merges the mean log probabil-\nity ranking with NR, ranking according to\nlog p(x|y) + log p(y|x). As the state of the art\nin code reranking, these methods represent a\nstrong baseline.\nD\nComparison with Coder-Reviewer\nRanker\nThe comparison with the Code Reviewer Ranker\nbaseline, specifically with the Normalized Reviewer\n(NR) and Normalized Coder-Reviewer (NCR) vari-\nants, is in Table 3.\nAs the state of the art in\ncode reranking, these methods represent a strong\nbaseline. Our results demonstrate that the WUCS\nand Consensus-WUCS methods are highly compet-\nitive. Consensus-WUCS consistently outperforms\nNR and often surpasses NCR as well, despite the\nfact that NR and NCR require a second forward\npass, which doubles the inference cost and adds\nlatency overhead.\nIn the HumanEval dataset, Consensus-WUCS\nyields the highest accuracy for the Llama-13B and\nLlama-30B models.\nSimilarly, in the MBPP-S\ndataset, Consensus-WUCS delivers superior perfor-\nmance for the Llama-13B and Llama-30B models,\nand closely matches the NCR for Codex models. In\nthe MBPP dataset, the Consensus-WUCS method\nranks as the best for Code-Cushman, Llama-13B,\nand Llama-30B models.\nNotably in 40% of the experiments (6 out of 15),\nConsensus-WUCS outperforms all other methods,\nincluding the highly competitive NCR. Further-\nmore, Consensus-WUCS ranks second in 8 out of\nthe 15 experiments, reinforcing its strong perfor-\nmance across diverse models and datasets.\nOur results present evidence of the effectiveness\nof WUCS and Consensus-WUCS, which hold their\nown against much more heavyweight state-of-the-\nart methods and frequently deliver superior perfor-\nmance.\nE\nImprovements are consistent across\ndifferent generation temperatures\nIn Figure 6 (Supplement) we show how UCS rerank-\ning behaves for MBPP as the decoding sampling\ntemperature increases. While accuracy can vary\nacross temperatures, the ranking of the different\nmethods remains consistent.\nConsensus-WUCS\ndominates in terms of accuracy for most of the tem-\nperature regimes until you hit the temperature of\n1. Importantly, for lower temperatures where we\nget the best results, Both Consensus-WUCS as well\nas WUCS get the best accuracy. While just UCS\nis on par with mean log-probability ranking until\na temperature of 0.4 after which it falls behind,\nwe note that UCS does not use any probability\ninformation about the generation and thus a fair\ncomparison would be to that of random ranking\nwhich it is consistency better than for almost the\nentire temperature range.\nWUCS\nConsensus-WUCS\nN. Reviewer\nN. Coder-Reviewer\nHumanEval\nCodex002\n0.558\n0.568\n0.524\n0.576\nCodex001\n0.426\n0.445\n0.42\n0.482\nCode-Cushman\n0.373\n0.381\n0.358\n0.385\nLlama-13B\n0.187\n0.192\n0.164\n0.181\nLlama-30B\n0.263\n0.267\n0.219\n0.241\nMBPP-S\nCodex002\n0.58\n0.589\n0.559\n0.595\nCodex001\n0.535\n0.546\n0.509\n0.55\nCode-Cushman\n0.472\n0.488\n0.455\n0.512\nLlama-13B\n0.266\n0.277\n0.228\n0.266\nLlama-30B\n0.363\n0.373\n0.302\n0.325\nMBPP\nCodex002\n0.587\n0.594\n0.631\n0.592\nCodex001\n0.52\n0.525\n0.532\n0.545\nCode-Cushman\n0.405\n0.42\n0.398\n0.339\nLlama-13B\n0.195\n0.199\n0.185\n0.2\nLlama-30B\n0.287\n0.294\n0.289\n0.283\nTable 3: Comparison with Coder-Reviewer Reranker. Best results are colored in first, second.\nF\nVarying the maximum n-gram length\ndoes not change results\nAs mentioned in Section 3, UCS only considers uni-\ngrams. Here we consider Ngram Consistency Score\n\u2013 the more generalized version. To account for the\nfact that a sentence will have fewer n-grams, the\nmore n increases, we multiply p(ti,k\nj ) by\n|gi|\n|gi|\u2212|ti,k\nj\n|\u22121\nwhere ti,k\nj\nis now the kth appearance of the jth\nn-gram in the ith generation. In Figure 7 (Supple-\nment), we show how the ranking behaves as the n\nincreases. As can be seen, while there is a slight\nimprovement going from n = 1 to n = 4, the im-\nprovement flattens after that point. 4-grams is also\nwhat is conventionally used when computing BLEU\nscore so it is interesting that the same value ends\nup being optimal in the drastically different setting\nof code generation with each word being a token\ninstead of an English word.\nG\nIncreasing number of samples maintains\nreranking strength\nIn Figure 8 (Supplement), we show how the perfor-\nmance changes for MBPP and Xsum as the number\nof samples increases. All variants of UCS are able\nto maintain accuracy (although Consensus-WUCS\nsees a drop in the beginning for Xsum but main-\ntains its performance subsequently) even as the\nnumber of samples increases from 5 to 100. Mean-\nwhile, the mean log probability ranking drastically\ndeclines in terms of accuracy, quickly falling below\neven random selection. This is likely due to the\ntendency of mean log probability ranking to choose\ndegenerate sequences (Holtzman et al., 2019) which\nUCS variants seem to be able to avoid.\nH\nAda model embeddings also give a\nboost\nTo understand how generalizable the intuition be-\nhind the GCS metric (as opposed to the UCS met-\nric) is for other similarity functions, we took the\ngenerations and used the text-ada-embedding-002\nmodel by OpenAI to generate embedding vectors for\nthe generations. We then used cosine similarity be-\ntween the generations as the similarity function and\nused GCSCosine Similarity to rank. The results are in\nTable 4. Using OpenAI embeddings as well results\nin improved performance over Random selection as\nwell as mean log probability ranking validating our\nintuition that choosing the generation that is on\naverage, the most similar to all other generations is\na good ranking metric. That said, this particular\nsimilarity function underperforms UCS, especially\nfor code generation so we did not investigate it\nfurther.\nI\nNormalizing inner product degrades\nperformance\nNeural generation models are well known to gener-\nate repetitive sequences (Zhang et al., 2022; Welleck\net al., 2019). In (Welleck et al., 2019), they mod-\nify the standard log-likelihood object for language\nmodels to minimize the probability of tokens imme-\ndiately preceding the current token. This effectively\npushes the model to generate unique new tokens\nand they show significant improvements in their\nmodel after they do this. If we normalize the inner\nproduct, then we would be effectively \"canceling\nout\" the contribution to the similarity score by\nhaving more unique tokens.\nWe evaluated the effect of normalizing the in-\nner product by the vector norms. To understand\nbetter whether our performance is just an effect\nof selecting longer and more diverse sequences or\nwhether the similarity metric itself is useful as well,\nwe ran ablations where we evaluated ranking based\non the longest sequence, as well as based on mean\nacross the elements of vi as defined in Section 3\n\u2013 which takes into account the sequence diversity.\nThe results are in Table 5 in the Supplement. Nor-\nmalization results in a decline in performance. Fur-\nthermore neither ranking by the longest sequence\nRandom\nMean-logp\nGSCAda\nConsensus-WUCS\nHumanEval\n0.437\n0.533\n0.487\n0.568\nMBPP\n0.533\n0.416\n0.579\n0.594\nMBBP-S\n0.549\n0.568\n0.601\n0.589\nMiniF2F (BLEU)\n0.558\n0.556\n0.584\n0.562\nXsum (Rouge-2)\n0.197\n0.214\n0.219\n0.219\nTable 4: Performance of cosine similarity of ada embedding as the similarity function. Metric is accuracy\nfor HumanEval, MBPP, MBPP-S and BLEU for MiniF2F. Best results are colored in first, second.\nRandom\nWUCS\nWUCS-normalized\nLongest\nMost Diverse\nHumanEval\nCodex002\n0.435\n0.558\n0.462\n0.441\n0.51\nCodex001\n0.345\n0.426\n0.382\n0.338\n0.369\nLlama-30B\n0.207\n0.263\n0.235\n0.208\n0.215\nRandom\nWUCS\nWUCS-normalized\nLongest\nMost Diverse\nMBPP\nCodex002\n0.536\n0.587\n0.576\n0.529\n0.52\nCodex001\n0.475\n0.52\n0.517\n0.475\n0.457\nLlama-30B\n0.262\n0.287\n0.278\n0.263\n0.245\nRandom\nWUCS\nWUCS-normalized\nLongest\nMost Diverse\nXsum\nCodex002\n0.197\n0.215\n0.211\n0.197\n0.188\nLlama-30B\n0.107\n0.122\n0.12\n0.107\n0.116\nGPT-J\n0.065\n0.07\n0.07\n0.065\n0.069\nTable 5: Impact of normalization. Best results are\ncolored in first, second.\nnor ranking by sequence diversity is sufficient to give\nthe results we see as neither result in a consistent\nimprovement even against the Random selection\nbaseline.\nHumanEval\nMBPP\nXsum\nMiniF2F\nWMT-14 French To English\nWMT14 German to English\nRatio\n1.95\n1.34\n1.21\n1.08\n1.07\n1.08\nTable 6: Diversity ratio between best and worst generations from Codex002 model for various datasets\nlogprobs used\nMedoid\nMean-logp\nWUCS\nConsensus-WUCS\nHumanEval\nCodex002\n0.437\n0.539\n0.558\n0.568\nCodex001\n0.354\n0.408\n0.426\n0.445\nCode-\nCushman\n0.335\n0.355\n0.373\n0.381\nLlama-13B\n0.17\n0.17\n0.187\n0.192\nLlama-30B\n0.225\n0.228\n0.263\n0.267\nMBPP-S\nCodex002\n0.583\n0.57\n0.580\n0.589\nCodex001\n0.532\n0.515\n0.535\n0.546\nCode-\nCushman\n0.467\n0.456\n0.472\n0.488\nLlama-13B\n0.284\n0.27\n0.266\n0.277\nLlama-30B\n0.357\n0.348\n0.363\n0.373\nMBPP\nCodex002\n0.563\n0.512\n0.587\n0.594\nCodex001\n0.505\n0.503\n0.520\n0.525\nCode-\nCushman\n0.343\n0.319\n0.405\n0.420\nLlama-13B\n0.202\n0.197\n0.195\n0.199\nLlama-30B\n0.276\n0.273\n0.287\n0.294\nTable 7: Accuracy of generated code for HumanEval,\nMBPP, MBBP-S. Best results are colored in first,\nsecond. Italics for best in category (logprobs used vs\nnot)\nlogprobs used\nMedoid\nMean-logp\nWUCS\nConsensus-WUCS\nMiniF2F BLEU\nCodex002\n58.2\n52.9\n55.8\n56.2\nLlama-13B\n24.9\n24.2\n24.7\n24.8\nLlama-30B\n26.4\n25.6\n25.7\n25.7\nGPT-J\n24.8\n24\n24.8\n24.8\nXsum Rouge2\nCodex002\n21.8\n21.4\n21.5\n21.9\nLlama-13B\n10.3\n10.3\n10.6\n10.6\nLlama-30B\n12\n12.2\n12.2\n12.3\nGPT-J\n6.9\n6.6\n7\n6.9\nXsum RougeL\nCodex002\n36.3\n35.1\n35.3\n35.6\nLlama-13B\n20.7\n20.3\n21\n20.9\nLlama-30B\n22.7\n22.8\n23.1\n23.1\nGPT-J\n17.5\n16.6\n17.8\n17.5\nWMT14 French \u2192\nEnglish BLEU\nCodex002\n35.9\n36.6\n36.5\n37\nLlama-13B\n4.2\n4.5\n4.5\n4.6\nLlama-30B\n4\n4\n4.1\n4.1\nGPT-J\n3.8\n3.9\n4\n4\nWMT14 German \u2192\nEnglish BLEU\nCodex002\n31.2\n33.2\n32.1\n34\nLlama-13B\n3.1\n4\n3.5\n3.6\nLlama-30B\n3.5\n3.9\n3.8\n3.9\nGPT-J\n3.2\n3.2\n3.3\n3.3\nAqua\nMultiarith\nStrategyQA\nCodex001\n+2.8%\n+4.4%\n+2.5%\nCodex002\n-\n+7.1%\n+3.3%\nLaMDA-137\n+1.9%\n+4.4%\n+3.9%\nUL2-20B\n-1%\n-0.1%\n-0.1%\nTable 8: Ratio of average GSC score for correct\ngenerations by average GSC score for incorrect\ngenerations.\n2\n5\n10\n25\n50\nd\n50\n60\n70\n80\n90\n%\n% of times best generation extracted, n=25, l=2\n2\n5\n10\n25\n50\nd\n40\n50\n60\n70\n80\n90\n%\n% of times best generation extracted, n=100, l=2\n2\n5\n10\n25\n50\nd\n40\n50\n60\n70\n80\n90\n100\n%\n% of times best generation extracted, n=250, l=2\n2\n5\n10\n25\n50\nl\n20\n30\n40\n50\n60\n70\n80\n%\n% of times best generation extracted, n=25, d=2\n2\n5\n10\n25\n50\nl\n30\n40\n50\n60\n70\n%\n% of times best generation extracted, n=25, d=10\n2\n5\n10\n25\n50\nd\n70\n75\n80\n85\n90\n95\n%\n(agreement/best agreement)*100, n=25, l=2\n2\n5\n10\n25\n50\nd\n65\n70\n75\n80\n85\n90\n95\n100\n%\n(agreement/best agreement)*100, n=100, l=2\n2\n5\n10\n25\n50\nd\n70\n75\n80\n85\n90\n95\n100\n%\n(agreement/best agreement)*100, n=250, l=2\n2\n5\n10\n25\n50\nl\n20\n40\n60\n80\n%\n(agreement/best agreement)*100, n=25, d=2\n% of best\nAvg % of best\n2\n5\n10\n25\n50\nl\n20\n40\n60\n80\n100\n%\n(agreement/best agreement)*100, n=25, d=10\n% of best\nAvg % of best\n0.2\n0.4\n0.6\n0.8\n1.0\nTemperature\n0.30\n0.35\n0.40\n0.45\n0.50\nAccuracy\nMBPP accuracy vs temperature\nRandom\nLogprob\nUCS\nWUCS\nConsensus-WUCS\nFigure 6: Accuracy for MBPP as the decoding sampling\ntemperature increases.\n2\n4\n6\n8\n10\nMax ngram length\n0.580\n0.585\n0.590\n0.595\n0.600\nAccuracy\nMBPP accuracy vs max ngram length\nUCS\nWUCS\nConsensus-WUCS\nFigure 7: Accuracy for MBPP as the n in n-gram\nincreases.\n20\n40\n60\n80\n100\nNumber of generations\n0.3\n0.4\n0.5\n0.6\nAccuracy\nMBPP accuracy vs number of generations\nRandom\nLogprob\nUCS\nWUCS\nConsensus-WNCS\n20\n40\n60\n80\n100\nNumber of generations\n0.340\n0.345\n0.350\n0.355\n0.360\n0.365\n0.370\nRouge-L\nXsum Rouge-L vs number of generations\nRandom\nLogprob\nUCS\nWUCS\nConsensus-WNCS\nFigure 8: Accuracy for MBPP and Rouge-2 for Xsum\nas the number of generations increase.\n"
  },
  {
    "title": "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation",
    "link": "https://arxiv.org/pdf/2307.06940.pdf",
    "upvote": "9",
    "text": "Animate-A-Story:\nStorytelling with Retrieval-Augmented Video Generation\nYINGQING HE\u2217, Hong Kong University of Science and Technology, China\nMENGHAN XIA\u2217, Tencent AI Lab, China\nHAOXIN CHEN\u2217, Tencent AI Lab, China\nXIAODONG CUN, Tencent AI Lab, China\nYUAN GONG, Tsinghua Shenzhen International Graduate School, Tsinghua University, China\nJINBO XING, The Chinese University of Hong Kong, China\nYONG ZHANG\u2020, Tencent AI Lab, China\nXINTAO WANG, Tencent AI Lab, China\nCHAO WENG, Tencent AI Lab, China\nYING SHAN, Tencent AI Lab, China\nQIFENG CHEN\u2020, Hong Kong University of Science and Technology, China\nA <new1> teddy bear is sleeping in his comfortable bed.\nA <new1> teddy bear walks through a beautiful garden in winter with a big snowstorm .\nA <new1> teddy bear walks through the corridors of a golden palace .\na <new1> teddy bear talks with Father Christmas before a big castle\na <new1> teddy bear rides on a bicycle, passes by a snowy forest\nA <new1> teddy bear is drinking a glass of wine in a room, with flowers and a window behind him.\n1\n2\n3\n4\n5\n6\nFig. 1. Our synthesized videos with a consistent character (user-provided) and storyline. By utilizing text-retrieved video clips as structure guidance, our system\nmanages to generate high-quality videos of similar structures, while also accommodating arbitrary scene appearances based on text prompts. Furthermore, our\nproposed concept personalization method ensures consistent character rendering across different plot scenarios. Each clip is visualized with three keyframes.\nGenerating videos for visual storytelling can be a tedious and complex pro-\ncess that typically requires either live-action filming or graphics animation\nrendering. To bypass these challenges, our key idea is to utilize the abun-\ndance of existing video clips and synthesize a coherent storytelling video by\ncustomizing their appearances. We achieve this by developing a framework\ncomprised of two functional modules: (i) Motion Structure Retrieval, which\nprovides video candidates with desired scene or motion context described\nby query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which\ngenerates plot-aligned videos under the guidance of motion structure and\n\u2217First authors\n\u2020Corresponding authors\nProject page : https://videocrafter.github.io/Animate-A-Story\ntext prompts. For the first module, we leverage an off-the-shelf video re-\ntrieval system and extract video depths as motion structure. For the second\nmodule, we propose a controllable video generation model that offers flexible\ncontrols over structure and characters. The videos are synthesized by fol-\nlowing the structural guidance and appearance instruction. To ensure visual\nconsistency across clips, we propose an effective concept personalization\napproach, which allows the specification of the desired character identities\nthrough text prompts. Extensive experiments demonstrate that our approach\nexhibits significant advantages over various existing baselines.\nAdditional Key Words and Phrases: Story Visualization, Video Diffusion\nModels, Retrieval-augmented Generation, Personalized Generation\narXiv:2307.06940v1  [cs.CV]  13 Jul 2023\n2\n\u2022\nYingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, and Qifeng Chen\n1\nINTRODUCTION\nCreating engaging storytelling videos is a complex and laborious\nprocess that typically involves live-action filming or CG animation\nproduction. This technical nature not only demands significant\nresources from professional content creators but also creates barriers\nfor the general public in effectively utilizing this powerful medium.\nRecently, significant progress has been made in text-to-video (T2V)\ngeneration, allowing for the automatic generation of videos based\non textual descriptions [He et al. 2022; Ho et al. 2022a; Singer et al.\n2022; Zhou et al. 2022].\nHowever, the effectiveness of these video generation techniques is\nstill limited, yielding results that fall short of expectations and hinder\ntheir practical application. Additionally, the layout and composition\nof the generated video cannot be controlled through text, which is\ncrucial for visualizing an appealing story and filming a movie. For\nexample, close-ups, long shots, and composition can assist directors\nin conveying implicit information to the audience. Current text-to-\nvideo generation models can hardly generate proper motions and\nlayouts that meet the requirement of film.\nTo overcome these challenges, we propose a novel video genera-\ntion approach incorporating the abundance of existing video content\ninto the T2V generation process, which we refer to as retrieval-\naugmented video generation. Specifically, our approach retrieves\nvideos from external databases based on text prompts and utilizes\nthem as a guidance signal for the T2V generation. Building on this\nidea, our approach also enables users to have greater control over\nthe layout and composition of the generated videos when animating\na story, by utilizing the input retrieved videos as a structure refer-\nence. Additionally, the quality of the generated videos is enhanced\nby leveraging the rich knowledge and information contained in the\nretrieved videos, which can be used to improve the realism and\ncoherence of the generated scenes. The text prompt is now responsi-\nble for rendering the appearances of scenes and objects to generate\nnovel videos.\nHowever, such retrieval-guided video generation processes still\nsuffer from the inconsistency problem of the character across differ-\nent video clips. Besides, the character\u2019s appearance is controlled by\nthe text prompt and generated in a stochastic way which lacks user\ncontrol. To further tackle this issue, we study existing literature on\npersonalization [Ruiz et al. 2023] to finetune the generation model\nto re-render the appearance of the character. and propose a novel\napproach (TimeInv) to better represent personalized concepts and\nimprove performance.\nBy incorporating the two core modules: retrieval-enhanced T2V\ngeneration and video character rerendering, our approach provides\na more efficient and accessible way for content creators to pro-\nduce high-quality animated videos. To justify its effectiveness, we\nevaluate our method from the following perspectives: First, our\nretrieval-enhanced T2V generation model is compared against ex-\nisting baselines, demonstrating notable superiority in video genera-\ntion performance. Second, we justify the advantage of our proposed\npersonalization method in relation to existing competitors. Fur-\nthermore, we conduct comprehensive experiments of the overall\neffectiveness of our proposed storytelling video synthesis frame-\nwork, suggesting its potential for practical applications.\nOur contribution is summarized as the following:\n\u2022 We present a novel retrieval-augmented paradigm for sto-\nrytelling video synthesis, which enables the use of existing\ndiverse videos for storytelling purposes for the first time.\nExperimental results demonstrate the framework\u2019s effec-\ntiveness, positioning it as a novel video-making tool with\nremarkable convenience.\n\u2022 We propose an adjustable structure-guided text-to-video\nmodel, which effectively resolves the conflict between struc-\nture guidance and character generation.\n\u2022 We propose TimeInv, a new concept personalization ap-\nproach that outperforms existing competitors notably.\n2\nRELATED WORK\n2.1\nVideo Generation\nNumerous earlier works [Saito et al. 2017; Skorokhodov et al. 2022;\nTulyakov et al. 2018; Vondrick et al. 2016; Wu et al. 2021] focus\non unconditional video generation methods, employing generative\nadversarial networks (GANs) or variational auto-encoder (VAE) to\nmodel video distributions. For instance, VGAN [Vondrick et al. 2016]\nseparately models the foreground and background by mapping ran-\ndom noise to space-time cuboid feature maps. The fused feature\nmap signifies the generated video and is input into a discriminator.\nTGAN [Saito et al. 2017] generates a noise sequence with a tem-\nporal generator and then uses an image generator to transform it\ninto images. These images are concatenated and fed into a video\ndiscriminator. StyleGAN-V [Skorokhodov et al. 2022] leverages the\ncapabilities of StyleGAN. It uses multiple random noises to control\nmotion and an additional noise to manage appearance.\nSeveral methods [Ge et al. 2022; Yan et al. 2021; Yu et al. 2023a]\naim to capture spatio-temporal dependencies using transformers\nin the latent space. Initially, they project videos into a latent space\nby learning a VAE or VQGAN [Esser et al. 2021], followed by train-\ning a transformer to model the latent distribution. For example,\nTATS [Ge et al. 2022] trains a time-agnostic VQGAN and subse-\nquently learns a time-sensitive transformer based on the latent\nfeatures. VideoGPT [Yan et al. 2021] follows a similar pipeline. To\nenable text control over generated content, some works [Hong et al.\n2022; Villegas et al. 2022] extract visual and text tokens, projecting\nthem into the same latent space. A transformer is consistently used\nto model the interdependencies among them.\nRecently, text-to-image (T2I) generation [Ramesh et al. 2022; Rom-\nbach et al. 2022; Saharia et al. 2022] has achieved significant advance-\nments in generating high-quality images, primarily due to diffusion-\nbased models. Capitalizing on the progress of T2I generation, the\nfield of text-to-video generation has experienced breakthroughs as\nwell. VDM [Ho et al. 2022b] is the first work employing diffusion\nmodels for video generation. Make-a-video [Singer et al. 2022] and\nImagen Video [Ho et al. 2022a] are cascade models that initially\nmodel video distribution at low-resolution and then apply spatio-\ntemporal interpolation to increase the resolution and time duration.\nInspired by LDM [Rombach et al. 2022], several works [Blattmann\net al. 2023; He et al. 2022; Luo et al. 2023; Mei and Patel 2022; Yu\net al. 2023b; Zhou et al. 2022] extend LDM for video generation. For\nexample, LVDM [He et al. 2022] inflates LDM into a video version\nAnimate-A-Story:\nStorytelling with Retrieval-Augmented Video Generation\n\u2022\n3\nby introducing temporal attention layers. It employs the pretrained\nLDM as initialization and trains the learnable parameters with video\ndata. Similar to LDM, the text embedding is injected into the UNet\nusing the cross-attention mechanism. Video LDM [Blattmann et al.\n2023] shares the same concept as LVDM, but with the distinction\nthat it fixes the spatial weights of LDM.\n2.2\nStructure-guided Video Generation\nMirroring the evolution of T2I, numerous works [Wang et al. 2023a,b;\nXing et al. 2023; Yang et al. 2023; Zhang et al. 2023] investigate the ca-\npability of conditional video generation based on pretrained text-to-\nimage or text-to-video models. For instance, Make-Your-Video [Xing\net al. 2023] uses depth as an additional condition besides text. The\nspatial weights of Stable Diffusion are fixed, while the newly added\ntemporal weights are learned on video data. Since depth is extracted\nfrom the video, it can re-render the appearance of the source video.\nFollow-Your-Pose [Ma et al. 2023] utilize pose as a condition to\nguide the human-like character video synthesis process. VideoCom-\nposer [Wang et al. 2023b], an extension of Composer [Huang et al.\n2023], takes multiple types of images as conditions, such as RGB im-\nages, sketches, depths, etc. These conditions are fused in the latent\nspace and interact with the UNet via cross attention.\n2.3\nConcept Customization\nGenerating an image with a specified object is referred to as cus-\ntomization or personalization. Numerous works [Alaluf et al. 2023;\nGal et al. 2022; Kumari et al. 2023; Ruiz et al. 2023; Wei et al. 2023]\nexplore this task from various perspectives. Textual inversion [Gal\net al. 2022], the first inversion work on Stable Diffusion, optimizes\na token without tuning the model for a given object\u2019s images. In\ncontrast, Dreambooth [Gal et al. 2022] learns a token and fine-tunes\nthe entire model parameters. Multi-concept [Kumari et al. 2023]\nis the first to propose a method for inverting multiple concepts.\nELITE [Wei et al. 2023] trains an encoder to map visual images to\ntext embeddings for customized text-to-image generation, rather\nthan optimization. NeTI [Alaluf et al. 2023] introduces a novel text-\nconditioning latent space that depends on both the timestep and\nUNet layers. It learns a mapping that projects timestep and layer\nindex into the embedding space.\n3\nMETHOD\nOur goal is to develop a framework that can automatically generate\nhigh-quality storytelling videos based on storyline scripts or with\nminimal interactive effort. To achieve this, we propose to retrieve\nexisting video assets to enhance the performance of T2V generation\n(see Sec. 3.1). Specifically, we extract the structures from retrieved\nvideos, which will then serve as guidance signals provided to the\nT2V process (see Sec. 3.2). Additionally, we perform video character\nrerendering based on the proposed TimeInv approach to synthesize\nconsistent characters across different video clips (see Sec. 3.3). In\nthe following sections, we will delve into the key technical designs\nthat enable its functionality.\nPlot 1\nMotion structure \nretrieval\nStoryboard description\nStructure-guided\ntext-to-video synthesis\nText prompts\nVideo \ndatabase\nText queries\nPlot i\nPlot n\n\u22ef\n\u22ef\n\u22ef\n\u22ef\nStory script\nFig. 2. Flowchart of our retrieval-augmented video synthesis framework.\nGiven a textual story script, we first extract the key plots and modulate\ntheir descriptions as text queries and prompts. Each plot is transformed\ninto a generated video clip through two modules: a video retrieval system\nand a structure-guided text-to-video model.\n3.1\nRetrieval-augmented Text-to-Video Generation\nAs illustrated in Fig. 2, our video generation framework involves\nthree procedures: text processing, video retrieval, and video synthe-\nsis. In the text processing stage, we extract the key plots from the\nstory script through storyboard analysis. To simplify the problem,\nwe regulate an individual plot as a single event without a shot tran-\nsition. For instance, \"a boy ran into a wolf in the forest\" is a single\nplot, while \"a boy ran into a wolf in the forest and he killed the wolf\nwith a gun\" should be separated into two plots. For each plot, we\nfurther adjust and decorate the description so that they can serve\nas effective text queries and text prompts respectively. This stage\nis completed manually or using the assistance of large language\nmodels (LLMs) like GPT-4 [OpenAI 2023].\nAfter that, we process each plot separately using two sequen-\ntially conducted modules. Given the text query, we can obtain video\ncandidates showing desired scenario through an off-the-shelf text-\nbased video retrieval engine [Bain et al. 2021] that associates with\na database with about 10M open-world videos collected from the\nInternet. Since the video appearance may not match the plot pre-\ncisely, we only take the motion structure of it by applying a depth\nestimation algorithm to it. This extends the usability of existing\nvideos. As the example illustrated in Fig. 3, to synthesize the video\nof \"Santa Claus playing with a wolf in the forest\", we can use the\n4\n\u2022\nYingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, and Qifeng Chen\n4\nText\u00a0prompt:\u00a0An\u00a0astronaut\u00a0walking\u00a0on\u00a0Mars,\u00a0strong\u00a0sandstorm\u00a0blowing\n\u0d48 \u123a\ud835\udc47 \u0d46 1\u123b \n\ud835\udc67\u0bcd\n\ud835\udc67\u0bcd\u0b3f\u0b35\nCLIP\u00a0text\u00a0encoder\n\ud835\udc67\u0b34\nStructure\u00a0encoder\nGenerated\u00a0video\n\u22ef\n\u22ef\n\u22ef\n\u22ef\nDepth\u00a0guidance\nFig. 3. Overview of our adjustable structure-guided text-to-video model.\nWe use the depth information from source videos to guide the video syn-\nthesis process. The model consists of two branches: a general text-to-video\nsynthesis branch which is a video diffusion model in the latent space, and a\nside branch for encoding and imposing structure control. The controlling\nmechanism is elementwise feature addition. Notably, the depth control is\nadjustable and this property is crucial for further character rerendering,\nwhich we will illustrate in Sec. 3.3.\nmotion structure in the video of \"a man playing with a dog in the\npark\", which is quite common in the video database. Utilizing the\nmotion structure as guidance, we can synthesize plot-aligned videos\nthrough text prompts. Next, we describe the structure-guided T2V\nmodel in detail.\n3.2\nStructure-Guided Text-to-Video Synthesis\nPreliminary. Denoising Diffusion Probabilistic Models (DDPM) [Ho\net al. 2020], also called Diffusion Models (DM) for short, learn to\nmodel an empirical data distribution \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(x) by building the map-\nping from a standard Gaussian distribution to the target distribution.\nParticularly, the forward process is formulated as a fixed diffusion\nprocess that is denoted as:\n\ud835\udc5e(x\ud835\udc61 |x\ud835\udc61\u22121) B N (x\ud835\udc61,\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61x\ud835\udc61\u22121, \ud835\udefd\ud835\udc61\ud835\udc3c).\n(1)\nThis is a Markov chain that adds noise into each sample x0 gradually\nwith the variance schedule \ud835\udefd\ud835\udc61 \u2208 (0, 1) where \ud835\udc61 \u2208 {1, ...,\ud835\udc47 } with \ud835\udc47\nbeing the total steps of the diffusion chain. The reverse process\nis realized through a denoiser model f\ud835\udf03 that takes the diffused x\ud835\udc61\ntogether with the current time step \ud835\udc61 as input and it is optimized by\nminimizing the denoising score matching objective:\n\ud835\udc3f(\ud835\udf03) = Ex0\u223c\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e,\ud835\udc61 \u2225\ud835\udf16\ud835\udc61 \u2212 f\ud835\udf03 (x\ud835\udc61; c,\ud835\udc61)\u22252\n2,\n(2)\nwhere c is optional conditioning information (e.g. text embedding)\nand the supervision \ud835\udf16\ud835\udc61 is a randomly sampled noise used in the\nforward diffusion process: x0 \u2192 x\ud835\udc61. Extended from DM, LDM\nformulates the diffusion and denoising process at a latent space.\nThat is usually a compressive representation space that is separated\nand learned through a Variational Autoencoder(VAE) [Kingma and\nWelling 2014] comprised of an encoder E and a decoder D.\nWe employ a conditional LDM to learn controllable video synthe-\nsis, as the overview depicted in Fig. 3. In our approach, the videos\nare transformed (or reconstructed) into (or from) the latent space\nin a frame-wise manner. Specifically, our encoder E downsamples\nthe input RGB-image x \u2208 R3\u00d7\ud835\udc3b \u00d7\ud835\udc4a with a downsampling factor\nof 8 and outputs a latent representation z \u2208 R4\u00d7\ud835\udc3b \u2032\u00d7\ud835\udc4a \u2032, where\n\ud835\udc3b\u2032 = \ud835\udc3b/8,\ud835\udc4a \u2032 = \ud835\udc4a /8, enabling the denoiser model to work on a\nmuch lower dimensional data and thus improves the running time\nand memory efficiency. The latent diffusion model is conditioned\non both the motion structure and text prompt. Given a video clip\nx \u2208 R\ud835\udc3f\u00d73\u00d7\ud835\udc3b \u00d7\ud835\udc4a with \ud835\udc3f frames, we obtain its latent representation\nz through z = E(x), where z \u2208 R\ud835\udc3f\u00d74\u00d7\ud835\udc3b \u2032\u00d7\ud835\udc4a \u2032. In the fixed forward\nprocess, z B z0 is diffused into a pure noise tensor z\ud835\udc47 by \ud835\udc47 steps.\nIn the reverse process (namely the denoising process), the denoiser\nmodel predicts the previous-step data z\ud835\udc61\u22121 from current noisy data\nz\ud835\udc61 by taking the embedded text prompt and frame-wise depth maps\nas conditions, and a clean data z\u2032\n0 can be sampled from random noise\nz\ud835\udc47 in a recurrent manner. Specifically, the denoiser model is a 3D\nU-Net which we adopt the architecture from [He et al. 2022]. We\nadopt CLIP [Radford et al. 2021] as the textual encoder to extract\nvisual-aligned tokens from the input text prompts. For the depth\nestimator, we choose the Midas depth estimation model [Ranftl et al.\n2022] due to its robustness on various videos. The depth maps are\nencoded through a CNN-based structure encoder and the multi-\nscale features are added to the feature maps of the denoiser U-Net\nfor structural modulation. Different from structure control, seman-\ntic control via textual prompt affects the backbone features via a\ncross-attention module[Rombach et al. 2022].\n3.3\nVideo Character Rerendering\nAbove mentioned video synthesis framework manages to provide\nvideos with high-quality and diverse motion. However, the gen-\nerated character appearance controlled by text prompts varies in\ndifferent video clips. To overcome this challenge, we formulate this\nproblem with the following objective: Given a pre-trained video gen-\neration model, and user-specified characters, our goal is to generate\nconsistent characters across different video clips; we referred to this\ntask as video character rerendering. To do so, we examined existing\nliterature on personalization approaches of image diffusion models.\nHowever, there are several challenges associated with directly ap-\nplying these methods for video personalization. 1) How to leverage\nimage data for personalizing video models? One straightforward\napproach for video personalization is to utilize video data that por-\ntrays a specific character. However, the video data of a consistent\ncharacter is much harder to collect than images. 2) How to adjust\nthe tradeoff between the concept compositionality and the charac-\nter fidelity? This challenge also exhibits in image personalization\nliterature. In this section, we will explain preliminary approaches\nand our method in detail.\nPreliminary: Textual Inversion. Textual Inversion is an image\npersonalization approach that aims to represent a new concept to\na new token \ud835\udc46\u2217 and learns a corresponding new token embedding\nvector \ud835\udc63\u2217 in the CLIP text encoder \ud835\udc50\ud835\udf03. The \ud835\udc63\u2217 is directly optimized\nwith 3-10 images depicting a specific concept. The training objective\nis the same as the original loss of diffusion models, which can be\ndefined as:\n\ud835\udc63\u2217 = arg min\n\ud835\udc63\nE\ud835\udc67\u223cE(x),y,\ud835\udf16\u223cN(0,I),\ud835\udc61\nh\n\u2225\ud835\udf16 \u2212 f\ud835\udf03 (z\ud835\udc61,\ud835\udc61, c)\u22252\n2\ni\n,\n(3)\nAnimate-A-Story:\nStorytelling with Retrieval-Augmented Video Generation\n\u2022\n5\nU-Net\nDreamBooth\nTextual Inversion\n\u201cphoto of a V* cat\u201d\nphoto of a V* cat\nt\nCharacter \nimages \nfor training\nFrozen\nTrainable\nc\n\ud835\udc33!\nCustom Diffusion\nt\nc\n\ud835\udc33!\nOurs\nt\nc\n\ud835\udc33!\n\ud835\udc16!\n\ud835\udc16\"\n\ud835\udc16#\nTokenizer\nText transformer\nToken embedding\nCLIP\ntext \nencoder\nt\nc\n\ud835\udc33!\n\ud835\udc16!\n\ud835\udc16\"\n\ud835\udc16#\n\ud835\udc16!$\n\ud835\udc16\"\n$\n\ud835\udc16#$\n\u2a01\n\u2a01\n\u2a01\n \u00d7T\nphoto of a V* cat\nphoto of a V* cat\nFig. 4. Concept diagram of different approaches for personalization. To\novercome the inconsistency problem of generated characters, we study\nexisting personalization approaches and propose a new method to rerender\nthe appearance of the target character. We keep all parameters from the CLIP\ntext encoder and the denoiser U-Net frozen and learn timestep-dependent\ntoken embeddings to represent the semantic features of target characters.\nAdditionally, we insert a new branch to the projection layers of q, k, v in\nattention modules and modulate the pre-trained weight to better represent\nthe character.\nAfter training, the new token can be combined with other word\ntokens to form a sentence. This token sequence can then be passed\nthrough the text encoder to obtain conditional text token embed-\ndings that facilitate the control of image generation for producing\nthe desired concept.\nTimestep-variable textual inversion (TimeInv). However,\noptimizing the single token embedding vector has limited expres-\nsive capacity because of its limited optimized parameter size. In\naddition, using one word to describe concepts with rich visual fea-\ntures and details is very hard and insufficient. Hence, it tends to\nsuffer from unsatisfactory results regarding the fidelity of concepts.\nTo tackle this problem, we propose timestep-variable textual inver-\nsion (TimeInv). TimeInv is based on the observation that different\ntimesteps control the rendering of different image attributes dur-\ning the inference stage. For example, the previous timesteps of the\ndenoising process control the global layout and object shape, and\nthe later timesteps of the denoising process control the low-level\ndetails like texture and color [Voynov et al. 2022]. To better learn the\ntoken depicting the target concept, we design a timestep-dependent\ntoken embedding table to store the controlling token embedding at\nall timesteps. During training, we sample random timesteps among\nall ddpm timesteps to directly optimize the timestep-embedding\nmapping table V \u2208 R\ud835\udc47 \u00d7\ud835\udc51 where \ud835\udc47 is the total timesteps of diffusion\nprocess and \ud835\udc51 is the dimension of token embeddings. The training\nobjective can be defined as:\nV := arg min\n\ud835\udc631:\ud835\udc47\nE\ud835\udc67\u223cE(\ud835\udc65),\ud835\udc66,\ud835\udf16\u223cN(0,1),\ud835\udc61\nh\n\u2225\ud835\udf16 \u2212 \ud835\udc53\ud835\udf03 (z\ud835\udc61,\ud835\udc61, c\ud835\udf03 (\ud835\udc66,\ud835\udc61))\u22252\n2\ni\n.\n(4)\nDuring inference, the token embedding is retrieved based on the\ncurrent denoising timestep and then composite to a sequence of\ntoken embeddings, defined as \ud835\udc63\ud835\udc61\u2217 = \ud835\udc49\ud835\udc61.\nVideo customization with image data. Another challenge for\nvideo personalization is how to leverage image data to optimize\n\ud835\udf0f = 0.1\n\ud835\udf0f = 1\n\ud835\udf0f = 0.5\n\ud835\udf0f = 0.2\ndepth\nretrieved video\nFig. 5. Effectiveness of adjusting the \ud835\udf0f. Small \ud835\udf0f can relax the depth control\nto make the shape render towards the character shape while maintaining a\ncoarse layout and action control from the depth. This technique can generate\nvideos of teddy bears without the need to retrieve the motion video of teddy\nbears, which is very hard to collect since there is a lack of real videos of\nteddy bears with diverse motions (e.g., doing yoga).\nvideo generation models. Directly repeating images to videos and\nthen optimizing the tokens will lead to the motion omission problem.\nSince the static motion tends to bind with the target concept and\nhard to generate concepts with diverse motions. Thanks to the\npreviously introduced structure-guided module, now we can learn\nthe concept using static structure guidance. Specifically, we repeat\nthe concept image to a pseudo video with \ud835\udc3f frames and extract\nframewise depth signals to control the video generation model to\nsynthesize static concept videos. During inference, it can easily\ncombine the target concept with other motion guidance to generate\na concept with diverse actions.\nLow-rank weight modulation. Using textual inversion only\nis still hard to capture the appearance details of the given charac-\nter. Instead of previous approaches that directly optimize model\nparameters, we add additional low-rank [Hu et al. 2021] matrices to\nthe pre-trained linear layers in attention modules, without hurting\nthe concept generation and composition ability in the pre-trained\nmodel. The low-rank matrices comprise two trainable linear lay-\ners. We insert these matrices in the cross and spatial self-attention\nmodules in our model.\nConflict between structure guidance and concept genera-\ntion. Although the concept can be successfully injected into video\ngeneration with our tailored design, there still exists a severe concept-\nguidance conflict issue. Specifically, if we want to learn a person-\nalized teddy bear and then use a source video to provide motion\nguidance, it is challenging and time-consuming to collect teddy bear\nmoving videos. Besides, the shape provided by the depth will se-\nverely affect the id similarity because the generated shape needs to\nfollow the id shape. Hence it is crucial for a depth-guidance model\nto have the ability to relax its depth control. To solve this, we make\nour depth-guidance module to be adjustable via timestep clamping\nduring sampling. Concretely, we apply depth guidance on the fea-\nture only for the timesteps \ud835\udc61 = \ud835\udc47, ...,\ud835\udf0f and drop the depth feature\nafter timestep \ud835\udf0f. We also experimented with feature rescaling during\ninference in early attempts, which shows worse depth adjustment\nthan the timestep clamping.\n4\nEXPERIMENT\nAs the storyboard splitting is conducted manually or assisted by\nLLMs, we mainly evaluate the retrieval-augmented video generation,\n6\n\u2022\nYingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, and Qifeng Chen\n\u201cA <new1> teddy bear is drinking a glass \nof wine in a room, with flowers and a \nwindow behind him.\u201d\n\u201cA <new1> teddy bear walks through a \nbeautiful garden in winter with a big \nsnowstorm.\u201d\n\u201ca <new1> teddy bear rides on a \nbicycle, passes by a snowy forest.\u201d\nOurs\nw/o TimeInv\nw/o character\nrerendering\nw/o guidance &\ncharacter rerendering\nRetrieved\nvideo\nText\nprompt\nFig. 6. Abalation results of the core components in our pipeline, including structure guidance, character rerendering, and TimeInv.\nas the major technical innovation of this paper. Concretely, we first\nvalidate the effectiveness of our overall pipeline for storytelling\nvideo synthesis in Sec. 4.2, and then evaluate the video synthesis\nquality, and the concept customization performance in Sec. 4.3 and\nSec. 4.4 respectively.\n4.1\nImplementation Details\nOur video generation model is trained in three stages. Firstly, we\ntrain the base text-to-video model on WebVid-10M [Bain et al. 2021]\ndataset, with the spatial parameters initialized with the publicly\navailable pre-trained Stable Diffusion Image LDM [Rombach et al.\n2022]. WebVid-10M consists of 10.7M video-caption pairs with a\ntotal of 52K video hours. For training, we resize the videos into\nresolution 256 \u00d7 256 and sample 16 frames with a frame stride of\n8. Secondly, to equip the model with depth guidance, we train the\nstructure encoder on the same training set with the pre-trained\nbase model frozen. At last, for concept customization, the depth-\nguided text-to-video model is fintuned along with specific textural\ntokens optimization, which is performed on task-dependent small\ndataset. Although our model is trained with fixed resolution, we\nfind it support other resolutions well in inference phase.\nAnimate-A-Story:\nStorytelling with Retrieval-Augmented Video Generation\n\u2022\n7\nA <new1> teddy bear is drinking a \nglass of wine in a room, with \nflowers and a window behind him\nA sad <new1> teddy bear is \ndrinking a glass of wine in a bed \nroom\nCharacter \nImages\nDreamBooth-Video\nTextual Inversion-Video\nCustom Diffusion-Video\nOurs\nRetrieved Videos\n& Prompts\na <new1> cat lies in a big \nchair in a palace.\na <new1> cat walks in New \nYork street\nFig. 7. Quantitative comparisons with previous personalization approaches. We show the results of two characters using four different approaches. For each\napproach, we show one video clip and fix the same random seed. Each video clip shows two frames with a frame sampling stride of 8. Readers can zoom in for\na better view.\n4.2\nEvaluation on Storytelling Video Synthesis\nSince we are the first work to tackle the task of storytelling video\nsynthesis task, there are no existing baselines for comparison. So\nwe conduct ablation experiments to justify the effectiveness of our\ndesign choices, as shown in Fig. 6. We can see that the generation\nquality of the target character deteriorates without using TimeInv.\nIn addition, the concept compositional ability is been damaged with-\nout TimeInv. For example, the model fails to generate the flowers\nand window in the third clip. The absence of a personalization pro-\ncess makes it difficult to maintain consistency of character across\nvarious clips. Without the inclusion of video retrieval, it becomes\nchallenging to control the character position and the overall lay-\nout. Additionally, the generation performance is inferior compared\nto retrieval-augmented generation, as evidenced by the quantita-\ntive results presented in Table 1. Video results can be checked in\nsupplementary results.\n4.3\nEvaluation on Text-to-Video Synthesis\nBaselines and evaluation metrics. We compare our approach\nwith existing available (depth-guided) text-to-video models, includ-\ning T2V-Zero combined with ControlNet to inject depth control [Khacha-\ntryan et al. 2023], and two open-sourcing text2video generation\nmodels ModelScope [damo vilab 2023] and LVDM [He et al. 2022].\nWe measure the video generation performance via FVD and KVD.\nThe used real video dataset is UCF-101 [Soomro et al. 2012] and\nwe sample 2048 real videos and their corresponding class names.\nWe use the class name as a text prompt and then sample one fake\nvideo for each prompt. For all three models, we use DDIM 50 steps\nMethod\nCondition\nFVD\u2193\nKVD\u2193\nT2V-Zero + ControlNet [Khachatryan et al. 2023]\ntext + depth\n4685.27\n168.20\nModelScope [damo vilab 2023]\ntext\n2616.06\n2052.84\nLVDM [He et al. 2022]\ntext\n917.63\n116.63\nOurs\ntext + depth\n516.15\n47.78\nTable 1. Quantitative comparison with open-sourcing video generation\nmodels on UCF-101 under zero-shot setting.\nand their default classifier-free guidance scales, which are 9 for\nModelScope and T2V-Zero, and 10 for LVDM.\nResults. In Tab. 1, we show quantitative results of the perfor-\nmance of video synthesis. As can be seen, equipped with depth\nstructure guidance, text-to-video synthesis achieves significantly\nbetter performance than video synthesis from pure text. In addition,\nour approach also surpasses the existing depth-guided text-to-video\ngeneration method, T2V-Zero combined with ControlNet, demon-\nstrating the superiority of our model.\n4.4\nEvaluation on Personalization\nBaselines and evaluation metrics. To evaluate the effectiveness\nof our personalization module, we compare our approach with the\nprevious three baseline approaches: Dreambooth[Ruiz et al. 2023],\nTextual inversion [Gal et al. 2022], and Custom Diffusion [Kumari\net al. 2023]. For quantitative evaluation, we measure the seman-\ntic alignment between generated videos and text and the concept\nfidelity between generated videos and the user-provided concept\nimages. For each approach, we sample 10 random videos using 20\nprompt-video pairs constructing 200 generated videos in total. The\n8\n\u2022\nYingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, and Qifeng Chen\nprompts range from different backgrounds and different composi-\ntions with other objects. Then the semantic alignment is computed\nby the cosine similarity between the CLIP text embedding and the\nCLIP image embedding of each frame, then we average the scores\nfrom all frames to obtain the alignment score between a text and\na generated video clip. The concept fidelity is measured by the av-\nerage alignment between generated video clips and corresponding\nconcept images. For each pair, we compute the cosine similarity\nbetween each frame of the clip and the target concept image, and\nthen we average the scores from all frames. Different approaches\nshare the same set of random seeds for a fair comparison.\nImplementation details. We adopt character images from Cus-\ntom Diffusion, and we experiment with a teddy bear and a cat, con-\ntaining 7 and 5 images respectively. We use <new1> as the pseudo\nword to represent the given character in the input text prompt. For\nthe character of the teddy bear, we train our approach and base-\nline approaches to 1,000 steps. The learning rate we used for our\napproach and textual inversion is 1.0e-04, while we use the learn-\ning rate of 1.0e-5 for dreambooth and custom diffusion since they\ndirectly optimize the pre-trained model parameters. For all these\napproaches, we use real videos of the same category as the regular-\nization dataset to prevent the overfitting issue. The regularization\ndata are retrieved from the WebVid dataset. For each character, we\nretrieved 200 real videos. We also use data augmentation of target\ncharacter images in all approaches to enrich the diversity of the\ntraining dataset following Custom Diffusion. During inference, we\nuse DDIM sampling with 50 sampling timesteps and the classifier-\nfree guidance of 15 for all approaches.\nResults. In Fig. 7, we present qualitative results of comparisons\nwith baseline personalization approaches in the video generation\nsetting. As can be seen, DreamBooth updates the whole pre-trained\nparameters thus it tend to suffer from the overfitting problem, hin-\ndering its generation diversity (e.g., the background of the third\nand fourth row is very similar to the training character images).\nTextual Inversion only optimizes a single token embedding to rep-\nresent the target character\u2019s appearance, so it is hard to capture\nthe character details and exhibits poor character fidelity. Custom\nDiffusion updates the linear layer for computing k and v in attention\nmodules inside the pre-trained network, combined together with\ntextual inversion. Although it achieves better character fidelity, it\nfrequently shows artifacts in the generated character appearance.\nIn Tab. 2, we provide quantitative results of comparisons. Our\nproposed TimeInv can serve as a replacement for Textual Inver-\nsion and can be combined with custom diffusion, achieving better\nsemantic alignment.\nBesides the video generation setting, we also evaluate the pro-\nposed timestep-variable textual inversion in the common image\npersonalization setting. We use the Custom Diffusion codebase and\ncompare the performance between TimeInv and Textual Inversion.\nThe results are shown in Fig. 8. We can see that combining TimeInv\nwith Custom Diffusion shows better background diversity, and con-\ncept compositionally (e.g., the ball appears more frequently than\nthe Custom Diffusion + Textual Inversion). Comparing TimeInv\nwith Textual Inversion directly without updating model parameters\nshows that TimeInv has better character similarity (i.e., the unique\ntexture of the cat).\nMethod\nTeddy Bear\nCat\nSem.\nID\nSem.\nID\nDreamBooth [Ruiz et al. 2023]-Video\n0.272\n0.778\n0.255\n0.869\nTextual Inversion [Gal et al. 2022]-Video\n0.263\n0.666\n0.252\n0.743\nCustom diffusion [Kumari et al. 2023]-Video\n0.275\n0.849\n0.256\n0.841\nOurs\n0.295\n0.853\n0.257\n0.902\nTable 2. Quantitative comparison with previous personalization approaches.\nCustom diffusion + Textual Inversion\nCustom diffusion + TimeInv (Ours)\nTextual Inversion\nTimeInv (Ours)\nCharacter Images\n\u201c<new1> cat \nplaying with a ball\u201d\nPrompt\nFig. 8. Effectiveness of the proposed Timestep-variable Textual Inversion\n(TimeInv) on image personalization using the pre-trained Stable Diffusion.\nResults in the same column are compared under the same training step and\nrandom seeds. This demonstrates that our approach can serve as a general\napproach for personalization on both image and video generation tasks.\n5\nCONCLUSION\nWe introduce a novel retrieval-based pipeline for storytelling video\nsynthesis. This system enables better video synthesis quality, layout,\nmotion control, and character personalization for producing a cus-\ntomized and character-consistent storytelling video. We incorporate\na structure-guided video generation module to a base text-to-video\nmodel, and we devise a new personalization method to boost the\ncharacter control performance. We also solve the character-depth\nconfliction problem with the adjustable depth controlling module.\nWhile we have shown the ability of our system for the challenging\nstoryline synthesis task, there is much room for future improve-\nment from multiple aspects. For example, a general character control\nmechanism without finetuning and a better cooperation strategy\nbetween character control and structure control can be potential\ndirections.\nREFERENCES\nYuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. 2023.\nA Neu-\nral Space-Time Representation for Text-to-Image Personalization. arXiv preprint\narXiv:2305.15391 (2023).\nMax Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in Time:\nA Joint Video and Image Encoder for End-to-End Retrieval. In IEEE International\nConference on Computer Vision (ICCV).\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim,\nSanja Fidler, and Karsten Kreis. 2023. Align your latents: High-resolution video\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 22563\u201322575.\ndamo vilab. 2023. modelscope-text-to-video-synthesis. https://huggingface.co/damo-\nvilab/modelscope-damo-text-to-video-synthesis\nPatrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-\nresolution image synthesis. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. 12873\u201312883.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022).\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin\nHuang, and Devi Parikh. 2022. Long video generation with time-agnostic vqgan\nAnimate-A-Story:\nStorytelling with Retrieval-Augmented Video Generation\n\u2022\n9\nand time-sensitive transformer. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVII. Springer,\n102\u2013118.\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022. Latent\nVideo Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths.\narXiv preprint arXiv:2211.13221 (2022).\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022a.\nImagen video: High definition video generation with diffusion models. arXiv preprint\narXiv:2210.02303 (2022).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in neural information processing systems 33 (2020), 6840\u20136851.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi,\nand David J Fleet. 2022b. Video diffusion models. arXiv preprint arXiv:2204.03458\n(2022).\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. 2022. Cogvideo:\nLarge-scale pretraining for text-to-video generation via transformers. arXiv preprint\narXiv:2205.15868 (2022).\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language\nmodels. arXiv preprint arXiv:2106.09685 (2021).\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. 2023.\nComposer: Creative and controllable image synthesis with composable conditions.\narXiv preprint arXiv:2302.09778 (2023).\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel,\nZhangyang Wang, Shant Navasardyan, and Humphrey Shi. 2023. Text2video-zero:\nText-to-image diffusion models are zero-shot video generators. arXiv preprint\narXiv:2303.13439 (2023).\nDiederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In\nInternational Conference on Learning Representations (ICLR).\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.\n2023. Multi-concept customization of text-to-image diffusion. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1931\u20131941.\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen,\nDeli Zhao, Jinren Zhou, and Tieniu Tan. 2023. Decomposed Diffusion Models for\nHigh-Quality Video Generation. arXiv preprint arXiv:2303.08320 (2023).\nYue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen.\n2023. Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free\nVideos. arXiv preprint arXiv:2304.01186 (2023).\nKangfu Mei and Vishal M Patel. 2022. VIDM: Video Implicit Diffusion Models. arXiv\npreprint arXiv:2212.00235 (2022).\nOpenAI. 2023. GPT-4 Technical Report. arXiv (2023). arXiv:2303.08774\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From\nNatural Language Supervision. In Proceedings of the International Conference on\nMachine Learning (ICML).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.\n2022. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-\nShot Cross-Dataset Transfer. IEEE Trans. Pattern Anal. Mach. Intell. 44, 3 (2022),\n1623\u20131637.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 22500\u201322510.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. 2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems 35 (2022), 36479\u201336494.\nMasaki Saito, Eiichi Matsumoto, and Shunta Saito. 2017. Temporal generative adver-\nsarial nets with singular value clipping. In Proceedings of the IEEE international\nconference on computer vision. 2830\u20132839.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan\nHu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: Text-to-video\ngeneration without text-video data. arXiv preprint arXiv:2209.14792 (2022).\nIvan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. 2022. Stylegan-v: A\ncontinuous video generator with the price, image quality and perks of stylegan2. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n3626\u20133636.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: A dataset of\n101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402\n(2012).\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. 2018. Mocogan: De-\ncomposing motion and content for video generation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 1526\u20131535.\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo,\nHan Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru\nErhan. 2022. Phenaki: Variable length video generation from open domain textual\ndescription. arXiv preprint arXiv:2210.02399 (2022).\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. 2016. Generating videos with\nscene dynamics. Advances in neural information processing systems 29 (2016).\nAndrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2022. Sketch-guided text-to-image\ndiffusion models. arXiv preprint arXiv:2211.13752 (2022).\nFu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng\nLi. 2023a. Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-\nDenoising. arXiv:2305.18264 [cs.CV]\nXiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang,\nYujun Shen, Deli Zhao, and Jingren Zhou. 2023b. VideoComposer: Compositional\nVideo Synthesis with Motion Controllability. arXiv preprint arXiv:2306.02018 (2023).\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo.\n2023. Elite: Encoding visual concepts into textual embeddings for customized\ntext-to-image generation. arXiv preprint arXiv:2302.13848 (2023).\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro,\nand Nan Duan. 2021. Godiva: Generating open-domain videos from natural descrip-\ntions. arXiv preprint arXiv:2104.14806 (2021).\nJinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He,\nHanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. 2023. Make-Your-\nVideo: Customized Video Generation Using Textual and Structural Guidance. arXiv\npreprint arXiv:2306.00943 (2023).\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. 2021. Videogpt: Video\ngeneration using vq-vae and transformers. arXiv preprint arXiv:2104.10157 (2021).\nShuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. 2023. Rerender A Video:\nZero-Shot Text-Guided Video-to-Video Translation. arXiv preprint arXiv:2306.07954\n(2023).\nLijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang, Alexan-\nder G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. 2023a. Magvit:\nMasked generative video transformer. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 10459\u201310469.\nSihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. 2023b. Video probabilistic\ndiffusion models in projected latent space. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 18456\u201318466.\nYabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and\nQi Tian. 2023. ControlVideo: Training-free Controllable Text-to-Video Generation.\narXiv preprint arXiv:2305.13077 (2023).\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. 2022.\nMagicvideo: Efficient video generation with latent diffusion models. arXiv preprint\narXiv:2211.11018 (2022).\n"
  },
  {
    "title": "Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models",
    "link": "https://arxiv.org/pdf/2307.06925.pdf",
    "upvote": "9",
    "text": "Domain-Agnostic Tuning-Encoder for Fast Personalization of\nText-To-Image Models\nMOAB ARAR, Tel Aviv University, Israel\nRINON GAL\u2217, Tel Aviv University, NVIDIA, Israel\nYUVAL ATZMON, NVIDIA, Israel\nGAL CHECHIK, NVIDIA, Israel\nDANIEL COHEN-OR, Tel Aviv University, Israel\nARIEL SHAMIR, Reichman University (IDC), Israel\nAMIT H. BERMANO, Tel Aviv University, Israel\nEncoder\nText-to-image \nDiffusion\n\u201c\u2026a firefighter\u201d\n\u201c\u2026on the beach\u201d\n\u201cancient mosaic\u2026\u201d\n\u201cvector art\u2026\u201d\n\u201cBanksy art \u2026\u201d\n\u201cUkyio-e\u2026\u201d\n\u201c..Monet painting\u201d\n\u201c... playdough\u201d\n\u201c \u2026 in a tophat\u201d\n\u201c\u2026as terminator\u201d\n\u05f4\u05f3 watercolor \u2026\u201d\n\u201c\u2026 Pixar movie\u201d\nFig. 1. Our domain-agnostic tuning-encoder can personalize a text-to-image diffusion model to a given concept using 12 or fewer training steps, allowing for\ngeneral one-shot inference-time tuning. The personalized models are used to generate images of the concept in new settings using natural language prompts.\nText-to-image (T2I) personalization allows users to guide the creative image\ngeneration process by combining their own visual concepts in natural lan-\nguage prompts. Recently, encoder-based techniques have emerged as a new\neffective approach for T2I personalization, reducing the need for multiple im-\nages and long training times. However, most existing encoders are limited to\na single-class domain, which hinders their ability to handle diverse concepts.\nIn this work, we propose a domain-agnostic method that does not require\nany specialized dataset or prior information about the personalized concepts.\nWe introduce a novel contrastive-based regularization technique to maintain\nhigh fidelity to the target concept characteristics while keeping the predicted\nembeddings close to editable regions of the latent space, by pushing the pre-\ndicted tokens toward their nearest existing CLIP tokens. Our experimental\nresults demonstrate the effectiveness of our approach and show how the\nlearned tokens are more semantic than tokens predicted by unregularized\nmodels. This leads to a better representation that achieves state-of-the-art\nperformance while being more flexible than previous methods 1.\n1\nINTRODUCTION\nThe rapid advancement of generative models has revolutionized\ncontent creation, enabling effortless generation of diverse artworks.\n\u2217Work was done during an internship at NVIDIA\n1Our project page available at https://datencoder.github.io\nPart of their true potential lies in personalization, allowing users to\ntailor outputs to unique personal concepts. Personalizing a model in-\nvolves customizing it to capture and manifest unique characteristics\nof personal belongings, memories, or self-portraits. However, early\npersonalization methods [Gal et al. 2022; Ruiz et al. 2022] rely on the\navailability of multiple images and require lengthy optimization.\nAn effective alternative is pre-training predictive models for tar-\ngeting concepts. These approaches train an encoder to predict a\ntext embedding that accurately reconstructs a given desired target\nconcept. Using the obtained embeddings, one can generate scenes\nportraying the given concept. Still, such methods face limitations.\nFirst, they rely on a single-class domain, which constrains their abil-\nity to capture the long tail distribution of diverse concepts. Second,\nsome approaches necessitate external priors, such as segmentation\nmasks or multi-view input, to effectively capture the characteristics\nof the target concept while discarding spurious background features.\nIn this work, we follow E4T [Gal et al. 2023], an approach which\nleverages the encoder as a form of initialization for brief (5-15 itera-\ntion) fine-tuning. E4T trains an encoder for each individual domain,\nand requires roughly 70GB of VRAM for inference-time tuning. Our\napproach can tackle multiple domains, and reduces inference-time\narXiv:2307.06925v1  [cs.CV]  13 Jul 2023\n2\n\u2022\nMoab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano\nmemory requirements. We consider two goals while designing our\nencoder: (1) the ability to edit the target concepts, and (2) the ability\nto faithfully capture distinguishable characteristics of the target. We\nachieve the first goal by regularizing the model to predict words\nwithin the editable region of the generative model. Unlike prior\nsingle-domain methods, we do not rely on a coarse description of\nthe target domain. Instead, we use a contrastive-based approach\nto push the predicted embedding toward meaningful regions in\nthe word embedding space. Intuitively, ensuring the prediction is\nnear words that semantically describe the concept class will better\npreserve the model\u2019s prior knowledge of the concept class.\nFor the second goal, we introduce a hyper-network to capture the\ndistinctive features of the target concepts with higher fidelity. To\nensure a manageable model size, we employ a strategy of predicting\na low-rank decomposition of the weights of the UNET-denoiser\nmodel, following the approach outlined in Hu et al. [2021] and\nGal et al. [2023]. Finally, the joint embedding and hyper-network\npredictions are used to initialize a regularized LoRA training process,\nrequiring 12 or fewer optimization steps. Importantly, this reduces\nmemory requirements from roughly 70GB to fewer than 30GB and\nshortens training and inference times.\nWe compare our method to existing encoders and optimization-\nbased approaches and demonstrate that it can achieve high quality\nand fast personalization across many different domains.\n2\nRELATED WORK\nText-driven image generation using diffusion models. Text-\nto-image synthesis has made significant progress in recent years,\ndriven mainly by pre-trained diffusion models [Ho et al. 2020a]\nand especially by large models [Balaji et al. 2022; Nichol et al. 2021;\nRamesh et al. 2022; Rombach et al. 2022] trained on web-scale data\nlike [Schuhmann et al. 2021]. Our approach builds upon these pre-\ntrained models to extend their vocabulary and generate personalized\nconcepts. Specifically, we use the Stable-Diffusion model [Rombach\net al. 2022]. We expect it to generalize to diffusion-based generators\nwith similar attention-based architectures [Saharia et al. 2022].\nText-based image editing. Following the success of CLIP-based\n[Radford et al. 2021] editing methods [Bar-Tal et al. 2022; Gal et al.\n2021; Michel et al. 2021; Patashnik et al. 2021], a large body of\nwork sought to leverage the power of recent large-scale text-to-\nimage models [Balaji et al. 2022; Kang et al. 2023; Ramesh et al.\n2022; Rombach et al. 2022; Saharia et al. 2022; Sauer et al. 2023]\nin order to manipulate images using text-based guidance. Prompt-\nto-Prompt [Hertz et al. 2022] propose a method for manipulating\ngenerated images by re-using an initial prompt\u2019s attention masks.\nIn a follow-up work, Mokady et al. [2022] extend this approach to\nreal images by encoding them into the null-conditioning space of\nclassifier-free guidance [Ho and Salimans 2021]. Tumanyan et al.\n[2022] and Parmar et al. [2023] extract reference attention maps\nor features using DDIM [Song et al. 2020] based reconstructions.\nThese are then used to preserve image structure under new prompts.\nOthers train an instruction-guided image-to-image translation net-\nwork using synthetic data [Brooks et al. 2023] or tune the model to\nreconstruct an image and use conditioning-space walks to modify\nit [Kawar et al. 2022]. Such approaches can also be used to edit 3D\ncontent, for example modifying shapes textures using depth and\ntext guided diffusion models [Richardson et al. 2023].\nCommon to these image-editing approaches is a desire to preserve\nthe content of the original image. In contrast, our method deals with\nmodel personalization which aims to capture a concept for later\nuse in novel scenes. There, the aim is to learn the semantics and\napperance of a subject, but not its specific structure in the image.\nInversion. In the context of Generative Adversarial Networks\n(GANs, [Goodfellow et al. 2014]), inversion is the task of finding\na latent representation that will reproduce a specific image when\npassed through a pre-trained generator [Xia et al. 2021; Zhu et al.\n2016]. There, methods are split into two main camps. In the first are\noptimization methods, which iterative search the latent space for a\ncode that synthesizes an image with some minimal reconstruction\nloss [Abdal et al. 2019, 2020; Gu et al. 2020; Zhu et al. 2020a]. In the\nsecond are encoder based methods, which train a neural network\nto directly predict such latents [Bai et al. 2022; Parmar et al. 2022;\nPidhorskyi et al. 2020; Richardson et al. 2020; Tov et al. 2021; Wang\net al. 2022; Zhu et al. 2020b].\nWith diffusion models, the inversion latent space can be the initial\nnoise map that will later be denoised into a given target [Dhariwal\nand Nichol 2021; Ramesh et al. 2022; Song et al. 2020]. In a more\nrecent line of work, inversion has been used to refer to finding a\nconditioning code that can be used to synthesize novel images of a\ngiven concept [Gal et al. 2022]. There, the goal is not to recreate a\nspecific image, but to capture the semantics of a concept outlined\nin one or more target images and later re-create it in new scenes.\nOur approach similarly aims to encode a concept.\nPersonalization. Personalization methods aim to tune a model\nto a specific individual target. Often, the goal is to combine some\nlarge-scale prior knowledge with unique information associated\nwith an end-user. These can include personalized recommendation\nsystems [Amat et al. 2018; Benhamdi et al. 2017; Cho et al. 2002;\nMartinez et al. 2009], federated learning [Fallah et al. 2020; Jiang\net al. 2019; Mansour et al. 2020; Shamsian et al. 2021], or the creation\nof generative models tuned on specific scenes or individuals [Alaluf\net al. 2021; Bau et al. 2019; Cao et al. 2022; Cohen et al. 2022; Dinh\net al. 2022; Nitzan et al. 2022; Roich et al. 2021]. In text-to-image\npersonalization, the goal is to teach pre-trained models to synthesize\nnovel images of a specific target concept, guided by natural language\nprompts. Initial work in this field employed direct optimization\napproaches, either tuning a set of text embeddings to describe the\nconcept [Gal et al. 2022; Voynov et al. 2023], modifying the denoising\nnetwork itself [Han et al. 2023; Ruiz et al. 2022], or a mixture of\nboth [sim 2023; Kumari et al. 2022; Tewel et al. 2023]. However, such\noptimization-based approaches require lengthy training sessions,\ntypically requiring dozens of minutes for every concept.\nMore recently, encoder-based approaches emerged [Gal et al. 2023;\nLi et al. 2023; Shi et al. 2023; Wei et al. 2023; Zhou et al. 2023], which\ntrain a neural network to predict some latent representation that can\nbe injected into the network to synthesize new images of the concept.\nThese either require subject-specific segmentation masks [Wei et al.\n2023] or use single-domain training to both regularize the model\nand allow it to infer the target from the single image [Gal et al. 2023;\nShi et al. 2023]. In an alternative approach, a model can be trained\nDomain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models\n\u2022\n3\nLoRA\n+\nLoRA\na photo of a horse\nSelf-Attention\nCross-Attention\nSelf-Attention\nCross-Attention\na photo of a [S*]\n Prior-Preserving Dual-Path Adaptation\n[S*]\nhorse\nactual future fish\ncat\npoints\nhr\nhorse ml dog nap\n\ud83d\udd70\n\ud83d\ude0a\n\ud83c\udf49\n\u26c5\nworldcup\nberry\ncoast\nCLIP Tokens\nNearest-Neighbor Lookup\nContrastive Loss\nEmbedding Regularization\nTarget concept - Ic\nE4T Backbone\nHyperNetwork\nToken Embedder\na photo of a [S*]\n[v*]\n Low-Rank\nWeight Updates\nUNET Denoiser\nFig. 2. Method overview. (top) Our method consists of a feature-extraction backbone which follows the E4T approach and uses a mix of CLIP-features\nfrom the concept image, and denoiser-based features from the current noisy generation. These features are fed into an embedding prediction head, and\na hypernetwork which predicts LoRA-style attention-weight offsets. (bottom, right) Our embeddings are regularized by using a nearest-neighbour based\ncontrastive loss that pushes them towards real words, but away from the embeddings of other concepts. (bottom, left) We employ a dual-path adaptation\napproach where each attention branch is repeated twice, once using the soft-embedding and the hypernetwork offsets, and once with the vanilla model and a\nhard-prompt containing the embedding\u2019s nearest neighbor. These branches are linearly blended to better preserve the prior.\nto synthesize novel images from dual conditions: a text prompt, and\na set of images depicting the target [Chen et al. 2023]. However,\nthis approach is based on apprenticehsip learning, where the model\nis trained on outputs from half a million pre-trained personalized\nmodels. Such an approach therefore requires roughly 14 A100 GPU-\nyears, making it infeasible for most practitioners.\nOur method follows the encoder-based approach, but extends it\nbeyond the single-domain without use of any segmentation masks\nor additional labels. Moreover, compared to prior encoder-based\ntuning approaches [Gal et al. 2023], our tuning-phase is quicker and\nhas reduced memory overhead.\n3\nPRELIMINARIES\nTo put our contribution in context, we begin with an overview\nof two recent text-to-image personalization approaches: Textual\nInversion [Gal et al. 2022] and E4T [Gal et al. 2023] which serve as\na basis for our work.\n3.1\nTextual Inversion\nTextual Inversion (TI) introduced the topic of text-to-image (T2I)\npersonalization, where a pre-trained T2I diffusion model is taught\nhow to reason about unique, user-provided concepts which were\nunseen during training. In TI, the authors propose to tackle this\ntask by learning a novel word-embedding, \ud835\udc63\u2217, that will represent\na concept visualized in a small (3-5) image set. To find such an\nembedding, the authors leverage the simple diffusion denoising\nloss [Ho et al. 2020b]:\n\ud835\udc3f\ud835\udc37\ud835\udc56\ud835\udc53 \ud835\udc53 \ud835\udc62\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b := E\ud835\udc67,\ud835\udc66,\ud835\udf16\u223cN(0,1),\ud835\udc61\nh\n\u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc66)\u22252\n2\ni\n,\n(1)\nwhere \ud835\udf16 is the unscaled noise sample, \ud835\udf16\ud835\udf03 is the denoising network, \ud835\udc61\nis the time step, \ud835\udc67\ud835\udc61 is an image or latent noised to time \ud835\udc61, and \ud835\udc50 is\nsome conditioning prompt containing an arbitrary string \ud835\udc46\u2217 that is\nmapped to the embedding \ud835\udc63\u2217.\nOnce learned, this embedding can be invoked in future prompts\n(by including the placeholder \ud835\udc46\u2217, e.g. \u201ca photo of \ud835\udc46\u2217\") in order to\ngenerate images of the concept in novel contexts and scenes.\n3.2\nEncoder for Tuning (E4T):\nAlthough optimization-based approaches like TI can reconstruct\nthe target concept, they require many iterations to converge. In-\ndeed, personalizing a model with TI typically requires dozens of\nminutes even on commercial-grade GPUs. Recently, encoder-based\napproaches have emerged that train a neural network to directly\nmap an image of a concept to a novel embedding. More concretely,\n4\n\u2022\nMoab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano\ngiven an input image \ud835\udc3c\ud835\udc50 depicting the concept, the encoder \ud835\udc38 is\ntrained to predict a suitable embedding: \ud835\udc63\u2217 = \ud835\udc38(\ud835\udc3c;\ud835\udf03). This encoder\ncan be pretrained on a large set of images using the same denoising\ngoal of eq. (1), allowing it to later generalize to new concepts.\nIn E4T, this encoder is pre-trained on a single target domain (e.g.\nhuman faces, cats or artistic styles). However, in order to prevent\noverfitting and preserve editability, it regularizes the predicted em-\nbeddings by restricting them to a region close to the embedding of\na word describing the single domain (e.g. \u201cface\", \"cat\" or \"art\"). This\nregularization comes at the cost of identity preservation, which the\nauthors later restore through an inference-time tuning session using\na single target image of the concept and a few seconds of training.\nOur goal is to extend this encoder-based tuning approach to an\nunrestricted domain, allowing a user to quickly personalize a model\neven for rare concepts for which large training sets may not exist.\n4\nMETHOD\n4.1\nArchitecture Desgin\nWe adopt the E4T architecture, which features an iterative-refinement\ndesign. Specifically, we utilize a pre-trained CLIP [Radford et al.\n2021] ViT-H-14 visual encoder and StableDiffusion\u2019s UNET-Encoder\nas feature-extraction backbones. We extract the spatial features for\nthe given input image from each backbone\u2019s last layer. Following\nE4T, when extracting features from the UNET-Encoder, we pro-\nvide it with an empty prompt. The features are processed by a\nconvolutional-based network and shared between two prediction\nheads: a token embedder and a HyperNetwork. The token embedder\npredicts word embeddings that will be used to represent our target\nconcept \ud835\udc3c\ud835\udc50. The HyperNetwork predicts weight-modulations for\nStable Diffusion\u2019s denoising UNET. Next we discuss some important\naspects about each prediction head.\nHyperNetwork:\nIt is challenging to capture the fine details of\nthe target concept by using only a token embedding. Previous works\nshowed that modulating subsets of the denoiser weights can im-\nprove reconstruction quality with minor harm to the model\u2019s prior.\nTherefore, we seek to predict a set of weight modulations to help\ntune the denoiser for better identity preservation. Moreover, we\nmake use of Stable Diffusion [Rombach et al. 2022], which consists\nof roughly a billion parameters. Adapting so many weights using\na HyperNetwork is computationally infeasible. Hence, we follow\nprior art [sim 2023; Gal et al. 2023; Kumari et al. 2022] and focus\non predicting modulations for a subset of Stable Diffusion\u2019s lay-\ners, and specifically for the attention projection matrices. However,\nStable Diffusion contains 96 such matrices, each containing an av-\nerage of 715, 946 parameters. Predicting such large matrices is still\nchallenging. Instead, we predict decomposed weights of the same\nform as Low-Rank Adaptation (LoRA) [Hu et al. 2021], where each\nweight, \ud835\udc4a \u2208 R\ud835\udc37\ud835\udc56\ud835\udc5b\u00d7\ud835\udc37\ud835\udc5c\ud835\udc62\ud835\udc61 , is modulated by injecting trainable rank\ndecomposition matrices. More specifically, for each concept \ud835\udc3c\ud835\udc50 and\neach projection matrix W, we predict two matrices , \ud835\udc34 \u2208 R\ud835\udc37\ud835\udc56\ud835\udc5b\u00d7\ud835\udc5f\nand \ud835\udc35 \u2208 R\ud835\udc5f \u00d7\ud835\udc37\ud835\udc5c\ud835\udc62\ud835\udc61 , where \ud835\udc5f is the decomposition rank. The the new\nmodulated matrices are:\n\ud835\udc4a \u2032 = \ud835\udc4a + \u0394\ud835\udc4a = \ud835\udc4a + \ud835\udc34 \u00d7 \ud835\udc35\n(2)\n.\nTo avoid breaking the model at the beginning of training, we\ninitialize the prediction layer of the matrix \ud835\udc35 to zero, and scale \u0394\ud835\udc4a\nby a constant factor following [Hu et al. 2021]. We further regularize\nthe weight-offsets by applying \ud835\udc3f2-regularization.\n4.2\nEmbedding Regularization\nLarge Language models are trained on a finite dictionary composed\nof tokens. Particularly, these models process words by dividing\nthem into a sequence of tokens from the dictionary, which are then\nconverted into appropriate embeddings {\ud835\udc47\ud835\udc56}\ud835\udc5b\n\ud835\udc56=1. In this tokenization\nprocess, each word is mapped to one or more high-dimensional\nvectors, which is used as input for transformer-based model.\nOur encoder\u2019s objective is to predict an embedding, \ud835\udc63\u2217 = \ud835\udc38(\ud835\udc3c\ud835\udc50),\nthat best describes a target-concept \ud835\udc3c\ud835\udc50. Previous works [Gal et al.\n2023] have shown that in under-constrained settings, encoders tend\nto use out-of-distribution embeddings. These tend to draw attention\naway from other words [Tewel et al. 2023], limiting the ability to\nlater manipulate the personalized concept via novel prompts. To\nprevent this attention-overfitting, we could use existing token em-\nbeddings to describe \ud835\udc3c\ud835\udc50. While these tokens are within the training\ndistribution and hence editable, they are not expressive enough to\ncapture personal concepts. We thus relax this hard constraint and\npredict embeddings close to existing tokens. Intuitively, constraining\n\ud835\udc38(\ud835\udc3c\ud835\udc50) near semantically related words balances the trade-off between\nreconstruction and editing. However, unlike in single-domain en-\ncoders, where a coarse description of the domain exists and is known\na-priori, in our setting there could be many semantically different\nwords describing different concepts in the training data. Moreover,\nthe domain encountered during inference may differ from those\nobserved in training.\nInspired by [Huang et al. 2023; Miech et al. 2020], we make use\nof a \"nearest-neighbor\" contrastive-learning objective with dual\ngoals: (1) push the predicted embedding close to their nearest CLIP\ntokens, and (2) map different concept images to different embed-\ndings. Concretely, given \ud835\udc63\u2217 = \ud835\udc38(\ud835\udc3c\ud835\udc50), we find N (\ud835\udc63\u2217), the set of nearest\nCLIP-tokens to \ud835\udc63\u2217 in terms of the cosine distance metric. These CLIP\ntokens, \ud835\udc47\ud835\udc56 \u2208 N (\ud835\udc63\u2217) serve as positive examples in the contrastive\nloss. For every other image \ud835\udc3c\u2032 \u2260 \ud835\udc3c\ud835\udc50 in the current mini-batch, we\nuse the embedding \ud835\udc63\u2032 = \ud835\udc38(\ud835\udc3c\u2032) as our negative sample. Therefore,\nour loss is defined by:\n\ud835\udc3f\ud835\udc50 (\ud835\udc63\u2217) = \u2212 log\n\u00cd\nN(\ud835\udc63\u2217) exp (\ud835\udc63\u2217 \u00b7 \ud835\udc47\ud835\udc56/\ud835\udf0f)\n\u00cd\nN(\ud835\udc63\u2217) exp (\ud835\udc63\u2217 \u00b7 \ud835\udc47\ud835\udc56/\ud835\udf0f) + \u00cd\n\ud835\udc63\u2032\u2260\ud835\udc63\u2217 exp (\ud835\udc63\u2217 \u00b7 \ud835\udc63\u2032/\ud835\udf0f)\n(3)\n. As opposed to previous methods [Huang et al. 2023], using the\nnearest neighbors embeddings as positive samples requires no su-\npervision or prior knowledge on the target domain, canceling the\nneed for a pre-defined list of positive and negative tokens in ad-\nvance. Finally, we additionally employ an L2-regularization term to\nprevent the norm of the embeddings from increasing significantly:\n\ud835\udc3f\ud835\udc3f2(\ud835\udc63\u2217) = ||\ud835\udc63\u2217||2\n(4)\nDomain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models\n\u2022\n5\nInput\nPrompt\nTextual Inversion\nDreamBooth\nLoRA\nELITE\nOurs\n(Few-shot)\n(Few-shot)\n(One-shot)\n(Few-shot)\n( w/ Seg Mask)\n(One-shot)\n[\ud835\udc46\u2217] themed backpack\n[\ud835\udc46\u2217] in the style of Monet\nPixar rendering of [\ud835\udc46\u2217]\nVector art of [\ud835\udc46\u2217]\n[\ud835\udc46\u2217] near Mount Fuji\nUkiyo-e painting of [\ud835\udc46\u2217]\n[\ud835\udc46\u2217] with a tophat\n[\ud835\udc46\u2217] with a tophat\nManga drawing of [\ud835\udc46\u2217]\nFig. 3. Qualitative comparison with existing methods. Our method achieves comparable quality to the state-of-the-art using only a single image and 12 or\nfewer training steps. Notably, it generalizes to unique objects which recent encoder-based methods struggle with.\n6\n\u2022\nMoab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano\n4.3\nHyper-weights Regularization\nOur encoder also contains a hypernetwork branch, whose predicted\nweights can also overfit the model to a given image [Gal et al. 2023].\nTo address the issue of overfitting caused by the hyper-network\npredictions, we propose a modification to the UNET forward pass.\nWe begin by duplicating each block into two copies. The first block\nuses the original UNET\u2019s weights, and for the second, we use the\nhypernetwork-modulated weights. Moreover, in the first (original\nweight) branch, we replace our predicted word embeddings with\nthose of the nearest neighbor token. The outputs of the two paths\nare then linearly blended with a coefficient of \ud835\udefc\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc51. This dual-call\napproach ensures that one path is free from attention-overfitting,\nand can thereby strike a balance between capturing the identity and\npreserving the model\u2019s prior knowledge (see Fig 2).\nSpecifically, given the weight modulations \ud835\udc4a\u0394 and the predicted\nword embedding \ud835\udc63\u2217 from our encoder \ud835\udc38, we first identify the nearest\nhard-token embedding \ud835\udc63\u210e to the model\u2019s prediction \ud835\udc63\u2217. We then\ncompose two text prompts, \ud835\udc36 and \ud835\udc36\u210e, which consist of \ud835\udc63\u2217 and \ud835\udc63\u210e\nrespectively. In other words, \ud835\udc36 and \ud835\udc36\u210e are derived from the same\nprompt, but one uses the learned embedding while the other uses\nonly real-token (\"hard\") embeddings.\nFor each block \ud835\udc35 of the UNET-denoiser, which receives a feature\nmap \ud835\udc53 \u2208 R\ud835\udc58\u00d7\ud835\udc58\u00d7\ud835\udc37, text condition \ud835\udc36, and weight modulation \ud835\udc4a\u0394, we\nmodify the block using the dual-path approach:\n\ud835\udc5c\ud835\udc62\ud835\udc61 = \ud835\udefc\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc51 \u00b7 \ud835\udc35(\ud835\udc53 ,\ud835\udc36,\ud835\udc4a\u0394) + (1 \u2212 \ud835\udefc\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc51) \u00b7 \ud835\udc35(\ud835\udc53 ,\ud835\udc36\u210e, \u2205)\n(5)\n.\n4.4\nInference-time Personalization\nAs a final step, we follow E4T and employ a brief tuning phase at\ninference time. While E4T tunes both the model and the encoder at\ninference time, we find that this proccess requires significant mem-\nory (roughly 70GB with the recommended minimal batch size of\n16). To reduce this requirement, we note that our model predicts the\nsame embedding and weight decomposition used by LoRA [sim 2023;\nHu et al. 2021]. As such, we can use its output as an initialization for\na short LoRA-tuning run, with the addition of an L2-regularization\nterm that aims to keep both weights and embedding close to the\noriginal encoder prediction.\n5\nEXPERIMENTS\n5.1\nExperimental setup\nPre-training: We initiated our experiments by pre-training our\nmodel on the ImageNet-1K and Open-Images datasets [Kuznetsova\net al. 2020; Russakovsky et al. 2015]. ImageNet-1K consists of 1.28\nmillion training images from 1,000 distinct classes. For OpenIm-\nages dataset, we crop the largest object from each training image to\navoid training on multiple-subjects at the same time. Together, our\ntraining data consists of around 3M images. The pre-training phase\nemployed a pre-trained CLIP model with a ViT-H-14 encoder as\nthe backbone architecture. The token-embedder and hyper-network\nwere trained using a learning rate of lr=1e-4 with linear warm-up\nand cosine-decay scheduling. For ablation purposes, we conducted\n50,000 iterations during training. For our final model and compar-\nisons to prior art, we extended the training to 150,000 steps.\nInference-tuning Phase: During the inference-time tuning phase,\nwe used a single-forward pass to obtain the initial prediction of the\nhyper-weights and word-embedding for the text-to-image model\nadaptation. Subsequently, we optimized the initial prediction using\na learning rate of lr=2e-3 and a balancing factor of \ud835\udefc\ud835\udc4f\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc51 = 0.25\n(see Eq. 5). We found that up to 12 optimization steps were sufficient\nto achieve satisfactory results for various concepts, compared to the\nrecommended 2, 000 for LoRA-PTI [sim 2023; Roich et al. 2021].\nEvaluation Metric: We follow TI [Gal et al. 2022] and employ\na CLIP text-to-image similarity score as the evaluation metric to\nassess the proximity of the generated images to the input prompt.\nTo measure identity preservation, we utilized the image-to-image\nCLIP similarity loss between the single-image training set and the\ngenerated results. All reported metrics are based on a pre-trained\nViT-B-16 model. Our evaluation set contains 17 images taken from\nprior work [Gal et al. 2022; Kumari et al. 2022; Ruiz et al. 2022]. These\ncover diverse categories ranging from pets (e.g., dogs) to personal\nitems (e.g., backpacks) and even buildings.\n5.2\nThe importance of contrastive regularization\nInput\nA photo of [S*] in the Gladiator Movie\nAn ice sculpture of [S*]\nL2 reg-only\nNN-reg\nContrastive Reg\nw/o Reg\nFig. 4. The effects of removing or changing the embedding regularization.\nRemoval of regularization leads to overfitting or mode collapse with poor\nquality results. Na\u00efve regularizations tend to struggle with preserving the\nconcept details. Our contrastive-based regularization can achieve a tradeoff\nbetween the two.\nOur approach utilizes contrastive learning to improve the qual-\nity of predicted embeddings. To visualize the benefit of this regu-\nlarization, we train our model in four settings: First, without any\nregularization. Second, we omit all regularization except for the L2\nloss on the predicted embedding. Third, we replace the contrastive\nloss with one that minimizes the cosine-distance between predicted\nembeddings and their nearest neighbor - a loss inspired by the code-\nbook losses employed in VQGAN [Esser et al. 2021]. Finally, we use\nour proposed contrastive-based alternative.\nAs seen in Fig 4, incorporating our contrastive-based loss im-\nproves results. In particular, omitting any regularization tends to\noverfit the input image. For example, in the generated image of \"A\nphoto of [S*] in the gladiator movie,\" the word gladiator is over-\nlooked. And the model overfits the predicted token. On the other\nDomain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models\n\u2022\n7\n0.240\n0.245\n0.250\n0.255\n0.260\n0.265\n0.270\nPrompt-Similarity\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nIdentity Similarity\nOurs (one-shot)\nLoRA (one-shot)\nLoRA (few-shot)\nDB (one-shot)\nDB (few-shot)\nTI (one-shot)\nTI (few-shot)\n(a)\n0.20\n0.22\n0.24\n0.26\n0.28\n0.30\nPrompt-Similarity\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\nIdentity Similarity\nOurs\nw/o finetuning\nw/o dual-path\nw/o dual-path and tuning\nw/o hyper-network\ntune word-embedding only\ntune hyper-weights only\n(b)\nFig. 5. Quantitative evaluation results. (a) Comparisons to prior work. Our method presents an appealing point on the identity-prompt similarity trade-off\ncurve, while being orders of magnitude quicker than optimization-based methods. (b) Ablation study results. Removing regularization typically leads to quick\noverfitting, where editability suffers. Skipping the fine-tuning step harms identity preservation, in line with E4T [Gal et al. 2023].\nhand, using our contrastive loss, the generated photo faithfully de-\nscribes the input prompt while preserving features of the target\nconcept (i.e., the horse). The contrastive loss function also helps to\nprevent mode collapse by repelling tokens of different images via\nnegative samples. For example, unlike the contrastive-based method,\nthe nearest-neighbor approach does not address mode collapse. It\nyields less favorable results (See Fig 4).\n5.3\nComparison with existing methods\nWe commence our evaluation with a qualitative analysis, demon-\nstrating the ability of our method to capture a remarkable level\nof detail using a single image and a fraction of the training steps.\nFigure 3 showcases the outcomes of multi-domain personalization\nby comparing different approaches. Specifically, we compare our\nmethod with Textual-Inversion [Gal et al. 2022], Dream-Booth[Ruiz\net al. 2022], and popular publicly available LoRA library for Stable\nDiffusion [sim 2023]. We also compare our method to ELITE [Wei\net al. 2023], a state-of-the-art multi-domain personalization encoder.\nFor DreamBooth and Textual Inversion, we use the HuggingFace\nDiffusers implementation [Patil and Cuenca 2022]. Our results are\non-par with full tuning-based methods (DreamBooth, LoRA) and sig-\nnificantly outperform the purely-encoder based approach of ELITE,\neven though the latter has access to additional supervision in the\nform of segmentation masks. Notably, all tuning-based methods\nrequire access to multiple images of the target concept, while our\napproach utilizes only a single input. Additional results generated\nusing our method can be found in fig. 6.\nNext, we compare our method to the tuning-based approaches us-\ning the CLIP-based metrics. Results are shown in fig. 5a. Our method\nachieves better identity preservation and editability than LoRA, but\nexhibits a tradeoff when compared to DreamBooth. Notably, it out-\nperforms all baselines when they are trained using only a single\nimage. Overall, our approach is competitive with the state-of-the-art\nwhile using only a single image and 12 tuning iterations.\n5.4\nAblation Analysis\nWe conduct an ablation study to better understand the importance\nof each component in our method. We examine the following setups:\nremoving the dual-path regularization approach, skipping the fine-\ntuning step, and omitting the hypernetwork branch. We observe\nthat the final tuning step is crucial, inline with the observation\nfrom E4T. In particular, when using our baseline without finetuning,\nwe witness a 20% drop in the object similarity metric. Turning off\nthe dual-path during tuning harms prompt-to-image alignment by\nnearly 30%, suggesting heavy overfitting. Hence, we can conclude\nthat the dual-path approach can successfully preserve the prior and\ndiminish overfitting.\nAnother important component of our method is the hyper-network,\nwhich predicts weight modulations to calibrate the generator with\nour target concept. In our ablation study, we found that omitting\nthe hyper-network at training time negatively impacts the align-\nment of the generated images with the text prompts. We believe\nthis is because the network must encode more information about\nthe object in the word-embedding, causing attention-overfitting as\ndescribed in the method sections.\n6\nLIMITATIONS\nWhile our approach can extend existing tuning-encoders to multi-\nclass domains, it is still limited by our training data. As such, domains\nwhich are poorly represented in the dataset may be hard to encode.\nAs such, a model trained on ImageNet may struggle with cluttered\nscenes or with human faces.\nWe believe this limitation can be overcome by training on more\ngeneral, large-scale datasets such as LAION [Schuhmann et al. 2021].\nHowever, such an investigation is beyond our resources.\nWhile our method can work across a more general domain, it still\nrequires a tuning-step to increase downstream similarity. However,\n8\n\u2022\nMoab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano\nas the memory requirements and iterations required for such tuning-\napproaches decreases, they become negligible compared to the time\nrequired for synthesis.\n7\nCONCLUSION\nWe presented a method for generalizing the tuning-encoder ap-\nproach beyond a single class domain. Our approach restricts over-\nfitting by ensuring predicted embeddings lie close to the real word\ndomain, and by utilizing a dual-pass approach where the network\nblends predictions from hard- and soft-prompts. This in turn al-\nlows us to quickly personalize a model at inference-time, speed-\ning up personalization by two orders of magnitude compared to\noptimization-based approaches.\nIn the future, we hope to further reduce the tuning requirements\nso that our method can be used on consumer-grade GPUs, allowing\nend-users to quickly personalize models on their own machine.\nACKNOWLEDGMENTS\nThe first author is supported by the Miriam and Aaron Gutwirth\nscholarship. This work was partially supported by Len Blavatnik\nand the Blavatnik family foundation, the Deutsch Foundation, the\nYandex Initiative in Machine Learning, BSF (grant 2020280) and ISF\n(grants 2492/20 and 3441/21).\nREFERENCES\n2023. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning.\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: How to embed\nimages into the stylegan latent space?. In Proceedings of the IEEE/CVF International\nConference on Computer Vision. 4432\u20134441.\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2020. Image2stylegan++: How to edit\nthe embedded images?. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition. 8296\u20138305.\nYuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit H. Bermano. 2021.\nHyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.\narXiv:2111.15666 [cs.CV]\nFernando Amat, Ashok Chandrashekar, Tony Jebara, and Justin Basilico. 2018. Artwork\nPersonalization at Netflix. In Proceedings of the 12th ACM Conference on Recom-\nmender Systems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association\nfor Computing Machinery, New York, NY, USA, 487\u2013488. https://doi.org/10.1145/\n3240323.3241729\nQingyan Bai, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, and Yujun Shen.\n2022. High-fidelity GAN inversion with padding space. In European Conference on\nComputer Vision. Springer, 36\u201353.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis,\nMiika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. 2022. ediffi: Text-\nto-image diffusion models with an ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324 (2022).\nOmer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022.\nText2LIVE: Text-Driven Layered Image and Video Editing.\narXiv preprint\narXiv:2204.02491 (2022).\nDavid Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu,\nand Antonio Torralba. 2019. Semantic Photo Manipulation with a Generative Image\nPrior. 38, 4 (2019). https://doi.org/10.1145/3306346.3323023\nSoulef Benhamdi, Abdesselam Babouri, and Raja Chiky. 2017. Personalized recom-\nmender system for e-Learning environment. Education and Information Technologies\n22, 4 (2017), 1455\u20131477.\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. InstructPix2Pix: Learning\nto Follow Image Editing Instructions. In CVPR.\nChen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shunsuke\nSaito, Stephen Lombardi, Shih-en Wei, Danielle Belko, Shoou-i Yu, Yaser Sheikh,\nand Jason Saragih. 2022. Authentic Volumetric Avatars From a Phone Scan. ACM\nTrans. Graph. (2022).\nWenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and\nWilliam W. Cohen. 2023. Subject-driven Text-to-Image Generation via Apprentice-\nship Learning. ArXiv abs/2304.00186 (2023).\nYoon Ho Cho, Jae Kyeong Kim, and Soung Hie Kim. 2002. A personalized recommender\nsystem based on web usage mining and decision tree induction. Expert systems with\nApplications 23, 3 (2002), 329\u2013342.\nNiv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, and Yuval Atzmon. 2022. \"This\nis my unicorn, Fluffy\": Personalizing frozen vision-language representations. In\nEuropean Conference on Computer Vision (ECCV).\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image\nsynthesis. Advances in Neural Information Processing Systems 34 (2021), 8780\u20138794.\nTan M Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son Hua. 2022. Hyperinverter:\nImproving stylegan inversion via hypernetwork. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 11389\u201311398.\nPatrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. 2021. Taming Transformers for High-\nResolution Image Synthesis. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, virtual, June 19-25, 2021. Computer Vision Foundation /\nIEEE, 12873\u201312883. https://doi.org/10.1109/CVPR46437.2021.01268\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized federated\nlearning: A meta-learning approach. arXiv preprint arXiv:2002.07948 (2020).\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022).\nRinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. 2023. Designing an encoder for fast personalization of text-to-image\nmodels. arXiv preprint arXiv:2302.12228 (2023).\nRinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. 2021.\nStylegan-nada: Clip-guided domain adaptation of image generators. arXiv preprint\narXiv:2108.00946 (2021).\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets.\nAdvances in neural information processing systems 27 (2014).\nJinjin Gu, Yujun Shen, and Bolei Zhou. 2020. Image Processing Using Multi-Code GAN\nPrior. arXiv:1912.07116 [cs.CV]\nLigong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and\nFeng Yang. 2023. SVDiff: Compact Parameter Space for Diffusion Fine-Tuning.\narXiv:2303.11305 [cs.CV]\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or. 2022. Prompt-to-prompt image editing with cross attention control.\n(2022).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020a. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020b. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\nJonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. In NeurIPS\n2021 Workshop on Deep Generative Models and Downstream Applications.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nand Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models.\nArXiv abs/2106.09685 (2021).\nZiqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan, and Ziwei Liu. 2023.\nReVersion: Diffusion-Based Relation Inversion from Images.\narXiv preprint\narXiv:2303.13495 (2023).\nYihan Jiang, Jakub Kone\u010dn`y, Keith Rush, and Sreeram Kannan. 2019. Improving fed-\nerated learning personalization via model agnostic meta learning. arXiv preprint\narXiv:1909.12488 (2019).\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris,\nand Taesung Park. 2023. Scaling up GANs for Text-to-Image Synthesis. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nBahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, In-\nbar Mosseri, and Michal Irani. 2022. Imagic: Text-Based Real Image Editing with\nDiffusion Models. arXiv preprint arXiv:2210.09276 (2022).\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.\n2022. Multi-Concept Customization of Text-to-Image Diffusion. arXiv preprint\narXiv:2212.04488 (2022).\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-\nTuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom\nDuerig, and Vittorio Ferrari. 2020. The Open Images Dataset V4: Unified image\nclassification, object detection, and visual relationship detection at scale. IJCV\n(2020).\nDongxu Li, Junnan Li, and Steven C. H. Hoi. 2023.\nBLIP-Diffusion: Pre-trained\nSubject Representation for Controllable Text-to-Image Generation and Editing.\narXiv:2305.14720 [cs.CV]\nYishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. 2020. Three\napproaches for personalization with applications to federated learning.\narXiv\npreprint arXiv:2002.10619 (2020).\nAna Belen Barragans Martinez, Jose J Pazos Arias, Ana Fernandez Vilas, Jorge Garcia\nDuque, and Martin Lopez Nores. 2009. What\u2019s on TV tonight? An efficient and\neffective personalized recommender system of TV programs. IEEE Transactions on\nConsumer Electronics 55, 1 (2009), 286\u2013294.\nDomain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models\n\u2022\n9\nOscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka.\n2021.\nText2Mesh: Text-Driven Neural Stylization for Meshes.\narXiv preprint\narXiv:2112.03221 (2021).\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and An-\ndrew Zisserman. 2020. End-to-end learning of visual representations from uncurated\ninstructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 9879\u20139889.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Null-\ntext Inversion for Editing Real Images using Guided Diffusion Models. arXiv preprint\narXiv:2211.09794 (2022).\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. 2021.\nGlide: Towards photorealistic\nimage generation and editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741 (2021).\nYotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman,\nInbar Mosseri, Yael Pritch, and Daniel Cohen-Or. 2022. MyStyle: A Personalized\nGenerative Prior. arXiv preprint arXiv:2203.17272 (2022).\nGaurav Parmar, Yijun Li, Jingwan Lu, Richard Zhang, Jun-Yan Zhu, and Krishna Kumar\nSingh. 2022. Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n11399\u201311409.\nGaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan\nZhu. 2023. Zero-shot Image-to-Image Translation. arXiv:2302.03027 [cs.CV]\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski.\n2021. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. arXiv preprint\narXiv:2103.17249 (2021).\nSuraj Patil and Pedro Cuenca. 2022. HuggingFace DreamBooth Implementation. https:\n//huggingface.co/docs/diffusers/training/dreambooth.\nStanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. 2020. Adversarial\nLatent Autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 14104\u201314113.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.\nLearning transferable visual models from natural language supervision. arXiv\npreprint arXiv:2103.00020 (2021).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro,\nand Daniel Cohen-Or. 2020. Encoding in Style: a StyleGAN Encoder for Image-to-\nImage Translation. arXiv preprint arXiv:2008.00951 (2020).\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023.\nTexture: Text-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721 (2023).\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2021. Pivotal\ntuning for latent-based editing of real images. arXiv preprint arXiv:2106.05744 (2021).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-Resolution Image Synthesis with Latent Diffusion Models. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA,\nUSA, June 18-24, 2022. IEEE, 10674\u201310685. https://doi.org/10.1109/CVPR52688.2022.\n01042\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2022. DreamBooth: Fine Tuning Text-to-image Diffusion Models for\nSubject-Driven Generation. (2022).\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.\nBerg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV) 115, 3 (2015), 211\u2013252.\nhttps:\n//doi.org/10.1007/s11263-015-0816-y\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,\nSeyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gon-\ntijo Lopes, et al. 2022. Photorealistic Text-to-Image Diffusion Models with Deep\nLanguage Understanding. arXiv preprint arXiv:2205.11487 (2022).\nAxel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. 2023. StyleGAN-\nT: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis.\nInternational Conference on Machine Learning abs/2301.09515. https://arxiv.org/abs/\n2301.09515\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clay-\nton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021.\nLaion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114 (2021).\nAviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. 2021. Personalized feder-\nated learning using hypernetworks. In International Conference on Machine Learning.\nPMLR, 9489\u20139502.\nJing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. 2023. InstantBooth: Personalized\nText-to-Image Generation without Test-Time Finetuning. arXiv:2304.03411 [cs.CV]\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit\nModels. In International Conference on Learning Representations.\nYoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. 2023. Key-Locked Rank One\nEditing for Text-to-Image Personalization. arXiv preprint arXiv:2305.01644 (2023).\nOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021.\nDesigning an Encoder for StyleGAN Image Manipulation.\narXiv preprint\narXiv:2102.02766 (2021).\nNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2022. Plug-and-Play\nDiffusion Features for Text-Driven Image-to-Image Translation. arXiv preprint\narXiv:2211.12572 (2022).\nAndrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. 2023. \ud835\udc43+: Extended\nTextual Conditioning in Text-to-Image Generation. arXiv preprint arXiv:2303.09522\n(2023).\nTengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. 2022. High-\nFidelity GAN Inversion for Image Attribute Editing. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo.\n2023. Elite: Encoding visual concepts into textual embeddings for customized\ntext-to-image generation. arXiv preprint arXiv:2302.13848 (2023).\nWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan\nYang. 2021. GAN Inversion: A Survey. arXiv:2101.05278 [cs.CV]\nYufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. 2023. Enhancing Detail Preserva-\ntion for Customized Text-to-Image Generation: A Regularization-Free Approach.\narXiv:2305.13579 [cs.CV]\nJiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. 2020b. In-domain gan inversion\nfor real image editing. arXiv preprint arXiv:2004.00049 (2020).\nJun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A Efros. 2016. Generative\nvisual manipulation on the natural image manifold. In European conference on\ncomputer vision. Springer, 597\u2013613.\nPeihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. 2020a. Improved StyleGAN\nEmbedding: Where are the Good Latents? arXiv:2012.09036 [cs.CV]\n10\n\u2022\nMoab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano\nAbstract art of [S*]\n[S*] watercolor paint\n[S*]  Egyptian drawing\n[S*] in assassin\u2019s creed\n[S*] on the beach\n[S*]  embroidery\nAcrylic painting of [S*]\n[S*] in Monet style\n[S*] near Eiffel tower\n[S*] on Mount Fuji\nVector art of [S*]\nUkiyo-e painting of [S*]\nOil painting of [S*]\n[S*] in the Maldives\n[S*] with a tophat\n[S*] marble sculpture\nIce sculpture of [S*]\nKawaii drawing of [S*]\n[S*]  Charcoal drawing\nComic drawing of [S*]\n[S*] on the moon\nManga drawing of [S*]\n[S*] made of playdoguh\n[S*] in assassin\u2019s creed\nFig. 6. Additional qualitative results generated using our method. The left-most column shows the input image, followed by 4 personalized generations for\neach subject.\n"
  },
  {
    "title": "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events",
    "link": "https://arxiv.org/pdf/2307.06439.pdf",
    "upvote": "9",
    "text": "work in progress 1\u201315, 2023\nDistilling Large Language Models for Biomedical Knowledge\nExtraction: A Case Study on Adverse Drug Events\nYu Gu, Sheng Zhang, Naoto Usuyama, Yonas Woldesenbet, Cliff Wong, Pra-\nneeth Sanapathi, Mu Wei, Naveen Valluri, Erika Strandberg, Tristan Naumann,\nHoifung Poon\nMicrosoft Research\nAbstract\nLarge language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities\nacross a wide range of tasks, including health applications. In this paper, we study how\nLLMs can be used to scale biomedical knowledge curation. We find that while LLMs al-\nready possess decent competency in structuring biomedical text, by distillation into a task-\nspecific student model through self-supervised learning, substantial gains can be attained\nover out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box\nmodel access. We conduct a case study on adverse drug event (ADE) extraction, which is\nan important area for improving care. On standard ADE extraction evaluation, a GPT-3.5\ndistilled PubMedBERT model attained comparable accuracy as supervised state-of-the-\nart models without using any labeled data. Despite being over 1,000 times smaller, the\ndistilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and\nGPT-4 by over 5 absolute points. Ablation studies on distillation model choice (e.g., Pub-\nMedBERT vs BioGPT) and ADE extraction architecture shed light on best practice for\nbiomedical knowledge extraction. Similar gains were attained by distillation for other stan-\ndard biomedical knowledge extraction tasks such as gene-disease associations and protected\nhealth information, further illustrating the promise of this approach.\n1. Introduction\nAdverse drug events (ADEs) pose a significant public health challenge because they repre-\nsent injuries resulting from medical interventions related to drug use, including medication\nerrors, adverse drug reactions, allergic reactions, and overdoses (Donaldson et al., 2000). In\nthe United States, ADEs are prevalent and are considered to be among the leading causes of\nincreased mortality, extended hospital stays, and elevated healthcare costs (Classen et al.,\n1997). Curating ADEs from biomedical text is thus essential to ensuring and improving\npatient safety, but remains expensive and time consuming because it is predominantly done\nmanually. (Chen et al., 2020).\nAutomated systems for evidence-based pharmacovigilance can help address the chal-\nlenges of manual ADE identification, particularly for pharmaceutical and healthcare com-\npanies (Gurulingappa et al., 2012). However, constructing a gold standard corpus for ADE\nidentification remains challenging due to the need for multiple specialized annotators with\nextensive biomedical backgrounds.\nLarge language models (LLMs), such as GPT-4, have demonstrated impressive zero-\nshot and few-shot capabilities in both general domains (OpenAI, 2023; Bubeck et al., 2023)\n\u00a9 2023 Y. Gu et al.\narXiv:2307.06439v1  [cs.CL]  12 Jul 2023\nDistilling LLMs for Biomedical Knowledge Extraction\nand health applications (Lee et al., 2023).\nIn this paper, we study how LLMs can be\nleveraged to scale biomedical knowledge extraction, using ADEs curation as a case study.\nOur study revealed that state-of-the-art LLMs, such as GPT-3.5 or GPT-4, already perform\ncompetitively in ADE extraction in zero-shot or few-shot settings, but still trail state-of-\nthe-art supervised systems by a large margin. Interestingly, by leveraging LLMs as a noisy\nteacher to annotate large unlabeled data, we can distill its capabilities into a task-specific\nstudent model that is not only more efficient, but also substantially outperforms the teacher\nmodel in end applications. On standard ADE extraction evaluation, PubMedBERT (Gu\net al., 2021) distilled from GPT-3.5 attained comparable accuracy as supervised state-of-\nthe-art models without using any labeled examples. Despite being over 1,000 times smaller,\nthe distilled model outperformed its noisy teacher GPT-3.5 by over six (6) absolute points in\nF1 and GPT-4 by over five (5) absolute points. Unlike GPT-3.5 or GPT-4, such a distilled\nmodel offers white-box access and can be further fine-tuned or customized for specialized\nuses.\nWe found similar gains from LLM distillation for other standard biomedical knowledge\nextraction tasks such as gene-disease associations and protected health information (PHI),\nfurther illustrating the promise of this approach. We also conduct ablation studies on key\ndistillation design such as neural architecture and model choice, which help establish best\npractice for biomedical knowledge extraction. To facilitate future research in this direction,\nwe will release our distilled models.\nGeneralizable Insights about Machine Learning in the Context of Healthcare\n\u2022 Knowledge distillation from LLMs and self-supervision techniques boost the perfor-\nmance of information extraction tasks in the biomedical domain, which provides a\ngeneral and reliable solution to various healthcare applications.\n\u2022 The proposed end-to-end architecture for ADE extraction underscores the importance\nof adapting machine learning models to the unique challenges and requirements of\nhealthcare-related problems, increasing their relevance and impact in clinical settings.\n\u2022 The successful application of our approach to ADE extraction emphasizes the potential\nfor transferring knowledge from LLMs to other natural language processing tasks in\nhealthcare, contributing to a broader understanding of machine learning techniques\nin this domain.\n2. Related Work\nThere are two key areas of related work: end-to-end ADE extraction and knowledge distil-\nlation.\n2.1. End-to-end ADE Extraction\nA variety of approaches have been proposed for ADE extraction. Among these, SpERT (Eberts\nand Ulges, 2019) utilizes lightweight reasoning on BERT embeddings for joint entity and\nrelation extraction, demonstrating the potential for combining these tasks. REBEL (Cabot\n2\nDistilling LLMs for Biomedical Knowledge Extraction\nand Navigli, 2021), an autoregressive seq2seq model based on BART, simplifies relation ex-\ntraction by representing triplets as text sequences and achieves state-of-the-art performance\non multiple benchmarks. The table-sequence encoder model (Wang and Lu, 2020) employs\ntwo distinct encoders to capture different information types during the learning process,\nshowcasing significant improvements over existing single-encoder approaches.\n2.2. Knowledge Distillation\nEarlier LLMs, such as GPT-3 (Ouyang et al., 2022; Agrawal et al., 2022), demonstrated\ngreat potential but fell short of competitive results on biomedical natural language pro-\ncessing (NLP) tasks (Guti\u00b4errez et al., 2022; Moradi et al., 2022). However, the creation of\nGPT-3.5 and GPT-4 (OpenAI, 2023), the latest generation of domain-agnostic LLMs, has\ngenerated new opportunities for advancing medicine, health, and public understanding of\nthe capabilities and limitations of these models (Lee et al., 2023).\nIn this work, we concentrate on knowledge distillation of LLMs using self-supervision\ntechniques (Agrawal et al., 2022; Smith et al., 2022). In other words, we use these LLMs as\nlabelers in the biomedical domain, capitalizing on their powerful language understanding\ncapabilities to generate high-quality labels for various tasks. Our experiments highlight\nthe advantages of this approach for enhancing performance on challenging biomedical NLP\ntasks, especially ADE extraction, illustrating the potential of self-supervised distillation for\nharnessing the power of state-of-the-art LLMs in specialized domains.\n3. Methods\n3.1. Task Definition\nIn this study, we focus on end-to-end ADE extraction, which involves two separate NLP sub-\ntasks: (1) identifying adverse event (AE) mentions using named entity recognition (NER),\nwhere a drug causation is not yet assigned, and (2) assigning causation to drugs through\nrelation extraction (RE), which aims to find the relations between AEs and corresponding\ndrugs.\nThe first sub-task, AE entity extraction, focuses on locating and identifying mentions of\nadverse events within the given text. This step is crucial for gathering information about\npotential negative effects associated with drugs, without considering causation at this stage.\nThe second sub-task, ADE relation extraction, aims to establish causal links between\nthe extracted AE entities and drugs in the context. This step is essential for understanding\nthe relationships between drugs and their adverse effects, enabling more informed decisions\nregarding drug safety and usage.\nTo validate our proposed method, we utilize the ADE corpus (Gurulingappa et al., 2012),\na dataset systematically annotated for supporting the automatic extraction of drug-related\nadverse effects from medical reports. This dataset allows us to evaluate the performance\nof our approach on both subtasks, providing a comprehensive assessment of the end-to-end\nADE extraction process.\n3\nDistilling LLMs for Biomedical Knowledge Extraction\nReduction\nof\nmethylprednisolone\ndosage\nrather\nthan\ninsulin\ntherapy\nresulted\nbetter\ncontrol\nof\nglyc\n##emia\nPubMedBERT\nin\nDRUG_1\nDRUG_2\nAE_1\nO\nTokenization\nFinal Layer \nEmbeddings\nDrug-Aware\nEmbeddings\nAE Entity \nExtraction\nADE Relation \nExtraction\nReduction of methylprednisolone dosage rather than insulin therapy resulted in better control of glycemia\nInput Text\nDrug Extraction\nConcatenation\n[CLS]\n[SEP]\nPooling \nPooling \nFully Connected \n\u2026\nFully Connected \nO\nO\nO\nO\n\u2026\nO\n\u2026\nAE_1 related to DRUG_1\nO\nO\nO\nO\nO\nO\n\u2026\nO\nNone related to DRUG_2\nFigure 1: Our unified neural architecture for extracting adverse-event arguments and\nassigning causation relations for each drug entity in question (DRUG 1 and\nDRUG 2 in this case). The input sequence is first passed through an encoder\n(PubMedBERT) and then augmented by concatenation with the drug represen-\ntation, which is generated by mean-pooling the encoding of all mention tokens. A\nlinear fully connected layer is then applied for token classification using softmax,\npredicting adverse event tokens pertaining to the designated drug entity. This\narchitecture significantly reduces computational complexity from enumerating all\npairwise combinations of adverse events and drugs to only enumerating drug en-\ntities, facilitating efficient and accurate adverse drug event extraction.\n3.2. A Unified Neural Architecture for ADE Extraction\nTraditional methods for ADE extraction typically treat the two subtasks, AE identification\n(NER) and ADE relation extraction (RE), as separate processes. However, in situations\nwhere multiple AEs (N mentions) and drugs (M mentions) coexist in the same context, this\napproach necessitates O(NM) inferences, leading to a bottleneck for large-scale processing.\nRecent studies attempt to tackle this challenge by jointly extracting drug and ADE\nentities, even though drug extraction has been largely addressed in prior work(Santosh et al.,\n2021; Cabot and Navigli, 2021). In this paper, we propose a novel unified architecture that\nconcentrates on efficient and precise extraction of ADE entities and causation assignment.\nOur model introduces a drug-centric structure, designed to simultaneously handle ADE\nNER and relation extraction in one pass.\nAs illustrated in Figure 1, the input sequence undergoes processing to obtain the fi-\nnal layer hidden state output for each drug entity.\nDenote the input sequence as x =\n4\nDistilling LLMs for Biomedical Knowledge Extraction\nx1, x2, . . . , xT , where xi is the i-th token, and T is the sequence length. The output of\nthe final layer hidden state is represented as H = h1, h2, . . . , hT , where hi \u2208 Rd is the\nd-dimensional hidden state corresponding to the i-th token.\nWe then create a new input sequence for each drug entity. Given a set of drug entities\nD = d1, d2, . . . , dM, where dj is the j-th drug entity, for each drug, hidden states of drug\nentity are mean-pooled. The resulting pooled token \u00afdj is concatenated to every hidden\nstate output token of the input sequence, effectively integrating drug information into each\ntoken:\n\u02dchj,i = concat(hi, \u00afdj)\n(1)\nwhere \u02dchj,i \u2208 R2d is the concatenated hidden state for the i-th token in the new input\nsequence created for the j-th drug entity.\nSubsequently, a linear layer is applied on top of the concatenated tokens for binary token\nclassification using sigmoid. This process transforms the task into predicting ADE tokens\nwhile considering the causation drugs. The linear layer and sigmoid are defined as:\nzj,i = W\u02dchj,i + b\n(2)\npj,i = \u03c3(zj,i) =\n1\n1 + exp(\u2212zj,i)\n(3)\nwhere W \u2208 Rd\u2032 and b \u2208 R are learnable parameters of the linear layer, with d\u2032 = 2d\nbeing the dimensionality of the concatenated hidden states, and pj,i represents the predicted\nprobability of the i-th token in the new input sequence created for the j-th drug entity being\nan ADE mention.\nThe proposed architecture substantially simplifies the problem, converting the original\ntwo tasks (NER and RE) into a single, unified task. As a result, the computational require-\nment is dramatically reduced from O(NM) (all pairwise combinations of adverse events\nand drugs) to O(M) (all drug entities), enabling our end-to-end model to perform more\nefficiently and accurately in large-scale ADE extraction.\n3.3. Knowledge Distillation from LLMs\nWe employ knowledge distillation (see Figure 2) using GPT-3.5 as the teacher model.\n3.3.1. Data Curation and Preprocessing\nWe adapt the methodology from Gurulingappa et al. (2012) to curate a corpus focused\non drug-related adverse events. First, we perform a PubMed search with \u201cdrug therapy\u201d\nand \u201cadverse effects\u201d as MeSH terms, limiting the language to English. This search yields\napproximately 50,000 PubMed abstracts related to drug-related adverse events. The query\nis as follows:\n\u201cadverse effects\u201d[sh] AND (hasabstract[text] AND Case Reports[ptyp]) AND\n\u201cdrug therapy\u201d[sh] AND English[lang] AND (Case Reports[ptyp])\n5\nDistilling LLMs for Biomedical Knowledge Extraction\nPRE \nPROCESSING\nTask-Related \nUnlabeled Corpus\nTEACHER  \nLLM\nSELF \nSUPERVISION \nLABELS\nSTUDENT \nMODEL\nSTUDENT \nTRAINING\n- MeSH filtering\n- drug entity extractoin\n- 50k ADE & drug related \nPubMed abstracts\n- GPT-3.5\n- GPT-4\n- PubMedBERT\n- BioGPT\nKNOWLEDGE DISTILLATION\nFigure 2: Our knowledge distillation framework for self-supervising ADE extraction using\nLLMs. We first filter PubMed abstracts and select drug-related ones as the unla-\nbeled corpus for ADE self-supervision. We then call upon the teacher LLM (e.g.,\nGPT-3.5) to generate ADE annotations and train a student model.\nTo efficiently recognize and normalize drug names in the abstracts, we compile a list of\noncology drug names, synonyms, and abbreviations from the NCI Thesaurus. We construct\na trie from this list for rapid search and identification within plain text. Next, we split each\nabstract into sentences, retaining only those containing identified drug names. This process\nresults in a refined ADE related dataset suitable for knowledge distillation.\n3.3.2. Teacher and Student Models in Knowledge Distillation\nOur knowledge distillation process involves two models: the teacher model, which serves\nas the source of self-supervision, and the student model, which learns from self-supervised\nlabels produced by the teacher model.\nTeacher LLM We employ GPT-3.5 (Ouyang et al., 2022) as our teacher model. This\nadvanced language model has demonstrated remarkable performance across various NLP\ntasks, showcasing its strong understanding and reasoning capabilities. To access GPT-3.5,\nwe utilize Azure OpenAI Service, which allows us to interact with the model efficiently and\nsecurely. Through the API, we can submit input prompts and receive generated responses,\nfrom which we will generate self-supervised data to train our student model.\nStudent Models We consider the following state-of-the-art pretrained models for biomed-\nical NLP: 1) PubMedBERT (Gu et al., 2021) and PubMedBERT-Large (Tinn et al., 2021)\nare domain-specific language models pretrained on PubMed text; 2) BioGPT (Luo et al.,\n2022) is a domain-specific generative pretrained transformer model pretrained on PubMed\ntext.\n6\nDistilling LLMs for Biomedical Knowledge Extraction\n3.3.3. Knowledge Distillation Process\nWe outline the knowledge distillation process, which includes generating input-output pairs,\ntraining the student models, and evaluating their performance.\nGenerating Input-Output Pairs We split our ADE-related unlabeled corpus into sen-\ntences and input them to GPT-3.5. We then filter the responses to include only sentences\nwith positive ADE relations, and subsample 40,000 sentences for student model training.\nTraining the Student Models We fine-tune the student models using the generated\ninput-output pairs as labeled examples. For PubMedBERT, we fine-tune the entire model\nusing our proposed architecture. For BioGPT, we employ prefix soft tuning (Li and Liang,\n2021) as standard for GPT models.\nPrompt Design We experiment with zero-shot and few-shot settings, utilizing in-context\nlearning or prompt-based learning. For the zero-shot setting, we provide a task descrip-\ntion in the prompt and instruct the model to return \u201cnone\u201d if no ADE is found, which\nhelps reduce hallucination. For the few-shot setting, we use the same prompt and add five\nrandomly sampled examples (Figure 3).\nPost-Processing In pratice, we found that GPT-3.5 and GPT-4 may fail to identifying\nthe exact span of adverse events and often hallucinate non-existing spans. Therefore, we\nadapt the prompt to ask for the strings only and identify the mentions by string matching.\nEvaluation We employ the same evaluation metric for both supervised learning and the\nmodel-distilled self-supervision approaches, ensuring a fair comparison between the two\nmethods. This metric accounts for the precision, recall, and F1-score, providing a compre-\nhensive assessment of the models\u2019 performance in the ADE extraction task.\n4. Experiments\n4.1. Evaluation Approach and Study Design\nTo assess the efficacy of our proposed method, we first provide details on the evaluation\napproach and study design. The ADE dataset (Gurulingappa et al., 2012) comprises 6,821\nADE relations in 4,272 sentences. As no official train/dev/test split is provided, we divide\nthe dataset into 8:1:1 for train/dev/test split in our study.\nWe conduct an end-to-end evaluation wherein the correctness of an ADE is determined\nonly when both entity extraction and its corresponding drug relation are accurate. We\nreport results in terms of lenient F1 score as the primary metric in this study. Lenient\nF1 score is calculated by considering a true positive when the extracted entity is partially\nor completely correct, allowing for some flexibility in the boundaries of the extracted en-\ntities, while maintaining strict accuracy requirements for the relations between entities.\nThis choice is motivated by the low inter-annotator agreement ratio pertaining to the exact\nboundaries of ADE entities (Henry et al., 2020; Gurulingappa et al., 2012), and our obser-\nvation of inconsistent mention boundaries of adverse events in the dataset, as detailed in\nAppendix A.\n4.2. ADE Extraction Results\nTable 1 compares how various methods perform on ADE extraction: LLM (out-of-box),\ndistillation, supervised.\nImpressively, out of box, GPT-3.5 and GPT-4 already perform\n7\nDistilling LLMs for Biomedical Knowledge Extraction\nPrompt: Extract the adverse events each drug causes in the Message. If no ADE\nis found, return None.\nExample 1:\nMessage: We postulate that the bolus of sulprostone resulted in possible coronary\nspasm that resulted in cardiac arrest.\nAnnotations: sulprostone: cardiac arrest|coronary spasm\nExample 2:\nMessage: In each of the three reported patients, alteration of eyelid appearance\nwith deepening of the lid sulcus was evident as the result of topical bimatoprost\ntherapy.\nAnnotations: bimatoprost: alteration of eyelid appearance|deepening of the lid\nsulcus\nExample 3:\nMessage: Immobilization, while Paget\u2019s bone disease was present, and perhaps\nenhanced activation of dihydrotachysterol by rifampicin, could have led to\nincreased calcium - release into the circulation.\nAnnotations: dihydrotachysterol: increased calcium - release\nExample 4:\nMessage: In two patients clozapine was reinstated after risperidone was discontin-\nued; serum triglyceride levels increased.\nAnnotations: clozapine: serum triglyceride levels increased\nExample 5:\nMessage: The cause of these previously unreported side effects of niacin therapy is\nuncertain but may be related to prostaglandin - mediated vasodilatation, hyper-\nalgesia of sensory nerve receptors, and potentiation of inflammation in the gingiva\nwith referral of pain to the teeth.\nAnnotations:\nniacin:\nhyperalgesia of sensory nerve receptors|pain to the\nteeth|potentiation of inflammation in the gingiva|prostaglandin - mediated vasodi-\nlatation\nFigure 3: Our GPT five-shot prompt for ADE extraction and distillation. The examples are\nchosen randomly. Our zero-shot prompt is similar, except without the examples.\ncompetitively, especially with in-context learning (five-shot). However, they still trail su-\npervised models by a large margin. Interesting, through LLM distillation, a PubMedBERT\nmodel already attains comparable accuracy as the supervised state of the art, while using\nzero labeled example. Although being over three orders of magnitude smaller, this PubMed-\nBERT model outperforms its teacher GPT-3.5 by over six absolute points and outperforms\nGPT-4 by over five absolute points. Compared with PubMedBERT, the distilled BioGPT\nperforms less well. This is not surprising as it\u2019s broadly in line with the observations by Luo\n8\nDistilling LLMs for Biomedical Knowledge Extraction\nTable 1: Comparison of LLMs (out-of-box), distillation, and supervised methods on the\nstandard adverse drug event extraction evaluation (Gurulingappa et al., 2012).\nDespite of being over 1,000 times smaller, the distilled PubMedBERT model sub-\nstantially outperforms its teacher LLM (five-shot GPT-3.5) and attains test F1\n(lenient) comparable to supervised state of the art.\nMethod\nTeacher LLM\nModel\nTraining\nInstances\nTest F1\nLLM out-of-box\n-\nzero-shot GPT-3.5\n-\n78.22\nLLM out-of-box\n-\nzero-shot GPT-4\n-\n84.92\nLLM out-of-box\n-\n5-shot GPT-3.5\n-\n85.21\nLLM out-of-box\n-\n5-shot GPT-4\n-\n86.45\nDistillation\n5-shot GPT-3.5\nBioGPT\n40,000\n84.21\nDistillation\n5-shot GPT-3.5\nPubMedBERT\n40,000\n91.99\nSupervised Learning\n-\nBioGPT\n3,417\n88.08\nSupervised Learning\n-\nPubMedBERT\n3,417\n93.36\net al. (2022): GPT models are superior for generation tasks such as question answering and\nsummarization, but face more challenges in structuring tasks such as knowledge extraction.\nWe leave more in-depth exploration between GPT and BERT models to future work.\nFigure 4 shows the supervised learning curve for PubMedBERT on ADE extraction,\nand how the few-shot LLMs and distillation (also with PubMedBERT) compare. Out of\nbox, LLMs still trail supervised methods by some distance. However, with distillation and\nwithout required any labeled data, this gap can be substantially reduced, which bodes well\nfor general applications where we can\u2019t afford extensive annotation but still want to attain\nhigher accuracy than the original LLMs. There are also additional benefits, such as cost,\nefficiency, white-box model access.\n4.3. Comparison on ADE Extraction Models\nTo compare our propose neural architecture 1 with prior approaches, we follow prior work\nto perform 10-fold cross-validation on the ADE corpus and report \u201dstrict\u201d F1 scores, where\nan adverse event entity is deemed correct only when the mention span matches the gold\nexactly. As shown in Table 2, our models outperform all prior state of the art, indicating\nthat the proposed neural architecture is advantageous for ADE extraction.\n4.4. LLM Distillation for other Biomedical NLP Tasks\nwe evaluate the impact of LLM distillation on other biomedical NLP tasks, as shown in\nTable 3. Below is the task description:\n\u2022 GAD - The Gene-Disease Association (GAD) (Becker et al., 2004) task focuses on\nidentifying associations between genes and diseases from biomedical literature. This\ntask requires the extraction of gene and disease entities from text, as well as the\n9\nDistilling LLMs for Biomedical Knowledge Extraction\nFigure 4: Comparison of distillation and low-resource supervised methods on the basis of\nLenient F1 scores across various training sizes. As the training size decreases,\nthe performance of the supervised model gradually degrades, with knowledge\ndistillation offering a competitive alternative.\nTable 2: Comparison of our proposed neural architecture with prior state-of-the-art meth-\nods in the supervised setting on the standard adverse drug event extraction evalua-\ntion. To enable head-to-head comparison, we follow prior methods to report strict\nF1 with 10-fold cross validation. So the numbers are not directly comparable with\nour other reported results.\nModel\nTest F1 (Strict with 10-fold CV)\nSpERT (Eberts and Ulges, 2019)\n79.24\nTable-Sequence (Wang and Lu, 2020)\n80.01\nSpERT.PL (Santosh et al., 2021)\n82.03\nREBEL (Cabot and Navigli, 2021)\n82.20\nOurs (PubMedBERT)\n84.27\nOurs (PubMedBERT-Large)\n84.53\ndetermination of their relationships. The performance of models on this task is crucial\nfor understanding genetic influences on diseases and advancing precision medicine.\n\u2022 PHI (i2b2 2014) - The Protected Health Information (PHI) task, specifically the\ni2b2 2014 shared task (Uzuner et al., 2014), aims at identifying and redacting personal\nidentifiers in clinical text. The goal is to remove any information that could be used\nto trace back to individual patients, ensuring privacy and compliance with regulations\nsuch as the Health Insurance Portability and Accountability Act (HIPAA).\n10\nDistilling LLMs for Biomedical Knowledge Extraction\nTable 3: Comparison of applying GPT-3.5 out-of-box vs. distilling into a PubMedBERT\nstudent model on additional biomedical NLP tasks. GAD and PHI are standard\nbiomedical knowledge extraction tasks, whereas MedNLI is a text-entailment task.\nFor simplicity, during distillation, we only use the unlabeled text in the training\ndata of each task (with labels excluded) for LLM-powered self-supervision. Adding\nmore unlabeled text (e.g., from PubMed) may further improve the performance.\nTask\nMethod\nModel\nTest F1\nGAD\nLLM\nGPT-3.5 (few-shot)\n49.25\nDistillation\nPubMedBERT\n56.42\nPHI(i2b2 2014)\nLLM\nGPT-3.5 (few-shot)\n64.20\nDistillation\nPubMedBERT\n73.89\nMedNLI\nLLM\nGPT-3.5 (few-shot)\n82.21\nDistillation\nPubMedBERT\n80.24\n\u2022 MedNLI - The Medical Natural Language Inference (MedNLI) (Romanov and Shiv-\nade, 2018) task is based on the NLI task, which involves determining the relationship\nbetween a pair of sentences (entailment, contradiction, or neutral). In the context of\nMedNLI, the sentences are derived from clinical text, making this task valuable for\nunderstanding complex relationships in medical documents.\nAs Table 3 shows, LLM distillation attains similar gains for GAD and PHI, which are\nboth information extraction tasks not unlike ADE extraction. For MedNLI, however, GPT-\n3.5 slightly outperforms its distilled student model.\nThis is not surprising, as MedNLI\nis a textual-entailment task, which is particularly suited for generative models like GPT.\nMoreover, for simplicity, we only use the unlabeled text from the training data (with labels\nremoved) for distillation in these experiments. Better distilled models may be attained if\nwe apply LLM self-supervision to a larger unlabeled dataset, as in ADE extraction.\n5. Discussion\nIn this study, we investigated the potential of using LLMs for scaling biomedical knowledge\ncuration. We found that LLMs, such as GPT-4, already possess a reasonable capability\nin structuring biomedical text and substantial gains can be attained by distilling LLMs\ninto task-specific student models through self-supervised learning. This approach provides\nadditional advantages, such as efficiency, and white-box model access.\nWe conducted a case study on adverse drug event (ADE) extraction, a key health area\nin its own right. Our GPT-3.5 distilled PubMedBERT model achieved comparable accu-\nracy to supervised state-of-the-art methods without using any labeled data. Despite being\nover 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over six\nabsolute points in F1 and GPT-4 by over five absolute points.\nAblation studies on distillation model choice (e.g., PubMedBERT vs. BioGPT) and\nADE extraction architecture shed light on best practices for biomedical knowledge extrac-\ntion. Similar gains were attained by distillation for other standard biomedical knowledge\n11\nDistilling LLMs for Biomedical Knowledge Extraction\nextraction tasks, such as gene-disease associations and protected health information, further\nillustrating the promise of this approach.\nThese findings suggest that LLM distillation and domain-specific models, like PubMed-\nBERT, can significantly contribute to the advancement of machine learning in healthcare.\nBy harnessing the knowledge and capabilities of large language models, we can develop\nmore efficient, cost-effective, and powerful solutions for various healthcare applications.\nLimitations\nDespite the promising results, our study has several limitations:\nFirstly, at the time of this work, the GPT-4 model has just been released. Due to time\nconstraints, we did not conduct the distillation process using GPT-4 as the teacher model.\nIn our few-shot setting, GPT-4 exhibited marginally better performance compared to GPT-\n3.5. Although we suspect that GPT-4 might be a better teacher, the expected gains are\nlikely to be marginal.\nSecondly, during the evaluation process, we assumed the presence of gold drug entities.\nThis assumption is not held by several prior works that we compared our approach against.\nThis difference in methodology might lead to a slight advantage in our setting, as our\nmethod relies on accurate drug entity identification to perform effectively.\nLastly, for knowledge distillation on other clinical tasks, we used the training corpus as\ninput for the teacher model. However, given the relatively small size of these corpora, we\nhave not been able to fully explore the true potential of distillation on these tasks. The\nlimited data might restrict the effectiveness of the distillation process, and we acknowledge\nthat there might be room for improvement with more extensive data and experimentation.\nIn summary, the limitations of our study include the use of GPT-3.5 instead of GPT-\n4 as the teacher model, the assumption of gold drug entities during evaluation, and the\nunexplored potential of distillation on other clinical tasks due to small training corpora.\nFuture work could address these limitations by incorporating the latest language models,\nrefining the evaluation process, and exploring the impact of larger training sets on knowledge\ndistillation performance.\nFuture Work\nTo address the limitations and further enhance the performance of ADE\nextraction and other clinical tasks, several avenues for future research can be explored:\n\u2022 Incorporating additional domain-specific knowledge sources: Leveraging external domain-\nspecific knowledge, such as ontologies and databases, could help improve model per-\nformance and address the issue of inconsistent annotations in the ADE dataset.\n\u2022 Expanding training corpus for other clinical tasks: Increasing the training corpus for\nother clinical tasks using LLMs on unlabeled data could lead to improved performance\nin those tasks.\n\u2022 Evaluating on a broader range of clinical tasks and datasets: Exploring the application\nof our proposed method on additional clinical tasks and datasets can provide further\ninsights into the generalizability and adaptability of our approach in various healthcare\ncontexts.\n\u2022 Investigating the use of GPT-4 in knowledge distillation: Evaluating the potential\nbenefits of incorporating GPT-4 in the knowledge distillation process could lead to\nfurther improvements in model performance across different clinical tasks.\n12\nDistilling LLMs for Biomedical Knowledge Extraction\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large\nlanguage models are few-shot clinical information extractors. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pages 1998\u20132022,\n2022.\nKevin G Becker, Kathleen C Barnes, Tami J Bright, S Hong Wang, and The Genetic\nAssociation Information Network. The genetic association database. Nature genetics, 36\n(5):431\u2013432, 2004.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,\nEce Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid\nPalangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence:\nEarly experiments with gpt-4, 2023.\nPere-Llu\u00b4\u0131s Huguet Cabot and Roberto Navigli.\nRebel: Relation extraction by end-to-\nend language generation. In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 2370\u20132381, 2021.\nLong Chen, Yu Gu, Xin Ji, Zhiyong Sun, Haodan Li, Yuan Gao, and Yang Huang. Extract-\ning medications and associated adverse drug events using a natural language processing\nsystem combining knowledge base and deep learning. Journal of the American Medical\nInformatics Association, 27(1):56\u201364, 2020.\nDavid C Classen, Stanley L Pestotnik, R Scott Evans, James F Lloyd, and John P Burke.\nAdverse drug events in hospitalized patients: excess length of stay, extra costs, and\nattributable mortality. Jama, 277(4):301\u2013306, 1997.\nMolla S Donaldson, Janet M Corrigan, Linda T Kohn, et al. To err is human: building a\nsafer health system. 2000.\nMarkus Eberts and Adrian Ulges.\nSpan-based joint entity and relation extraction with\ntransformer pre-training. arXiv preprint arXiv:1909.07755, 2019.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan\nNaumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretrain-\ning for biomedical natural language processing. ACM Transactions on Computing for\nHealthcare (HEALTH), 3(1):1\u201323, 2021.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin\nHofmann-Apitius, and Luca Toldo. Development of a benchmark corpus to support the\nautomatic extraction of drug-related adverse effects from medical case reports. Journal\nof biomedical informatics, 45(5):885\u2013892, 2012.\nBernal Jim\u00b4enez Guti\u00b4errez, Nikolas McNeal, Clay Washington, You Chen, Lang Li, Huan\nSun, and Yu Su. Thinking about gpt-3 in-context learning for biomedical ie? think again.\narXiv preprint arXiv:2203.08410, 2022.\n13\nDistilling LLMs for Biomedical Knowledge Extraction\nSam Henry, Kevin Buchan, Michele Filannino, Amber Stubbs, and Ozlem Uzuner. 2018\nn2c2 shared task on adverse drug events and medication extraction in electronic health\nrecords. Journal of the American Medical Informatics Association, 27(1):3\u201312, 2020.\nPeter Lee, Carey Goldberg, and Isaac Kohane. The AI Revolution in Medicine: GPT-4 and\nBeyond. Pearson, 2023.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for gen-\neration.\nIn Proceedings of the 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582\u20134597, Online, August 2021. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https:\n//aclanthology.org/2021.acl-long.353.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.\nBiogpt: generative pre-trained transformer for biomedical text generation and mining.\nBriefings in Bioinformatics, 23(6), 2022.\nMilad Moradi, Kathrin Blagec, Florian Haberl, and Matthias Samwald. Gpt-3 models are\npoor few-shot learners in the biomedical domain, 2022.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback.\nAdvances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nAlexey Romanov and Chaitanya Shivade. Lessons from natural language inference in the\nclinical domain. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 875\u2013886, 2018.\nTYSS Santosh, Prantika Chakraborty, Sudakshina Dutta, Debarshi Kumar Sanyal, and\nPartha Pratim Das. Joint entity and relation extraction from scientific documents: role\nof linguistic information and entity types. EEKE@ JCDL, 21, 2021.\nRyan Smith, Jason A. Fries, Braden Hancock, and Stephen H. Bach. Language models in\nthe loop: Incorporating prompting into weak supervision, 2022.\nRobert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jian-\nfeng Gao, and Hoifung Poon. Fine-tuning large neural language models for biomedical\nnatural language processing, 2021.\n\u00a8Ozlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2014 i2b2/uthealth\nshared task on diagnosis and procedure coding for clinical text. In Proceedings of the third\nworkshop on building and evaluating resources for biomedical text mining (BioTxtM2014),\npages 56\u201362, 2014.\nJue Wang and Wei Lu. Two are better than one: Joint entity and relation extraction with\ntable-sequence encoders. arXiv preprint arXiv:2010.03851, 2020.\n14\nDistilling LLMs for Biomedical Knowledge Extraction\nAppendix A. Annotation Inconsistencies\nIn this appendix section, we address the presence of annotation inconsistencies in the ADE\ncorpus.\nTable 4 showcases examples of these inconsistencies, particularly in ambiguous\nboundaries, which can potentially impact the performance of machine learning models\ntrained on this dataset.\nResearchers and practitioners should be cognizant of these in-\nconsistencies when working with the ADE corpus to develop or assess their models.\nTable 4: Examples demonstrating inconsistencies in annotation criteria within the ADE\ncorpus. ADE mention annotations are underlined, while discrepancies in the in-\nclusion of similar words are shown in bold.\nExamples\n\u2022 CONCLUSIONS: Peripheral administration of low-dose vasopressin for septic shock\nshould be discouraged because of the risk of ischemic skin complications.\n\u2022 Warfarin-associated bleeding complication saved life\n\u2022 Acute pulmonary reactions to nitrofurantoin are an uncommon side effect of therapy\nand can cause minor or life-threatening pulmonary dysfunction.\n\u2022 Several hypersensitivity reactions to cloxacillin have been reported\n\u2022 We stress the potential of benzarone to cause hepatotoxicity, which usually resembles\nsevere chronic active hepatitis.\n\u2022 Epoprostenol may be associated rarely with severe erythroderma.\n\u2022 In one patient the vasculitis resolved after termination of the ciprofloxacin therapy; in\nthe other patient the ciprofloxacin-induced hemorrhagic vasculitis was superimposed\non a severe forefoot infection, leading to progressive gangrene and a below-knee\namputation.\n\u2022 The potential for progressive brain injury and subsequent disability related to in-\ntraventricular IL-2 therapy is discussed.\n\u2022 Lethal anuria complicating high dose ifosfamide chemotherapy in a breast cancer\npatient with an impaired renal function.\n\u2022 Late lethal hepatitis B virus reactivation after rituximab treatment of low-grade cu-\ntaneous B-cell lymphoma.\n15\n"
  },
  {
    "title": "Generating Benchmarks for Factuality Evaluation of Language Models",
    "link": "https://arxiv.org/pdf/2307.06908.pdf",
    "upvote": "7",
    "text": "Generating Benchmarks for Factuality Evaluation of Language Models\nDor Muhlgay\u2217 Ori Ram\nInbal Magar\nYoav Levine\nNir Ratner\nYonatan Belinkov Omri Abend Kevin Leyton-Brown Amnon Shashua Yoav Shoham\nAI21 Labs\nAbstract\nBefore deploying a language model (LM)\nwithin a given domain, it is important to mea-\nsure its tendency to generate factually incor-\nrect information in that domain. Existing meth-\nods for factuality evaluation of LLM genera-\ntion focus on facts sampled from the LM itself,\nand thus do not control the set of evaluated\nfacts and might under-represent domain spe-\ncific or rare facts. We propose FACTOR: Fac-\ntual Assessment via Corpus TransfORmation, a\nscalable approach for evaluating LM factuality.\nFACTOR automatically transforms a factual\ncorpus of interest into a benchmark evaluat-\ning an LM\u2019s propensity to generate true facts\nfrom the corpus vs. similar but incorrect state-\nments. We use our framework to create three\nbenchmarks: Wiki-FACTOR, News-FACTOR\nand Expert-FACTOR. We show that: (i) our\nbenchmark scores increase with model size and\nimprove when the LM is augmented with re-\ntrieval; (ii) benchmark score and perplexity do\nnot always agree on model ranking; (iii) when\nperplexity and benchmark score disagree, the\nlatter better reflects factuality in open-ended\ngeneration, as measured by human annotators.\nWe make our data and code publicly available1.\n1\nIntroduction\nDespite rapid improvements in their capabilities,\nlarge Language Models (LMs) still tend to generate\nfactually inaccurate or erroneous text (Lin et al.,\n2022; Maynez et al., 2020; Huang et al., 2020).\nSuch phenomena can pose a significant hurdle to\ndeploying LMs in important or sensitive settings,\nmotivating the development of methods for evalu-\nating LM factuality in open-ended generation.\nMethods for directly evaluating an LM\u2019s propen-\nsity towards factual generation were recently pro-\nposed by Lee et al. (2022) and Min et al. (2023).\nThese methods suggest sampling generations from\n\u2217Corresponding author: dorm@ai21.com\n1https://github.com/AI21Labs/factor\nPrefix: Steve Jobs had a long power \nstruggle with the company's board.\nCompletions:\n(a) In 1985, he was forced out of Apple.\n(b) In 1985, he was forced out of NeXT.\n(c) In 1985, he quit Apple.\n(d) In 1988, he was forced out of Apple.\nLM\n\u2713\n\u2717\n\u2717\n\u2717\n(a)...\n(b)...\n(c)...\n(d)...\nFigure 1: Each example in our evaluation task (dubbed FAC-\nTOR) consists of a prefix and four completions, of which only\none is factually correct (completion (a) in this example). The\nnon-factual completions (b), (c) and (d), marked in red, are\ngenerated according to different factual error types, detailed in\nTable 1. The evaluated model assigns likelihood scores to each\ncompletion separately. It is considered \u201ccorrect\u201d if it assigns\nthe highest likelihood to the factually correct completion over\nall non-factual alternatives.\na model, applying an automatic pipeline for fact\nverification, and then assigning a score correspond-\ning to the percentage of factually correct gener-\nated statements. In task-specific domains, such\nas long-form question answering, evaluation is\nusually done by assessing the relevance of a sam-\npled generation against a reference text (Lin, 2004;\nFabbri et al., 2022). However, the sampling ap-\nproach may introduce bias: by scoring the accu-\nracy of facts that an LM tends to generate in an\nopen-ended setting, high-likelihood facts are over-\nrepresented, while the \u201clong-tail\u201d of rare facts is\nunder-represented.\nCurrently, there are no metrics suited to measur-\ning LM factuality with respect to a controlled set of\nfacts in a generation setting. A common proxy is\nmeasuring LM perplexity; this was widely adopted\nto evaluate retrieval-augmented LMs (Khandelwal\net al., 2020; Borgeaud et al., 2022; Ram et al., 2023;\nShi et al., 2023). However, perplexity is affected\narXiv:2307.06908v2  [cs.CL]  4 Feb 2024\nby many linguistic phenomena, and so cannot be\ndirectly linked to factuality.\nThis paper introduces a novel framework for\ntesting a model\u2019s tendency to generate factual in-\nformation from a given factual corpus: Factual As-\nsessment via Corpus TransfORmation (FACTOR).\nThe key idea is automatically perturbing factual\nstatements taken from the corpus to create a con-\nstant number of similar but false variations for each\ntrue statement (Figure 1). We employed Instruct-\nGPT (Ouyang et al., 2022) to generate the false\nvariations for each true statement. The LM\u2019s FAC-\nTOR accuracy on our benchmark is defined as the\npercentage of examples for which it assigns higher\nlikelihood to the factual completion than to any of\nthe false variations.\nWe applied FACTOR to the Wikipedia and News\ndomains, as well as to a diverse collection of do-\nmain specific question-answer pairs (e.g., medicine,\ntechnology, law); constructing new benchmarks\ndubbed Wiki-FACTOR, News-FACTOR and Expert-\nFACTOR. We used these datasets to evaluate a large\nsuite of LMs from the OPT (Zhang et al., 2022),\nGPT-2 (Radford et al., 2019), and GPT-Neo (Black\net al., 2021) families, ranging from 110M to 66B\nparameters. We show in \u00a75.1 that, as expected,\nFACTOR scores increase with model size. How-\never, even the largest models we evaluated achieved\nscores of only 58% for Wiki-FACTOR, 68% for\nNews-FACTOR, and 55% for Expert-FACTOR,\nindicating that these benchmarks are challenging\neven for large LMs. In \u00a75.2 we show that consis-\ntent FACTOR score improvements can be achieved\nby augmenting the LMs with the simple retrieval\ncomponent used by Ram et al. (2023). This directly\ndemonstrates that retrieval augmentation improves\nfactuality in the LM setting; FACTOR is thus posed\nas a prominent approach for measuring retrieval-\naugmented LMs.\nWe further show that FACTOR accuracy and LM\nperplexity are correlted but can sometime induce\ndifferent orderings between LMs (\u00a75.3). This high-\nlights that FACTOR and perplexity capture differ-\nent aspects of the LMs\u2019 performance (see Figure 2).\nIn \u00a76, we report findings of a manual annotation\neffort over 1, 200 generated completions, which\nreinforces FACTOR accuracy as predictive of fac-\ntuality in open-ended generation.\nFigure 2: Wiki-FACTOR scores versus LM perplexity on\nWikipedia for LMs from the GPT-Neo model family (blue cir-\ncle, sizes 1.3B-20B) and the OPT model family (red triangle,\n1.3B-66B). Labels indicate sizes (in billions). The two may\ndisagree on ranking, e.g., the OPT-66B LM has higher per-\nplexity but better Wiki-FACTOR accuracy than the GPT-J-6B\nLM (marked in green circle). In \u00a76 we annotate text generated\nout of both models and show that better Wiki-FACTOR is\npredictive of more factual text generation.\n2\nRelated Work\nFactuality Evaluation\nThe subject of factual-\nity evaluation has been extensively studied in\ndownstream tasks such as summarization, fact-\nverification and dialog (Honovich et al., 2022;\nHuang et al., 2021; Chen et al., 2021; Tam et al.,\n2023). These works typically focus on factual con-\nsistency, evaluating whether a generated text is sup-\nported by a reference text or context (e.g., source\ndocument and generated summary).\nAnother popular approach suggests probing\nLMs\u2019 internal factual knowledge by using slot\nfilling tasks, e.g., \u201cBarack Obama was born is\n[MASK]\u201d (Petroni et al., 2019, 2021; Roberts et al.,\n2020; Jiang et al., 2020; Elazar et al., 2021; Li et al.,\n2022; Zhong et al., 2021; Peng et al., 2022; Mallen\net al., 2023). These works test LMs in a simplified,\nsynthetic setting.\nFACTOR differs from the above methods as\nit aims at evaluating factuality in a natural open-\nended text generation setting. In such setting, the\ncontext may be needed to reason over the evaluated\nfactual statement, while the factual statement may\nnot be evident in the context (unlike summariza-\ntion).\nRecent works proposed scoring the factuality\nof free-form LM generations samples (Min et al.,\n2023; Lee et al., 2022). However, these approaches\nlack control over the evaluated facts and are biased\ntowards common facts generated by the LM.\nContrastive Datasets\nContrastive evaluation, in\nwhich a model is tested to discern between similar\npositive and negative examples, is widely used in\nvarious tasks (Sennrich, 2017; Burlot and Yvon,\n2017; Glockner et al., 2018; Kaushik et al., 2020).\nFor factuality evaluation, negative examples are\nobtained by perturbing factual claims. This is done\nthrough human annotation, rule-based or model\nbased heuristics (Schuster et al., 2021; Liu et al.,\n2022; Gupta et al., 2022). Following recent works\non benchmarks generation (Perez et al., 2023), we\nemployed Instruct-GPT to generate non-factual\nclaims, as described in the following section.\n3\nThe FACTOR Evaluation Approach\nThis section outlines our proposed approach: Fac-\ntual Assessment via Corpus TransfORmation, or\nFACTOR. Given a corpus, we define a multi-choice\ntask where each example is comprised of a multi-\nsentence prefix, a single factual next sentence com-\npletion, and three non-factual alternative comple-\ntions (Figure 1). In \u00a73.1 we present several prop-\nerties required of a FACTOR benchmark, and de-\nscribe the error verticals along which we generate\nnon-factual alternatives. We then explain our FAC-\nTOR dataset creation pipeline, which automatically\ngenerates a FACTOR benchmark from a given cor-\npus (\u00a73.2). Finally, we apply this pipeline to two\ncorpora Wikipedia and news, and a long-form ques-\ntion answering dataset, creating Wiki-FACTOR,\nNews-FACTOR and Expert-FACTOR. We verify\nthe quality of these datasets through manual anno-\ntations against the required properties (\u00a73.3).\n3.1\nThe Evaluation Task: FACTOR\nWe describe the FACTOR multi-choice factual eval-\nuation task. Each example of our task contains a\nprefix text t, along with four possible full sentence\ncompletions, of which only one is factually correct.\nWe choose the original completion (i.e., the contin-\nuation of t in the corpus) as the factually correct\none. The correct completion is denoted as c+, and\nthe non-factual completions as C\u2212 = {c\u2212\n1 , c\u2212\n2 , c\u2212\n3 }.\nWe evaluate models by measuring the percentage\nof examples where they assign the highest mean\nlog-probability to c+. Formally, a model is correct\non a given example if:\nc+ = argmax\nc\u2208{c+}\u222aC\u2212\nlog p(c|t)\n|c|\n,\n(1)\nwhere |c| is the length of completion c in tokens.\nWe refer to the percentage of correct examples as\nthe FACTOR accuracy.\nWe require each of the \u201cincorrect\u201d completions\nc\u2212 \u2208 C\u2212 to satisfy the following properties:\n1. Non-factuality: c\u2212 contains a false claim;\n2. Fluency: c\u2212 is grammatical;\n3. Similarity to the factual completion: c\u2212 has a\nsmall edit-distance from c+.\nThe second and third properties make it harder\nto distinguish between the factual and non-factual\ncompletions for reasons other than their factual cor-\nrectness, such as fluency or style. Furthermore, it is\ndesirable that the non-factual completions be logi-\ncal and self-consistent, to make them more difficult\nto eliminate. For example, modifying c+ =\u201cThey\ngot married in 2010 and divorced in 2017\u201d by\nchanging 2017 to 2009, results in a non-factual\ncompletion which can be discarded by knowing the\ntemporal relation between marriage and divorce.\nError Types\nNon-factual completions in a FAC-\nTOR dataset should cover diverse factuality error\ntypes. To do so, we adopt the error typology in-\ntroduced in FRANK (Pagnoni et al., 2021). While\nthey introduced their error typology to categorize\nfactual inconsistencies of generated summaries\nw.r.t. the source document, we instead leverage this\ntypology to vary the type of factual inconsistencies\nthat hold between non-factual completions and the\nprefix and completion (t and c+). We focus on the\nfive error types from two error categories: semantic\nframe and discourse (examples in Table 1):\n\u2022 Predicate error: a predicate that is inconsistent\nwith c+ or t.\n\u2022 Entity error: The subject or object of a predi-\ncate are inconsistent with c+ or t.\n\u2022 Circumstance error: The completion contains\ninformation describing the circumstance of a\npredicate (e.g., location, time, manner) that is\ninconsistent with c+ or t.\n\u2022 Coreference error: The contradiction is incon-\nsistent with a pronoun/reference in c+ or t,\nreferring to a wrong or non-existing entity.\n\u2022 Link error: c\u2212 is inconsistent with c+ or t in\nthe way that different statements are linked\ntogether (causal/temporal links).\nOriginal text\n(completion in bold)\n...In 1982, Donne was appointed as the first Queen\u2019s Representative\nto the Cook Islands. After completing his term, he became Chief\nJustice of Nauru and Tuvalu in 1985.\nError Type\nExample\nEntity\nAfter completing his term, he became the Queen\u2019s Representative to\nthe Cook Islands in 1985.\nPredicate\nAfter completing his term, he declined the position of Chief Justice\nof Nauru and Tuvalu in 1985.\nCircumstance\nAfter completing his term, he became Chief Justice of Nauru and\nTuvalu in 1987.\nCoreference\nAfter completing her term, she became Chief Justice of Nauru and\nTuvalu in 1985.\nLink\nBefore completing his term, he became Chief Justice of Nauru and\nTuvalu in 1985.\nTable 1: Error types examples. The original text (top) consists of a prefix and a completion sentence (marked in bold). Each\nexample introduce different perturbation over the original completion of different type (edit marked in red).\n3.2\nGenerating FACTOR Benchmarks\nGiven an evaluation corpus, we generate a FAC-\nTOR benchmark automatically. The process is de-\nsigned to meet the requirements presented in \u00a73.1,\nand follows a four-stage pipeline: (1) prefix and\ncompletion selection, (2) non-factual completion\ngeneration, (3) non-factual completion filtering,\nand (4) non-factual completion selection.\n3.2.1\nPrefix and Factual Completion Selection\nWe select a single sentence from each document as\na factual completion c+. We exclude headlines and\nsentences with less than 10 words. The prefix t is\nthe entire text preceding c+ in the document.\n3.2.2\nNon-factual Completions Generation\nGiven a prefix t and its original completion c+,\nwe use InstructGPT (davinci-003; Ouyang et al.\n2022) to generate a set of contradictory comple-\ntions. We designed a specific prompt instructing\nthe model to generate contradictions corresponding\nto each type of error.2 We only apply each prompt\nto sentences that are relevant to its error type (de-\ntermined through simple heuristics, see App. A.1).\nThe prompts are designed as follows:\n\u2022 Multiple contradiction generation: the model\nis prompted to generate multiple subsequent\ncontradictions in each sampling operation.\nPreliminary experiments showed that this sam-\npling practice improves diversity compared to\nmultiple independent completion sampling.\n2App. D lists the full prompts for each error type.\n\u2022 Edit planning: for each contradiction, the\nmodel first explicitly generates the planned\nedits over the original completion, and then\napplies those edits by writing the entire modi-\nfied completion (similar to chain-of-thought\nprompting; Wei et al. 2022). For instance, the\ncoreference error in Table 1 is generated by\nexplicitly writing the edits (\"Changes: \u2018his\u2019\nto \u2018her\u2019\") and then the contradiction. This\nencourages the model to make minimal edits.\n3.2.3\nNon-factual Completions Filtering\nWe considered the set of generated completions as\ncandidates for non-factual completions. We applied\nautomatic tools to filter out (i) non-contradictory\nand (ii) non-fluent completions.\nNon-Contradictory Completions\nGiven a can-\ndidate completion c, we assert that it is indeed\ncontradictory to the original completion c+ by ap-\nplying an NLI model.3 The premise is set to be\nc+ along with its near context (i.e., the last tokens\nof the prefix t; denoted by tnear). The hypothesis\nis set to be c, also preceded by tnear. We selected\ngenerations classified as contradictory by the NLI\nmodel with a probability higher than \u03c4NLI, i.e.:\npNLI(contradiction | [tnear; c+], [tnear; c])) > \u03c4NLI\nWe chose \u03c4NLI = 0.6 (except for contradictions\ngenerated by the coreference error prompt, where\nwe set \u03c4NLI = 0.3) after using a manual validation\nprocess detailed App. A.2.\n3We used DeBERTa-large model (He et al., 2021) fine-\ntuned on the MNLI dataset (Williams et al., 2018) from Hug-\nging Face: microsoft/deberta-large-mnli.\nProperty\nWiki\nNews\nExpert\nNon-factual\n97.6\n98.3\n97.5\nFluent\n94.0\n97.0\n96.7\nSelf-Consistent\n87.4\n87.3\n83.8\nEdit-Distance\n2.3\u00b1(1.4)\n2.1\u00b1(1.4)\n4.0\u00b1(3.1)\nTable 2: Validation results: percentage of generation that\nmeet each desired property, estimated by manual annotation\nover sub-samples (top), and mean edit-distance between the\ngenerations and their factual completion (bottom).\nNon-Fluent Completions\nTo verify that c is a\nfluent completion we use GPT2-Small (Radford\net al., 2019) scores, similar to Gupta et al. (2022):\nWe filter out generations with mean log-likelihood\nlower than the original completion\u2019s by a fixed\nmargin \u03c4LM. Using a manual validation, we set\n\u03c4LM = 0.2 (see App. A.2). Formally, we selected a\ncompletion c if it satisfies:\nlog p(c)\n|c|\n> log p(c+)\n|c+|\n\u2212 \u03c4LM\n3.2.4\nNon-factual Completion Selection\nFinally,\nwe\nselect\nnon-factual\ncompletions\nc\u2212\n1 , c\u2212\n2 , c\u2212\n3\nfrom the filtered candidates.\nFor\nincreased error type diversity, we choose one\ncompletion per type, and repeat types only when\nnot enough generations meet the \u00a73.2.3\u2019s criteria.\n3.3\nApplying FACTOR to Knowledge\nIntensive Domains\nWe focused on three knowledge intensive domains:\nWikipedia (encyclopedic knowledge), news (cur-\nrent events) and long-form question answering in\nspecific domains. We constructed the following\nevaluation datasets:\n\u2022 Wiki-FACTOR: based on the Wikipedia sec-\ntion of The Pile\u2019s validation split (Gao et al.,\n2021), containing 2994 examples.\n\u2022 News-FACTOR: based on Reuters articles pub-\nlished after 1/10/2021, extracted from The\nRefinedWeb Dataset (Penedo et al., 2023).\nThe dataset consists of 1036 examples.\n\u2022 Expert-FACTOR: based on the validation and\ntest splits of ExpertQA (Malaviya et al., 2023),\na long-form expert-curated question answer-\ning dataset spanning various fields, which\nsuits the motivation of FACTOR to evaluate\nrare facts. Each document in the corpus is a\nconcatenation of a question-answer pair. The\ndataset consists of 236 examples.\nType\nWiki\nNews\nExpert\nPredicate\n25.4\n31.3\n47.1\nEntity\n42.8\n48.0\n38.8\nCircumstance\n24.2\n16.0\n7.1\nCoreference\n4.4\n2.3\n2.9\nLink\n3.2\n2.3\n4.2\nTable 3: Annotated error type distribution for Wiki-FACTOR\n(Wiki), News-FACTOR (News), Expert-FACTOR (Expert).\n3.3.1\nDataset Validation\nTo validate that our FACTOR benchmarks meet\nthe required properties detailed in \u00a73.1, we man-\nually evaluated a sub-sample from each dataset.\nWe sampled 138 examples from Wiki-FACTOR,\n100 examples from News-FACTOR and 80 exam-\nples from Expert-FACTOR, containing 414, 300\nand 240 generations overall. Each generation was\nannotated w.r.t. the properties manifested in \u00a73.1,\nnamely whether they were (1) non-factual, (2) flu-\nent, and (3) self-consistent. To assess datasets diver-\nsity, we annotated the contradictions in accordance\nwith the error typology of Pagnoni et al. (2021),\ndescribed in \u00a73.1. We verified that the non-factual\ncompletions are minimally edits variants of the fac-\ntual completion by measuring mean edit distances.\nValidation results in Table 2 show that for all\ndatasets, almost every generated completion indeed\ncontradicts the original one, was fluent, and was\nself consistent. Table 3 shows the error type dis-\ntribution, indicating that FACTOR yields diverse\ncontradiction types. Semantic frame errors (Entity,\nPredicate, and Circumstance) were more prevalent\nthan discourse errors (Link and Coreference), as\nmore sentences are suited for these type of errors.\n4\nExperimental Setup\nWe used FACTOR benchmarks to evaluate factual\nknowledge of LLMs across varying model families.\nWe describe the experimental setup below.\n4.1\nDatasets\nThe Wiki-FACTOR, News-FACTOR and Expert-\nFACTOR datasets are described in \u00a73.3. For per-\nplexity evaluation (\u00a75.3), we selected a subset of\n300 Wikipedia articles from the documents Wiki-\nFACTOR is based on (\u223c367K tokens).\n4.2\nModels\nWe performed our experiments over a set of open\nsource models:\nfour models of GPT-2 family\n(110M\u20131.5B; Radford et al. 2019), five models\nModel Size (# billions)\nFACTOR Accuracy\n20\n30\n40\n50\n60\n0.1\n1\n10\n100\nGPT-2\nGPT-Neo\nOPT\n(a) Wiki-FACTOR\nModel Size (# billions)\nFACTOR Accuracy\n25\n35\n45\n55\n65\n75\n0.1\n1\n10\n100\nGPT-2\nGPT-Neo\nOPT\n(b) News-FACTOR\nModel Size (# billions)\nFACTOR Accuracy\n20\n30\n40\n50\n60\n0.1\n1\n10\n100\nGPT-2\nGPT-Neo\nOPT\n(c) Expert-FACTOR\nFigure 3: Accuracy per model size for Wiki-FACTOR (left), News-FACTOR (center), and Expert-FACTOR (right) for models\nfrom GPT-2 (yellow square), GPT-Neo (blue circle), and OPT (red triangle) families.\nfrom the GPT-Neo family (125M\u201320B; Black et al.\n2021, 2022; Wang and Komatsuzaki 2021), and\neight models of OPT (125M\u201366B; Zhang et al.\n2022). We capped the sequence length at 1024\ntokens to compare all models directly.\nThe corpora that our FACTOR benchmarks were\nconstructed from were not used for training any of\nthe examined models. News-FACTOR is based on\narticles published after 1/10/2021, while Expert-\nFACTOR is based on examples written in 2023.\nBoth are beyond the models\u2019 data cutoff date. Wiki-\nFACTOR is based on Wikipedia documents from\nThe Pile\u2019s validation split, which is not part in any\nof the models\u2019 training sets. (OPT and GPT-Neo\nmodels were trained on The Pile\u2019s training split,\nGPT-2 models were not trained on Wikipedia).\n4.3\nRetrieval-Augmented Models\nIn \u00a75.2, we present evaluations of retrieval-\naugmented variants of the models. To that end, we\nadopted the In-Context RALM (IC-RALM) frame-\nwork of Ram et al. (2023), where the retrieved doc-\nument is prepended to the LLM\u2019s input, without\nany further training or specialized LLM architec-\nture. In IC-RALM, a retriever is called every s\ntokens (i.e., the stride), with a query comprised of\nthe last \u2113 tokens. The LLM is run with the concate-\nnated input to assign log-probabilities to the next s\ntokens. We used the lexical BM25 (Robertson and\nZaragoza, 2009) over Wikipedia corpus,4 exclud-\ning the evaluated docs; and set s = 8, \u2113 = 32.\n5\nFactual Knowledge Evaluation Results\nThis section describes the experimental evaluation\nof LLM factuality using our FACTOR benchmarks.\nIn \u00a75.1 we show that FACTOR accuracy increases\nwith model size but also depends on the training\n4We used the Wikipedia corpus of Karpukhin et al. (2020),\nbased on the dump from Dec. 20, 2018.\ndata (different model families differ in scores). In\n\u00a75.2, we show that retrieval augmentation of the\nLM improves FACTOR accuracy, positioning it as\nthe first automatic measure of factuality improve-\nment for retrieval augmented LMs. Finally, in \u00a75.3,\nwe show that the pairwise model ranking of corpus\nperplexity and FACTOR accuracy can differ signifi-\ncantly. This outcome, along with manual validation\nof the correlation between FACTOR accuracy and\nfactual generation in \u00a76, solidifies FACTOR accu-\nracy as a novel automatic measure for evaluating\nthe proneness of an LM to generate factual infor-\nmation in a certain domain.\n5.1\nFactual Knowledge Improves with Model\nSize\nWe evaluate GPT-2, GPT-Neo, and OPT models\non Wiki-FACTOR, News-FACTOR and Expert-\nFACTOR (Figure 3).\nLarger models generally\noutperform smaller ones within the same model\nfamily.\nHowever, even the largest models are\ncapped at 58.0% (GPT-NeoX-20B), 68.1% (OPT-\n66B) and 55.9% (OPT-30B) on Wiki-FACTOR,\nNews-FACTOR and Expert-FACTOR respectively,\nindicating the benchmarks are challenging. Re-\ncent works (Chuang et al., 2023; Kai et al., 2024)\nuse Wiki-FACTOR and News-FACTOR to evaluate\nmodels from the LLaMA family (Touvron et al.,\n2023) and show similar trends.\nWe observe that all models achieve higher FAC-\nTOR accuracy on news comparing to the other two\ndomains. This may be because news articles cover\nspecific events, making the prefix more useful for\ndetecting factual completions (further discussion\nin App. B.2). When comparing different model-\nfamilies, we find that the OPT models leads on\nNews-FACTOR, while the GPT-Neo family leads\non Wiki-FACTOR. This implies that the different\ndata sources used for training these two model fam-\nFACTOR Accuracy\n20\n30\n40\n50\n60\nGPT-Neo-125M\nGPT-Neo-1.3B\nGPT-Neo-2.7B\nGPT-J-6B\nGPT-Neox-20B\nLM\nRetrieval Augmented LM\nFACTOR Accuracy\n20\n30\n40\n50\n60\nOPT-125M\nOPT-350M\nOPT-1.3B\nOPT-2.7B\nOPT-6.7B\nOPT-13B\nOPT-30B\nOPT-66B\nLM\nRetrieval Augmented LM\nFigure 4: Factual accuracy over Wiki-FACTOR for GPT-Neo and OPT models, compared to their IC-RALM variants. IC-RALM\nleads to consistent improvement for all models.\nilies are suited to different domains.\n5.2\nThe Effect of Retrieval Augmentation on\nFactual Knowledge\nNext, we ask: Can FACTOR accuracy be improved\nby augmenting models with a retrieval component?\nImportantly, while a clear motivation for retrieval\naugmentation is factual grounding of LMs, no ex-\nisting metrics allow direct measurement of it in a\ntext generation setting. We propose FACTOR ac-\ncuracy as an alternative to the course measure of\nLM perplexity, which is often used to assess these\nmethods (Khandelwal et al., 2020; Borgeaud et al.,\n2022; Ram et al., 2023; Shi et al., 2023).\nWe compared the FACTOR accuracy of LLMs\nto that of their retrieval-augmented counterparts,\nimplemented following the IC-RALM framework\n(\u00a74.3; Ram et al. 2023). Figure 4 show the re-\nsults for GPT-Neo and OPT Wiki-FACTOR. We\nobserved consistent gains from augmenting the\nmodels with retrieval. These results highlight that\ngrounding the model in an external corpus can im-\nprove its factuality. Since the retriever used in our\nexperiments is used in an \u201coff-the-shelf\u201d manner,\nwe speculate that further performance boosts may\nbe gained by a retriever system specialized for this\ntask (Izacard et al., 2022; Ram et al., 2023).\nAnother interesting finding is that the relative\ngains in FACTOR accuracy obtained by IC-RALM,\nare more moderate compared to the relative gains in\nperplexity over WikiText-103 (Merity et al., 2016),\nreported by Ram et al. (2023). We explore the\nconnection between the two in the next section.\n5.3\nPerplexity Correlates but is not Always\nAligned with FACTOR Accuracy\nWe investigate whether FACTOR accuracy adds ad-\nditional information beyond perplexity, when used\nas a comparative metric for selecting which LM\nto use within a certain corpus. Figure 2 shows the\nFACTOR accuracy of models on Wiki-FACTOR,\ncompared to their token-level perplexity on the\nWikipedia section of The Pile\u2019s validation set (\u00a74.1)\n(App. B.1 includes all evaluated models). Over-\nall, we observe a high correlation between the two\nmetrics. However, there are cases where they dis-\nagree (i.e., a pair of models where one is better\nwhen measured by perplexity but worse in terms\nof FACTOR accuracy). For example, GPT-Neo-\n2.7B is significantly better than OPT-2.7B in terms\nof perplexity (9.0 vs. 10.1), but slightly worse in\nterms of FACTOR accuracy (46.3% vs. 46.6%).\nIn addition, GPT-J-6B has lower perplexity com-\npared to OPT-66B (7.4 vs. 7.6), while OPT-66B is\nsignificantly better in terms of FACTOR accuracy\n(57.7% vs. 53.5%). This finding suggests that (i)\nFACTOR accuracy offers a complementary view\nof models\u2019 performance, not necessarily captured\nby perplexity, and (ii) improvements in perplexity\ndo not necessarily imply better factuality.\n6\nFactuality in Open-Ended Generation\nThis section explores the connection between FAC-\nTOR accuracy and factuality in open-ended gener-\nation, via human annotations.\n6.1\nExperimental Setup\nWe selected tuples of prefix, original completion\nand non-factual completion (t, c+, c\u2212) from Wiki-\nFACTOR. We then manually identified the minimal\nfactual claim modified by c\u2212, denoted by f. For\nexample, the predicate error from Table 1, in which\n\u201cbecame\u201d was replaced with \u201cdeclined the position\nof\u201d, the edit relates to the minimal fact \u201cDonne\nbecame Chief Justice of Nauru and Tuvalu\u201d.\nWe let LLMs generate free text, conditioned on\nthe prefix and the completion until the edit induced\nby c\u2212. Formally, let c be the common prefix of c+\nand c\u2212 (in the predicate error example, c is \u201cAfter\ncompleting his term, he\"). The LLM is conditioned\non the concatenation of t and c. The LLM might\ngenerate the correct fact, text violating it, or other\ncompletion that does not refer to it. For each exam-\nple we manually annotated whether the generated\ntext is true, false, or neutral w.r.t. f.\nWe analyzed two models with a similar token-\nlevel perplexity but a significant gap in FACTOR\naccuracy: GPT-J 6B and OPT-66B (marked in a\ngreen circle in Figure 2). For each model, we con-\nsidered two groups of examples: examples with\nc+, c\u2212 pairs for which the model was right, i.e.,\nthe model assigns larger mean log-likelihood to c+\ncompared to c\u2212, and pairs for which the model was\nwrong (the complement set). We sampled three\ngenerations per example for 100 examples from\neach group and for each model. Overall, we cre-\nated 1200 generations. We filtered some of the\nsamples due to ill-formatted generations or non-\ncontradictory completions (14.5% of all samples).\n6.2\nResults\nWe assess model\u2019s knowledge of the minimal facts\nthrough manual annotation. We only considered\nrelevant generations for their minimal fact f, ex-\ncluding \"neutral\" generations (59.5% and 54.3%\nfor GPT-J 6B and OPT-66B, respectively). For each\nmodel, we measure the percentage of generated\ntexts that are true w.r.t. f in the \"right\" and \"wrong\"\nsubsets separately. We obtained the overall FAC-\nModel\nSubset\nFact. Accuracy\nGPT-J 6B\nRight\n30.0%\nWrong\n10.5%\nAll (Weighted)\n24.8%\nOPT-66B\nRight\n46.6%\nWrong\n4.6%\nAll (Weighted)\n38.8%\nTable 4: Manual factuality annotation results for OPT-66B and\nGPT-J 6B. For each model, we present the results per right\nand wrong subsets. Bottom row shows the weighted average\nbetween the right and wrong variants w.r.t to the right/wrong\npairs of Wiki-FACTOR.\nTOR accuracy by weighting the subsets results ac-\ncording to their distribution in Wiki-FACTOR. Re-\nsults in Table 4 (full results in App. B.2).\nAccuracy over Wiki-FACTOR is linked with\nfactuality in open-ended generation.\nFor cases\nwhere models were wrong, they generated more\nfalse claims regarding their minimal fact. For ex-\nample, OPT-66B only generated a true claim 4.6%\nof the times it was wrong, compared to 46.6% for\nwhen it was right. This suggests that FACTOR\naccuracy can shed light on the model\u2019s ability to\ngenerate factual claims accurately.\nAs a comparative metric, accuracy over Wiki-\nFACTOR aligns with factuality in open-ended\ngeneration.\nThere were gaps in factuality anno-\ntation between OPT-66B and GPT-J 6B: OPT-66B\ngenerated true claims 38.8% of the time, while\nGPT-J 6B generated only 24.8%. This aligns with\nthe models\u2019 performance over Wiki-FACTOR, de-\nspite sharing similar perplexity on Wiki. This sug-\ngests that FACTOR is a better proxy for measuring\nmodel factuality in a specific domain.\n7\nDiscussion\nThis paper introduces FACTOR, a novel way to\nevaluate LMs\u2019 factuality. FACTOR creates an eval-\nuation benchmark from a corpus, consisting of fac-\ntual statements and non-factual variations. By com-\nparing the LM\u2019s likelihood of factual claims with\nnon-factual variants, FACTOR score captures the\nLM\u2019s propensity to generate factual information.\nMetrics for measuring factual knowledge over a\ngiven corpus are lacking. Prior works used perplex-\nity, which may be affected by factors other than\nfactual knowledge and does not contrast facts with\nfalse statements. FACTOR focuses the language\nmodeling task on factuality by taking a contrastive\napproach. Our experiments show that FACTOR\nranks models differently than perplexity and is\nmore aligned with factuality in open-ended gen-\neration. These findings highlight the importance of\nnegative examples for evaluating factuality. More-\nover, they indicate that incorporating negative ex-\namples into training sets might also help optimizing\nmodels to be more factual. We leave investigation\nof training with FACTOR style data to future work.\nOur work joins recent studies on factuality eval-\nuation in a text-generation setting, which proposed\nto evaluate models by fact-checking the model\u2019s\ngenerations (Lee et al., 2022; Min et al., 2023). As\nFACTOR focuses on evaluation over a controlled\nset of facts, we see these two approaches as com-\nplementary; together, they yield a more holistic\nassessment of LM factuality.\nLimitations\nWe point to several limitations of our work. First,\nsince FACTOR benchmarks are generated in an\nautomated way, they may not fully comply with\nthe requirements we define in \u00a73.1, as analyzed in\n\u00a73.3. Second, generating FACTOR benchmarks\nfor different domains may pose new challenges.\nFor instance, the selection of factual completions\nis straightforward in knowledge-intensive domains,\nwhere nearly every sentence in the corpus contains\nfactual information. However, in general cases, a\nmore intricate approach is needed to identify such\nsentences. Moreover, the generation of non-factual\ncompletions is based on a prompted model, specif-\nically designed for the Wikipedia domain. While\nwe observed those prompts applied well for the\nnews domain, their effectiveness may vary in other,\nmore specific domains.\nEthics Statement\nLanguage models\u2019 tendency to generate factually\ninaccurate text raises significant issues. FACTOR\nallows automatic evaluation of factuality, which\ncan be used to efficiently measure and develop\nmethods for mitigating these risks. However, we\nstress that when deploying such models in sensitive\nsettings, automatic evaluations may not be suffi-\ncient, and human evaluation is required.\nReferences\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021.\nGPT-Neo:\nLarge\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95\u2013136, virtual+Dublin. Association\nfor Computational Linguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML.\nFranck Burlot and Fran\u00e7ois Yvon. 2017. Evaluating the\nmorphological competence of machine translation\nsystems. In Proceedings of the Second Conference\non Machine Translation, pages 43\u201355, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nYiran Chen, Pengfei Liu, and Xipeng Qiu. 2021.\nAre factuality checkers reliable? adversarial meta-\nevaluation of factuality in summarization. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 2082\u20132095, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James Glass, and Pengcheng He. 2023. DoLa:\nDecoding by contrasting layers improves factuality\nin large language models.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012\u20131031.\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. QAFactEval: Improved QA-\nbased factual consistency evaluation for summariza-\ntion. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 2587\u20132601, Seattle, United States. Asso-\nciation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021.\nThe Pile: An\n800gb dataset of diverse text for language modeling.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking NLI systems with sentences that\nrequire simple lexical inferences. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 650\u2013655, Melbourne, Australia. Association\nfor Computational Linguistics.\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. DialFact: A benchmark for\nfact-checking in dialogue.\nIn Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3785\u20133801, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-enhanced\nBERT with disentangled attention. In International\nConference on Learning Representations.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the Second\nDialDoc Workshop on Document-grounded Dialogue\nand Conversational Question Answering, pages 161\u2013\n175, Dublin, Ireland. Association for Computational\nLinguistics.\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020.\nChallenges in building intelligent open-domain dia-\nlog systems. ACM Trans. Inf. Syst., 38(3).\nYichong Huang, Xiachong Feng, Xiaocheng Feng, and\nBing Qin. 2021. The factual inconsistency problem\nin abstractive text summarization: A survey. arXiv\npreprint arXiv:2104.14839.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423\u2013438.\nJushi Kai, Tianhang Zhang, Hai Hu, and Zhouhan Lin.\n2024. SH2: Self-highlighted hesitation helps you\ndecode more truthfully.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\n2020. Learning the difference that makes a differ-\nence with counterfactually-augmented data. In Inter-\nnational Conference on Learning Representations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-\ncale Fung, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2022. Factuality enhanced language models for\nopen-ended text generation. In Advances in Neural\nInformation Processing Systems.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,\nChengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang,\nand Qun Liu. 2022. How pre-trained language mod-\nels capture factual knowledge? a causal-inspired\nanalysis. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 1720\u20131732,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214\u20133252, Dublin,\nIreland. Association for Computational Linguistics.\nTianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,\nZhifang Sui, Weizhu Chen, and Bill Dolan. 2022.\nA token-level reference-free hallucination detection\nbenchmark for free-form text generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 6723\u20136737, Dublin, Ireland. Association\nfor Computational Linguistics.\nChaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth\nSieber, Mark Yatskar, and Dan Roth. 2023. Expertqa:\nExpert-curated questions and attributed answers. In\narXiv.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802\u20139822, Toronto,\nCanada. Association for Computational Linguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906\u20131919, On-\nline. Association for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFActscore: Fine-grained Atomic Evaluation of Fac-\ntual Precision in Long Form Text Generation. arXiv\npreprint arXiv:2305.14251.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730\u201327744.\nCurran Associates, Inc.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812\u20134829, Online. As-\nsociation for Computational Linguistics.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora with\nweb data, and web data only.\nHao Peng, Xiaozhi Wang, Shengding Hu, Hailong Jin,\nLei Hou, Juanzi Li, Zhiyuan Liu, and Qun Liu.\n2022. COPEN: Probing conceptual knowledge in\npre-trained language models. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 5015\u20135035, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nEthan Perez, Sam Ringer, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Karina\nNguyen, Edwin Chen, Scott Heiner, Craig Pettit,\nCatherine Olsson, Sandipan Kundu, Saurav Kada-\nvath, et al. 2023. Discovering language model behav-\niors with model-written evaluations. In Findings of\nACL.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523\u20132544, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases?\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463\u20132473, Hong Kong, China. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418\u20135426,\nOnline. Association for Computational Linguistics.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333\u2013389.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624\u2013643, Online. As-\nsociation for Computational Linguistics.\nRico Sennrich. 2017. How grammatical is character-\nlevel neural machine translation? assessing MT qual-\nity with contrastive translation pairs. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 376\u2013382, Valencia, Spain.\nAssociation for Computational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models.\narXiv\npreprint arXiv:2301.12652.\nDerek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah\nKwan, Mohit Bansal, and Colin Raffel. 2023. Evalu-\nating the factual consistency of large language mod-\nels through summarization. In Findings of ACL.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand efficient foundation language models.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A\n6 Billion Parameter Autoregressive Language Model.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open pre-\ntrained transformer language models.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017\u20135033, Online. Association\nfor Computational Linguistics.\nA\nTechnical Details of FACTOR Data\nPipeline\nA.1\nIdentifying Sentences\u2019 Relevant Error\nTypes\nFor each sentence, we identify the types of edits\nwe can apply to it. First, we use a part-of-speech\ntagger to detect relevance for entity error (detect-\ning nouns), predicate error (detecting verbs) and\ncoreference error (detecting pronouns). For circum-\nstances errors, we use Named-Entity Recognition\ntaggers to identify sentences containing locations,\ndates, and time entities. Finally, we search for tem-\nporal/causal link words from a predefined set of\nwords, which implies relevance for link errors.\nA.2\nSetting Filters Thresholds\nAs discussed in \u00a73.2.3, we applied two filters to\nensure the quality of the potential completions\u2013an\nNLI filter (to filter out non-contradictory comple-\ntions) and an LM filter (to filter out non-fluent\ncompletions). To choose the thresholds \u03c4NLI and\n\u03c4LM, we manually annotated 40 samples w.r.t to\nToken Perplexity\nFACTOR Accuracy\n20\n30\n40\n50\n60\n10.00\n15.00\n20.00\n25.00\nGPT-2\nGPT-Neo\nOPT\nFigure 5: Accuracy per token perplexity over Wiki-FACTOR.\nthe properties specified in \u00a73.1 (i.e., (1) contradic-\ntory and (2) fluent and self-consistent). We have\ntested thresholds 0.1-0.9, and chose the threshold\nwhich achieved highest precision without filtering\nout too many samples (max 35% of the samples).\nFor the NLI filter we used DeBERTa-largs model\nfine-tuned on the MNLI dataset. Best threshold\nwas \u03c4NLI = 0.6, with precision of 0.96. Manually\nevaluating the different contradiction types we have\nnoticed this threshold was too harsh for corefrence\ncontradiction (87.5% of the completions were fil-\ntered out. Therefore we reduced its threshold to\n0.3 which filtered out 75% of the samples). For the\nLM filter we used GPT2-Small. Best threshold was\n\u03c4LM = 0.2, with precision of 0.78.\nB\nExtended Results and Discussion\nB.1\nComparison between Perplexity and\nFACTOR Accuracy over Wikipedia\nFigure 5 presents Wiki-FACTOR scores versus LM\nperplexity on Wikipedia. The figure extends Figure\n2, presenting all evaluated LMs: models from the\nGPT-Neo family (blue circle), OPT family (red\ntriangle) and GPT2 family (yellow square).\nB.2\nFactuality in Open-ended Generation\nTable 6 shows the extended results for the man-\nual factuality annotation for open-ended generation\nexperiment \u00a76. In addition to the overall results,\nwe include the distribution of Neutral/True/False\nannotations. Notably, most generations are neu-\ntral for both models. This highlights the limitation\nof sampled-based approach for assessing model\u2019s\nfactual knowledge.\nB.3\nKnowledge of Unseen Facts\nAs seen in Figure 3 in \u00a75.1, FACTOR-accuracy\nis often way above the random baseline of 25%,\nindicating that some models succeed in predict-\ning unseen facts. It is possible that the knowledge\nof these facts is derived from another document\nin the training data (for example, Wikipedia con-\ntains many different articles related to each other,\nsharing similar factual statements). Another possi-\nbility is that an unseen fact is implied by the prefix.\nWe hypothesize that this leads to higher FACTOR\nscores in the news domain, which often covers\nspecific events, making the prefix more useful for\ndetecting factual completions. Analysis of these\ncases is non-trivial, and is left for future work.\nC\nDataset Licenses\nTable 5 details the license for each corpus we used\nin the paper:\nDataset\nLicense\nThe Pile\nMIT\nThe RefinedWeb\nODC-By 1.0\nExpertQA\nMIT\nTable 5: Datasets\u2019 licenses\nD\nPrompts for Contradictions Generation\nWe prompted the model to generate multiple candi-\ndate completions, For each of the five error types:\nentity (Table 7), circumstance (Table 8), corefer-\nence (Table 9), predicate (Table 10 and 11) and\nlink (Table 12). The prompts are concatenated to\na given a completion and its near context, with\nthe exception of link-prompt where only the com-\npletion is given (we found that the instruct model\ntends to repeat the context when it\u2019s appended to\nthis particular prompt). The prompts instruct the\nmodel to first plan its local edits, and then generate\nthe contradiction.\nModel\nVariant\nNeutral\nTrue (T)\nFalse (F)\nFact. Accuracy\n\u0000=\nT\nT+F\n\u0001\nGPT-J 6B\nRight\n62.4%\n11.3%\n26.3%\n30.0%\nWrong\n48.8%\n5.4%\n45.8%\n10.5%\nAll (Weighted)\n59.5%\n10.0%\n30.5%\n24.8%\nOPT-66B\nRight\n54.1%\n21.4%\n24.5%\n46.6%\nWrong\n55.1%\n2.1%\n42.8%\n4.6%\nAll (Weighted)\n54.3%\n17.7%\n28.4%\n38.8%\nTable 6: Manual factuality annotation results for OPT-66B and GPT-J 6B. For each model, we present the results per right and\nwrong subsets. Bottom row shows the weighted average between the right and wrong variants w.r.t to the right/wrong pairs of\nWiki-FACTOR.\nType\nPrompt\nEntity\nGiven a context and a completion, write diverse alternative completions that contradict\nthe original completion meaning.\nFirst, identify if the completion contains an entity. Then, write the contradiction by\nmodifying an entity or it\u2019s property, add additional modifications if necessary.\nMake sure the changes you make are minimal (so only change necessary details to make\nthe sentence plausible). Do not modify dates or quantities.\n##\nContext: \"Sorry\" is a song by American singer Madonna from her tenth studio album\nConfessions on a Dance Floor (2005). It was written and produced by Madonna and\nStuart Price, and released as the second single from the album on February 7, 2006.\nIt later appeared on Celebration, her 2009 greatest hits album. An uptempo dance song,\n\" Sorry \" was one of the first tracks developed for the album and had numerous remix\ntreatments before the ultimate version of the track was finalized.\nCompletion: One of the remixes was done by the known band the Pet Shop Boys,\nfeaturing added lyrics by the band.\n1. Change: \"Pet Shop Boys\" to \"Maddona\".\nContradiction: One of the remixes was done by the known singer Maddona,\nfeaturing added lyrics by the singer. 2. Change: \"Pet Shop Boys\" to \"Depeche Mode\".\nContradiction: One of the remixes was done by the known band Depeche Mode,\nfeaturing added lyrics by the band.\n3. Change: \"known\" to \"unfamiliar\".\nContradiction: One of the remixes was done by the unfamiliar band Pet Shop Boys,\nfeaturing added lyrics by the band.\n4. Change: \"Pet Shop Boys\" to \"the Killers\".\nContradiction: One of the remixes was done by the known band the Killers,\nfeaturing added lyrics by the band.\n##\nContext: {context}\nCompletion: {completion}\nTable 7: Prompt for entity-errors generation\nType\nPrompt\nCircumstance\nGiven a context and a completion, write diverse alternative completions that contradict the\noriginal completion meaning.\nFirst, identify if the completion describes the circumstances of an event (location or time). If\ncircumstances are mentioned, modify it to contradict the completion. Do not add time or location if\nthey didn\u2019t appear in the original completion. Make sure the changes you make are minimal.\n##\nContext: The kingdom had been in long gradual decline since the early 13th century. Had Pagan\npossessed a stronger central government, the collapse could have been temporary, and the country\n\"could have risen again\". But the dynasty could not recover, and because the Mongols refused to fill\nthe power vacuum, no viable center emerged in the immediate aftermath. As a result, several minor\nstates fought it out for supremacy for the better part of the 14th century.\nCompletion: It was only in the late 14th century that two relatively strong powers emerged in the\nIrrawaddy basin, restoring some semblance of normalcy.\n1. Change: \"14th\" to \"15th\".\nContradiction: It was only in the late 15th century that two relatively strong powers emerged in the\nIrrawaddy basin, restoring some semblance of normalcy. 2. Change: \"Irrawaddy\" to \"Chindwin\".\nContradiction: It was only in the late 14th century that two relatively strong powers emerged in the\nChindwin basin, restoring some semblance of normalcy.\n3. Change: \"late\" to \"mid\".\nContradiction: It was only in the mid 14th century that two relatively strong powers emerged in the\nIrrawaddy basin, restoring some semblance of normalcy.\n##\nContext: {context}\nCompletion: {completion}\nTable 8: Prompt for circumstance-errors generation\nType\nPrompt\nCoreference\nGiven a context and a completion, write diverse alternative completions that contradict\nthe original completion meaning. First, decide if the completion contains a pronoun\n(such as: he, she, it, they, his, her, its, theirs...) and write the entity it refers to.\nWrite the contradiction by modifying the pronoun to contradict the original coreference.\n##\nContext: His stance in favor of prohibition cost him the votes of four legislators in his\nown party and the seat went to Republican William O. Bradley. Six years later\nBeckham secured the seat by popular election, but he lost his re-election bid largely\nbecause of his pro-temperance views and his opposition to women\u2019s suffrage.\nCompletion: Though he continued to play an active role in state politics for\nanother two decades, he never returned to elected office, failing in his gubernatorial\nbid in 1927 and his senatorial campaign in 1936.\n1. Pronoun: he\nChange: \"he\" to \"Bradley\".\nContradiction: Though Bradley continued to play an active role in state politics for\nanother two decades, he never returned to elected office, failing in his gubernatorial\nbid in 1927 and his senatorial campaign in 1936.\n2. Pronoun: he\nChange: \"he\" to \"Bradley\".\nContradiction: Though he continued to play an active role in state politics for\nanother two decades, Bradley never returned to elected office, failing in his\ngubernatorial bid in 1927 and his senatorial campaign in 1936.\n3. Pronoun: his\nChange: \"his\" to \"Bradley\u2019s\".\nContradiction: Though he continued to play an active role in state politics for\nanother two decades, he never returned to elected office, failing in Bradley\u2019s\ngubernatorial bid in 1927 and his senatorial campaign in 1936.\n##\nContext: The early 6th century saw another queen ruling the city, known only as the\n\"Lady of Tikal\", who was very likely a daughter of Chak Tok Ich \u2019aak II.\nCompletion: She seems never to have ruled in her own right, rather being partnered\nwith other rulers.\n1. Pronoun: She\nChange: \"She\" to \"He\" and \"her\" to \"his\".\nContradiction: He seems never to have ruled in his own right, rather being partnered\nwith other rulers.\n2. Pronoun: She\nChange: \"She\" to \"The king\" and \"her\" to \"his\".\nContradiction: The king seems never to have ruled in his own right, rather\nbeing partnered with other rulers.\n3. Pronoun: She\nChange: \"She\" to \"Chak Tok Ich\".\nContradiction: Chak Tok Ich seems never to have ruled in her own right, rather\nbeing partnered with other rulers.\n##\nContext: {context}\nCompletion: {completion}\nTable 9: Prompt for coreference-errors generation\nType\nPrompt\nPredicate\nGiven a context and a completion, write diverse alternative completions, that contradict the original\ncompletion meaning by modifying verbs.\nFirst, Identify a verb in the original completion, and then write the contradiction by modifying it. Make sure\nthe contradictions are grammatically correct, fluent and consistent. Make any necessary additional\nmodifications to ensure that.\n##\nContext: Homarus gammarus is a large crustacean, with a body length up to 60 centimetres (24 in) and\nweighing up to 5 \u2013 6 kilograms (11 \u2013 13 lb), although the lobsters caught in lobster pots are usually\n23 \u2013 38 cm (9 \u2013 15 in) long and weigh 0.7 \u2013 2.2 kg (1.5 \u2013 4.9 lb).\nCompletion: Like other crustaceans, lobsters have a hard exoskeleton which they must shed in order to grow,\nin a process called ecdysis (moulting).\n1. Change: \"shed\" to \"retain\". Additional changes: \"in order to grow\" to \"in order to survive\".\nContradiction: Like other crustaceans, lobsters have a hard exoskeleton which they must retain in order to\nsurvive, in a process called ecdysis (moulting).\n2. Change: \"grow\" to \"maintain their size\".\nContradiction: Like other crustaceans, lobsters have a hard exoskeleton which they must shed in order to\nmaintain their size, in a process called ecdysis (moulting).\n3. Change: \"shed\" to \"keep\". Additional changes: \"in order to grow\" to \"in order to strengthen\".\nContradiction: Like other crustaceans, lobsters have a hard exoskeleton which they must keep in order to\nstrengthen, in a process called ecdysis (moulting).\n##\nContext: The ridge offered a natural avenue of approach to the airfield, commanded the surrounding area\nand was almost undefended. Edson and Thomas tried to persuade Vandegrift to move forces to defend\nthe ridge, but Vandegrift refused, believing that the Japanese were more likely to attack along the coast.\nCompletion: Finally, Thomas convinced Vandegrift that the ridge was a good location for Edson\u2019s Raiders\nto rest from their actions of the preceding month.\n1. Change: \"rest\" to \"keep up\".\nContradiction: Finally, Thomas convinced Vandegrift that the ridge was a good location for Edson\u2019s\nRaiders to keep up with their actions of the preceding month.\n2. Change: \"convinced Vandegrift\" to \"made Vandegrift doubt\".\nContradiction: Finally, Thomas made Vandegrift doubt that the ridge was a good location for Edson\u2019s\nRaiders to rest from their actions of the preceding month. 3. Change: \"rest\" to \"continue\".\nContradiction: Finally, Thomas convinced Vandegrift that the ridge was a good location for Edson\u2019s\nRaiders to continue their actions of the preceding month.\n##\nContext: According to a report titled Wolves in Sheep\u2019s Clothing, which documents the increase in\npotentially violent, profane, and sexual content in children\u2019s programming, the Parents Television Council,\na watchdog media group, and fans believed the SpongeBob SquarePants episode\" Sailor Mouth \"was\nan implicit attempt to promote and satirize use of profanity among children.\nCompletion: The episode originally aired during the 2001 \u2013 02 television season, ironically the season\nin which the PTC named SpongeBob SquarePants among the best programs on cable television,\nbut the report cited a repeat broadcast of the episode from 2005 to prove its point that it promoted use of\nprofanity among children.\n1. Change: \"prove\" to \"refute\". Additional changes: \"best\" to \"most profane\".\nContradiction: The episode originally aired during the 2001 \u2013 02 television season, ironically the season\nin which the PTC named SpongeBob SquarePants among the most profane programs on cable television,\nbut the report cited a repeat broadcast of the episode from 2005 to refute its point that it promoted use of\nprofanity among children.\n2. Change: \"originally aired\" to \"pulled off\".\nContradiction: The episode was pulled off from the 2001 \u2013 02 television season, ironically the season\nin which the PTC named SpongeBob SquarePants among the best programs on cable television,\nbut the report cited a repeat broadcast of the episode from 2005 to prove its point that it promoted use of\nprofanity among children.\n##\nContext: {context}\nCompletion: {completion}\nTable 10: Prompt for predicate-errors generation (the rest of the prompt is in table 11)\nType\nPrompt\nPredicate\nContext: By Part II of the series, Shikamaru is capable of utilizing multiple shadow-based techniques at\nonce and can lift his shadow from the ground in order to interact with physical objects; for instance, he can\npierce enemies with the shadow tendrils or use them to throw weapons. Shikamaru approaches the exams\nwith a sense of apathy; when he battles the Sunagakure ninja Temari, he defeats her\nbut forfeits his match to her, due to his chakra being low.\nCompletion: Despite this loss, he is the only ninja among his peers to be promoted to the rank of Chunin,\nas the overseers of the exams were impressed by the insight and intelligence he demonstrated against Temari.\n1. Change: \"promoted\" to \"demoted\". Additional changes: \"Despite\" to \"Due\", \"as\" to \"although\".\nContradiction: Due to this loss, he is the only ninja among his peers to be demoted to the rank of Chunin,\nalthough the overseers of the exams were impressed by the insight and intelligence he demonstrated against\nTemari.\n2. Change: \"were impressed\" to \"underappreciated\". Additional changes: \"as\" to \"although\".\nContradiction: Despite this loss, he is the only ninja among his peers to be promoted to the rank of Chunin,\nalthough the overseers of the exams underappreciated the insight and intelligence he demonstrated against\nTemari.\n3. Change: \"demonstrated\" to \"failed to demonstrate\". Additional changes: \"as\" to \"although\",\n\"impressed\" to \"disappointed\".\nContradiction: Despite this loss, he is the only ninja among his peers to be promoted to the rank of Chunin,\nalthough the overseers of the exams were disappointed by the insight and intelligence he failed to\ndemonstrate against Temari.\n##\nContext: {context}\nCompletion: {completion}\nTable 11: Prompt for predicate-errors generation (continue of the prompt in table 10)\nType\nPrompt\nLink\nGiven a sentence, write contradictory sentences by modifying a temporal link.\nFirst, identify a link between events, and then modify it. Make sure the contradictions are grammatically\ncorrect and fluent. If no such link exists, answer \"NA\".\n##\nSentence: Prior to filming, a week was spent reinforcing the roof of the liquor store to ensure it would not\ncollapse if it were to be intruded by a group of fans.\n1. Change: \"prior to\" to \"after\".\nContradiction: After filming, a week was spent reinforcing the roof of the liquor store to ensure it would not\ncollapse if it were to be intruded by a group of fans.\n##\nSentence: Lewis McAllister, a businessman in Tuscaloosa, Alabama, was the first Republican to serve in the\nMississippi House of Representatives since Reconstruction, 1962-1968; he resided in Meridian prior to 1971.\n1. Change: \"prior to\" to \"after\".\nContradiction: Lewis McAllister, a businessman in Tuscaloosa, Alabama, was the first Republican to serve\nin the Mississippi House of Representatives since Reconstruction, 1962-1968; he resided in Meridian\nafter 1971.\n2. Change: \"since\" to \"before\"\nContradiction: Lewis McAllister, a businessman in Tuscaloosa, Alabama, was the first Republican to serve\nin the Mississippi House of Representatives before Reconstruction, 1962-1968; he resided in Meridian prior\nto 1971.\n##\nSentence: The decline of the railroad industry caused significant job losses, resulting in a population decline\nas workers left for other areas.\n1. Change: \"caused\" to \"caused by\".\nContradiction: The decline of the railroad industry, caused by significant job losses, resulting a\npopulation decline as workers left for other areas.\n2. Change: \"resulting\" to \"was the result of\".\nContradiction: The decline of the railroad industry caused significant job losses, was the result of a population\ndecline, as workers left for other areas.\n##\nSentence: {completion}\nTable 12: Prompt for link-errors generation\n"
  },
  {
    "title": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation",
    "link": "https://arxiv.org/pdf/2307.06350.pdf",
    "upvote": "5",
    "text": "T2I-CompBench: A Comprehensive Benchmark for\nOpen-world Compositional Text-to-image Generation\nKaiyi Huang1\nKaiyue Sun1\nEnze Xie2\nZhenguo Li2\nXihui Liu1\u2217\n1 The University of Hong Kong\n2 Huawei Noah\u2019s Ark Lab\n{huangky, kaiyue}@connect.hku.hk\n{xie.enze, li.zhenguo}@huawei.com\nxihuiliu@eee.hku.hk\nA red book and a yellow vase\nAn oval coffee table and a rectangular rug\nA metallic spoon and a glass vase\nThe sharp blue scissors cut through \nthe thick white paper\nA woman is holding a yoga mat\nA book on the left of a bird\nAttribute-color\nAttribute-shape\nAttribute-texture\nObject relationships\nSpatial relationship\nNon-spatial relationship\nComplex compositions \nAttribute binding\nFigure 1: Failure cases of Stable Diffusion v2 [1]. Our compositional text-to-image generation\nbenchmark consists of three categories: attribute binding (including color, shape, and texture), object\nrelationship (including spatial relationship and non-spatial relationship), and complex compositions.\nAbstract\nDespite the stunning ability to generate high-quality images by recent text-to-\nimage models, current approaches often struggle to effectively compose objects\nwith different attributes and relationships into a complex and coherent scene. We\npropose T2I-CompBench, a comprehensive benchmark for open-world composi-\ntional text-to-image generation, consisting of 6,000 compositional text prompts\nfrom 3 categories (attribute binding, object relationships, and complex composi-\ntions) and 6 sub-categories (color binding, shape binding, texture binding, spatial\nrelationships, non-spatial relationships, and complex compositions). We further\npropose several evaluation metrics specifically designed to evaluate compositional\ntext-to-image generation and explore the potential and limitations of multimodal\nLLMs for evaluation. We introduce a new approach, Generative mOdel finetun-\ning with Reward-driven Sample selection (GORS), to boost the compositional\ntext-to-image generation abilities of pretrained text-to-image models. Extensive\nexperiments and evaluations are conducted to benchmark previous methods on T2I-\nCompBench, and to validate the effectiveness of our proposed evaluation metrics\nand GORS approach. Project page is available at https://karine-h.github.io/T2I-\nCompBench/.\n\u2217Corresponding Author\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2307.06350v2  [cs.CV]  30 Oct 2023\n1\nIntroduction\nRecent progress in text-to-image generation [2, 1, 3, 4, 5, 6] has showcased remarkable capabilities in\ncreating diverse and high-fidelity images based on natural language prompts. However, we observe\nthat even state-of-the-art text-to-image models often fail to compose multiple objects with different\nattributes and relationships into a complex and coherent scene, as shown in the failure cases of Stable\nDiffusion [1] in Figure 1. For example, given the text prompt \u201ca blue bench on the left of a green\ncar\u201d, the model might bind attributes to the wrong objects or generate the spatial layout incorrectly.\nPrevious works have explored compositional text-to-image generation from different perspectives,\nsuch as concept conjunction [7], attribute binding (focusing on color) [8, 9], and spatial relation-\nship [10]. Most of those works focus on a sub-problem and propose their own benchmarks for evaluat-\ning their methods. However, there is no consensus on the problem definition and standard benchmark\nof compositional text-to-image generation. To this end, we propose a comprehensive benchmark\nfor open-world compositional text-to-image generation, T2I-CompBench. T2I-CompBench consists\nof three categories and six sub-categories of compositional text prompts: (1) Attribute binding.\nEach text prompt in this category contains at least two objects and two attributes, and the model\nshould bind the attributes with the correct objects to generate the complex scene. This category is\ndivided into three sub-categories (color, shape, and texture) based on the attribute type. (2) Object\nrelationships. The text prompts in this category each contain at least two objects with specified\nrelationships between the objects. Based on the type of the relationships, this category consists of\ntwo sub-categories, spatial relationship and non-spatial relationship. (3) Complex compositions,\nwhere the text prompts contain more than two objects or more than two sub-categories mentioned\nabove. For example, a text prompt that describes three objects with their attributes and relationships.\nAnother challenge is the assessment of compositional text-to-image models. Most previous works\nevaluate the models by image-text similarity or text-text similarity (between the caption predicted\nfrom the generated images and the original text prompts) with CLIPScore [11, 12] or BLIP [13, 14].\nHowever, both metrics do not perform well for compositionality evaluation due to the ambiguity\nof image captioning and the difficulty of compositional vision-language understanding. To address\nthis challenge, we propose several evaluation metrics for different categories of compositional\nprompts. We propose disentangled BLIP-VQA for attribute binding evaluation to overcome the\nambiguous attribute correspondences, UniDet-based metric for spatial relationship evaluation, and\n3-in-1 metric for complex prompts. We further investigate the potential and limitations of multimodal\nlarge language models such as MiniGPT-4 [15] with Chain-of-Thought [16] for compositionality\nevaluation.\nFinally, we propose a new approach, Generative mOdel finetuning with Reward-driven Sample\nselection (GORS), for compositional text-to-image generation. We finetune the state-of-the-art Stable\nDiffusion v2 [1] model with generated images that highly align with the compositional prompts,\nwhere the fine-tuning loss is weighted by the reward which is defined as the alignment score between\ncompositional prompts and generated images. This approach is simple but effective in boosting the\nmodel\u2019s compositional abilities and can serve as a new baseline for future explorations.\nIn summary, our contributions are three-folded. (1) We propose a comprehensive benchmark for\ncompositional text-to-image generation which consists of 6,000 prompts from 3 categories (attribute\nbinding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape\nbinding, texture binding, spatial relationships, non-spatial relationships, and complex compositions).\n(2) We propose evaluation metrics that are specifically designed for compositional text-to-image\nevaluation. Experiments validate that the proposed evaluation metrics are highly correlated with\nhuman perception. (3) We benchmark several previous text-to-image models on our proposed\nbenchmark and evaluation metrics, and propose a simple and effective approach, GORS, for boosting\nthe compositionality of text-to-image models.\n2\nRelated work\nText-to-image generation. Early works [18, 19, 20, 21, 22, 23] explore different network archi-\ntectures and loss functions based on generative adversarial networks (GAN) [24]. DALL\u00b7E [25]\nachieves impressive results with a transformer and discrete variational autoencoder (VAE) trained\non web-scale data. Recently, diffusion models have achieved remarkable success for text-to-image\n2\nTable 1: Comparison of compositional text-to-image benchmarks.\nBenchmark\nPrompts number and coverage\nVocabulary diversity\nCC-500 [8]\n500 attr bind (color)\n196 nouns, 12 colors\nABC-6K [8]\n6,000 attr bind (color)\n3,690 nouns, 33 colors\nAttn-Exct [9]\n210 attr bind (color)\n24 nouns, 11 colors\nHRS-comp [17]\n1,000 attr bind (color, size), 2,000 rel (spatial, action)\n620 nouns, 5 colors, 11 spatial, 569 actions\nT2I-CompBench\n3,000 attr bind (color, shape, texture)\n2,000 rel (spatial, non-spatial), 1,000 complex\n2,316 nouns, 33 colors, 32 shapes, 23 textures\n7 spatial rel, 875 non-spatial rel\ngeneration [26, 27, 1, 28, 29]. Current state-of-the-art models such as Stable Diffusion [1] still\nstruggle to compose multiple objects with attributes and relationships in a complex scene. Some\nrecent works attempt to align text-to-image models with human feedback [30, 31]. Concurrent\nwork RAFT [32] proposes reward-ranked fine-tuning to align text-to-image models with certain\nmetrics. Our proposed GORS approach is a simpler finetuning approach that does not require multiple\niterations of sample generation and selection.\nCompositional text-to-image generation. Researchers have delved into various aspects of com-\npositionality in text-to-image generation to achieve visually coherent and semantically consistent\nresults [7, 8, 33, 10]. Previous work focused on concept conjunction and negation [7], attribute\nbinding with colors [8, 9, 34], and spatial relationships between objects [35, 10]. However, those\nwork each target at a sub-problem, and evaluations are conducted in constrained scenarios. Our work\nis the first to introduce a comprehensive benchmark for compositional text-to-image generation.\nBenchmarks for text-to-image generation. Early works evaluate text-to-image on CUB birds [36],\nOxford flowers [37], and COCO [38] which are easy with limited diversity. As the text-to-image\nmodels become stronger, more challenging benchmarks have been introduced. DrawBench [3]\nconsists of 200 prompts to evaluate counting, compositions, conflicting, and writing skills. DALL-\nEVAL [39] proposes PaintSkills to evaluate visual reasoning skills, image-text alignment, image\nquality, and social bias by 7,330 prompts. HE-T2I [40] proposes 900 prompts to evaluate counting,\nshapes, and faces for text-to-image. Several compositional text-to-image benchmarks have also been\nproposed. Park et al. [34] proposes a benchmark on CUB Birds [36] and Oxford Flowers [37] to\nevaluate the models\u2019 ability to generate images with object-color and object-shape compositions.\nABC-6K and CC500 [8] benchmarks are proposed to evaluate attribute binding for text-to-image\nmodels, but they only focused on color attributes. Concurrent work HRS-Bench [17] is a general-\npurpose benchmark that evaluates 13 skills with 45,000 prompts. Compositionality is only one of the\n13 evaluated skills which is not extensively studied. We propose the first comprehensive benchmark\nfor open-world compositional text-to-image generation, shown in Table 1.\nEvaluation metrics for text-to-image generation. Existing metrics for text-to-image generation can\nbe categorized into fidelity assessment, alignment assessment, and LLM-based metrics. Traditional\nmetrics such as Inception Score (IS) [41] and Frechet Inception Distance (FID) [42] are commonly\nused to evaluate the fidelity of synthesized images. To assess the image-text alignment, text-image\nmatching by CLIP [11] and BLIP2 [14] and text-text similarity by BLIP [13] captioning and CLIP\ntext similarity are commonly used. Some concurrent works leverage the strong reasoning abilities of\nlarge language models (LLMs) for evaluation [43, 44]. However, there was no comprehensive study\non how well those evaluation metrics work for compositional text-to-image generation. We propose\nevaluation metrics specifically designed for our benchmark and validate that our proposed metrics\nalign better with human perceptions.\n3\nT2I-CompBench\nCompositionality of text-to-image models refers to the ability of models to compose different concepts\ninto a complex and coherent scene according to text prompts. It includes composing attributes with\nobjects, composing different objects with specified interactions and spatial relationships, and complex\ncompositions. To provide a clear definition of the problem and to build our benchmark, we introduce\nthree categories and six sub-categories of compositionality, attribute binding (including three sub-\ncategories: color, shape, and texture), object relationships (including two sub-categories: spatial\nrelationship and non-spatial relationship), and complex compositions. We generate 1,000 text prompts\n(700 for training and 300 for testing) for each sub-category, resulting in 6,000 compositional text\n3\nprompts in total. We take the balance between seen v.s. unseen compositions in the test set, prompts\nwith fixed sentence template v.s. natural prompts, and simple v.s. complex prompts into consideration\nwhen constructing the benchmark. The text prompts are generated with either predefined rules\nor ChatGPT [45], so it is easy to scale up. Comparisons between our benchmark and previous\nbenchmarks are shown in Table. 1.\n3.1\nAttribute Binding\nA critical challenge for compositional text-to-image generation is attribute binding, where attributes\nmust be associated with corresponding objects in the generated images. We find that models tend\nto confuse the association between attributes and objects when there are more than one attribute\nand more than one object in the text prompt. For example, with the text prompt \u201cA room with blue\ncurtains and a yellow chair\u201d, the text-to-image model might generate a room with yellow curtains and\na blue chair. We introduce three sub-categories, color, shape, and texture, according to the attribute\ntype, and construct 1000 text prompts for each sub-category. For each sub-category, there are 800\nprompts with the fixed sentence template \u201ca {adj} {noun} and a {adj} {noun}\u201d (e.g., \u201ca red flower\nand a yellow vase\u201d) and 200 natural prompts without predefined sentence template (e.g., \u201ca room\nwith blue curtains and a yellow chair\u201d). The 300-prompt test set of each sub-category consists of 200\nprompts with seen adj-noun compositions (adj-noun compositions appeared in the training set) and\n100 prompts with unseen adj-noun compositions (adj-noun compositions not in the training set).\nColor. Color is the most commonly-used attribute for describing objects in images, and current\ntext-to-image models often confuse the colors of different objects. The 1,000 text prompts related to\ncolor binding are constructed with 480 prompts from CC500 [8], 200 prompts from COCO [38], and\n320 prompts generated by ChatGPT.\nShape. We define a set of shapes that are commonly used for describing objects in images: long, tall,\nshort, big, small, cubic, cylindrical, pyramidal, round, circular, oval, oblong, spherical, triangular,\nsquare, rectangular, conical, pentagonal, teardrop, crescent, and diamond. We provide those shape\nattributes to ChatGPT and ask ChatGPT to generate prompts by composing those attributes with\narbitrary objects, for example, \u201ca rectangular clock and a long bench\u201d.\nTexture. Textures are also commonly used to describe the appearance of objects. They can capture the\nvisual properties of objects, such as smoothness, roughness, and granularity. We often use materials\nto describe the texture, such as wooden, plastic, and rubber. We define several texture attributes\nand the objects that can be described by each attribute. We generate 800 text prompts by randomly\nselecting from the possible combinations of two objects each associated with a textural attribute, e.g.,\n\u201cA rubber ball and a plastic bottle\u201d. We also generate 200 natural text prompts by ChatGPT.\n3.2\nObject Relationship\nWhen composing objects in a complex scene, the relationship between objects is a critical factor. We\nintroduce 1,000 text prompts for spatial relationships and non-spatial relationships, respectively.\nSpatial relationships. We use \u201con the side of\u201d, \u201cnext to\u201d, \u201cnear\u201d, \u201con the left of\u201d, \u201con the right of\u201d,\n\u201con the bottom of\u201d, and \u201con the top of\u201d to define spatial relationships. The two nouns are randomly\nselected from persons (e.g., man, woman, girl, boy, person, etc.), animals (e.g., cat, dog, horse, rabbit,\nfrog, turtle, giraffe, etc.), and objects (e.g., table, chair, car, bowl, bag, cup, computer, etc.). For spatial\nrelationships including left, right, bottom, and top, we construct contrastive prompts by swapping the\ntwo nouns, for example, \u201ca girl on the left of a horse\u201d and \u201ca horse on the left of a girl\u201d.\nNon-spatial relationship. Non-spatial relationships usually describe the interactions between two\nobjects. We prompt ChatGPT to generate text prompts with non-spatial relationships (e.g., \u201cwatch\u201d,\n\u201cspeak to\u201d, \u201cwear\u201d, \u201chold\u201d, \u201chave\u201d, \u201clook at\u201d, \u201ctalk to\u201d, \u201cplay with\u201d, \u201cwalk with\u201d, \u201cstand on\u201d, \u201csit\non\u201d, etc.) and arbitrary nouns.\n4\nA blue \nbackpack \nand a red \nbench\nA suitcase \non the right \nof a cow\nThe fluffy cat \nwas lying on\nthe soft \nblanket\nBLIP-VQA\nA blue backpack? \nA red bench?\nUniDet\nThe relative \npositions of \n\u201cright\u201d\nx1>x2?\n|x1-x2|>|y1-y2|?\nIoU<threshold?\nProbability of \n\u201cyes\u201d\nA suitcase\n(x1,y1)\nA cow\n(x2,y2)\nQuestion \ngeneration\nminiGPT4-CoT\nQ: Describe the image\nmGPT4: The image \nshows ...\nQ: Evaluate the alignment \nwith the text\nmGPT4: score:..., \nexplanation:...\nScore\n(a) Disentangled BLIP-VQA for attribute binding evaluation.\n(c) MiniGPT4-CoT as a potential unified metric.\n(b) UniDet-based metric for spatial relationship evaluation.\nFigure 2: Illustration of our proposed evaluation metrics: (a) Disentangled BLIP-VQA for attribute\nbinding evaluation, (b) UniDet for spatial relationship evaluation, and (c) MiniGPT4-CoT as a\npotential unified metric.\n3.3\nComplex Compositions\nTo test text-to-image generation approaches with more natural and challenging compositional prompts\nin the open world, we introduce 1,000 text prompts with complex compositions of concepts beyond\nthe pre-defined patterns. Regarding the number of objects, we create text prompts with more than\ntwo objects, for example, \u201ca room with a blue chair, a black table, and yellow curtains\u201d. In terms of\nthe attributes associated with objects, we can use multiple attributes to describe an object (denoted as\nmultiple attributes, e.g., \u201ca big, green apple and a tall, wooden table\u201d), or leverage different types\nof attributes in a text prompt (denoted as mixed attributes, e.g., the prompt \u201ca tall tree and a red car\u201d\nincludes both shape and color attributes). We generate 250 text prompts with ChatGPT for each of\nthe four scenarios: two objects with multiple attributes, two objects with mixed attributes, more than\ntwo objects with multiple attributes, and more than two objects with mixed attributes. Relationship\nwords can be adopted in each scenario to describe the relationships among two or more objects. For\neach scenario, we split 175 prompts for the training set and 75 prompts for the test set.\n4\nEvaluation Metrics\nEvaluating compositional text-to-image generation is challenging as it requires comprehensive and\nfine-grained cross-modal understanding. Existing evaluation metrics leverage vision-language models\ntrained on large-scale data for evaluation. CLIPScore [11, 12] calculates the cosine similarity between\ntext features and generated-image features extracted by CLIP. Text-text similarity by BLIP-CLIP [9]\napplies BLIP [13] to generate captions for the generated images, and then calculates the CLIP text-text\ncosine similarity between the generated captions and text prompts. Those evaluation metrics can\nmeasure the coarse text-image similarity, but fails to capture fine-grained text-image correspondences\nin attribute binding and spatial relationships. To address those limitations, we propose new evaluation\nmetrics for compositional text-to-image generation, shown in Fig. 2. Concretely, we propose\ndisentangled BLIP-VQA for attribute binding evaluation, UniDet-based metric for spatial relationship\nevaluation, and 3-in-1 metric for complex prompts. We further investigate the potential and limitations\nof multimodal large language models such as MiniGPT-4 [15] with Chain-of-Thought [16] for\ncompositionality evaluation.\n4.1\nDisentangled BLIP-VQA for Attribute Binding Evaluation\nWe observe that the major limitation of the BLIP-CLIP evaluation is that the BLIP captioning models\ndo not always describe the detailed attributes of each object. For example, the BLIP captioning model\nmight describe an image as \u201cA room with a table, a chair, and curtains\u201d, while the text prompt for\ngenerating this image is \u201cA room with yellow curtains and a blue chair\u201d. So explicitly comparing the\ntext-text similarity might cause ambiguity and confusion.\nTherefore, we leverage the visual question answering (VQA) ability of BLIP [13] for evaluating\nattribute binding. For instance, given the image generated with the text prompt \u201ca green bench\n5\nand a red car\u201d, we ask two questions separately: \u201ca green bench?\u201d, and \u201ca red car?\u201d. By explicitly\ndisentangling the complex text prompt into two independent questions where each question contains\nonly one object-attribute pair, we avoid confusion of BLIP-VQA. The BLIP-VQA model takes the\ngenerated image and several questions as input and we take the probability of answering \u201cyes\u201d as the\nscore for a question. We compute the overall score by multiplying the probability of answering \u201cyes\u201d\nfor each question. The proposed disentangled BLIP-VQA is applied to evaluate the attribute binding\nfor color, shape, and texture.\n4.2\nUniDet-based Spatial Relationship Evaluation\nMost vision-language models perform poorly in reasoning spatial relationships such as \u201cleft\u201d and\n\u201cright\u201d. Therefore, we introduce a detection-based spatial relationship evaluation metric. We first use\nUniDet [46] to detect objects in the generated image. Then we determine the spatial relationship\nbetween two objects by comparing the locations of the centers of the two bounding boxes. Denote\nthe center of the two objects as (x1, y1) and (x2, y2), respectively. The first object is on the left of\nthe second object if x1 < x2, |x1 \u2212 x2| > |y1 \u2212 y2|, and the intersection-over-union (IoU) between\nthe two bounding boxes is below the threshold of 0.1. Other spatial relationships \u201cright\u201d, \u201ctop\u201d, and\n\u201cbottom\u201d are evaluated similarly. We evaluate \u201cnext to\u201d, \u201cnear\u201d, and \u201con the side of\u201d by comparing\nthe distances between the centers of two objects with a threshold.\n4.3\n3-in-1 Metric for Complex Compositions Evaluation\nSince different evaluation metrics are designed for evaluating different types of compositionality,\nthere is no single metric that works well for all categories. We empirically find that the Disentangled\nBLIP-VQA works best for attribute binding evaluation, UniDet-baased metric works best for spatial\nrelationship evaluation, and CLIPScore works best for non-spatial relationship evaluation. Thus, we\ndesign a 3-in-1 evaluation metric which computes the average score of CLIPScore, Disentangled\nBLIP-VQA, and UniDet, as the evaluation metric for complex compositions.\n4.4\nEvaluation with Multimodal Large Language Models\nBy aligning a pretrained visual encoder with a frozen large language model, multimodal large\nlanguage models such as MiniGPT-4 [15] have demonstrated great abilities in vision-language cross-\nmodal understanding. We leverage MiniGPT-4 with Chain-of-Thought as an evaluation metric by\nfeeding the generated images to the model and asking two questions: \u201cdescribe the image\u201d and\n\u201cpredict the image-text alignment score\u201d. More information on the prompt design is provided in the\nappendix. We believe more advanced multimodal LLMs have the potential to be a unified evaluation\nmetric in the future, but the current models exhibit limitations such as inaccurate understanding of\nimages and hallucination issues.\n5\nMethod\nWe introduce a simple but effective approach, Generative mOdel finetuning with Reward-driven\nSample selection (GORS), to improve the compositional ability of pretrained text-to-image models.\nOur approach finetunes a pretrained text-to-image model such as Stable Diffusion [1] with generated\nimages that highly align with the compositional prompts, where the fine-tuning loss is weighted by\nthe reward which is defined as the alignment score between compositional prompts and generated\nimages.\nSpecifically, given the text-to-image model p\u03b8 and a set of text prompts y1, y2, \u00b7 \u00b7 \u00b7 , yn, we first\ngenerate k images for each text prompt, resulting in kn generated images x1, x2, \u00b7 \u00b7 \u00b7 , xkn. Text-\nimage alignment scores s1, s2, \u00b7 \u00b7 \u00b7 , skn are predicted as rewards. We select the generated images\nwhose rewards are higher than a threshold to fine-tune the text-to-image model. The selected set of\nsamples are denoted as Ds. During fine-tuning, we weight the loss with the reward of each sample.\nGenerated images that align with the compositional prompt better are assigned higher loss weights,\nand vice versa. The loss function for fine-tuning is\nL(\u03b8) = E(x,y,s)\u2208Ds\nh\ns \u00b7 \u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, y)\u22252\n2\ni\n,\n(1)\n6\nTable 2: Benchmarking on attribute binding (color and shape), with scores unnormalized. Blue\nrepresents the proposed metric for the category, and green indicates the human evaluation, applicable\nto the following Table 3 and Table 4.\nModel\nColor\nShape\nCLIP\nB-CLIP\nB-VQA-n\nB-VQA\nmGPT-CoT\nHuman\nCLIP\nB-CLIP\nB-VQA-n\nB-VQA\nmGPT-CoT\nHuman\nStable v1-4 [1]\n0.3214\n0.7454\n0.5875\n0.3765\n0.7424\n0.6533\n0.3112\n0.7077\n0.6771\n0.3576\n0.7197\n0.6160\nStable v2 [1]\n0.3335\n0.7616\n0.7249\n0.5065\n0.7764\n0.7747\n0.3203\n0.7191\n0.7517\n0.4221\n0.7279\n0.6587\nComposable v2 [7]\n0.3178\n0.7352\n0.5744\n0.4063\n0.7524\n0.6187\n0.3092\n0.6985\n0.6125\n0.3299\n0.7124\n0.5133\nStructured v2 [8]\n0.3319\n0.7626\n0.7184\n0.4990\n0.7822\n0.7867\n0.3178\n0.7177\n0.7500\n0.4218\n0.7228\n0.6413\nAttn-Exct v2 [9]\n0.3374\n0.7810\n0.8362\n0.6400\n0.8194\n0.8240\n0.3189\n0.7209\n0.7723\n0.4517\n0.7299\n0.6360\nGORS-unbaised (ours)\n0.3390\n0.7667\n0.8219\n0.6414\n0.7987\n0.8253\n0.3175\n0.7149\n0.7630\n0.4546\n0.7263\n0.6573\nGORS (ours)\n0.3395\n0.7681\n0.8471\n0.6603\n0.8067\n0.8320\n0.2973\n0.7201\n0.7937\n0.4785\n0.7303\n0.7040\nwhere (x, y, s) is the triplet of the image, text prompt, and reward, and zt represents the latent features\nof x at timestep t. We adopt LoRA [47] for efficient finetuning.\n6\nExperiments\n6.1\nExperimental Setup\nEvaluated models. We evaluate the performance of 6 text-to-image models on T2I-CompBench.\nStable Diffusion v1-4 and Stable Diffusion v2 [1] are text-to-image models trained on large amount of\nimage-text pairs. Composable Diffusion [7] is designed for conjunction and negation of concepts\nfor pretrained diffusion models. Structured Diffusion [8] and Attend-and-Excite [9] are designed\nfor attribute binding for pretrained diffusion models. We re-implement those approaches on Stable\nDiffusion v2 to enable fair comparisons. GORS is our proposed approach which finetunes Stable\nDiffusion v2 with selected samples and their rewards. Since calculating the rewards for GORS with\nthe automatic evaluation metrics can lead to biased results, we also provide alternative reward models\n(Appendix D.3) which are different from the evaluation metrics, which is denoted as GORS-unbiased.\nImplemenrtation details. Please find the implementation details in the appendix.\n6.2\nEvaluation Metrics\nWe generate 10 images for each text prompt in T2I-CompBench for automatic evaluation.\nPrevious metrics. CLIPScore [11, 12] (denoted as CLIP) calculates the cosine similarity between\ntext features and generated-image features extracted by CLIP. BLIP-CLIP [9] (denoted as B-CLIP)\napplies BLIP [13] to generate captions for the generated images, and then calculates the CLIP text-text\ncosine similarity between the generated captions and text prompts. BLIP-VQA-naive (denoted as\nB-VQA-n) applies BLIP VQA to ask a single question (e.g., a green bench and a red car?) with the\nwhole prompt.\nOur proposed metrics. Disentangled BLIP-VQA (denoted as B-VQA) is our proposed evaluation\nmetric for attribute binding. UniDet is our proposed UniDet-based spatial relationship evaluation\nmetric. 3-in-1 computes the average score of CLIPScore, Disentangled BLIP-VQA, and UniDet, as\nthe evaluation metric for complex compositions. MiniGPT4-Chain-of-Thought (denoted as mGPT-\nCoT) serves as a potential unified metric for all types of compositional prompts based on multimodal\nLLM.\nHuman evaluation. For human evaluation of each sub-category, we randomly select 25 prompts\nand generate 2 images per prompt, resulting in 300 images generated with 150 prompts per model\nin total. The testing set includes 300 prompts for each sub-category, resulting in 1800 prompts in\ntotal. The prompt sampling rate for human evaluation is 8.33%. We utilize Amazon Mechanical Turk\nand ask three workers to score each generated-image-text pair independently based on the image-text\nalignment.The worker can choose a score from {1, 2, 3, 4, 5} and we normalize the scores by dividing\nthem by 5. We then compute the average score across all images and all workers.\n7\nTable 3: Benchmarking on attribute binding (texture) and spatial relationship.\nModel\nTexture\nSpatial\nCLIP\nB-CLIP\nB-VQA-n\nB-VQA\nmGPT-CoT\nHuman\nCLIP\nB-CLIP\nUniDet\nmGPT-CoT\nHuman\nStable v1-4 [1]\n0.3081\n0.7111\n0.6173\n0.4156\n0.7836\n0.7227\n0.3142\n0.7667\n0.1246\n0.8338\n0.3813\nStable v2 [1]\n0.3185\n0.7240\n0.7054\n0.4922\n0.7851\n0.7827\n0.3206\n0.7723\n0.1342\n0.8367\n0.3467\nComposable v2 [7]\n0.3092\n0.6995\n0.5604\n0.3645\n0.7588\n0.6333\n0.3001\n0.7409\n0.0800\n0.8222\n0.3080\nStructured v2 [8]\n0.3167\n0.7234\n0.7007\n0.4900\n0.7806\n0.7760\n0.3201\n0.7726\n0.1386\n0.8361\n0.3467\nAttn-Exct v2 [9]\n0.3171\n0.7206\n0.7830\n0.5963\n0.8062\n0.8400\n0.3213\n0.7742\n0.1455\n0.8407\n0.4027\nGORS-unbiased (ours)\n0.3216\n0.7291\n0.7778\n0.6025\n0.7985\n0.8413\n0.3237\n0.7882\n0.1725\n0.8241\n0.4467\nGORS (ours)\n0.3233\n0.7315\n0.7991\n0.6287\n0.8106\n0.8573\n0.3242\n0.7854\n0.1815\n0.8362\n0.4560\nTable 4: Benchmarking on the non-spatial relationship and complex compositions.\nModel\nNon-spatial\nComplex\nCLIP\nB-CLIP\nmGPT-CoT\nHuman\nCLIP\nB-CLIP\n3-in-1\nmGPT-CoT\nHuman\nStable v1-4 [1]\n0.3079\n0.7565\n0.8170\n0.9653\n0.2876\n0.6816\n0.3080\n0.8075\n0.8067\nStable v2 [1]\n0.3127\n0.7609\n0.8235\n0.9827\n0.3096\n0.6893\n0.3386\n0.8094\n0.8480\nComposable v2 [7]\n0.2980\n0.7038\n0.7936\n0.8120\n0.3014\n0.6638\n0.2898\n0.8083\n0.7520\nStructured v2 [8]\n0.3111\n0.7614\n0.8221\n0.9773\n0.3084\n0.6902\n0.3355\n0.8076\n0.8333\nAttn-Exct v2 [9]\n0.3109\n0.7607\n0.8214\n0.9533\n0.2913\n0.6875\n0.3401\n0.8078\n0.8573\nGORS-unbaised (ours)\n0.3158\n0.7641\n0.8353\n0.9534\n0.3137\n0.6888\n0.3470\n0.8122\n0.8654\nGORS (ours)\n0.3193\n0.7619\n0.8172\n0.9853\n0.2973\n0.6841\n0.3328\n0.8095\n0.8680\n6.3\nQuantitative and Qualitative Evaluation\nThe quantitative evaluation results are reported on attribute binding (color), attribute binding (shape)\n(Table 2), attribute binding (texture), spatial relationship (Table 3), non-spatial relationship, and\ncomprehensive compositions (Table 4), respectively. Qualitative results are shown Figure 3.\nComparisons across evaluation metrics. Previous evaluation metrics, CLIP and BLIP-CLIP, predict\nsimilar scores for different models and cannot reflect the differences between models. Our proposed\nmetrics, BLIP-VQA for attribute binding, UniDet for spatial relationship, CLIP for non-spatial\nrelationship, and 3-in-1 for complex compositions, highly align with the human evaluation scores.\nComparisons across text-to-image models. (1) Stable Diffusion v2 consistently outperforms Stable\nDiffusion v1-4 in all types of compositional prompts and evaluation metrics. (2) Although Structured\nDiffusion built upon Stable Diffusion v1-4 shows great performance improvement in attribute binding\nas reported in Feng et al. [8], Structured Diffusion built upon Stable Diffusion v2 only brings slight\nperformance gain upon Stable Diffusion v2. It indicates that boosting the performance upon a better\nbaseline of Stable Diffusion v2 is more challenging. (3) Composable Diffusion built upon Stable\nDiffusion v2 does not work well. A similar phenomenon was also observed in previous work [9]\nthat Composable Diffusion often generates images containing a mixture of the subjects. In addition,\nComposable Diffusion was designed for concept conjunctions and negations so it is reasonable that\nit does not perform well in other compositional scenarios. (4) Attend-and-Excite built upon Stable\nDiffusion v2 improves the performance in attribute binding. (5) Previous methods Composable\nDiffusion [7], Structure Diffusion [8] and Attend-and-Excite [9] are designed for concept conjunction\nor attribute binding, so they do not result in significant improvements in object relationships. (6)\nOur proposed approach, GORS, outperforms previous approaches across all types of compositional\nprompts, as demonstrated by the automatic evaluation, human evaluation, and qualitative results. The\nevaluation results of GORS-unbiased and GORS significantly exceed the baseline Stable v2. Besides,\nGORS-unbiased achieves on-par performance with GORS, indicating that our proposed approach is\ninsensitive to the reward model used for selecting samples, and that the proposed approach works\nwell as long as high-quality samples are selected.\nComparisons across compositionality categories. According to the human evaluation results,\nspatial relationship is the most challenging sub-category for text-to-image models, and attribute\nbinding (shape) is also challenging. Non-spatial relationship is the easiest sub-category.\n8\nA green banana and \na brown horse\nA brown boat \nand a blue cat\nA bathroom with \ngreen tile and a red \nshower curtain\nStable\nv1-4\nStable \nv2\nComposable\nv2\nStructure\nv2\nAttn-Exct\nv2\nGORS\n(ours)\nA round bagel and a \nrectangular \nsandwich press\nA black white and \ngold vase sitting on \na counter\nA leather shoes and \na glass cup\nFigure 3: Qualitative comparison between our approach and previous methods.\n6.4\nHuman Correlation of the Evaluation Metrics\nWe calculate Kendall\u2019s tau (\u03c4) and Spearman\u2019s rho (\u03c1) to evaluate the ranking correlation between\nautomatic evaluation and human evaluation. The scores predicted by each evaluation metric are\nnormalized to 0-1 for better comparison.\nThe human correlation results are illustrated in Table 5. The results verify the effectiveness of\nour proposed evaluation metrics, BLIP-VQA for attribute binding, UniDet-based metric for spa-\ntial relationships, CLIPScore for non-spatial relationships, and 3-in-1 for complex compositions.\nMiniGPT4-CoT does not perform well in terms of correlation with human perception, but we believe\nthat the multimodal LLM-based evaluation metrics have the potential to become a unified evaluation\nmetric in the future. We leave explorations on boosting the performance of multimodal LLM for\ncompositional prompts to future work.\n6.5\nAblation study\nWe conduct ablation studies with the attribute binding (color) sub-category on our proposed GORS ap-\nproach and MiniGPT4-CoT evaluation metric.\n9\nTable 5: The correlation between automatic evaluation metrics and human evaluation. Our proposed\nmetrics demonstrate a significant improvement over existing metrics in terms of Kendall\u2019s \u03c4 and\nSpearmanr\u2019s \u03c1.\nMetric\nAttribute-color\nAttribute-shape\nAttribute-texture\nSpatial rel\nNon-spatial rel\nComplex\n\u03c4(\u2191)\n\u03c1(\u2191)\n\u03c4(\u2191)\n\u03c1(\u2191)\n\u03c4(\u2191)\n\u03c1(\u2191)\n\u03c4(\u2191)\n\u03c1(\u2191)\n\u03c4(\u2191)\n\u03c1(\u2191)\n\u03c4(\u2191)\n\u03c1(\u2191)\nCLIP\n0.1938\n0.2773\n0.0555\n0.0821\n0.2890\n0.4008\n0.2741\n0.3548\n0.2470\n0.3161\n0.0650\n0.0847\nB-CLIP\n0.2674\n0.3788\n0.1692\n0.2413\n0.2999\n0.4187\n0.1983\n0.2544\n0.2342\n0.2964\n0.1963\n0.2755\nB-VQA-n\n0.4602\n0.6179\n0.2280\n0.3180\n0.4227\n0.5830\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nB-VQA\n0.6297\n0.7958\n0.2707\n0.3795\n0.5177\n0.6995\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nUniDet\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n0.4756\n0.5136\n\u2212\n\u2212\n\u2212\n\u2212\n3-in-1\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n0.2831\n0.3853\nmGPT\n0.1197\n0.1616\n0.1282\n0.1775\n0.1061\n0.1460\n0.0208\n0.0229\n0.1181\n0.1418\n0.0066\n0.0084\nmGPT-CoT\n0.3156\n0.4151\n0.1300\n0.1805\n0.3453\n0.4664\n0.1096\n0.1239\n0.1944\n0.2137\n0.1251\n0.1463\nmGPT-CLIP\n0.2301\n0.3174\n0.0695\n0.0963\n0.2004\n0.2784\n0.1478\n0.1950\n0.1507\n0.1942\n0.1457\n0.2014\nTable 6: Ablation studies on fine-tuning strategy and threshold.\nMetric\nFT U-Net only\nFT CLIP only\nHalf threshold\n0 threshold\nGORS (ours)\nB-VQA\n0.6216\n0.5507\n0.6157\n0.6130\n0.6570\nmGPT-CoT\n0.7840\n0.7663\n0.7886\n0.7879\n0.7899\nFinetuning strategy. Our approach finetunes both the CLIP text encoder and the U-Net of Stable\nDiffusion with LoRA [47]. We investigate the effects of finetuning CLIP only and U-Net only with\nLoRA. As shown in Table 6, our model which finetunes both CLIP and U-Net performs better.\nThreshold of selecting samples for finetuning. Our approach fine-tunes Stable Diffusion v2 with\nthe selected samples that align well with the compositional prompts. We manually set a threshold for\nthe alignment score to select samples with higher alignment scores than the threshold for fine-tuning.\nWe experiment with setting the threshold to half of its original value, and setting the threshold to\n0 (i.e., use all generated images for finetuning with rewards, without selection). Results in Table 6\ndemonstrate that half threshold and zero threshold will lead to worse performance.\nMiniGPT4 without Chain-of-Thought. We compare the evaluation metric of MiniGPT-4 with and\nwithout Chain-of-Thought, and with MiniGPT-4 for captioning and CLIP for text-text similarity,\ndenoted as mGPT-CLIP. As shown in Table 5, Chain-of-Thought improves the human correlation of\nMiniGPT4-based evaluation by a large margin.\n7\nConclusion and Discussions\nWe propose T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-\nimage generation, consisting of 6,000 prompts from 3 categories and 6 sub-categories. We propose\nnew evaluation metrics and an improved baseline for the benchmark, and validate the effectiveness\nof the metrics and method by extensive evaluation. One limitation is that we do not have a unified\nevaluation metric for all categories of the benchmark. Please refer to appendix for failure cases of our\nproposed metrics. Additionally, our dataset primarily focuses on 2D spatial relationships. We suggest\nthat assessing 3D spatial relationships could integrate depth maps into our existing UniDet-based\nevaluation metric, which we leave for future work. When studying generative models, researchers\nneed to be aware of the potential negative social impact, for example, it might be abused to generate\nfake news. We also need to be aware of the bias from the image generators and the evaluation metrics\nbased on pretrained multimodal models. More discussions on the limitation and potential negative\nsocial impact are provided in the appendix.\nAcknowledgements\nThis work is supported in part by HKU Startup Fund, HKU Seed Fund for Basic Research, and HKU\nSeed Fund for Translational and Applied Research.\n10\nReferences\n[1]\nRobin Rombach et al. \u201cHigh-resolution image synthesis with latent diffusion models\u201d. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022,\npp. 10684\u201310695.\n[2]\nJonathan Ho et al. \u201cCascaded diffusion models for high fidelity image generation\u201d. In: The\nJournal of Machine Learning Research 23.1 (2022), pp. 2249\u20132281.\n[3]\nChitwan Saharia et al. \u201cPhotorealistic text-to-image diffusion models with deep language\nunderstanding\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 36479\u2013\n36494.\n[4]\nPrafulla Dhariwal and Alexander Nichol. \u201cDiffusion models beat gans on image synthesis\u201d. In:\nAdvances in neural information processing systems 34 (2021), pp. 8780\u20138794.\n[5]\nAlexander Quinn Nichol and Prafulla Dhariwal. \u201cImproved denoising diffusion probabilistic\nmodels\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 8162\u20138171.\n[6]\nHuiwen Chang et al. \u201cMuse: Text-to-image generation via masked generative transformers\u201d.\nIn: arXiv preprint arXiv:2301.00704 (2023).\n[7]\nNan Liu et al. \u201cCompositional visual generation with composable diffusion models\u201d. In:\nComputer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327,\n2022, Proceedings, Part XVII. Springer. 2022, pp. 423\u2013439.\n[8]\nWeixi Feng et al. \u201cTraining-Free Structured Diffusion Guidance for Compositional Text-to-\nImage Synthesis\u201d. In: ICLR. 2023.\n[9]\nHila Chefer et al. \u201cAttend-and-excite: Attention-based semantic guidance for text-to-image\ndiffusion models\u201d. In: (2023).\n[10]\nQiucheng Wu et al. \u201cHarnessing the Spatial-Temporal Attention of Diffusion Models for\nHigh-Fidelity Text-to-Image Synthesis\u201d. In: arXiv preprint arXiv:2304.03869 (2023).\n[11]\nAlec Radford et al. \u201cLearning transferable visual models from natural language supervision\u201d.\nIn: International conference on machine learning. PMLR. 2021, pp. 8748\u20138763.\n[12]\nJack Hessel et al. \u201cClipscore: A reference-free evaluation metric for image captioning\u201d. In:\narXiv preprint arXiv:2104.08718 (2021).\n[13]\nJunnan Li et al. \u201cBlip: Bootstrapping language-image pre-training for unified vision-language\nunderstanding and generation\u201d. In: International Conference on Machine Learning. PMLR.\n2022, pp. 12888\u201312900.\n[14]\nJunnan Li et al. \u201cBlip-2: Bootstrapping language-image pre-training with frozen image en-\ncoders and large language models\u201d. In: arXiv preprint arXiv:2301.12597 (2023).\n[15]\nDeyao Zhu et al. \u201cMinigpt-4: Enhancing vision-language understanding with advanced large\nlanguage models\u201d. In: arXiv preprint arXiv:2304.10592 (2023).\n[16]\nJason Wei et al. \u201cChain-of-thought prompting elicits reasoning in large language models\u201d. In:\nAdvances in Neural Information Processing Systems 35 (2022), pp. 24824\u201324837.\n[17]\nEslam Mohamed Bakr et al. \u201cHRS-Bench: Holistic, Reliable and Scalable Benchmark for\nText-to-Image Models\u201d. In: arXiv preprint arXiv:2304.05390 (2023).\n[18]\nScott Reed et al. \u201cGenerative adversarial text to image synthesis\u201d. In: ICML. 2016.\n[19]\nScott E Reed et al. \u201cLearning what and where to draw\u201d. In: NeurIPS. 2016.\n[20]\nHan Zhang et al. \u201cStackgan: Text to photo-realistic image synthesis with stacked generative\nadversarial networks\u201d. In: ICCV (2017).\n[21]\nTao Xu et al. \u201cAttngan: Fine-grained text to image generation with attentional generative\nadversarial networks\u201d. In: CVPR (2018).\n[22]\nMinfeng Zhu et al. \u201cDm-gan: Dynamic memory generative adversarial networks for text-to-\nimage synthesis\u201d. In: CVPR. 2019.\n[23]\nHan Zhang et al. \u201cCross-Modal Contrastive Learning for Text-to-Image Generation\u201d. In: CVPR.\n2021.\n[24]\nIan Goodfellow et al. \u201cGenerative adversarial nets\u201d. In: 2014.\n[25]\nAditya Ramesh et al. \u201cZero-shot text-to-image generation\u201d. In: ICML. 2021.\n[26]\nAditya Ramesh et al. \u201cHierarchical text-conditional image generation with clip latents\u201d. In:\narXiv preprint arXiv:2204.06125 (2022).\n11\n[27]\nAlex Nichol et al. \u201cGlide: Towards photorealistic image generation and editing with text-guided\ndiffusion models\u201d. In: ICML. 2022.\n[28]\nChitwan Saharia et al. \u201cPhotorealistic text-to-image diffusion models with deep language\nunderstanding\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 36479\u2013\n36494.\n[29]\nOran Gafni et al. \u201cMake-a-scene: Scene-based text-to-image generation with human priors\u201d.\nIn: ECCV. 2022.\n[30]\nShu Zhang et al. \u201cHIVE: Harnessing Human Feedback for Instructional Visual Editing\u201d. In:\narXiv preprint arXiv:2303.09618 (2023).\n[31]\nKimin Lee et al. \u201cAligning text-to-image models using human feedback\u201d. In: arXiv preprint\narXiv:2302.12192 (2023).\n[32]\nHanze Dong et al. \u201cRaft: Reward ranked finetuning for generative foundation model alignment\u201d.\nIn: arXiv preprint arXiv:2304.06767 (2023).\n[33]\nZhiheng Li et al. \u201cStylet2i: Toward compositional and high-fidelity text-to-image synthesis\u201d.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n2022, pp. 18197\u201318207.\n[34]\nDong Huk Park et al. \u201cBenchmark for compositional text-to-image synthesis\u201d. In: Thirty-fifth\nConference on Neural Information Processing Systems Datasets and Benchmarks Track (Round\n1). 2021.\n[35]\nMinghao Chen, Iro Laina, and Andrea Vedaldi. \u201cTraining-free layout control with cross-\nattention guidance\u201d. In: arXiv preprint arXiv:2304.03373 (2023).\n[36]\nCatherine Wah et al. \u201cThe caltech-ucsd birds-200-2011 dataset\u201d. In: (2011).\n[37]\nMaria-Elena Nilsback and Andrew Zisserman. \u201cAutomated flower classification over a large\nnumber of classes\u201d. In: 2008 Sixth Indian Conference on Computer Vision, Graphics & Image\nProcessing. IEEE. 2008, pp. 722\u2013729.\n[38]\nTsung-Yi Lin et al. \u201cMicrosoft coco: Common objects in context\u201d. In: Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13. Springer. 2014, pp. 740\u2013755.\n[39]\nJaemin Cho, Abhay Zala, and Mohit Bansal. \u201cDall-eval: Probing the reasoning skills and social\nbiases of text-to-image generative transformers\u201d. In: arXiv preprint arXiv:2202.04053 (2022).\n[40]\nVitali Petsiuk et al. \u201cHuman evaluation of text-to-image models on a multi-task benchmark\u201d.\nIn: arXiv preprint arXiv:2211.12112 (2022).\n[41]\nTim Salimans et al. \u201cImproved techniques for training gans\u201d. In: Advances in neural informa-\ntion processing systems 29 (2016).\n[42]\nMartin Heusel et al. \u201cGans trained by a two time-scale update rule converge to a local nash\nequilibrium\u201d. In: Advances in neural information processing systems 30 (2017).\n[43]\nYujie Lu et al. \u201cLLMScore: Unveiling the Power of Large Language Models in Text-to-Image\nSynthesis Evaluation\u201d. In: arXiv preprint arXiv:2305.11116 (2023).\n[44]\nYixiong Chen. \u201cX-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation\nwith Visual Large Language Models\u201d. In: arXiv preprint arXiv:2305.10843 (2023).\n[45]\nOpenAI. https://openai.com/blog/chatgpt/. 2023.\n[46]\nXingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. \u201cSimple multi-dataset detection\u201d. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022,\npp. 7571\u20137580.\n[47]\nEdward J Hu et al. \u201cLora: Low-rank adaptation of large language models\u201d. In: arXiv preprint\narXiv:2106.09685 (2021).\n[48]\nShuai Shao et al. \u201cObjects365: A large-scale, high-quality dataset for object detection\u201d. In:\nProceedings of the IEEE/CVF international conference on computer vision. 2019, pp. 8430\u2013\n8439.\n[49]\nAlina Kuznetsova et al. \u201cThe open images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale\u201d. In: International Journal of Computer\nVision 128.7 (2020), pp. 1956\u20131981.\n[50]\nGerhard Neuhold et al. \u201cThe mapillary vistas dataset for semantic understanding of street\nscenes\u201d. In: Proceedings of the IEEE international conference on computer vision. 2017,\npp. 4990\u20134999.\n12\n[51]\nhttps://github.com/huggingface/diffusers/blob/main/examples/text_to_\nimage.\n[52]\nIlya Loshchilov and Frank Hutter. \u201cDecoupled weight decay regularization\u201d. In: arXiv preprint\narXiv:1711.05101 (2017).\n[53]\nShilong Liu et al. \u201cGrounding DINO: Marrying DINO with Grounded Pre-Training for Open-\nSet Object Detection\u201d. In: arXiv preprint arXiv:2303.05499 (2023).\n[54]\nLiunian Harold Li et al. \u201cGrounded language-image pre-training\u201d. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10965\u201310975.\n13\nA\nImplementation Details\nEvaluation details. We employ pre-trained models for our proposed evaluation metrics. For BLIP-\nVQA, we utilize the BLIP w/ ViT-B and CapFilt-L [13] pretrained on image-text pairs and fine-tuned\non VQA. We employ the UniDet [46] model Unified_learned_COIM_RS200_6x+2x trained on 4\nlarge-scale detection datasets (COCO [38], Objects365 [48], OpenImages [49], and Mapillary [50]).\nFor CLIPScore, we use the \u201cViT-B/32\u201d pretrained CLIP model [12, 11]. Finally, for MiniGPT4-CoT,\nwe utilize the Vicuna 13B of MiniGPT4 [15] variant with a temperature setting of 0.7 and a beam\nsize of 1.\nTraining details. We implement our proposed FT-SSWL upon the codebase of diffusers [51] (Apache\nLicense), and finetune the self-attention layers of the CLIP text encoder and the attention layers of\nU-net using LoRA [47]. The model is trained by AdamW optimizer [52] with \u03b21=0.9, \u03b22=0.999,\n\u03f5=1e-8, and weight decay of 0.01. The batch size is 5. The model is trained on 8 32GB NVIDIA\nv100 GPUs, for 50000-100000 steps.\nB\nT2I-CompBench Dataset Construction\nThis section provides the details of the prompts that ChatGPT uses for generating the text prompts in\nT2I-CompBench. The text prompts in T2I-CompBench is available at this link.\nColor. The prompt for ChatGPT is: Please generate prompts in the format of \u201ca {adj} {noun} and a\n{adj} {noun}\u201d by using the color adj. , such as \u201ca green bench and a red car\u201d.\nShape. (1) For fixed sentence template, the prompt for ChatGPT is: Please generate prompts in\nthe format of \u201ca {adj} {noun} and a {adj} {noun}\u201d by using the shape adj.: long, tall, short, big,\nsmall, cubic, cylindrical, pyramidal, round, circular, oval, oblong, spherical, triangular, square,\nrectangular, conical, pentagonal, teardrop, crescent, and diamond. (2) For natural prompts, the\nprompt for ChatGPT is: Please generate objects with shape adj. in a natural format by using the\nshape adj.: long, tall, short, big, small, cubic, cylindrical, pyramidal, round, circular, oval, oblong,\nspherical, triangular, square, rectangular, conical, pentagonal, teardrop, crescent, and diamond.\nTexture. (1) We generate 200 natural text prompts by ChatGPT with the following prompt: Please\ngenerate objects with texture adj. in a natural format by using the texture adj.: rubber, plastic,\nmetallic, wooden, fabric, fluffy, leather, glass. (2) Besides the ChatGPT-generated text prompts, we\nalso provide the predefined texture attributes and objects that can be described by each texture, as\nshown in Table 7. We generate 800 text prompts by randomly selecting from the possible combinations\nof two objects each associated with a textural attribute, e.g., \u201cA rubber ball and a plastic bottle\u201d.\nTable 7: Textural attributes and associated objects to construct the attribute-texture prompts.\nTextures\nObjects\nRubber\nband, ball, tire, gloves, sole shoes, eraser, boots, mat\nPlastic\nBottle, bag, toy, cutlery, chair, phone case, container, cup, plate\nMetallic\ncar, jewelry, watch, keychain, desk lamp, door knob, spoon, fork, knife, key, ring, necklace, bracelet, earring\nWooden\nchair, table, picture frame, toy, jewelry box, door, floor, chopsticks, pencils, spoon, knife\nFabric\nbag, pillow, curtain, shirt, pants, dress, blanket, towel, rug, hat, scarf, sweater, jacket\nFluffy\npillow, blanket, teddy bear, rug, sweater, clouds, towel, scarf, hat\nLeather\njacket, shoes, belt, bag, wallet, gloves, chair, sofa, hat, watch\nGlass\nbottle, vase, window, cup, mirror, jar, table, bowl, plate\nNon-spatial relation. The prompt for ChatGPT is: Please generate natural prompts that contain\nsubjects and objects by using relationship words such as wear, watch, speak, hold, have, run, look at,\ntalk to, jump, play, walk with, stand on, and sit on.\nComplex. (1) For 2 objects with mixed attributes, the prompt for ChatGPT is: Please generate natural\ncompositional phrases, containing 2 objects with each object one adj. from {color, shape, texture}\ndescriptions and spatial (left/right/top/bottom/next to/near/on side of) or non-spatial relationships.\n(2) For 2 objects with multiple attributes, the prompt for ChatGPT is: Please generate natural\ncompositional phrases, containing 2 objects with several adj. from {color, shape, texture} descriptions\nand spatial (left/right/top/bottom/next to/near/on side of) or non-spatial relationships. (3) For multiple\n1\nobjects with mixed attributes, the prompt for ChatGPT is: Please generate natural compositional\nphrases, containing multiple objects (number>2) with each one adj. from {color, shape, texture}\ndescriptions and spatial (left/right/top/bottom/next to/near/on side of) non-spatial relationships.\n(4) For multiple objects with multiple attributes, the prompt for ChatGPT is: Please generate\nnatural compositional phrases, containing multiple objects (number>2) with several adj. from {color,\nshape, texture} descriptions and spatial (left/right/top/bottom/next to/near/on side of) or non-spatial\nrelationships.\nC\nEvaluation Metrics\nC.1\nPrompts for MiniGPT4-CoT and MiniGPT4 Evaluation\nMiniGPT4-Chain-of-Thought. In this part, we detail the prompts used for the MiniGPT4-CoT\nevaluation metric. For each sub-category, we ask two questions in sequence: \u201cdescribe the image\u201d and\n\u201cpredict the image-text alignment score\u201d. Specifically, Table 8 shows the MiniGPT4-CoT prompts for\nevaluating attribute binding (color, shape, texture). Table 9, Table 10, and Table 11 demonstrate the\nprompt templates used for spatial relationships, non-spatial relationships, and complex compositions,\nrespectively.\nTable 8: Prompts details for mGPT4-CoT evaluation on attribute binding.\nDescribe\nYou are my assistant to identify any objects and their color (shape, texture) in the image.\nBriefly describe what it is in the image within 50 words.\nPredict\nAccording to the image and your previous answer, evaluate if there is {adj.+noun} in the image.\nGive a score from 0 to 100, according the criteria:\n100: there is {noun}, and {noun} is {adj}.\n75: there is {noun}, {noun} is mostly {adj}.\n20: there is {noun}, but it is not {adj}.\n10: no {noun} in the image.\nProvide your analysis and explanation in JSON format with the following keys: score (e.g., 85),\nexplanation (within 20 words).\nTable 9: Prompts details for mGPT4-CoT evaluation on spatial relationship.\nDescribe\nYou are my assistant to identify objects and their spatial layout in the image.\nBriefly describe the image within 50 words.\nPredict\nAccording to the image and your previous answer, evaluate if the text \"{xxx}\" is correctly portrayed in the image.\nGive a score from 0 to 100, according the criteria:\n100: correct spatial layout in the image for all objects mentioned in the text.\n80: basically, spatial layout of objects matches the text.\n60: spatial layout not aligned properly with the text.\n40: image not aligned properly with the text.\n20: image almost irrelevant to the text.\nProvide your analysis and explanation in JSON format with the following keys: score (e.g., 85),\nexplanation (within 20 words).\nTable 10: Prompts details for mGPT4-CoT evaluation on non-spatial relationship.\nDescribe\nYou are my assistant to identify the actions, events, objects and their relationships in the image.\nBriefly describe the image within 50 words.\nPredict\nAccording to the image and your previous answer, evaluate if the text \"{xxx}\" is correctly portrayed in the image.\nGive a score from 0 to 100, according the criteria:\n100: the image accurately portrayed the actions, events and relationships between objects described in the text.\n80: the image portrayed most of the actions, events and relationships but with minor discrepancies.\n60: the image depicted some elements, but action relationships between objects are not correct.\n40: the image failed to convey the full scope of the text.\n20: the image did not depict any actions or events that match the text.\nProvide your analysis and explanation in JSON format with the following keys: score (e.g., 85),\nexplanation (within 20 words).\nMiniGPT-4 without Chain-of-Thought To guide miniGPT4 in addressing specific compositional\nproblems, we utilize predefined prompts that prompt miniGPT4 to provide a score ranging from 0\n2\nTable 11: Prompts details for mGPT4-CoT evaluation on complex compositions.\nDescribe\nYou are my assistant to evaluate the correspondence of the image to a given text prompt.\nBriefly describe the image within 50 words, focus on the objects in the image and their attributes (such as color, shape, texture),\nspatial layout and action relationships.\nPredict\nAccording to the image and your previous answer, evaluate how well the image aligns with the text prompt: {xxx}.\nGive a score from 0 to 100, according the criteria:\n100: the image perfectly matches the content of the text prompt, with no discrepancies.\n80: the image portrayed most of the actions, events and relationships but with minor discrepancies.\n60: the image depicted some elements in the text prompt, but ignored some key parts or details.\n40: the image did not depict any actions or events that match the text.\n20: the image failed to convey the full scope in the text prompt.\nProvide your analysis and explanation in JSON format with the following keys: score (e.g., 85),\nexplanation (within 20 words).\nto 100. For attribute binding, we focus on the presence of specific objects and their corresponding\nattributes. We utilize a prompt template such as \u201cIs there {object} in the image? Give a score from 0\nto 100. If {object} is not present or if {object} is not {color/shape/texture description}, give a lower\nscore.\u201d We leverage this question for each noun phrase in the text and compute the average score. For\nthe spatial relationships, non-spatial relationships, and complex compositions, we employ a more\ngeneral prompt template such as \u201cRate the overall alignment between the image and the text prompt\n{prompt}. Give a score from 0 to 100.\u201d.\nC.2\nHuman Evaluation\nWe conducted human evaluations on Amazon Mechanical Turk (AMT). Specifically, we ask the\nannotators to rate the alignment between a generated image and the text prompt used to generate the\nimage. Figure 4, 5, 6, 7, 8, 9 show the interfaces for human evaluation over the 6 sub-categories. We\nrandomly sample 25 prompts from each sub-category and each model, and then randomly select 2\nimages per prompt. In total, we gather 1, 800 text-image pairs for human evaluation experiments.\nEach image-text pair is rated by 3 human annotators with a score from 1 to 5 according to the\nimage-text alignment. The estimated hourly wage paid to each participant is 9 USD. We spend 270\nUSD in total on participant compensation.\n3\nworker.\nWe also check the results and reject all tasks from workers, who obviously did not follow the\ninstructions.\nThank you for reading and cooperation!\nText Prompt: a brown backpack and a blue cow\nImage:\nRate the matching degree of objects' color attributes between the Image and Text Prompt:\n 5 - Perfect: all/both objects match their attributes in the text prompt\n 4 - Good: basic level of alignment\n 3 - Not okay: merely aligned with the text prompt\n 2 - Bad: not aligned properly with the text prompt\n 1 - Poor: almost irrelevant to the text prompt\nFigure 4: AMT Interface for the image-text alignment evaluation on attribute binding (color).\nworker.\nWe also check the results and reject all tasks from workers, who obviously did not follow the\ninstructions.\nThank you for reading and cooperation!\nText Prompt: an oval sink and a rectangular mirror\nImage:\nRate the matching degree of objects' shape attributes between the Image and Text Prompt:\n 5 - Perfect: all/both objects match their attributes in the text prompt\n 4 - Good: basic level of alignment\n 3 - Not okay: merely aligned with the text prompt\n 2 - Bad: not aligned properly with the text prompt\n 1 - Poor: almost irrelevant to the text prompt\nFigure 5: AMT Interface for the image-text alignment evaluation on attribute binding (shape).\n4\nworker.\nWe also check the results and reject all tasks from workers, who obviously did not follow the\ninstructions.\nThank you for reading and cooperation!\nText Prompt: The glass jar and fluffy ribbon hold the metallic candy on the wooden\ntable\nImage:\nRate the matching degree of objects' texture attributes between the Image and Text Prompt:\n 5 - Perfect: all/both objects match their attributes in the text prompt\n 4 - Good: basic level of alignment\n 3 - Not okay: merely aligned with the text prompt\n 2 - Bad: not aligned properly with the text prompt\n 1 - Poor: almost irrelevant to the text prompt\nFigure 6: AMT Interface for the image-text alignment evaluation on attribute binding (texture).\nPlease check the Image and Text Prompt carefully! We anticipate that it takes at the very\nleast about 10 seconds to perform a task (in \"easy\" cases). If a worker consistently submits\nanswers after 3-4 seconds or so, we will reject such submissions and consider blocking the\nworker.\nWe also check the results and reject all tasks from workers, who obviously did not follow the\ninstructions.\nThank you for reading and cooperation!\nText Prompt: a vase on the right of a cat\nImage:\nRate the matching degree of objects' spatial layout between the Image and Text Prompt:\n 5 - Perfect: correct spatial layout\n 4 - Good: basically correct spatial layout\n 3 - Not okay: spatial layout not aligned properly with the text\n 2 - Bad: image not aligned properly with the text\n 1 - Poor: image almost irrelevant to the text prompt\nFigure 7: AMT Interface for the image-text alignment evaluation on spatial relationships.\n5\nText Prompt: A boat is sailing on a lake\nText Prompt: A boat is sailing on a lake\nText Prompt: A boat is sailing on a lake\nText Prompt: A boat is sailing on a lake\nText Prompt: A boat is sailing on a lake\nImage: \nRate the matching degree of objects' relationship between the Image and Text Prompt:\n 5 - Perfect: accurate alignment\n 4 - Good: basic level of alignment\n 3 - Not okay: action relationship not correct.\n 2 - Bad: image not aligned properly with the text\n 1 - Poor: image almost irrelevant to the text \nFigure 8: AMT Interface for the image-text alignment evaluation on non-spatial relationships.\nworker.\nWe also check the results and reject all tasks from workers, who obviously did not follow the\ninstructions.\nThank you for reading and cooperation!\nText Prompt: The crisp apple lay beside the rough stone and the silky fabric\nImage: \nRate the overall alignment of Image and Text Prompt:\n 5 - Perfect: accurate alignment\n 4 - Good: basic level of alignment\n 3 - Not okay: ignored key parts\n 2 - Bad: image not aligned properly with the text\n 1 - Poor: image almost irrelevant to the text \nFigure 9: AMT Interface for the image-text alignment evaluation on complex compositions.\n6\nD\nAdditional Results\nD.1\nQuantitative Results of Seen and Unseen Splits\nWe provide the seen and unseen splits for the test set, where the unseen set consists of attribute-object\npairs that do not appear in the training set. The unseen split tends to include more uncommon\nattribute-object combinations than seen split. The performance comparison of seen and unseen splits\nfor attribute binding is shown in Table 12. Our observations reveal that our model exhibits slightly\nlower performance on the unseen set than the seen set.\nTable 12: Performances of our model on attribute binding (color, shape, and texture) for seen and\nunseen sets.\nMetric\nColor\nShape\nTexture\nSeen\nunseen\nSeen\nunseen\nSeen\nunseen\nCLIP\n0.3422\n0.3283\n0.2926\n0.3068\n0.3240\n0.3219\nB-CLIP\n0.7716\n0.7612\n0.7425\n0.6752\n0.7569\n0.6809\nB-VQA\n0.7192\n0.5426\n0.5500\n0.3356\n0.7647\n0.3567\nmGPT\n0.6626\n0.6780\n0.6381\n0.6307\n0.6773\n0.6580\nmGPT-CoT\n0.8082\n0.8038\n0.7510\n0.6888\n0.8453\n0.7412\nD.2\nMiniGPT-4 Evaluation without Chain-of-Thought\nTable 13 shows the additional results of benchmarking on T2I-CompBench of 6 models with MiniGPT-\n4 without Chain-of-Thought. Results indicate that MiniGPT-4 evaluation without Chain-of-Thought\ndoes not strictly align with human evaluation results.\nTable 13: mGPT benchmarking on 6 sub-categories in T2I-CompBench.\nModel\nColor\nShape\nTexture\nSpatial\nNon-spatial\nComplex\nmGPT\nHuman\nmGPT\nHuman\nmGPT\nHuman\nmGPT\nHuman\nmGPT\nHuman\nmGPT\nHuman\nStable v1-4 [1]\n0.6238\n0.6533\n0.6130\n0.6160\n0.6247\n0.7227\n0.8524\n0.3813\n0.8507\n0.9653\n0.8752\n0.8067\nStable v2 [1]\n0.6476\n0.7747\n0.6154\n0.6587\n0.6339\n0.7827\n0.8572\n0.3467\n0.8644\n0.9827\n0.8775\n0.8480\nComposable v2 [7]\n0.6412\n0.6187\n0.6153\n0.5133\n0.6030\n0.6333\n0.8504\n0.3080\n0.8806\n0.8120\n0.8858\n0.7520\nStructured v2 [8]\n0.6511\n0.7867\n0.6198\n0.6413\n0.6439\n0.7760\n0.8591\n0.3467\n0.8607\n0.9773\n0.8732\n0.8333\nAttn-Exct v2 [9]\n0.6683\n0.8240\n0.6175\n0.6360\n0.6482\n0.8400\n0.8536\n0.4027\n0.8684\n0.9533\n0.8725\n0.8573\nGORS-unbiased (ours)\n0.6668\n0.8253\n0.6399\n0.6573\n0.6389\n0.8413\n0.8675\n0.4467\n0.8845\n0.9534\n0.8876\n0.8654\nGORS (ours)\n0.6677\n0.8320\n0.6356\n0.7040\n0.6709\n0.8573\n0.8584\n0.4560\n0.8863\n0.9853\n0.8892\n0.8680\nD.3\nReward models to Select Samples for GORS-unbiased\nTo avoid the bias from selecting samples by evaluation metrics as reward, we introduce new reward\nmodels which are different from our proposed evaluation metrics. Specifically, we adopt Grounded-\nSAM [53] as the reward model for the attribute binding category. We extract the segmentation masks\nof attributes and their associated nouns separately with Grounded-SAM, and use the Intersection-\nover-Union (IoU) between the attribute masks and the noun masks together with the grounding mask\nconfidence to represent the attribute binding performance. We apply GLIP-based [54] selection\nmethod for spatial relationships. For non-spatial relationships, we adopt BLIP [13] to generate image\ncaptions and CLIP [11, 12] to measure the text-text similarity between the generated captions and the\ninput text prompts. For complex compositions, we integrate the 3 aforementioned reward models as\nthe total reward. Those sample selection models are different from the models used as evaluation\nmetrics. The models trained with the new reward models are denoted as GORS-unbiased.\nD.4\nScalability of our proposed approach\nTo demonstrate the scalability of our proposed approach, we introduce additional 700 prompts of\ncomplex compositions to form an extended training set of 1,400 complex prompts. The new prompts\nare generated with the same methodology as described in the appendix B and they are accessible\nthrough this link. We conduct 6 experiments to train the models with different training set sizes, i.e.,\n25 prompts, 275 prompts, 350 prompts, 700 prompts, 1050 prompts, and 1400 prompts. The results\n7\nTable 14: Performances of our model on complex compositons on the 3-in-1 metric\nours (25)\nours (275)\nours (350)\nours (700)\nours (1050)\nours (1400)\n0.2596\n0.3086\n0.3299\n0.3328\n0.3371\n0.3504\nin Table 14 show the performance of our model grows with the increase of the training set sizes. The\nresults indicate the potential to achieve better performance by scaling up the training set.\nD.5\nQualitative Results of Ablation Study\nWe show the qualitative results of the variants in ablation study in Figure 10. When only CLIP is\nfine-tuned with LoRA, the generated images do not bind attributes to correct objects (for example,\nFigure 10 Row. 3 Col. 3 and Row. 6 Col. 3). Noticeable improvements are observed in the generated\nimages when U-Net is fine-tuned by LoRA, particularly when both CLIP and U-Net are finetuned\ntogether. Furthermore, we delve into the effect of the threshold for selecting images aligned with\ntext prompts for fine-tuning. A higher threshold value enables the selection of images that are highly\naligned with text prompts for finetuning, ensuring that only well-aligned examples are incorporated\ninto the finetuning process. In contrast, a lower threshold leads to the inclusion of misaligned images\nduring finetuning, which can degrade the compositional ability of the finetuned text-to-image models\n(for example, Figure 10 last two columns in Row. 2).\n8\nGORS (ours)\nFT U-Net only\nFT CLIP only\nHalf threshold\n0 threshold\nA brown backpack and a blue bear\nA brown giraffe and a blue vase\nA white car and a red sheep\nFigure 10: Qualitative comparison of ablation study on fine-tuning strategy and threshold.\n9\nD.6\nQualitative Results and Comparison with Prior Work\nAdditional results and comparisons are shown in Figure 11 and Figure 12\nA man in a black \njacket standing in a \nkitchen next to a \ngray dog\nA spherical snowball \nand a conical \nsnowman hat\nA wooden floor \nand a glass bottle\nA blue horse \nand a brown vase\nA brown banana \nand a green dog\nStable\nv1-4\nStable \nv2\nComposable\nv2\nStructure\nv2\nAttn-Exct\nv2\nGORS\n(ours)\nFigure 11: Qualitative comparison between our approach and previous methods.\n10\nA plastic container \nand a leather chair\nA plastic bag \nand a glass jar\nThe wooden spoon \nand metallic ladle \nserve the fluffy soup \nin the plastic bowl\nA man on the \nleft of a giraffe\nA suitcase on the \nright of a mouse\nStable\nv1-4\nStable \nv2\nComposable\nv2\nStructure\nv2\nAttn-Exct\nv2\nGORS\n(ours)\nFigure 12: Qualitative comparison between our approach and previous methods.\n11\nAn oval coffee table and a rectangular rug\nAn oval sink and a rectangular mirror\nA round muffin and a square napkin\nFigure 13: Failure cases of the evaluation metric BLIP-VQA.\nE\nLimitation and Potential Negative Social Impacts\nOne limitation of our work is the absence of a unified metric for all forms of compositionality. Future\nresearch can explore the potential of multimodal LLM to develop a unified metric. Our proposed\nevaluation metrics are not perfect. As shown by the failure cases in Fig. 13, BLIP-VQA may fail\nin challenging cases, for example, the objects\u2019 shapes are not fully visible in the image, shape\u2019s\ndescription is uncommon or the objects are not easy to recognize. The UniDet-based evaluation\nmetric is limited to evaluating 2D spatial relationships and we leave 3D spatial relationships for\nfuture study. Researchers need to be aware of the potential negative social impact from the abuse of\ntext-to-image models and the biases of hallucinations from image generators as well as pre-trained\nmultimodal models and multimodal LLMs. Future research should exercise caution when working\nwith generated images and LLM-generated content and devise appropriate prompts to mitigate the\nimpact of hallucinations and bias in those models.\n12\n"
  }
]