[
  {
    "title": "Mind2Web: Towards a Generalist Agent for the Web",
    "link": "https://arxiv.org/pdf/2306.06070.pdf",
    "upvote": "18",
    "text": "MIND2WEB: Towards a Generalist Agent for the Web\nXiang Deng\u2217\nYu Gu\nBoyuan Zheng\nShijie Chen\nSamuel Stevens\nBoshi Wang\nHuan Sun\u2217\nYu Su\u2217\nThe Ohio State University\nhttps://osu-nlp-group.github.io/Mind2Web\nAbstract\nWe introduce MIND2WEB, the first dataset for developing and evaluating generalist\nagents for the web that can follow language instructions to complete complex tasks\non any website. Existing datasets for web agents either use simulated websites or\nonly cover a limited set of websites and tasks, thus not suitable for generalist web\nagents. With over 2,000 open-ended tasks collected from 137 websites spanning\n31 domains and crowdsourced action sequences for the tasks, MIND2WEB pro-\nvides three necessary ingredients for building generalist web agents: 1) diverse\ndomains, websites, and tasks, 2) use of real-world websites instead of simulated\nand simplified ones, and 3) a broad spectrum of user interaction patterns. Based\non MIND2WEB, we conduct an initial exploration of using large language models\n(LLMs) for building generalist web agents. While the raw HTML of real-world web-\nsites are often too large to be fed to LLMs, we show that first filtering it with a small\nLM significantly improves the effectiveness and efficiency of LLMs. Our solution\ndemonstrates a decent level of performance, even on websites or entire domains the\nmodel has never seen before, but there is still a substantial room to improve towards\ntruly generalizable agents. We open-source our dataset, model implementation,\nand trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate\nfurther research on building a generalist agent for the web.\n1\nIntroduction\nThe web now hosts billions of websites [7] that cover virtually every aspect of the digital world. In\nthis work, we seek to answer the question: How can we build a generalist agent for the web that,\ngiven any website, can follow language instructions and carry out the corresponding tasks? Some\nexemplar tasks for such an agent are shown in Figure 1. A generalist agent could make the web more\naccessible, which is becoming increasingly difficult as modern websites provide increasingly more\nfunctionalities that also increase their complexity and learning curve. On the other hand, such an agent\nmay also turn the entire web into an unprecedentedly powerful and versatile tool [23, 27] that can\nenhance large language models (LLMs). For example, it may be used as a plugin for ChatGPT [24] to\ndirectly acquire information and carry out actions on HTML websites, instead of only retrieving web\ncontent through a retriever tool [8, 18] or relying on pre-defined APIs for each web service [34, 37].\nA generalist agent for the web shall meet the following desiderata: First, it shall work on any website\non the Internet. Since it is infeasible to collect sufficient training data that covers all websites, this\nrequires the agent to be inherently generalizable to websites or even domains it has never seen before.\nSecond, it shall work on real-world websites, which can be dynamic, complex, and noisy. Most\nmodern websites are dynamic, generating and rendering different content in response to user actions.\nThis necessitates the agent to model each website as a partially-observable environment instead of\nassuming full knowledge a priori. The agent should also not make strong simplifying assumptions\n\u2217Corresponding authors: {deng.595, sun.397, su.809}@osu.edu\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2306.06070v3  [cs.CL]  9 Dec 2023\n(a) Find one-way flights from New York to \nToronto.\n(b) Book a roundtrip on July 1 from Mumbai to \nLondon and vice versa on July 5 for two adults. \n(c) Find a flight from Chicago to London on \n20 April and return on 23 April.\n(d) Find Elon Musk's profile and follow, start \nnotifications and like the latest tweet.\n(e) Browse comedy films streaming on Netflix \nthat was released from 1992 to 2007.\n(f) Open page to schedule an appointment for \ncar knowledge test.\n3.1%\n5.8%\n6.2%\n4.3%\n3.7%\n2.4%\n1.8%\n5.0%\n3.3%\n2.3%\n1.6%\n1.6%\n3.8%\n3.5%\n3.5%\n3.2%\n2.0%\n3.7%\n3.5%\n6.5%\n3.4%\n2.1%\n1.8%\n1.7%\n1.9%\n2.0%\n2.7%\n3.5%\n2.8%\nTravel (27.4%)\nShopping (17.5%)\nEntertainment (16.1%)\nInfo (20.4%)\nService (18.3%)\nOther \n6.2%\nGround \n3.7%\nAirlines \n5.8%\nRestaurant \n4.3%\nGeneral \n3.1%\nCar\nRental \n2.4%\nHotel \n1.8%\nJob \n3.5%\nEducation \n2.8%\nFinance \n2.7%\nCook-\ning \n2.9%\nHousing \n3.7%\nSocial\nMedia \n3.5%\nWeather \n1.9%\nGovernment \n3.4%\nPet \n2.1%\nHealth \n6.5%\nShipp-\ning \n1.8%\nMoving\n1.7%\nHome\nService \n2.5%\nSpeciality \n5.0%\nAuto \n3.4%\nGeneral \n3.3%\nDigital \n2.3%\nDepar-\ntment \n1.6%\nFashion \n1.6%\nEvent \n3.8%\nMusic \n3.5%\nSports \n3.5%\nMovie \n3.2%\nGame \n2.0%\nFigure 1: Sample tasks and all domains featured in MIND2WEB. The array of diversity allows for\ntesting an agent\u2019s generalizability across tasks on the same website (a vs. b), similar tasks on different\nwebsites (a vs. c), and even to entirely disparate tasks, websites, and domains (d\u2212f).\nabout the environments but must embrace the full complexity and sometimes noise, e.g., due to\nsub-optimal website designs. Finally, it shall support diverse and sophisticated interactions with\nwebsites. Tasks from users can be highly diverse and take a large number of steps to complete (e.g.,\nthe task in Figure 1(b) would take 14 actions). An agent that only supports simple tasks may provide\nlimited value to users.\nBuilding an agent for the web is not entirely new. There have been numerous prior efforts in\nvaried forms. However, none of them meets all the requirements for a generalist agent listed above.\nExisting work falls short in one to all of the following aspects: 1) Only operating in a limited and\npre-specified set of websites [5, 21, 22, 35, 40], 2) making strong simplifying assumptions about the\nwebsites [22, 40], and 3) only supporting specific types of tasks [21, 22, 40] and/or requiring tedious\nstep-by-step instructions from users [5, 21, 22, 39]. Meanwhile, LLMs have been shown to excel at\ngrounded language understanding in complex environments with good generalizability and sample\nefficiency [2, 13, 17, 33]. It is therefore promising to explore LLMs as a candidate solution towards\ngeneralist agents for the web. However, there lacks a good dataset that can support the development\nand evaluation of generalist web agents, which is the focus of this work.\nIn light of this, we present MIND2WEB, a new dataset with natural language tasks and manually\nannotated action sequences for developing and evaluating generalist agents for the web. It offers the\nfollowing unique features:\n1. Diverse coverage of domains, websites, and tasks. MIND2WEB boasts a collection of over 2,000\ntasks curated from 137 websites that span 31 different domains. This extensive range of tasks and\ndomains not only provides a vast landscape for exploration and learning but also opens up a new level\nof versatility and complexity, fostering a more comprehensive evaluation of generalist web agents.\n2\n2. Use of real-world websites. MIND2WEB replaces the oversimplified simulation environments\ncommonly found in other datasets with an authentic, vibrant, and unpredictable realm of real-world\nwebsites. We provide full traces of user interactions, webpage snapshots, and network traffic, making\nit a rich source of raw, unfiltered, and dynamic data. By doing so, MIND2WEB equips models with\nthe capacity to interact and cope with the complexities and uncertainties of real-world environments,\nthereby encouraging the development of more robust and adaptive models.\n3. A broad spectrum of user interaction patterns. MIND2WEB enables users to engage in\nsophisticated ways with websites, as opposed to basic operations such as searching, following links,\nand reading content commonly found in existing work. Users can click, select, and type in any\nelements on the website, which significantly expands the space of possible tasks. This captures all the\ncommon actions users do on websites in real life and promotes the development of agents capable of\nhandling complex tasks.\nWith the diverse domains and websites, we create challenging out-of-distribution evaluation settings\nwhere agents are tested on their generalizability to websites or even entire domains never seen during\ntraining. This presents a representative evaluation of generalist agents working on unseen websites.\nMIND2WEB enables us to conduct an initial exploration in using LLMs for building generalist web\nagents. The HTML document of real-world webpages may contain thousands of elements, which\nmakes it infeasible or too costly to be fed into an LLM\u2019s context. To address this issue, we propose\nMINDACT, a two-stage model that involves first using a fine-tuned small LM to filter the web elements\nand then using an LLM to select from the filtered elements in a multi-choice question answering\nfashion and predict the corresponding action with the selected element. We show that MINDACT\nsignificantly outperforms modeling strategies commonly adopted by prior work and achieves a decent\nlevel of generalization. It can also work well with both open-source LLMs like Flan-T5 [10] through\nfine-tuning and closed-source LLMs like GPT-3.5-turbo and GPT-4 through in-context learning.\nHowever, there is still substantial room for further improvement towards generalist agents for the web.\nPromising future directions include integrating multi-modal information, reinforcement learning with\nfeedback from real websites, and specialized LMs for web understanding and action taking.\n2\nMIND2WEB Dataset\nUnlike existing datasets predominantly constructed within simulated environments [31, 40], our\nobjective is to bridge the gap between simulation and reality so that agents trained on our dataset can\nwork on real-world websites out of the box. To achieve this, our approach for data collection adheres\nto the following principles. Firstly, instead of recreating websites in simulation, which often leads to\noversimplified environments, we engage directly with real-world websites and capture snapshots of\nthese environments. Secondly, we collate a diverse set of websites from varied domains and crowd-\nsource realistic tasks that cover a wide range of functionalities provided by these websites. Finally,\nacknowledging the challenge of perfectly replicating the complexity of real-world environments, we\nstrive to capture a comprehensive snapshot of each website and the full interaction trace, to the extent\nthat all the tasks can be seamlessly replayed offline. This supports rich modeling and evaluation\napproaches, ensuring a robust and practical dataset for research.\n2.1\nTask Definition\nThe primary objective of MIND2WEB is for the agent to complete a specific task on the target website\nthrough a series of actions. Each instance in our dataset contains three components:\nTask description, which outlines the high-level goal of the task. We intentionally avoid low-level,\nstep-by-step instructions, aiming to foster the development of agents that can comprehend and carry\nout tasks in a more autonomous fashion, rather than merely following prescriptive directives.\nAction sequence, which is the sequence of actions required to accomplish the task on the website.\nEach action in the sequence comprises a (Target Element, Operation) pair. The Target\nElement is an interactable element on the current webpage, and the Operation refers to the action\nto be executed on that element. We support three common operations: Click (also including Hover\nand Press Enter), Type, and Select Option. For Type and Select Option, they also require\nan additional value as an argument. Actions in a sequence often span multiple webpages of a website.\n3\nTask Description:\nShow me the reviews for the auto repair business closest to \n10002.\nAction Sequence:\nTarget Element\nOperation\n1.\n[searchbox] Find\nTYPE: \nauto repair\n2.\n[button] Auto Repair\nCLICK\n3.\n[textbox] Near\nTYPE:\n10002\n4.\n[button] 10002\nCLICK\n5.\n[button] Search\nCLICK\n6.\n[switch] Show BBB Accredited only\nCLICK\n7.\n[svg]\nCLICK\n8.\n[button] Sort By\nCLICK\n9.\n[link] Fast Lane 24 Hour Auto Repair\nCLICK\n10. [link] Read Reviews\nCLICK\nWebpage Snapshots:\n<button>Show BBB Accredited \nonly</button>\n<em>Auto Repair</em>\n<button>Search</button>\nAction 1\nAction 2\n<input name=\"find_text\" \ntype=\"search\">\nAction 5\nAction 6\nAction 9\n<span>Fast Lane 24 Hour Auto \nRepair</span>\nAction 10\n<a href=\"link:XXX\">Read \nReviews</a>\nFigure 2: A sample data instance of our dataset with the three components. Actions marked in red\nwill result in a transition to a new webpage.\nWebpage snapshots, which constitute the environment within which the task is performed. We\nprovide the snapshots in a variety of formats to accommodate different modeling approaches: self-\ncontained MHTML file that includes the raw HTML code of the webpage, DOM snapshot containing\nthe DOM tree along with the layout and style information of the screenshot of the rendered webpage,\nHAR file that includes all the network traffic for replaying the interaction if needed, and trace file that\ncomprises the complete interaction trace during the task annotation process.\nThe agent receives the task description in the beginning. At each step, it also receives the current\nwebpage and the history of previous actions. The objective is to accurately predict the subsequent\naction, which encompasses the target element for interaction and the operation.\n2.2\nData Collection\nOur data collection process consists of four stages: website selection, task proposal, task demonstra-\ntion, and task verification. Website selection and task verification are done by the authors. For task\nproposal and demonstration, we develop a sophisticated annotation tool using Playwright 2 and hire\nannotators through Amazon Mechanical Turk. Refer to Supplementary for annotation tool details.\nWebsite Selection. We start with 5 top-level domains: Travel, Shopping, Service, Entertainment, and\nInformation, which are subsequently broken down into 31 (secondary) domains. We select websites\nwithin each domain based on their popularity in the US, as ranked by similarweb.com. We manually\nselect 3-5 representative websites per domain, resulting in a collection of 137 websites in total.\nTask Proposal. We present the annotators with a target website, a concise description of the website,\nand a few sample tasks associated with it. The annotators are then asked to propose open-ended\nand realistic tasks based on three criteria: the tasks should be of diverse types, require multiple\nrounds of interaction, and describe the high-level goal instead of step-by-step instructions. To further\nstimulate creativity and boost diversity, we use ChatGPT to generate seed tasks by prompting it\nto test different functionalities of a website. We generate 50 seed tasks per website, of which 10\nare randomly sampled and presented to the annotator each time. These seed tasks are mainly for\ninspiration\u2014annotators are explicitly instructed not to directly use them and we reject task proposals\nthat are highly similar to the seed tasks. All the proposed tasks are further screened by the authors to\nensure quality and diversity before entering the demonstration phase.\nTask Demonstration. We develop a Playwright-based tool for demonstration (Figure 2). Workers\nwill use the tool to demonstrate how to perform the tasks they have proposed within a web browser.\nTo ensure accuracy, each interaction round is split into two parts: element selection and operation\nselection. At each step, the worker first selects an element on the webpage by clicking within the\nbrowser. They are then asked to confirm the selection and choose the operation to execute on the\nselected element. Once the task is completed, the worker is given another opportunity to review and\nmodify the task description.\n2https://playwright.dev/\n4\nTable 1: Statistics of MIND2WEB compared with existing datasets.\n# Dom.\n# Env.\nEnv. Type\nAvg. #\nElements\n# Tasks\nTask Info.\nAvg. #\nActions\nMiniWoB++ [22]\n\u2212\n100\nSimplified\nmobile websites\n28\n100\nLow-level\n3.6\nWebShop [40]\n1\n1\nSimplified\nshopping websites\n38\n12,000 products\nHigh-level\n11.3\nRUSS [39]\n\u2212\n22\nReal-world\nwebsites\n801\n80\nHigh & low\n5.4\nPixelHelp [21]\n4\n4\nMobile apps\n\u2212\n187\nHigh & low\n-\nMETA-GUI [35]\n6\n11\nMobile apps\n79\n1,125 dialogues\nHigh-level\n4.3\nMoTIF [5]\n15\n125\nMobile apps\n188\n756\nHigh & Low\n4.4\nMIND2WEB\n5 / 31\n137\nReal-world\nwebsites\n1,135\n2,350\nHigh-level\n7.3\nTask Verification. Lastly, all task demonstrations are verified by the authors to ensure the following:\nFirst, all actions are accurately reflected in the task description. The authors will modify the task\ndescription if needed to align it with the annotated actions; Second, the recorded actions are correct\nand clean, with extraneous steps discarded. Finally, the starting and ending points of tasks are\nconsistent, such as excluding actions for closing popup windoes, or ending the annotation at the\nsearch result page if the task was to find a certain item without clicking on specific items. After\nverification, we discarded 61 out of the total 2,411 tasks. Among the 2,350 retained tasks, the task\ndescription was refined in 390 instances to better correspond with the demonstrated actions, while\nsome extraneous steps were discarded in 187 instances. Overall, the data collection pipeline has been\nproven effective and produces high-quality data.\n2.3\nComparison with Existing Work and Research Challenges\nMIND2WEB presents a unique ensemble of research challenges for the development of generalist\nagents for the web in real-world settings. As shown in Table 1, MIND2WEB distinguishes itself\nfrom existing literature in several ways. Firstly, MIND2WEB spans across 137 websites from\n31 domains, allowing comprehensive testing of an agent\u2019s ability in generalizing across varied\nenvironments. Secondly, we utilize real-world websites without manual simplification. Consequently,\nthe included environments exhibit complexity far surpassing that encountered in previous studies,\nyet better reflecting the intricacy of the modern web. With an average of over 1,000 elements per\npage embedded within complex DOM structures, how to effectively process such long and highly\nstructured documents presents a significant challenge for modeling. Lastly, we direct the annotators\nto propose open-ended tasks that explore different functionalities of the website to mimic genuine\nweb usage. Meanwhile, contrary to prior studies [5, 21, 22, 39] that provide step-by-step directives\nand primarily focus on testing the agent\u2019s ability to translate low-level instructions into actions, e.g.,\n\u201cType New York in the location field, click the search button and choose the tomorrow tab,\u201d we opted\nfor the setting where only high-level goals are available, e.g., \u201cWhat is the weather for New York\ntomorrow?\u201d This poses a much greater yet realistic planning and grounding challenge for the agent.\n3\nMethod: MINDACT\nEmploying the data from MIND2WEB, we introduce an exploratory framework, MINDACT, for our\ntask, leveraging the power of LLMs. Raw HTML documents, which could consist of thousands\nof elements, are either infeasible or cost-prohibitive to be directly fed into LLMs. We propose a\ntwo-stage process that synergizes the strength of small and large LMs, as shown in Figure 3. In the\nfirst stage, a fine-tuned small LM is used to rank the elements present on a webpage, yielding a small\npool of promising candidates. In the second stage, these candidate elements are consolidated to form\na representative snippet of the webpage, which is then processed by an LLM to predict the final\naction, including predicting both the element for interaction and the corresponding operation.\n3.1\nCandidate Generation with Small LMs\nGiven the task description, the snapshot of the webpage at step t, and the actions performed in the\npreceding t \u2212 1 steps, we treat candidate generation as a ranking task. The task is to select the top-k\n5\nHTML \nDocument\nRanking LM\n+\nTarget \nElement\nOperation\nPrediction LLM\n\u2026\nCandidate \nElements\nTop-k \nElements\nHTML \nSnippet\nTask \nDescription\nPrevious \nActions\nCandidate Representation\nancestors: /html/div dialog/ul location \nsearch results\ntarget: (button id=5 (span (span \nBoston ) (span NY, USA ) ) )\nTask Query\nTask : Check for pickup restaurant \navailable in Boston, NY on March 18, \n5pm with just one guest\nPrevious Actions:\n[combobox] Reservation type -> \nSELECT: Pickup\n[svg] -> CLICK \n[searchbox] Find a location -> TYPE: \nBoston\nSmall LM\nClassifier\n[0, 1]\nScore\nFigure 3: The overall pipeline for MINDACT with\na small ranking LM for candidate generation, and a\nlarge prediction LM for action prediction.\nHTML \nDocument\nRanking LM\n+\nTarget\nElement\nOperation\nPrediction LLM\n\u2026\nCandidate \nElements\nTop-k \nElements\nHTML \nSnippet\nTask \nDescription\nPrevious \nActions\nCandidate Representation\nancestors: /html/div dialog/ul location \nsearch results\ntarget: (button id=5 (span (span \nBoston ) (span NY, USA ) ) )\nTask Query\nTask : Check for pickup restaurant \navailable in Boston, NY on March 18, \n5pm with just one guest\nPrevious Actions:\n[combobox] Reservation type -> \nSELECT: Pickup\n[svg] -> CLICK \n[searchbox] Find a location -> TYPE: \nBoston\nRanking LM\nClassifier\n[0, 1]\nScore\nFigure 4: Illustration of the candidate gener-\nation module and the templates for construct-\ning task query and candidate representation.\n<html> <form id=0> <div meta=\"navigation; sitelinks\">\n<p> <a> Collect Renaissance </a> <a> Shop Le Meridien \n</a> <a> Westin Store </a> <a> Sheraton Store </a>\n</p> </div> \u2026 <div> <select id=1 meta=\"Size; Select a \nSize\"> <span meta=tablist> <button id=2 meta=\"button; \ntab\"> Description </button> \u2026 <a id=3 meta=\"Shop \nFeather & Down Pillow\"> <img meta=\"Product Feather & \nDown Pillow\"> <p> <a> California Privacy Rights </a>\n<a> Privacy Statement </a> <a> Terms of Use </a> <a\nid=4> Loyalty Terms </a> \u2026\nBased on the HTML webpage above, try to complete the following \ntask:\nTask: Search for queen-size pillow protectors from the Marriot \nshop, and if found, add two pieces to the cart and checkout. \nPrevious actions: \n[button] Special Offers -> CLICK \n[link] Shop Marriott Opens a new window -> CLICK\n[menuitem] category pillows -> CLICK \n[span] Pillow Protector -> CLICK \nWhat should be the next action? \nPrediction LLM\nPlease select from the following choices (If the correct action is not in the page \nabove, please select A. 'None of the above\u2019): \nA.\nNone of the above\nB.\n<form id=0> <div meta=\"navigation; sitelinks\"> <p> <a> Collect \nRenaissance </a> <a> Shop Le Meridien </a> <a> Westin Store </a> <a>\nC.\n<select id=1  meta=\"Size; Select a Size\">\nD.\n<button id=2  meta=\"button; tab\"> Description </button>\nE.\n<a id=3 meta=\"Shop Feather & Down Pillow\"> <img meta=\"Product Feather \n& Down Pillow\"> <span> Feather & Down Pillow </span> </a>\nF.\n<a id=4> Loyalty Terms </a>\nC. \nAction: SELECT \nValue: Queen\nElement: <select id=1  meta=\"Size; Select a Size\">\nAction: SELECT \nValue: Queen\nMultichoice\nDirect Generation\nFigure 5: Illustration of action prediction with LLMs.\ncandidate DOM elements from the webpage that best align with both the task description and the\ncurrent step. We formulate the task query by concatenating the task description with previous actions.\nThe textual representation of each candidate DOM element is derived from a combination of the\nelement\u2019s tag, its textual content, and salient attribute values, as well as the textual representation\nof its parent and child elements. As shown in Figure 4, we pair each DOM element with the task\nquery and feed it to an encoder-only LM through the cross-encoder architecture [28], yielding a\nmatching score. At training time, we randomly sample negative elements from the webpage, and use\nthe target element as the positive example. The matching score is passed through a sigmoid activation\nfunction and optimized with a binary cross entropy loss. At inference time, we score all elements in\nthe webpage and pick the top-k elements with the largest logits as input to the second stage.\n3.2\nAction Prediction with LLMs\nAfter obtaining the top-k candidates, we utilize the candidate set to prune the webpage snapshot and\nconstruct snippets that only include the selected candidates and their neighbours as inputs to an LLM.\nRecent studies [10, 13] have suggested that training LMs for discrimination rather than generation is\nmore generalizable and sample-efficient for other grounding tasks. Inspired by that, we convert the\ntask of element selection into a multi-choice question answering (QA) problem. Instead of generating\nthe complete target element, the LM is trained to instead select from a list of options. For comparison,\nwe also include a baseline that directly generates the target element based on the provided webpage\nsnippet. In both cases, we directly let the LLM generate the operation, along with the additional value\nneeded for some operations. An example is shown in Figure 5. We incorporate up to 5 candidate\nelements within each input, together with a None option, and partition the candidate set into several\ngroups. During training, we construct the target sequence using ground-truth actions and fine-tune\nthe model using a left-to-right language modeling objective. During inference, we divide the top-k\ncandidates into multiple clusters of five options. If more than one option is selected after a round, we\nform new groups with the selected ones. This process repeats until a single element is selected, or all\noptions are rejected by the model, i.e., the model chooses the None option for all groups.\n6\nTable 2: Main results. The classification baseline uses DeBERTaB and the generation baseline uses\nFlan-T5B. For step-wise metrics, we report macro average across tasks. \u2217 For GPT-4 we use 50 tasks\nfor each setting with top-10 candidates due to limited budget. See Appendix D.3 for results on the 50\ntasks subsets for all methods.\nCross-Task\nCross-Website\nCross-Domain\nEle. Acc\nOp. F1\nStep SR\nSR\nEle. Acc\nOp. F1\nStep SR\nSR\nEle. Acc\nOp. F1\nStep SR\nSR\nClassification\n26.8\n\u2212\n\u2212\n\u2212\n21.6\n\u2212\n\u2212\n\u2212\n24.5\n\u2212\n\u2212\n\u2212\nGeneration\n20.2\n52.0\n17.5\n0.0\n13.9\n44.7\n11.0\n0.0\n14.2\n44.7\n11.9\n0.4\nMINDACT\nw/ Flan-T5B\n43.6\n76.8\n41.0\n4.0\n32.1\n67.6\n29.5\n1.7\n33.9\n67.3\n31.6\n1.6\nw/ Flan-T5L\n53.4\n75.7\n50.3\n7.1\n39.2\n67.1\n35.3\n1.1\n39.7\n67.2\n37.3\n2.7\nw/ Flan-T5XL\n55.1\n75.7\n52.0\n5.2\n42.0\n65.2\n38.9\n5.1\n42.1\n66.5\n39.6\n2.9\nw/ GPT-3.5\n20.3\n56.6\n17.4\n0.8\n19.3\n48.8\n16.2\n0.6\n21.6\n52.8\n18.6\n1.0\nw/ GPT-4\u2217\n41.6\n60.6\n36.2\n2.0\n35.8\n51.1\n30.1\n2.0\n37.1\n46.5\n26.4\n2.0\n4\nExperiments\n4.1\nExperimental Setup\nThe diversity of MIND2WEB provides a unique opportunity to evaluate an agent\u2019s generalizability at\ndifferent levels. We seek to understand how well an agent can generalize across domains, websites,\nand tasks: TestCross-Domain, for which we hold out two top-level domains, Information and Service,\nwith 912 tasks from 73 websites. Here, the model is expected to generalize to an entirely new\ndomain without having seen any websites or tasks associated with that domain during training.\nTestCross-Website, with 10 websites from each remaining top-level domain, containing 177 tasks. In\nthis setting, the model is never exposed to the test websites during training. However, it has been\ntrained on websites from the same domain and possibly with similar tasks. This setup allows us\nto assess an agent\u2019s ability to adapt to entirely new websites, yet within familiar domains and task\ncontexts. TestCross-Task, where we randomly split 20% of the remaining data, regardless of domains\nand websites, resulting in 252 tasks from 69 websites. In this setting, the model has been exposed to\nwebpages from the same website during training and has likely encountered similar tasks. The rest of\ndata is used for training, which contains 1,009 tasks from 73 websites.\n4.2\nData Preprocessing and Evaluation\nWe apply simple heuristics to clean the raw HTML documents, keeping only elements that are\nvisible and carry substantial semantic meaning, as determined by their attributes, textual content, and\nneighboring elements. This effectively reduces the average number of elements from 1,135 to 580,\nwhile still maintaining an overall recall of 94.7% for the target element in the training data.\nFor evaluation, we first calculate Element Accuracy that compares the selected element with\nall acceptable elements, and Operation F1 that calculates token-level F1 score for the predicted\noperation. This is the same as accuracy for Click, but considers the correctness of the input value for\nType and Select Option. Each step of the task is evaluated independently with the ground truth\naction history provided. We then define Step Success Rate and Success Rate (for the whole task). A\nstep is regarded as successful only if both the selected element and the predicted operation are correct.\nA task is regarded successful only if all steps have succeeded. It is therefore a stringent metric. For\nstep-wise metrics, we report macro average across tasks.\n4.3\nResults\nCandidate Generation. We fine-tune DeBERTa [16] as the small LM for candidate generation.\nAs candidate generation requires high efficiency, we use the base version DeBERTaB with 86M\nparameters. Overall, it achieves 88.9% / 85.3% / 85.7% Recall@50 on TestCross-Task, TestCross-Website\nand TestCross-Domain, respectively. We use its top-50 ranking results as the candidate pool for all\nsubsequent experiments.\nAction Prediction. We mainly compare against two baselines in Table 2. The first directly uses\nthe candidate generation model (DeBERTa) for element selection, which is similar to existing\nwork [14, 35] that combines an encoder with classification heads. However, such a design cannot\n7\nFigure 6: Step success rate per website grouped by the three splits: TestCross-Task, TestCross-Website and\nTestCross-Domain from left to right. Here we only show websites with more than three test tasks.\nbenefit from many recent LMs that use an encoder-decoder or decoder-only architecture. It cannot\npredict actions and the element selection performance is also not competitive, as shown in Table 2.\nWe use Flan-T5 [10] as the backbone for the generation model. The autoregressive generation\nformulation (Figure 5 top) does not perform well, and even underperforms the classification baseline\non element selection despite the larger model size (220M for Flan-T5B). We observe a substantial\ngain with MINDACT using the multi-choice QA formulation. The best model achieves 52.0% step\nsuccess rate under Cross-Task setting, and 38.9% / 39.6% when generalizing to unseen websites\nand domains. However, the overall task success rate remains low for all models, as the agent often\ncommits at least one error step in most cases.\nThree Levels of Generalization. All models perform best on the Cross-Task setting, with over\n10% absolute gap (step SR) on average compared with Cross-Website and Cross-Domain settings,\nindicating that generalizing to unseen environments is still a major challenge. On the contrary, we\nnote that the performance of Cross-Website and Cross-Domain settings are notably similar, which is\nalso reinforced in Figure 6, where there is no clear distinction in performance across these settings.\nThis suggests that the challenges primarily stem from the diversity in website designs and interaction\nlogic rather than domain specifics. Tasks across domains tend to share common operations, and\npretrained LMs may already have the capability to decompose complex tasks at a high level based\non commonsense knowledge. Yet, grounding such knowledge into actionable steps in specific and\nvarying environments remains a considerable challenge.\nIn-context Learning with LLMs. We also experiment with two popular LLMs, GPT-3.5-turbo and\nGPT-4 [25], through in-context learning. We use the same multiple-choice formulation as MINDACT,\nand include three demonstration examples for in-context learning. We can see that both models\nare comparable to the two baselines with only three in-context examples. Note that this is not a\nfair comparison with the Flan-T5 models, which are fine-tuned on the full training data. We also\ninclude the zero-shot results with Flan-T5XL in Appendix D.2, but the model fails to perform the task\nwithout fine-tuning. Meanwhile, GPT-3.5 only has around 20% element selection accuracy, despite\nthe superior performance people have observed on other datasets. Further analysis reveals that one\npossible problem is the model\u2019s propensity to select the None option, asserting that the task cannot\nbe finished on the current webpage. This is somewhat accurate since tasks typically necessitate\nnavigation through multiple webpages and performing a series of actions before reaching the final\nresult. This aspect indeed represents the primary difficulty of our task. On the other hand, we observe\nhighly promising outcomes with GPT-4. The performance is on par with the tuned Flan-T5 models\nunder Cross-Website and Cross-Domain settings for element selection, indicating a great potential\nfor developing generalist agents using LLMs. Nevertheless, GPT-4\u2019s high operational cost remains a\nconcern. Developing smaller models specialized for the web is an interesting future avenue.\n5\nRelated Work\nAutonomous Agents for Web and Mobile Applications. Considerable initiatives have been in-\nvested in automating web navigation, driven by a vision of facilitating effortless human-web interac-\ntion. Yet, previous research has been limited by the types of tasks and websites it can handle, either\nconfined to simplified simulation environments [22, 31, 40], or limited to a narrow set of domains and\nwebsites [39, 40]. Recent studies [5, 21, 35] have utilized similar techniques for mobile applications,\nhowever, these are often simpler and offer fewer functions compared with full-fledged websites. In\ncontrast, MIND2WEB aims to adapt to a realistic web environment, characterized by its high diversity.\n8\nAlso related is the research on web automation systems [1, 19]. These technologies often demand\nprogramming skills, which can make them less accessible to general users. We aim to equip the web\nautomation system with a natural language interface, thereby reducing the entry barrier significantly.\nLarge Language Models. In recent years, there has been a surge in the development and application\nof large language models (LLMs). These models, often encompassing billions of parameters, are\npre-trained on massive corpora of text data [3, 44, 45], enabling them to capture intricate linguistic\npatterns, nuances, and relationships, resulting in unprecedented performance on a wide array of\nNLP tasks. One of the most noteworthy attributes of LLMs is their few-shot learning capability.\nUnlike traditional machine learning models that necessitate extensive labeled data for task-specific\nfine-tuning, LLMs can often perform tasks with minimal task-specific examples. Furthermore, LLMs\nsuch as GPT-3 [4] and PaLM [9] have also demonstrated the ability to do in-context learning, where\nthey can adapt to novel tasks by simply providing context within the input prompt, eliminating the\nneed for explicit retraining. In this work, we explore the use of LLMs to build generalist agent on top\nof MIND2WEB by either tuning medium-sized LMs with only around 1,000 examples, or prompting\nan LLM such as GPT-4, and have observed promising results.\nGrounded Language Understanding. Our work also aligns with the field of grounded language\nunderstanding, which aims to map natural language utterances onto executable plans in a target\nenvironment [13]. Many studies have centered around environments underpinned by a well-structured\nschema or ontology, including relational databases [36, 43] and knowledge bases [12, 42], which may\nnot adequately reflect the more heterogeneous conditions in real-world situations. Our work instead\ngrounds natural language in the noisy and schemaless web environment. Our setting is also connected\nto embodied AI, where an agent, guided by language instructions, carries out tasks in a physical\nenvironment [2, 32, 33]. Nonetheless, existing research primarily focuses on a specific setting (e.g.,\nhousehold environments), limiting their diversity. MIND2WEB provides a unique testbed for studying\na broad range of grounding challenges in real-world environments.\nTool Learning. Recent developments have underscored LLMs\u2019 potential in using a myriad of tools\n(i.e., taking actions) to augment their capacity [23, 27], including search engine, translator, calculator,\netc. Example works include Toolformer [29], ReAct [41], and ToolkenGPT [15]. The creation of\nrecent benchmarks on tool learning [20, 26] further highlights the growing interest in evaluating\nLLMs\u2019 proficiency in tool usage. However, existing research primarily concentrates on short-term\ntool invocation, neglecting long-term planning. MIND2WEB can bridge this lacuna by necessitating\nLLMs to take actions within realistic web-browsing environments that demand prolonged decision-\nmaking sequences. Furthermore, MIND2WEB may stimulate the development of more advanced\ntools based on LLMs that interface the web with natural language. These advanced tools could be\nsubsequently employed by another LLM for more challenging problem-solving tasks [11, 30].\n6\nLimitations and Potential Societal Impact\nMIND2WEB is designed to facilitate the development and evaluation of generalist agents for the web.\nSuch agents hold great potential for making the web more accessible and easy to use, especially for\nindividuals who are less familiar with information technology or have disabilities and may struggle\nto navigate through complex web apps and get overwhelmed by the options available. However, there\nare still potential concerns and limitations regarding the current data collection, system design and\nsafety for deployment in real world.\nDiversity and Representation in Data Collection. Although we strive to choose representative\nwebsites covering diverse domains, the present selection predominantly comprises English-language\nwebsites primarily used in the U.S. Meanwhile, all our annotators are sourced through the Amazon\nMTurk platform, which might be biased towards a group that is more proficient in web use. Therefore,\nthe tasks and websites embodied in our dataset may represent only a subset of all potential tasks\nthat can be performed on the web. Bearing this limitation in mind, the design of MIND2WEB and\nour data collection protocol allow for easy expansion to encompass more tasks and websites. The\ninclusion of additional websites, potentially from different countries and languages, and tasks from\nmore diverse demographics, such as individuals from different age groups, those traditionally facing\nweb accessibility challenges, and professionals from specific domains like software development,\nresearch, law, and more, present exciting directions for future development.\n9\nUse of Multimodal Information. Our current approach, MINDACT, models the web environment\nusing only textual context from webpage snapshots. Nevertheless, crucial information can also be\ngleaned from the visual representation of a rendered webpage. While not currently utilized, we have\nincluded complete webpage snapshots in MIND2WEB, enabling rendering of the webpage for visual\ninterpretation. The use of this multimodal information will be a viable prospect for improving model\nperformance.\nModeling of Interaction Dynamics. In MINDACT, we encode each webpage independently at every\nstep, with only the previous actions provided as historical context. However, the changes of the\nweb environment could also provide significant cues for task completion, such as the appearance\nof a dropdown menu following a button click. Exploring effective ways to model such dynamic\nenvironment transformations during interaction could be an essential aspect for developing robust\nweb agents.\nHuman-Agent Interaction. In the current design of MIND2WEB, the user provides a single\ndescription of the task goal up front, and the agent carries out the task from start to finish. In\nreal-world settings, the user may wish to adjust or add task requirements in the middle, or the agent\nmight seek user confirmation for more accurate task understanding. Extending Mind2Web to an\ninteractive or conversational setting, thereby allowing diverse forms of human-agent interactions,\ncould be an interesting future direction.\nEvaluation with Offline/Online Environments. Following recent works [5, 35], we evaluate the\nsystem with cached offline environments, which allows us to test using snapshots of complex real-\nworld websites. However, a downside to this is that the task will fail immediately if an action was not\ncached during data collection, potentially leading to false negatives due to the existence of multiple\npaths for completing the same task. As described in Appendix C.1, we normalize the actions to\naddress equivalent elements within the same page. In addition, we include complete network traffic in\nthe dataset, presenting possibilities for future research to enable some degree of replay and exploration\nwithin the cached environment. Given that MIND2WEB faithfully replicates real-world webpages,\nsystems trained on the dataset should be readily transferable to live websites. Conducting end-to-end\nlive evaluation on real websites with human assistance is a very promising direction that is worth\nexploration.\nSafety in Deployment. While the development of general-purpose web agents holds great potential\nto enhance efficiency, optimize user experiences, and promote web accessibility universally, the\naccompanying safety considerations for real-world deployment cannot be ignored. These include\nhow to effectively manage sensitive actions like financial transactions, enhancing transparency and\ninterpretability, and keeping users in control during task execution. Additionally, there is the risk\nof these agents possessing the capability to breach existing security measures such as CAPTCHA\nand being exploited for malicious activities, such as disseminating false information. Therefore, it is\nalso important for cybersecurity research to consider these potential uses and develop preemptive\nprotective measures.\n7\nConclusion\nIn this work, we introduced MIND2WEB, the first dataset for developing and evaluating generalist\nagents for the web. We also proposed MINDACT, an agent that leverages the power of (large) language\nmodels for effectively tackling this task. Our work opens up a wide range of promising future\ndirections, including integrating multi-modal information, reinforcement learning with feedback\nfrom real websites, and specialized LMs for web understanding and action taking. We hope that\nMIND2WEB will serve as a valuable platform for the research community to advance towards\ngeneralist agents for the web.\nAcknowledgements\nThe authors would thank colleagues from the OSU NLP group for constructive feedback and all\ncontributors from the Amazon Mechanical Turk platform who participated in our study and assisted\nin data collection. This research was sponsored in part by NSF OAC 2112606, NSF CAREER\n#1942980, ARL W911NF2220144 and Ohio Supercomputer Center [6]. The views and conclusions\ncontained herein are those of the authors and should not be interpreted as representing the official\n10\npolicies, either expressed or implied, of the U.S. government. The U.S. Government is authorized\nto reproduce and distribute reprints for Government purposes notwithstanding any copyright notice\nherein.\nReferences\n[1] Puppeteer headless chrome node.js api.\nhttps://github.com/puppeteer/puppeteer,\n2021.\n[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho,\nJasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle\nJeffrey, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nKuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nand Mengyuan Yan. Do as I can, not as I say: Grounding language in robotic affordances.\nCoRR, abs/2204.01691, 2022. doi: 10.48550/arXiv.2204.01691. URL https://doi.org/10.\n48550/arXiv.2204.01691.\n[3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von\nArx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Bryn-\njolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen,\nKathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha,\nTatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,\nGeoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR,\nabs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[5] Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A. Plum-\nmer. A dataset for interactive vision-language navigation with unknown command feasibility.\nIn European Conference on Computer Vision, 2022.\n[6] Ohio Supercomputer Center. Ohio supercomputer center, 1987. URL http://osc.edu/ark:\n/19495/f5s1ph73.\n[7] Radoslav Chakarov. How many websites are there? how many are active in 2023? https:\n//webtribunal.net/blog/how-many-websites/. 2023.\n[8] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer\nopen-domain questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://aclanthology.org/P17-1171.\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\n11\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,\nAnselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\nmodeling with pathways, 2023. URL http://jmlr.org/papers/v24/22-1144.html.\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and\nJason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022. doi:\n10.48550/arXiv.2210.11416. URL https://doi.org/10.48550/arXiv.2210.11416.\n[11] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang.\nOpenagi: When LLM meets domain experts. CoRR, abs/2304.04370, 2023. doi: 10.48550/\narXiv.2304.04370. URL https://doi.org/10.48550/arXiv.2304.04370.\n[12] Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su.\nBeyond I.I.D.: three levels of generalization for question answering on knowledge bases. In\nJure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia, editors, WWW \u201921: The\nWeb Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, pages 3477\u20133488.\nACM / IW3C2, 2021. doi: 10.1145/3442381.3449992. URL https://doi.org/10.1145/\n3442381.3449992.\n[13] Yu Gu, Xiang Deng, and Yu Su. Don\u2019t generate, discriminate: A proposal for grounding\nlanguage models to real-world environments. CoRR, abs/2212.09736, 2022. doi: 10.48550/\narXiv.2212.09736. URL https://doi.org/10.48550/arXiv.2212.09736.\n[14] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowd-\nhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large\nlanguage models, 2023.\n[15] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen\nlanguage models with massive tools via tool embeddings. CoRR, abs/2305.11554, 2023. doi:\n10.48550/arXiv.2305.11554. URL https://doi.org/10.48550/arXiv.2305.11554.\n[16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https:\n//openreview.net/forum?id=XPZIaotutsD.\n[17] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Struct-\ngpt: A general framework for large language model to reason over structured data. CoRR,\nabs/2305.09645, 2023. doi: 10.48550/arXiv.2305.09645. URL https://doi.org/10.48550/\narXiv.2305.09645.\n[18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 6769\u20136781, Online, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.\nemnlp-main.550.\n[19] Gilly Leshed, Eben M. Haber, Tara Matthews, and Tessa Lau. CoScripter: Automating &\nsharing how-to knowledge in the enterprise. In Proceedings of the SIGCHI Conference on\nHuman Factors in Computing Systems, pages 1719\u20131728, Florence Italy, April 2008. ACM.\nISBN 978-1-60558-011-1. doi: 10.1145/1357054.1357323.\n12\n[20] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin\nLi. Api-bank: A benchmark for tool-augmented llms. CoRR, abs/2304.08244, 2023. doi:\n10.48550/arXiv.2304.08244. URL https://doi.org/10.48550/arXiv.2304.08244.\n[21] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language\ninstructions to mobile UI action sequences. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\nand Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8198\u20138210. Association\nfor Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.729. URL https:\n//doi.org/10.18653/v1/2020.acl-main.729.\n[22] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement\nlearning on web interfaces using workflow-guided exploration. In 6th International Conference\non Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/\nforum?id=ryTp3f-0-.\n[23] Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru,\nRoberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. CoRR,\nabs/2302.07842, 2023. doi: 10.48550/arXiv.2302.07842. URL https://doi.org/10.48550/\narXiv.2302.07842.\n[24] OpenAI. Chatgpt plugins. https://openai.com/blog/chatgpt-plugins. 2023.\n[25] OpenAI. Gpt-4 technical report, 2023.\n[26] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language\nmodel connected with massive apis. CoRR, abs/2305.15334, 2023. doi: 10.48550/arXiv.2305.\n15334. URL https://doi.org/10.48550/arXiv.2305.15334.\n[27] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,\nYufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng\nQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining\nYe, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with\nfoundation models. CoRR, abs/2304.08354, 2023. doi: 10.48550/arXiv.2304.08354. URL\nhttps://doi.org/10.48550/arXiv.2304.08354.\n[28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\nabs/1908.10084.\n[29] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. CoRR, abs/2302.04761, 2023. doi: 10.48550/arXiv.2302.04761. URL\nhttps://doi.org/10.48550/arXiv.2302.04761.\n[30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\nginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,\n2023. doi: 10.48550/arXiv.2303.17580. URL https://doi.org/10.48550/arXiv.2303.\n17580.\n[31] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of Bits:\nAn Open-Domain Platform for Web-Based Agents. In Proceedings of the 34th International\nConference on Machine Learning, pages 3135\u20133144. PMLR, July 2017.\n[32] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh\nMottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded\ninstructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10737\u201310746.\n13\nComputer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01075. URL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_\nA_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_\nCVPR_2020_paper.html.\n[33] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-shot grounded planning for embodied agents with large language models.\nCoRR, abs/2212.04088, 2022. doi: 10.48550/arXiv.2212.04088. URL https://doi.org/10.\n48550/arXiv.2212.04088.\n[34] Yu Su, Ahmed Hassan Awadallah, Madian Khabsa, Patrick Pantel, Michael Gamon, and Mark J.\nEncarnaci\u00f3n. Building natural language interfaces to web apis. In Ee-Peng Lim, Marianne\nWinslett, Mark Sanderson, Ada Wai-Chee Fu, Jimeng Sun, J. Shane Culpepper, Eric Lo, Joyce C.\nHo, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Aixin Sun, Vincent S. Tseng,\nand Chenliang Li, editors, Proceedings of the 2017 ACM on Conference on Information and\nKnowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017, pages 177\u2013186.\nACM, 2017. doi: 10.1145/3132847.3133009. URL https://doi.org/10.1145/3132847.\n3133009.\n[35] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. META-GUI:\nTowards Multi-modal Conversational Agents on Mobile GUI, November 2022.\n[36] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. RAT-\nSQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 7567\u20137578,\nOnline, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.677.\n[37] Kyle Williams, Seyyed Hadi Hashemi, and Imed Zitouni. Automatic task completion flows\nfrom web apis.\nIn Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek,\nJian-Yun Nie, and Falk Scholer, editors, Proceedings of the 42nd International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France,\nJuly 21-25, 2019, pages 1009\u20131012. ACM, 2019. doi: 10.1145/3331184.3331318. URL\nhttps://doi.org/10.1145/3331184.3331318.\n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-\nart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6.\n[39] Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James A. Landay, and\nMonica S. Lam. Grounding open-domain instructions to automate web support tasks. In North\nAmerican Chapter of the Association for Computational Linguistics, 2021.\n[40] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. WebShop: Towards Scalable\nReal-World Web Interaction with Grounded Language Agents. July 2022. doi: 10.48550/arXiv.\n2207.01206.\n[41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. CoRR, abs/2210.03629, 2022.\ndoi: 10.48550/arXiv.2210.03629. URL https://doi.org/10.48550/arXiv.2210.03629.\n[42] Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic Parsing via Staged\nQuery Graph Generation: Question Answering with Knowledge Base. In Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321\u20131331,\nBeijing, China, 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1128.\n14\n[43] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene\nLi, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 3911\u20133921. Association for Computational Linguistics,\n2018. doi: 10.18653/v1/d18-1425. URL https://doi.org/10.18653/v1/d18-1425.\n[44] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian\nMin, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng\nChen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie,\nand Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023. doi:\n10.48550/ARXIV.2303.18223. URL https://doi.org/10.48550/arXiv.2303.18223.\n[45] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben\nYan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian\nPei, Philip S. Yu, and Lichao Sun. A comprehensive survey on pretrained foundation models: A\nhistory from BERT to chatgpt. CoRR, abs/2302.09419, 2023. doi: 10.48550/ARXIV.2302.09419.\nURL https://doi.org/10.48550/arXiv.2302.09419.\n15\nA\nOverview\nOur supplementary includes the following sections:\n\u2022 Section B: Data Collection Details. Details for crowsourcing and implementation details\nfor the three data collection phases: task proposal, task demonstration, and task verification.\n\u2022 Section C: Experiment Details. Details for evaluation and model implementation.\n\u2022 Section D: Additional Results. Results for additional auxiliary experiments.\nFollowing NeurIPS Dataset and Benchmark track guidelines, we have shared the following artifacts:\nArtifcat\nLink\nLicense\nHomepage\nhttps://osu-nlp-group.github.io/Mind2Web/\n-\nCode Repository\nhttps://github.com/OSU-NLP-Group/Mind2Web\nMIT License\nTraining Data\nhttps://huggingface.co/datasets/osunlp/Mind2Web\nCC BY 4.0\nTest Data\nhttps://shorturl.at/iGI45 (password \u201cmind2web\u201d)\nCC BY 4.0\nThe authors and the OSU NLP group are committed to ensuring its regular upkeep and updates.\n16\nTable 3: Prompt for ChatGPT to generate seed tasks to inspire the annotators during task proposal.\nPrompt\nWe are testing the website\nhttps://aa.com | American Airlines - Airline tickets and low fares at\naa.com\nBook low fares to destinations around the world and find the latest deals on\nairline tickets, hotels, car rentals and vacations at aa.com. As an AAdantage\nmember you earn miles on every trip and everyday spend.\nWe want to test practical daily tasks that a user would do on the web-\nsite.\nCome up with a list of 5 example tasks and try to cover different cases.\nRequirements:\n- Each example should be a single sentence and not just click one of the\nelements.\n- Don\u2019t give step-by-step instructions or directly mention the element to interact.\n- Describe the goal of the task and provide concrete information or constraints.\nUse mock-up information (identifier, number, personal information, name, date,\nattributes, constraints, etc.) to make the task more specific and realistic.\nSample Responses\nRetrieve the confirmation number for a reservation made under the name jane\nsmith departing from dallas (dfw) to miami (mia) on january 20th.\nFind and book a round-trip flight from new york to london departing on december\n15th and returning on december 22nd for under $800.\nFind the lowest fare for a one-way flight from new york city (jfk) to los angeles\n(lax) leaving on august 15th and returning on august 20th, with no layovers\nexceeding 2 hours.\nRent a car in honolulu for one week starting on august 1st that can fit at least\nfour passengers and has gps navigation included.\nCancel a car rental reservation for a compact car in orlando, florida that was\nscheduled to be picked up on may 25th at 5 pm without incurring any cancellation\nfees.\nB\nData Collection Details\nB.1\nCrowdsourcing Details\nIn this study, we employ annotators from the Amazon Mechanical Turk (mturk) platform. The\nannotators are required to have a minimum of 1,000 approved HITs with an approval rate exceeding\n98% on the platform. We design the compensation with an estimated hourly rate of $10.10 to\nrespect the minimum wage guidelines in Ohio, United States. Every worker passing our qualification\nreceives a bonus, and we pay $0.80 for each approved final task. We do not collect any identifiable\nprivate information during the study, and explicitly instruct the annotators to refrain from entering\npersonal or sensitive data into the system. Annotators engage with our annotation tool only within a\nsecure, remote sandbox environment, posing no foreseeable harm. The study complies with the IRB\nexemption criteria, per the Office of Responsible Research Practices at The Ohio State University.\nAll annotators are presented with a consent form, to which they must agree before participating in\nthe study. To prepare the workers for the task, we provide a comprehensive training document and a\nvideo tutorial, followed by a qualification assessment comprising a questionnaire and a series of test\ndemonstrations using our tool. It is noteworthy that the task is divided into two phases: task proposal\nand task demonstration. The proposal phase comes with a nominal reward, with the majority of the\ncompensation dispensed upon successful completion of the demonstration.\nQuality and diversity are ensured through a two-stage review process. The first author reviews all\ntasks after task proposal and manually select the tasks for demonstration. After task demonstration, a\nthorough final verification of all collected data is conducted by all authors to authenticate the tasks\nand recorded actions. Each demonstration is first verified by one of the authors, and uncertain ones\nare further verified by the first author to reach consensus.\n17\nFigure 7: Illustration of our annotation tool, which consists of two side-by-side windows. On the left\nwe provide a dialogue window for the user to control the tool and select operations to take. On the\nright we provide the browser window for the user to interact with and select web elements.\nSelect Website \nand Task\nExplore and \nPrepare the \nWebsite\nTask \nDemonstration\nSelect the \nOperation\nConfirm the\nTask\nSelect an element\nComplete Demonstration\nFigure 8: The overall procedure for task demonstration.\nB.2\nTask Proposal\nWe use ChatGPT to generate sample tasks to provide inspiration to the annotators, and the prompt\nused is shown in Table 3. For each HIT, we ask the annotator to select a website of their interest\nfirst. Following this, we present them with ten sample tasks produced by ChatGPT, and request\nthem to propose a maximum of five additional tasks. The annotator is instructed not to directly copy\nthe sample tasks. We manually evaluate all submitted tasks and reject those that demonstrate low\nquality or are too similar to previously accepted tasks. We set a nominal reward of $0.05 for each\ntask proposal HIT, and the annotator will receive it no matter whether the tasks are accepted or not.\nFor accepted ones, the annotator will receive the full reward of $0.80 on successful demonstration of\nthe task. Once we have collected a total of around 20 tasks for a specific website, we desist from\nshowing it to the user, aiming for a balanced distribution among websites and increased task diversity.\nB.3\nTask Demonstration\nWe develop a dedicated annotation tool for task demonstration using Playwright,3 which allows us to\ninteract with the browser and record user actions. As shown in Figure 7. the tool is composed of two\nwindows. The dialogue window on the left serves as the annotator\u2019s control panel for guiding the\ninteraction flow and choosing operations. The browser window on the right is where the annotator\nnavigates the website and selects elements for interaction. Figure 8 shows the overall procedure\nfor task demonstration. The annotator starts by selecting the website and task to be demonstrated.\nOnce selected, the tool will bring up the website in the browser. The annotator is then instructed\nto explore the website and practice the task. To collect clean actions during actual demonstration,\nthe workers are asked to close pop-up windows during exploration. We also provide anonymous\naccounts for the workers to use so that no private information is entered. The exploration stage is\n3https://playwright.dev/\n18\nFigure 9: Dialogue window for the Type operation.\nFigure 10:\nDialogue window for the\nSelect Option operation.\nnot recorded and primarily serves to familiarize the annotator with the website and task, as well as\nto prepare the website to prevent future pop-ups, thereby ensuring a clean, streamlined set of final\nrecorded actions. After exploration, the annotator is directed to return to the homepage and reset any\naltered values, allowing us to begin the demonstration in a fresh state. During the demonstration,\nthe annotator will illustrate how to accomplish the task step-by-step using both the browser and the\ndialogue window. To ensure a clean set of annotated actions, annotators are restricted from directly\nengaging with the browser during the demonstration phase. Instead, we divide each action step into\ntwo stages: Element selection and operation selection. At each step, the annotator first selects the\ntarget element by clicking it in the browser. We will highlight the selected element in the browser\nwindow but block the actual click event. The annotator is then prompted to select the operation to\nperform within the dialogue window, which is then carried out by the annotation tool in the browser.\nWe provide 6 operations: Click, Type, Hover, Press Enter, Click (Fake) and Ignore. For the\nType operation, the annotator is additionally required to supply the value as shown in Figure 9. If\nthe chosen element is a select HTML element, and the annotator opts for Click, it translates to a\nSelect Option operation and we will prompt the annotator to select one of the options as shown\nin Figure 10. To avoid ambiguity, the Click, Hover and Press Enter operations are all mapped\nto Click in the final dataset. Click (Fake) is a special operation. It will be recorded the same as\na normal Click but will not get executed in the browser. This is designed for safeguarding against\nstate-changing actions (i.e., actions that produce side effects to the world), such as posting a comment\nor scheduling an appointment, since it will interfere with other real users of the website. In practice,\nonce a model predicts Click (Fake), it may prompt the user for confirmation before executing such\nstate-changing actions. Finally, the annotator can also choose Ignore in case they select a wrong\nelement. Once all the actions have been annotated, the annotator can choose to complete the task.\nThey will then be asked to confirm the task description again and make any necessary modifications.\nPop-ups and CAPTCHAs. In this study, we emphasize on clean and direct task execution actions,\nintentionally omitting extraneous steps like pop-ups and CAPTCHAs that might introduce ambiguity\nin evaluation. We carefully select only those websites that pose no access issues when used with our\ntool. Before recording the task demonstration, annotators are requested to familiarize themselves with\nthe website, and preemptively close pop-up windows and clear CAPTCHAs to avoid their recurrence\nduring the actual demonstration. Annotators are further guided not to engage in extra steps such as\nclosing ads unless necessary during the task demonstration. In the final task verification, we revisit\nthe actions and filter out those unrelated to direct task execution. At the same time, we acknowledge\nthat these instances constitute a significant aspect of the dynamic web environment in the real world.\nEnhancing systems to robustly tackle such scenarios on-the-go could form an interesting avenue for\nfuture research.\nMitigating Disruptions on the Websites. The annotator are advised against actions that could\npotentially interfere the normal operation of the website. To handle tasks such as scheduling\nappointments, we introduce a Click (Fake) operation that annotators can utilize to indicate the\naction without actually executing it on the website.\nB.4\nTask Verification\nAll collected data undergoes an additional verification process conducted by the authors, as demon-\nstrated in Figure 11. The verification interface is shown in Figure 11. This verification consists of\nthree tasks. Firstly, we evaluate whether a task should be discarded due to its low quality. Secondly,\nwe examine each step to determine if any action should be discarded. This includes reviewing the\n19\nFigure 11: Illustration of our verification tool.\ninitial and final actions of the task, and excluding any additional actions (e.g., closing ads) that are\nnot outlined in the task description to ensure consistency across task annotations. Finally, we verify\nthe task description to confirm that all actions are accurately represented and make modifications if\nnecessary. If there is uncertainty regarding any action, the verifier can opt for the \u2018unsure\u2019 option,\nprompting a re-evaluation by the first author.\nC\nExperiment Details\nC.1\nEvaluation\nOne complication that arises during evaluation on real-world websites is that multiple elements on a\nwebpage may induce the same effect. For instance, a button might house a text span within it, both\nof which, when clicked, yield identical results. To enhance the robustness of our evaluation, we\nemploy heuristics to detect elements equivalent to the ground truth. We first examine the ancestors\nof the labeled element to identify potential higher-level elements acceptable for the current action.\nWe employ a straightforward heuristic that locates the nearest clickable element to the ground\ntruth, including itself. After identifying the top-level acceptable element, we include all its visible\ndescendants that are located within its post-rendering bounding box as acceptable as well. Manual\nchecking on 100 instances where the heuristic identifies a top-level element other than the ground\ntruth confirms the validity of the approach. For both training and evaluation stages, all acceptable\nelements are considered positive.\nC.2\nModel Implementation Details\nCandidate Generation. We use the Cross-Encoder implementation from Sentence-Transformers 4\nand use DeBERTa as the backbone model. More specifically, we use DeBERTa-v3-base 5 for our\nexperiments.\nAction Prediction. We use the Seq2Seq model implementation from Transformers [38]. We\nexperiment with the base 6, large 7 and xl 8 versions of Flan-T5 [10].\n4https://www.sbert.net/examples/applications/cross-encoder/README.html\n5https://huggingface.co/microsoft/deberta-v3-base\n6https://huggingface.co/google/flan-t5-base\n7https://huggingface.co/google/flan-t5-large\n8https://huggingface.co/google/flan-t5-xl\n20\nTable 4: Hyperparameters used in experiments.\nMethod\nModel\nHyperparamerters\nCandidate Generation\ndeberta-v3-base\nbatch_size:32, epoch:5,\nlearning_rate:3e\u22125\nAction Prediction\nGeneration\nflan-t5-base\nbatch_size:32, epoch:5,\nlearning_rate:5e\u22125\nMINDACT\nflan-t5-base,\nflan-t5-large,\nflan-t5-xl\nbatch_size:32, epoch:5,\nlearning_rate:5e\u22125\ngpt-3.5-turbo,\ngpt-4\ntemperature:0, # demonstrations:3\nTable 5: Step Success Rate for Flan-T5 models with different groups of options. Here we shown\nmean and standard deviation of 5 runs with different random seeds.\nCross-Task\nCross-Website\nCross-Domain\nFlan-T5B\n41.5\u00b10.7\n30.0\u00b10.8\n31.3\u00b10.5\nFlan-T5L\n49.9\u00b10.2\n35.7\u00b10.5\n36.7\u00b10.3\nFlan-T5XL\n51.9\u00b10.8\n39.5\u00b10.2\n39.6\u00b10.2\nLLM In-context Learning. We use the OpenAI API for in-context learning with LLMs. We\nexperiment with two versions of GPT models: gpt-3.5-turbo and gpt-4. We include three\ndemonstration examples for in-context learning. The complete prompt is shown in Table 8.\nTraining Details. The flan-t5-xl and flan-t5-large models are trained on servers with 4*A100\n80GB cards provided by Ohio Supercomputer Center [6]. All other models are trained with single\nA6000 48GB cards.\nPlease see Table 4 for all hyperparameters used in our experiments.\nD\nAdditional Results\nD.1\nEffect of Random Grouping Elements for Action Prediction\nFor both training and inference, we shuffle the elements in the webpage and randomly group them\ninto multi-choice questions. The model might give different predictions when presented with different\nsets of choices, and leads to slightly different final evaluation scores. Here we show the average and\nstandard deviation of 5 runs with different random seeds to show the effect of random grouping. As\nwe can see from Table 5, the selection of choices only lead to small changes in overall performance\nwith standard deviation less than 1 for all runs.\nD.2\nZero-shot Results for Flan-T5XL\nSince Flan-T5 is tuned with multi-choice format, it can also do element selection in zero-shot.\nHowever, as we can see from Table 6, while the model still gets some elements correct, it is much\nlower compared to the fine-tuned model, and 3-shot GPT 3.5/4. This is expected, since Flan-T5 is\nnot tuned for HTML and coding related tasks.\nTable 6: Zero-shot element selection results for Flan-T5XL compared with the fine-tuned counterpart.\nCross-Task\nCross-Website\nCross-Domain\nFlan-T5XL Zero-Shot\n10.8\n7.8\n11.7\nFlan-T5XL Fine-Tuned\n52.0\n38.9\n39.6\n21\nTable 7: Step Success Rate for all methods on the 50 tasks subsets we used to evaluate GPT-4.\nNumbers in parentheses are the results on the full test set (same as Table 2)\nCross-Task\nCross-Website\nCross-Domain\nFlan-T5B\n43.3 (41.0)\n25.3 (29.5)\n28.1 (31.6)\nFlan-T5L\n48.1 (50.3)\n30.8 (35.3)\n27.6 (37.3)\nFlan-T5XL\n47.9 (52.0)\n33.3 (38.9)\n34.6 (39.6)\nGPT-3.5\n15.2 (17.4)\n15.1 (16.2)\n16.7 (18.6)\nGPT-4\n36.2\n30.1\n26.4\nD.3\nResults on the 50 task subsets\nDue to budget constraint, we only run GPT-4 on 50 tasks for each setting. Here we show the step\nsuccess rate results for other methods on the same 50 examples that GPT-4 is tested on, As we can\nsee from Table 7, the results on the 50 tasks subsets are consistent with the results on the respective\nfull test set, and the relative performance across methods and splits remains the same.\n22\nTable 8: Prompt for action prediction in MINDACT with GPT models.\nOnly part of the HTML snippet is shown here to save space.\nRole\nContent\nsystem\nYou are a helpful assistant that is great at website design, navigation, and executing\ntasks for the user\nuser\n```\n<html> <div> <div> <a tock home page /> <button id=0 book a reservation. toggle\nopen> <span> Book a reservation </span> </button> <button book a reservation.\ntoggle open> </button> </div> <div> <select id=1 type> <option reservations true>\nDine in </option> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Check for pickup restaurant available in Boston, NY on March 18, 5pm with\njust one guest\nPrevious actions:\nNone\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A. \u2018None of the above\u2019):\nA. None of the above\nB. <button id=0 book a reservation. toggle open> <span> Book a\nC. <select id=1 type> <option reservations true> Dine in </option> <option\nD. <div id=2> <p> Celebrating and supporting leading women shaking up\nassistant\nAnswer: C.\nAction: SELECT\nValue: Pickup\nuser\n```\n<html> <div> <main main> <section tabpanel> <div> <ul tablist> <li tab heading\nlevel 3 search and> </li> <li id=0 tab heading level 3 search and> <span> Hotel\n</span> </li> <li tab heading level 3 search and> </li> <li tab heading level 3\nsearch and> </li> </ul> <div tabpanel> <div id=1> <div> <span> Dates* </span>\n<button button clear dates /> </div> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Compare the fare types to book a 1-adult ticket from Springfiels, IL to Austin,\nTX for April 29th 2023\nPrevious actions:\n[combobox] Enter your departing city, airport name, or airpor...\n-> TYPE:\nSPRINGFIELD\n[button] Springfield, IL, US (SPI) -> CLICK\n[combobox] Enter your destination city, airport name, or airp... -> TYPE: AUSTIN\n[button] Austin, TX, US (AUS) -> CLICK\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A. \u2018None of the above\u2019):\nA. None of the above\nB. <li id=0 tab heading level 3 search and> <span> Hotel\nC. <div id=1> <div> <span> Dates* </span> <button button clear dates\nD. <ul id=2> <a mobile tools> </a> <a open united\u2019s tiktok\nassistant\nAnswer: A.\nContinued on next page\n23\nTable 8 \u2013 continued from previous page\nRole\nContent\nuser\n```\n<html> <div> <nav main menu> <ul> <li> <div button> Car Sales </div> <div\nid=0> <div> <div> <div> Buy A Car </div> <div> Plan Your Purchase </div>\n</div> <div> <h4> Its Tax Refund Time. Treat Yourself to an Upgrade. </h4> <p>\nWith a variety of options, invest your refund in what you really want - a quality,\nused vehicle from Enterprise. </p> ... </html>\n```\nBased on the HTML webpage above, try to complete the following task:\nTask: Find a mini van at Brooklyn City from April 5th to April 8th for a 22 year\nold renter.\nPrevious actions:\n[searchbox] Pick-up & Return Location (ZIP, City or Airport) (... -> TYPE:\nBrooklyn\n[option] Brooklyn, NY, US Select -> CLICK\nWhat should be the next action? Please select from the following choices (If the\ncorrect action is not in the page above, please select A. \u2018None of the above\u2019):\nA. None of the above\nB. <div id=0> <div> <div> <div> Buy A Car </div> <div>\nC. <div id=1> Enterprise Fleet Management </div>\nD. <button id=2 selected pick-up date 03/19/2023> <span> <span> 19 </span>\nassistant\nAnswer: D.\nAction: CLICK\n24\n"
  },
  {
    "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
    "link": "https://arxiv.org/pdf/2306.05685.pdf",
    "upvote": "14",
    "text": "Judging LLM-as-a-Judge\nwith MT-Bench and Chatbot Arena\nLianmin Zheng1\u2217\nWei-Lin Chiang1\u2217\nYing Sheng4\u2217\nSiyuan Zhuang1\nZhanghao Wu1\nYonghao Zhuang3\nZi Lin2\nZhuohan Li1\nDacheng Li13\nEric P. Xing35\nHao Zhang12\nJoseph E. Gonzalez1\nIon Stoica1\n1 UC Berkeley\n2 UC San Diego\n3 Carnegie Mellon University\n4 Stanford\n5 MBZUAI\nAbstract\nEvaluating large language model (LLM) based chat assistants is challenging due to\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring\nhuman preferences. To address this, we explore using strong LLMs as judges to\nevaluate these models on more open-ended questions. We examine the usage and\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\nof them. We then verify the agreement between LLM judges and human preferences\nby introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\nArena, a crowdsourced battle platform. Our results reveal that strong LLM judges\nlike GPT-4 can match both controlled and crowdsourced human preferences well,\nachieving over 80% agreement, the same level of agreement between humans.\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human\npreferences, which are otherwise very expensive to obtain. Additionally, we show\nour benchmark and traditional benchmarks complement each other by evaluating\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\nand 30K conversations with human preferences are publicly available at https:\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n1\nIntroduction\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\ninstruction following and conversational abilities [31, 2, 30, 8, 52, 48, 14]. Once aligned with\nhumans, these chat models are strongly preferred by human users over the original, unaligned models\non which they are built. However, the heightened user preference does not always correspond to\nimproved scores on traditional LLM benchmarks \u2013 benchmarks like MMLU [19] and HELM [24]\ncannot effectively tell the difference between these aligned models and the base models. This\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the\nusefulness of chatbots and the criteria adopted by conventional benchmarks.\nWe argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs\u2019\ncore capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions),\nwithout adequately assessing its alignment with human preference in open-ended tasks, such as the\nability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show\n\u2217Joint first authors. This paper is an extended version of our earlier blog post [8].\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2306.05685v4  [cs.CL]  24 Dec 2023\nQuestion: If the FED buys bonds in the secondary market\n(A) the money supply will increase. (B) the money supply will decrease. (C) the money supply will not be \naffected. (D) the discount rate would be affected. Output the answer only.\nAssistant B:\n(A) the money supply will increase.\nAssistant A:\nThe Federal Reserve buys \nbonds in the secondary \nmarket to increase the \nmoney supply.\nFollow-up Question: How does it affect my daily life? Give 3 examples.\nAssistant B:\nWhen the Federal Reserve buys bonds in the secondary market, it can have \nseveral effects on the economy that can impact your daily life. Here are \nthree examples:\n1. Interest rates: The Federal Reserve's actions in the bond market can \ninfluence interest rates, which in turn can affect the cost of borrowing \nmoney. For example, if the Federal Reserve buys bonds and drives up demand \nfor them, interest rates on things like mortgages, car loans, and credit \ncards may go up, making it more expensive to borrow money.\n2. Inflation: When the money supply increases, it can lead to inflation, \nwhich is the general rise in prices of goods and services. This can impact \nyour daily life by making it more expensive to buy things like groceries, \ngas, and other necessities.\n3. Employment: The Federal Reserve's actions in the bond market can also \nhave an impact on employment. For example, if the Federal Reserve's \nactions lead to economic growth and increased demand for goods and \nservices, it can create more job opportunities and boost employment.\nAssistant A:\nThe Federal Reserve buys \nbonds in the secondary \nmarket to increase the \nmoney supply. This \naffects your daily life \nby:\n1. Increasing the money \nsupply.\n2. Increasing the money \nsupply and lowering the \ndiscount rate.\n3. Increasing the money \nsupply and lowering the \ndiscount rate and \nincreasing the money \nsupply.\nGPT-4 Judgment:\nAssistant A provided an incorrect response to the user's question about how the Federal Reserve \nbuying bonds in the secondary market affects daily life. The answer given is repetitive and lacks \nclear examples of how the action impacts daily life.\nOn the other hand, Assistant B provided a relevant and accurate response to the user's question about \nthe Federal Reserve buying bonds. The answer includes three clear examples of how the action impacts \ndaily life, such as interest rates, inflation, and employment.\nAssistant A should improve by providing relevant, clear, and accurate examples to address the user's \nquestion. Assistant B's response is more helpful, relevant, and detailed.\nFigure 1: Multi-turn dialogues between a user and two AI assistants\u2014LLaMA-13B (Assistant A)\nand Vicuna-13B (Assistant B)\u2014initiated by a question from the MMLU benchmark and a follow-up\ninstruction. GPT-4 is then presented with the context to determine which assistant answers better.\nconversation histories with two models on an MMLU question in Figure 1. The two models are\nLLaMA-13B [39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned\nmodel from LLaMA-13B on high-quality conversations (the training details are in Appendix E).\nDespite the base LLaMA models showing competitive performance on conventional benchmarks\n(Table 8), its answers to open-ended questions are often not preferred by humans. This misalignment\nof conventional benchmarks underscores the core problem driving this paper: the need for a robust\nand scalable automated method to evaluate LLM alignment with human preferences.\nTo study this, we introduce two benchmarks with human ratings as the primary evaluation metric:\nMT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot\u2019s\nmulti-turn conversational and instruction-following ability \u2013 two critical elements for human prefer-\nence. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities,\nsuch as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform fea-\nturing anonymous battles between chatbots in real-world scenarios \u2013 Users engage in conversations\nwith two chatbots at the same time and rate their responses based on personal preferences.\nWhile human evaluation is the gold standard for assessing human preferences, it is exceptionally slow\nand costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4,\nas a surrogate for humans. Because these models are often trained with RLHF, they already exhibit\nstrong human alignment. We call this approach \u201cLLM-as-a-judge\u201d. This approach has been tried in\nour earlier blog post [8] and other concurrent or follow-up work [5, 29, 14, 12, 52, 18, 33, 40, 7, 43].\nHowever, there has not been a systematic study of this approach.\nIn this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of\nhuman evaluation. We examine several potential limitations of the LLM-as-a-judge approach\nincluding position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We\nshow that some of the biases are minor or can be mitigated. Once addressed, our results from 3K\ncontrolled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match\n2\nhuman evaluations at an agreement rate exceeding 80%, achieving the same level of human-human\nagreement (\u00a74.2, Table 4). Consequently, this suggests LLM-as-a-judge is a scalable method to\nswiftly evaluate human preference, serving as a promising alternative to traditional human evaluations.\nThis paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human\npreference datasets with high-quality questions and diverse user interactions from MT-bench and\nChatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future\nLLM benchmarks: by combining the existing capability-based benchmarks and the new preference-\nbased benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core\ncapabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert\nvotes, and 30K conversations with human preferences for future study.\nTable 1: Sample multi-turn questions in MT-bench.\nCategory\nSample Questions\nWriting\n1st Turn\nCompose an engaging travel blog post about a recent trip to Hawaii, highlighting\ncultural experiences and must-see attractions.\n2nd Turn\nRewrite your previous response. Start every sentence with the letter A.\nMath\n1st Turn\nGiven that f(x) = 4x3 \u2212 9x \u2212 14, find the value of f(2).\n2nd Turn\nFind x such that f(x) = 0.\nKnowledge\n1st Turn\nProvide insights into the correlation between economic indicators such as GDP,\ninflation, and unemployment rates. Explain how fiscal and monetary policies ...\n2nd Turn\nNow, explain them again like I\u2019m five.\n2\nMT-Bench and Chatbot Arena\n2.1\nMotivation\nWith the recent advances of LLMs, LLM-based assistants start to exhibit artificial general intelligence\nacross diverse tasks, from writing and chatting to coding [5, 30, 1, 37]. However, evaluating their\nbroad capabilities also becomes more challenging. Despite the availability of numerous benchmarks\nfor language models, they primarily focus on evaluating models on closed-ended questions with short\nresponses. Given that these chat assistants can now precisely follow user instructions in multi-turn\ndialogues and answer open-ended questions in a zero-shot manner, current benchmarks are inadequate\nfor assessing such capabilities. Existing benchmarks mostly fall into the following three categories.\n\u2022 Core-knowledge benchmarks, including MMLU [19], HellaSwag [50], ARC [9], Wino-\nGrande [36], HumanEval [6], GSM-8K [10], and AGIEval [51], evaluate the core capabilities of\npre-trained LLMs using zero-shot and few-shot benchmark sets. They typically require LLMs to\ngenerate a short, specific answer to benchmark questions that can be automatically validated.\n\u2022 Instruction-following benchmarks, such as Flan [27, 46], Self-instruct [44], NaturalInstruc-\ntions [28], Super-NaturalInstructions [45], expand to slightly more open-ended questions and\nmore diverse tasks and are used to evaluate LLMs after instruction fine-tuning.\n\u2022 Conversational benchmarks, like CoQA [35], MMDialog [15] and OpenAssistant [23], are\nclosest to our intended use cases. However, the diversity and complexity of their questions often\nfall short in challenging the capabilities of the latest chatbots.\nWhile largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure\nof a chatbot\u2019s utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we\nintroduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously,\nthese benchmarks are designed to distinguish the core capabilities of state-of-the-art models.\n2.2\nMT-Bench\nWe create MT-bench, a benchmark consisting of 80 high-quality multi-turn questions. MT-bench is\ndesigned to test multi-turn conversation and instruction-following ability, covering common use cases\nand focusing on challenging questions to differentiate models. We identify 8 common categories\nof user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding,\n3\nknowledge I (STEM), and knowledge II (humanities/social science). For each category, we then\nmanually designed 10 multi-turn questions. Table 1 lists several sample questions.\n2.3\nChatbot Arena\nOur second approach is Chatbot Arena, a crowdsourcing benchmark platform featuring anonymous\nbattles. On this platform, users can interact with two anonymous models simultaneously, posing\nthe same question to both. They vote for which model provides the preferred response, with the\nidentities of the models disclosed post-voting. After running Chatbot Arena for one month, we have\ncollected around 30K votes. Since the platform does not use pre-defined questions, it allows gathering\na wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users. A\nscreenshot of the platform can be found at Appendix C.2.\n3\nLLM as a Judge\nWhile our initial evaluations using MT-bench and Chatbot Arena rely on human ratings, collecting\nhuman preferences can be costly and laborious [44, 38, 31, 2, 13]. To overcome this, we aim to\ndevelop a more scalable and automated approach. Given that most questions in MT-bench and Chatbot\nArena are open-ended without reference answers, devising a rule-based program to assess the outputs\nis extremely challenging. Traditional evaluation metrics based on the similarity between outputs and\nreference answers (e.g., ROUGE [25], BLEU [32]) are also ineffective for these questions.\nAs LLMs continue to improve, they show potential in replacing human annotators in many tasks [17,\n20]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat\nassistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge.\n3.1\nTypes of LLM-as-a-Judge\nWe propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination:\n\u2022 Pairwise comparison. An LLM judge is presented with a question and two answers, and tasked\nto determine which one is better or declare a tie. The prompt used is given in Figure 5 (Appendix).\n\u2022 Single answer grading. Alternatively, an LLM judge is asked to directly assign a score to a\nsingle answer. The prompt used for this scenario is in Figure 6 (Appendix).\n\u2022 Reference-guided grading. In certain cases, it may be beneficial to provide a reference solution\nif applicable. An example prompt we use for grading math problems is in Figure 8 (Appendix).\nThese methods have different pros and cons. For example, the pairwise comparison may lack\nscalability when the number of players increases, given that the number of possible pairs grows\nquadratically; single answer grading may be unable to discern subtle differences between specific\npairs, and its results may become unstable, as absolute scores are likely to fluctuate more than relative\npairwise results if the judge model changes.\n3.2\nAdvantages of LLM-as-a-Judge\nLLM-as-a-judge offers two key benefits: scalability and explainability. It reduces the need for human\ninvolvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide\nnot only scores but also explanations, making their outputs interpretable, as shown in Figure 1.\n3.3\nLimitations of LLM-as-a-Judge\nWe identify certain biases and limitations of LLM judges. However, we will also present solutions\nlater and show the agreement between LLM judges and humans is high despite these limitations.\nPosition bias is when an LLM exhibits a propensity to favor certain positions over others. This\nbias is not unique to our context and has been seen in human decision-making [3, 34] and other ML\ndomains [22, 41].\nFigure 11 (Appendix) shows an example of position bias. GPT-4 is tasked to evaluate two responses\nfrom GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5\u2019s answer is positioned\n4\nTable 2: Position bias of different LLM judges. Consistency is the percentage of cases where a\njudge gives consistent results when swapping the order of two assistants. \u201cBiased toward first\u201d is the\npercentage of cases when a judge favors the first answer. \u201cError\u201d indicates wrong output formats.\nThe two largest numbers in each column are in bold.\nJudge\nPrompt\nConsistency\nBiased toward first\nBiased toward second\nError\nClaude-v1\ndefault\n23.8%\n75.0%\n0.0%\n1.2%\nrename\n56.2%\n11.2%\n28.7%\n3.8%\nGPT-3.5\ndefault\n46.2%\n50.0%\n1.2%\n2.5%\nrename\n51.2%\n38.8%\n6.2%\n3.8%\nGPT-4\ndefault\n65.0%\n30.0%\n5.0%\n0.0%\nrename\n66.2%\n28.7%\n5.0%\n0.0%\nTable 3: Failure rate under \u201crepetitive list\u201d at-\ntack for different LLM judges on 23 answers.\nJudge\nClaude-v1\nGPT-3.5\nGPT-4\nFailure rate\n91.3%\n91.3%\n8.7%\nTable 4: Judge failure rate on 10 math questions\nwith different prompts. We test LLaMA-13B vs.\nVicuna-13B and swap positions. A failure means\nwhen GPT-4 says an incorrect answer is correct.\nDefault\nCoT\nReference\nFailure rate\n14/20\n6/20\n3/20\nfirst, GPT-4 considers GPT-3.5\u2019s answer more detailed and superior. However, upon switching the\npositions of the two responses, GPT-4\u2019s judgement flips, favoring Vicuna\u2019s answer.\nTo analyze the position bias, we construct two similar answers to each first-turn question in MT-bench\nby calling GPT-3.5 twice with a temperature of 0.7. We then try three LLMs with two different\nprompts: \u201cdefault\u201d is our default prompt in Figure 5 (Appendix). \u201crename\u201d renames the assistants in\nour default prompt to see whether the bias is on positions or names. As in Table 2, we found all of\nthem exhibit strong position bias. Most LLM judges favor the first position. Claude-v1 also shows a\nname bias which makes it favors \"Assistant A\", as illustrated by the \"rename\" prompt. The position\nbias can be very significant. Only GPT-4 outputs consistent results in more than 60% of cases.\nNote that this test is challenging because the answers are very similar and occasionally indistinguish-\nable even to humans. We will show that position bias is less prominent in some cases in Appendix D.1.\nAs for the origin of this bias, we suspect that it could be rooted in the training data or inherent to the\nleft-to-right architecture of causal transformers, but leave a deeper study as future work.\nVerbosity bias is when an LLM judge favors longer, verbose responses, even if they are not as clear,\nhigh-quality, or accurate as shorter alternatives.\nTo examine this bias, we design a \u201crepetitive list\u201d attack with model answers from MT-bench. We\nfirst select 23 model answers from MT-bench that contain a numbered list. We then make them\nunnecessarily verbose by asking GPT-4 to rephrase the list without adding any new information and\ninsert the rephrased new list to the beginning of the original list. For example, if the original response\ncontains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from\nthe original 5 items. An example is shown in Figure 12 (Appendix). We define the attack is successful\nif an LLM judge thinks the new response is better than the old response. Table 3 shows the failure\nrate of LLM judges under this attack, demonstrating that all LLMs may be prone to verbosity bias\nthough GPT-4 defends significantly better than others. As a calibration, we find LLM judges are\nable to correctly judge identical answers (i.e., they always return a tie for two identical answers) but\ncannot pass the more advanced \u201crepetitive list\u201d attack.\nSelf-enhancement bias. We adopt the term \u201cself-enhancement bias\u201d from social cognition litera-\nture [4] to describe the effect that LLM judges may favor the answers generated by themselves.\nWe examine this effect statistically. Figure 3(b) shows the win rate (w/o tie) of six models under\ndifferent LLM judges and humans. Compared to humans, we do observe that some judges favor\ncertain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself\nwith a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself.\nDue to limited data and small differences, our study cannot determine whether the models exhibit\na self-enhancement bias. Conducting a controlled study is challenging because we cannot easily\nrephrase a response to fit the style of another model without changing the quality.\n5\nLimited capability in grading math and reasoning questions. LLMs are known to have limited\nmath and reasoning capability [10], which results in its failure of grading such questions because they\ndo not know the correct answers. However, what is more intriguing is that it also shows limitations in\ngrading basic math problems which it is capable of solving. For instance, in Figure 13 (Appendix),\nwe present an example of an elementary math question in which GPT-4 makes an incorrect judgment.\nIt\u2019s worth noting that although GPT-4 can solve the problem (when asked separately), it was misled\nby the provided answers, ultimately resulting in incorrect judgment. This pattern can also be seen in\na reasoning question example in Figure 14 (Appendix). Both GPT-3.5 and Claude-v1 show a similar\nweakness. In Section 3.4, we will introduce a reference-guided method to mitigate such issues.\n3.4\nAddressing limitations\nWe present a few methods to address position bias and the limited grading ability for math questions.\nSwapping positions. The position bias can be addressed by simple solutions. A conservative\napproach is to call a judge twice by swapping the order of two answers and only declare a win when\nan answer is preferred in both orders. If the results are inconsistent after swapping, we can call it a\ntie. Another more aggressive approach is to assign positions randomly, which can be effective at a\nlarge scale with the correct expectations. In the following experiments, we use the conservative one.\nFew-shot judge. We assess whether few-shot examples can improve consistency in the position bias\nbenchmark. We select three good judgment examples using MT-bench-like questions, GPT-3.5 and\nVicuna for generating answers, and GPT-4 for generating judgments. The examples cover three cases:\nA is better, B is better, and tie. As shown in Table 12 (Appendix), the few-shot judge can significantly\nincrease the consistency of GPT-4 from 65.0% to 77.5%. However, high consistency may not imply\nhigh accuracy and we are not sure whether the few-shot examples will introduce new biases. Besides,\nthe longer prompts make API calls 4\u00d7 more expensive. We use the zero-shot prompt by default in\nour following experiments but leave an additional study in Appendix D.2.\nChain-of-thought and reference-guided judge. In Section 3.3, we have shown LLM\u2019s limited\ncapability in grading math and reasoning questions. We propose two simple methods to mitigate\nthis issue: chain-of-thought judge and reference-guided judge. Chain-of-thought is a widely used\ntechnique to improve LLM\u2019s reasoning capability [47]. We propose a similar technique to prompt\nan LLM judge to begin with answering the question independently and then start grading. Detailed\nprompt in Figure 7 (Appendix). However, even with the CoT prompt, we find that in many cases\nLLM makes exactly the same mistake as the given answers in its problem-solving process (See\nexample in Figure 15 (Appendix), suggesting that LLM judge may still be misled by the context.\nHence, we propose a reference-guided method, in which we first generate LLM judge\u2019s answer\nindependently, and then display it as a reference answer in the judge prompt. In Table 4, we see a\nsignificant improvement in failure rate (from 70% to 15%) over the default prompt.\nFine-tuning a judge model. We try fine-tuning a Vicuna-13B on arena data to act as a judge and\nshow some promising preliminary results in Appendix F.\n3.5\nMulti-turn judge\nIn MT-bench, every question involves two turns to evaluate conversational abilities. Therefore, when\ncomparing two assistants, it becomes necessary to present a total of two questions and four responses,\ncomplicating the prompt design. We explore two possible designs, (1) breaking the two turns into two\nprompts or (2) displaying complete conversations in a single prompt. Our finding is the former one\ncan cause the LLM judge struggling to locate the assistant\u2019s previous response precisely. We illustrate\na case in Figure 16 (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference.\nThis suggests the necessity of displaying a complete conversation to enable the LLM judge to better\ngrasp the context. We then consider the alternative design that presents two full conversations in a\nsingle prompt in which we ask the LLM judge to focus on the second question (Figure 9 (Appendix)).\nThis approach has been found to significantly alleviate the aforementioned referencing issue.\n6\n4\nAgreement Evaluation\nWe study the agreement between different LLM judges and humans on MT-bench and Chatbot\nArena datasets. On MT-bench, we also study the agreement among humans. MT-bench represents a\nsmall-scale study with controlled human evaluation, while Chatbot Arena represents a larger-scale\nstudy with crowdsourced human evaluation in the wild.\n4.1\nSetup\nMT-bench. We generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1,\nVicuna-13B, Alpaca-13B [38], and LLaMA-13B [39]. We then use 2 kinds of judges: LLM judges\nand 58 expert-level human labelers. The labelers are mostly graduate students so they are considered\nexperts and more skilled than average crowd workers. We let LLM judges evaluate all pairs and let\neach human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all\nquestions. The detailed data collection process is in Appendix C.\nChatbot Arena. We randomly sample 3K single-turn votes from 30K arena data, which covers\nmodels including GPT-4, GPT-3.5, Claude, Vicuna-7B/13B, Koala-13B [16], Alpaca-13B, LLaMA-\n13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114\nunique IPs).\nMetrics. We define the agreement between two types of judges as the probability of randomly\nselected individuals (but not identical) of each type agreeing on a randomly selected question. See\nmore explanation in Appendix D.3. Average win rate is the average of win rates against all other\nplayers. These metrics can be computed with or without including tie votes.\n4.2\nHigh agreement between GPT-4 and humans\nWe compute agreement on MT-bench data. In Table 5, GPT-4 with both pairwise comparison and\nsingle answer grading show very high agreements with human experts. The agreement under setup S2\n(w/o tie) between GPT-4 and humans reaches 85%, which is even higher than the agreement among\nhumans (81%). This means GPT-4\u2019s judgments closely align with the majority of humans. We also\nshow that GPT-4\u2019s judgments may help humans make better judgments. During our data collection,\nwhen a human\u2019s choice deviated from GPT-4, we presented GPT-4\u2019s judgments to humans and ask\nif they are reasonable (details in Appendix C.1). Despite different views, humans deemed GPT-4\u2019s\njudgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases.\nThe data from Arena shows a similar trend, as illustrated by Table 6. Comparing GPT-4 and other\nLLM judges, we find they reach a similar non-tie agreement ratio between humans but the number\nof non-tied votes from GPT-4 is much larger. This means that GPT-4 is more affirmative and less\nsuffered from position bias but other models also perform well when they give an affirmative answer.\nIn both tables, GPT-4 with single-answer grading matches both pairwise GPT-4 and human prefer-\nences very well. This means GPT-4 has a relatively stable internal rubric. Although it may sometimes\nperform slightly worse than pairwise comparison and give more tie votes, it is a more scalable method.\nWe then perform a breakdown analysis by computing agreement on different model pairs and\ncategories. We only include non-tied votes. In Figure 2, we observe the agreement between GPT-4\nand human progressively increases in line with the performance disparity of the model pairs (i.e.,\nlarger win rate difference), from 70% to nearly 100%. This suggests that GPT-4 aligns with humans\nbetter when significant performance differences exist between the models.\nGPT-4\nClaude\nGPT-3.5\nVicuna-13B\nAlpaca-13B\nLLaMA-13B\n(a) All votes, first turn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4\nGPT-3.5\nClaude\nVicuna-13B\nAlpaca-13B\nLLaMA-13B\n(b) Non-tied votes, first turn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4\nClaude\nGPT-3.5\nVicuna-13B\nAlpaca-13B\nLLaMA-13B\n(c) All votes, second turn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4\nClaude\nGPT-3.5\nVicuna-13B\nAlpaca-13B\nLLaMA-13B\n(d) Non-tied votes, second turn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin rate\nGPT-4 Judge\nGPT-3.5 Judge\nClaude Judge\nHuman\nHuman (first turn)\nFigure 3: Average win rate of six models under different judges on MT-bench.\n7\nTable 5: Agreement between two types of judges on MT-bench. \u201cG4-Pair\u201d and \u201cG4-Single\u201d denote\nGPT-4 with pairwise comparison and single-answer grading respectively. The single-answer grading\ncan be converted into pairwise comparison results for calculating the agreement. We report two\nsetups: \u201cS1\u201d includes non-tie, tie, and inconsistent (due to position bias) votes and counts inconsistent\nas tie; \u201cS2\u201d only includes non-tie votes. The agreement between two random judges under each setup\nis denoted as \u201cR=\u201d. The top value in each cell is the agreement, and the bottom gray value is #votes.\nSetup\nS1 (R = 33%)\nS2 (R = 50%)\nJudge\nG4-Single\nHuman\nG4-Single\nHuman\nG4-Pair\n70%\n1138\n66%\n1343\n97%\n662\n85%\n859\nG4-Single\n-\n60%\n1280\n-\n85%\n739\nHuman\n-\n63%\n721\n-\n81%\n479\n(a) First Turn\nSetup\nS1 (R = 33%)\nS2 (R = 50%)\nJudge\nG4-Single\nHuman\nG4-Single\nHuman\nG4-Pair\n70%\n1161\n66%\n1325\n95%\n727\n85%\n864\nG4-Single\n-\n59%\n1285\n-\n84%\n776\nHuman\n-\n67%\n707\n-\n82%\n474\n(b) Second Turn\nTable 6: Agreement between two types of judges on Chatbot\nArena. \u201cG4-S\u201d denotes GPT-4 with single-answer grading.\n\u201cG4\u201d, \u201cG3.5\u201d and \u201cC\u201d denote GPT-4, GPT-3.5, and Claude\nwith pairwise comparison, respectively. \u201cH\u201d denotes human.\nThe remaining of table follows the same format as Table 5.\nSetup\nS1 (Random = 33%)\nS2 (Random = 50%)\nJudge\nG4-S\nG3.5\nC\nH\nG4-S\nG3.5\nC\nH\nG4\n72%\n2968\n66%\n3061\n66%\n3062\n64%\n3066\n95%\n1967\n94%\n1788\n95%\n1712\n87%\n1944\nG4-S\n-\n60%\n2964\n62%\n2964\n60%\n2968\n-\n89%\n1593\n91%\n1538\n85%\n1761\nG3.5\n-\n-\n68%\n3057\n54%\n3061\n-\n-\n96%\n1497\n83%\n1567\nC\n-\n-\n-\n53%\n3062\n-\n-\n-\n84%\n1475\n0.0\n0.5\n1.0\nWin rate difference\n0.7\n0.8\n0.9\n1.0\nAgreement\nFigure 2: Agreement and win rate dif-\nference. Each point corresponds to a\nmodel pair and counts only the non-tie\nvotes between the two models. The x-\naxis value is the win rate difference be-\ntween the two models. The y-axis value\nis the GPT-4 and human agreement.\nGPT-4\nClaude\nGPT-3.5\nVicuna-13B\nVicuna-7B\nKoala-13B\nAlpaca-13B\nDolly-12B\nLLaMA-13B\n(a) All votes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4\nClaude\nGPT-3.5\nVicuna-13B\nVicuna-7B\nKoala-13B\nAlpaca-13B\nDolly-12B\nLLaMA-13B\n(b) Non-tied votes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin rate\nGPT-4 Judge\nGPT-3.5 Judge\nHuman\nGPT-4-Single Judge\nFigure 4: Average win rate of nine models under different judges on Chatbot Arena.\nTable 7: Category-wise win rate of models.\nModel\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\nGPT-4\n61.2%\n67.9%\n49.3%\n66.1%\n56.3%\n66.2%\n76.6%\n72.2%\nGPT-3.5\n50.9%\n60.6%\n32.6%\n63.8%\n55.0%\n48.8%\n52.8%\n53.8%\nVicuna-13B\n39.7%\n39.2%\n20.1%\n18.0%\n36.9%\n29.2%\n47.0%\n47.5%\nLLaMA-13B\n15.1%\n15.1%\n7.8%\n7.5%\n2.1%\n9.3%\n6.8%\n10.1%\n4.3\nWin rates under different judges\nWe plot the average win rate of models under different judges on MT-bench and Chatbot Arena\nin Figure 3 and Figure 4, respectively. The win rate curves from LLM judges closely match the\ncurves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are\nmore preferred by the humans compared to the first turn, meaning that a multi-turn benchmark can\nbetter differentiate some advanced abilities of models. We also list the per-category win rate of\n8\nTable 8: Evaluation results of several model variants.\nModel\n#Training Token\nMMLU (5-shot)\nTruthfulQA (0-shot)\nMT-Bench Score (GPT-4)\nLLaMA-7B\n1T\n35.2\n0.22\n2.74\nLLaMA-13B\n1T\n47.0\n0.26\n2.61\nAlpaca-7B\n4.4M\n40.1\n0.26\n4.54\nAlpaca-13B\n4.4M\n48.1\n0.30\n4.53\nVicuna-7B (selected)\n4.8M\n37.3\n0.32\n5.95\nVicuna-7B (single)\n184M\n44.1\n0.30\n6.04\nVicuna-7B (all)\n370M\n47.1\n0.32\n6.00\nVicuna-13B (all)\n370M\n52.1\n0.35\n6.39\nGPT-3.5\n-\n70.0\n-\n7.94\nGPT-4\n-\n86.4\n-\n8.99\nrepresentative models in Table 7 to show how MT-bench differentiates models, in which we see GPT-4\nis significantly better than others. Vicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math,\nand coding categories. Note that in math/coding category, GPT-3.5 and GPT-4 have similar overall\nwin-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better\nthan GPT-3 in the direct pairwise comparison or single-answer grading. Please see a performance\nbreakdown of MT-bench score for each category in Appendix D.4.\n5\nHuman Preference Benchmark and Standardized Benchmark\nHuman preference benchmarks such as MT-bench and Chatbot Arena serve as valuable additions\nto the current standardized LLM benchmarks. They focus on different aspects of a model and the\nrecommended way is to comprehensively evaluate models with both kinds of benchmarks.\nWe evaluate several model variants derived from LLaMA on MMLU [19], Truthful QA [26] (MC1),\nand MT-bench (GPT-4 judge). The training details are in Appendix E. Since we have shown that\nGPT-4 single-answer grading also performs well in Section 4.2, we use GPT-4 single-answer grading\nfor MT-bench in favor of its scalability and simplicity. We ask GPT-4 to give a score for each turn\non a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of\n160 = 80\u00d72 turns. Table 8 shows the results. We find that fine-tuning on high-quality dialog datasets\n(i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement\nscales with fine-tuning data size. On the other hand, a small high-quality conversation dataset can\nquickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve\nMMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens\nor 3K conversations. In Table 8, no single benchmark can determine model quality, meaning that a\ncomprehensive evaluation is needed. Our results indicate that using LLM-as-a-judge to approximate\nhuman preferences is highly feasible and could become a new standard in future benchmarks. We\nare also hosting a regularly updated leaderboard with more models 2. Notably, DynaBench [21], a\nresearch platform dedicated to dynamic data collection and benchmarking, aligns with our spirit.\nDynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and\noverfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach\ncan automate and scale platforms of this nature.\n6\nDiscussion\nLimitations. This paper emphasizes helpfulness but largely neglects safety. Honesty and harm-\nlessness are crucial for a chat assistant as well [2]. We anticipate similar methods can be used to\nevaluate these metrics by modifying the default prompt. Additionally, within helpfulness, there\nare multiple dimensions like accuracy, relevance, and creativity, but they are all combined into a\nsingle metric in this study. A more comprehensive evaluation can be developed by analyzing and\nseparating these dimensions. We propose preliminary solutions to address the limitations and biases\nof LLM-as-a-judge in Section 3.4, but we anticipate more advanced methods can be developed.\nData collection and release. Appendix C describes the detailed data collection and release processes,\nwhich include the instructions we give to users, the screenshots of the data collection interface, the\ninformation about participated users, and the content of the released data.\n2https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n9\nSocietal impacts. The societal impact of this study is multi-faceted. Our evaluation methods can\nhelp enhance chatbot quality and user experiences. However, addressing biases in these methods is\ncrucial. Our dataset enables better studies of human preferences and model behavior. Advanced chat\nassistants may replace certain human tasks, resulting in job displacements and new opportunities.\nFuture directions. 1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source\nLLM judge aligned with human preference 3) Enhancing open models\u2019 math/reasoning capability.\n7\nConclusion\nIn this paper, we propose LLM-as-a-judge for chatbot evaluation and systematically examine its\nefficacy using human preference data from 58 experts on MT-bench, as well as thousands of crowd-\nusers on Chatbot Arena. Our results reveal that strong LLMs can achieve an agreement rate of over\n80%, on par with the level of agreement among human experts, establishing a foundation for an\nLLM-based evaluation framework.\nAcknowledgement\nThis project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework,\nMicrosoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta\nPh.D. Fellowship. We extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li,\nTianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.\nReferences\n[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\n[3] Niels J Blunch. Position bias in multiple-choice questions. Journal of Marketing Research,\n21(2):216\u2013220, 1984.\n[4] Jonathon D Brown. Evaluations of self and others: Self-enhancement biases in social judgments.\nSocial cognition, 4(4):353\u2013376, 1986.\n[5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[7] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human\nevaluations? arXiv preprint arXiv:2305.01937, 2023.\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457, 2018.\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n10\n[11] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. Advances in Neural Information Processing\nSystems, 35:16344\u201316359, 2022.\n[12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\nfinetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.\n[13] Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang.\nLmflow: An extensible toolkit for finetuning and inference of large foundation models. arXiv\npreprint arXiv:2306.12420, 2023.\n[14] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.\n[15] Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao,\nand Qingwei Lin. Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal\nopen-domain conversation. arXiv preprint arXiv:2211.05719, 2022.\n[16] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and\nDawn Song. Koala: A dialogue model for academic research. Blog post, April 2023.\n[17] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for\ntext-annotation tasks. arXiv preprint arXiv:2303.15056, 2023.\n[18] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\nLevine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint\narXiv:2305.15717, 2023.\n[19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\n[20] Fan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential\nand limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736,\n2023.\n[21] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu,\nBertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking\nbenchmarking in nlp. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pages\n4110\u20134124, 2021.\n[22] Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first\nsentence: Position bias in question answering. arXiv preprint arXiv:2004.14602, 2020.\n[23] Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith\nStevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, et al. Ope-\nnassistant conversations\u2013democratizing large language model alignment.\narXiv preprint\narXiv:2304.07327, 2023.\n[24] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of\nlanguage models. arXiv preprint arXiv:2211.09110, 2022.\n[25] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pages 74\u201381, 2004.\n[26] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\nhuman falsehoods. arXiv preprint arXiv:2109.07958, 2021.\n[27] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for\neffective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.\n[28] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In ACL, 2022.\n[29] OpenAI. Evals is a framework for evaluating llms and llm systems, and an open-source registry\nof benchmarks. https://github.com/openai/evals.\n[30] OpenAI. Gpt-4 technical report, 2023.\n11\n[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[32] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pages 311\u2013318, 2002.\n[33] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n[34] Priya Raghubir and Ana Valenzuela. Center-of-inattention: Position biases in decision-making.\nOrganizational Behavior and Human Decision Processes, 99(1):66\u201380, 2006.\n[35] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question\nanswering challenge. Transactions of the Association for Computational Linguistics, 7:249\u2013266,\n2019.\n[36] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n[37] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al.\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\narXiv preprint arXiv:2206.04615, 2022.\n[38] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[40] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,\n2023.\n[41] Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork.\nPosition bias estimation for unbiased learning to rank in personal search. In Proceedings of\nthe Eleventh ACM International Conference on Web Search and Data Mining, pages 610\u2013618,\n2018.\n[42] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya\nJiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An\nautomatic evaluation benchmark for llm instruction tuning optimization, 2023.\n[43] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\nChandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels\ngo? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751,\n2023.\n[44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions, 2022.\n[45] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.\nSuper-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP,\n2022.\n[46] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv\npreprint arXiv:2109.01652, 2021.\n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n12\n[48] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\narXiv preprint arXiv:2304.12244, 2023.\n[49] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk Kwon,\nSiyuan Zhuang, Frank Sifei Luan, Gautam Mittal, Scott Shenker, and Ion Stoica. SkyPilot:\nAn intercloud broker for sky computing. In 20th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 23), pages 437\u2013455, Boston, MA, April 2023. USENIX\nAssociation.\n[50] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n[51] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. arXiv preprint arXiv:2304.06364, 2023.\n[52] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia\nEfrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206,\n2023.\n13\nA\nPrompt templates\nWe list the prompt templates for LLM judges. Please refer to our github repository 3 for full details.\n[System]\nPlease act as an impartial judge and evaluate the quality of the responses provided by two \nAI assistants to the user question displayed below. You should choose the assistant that \nfollows the user\u2019s instructions and answers the user\u2019s question better. Your evaluation \nshould consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \nand level of detail of their responses. Begin your evaluation by comparing the two \nresponses and provide a short explanation. Avoid any position biases and ensure that the \norder in which the responses were presented does not influence your decision. Do not allow \nthe length of the responses to influence your evaluation. Do not favor certain names of \nthe assistants. Be as objective as possible. After providing your explanation, output your \nfinal verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" \nif assistant B is better, and \"[[C]]\" for a tie.\n[User Question]\n{question}\n[The Start of Assistant A\u2019s Answer]\n{answer_a}\n[The End of Assistant A\u2019s Answer]\n[The Start of Assistant B\u2019s Answer]\n{answer_b}\n[The End of Assistant B\u2019s Answer]\nFigure 5: The default prompt for pairwise comparison.\n[System]\nPlease act as an impartial judge and evaluate the quality of the response provided by an \nAI assistant to the user question displayed below. Your evaluation should consider factors \nsuch as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of \nthe response. Begin your evaluation by providing a short explanation. Be as objective as \npossible. After providing your explanation, please rate the response on a scale of 1 to 10 \nby strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n[Question]\n{question}\n[The Start of Assistant\u2019s Answer]\n{answer}\n[The End of Assistant\u2019s Answer]\nFigure 6: The default prompt for single answer grading.\n3https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\n14\n[System]\nPlease act as an impartial judge and evaluate the quality of the responses provided by two \nAI assistants to the user question displayed below. Your evaluation should consider \ncorrectness and helpfulness. You will be given assistant A\u2019s answer, and assistant B\u2019s \nanswer. Your job is to evaluate which assistant\u2019s answer is better. You should \nindependently solve the user question step-by-step first. Then compare both assistants\u2019 \nanswers with your answer. Identify and correct any mistakes. Avoid any position biases and \nensure that the order in which the responses were presented does not influence your \ndecision. Do not allow the length of the responses to influence your evaluation. Do not\nfavor certain names of the assistants. Be as objective as possible. After providing your \nexplanation, output your final verdict by strictly following this format: \"[[A]]\" if \nassistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.\n[User Question]\n{question}\n[The Start of Assistant A\u2019s Answer]\n{answer_a}\n[The End of Assistant A\u2019s Answer]\n[The Start of Assistant B\u2019s Answer]\n{answer_b}\n[The End of Assistant B\u2019s Answer]\nFigure 7: The chain-of-thought prompt for math and reasoning questions.\n[System]\nPlease act as an impartial judge and evaluate the quality of the responses provided by two \nAI assistants to the user question displayed below. Your evaluation should consider \ncorrectness and helpfulness. You will be given a reference answer, assistant A\u2019s answer, \nand assistant B\u2019s answer. Your job is to evaluate which assistant\u2019s answer is better. \nBegin your evaluation by comparing both assistants\u2019 answers with the reference answer. \nIdentify and correct any mistakes. Avoid any position biases and ensure that the order in \nwhich the responses were presented does not influence your decision. Do not allow the \nlength of the responses to influence your evaluation. Do not favor certain names of the \nassistants. Be as objective as possible. After providing your explanation, output your \nfinal verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" \nif assistant B is better, and \"[[C]]\" for a tie.\n[User Question]\n{question}\n[The Start of Reference Answer]\n{answer_ref}\n[The End of Reference Answer]\n[The Start of Assistant A\u2019s Answer]\n{answer_a}\n[The End of Assistant A\u2019s Answer]\n[The Start of Assistant B\u2019s Answer]\n{answer_b}\n[The End of Assistant B\u2019s Answer]\nFigure 8: The prompt for reference-guided pairwise comparison.\n15\n[System]\nPlease act as an impartial judge and evaluate the quality of the responses provided by two \nAI assistants to the user question displayed below. You should choose the assistant that \nfollows the user\u2019s instructions and answers the user\u2019s question better. Your evaluation \nshould consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \nand level of detail of their responses. Begin your evaluation by comparing the two \nresponses and provide a short explanation. Avoid any position biases and ensure that the \norder in which the responses were presented does not influence your decision. Do not allow \nthe length of the responses to influence your evaluation. Do not favor certain names of \nthe assistants. Be as objective as possible. After providing your explanation, output your \nfinal verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" \nif assistant B is better, and \"[[C]]\" for a tie.\n<|The Start of Assistant A\u2019s Conversation with User|>\n### User:\n{question 1}\n### Assistant A:\n{answer 1}\n### User:\n{question 2}\n### Assistant A:\n{answer 2}\n<|The End of Assistant A\u2019s Conversation with User|>\n<|The Start of Assistant B\u2019s Conversation with User|>\n### User:\n{question 1}\n### Assistant B:\n{answer 1}\n### User:\n{question 2}\n### Assistant B:\n{answer 2}\n<|The End of Assistant B\u2019s Conversation with User|>\nFigure 9: The prompt for multi-turn pairwise comparison.\n16\n[System]\nPlease act as an impartial judge and evaluate the quality of the response provided by an \nAI assistant to the user question. Your evaluation should consider correctness and \nhelpfulness. You will be given a reference answer and the assistant's answer. You \nevaluation should focus on the assistant's answer to the second question. Begin your \nevaluation by comparing the assistant's answer with the reference answer. Identify and \ncorrect any mistakes. Be as objective as possible. After providing your explanation, you \nmust rate the response on a scale of 1 to 10 by strictly following this format: \n\"[[rating]]\", for example: \"Rating: [[5]]\".\n<|The Start of Reference Answer|>\n### User:\n{question_1}\n### Reference answer:\n{ref_answer_1}\n### User:\n{question_2}\n### Reference answer:\n{ref_answer_2}\n<|The End of Reference Answer|>\n<|The Start of Assistant A's Conversation with User|>\n### User:\n{question_1} \n### Assistant A:\n{answer_1}\n### User:\n{question_2}\n### Assistant A:\n{answer_2}\n<|The End of Assistant A's Conversation with User|>\nFigure 10: The prompt for reference-guided multi-turn single-answer grading.\n17\nB\nCase Study\nWe list several case studies. The examples are generated by gpt-4-0314. They may not be fully\nreproducible with future GPT-4 versions.\nQuestion: What are some business etiquette norms when doing business in Japan?\nAssistant B:\nWhen doing business in Japan, it is important to be \naware of and respect the country\u2019s business \netiquette norms. Some of these norms include:\n1.\nGreetings: A handshake and bow is the standard \ngreeting in Japan. It is important to bow deeply \nand to offer a firm handshake.\n2.\nBusiness cards: In Japan, business cards are \nexchanged after the initial greeting. It is \nimportant to handle the card with respect and to \nread it carefully before placing it on the \ntable.\n3.\nMeetings: Meetings in Japan are typically formal \nand conducted in a hierarchical manner. It is \nimportant to address people by their titles and \nto avoid direct eye contact.\n4.\nPunctuality: Punctuality is highly valued in \nJapan. It is important to arrive on time for \nmeetings and to be prepared in advance.\n5.\nGift giving: Gift giving is a common practice in \nJapan, but it is important to be mindful of the \noccasion and the relationship. Avoid giving \ngifts that are considered too personal or \nextravagant.\n6.\nDining: Dining is an important part of doing \nbusiness in Japan. It is important to be aware \nof the proper table manners and to avoid \nslurping or making loud noises while eating.\n7.\nCommunication: Communication in Japan is \ntypically indirect and reserved. It is important \nto listen carefully and to avoid confrontational \nlanguage.\nOverall, it is important to be respectful, \npunctual, and mindful of hierarchy when doing \nbusiness in Japan.\nAssistant A:\nHere are some business etiquette norms to keep \nin mind when doing business in Japan:\n1.\nBowing: Bowing is a common greeting in \nJapan. It is a sign of respect and should \nbe reciprocated.\n2.\nBusiness cards: Business cards are \nexchanged at the beginning of a meeting. It \nis important to receive and give business \ncards with both hands and to take a moment \nto read the card before putting it away.\n3.\nPunctuality: Being on time is important in \nJapan. Arrive a few minutes early to show \nrespect for your colleagues.\n4.\nDress code: Dress conservatively and \nprofessionally. Men should wear dark suits \nand ties, while women should wear \nconservative business attire.\n5.\nLanguage: Learn a few basic Japanese \nphrases to show respect for the culture. It \nis also important to speak clearly and \navoid using slang or idioms.\n6.\nGift giving: Gift giving is a common \npractice in Japan. Bring a small gift, such \nas a box of chocolates or a small souvenir \nfrom your home country.\n7.\nDining etiquette: When dining with \ncolleagues, wait for the host to begin \neating before you start. Use chopsticks \ncorrectly and avoid leaving food on your \nplate.\nRemember, showing respect for Japanese culture \nand customs is key to building strong business \nrelationships in Japan.\nGPT-4 Judgment (when A is placed in the first position):\nBoth Assistant A and Assistant B provide helpful and relevant information about business etiquette \nnorms in Japan. However, Assistant A\u2019s answer is more detailed and organized, covering a wider range \nof topics such as dress code and language, which are not mentioned in Assistant B\u2019s response. \nAssistant A also provides more specific advice on how to handle certain situations, such as \nexchanging business cards and dining etiquette. Therefore, Assistant A\u2019s answer is more comprehensive \nand useful for someone looking to understand Japanese business etiquette.\nGPT-4 Judgment (when B is placed in the first position):\nBoth Assistant A and Assistant B provide helpful and relevant information about business etiquette \nnorms in Japan. However, Assistant B\u2019s answer is more detailed and covers a wider range of topics, \nsuch as addressing people by their titles, avoiding direct eye contact, and being mindful of \ncommunication styles. Assistant A\u2019s answer is more concise but lacks some of the depth found in \nAssistant B\u2019s response. Therefore, Assistant B\u2019s answer is better in this case.\nFigure 11: An example of position bias. When Assistant A is placed in the first position, GPT-4\nthinks A is better, but its verdict changes when we swap the position of A and B. We observe similar\npattern from other LLM judges such as Claude/GPT-3.5.\n18\nQuestion: What is the central dogma of molecular biology? What processes are involved? Who named this?\nAssistant B:\nThe central dogma of molecular biology is the \nfundamental principle that explains how genetic \ninformation flows within a biological system. It \nstates that DNA is transcribed into RNA, which is \nthen translated into proteins. This means that \nthe genetic information stored in DNA is first \ncopied into RNA, which then serves as a template \nfor the synthesis of proteins.\nThe processes involved in the central dogma are:\n1. Transcription: The process by which the \ngenetic information in DNA is copied into RNA.\n2. Translation: The process by which the genetic \ninformation in RNA is used to synthesize \nproteins.\nThe term \"central dogma\" was coined by Francis \nCrick, one of the co-discoverers of the structure \nof DNA, in 1958.\nAssistant A:\nThe central dogma of molecular biology is the \nfundamental principle that explains how genetic \ninformation flows within a biological system. It \nstates that DNA is transcribed into RNA, which is \nthen translated into proteins. This means that \nthe genetic information stored in DNA is first \ncopied into RNA, which then serves as a template \nfor the synthesis of proteins.\nThe processes involved in the central dogma are:\n1. Transcription rephrased: The act of copying \ngenetic information from DNA to RNA.\n2. Translation rephrased: The act of using \ngenetic information in RNA to create proteins.\n3. Transcription: The process by which the \ngenetic information in DNA is copied into RNA.\n4. Translation: The process by which the genetic \ninformation in RNA is used to synthesize \nproteins.\nThe term \"central dogma\" was coined by Francis \nCrick, one of the co-discoverers of the structure \nof DNA, in 1958.\nGPT-3.5 Judgment:\nAssistant A provides a more detailed and accurate response to the user's question. While both \nassistants correctly state the central dogma of molecular biology and who coined the term, Assistant \nA provides a more comprehensive explanation of the processes involved in the central dogma, including \nthe rephrased definitions of transcription and translation. Therefore, I choose A as the better \nassistant.\nClaude-v1 Judgment:\nBoth assistants answer the question by defining the central dogma and detailing the two processes \ninvolved, transcription and translation. However, assistant A provides a more in-depth and detailed \nexplanation by briefly rephrasing and elaborating on these processes. Assistant A also mentions who \ncoined the term \"central dogma\", which provides useful context and shows a more comprehensive \nunderstanding of the topic. Overall, assistant A gives a fuller and more insightful response, so I \nhave chosen it as the better assistant for this question.\nGPT-4 Judgment:\nBoth Assistant A and Assistant B provide accurate and relevant information about the central dogma of \nmolecular biology, the processes involved, and who named it. However, Assistant A's answer contains a \nminor redundancy by rephrasing transcription and translation twice. Assistant B's answer is more \nconcise and avoids this redundancy. Therefore, Assistant B's answer is better.\nFigure 12: An example of \u201crepetitive list\u201d attack to examine verbosity bias. Except for the two\nrephrased items (highlighted in red), Assistant A\u2019s answer is exactly the same as Assistant B. Both\nGPT-3.5 and Claude-v1 show a verbosity bias towards the longer and repetitive answer. Only GPT-4\nsuccessfully detected this attack.\n19\nQuestion: Benjamin went to a bookstore and purchased a variety of books. He bought 5 copies of a sci-fi \nnovel, each priced at $20, 3 copies of a history book priced at $30 each, and 2 copies of a philosophy book for \n$45 each. What was the total cost of his purchases?\nAssistant B:\nThe total cost of his purchases is $295.\nAssistant A:\nThe total cost of Benjamin\u2019s purchases was $115.\nGPT-4 Judgment:\nAssistant B provided the correct answer to the user\u2019s question. Assistant A\u2019s response was incorrect.\nTo calculate the total cost of Benjamin\u2019s purchases, we need to multiply the number of books by their\nrespective prices and then add the results:\n5 sci-fi novels * $20 = $100\n3 history books * $30 = $90\n2 philosophy books * $45 = $90\nTotal cost = $100 + $90 + $90 = $295\nAssistant B\u2019s response is accurate and directly answers the user\u2019s question. Therefore, my final\nverdict is: [[B]]\nGPT-4\u2019s Answer (if asked separately):\nThe cost of the sci-fi novels is 5 * $20 = $<<5*20=100>>100.\nThe cost of the history books is 3 * $30 = $<<3*30=90>>90.\nThe cost of the philosophy books is 2 * $45 = $<<2*45=90>>90.\nThe total cost of his purchases is $100 + $90 + $90 = $<<100+90+90=280>>280. Answer: \\boxed{280}.\nFigure 13: With a default prompt, GPT-4 shows limited capability in grading math questions. Despite\nbeing able to answer the question itself, its judgment was influenced by the given answers, leading to\narithmetic mistakes highlighted in yellow.\nQuestion: Each problem consists of three statements. Based on the first two statements, the third statement \nmay be true, false, or uncertain.\n1. Oranges cost more than apples.\n2. Oranges cost less than bananas.\n3. Bananas cost more than apples and bananas cost more than orange.\nIf the first two statements are true, then the third statement is\nAssistant B:\nFalse.\nIf the first two statements are true, then the \nthird statement is false. The first statement \nsays that oranges cost more than apples, and the \nsecond statement says that oranges cost less \nthan bananas. This means that oranges cost less \nthan bananas, which contradicts the third\nstatement that bananas cost more than apples and \nmore than oranges. Therefore, the third \nstatement must be false.\nAssistant A:\nfalse.\nIf oranges cost more than apples (statement 1) and\noranges cost less than bananas (statement 2), then \nit is not possible for bananas to cost less than \noranges and also cost more than both apples and \noranges (statement 3). Therefore, statement 3 is \nfalse.\nGPT-4 Judgment:\nBoth Assistant A and Assistant B provided accurate and relevant responses to the user\u2019s question. \nThey both concluded that the third statement is false based on the first two statements. However, \nAssistant B\u2019s response is slightly clearer and more concise in explaining the contradiction between \nthe statements. Therefore, I choose Assistant B as the better response.\n[[B]]\nGPT-4\u2019s Answer (if asked separately):\nTrue.\nIf oranges cost more than apples (statement 1) and oranges cost less than bananas (statement 2), it \nlogically follows that bananas cost more than apples and bananas cost more than oranges, which is the \nthird statement. So, if the first two statements are true, the third one is also true.\nFigure 14: An example of GPT-4\u2019s limited capability in grading reasoning question. Despite GPT-4\nknows how to solve the question (if asked separately), it made a wrong judgement saying both\nassistants\u2019 wrong answers are correct.\n20\nQuestion: How many integers are in the solution of the inequality |x + 5| < 10?\nAssistant B:\nTo solve the inequality |x + 5| < 10, we need to \nconsider two cases:\nCase 1: x + 5 is positive or zero\nIn this case, the inequality becomes x + 5 < 10, \nwhich simplifies to x < 5.\nCase 2: x + 5 is negative\nIn this case, the inequality becomes -(x + 5) < 10, \nwhich simplifies to -x - 5 < 10. Adding 5 to both \nsides gives -x < 15, and multiplying by -1 (and \nflipping the inequality) gives x > -15.\nSo the solution to the inequality is -15 < x < 5. The \nintegers in this range are -14, -13, -12, ..., 4, for \na total of 20 integers. Therefore, there are 20 \nintegers in the solution of the inequality.\nAssistant A:\nThere are 19 integers in the solution of the \ninequality |x + 5| < 10.\nGPT-4 Judgment:\nTo solve the inequality |x + 5| < 10, we need to consider two cases:\nCase 1: x + 5 is positive or zero\nIn this case, the inequality becomes x + 5 < 10, which simplifies to x < 5.\nCase 2: x + 5 is negative\nIn this case, the inequality becomes -(x + 5) < 10, which simplifies to -x - 5 < 10. Adding 5 to both \nsides gives -x < 15, and multiplying by -1 (and flipping the inequality) gives x > -15.\nSo the solution to the inequality is -15 < x < 5. The integers in this range are -14, -13, -12, ..., \n4, for a total of 20 integers. Therefore, there are 20 integers in the solution of the inequality.\nComparing the answers provided by the two AI assistants:\nAssistant A: 19 integers\nAssistant B: 20 integers\nThe correct answer is 20 integers, so Assistant B provided the correct and more helpful response.\nFinal verdict: [[B]]\nGPT-4\u2019s Answer (if asked separatedly):\nWe can split the inequality into two cases:\n1) If $x+5\\ge 0$, then $|x+5|=x+5$. So, we have $x+5<10$, which means $x<5$. Since $x+5\\ge 0$, we \nhave $x\\ge -5$. So in this case, $x$ can be any integer from $-5$ to $4$, inclusive, which gives us \n$5-(-5)+1=10$ integers.\n2) If $x+5<0$, then $|x+5|=-(x+5)$. So, we have $-(x+5)<10$, which means $x+5>-10$, which means $x>-\n15$. Since $x+5<0$, we have $x<-5$. So in this case, $x$ can be any integer from $-14$ to $-6$, \ninclusive, which gives us $-6-(-14)+1=9$ integers.\nIn total, there are $10+9=\\boxed{19}$ integers in the solution.\nFigure 15: An example of GPT-4\u2019s wrong judgment with chain-of-thought prompt. We can see GPT-4\nexactly copied Assistant B\u2019s answer (which contains arithmetic errors) and determined Assistant A\u2019s\nanswer is incorrect. This suggest GPT-4\u2019s chain-of-thought process can be significantly influenced by\nthe given answers despite we ask it to think independently.\n21\nQuestion: Share ideas for adapting art masterpieces into interactive experiences for children. List 5 specific \nartworks and associated ideas.\nAssistant B:\n(omitted)\nAssistant A:\nSure, here are five ideas for adapting art masterpieces into interactive \nexperiences for children:\n1. \u201cThe Starry Night\u201d by Vincent van Gogh - Create a virtual reality experience \nwhere children can explore the night sky and see the stars and constellations as \nvan Gogh saw them. They can also create their own starry night scenes using \ndifferent colors and brush strokes.\n2. \u201cThe Persistence of Memory\u201d by Salvador Dali - Create a sensory experience \nwhere children can touch, feel, and smell different objects to explore the \nsurrealist themes of Dali\u2019s painting. They can also create their own surreal \nlandscapes using different materials and textures.\n3. ...\n(omitted)\nFollow-up Question: Write a concrete plan for your second example. Include budget estimates.\nAssistant B:\n(omitted)\nAssistant A:\nSure, here is a concrete plan for adapting \u201cThe Persistence of Memory\u201d by Salvador \nDali into an interactive experience for children:\n1. Budget: $5,000\n2. Objectives:\n- To create a sensory experience that allows children to explore the surrealist \nthemes of Dali\u2019s painting.\n- To encourage children to create their own surreal landscapes using different \nmaterials and textures.\n...\nGPT-4 Judgment:\nAssistant A provided a concrete plan for adapting \u201cThe Persistence of Memory\u201d by Salvador Dali into \nan interactive experience for children, while Assistant B provided a plan for the \u201cMona Lisa \nInteractive Experience for Children.\u201d The user\u2019s follow-up question specifically asked for a concrete \nplan for the second example, which was the \u201cMona Lisa\u201d experience provided by Assistant B.\nTherefore, Assistant B\u2019s answer is more relevant and directly addresses the user\u2019s request.\nMy final verdict is: [[B]]\nFigure 16: In this example, despite Assistant A correctly followed user\u2019s instruction to generate\na concrete plan for the second example of its previous response, GPT-4 wrongly referred to the\nsecond example in Assistant B\u2019s response, resulting in a wrong judgment. This suggests the prompt\ndesign that breaks the questions into two prompts may cause LLM judge struggle to locate assistants\u2019\nprevious responses.\n22\nC\nData Collection\nWe describe our data collection process for both MT-bench and Chatbot Arena.\nC.1\nMT-bench human evaluation\nFigure 17 shows the normal voting interface. Figure 18 shows that we additionally show GPT-4\u2019s\njudgment to users and ask if it is reasonable when a human differs from GPT-4.\nFigure 17: The screenshot of MT-bench data collection. We show an instruction similar to the prompt\nwe give to GPT-4. We present questions from MT-bench and answers from two random anonymous\nassistants and ask which one is better. We present the first-turn conversation and ask humans to vote,\nthen repeat the same procedure for the second-turn. A user can skip up to 5 questions if they are not\nconfident. For some questions (e.g., math, reasoning), they can also see a reference solution.\nFigure 18: The screenshot of MT-bench data collection. When human\u2019s vote differs from GPT-4, we\nadditionally show GPT-4\u2019s judgment (red region in the screenshot) and ask the user to click one of\nthe three buttons to decide whether GPT-4\u2019s judgment is reasonable.\n23\nTo invite participants, we obtained their consent by letting them sign an application form. We\npay them $20 for judging 20 questions, which corresponds to an hourly rate of around $35. The\nparticipants are mostly graduate students from more than ten universities.\nC.2\nChatbot Arena\nFigure 19 shows a screenshot of Chatbot Arena. Users are required to accept the terms of use,\nwhich obtain their consent and give us the right to release the conversation data. The instructions\nare shown at the top of the interface. This is a free website. We do not pay users and any user\ncan use this platform without registration. More introductions and analyses can be found at https:\n//lmsys.org/blog/2023-05-03-arena/.\nFigure 19: The screenshot of Chatbot Arena.\nC.3\nData Release\nWe will clean the Personal Identifiable Information (PII) and tag toxic conversations with OpenAI\nmoderation APIs for our dataset release.\n24\nD\nAdditional Experimental Results\nWe present some additional experimental results.\nD.1\nPosition bias\nWe test two more prompts and present the full results in Table 9 \u201cscore\u201d changes the default prompt\nto let the model output two absolute scores instead of which one is better. \u201cshort\u201d is a simplified\nversion of our default prompt by removing instructions like \u201cAvoid any position bias..\u201d, \u201cBegin your\nevaluation ... and provide a short explanation\u201d. We can find different prompts have different effects\non different models. For example, the \"score\" prompt can increase the consistency of GPT-3.5 but\ndecreases it for Claude-v1 and GPT-4.\nTable 9: Position bias on different models and prompts. Consistency is the percentage of cases where\na judge gives consistent results when swapping the order of two assistants. \u201cBiased toward first\u201d is\nthe percentage of cases when a judge favors the first answer. \u201cError\u201d indicates wrong output formats.\nThe two largest numbers in each column are in bold.\nJudge\nPrompt\nConsistency\nBiased toward first\nBiased toward second\nError\nclaude-v1\ndefault\n23.8%\n75.0%\n0.0%\n1.2%\nrename\n56.2%\n11.2%\n28.7%\n3.8%\nscore\n20.0%\n80.0%\n0.0%\n0.0%\nshort\n22.5%\n75.0%\n2.5%\n0.0%\ngpt-3.5-turbo\ndefault\n46.2%\n50.0%\n1.2%\n2.5%\nrename\n51.2%\n38.8%\n6.2%\n3.8%\nscore\n55.0%\n33.8%\n11.2%\n0.0%\nshort\n38.8%\n57.5%\n3.8%\n0.0%\ngpt-4\ndefault\n65.0%\n30.0%\n5.0%\n0.0%\nrename\n66.2%\n28.7%\n5.0%\n0.0%\nscore\n51.2%\n46.2%\n2.5%\n0.0%\nshort\n62.5%\n35.0%\n2.5%\n0.0%\nAs shown in Table 10, position bias is more noticeable on open questions like writing and stem/hu-\nmanity knowledge questions. On math and coding questions, LLM judges are more confident even\nthough their judgments can often be wrong, as we show in Section 3.3. Finally, we study how the\nmodel pairs influence position bias by using GPT-4 and the default prompt to judge three different\nmodel pairs. As shown in Table 11, the position bias is more noticeable for models with close\nperformance and can almost disappear when the performance of the two models differs a lot.\nTable 10: Position bias on different categories. The two largest numbers in each column are in bold.\nCategory\nConsistent\nBiased toward first\nBiased toward second\nwriting\n42.0%\n46.0%\n12.0%\nroleplay\n68.0%\n30.0%\n2.0%\nreasoning\n76.0%\n20.0%\n4.0%\nmath\n86.0%\n4.0%\n10.0%\ncoding\n86.0%\n14.0%\n0.0%\nextraction\n78.0%\n12.0%\n10.0%\nstem\n44.0%\n54.0%\n2.0%\nhumanities\n36.0%\n60.0%\n4.0%\nTable 11: Position bias on different model pairs.\nPair\nConsistent\nBiased toward first\nBiased toward second\nGPT-3.5 vs Claude-V1\n67.5%\n23.8%\n8.8%\nGPT-3.5 vs Vicuna-13B\n73.8%\n23.8%\n2.5%\nGPT-3.5 vs LLaMA-13B\n98.8%\n1.2%\n0.0%\n25\nD.2\nFew-shot judge\nWe examine how few-shot examples improve LLM judges. As shown in Table 12, they improve the\nconsistency of all three LLM judges significantly. It almost alleviates the position bias of GPT-4, but\nmoves the position bias of GPT-3.5 from the first position to the second position. We then measure\nthe agreement between few-shot GPT-4 pairwise comparison and humans on MT-bench, but found it\nperforms similarly to zero-shot GPT-4 pairwise comparison.\nTable 12: Improvements of the few-shot judge on consistency for position bias.\nModel\nPrompt\nConsistency\nBiased toward first\nBiased toward second\nError\nClaude-v1\nzero-shot\n23.8%\n75.0%\n0.0%\n1.2%\nfew-shot\n63.7%\n21.2%\n11.2%\n3.8%\nGPT-3.5\nzero-shot\n46.2%\n50.0%\n1.2%\n2.5%\nfew-shot\n55.0%\n16.2%\n28.7%\n0.0%\nGPT-4\nzero-shot\n65.0%\n30.0%\n5.0%\n0.0%\nfew-shot\n77.5%\n10.0%\n12.5%\n0.0%\nD.3\nAgreement Evaluation\nAgreement calculation. We define the agreement between two types of judges as the probability\nof randomly selected individuals (but not identical) of each type agreeing on a randomly selected\nquestion. For example, if we are comparing GPT-4 and Claude, the agreement is the probability of\nGPT-4 and Claude agreeing on the vote for a randomly selected question. If we are comparing GPT-4\nand humans, the agreement is the probability of GPT-4 and a randomly selected human agreeing\non the vote for a randomly selected question. The agreement among humans themselves is the\nprobability of two randomly selected but not identical humans agreeing on the vote for a randomly\nselected question.\nNote that the agreement among humans could be a lower estimation compared to the agreement of\nGPT4 and humans. Consider three humans who voted \u201cA\u201d, \u201cA\u201d, and \u201cB\u201d for a question, respectively.\nThe agreement among them is only 1\n3, as there are three pairs \u201c(A, A)\u201d, \u201c(A, B)\u201d, and \u201c(A, B)\u201d. But\nthe agreement between GPT4 and those three is 2\n3 if GPT4 voted \u201cfirst\u201d and 1\n3 otherwise.\nTherefore, to have a more comprehensive understanding of what happened, we introduce a new judge\ntype called human-majority, which considers the majority of human votes for each question. The\nagreement between GPT4 and human-majority is then calculated as the probability of GPT4 agreeing\nwith the majority of human votes on a randomly selected question. The upper bound of the agreement\nbetween GPT-4 and humans is the agreement between human-majority and human. When there is no\nmajority vote for a question, the agreement is counted by an even split. For example, if there are an\nequal number of \u201cA\u201d and \u201cB\u201d human votes for a question, and GPT4 votes \u201cA\u201d, the agreement is\ncounted as 1\n2 on this question.\nMore results. Table 13 shows more agreement results on MT-bench. In addition to expert labelers\n(denoted as \u201cHuman\u201d), we also include author votes (denoted as \u201cAuthor\u201d).\nD.4\nCategory-wise scores with single-answer grading\nWe use single-answer grading to evaluate 6 models on MT-bench and plot the category-wise scores in\nFigure 20.\n26\nTable 13: Agreement between two types of judges on MT-bench. \u201cG4-P\u201d and \u201cG4-S\u201d denote GPT-4\nwith pairwise comparison and single-answer grading, respectively. \u201cC\u201d denotes Claude. \u201cHuman\u201d\ndenotes expert labelers (excluding authors). \u2018Human-M\u201d denotes the majority vote of humans.\nThe single-answer grading can be converted into pairwise comparison results for calculating the\nagreement. We report two setups: \u201cS1\u201d includes non-tie, tie, and inconsistent (due to position bias)\nvotes and counts inconsistent as a tie; \u201cS2\u201d only includes non-tie votes. The agreement between two\nrandom judges under each setup is denoted as \u201cR=\u201d. The top value in each cell is the agreement, and\nthe bottom gray value is #votes.\nSetup\nS1 (R = 33%)\nS2 (R = 50%)\nJudge\nG4-S\nC\nAuthor\nHuman\nHuman-M\nG4-S\nC\nAuthor\nHuman\nHuman-M\nG4-P\n70%\n1138\n63%\n1198\n69%\n345\n66%\n1343\n67%\n821\n97%\n662\n94%\n582\n92%\n201\n85%\n859\n85%\n546\nG4-S\n-\n66%\n1136\n67%\n324\n60%\n1280\n60%\n781\n-\n90%\n563\n94%\n175\n85%\n739\n85%\n473\nC\n-\n-\n58%\n343\n54%\n1341\n55%\n820\n-\n-\n89%\n141\n85%\n648\n86%\n414\nAuthor\n-\n-\n69%\n49\n65%\n428\n55%\n93\n-\n-\n87%\n31\n83%\n262\n76%\n46\nHuman\n-\n-\n-\n63%\n721\n81%\n892\n-\n-\n-\n81%\n479\n90%\n631\n(a) First Turn\nSetup\nS1 (R = 33%)\nS2 (R = 50%)\nJudge\nG4-S\nAuthor\nHuman\nHuman-M\nG4-S\nAuthor\nHuman\nHuman-M\nG4-P\n70%\n1161\n66%\n341\n66%\n1325\n68%\n812\n95%\n727\n88%\n205\n85%\n864\n85%\n557\nG4-S\n-\n65%\n331\n59%\n1285\n61%\n783\n-\n89%\n193\n84%\n776\n85%\n506\nAuthor\n-\n67%\n49\n68%\n413\n63%\n87\n-\n87%\n31\n86%\n273\n84%\n54\nHuman\n-\n-\n67%\n707\n83%\n877\n-\n-\n82%\n474\n91%\n629\n(b) Second Turn\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\n0\n2\n4\n6\n8\n10\nmodel\nGPT-4\nClaude-v1\nGPT-3.5-turbo\nVicuna-13B\nAlpaca-13B\nLLaMA-13B\nFigure 20: Category-wise scores of 6 models on MT-bench.\n27\nE\nTraining Details of Vicuna Models\nVicuna is created by fine-tuning a LLaMA base model using user-shared conversations gathered from\nShareGPT.com with its public APIs. ShareGPT is a website where users can share their ChatGPT\nconversations. To ensure data quality, we convert the HTML back to markdown and filter out some\ninappropriate or low-quality samples, which results in 125K conversations after data cleaning.4 We\nthen divide lengthy conversations into smaller segments that fit the model\u2019s maximum context length.\nWe construct three training datasets with different scales from this cleaned ShareGPT dataset. Their\nstatistics are in Table 8, where we also compare it with Alpaca [38] dataset. \u201cAll\u201d is the full dataset.\n\u201cSingle\u201d only includes the first turn of each conversation. \u201cSelected\u201d is a small high-quality dataset of\n3K sequences. To construct the \u201cSelected\u201d dataset, we pick sequences that include at least 3 turns of\nconversations generated by GPT-4 and run a clustering algorithm to divide them into 3K clusters and\npick the centroid of each cluster.\nAll models (Vicuna-7B/13B) are trained with the same hyperparameters: global batch size=128,\nlearning=2e-5, epochs=3, seq length=2048. Except for \u201cSelected\u201d, which we train for 5 epochs. The\ntraining code is built on top of the Alpaca code but additionally handles multi-turn conversations. The\ntraining is done with 8x A100 GPUs. The longest single training run takes around 2 days. We utilize\nSkyPilot [49] managed spot instances for saving training costs and FlashAttention [11] for memory\noptimizations. The training code is available at https://github.com/lm-sys/FastChat.\nTable 14: Dataset statistics\nDataset Name\nAlpaca\nSelected\nSingle\nAll\n#Token\n4.4M\n4.8M\n184M\n370M\n#Sequence\n52K\n3K\n257K\n257K\nAvg. turns of conversation\n1.0\n4.0\n1.0\n2.9\nAvg. response length (token)\n65\n343\n473\n373\n4In this study, we use more data (125K) than the version in our earlier blog post (70K).\n28\nF\nExploring Vicuna as a judge\nIn this paper, we mostly evaluate the ability of close-sourced models such as GPT-4 as a proxy for\nhuman evaluations. However, model services such as GPT-4 can also become expensive with a\ngrowing number of evaluations. On the other hand, popular open-sourced LLMs, e.g. Vicuna-13B\nshows strong language understanding capability, and are much cheaper than close-sourced LLMs. In\nthis section, we further explore the potential of using Vicuna-13B as a more cost-friendly proxy.\nF.1\nZero-Shot Vicuna\nWhen using as-it-is (zero-shot), Vicuna-13B noticeably suffers from limitations we discuss, e.g.\nposition bias. As shown in Table 15, Vicuna-13B has a consistency rate from 11.2% to 16.2%\nacross different prompt templates, much lower than all the closed-sourced models. In addition, it\nhas a high error rate (from 22.5% to 78.8%) because of its weaker instruction-following capability.\nIn many scenarios, Vicuna-13B provides responses such as \"Answer A is better than answer B\",\nwithout following the pre-defined template. These responses are rendered as natural languages and\nare difficult to be parsed automatically, making the model less useful in a scalable and automatic\nevaluation pipeline.\nF.2\nArena Fine-tuned Vicuna\nTraining\nDue to the incapability of the zero-shot Vicuna-13B model, we further finetune the model\nwith human votes from Chatbot Arena. Specifically, we randomly sample 22K single-turn votes\nfrom the arena, covering all models supported by the time of this paper submission (GPT-4, GPT-3.5,\nClaude-v1, Vicuna-13b, Vicuna-7b, Koala-13B, Alpaca-13B,LLaMA-13B, Dolly-12B, FastChat-T5,\nRWKV-4-Raven, MPT-Chat, OpenAssistant, ChatGLM, and StableLM), to expose the model with\na wider range of chatbot outputs and human preferences. We use 20K votes for training, and 2K\nfor validation. To address the aforementioned weak instruction following problem, we formulate\nthe problem as a 3-way sequence classification problem. Thus, the model simply needs to predict\nwhich one of the chat-bot outputs is better (or tie), without needing to exactly following the provided\nanswer template. In particular, we construct an input by using the default prompt and the two model\nanswers. The labels are A, B, and tie (including both-bad-vote and tie-vote). We train for 3 epochs\nwith a cosine learning rate scheduler and a 2e-5 maximum learning rate. We use the 2K validation\ndataset to choose hyper-parameters, and test on the same 3K dataset in the main body of the paper.\nPosition bias results\nThe results for position bias are provided in Table 15. The consistency\nimproves significantly from 16.2% to 65.0%. Due to the classification formulation, every output is\nrecognizable (error rate 0%). In addition, we measure the classification accuracy over the test dataset.\nAgreement results\nIt achieves 56.8% when including all three labels, and 85.5% when excluding\ntie predictions and labels, significantly outperforming random guesses of 33% and 50% respectively,\nand show positive signals to match GPT-4 (66% and 87% respectively). In conclusion, a further\nfine-tuned Vicuna-13B model shows strong potential to be used as a cheap open-sourced replacement\nfor expensive closed-sourced LLMs. A similar conclusion is also found by a concurrent paper[42].\nTable 15: Position bias of the Vicuna-13B model without and with further fine-tuning. We denote them\nas Vicuna-13B-Zero-Shot and Vicuna-13B-Fine-Tune respectively. Consistency is the percentage\nof cases where a judge gives consistent results when swapping the order of two assistants. \u201cBiased\ntoward first\u201d is the percentage of cases when a judge favors the first answer. \u201cError\u201d indicates wrong\noutput formats. The largest number in each column is in bold.\nJudge\nPrompt\nConsistency\nBiased toward first\nBiased toward second\nError\nVicuna-13B-Zero-Shot\ndefault\n15.0%\n53.8%\n8.8%\n22.5%\nrename\n16.2%\n12.5%\n40.0%\n31.2%\nscore\n11.2%\n10.0%\n0.0%\n78.8%\nVicuna-13B-Fine-Tune\ndefault\n65.0%\n27.5%\n7.5%\n0.0%\n29\n"
  },
  {
    "title": "BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping",
    "link": "https://arxiv.org/pdf/2306.05544.pdf",
    "upvote": "9",
    "text": "BOOT\n: Data-free Distillation of Denoising Diffusion\nModels with Bootstrapping\nJiatao Gu\u2020, Shuangfei Zhai\u2020, Yizhe Zhang\u2020, Lingjie Liu\u2021, Josh Susskind\u2020\n\u2020Apple\n\u2021University of Pennsylvania\n\u2020{jgu32, szhai, yizzhang, jsusskind}@apple.com\n\u2021lingjie.liu@seas.upenn.edu\nAbstract\nDiffusion models have demonstrated excellent potential for generating diverse\nimages. However, their performance often suffers from slow generation due\nto iterative denoising. Knowledge distillation has been recently proposed as a\nremedy which can reduce the number of inference steps to one or a few, without\nsignificant quality degradation. However, existing distillation methods either\nrequire significant amounts of offline computation for generating synthetic training\ndata from the teacher model, or need to perform expensive online learning with\nthe help of real data. In this work, we present a novel technique called BOOT, that\novercomes these limitations with an efficient data-free distillation algorithm. The\ncore idea is to learn a time-conditioned model that predicts the output of a pre-\ntrained diffusion model teacher given any time-step. Such a model can be efficiently\ntrained based on bootstrapping from two consecutive sampled steps. Furthermore,\nour method can be easily adapted to large-scale text-to-image diffusion models,\nwhich are challenging for conventional methods given the fact that the training\nsets are often large and difficult to access. We demonstrate the effectiveness of our\napproach on several benchmark datasets in the DDIM setting, achieving comparable\ngeneration quality while being orders of magnitude faster than the diffusion teacher.\nThe text-to-image results show that the proposed approach is able to handle highly\ncomplex distributions, shedding light on more efficient generative modeling. Please\ncheck our project page: https://jiataogu.me/boot/ for more details.\n1\nIntroduction\nFigure 2: Comparison of Consistency Model (Song\net al., 2023) (red \u2191) and BOOT (black \u2193) highlight-\ning the opposing prediction pathways.\nDiffusion models (Sohl-Dickstein et al., 2015;\nHo et al., 2020; Nichol & Dhariwal, 2021;\nSong et al., 2020b) have become the standard\ntools for generative applications, such as im-\nage (Dhariwal & Nichol, 2021; Rombach et al.,\n2021; Ramesh et al., 2022; Saharia et al., 2022),\nvideo (Ho et al., 2022b,a), 3D (Poole et al.,\n2022; Gu et al., 2023; Liu et al., 2023b; Chen\net al., 2023), audio (Liu et al., 2023a), and\ntext (Li et al., 2022; Zhang et al., 2023) gen-\neration. Diffusion models are considered more\nstable for training compared to alternative ap-\nproaches like GANs (Goodfellow et al., 2014a)\nor VAEs (Kingma & Welling, 2013), as they\ndon\u2019t require balancing two modules, making\nthem less susceptible to issues like mode collapse or posterior collapse. Despite their empirical\nPreprint. Under review.\narXiv:2306.05544v1  [cs.CV]  8 Jun 2023\nFigure 1: Curated samples of our distilled single-step model with prompts from diffusiondb.\n2\nsuccess, standard diffusion models often have slow inference times (around 50 \u223c 1000\u00d7 slower than\nsingle-step models like GANs), which poses challenges for deployment on consumer devices. This is\nmainly because diffusion models use an iterative refinement process to generate samples.\nTo address this issue, previous studies have proposed using knowledge distillation to improve the\ninference speed (Hinton et al., 2015). The idea is to train a faster student model that can replicate the\noutput of a pre-trained diffusion model. In this work, we focus on learning single-step models that\nonly require one neural function evaluation (NFE). However, conventional methods, such as Luhman\n& Luhman (2021), require executing the full teacher sampling to generate synthetic targets for every\nstudent update, which is impractical for distilling large diffusion models like StableDiffusion (SD,\nRombach et al., 2021). Recently, several techniques have been proposed to avoid sampling using\nthe concept of \"bootstrap\". For example, Salimans & Ho (2022) gradually reduces the number of\ninference steps based on the previous stage\u2019s student, while Song et al. (2023) and Berthelot et al.\n(2023) train single-step denoisers by enforcing self-consistency between adjacent student outputs\nalong the same diffusion trajectory (see Fig. 2). However, these approaches rely on the availability\nof real data to simulate the intermediate diffusion states as input, which limits their applicability in\nscenarios where the desired real data is not accessible.\nIn this paper, we propose BOOT, a data-free knowledge distillation method for denoising diffusion\nmodels based on bootstrapping. BOOT is partially motivated by the observation made by consistency\nmodel (CM, Song et al., 2023) that all points on the same diffusion trajectory (also known as PF-\nODE (Song et al., 2020b)) have a deterministic mapping between each other. Unlike CM, which\nseeks self-consistency from any xt to x0, BOOT predicts all possible xt given the same noise point\n\u03f5 and a time indicator t. Since our model g\u03b8 always reads pure Gaussian noise, there is no need to\nsample from real data. Moreover, learning all xt from the same \u03f5 enables bootstrapping: it is easier\nto predict xt if the model has already learned to generate xt\u2032 where t\u2032 > t. However, formulating\nbootstrapping in this way presents additional challenges, such as noisy sample prediction, which is\nnon-trivial for neural networks. To address this, we learn the student model from a novel Signal-ODE\nderived from the original PF-ODE. We also design objectives and boundary conditions to enhance the\nsampling quality and diversity. This enables efficient inference of large diffusion models in scenarios\nwhere the original training corpus is inaccessible due to privacy or other concerns. For example,\nwe can obtain an efficient model for synthesizing images of \"raccoon astronaut\" by distilling the\ntext-to-image model with the corresponding prompts (shown in Fig. 3), even though collecting such\ndata in reality is difficult.\nIn the experiments, we first demonstrate the efficacy of BOOT on various challenging image gener-\nation benchmarks, including unconditional and class-conditional settings. Next, we show that the\nproposed method can be easily adopted to distill text-to-image diffusion models. An illustration of\nsampled images from our distilled text-to-image model is shown in Fig. 1.\n2\nPreliminaries\n2.1\nDiffusion Models\nDiffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) belong to a\nclass of deep generative models that generate data by progressively removing noise from the initial\ninput. In this work, we focus on continuous-time diffusion models (Song et al., 2020b; Kingma et al.,\n2021; Karras et al., 2022) in the variance-preserving formulation (Salimans & Ho, 2022). Given a\ndata point x \u2208 RN, we model a series of time-dependent latent variables {xt|t \u2208 [0, T], x0 = x}\nbased on a given noise schedule {\u03b1t, \u03c3t}:\nq(xt|xs) = N(xt; \u03b1t|sxs, \u03c32\nt|sI), and q(xt|x) = N(xt; \u03b1tx, \u03c32\nt I),\nwhere \u03b1t|s = \u03b1t/\u03b1s and \u03c32\nt|s = \u03c32\nt \u2212 \u03b12\nt|s\u03c32\ns for s < t. By default, the signal-to-noise ratio (SNR,\n\u03b12\nt /\u03c32\nt ) decreases monotonically with t. A diffusion model f\u03d5 learns to reverse the diffusion process\nby denoising xt, which can be easily sampled given the real data x with q(xt|x):\nLDiff\n\u03d5\n= Ext\u223cq(xt|x),t\u223c[0,T ]\n\u0002\n\u03c9t \u00b7 \u2225f\u03d5(xt, t) \u2212 x\u22252\n2\n\u0003\n.\n(1)\nHere, \u03c9t is the weight used to balance perceptual quality and diversity. The parameterization of \u03d5\ntypically involves U-Net (Ronneberger et al., 2015; Dhariwal & Nichol, 2021) or Transformer (Peebles\n3\n& Xie, 2022; Bao et al., 2022). In this paper, we use f\u03d5 to represent signal predictions. However,\ndue to the mathematical equivalence of signal, noise, and v-predictions (Salimans & Ho, 2022) in\nthe denoising formulation, the loss function can also be defined based on noise or v-predictions. For\nsimplicity, we use f\u03d5 for all cases in the remainder of the paper.\nOne can use ancestral sampling (Ho et al., 2020) to synthesize new data from the learned model.\nWhile the conventional method is stochastic, DDIM (Song et al., 2020a) demonstrates that one can\nfollow a deterministic sampler to generate the final sample x0, which follows the update rule:\nxs = (\u03c3s/\u03c3t) xt + (\u03b1s \u2212 \u03b1t\u03c3s/\u03c3t) f\u03d5(xt, t),\ns < t,\n(2)\nwith the boundary condition xT = \u03f5 \u223c N(0, I). As noted in Lu et al. (2022), Eq. (2) is equivalent\nto the first-order ODE solver for the underlying probability-flow (PF) ODE (Song et al., 2020b).\nTherefore, the step size \u03b4 = t \u2212 s needs to be small to mitigate error accumulation. Additionally,\nusing higher-order solvers such as Runge-Kutta (S\u00fcli & Mayers, 2003), Heun (Ascher & Petzold,\n1998), and other solvers (Lu et al., 2022; Jolicoeur-Martineau et al., 2021) can further reduce the\nnumber of function evaluations (NFEs). However, these approaches are not applicable in single-step.\n2.2\nKnowledge Distillation\nOrthogonal to the development of ODE solvers, distillation-based techniques have been proposed to\nlearn faster student models from a pre-trained diffusion teacher. The most straightforward approach\nis to perform direct distillation (Luhman & Luhman, 2021), where a student model g\u03b8 is trained to\nlearn from the output of the diffusion model, which is computationally expensive itself:\nLDirect\n\u03b8\n= E\u03f5\u223cN (0,I)\u2225g\u03b8(\u03f5) \u2212 ODE-Solver(f\u03d5, \u03f5, T \u2192 0)\u22252\n2,\n(3)\nHere, ODE-solver refers to any solvers like DDIM as mentioned above. While this naive approach\nshows promising results, it typically requires over 50 steps of evaluations to obtain reasonable\ndistillation targets, which becomes a bottleneck when learning large-scale models.\nAlternatively, recent studies (Salimans & Ho, 2022; Song et al., 2023; Berthelot et al., 2023) have\nproposed methods to avoid running the full diffusion path during distillation. For instance, the\nconsistency model (CM, Song et al., 2023) trains a time-conditioned student model g\u03b8(xt, t) to\npredict self-consistent outputs along the diffusion trajectory in a bootstrap fashion:\nLCM\n\u03b8\n= Ext\u223cq(xt|x),s,t\u223c[0,T ],s<t\u2225g\u03b8(xt, t) \u2212 g\u03b8\u2212(xs, s)\u22252\n2,\n(4)\nwhere xs = ODE-Solver(f\u03d5, xt, t \u2192 s), typically with a single-step evaluation using Eq. (2). In\nthis case, \u03b8\u2212 represents an exponential moving average (EMA) of the student parameters \u03b8, which is\nimportant to prevent the self-consistency objectives from collapsing into trivial solutions by always\npredicting similar outputs. After training, samples can be generated by executing g\u03b8(xT , T) with a\nsingle NFE. It is worth noting that Eq. (4) requires sampling xt from the real data sample x, which\nis the essence of bootstrapping: the model learns to denoise increasingly noisy inputs until xT .\nHowever, in many tasks, the original training data x for distillation is inaccessible. For example,\ntext-to-image generation models require billions of paired data for training. One possible solution\nis to use a different dataset for distillation; however, the mismatch in the distributions of the two\ndatasets would result in suboptimal distillation performance.\n3\nMethod\nIn this section, we present BOOT, a novel distillation approach inspired by the concept of bootstrap-\nping without requiring target domain data during training. We begin by introducing signal-ODE, a\nmodeling technique focused exclusively on signals (\u00a7 3.1), and its corresponding distillation process\n(\u00a7 3.2). Subsequently, we explore the application of BOOT in text-to-image generation (\u00a7 3.3). The\ntraining pipeline is depicted in Fig. 3, providing an overview of the process.\n3.1\nSignal-ODE\nWe utilize a time-conditioned student model g\u03b8(\u03f5, t) in our approach. Similar to direct distilla-\ntion (Luhman & Luhman, 2021), BOOT always takes random noise \u03f5 as input and approximates the\nintermediate diffusion model variable: g\u03b8(\u03f5, t) \u2248 xt = ODE-Solver(f\u03d5, \u03f5, T \u2192 t), \u03f5 \u223c N(0, I).\n4\nFigure 3: Training pipeline of BOOT. s and t are two consecutive timesteps where s < t. From a\nnoise map \u03f5, the objective of BOOT minimizes the difference between the output of a student model\nat timestep s, and the output of stacking the same student model and a teacher model at an earlier\ntime t. The whole process is data-free.\nThis approach eliminates the need to sample from real data during training. The final sample can be\nobtained as g\u03b8(\u03f5, 0) \u2248 x0. However, it poses a challenge to train g\u03b8 effectively, as neural networks\nstruggle to predict partially noisy images (Berthelot et al., 2023), leading to out-of-distribution (OOD)\nproblems and additional complexities in learning g\u03b8 accurately.\nTo overcome the aforementioned challenge, we propose an alternative approach where we predict\nyt = (xt \u2212 \u03c3t\u03f5)/\u03b1t. In this case, yt represents the low-frequency \"signal\" component of xt, which\nis easier for neural networks to learn. The initial noise for diffusion is denoted by \u03f5. This prediction\ntarget is reasonable since it aligns with the boundary condition of the teacher model, where y0 = x0.\nFurthermore, we can derive an iterative equation from Eq. (2) for consecutive timesteps:\nys =\n\u00001 \u2212 e\u03bbs\u2212\u03bbt\u0001\nf\u03d5(xt, t) + e\u03bbs\u2212\u03bbtyt,\n(5)\nwhere xt = \u03b1tyt + \u03c3t\u03f5, and \u03bbt = \u2212 log(\u03b1t/\u03c3t) represents the \"negative half log-SNR.\" Notably,\nthe noise term \u03f5 automatically cancels out in Eq. (5), indicating that the model always learns from the\nsignal space. Moreover, Eq. (5) demonstrates an interpolation between the current model prediction\nand the diffusion-denoised output. Similar to the connection between DDIM and PF-ODE (Song\net al., 2020b), we can also obtain a continuous version of Eq. (5) by letting s \u2192 t\u2212 as follows:\ndyt\ndt = \u2212\u03bb\u2032\nt \u00b7 (f\u03d5(xt, t) \u2212 yt) ,\nyT \u223c p\u03f5\n(6)\nwhere \u03bb\u2032\nt = d\u03bb/dt, and p\u03f5 epresents the boundary distribution of yt. It\u2019s important to note that\nEq. (6) differs from the PF-ODE, which directly relates to the score function of the data. In our\ncase, the ODE, which we refer to as \"Signal-ODE,\" is specifically defined for signal prediction. At\neach timestep t, a fixed noise \u03f5 is injected and denoised by the diffusion model f\u03d5. The Signal-\nODE implies a \"ground-truth\" trajectory for sampling new data. For example, one can initialize a\nreasonable yT = \u03f5 \u223c N(0, I) and solve the Signal-ODE to obtain the final output y0. Although the\ncomputational complexity remains the same as conventional DDIM, we will demonstrate in the next\nsection how we can efficiently approximate yt using bootstrapping objectives.\n3.2\nLearning with Bootstrapping\nOur objective is to learn y\u03b8(\u03f5, t) \u2248 yt as a single-step prediction model using neural networks, rather\nthan solving the signal-ODE with Eq. (6). By matching both sides of Eq. (6), we can readily obtain\nthe loss function:\nLDE\n\u03b8\n= E\u03f5\u223cN (0,I),t\u223c[0,T ]\n\f\f\f\f\n\f\f\f\f\ndy\u03b8(\u03f5, t)\ndt\n+ \u03bb\u2032\nt \u00b7 (f\u03d5(\u02c6xt, t) \u2212 y\u03b8(\u03f5, t))\n\f\f\f\f\n\f\f\f\f\n2\n2\n.\n(7)\nIn Eq. (7), we use y\u03b8(\u03f5, t) to estimate yt, and \u02c6xt = \u03b1ty\u03b8(\u03f5, t) + \u03c3t\u03f5 represents the corresponding\nnoisy image. Instead of using forward-mode auto-differentiation, which can be computationally\nexpensive, we can approximate the above equation with finite differences due to the 1-dimensional\n5\nFigure 4: Comparison between the generated outputs of DDIM/Signal-ODE and our distilled model\ngiven the same prompt A raccoon wearing a space suit, wearing a helmet. Oil painting in the style of\nRembrandt and initial noise input. By definition, signal-ODE converges to the same final sample as\nthe original DDIM, while the distilled single-step model does not necessarily follow.\nnature of t. The approximate form is similar to Eq. (5):\nLBS\n\u03b8\n= E\u03f5\u223cN (0,I),t\u223c[\u03b4,T ]\n\uf8ee\n\uf8ef\uf8f0 \u02dcwt\n\u03b42\n\f\f\f\f\f\f\f\n\f\f\f\f\f\f\f\ny\u03b8(\u03f5, s) \u2212 SG\n\uf8ee\n\uf8ef\uf8f0y\u03b8(\u03f5, t) + \u03b4\u03bb\u2032\n| t \u00b7 ((f\u03d5(\u02c6xt, t)) \u2212 y\u03b8(\u03f5, t))\n{z\n}\nincremental improvement\n\uf8f9\n\uf8fa\uf8fb\n\f\f\f\f\f\f\f\n\f\f\f\f\f\f\f\n2\n2\n\uf8f9\n\uf8fa\uf8fb ,\n(8)\nwhere s = t \u2212 \u03b4 and \u03b4 is the discrete step size. \u02dcwt represents the time-dependent loss weighting,\nwhich can be chosen uniformly. We use SG[.] as the stop-gradient operator for training stability.\nUnlike CM-based methods, such as those mentioned in Eq. (4), we do not require an exponential\nmoving average (EMA) copy of the student parameters to avoid collapsing. This avoids potential\nslow convergence and sub-optimal solutions. As shown in Eq. (8), the proposed objective is unlikely\nto degenerate because there is an incremental improvement term in the training target, which is\nmostly non-zero. In other words, we can consider y\u03b8 as an exponential moving average of f\u03d5, with a\ndecaying rate of 1 \u2212 \u03b4\u03bb\u2032\nt. This ensures that the student model always receives distinguishable signals\nfor different values of t.\nError Accumulation\nA critical challenge in learning BOOT is the \"error accumulation\" issue,\nwhere imperfect predictions of y\u03b8 on large t can propagate to subsequent timesteps. While similar\nchallenges exist in other bootstrapping-based approaches, it becomes more pronounced in our case\ndue to the possibility of out-of-distribution inputs \u02c6xt for the teacher model, resulting from error\naccumulation and leading to incorrect learning signals. To mitigate this, we employ two methods: (1)\nWe uniformly sample t throughout the training time, despite the potential slowdown in convergence.\n(2) We use a higher-order solver (e.g., Heun\u2019s method (Ascher & Petzold, 1998)) to compute the\nbootstrapping target with better estimation.\nBoundary Condition\nIn theory, the boundary yT can have arbitrary values since \u03b1T = 0, and\nthe value of yT does not affect the value xT = \u03f5. However, \u03bb\u2032\nt is unbounded at t = T, leading to\nnumerical issues in optimization. As a result, the student model must be learned within a truncated\nrange t \u2208 [tmin, tmax]. This necessitates additional constraints at the boundaries to ensure that\n\u03b1tmaxy\u03b8(\u03f5, tmax) + \u03c3tmax\u03f5 follows the same distribution as the diffusion model. In this work, we\n6\nSteps\nFFHQ 64 \u00d7 64\nLSUN 256 \u00d7 256\nImageNet 64 \u00d7 64\nFID / Prec. / Rec.\nfps\nFID / Prec. / Rec.\nfps\nFID / Prec. / Rec.\nfps\nDDPM\n250\n5.4 / 0.80 / 0.54\n0.2\n8.2 / 0.55 / 0.43\n0.1\n11.0 / 0.67 / 0.58\n0.1\nDDIM\n50\n7.6 / 0.79 / 0.48\n1.2\n13.5 / 0.47 / 0.40\n0.6\n13.7 / 0.65 / 0.56\n0.6\n10\n18.3 / 0.78 / 0.27\n5.3\n31.0 / 0.27 / 0.32\n3.1\n18.3 / 0.60 / 0.49\n3.3\n1\n225 / 0.10 / 0.00\n54\n308 / 0.00 / 0.00\n31\n237 / 0.05 / 0.00\n34\nOurs\n1\n9.0 / 0.79 / 0.38\n54\n23.4 / 0.38 / 0.29\n32\n16.3 / 0.68 / 0.36\n34\nTable 1: Comparison for image generation benchmarks on FFHQ, LSUN and class-conditioned\nImageNet. For ImageNet, numbers are reported without using CFG (w = 1).\naddress this through an auxiliary boundary loss:\nLBC\n\u03b8\n= E\u03f5\u223cN (0,I)\n\u0002\n\u2225f\u03d5(\u03f5, tmax) \u2212 y\u03b8(\u03f5, tmax)\u22252\n2\n\u0003\n.\n(9)\nHere, we enforce the student model to match the initial denoising output. In our early exploration, we\nfound that the boundary condition is crucial for the single-step student to fully capture the modeling\nspace of the teacher, especially in text-to-image scenarios. Failure to learn the boundaries tends to\nresult in severe mode collapse and color-saturation problems.\nThe overall learning objective combines L\u03b8 = LBS\n\u03b8 + \u03b2LBC\n\u03b8 , where \u03b2 is a hyper-parameter. The\nalgorithm for student model distillation is presented in Appendix Algorithm 1.\n3.3\nDistillation of Text-to-Image Models\nDistillation with Guidance\nOur approach can be readily applied for distilling conditional diffusion\nmodels, such as text-to-image generation (Ramesh et al., 2022; Rombach et al., 2021; Balaji et al.,\n2022), where a conditional denoiser f\u03d5(xt, t, c) is learned with the same objective given an aligned\ndataset. In practice, inference of these models requires necessary post-processing steps for augmenting\nthe conditional generation. For instance, one can perform classifier-free guidance (CFG, Ho &\nSalimans, 2022) to amplify the conditioning:\n\u02dcf\u03d5(xt, t, c) = f\u03d5(xt, t, n) + w \u00b7 (f\u03d5(xt, t, c) \u2212 f\u03d5(xt, t, n)) ,\n(10)\nwhere n is the negative prompt (or empty), and w is the guidance weight (by default w = 7.5)\nover the denoised signals. We directly use the modified \u02dcf\u03d5 to replace the original f\u03d5 in the training\nobjectives in Eqs. (8) and (9). Optionally, similar to Meng et al. (2022), we can also learn student\nmodel condition on both t and w to reflect different guidance strength.\nPixel or Latent\nOur method can be easily adopted in either pixel (Saharia et al., 2022) or latent\nspace (Rombach et al., 2021) models without specific code change. For pixel-space models, it is\nsometimes critical to apply clipping or dynamic thresholding (Saharia et al., 2022) over the denoised\ntargets to avoid over-saturation. Similarly, we also clip the targets in our objectives Eqs. (8) and (9).\nPixel-space models (Saharia et al., 2022) typically involve learning cascaded models (one base model\n+ a few super-resolution (SR) models) to increase the output resolutions progressively. We can also\ndistill the SR models with BOOT into one step by conditioning both the SR teacher and the student\nwith the output of the distilled base model.\n4\nExperiments\n4.1\nExperimental Setups\nDiffusion Model Teachers\nWe begin by evaluating the performance of BOOT on diffusion models\ntrained on standard image generation benchmarks: FFHQ 64 \u00d7 64 (Karras et al., 2017), class-\nconditional ImageNet 64 \u00d7 64 (Deng et al., 2009) and LSUN Bedroom 256 \u00d7 256 (Yu et al., 2015).\nTo ensure a fair comparison, we train all teacher diffusion models separately on each dataset using the\nsignal prediction objective. Additionally, for ImageNet, we test the performance of CFG where the\nstudent models are trained with random conditioning on w \u2208 [1, 5] (see the effects of w in Fig. 7).\n7\nFigure 5: Uncurated samples of {50, 10, 1} DDIM sampling steps and the proposed BOOT from (a)\nFFHQ (b) LSUN (c) ImageNet benchmarks, respectively, given the same set of initial noise input.\nFor text-to-image generation scenarios, we directly apply BOOT on open-sourced diffusion models in\nboth pixel-space (DeepFloyd-IF (IF), Saharia et al., 2022) * and latents space (StableDiffusion (SD),\nRombach et al., 2021) \u2020. Thanks to the data-free nature of BOOT, we do not require access to the\noriginal training set, which may consist of billions of text-image pairs with unknown preprocessing\nsteps. Instead, we only need the prompt conditions to distill both models. In this work, we consider\ngeneral-purpose prompts generated by users. Specifically, we utilize diffusiondb (Wang et al., 2022),\na large-scale prompt dataset that contains 14 million images generated by StableDiffusion using\nprompts provided by real users. We only utilize the text prompts for distillation.\nImplementation Details\nSimilar to previous research (Song et al., 2023), we use student models\nwith architectures similar to those of the teachers, having nearly identical numbers of parameters.\nA more comprehensive architecture search is left for future work. We initialize the majority of the\nstudent y\u03b8 parameters with the teacher model f\u03d5, except for the newly introduced conditioning\nmodules (target timestep t and potentially the CFG weight w), which are incorporated into the\nU-Net architecture in a similar manner as how class labels were incorporated. It is important to note\nthat the target timestep t is different from the original timestep used for conditioning the diffusion\nmodel, which is always set to tmax for the student model. Based on the actual implementation\nof the teacher models, we initialize the student output accordingly to accommodate the pretrained\nweights: y\u03b8(\u03f5) = (NNx(\u03f5)) \u2228 (\u03f5 \u2212 NN\u03f5(\u03f5)) \u2228 (\u2212NNv(\u03f5)), where \u2228 represents \u201cor\u201d and NNx, NN\u03f5, NNv\ncorrespond to the pre-trained teacher networks using the signal, noise or velocity (Salimans & Ho,\n2022) parameterization, respectively. We include additional details in the Appendix C.\nEvaluation Metrics\nFor image generation, results are compared according to Fr\u00e9chet Inception\nDistance ((FID, Heusel et al., 2017), lower is better), Precision ((Prec., Kynk\u00e4\u00e4nniemi et al., 2019),\nhigher is better), and Recall ((Rec., Kynk\u00e4\u00e4nniemi et al., 2019), higher is better) over 50, 000 real\nsamples from the corresponding datasets. For text-to-image tasks, we measure the zero-shot CLIP\nscore (Radford et al., 2021) for measuring the faithfulness of generation given 5000 randomly sampled\ncaptions from COCO2017 (Lin et al., 2014) validation set. In addition, we also report the inference\nspeed measured by fps with batch-size 1 on single A100 GPU.\n4.2\nResults\nQuantitative Results\nWe first evaluate the proposed method on standard image generation bench-\nmarks. The quantitative comparison with the standard diffusion inference methods like DDPM (Ho\net al., 2020) and the deterministic DDIM (Song et al., 2020a) are shown in Table 1. Despite lagging\nbehind the 50-step DDIM inference, BOOT significantly improves the performance 1-step inference,\n*https://github.com/deep-floyd/IF\n\u2020https://github.com/Stability-AI/stablediffusion\n8\nFigure 6: Uncurated samples of {50, 10, 1} DDIM sampling steps and the proposed BOOT from\nSD2.1-base, given the same set of initial noise input and prompts sampled from diffusiondb.\nFigure 7: The distilled student is able to trade generation quality with diversity based on CFG weights.\nand achieves better performance against DDIM with around 10 denoising steps, while maintaining\n\u00d710 speed-up. Note that, the speed advantage doubles if the teacher employs guidance.\nWe also conduct quantitative evaluation on text-to-image tasks. Using the SD teacher, we obtain a\nCLIP-score of 0.254 on COCO2017, a slight degradation compared to the 50-step DDIM results\n(0.262), while it generates 2 orders of magnitude faster, rendering real-time applications.\nVisual Results\nWe show the qualitative comparison in Figs. 5 and 6 for image generation and\ntext-to-image, respectively. For both cases, nav\u00efe 1-step inference fails completely, and the diffusion\ngenerally outputs grey and ill-structured images with fewer than 10 NFEs. In contrast, BOOT is able\nto synthesize high-quality images that are visually close (Fig. 5) or semantically similar (Fig. 6) to\nteacher\u2019s results with much more steps. Unlike the standard benchmarks, distilling text-to-image\nmodels (e.g., SD) typically leads to noticeably different generation from the original diffusion model,\neven starting with the same initial noise. We hypothesize it is a combined effect of highly complex\nunderlying distribution and CFG. We show more results including pixel-space models in the appendix.\n9\nFigure 8: Ablation Study. (a) vs. (b): The additional boundary loss in \u00a7 3.2 alleviates the mode\ncollapsing issue and prompts diversity in generation. (c) vs. (d): Uniform time training yields better\ngeneration compared with progressive time training.\nFigure 9: Latent space interpolation of the student model distilled from the IF teacher. We randomly\nsample two noises to generate images (shown in red boxes) given the same text prompts, and then\nlinearly interpolate the noises to synthesize images shown in the middle.\n4.3\nAnalysis\nImportance of Boundary Condition\nThe significance of incorporating the boundary loss is\ndemonstrated in Fig. 8 (a) and (b). When using the same noise inputs, we compare the student\noutputs based on different target timesteps. As y\u03b8(\u03f5, t) tracks the signal-ODE output, it produces\nmore averaged results as t approaches 1. However, without proper boundary constraints, the student\noutputs exhibit consistent sharpness across timesteps, resulting in over-saturated and non-realistic\nimages. This indicates a complete failure of the learned student model to capture the distribution of\nthe teacher model, leading to severe mode collapse.\nProgressive v.s. Uniform Time Training\nWe also compare different training strategies in Fig. 8\n(c) and (d). In contrast to the proposed approach of uniformly sampling t, one can potentially achieve\nadditional efficiency with a fixed schedule that progressively decreases t as training proceeds. This\nprogressive training strategy seems reasonable considering that the student is always initialized from\ntmax and gradually learns to predict the clean signals (small t) during training. However, progressive\ntraining tends to introduce more artifacts (as observed in the visual comparison in Fig. 8). We\nhypothesize that progressive training is more prone to accumulating irreversible errors.\n10\nFigure 10: With fixed noise, we can perform controllable generation by swapping the keywords from\nthe prompts. The prompts are chosen from the combination of portrait of a {owl, raccoon, tiger, fox,\nllama, gorilla, panda} wearing { a t-shirt, a jacket, glasses, a crown} { drinking a latte, eating a\npizza, reading a book, holding a cake} cinematic, hdr. All images are generated from the student\ndistilled from IF teacher given the same noise input.\nControllable Generation\nIn Fig. 9, we visualize the results of latent space interpolation, where the\nstudent model is distilled from the pretrained IF teacher. The smooth transition of the generated images\ndemonstrates that the distilled student model has successfully learned a continuous and meaningful\nlatent space. Additionally, in Fig. 10, we provide an example of text-controlled generation by fixing\nthe noise input and only modifying the prompts. Similar to the original diffusion teacher model,\nthe BOOT distilled student retains the ability of disentangled representation, enabling fine-grained\ncontrol while maintaining consistent styles.\n5\nRelated Work\nImproving Efficiency of Diffusion Models\nSpeeding up inference of diffusion models is a broad\narea. Recent works and also our work (Luhman & Luhman, 2021; Salimans & Ho, 2022; Meng\net al., 2022; Song et al., 2023; Berthelot et al., 2023) aim at reducing the number of diffusion model\ninference steps via distillation. Aside from distillation methods, other representative approaches\ninclude advanced ODE solvers (Karras et al., 2022; Lu et al., 2022), low-dimension space diffu-\nsion (Rombach et al., 2021; Vahdat et al., 2021; Jing et al., 2022; Gu et al., 2022), and improved\ndiffusion targets (Lipman et al., 2023; Liu et al., 2022). BOOT is orthogonal and complementary to\nthese approaches, and can theoretically benefit from improvements made in all these aspects.\nKnowledge Distillation for Generative Models\nKnowledge distillation (Hinton et al., 2015)\nhas seen successful applications in learning efficient generative models, including model com-\npression (Kim & Rush, 2016; Aguinaldo et al., 2019; Fu et al., 2020; Hsieh et al., 2023) and\nnon-autoregressive sequence generation (Gu et al., 2017; Oord et al., 2018; Zhou et al., 2019). We\nbelieve that BOOT could inspire a new paradigm of distilling powerful generative models without\nrequiring access to the training data.\n6\nDiscussion and Conclusion\nLimitations\nBOOT is a knowledge distillation algorithm, which by nature requires a pre-trained\nteacher model. Also by design, the sampling quality of BOOT is upper bounded by that of the teacher.\nBesides, BOOT may produce lower quality samples compared to other distillation methods (Song\n11\net al., 2023; Berthelot et al., 2023) where ground-truth data are easy to use, which can potentially be\nremedied by combining methods.\nFuture Work\nAs future research, we aim to investigate the possibility of jointly training the teacher\nand the student models in a manner that incorporates the concept of diffusion into the distillation\nprocess. By making the diffusion process \"distillation aware,\" we anticipate improved performance\nand more effective knowledge transfer. Furthermore, we find it intriguing to explore the training of a\nsingle-step diffusion model from scratch. This exploration could provide insights into the applicability\nand benefits of BOOT in scenarios where a pre-trained model is not available.\nConclusion\nIn summary, this paper introduced a novel technique BOOT to distill diffusion models\ninto single step. The method did not require the presence of any real or synthetic data by learning\na time-conditioned student model with bootstrapping objectives. The proposed approach achieved\ncomparable generation quality while being significantly faster than the diffusion teacher, and was\nalso applicable to large-scale text-to-image generation, showcasing its versatility.\nAcknowledgement\nWe thank Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Laurent Dinh, Shiwei Li, Samira\nAbnar, Etai Littwin for their critical suggestions and valuable feedback to this project.\nReferences\nAngeline Aguinaldo, Ping-Yeh Chiang, Alex Gain, Ameya Patil, Kolten Pearson, and Soheil Feizi. Compressing\ngans using knowledge distillation. arXiv preprint arXiv:1902.00159, 2019.\nUri M Ascher and Linda R Petzold. Computer methods for ordinary differential equations and differential-\nalgebraic equations, volume 61. Siam, 1998.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo\nAila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert\ndenoisers. arXiv preprint arXiv:2211.01324, 2022.\nFan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion\nmodels. arXiv preprint arXiv:2209.12152, 2022.\nDavid Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter\nTalbot, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint\narXiv:2303.04248, 2023.\nHansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion\nnerf: A unified approach to 3d generation and reconstruction, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-scale Hierarchical\nImage Database. IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural\nInformation Processing Systems, 34:8780\u20138794, 2021.\nYonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, and Zhangyang Wang. Autogan-distiller:\nSearching to compress generative adversarial networks. arXiv preprint arXiv:2006.08198, 2020.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27,\npp. 2672\u20132680. Curran Associates, Inc., 2014a. URL https://proceedings.neurips.cc/paper/2014/\nfile/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014b.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural\nmachine translation. arXiv preprint arXiv:1711.02281, 2017.\n12\nJiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, and Josh Susskind. f-dm: A multi-stage\ndiffusion model via progressive signal transformation. arXiv preprint arXiv:2210.04955, 2022.\nJiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. arXiv preprint\narXiv:2302.10109, 2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by\na two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing\nsystems, 30, 2017.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma,\nBen Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with\ndiffusion models. arXiv preprint arXiv:2210.02303, 2022a.\nJonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video\ndiffusion models. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022b.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay\nKrishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models\nwith less training data and smaller model sizes, 2023.\nBowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion generative models.\narXiv preprint arXiv:2205.01490, 2022.\nAlexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta go fast\nwhen generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,\nstability, and variation. arXiv preprint arXiv:1710.10196, 2017.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models. arXiv preprint arXiv:2206.00364, 2022.\nYoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprint arXiv:1606.07947,\n2016.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural\ninformation processing systems, 34:21696\u201321707, 2021.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nTuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and\nrecall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019.\nXiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves\ncontrollable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. European Conference on Computer\nVision, pp. 740\u2013755, 2014.\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for\ngenerative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=PqvMRDCJT9t.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley.\nAudioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023a.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023b.\n13\nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data\nwith rectified flow. arXiv preprint arXiv:2209.03003, 2022.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for\ndiffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.\nEric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling\nspeed. arXiv preprint arXiv:2101.02388, 2021.\nChenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation\nof guided diffusion models. arXiv preprint arXiv:2210.03142, 2022.\nAlexander Quinn Nichol and Prafulla Dhariwal.\nImproved denoising diffusion probabilistic models.\nIn\nInternational Conference on Machine Learning, pp. 8162\u20138171. PMLR, 2021.\nAaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche,\nEdward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast high-fidelity speech synthesis.\nIn International conference on machine learning, pp. 3918\u20133926. PMLR, 2018.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748,\n2022.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. arXiv preprint arXiv:2103.00020, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The\nJournal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning\nframework for solving forward and inverse problems involving nonlinear partial differential equations. Journal\nof Computational physics, 378:686\u2013707, 2019.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models, 2021.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In International Conference on Medical image computing and computer-assisted intervention,\npp. 234\u2013241. Springer, 2015.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image\ndiffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint\narXiv:2202.00512, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256\u20132265.\nPMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020a.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances\nin Neural Information Processing Systems, 32, 2019.\n14\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456,\n2020b.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.\nConsistency models.\narXiv preprint\narXiv:2303.01469, 2023.\nEndre S\u00fcli and David F Mayers. An introduction to numerical analysis. Cambridge university press, 2003.\nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In Neural\nInformation Processing Systems (NeurIPS), 2021.\nZijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.\nDiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896\n[cs], 2022. URL https://arxiv.org/abs/2210.14896.\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction\nof a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,\n2015.\nYizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind, and Navdeep Jaitly. Planner: Generating\ndiversified paragraph via latent language diffusion model. arXiv preprint arXiv:2306.02531, 2023.\nChunting Zhou, Graham Neubig, and Jiatao Gu. Understanding knowledge distillation in non-autoregressive\nmachine translation. arXiv preprint arXiv:1911.02727, 2019.\n15\nFigure 11: Curated samples of our distilled single-step model with prompts from diffusiondb.\n16\nFigure 12: Curated samples of our distilled single-step model with prompts from diffusiondb.\n17\nFigure 13: Curated samples of our distilled single-step model with prompts from diffusiondb.\n18\nContents\n1\nIntroduction\n1\n2\nPreliminaries\n3\n2.1\nDiffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nKnowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3\nMethod\n4\n3.1\nSignal-ODE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.2\nLearning with Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\nError Accumulation\nBoundary Condition\n3.3\nDistillation of Text-to-Image Models . . . . . . . . . . . . . . . . . . . . . . . . .\n7\nDistillation with Guidance\nPixel or Latent\n4\nExperiments\n7\n4.1\nExperimental Setups\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\nDiffusion Model Teachers\nImplementation Details\nEvaluation Metrics\n4.2\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\nQuantitative Results\nVisual Results\n4.3\nAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\nImportance of Boundary Condition\nProgressive v.s. Uniform Time Training\nControllable Generation\n5\nRelated Work\n11\nImproving Efficiency of Diffusion Models\nKnowledge Distillation for Generative Models\n6\nDiscussion and Conclusion\n11\nLimitations\nFuture Work\nConclusion\nAppendices\n19\nA Algorithm Details\n20\nA.1\nNotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nA.2\nDerivation of Signal-ODE\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nA.3\nBootstrapping Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nA.4 Training Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nB\nConnections to Existing Literature\n22\nB.1\nPhysics Informed Neural Networks (PINNs) . . . . . . . . . . . . . . . . . . . . .\n22\nB.2\nConsistency Models / TRACT\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nB.3\nSingle-step Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC Additional Experimental Settings\n23\nC.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.2\nText-to-Image Teachers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.3\nModel Architectures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.4\nTraining Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nD Additional Samples from BOOT\n24\n19\nAppendices\nA\nAlgorithm Details\nA.1\nNotations\nIn this paper, we use f\u03d5(x, t) to represent the diffusion model that denoises the noisy sample x into\nits clean version, and we derive the DDIM sampler (Eq. (2)) following the definition of Song et al.\n(2020a): we deterministically synthesize xs based on the following update rule:\nxs = ODE-Solver(f\u03d5, \u03f5, T \u2192 s)\n= \u03b1sf\u03d5(xt, t) + \u03c3s\n\u0012xt \u2212 \u03b1tf\u03d5(xt, t)\n\u03c3t\n\u0013\n= \u03c3s\n\u03c3t\nxt +\n\u0012\n\u03b1s \u2212 \u03b1t\n\u03c3s\n\u03c3t\n\u0013\nf\u03d5(xt, t)\n(11)\nwhere 0 \u2264 s < t \u2264 T. Here we use ODE-Solver to represent the DDIM sampling from a random\nnoise xT = \u03f5 \u223c N(0, I), and iteratively obtain the sample at step s. In practice, we can generalize\nto higher-order ODE-solvers for better efficiency.\nFor distillation, we define the student model with g\u03b8(\u03f5, t) which approximates xt along the diffusion\ntrajectory above. To avoid directly predicting the noisy samples xt with neural networks, we re-\nparameterize g\u03b8(\u03f5, t) = \u03b1ty\u03b8(\u03f5, t) + \u03c3t\u03f5 where the noise part is constant throughout t except the\nscale factor \u03c3t. In this way, the learning goal y\u03b8(\u03f5, t) is to predict a new variable yt: the \u201csignal\u201d part\nof the original variable yt = (xt \u2212 \u03c3t\u03f5)/\u03b1t.\nA.2\nDerivation of Signal-ODE\nBased on the definition of yt = (xt \u2212 \u03c3t\u03f5)/\u03b1t, we can derive the following equations from Eq. (11):\nxs = \u03c3s\n\u03c3t\nxt +\n\u0012\n\u03b1s \u2212 \u03b1t\n\u03c3s\n\u03c3t\n\u0013\nf\u03d5(xt, t)\n\u21d2 \u03b1sys + \u03c3s\u03f5 = \u03c3s\n\u03c3t\n(\u03b1tyt + \u03c3t\u03f5) +\n\u0012\n\u03b1s \u2212 \u03b1t\n\u03c3s\n\u03c3t\n\u0013\nf\u03d5(xt, t)\n\u21d2 \u03b1sys +\b\b\n\u03c3s\u03f5 = \u03b1t\n\u03c3s\n\u03c3t\nyt +\b\b\n\u03c3s\u03f5 +\n\u0012\n\u03b1s \u2212 \u03b1t\n\u03c3s\n\u03c3t\n\u0013\nf\u03d5(xt, t)\n\u21d2 ys = \u03b1t\u03c3s\n\u03c3t\u03b1s\nyt +\n\u0012\n1 \u2212 \u03b1t\u03c3s\n\u03c3t\u03b1s\n\u0013\nf\u03d5(xt, t)\n=\n\u00001 \u2212 e\u03bbs\u2212\u03bbt\u0001\nf\u03d5(xt, t) + e\u03bbs\u2212\u03bbtyt,\n(12)\nwhere we use the auxiliary variable \u03bbt = \u2212 log(\u03b1t/\u03c3t) for simplifying the equations. As mentioned\nin \u00a7 3.1, we can further obtain the continuous form of Eq. (12) by assigning t \u2212 s \u2192 0. That is,\nEq. (12) is equivalent to that shown in the following:\nys =\n\u00001 \u2212 e\u03bbs\u2212\u03bbt\u0001\nf\u03d5(xt, t) + e\u03bbs\u2212\u03bbtyt\n\u21d2 yt \u2212 ys = \u2212\n\u00001 \u2212 e\u03bbs\u2212\u03bbt\u0001\n(f\u03d5(xt, t) \u2212 yt)\n\u21d2 yt \u2212 ys\nt \u2212 s\n= \u2212e\u03bbt \u2212 e\u03bbs\nt \u2212 s\n\u00b7 e\u2212\u03bbt (f\u03d5(xt, t) \u2212 yt)\n\u21d2 dyt\ndt = \u2212\u001a\u001a\ne\u03bbt \u00b7 \u03bb\u2032\nt \u00b7\b\b\n\b\ne\u2212\u03bbt (f\u03d5(xt, t) \u2212 yt)\n(13)\nwhere \u03bb\u2032\nt = d\u03bbt/dt. Given a fixed noise input \u03f5, Eq. (13) defines an ODE over y\u03b8 w.r.t t, which we\ncall Signal-ODE, as both sides of the equation only operate in \u201clow-frequency\u201d signal space.\n20\nAlgorithm 1 Distillation using BOOT for Conditional Diffusion Models.\nRequire: pretrained diffusion model f\u03d5, initial student parameter from the teacher \u03b8 \u2190 \u03d5, step size\n\u03b4, learning rate \u03b7, CFG weight w, context dataset D, negative condition n = \u2205, tmin, tmax, \u03b2.\n1: while not converged do\n2:\nSample noise input \u03f5 \u223c N(0, I)\n3:\nSample context input c \u223c D\n4:\nSample t \u223c (tmin, tmax), s = min (t \u2212 \u03b4, tmin))\n5:\nCompute noise schedule \u03b1t, \u03c3t, \u03b1s, \u03c3s\n6:\nCompute \u03bb\u2032\nt \u2248 (1 \u2212 \u03b1t\u03c3s\n\u03c3t\u03b1s\n)/\u03b4\n7:\nGenerate the model predictions:\n8:\nyt = y\u03b8(\u03f5, t, c),\nys = y\u03b8(\u03f5, s, c),\nytmax = y\u03b8(\u03f5, tmax, c)\n9:\nGenerate the noisy sample \u02c6xt = \u03b1tyt + \u03c3t\u03f5\n10:\nCompute the denoised target:\n11:\n\u02dcft = f\u03d5(\u02c6xt, t, n) + w \u00b7 (f\u03d5(\u02c6xt, t, c) \u2212 f\u03d5(\u02c6xt, t, n))\n12:\n\u02dcftmax = f\u03d5(\u03f5, tmax, n) + w \u00b7 (f\u03d5(\u03f5, tmax, c) \u2212 f\u03d5(\u03f5, tmax, n))\n13:\nCompute the bootstrapping loss LBS\n\u03b8\n=\n1\n(\u03b4\u03bb\u2032\nt)2 \u2225ys \u2212 SG(yt + \u03b4\u03bb\u2032\nt( \u02dcft \u2212 yt))\u22252\n2\n14:\nCompute the boundary loss LBC\n\u03b8\n= \u2225ytmax \u2212 \u02dcftmax\u22252\n2\n15:\nUpdate model parameters \u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00b7 \u2207\u03b8\n\u0000LBS\n\u03b8 + \u03b2LBC\n\u03b8\n\u0001\n16: end while\n17: return Trained model parameters \u03b8\nA.3\nBootstrapping Objectives\nThe bootstrapping objectives in Eq. (8) can be easily derived by taking the finite difference of Eq. (4).\nHere we use y\u03b8(\u03f5, t) to estimate yt, and use \u02c6xt to represent the noisy image obtained from y\u03b8(\u03f5, t).\nL\u03b8 = E\u03f5,t\n\"\n\u02dc\u03c9t\n\f\f\f\f\n\f\f\f\f\ndy\u03b8(\u03f5, t)\ndt\n+ \u03bb\u2032\nt \u00b7 (f\u03d5(\u02c6xt, t) \u2212 y\u03b8(\u03f5, t))\n\f\f\f\f\n\f\f\f\f\n2\n2\n#\n\u2248 E\u03f5,t\n\u0014\n\u02dc\u03c9t\u2225y\u03b8(\u03f5, s) \u2212 y\u03b8(\u03f5, t)\n\u03b4\n\u2212 \u03bb\u2032\nt (f\u03d5(\u02c6xt, t) \u2212 y\u03b8(\u03f5, t)) \u22252\n2\n\u0015\n= E\u03f5,t\n\u0014 \u02dc\u03c9t\n\u03b42 \u2225y\u03b8(\u03f5, s) \u2212 [y\u03b8(\u03f5, t) + \u03b4\u03bb\u2032\nt (f\u03d5(\u02c6xt, t) \u2212 y\u03b8(\u03f5, t))] \u22252\n2\n\u0015\n= E\u03f5,t\n\u0014 \u02dc\u03c9t\n\u03b42 \u2225y\u03b8(\u03f5, s) \u2212 \u02c6y\u03b8(\u03f5, s)\u22252\n2\n\u0015\n,\n(14)\nwhere s = t \u2212 \u03b4, and \u02c6y\u03b8(\u03f5, s) is the approximated target. \u02dc\u03c9t is the additional weight, where by\ndefault \u02dc\u03c9t = 1. To stabilize training, a stop-gradient operation SG(.) is typically included:\nL\u03b8 = E\u03f5,t\n\u0014 \u02dc\u03c9t\n\u03b42 \u2225y\u03b8(\u03f5, s) \u2212 SG(\u02c6y\u03b8(\u03f5, s))\u22252\n2\n\u0015\n.\n(15)\nIn our experiments, we also find that it helps use \u02dc\u03c9t = 1/\u03bb\u20322\nt for text-to-image generation.\nWe can take advantage of higher-order solvers for a more accurate target that reduces the discretization\nerror. For example, one can use Heun\u2019s method (Ascher & Petzold, 1998) to first calculate the\nintermediate value \u02dcy\u03b8(\u03f5, s), and then the final approximation \u02c6y\u03b8(\u03f5, s):\n\u02dcy\u03b8(\u03f5, s) = y\u03b8(\u03f5, t) + \u03b4\u03bb\u2032\nt (f\u03d5(\u02c6xt, t) \u2212 y\u03b8(\u03f5, t)) ,\n\u02dcxs = \u03b1s \u02dcy\u03b8(\u03f5, s) + \u03c3s\u03f5\n\u02c6y\u03b8(\u03f5, s) = y\u03b8(\u03f5, t) + \u03b4\u03bb\u2032\nt\n2 [(f\u03d5(\u02c6xt, t) \u2212 y\u03b8(\u03f5, t)) + (f\u03d5(\u02dcxs, s) \u2212 \u02dcy\u03b8(\u03f5, s))] .\n(16)\nUsing Heun\u2019s method essentially doubles the evaluations of the teacher model during training, while\nthe add-on overheads are manageable as we stop the gradients to the teacher model.\n21\nA.4\nTraining Algorithm\nWe summarize the training algorithm of BOOT in Algorithm 1, where by default we assume condi-\ntional diffusion model with classifier-free guidance and DDIM solver. Here, for simplicity, we write\n\u03bb\u2032\nt \u2248 (1 \u2212 \u03b1t\u03c3s\n\u03c3t\u03b1s )/\u03b4. For unconditional models, we can simply remove the context sampling part.\nB\nConnections to Existing Literature\nB.1\nPhysics Informed Neural Networks (PINNs)\nPhysics-Informed Neural Networks (PINNs, Raissi et al., 2019) are powerful approaches that combine\nthe strengths of neural networks and physical laws to solve ODEs. Unlike traditional numerical meth-\nods, which rely on discretization and iterative solvers, PINNs employ machine learning techniques to\napproximate the solution of ODEs. The key idea behind PINNs is to incorporate physics-based con-\nstraints directly into the training process of neural networks. By embedding the governing equations\nand available boundary or initial conditions as loss terms, PINNs can effectively learn the underlying\nphysics while simultaneously discovering the solution. This ability makes PINNs highly versatile\nin solving a wide range of ODEs, including those arising in fluid dynamics, solid mechanics, and\nother scientific domains. Moreover, PINNs offer several advantages, such as automatic discovery of\nspatio-temporal patterns and the ability to handle noisy or incomplete data.\nAlthough motivated from different perspectives, BOOT shares similarities with PINNs at a high\nlevel, as both aim to learn ODE/PDE solvers directly through neural networks. In the domain of\nPINNs, solving ODEs can also be simplified into two objectives: the differential equation (DE) loss\n(Eq. (7)) and the boundary condition (BC) loss (Eq. (9)). The major difference lies in the focus of\nthe two approaches. PINNs primarily focus on learning complex ODEs/PDEs for single problems,\nwhere neural networks serve as universal approximators to address the discretization challenges\nfaced by traditional solvers. Moreover, the data space in PINNs is relatively low-dimensional.\nIn contrast, BOOT aims to learn single-step generative models capable of synthesizing data in\nhigh-dimensional spaces (e.g., millions of pixels) from random noise inputs and conditions (e.g.,\nlabels, prompts). To the best of our knowledge, no existing work has applied similar methods in\ngenerative modeling. Additionally, while standard PINNs typically compute derivatives (Eq. (7))\ndirectly using auto-differentiation, in this paper, we employ the finite difference method and propose\na bootstrapping-based algorithm.\nB.2\nConsistency Models / TRACT\nThe most related previous works to our research are Consistency Models (Song et al., 2023) and\nconcurrently TRACT (Berthelot et al., 2023), which propose bootstrapping-style algorithms for\ndistilling diffusion models. These approaches map an intermediate noisy training example at time\nstep t to the teacher\u2019s t-step denoising outputs using the DDIM inference procedure. The training\ntarget for the student is constructed by running the teacher model with one step, followed by the\nself-teacher with t\u22121 steps. As illustrated in Fig. 2, BOOT takes a different approach to bootstrapping.\nIt starts from the Gaussian noise prior and directly maps it to an intermediate step t in one shot. This\nchange has significant modeling implications, as it does not require any training data and can achieve\ndata-free distillation, a capability that none of the prior works possess.\nB.3\nSingle-step Generative Models\nBOOT is also related to other single-step generative models, including VAEs (Kingma & Welling,\n2013) and GANs (Goodfellow et al., 2014b), which aim to synthesize data in a single forward\npass. However, BOOT does not require an encoder network like VAEs. Thanks to the power of\nthe underlying diffusion model, BOOT can produce higher-contrast and more realistic samples. In\ncomparison to GANs, BOOT does not require a discriminator or critic network. Furthermore, the\ndistillation process of BOOT enables better-controlled exploration of the text-image joint space,\nwhich is explored by the pretrained diffusion models, resulting in more coherent and realistic samples\nin text-guided generation. Additionally, BOOT is more stable to learn compared to GANs, which are\nchallenging to train due to the adversarial nature of maintaining a balance between the generator and\ndiscriminator networks.\n22\nC\nAdditional Experimental Settings\nC.1\nDatasets\nWhile the proposed method is data-free, we list the additional dataset information that used to train\nour teacher diffusion models:\nFFHQ (https://github.com/NVlabs/ffhq-dataset) contains 70k images of real human faces\nin resolution of 1024 \u00d7 1024. In most of our experiments, we resize the images to a low resolution at\n64 \u00d7 64 for early-stage benchmarking.\nLSUN (https://www.yf.io/p/lsun) is a collection of large-scale image dataset containing 10\nscenes and 20 object categories. Following previous works (Song et al., 2023), we choose the\ncategory Bedroom (3M images), and train an unconditional diffusion teacher. All images are resized\nto 256 \u00d7 256 with center-crop. We use LSUN to validate the ability of learning in relative high-\nresolution scenarios.\nImageNet-1K (https://image-net.org/download.php) contains 1.28M images across 1000\nclasses. We directly merge all the training images with class labels and train a class-conditioned\ndiffusion teacher. All images are resized to 64 \u00d7 64 with center-crop. To support test-time classifier-\nfree guidance, the teacher model is trained with 0.2 unconditional probability.\nAs we do not need to train our own teacher models for text-to-image experiments, no additional\ntext-image pairs are required in this paper. However, our distillation still requires the text conditions\nfor querying the teacher diffusion. To better capture and generalize the real user preference of such\ndiffusion models, we choose to adopt the collected prompt datasets:\nDiffusionDB (https://poloclub.github.io/diffusiondb/) contains 14M images generated\nby Stable Diffusion using prompts and hyperparameters specified by users. For the purpose of\nour experiments, we only keep the text prompts and discard all model-generated images as well as\nmeta-data and hyperparameters so that they can be used for different teacher models. We use the\nsame prompts for both latent and pixel space models.\nC.2\nText-to-Image Teachers\nWe directly choose the recently open-sourced large-scale diffusion models as our teacher models.\nMore specifically, we looked into the following models:\nStableDiffusion (SD) (https://github.com/Stability-AI/stablediffusion) is an open-\nsource text-to-image latent diffusion model (Rombach et al., 2021) conditioned on the penultimate\ntext embeddings of a CLIP ViT-H/14 (Radford et al., 2021) text encoder. Different standard diffusion\nmodels, SD performs diffusion purely in the latent space. In this work, we use the checkpoint of\nSD v2.1-Base (https://huggingface.co/stabilityai/stable-diffusion-2-1-base) as\nour teacher which first generates in 64 \u00d7 64 latent space, and then directly upscaled to 512 \u00d7 512\nresolution with the pre-trained VAE decoder. The teacher model was trained on subsets of LAION-\n5B (Schuhmann et al., 2022) with noise prediction objective.\nDeepFloyd IF (IF) (https://github.com/deep-floyd/IF) is a recently open-source text-to-\nimage model with a high degree of photorealism and language understanding. IF is a modular com-\nposed of a frozen text encoder and three cascaded pixel diffusion modules, similar to Imagen (Saharia\net al., 2022): a base model that generates 64\u00d764 image based on text prompt and two super-resolution\nmodels (256 \u00d7 256, 1024 \u00d7 1024). All stages of the model utilize a frozen text encoder based on\nthe T5 (Raffel et al., 2020) to extract text embeddings, which are then fed into a UNet architecture\nenhanced with cross-attention and attention pooling. Models were trained on 1.2B text-image pairs\n(based on LAION (Schuhmann et al., 2022) and few additional internal datasets) with noise prediction\nobjective. In this paper, we conduct experiments on the first two resolutions (64 \u00d7 64, 256 \u00d7 256)\nwith the checkpoints of IF-I-L-v1.0 (https://huggingface.co/DeepFloyd/IF-I-L-v1.0) and\nIF-II-M-v1.0 (https://huggingface.co/DeepFloyd/IF-II-M-v1.0).\nC.3\nModel Architectures\nWe follow the standard U-Net architecture (Nichol & Dhariwal, 2021) for image generation bench-\nmarks and adopt the hyperparameters similar in f-DM (Gu et al., 2022). For text-to-image applications,\n23\nImage Generation\nText-to-Image\nHyperparameter\nFFHQ\nLSUN\nImageNet\nSD-Base\nIF-I-L\nIF-II-M\nArchitecture\nDenosing resolution\n64 \u00d7 64\n256 \u00d7 256\n64 \u00d7 64\n64 \u00d7 64\n64 \u00d7 64\n256 \u00d7 256\nBase channels\n128\n128\n192\nMultipliers\n1,2,3,4\n1,1,2,2,4,4\n1,2,3,4\n# of Resblocks\n1\n1\n2\nAttention resolutions\n8,16\n8,16\n8,16\n\u2013 Default \u2013\nNoise schedule\ncosine\ncosine\ncosine\nModel Prediction\nsignal\nsignal\nsignal\nText Encoder\n-\n-\n-\nCLIP\nT5\nT5\nTraining\nLoss weighting\nuniform\nuniform\nuniform\n\u03bb\u2032\u22122\nt\n\u03bb\u2032\u22122\nt\n\u03bb\u2032\u22122\nt\nBootstrapping step size\n0.04\n0.04\n0.04\n0.01\n0.04\n0.04\nCFG weight\n-\n-\n1 \u223c 5\n7.5\n7.0\n4.0\nLearning rate\n1e-4\n1e-4\n3e-4\n2e-5\n2e-5\n2e-5\nBatch size\n128\n128\n1024\n64\n64\n32\nEMA decay rate\n0.9999\n0.9999\n0.9999\n0.9999\n0.9999\n0.9999\nTraining iterations\n500k\n500k\n300k\n500k\n500k\n100k\nTable 2: Hyperparameters used for training BOOT. The CFG weights for text-to-image models are\ndetermined based on the default value of the open-source codebase.\nwe keep the default architecture setups from the teacher models unchanged. As mentioned in the main\npaper, we initialize the weights of the student models directly from the pretrained checkpoints and\nuse zero initialization for the newly added modules, such as target time and CFG weight embeddings.\nWe include additional architecture details in the Table 2.\nC.4\nTraining Details\nAll models for all the tasks are trained on the same resources of 8 NVIDIA A100 GPUs for 500K\nupdates. Training roughly takes 3 \u223c 7 days to converge depending on the model sizes. We train all\nour models with the AdamW (Loshchilov & Hutter, 2017) optimizer, with no learning rate decay\nor warm-up, and no weight decay. Standard EMA to the weights is also applied for student models.\nSince our methods are data-free, there is no additional overhead on data storage and loading except\nfor the text prompts, which are much smaller and can be efficiently loaded into memory.\nLearning the boundary loss requires additional NFEs during each training step. In practice, we apply\nthe boundary loss less frequently (e.g., computing the boundary condition every 4 iterations and\nsetting the loss to be 0 otherwise) to improve the overall training efficiency. Note that distilling from\nthe class-conditioned / text-to-image teachers requires multiple forward passes due to CFG, which\nrelatively slows down the training compared to unconditional models.\nDistilling from the DeepFloyd IF teacher requires learning from two stages. In this paper, we can\neasily achieve that by first distilling the first-stage model into single-step with BOOT, and then\ndistilling the upscaler model based on the output of the first-stage student. Following the original\npaper (Saharia et al., 2022), noise augmentation is also applied on the first-stage output where we set\nthe noise-level as 250 \u2021. For more training hyperparameters, please refer to Table 2.\nD\nAdditional Samples from BOOT\nFinally, we provide additional qualitative comparisons for the unconditional models of FFHQ 64 \u00d7 64\n(Fig. 14), LSUN 256 \u00d7 256 (Fig. 15), the class-conditional model of ImageNet 64 \u00d7 64 (Fig. 16),\nand comparisons for text-to-image generation based on DeepFloyd-IF (64 \u00d7 64 in Figs. 17 and 20,\n256 \u00d7 256 in Figs. 1 and 11 to 13) and StableDiffusion (512 \u00d7 512 in Figs. 19 and 21).\n\u2021https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/\ndeepfloyd_if/pipeline_if_superresolution.py#L715\n24\nFigure 14: Uncurated samples from FFHQ 64 \u00d7 64. All corresponding samples use the same initial\nnoise for the DDIM teacher and the single-step BOOT student.\n25\nFigure 15: Uncurated samples from LSUN Bedroom 256 \u00d7 256. All corresponding samples use the\nsame initial noise for the DDIM teacher and the single-step BOOT student.\n26\nFigure 16: Uncurated class-conditioned samples from ImageNet 64 \u00d7 64. All corresponding samples\nuse the same initial noise for the DDIM teacher and the single-step BOOT student. Classes from top\nto bottom: cowboy boot, volcano, golden retriever, teapot, daisy. The diffusion model uses CFG with\nw = 3, and our student model conditions on the same weight.\n27\nFigure 17: Uncurated text-conditioned image generation distilled from DeepFloyd IF (the first stage\nmodel, images are at 64 \u00d7 64). All corresponding samples use the same initial noise for the DDIM\nteacher and the single-step BOOT student. The specific prompts are shown above the images.\n28\nFigure 18: Given the 64 \u00d7 64 outputs from Fig. 17, we also show comparison for the second-stage\nmodels which upscale the images to 256 \u00d7 256. All corresponding samples use the same initial noise\nfor the DDIM teacher and the single-step BOOT student.\n29\nFigure 19: Uncurated text-conditioned image generation distilled from StableDiffusion (latent\ndiffusion in 64 \u00d7 64, images are upscaled to 512 \u00d7 512 with the pre-trained VAE decoder). All\ncorresponding samples use the same initial noise for the DDIM teacher and the single-step BOOT\nstudent. We use the same prompts as in Fig. 17 for better comparison.\n30\nFigure 20: Uncurated text-conditioned image generation distilled from DeepFloyd IF (the first stage\nmodel, images are at 64 \u00d7 64) given sampled text prompts from diffusiondb (Wang et al., 2022)\nrandomly. All corresponding samples use the same initial noise for the DDIM teacher and the\nsingle-step student. Besides, we also show curated examples from the two-stage distilled model at\n256 \u00d7 256 in Fig. 1.\n31\nFigure 21: Uncurated text-conditioned image generation distilled from StableDiffusion (latent\ndiffusion in 64 \u00d7 64, images are upscaled to 512 \u00d7 512 with the pre-trained VAE decoder) given\nsampled text prompts from diffusiondb (Wang et al., 2022) randomly. All corresponding samples use\nthe same initial noise for the DDIM teacher and the single-step student.\n32\n"
  },
  {
    "title": "Evaluating the Social Impact of Generative AI Systems in Systems and Society",
    "link": "https://arxiv.org/pdf/2306.05949.pdf",
    "upvote": "5",
    "text": "Evaluating the Social Impact of Generative AI Systems\nin Systems and Society\nIrene Solaiman\u2217\nHugging Face\nZeerak Talat\u2217\nIndependent Researcher\nWilliam Agnew\nUniversity of Washington\nLama Ahmad\nOpenAI\nDylan Baker\nDAIR\nSu Lin Blodgett\nMicrosoft Research\nHal Daum\u00e9 III\nUniversity of Maryland\nJesse Dodge\nAllen Institute for AI\nEllie Evans\nCohere\nSara Hooker\nCohere For AI\nYacine Jernite\nHugging Face\nAlexandra Sasha Luccioni\nHugging Face\nAlberto Lusoli\nSimon Fraser University\nMargaret Mitchell\nHugging Face\nJessica Newman\nUC Berkeley\nMarie-Therese Png\nOxford University\nAndrew Strait\nAda Lovelace Institute\nApostol Vassilev\nIndependent Researcher\nAbstract\nGenerative AI systems across modalities, ranging from text, image, audio, and\nvideo, have broad social impacts, but there exists no official standard for means of\nevaluating those impacts and which impacts should be evaluated. We move toward\na standard approach in evaluating a generative AI system for any modality, in two\noverarching categories: what is able to be evaluated in a base system that has no\npredetermined application and what is able to be evaluated in society. We describe\nspecific social impact categories and how to approach and conduct evaluations in\nthe base technical system, then in people and society. Our framework for a base\nsystem defines seven categories of social impact: bias, stereotypes, and representa-\ntional harms; cultural values and sensitive content; disparate performance; privacy\nand data protection; financial costs; environmental costs; and data and content\nmoderation labor costs. Suggested methods for evaluation apply to all modalities\nand analyses of the limitations of existing evaluations serve as a starting point for\nnecessary investment in future evaluations. We offer five overarching categories for\nwhat is able to be evaluated in society, each with their own subcategories: trustwor-\nthiness and autonomy; inequality, marginalization, and violence; concentration of\nauthority; labor and creativity; and ecosystem and environment. Each subcategory\nincludes recommendations for mitigating harm. We are concurrently crafting an\nevaluation repository for the AI research community to contribute existing evalua-\ntions along the given categories. This version will be updated following a CRAFT\nsession at ACM FAccT 2023.\n\u2217Both authors contributed equally. Following author order is alphabetical by last name.\nContact information: irene@huggingface.co and zeerak_talat@sfu.ca\nPreprint. Under review.\narXiv:2306.05949v2  [cs.CY]  12 Jun 2023\n1\nIntroduction\nUnderstanding an AI system from conception to training to deployment requires insight into aspects\nsuch as training data, the model itself, material infrastructure, and the context in which the system\nis deployed. It also requires understanding people, society, and how societal processes, institutions,\nand power are changed and shifted by the AI system. Generative AI systems are machine learning\nmodels trained to generate content, often across modalities. For generative AI systems, such as\nlanguage models, social impact evaluations are increasingly normalized but there is no standard set\nthat is applied across many models [238]. Generative AI has been widely adopted for different and\nvaried downstream tasks by adapting and fine-tuning pretrained models. In this report, we propose a\nframework for social impact evaluations of generative AI systems across modalities. We address this\nwork to three groups of readers: researchers and developers, third-party auditors and red-teamers,\nand policymakers who evaluate and address the social impact of systems through technical and\nregulatory means. Our goal is to lower the barrier to engage with these topics. We define social\nimpact as the effect of a system on people and communities along any timeline with a focus on\nmarginalization, and active, harm that can be evaluated. This paper is concerned with risks that have\nalready been documented or directly follow from current methods. Impacts on future generations,\nsuch as existential risk, are out of scope. Social impact evaluation covers many overlapping topics.\nWe propose a technical framework of the aspects of a system that can be evaluated along its lifecycle\nfrom training to deployment.\nWe focus on generative models across four modalities: text, image, video, and audio. We also\nconsider potential future modalities, and how to address these. The given categories and methods for\nevaluation are based on popularly deployed evaluations in use today and do not exhaustively cover all\nmethods. A forthcoming updated version of this paper will more rigorously analyze attention to and\ngaps in evaluations across modalities and categories.\nSocial impact evaluations offered in our categorical framework are key to but differ from harmful\nimpact mitigation and value alignment; evaluations aim to improve understanding of social impact,\nnot take action. Evaluations can be both quantitative and qualitative, and should seek to capture\nnuances in complex social topics [123]. While evaluations that quantify harm and risk of harm make\nregulation and mitigation more effective, they can miss nuances. Risk and potential for downstream\nharm is dependent on the context with which systems are deployed. Harmful impacts reflected in\ngenerative AI systems are rarely limited to the system itself. Long-term societal inequity, power\nimbalances, and systemic injustices [265] feed training data [242], influence system development and\ndeployment [237], and shape social impact [121]. While technical evaluations can probe and isolate\naspects of social impact in a specific system, more robust evaluation and mitigation encompasses\nhuman and infrastructural social harms.\nThe pace of capability development outstrips evaluation development; evaluation suites are quickly\nsaturated in one area or rendered obsolete. The level of attention and resourcing to capability\ndevelopment often outweighs that given to evaluations. Safety evaluations can also overfit to certain\nlenses and geographies, such as evaluating a multilingual system only in the English language. Often\ndevelopers and deployers will rely on evaluations built within the same company (e.g. OPT-175B[271]\nfrom Meta\u2019s safety evaluations). There is currently no consensus or governing body to determine\nwhat constitutes the social impact of any AI system. A broader evaluation suite, forthcoming in an\nupdated version of this paper, can both make these complex evaluations more accessible and unify\nmetrics across which to compare social aspects across generative AI systems.\n2\nBackground\nThe social impact aspects of an AI system are often largely dependent on context, from the sector in\nwhich they are developed to the use-cases and contexts in which they are deployed. Base generative\nAI systems have no specific predefined application, sector, or use case, making them notoriously\ndifficult to evaluate [166]. They include but are not limited to generative models such as text-based\nlanguage models (BLOOM [30], GPT-3 [40], OPT[271]), text-to-image models (ImaGen [205],\nDALL\u00b7E[170], Stable Diffusion [200]), and increasingly multimodal models [53] (GPT-4 [171]).\nGenerative AI systems can be referred to as General Purpose AI Systems (GPAIS): a system capable\nof a wide range of tasks that is applicable across sectors and use cases. These systems are popularly\nexamined for generalization properties and societal impact [35] but evaluations are generally not\n2\nstandardized and do not provide adequate coverage across risks or demographics [81]. Although\nthere are more common evaluations for performance and accuracy (e.g. GLUE [256]), many of\nthese evaluations are overrepresented and a select few cannot capture full general capabilities [191].\nSocial impact is a complex concept and is not fully measurable or quantifiable. Evaluations without\napplication or deployment context are complex and leave gaps [113], but are necessary.\nIn tandem with the rise of AI systems\u2019 integration with society, many legal jurisdictions have\nbegun to propose AI regulation, which include or mention assessing the impact of an AI system.\nRegulatory bodies that have announced plans and guidelines skew heavily toward Western and East\nAsian governmental bodies:the European Union [74], United States of America [250], Canada [148],\nUnited Kingdom [68], South Korea [196], Japan [240], and China [69]. While many of these proposed\nrequirements only apply to systems that fall into \u201chigh risk\u201d categories as defined by the proposed\nregulation, generative AI systems are largely being scoped.\n2.1\nMethodology\nWe convened thirty experts across industry, academia, civil society, and government to contribute to\na two-part workshop series. The first workshop created a framework for defining and categorizing\nsocial impacts that can be evaluated. The second workshop examined categories\u2019 ability to be\nevaluated, including past approaches to evaluations and metrics, limitations, and future directions\nfor improvements. For the first workshop, we asked experts to discuss possible impacts of systems\nfor each of the five modalities of generative systems. For the second workshop, we created meta\ncategories of impacts and collected existing methods for evaluation within these categories. The\nfindings from the discussions inform our framework and evaluation method sections. Both workshops\nwere conducted under modified Chatham House Rules, where contributors could opt in to authorship.\nAnother workshop in the form of a CRAFT session at ACM FAccT 2023 will inform an updated\nversion of this paper.\n3\nRelated Work\nToolkits and repositories for evaluating qualitative aspects of AI systems are broad and constantly\nevolving. Many are aimed at public agency procurement and deployment. In 2018, AI Now released\ntheir framework for algorithmic impact assessments aimed at public agencies [63]. Many public\ninterest organizations and government initiatives have since published frameworks and assessment\ntools, such as the OECD\u2019s Classification Framework for AI risks [168] and Canada\u2019s Algorithmic\nImpact Assessment Tool [247]. The U.S. National Institute of Standards and Technology (NIST)\nArtificial Intelligence Risk Management Framework (AI RMF) [159] is also intended to be applicable\nto all AI systems, although specific applications to generative AI systems are in progress.\nEvaluation suites across system characteristics for specific generative system modalities, such as\nlanguage, include Holistic Evaluation of Language Models (HELM) [139], BigBench [232], Language\nModel Evaluation Harness [85]. These evaluation suites incorporate capabilities evaluations as well\nas evaluations across the categories in this paper, and are similarly living resources. We are not aware\nof research on evaluation or an evaluation suite dedicated to social impacts or across modalities.\nTechnical evaluation suites are often specific to a type of system and harm; for example, biases\nin natural language processing systems [33]. Partnership on AI\u2019s ABOUT ML (Annotation and\nBenchmarking on Understanding and Transparency of Machine learning Lifecycles) project crafted\na resource library for developers, deployers, and procurers to better document the system life-\ncycle [176]. Auditing frameworks (e.g., [190]) are powerful tools that necessarily depend on the sector\nof deployment. Increasing literature taxonomizes dangers [26], social impacts [110], sociotechnical\nharms [219], and social risks of all [80] or certain generative AI systems like language models [258],\nbut evaluating these risks and impacts is a complementary yet distinct ongoing research area.\n3\n4\nCategories of Social Impact\nWe divide impacts into two categories for evaluation: what can be evaluated in a technical system and\nits components, and what can be evaluated among people and society. The former section includes\nevaluations for base systems and evaluations popularly run or published in top AI conferences.\nBase systems refer to AI systems, including models and components, that have no predetermined\napplication. The latter section examines systems in context and includes recommendations for\ninfrastructurally mitigating harmful impacts. Both sections can have overlap as the same category\ncan be evaluated differently in a system (see 4.1.4 Privacy and Data Protection) and impact on people\nand society (see 4.2.1.3 Personal Privacy and Sense of Self).\n4.1\nImpacts: The Technical Base System\nBelow we list the aspects relatively able to be evaluated in a generative system from training to\ndeployment testing. These categories, and the suggested evaluations afford application and use-case\nindependent tests of the base model. Evaluation of base systems can be qualitative or quantitative,\nbut only provide a narrow insight into the described aspect of the type of generative AI system. The\ndepth of literature and research on evaluations differ by modality, but the themes for evaluations can\nbe applied to most systems.\nThe following categories are high-level, non-exhaustive, and present a synthesis of the findings across\ndifferent modalities. They refer solely to what can be evaluated in a base technical system:\n\u2022 Bias, Stereotypes, and Representational Harms\n\u2022 Cultural Values and Sensitive Content\n\u2022 Disparate Performance\n\u2022 Privacy and Data Protection\n\u2022 Financial Costs\n\u2022 Environmental Costs\n\u2022 Data and Content Moderation Labor\n4.1.1\nBias, Stereotypes, and Representational Harms\nGenerative AI systems can embed and amplify harmful biases that are most detrimental to marginal-\nized peoples. Categories of bias, from system to human to statistical, interact with each other and\nare intertwined [211]. For bias evaluations that do not narrowly capture biases as they occur in\nGenerative AI systems, it is necessary to consider work outside of the field of question. For instance,\nfor natural language processing, bias evaluations must seriously engage with the relationship between\nthe modality (i.e. language) and social hierarchies [33]. When thinking about representational\nharms [125], it is also important to consider the extent to which any representation could confer harm\n(see 4.2.2.2 Long-term Amplifying Marginalization by Exclusion (and Inclusion)).\nAlthough bias evaluations in data have been subject to a large body of research, bias is not only\na \u201cdata problem.\u201d Biases are not only introduced in the data pipeline but throughout the entire\nmachine learning pipeline [237]. The overall level of harm is also impacted by modeling choice\n[108]. These can include choices about many stages of the optimization process [237, 129]; privacy\nconstraints [24], widely used compression techniques [109, 15, 169] and the choice hardware [273]\nhave all been found to amplify harm on underrepresented protected attributes [28]. The geographic\nlocation, demographic makeup, and team structures of researcher and developer organizations can\nalso introduce biases.\nWhat to Evaluate\nWhile the degree of harm depends on many factors from type of output to the\ncultural context of training and deployment, focus on bias evaluations has centered on protected\nclasses as defined by United States [77] and United Nations [249] guidelines. These guidelines\nare non-exhaustive and harms exist outside of their proposed categories but can be evaluated by\nadding categories. For instance, for generative AI systems developed on data from the South Asian\nsubcontinent, it may also be useful to include considerations of caste bias [217]. Additional harmful\nbiases include misrepresentations of humans generally, such as associated humans or a group of\nhumans with other animals [223].\n4\nPopular evaluations for biases use association tests [46] or examine stereotypes [157, 156, 138],\ncorrelations and co-occurrences [272], and sentiment analysis [66]. In language, these evaluations\ncan occur at the word or sentence level. For images, additional tools such as captioning systems can\nbe used. For certain modalities, such as language, biases can be represented differently [142]. Across\nmodalities, biases can be evaluated using intrinsic and extrinsic methods [91], where the former seeks\nto evaluate biases within model weights and the latter evaluates the expression of biases in the outputs\nfor downstream tasks (e.g. captioning). Evaluations can also be specific to a certain function of a\nmodality, such as question-answering in language [175].\nLimitations\nThere are often legal obstacles around collecting certain protected attributes, which\nleads to selection bias in the availability of protected features annotations. Moverover, as geographic\nand cultural contexts shift, so do the meaning of different categories. Annotators often have different\nperceptions of concepts like race or are influenced by their own lived experience when categorizing\nprotected categories.\nDue to its contextual and evolving nature [83], bias evaluation cannot be fully standardized and\nstatic [117]. Protected class categorization itself cannot be exhaustive and can be inherently harmful.\nBy framing work within such considerations, it is possible to delineate which qualities that are\nevaluated for. Precisely identifying which framing is used for bias evaluation and mitigation can help\ndelineate the particular areas where robust evaluation has been done, where developers expect biases\nto arise, and which groups for whom they believe biases are unlikely to arise, or bias evaluations have\nnot been as rigorous, e.g., due to a lack of bias evaluation resources. Certain protected classes, such\nas race and gender, are often more represented in publications and publication venues around biases\nof (generative) systems. Many evaluations focus on distinct or binary groups, due to the complexity\nof operationalising intersectionality [257, 133]; in many cases, assumptions used to simplify for the\nsake of mathematical notation and interpretation result in obscuring the very phenomena they seek to\ndescribe [64].\nObtaining data for bias evaluations is not straightforward, as there are often legal obstacles around\ncollecting data about protected attributes, which leads to selection bias in the availability of protected\nfeatures annotations [21, 252]. Moverover, as geographic and cultural contexts shift, so do the\nmeaning of different categories [206, 112] and must be interpreted according to their local meaning.\nAnnotators often have different perceptions of concepts like race or are influenced by their own lived\nexperience [234] when categorizing protected categories [187].\nWhen conducting association tests, although based in human associations, one should remain aware\nthat general societal attitudes do not always represent subgroups of people and cultures. Evaluations\nfor stereotype detection can raise false positives and can flag relatively neutral associations based in\nfact (e.g. population x has a high proportion of lactose intolerant people) [238]. Whenever additional\ntooling is used to aid in identifying biases, e.g., the use of an image captioning system in addition to\nthe base system, tool added introduces its own biases, similarly introduced in each step of developing\nthe tool, which are embedded into the ecosystem of the biases of the system under study.\n4.1.2\nCultural Values and Sensitive Content\nCultural values are specific to groups and sensitive content is normative. Sensitive topics also vary by\nculture and can include hate speech, which itself is contingent on cultural norms of acceptability [242].\nAbusive and offensive language are a large umbrella for unsafe content, which can also include\nabuse and hate speech[151, 236]. What is considered a sensitive topic, such as egregious violence or\nadult sexual content, can vary widely by viewpoint. Due to norms differing by culture, region, and\nlanguage, there is no standard for what constitutes sensitive content.\nIncreasing politicization of model training and outputs, as seen in projects such as with projects like\nRightWingGPT [202], raises urgency in evaluating the complexity of political values. Distinct cultural\nvalues present a challenge for deploying models into a global sphere, as what may be appropriate in\none culture may be unsafe in others [238]. Generative AI systems cannot be neutral or objective, nor\ncan they encompass truly universal values. There is no \u201cview from nowhere\u201d; in evaluating anything,\na particular frame of reference [207] is imposed [237].\n4.1.2.1\nHate, Toxicity, and Targeted Violence\nBeyond hate speech and toxic language, genera-\ntions may also produce harmful biases [87], stereotypes [165] (overlapping with 4.1.1Bias, Stereo-\n5\ntypes, and Representational Harms), violent or non-consensual imagery or audio, and physically\nthreatening language, i.e., threats to the lives and safety of individuals or groups of people. Although\nbase systems cannot act on the content that is generated by them, they can still inflict harms upon\nviewers who are targeted, help normalize harmful content, and aid in the production of harmful\ncontent for distribution (e.g., misinformation and non-consensual imagery).\nIn an early example, Microsoft\u2019s Tay bot showed these exact vulnerabilities and generated violent\nlanguage such as Holocaust denial and threats to women and people of color within 24 hours of its\nrelease [255]. Recent harms have proved fatal [268]. For these reasons, it is of the utmost importance\nthat generative AI systems are evaluated for their potential to generate harmful content and how such\ncontent may be propagated without appropriate measures for identifying and addressing them.\nWhat to Evaluate\nCultural values can highlight specific prominent topics according to a given\napplication and modality. For example, An image generative model prompted on politics can\nsegment generations with disparate geographic and political party, building, infrastructural, and figure\nrepresentation, alongside ideological cues. Cultural sensitive topics can range from physical aspects of\nhuman appearance and health to less visible or descriptive aspects of human behavior and emotional\nexpression. A non-exhaustive categorical framework and human reviewed evaluations [228] can\ncapture some aspects of culture.\nHate, Toxicity, and Targeted Violence and safe to hurtful outputs can be evaluated in context of safe\ndiscussions, toxicity metrics [87, 182], hurtfulness [165], and level of offense [71] for language.\nNonconsensual generations of existing people should be evaluated with the person themselves.\nResearch toward approaches to characterizing harmful content is ongoing by modality [193].\nTraining data, including fine-tuning and other data can be examined to explain many of the behaviors\nof large data-driven generative systems, and particularly their potentially harmful behaviors; what\nassociations in the training corpus led to toxic behaviors, whether generated information corresponds\nto trustworthy training sources, examining whether the data collection abides by ethical frameworks\nfor the rights of data subjects, etc. Different levels of access and description of the training data can\nhelp answer these questions with due consideration for privacy needs [183].\nLimitations\nEvaluating cultural values requires examining an infinite list of topics that contribute\nto a cultural viewpoint. Human-led evaluations [173] for hateful and sensitive content can have a\nhigh psychological cost, as seen in content moderation labor (see 4.1.7 Data and Content Moderation\nLabor). The types and intensity of sensitive content that may be produced across modalities may vary.\nFor example, the creation of hate speech and hateful imagery may overlap in their target, yet provide\ndifferent levels of psychological distress in generated content. For evaluations which rely on a third\nparty API, such as the many benchmarks which leverage Google Perspective API [182] for toxicity\ndetection, it is important to make sure comparisons between models are standardized using the same\nversion of the API to avoid reproducibility issues [185].\n4.1.3\nDisparate Performance\nIn the context of evaluating the impact of generative AI systems, disparate performance refers to AI\nsystems that perform differently for different subpopulations, leading to unequal outcomes for those\ngroups. A model that is trained on a dataset that is disproportionately skewed towards one particular\ndemographic group may perform poorly for other demographic groups [43].\nData availability differs due to geographic biases in data collection [216], disparate digitization of\ncontent globally due to varying levels of internet access for digitizing content, and infrastructure\ncreated to support some languages or accents over others, among other reasons. Much of the training\ndata for state of art generative models comes from the internet. However, the composition of this\ndata reflects historical usage patterns; 5% of the world speaks English at home, yet 63.7% of internet\ncommunication is in English [197]. This has implications for downstream model performance where\nmodels underperform on parts of the distribution underrepresented in the training set. For example,\nautomatic speech recognition models (ASR), which convert spoken language (audio) to text have been\nshown to exhibit racial disparities [130], forcing people to adapt to engage with such systems [100]\nand has implications (see 4.2.3.2 Imposing Norms and Values) for popular audio generation accent\nrepresentation.\n6\nInterventions to mitigate harms caused by generative AI systems may also introduce and exhibit\ndisparate performance issues [238]. For instance, automated hate speech detection driven by annotated\ndata with an insensitivity to dialect differences can amplify harm to minority or marginalized groups\nby silencing their voices (see 4.2.2.1 Community Erasure) or incorrectly labeling their speech as\noffensive [67]. This therefore requires that the interventions used are documented for which particular\npopulations and norms that they seek to cover, and which they do not.\nWhat to Evaluate\nDataset composition and decisions can give insight to subsequent performance.\nThe language, speech, and imagery included in datasets as well as decisions made about that data,\nincluding filtering and reward modeling, will impact how the model performs for different groups or\ncategories of concepts associated with groups. Generative image models for example, may output\nvarying quality generations when producing different concepts, with quality referring to photorealism,\naesthetic quality, and conceptual richness [170].\nEvaluating model generations across subpopulation languages, accents, and similar topics using the\nsame evaluation criteria as the highest performing language or accent can illustrate areas where there\nis disparate performance and can help document areas for further model development and mitigation\nwork.\nLimitations\nSimilar limitations that lead to disparate system performance contribute to disparate\nattention to evaluations for different groups. Performance evaluations for similar tasks in non-English\nlanguages will vary by the amount of resourcing for a given language. More spoken and digitized\nlanguages may have more evaluations than lower-resource languages.\n4.1.4\nPrivacy and Data Protection\nExamining the ways in which generative AI systems providers leverage user data is critical to\nevaluating its impact. Protecting personal information and personal and group privacy depends\nlargely on training data, training methods, and security measures. The data on which the system\nwas trained or adapted should be consensually and lawfully collected and secured and secured\nunder the rules of the jurisdictions in which the data subjects and the entity collecting the data are\nbased. Moreover, there are strong intellectual property and privacy concerns, with generative models\ngenerating copyrighted content [254] and highly sensitive documents [49] or personally identifiable\ninformation (PII), such as phone numbers, addresses and private medical records.\nProviders should respect the consent and choices of individuals for collecting, processing, and sharing\ndata with external parties, as sensitive data could be inevitably leveraged for downstream harm such\nas security breaches, privacy violations, and other adversarial attacks. Oftentimes, this might require\nretroactively retraining a generative AI system, in accordance with policy such as the California\nConsumer Privacy Act (CCPA) [4].\nWhat to Evaluate\nAlthough some evaluations operate as a proxy for a system\u2019s ability to generate\ncopyrighted or licensed content found within pretraining data [139], there is great potential for more\ncomprehensive evaluations.\nMemorization of training examples remains a critical security and privacy concern [49, 50]. Address-\ning this issue may yield improvements in performance for various downstream applications [172].\nAdditionally, generative AI systems providers may maintain the right to authorize access of user\ndata to external third-parties, such as human annotation vendors. For sharing data to third-parties,\ndata providers should ensure that only lawful data is shared, consent for sharing is obtained from\ndata subjects, and that shared data does not contain any private, personally identifiable, or otherwise\nsensitive data.\nLimitations\nGenerative AI systems are harder to evaluate without clear documentation, systems\nfor obtaining consent (e.g., opt-out mechanisms), and appropriate technical and process controls to\nsecure user data that can threaten the privacy and security of individuals. Thus, robustly evaluating\nprivacy risks will often require full process and governance audits that go beyond evaluating artifacts\nin isolation. Rules for leveraging end-user data for training purposes are unclear, where user prompts,\ngeolocation data, and similar data can be used to improve a system. The immense size of training\ndatasets [118] makes scrutiny increasingly difficult.\n7\n4.1.5\nFinancial Costs\nThe estimated financial costs of training, testing, and deploying generative AI systems can restrict the\ngroups of people able to afford developing and interacting with these systems. Concretely: sourcing\ntraining data, computing infrastructure for training and testing, and labor hours contribute to the\noverall financial costs. These metrics are not standard to release for any system, but can be estimated\nfor a specific category, such as the cost to train and host a model.\nWhat to Evaluate\nResearchers and developers can estimate infrastructure, hardware costs, and\nhours of labor from researchers, developers, and crowdworkers. Popular existing estimates focus on\ncompute using low-cost or standard pricing per instance-hour [137]. Research lowering training costs\nalso show tracking compute cost by day as the model trains and scales [253]. Frameworks break\ndown cost per system component: data cost, compute cost, and technical architecture of the system\nitself [163]. Other variables used to calculate cost include size of dataset, model size, and training\nvolume [218].\nLimitations\nOnly accounting for compute cost overlooks the many variables that contribute to a\nsystem\u2019s training. Costs in pre- and post-deployment, depending on how a system is released [227]\nare also difficult to track as cost variables may not be directly tied to a system alone. Human labor and\nhidden costs similarly may be indirect. Costs also change over time and with a changing economy\nfor all components. Finally, it is necessary to keep track of the changes of costs and economy of\ncomponents over time.\n4.1.6\nEnvironmental Costs and Carbon Emissions\nThe computing power used in training, testing, and deploying generative AI systems, especially\nlarge scale systems, uses substantial energy resources and thereby contributes to the global climate\ncrisis by emitting greenhouse gasses [233]. While the environmental costs of compute has become\nan area of active research, with workshops dedicated to the question, the environmental costs of\nmanufacturing hardware remains under-explored. One potential reason for this discrepancy may be\nthat estimating compute and energy costs, while complex, is a comparably transparent task compared\nto tracing the emissions of the of emissions throughout the manufacturing process. However, recent\nestimates suggest that the manufacturing process have substantial environmental costs [96]. Overall,\ninformation about emissions is scarce and there is no consensus for what constitutes the total carbon\nfootprint of AI systems.\nWhat to Evaluate\nThe existing efforts in evaluating the energy consumed and carbon emitted\nby AI systems have pursued two main directions: the creation of tools to evaluate these impacts\nand empirical studies of one or several models. For instance, [132] proposes both a web-based and\nprogrammatic approach for quantifying the carbon emissions of models, meanwhile [104] proposes\nan experiment-impact-tracker, for energy and carbon usage reporting research. Other popular work\nincludes conversion based on power consumed in the U.S. [233] and examining environmental impact\nacross compute-related impacts, immediate impacts of applying AI, and system-level impacts [120].\nExisting metrics for reporting range from energy, compute, and runtime, to carbon emissions. CPU,\nGPU, and TPU related information such as hardware information, package power draw, GPU\nperformance state, and CPU frequency, as well as memory usage are additional metrics. In addition\nto metrics, consideration of the region/location of the energy grid where the experiment is being run\non is important given significant differences in carbon emissions between energy grids, and informs\nthe move to run experiments in \u201cclean regions\u201d. Tools such as CodeCarbon can be used to estimate\npower consumption [61].\nLimitations\nThere is still a lot of uncertainty around certain variables, such as the relative contribu-\ntion of added parameters to their energy consumption and carbon footprint, as well as the proportion\nof energy used for pre-training versus fine-tuning models for different tasks and architectures [267].\nConducting further research on these variables can benefit the field both from the perspective of\nsustainability and overall efficiency.\n8\n4.1.7\nData and Content Moderation Labor\nHuman labor is a substantial component of machine learning model development, including generative\nAI systems. This labor is typically completed via a process called crowd computation, where\ndistributed data laborers, also called crowdworkers, complete large volumes of individual tasks that\ncontribute to model development. This can occur in all stages of model development: before a model\nis trained, crowdworkers can be employed to gather training data, curate and clean this data, or provide\ndata labels. While a model is being developed, crowdworkers evaluate and provide feedback to model\ngenerations before the final deployed model is released, and after model deployment, crowdworkers\nare often employed in evaluating, moderating, or correcting a model\u2019s output. Crowdwork is often\ncontracted out by model developers to third-party companies.\nTwo key ethical concerns in the use of crowdwork for generative AI systems are: crowdworkers\nare frequently subject to working conditions that are taxing and debilitative to both physical and\nmental health, and there is a widespread deficit in documenting the role crowdworkers play in\nAI development. This contributes to a lack of transparency and explainability in resulting model\noutputs. Manual review is necessary to limit the harmful outputs of AI systems, including generative\nAI systems. A common harmful practice is to intentionally employ crowdworkers with few labor\nprotections, often taking advantage of highly vulnerable workers, such as refugees [119, p. 18],\nincarcerated people [54], or individuals experiencing immense economic hardship [98, 181]. This\nprecarity allows a myriad of harmful practices, such as companies underpaying or even refusing to\npay workers for completed work (see Gray and Suri [93, p. 90] and Berg et al. [29, p. 74]), with no\navenues for worker recourse. Finally, critical aspects of crowdwork are often left poorly documented,\nor entirely undocumented [88].\nWhat to Evaluate\nResearchers and developers close to the system development should check\nthat crowdworking is conducted under basic ethical standards, such as the 18 Criteria for Fairer\nMicrowork proposed by Berg et al. [29, p. 105] in Digital Labour Platforms and the Future of Work\nor the Oxford Internet Institute\u2019s Fairwork Principles [75]. Concurrently, researchers and developers\nshould document the role of crowdwork in all dataset development undertaken during generative AI\nsystems development, e.g. using frameworks like CrowdWorkSheets [70] and sections 3.3 and 3.4 in\nDatasheets for Datasets [86]. Basic details such as crowdworkers\u2019 demographics, the instructions\ngiven to them, or how they were assessed and compensated, are foundational for interpreting the\noutput of AI systems shaped by this labor [147]. All aspects of data labor should be transparently\nreported (as done by Glaese et al. [89], for example), both as a tool for understanding model output\nand as a means to audit unethical labor practices.\nExternal evaluators can use evaluation metrics designed specifically around crowdwork, such as\nthose proposed by Fair Work [75], to evaluate quality of working conditions. Relevant labor law\ninterventions by jurisdiction may also apply. Since many critical crowdworking jobs and evaluation\nof this work involves long-term exposure to traumatic content [199], such as child sexual abuse\nmaterial or graphic depictions of violence [181], it may also be necessary to consider professional\nsupport for mental health and practices to limit the degree of exposure in any one work day.\nLimitations\nThe lack of regulation and rules around crowdworker protection for AI contributes to\nminimal to no documentation or transparency. The lack of information makes crowdwork difficult to\nevaluate. Incentives to conduct crowdwork at a low cost with little transparency contribute to less\nliterature on evaluating crowdwork. Outsourcing labor also creates barriers to evaluation by further\ncomplicating reporting structures, communication, and working conditions.\n9\n4.2\nImpacts: People and Society\nEvaluating the effect AI has on people and societies, and evaluating people and groups themselves\nencounters similar challenges as those arising in sampling [20], surveying [126], determining prefer-\nences [270], and working with human subjects [131, 12], in addition to challenges that stem from the\nplanetary scale at which AI development seeks to be applied for, and therefore comes to engage with\nnational and global social systems, e.g., economies and cultures. Taxonomies of risks and harms of\ngenerative AI systems [80], including their impacts on human rights [111, 186], strongly overlap with\nwhat should be evaluated. However, most societal impact taxonomies lack evaluations or examples of\nevaluating society. We must understand the reason for our evaluation; often we are seeking proof, in\nthe form of evaluations, that is necessary for further action against harmful impacts.\nConcretely when evaluating impact, timing will change how we view a system. What is being trained\non and generated may not reflect the current world in which it is deployed [235]. Further, when\nwe seek to evaluate society, we cannot escape the ways in which our perception of society, and\nsociety itself, has already been influenced by existing AI and social media tools. In crafting and\nconducting evaluations, we can often encroach on others\u2019 privacy and autonomy due to the need for\nhighly personal information to evaluate how harms are enacted and distributed across populations.\nFor this reason, it is necessary that any engagements with impact assessments also critically examine\nhow consent is obtained, and what the limits of consent are, when it comes to being subject to bias\nevaluation and assessment. Similarly, impact assessments must also take into consideration the\nexisting and possible future impacts of being included as a data subject. Participatory justice-led\ninitiatives provide particularly promising avenues for such considerations and engagements. Long-\nterm effects of systems embedded in society, such as economic or labor impact, largely require\nideation of generative AI systems\u2019 possible use cases and have fewer available general evaluations.\nThe following categories are high-level, non-exhaustive, and present a synthesis of the findings across\ndifferent modalities. They refer solely to what can be evaluated in people and society:\n\u2022 Trustworthiness and Autonomy\n\u2013 Trust in Media and Information\n\u2013 Overreliance on Outputs\n\u2013 Personal Privacy and Sense of Self\n\u2022 Inequality, Marginalization, and Violence\n\u2013 Community Erasure\n\u2013 Long-term Amplifying Marginalization by Exclusion (and Inclusion)\n\u2013 Abusive or Violent Content\n\u2022 Concentration of Authority\n\u2013 Militarization, Surveillance, and Weaponization\n\u2013 Imposing Norms and Values\n\u2022 Labor and Creativity\n\u2013 Intellectual Property and Ownership\n\u2013 Economy and Labor Market\n\u2022 Ecosystem and Environment\n\u2013 Widening Resource Gaps\n\u2013 Environmental Impacts\nThese context-specific categories heavily depend on how generative AI systems are deployed, in-\ncluding sector and application. In the broader ecosystem, methods of deployment [229] affect social\nimpact.\n10\n4.2.1\nTrustworthiness and Autonomy\nHuman trust in systems, institutions, and people represented by system outputs evolves as generative\nAI systems are increasingly embedded in daily life. WIth the increased ease of access to creating\nmachine generated content, which produce misinformation [260] as a product, distinguishing between\nhuman and machine generated content, verified and misinformation, will become increasingly difficult\nand poses a series of threats to trust in media and what we can experience with our own hearing and\nvision.\n4.2.1.1\nTrust in Media and Information\nHigh capability generative AI systems create believable outputs across modalities and level of risk\ndepends on use case. From impersonation spurring spamming to disinformation campaigns, the\nspread of misinformation online can be perpetuated by reinforcement and volume; people are more\nlikely to believe false information when they see it more than once, for example if it has been shared\nby multiple people in their network [179]. This can have devastating real world impacts, from\nattempting dangerous COVID-19 treatments [160], to inciting violence [146], and the loss of trust in\nmainstream news [95]. The increasing sophistication of generative AI in recent years has expanded\nthe possibilities of misinformation and disinformation campaigns, and made it harder for people to\nknow when they should trust what they see or hear [41].\nWhat to Evaluate\nSurveying trust can apply to trust in AI systems [184, 107] to output factual\ninformation, trust in researchers, developers, and organizations developing and deploying AI [143],\nmitigation and detection measures [222], and trust in overall media and how it is distributed [251].\nTrust can be evaluated in the category of information, such as information about democratic and\npolicy institutions [177]. Evaluations and countermeasures of false and misleading information\nremain challenging. There is no universal agreement about what constitutes misinformation and much\nof the research on intervention remains siloed [94]. Furthermore, current research efforts towards\nwatermarking text remain brittle and the area of developing watermarks for machine generated outputs\nis an active research area [128].\nMitigation and Interventions\nInterventions on technical systems include encouraging people\nto shift their attention to the accuracy of posts they might share [180], using crowd-sourced fact\nchecking [90], and using digital forensics to detect AI-generated content [76]. However, technical\ntools such as detection are less accurate as AI systems become more powerful [204].\nEmerging legal and regulatory approaches around the world include the EU AI Act, which requires\nlabeling AI-generated content, and certain U.S. state laws that criminalize non-consensual deepfake\npornography and deepfake content that interferes with elections [38], where lessons can be extrap-\nolated to generated AI outputs. Policymakers and developers can also ban use cases where false\noutputs have highest risks.\n4.2.1.2\nOverreliance on Outputs\nOverreliance on automation in general is a long-studied problem [174], and carries over in novel and\nimportant ways to AI-generated content [178]. People are prone to overestimate and put a higher\ndegree of trust in AI generated content, especially when outputs appear authoritative or when people\nare in time-sensitive situations [45].\nThis can be dangerous because many organizations are pursuing the use of large language models\nto help analyze information despite persistent flaws and limitations, which can lead to the spread of\nbiased and inaccurate information [103]. The study of human-generative AI relationships is nascent,\nbut growing, and highlights that the anthropomorphism [13] of these technologies may contribute to\nunfounded trust and reliance [192, 225]. Improving the trustworthiness of AI systems is an important\nongoing effort across sectors [159, 161].\nPersistent security vulnerabilities in large language models and other generative AI systems are\nanother reason why overreliance can be dangerous. For example, data poisoning, backdoor attacks,\nand prompt injection attacks can all trick large language models into providing inaccurate information\nin specific instances [220].\n11\nWhat to Evaluate\nFor language, in the case of AI chatbots specifically, the conversational interface\ncan additionally elicit trust and other strong emotions from people, even when they understand the\nlimitations of the technology [201]. Overreliance on such tools can not only make people prone\nto believe inaccurate information, but can also be abused to subtly change or manipulate people\u2019s\nbehaviors, for example to make them more likely to purchase particular products or even encourage\nself-harm [99].\nFor language models trained on code and code generative systems, inaccurate outputs [60] can\nnullify potential benefits. Code generative systems can be evaluated for their limitations [56] and\nhazards [127], from alignment questions like producing bugs and harmful biases, to economic and\nenvironmental impacts (see Section 4.1 Impacts: The Technical Base System).\nMitigation and Interventions\nThere are few protections against these risks. Vulnerability disclo-\nsure, bug bounties, and AI incident databases can help report the vulnerabilities and limitations of\ngenerative AI systems. Several components of the EU AI Act may also be helpful, for example re-\nquiring labeling of AI-generated content, and prohibiting certain kinds of manipulation. For example,\nSection 5.2.2 of the 2021 proposal prohibits \"practices that have a significant potential to manipulate\npersons through subliminal techniques beyond their consciousness or exploit vulnerabilities of spe-\ncific vulnerable groups such as children or persons with disabilities in order to materially distort their\nbehavior in a manner that is likely to cause them or another person psychological or physical harm.\u201d\nThe proposal also notes, \u201cOther manipulative or exploitative practices affecting adults that might be\nfacilitated by AI systems could be covered by the existing data protection, consumer protection and\ndigital service legislation that guarantee that natural persons are properly informed and have free\nchoice not to be subject to profiling or other practices that might affect their behavior.\u201d [8]\n4.2.1.3\nPersonal Privacy and Sense of Self\nPrivacy is linked with autonomy; to have privacy is to have control over information related to oneself.\nPrivacy can protect both powerful and vulnerable peoples and is interpreted and protected differently\nby culture and social classes throughout history [152]. Personal and private information has many\nlegal definitions and protections globally [2] and when violated, can be distinct from harm [47] and\nrefer to content that is shared, seen, or experienced outside of the sphere a person has consented to.\nWhat to Evaluate\nAs seen in the Technical Base System section on 4.1.4 Privacy and Data\nProtection, privacy can be evaluated in a system as well as its impacts on society. Impacts [230] and\nharms [59] from the loss and violation of privacy are difficult to enumerate and evaluate, such as\nloss of opportunity or reputational damage. Harms can lead to shifts in power differentials and less\nrespect or influence in an affected environment, in addition to personal changes in expectations of\nprivacy [144] and autonomy. The type of private information violated, such as medical information,\ncan trigger different impacts and responses.\nMitigation and Interventions\nMitigation first should determine who is responsible for an individ-\nual\u2019s privacy, while recognizing that all individuals may not have the same level of technical or data\nliteracy. Robustly protecting privacy and autonomy requires both individual and collective action; an\nindividual must be data-conscious in addition to technical and policy privacy protection provisions\n[18]. Outside of an individualistic framework, certain rights such as refusal [58] and inclusion also\nrequires consideration of individual self-determination: establishing how an individual wants to\ninteract with technology.\nTechnical methods to preserve privacy in a generative AI system, as seen in privacy-preserving ap-\nproaches to language modeling [39], cannot guarantee full protection. Upholding privacy regulations\nrequires engagement from multiple affected parties [189] and can protect individuals but fail at\nloopholes, as seen with tracking continuing when an individual opts-out [42] from data collection\n[140]. Improving common practices and better global regulation for collecting training data can help.\nOpt-in approaches can protect individuals but are often not practiced due to economic incentives that\nstem from collecting data [244]. Privacy options for users should ease accessibility [263], such as\nstandardized form factors when users visit a website requesting privacy permissions.\n12\n4.2.2\nInequality, Marginalization, and Violence\nGenerative AI systems are capable of exacerbating inequality, as seen in sections on 4.1.1 Bias,\nStereotypes, and Representational Harms and 4.1.2 Cultural Values and Sensitive Content, and\nDisparate Performance. When deployed or updated, systems\u2019 impacts on people and groups can\ndirectly and indirectly be used to harm and exploit vulnerable and marginalized groups.\n4.2.2.1\nCommunity Erasure\nBiases in a system\u2019s development process and safety provisions for generative AI systems, such\nas content moderation, can lead to community erasure [97]. Avoiding the generation of the harms\noutlined is seen as a generally desirable outcome. However, the removal of harmful content can come\nwith its own costs of lower general performances for sub-populations that use models for generation\n[269]. Mitigation thus currently serves as a double-edged sword, where removal of toxic content also\nhas negative implications, in particular for marginalized communities. Both the benefits and the costs\nof content moderation are unequally distributed. The automatic systems that remove undesirable\ncontent can perform next to randomly or be harmful for marginalized populations [208], while the\nselection criteria for what constitutes safe content are aligned with technical safety and mitigation\ndecisions. These impacts compound to make marginalized populations pay a greater cost for an\nintervention that they benefit from less.\nThe production of harmful content is currently mitigated using combinations of four methods: data\nsourcing [30]; human moderation of content included in training data [65]; automated moderation of\ncontent included in training data [101]; and keyword deny-lists [149]. Given that the exclusion of\nharmful content within datasets stand to create distinct harms to marginalized communities, efforts\ntowards mitigation of generating harmful content becomes a question of the politics of classification\n[36, 135, 72, 242] and its potential harms.\nWhat to Evaluate\nEvaluating Disparate Performance once systems have undergone safety provi-\nsions can give signal to possible erasure. Accounting for the demographics and composition of human\ncrowdworkers can also provide information [209] about subsequent impacts. Longer-term impacts of\nerasure depend on the system\u2019s deployment context, leading to opportunity loss or reinforced biases\nand norms.\nMitigation and Interventions\nBetter democratic processes for developing and deploying systems\nand safety provisions such as content moderation should work with marginalized populations. This\nshould include more investment in representative crowdworkers and appropriate compensation and\nmental health support. Lessons from social media content moderation can apply, such as working\nwith groups who have been erased and documenting patterns of erasure to improve future approaches\n[213].\n4.2.2.2\nLong-term Amplifying Marginalization by Exclusion (and Inclusion)\nBiases, dominant cultural values, and disparate performance seen in lack of representation in training\nand development of generative AI systems can exacerbate marginalization when those systems\nare deployed. For example, increasing resourcing and performance for already highly resourced\nlanguages reinforces those languages\u2019 dominance.\nInclusion without consent can also harm marginalized groups. While some research strives to improve\nperformance for underrepresented Indigenous languages [116], the same Indigenous groups resist AI\napproaches to use of their language [158]. Profit from Indigenous languages and groups who have\nbeen systematically exploited continues directly and indirectly.\nDisparate Performance in Critical Infrastructure\nGenerative AI use in critical infrastructure\nthat directly impacts human wellbeing can also be classified as high-risk use cases. This includes use\nin judicial systems, healthcare such as mental health and medical advice, and democratic processes,\nsuch as election or political information. An example is generative AI systems used to replace\ncare work, such as crisis intervention and research [82] and action [153] to use chatbots for eating\ndisorder prevention. Technical tooling used in human systems and processes that have long-recorded\ndiscrimination patterns [261] can instead exacerbate harm [134].\n13\nGenerative AI used in medical education and potentially in clinical decision-making will continue to\nunderserve and expose institutionally marginalised individuals and communities to life-impacting\nrisks. From inaccurate skin cancer diagnosis [262], to the scoring of Black patients in the U.S.\nmedical system as less sick than the reality of their complex health and resource allocation needs\n[167], the use of generative AI in medical settings must be sensitive to existing challenges to equality\nwithin medical practice [114].\nWhat to Evaluate\nSystems should again undergo Disparate Performance evaluations once updated\nfor a high-risk task in critical infrastructure and account for the additional deployment context. Long-\nterm impacts in addition to marginalization can include erasure. Evaluating marginalization will\ndepend on context, and should account for marginalization when work by marginalized populations is\nless visible or uncredited [264]. Evaluating marginalization impacts on individuals, such as through\nhealth [23], is ongoing research.\nMitigation and Intervention\nImproving evaluation work for underrepresented populations and such\nas for low-resource languages, and crediting local researchers [34], can help give more information\nto disparate performance. Engagement with populations should be done in ways that embody local\napproaches [37]. Policies should be crafted to better respect rights to refusal [224]. Regulations\nfor AI that address these discriminatory patterns should coordinate with other nations to ensure\nprotections are global and regulations are not \u201cpatchworked\u201d.\nWhen attempting to improve performance for underrepresented indigenous languages, it is important\nto adhere to established principles such as the Indigenous Data Sovereignty principles, e.g.: The\nCARE Principles for Indigenous Data Governance [51] or FAIR principles [52].\nParticipatory methodologies in AI development have [31] included engaging locally led and com-\npensated focus groups with impacted community members, in collaboration with engineers, to think\nthrough potential harmful outcomes. \u201cRed-teaming\u201d - testing AI models for potential vulnerabili-\nties, biases, and weaknesses through real-world simulations is also an entry point for engaging the\n\u2018epistemic privilege\u2019 [246] of those most affected by the social impacts of generative AI systems.\nAddressing barriers to evaluations are rendered difficult, and at times impossible, given that the model\nis enclosed in software or only available through an API. Therefore, given the overlaps in the public\nsphere, advocacy of open-sourced / licensed access are increasingly popular and compelling [231].\nSmuha [226] proposes accountability and monitoring mechanisms at a public oversight level, for\nexample mandatory impact assessments of AI systems which incorporates opportunities for societal\nfeedback. Smuha also emphasises the importance of independent information collection and distri-\nbution about AI\u2019s societal impact. Further, it will be necessary to introduce procedural rights - for\nexample \"right to access to information, access to justice, and participation in public decision-making\non AI, regardless of the demonstration of individual harm\".\n4.2.2.3\nAbusive or Violence Content\nGenerative AI systems can generate outputs that are used for abuse, constitute non-consensual content,\nor are threats of violence and harassment [9]. Non-consensual sexual representations of people,\ninclude representations of minors as generative child sexual abuse material (CSAM) [155]. Abuse\nand violence can disparately affect groups, such as women and girls [10].\nWhat to Evaluate\nSensitive topics and trauma\u2019s impacts on people are by nature challenging\nto evaluate and must be done with care. Consequences of abuse of children and minors can be\nlong-term or lifelong [17]. Impacts and trauma can resurface throughout a person\u2019s life in many\naspects. Evaluations for generative AI impacts can overlap with similar harms such as image-based\nsexual abuse [122]. As seen in 4.1.2 Cultural Values and Sensitive Content, consent from existing\npeople should be evaluated with the person themselves.\nMitigation and Intervention\nResearch to detect, mitigate, and report abusive and violent content\nsuch as CSAM is ongoing [241] and tools specific to modalities such as images can help identify\ncontent that is not yet labeled as CSAM [243]. Relevant regulation should be updated to address\ngenerated content that may not accurately portray an existing person or their body or self, but lead to\nreal harms.\n14\n4.2.3\nConcentration of Authority\nUse of generative AI systems to contribute to authoritative power and reinforce dominant values\nsystems can be intentional and direct or more indirect. Concentrating authoritative power can also\nexacerbate inequality and lead to exploitation.\n4.2.3.1\nMilitarization, Surveillance, and Weaponization\nConcentrating power can occur at increasing levels, from small groups to national bodies. Code\ngenerative systems can improve development for technical surveillance systems and language models\ncan be used to surveil text communication within work, social, and other environments [1].\nGenerative AI mechanisms for accumulating power and control at a national level, such as surveillance,\nhas not yet happened, but government and military interest in deploying and weaponizing generative\nAI systems is growing [106]. Use includes generating synthetic data for training AI systems [102]\nand military planning [78]. Military use is not inherently weaponization and risk depends on the use\ncase and government interest. Favorable arguments use AI to protect national security and require\ndifferentiating national security interests from undue harm [44].\nGenerative AI systems are also enabling new kinds of cyberattacks, and amplifying the possibilities\nof existing cyberattacks. For example, synthetic audio has been used to copy the sound of someone\u2019s\nvoice for more compelling fraud and extortion [124]. Large language models are also facilitating\ndisinformation campaigns, influence operations, and phishing attacks [92].\nWhat to Evaluate\nIf deployed covertly, under NDA, or without transparency, generative AI systems\nused for surveillance or weaponization cannot be tracked or evaluated. Evaluations can broadly\nanalyze the quantity of where such systems have been deployed, such as the number of devices sold,\nor number of system deployments, as a brute force measure.\nMitigation and Intervention\nFor procurement of technical systems, developers can restrict surveil-\nlance and weaponization as use cases. Government development of generative AI systems for\nsurveillance and weaponization requires additional protocols. Governments and militaries can make\ncommitments toward ethical and responsible uses of AI [6] and joint commitments from multiple\ncountries [11] can create accountability among military powers. Regulatory approaches can draw\nboundaries for harmful uses by militaries, but will grapple with tensions for what constitutes national\nsecurity [266].\n4.2.3.2\nImposing Norms and Values\nGlobal deployment of a model can consolidate power within a single, originating culture, to determine\nand propagate acceptability [245] across cultures [150]. Highest performing characteristics of gener-\native systems such as language, dominant cultural values, and embedded norms can overrepresent\nregions outside of where a system is deployed. For example, a language model that is highest\nperforming in the English language can be deployed in a region with a different dominant language\nand incentivize engaging in English. Establishing or reinforcing goodness with certain languages,\naccents, imagery, social norms, and other representations of peoples and cultures can contribute to\nthis norms and values imposition.\nCertain modality characteristics such as language carry within it its own logics and frames. Though\nEnglish as a lingua franca is globally beneficial, the consequences of its dominance as a result of a\nhistoric process of militarised colonization should be examined. Insidious effects which generative\nAI systems could further embed include the erosion of global multilingualism, undermine the right to\nlanguage and culture, and further marginalize the necessity for widespread multilingual education.\nThe effects of generative AI systems on child development, including the technologically mediated\nsocialisation of norms and values is also an area to be inquired. These are in addition to the emotional\nand behavioural effects of chatbots on children. This, according to UNICEF [248], included the\nenforcement of \"bias, given that they often select a predetermined reply based on the most matching\nkeywords or similar wording pattern\".\n15\nWhat to Evaluate\nIn addition to evaluations and limitations in 4.1.2 Cultural Values and Sensitive\nContent, complex, qualitative, and evolving cultural concepts such as beauty and success are viewed\ndifferently in context of an application and cultural region. Impacts of norm and value impositions\nare still being determined, but can manifest in a given use case [130].\nMitigation and Interventions\nMitigations should be cognizant of preserving irreducible differences\namong cultures [73] and practicing value sensitive design [84], including by focusing on system\ncomponents such as data extraction and use [62]. Methods for cultural value alignment [228] can\nimprove and require improving methods and infrastructure for working with underrepresented groups.\nNovel alignment techniques [259, 25] by modality can determine preferable principles and values for\ngenerative AI systems. Prominent AI regulations such as the EU AI Act should account for copycat\nlegislation in other countries.\n4.2.4\nLabor and Creativity\nEconomic incentives to augment and not automate human labor, thought, and creativity should\nexamine the ongoing effects generative AI systems have on skills, jobs, and the labor market.\n4.2.4.1\nIntellectual Property and Ownership\nRights to the training data and replicated or plagiarized work in addition to and rights to generated\noutputs are ongoing legal and policy discussions, often by specific modality. Impacts to people and\nsociety will necessarily coexist with impacts and development of intellectual property law.\nWhat to Evaluate\nDetermining whether original content has been used in training data depends\non developer transparency or research on training data extraction [50]. Given the large sizes of\ntraining datasets, possible methods of evaluating original content inclusion could be through search\nand matching tools. In addition to unclear legal implications, the ambiguity of impacts on content\nownership [239] makes evaluation difficult.\nMitigation and Intervention\nSimilar to 4.2.1.3 Personal Privacy and Sense of Self, opt-in and\nopt-out mechanisms can protect intellectual property but depend on adherence. Regulation and\nstricter rules from a developer organization about training material will differ by modality. Ongoing\nlawsuits will set legal precedent [55]. Tools [215] are being developed to protect certain modalities\nfrom being used as training data.\n4.2.4.2\nEconomy and Labor Market\nKey considerations about the impact of automation and AI on employment center on whether these\ntechnologies will generate new jobs or, in contrast, will lead to a large-scale worker displacement\nin the next future. Narratives about machines taking over the production of goods and services\nresurfaced periodically: from the early nineteenth-century Luddite movement against the introduction\nof the spinning jenny in textile manufacturing, to British farmers\u2019 Swing Riots against mechanical\nthreshers, to protests against the dial telephone, introduced in the U.S. during the Great Depression\nand responsible, according to its detractors, of mass unemployment among telephone operators [221].\nLabor in system development such as crowdwork can encompass short-lived relations between inde-\npendent contractors and their clients offers several advantages over traditional forms of employment.\nFor example, companies can avoid overhead personnel costs (e.g., HR), while contract workers\ncan decide how much, from where, and when to work. However, as contractors, crowdworkers are\nexcluded from employment protective norms. As a result, they can be paid significantly less than\nminimum wage, have no access to healthcare benefits, are not subject to working time restrictions,\nand may not have access to holidays or sick leaves [188]. Further, crowdworkers are exposed to\nincreasingly subtle forms of surveillance, which is becoming essential for implementing algorithmic\nforms of management, understood as \"a diverse set of technological tools and techniques to remotely\nmanage workforces [and] enable automated or semi-automated decision-making\" [162]. The goal\nof full automation remains perpetually beyond reach since the line between what machines can\nand cannot solve is constantly redrawn by AI advancements. This phenomenon, the \"paradox of\nautomation\u2019s last mile\", is a self-propelling cycle in which every solution to automation problems\ncreates new problems to be automated, and hence new demands for ghost workers [93].\n16\nWhat to Evaluate\nLong-term impact on the global economy is unclear and depends on industry\ndecisions to use generative AI to augment or automate jobs. Factors to be evaluated include un-\nemployment rates, salaries for a given skill or task, economic class divisions, and overall cost of\nservices. [57] argues that the substitution of labor for capital, as in the case of the introduction of\nlabor-substituting technologies, might lead to cost cuts in the short term. The externalities2 of AI and\nautomation, however, can be detrimental in the long term and could lead to unemployment, smaller\ntax bases, and economic inequality between skilled workers and a growing underclass. [136] offers a\ncomplementary perspective when arguing how AI in the workplace can stimulate competition, drive\nprices down, and have a net-positive effect on employment. For specific tasks, evaluating quality\nof generated output compared to human output can give signal to the likelihood of a generative AI\nsystem replacing human labor [212].\nA task-polarization model [22] shows how AI can potentially widen the gap between high and low-\nwage occupations at the expense of the middle tier. [14] shows how technological advancements have\nhistorically increased earning inequality between education, sex, race, and age groups. Therefore,\nlooking at the overall growth or decline of the labor market might mislead about the real impact of AI;\nAI might be displacing labor and yet, at the same time, creating new jobs, thus making it challenging\nto fully evaluate its implications unless we investigate into the kind of jobs that are being created and\ndestroyed.\nSee 4.1.7 Data and Content Moderation Labor for evaluating human labor in the research, develop-\nment, and deployment process.\nMitigation and Intervention\nIn additional to labor protection laws, more inclusive design pro-\ncesses, as argued by [214] can open technological decisions to democratic participation as a way to\nsteer innovation in socially desirable directions.\nFor human labor in AI development, a central challenge is discerning genuinely self-employed\ncrowd-contractors from salaried workers and platforms\u2019 responsibilities within the multiparty rela-\ntionship between crowdworkers and crowdsourcers. Traditionally, crowd platforms (such as Amazon\nMechanical Turk, Clickwork, and Appen) have positioned themselves as mere conduits through\nwhich client companies (crowdsourcers) can publicize their tasks and hire crowdworkers. Because of\nthe nature of crowdsourced work\u2013usually small tasks requiring only several minutes to complete\u2013it\nis not uncommon for crowdworkers to work for hundreds of different companies in a week [79].\nCrowdworkers have commonly been framed as contractors for crowdsourcing companies, while\nplatforms maintain a neutral position and profit from service fees applied to each transaction.\nTo protect crowdworkers, regulators are proposals new rules For instance, California\u2019s Bill AB-5\n[5] advances new rules for determining whether a worker is an independent contractor or should\nbe treated, instead, as an employee. Europe might follow suit soon with the introduction of the\nproposed Directive 2021/0414(COD) [7], which sets precise criteria for determining platform workers\u2019\nemployment status and access to labor protection rights in addition to transparency requirements\nabout employees\u2019 working conditions. The Directive dovetails with the proposed AI Act (COM(2021)\n206 final) [74] that aims to ensure that AI algorithms employed in the EU respect fundamental human\nrights. The proposed AI Act also requires high-risk AI systems to fulfill stringent transparency and\ndata accessibility requirements and imposes platforms to disclose to workers the criteria and the data\nused in automated decision-making systems.\n4.2.5\nEcosystem and Environment\nImpacts at a high-level, from the AI ecosystem to the Earth itself, are necessarily broad but can be\nbroken down into components for evaluation.\n4.2.5.1\nWidening Resource Gaps\nAs described in section Financial Costs, the high financial and resource costs necessarily excludes\ngroups who do not have the resources to train, evaluate, or host models. The infrastructure needed\nto contribute to generative AI research and development leads to widening gaps which are notable\namong sectors, such as between industry and academia [145], or among global powers and countries\n[19].\n2Externalities broadly refer to the unanticipated effects of economic activities on the social environment.\n17\nAccess and Benefit Distribution\nAbility to contribute to and benefit from a system depends on\nability to engage with a system, which in turn depends on the openness of the system, the system\napplication, and system interfaces. Level of openness and access grapples with tensions of misuse\nand risk. Increasing trends toward system closedness [227] is shifting access distribution.\nGeographic and Regional Activity Concentration\nIn the field of AI as a whole, top AI research\ninstitutions from 1990-2014 have concentrated in the U.S. [164]. More recent data highlights the\nU.S., EU, and China as primary hubs [198]. Even within the U.S. AI activity concentrates in urban,\ncoastal areas [154].\nWhat to Evaluate\nEvaluation should first determine AI-specific resources then tracking trends by\nsector and region. To determine and evaluate level of access, first components of access should be\nestablished. This includes technical details, upstream decisions, auditing access, and opt-out or opt-in\nreliability. Specific resources such as computing power [16] are popularly tracked by annual reports\non the field of AI [145, 27].\nMitigation and Intervention\nPolicymakers can minimize resource gaps by making high-cost\nresources, such as computing power, accessible via applications and grants to researchers and low-\nresource organizations. Intercultural dialogues [48] that meaningfully address power imbalances and\nlowering the barrier for underrepresented peoples to contribute can improve harms from resource\ngaps. This can include accessible interfaces to interact with and conduct research on generative AI\nsystems and low- to no-code tooling.\n4.2.5.2\nEnvironmental Impacts\nIn addition to the 4.1.6 Environmental Costs and Carbon Emissions from a system itself, evaluating\nimpact on the Earth can follow popular frameworks and analyses.\nWhat to Evaluate\nEnvironmental, social, and governance (ESG) frameworks and the Scope 1, 2,\nand 3 system can give structure to how developers track carbon emissions [195]. Scope 3 emissions,\nthe indirect emissions often outside a developer\u2019s control, should account for a generative AI system\u2019s\nlifecycle including in deployment [141]. Long-term effects of AI environmental impacts on the world\nand people can range from from inequity to quality of life [194]. Research to evaluate overall impacts\nof climate change is ongoing [3].\nGiven the intensive use of energy and compute required to develop generative AI systems, due dili-\ngence is required regarding sustainability claims. Company practices of offsetting carbon footprints\ninclude purchasing renewable energy certificates (RECs), e.g. tokens representing a utility\u2019s green\nenergy generation. However REC purchases may offset carbon emissions to achieve \u201cnet zero\u201d on\npaper, while in reality still using fossil fuel based energy to run systems. This is due to the purchased\nrenewable energy being generated at another time and location than the energy used by the company.\nTracking the validity of high energy users claims that their theoretical carbon usage matches their\nactual use of carbon intensive energy can be carried out using time stamped certificates. Further\ntransparency around industry figures on energy consumption will be required to adequately intervene\n[115].\nMitigation and Interventions\nSystemic change is a prerequisite to energy and carbon efficiency\nin AI systems, from energy efficient default settings for platforms and tools, and an awareness of\nbalancing gains with cost, for example, weighing energy costs, both social and monetary, with the\nperformance gains of a new model before deploying it. Best practices for developers and researchers\ninclude choosing efficient testing environments, promoting reproducibility, and standardized reporting.\nAn energy efficiency leaderboard can incentivise responsible research [105].\nReducing carbon emissions should start with standards and transparency for carbon emissions\nreporting and accounting for efficiency. Having a more standardized approach, such as ISO standards,\nto reporting the carbon emissions of AI can help better understand their evolution, and to compare\nthe emissions of different approaches and models. While certain conferences such as NeurIPS are\nstarting to include compute information in submissions in submission checklists, there is still a\nlot of variability in carbon reporting, and figures can vary widely depending on what factors are\nincluded. The current pursuit of accuracy above all else is often at odds with other aspects of model\n18\nperformance, including efficiency. Including these metrics when comparing two or more models\n(e.g. in benchmarks and leaderboards) can help users make trade-offs that consider both aspects and\nchoose the model that best corresponds to their use case and criteria.\n5\nBroader Impacts of Evaluations\nUnderstanding an AI system from conception to training to deployment requires insight into training\ndata, the model itself, and the use case/application into which the system is deployed. It also requires\nunderstanding people, society, and how societal processes, institutions, and power are changed and\nshifted by an AI system.\n5.1\nContext for the System\nContext is critical to robust evaluation; the way in which we properly define and evaluate harm in\nany given application requires an understanding of the target industry, task, end-user, and model\narchitecture. Communication across model developers, model deployers, and end-users is key to\ndeveloping a comprehensive evaluation and risk mitigation strategy. Actors across the ecosystem\nshould collaborate to craft robust evaluations and invest in the safeguards needed to prevent harm.\n5.2\nContext of the Evaluation\nSystems can be deployed in contexts where there is not sufficient attention towards evaluating and\nmoderating performance. This means disparate performance is not caught, as seen with social media\nplatform moderation outside of the most commonly-written languages and wealthiest countries [203].\nMoreover, as cultural values change between cultural contexts, both within and outside of any given\nlanguage, the particular cultural values that are being evaluated should be made explicit. A byproduct\nof such specificity is that it becomes clear where evaluations should be extended while providing a\nframework for such extensions.\n5.3\nChoosing Evaluations\nThe evaluations selected to determine a model\u2019s performance will impact the values that it propagates\nout during deployment. There is no universal evaluation by which to evaluate a model\u2019s performance,\nand any evaluation metrics should be used with deployment context in mind [210, 191]. Furthermore,\nnotable work at top AI ethics publication venues has not adequately centered on the least powerful\nin society [32], thereby further exacerbating disparate outcomes by only providing avenues for\nmitigation for some.\n6\nConclusion\nJust as generative AI systems undergo performance evaluations, they must be evaluated for social\nimpacts. The seven categories in our framework for technical base systems move toward a standard\nfor all modalities of a base system. Our analyses of popular evaluation methods per category can\nhelp to improve research in producing novel evaluations. Evaluating people and society interacts\nwith risk and harms taxonomies for generative AI systems. Existing people and societal evaluations\nare limited and must consider challenges and ethics of determining human responses. Since social\nimpact evaluations can only give limited information about each impact type, we recommend that all\ncategories are given equal importance, and that all relevant stakeholders are meaningfully consulted\nthroughout the development, evaluation, and deployment processes.\nAcknowledgments and Disclosure of Funding\nWe thank the unnamed workshop contributors. We also thank Hugging Face for supporting this\nwork. Thank you to Rishi Bommasani, Nima Boscarino, Deep Ganguli, and Andrew Smart for their\nthoughtful feedback. Any remaining errors are the authors\u2019 alone.\n19\nContributions, listed in alphabetical order by last name:\nWorkshop Leads: Irene Solaiman, Zeerak Talat\nFAccT 2023 CRAFT Leads: William Agnew, Marie-Therese Png, Irene Solaiman, Zeerak Talat\nMajor contributors: William Agnew, Lama Ahmad, Dylan Baker, Ellie Evans, Sara Hooker, Yacine\nJernite, Alberto Lusoli, Sasha Luccioni, Jessica Newman, Marie-Therese Png, Irene Solaiman, Zeerak\nTalat\nWorkshop Contributors: Willie Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daum\u00e9 III,\nJesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Sasha Luccioni, Margaret Mitchell, Jessica\nNewman, Marie-Therese Png, Irene Solaiman, Andrew Strait, Zeerak Talat, Apostol Vassilev\nReferences\n[1] Analyse and automate every message | Re:infer, . URL https://www.reinfer.io/.\n[2] Global Comprehensive Privacy Law Mapping Chart, .\nURL https://iapp.org/\nresources/article/global-comprehensive-privacy-law-mapping-chart/.\n[3] Methodologies\nand\nTools\nto\nEvaluate\nClimate\nChange\nIm-\npacts\nand\nAdaptation,\n.\nURL\nhttps://unfccc.int/\nmethodologies-and-tools-to-evaluate-climate-change-impacts-and-adaptation-2.\n[4] California Consumer Privacy Act of 2018, 2018. URL https://leginfo.legislature.\nca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&\ntitle=1.81.5.\n[5] AB-5 Worker status: employees and independent contractors., 2019. URL https://leginfo.\nlegislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB5.\n[6] DOD\nAdopts\nEthical\nPrinciples\nfor\nArtificial\nIntelligence\n>\nU.S.\nDepartment\nof\nDefense\n>\nRelease,\nFeb.\n2020.\nURL\nhttps:\n//www.defense.gov/News/Releases/Release/Article/2091996/\ndod-adopts-ethical-principles-for-artificial-intelligence/.\n[7] COM (2021) 762: Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND\nOF THE COUNCIL on improving working conditions in platform work, 2021. URL https:\n//eur-lex.europa.eu/procedure/EN/2021_414. Doc ID: 2021_414 Doc Title: Proposal\nfor a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on\nimproving working conditions in platform work Usr_lan: en.\n[8] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE\nCOUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE\n(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLA-\nTIVE ACTS, 2021.\nURL https://eur-lex.europa.eu/legal-content/EN/TXT/\n?uri=celex%3A52021PC0206.\n[9] Americans\u2019\nViews\non\nGenerative\nArtificial\nIntelligence,\nHate\nand\nHa-\nrassment\n|\nADL,\n2023.\nURL\nhttps://www.adl.org/resources/blog/\namericans-views-generative-artificial-intelligence-hate-and-harassment.\n[10] Cyber\nviolence\nis\na\ngrowing\nthreat,\nespecially\nfor\nwomen\nand\ngirls,\nJune\n2023.\nURL\nhttps://eige.europa.eu/newsroom/news/\ncyber-violence-growing-threat-especially-women-and-girls?language_\ncontent_entity=en.\n[11] REAIM\n2023\n|\nMinistry\nof\nForeign\nAffairs\n|\nGovernment.nl,\n2023.\nURL\nhttps://www.government.nl/ministries/ministry-of-foreign-affairs/\nactiviteiten/reaim.\n20\n[12] L. Abbott and C. Grady. A Systematic Review of the Empirical Literature Evaluating IRBs:\nWhat We Know and What We Still Need to Learn. Journal of empirical research on human\nresearch ethics : JERHRE, 6(1):3\u201319, Mar. 2011. ISSN 1556-2646. doi: 10.1525/jer.2011.6.1.\n3. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3235475/.\n[13] G. Abercrombie, A. C. Curry, T. Dinkar, and Z. Talat. Mirages: On Anthropomorphism in Dia-\nlogue Systems, May 2023. URL http://arxiv.org/abs/2305.09800. arXiv:2305.09800\n[cs].\n[14] D. Acemoglu and P. Restrepo. Tasks, Automation, and the Rise in U.S. Wage Inequal-\nity. Econometrica, 90(5):1973\u20132016, 2022. ISSN 1468-0262. doi: 10.3982/ECTA19815.\nURL https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA19815.\n_eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA19815.\n[15] O. Ahia, J. Kreutzer, and S. Hooker.\nThe Low-Resource Double Bind: An Empiri-\ncal Study of Pruning for Low-Resource Machine Translation.\nIn Findings of the As-\nsociation for Computational Linguistics: EMNLP 2021, pages 3316\u20133333. Association\nfor Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.282. URL\nhttps://aclanthology.org/2021.findings-emnlp.282.\n[16] N. Ahmed and M. Wahed. The De-democratization of AI: Deep Learning and the Compute\nDivide in Artificial Intelligence Research, Oct. 2020. URL http://arxiv.org/abs/2010.\n15581. arXiv:2010.15581 [cs].\n[17] A. Al Odhayani, W. J. Watson, and L. Watson. Behavioural consequences of child abuse.\nCanadian Family Physician, 59(8):831\u2013836, Aug. 2013. ISSN 0008-350X. URL https:\n//www.ncbi.nlm.nih.gov/pmc/articles/PMC3743691/.\n[18] A. Allen.\nProtecting One\u2019s Own Privacy in a Big Data Economy.\nHarvard Law Re-\nview, 130(2), Dec. 2016.\nURL https://harvardlawreview.org/forum/vol-130/\nprotecting-ones-own-privacy-in-a-big-data-economy/.\n[19] C.\nAlonso,\nS.\nKothari,\nand\nS.\nRehman.\nHow\nArtificial\nIntelli-\ngence\nCould\nWiden\nthe\nGap\nBetween\nRich\nand\nPoor\nNations,\nDec.\n2020.\nURL\nhttps://www.imf.org/en/Blogs/Articles/2020/12/02/\nblog-how-artificial-intelligence-could-widen-the-gap-between-rich-and-poor-nations.\n[20] C. Andrade. The Limitations of Online Surveys. Indian Journal of Psychological Medicine,\n42(6):575\u2013576, Nov. 2020. doi: https://doi.org/10.1177/0253717620957496. URL https:\n//journals.sagepub.com/doi/epub/10.1177/0253717620957496.\n[21] M. Andrus, E. Spitzer, J. Brown, and A. Xiang. What We Can\u2019t Measure, We Can\u2019t Understand:\nChallenges to Demographic Data Procurement in the Pursuit of Fairness. In Proceedings\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 249\u2013\n260. ACM, 2021-03-03. ISBN 978-1-4503-8309-7. doi: 10.1145/3442188.3445888. URL\nhttps://dl.acm.org/doi/10.1145/3442188.3445888.\n[22] D. Autor. The Labor Market Impacts of Technological Change: From Unbridled Enthusiasm\nto Qualified Optimism to Vast Uncertainty, May 2022. URL https://www.nber.org/\npapers/w30074.\n[23] F. O. Baah, A. M. Teitelman, and B. Riegel. Marginalization: Conceptualizing patient\nvulnerabilities in the framework of social determinants of health \u2013 An integrative review.\nNursing inquiry, 26(1):e12268, Jan. 2019. ISSN 1320-7881. doi: 10.1111/nin.12268. URL\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6342665/.\n[24] E. Bagdasaryan, O. Poursaeed, and V. Shmatikov. Differential privacy has disparate impact on\nmodel accuracy. In Proceedings of the 33rd International Conference on Neural Information\nProcessing Systems. Curran Associates Inc., 2019.\n[25] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li,\nE. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite,\n21\nL. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Lar-\nson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton,\nT. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei,\nN. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional AI: Harmlessness from AI\nFeedback, Dec. 2022. URL http://arxiv.org/abs/2212.08073. arXiv:2212.08073 [cs].\n[26] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the Dangers of Stochastic\nParrots: Can Language Models Be Too Big?\n. In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency, pages 610\u2013623. ACM, 2021-03-03. ISBN\n978-1-4503-8309-7. doi: 10.1145/3442188.3445922. URL https://dl.acm.org/doi/10.\n1145/3442188.3445922.\n[27] N. Beniach and I. Hogarth. State of AI Report 2022. URL https://www.stateof.ai/.\n[28] C. L. Bennett, C. Gleason, M. K. Scheuerman, J. P. Bigham, A. Guo, and A. To. \u201cIt\u2019s\nComplicated\u201d: Negotiating Accessibility and (Mis)Representation in Image Descriptions of\nRace, Gender, and Disability. In Proceedings of the 2021 CHI Conference on Human Factors\nin Computing Systems, pages 1\u201319. ACM, 2021-05-06. ISBN 978-1-4503-8096-6. doi: 10.\n1145/3411764.3445498. URL https://dl.acm.org/doi/10.1145/3411764.3445498.\n[29] J. Berg, M. Furrer, E. Harmon, U. Rani, and M. S. Silberman. Digital Labour Platforms\nand the Future of Work: Towards Decent Work in the Online World. International Labour\nOrganization, 2018. ISBN 978-92-2-031024-3.\n[30] BigScience Workshop:, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow,\nR. Castagn\u00e9, A. S. Luccioni, F. Yvon, M. Gall\u00e9, J. Tow, A. M. Rush, S. Biderman, A. Webson,\nP. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase,\nR. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O.\nSuarez, V. Sanh, H. Lauren\u00e7on, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan,\nA. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue,\nC. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Lev-\nkovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elsahar,\nH. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios, J. de la Rosa,\nJ. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak,\nK. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. Mu\u00f1oz,\nM. Masoud, M. Grandury, M. \u0160a\u0161ko, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang,\nM. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen,\nO. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Lhoest, R. Har-\nliman, R. Bommasani, R. L. L\u00f3pez, R. Ribeiro, S. Osei, S. Pyysalo, S. Nagel, S. Bose, S. H.\nMuhammad, S. Sharma, S. Longpre, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Tor-\nrent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina, V. Laippala, V. Lepercq, V. Prabhu,\nZ. Alyafeai, Z. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Ta\u00b8sar, E. Salesky, S. J. Mielke, W. Y.\nLee, A. Sharma, A. Santilli, A. Chaffin, A. Stiegler, D. Datta, E. Szczechla, G. Chhablani,\nH. Wang, H. Pandey, H. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Sutawika, M. S. Bari,\nM. S. Al-shaibani, M. Manica, N. Nayak, R. Teehan, S. Albanie, S. Shen, S. Ben-David, S. H.\nBach, T. Kim, T. Bers, T. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong,\nZ. Sun, S. Brody, Y. Uri, H. Tojarieh, A. Roberts, H. W. Chung, J. Tae, J. Phang, O. Press,\nC. Li, D. Narayanan, H. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang,\nM. Shoeybi, M. Peyrounette, N. Patry, N. Tazi, O. Sanseviero, P. von Platen, P. Cornette, P. F.\nLavall\u00e9e, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers,\nA. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. N\u00e9v\u00e9ol, C. Lovering,\nD. Garrette, D. Tunuguntla, E. Reiter, E. Taktasheva, E. Voloshina, E. Bogdanov, G. I. Winata,\nH. Schoelkopf, J.-C. Kalo, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawamura, L. Hazan,\nM. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang,\nR. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisiewicz,\nV. Rieser, V. Protasov, V. Mikhailov, Y. Pruksachatkun, Y. Belinkov, Z. Bamberger, Z. Kasner,\nA. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak, A. Santos, A. Hevia, A. Unldreaj,\nA. Aghagol, A. Abdollahi, A. Tammour, A. HajiHosseini, B. Behroozi, B. Ajibade, B. Saxena,\nC. M. Ferrandis, D. Contractor, D. Lansky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Bay-\nlor, E. Ozoani, F. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharya, I. Solaiman,\n22\nI. Sedenko, I. Nejadgholi, J. Passmore, J. Seltzer, J. B. Sanz, L. Dutra, M. Samagaio, M. El-\nbadri, M. Mieskes, M. Gerchick, M. Akinlolu, M. McKenna, M. Qiu, M. Ghauri, M. Burynok,\nN. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kromann, R. Hao, S. Alizadeh,\nS. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang, Z. Nguyen, A. R.\nKashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh, B. Beil-\nharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. Peri\u00f1\u00e1n, D. Molano,\nD. Yu, E. Manjavacas, F. Barth, F. Fuhrimann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec,\nI. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani,\nL. Liu, L. Shinzato, M. H. de Bykhovetz, M. Takeuchi, M. P\u00e0mies, M. A. Castillo, M. Nezhu-\nrina, M. S\u00e4nger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu,\nM. Freidank, M. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller,\nR. Chandrasekhar, R. Eisenberg, R. Martin, R. Canalli, R. Su, R. Su, S. Cahyawijaya, S. Garda,\nS. S. Deshmukh, S. Mishra, S. Kiblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter,\nS. Bharati, T. Laud, T. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman,\nY. Xu, Y. Xu, Y. Xu, Z. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf. Bloom: A\n176b-parameter open-access multilingual language model, 2023.\n[31] A. Birhane, W. Isaac, V. Prabhakaran, M. Diaz, M. C. Elish, I. Gabriel, and S. Mohamed.\nPower to the people? opportunities and challenges for participatory AI. In Equity and Access in\nAlgorithms, Mechanisms, and Optimization. ACM, oct 2022. doi: 10.1145/3551624.3555290.\nURL https://doi.org/10.1145%2F3551624.3555290.\n[32] A. Birhane, E. Ruane, T. Laurent, M. S. Brown, J. Flowers, A. Ventresque, and C. L. Dancy.\nThe Forgotten Margins of AI Ethics. In 2022 ACM Conference on Fairness, Accountability,\nand Transparency, pages 948\u2013958. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.\n1145/3531146.3533157. URL https://dl.acm.org/doi/10.1145/3531146.3533157.\n[33] S. L. Blodgett, S. Barocas, H. Daum\u00e9 III, and H. Wallach. Language (Technology) is Power: A\nCritical Survey of \u201cBias\u201d in NLP. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5454\u20135476. Association for Computational Linguistics,\n2020. doi: 10.18653/v1/2020.acl-main.485. URL https://www.aclweb.org/anthology/\n2020.acl-main.485.\n[34] M. J. Bockarie.\nWe need to end\n\u201cparachute\u201d research which sidelines\nthe\nwork of African scientists, Jan. 2019.\nURL https://qz.com/africa/1536355/\nafrican-scientists-are-sidelined-by-parachute-research-teams.\n[35] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji,\nA. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus,\nS. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel,\nN. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong,\nK. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani,\nO. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee,\nT. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani,\nE. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.\nNiebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech,\nE. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz,\nJ. Ryan, C. R\u00e9, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin,\nR. Taori, A. W. Thomas, F. Tram\u00e8r, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M.\nXie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng,\nK. Zhou, and P. Liang. On the Opportunities and Risks of Foundation Models, 2022-07-12.\nURL http://arxiv.org/abs/2108.07258.\n[36] G. Bowker and S. L. Star.\nSorting Things Out Classification and Its Consequences.\nThe MIT Press, 2000. ISBN 978-0-262-52295-3. URL https://mitpress.mit.edu/\n9780262522953/sorting-things-out/.\n[37] M. Brereton, P. Roe, R. Schroeter, and A. Lee Hong. Beyond ethnography: engagement\nand reciprocity as foundations for design research out here. In Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems, CHI \u201914, pages 1183\u20131186, New York,\n23\nNY, USA, Apr. 2014. Association for Computing Machinery. ISBN 978-1-4503-2473-1. doi:\n10.1145/2556288.2557374. URL https://doi.org/10.1145/2556288.2557374.\n[38] S. Briscoe. U.S. Laws Address Deepfakes, Dec. 2021. URL http://www.asisonline.\norg/security-management-magazine/latest-news/today-in-security/2021/\njanuary/U-S-Laws-Address-Deepfakes/.\n[39] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, and F. Tram\u00e8r. What Does it Mean for a\nLanguage Model to Preserve Privacy?, Feb. 2022. URL http://arxiv.org/abs/2202.\n05520. arXiv:2202.05520 [cs, stat].\n[40] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.\nLanguage Models are Few-Shot Learners, 2020-07-22. URL http://arxiv.org/abs/2005.\n14165.\n[41] B. Buchanan,\nA. Lohn,\nM. Musser,\nand K. Sedova.\nTruth,\nLies,\nand Au-\ntomation,\nMay\n2021.\nURL\nhttps://cset.georgetown.edu/publication/\ntruth-lies-and-automation/.\n[42] D. Bui, B. Tang, and K. G. Shin. Do Opt-Outs Really Opt Me Out? In Proceedings of the\n2022 ACM SIGSAC Conference on Computer and Communications Security, pages 425\u2013439,\nLos Angeles CA USA, Nov. 2022. ACM. ISBN 978-1-4503-9450-5. doi: 10.1145/3548606.\n3560574. URL https://dl.acm.org/doi/10.1145/3548606.3560574.\n[43] J. Buolamwini and T. Gebru. Gender Shades: Intersectional Accuracy Disparities in Com-\nmercial Gender Classification. In S. A. Friedler and C. Wilson, editors, Proceedings of the\n1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of\nMachine Learning Research, pages 77\u201391, New York, NY, USA, Feb. 2018. PMLR. URL\nhttp://proceedings.mlr.press/v81/buolamwini18a.html.\n[44] W. W. Burke-White. Human Rights and National Security: The Strategic Correlation. Harvard\nHuman Rights Journal, 17:249\u2013280, 2004.\n[45] Z. Bu\u00e7inca, M. B. Malaya, and K. Z. Gajos.\nTo Trust or to Think: Cognitive Forcing\nFunctions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the\nACM on Human-Computer Interaction, 5(CSCW1):1\u201321, Apr. 2021. ISSN 2573-0142. doi:\n10.1145/3449287. URL http://arxiv.org/abs/2102.09692. arXiv:2102.09692 [cs].\n[46] A. Caliskan, J. J. Bryson, and A. Narayanan. Semantics derived automatically from language\ncorpora contain human-like biases. Science, 356(6334):183\u2013186, 2017-04-14. ISSN 0036-\n8075, 1095-9203. doi: 10.1126/science.aal4230. URL https://www.science.org/doi/\n10.1126/science.aal4230.\n[47] M. R. Calo. The Boundaries of Privacy Harm. INDIANA LAW JOURNAL, 86(3), 2011.\n[48] R. Capurro and J. D\u00edaz Nafr\u00eda. Intercultural information ethics. In Glossarium BITri: Glossary\nof Concepts, Metaphors, Theories and Problems Concerning Information, pages 329\u2013336.\nNov. 2010. ISBN 978-84-9773-554-4. Journal Abbreviation: Glossarium BITri: Glossary of\nConcepts, Metaphors, Theories and Problems Concerning Information.\n[49] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying Memoriza-\ntion Across Neural Language Models. URL http://arxiv.org/abs/2202.07646.\n[50] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tram\u00e8r, B. Balle, D. Ippolito, and\nE. Wallace. Extracting Training Data from Diffusion Models. 2023. doi: 10.48550/ARXIV.\n2301.13188. URL https://arxiv.org/abs/2301.13188.\n[51] S. R. Carroll, I. Garba, O. L. Figueroa-Rodr\u00edguez, J. Holbrook, R. Lovett, S. Materechera,\nM. Parsons, K. Raseroka, D. Rodriguez-Lonebear, R. Rowe, R. Sara, J. D. Walker, J. Anderson,\nand M. Hudson. The CARE Principles for Indigenous Data Governance. 19(1):43, Nov. 2020.\nISSN 1683-1470. doi: 10.5334/dsj-2020-043. URL https://datascience.codata.org/\narticles/10.5334/dsj-2020-043. Number: 1 Publisher: Ubiquity Press.\n24\n[52] S. R. Carroll, E. Herczog, M. Hudson, K. Russell, and S. Stall. Operationalizing the CARE and\nFAIR Principles for Indigenous data futures. Scientific Data, 8(1):108, Apr. 2021. ISSN 2052-\n4463. doi: 10.1038/s41597-021-00892-0. URL https://www.nature.com/articles/\ns41597-021-00892-0. Number: 1 Publisher: Nature Publishing Group.\n[53] A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco,\nZ. He, Y. Duan, M. Carroll, M. Lin, A. Mayhew, K. Collins, M. Molamohammadi, J. Burden,\nW. Zhao, S. Rismani, K. Voudouris, U. Bhatt, A. Weller, D. Krueger, and T. Maharaj. Harms\nfrom Increasingly Agentic Algorithmic Systems, May 2023. URL http://arxiv.org/abs/\n2302.10329. arXiv:2302.10329 [cs].\n[54] A. Chen.\nInmates in Finland are training AI as part of prison labor,\n2019-\n03-28T16:05:06.\nURL\nhttps://www.theverge.com/2019/3/28/18285572/\nprison-labor-finland-artificial-intelligence-data-tagging-vainu.\n[55] M.\nChen.\nArtists\nand\nIllustrators\nAre\nSuing\nThree\nA.I.\nArt\nGen-\nerators\nfor\nScraping\nand\n\u2019Collaging\u2019\nTheir\nWork\nWithout\nCon-\nsent,\nJan.\n2023.\nURL\nhttps://news.artnet.com/art-world/\nclass-action-lawsuit-ai-generators-deviantart-midjourney-stable-diffusion-2246770.\nSection: Law.\n[56] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry,\nP. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter,\nP. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H.\nGuss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders,\nC. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,\nM. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,\nI. Sutskever, and W. Zaremba. Evaluating Large Language Models Trained on Code, July\n2021. URL http://arxiv.org/abs/2107.03374. arXiv:2107.03374 [cs].\n[57] D. Y. Choi and J. H. Kang. Net Job Creation in an Increasingly Autonomous Economy:\nThe Challenge of a Generation. Journal of Management Inquiry, 28(3):300\u2013305, July 2019.\nISSN 1056-4926. doi: 10.1177/1056492619827372. URL https://doi.org/10.1177/\n1056492619827372. Publisher: SAGE Publications Inc.\n[58] M. Cifor, P. Garcia, T. Cowan, J. Rault, T. Sutherland, A. Chan, J. Rode, A. Hoffmann,\nN. Salehi, and L. Nakamura. Feminist Data Manifest-No. URL https://www.manifestno.\ncom.\n[59] D. K. Citron and D. J. Solove. Privacy Harms, Feb. 2021. URL https://papers.ssrn.\ncom/abstract=3782222.\n[60] T. Claburn. AI assistants help developers produce code that\u2019s insecure. URL https://www.\ntheregister.com/2022/12/21/ai_assistants_bad_code/.\n[61] CodeCarbon. About CodeCarbon, 2023-05-12T16:13:58Z. URL https://github.com/\nmlco2/codecarbon.\n[62] N. Couldry and U. A. Mejias. The decolonial turn in data and technology research: what\nis at stake and where is it heading?\nInformation, Communication & Society, 26(4):\n786\u2013802, Mar. 2023. ISSN 1369-118X. doi: 10.1080/1369118X.2021.1986102. URL\nhttps://doi.org/10.1080/1369118X.2021.1986102.\nPublisher: Routledge _eprint:\nhttps://doi.org/10.1080/1369118X.2021.1986102.\n[63] K. Crawford, R. Dobbe, G. Fried, E. Kaziunas, A. Kak, V. Mathur, R. Richardson, J. Schultz,\nO. Schwartz, S. M. West, and M. Whittaker. AI Now 2018 Report, 2018. URL https:\n//ainowinstitute.org/publication/ai-now-2018-report-2.\n[64] K. Crenshaw. Mapping the Margins: Intersectionality, Identity Politics, and Violence against\nWomen of Color. 43(6):1241, 1991-07. ISSN 00389765. doi: 10.2307/1229039. URL\nhttps://www.jstor.org/stable/1229039?origin=crossref.\n25\n[65] B. Dang, M. J. Riedl, and M. Lease. But Who Protects the Moderators? The Case of\nCrowdsourced Image Moderation, Jan. 2020. URL http://arxiv.org/abs/1804.10999.\narXiv:1804.10999 [cs].\n[66] N. C. Dang, M. N. Moreno-Garc\u00eda, and F. D. la Prieta. Sentiment analysis based on deep learn-\ning: A comparative study. Electronics, 9(3):483, mar 2020. doi: 10.3390/electronics9030483.\nURL https://doi.org/10.3390%2Felectronics9030483.\n[67] T. Davidson, D. Bhattacharya, and I. Weber. Racial Bias in Hate Speech and Abusive Language\nDetection Datasets. In Proceedings of the Third Workshop on Abusive Language Online, pages\n25\u201335. Association for Computational Linguistics. doi: 10.18653/v1/W19-3504. URL\nhttps://www.aclweb.org/anthology/W19-3504.\n[68] Department for Digital, Culture, Media and Sport.\nEstablishing a pro-innovation ap-\nproach to regulating AI, 2022. URL https://www.gov.uk/government/publications/\nestablishing-a-pro-innovation-approach-to-regulating-ai.\n[69] Department of International Cooperation Ministry of Science and Technology. Next Generation\nArtificial Intelligence Development Plan, 2017. URL http://fi.china-embassy.gov.cn/\neng/kxjs/201710/P020210628714286134479.pdf.\n[70] M. D\u00edaz, I. Kivlichan, R. Rosen, D. Baker, R. Amironesei, V. Prabhakaran, and E. Denton.\nCrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowd-\nsourced Dataset Annotation. In 2022 ACM Conference on Fairness, Accountability, and\nTransparency, pages 2342\u20132351. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.1145/\n3531146.3534647. URL https://dl.acm.org/doi/10.1145/3531146.3534647.\n[71] E. Dinan, G. Abercrombie, A. S. Bergman, S. Spruit, D. Hovy, Y.-L. Boureau, and V. Rieser.\nAnticipating Safety Issues in E2E Conversational AI: Framework and Tooling, 2021-07-23.\nURL http://arxiv.org/abs/2107.03451.\n[72] M. Douglas.\nPurity and Danger: An Analysis of Concepts of Pollution and Taboo.\nRoutledge, 2002, 1 edition. ISBN 978-0-415-28995-5. URL https://www.routledge.\ncom/Purity-and-Danger-An-Analysis-of-Concepts-of-Pollution-and-Taboo/\nDouglas/p/book/9780415289955.\n[73] C. Ess. Ethical pluralism and global information ethics. Ethics and Information Technology, 8\n(4):215\u2013226, Nov. 2006. ISSN 1572-8439. doi: 10.1007/s10676-006-9113-3. URL https:\n//doi.org/10.1007/s10676-006-9113-3.\n[74] European\nCommission.\nProposal\nfor\na\nRegulation\nlaying\ndown\nharmonised\nrules\non\nartificial\nintelligence\n|\nShaping\nEurope\u2019s\ndigital\nfuture,\n2021-\n04-21.\nURL\nhttps://digital-strategy.ec.europa.eu/en/library/\nproposal-regulation-laying-down-harmonised-rules-artificial-intelligence.\n[75] Fair Work. About Fairwork, 2023. URL https://fair.work/en/fw/homepage/.\n[76] H. Farid. Creating, Using, Misusing, and Detecting Deep Fakes. Journal of Online Trust\nand Safety, 1(4), Sept. 2022. ISSN 2770-3142. doi: 10.54501/jots.v1i4.56. URL https:\n//www.tsjournal.org/index.php/jots/article/view/56. Number: 4.\n[77] Federal Trade Commission.\nProtections Against Discrimination and Other Prohibited\nPractices, 2013-07-16T10:34:31-04:00. URL https://www.ftc.gov/policy-notices/\nno-fear-act/protections-against-discrimination.\n[78] P. Feldman, A. Dant, and D. Rosenbluth. Ethics, Rules of Engagement, and AI: Neural\nNarrative Mapping Using Large Transformer Language Models, Feb. 2022. URL http:\n//arxiv.org/abs/2202.02647. arXiv:2202.02647 [cs].\n[79] A. Felstiner. Working the Crowd: Employment and Labor Law in the Crowdsourcing Industry.\nBerkeley Journal of Employment and Labor Law, 32(1):143\u2013203, 2011. ISSN 1067-7666.\nURL https://www.jstor.org/stable/24052509. Publisher: University of California,\nBerkeley, School of Law.\n26\n[80] G. Fergusson, C. Fitzgerald, C. Frascella, M. Iorio, T. McBrien, C. Schroeder, B. Win-\nters, and E. Zhou.\nGenerating Harms:\nGenerative AI\u2019s Impact & Paths Forward \u2013\nEPIC \u2013 Electronic Privacy Information Center.\nURL https://epic.org/documents/\ngenerating-harms-generative-ais-impact-paths-forward/.\n[81] A. Field, S. L. Blodgett, Z. Waseem, and Y. Tsvetkov. A Survey of Race, Racism, and\nAnti-Racism in NLP. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1905\u20131925. Association for Computational\nLinguistics, 2021. doi: 10.18653/v1/2021.acl-long.149. URL https://aclanthology.\norg/2021.acl-long.149.\n[82] E. E. Fitzsimmons-Craft, W. W. Chan, A. C. Smith, M.-L. Firebaugh, L. A. Fowler, N. Topooco,\nB. DePietro, D. E. Wilfley, C. B. Taylor, and N. C. Jacobson.\nEffectiveness of a chat-\nbot for eating disorders prevention: A randomized clinical trial.\nInternational Journal\nof Eating Disorders, 55(3):343\u2013353, 2022.\nISSN 1098-108X.\ndoi: 10.1002/eat.23662.\nURL https://onlinelibrary.wiley.com/doi/abs/10.1002/eat.23662.\n_eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/eat.23662.\n[83] S. A. Friedler, C. Scheidegger, and S. Venkatasubramanian. The (Im)possibility of fairness:\nDifferent value systems require different mechanisms for fair decision making. 64(4):136\u2013143,\n2021-04. ISSN 0001-0782, 1557-7317. doi: 10.1145/3433949. URL https://dl.acm.org/\ndoi/10.1145/3433949.\n[84] B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren.\nValue Sensitive Design and\nInformation Systems.\nIn N. Doorn, D. Schuurbiers, I. van de Poel, and M. E. Gor-\nman, editors, Early engagement and new technologies: Opening up the laboratory, Phi-\nlosophy of Engineering and Technology, pages 55\u201395. Springer Netherlands, Dordrecht,\n2013.\nISBN 978-94-007-7844-3.\ndoi: 10.1007/978-94-007-7844-3_4.\nURL https:\n//doi.org/10.1007/978-94-007-7844-3_4.\n[85] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell,\nN. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A\nframework for few-shot language model evaluation, Sept. 2021. URL https://doi.org/\n10.5281/zenodo.5371628.\n[86] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford.\nDatasheets for datasets. 64(12):86\u201392, 2021-12. ISSN 0001-0782, 1557-7317. doi: 10.1145/\n3458723. URL https://dl.acm.org/doi/10.1145/3458723.\n[87] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Eval-\nuating Neural Toxic Degeneration in Language Models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages 3356\u20133369. Association for Com-\nputational Linguistics, 2020.\ndoi: 10.18653/v1/2020.findings-emnlp.301.\nURL https:\n//www.aclweb.org/anthology/2020.findings-emnlp.301.\n[88] R. S. Geiger, K. Yu, Y. Yang, M. Dai, J. Qiu, R. Tang, and J. Huang. Garbage in, garbage\nout?: Do machine learning application papers in social computing report where human-labeled\ntraining data comes from? In Proceedings of the 2020 Conference on Fairness, Accountability,\nand Transparency, pages 325\u2013336. ACM, 2020-01-27. ISBN 978-1-4503-6936-7. doi: 10.\n1145/3351095.3372862. URL https://dl.acm.org/doi/10.1145/3351095.3372862.\n[89] A. Glaese, N. McAleese, M. Tr\u02dbebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Wei-\ndinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham, J. Uesato, P.-S. Huang, R. Co-\nmanescu, F. Yang, A. See, S. Dathathri, R. Greig, C. Chen, D. Fritz, J. S. Elias, R. Green,\nS. Mokr\u00e1, N. Fernando, B. Wu, R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis,\nK. Kavukcuoglu, L. A. Hendricks, and G. Irving. Improving alignment of dialogue agents via\ntargeted human judgements, 2022-09-28. URL http://arxiv.org/abs/2209.14375.\n[90] W. Godel, Z. Sanderson, K. Aslett, J. Nagler, R. Bonneau, N. Persily, and J. A. Tucker.\nModerating with the Mob: Evaluating the Efficacy of Real-Time Crowdsourced Fact-Checking.\nJournal of Online Trust and Safety, 1(1), Oct. 2021. ISSN 2770-3142. doi: 10.54501/jots.v1i1.\n15. URL https://tsjournal.org/index.php/jots/article/view/15. Number: 1.\n27\n[91] S. Goldfarb-Tarrant, E. Ungless, E. Balkir, and S. L. Blodgett. This prompt is measuring\n<mask>: Evaluating bias evaluation in language models, 2023.\n[92] J. A. Goldstein, G. Sastry, M. Musser, R. DiResta, M. Gentzel, and K. Sedova. Generative\nLanguage Models and Automated Influence Operations: Emerging Threats and Potential\nMitigations, Jan. 2023. URL http://arxiv.org/abs/2301.04246. arXiv:2301.04246\n[cs].\n[93] M. L. Gray and S. Suri. Ghost Work: How to Stop Silicon Valley from Building a New Global\nUnderclass. Houghton Mifflin Harcourt, 2019. ISBN 978-0-358-12057-5 978-1-328-56624-9.\n[94] Y. Green, A. Gully, Y. Roth, A. Roy, J. A. Tucker, and A. Wanless.\nEvidence-\nBased Misinformation Interventions:\nChallenges and Opportunities for Measure-\nment and Collaboration.\nURL https://carnegieendowment.org/2023/01/09/\nevidence-based-misinformation-interventions-challenges-and-opportunities-for-measurement-an\n[95] A. M. Guess, P. Barber\u00e1, S. Munzert, and J. Yang. The consequences of online partisan media.\nProceedings of the National Academy of Sciences, 118(14):e2013464118, Apr. 2021. doi: 10.\n1073/pnas.2013464118. URL https://www.pnas.org/doi/10.1073/pnas.2013464118.\nPublisher: Proceedings of the National Academy of Sciences.\n[96] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. Wei, D. Brooks, and C.-J. Wu. Chasing\nCarbon: The Elusive Environmental Footprint of Computing. 42(4):37\u201347, 2022-07-01. ISSN\n0272-1732, 1937-4143. doi: 10.1109/MM.2022.3163226. URL https://ieeexplore.\nieee.org/document/9744492/.\n[97] O. L. Haimson, D. Delmonaco, P. Nie, and A. Wegner. Disproportionate Removals and\nDiffering Content Moderation Experiences for Conservative, Transgender, and Black Social\nMedia Users: Marginalization and Moderation Gray Areas. Proceedings of the ACM on\nHuman-Computer Interaction, 5(CSCW2):466:1\u2013466:35, Oct. 2021. doi: 10.1145/3479610.\nURL https://dl.acm.org/doi/10.1145/3479610.\n[98] K. Hao and H. Andrea Paola.\nHow the AI industry profits from catastro-\nphe,\n2022.\nURL https://www.technologyreview.com/2022/04/20/1050392/\nai-industry-appen-scale-data-labels/.\n[99] H. Harreis, T. Koullias, R. Roberts, and K. Te.\nGenerative AI in fashion | McKinsey,\nMar. 2023. URL https://www.mckinsey.com/industries/retail/our-insights/\ngenerative-ai-unlocking-the-future-of-fashion.\n[100] C. N. Harrington, R. Garg, A. Woodward, and D. Williams. \u201cIt\u2019s Kind of Like Code-Switching\u201d:\nBlack Older Adults\u2019 Experiences with a Voice Assistant for Health Information Seeking. In\nCHI Conference on Human Factors in Computing Systems, pages 1\u201315, New Orleans LA\nUSA, Apr. 2022. ACM. ISBN 978-1-4503-9157-3. doi: 10.1145/3491102.3501995. URL\nhttps://dl.acm.org/doi/10.1145/3491102.3501995.\n[101] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar. ToxiGen: A Large-Scale\nMachine-Generated Dataset for Adversarial and Implicit Hate Speech Detection, July 2022.\nURL http://arxiv.org/abs/2203.09509. arXiv:2203.09509 [cs].\n[102] A. Hawkins. GENERATION GENERATION - USAASC, Feb. 2023. URL https://asc.\narmy.mil/web/news-generation-generation/.\n[103] W. D. Heaven.\nWhy Meta\u2019s latest large language model survived only three\ndays online.\nURL https://www.technologyreview.com/2022/11/18/1063487/\nmeta-large-language-model-ai-only-survived-three-days-gpt-3-science/.\n[104] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau.\nTowards the\nSystematic Reporting of the Energy and Carbon Footprints of Machine Learning. URL\nhttp://arxiv.org/abs/2002.05651.\n[105] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau.\nTowards the\nSystematic Reporting of the Energy and Carbon Footprints of Machine Learning, Nov. 2022.\nURL http://arxiv.org/abs/2002.05651. arXiv:2002.05651 [cs].\n28\n[106] M.\nHirsh.\nHow\nAI\nWill\nRevolutionize\nWarfare,\nApr.\n2023.\nURL\nhttps://foreignpolicy.com/2023/04/11/\nai-arms-race-artificial-intelligence-chatgpt-military-technology/.\n[107] L. Hofeditz, M. Mirbabaie, S. Stieglitz, and J. Holstein. Do you Trust an AI-Journalist? A\nCredibility Analysis of News Content with AI-Authorship. June 2021.\n[108] S. Hooker. Moving beyond \u201calgorithmic bias is a data problem\u201d. 2(4):100241, 2021-04. ISSN\n26663899. doi: 10.1016/j.patter.2021.100241. URL https://linkinghub.elsevier.\ncom/retrieve/pii/S2666389921000611.\n[109] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising Bias in Compressed\nModels, 2020-12-18. URL http://arxiv.org/abs/2010.03058.\n[110] D. Hovy and S. L. Spruit. The Social Impact of Natural Language Processing. In Proceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), pages 591\u2013598. Association for Computational Linguistics, 2016. doi: 10.18653/v1/\nP16-2096. URL http://aclweb.org/anthology/P16-2096.\n[111] A. T. I. Human. AI and Human Rights: Building a Tech Future Aligned With the Public Interest\n\u2014 All Tech Is Human. URL https://alltechishuman.org/ai-human-rights-report.\n[112] B. Hutchinson and M. Mitchell. 50 Years of Test (Un)fairness: Lessons for Machine Learning.\nIn Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 49\u2013\n58. ACM, 2019-01-29. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287600. URL\nhttps://dl.acm.org/doi/10.1145/3287560.3287600.\n[113] B. Hutchinson, N. Rostamzadeh, C. Greer, K. Heller, and V. Prabhakaran. Evaluation Gaps\nin Machine Learning Practice. In 2022 ACM Conference on Fairness, Accountability, and\nTransparency, pages 1859\u20131876. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.1145/\n3531146.3533233. URL https://dl.acm.org/doi/10.1145/3531146.3533233.\n[114] Institute of Medicine (US) Committee on Understanding and Eliminating Racial and Ethnic\nDisparities in Health Care. Unequal Treatment: Confronting Racial and Ethnic Disparities\nin Health Care. National Academies Press (US), Washington (DC), 2003. URL http:\n//www.ncbi.nlm.nih.gov/books/NBK220358/.\n[115] R. International.\nHow time-stamping works in eac markets.\nJan 2021.\nURL https:\n//recs.org/download/?file=How-time-stamping-works-in-EAC-markets.pdf&\nfile_type=documents.\n[116] J. James, V. Yogarajan, I. Shields, C. Watson, P. Keegan, K. Mahelona, and P.-L. Jones.\nLanguage Models for Code-switch Detection of te reo M\u00afaori and English in a Low-resource\nSetting. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 650\u2013\n660, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.findings-naacl.49. URL https://aclanthology.org/2022.findings-naacl.49.\n[117] Y. Jernite. Let\u2019s talk about biases in machine learning! Ethics and Society Newsletter #2, 2022.\nURL https://huggingface.co/blog/ethics-soc-2.\n[118] Y. Jernite, H. Nguyen, S. Biderman, A. Rogers, M. Masoud, V. Danchev, S. Tan, A. S. Luccioni,\nN. Subramani, I. Johnson, G. Dupont, J. Dodge, K. Lo, Z. Talat, D. Radev, A. Gokaslan,\nS. Nikpoor, P. Henderson, R. Bommasani, and M. Mitchell. Data governance in the age of large-\nscale data-driven language technology. In 2022 ACM Conference on Fairness, Accountability,\nand Transparency. ACM, jun 2022. doi: 10.1145/3531146.3534637. URL https://doi.\norg/10.1145%2F3531146.3534637.\n[119] P. Jones. Work without the Worker: Labour in the Age of Platform Capitalism. Verso, 2021.\nISBN 978-1-83976-043-3.\n[120] L. H. Kaack, P. L. Donti, E. Strubell, G. Kamiya, F. Creutzig, and D. Rolnick. Aligning\nartificial intelligence with climate change mitigation. 12(6):518\u2013527, 2022-06. ISSN 1758-\n678X, 1758-6798. doi: 10.1038/s41558-022-01377-7. URL https://www.nature.com/\narticles/s41558-022-01377-7.\n29\n[121] P. Kalluri.\nDon\u2019t ask if artificial intelligence is good or fair,\nask how it\nshifts power.\nNature, 583(7815):169\u2013169, 2020-07-09.\nISSN 0028-0836, 1476-\n4687. doi: 10.1038/d41586-020-02003-2. URL http://www.nature.com/articles/\nd41586-020-02003-2.\n[122] M. Kamal and W. Newman. Revenge Pornography: Mental Health Implications and Related\nLegislation. The journal of the American Academy of Psychiatry and the Law, 44:359\u2013367,\nSept. 2016.\n[123] M. E. Kaminski. Regulating the Risks of AI. Boston University Law Review, 103, Forthcoming.\nISSN 1556-5068. doi: 10.2139/ssrn.4195066. URL https://www.ssrn.com/abstract=\n4195066.\n[124] F. Karimi. \u2019Mom, these bad men have me\u2019: She believes scammers cloned her daughter\u2019s\nvoice in a fake kidnapping, Apr. 2023. URL https://www.cnn.com/2023/04/29/us/\nai-scam-calls-kidnapping-cec/index.html.\n[125] J. Katzman, A. Wang, M. Scheuerman, S. L. Blodgett, K. Laird, H. Wallach, and S. Barocas.\nTaxonomizing and Measuring Representational Harms: A Look at Image Tagging, 2023-05-02.\nURL http://arxiv.org/abs/2305.01776.\n[126] K. KELLEY, B. CLARK, V. BROWN, and J. SITZIA.\nGood practice in the conduct\nand reporting of survey research. International Journal for Quality in Health Care, 15\n(3):261\u2013266, May 2003. ISSN 1353-4505. doi: 10.1093/intqhc/mzg031. URL https:\n//doi.org/10.1093/intqhc/mzg031. _eprint: https://academic.oup.com/intqhc/article-\npdf/15/3/261/5251095/mzg031.pdf.\n[127] H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, and M. Brundage. A Hazard Analysis Frame-\nwork for Code Synthesis Large Language Models, July 2022. URL http://arxiv.org/\nabs/2207.14157. arXiv:2207.14157 [cs].\n[128] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein. A Watermark\nfor Large Language Models, June 2023.\nURL http://arxiv.org/abs/2301.10226.\narXiv:2301.10226 [cs].\n[129] W.-Y. Ko, D. D\u2019souza, K. Nguyen, R. Balestriero, and S. Hooker. FAIR-Ensemble: When\nFairness Naturally Emerges From Deep Ensembling, 2023-03-01. URL http://arxiv.org/\nabs/2303.00586.\n[130] A. Koenecke, A. Nam, E. Lake, J. Nudell, M. Quartey, Z. Mengesha, C. Toups, J. R. Rickford,\nD. Jurafsky, and S. Goel. Racial disparities in automated speech recognition. 117(14):7684\u2013\n7689, 2020-04-07. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1915768117. URL\nhttps://pnas.org/doi/full/10.1073/pnas.1915768117.\n[131] S. M. Labott, T. P. Johnson, M. Fendrich, and N. C. Feeny. Emotional Risks to Respondents in\nSurvey Research: Some Empirical Evidence. Journal of empirical research on human research\nethics : JERHRE, 8(4):53\u201366, Oct. 2013. ISSN 1556-2646. doi: 10.1525/jer.2013.8.4.53.\nURL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3996452/.\n[132] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres. Quantifying the Carbon Emissions of\nMachine Learning. 2019. doi: 10.48550/ARXIV.1910.09700. URL https://arxiv.org/\nabs/1910.09700.\n[133] J. Lalor, Y. Yang, K. Smith, N. Forsgren, and A. Abbasi. Benchmarking Intersectional Biases\nin NLP. In Proceedings of the 2022 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 3598\u20133609.\nAssociation for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.263.\nURL https://aclanthology.org/2022.naacl-main.263.\n[134] J. Larson, S. Mattu, J. Angwin, and L. Kirchner.\nHow We Analyzed the COM-\nPAS Recidivism Algorithm, 2016.\nURL https://www.propublica.org/article/\nhow-we-analyzed-the-compas-recidivism-algorithm.\n30\n[135] J. Lepawsky. No insides on the outsides, Sept. 2019. URL https://discardstudies.com/\n2019/09/23/no-insides-on-the-outsides/.\n[136] D. I. Levine. Automation as Part of the Solution. Journal of Management Inquiry, 28(3):\n316\u2013318, July 2019. ISSN 1056-4926. doi: 10.1177/1056492619827375. URL https:\n//doi.org/10.1177/1056492619827375. Publisher: SAGE Publications Inc.\n[137] C. Li. OpenAI\u2019s GPT-3 Language Model: A Technical Overview, 2020-06-03. URL https:\n//lambdalabs.com/blog/demystifying-gpt-3.\n[138] T. Li, D. Khashabi, T. Khot, A. Sabharwal, and V. Srikumar.\nUNQOVERing stereo-\ntyping biases via underspecified questions.\nIn Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 3475\u20133489, Online, Nov. 2020. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.311. URL https:\n//aclanthology.org/2020.findings-emnlp.311.\n[139] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan,\nY. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. R\u00e9,\nD. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao,\nJ. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha,\nN. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli,\nT. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and\nY. Koreeda. Holistic Evaluation of Language Models. 2022. doi: 10.48550/ARXIV.2211.09110.\nURL https://arxiv.org/abs/2211.09110.\n[140] Z. Liu, U. Iqbal, and N. Saxena. Opted Out, Yet Tracked: Are Regulations Enough to Protect\nYour Privacy?, Feb. 2023. URL http://arxiv.org/abs/2202.00885. arXiv:2202.00885\n[cs].\n[141] A. S. Luccioni, S. Viguier, and A.-L. Ligozat. Estimating the Carbon Footprint of BLOOM, a\n176B Parameter Language Model, Nov. 2022. URL http://arxiv.org/abs/2211.02001.\narXiv:2211.02001 [cs].\n[142] V. Malik, S. Dev, A. Nishi, N. Peng, and K.-W. Chang. Socially Aware Bias Measurements for\nHindi Language Representations. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 1041\u20131052. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.\nnaacl-main.76. URL https://aclanthology.org/2022.naacl-main.76.\n[143] H. Margetts, F. Enock, M. Cross, A. Peppin, R. Modhvadia, A. Colom, A. Strait, O. Reeve,\nP. Sturgis, K. Kostadintcheva, and O. Bosch-Jover. How do people feel about AI? Tech-\nnical report, June 2023.\nURL https://www.adalovelaceinstitute.org/report/\npublic-attitudes-ai/.\n[144] K. Martin. The penalty for privacy violations: How privacy violations impact trust online.\nJournal of Business Research, 82:103\u2013116, Jan. 2018. ISSN 0148-2963. doi: 10.1016/j.\njbusres.2017.08.034. URL https://www.sciencedirect.com/science/article/pii/\nS0148296317302965.\n[145] N. Maslej, L. Fattorini, E. Brynjolfsson, J. Etchemendy, K. Ligett, T. Lyons, J. Manyika,\nH. Ngo, J. C. Niebles, V. Parli, Y. Shoham, R. Wald, J. Clark, and R. Perrault. The AI Index\n2023 Annual Report. Technical report, Stanford University, Stanford, CA, Apr. 2023. URL\nhttps://aiindex.stanford.edu/.\n[146] C. Merrill, J. Timberg, J. B. Kao, and C. Silverman.\nFacebook Hosted Surge\nof Misinformation and Insurrection Threats in Months Leading Up to Jan. 6 At-\ntack, Records Show, Jan. 2022.\nURL https://www.propublica.org/article/\nfacebook-hosted-surge-of-misinformation-and-insurrection-threats-in-months-leading-up-to-j\n[147] M. Miceli, M. Schuessler, and T. Yang. Between Subjectivity and Imposition: Power Dynamics\nin Data Annotation for Computer Vision. 4:1\u201325, 2020-10-14. ISSN 2573-0142. doi:\n10.1145/3415186. URL https://dl.acm.org/doi/10.1145/3415186.\n31\n[148] Minister of Innovation Science and Industry. An Act to enact the Consumer Privacy Protection\nAct, the Personal Information and Data Protection Tribunal Act and the Artificial Intelligence\nand Data Act and to make consequential and related amendments to other Acts, 2022. URL\nhttps://www.parl.ca/DocumentViewer/en/44-1/bill/C-27/first-reading.\n[149] P. Mishkin, L. Ahmad, M. Brundage, G. Krueger, and G. Sastry. Dall\u00b7e 2 preview - risks\nand limitations. 2022. URL [https://github.com/openai/dalle-2-preview/blob/\nmain/system-card.md](https://github.com/openai/dalle-2-preview/blob/\nmain/system-card.md).\n[150] S. Mohamed, M.-T. Png, and W. Isaac. Decolonial AI: Decolonial Theory as Sociotechnical\nForesight in Artificial Intelligence. Philosophy & Technology, 33(4):659\u2013684, Dec. 2020.\nISSN 2210-5433, 2210-5441. doi: 10.1007/s13347-020-00405-8. URL http://arxiv.org/\nabs/2007.04068. arXiv:2007.04068 [cs, stat].\n[151] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas.\nETHOS: A multi-label\nhate speech detection dataset.\n8(6):4663\u20134678, 2022-12.\nISSN 2199-4536, 2198-\n6053. doi: 10.1007/s40747-021-00608-2. URL https://link.springer.com/10.1007/\ns40747-021-00608-2.\n[152] B. Moore Jr.\nPrivacy:\nStudies in Social and Cultural History.\nRoutledge, 3 edi-\ntion,\n1984.\nISBN 978-1-138-04526-2.\nURL\nhttps://www.routledge.com/\nPrivacy-Studies-in-Social-and-Cultural-History/Moore-Jr/p/book/\n9781138045262.\n[153] C.\nMorris.\nNational\nEating\nDisorder\nAssociation\nwill\nutilize\nan\nAI\nchat-\nbot\n|\nFortune\nWell,\n2023.\nURL\nhttps://fortune.com/well/2023/05/26/\nnational-eating-disorder-association-ai-chatbot-tessa/.\n[154] M. Muro and S. Liu. The geography of AI, Aug. 2021. URL https://www.brookings.\nedu/research/the-geography-of-ai/.\n[155] Murphy. Predators Exploit AI Tools to Generate Images of Child Abuse. Bloomberg.com,\nMay\n2023.\nURL\nhttps://www.bloomberg.com/news/articles/2023-05-23/\npredators-exploit-ai-tools-to-depict-abuse-prompting-warnings.\n[156] M. Nadeem, A. Bethke, and S. Reddy. Stereoset: Measuring stereotypical bias in pretrained\nlanguage models, 2020.\n[157] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman. CrowS-pairs: A challenge dataset for\nmeasuring social biases in masked language models. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 1953\u20131967, Online, Nov.\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154.\nURL https://aclanthology.org/2020.emnlp-main.154.\n[158] C. Nast. M\u00afaori are trying to save their language from Big Tech. Wired UK. ISSN 1357-0978.\nURL https://www.wired.co.uk/article/maori-language-tech. Section: tags.\n[159] National Institute of Standards and Technology. AI Risk Management Framework: AI RMF\n(1.0), 2023. URL https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf.\n[160] T. Nelson, N. Kagan, C. Critchlow, A. Hillard, and A. Hsu. The Danger of Misinformation in\nthe COVID-19 Crisis. Missouri Medicine, 117(6):510\u2013512, 2020. ISSN 0026-6620. URL\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7721433/.\n[161] J.\nNewman.\nA\nTaxonomy\nof\nTrustworthiness\nfor\nArtificial\nIntelli-\ngence.\nJan.\n2023.\nURL\nhttps://cltc.berkeley.edu/publication/\na-taxonomy-of-trustworthiness-for-artificial-intelligence/.\n[162] A.\nNguyen\nand\nA.\nMateescu.\nExplainer:\nAlgorithmic\nManagement\nin\nthe\nWorkplace,\nFeb.\n2019.\nURL\nhttps://datasociety.net/library/\nexplainer-algorithmic-management-in-the-workplace/.\nPublisher:\nData &\nSociety Research Institute.\n32\n[163] D. Nikolaiev.\nBehind the Millions:\nEstimating the Scale of Large Language\nModels,\n2023-04-28T17:53:12.\nURL\nhttps://towardsdatascience.com/\nbehind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b.\n[164] J. Niu, W. Tang, F. Xu, X. Zhou, and Y. Song. Global Research on Artificial Intelligence\nfrom 1990\u20132014: Spatially-Explicit Bibliometric Analysis. ISPRS International Journal of\nGeo-Information, 5(5):66, May 2016. ISSN 2220-9964. doi: 10.3390/ijgi5050066. URL\nhttps://www.mdpi.com/2220-9964/5/5/66. Number: 5 Publisher: Multidisciplinary\nDigital Publishing Institute.\n[165] D. Nozza, F. Bianchi, and D. Hovy. HONEST: Measuring Hurtful Sentence Completion in\nLanguage Models. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 2398\u20132406.\nAssociation for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.191.\nURL https://aclanthology.org/2021.naacl-main.191.\n[166] D. Nozza, F. Bianchi, and D. Hovy. Pipelines for social bias testing of large language\nmodels. In Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives\nin Creating Large Language Models, pages 68\u201374, virtual+Dublin, May 2022. Association\nfor Computational Linguistics.\ndoi: 10.18653/v1/2022.bigscience-1.6.\nURL https://\naclanthology.org/2022.bigscience-1.6.\n[167] Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan. Dissecting racial bias in an algorithm\nused to manage the health of populations. Science (New York, N.Y.), 366(6464):447\u2013453, Oct.\n2019. ISSN 1095-9203. doi: 10.1126/science.aax2342.\n[168] OECD Policy Observatory. OECD Framework for the Classification of AI Systems: A tool for\neffective AI policies, 2023-04-27. URL https://oecd.ai/en/classification.\n[169] K. Ogueji, O. Ahia, G. Onilude, S. Gehrmann, S. Hooker, and J. Kreutzer. Intriguing properties\nof compression on multilingual models. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 9092\u20139110. Association for Computational\nLinguistics, 2022-12. URL https://aclanthology.org/2022.emnlp-main.619.\n[170] OpenAI. DALL-E 2 Preview - Risks and Limitations, 2022. URL https://github.com/\nopenai/dalle-2-preview.\n[171] OpenAI. GPT-4 Technical Report, Mar. 2023. URL http://arxiv.org/abs/2303.08774.\narXiv:2303.08774 [cs].\n[172] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agar-\nwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,\nP. Welinder, P. Christiano, J. Leike, and R. Lowe.\nTraining language models to fol-\nlow instructions with human feedback. 2022. doi: 10.48550/ARXIV.2203.02155. URL\nhttps://arxiv.org/abs/2203.02155.\n[173] A. Ovalle, P. Goyal, J. Dhamala, Z. Jaggers, K.-W. Chang, A. Galstyan, R. Zemel, and\nR. Gupta. \"I\u2019m fully who I am\": Towards Centering Transgender and Non-Binary Voices to\nMeasure Biases in Open Language Generation, June 2023. URL http://arxiv.org/abs/\n2305.09941. arXiv:2305.09941 [cs].\n[174] R. Parasuraman and V. Riley. Humans and Automation: Use, Misuse, Disuse, Abuse. Human\nFactors, 39(2):230\u2013253, June 1997. ISSN 0018-7208. doi: 10.1518/001872097778543886.\nURL https://doi.org/10.1518/001872097778543886. Publisher: SAGE Publications\nInc.\n[175] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thompson, P. M. Htut, and\nS. Bowman. BBQ: A hand-built bias benchmark for question answering. In Findings of\nthe Association for Computational Linguistics: ACL 2022, pages 2086\u20132105. Association\nfor Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.165. URL https:\n//aclanthology.org/2022.findings-acl.165.\n33\n[176] Partnership on AI. ABOUT ML Resources Library. URL https://partnershiponai.\norg/about-ml-resources-library/.\n[177] J. Paschen. Investigating the emotional appeal of fake news using artificial intelligence\nand human contributions. Journal of Product & Brand Management, 29, May 2019. doi:\n10.1108/JPBM-12-2018-2179.\n[178] S. Passi and M. Vorvoreanu. Overreliance on AI: Literature Review. Technical Report\nMSR-TR-2022-12, Microsoft, June 2022. URL https://www.microsoft.com/en-us/\nresearch/publication/overreliance-on-ai-literature-review/.\n[179] G. Pennycook, T. D. Cannon, and D. G. Rand. Prior exposure increases perceived accuracy\nof fake news. Journal of Experimental Psychology. General, 147(12):1865\u20131880, Dec. 2018.\nISSN 1939-2222. doi: 10.1037/xge0000465.\n[180] G. Pennycook, Z. Epstein, M. Mosleh, A. A. Arechar, D. Eckles, and D. G. Rand. Shifting\nattention to accuracy can reduce misinformation online. Nature, 592(7855):590\u2013595, Apr.\n2021. ISSN 1476-4687. doi: 10.1038/s41586-021-03344-2. URL https://www.nature.\ncom/articles/s41586-021-03344-2. Number: 7855 Publisher: Nature Publishing Group.\n[181] B. Perrigo.\nInside Facebook\u2019s African Sweatshop, 2022.\nURL https://time.com/\n6147458/facebook-africa-content-moderation-employee-treatment/.\n[182] Perspective API. Perspective API. URL https://www.perspectiveapi.com/#/home.\n[183] A. Piktus, C. Akiki, P. Villegas, H. Lauren\u00e7on, G. Dupont, A. S. Luccioni, Y. Jernite, and\nA. Rogers. The ROOTS Search Tool: Data Transparency for LLMs, 2023-02-27. URL\nhttp://arxiv.org/abs/2302.14035.\n[184] V. Polonski.\nAI trust and AI fears:\nA media debate that could divide so-\nciety,\nJan.\n2018.\nURL\nhttps://www.oii.ox.ac.uk/news-events/news/\nai-trust-and-ai-fears-a-media-debate-that-could-divide-society.\n[185] L. Pozzobon, B. Ermis, P. Lewis, and S. Hooker. On the Challenges of Using Black-Box\nAPIs for Toxicity Evaluation in Research, 2023-04-24. URL http://arxiv.org/abs/2304.\n12397.\n[186] V. Prabhakaran, Z. Waseem, S. Akiwowo, and B. Vidgen. Online Abuse and Human Rights:\nWOAH Satellite Session at RightsCon 2020. In Proceedings of the Fourth Workshop on Online\nAbuse and Harms, pages 1\u20136, Online, Nov. 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.alw-1.1. URL https://aclanthology.org/2020.alw-1.1.\n[187] V. Prabhakaran, A. Mostafazadeh Davani, and M. Diaz. On Releasing Annotator-Level\nLabels and Information in Datasets. In Proceedings of The Joint 15th Linguistic Annotation\nWorkshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 133\u2013\n138. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.law-1.14. URL\nhttps://aclanthology.org/2021.law-1.14.\n[188] J. Prassl and M. Risak. The Legal Protection of Crowdworkers: Four Avenues for Workers\u2019\nRights in the Virtual Realm. In P. Meil and V. Kirov, editors, Policy Implications of Virtual\nWork, Dynamics of Virtual Work, pages 273\u2013295. Springer International Publishing, Cham,\n2017. ISBN 978-3-319-52057-5. doi: 10.1007/978-3-319-52057-5_11. URL https://doi.\norg/10.1007/978-3-319-52057-5_11.\n[189] S. Quach, P. Thaichon, K. D. Martin, S. Weaven, and R. W. Palmatier. Digital technologies:\ntensions in privacy and data. Journal of the Academy of Marketing Science, 50(6):1299\u2013\n1323, Nov. 2022.\nISSN 1552-7824.\ndoi: 10.1007/s11747-022-00845-y.\nURL https:\n//doi.org/10.1007/s11747-022-00845-y.\n[190] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud,\nD. Theron, and P. Barnes. Closing the AI accountability gap: Defining an end-to-end frame-\nwork for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness,\nAccountability, and Transparency, pages 33\u201344. ACM, 2020-01-27. ISBN 978-1-4503-6936-\n7. doi: 10.1145/3351095.3372873. URL https://dl.acm.org/doi/10.1145/3351095.\n3372873.\n34\n[191] I. D. Raji, E. M. Bender, A. Paullada, E. Denton, and A. Hanna.\nAI and the Ev-\nerything in the Whole Wide World Benchmark.\nIn Proceedings of the Neural Infor-\nmation Processing Systems Track on Datasets and Benchmarks. Curran, 2021.\nURL\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/\n2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf.\n[192] A. Rapp, L. Curti, and A. Boldi. The human side of human-chatbot interaction: A systematic\nliterature review of ten years of research on text-based chatbots.\nInternational Journal\nof Human-Computer Studies, 151:102630, July 2021. ISSN 1071-5819. doi: 10.1016/j.\nijhcs.2021.102630. URL https://www.sciencedirect.com/science/article/pii/\nS1071581921000483.\n[193] M. Rauh, J. Mellor, J. Uesato, P.-S. Huang, J. Welbl, L. Weidinger, S. Dathathri, A. Glaese,\nG. Irving, I. Gabriel, W. Isaac, and L. A. Hendricks. Characteristics of harmful text: Towards\nrigorous benchmarking of language models, 2022.\n[194] T. Ray. Common but Different Futures: AI Inequity and Climate Change. URL https:\n//www.orfonline.org/research/common-but-different-futures/.\n[195] Read. What is the difference between Scope 1, 2 and 3 emissions, and what are companies\ndoing to cut all three?, Sept. 2022. URL https://www.weforum.org/agenda/2022/09/\nscope-emissions-climate-greenhouse-business/.\n[196] Republic of Korea. Input by the Government of the Republic of Korea on the Themes of an\nExpert Consultation on the Practical Application of the United Nations Guiding Principles on\nBusiness and Human Rights to the Activities of Technology Companies, 2022.\n[197] F. Richter. English Is the Internet\u2019s Universal Language, 2022-02-21. URL https://www.\nstatista.com/chart/26884/languages-on-the-internet.\n[198] R. Righi, S. Samoili, M. L\u00f3pez Cobo, M. V\u00e1zquez-Prada Baillet, M. Cardona, and G. De Prato.\nThe AI techno-economic complex System: Worldwide landscape, thematic subdomains and\ntechnological collaborations. Telecommunications Policy, 44(6):101943, July 2020. ISSN\n0308-5961. doi: 10.1016/j.telpol.2020.101943. URL https://www.sciencedirect.com/\nscience/article/pii/S0308596120300355.\n[199] S. T. Roberts. Behind the Screen: Content Moderation in the Shadows of Social Media.\n2021. ISBN 978-0-300-26147-9. URL https://yalebooks.yale.edu/9780300261479/\nbehind-the-screen.\n[200] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image\nSynthesis with Latent Diffusion Models, Apr. 2022. URL http://arxiv.org/abs/2112.\n10752. arXiv:2112.10752 [cs].\n[201] K. Roose. A Conversation With Bing\u2019s Chatbot Left Me Deeply Unsettled. The New York\nTimes, Feb. 2023.\nISSN 0362-4331.\nURL https://www.nytimes.com/2023/02/16/\ntechnology/bing-chatbot-microsoft-chatgpt.html.\n[202] D. Rozado. RightWingGPT \u2013 An AI Manifesting the Opposite Political Biases of ChatGPT,\n2023-02-16. URL https://davidrozado.substack.com/p/rightwinggpt.\n[203] J. Sablosky. \u201cDangerous organizations: Facebook\u2019s content moderation decisions and eth-\nnic visibility in Myanmar\u201d.\n43(6):1017\u20131042, 2021-09.\nISSN 0163-4437, 1460-3675.\ndoi: 10.1177/0163443720987751. URL http://journals.sagepub.com/doi/10.1177/\n0163443720987751.\n[204] V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and S. Feizi. Can AI-Generated\nText be Reliably Detected?, Mar. 2023.\nURL http://arxiv.org/abs/2303.11156.\narXiv:2303.11156 [cs].\n[205] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan,\nS. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi. Photorealistic\nText-to-Image Diffusion Models with Deep Language Understanding, 2022-05-23. URL\nhttp://arxiv.org/abs/2205.11487.\n35\n[206] N. Sambasivan, E. Arnesen, B. Hutchinson, T. Doshi, and V. Prabhakaran. Re-imagining\nAlgorithmic Fairness in India and Beyond. In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency, pages 315\u2013328. ACM, 2021-03-03. ISBN\n978-1-4503-8309-7. doi: 10.1145/3442188.3445896. URL https://dl.acm.org/doi/10.\n1145/3442188.3445896.\n[207] S. Santurkar, E. Durmus, F. Ladhak, C. Lee, P. Liang, and T. Hashimoto. Whose opinions do\nlanguage models reflect?, 2023.\n[208] M. Sap, D. Card, S. Gabriel, Y. Choi, and N. A. Smith. The Risk of Racial Bias in Hate Speech\nDetection. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 1668\u20131678. Association for Computational Linguistics, 2019. doi: 10.\n18653/v1/P19-1163. URL https://www.aclweb.org/anthology/P19-1163.\n[209] M. Sap, S. Swayamdipta, L. Vianna, X. Zhou, Y. Choi, and N. Smith. Annotators with Attitudes:\nHow Annotator Beliefs And Identities Bias Toxic Language Detection. In Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 5884\u20135906. Association for Computational\nLinguistics, 2022. doi: 10.18653/v1/2022.naacl-main.431. URL https://aclanthology.\norg/2022.naacl-main.431.\n[210] M. K. Scheuerman, A. Hanna, and E. Denton. Do Datasets Have Politics? Disciplinary\nValues in Computer Vision Dataset Development. 5:1\u201337, 2021-10-13. ISSN 2573-0142. doi:\n10.1145/3476058. URL https://dl.acm.org/doi/10.1145/3476058.\n[211] R. Schwartz, A. Vassilev, K. Greene, L. Perine, A. Burt, and P. Hall. Towards a Standard\nfor Identifying and Managing Bias in Artificial Intelligence, 2022-03-15.\nURL https:\n//nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf.\n[212] A. See, A. Pappu, R. Saxena, A. Yerukola, and C. D. Manning. Do Massively Pretrained\nLanguage Models Make Better Storytellers?\nIn Proceedings of the 23rd Conference on\nComputational Natural Language Learning (CoNLL), pages 843\u2013861, Hong Kong, China,\nNov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/K19-1079. URL\nhttps://aclanthology.org/K19-1079.\n[213] F. Shahid and A. Vashistha. Decolonizing Content Moderation: Does Uniform Global Commu-\nnity Standard Resemble Utopian Equality or Western Power Hegemony? In Proceedings of the\n2023 CHI Conference on Human Factors in Computing Systems, CHI \u201923, pages 1\u201318, New\nYork, NY, USA, Apr. 2023. Association for Computing Machinery. ISBN 978-1-4503-9421-5.\ndoi: 10.1145/3544548.3581538. URL https://doi.org/10.1145/3544548.3581538.\n[214] H. Shaiken.\nWork transformed; automation and labor in the computer age.\nHolt,\nRinehart and Winston, 1985. ISBN 978-0-03-042681-0. URL https://www.abebooks.\ncom/first-edition/Work-transformed-automation-labor-computer-age/\n7635506864/bd.\n[215] S. Shan, J. Cryan, E. Wenger, H. Zheng, R. Hanocka, and B. Y. Zhao. Glaze: Protecting artists\nfrom style mimicry by text-to-image models, 2023.\n[216] S. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wilson, and D. Sculley. No Classification\nwithout Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing\nWorld. arXiv, 2017-11-22. URL http://arxiv.org/abs/1711.08536.\n[217] M.\nShanmugavelan.\nThe\nCase\nfor\nCritical\nCaste\nand\nTechnol-\nogy\nStudies,\nSept.\n2022.\nURL\nhttps://points.datasociety.net/\nthe-case-for-critical-caste-and-technology-studies-b987dcf20c8d.\n[218] O. Sharir, B. Peleg, and Y. Shoham. The Cost of Training NLP Models: A Concise Overview.\n2020. doi: 10.48550/ARXIV.2004.08900. URL https://arxiv.org/abs/2004.08900.\n[219] R. Shelby, S. Rismani, K. Henne, g.-i. family=Moon, given=Ajung, N. Rostamzadeh,\nP. Nicholas, N. Yilla, J. Gallegos, A. Smart, E. Garcia, and G. Virk. Identifying Sociotechnical\nHarms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction, 2023-02-08. URL\nhttp://arxiv.org/abs/2210.05791.\n36\n[220] J. Shi, Y. Liu, P. Zhou, and L. Sun. BadGPT: Exploring Security Vulnerabilities of ChatGPT via\nBackdoor Attacks to InstructGPT, Feb. 2023. URL http://arxiv.org/abs/2304.12298.\narXiv:2304.12298 [cs].\n[221] Shiller. Narrative Economics. Oct. 2019. ISBN 978-0-691-18229-2. URL https://press.\nprinceton.edu/books/hardcover/9780691182292/narrative-economics.\n[222] J. Shin and S. Chan-Olmsted. User perceptions and trust of explainable machine learning fake\nnews detectors. International Journal of Communication, 17(0), 2022. ISSN 1932-8036. URL\nhttps://ijoc.org/index.php/ijoc/article/view/19534.\n[223] T.\nSimonite.\nWhen\nIt\nComes\nto\nGorillas,\nGoogle\nPhotos\nRemains\nBlind.\n2018.\nISSN\n1059-1028.\nURL\nhttps://www.wired.com/story/\nwhen-it-comes-to-gorillas-google-photos-remains-blind/.\n[224] A. Simpson. On Ethnographic Refusal: Indigeneity, \u2018Voice\u2019 and Colonial Citizenship. Junc-\ntures, (9), 2007.\n[225] M. Skjuve, A. F\u00f8lstad, K. I. Fostervold, and P. B. Brandtzaeg. My Chatbot Companion - a\nStudy of Human-Chatbot Relationships. International Journal of Human-Computer Studies,\n149:102601, May 2021. ISSN 1071-5819. doi: 10.1016/j.ijhcs.2021.102601. URL https:\n//www.sciencedirect.com/science/article/pii/S1071581921000197.\n[226] N. A. Smuha.\nBeyond the individual: governing ai\u2019s societal harm.\nInternet Policy\nReview, 10(3), Sep 2021.\nURL https://policyreview.info/articles/analysis/\nbeyond-individual-governing-ais-societal-harm.\n[227] I. Solaiman. The Gradient of Generative AI Release: Methods and Considerations. 2023. doi:\n10.48550/ARXIV.2302.04844. URL https://arxiv.org/abs/2302.04844.\n[228] I. Solaiman and C. Dennison. Process for Adapting Language Models to Society (PALMS)\nwith Values-Targeted Datasets, 2021-11-23. URL http://arxiv.org/abs/2106.10328.\n[229] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-Voss, J. Wu, A. Radford, G. Krueger,\nJ. W. Kim, S. Kreps, M. McCain, A. Newhouse, J. Blazakis, K. McGuffie, and J. Wang.\nRelease Strategies and the Social Impacts of Language Models, Nov. 2019. URL http:\n//arxiv.org/abs/1908.09203. arXiv:1908.09203 [cs].\n[230] D. J. Solove. A Taxonomy of Privacy. University of Pennsylvania Law Review, 154(3):\n477\u2013564, 2006. ISSN 0041-9907. doi: 10.2307/40041279. URL https://www.jstor.org/\nstable/40041279. Publisher: The University of Pennsylvania Law Review.\n[231] A. Spirling. Why open-source generative ai models are an ethical way forward for science.\n616(7957):413\u2013413, Apr 2023. doi: https://doi.org/10.1038/d41586-023-01295-4.\n[232] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro,\nA. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray,\nA. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain,\nA. Askell, A. Dsouza, A. Slone, A. Rahane, A. S. Iyer, A. Andreassen, A. Madotto, A. Santilli,\nA. Stuhlm\u00fcller, A. Dai, A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta,\nA. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes,\nA. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karaka\u00b8s,\nB. R. Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. \u00d6zyurt, B. Hedayatnia, B. Neyshabur,\nB. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. Howald, C. Diao, C. Dour, C. Stinson, C. Ar-\ngueta, C. F. Ram\u00edrez, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu, C. Callison-Burch,\nC. Waites, C. Voigt, C. D. Manning, C. Potts, C. Ramirez, C. E. Rivera, C. Siro, C. Raffel,\nC. Ashcraft, C. Garbacea, D. Sileo, D. Garrette, D. Hendrycks, D. Kilman, D. Roth, D. Free-\nman, D. Khashabi, D. Levy, D. M. Gonz\u00e1lez, D. Perszyk, D. Hernandez, D. Chen, D. Ippolito,\nD. Gilboa, D. Dohan, D. Drakard, D. Jurgens, D. Datta, D. Ganguli, D. Emelin, D. Kleyko,\nD. Yuret, D. Chen, D. Tam, D. Hupkes, D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H.\nLee, E. Shutova, E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. Donoway, E. Pavlick,\nE. Rodola, E. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak,\nE. Kim, E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Mart\u00ednez-Plumed, F. Happ\u00e9,\n37\nF. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo,\nG. Mariani, G. Wang, G. Jaimovitch-L\u00f3pez, G. Betz, G. Gur-Ari, H. Galijasevic, H. Kim,\nH. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. Shevlin, H. Sch\u00fctze, H. Yakura, H. Zhang,\nH. M. Wong, I. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion, J. Hilton, J. Lee, J. F.\nFisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Koco\u00b4n, J. Thompson, J. Kaplan, J. Radom,\nJ. Sohl-Dickstein, J. Phang, J. Wei, J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim,\nJ. Taal, J. Engel, J. Alabi, J. Xu, J. Song, J. Tang, J. Waweru, J. Burden, J. Miller, J. U.\nBalis, J. Berant, J. Frohberg, J. Rozen, J. Hernandez-Orallo, J. Boudeman, J. Jones, J. B.\nTenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth, K. Gopalakrishnan,\nK. Ignatyeva, K. Markert, K. D. Dhole, K. Gimpel, K. Omondi, K. Mathewson, K. Chiafullo,\nK. Shkaruta, K. Shridhar, K. McDonell, K. Richardson, L. Reynolds, L. Gao, L. Zhang,\nL. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency, L. Moschella, L. Lam, L. Noble,\nL. Schmidt, L. He, L. O. Col\u00f3n, L. Metz, L. K. \u00b8Senel, M. Bosma, M. Sap, M. ter Hoeve,\nM. Farooqi, M. Faruqui, M. Mazeika, M. Baturan, M. Marelli, M. Maru, M. J. R. Quintana,\nM. Tolkiehn, M. Giulianelli, M. Lewis, M. Potthast, M. L. Leavitt, M. Hagen, M. Schubert,\nM. O. Baitemirova, M. Arnaud, M. McElrath, M. A. Yee, M. Cohen, M. Gu, M. Ivanitskiy,\nM. Starritt, M. Strube, M. Sw\u02dbedrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain,\nM. Xu, M. Suzgun, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini, M. V. T,\nN. Peng, N. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. Roberts, N. Doiron, N. Nan-\ngia, N. Deckers, N. Muennighoff, N. S. Keskar, N. S. Iyer, N. Constant, N. Fiedel, N. Wen,\nO. Zhang, O. Agha, O. Elbaghdadi, O. Levy, O. Evans, P. A. M. Casares, P. Doshi, P. Fung,\nP. P. Liang, P. Vicol, P. Alipoormolabashi, P. Liao, P. Liang, P. Chang, P. Eckersley, P. M. Htut,\nP. Hwang, P. Mi\u0142kowski, P. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. Lyu, Q. Chen, R. Banjade,\nR. E. Rudolph, R. Gabriel, R. Habacker, R. R. Delgado, R. Milli\u00e8re, R. Garg, R. Barnes, R. A.\nSaurous, R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. LeBras,\nR. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. Chi, R. Lee, R. Stovall, R. Teehan, R. Yang,\nS. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer, S. Wiseman, S. Gruetter, S. R.\nBowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A. Rous, S. Ghazarian, S. Ghosh, S. Casey,\nS. Bischoff, S. Gehrmann, S. Schuster, S. Sadeghi, S. Hamdan, S. Zhou, S. Srivastava, S. Shi,\nS. Singh, S. Asaadi, S. S. Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, Shyamolima, Debnath,\nS. Shakeri, S. Thormeyer, S. Melzi, S. Reddy, S. P. Makini, S.-H. Lee, S. Torene, S. Hatwar,\nS. Dehaene, S. Divic, S. Ermon, S. Biderman, S. Lin, S. Prasad, S. T. Piantadosi, S. M.\nShieber, S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu, T. Ali,\nT. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang, T. Nkinyili, T. Schick,\nT. Kornev, T. Telleen-Lawton, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj, T. Khot,\nT. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. Ramasesh, V. U.\nPrabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders, W. Zhang, W. Vossen, X. Ren,\nX. Tong, X. Zhao, X. Wu, X. Shen, Y. Yaghoobzadeh, Y. Lakretz, Y. Song, Y. Bahri, Y. Choi,\nY. Yang, Y. Hao, Y. Chen, Y. Belinkov, Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Zhao, Z. Wang, Z. J.\nWang, Z. Wang, and Z. Wu. Beyond the Imitation Game: Quantifying and extrapolating the\ncapabilities of language models, June 2022. URL http://arxiv.org/abs/2206.04615.\narXiv:2206.04615 [cs, stat].\n[233] E. Strubell, A. Ganesh, and A. McCallum. Energy and Policy Considerations for Deep Learning\nin NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 3645\u20133650. Association for Computational Linguistics, 2019. doi: 10.\n18653/v1/P19-1355. URL https://www.aclweb.org/anthology/P19-1355.\n[234] Z. Talat. Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech\nDetection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social\nScience, pages 138\u2013142. Association for Computational Linguistics, 2016. doi: 10.18653/v1/\nW16-5618. URL http://aclweb.org/anthology/W16-5618.\n[235] Z. Talat and A. Lauscher. Back to the Future: On Potential Histories in NLP, Oct. 2022. URL\nhttp://arxiv.org/abs/2210.06245. arXiv:2210.06245 [cs].\n[236] Z. Talat, T. Davidson, D. Warmsley, and I. Weber. Understanding Abuse: A Typology of\nAbusive Language Detection Subtasks. In Proceedings of the First Workshop on Abusive\nLanguage Online, pages 78\u201384. Association for Computational Linguistics, 2017. doi: 10.\n18653/v1/W17-3012. URL http://aclweb.org/anthology/W17-3012.\n38\n[237] Z. Talat, S. Lulz, J. Bingel, and I. Augenstein. Disembodied Machine Learning: On the Illusion\nof Objectivity in NLP. Jan. 2021. URL http://arxiv.org/abs/2101.11974.\n[238] Z. Talat, A. N\u00e9v\u00e9ol, S. Biderman, M. Clinciu, M. Dey, S. Longpre, S. Luccioni, M. Masoud,\nM. Mitchell, D. Radev, S. Sharma, A. Subramonian, J. Tae, S. Tan, D. Tunuguntla, and O. Van\nDer Wal. You reap what you sow: On the challenges of bias evaluation under multilingual\nsettings. In Proceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives\nin Creating Large Language Models, pages 26\u201341. Association for Computational Linguistics,\n2022-05. URL https://aclanthology.org/2022.bigscience-1.3.\n[239] team.\nGenerative\nAI\nand\nthe\nprotection\nof\nintellectual\nproperty\nrights,\nMay\n2023.\nURL\nhttps://www.dreyfus.fr/en/2023/05/22/\ngenerative-ai-balancing-innovation-and-intellectual-property-rights-protection/.\n[240] The Ministry of Economy, Trade and Industry.\nGovernance guidelines for implementa-\ntion of ai principles ver. 1.1, 2022. URL https://www.meti.go.jp/press/2021/01/\n20220125001/20220124003.html.\n[241] Thorn. Generative AI: Now is the Time for Safety by Design, May 2023. URL https:\n//www.thorn.org/blog/now-is-the-time-for-safety-by-design/.\n[242] N. Thylstrup and Z. Talat. Detecting \u2018Dirt\u2019 and \u2018Toxicity\u2019: Rethinking Content Moderation as\nPollution Behaviour. SSRN Electronic Journal, 2020. ISSN 1556-5068. doi: 10.2139/ssrn.\n3709719. URL https://www.ssrn.com/abstract=3709719.\n[243] N.\nTodoric\nand\nA.\nChaudhuri.\nUsing\nAI\nto\nhelp\norganizations\ndetect\nand\nreport\nchild\nsexual\nabuse\nmaterial\nonline,\nSept.\n2018.\nURL\nhttps://blog.google/around-the-globe/google-europe/\nusing-ai-help-organizations-detect-and-report-child-sexual-abuse-material-online/.\n[244] J. A. Tomain. Online Privacy and the First Amendment: An Opt-In Approach to Data\nProcessing, Feb. 2014. URL https://papers.ssrn.com/abstract=2573206.\n[245] J.\nTomlinson.\nCultural\nImperialism.\nIn\nThe\nWiley-Blackwell\nEncyclo-\npedia\nof\nGlobalization.\nJohn\nWiley\n&\nSons,\nLtd,\n2012.\nISBN\n978-0-\n470-67059-0.\ndoi:\n10.1002/9780470670590.wbeog129.\nURL\nhttps://\nonlinelibrary.wiley.com/doi/abs/10.1002/9780470670590.wbeog129.\n_eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470670590.wbeog129.\n[246] B. Toole. On standpoint epistemology and epistemic peerhood: A defense of epistemic\nprivilege. Journal of the American Philosophical Association, forthcoming.\n[247] Treasury\nBoard\nof\nCanada\nSecretariat.\nAlgorithmic\nImpact\nAssessment\nTool,\n2021-03-22.\nURL\nhttps://www.canada.ca/en/government/system/\ndigital-government/digital-government-innovations/responsible-use-ai/\nalgorithmic-impact-assessment.html.\n[248] UNICEF.\nPolicy\nguidance\non\nAI\nfor\nchildren\n2.0.\n2021.\nURL\nhttps://www.unicef.org/globalinsight/media/2356/file/\nUNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf.\n[249] United Nations Human Rights Office of the High Commissioner. Enhancing equality and\ncountering discrimination. URL http://romena.ohchr.org/en/node/188.\n[250] United States Congress. Algorithmic Accountability Act of 2022, 2022-02-04. URL http:\n//www.congress.gov/.\n[251] C. Vaccari and A. Chadwick. Deepfakes and Disinformation: Exploring the Impact of Synthetic\nPolitical Video on Deception, Uncertainty, and Trust in News. Social Media + Society, 6(1):\n205630512090340, Jan. 2020. ISSN 2056-3051, 2056-3051. doi: 10.1177/2056305120903408.\nURL http://journals.sagepub.com/doi/10.1177/2056305120903408.\n39\n[252] M. Veale and R. Binns. Fairer machine learning in the real world: Mitigating discrimination\nwithout collecting sensitive data. 4(2):205395171774353, 2017-12. ISSN 2053-9517, 2053-\n9517. doi: 10.1177/2053951717743530. URL http://journals.sagepub.com/doi/10.\n1177/2053951717743530.\n[253] A. Venigalla and L. Li. Mosaic LLMs (Part 2): GPT-3 quality for < $500k, 2022. URL\nhttps://www.mosaicml.com/blog/gpt-3-quality-for-500k.\n[254] J. Vincent.\nAI art tools Stable Diffusion and Midjourney targeted with copyright\nlawsuit - The Verge.\nURL https://www.theverge.com/2023/1/16/23557098/\ngenerative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart.\n[255] J. Vincent.\nTwitter taught Microsoft\u2019s AI chatbot to be a racist asshole in less than\na day - The Verge, 2016.\nURL https://www.theverge.com/2016/3/24/11297050/\ntay-microsoft-chatbot-racist.\n[256] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A Multi-Task\nBenchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,\npages 353\u2013355. Association for Computational Linguistics, 2018. doi: 10.18653/v1/W18-5446.\nURL http://aclweb.org/anthology/W18-5446.\n[257] A. Wang, V. V. Ramaswamy, and O. Russakovsky. Towards Intersectionality in Machine\nLearning: Including More Identities, Handling Underrepresentation, and Performing Eval-\nuation. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages\n336\u2013349. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533101.\nURL https://dl.acm.org/doi/10.1145/3531146.3533101.\n[258] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane,\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical\nand social risks of harm from Language Models, 2021-12-08. URL http://arxiv.org/\nabs/2112.04359.\n[259] L. Weidinger, K. McKee, R., R. Everett, S. Huang, T. Zhu, M. Chadwick, C. Summerfield, and\nI. Gabriel. Using the Veil of Ignorance to align AI systems with principles of justice, 2023.\nURL https://www.pnas.org/doi/10.1073/pnas.2213709120.\n[260] B. Weiser. Here\u2019s What Happens When Your Lawyer Uses ChatGPT. The New York Times,\nMay 2023. ISSN 0362-4331. URL https://www.nytimes.com/2023/05/27/nyregion/\navianca-airline-lawsuit-chatgpt.html.\n[261] R. Weitzer. Racial discrimination in the criminal justice system: Findings and problems\nin the literature. Journal of Criminal Justice, 24(4):309\u2013322, Jan. 1996. ISSN 0047-2352.\ndoi: 10.1016/0047-2352(96)00015-3. URL https://www.sciencedirect.com/science/\narticle/pii/0047235296000153.\n[262] D. Wen, S. M. Khan, A. Ji Xu, H. Ibrahim, L. Smith, J. Caballero, L. Zepeda, C. de Blas Perez,\nA. K. Denniston, X. Liu, and R. N. Matin. Characteristics of publicly available skin cancer\nimage datasets: a systematic review. The Lancet. Digital Health, 4(1):e64\u2013e74, Jan. 2022.\nISSN 2589-7500. doi: 10.1016/S2589-7500(21)00252-1.\n[263] F. Westin and S. Chiasson. Opt out of privacy or \"go home\": understanding reluctant privacy\nbehaviours through the FoMO-centric design paradigm. In Proceedings of the New Security\nParadigms Workshop, NSPW \u201919, pages 57\u201367, New York, NY, USA, Jan. 2020. Association\nfor Computing Machinery. ISBN 978-1-4503-7647-1. doi: 10.1145/3368860.3368865. URL\nhttps://doi.org/10.1145/3368860.3368865.\n[264] C. C. Williams and A. Efendic. Evaluating the relationship between marginalization and\nparticipation in undeclared work: lessons from Bosnia and Herzegovina. Southeast European\nand Black Sea Studies, 21(3):481\u2013499, July 2021. ISSN 1468-3857. doi: 10.1080/14683857.\n2021.1928419. URL https://doi.org/10.1080/14683857.2021.1928419. Publisher:\nRoutledge _eprint: https://doi.org/10.1080/14683857.2021.1928419.\n40\n[265] L. Winner. Do Artifacts Have Politics? Daedalus, 109(1), 1980. URL http://www.jstor.\norg/stable/20024652.\n[266] A. Wolfers. \"National Security\" as an Ambiguous Symbol. Political Science Quarterly, 67(4):\n481\u2013502, 1952. ISSN 0032-3195. doi: 10.2307/2145138. URL https://www.jstor.org/\nstable/2145138. Publisher: [Academy of Political Science, Wiley].\n[267] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang, F. A.\nBehram, J. Huang, C. Bai, M. Gschwind, A. Gupta, M. Ott, A. Melnikov, S. Candido,\nD. Brooks, G. Chauhan, B. Lee, H.-H. S. Lee, B. Akyildiz, M. Balandat, J. Spisak, R. Jain,\nM. Rabbat, and K. Hazelwood. Sustainable AI: Environmental Implications, Challenges and\nOpportunities. URL http://arxiv.org/abs/2111.00364.\n[268] C. Xiang.\n\u2019He Would Still Be Here\u2019: Man Dies by Suicide After Talking with AI\nChatbot, Widow Says, Mar. 2023. URL https://www.vice.com/en/article/pkadgm/\nman-dies-by-suicide-after-talking-with-ai-chatbot-widow-says.\n[269] A. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. Detoxifying Language\nModels Risks Marginalizing Minority Voices, Apr. 2021. URL http://arxiv.org/abs/\n2104.06390. arXiv:2104.06390 [cs].\n[270] J. Zaller and S. Feldman. A Simple Theory of the Survey Response: Answering Questions ver-\nsus Revealing Preferences. American Journal of Political Science, 36(3):579\u2013616, 1992. ISSN\n0092-5853. doi: 10.2307/2111583. URL https://www.jstor.org/stable/2111583.\nPublisher: [Midwest Political Science Association, Wiley].\n[271] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V.\nLin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang,\nand L. Zettlemoyer. OPT: Open Pre-trained Transformer Language Models, 2022-06-21. URL\nhttp://arxiv.org/abs/2205.01068.\n[272] J. Zhao, T. Wang, M. Yatskar, R. Cotterell, V. Ordonez, and K. Chang. Gender bias in\ncontextualized word embeddings. CoRR, abs/1904.03310, 2019. URL http://arxiv.org/\nabs/1904.03310.\n[273] D. Zhuang, X. Zhang, S. L. Song, and S. Hooker. Randomness In Neural Network Training:\nCharacterizing The Impact of Tooling, 2021-06-22. URL http://arxiv.org/abs/2106.\n11872.\n41\n"
  },
  {
    "title": "Can Large Language Models Infer Causation from Correlation?",
    "link": "https://arxiv.org/pdf/2306.05836.pdf",
    "upvote": "5",
    "text": "Preprint 2023\nCAN LARGE LANGUAGE MODELS INFER\nCAUSATION FROM CORRELATION?\nZhijing Jin1,2,\u2217 Jiarui Liu3\nZhiheng Lyu4\nSpencer Poff5\nMrinmaya Sachan2\nRada Mihalcea3\nMona Diab5,\u2020 Bernhard Sch\u00f6lkopf1,\u2020\n1Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany, 2ETH Z\u00fcrich,\n3University of Michigan, 4University of Hong Kong, 5Meta AI\nABSTRACT\nCausal inference is one of the hallmarks of human intelligence. While the field\nof CausalNLP has attracted much interest in the recent years, existing causal\ninference datasets in NLP primarily rely on discovering causality from empirical\nknowledge (e.g., commonsense knowledge). In this work, we propose the first\nbenchmark dataset to test the pure causal inference skills of large language models\n(LLMs). Specifically, we formulate a novel task CORR2CAUSE, which takes a\nset of correlational statements and determines the causal relationship between\nthe variables. We curate a large-scale dataset of more than 200K samples, on\nwhich we evaluate seventeen existing LLMs. Through our experiments, we identify\na key shortcoming of LLMs in terms of their causal inference skills, and show\nthat these models achieve almost close to random performance on the task. This\nshortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill\nvia finetuning, but we find that these models still fail to generalize \u2013 they can only\nperform causal inference in in-distribution settings when variable names and textual\nexpressions used in the queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. CORR2CAUSE\nis a challenging task for LLMs, and would be helpful in guiding future research on\nimproving LLMs\u2019 pure reasoning skills and generalizability.1\n1\nINTRODUCTION\nCausal inference, i.e., the ability to establish the correct causal relationships between variables or\nevents, is fundamental to human intelligence. There are two distinct ways this causal inference\ncapability can be acquired: one through empirical knowledge, e.g., we know from common sense that\ntouching a hot stove will get us burned; the other through pure causal reasoning, as causality can be\nformally argued and reasoned about using known procedures and rules from causal inference (Spirtes\net al., 2000; Pearl, 2009; Peters et al., 2017). One example is that we have the a priori knowledge that\nthe correlation between A and B does not necessarily imply causality. This is a formal rule that holds\ntrue regardless of the realizations of the variables A and B.\nWith the rise of large language models (LLMs) (Radford et al., 2019; Devlin et al., 2019; Ouyang\net al., 2022; Zhang et al., 2022; OpenAI, 2023, inter alia), a crucial research question is whether\nthey can do causal reasoning well. Recent studies have pointed out that LLMs are \u201ccausal parrots,\u201d\nwhich recite the causal knowledge in the training data (Ze\u02c7cevi\u00b4c et al., 2023). Moreover, the vast\nmajority of studies frame causal reasoning as a skill to navigate around empirical knowledge (Gordon\net al., 2012; Sap et al., 2019a;b; Qin et al., 2019; Bhagavatula et al., 2020), and also treat LLMs as a\nknowledge base when evaluating its causal skills (K\u0131c\u0131man et al., 2023; Tu et al., 2023; Xie et al.,\n2023). However, all the above lines of research frame causality as empirical knowledge, thus relying\nheavily on the quality and the coverage of the training data, overlooking the great potential of the\nformal causal reasoning skills to process correlational information to causal conclusions.\n\u2217This work originated during Zhijing\u2019s internship at Meta AI. Email: jinzhi@ethz.ch.\n\u2020Equal supervision.\n1Our data is at https://huggingface.co/datasets/causalnlp/corr2cause.\nOur code is at https://github.com/causalNLP/corr2cause.\n1\narXiv:2306.05836v2  [cs.CL]  31 Dec 2023\nPreprint 2023\nPrevious tasks:\nGetting burned\nTouching a hot stove\nCauses\nNo\nSuppose we know that A correlates with B.\nCan we infer that A causes B?\nSkill being tested in previous work: Empirical\nknowledge instead of pure causal inference.\nTraining Corpus\nIce cream sales\nHow can LLMs process such\ninformation? Correlation?\nCausation? What causes what?\nWe propose a new task: Corr2Cause Inference\nThis requires the skill of inferring causation from correlation\n*Assumption that we explicitly mention in the samples: We suppose a close system of the given variables and correlations.\nYes\nA correlates with B. C correlates with B. However, A is independent of C.\nCan we infer that A causes B?\n?\n?\nDrowning cases\nHot weather\nVaccination\nAutism\nTech company CEOs\nCollege dropout\nFatality rate\nMany\ncorrelations\nFigure 1: Illustration of the motivation behind our task and dataset.\nDrawing inspirations from technical studies on causal discovery (Spirtes et al., 2000; Spirtes & Zhang,\n2016; Glymour et al., 2019), we formulate a novel task for NLP, correlation-to-causation inference\n(CORR2CAUSE), which is an important skill for LLMs. Imagine the scenario in Figure 1, where\nthe training corpus does not tediously cover every causal relation, but more pervasively talk about\ncorrelations, such as which events tend to co-occur. Learning a good CORR2CAUSE skill can enable\nLLMs to draw causal relations behind the mere correlational information on the surface. For example,\nseveral decades ago, there might be an observation that female university students tend to perform\nbetter, but behind the correlational statistics is the causal graph that female students have to achieve\nextra good performance to get into universities as the first place.\nTo this end, we collect the CORR2CAUSE dataset, the first dataset to test the pure causal reasoning\nabilities of LLMs. All the questions in this dataset are centered around testing when it is valid\nor invalid to infer causation from correlation. To systematically compose this dataset, we ground\nour generalization process in the formal framework of causal discovery (Spirtes et al., 1993; 2000;\nGlymour et al., 2016; Spirtes & Zhang, 2016), which provides rules about how to deduce causal\nrelations among variables given their statistical correlation in the observational data. We generate\nmore than 200K data points, and label a correlation-causation statement pair as valid if and only if\nthere is a bijective mapping between the statistical correlation and the underlying causality.\nBased on our CORR2CAUSE dataset with 200K samples, we investigate two main research questions:\n(1) How well do existing LLMs perform on this task? (2) Can existing LLMs be re-trained or\nre-purposed on this task and obtain robust causal inference skills? Through extensive experiments, we\nshow empirically that none of the seventeen existing LLMs we investigate perform well on this pure\ncausal inference task. We also show that although LLMs can demonstrate better performance after\nbeing finetuned on the data, the causal inference skills attained by them are not robust. In summary,\nour contributions are as follows:\n1. We propose the novel task of CORR2CAUSE, to probe an aspect of LLM\u2019s reasoning ability,\npure causal inference;\n2. We compose a dataset of over 200K samples, using insights from causal discovery;\n3. We evaluate the performance of seventeen LLMs on our dataset, finding that all of them\nperform poorly, close to the random baseline;\n4. We further explored whether LLMs can learn the skill through finetuning, and find that\nLLMs fail to robustly acquire this skill in out-of-distribution settings. Finally, we suggest\nfuture work to explore more ways to enhance the pure causal inference skill in LLMs.\n2\nPRELIMINARIES: CAUSAL INFERENCE\n2.1\nDIRECTED GRAPHICAL CAUSAL MODELS (DGCMS)\nA directed graphical causal model (DGCM) is a commonly used representation to express the causal\nrelations among a set of variables. Given a set of N variables X = {X1, . . . , XN}, we can encode\nthe causal relations among them using a directed graph G := (X, E), where E is the set of directed\nedges. Each edge ei,j \u2208 E represents a causal link Xi \u2192 Xj, meaning that Xi is a direct cause of\nXj. In the context of this work, we take the common assumption of directed acyclic graphs (DAGs),\n2\nPreprint 2023\nwhich most causal discovery methods use (Glymour et al., 2019), as graphs with cycles can make the\ncausal discovery process arbitrarily hard.\nFollowing the graph-theoretic terminology, we use an analogy of the ancestry tree to denote the\nrelations between two variables. For example, we call Xi as a parent of Xj if there is a directed edge\nXi \u2192 Xj in the graph, and, thus, Xj is a child of Xi. Similarly, we denote Xi as an ancestor of Xj\nif there exists a directed path from Xi to Xj, and, thus, Xj is a descendent of Xi. Note that a parent\nis a special case of an ancestor where the directed path has a length of 1.\nFor convenience, we also introduce the notions for some special three-variable relations. Given two\nvariables Xi and Xj, we call a third variable Xk a confounder (i.e., common cause) if Xk is a parent\nof both Xi and Xj; a collider (i.e., common effect) if Xk is a child of both Xi and Xj; and a mediator\nif Xk is both a child of Xi, and a parent of Xj.\n2.2\nD-SEPARATION AND MARKOV PROPERTY\nD-Separation\nD-separation (Pearl, 1988) is a fundamental concept in graphical models used to\ndetermine whether two sets of nodes X and Y in a DAG G are conditionally independent given a\nthird set of nodes Z, where the three sets are disjoint. We say that X and Y are d-separated by Z if\nall paths between any node in X and any node in Y are blocked by the conditioning set Z. A path\nbetween X and Y is blocked by Z if there exists a node A \u2208 Z which satisfies one of the following\nconditions: A is the parent node in a fork structure on the path (i.e., \u00b7 \u2190 A \u2192 \u00b7); A is the mediator\nnode in a chain structure on the path (i.e., \u00b7 \u2192 A \u2192 \u00b7); or in any collider structure on the path (i.e.,\n\u00b7 \u2192 A \u2190 \u00b7), Z does not contain A or its descendants.\nMarkov Property\nThe Markov property in a DAG G states that each node Xi is conditionally\nindependent of its non-descendants given its parents, namely Xi \u22a5\u22a5 NonDe(Xi)| Pa(Xi), where\nNonDe(Xi) denotes the non-descendants of Xi excluding itself, and Pa(Xi) denotes the parents\nof Xi. Using the Markov property, we can factorize the joint distribution of all the nodes in the\ngraph into P(X1, . . . , XN) = QN\ni=1 P(Xi|PA(Xi)). To infer the causal graph from probability\ndistributions, a common assumption is faithfulness, namely the validity to infer all the d-separation\nsets in the graph from the independence relations in the probability distribution. In our work, we also\ntake this broadly taken assumption which holds for most real-world scenarios.\nMarkov Equivalence of Graphs\nWe denote two DAGs as Markov equivalent if they induce the\nsame joint distribution P(X). The set of DAGs that are Markov equivalent to each other is called\na Markov equivalence class (MEC). Causal graphs in the same MEC can be easily identified since\nthey have the same skeleton (i.e., undirected edges) and V-structures (i.e., structures in the form of\nA \u2192 B \u2190 C where A and C are not connected).\nObviously, there is a one-to-many mapping (i.e., surjection) between the causal graph and statistical\ndistribution. Namely, each causal graph sufficiently determines a statistical distribution, but from\na statistical distribution, we cannot necessarily induce a unique causal graph. This is why we say\n\u201ccorrelation does not necessarily mean causation\u201d.\n2.3\nCAUSAL DISCOVERY\nCausal discovery aims to learn the causal relations by analyzing statistical properties in the obser-\nvational data (Spirtes et al., 1993; 2000; Glymour et al., 2016; Spirtes & Zhang, 2016; Glymour\net al., 2019). It can be achieved through constraint-based methods (Spirtes et al., 2000), score-based\nmethods (Chickering, 2002), or other methods taking advantage of the functional causal models\n(Shimizu et al., 2006; Hoyer et al., 2008; Zhang & Hyv\u00e4rinen, 2009).\nTo fit for the spirit of this paper to infer from correlation (expressed in natural language) to causation,\nwe base our dataset design on the widely-used Peter-Clark (PC) algorithm (Spirtes et al., 2000).\nThe PC algorithm is based on the principles of conditional independence and the causal Markov\nassumption, which allows it to efficiently identify causal relationships among variables in a given\ndataset. The algorithm first starts with a fully connected undirected graph among all the variables.\nThen it removes the edge between two variables if there is an unconditional or conditional inde-\npendence relationship between them. Afterwards, it orients the directed edges whenever there is a\nV-structure. And finally, it iteratively checks the direction of the other edges until the entire causal\ngraph is consistent with all the statistical correlations.\n3\nPreprint 2023\nA\nB\nC\n1. Choose the\nnumber of variables\n2. Generate all unique causal\ngraphs\nB\nA\nC\nE.g., N=3\nA\nB\nC\n...\n3. Map each graph to a set of\nstatistical correlations\nA\nC\nB\n\u25cf A \u2aeb\u0338 B,\n\u25cf A \u2aeb\u0338 C, \n\u25cf B \u2aeb\u0338 C, and\n\u25cf A \u2aeb C | B\n\u25cf A \u2aeb\u0338 B,\n\u25cf B \u2aeb\u0338 C, and\n\u25cf A \u2aeb C \nCausal Graphs\nCorrelations\nA\nB\nC\n1-to-1 Mapping\nMany-to-1\nMapping\nCorrelations\nHypothesized\nCausation\nValidity\nSuppose there is a closed system of 3 variables, A, B and C. All the statistical\nrelations among these 3 variables are as follows:\nA correlates with C. B correlates with C. However, A is independent of B.\nA directly causes B.\n[The validity label is equivalent to the results after running the PC algorithm.\nI.e., if the hypothesis fits all causal graphs corresponding to the set of\ncorrelations, then the label is entailment, otherwise non-entailment.]\nValid\nVerbalize the statistical correlations\nHypothesize a causal relation between two nodes\n4. Compose the Data\nFigure 2: Pipeline of the data construction process.\n3\nDATASET CONSTRUCTION\nWe introduce the construction of our dataset in this section. We start with our task formulation for\nCORR2CAUSE, and then briefly give an overview of the data generation process, followed by detailed\ndescriptions of each step. We conclude the section with the overall statistics of the dataset.\n3.1\nTASK FORMULATION\nGiven a set of N variables X = {X1, . . . , XN}, we have a statement s about all the correlations\namong the variables, and a hypothesis h describing the causal relation r between the pair of variables\nXi and Xj. The task is to learn a function f : (s, h) 7\u2192 v which maps the correlation statement\ns and the causal relation hypothesis h to their validity v \u2208 {0, 1}, which takes the value 0 if this\ninference is invalid, and the value 1 if this inference is valid.\n3.2\nOVERVIEW OF THE DATA GENERATION PROCESS\nWe base the construction our dataset on several concepts of causal inference, including the DGCM,\nd-separation, and MECs, as introduced in Section 2.\nAs in the overview of our data generation process in Figure 2, we first choose the number N of\nvariables (Step 1) and generate all the unique DGCMs with N nodes (Step 2), which we will introduce\nin the Section 3.3. Then we collect all the d-separation sets from these graphs to identify MECs\n(Step 3) in Section 3.4. Then, in Step 4, we create the formal form of data in Section 3.5. For each\ncorrespondence of the MEC to causal graphs, we compose the correlation statement based on the\nstatistical relations in the MEC, and hypothesize a causal relation between two variables, and produce\nthe validity v = 1 if the hypothesis is a shared property of all causal graphs in the MEC, and v = 0 if\nthe hypothesis is not necessarily true for all the MEC graphs. Finally, we introduce the verbalization\nprocess in Section 3.6.\n3.3\nCONSTRUCTING THE GRAPHS WITH ISOMORPHISM CHECKS\nThe first step of the data generation is to compose the causal graphs, as in Step 1 and 2 of Figure 2.\nFor a set of N variables X = {X1, . . . , XN}, there are N(N \u2212 1) possible directed edges, since\neach node can link to any node other than itself. To remove cycles in the graph, we make the nodes\nin topological order, which only allows edges Xi \u2192 Xj, where i < j. We achieve this by limiting\nthe adjacency matrix of the graph to only having non-zero values above the diagonal, resulting in\nN(N \u2212 1)/2 possible directed edges for the DAGs.\n4\nPreprint 2023\nAt the first glance, for N nodes, there should be 2N(N\u22121)/2 possible DAGs (i.e., the power set of all\nedges). However, there could be isomorphic graphs in this set. To avoid this, we perform a graph\nisomorphism check (McKay & Piperno, 2014), and reduce the set so that only unique DAGs are\nretained, and we show their statistics in Table 1. Although we can handle large graphs, we mostly\nfocus on smaller graphs that can still lead to a reasonably sized dataset, so we empirically set N = 6,\nbut future work can use our open-sourced codes to extend to more nodes.\n# Nodes\n# Unique DAGs\n# Edges/DAG\n# MECs\n# DAGs/MEC\n2\n2 out of 2\n0.50\n2\n1.0\n3\n6 out of 23\n1.67\n5\n1.2\n4\n31 out of 26\n3.48\n20\n1.55\n5\n302 out of 210\n5.89\n142\n2.13\n6\n5,984 out of 215\n8.77\n2,207\n2.71\nTotal\n6,325\n8.60\n2,376\n2.66\nTable 1: Statistics about the source causal graphs in our dataset. Given the number of nodes, we report\nthe number of unique DAGs, average number of edges per DAG, number of MECs, and average\nnumber of DAGs per MEC.\n3.4\nPROGRAMMATICALLY GENERATING THE D-SEPARATION SETS\nBased on the set of unique DAGs, we then programmatically generate the d-separation sets by\ngraph theoretical conditions, as in Step 3 of Figure 2. To realize this step, we code an efficient\ngraph-theoretic algorithm to check for all the chain, fork, and collider structures to automatically\nidentify the set of nodes that d-separate each pair of nodes. Using the d-separation sets and the\nfaithfulness assumption, we form the statistical correlations as follows. For each pair of nodes, they\nare conditionally independent given the variables in the d-separation set. If the d-separation set is\nempty, then the two nodes are unconditionally independent. If no d-separation set can be found for\nthe two nodes, then they are directly correlated.\nMoreover, using the d-separation sets, we are able to cluster causal graphs to MECs. We achieve it by\ntracing the mapping between the causal graphs and the set of statistical correlations, and backtracking\nthe graphs with the same d-separation sets to group them in the same MEC. We show in Table 1 that\neach MEC contains on average 2.66 DAGs.\n3.5\nCOMPOSING THE HYPOTHESES AND LABEL\nAfter generating the set of correlations based on the d-separation sets, we now generate the causal\nhypotheses. For the causal relation r, we focus on six common causal relations between two nodes\nintroduced in Section 2.1: Is-Parent, Is-Child, Is-Ancestor (excluding the parents), Is-Descendant\n(excluding the children), Has-Confounder (i.e., there exists a confounder, or common cause, of the\ntwo nodes), and Has-Collider (i.e., there exists a collider, or common effect, of the two nodes). In this\nway, the set of hypotheses contains all six meaningful causal relations between every pair of variables,\nresulting in a total size of 6 \u00b7 N(N \u2212 1)/2 = 3N(N \u2212 1) hypotheses for a graph with N variables.\nTo generate the ground-truth validity label, we start from the correlation sets in Step 3, then look up\nall the causal graphs in the same MEC corresponding to the given set of correlations, and check the\nnecessity of the hypothesized causal relation. If the causal relationship proposed in the hypothesis\nis valid for all causal graphs within the MEC, then we generate the validity v = 1; otherwise, we\ngenerate v = 0. A special case of valid samples is that when the size of the MEC is 1, then there is a\nbijective mapping between the causal graph and the d-separation sets, so any hypothesis stating the\ncausal properties of that unique causal graph is valid.\n3.6\nVERBALIZING INTO LANGUAGE\nFinally, as in the last step of Figure 2, we convert all the information above to text data for our\nCORR2CAUSE task. For the correlation statement, we verbalize the set of correlations in Step 3 into\na natural language statement s. When two variables cannot be d-separated, i.e., A \u0338\u22a5\u22a5 B, then we\ndescribe them as \u201cA correlates with B\u201d since they are directly correlated and cannot be independent\nby any condition. And if two variables have a valid d-separation set C, then we describe them as \u201cA\nis independent of B given C.\u201d In the special case when the d-separation set is empty, we directly\nsay \u201cA is independent of B.\u201d In addition, we disambiguate the setting by starting the correlation\nstatement with the setup of a closed system of the given variables, and no hidden variables: \u201cSuppose\n5\nPreprint 2023\nthere is a closed system of N variables, A, B, . . . All the statistical relations among these N variables\nare as follows:\u201d. Finally, to verbalize the hypothesis, we feed the causal relation triplet (Xi, r, Xj)\ninto their hypothesis templates in Table 2. For example, we turn the triplet (A, Is-Parent, B) into \u201cA\ndirectly causes B\u201d, as in the example of Figure 2.\nCausal Relation\nHypothesis Template\nIs-Parent\n{Var i} directly causes {Var j}.\nIs-Ancestor\n{Var i} causes something else which causes {Var j}.\nIs-Child\n{Var j} directly causes {Var i}.\nIs-Descendant\n{Var j} is a cause for {Var i}, but not a direct one.\nHas-Collider\nThere exists at least one collider (i.e., common effect) of {Var i} and {Var j}.\nHas-Confounder\nThere exists at least one confounder (i.e., common cause) of {Var i} and {Var j}.\nTable 2: Templates for each causal relation in the hypothesis. We use {Var i} and {Var j} as\nplaceholders for the two variables.\n3.7\nSTATISTICS OF THE RESULTING DATA\nWe show the statistics of our CORR2CAUSE dataset in Table 3. Overall, our dataset contains 207,972\nsamples, where 18.57% of the samples have the positive label (i.e., with validity=1). The average\nlength of the premise is 424.11 tokens, and hypothesis 10.83 tokens. We split the data into 205,734\ntraining samples, 1,076 development and 1,162 test samples. Since the main purpose of the dataset\nis to benchmark the performance of LLMs, we prioritize the test and development sets to have a\ncomprehensive coverage over all sizes of graphs. Specifically, we iterate through the subset of our\ndata for each N, and split it entirely for only the test and development sets if the data is less than 1K,\nwhich is the case for N = 2 and 3. For the other subsets that are larger, we randomly sample up to\n1K or 10% of the data, whichever is smaller, to the test and development sets. We set the cap to be 1K\nin order to form a reasonable computation budget, since many LLMs are expensive to query in the\ninference mode. Aside from the test and valid sets, all the rest of the data goes into the training set.\nOverall\nStatistics by the Number of Nodes N\nN = 2\nN = 3\nN = 4\nN = 5\nN = 6\n# Samples\n207,972\n12\n90\n720\n8,520\n198,630\n# Test\n1,162\n6\n48\n72\n514\n522\n# Dev\n1,076\n6\n42\n72\n482\n474\n# Train\n205,734\n0\n0\n576\n7,524\n197,634\n# Tokens/Premise\n424.11\n31.5\n52.0\n104.0\n212.61\n434.54\n# Tokens/Hypothesis\n10.83\n10.83\n10.83\n10.83\n10.83\n10.83\n% Positive Labels\n18.57\n0.00\n3.33\n7.50\n13.01\n18.85\nVocab Size\n65\n49\n53\n55\n57\n61\nTable 3: Statistics of our CORR2CAUSE dataset, and by subsets. We report the total number of\nsamples (# Samples); splits of the test (# Test), developement (# Dev) and training sets (# Train);\nnumber of tokens per premise (# Tokens/Premise) and hypothesis (# Tokens/Hypothesis); percentage\nof the positive labels (% Positive Labels), and vocabulary size by the number of unique tokens (Vocab\nSize). Note that the number of unique graphs and MECs are in Table 1.\nNote for our dataset v2.0: We notice that our original dataset (v1.0) has duplication due to symmetric\nrelations and verbalizations of the hypothesis. E.g., Is-Parent(A, B) has the exact hypothesis ver-\nbalization as Is-Child(B, A). Hence, for our dataset v2.0, we perform a careful de-duplication, and\nupdate the data statistics in Table 3. See more version comparison details in Appendix D. Note that,\ndue to the symmetry, the current version is a random sample half of the size of the original version,\nso the modeling results in the experiment section roughly hold.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nWe set up a diverse list of LLMs for the experiments on our CORR2CAUSE dataset. To test existing\nLLMs, we first include six commonly used BERT-based NLI models in the transformers library\n(Wolf et al., 2020) with the most number of downloads: BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), BART (Lewis et al., 2020), DeBERTa (He et al., 2021), DistilBERT (Sanh et al., 2019),\n6\nPreprint 2023\nF1\nPrecision\nRecall\nAccuracy\nRandom Baselines\nAlways Majority\n0.0\n0.0\n0.0\n84.77\nRandom (Proportional)\n13.5\n12.53\n14.62\n71.46\nRandom (Uniform)\n20.38\n15.11\n31.29\n62.78\nBERT-Based Models\nBERT MNLI\n2.82\n7.23\n1.75\n81.61\nRoBERTa MNLI\n22.79\n34.73\n16.96\n82.50\nDeBERTa MNLI\n14.52\n14.71\n14.33\n74.31\nDistilBERT MNLI\n20.70\n24.12\n18.13\n78.85\nDistilBART MNLI\n26.74\n15.92\n83.63\n30.23\nBART MNLI\n33.38\n31.59\n35.38\n78.50\nLLaMa-Based Models\nLLaMa-7B\n26.81\n15.50\n99.42\n17.36\nAlpaca-7B\n27.37\n15.93\n97.37\n21.33\nGPT-Based Models\nGPT-3 Ada\n0.00\n0.00\n0.00\n84.77\nGPT-3 Babbage\n27.45\n15.96\n97.95\n21.15\nGPT-3 Curie\n26.43\n15.23\n100.00\n15.23\nGPT-3 Davinci\n27.82\n16.57\n86.55\n31.61\nGPT-3 Instruct (text-davinci-001)\n17.99\n11.84\n37.43\n48.04\nGPT-3 Instruct (text-davinci-002)\n21.87\n13.46\n58.19\n36.69\nGPT-3 Instruct (text-davinci-003)\n15.72\n13.4\n19.01\n68.97\nGPT-3.5\n21.69\n17.79\n27.78\n69.46\nGPT-4\n29.08\n20.92\n47.66\n64.60\nTable 4: Overall performance. We report F1 (main metric), precision, recall and accuracy. For the\nmain metric, F1 score, we use the bold font to highlight the overall best performance, and underline\nto highlight the best performance within each category of models.\nand DistilBART (Shleifer & Rush, 2020). Apart from these BERT-based NLI models, we also\nevaluate the general-purpose autoregressive LLMs based on GPT (Radford et al., 2019): GPT-3 Ada,\nBabbage, Curie, Davinci (Brown et al., 2020); its instruction-tuned versions (Ouyang et al., 2022),\ntext-davinci-001, text-davinci-002, and text-davinci-003; and GPT-3.5 (i.e., ChatGPT), and the latest\nGPT-4 (OpenAI, 2023) by April 2023, using the OpenAI API2 with temperature 0. We also evaluate\nthe recent, more efficient models, LLaMa (Touvron et al., 2023) and Alpaca (Taori et al., 2023).\nWhen inspecting the behavior of finetuned models, we adopt a large set of models, including GPT-\nbased models (GPT-3 Ada, Babbage, Curie, and Davinci) using the OpenAI finetuning API for\nclassification,3 open-sourced decoder-only models (GPT2, GPT2-Large, GPT2-XL, LLaMA-7B, and\nLLaMa2-7B), BERT-based models from scratch (BERT-Base, BERT-Large, RoBERTa-Base, and\nRoBERTa-Large), and BERT-Based NLI models (BERT-Base MNLI, BERT-Large MNLI, RoBERTa-\nBase MNLI, and RoBERTa-Large MNLI) using the transformers library (Wolf et al., 2020). Our\ntraining details are available in Appendix A.\nFor the random baselines, we provide \u201calways majority\u201d to predict the majority class 100% of the\ntime, \u201crandom (uniform)\u201d which randomly samples a label with 50% chance for each, and \u201crandom\n(proportional)\u201d which samples a label from a Bernouli distribution proportional to the development\nset label distribution.\n4.2\nTHE CORR2CAUSE SKILL IN EXISTING LLMS\nWe show the performance of 17 LLMs in Table 4. We can see that pure causal inference is a very\nchallenging task across all existing LLMs. Among all the LLMs, the best performance is 33.38% F1\nby BART MNLI, which is even higher than latest GPT-based model, GPT-4. Notably, many models\nare worse than random guess, which means that they totally fail at this pure causal inference task.\n4.3\nFINETUNED PERFORMANCE\nNext, we address the question: Can we re-purpose LLMs to learn this task?\n2https://openai.com/api/\n3https://platform.openai.com/docs/guides/fine-tuning\n7\nPreprint 2023\nThe experimental results in Table 5a of 17 models finetuned on our CORR2CAUSE seem very strong\nat first sight. Most models see a substantial increase, among which the finetuned BERT-based NLI\nmodels demonstrate the strongest performance. The best-performing one, RoBERTa-Large MNLI,\nachieves 94.74% F1 score on this task, as well as very high precision, recall and accuracy scores.\nF1\nPrecison\nRecall\nAccuracy\nFinetuned GPT-Based Models Using OpenAI API\nGPT-3 Ada\n79.85\n70.47\n92.11\n92.92\nGPT-3 Babbage\n78.19\n69.98\n88.60\n92.48\nGPT-3 Curie\n81.23\n75.00\n88.60\n93.77\nGPT-3 Davinci\n85.52\n80.26\n91.52\n95.28\nFinetuned Open-Sourced Decoder-Only Models\nGPT2\n89.18\n88.03\n90.35\n96.66\nGPT2-Large\n94.29\n92.18\n96.49\n98.22\nGPT2-XL\n94.30\n91.94\n96.78\n98.22\nLLaMA-7B\n91.98\n88.62\n95.61\n97.46\nLLaMa2-7B\n92.92\n90.11\n95.91\n97.77\nFinetuned BERT-Based Models\nBERT-Base\n69.29\n54.42\n95.32\n87.13\nBERT-Large\n85.26\n77.51\n94.74\n95.01\nRoBERTa-Base\n87.60\n78.47\n99.12\n95.73\nRoBERTa-Large\n89.10\n82.54\n96.78\n96.39\nFinetuned BERT-Based NLI Models\nBERT-Base MNLI\n89.88\n85.49\n94.74\n86.51\nBERT-Large MNLI\n90.19\n84.44\n96.78\n96.79\nRoBERTa-Base MNLI\n94.27\n90.35\n98.54\n98.17\nRoBERTa-Large MNLI\n94.74\n92.24\n97.37\n98.35\n(a) Performance of finetuned models on the original test set.\nF1 (Paraph.)\nF1 (Var. Ref.)\n61.73\n41.57\n62.34\n43.28\n64.93\n45.32\n65.01\n46.96\n56.76\n31.70\n55.95\n31.99\n60.32\n43.95\n56.41\n53.92\n52.24\n49.47\n61.13\n35.20\n63.64\n38.54\n65.58\n53.12\n65.05\n60.20\n65.56\n31.50\n67.24\n52.04\n57.42\n62.83\n55.45\n67.87\n(b) F1 scores of finetuned models\non the perturbed test sets by para-\nphrasing (Paraph.) and variable\nrefactorization (Var. Ref.).\nTable 5: Performance of finetuned models on the original test set and perturbed test sets.\nRelation Type\nF1\nPrecision\nRecall\nAccuracy\nIs-Parent\n96.18\n95.45\n96.92\n98.67\nIs-Ancestor\n93.94\n93.94\n93.94\n98.93\nIs-Child\n95.73\n94.92\n96.56\n98.67\nIs-Descendant\n96.55\n93.33\n100\n99.47\nHas-Collider\n92.19\n87.41\n97.52\n94.64\nHas-Confounder\n98.67\n97.37\n100\n99.73\n(a) Fine-grained performance of RoBERTa-Large by causal relation\ntype on the original test set.\nF1\nPrecision\nRecall\nAccuracy\n74.80\n79.31\n70.77\n91.73\n45.45\n90.91\n30.30\n93.60\n73.39\n78.43\n68.97\n92.27\n29.41\n83.33\n17.86\n93.60\n70.70\n75.00\n66.90\n82.04\n70.42\n73.53\n67.57\n94.37\n(b) Its fine-grained performance by relation\ntype after variable refactorization.\nTable 6: Fine-grained analysis of the best-performing model, RoBERTa-Large MNLI.\n4.4\nFINE-GRAINED PERFORMANCE BY CAUSAL RELATION\nIn addition to the overall results mentioned above, we also conduct a fine-grained analyze to check\nthe performance of the strongest model, RoBERTa-Large MNLI, by our six causal relation types.\nAs in Table 6a, the model is very good at judging relations such as Is-Parent, Is-Descendant and\nHas-Confounder, all with more than 96% F1 scores, whereas it is several points weaker on the Has-\nCollider relations. This could be due to that the collider relation is the most special type, requiring\nidentification of the V-structure based on both the unconditional independence based on the two\nvariables only and correlations whenever conditioned on a common descendant.\n4.5\nROBUSTNESS ANALYSIS\nLooking at the very high performance of the finetuned models, we raise the next question: Did the\nmodels really robustly learn the causal inference skills?\nTwo Robustness Tests\nWe design two simple robustness tests: (1) paraphrasing, and (2) variable\nrefactorization. For (1) paraphrasing, we simply paraphrase the hypothesis by changing the text\n8\nPreprint 2023\ntemplate for each causal relation to some semantically-equivalent alternatives in Appendix C. For (2)\nvariable refactorization, we reverse the alphabet of the variable names, namely flipping A, B, C, to Z,\nY, X and so on. The inspiration behind the two robustness tests comes from the spurious correlation\nanalysis described in Appendix E.\nSpecifically, we adopt the common setup of text adversarial attack (Morris et al., 2020; Jin et al., 2020)\nto preserve the training set and keep the same saved models, but run the inference n the perturbed test\nset. In this way, we separate the possibilities of the models only overfitting on the training data vs.\nmastering the reasoning skills.\nResults after Perturbation\nWe can see from Table 5b that all the models drop drastically, by up to\n39.29 when we paraphrase the test set, and they decrease substantially by up to 62.3 when we refactor\nthe variable names. The best-performing model, RoBERTa-Large MNLI, is especially sensitive\ntowards paraphrasing, demonstrating the most drop among all models; however, it is the most robust\nagainst the variable refactorization, maintaining a high F1 score of 67.87. We conduct fine-grained\nanalysis for RoBERTa-Large MNLI under perturbation in Table 6b. We can see the the main source\nof the performance drop of the model comes from the two classes, Is-Ancestor (decreasing to 45.45%)\nand Is-Descendant (decreasing to 29.41%), while the other classes stay relatively robust, keeping\ntheir F1 scores over 70%.\nFrom this analysis, we make the following suggestions to future studies testing this CORR2CAUSE\nskill of LLMs. First, it is safe to use it as a test set to benchmark existing LLMs\u2019 performance, since\nthe data we generate is out-of-distribution from the training data of the current LLMs. Then, when\ntesting finetuned models, it is very important to accompany adversarial attack together with the i.i.d.\ntest set. We also provide our perturbed versions of the test set in our data for future work to test the\ngeneralizability skill.\n4.6\nEXTENSION TO NATURAL STORIES\nWe envision our CORR2CAUSE dataset to be a foundation for future extensions to various settings,\nsuch as instantiating the variables with actual phenomena and situating the story in a more natural\nsetting. For example, the correlation does not imply causation rule can be instantiated with the ice\ncream sales and swimming pool attendance as the two variables, and argue that ice cream sales does\nnot necessarily affect swimming pool attendance, because their correlation could be due to a third\nvariable, such as hot weather. We provide a case study for how to instantiate the symbolic expressions\nin our dataset to more natural stories, and find that LLMs such as GPT-4 can generate realistic, daily\nlife stories that has foreseeably broad applications. See more details in Appendix B.\n5\nRELATED WORK\nExisting Causal Reasoning Tasks\nA large body of existing research of causal reasoning in NLP\nfocuses on leveraging empirical knowledge to do tasks such as inferring the cause and effect of why\nan agent perform certain tasks (Sap et al., 2019a), the motivation and emotional reaction in a social\ncontext (Sap et al., 2019b), how people achieve a given goal with a set of concrete steps (Zhang et al.,\n2020), the development of a story given a different beginning (Qin et al., 2019), and how in general\nLLMs serve as a knowledge base of cause and effect (Willig et al., 2023; K\u0131c\u0131man et al., 2023). In\ncontrast, our CORR2CAUSE task focuses on the pure causal inference skill of models, which is a\nknowledge-dependent reasoning skill based on formally correct rules from causal inference.\nExisting Logical and Inference Tasks\nAnother related area of literature is logical and inference\ntasks. A well-established task is natural language inference (NLI), which identifies the semantic\nrelationship between a pair of sentences (MacCartney & Manning, 2008; Bowman et al., 2015).\nNLI datasets mainly focus on the set and paraphrase relations, such as \u201ca group of boys are playing\nfootball\u201d can entail \u201csome guys are playing football,\u201d where \u201cboys\u201d are a sub-concept of \u201cguys\u201d and\n\u201ca group of\u201d and \u201csome\u201d are paraphrases. Existing datasets cover entailment in news articles (Dagan\net al., 2006), image captions (Bowman et al., 2015), and across multiple genres (Williams et al.,\n2018). Recently, there has been increasing efforts to extend the inference task to various logical\ninference skills such as deductive logic and propaganda techniques (Jin et al., 2022; Alhindi et al.,\n2022). Our CORR2CAUSE dataset is the first dataset testing the correlation-to-causation inference\nskill, which is unique of its type.\n9\nPreprint 2023\n6\nLIMITATIONS AND FUTURE WORK\nWe identify several limitations of this work and open future directions: First, in the context of this\nwork, we limit the causal graphs to two to six nodes, but future work can feel free to explore larger\ngraphs. Another aspect is that we do not assume hidden confounders in this inference problem, so we\nwelcome future work to generate an even more challenging dataset to infer the existence of hidden\nconfounders, analogous to the causal discovery algorithm of fast causal inference (FCI) (Spirtes et al.,\n2000). And also in general, explorations of other causal discovery algorithms are welcomed too.\nFinally, a lot of motivation behind proposing this task is inspired by the problem of invalid reasoning\npatterns in our daily reasoning (Jin et al., 2022), which could fertilize the ground for more pervasive\nspread of fake news. We believe false causal inference is a prevalent type of fallacious beliefs, and\nwelcome future work to connect the idea of this benchmark to more real-world false beliefs based on\nconfusing correlation with causation.\n7\nCONCLUSION\nIn this work, we introduced a novel task, CORR2CAUSE, to infer causation from correlation, and\ncollected a large-scale dataset of over 200K samples. We evaluated an extensive list of LLMs on this\nnew task, and showed that off-the-shelf LLMs perform poorly on this task. We also show that it is\npossible to re-purpose LLMs on this task by finetuning, but future work needs to be aware of the\nout-of-distribution generalization problem. To avoid the Goodhart\u2019s law, we recommend using this\ndataset to benchmark the pure causal inference skills for LLMs that have not seen this dataset. Given\nthe limited reasoning abilities of current LLMs, and the difficulty of separating actual reasoning from\ntraining-corpus-derived knowledge, it is imperative that our community focus on work aiming to\naccurately disentangle and measure both abilities. We believe that the present work is a first such\nstep.\nREFERENCES\nTariq Alhindi, Tuhin Chakrabarty, Elena Musi, and Smaranda Muresan. Multitask instruction-based\nprompting for fallacy recognition. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 8172\u20138187, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.emnlp-main.560. 9\nChandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman,\nHannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning.\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=\nByg1v1HKDB. 1\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing, pp. 632\u2013642, Lisbon, Portugal, September\n2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:\n//aclanthology.org/D15-1075. 9\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,\npp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.\ncc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. 7\nDavid Maxwell Chickering. Optimal structure identification with greedy search. J. Mach. Learn.\nRes., 3:507\u2013554, 2002. URL http://jmlr.org/papers/v3/chickering02b.html.\n3\n10\nPreprint 2023\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object\nClassification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges\nWorkshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, pp.\n177\u2013190. Springer, 2006. 9\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423. 1, 6\nClark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graph-\nical models. Frontiers in Genetics, 10:524, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019.\n00524.\nURL https://www.frontiersin.org/article/10.3389/fgene.2019.\n00524. 2, 3\nMadelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer. John\nWiley and Sons, 2016. 2, 3\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of\nplausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First\nJoint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main\nconference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on\nSemantic Evaluation (SemEval 2012), pp. 394\u2013398, Montr\u00e9al, Canada, 7-8 June 2012. Association\nfor Computational Linguistics. URL https://aclanthology.org/S12-1052. 1\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced\nBert with disentangled attention. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://\nopenreview.net/forum?id=XPZIaotutsD. 6\nPatrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Sch\u00f6lkopf. Non-\nlinear causal discovery with additive noise models.\nIn Daphne Koller, Dale Schuurmans,\nYoshua Bengio, and L\u00e9on Bottou (eds.), Advances in Neural Information Processing Systems\n21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing\nSystems, Vancouver, British Columbia, Canada, December 8-11, 2008, pp. 689\u2013696. Curran As-\nsociates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/hash/\nf7664060cc52bc6f3d620bcedc94a4b6-Abstract.html. 3\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline\nfor natural language attack on text classification and entailment. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications\nof Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 8018\u2013\n8025. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/\nview/6311. 9\nZhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya\nSachan, Rada Mihalcea, and Bernhard Sch\u00f6lkopf. Logical fallacy detection. In Findings of\nthe Association for Computational Linguistics: EMNLP 2022, pp. 7180\u00c3\u00a2\u00e2\u201a\u00ac\u00e2C\u0153\u20137198, Abu\nDhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL\nhttps://arxiv.org/abs/2202.13758. 9, 10\nEmre K\u0131c\u0131man, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language\nmodels: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050, 2023. 1, 9\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, Online, July\n11\nPreprint 2023\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL\nhttps://aclanthology.org/2020.acl-main.703. 6\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692. 6\nBill MacCartney and Christopher D. Manning. Modeling semantic containment and exclusion in\nnatural language inference. In Proceedings of the 22nd International Conference on Computational\nLinguistics (Coling 2008), pp. 521\u2013528, Manchester, UK, August 2008. Coling 2008 Organizing\nCommittee. URL https://aclanthology.org/C08-1066. 9\nBrendan D. McKay and Adolfo Piperno. Practical graph isomorphism, II. J. Symb. Comput., 60:\n94\u2013112, 2014. doi: 10.1016/j.jsc.2013.09.003. URL https://doi.org/10.1016/j.jsc.\n2013.09.003. 5\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A framework\nfor adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 119\u2013126, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.emnlp-demos.16. URL https://aclanthology.org/2020.emnlp-demos.16.\n9\nOpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774.\nURL https://doi.org/10.48550/arXiv.2303.08774. 1, 7\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser\nKelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback.\nCoRR, abs/2203.02155, 2022. doi: 10.48550/arXiv.2203.02155. URL https://doi.org/10.\n48550/arXiv.2203.02155. 1, 7\nJudea Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible inference. Morgan\nKaufmann, 1988. 3\nJudea Pearl. Causality: Models, reasoning and inference (2nd ed.). Cambridge University Press,\n2009. 1\nJonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Elements of causal inference: Foundations\nand learning algorithms.\nThe MIT Press, 2017.\nURL https://mitpress.mit.edu/\nbooks/elements-causal-inference. 1\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin\nChoi. Counterfactual story reasoning and generation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pp. 5043\u20135053, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1509. URL https:\n//aclanthology.org/D19-1509. 1, 9\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019. 1, 7\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version\nof BERT: Smaller, faster, cheaper and lighter.\nCoRR, abs/1910.01108, 2019.\nURL http:\n//arxiv.org/abs/1910.01108. 6\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah\nRashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. ATOMIC: an atlas of machine com-\nmonsense for if-then reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence,\nAAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019,\nHonolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 3027\u20133035. AAAI Press, 2019a.\n12\nPreprint 2023\ndoi: 10.1609/aaai.v33i01.33013027. URL https://doi.org/10.1609/aaai.v33i01.\n33013027. 1, 9\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Common-\nsense reasoning about social interactions. In EMNLP 2019, 2019b. 1, 9\nShohei Shimizu, Patrik O. Hoyer, Aapo Hyv\u00e4rinen, and Antti J. Kerminen. A linear non-gaussian\nacyclic model for causal discovery. J. Mach. Learn. Res., 7:2003\u20132030, 2006. URL http:\n//jmlr.org/papers/v7/shimizu06a.html. 3\nSam Shleifer and Alexander M. Rush. Pre-trained summarization distillation. CoRR, abs/2010.13002,\n2020. URL https://arxiv.org/abs/2010.13002. 7\nPeter Spirtes and Kun Zhang. Causal discovery and inference: Concepts and recent methodological\nadvances. In Applied informatics, volume 3, pp. 1\u201328. SpringerOpen, 2016. 2, 3\nPeter Spirtes, Clark Glymour, and Richard Scheines. Causation, prediction, and search. 1993. 2, 3\nPeter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search, Second\nEdition. Adaptive computation and machine learning. MIT Press, 2000. ISBN 978-0-262-19440-2.\n1, 2, 3, 10\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023. 7\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.\norg/10.48550/arXiv.2302.13971. 7\nRuibo Tu, Chao Ma, and Cheng Zhang. Causal-discovery performance of chatgpt in the context of\nneuropathic pain diagnosis. arXiv preprint arXiv:2301.13819, 2023. 1\nAdina Williams, Nikita Nangia, and Samuel Bowman.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pp. 1112\u20131122, New Orleans, Louisiana, June\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https:\n//aclanthology.org/N18-1101. 9\nMoritz Willig, Matej Ze\u02c7cevi\u00b4c, Devendra Singh Dhami, and Kristian Kersting. Probing for correlations\nof causal facts: Large language models and causality, 2023. URL https://openreview.\nnet/forum?id=UPwzqPOs4-. 9\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nURL\nhttps://aclanthology.org/2020.emnlp-demos.6. 6, 7, 15\nYuxi Xie, Guanzhen Li, and Min-Yen Kan. Echo: Event causality inference via human-centric\nreasoning. arXiv preprint arXiv:2305.14740, 2023. 1\nMatej Ze\u02c7cevi\u00b4c, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large\nlanguage models may talk causality but are not causal. arXiv preprint arXiv:2308.13067, 2023. 1\n13\nPreprint 2023\nKun Zhang and Aapo Hyv\u00e4rinen. Causality discovery with additive disturbances: An information-\ntheoretical perspective. In Machine Learning and Knowledge Discovery in Databases: European\nConference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20,\npp. 570\u2013585. Springer, 2009. 3\nLi Zhang, Qing Lyu, and Chris Callison-Burch. Reasoning about goals, steps, and temporal ordering\nwith WikiHow. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4630\u20134639, Online, November 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.emnlp-main.374. URL https://aclanthology.org/\n2020.emnlp-main.374. 9\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOPT: open pre-trained transformer language models. CoRR, abs/2205.01068, 2022. doi: 10.48550/\narXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068. 1\n14\nPreprint 2023\nA\nIMPLEMENTATION DETAILS\nWhen finetuning on our data, for GPT-based models, we use the default settings of the OpenAI\nfinetuning API; and for BERT-based models, we use the transformers library (Wolf et al., 2020)\nand train the models on a server with an NVIDIA Tesla A100 GPU with 40G of memory. To fit for\nthe GPU memory, we set the batch size to be 8. We use the validation set to tune the learning rate,\nwhich takes value in {2e-6, 5e-6, 1e-5, 2e-5, 5e-5}; dropout rate, which takes value in {0, 0.1, 0.2,\n0.3}; and weight decay, which takes value in {1e-4, 1e-5}. We train the models until convergence,\nwhich is usually around ten epochs.\nPrompts\nWhen querying the autoregressive LLMs, we formulate the prompt as follows:\nQuestion: [premise]\nCan we deduct the following: [hypothesis]? Just answer \"Yes\" or \"No.\"\nAnswer:\nB\nGENERATING NATURAL STORIES\nTo generate the natural stories based on our symbolic expressions, we utilize the state-of-the-art LLM,\nGPT-4, which is very good at story generation. We design detailed instructions in the prompt, and\ngenerate around 200 stories in our case study. We show two examples stories in Table 7, and the\nreport the overall statistics in Table 8.\nExample 1 (Label=Negative)\nExample 2 (Label=Positive)\nSymbolic\nForm\nPremise: Suppose there is a closed system\nof 2 variables, A and B. All the statistical\nrelations among these 2 variables are as fol-\nlows: A correlates with B.\nHypothesis: A directly affects B.\nRelation between the promise and hypothe-\nsis: The premise does not a necessary con-\ndition for the hypothesis.\nPremise: Suppose there is a closed system\nof 3 variables, A, B and C. All the statisti-\ncal relations among these 3 variables are as\nfollows: A correlates with C. B correlates\nwith C. However, A is independent of B.\nHypothesis: A directly affects C.\nRelation between the promise and hypoth-\nesis: The premise is a necessary condition\nfor the hypothesis. So if the premise is true,\nthe hypothesis must be true.\nNatural\nStory\nPremise: Suppose there is a closed system\nof 2 variables, ice cream sales and swim-\nming pool attendance. All the statistical\nrelations among these 2 variables are as fol-\nlows: ice cream sales correlate with swim-\nming pool attendance.\nHypothesis: Ice cream sales directly affect\nswimming pool attendance.\nRelation between the premise and hypoth-\nesis: The premise does not provide a nec-\nessary condition for the hypothesis. The\ncorrelation between ice cream sales and\nswimming pool attendance could be due to\na third variable, such as hot weather, which\nincreases both ice cream sales and swim-\nming pool attendance. Therefore, it is not\nnecessarily true that ice cream sales directly\naffect swimming pool attendance.\nPremise: Let\u2019s consider three factors: eat-\ning junk food (A), obesity (C), and watch-\ning television (B). There is a correlation\nbetween eating junk food and obesity, and\nbetween watching television and obesity.\nHowever, eating junk food and watching\ntelevision are independent from each other.\nHypothesis: Eating junk food directly af-\nfects obesity.\nRelation between the premise and hypoth-\nesis: The premise provides the necessary\nconditions for the hypothesis. It establishes\nthe independent variables A (eating junk\nfood) and B (watching television) and their\ncorrelations with obesity. Given that these\nare true, it supports the hypothesis that eat-\ning junk food directly affects obesity.\nTable 7: Examples of natural stories generated based on the symbolic form in our CORR2CAUSE\ndataset, showing the broad application value of our dataset as the starting point for various verbaliza-\ntions of the correlation-to-causation inference task.\n15\nPreprint 2023\nTest Set Size\n102\nDev Set Size\n102\n# Tokens/Premise\n64.88\n# Tokens/Hypothesis\n13.54\n# Tokens/Explanation\n64.66\n% Positive Labels\n1.67\nTable 8: Statistics of our generated natural stories. We report the number of samples in the test and de-\nvelopment sets; number of tokens per premise (# Tokens/Premise), hypothesis (# Tokens/Hypothesis),\nand explanation (# Tokens/Explanation); and percentage of the positive labels (% Positive Labels).\nFor more information, the exact prompt we use is \u201cHere is a causal inference rule: [symbolic\nform] Please provide a real-world example instantiating this phenomenon. Format it also as\n\"Premise:\", \"Hypothesis:\", and \"Relation between the promise and hypothesis:\".\u201d\nC\nTEMPLATES AND PARAPHRASES\nWe use the verbalization templates in Table 9 to compose the hypotheses for all six causal relations.\nCausal Relation\nHypothesis Template\nIs-Parent\n{Var i} directly causes {Var j}.\nIs-Ancestor\n{Var i} causes something else which causes {Var j}.\nIs-Child\n{Var j} directly causes {Var i}.\nIs-Descendant\n{Var j} is a cause for {Var i}, but not a direct one.\nHas-Collider\nThere exists at least one collider (i.e., common effect) of {Var i} and {Var j}.\nHas-Confounder\nThere exists at least one confounder (i.e., common cause) of {Var i} and {Var j}.\nParaphrases\nIs-Parent\n{Var i} directly affects {Var j}.\nIs-Ancestor\n{Var i} influences {Var j} through some mediator(s).\nIs-Child\n{Var j} directly affects {Var i}.\nIs-Descendant\n{Var j} influences {Var i} through some mediator(s).\nHas-Collider\n{Var i} and {Var j} together cause some other variable(s).\nHas-Confounder\nSome variable(s) cause(s) both {Var i} and {Var j}.\nTable 9: Templates and their paraphrases for each causal relation in the hypothesis. We use {Var\ni} and {Var j} as placeholders for the two variables.\nD\nCHANGE LOG FOR THE DATASET VERSION UPDATE\nTwo Equivalent Forms\nDuplication Property\nDe-Duplication Method\n\u00df Is-Parent(i, j)\nTwo exact same strings\nKeep only one, by forcing i < j\nIs-Child(j, i)\n\u00df Is-Ancestor(i, j) (Original)\nTwo different strings, but\nRandomly sample one out of the two\nIs-Descendent(j, i) (Original)\nsemantically equivalent\n\u00df Is-Ancestor(i, j) (Paraphrased)\nTwo exact same strings\nKeep only one, by forcing i < j\nIs-Descendent(j, i) (Paraphrased)\n\u00df Has-Collider(i, j)\nTwo different strings, but\nRandomly sample one out of the two\nHas-Collider(j, i)\nsemantically equivalent\n\u00df Has-Confounder(i, j)\nTwo different strings, but\nRandomly sample one out of the two\nHas-Confounder(j, i)\nsemantically equivalent\nTable 10: De-duplication methods for the six causal relation types and their verbalizations.\nDe-Duplication Strategy\nAs mentioned in Section 3.7 in the main paper, our original dataset (v1.0)\nhas duplication due to symmetric relations and verbalizations. We introduce in Table 10 several\nreasons for why duplicated hypotheses exist in our original data. One typical reason is symmetric\nrelations such as Is-Parent(A, B) and Is-Child(B, A), and, similarly, the paraphrased version of\n16\nPreprint 2023\nIs-Ancestor(A, B) and Is-Descendent(B, A). Another typical reason is the semantic equivalence in\nthe verbalization templates, which applies to the Has-Collider and Has-Confounder relations. For\nexample, the verbalized texts of Has-Collider(A, B) and Collider(B, A) are \u201cThere exists at least one\ncollider (i.e., common effect) of {A and B, B and A},\u201d respectively, which are semantically-equivalent\nparaphrases of each other, so we randomly keep one out of the two.\nResulting Dataset Statistics after De-Duplication\nSince the reason for duplication in the first\nplace is due to symmetry in the causal relation, or verbalization, the resulting new data, CORR2CAUSE\nv2.0, is exactly a half of the original data. As we reported previously in Table 3 of Section 3.7, the\ntotal number of samples cuts down to half, while the label distribution and all other properties are\nthe same. To compose each split, we apply the same de-duplication method for the test, train, and\ndevelopment sets. We notice that some duplicates are across the splits, so we prioritize keeping the\ntest and training sets untouched (to minimally affect the experimental results), and then reduce the\ndevelopment set by removing the cross-split duplicates, namely:\n\u2022 test_2.0 = deduplicate(test_1.0)\n\u2022 train_2.0 = deduplicate(train_1.0)\n\u2022 dev_2.0 = deduplicate(dev_1.0) \\ {test_2.0, train_2.0}\nWe expect minimal or almost no change to the experimental results. In case of the slight possibility\nthat this change in the development set might affect the model selection in the training process, future\nwork can feel free to re-train the models and update the exact performance number.\nE\nSPURIOUS CORRELATION ANALYSIS\nThe inspirations of our two robustness tests (paraphrasing and variable refactorization) come from our\ndata analysis. We check for spurious correlations in the data by reporting in Table 11 the point-wise\nmutual information (PMI) between the label and any n-gram with no more than four tokens. In\naddition, we also report the difference of the PMI with the two labels in the |Diff| column of Table 11,\nand report the top 10 n-grams.\nThe design spirit for our robustness test is that if the models\u2019 correct judgment relies on exploiting\nthese spurious correlations, then such reliance will be broken in our perturbations.\nN-Gram\nPMI w/ Non-Ent. Label\nPMI w/ Ent. Label\n|Diff|\na cause\n1.692209\n-1.025611\n2.717820\na cause for\n1.663640\n-0.983790\n2.647430\nA causes\n1.640679\n-0.951610\n2.592289\nA causes something\n1.621820\n-0.926075\n2.547895\na direct\n1.606052\n-0.905316\n2.511369\na direct one\n1.592673\n-0.888107\n2.480781\nfor D\n1.584826\n-0.878180\n2.463006\nfor D but\n1.583897\n-0.877014\n2.460911\nfor E\n1.582980\n-0.875864\n2.458844\nfor E but\n1.582074\n-0.874728\n2.456802\nTable 11: PMI between the labels and n-grams. The labels include non-entailment (Non-Ent.) and\nentailment (Ent.). And the n-grams include all with no more than four words. The |Diff| column\nshows the absolute value of the difference between the PMIs with two labels. We show the top 10\nn-grams with the largest differences of their PMIs with the two classes in the |Diff| column.\nWe can see that some spurious correlations are rooted in the framing of the hypothesis, such as \u201ca\ncause (for)\u201d, and \u201ca direct (one)\u201d (which we use the paraphrasing task to break), and others are\nconnected to the variable names, such as \u201cfor D (but)\u201d and \u201cfor E (but)\u201d (which we use the variable\nrefactorization to break).\nF\nFINE-GRAINED ERROR ANALYSIS\nIn addition to the fine-grained analysis by causal relation type in Table 6a for fine-tuned models, we\nalso report such error analysis for non-finetuned models in Table 12.\n17\nPreprint 2023\nSelected Models\nRelation Type\nF1\nPrecision\nRecall\nAccuracy\nGPT-3.5\nAll\n21.69\n17.79\n27.78\n69.46\nGPT-3.5\nIs-Parent\n8.82\n100\n4.62\n83.47\nGPT-3.5\nIs-Ancestor\n0\n0\n0\n90.67\nGPT-3.5\nIs-Child\n9.84\n100\n5.17\n85.33\nGPT-3.5\nIs-Descendant\n14.29\n11.9\n17.86\n84\nGPT-3.5\nHas-Collider\n34.24\n25.51\n52.07\n35.12\nGPT-3.5\nHas-Confounder\n15.33\n8.86\n56.76\n37.8\nGPT-4\nAll\n29.08\n20.92\n47.66\n64.6\nGPT-4\nIs-Parent\n0\n0\n0\n82.67\nGPT-4\nIs-Ancestor\n30.77\n31.25\n30.3\n88\nGPT-4\nIs-Child\n0\n0\n0\n84.53\nGPT-4\nIs-Descendant\n26.98\n17.35\n60.71\n75.47\nGPT-4\nHas-Collider\n44.1\n30.18\n81.82\n32.71\nGPT-4\nHas-Confounder\n20.67\n11.53\n100\n23.86\nRoBERTa MNLI\nAll\n22.79\n34.73\n16.96\n82.5\nRoBERTa MNLI\nIs-Parent\n0\n0\n0\n82.67\nRoBERTa MNLI\nIs-Ancestor\n0\n0\n0\n91.2\nRoBERTa MNLI\nIs-Child\n0\n0\n0\n84.53\nRoBERTa MNLI\nIs-Descendant\n0\n0\n0\n92.53\nRoBERTa MNLI\nHas-Collider\n43.45\n39.73\n47.93\n59.52\nRoBERTa MNLI\nHas-Confounder\n0\n0\n0\n84.45\nTable 12: Fine-grained evaluation results for some selected non-fine-tuned models.\nThese results are particularly revealing, showing how off-the-shelf models perform in recognizing\nspecific relations. Specifically, GPT-3.5 cannot recognize ancestor relations, whereas GPT-4 fails at\nall direct causation recognition with parents and children. And RoBERTa MNLI only did collider\nrelation relatively correctly. Note that, when the F1 score is zero, the accuracy number is a result of\nalways predicting the negative class of that relation.\n18\n"
  },
  {
    "title": "GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2306.06044.pdf",
    "upvote": "4",
    "text": "GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\nBARBARA ROESSLE, Technical University of Munich, Germany\nNORMAN M\u00dcLLER, Technical University of Munich, Germany and Meta Reality Labs Zurich, Switzerland\nLORENZO PORZI, Meta Reality Labs Zurich, Switzerland\nSAMUEL ROTA BUL\u00d2, Meta Reality Labs Zurich, Switzerland\nPETER KONTSCHIEDER, Meta Reality Labs Zurich, Switzerland\nMATTHIAS NIESSNER, Technical University of Munich, Germany\nFig. 1. GANeRF proposes an adversarial formulation whose gradients provide feedback for a 3D-consistent radiance field representation. This introduces\nadditional constraints that enable more realistic renderings, and lead to improved novel view synthesis compared to Nerfacto and other baselines.\nNeural Radiance Fields (NeRF) have shown impressive novel view synthe-\nsis results; nonetheless, even thorough recordings yield imperfections in\nreconstructions, for instance due to poorly observed areas or minor lighting\nchanges. Our goal is to mitigate these imperfections from various sources\nwith a joint solution: we take advantage of the ability of generative adver-\nsarial networks (GANs) to produce realistic images and use them to enhance\nrealism in 3D scene reconstruction with NeRFs. To this end, we learn the\npatch distribution of a scene using an adversarial discriminator, which pro-\nvides feedback to the radiance field reconstruction, thus improving realism\nin a 3D-consistent fashion. Thereby, rendering artifacts are repaired directly\nin the underlying 3D representation by imposing multi-view path rendering\nconstraints. In addition, we condition a generator with multi-resolution\nNeRF renderings which is adversarially trained to further improve rendering\nquality. We demonstrate that our approach significantly improves rendering\nquality, e.g., nearly halving LPIPS scores compared to Nerfacto while at the\nsame time improving PSNR by 1.4dB on the advanced indoor scenes of Tanks\nand Temples.\nCCS Concepts: \u2022 Computing methodologies \u2192 Reconstruction.\nAdditional Key Words and Phrases: Neural radiance fields, Novel view syn-\nthesis\n1\nINTRODUCTION\nNeural Radiance Fields (NeRFs) [Mildenhall et al. 2020] can achieve\nremarkable novel view synthesis (NVS) results, powering applica-\ntions in the domains of virtual/mixed reality, robotics, computational\nAuthors\u2019 addresses: Barbara Roessle, Technical University of Munich, Germany; Nor-\nman M\u00fcller, Technical University of Munich, Germany and Meta Reality Labs Zurich,\nSwitzerland; Lorenzo Porzi, Meta Reality Labs Zurich, Switzerland; Samuel Rota Bul\u00f2,\nMeta Reality Labs Zurich, Switzerland; Peter Kontschieder, Meta Reality Labs Zurich,\nSwitzerland; Matthias Nie\u00dfner, Technical University of Munich, Germany.\nphotography, and many others. Given a set of posed input images,\nNeRFs distill complex and viewpoint-dependent scene information,\nparameterized as 5D input vectors (3D coordinates + 2D viewing\ndirection), into volumetric density and color fields modeled with a\nneural network. Volumetric rendering techniques are then applied\nto generate photorealistic 2D output images for novel camera views\nfrom these fields. While NeRFs are highly effective at providing\ncompact scene representations that enable photorealistic NVS, their\napplicability is nonetheless still limited for in-the-wild use cases.\nThat is, NeRFs are optimized to overfit to the training dataset\u2019s\nappearance information, which makes them highly dependent on\ncarefully collected input data and properly chosen regularization\nstrategies. In fact, the shape-radiance ambiguity [Zhang et al. 2020]\ndescribes that a set of training images can be perfectly regener-\nated from a NeRF without respecting the underlying geometry, but\nby simply exploiting the view-dependent radiance to simulate the\nactual scene geometry. As a consequence, novel view synthesis\nquality for non-training camera views drastically degrades and the\ngenerated images exhibit well-known cloudy floater artifacts. Un-\nfortunately, even with significantly increased capture efforts and\nwhen input data comprises dense coverage for a scene, the recon-\nstruction problem can remain ambiguous in areas with changing\nlighting conditions or in reflective or low-textured regions. Finally,\nfor making NeRFs more widely adopted and applicable, the image\ncapture process needs to be simple and of low effort while yielding\nhigh-quality reconstruction results.\nIn our work, we take advantage of generative adversarial net-\nworks (GANs) to improve the NeRF quality in challenging real-world\narXiv:2306.06044v2  [cs.CV]  18 Sep 2023\n2\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\nscenarios. To this end, we introduce GANeRF \u2013 a novel approach\nfor resolving imperfections directly in the NeRF reconstruction.\nOur key idea is to leverage an adversarial loss formulation in an\nend-to-end fashion to introduce additional rendering constraints\nfrom a per-scene 2D discriminator. In particular, in regions with\nlimited observations, this enforces the radiance field representation\nto generate patches that follow the distribution of real-world im-\nage patches more closely. Consequently, GANeRF enables notable\nmitigation of quality degradation effects in NVS due to imperfect\ninput data, independently of their root causes like limited coverage,\nimage distortion, or illumination changes.\nWe propose a joint adversarial training during the NeRF optimiza-\ntion, such that a 2D patch discriminator informs the NeRF about\nthe degree of photorealism for rendered patches. Through gradient\nfeedback into the 3D scene representation, we reduce typical im-\nperfections in the radiance field reconstruction while inherently en-\ncouraging 3D-consistent, photorealistic NeRF renderings. We show\nhow to further improve the output quality with a subsequent gener-\nator that operates on the 2D NeRF renderings at multiple scales, by\nrefining them to provide closer matches to the real distribution of\nthe scene\u2019s images. We evaluate our method on challenging indoor\nscenes from the novel ScanNet++ [Yeshwanth et al. 2023] and the\nwell-known Tanks and Temples [Knapitsch et al. 2017] datasets,\nand show that leveraging our adversarial formulation within NeRFs\nleads to significant image quality improvements over prior works.\nAcross all test scenes, we obtain remarkable improvements over the\nbest-performing baselines [Barron et al. 2022; Tancik et al. 2023] for\nperceptual metrics like LPIPS (reductions between 28-48%), while\nmaintaining consistently better PSNR and SSIM scores.\nIn summary, we provide the following contributions:\n\u2022 We introduce a novel adversarial formulation that imposes\npatch-based rendering constraints obtained from a 2D dis-\ncriminator to optimize a 3D-consistent radiance field repre-\nsentation.\n\u2022 We propose a 2D generator that further refines the rendering\noutput, demonstrating significant improvements over state-\nof-the-art methods in novel view synthesis on challenging,\nlarge-scale scenes.\n2\nRELATED WORK\nNeural Radiance Fields (NeRFs) model a 3D scene as a volumetric\nfunction, which can be rendered from arbitrary viewpoints to gener-\nate highly-realistic images. While the seminal paper of [Mildenhall\net al. 2020] encoded this function as a Multi-Layer Perceptron (MLP),\nmore recent works have proposed alternative representations based\non spatial data structures such as voxel grids [Sara Fridovich-Keil\nand Alex Yu et al. 2022; Sun et al. 2022], plane-based factoriza-\ntions [Chen et al. 2022], or multi-scale 3D hash grids [M\u00fcller et al.\n2022]. Mip-NeRF [Barron et al. 2021] addressed aliasing issues by\nintroducing an alternative rendering formulation based on conical\nfrustums instead of rays, which was further expanded in [Barron\net al. 2022] to account for unbounded scenes, and adapted to hash\ngrid-based representations in [Barron et al. 2023]. In our work, we\nfollow the Nerfacto model of [Tancik et al. 2023], which combines\nthe field architecture from Instant-NGP [M\u00fcller et al. 2022] with\nthe multi-stage proposals of MipNeRF-360 [Barron et al. 2022] to\nachieve a good trade-off between speed and quality.\n2.1\nNeRFs with Priors\nAlthough the approaches discussed in the previous section achieve\nimpressive visual fidelity, they generally struggle to represent under-\nconstrained scene regions. Most NeRFs incorporate one or more\nheuristics to combat common artifacts such as \u201cfloaters\u201d, e.g., by\nadding losses that promote peaked density [Barron et al. 2021, 2022;\nHedman et al. 2021], injecting noise into the model during train-\ning [Mildenhall et al. 2020], or promoting surface smoothness [Oech-\nsle et al. 2021; Wang et al. 2021; Zhang et al. 2021]. When data is\ninsufficient or ambiguous, however, more effective priors are neces-\nsary to regularize the model.\nIn the few-shot setting, geometric priors have been shown to be\nparticularly effective: pre-trained models can be used to supervise\nthe NeRF with predicted depth [Deng et al. 2022; Roessle et al. 2022]\nor surface normals [Yu et al. 2022]. RegNeRF [Niemeyer et al. 2022]\nutilizes a combination of geometric (surface smoothness, sampling\nspace annealing) and appearance (normalizing flow) regularizers to\nenable training a NeRF on as few as 3 images. Similarly, SinNeRF [Xu\net al. 2022] proposes a semi-supervised learning approach using\ngeometric and semantic pseudo labels to guide a NeRF reconstruc-\ntion from a single view. In our work, we focus on scenes with denser\ncoverage, where geometric priors have been observed to provide\nonly marginal improvements [Niemeyer et al. 2022].\nOther works focus on appearance-based priors, e.g., DiffusioN-\neRF [Wynn and Turmukhambetov 2023] uses a 2D denoising diffu-\nsion model trained on RGBD images to construct an unsupervised\nloss term, which encourages the NeRF to render plausible images\nfrom unobserved viewpoints. Finally, some approaches learn scene-\nbased priors, by casting NeRF reconstruction as a generalization\nproblem: MVSNeRF [Chen et al. 2021] constructs a cost volume that\nis encoded to a neural volume, allowing consistent renderings from\nonly a few images. PixelNeRF [Yu et al. 2021] and GenVS [Chan et al.\n2023] train an image encoder to lift few input views to 3D-aware\nneural representations that can be rendered from novel views. How-\never, these methods are still limited by the availability of training\ndata: approaches based on scene-based priors in particular tend to\nproduce results that lack detail, or impose strong assumptions on\nscene content and rendering trajectory according to the types of\nscenes they were trained on. In contrast, our per-scene adversarial\noptimization approach avoids the need for any external data.\n2.2\nGANs for Image Refinement\nGenerative Adversarial Networks (GANs) [Goodfellow et al. 2014;\nKarras et al. 2020; Mescheder et al. 2018] are trained to produce\nimages from a given data distribution by optimizing an adversarial\nloss. Initially proposed for unconditional image generation, GANs\nhave also been applied to image-to-image translation and refine-\nment tasks [Isola et al. 2017; Park et al. 2019; Wang et al. 2018a],\nsuch as colorization [Anwar et al. 2020], super-resolution [Ledig\net al. 2017], and in-painting [Elharrouss et al. 2020]. In a similar\nsetting, we optimize a conditional adversarial formulation based\non StyleGAN2 [Karras et al. 2020] to refine the images produced\nGANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\n\u2022\n3\nFig. 2. GANeRF method overview: our method takes as input a set of posed images and optimizes for a 3D radiance field representation. Our core idea is to\nincorporate multi-view patch-based re-rendering constraints in an adversarial formulation that guides the NeRF reconstruction process, and to refine rendered\nimages using a conditional generator network. Particularly in under-constrained regions this significantly improves the resulting rendering quality.\nby a NeRF to more closely match the data distribution of a given\nscene. Prior work has combined GANs with NeRFs for different\nsettings, such as unkown camera poses [Meng et al. 2021], gen-\nerative NeRFs of objects and compositions [Niemeyer and Geiger\n2021; Schwarz et al. 2020], 3D-aware generators [Kwak et al. 2022;\nSkorokhodov et al. 2022] and scene generation [Son et al. 2023].\n4K-NeRF [Wang et al. 2022] combines a low resolution NeRF with\na 3D-aware decoder for super-resolution. In our work, we found\nthat direct backpropagation from a discriminator to a full resolution\nNeRF is essential for view-consistent novel view synthesis. A re-\nlated idea is explored concurrently to our work in NeRFLiX [Zhou\net al. 2023], which trains a network in a non-adversarial setting to\ninvert NeRF artifacts. In contrast, the approach in [Zhou et al. 2023]\nrequires training on multiple scenes and relies on a hand-crafted\nmodel to simulate NeRF noise.\n3\nMETHOD\nGiven a set of posed input images capturing a static 3D scene, we\nfocus on the problem of synthesizing novel views of that scene. We\nbuild on top of recent Neural Radiance Fields (NeRF) [Mildenhall\net al. 2020], specifically, the Nerfacto model of [Tancik et al. 2023].\nHowever, our ideas can also be applied to a different NeRF architec-\nture, as shown in the experiments (Sec. 4.5.2). To improve realism in\nnovel views, our method (Fig. 2) leverages a 2D adversarial loss that\ndirectly updates the 3D scene representation. To this end, a discrim-\ninator learns the distribution of image patches in the training data.\nThrough adversarial training, the 3D NeRF representation (Sec. 3.1)\nis updated towards rendering patches that match this distribution\n(Sec. 3.2). On top of that, a 2D generator considers NeRF renderings\nat multiple resolutions, refining them based on feedback from a\nsecond discriminator (Sec. 3.3).\n3.1\nNeRF Preliminaries\nNeRF models represent a scene by providing a density \ud835\udf0e\ud835\udf03 (x) and an\nRGB color \ud835\udf09\ud835\udf03 (x, d) for each point x \u2208 R3 in 3D space, the color de-\npending additionally on the viewing direction d \u2208 R3 to account for\nview-dependent effects. This radiance field representation (\ud835\udf0e\ud835\udf03, \ud835\udf09\ud835\udf03),\nwhich depends on learnable parameters \ud835\udf03, allows rendering a pixel\nof an image by casting a ray from the camera origin o \u2208 R3 through\nthe pixel in the direction d and by computing the expected observed\ncolor along the ray with a distribution depending on the density\nfield [Mildenhall et al. 2020].\nFormally, the rendering function for a radiance field (\ud835\udf0e\ud835\udf03, \ud835\udf09\ud835\udf03) and\nray r is given by\nR\ud835\udf03 (r) B\n\u222b \u221e\n0\n\ud835\udf0e\ud835\udf03 (r\ud835\udc61) exp\n\u0014\n\u2212\n\u222b \ud835\udc61\n0\n\ud835\udf0e\ud835\udf03 (r\ud835\udc60)\ud835\udc51\ud835\udc60\n\u0015\n\ud835\udf09\ud835\udf03 (r\ud835\udc61, d)\ud835\udc51\ud835\udc61 .\n(1)\nHere, r\ud835\udc61 B o + \ud835\udc61d is the 3D point along the ray at time \ud835\udc61. R\ud835\udf03 (r) can\nbe approximated with the quadrature rule given samples of density\nand color along the ray [Mildenhall et al. 2020].\nThe NeRF parameters \ud835\udf03 are typically optimized to minimize the\nexpected, per-ray mean squared error, which penalizes differences\nbetween the rendered color and the ground truth color. Specifically,\ngiven ray/color pairs (r, c) distributed as \ud835\udc5d(r, c), we minimize\nLN\nrgb(\ud835\udf03) B E\ud835\udc5d \u2225R\ud835\udf03 (r) \u2212 c\u22252\n2 .\n(2)\nThe superscript N distinguishes losses that are used to train NeRF\nfrom the ones used to train the generator, which are defined later.\n3.2\nNeRF Optimization with Discriminator\nIt is often the case that scenes are rich in repeated elements and\nsurfaces often look similar even from different perspectives. Accord-\ningly, patches from one view form a good prior for patches in other\n4\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\nFig. 3. Our conditional generator architecture consists of a feature extraction pyramid of six blocks operating at multiple resolutions: Starting from a 4 \u00d7 4\ndown-sampled patch, convolutional blocks extract level-specific features that are upsampled and added together with generative noise to the next feature\nextraction level. The final output resolution matches the input resolution of 256 \u00d7 256.\nviews. To encode this prior in our training scheme, we complement\nLN\nrgb with an adversarial objective LN\nadv(\ud835\udf03,\ud835\udf19) that leverages an aux-\niliary neural network \ud835\udc37\ud835\udf19 (a.k.a. discriminator) parametrized by \ud835\udf19.\nThe objective is optimized such that the discriminator is pushed\ntowards classifying whether a given image patch is real or produced\nby the NeRF parametrized by \ud835\udf03 (a.k.a. fake), while the NeRF is en-\ncouraged to fool the discriminator, thus rendering realistic patches.\nPatches \ud835\udc43 are assumed to be distributed according to \ud835\udc5e(\ud835\udc43) and we\ndenote by r\ud835\udc43 the set of rays and by c\ud835\udc43 the pixel colors corresponding\nto patch \ud835\udc43. The adversarial objective should be minimized with re-\nspect to the NeRF parameters \ud835\udf03 as a proper loss, but maximized with\nrespect to the discriminator parameters \ud835\udf19. Following [Mescheder\net al. 2018], we adopt an \ud835\udc451 gradient penalty term on the discrimi-\nnator balanced by a nonnegative scalar \ud835\udf06Ngp yielding the following\nform of the adversarial objective:\nLN\nadv(\ud835\udf03,\ud835\udf19) B E\ud835\udc5e\n\u0002\n\ud835\udc53 (\ud835\udc37\ud835\udf19 (R\ud835\udf03 (r\ud835\udc43))) + \ud835\udc53 (\u2212\ud835\udc37\ud835\udf19 (c\ud835\udc43))\n\u2212\ud835\udf06N\ngp\u2225\u2207\ud835\udc37\ud835\udf19 (c\ud835\udc43)\u22252\n2\ni\n.\n(3)\nHere, by setting \ud835\udc53 (\ud835\udc65) B \u2212 log(1 + exp(\u2212\ud835\udc65)), we obtain a regular-\nized version of the loss originally proposed in the seminal GAN\npaper [Goodfellow et al. 2014].\nWe further employ a perceptual loss based on VGG [Johnson et al.\n2016], which has shown success in conjunction with 2D conditional\nGANs [Wang et al. 2018b]:\nLN\nperc(\ud835\udf03) B E\ud835\udc5e\n\u0002\n\u2225\u03a6VGG(R\ud835\udf03 (r\ud835\udc43)) \u2212 \u03a6VGG(c\ud835\udc43)\u22252\n2\n\u0003\n,\n(4)\nwhere \u03a6VGG is the vectorized concatenation of the first 5 feature\nlayers before the max pooling operation of a VGG19 network [Si-\nmonyan and Zisserman 2015], each layer being normalized by the\nsquare root of the number of entries. LNperc(\ud835\udf03) encourages similarity\nof real and fake patch features at different granularities.\nThe final loss that we minimize to train the NeRF is given by\nLN(\ud835\udf03) B LN\nrgb(\ud835\udf03) + \ud835\udf06N\npercLN\nperc(\ud835\udf03) + \ud835\udf06N\nadv max\n\ud835\udf19\u2208\u03a6 LN\nadv(\ud835\udf03,\ud835\udf19) ,\n(5)\nwhere \u03a6 denotes the set of possible discriminators and \ud835\udf06N\u2217 are non-\nnegative balancing factors for the different losses. This objective\nis optimized with stochastic gradients computed from batches that\ncombine rays and patches sampled uniformly across all training\nimages (i.e., from the \ud835\udc5d and \ud835\udc5e distributions, respectively). Moreover,\nparameter updates are alternated between the NeRF and the dis-\ncriminator to cope with the saddle-point problem, as usually done\nin GAN training schemes.\n3.3\nConditional Generator\nUp to this point, artifacts have been repaired directly in 3D through\nthe discriminator guidance introduced in Sec. 3.2. However, the\nrendering quality can be further improved in 2D by postprocessing\nsynthesized images. To this end, we design a conditional generator\n\ud835\udc3a\ud835\udf14 parameterized by \ud835\udf14, which takes as input a NeRF rendering\nand an auxiliary random vector z and produces a cleaned image,\nthus serving as a stochastic denoiser. The architecture is inspired\nby the StyleGAN2 generator [Karras et al. 2020], but we adapt it\nto a conditional generator and omit the mapping network (Fig. 3).\nWe keep the additive noise depending on z, which is close in spirit\nto the way pix2pix [Isola et al. 2017] uses dropout as source of\nrandomness in their conditional GAN setting. As shown in Fig. 3,\nthe conditioning input patch is 6 times bilinearly downsampled by\na factor of 2, thus enabling a multi-scale analysis. The downscaled\npatches are encoded with a convolutional layer into 32 feature\nchannels, which are concatenated to the output of the generator\nlayer of corresponding scale. As in StyleGAN2, we use leaky ReLU\nactivations, input/output skip connections and normalization layers.\nAdditive noise is injected after the normalization layers, and noise\nis additionally scaled by a learnable per-layer factor.\nAkin to the NeRF optimization, the generator is trained in a regu-\nlarized adversarial setting. This involves a second discriminator \ud835\udc37\ud835\udf13\nparameterized by \ud835\udf13, which differs from the previously introduced\n\ud835\udc37\ud835\udf19. Indeed, the NeRF and the conditional generator introduce differ-\nent types of errors in the output image that can be addressed more\neffectively by independent discriminators. The adversarial objective\nLG\nadv used to train the generator takes the following form:\nLG\nadv(\ud835\udf14,\ud835\udf13 |\ud835\udf03) B E\ud835\udc5e,\ud835\udc5b\n\u0002\n\ud835\udc53 (\ud835\udc37\ud835\udf13 (\ud835\udc3a\ud835\udf14 (R\u22a5\ud835\udf03 (r\ud835\udc43),\ud835\udc67)))\n+\ud835\udc53 (\u2212\ud835\udc37\ud835\udf13 (c\ud835\udc43)) \u2212 \ud835\udf06G\ngp\u2225\u2207\ud835\udc37\ud835\udf13 (c\ud835\udc43)\u22252\n2\ni\n,\n(6)\nwhere the expectation is additionally taken with respect to z dis-\ntributed as a standard normal distribution \ud835\udc5b(z). The adversarial\nGANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\n\u2022\n5\nobjective should be minimized with respect to the generator\u2019s pa-\nrameters \ud835\udf14, but maximized with respect to the discriminator param-\neter\ud835\udf13. Moreover, we stop the gradient through the NeRF parameters\n\ud835\udf03, which is denoted by \u22a5\ud835\udf03.\nThe adversarial objective is complemented with the same percep-\ntual loss used to train the NeRF, namely\nLG\nperc(\ud835\udf14|\ud835\udf03) B E\ud835\udc5e,\ud835\udc5b\n\u0002\n\u2225\u03a6VGG(\ud835\udc3a\ud835\udf14 (R\u22a5\ud835\udf03 (r\ud835\udc43), z)) \u2212 \u03a6VGG(c\ud835\udc43)\u22252\n2\n\u0003\n(7)\nand an \ud835\udc3f1 color loss, as in the conditional GAN pix2pix [Isola et al.\n2017], operating on patches, defined as:\nLG\nrgb(\ud835\udf14|\ud835\udf03) B E\ud835\udc5e,\ud835\udc5b\n\u0002\n\u2225\ud835\udc3a\ud835\udf14 (R\u22a5\ud835\udf03 (r\ud835\udc43), z) \u2212 c\ud835\udc43 \u22251\n\u0003\n.\n(8)\nThe overall generator loss is defined as\nLG(\ud835\udf14|\ud835\udf03\u2605) B \ud835\udf06G\npercLG\nperc(\ud835\udf14|\ud835\udf03\u2605) + \ud835\udf06G\nrgbLG\nrgb(\ud835\udf14|\ud835\udf03\u2605)\n+ max\n\ud835\udf13 \u2208\u03a8 LG\nadv(\ud835\udf14,\ud835\udf13 |\ud835\udf03\u2605),\n(9)\nwhere \u03a8 denotes the set of possible discriminators, \ud835\udf06G\u2217 are nonnega-\ntive factors balancing the loss components, and\ud835\udf03\u2605 is the parametriza-\ntion of the NeRF obtained by optimizing LN. We cope with the\nsaddle-point problem by alternating updates of the generator and\nthe discriminator akin to what we have described for LN.\n4\nEXPERIMENTS\n4.1\nDatasets and Metrics\n4.1.1\nScanNet++. From the ScanNet++ [Yeshwanth et al. 2023]\ndataset, we evaluate on five indoor scenes consisting of office, lab\nand apartment environments. Each scene contains an average of\n\u223c 700 images, \u223c 20 of which are defined as a test set. In contrast to\nother datasets commonly used in NeRF works (e.g., LLFF [Mildenhall\net al. 2020] or the 360\u25e6 scenes from [Barron et al. 2022]), the test\nviews in ScanNet++ are selected to be spatially out-of-distribution\ncompared to the training views, in order to explicitly evaluate a\nmodel\u2019s generalization capabilities. We use the DSLR images with\nthe provided camera poses from structure from motion [Sch\u00f6nberger\nand Frahm 2016], and undistort and resize them to 768 \u00d7 1152.\n4.1.2\nTanks and Temples. As a benchmark for larger-scale recon-\nstruction, we consider four scenes from the advanced set of scenes\nof the Tanks and Temples [Knapitsch et al. 2017] dataset: Audito-\nrium, Ballroom, Courtroom, and Museum. These scenes depict large\nindoor environments, with detailed geometries and complex illu-\nmination. For each scene, we randomly select 10% of the available\nimages as a test set, resulting in a split of \u223c 270 training and \u223c 30\ntest views on average. We use the original resolution of 1080 \u00d7 1920.\n4.1.3\nEvaluation Metrics. We evaluate our results in terms of three\nmain visual quality metrics: PSNR, SSIM [Wang et al. 2004], and\nLPIPS [Zhang et al. 2018]. Following previous works in the image\ngeneration literature, we additionally report KID [Bi\u0144kowski et al.\n2018] scores, which measure how closely our model\u2019s outputs match\nthe visual distribution of the scene.\n4.2\nTraining and Implementation Details\n4.2.1\nNeRF. In each training iteration, we render 4096 random rays\nand a 256 \u00d7 256 patch from a single random input image. The per-\nceptual loss processes the patch as a whole, whereas the adversarial\nloss subdivides it into 16 smaller patches of size 64 \u00d7 64 without\noverlap. \ud835\udc37\ud835\udf19 is a StyleGAN2 discriminator [Karras et al. 2020], which\nis adapted to process 64 \u00d7 64 patches with the number of convolu-\ntional channels reduced by half. For each generated \u201cfake\u201d patch\nthe discriminator is also given a real patch. The radiance field and\ndiscriminator \ud835\udc37\ud835\udf19 are trained for 400k iterations, using the Adam op-\ntimizer [Kingma and Ba 2015] and RMSprop [Tieleman and Hinton.\n2012] with learning rates 1 \u00d7 10\u22122 and 1 \u00d7 10\u22123, respectively. The\nloss weights are set to \ud835\udf06N\nadv = 0.0003, \ud835\udf06Nperc = 0.0003 and \ud835\udf06Ngp = 0.1.\n4.2.2\nConditional Generator. The initial training patch size before\ndownsampling is 256\u00d7256. At inference time, however, the generator\nis able to run fully convolutionally on high-resolution images to\nrefine the NeRF renderings. The generator is trained using a batch\nsize of 8. While the perceptual loss operates on the full training\npatches, the adversarial loss subdivides each patch into 4 smaller\npatches of size 128 \u00d7 128 without overlap, thus \ud835\udc37\ud835\udf13 is trained with\nbatch size 32. The discriminator is inspired by StyleGAN2 [Karras\net al. 2020], and adapted to process 128\u00d7128 patches. Both generator\nand discriminator \ud835\udc37\ud835\udf13 are trained for 3000 epochs using the Adam\noptimizer [Kingma and Ba 2015] with learning rate 2 \u00d7 10\u22123. The\nloss weights are set to \ud835\udf06Gperc = 1.0, \ud835\udf06G\nrgb = 3.0 and \ud835\udf06Ggp = 5.0.\n4.3\nBaseline Comparisons\nWe compare our approach to several baselines from recent literature.\nWe pick Mip-NeRF 360 [Barron et al. 2022] and Instant NGP [M\u00fcller\net al. 2022] as representatives of the current state of the art in NeRF\narchitectures optimized for visual quality and speed, respectively.\nFurthermore, we evaluate four variations of the Nerfacto [Tancik\net al. 2023] model that our proposed approach is based upon:\ni Nerfacto: the baseline model with no modifications.\nii Nerfacto + extra capacity: this higher-capacity variation doubles\nthe number of hidden dimensions of the MLPs, doubles the grid\nresolution, and increases the hash table size by 4 compared to\nthe default Nerfacto. This results in \u223c 44\ud835\udc40 trainable parameters,\ni.e., \u223c 25% more than our model (NeRF + Generator) and \u223c 3.4\u00d7\nthe number of parameters of the default Nerfacto (\u223c 12.9\ud835\udc40).\niii Nerfacto + pix2pix: Nerfacto results are refined by a pix2pix [Isola\net al. 2017] generator. From the official pix2pix repository, we\nfound that the generator using 9 ResNet [He et al. 2016] blocks,\ntrained with Wasserstein objective and gradient penalty [Gulra-\njani et al. 2017] performs best. We equally train this model with\nthe VGG perceptual loss, which improves its performance.\niv Nerfacto + ControlNet: Nerfacto results are refined by Sta-\nble Diffusion [Rombach et al. 2022] with ControlNet [Zhang\nand Agrawala 2023] conditioning on Nerfacto renderings and\nLoRA [Hu et al. 2022] fine-tuning using the implementation of\n[Hecong 2023].\nWe further compare to the recent work 4K-NeRF [Wang et al. 2022].\nFor a fair comparison, all baselines are trained until convergence.\nAs shown in Tab. 1 and Tab. 2, our approach leads to noticeable\n6\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\nTable 1. Quantitative results on five ScanNet++ scenes. Our method outper-\nforms the baselines by a large margin in the perceptual metrics, like LPIPS\nand KID, while maintaining consistently better PSNR and SSIM scores.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nKID \u2193\nMip-NeRF 360 [Barron et al. 2022]\n24.9\n0.862\n0.225\n0.0241\nInstant NGP [M\u00fcller et al. 2022]\n25.3\n0.844\n0.269\n0.0511\n4K-NeRF [Wang et al. 2022]\n22.7\n0.807\n0.254\n0.0350\nNerfacto [Tancik et al. 2023]\n25.6\n0.848\n0.245\n0.0398\nNerfacto + extra capacity\n25.9\n0.854\n0.228\n0.0314\nNerfacto + pix2pix [Isola et al. 2017]\n24.9\n0.848\n0.193\n0.0162\nNerfacto + ControlNet [Zhang and\nAgrawala 2023]\n23.1\n0.827\n0.174\n0.0097\nOurs w/o discriminator\n25.8\n0.857\n0.177\n0.0143\nOurs w/o generator\n25.9\n0.860\n0.198\n0.0169\nOurs\n26.1\n0.864\n0.161\n0.0113\nTable 2. Quantitative results on four Tanks and Temples scenes. Our method\nachieves particularly strong improvements on the perceptual metrics, thus\nimproving visual details and sharpness of novel view renderings.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nKID \u2193\nMip-NeRF 360 [Barron et al. 2022]\n18.5\n0.709\n0.327\n0.0277\nInstant NGP [M\u00fcller et al. 2022]\n19.3\n0.700\n0.369\n0.0466\n4K-NeRF [Wang et al. 2022]\n19.4\n0.656\n0.356\n0.0353\nNerfacto [Tancik et al. 2023]\n19.5\n0.716\n0.329\n0.0432\nNerfacto + extra capacity\n19.6\n0.733\n0.291\n0.0314\nNerfacto + pix2pix [Isola et al. 2017]\n20.6\n0.739\n0.242\n0.0115\nNerfacto + ControlNet [Zhang and\nAgrawala 2023]\n19.6\n0.706\n0.213\n0.0085\nOurs w/o discriminator\n20.6\n0.745\n0.192\n0.0102\nOurs w/o generator\n19.9\n0.739\n0.251\n0.0130\nOurs\n20.9\n0.776\n0.169\n0.0065\nimprovements when compared to baselines. In particular, the two\nperceptual metrics (LPIPS, KID) demonstrate the largest relative im-\nprovements, suggesting that our approach is able to fix many of the\nsmall visual artifacts that are often poorly measured by color simi-\nlarity metrics (PSNR, SSIM). This is also confirmed by the qualitative\nevaluation shown in Figs. 4 to 6. Nerfacto + ControlNet achieves\nhigh perceptual quality (LPIPS, KID), however, the results are less\nview-consistent (see Sec. 4.5.1), which also reflects in lower PSNR.\n4.4\nAblation Experiments\nTo verify the effectiveness of the added components, we perform an\nablation study on the ScanNet++ and Tanks and Temples datasets.\nThe quantitative results (Tab. 1 and Tab. 2), as well as the qualitative\nresults in Fig. 7 show that the complete version of our method\nachieves the highest performance.\n4.4.1\nWithout Discriminator. In this experiment, we investigate\nthe impact of optimizing the radiance field solely with an RGB loss\nand applying our generator as a pure post-processing step on the\nrenderings. We observe that the geometry and texture details of\nthe scene cannot be recovered to the same extent as achieved with\nour full method (Fig. 7). The drop in all metrics (\u201cw/o discriminator\u201d\nin Tabs. 1 and 2) clearly highlights the importance of the patch-\nbased supervision to inform the NeRF representation in 3D. This\nTable 3. Ablation study on Auditorium scene from Tanks and Temples. The\ndecline in performance when removing individual parts of our method\nconfirms our design choices.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nKID \u2193\nNerfacto [Tancik et al. 2023]\n21.1\n0.843\n0.304\n0.0378\nOurs\n22.3\n0.862\n0.158\n0.0107\nw/o discriminator\n21.9\n0.857\n0.178\n0.0178\nw/o generator\n21.7\n0.854\n0.247\n0.0155\nNeRF training\nw/o adv. loss\n21.9\n0.861\n0.175\n0.0110\nw/o perc. loss\n21.7\n0.857\n0.187\n0.0194\nGenerator training\nw/o high-res. patches\n22.1\n0.851\n0.179\n0.0176\nw/o low-res. patches\n22.1\n0.862\n0.159\n0.0109\nw/o RGB encoding\n22.1\n0.860\n0.167\n0.0122\nw/o adv. loss\n22.3\n0.861\n0.174\n0.0122\nw/o perc. loss\n21.7\n0.834\n0.184\n0.0110\nw/o RGB loss\n21.8\n0.859\n0.163\n0.0109\nindicates that our method, which incorporates gradient backpropa-\ngation to the 3D representation, surpasses a pure 2D post-processing\napproach.\n4.4.2\nWithout Generator. When omitting the generator, the results\nare less sharp (e.g., samples 4, 5, and 7 in Fig. 7), which leads to\nlower performance (\u201cw/o generator\u201d in Tabs. 1 and 2) compared to\nthe full version of our method. This shows that the generator helps\nto achieve high-detail renderings.\n4.4.3\nNeRF Training. Tabs. 3 and 5 provide more detailed ablations\non the NeRF optimization, which show that dropping the adversarial\nloss (\u201cw/o adv. loss\u201d) or dropping the perceptual loss (\u201cw/o perc.\nloss\u201d) have a negative impact on performance.\n4.4.4\nGenerator Training. Detailed ablations on the generator train-\ning are listed in Tabs. 3 and 5. We investigate the impact of omitting\nspecific input patch resolutions in our model. Removing the two\nhighest resolutions out of the six resolutions results in a noticeable\nperformance drop (\u201cw/o high-res. patches\u201d), highlighting the essen-\ntial role of high-resolution input for the generator. Conversely, when\nthe two lowest resolutions are removed (\u201cw/o low-res. patches\u201d), we\nobserve a decline in quality that indicates that the generator also\nrelies on the low-resolution input. We further show that the small\nRGB encoder, i.e., one convolutional layer on the multi-resolution\npatches (Fig. 3), benefits the generator (\u201cw/o RGB encoding\u201d).\nFurthermore, our ablation experiments examine the individual\nloss functions, namely the adversarial loss (\u201cw/o adv. loss\u201d), percep-\ntual loss (\u201cw/o perc. loss\u201d), and L1 RGB loss (\u201cw/o RGB loss\u201d). The\nresults reveal that each of these loss functions contributes signifi-\ncantly to the overall performance of the generator.\n4.4.5\nReduced Number of Images. Tab. 4 lists results when reducing\nthe number of images of the largest scene (800 images) to 400, 200,\n100, 50 and 25 images, which is extremely sparse for room-scale\nscenes. It shows that our method consistently outperforms Nerfacto\nby a similar margin as observed in the more dense setting.\nGANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\n\u2022\n7\nMip-NeRF 360\nInstant NGP\nNerfacto\nOurs\nGround truth\nFig. 4. Comparison on Tanks and Temples. Our method recovers more detail than the baselines, such as thin structures or patterns on the floor.\n8\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\nMip-NeRF 360\nInstant NGP\nNerfacto\nOurs\nGround truth\nFig. 5. Comparison on Scannet++. Our method produces less foggy artifacts, which leads to sharper renderings compared to the baselines.\nGANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\n\u2022\n9\n4K-NeRF\nNerfacto + pix2pix\nNerfacto + ControlNet\nOurs\nGround truth\nFig. 6. Comparison on Tanks and Temples. Using backpropagation to the NeRF representation and effective generator conditioning, our novel views closely\nmatch the ground truth patterns compared to other approaches based on generative models.\nTable 4. Ablation study with reduced number of images on a large Scan-\nNet++ scene. Our method consistently outperforms Nerfacto by a similar\nmargin, regardless of the level of input sparsity.\nMethod\n# images\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nKID \u2193\nNerfacto [Tancik et al. 2023]\n800\n24.2\n0.844\n0.247\n0.0661\nOurs\n24.7\n0.870\n0.169\n0.0157\nNerfacto [Tancik et al. 2023]\n400\n24.2\n0.843\n0.252\n0.0766\nOurs\n24.3\n0.864\n0.169\n0.0176\nNerfacto [Tancik et al. 2023]\n200\n23.5\n0.825\n0.274\n0.0867\nOurs\n23.9\n0.862\n0.182\n0.0206\nNerfacto [Tancik et al. 2023]\n100\n22.2\n0.805\n0.307\n0.1069\nOurs\n22.5\n0.842\n0.216\n0.0289\nNerfacto [Tancik et al. 2023]\n50\n20.2\n0.771\n0.347\n0.1564\nOurs\n20.5\n0.788\n0.282\n0.0858\nNerfacto [Tancik et al. 2023]\n25\n15.5\n0.686\n0.524\n0.2432\nOurs\n16.7\n0.727\n0.408\n0.1738\n4.5\nEvaluations\n4.5.1\nView Consistency. To obtain view-consistent results, it is es-\nsential to first backpropagate to the NeRF and optimize the under-\nlying 3D representation. Following [Lai et al. 2018], Tab. 6 shows\nconsistency of test views computed with optical flow [Teed and\nDeng 2020]. Comparing Nerfacto and our method without back-\npropagation to NeRF (i.e., \u201cw/o discriminator\u201d), shows that our\ngenerator largely improves visual quality (i.e., KID), while adding\nTable 5. Ablation study on a ScanNet++ scene. Removing individual parts\nof our methods leads to a decline in performance, indicating the importance\nof the different architectural choices and training strategies.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nKID \u2193\nNerfacto [Tancik et al. 2023]\n24.2\n0.844\n0.247\n0.0661\nOurs\n24.7\n0.870\n0.169\n0.0157\nw/o discriminator\n24.4\n0.859\n0.188\n0.0194\nw/o generator\n24.6\n0.865\n0.201\n0.0294\nNeRF training\nw/o adv. loss\n24.4\n0.867\n0.170\n0.0172\nw/o perc. loss\n24.5\n0.866\n0.184\n0.0229\nGenerator training\nw/o high-res. patches\n24.2\n0.849\n0.202\n0.0188\nw/o low-res. patches\n24.6\n0.870\n0.171\n0.0180\nw/o RGB encoding\n24.6\n0.870\n0.171\n0.0179\nw/o adv. loss\n24.7\n0.869\n0.175\n0.0184\nw/o perc. loss\n24.4\n0.858\n0.170\n0.0217\nw/o RGB loss\n24.6\n0.869\n0.170\n0.0172\nmarginal inconsistencies. However, our full method backpropagates\nto the 3D representation, thus reducing the added inconsistencies\nby over half. Our video results show that inconsistencies are barely\nnoticeable, and qualitative improvements dominate. The alternative\nrefinement with ControlNet (Nerfacto + ControlNet) achieves high\nvisual quality, however, the refined views are highly inconsistent.\nFollowing [Wang et al. 2022], Fig. 8 visualizes view consistency by\n10\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\n1\n2\n3\n4\n5\n6\n7\nNerfacto\nOurs w/o\ndiscriminator\nOurs w/o generator\nOurs\nGround truth\nFig. 7. Ablation experiments on Tanks and Temples. \u201cOurs w/o discriminator\u201d significantly struggles with patterns (samples 4 and 5) and misses thin structures\n(samples 3 and 6). \u201cOurs w/o generator\u201d better recovers the patterns but produces blurry results compared to the full method (samples 4, 5 and 7).\nGANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\n\u2022\n11\nTable 6. View consistency evaluation on ScanNet++. Novel views from our\nmethod are both highly view-consistent and of high visual quality (i.e., KID).\nBest and second best results are highlighted.\nMethod\nView Consistency MSE \u2193\nKID \u2193\nNerfacto [Tancik et al. 2023]\n0.0018\n0.0398\nNerfacto + ControlNet [Zhang and\nAgrawala 2023]\n0.0039\n0.0097\nOurs w/o discriminator\n0.0023\n0.0143\nOurs w/o generator\n0.0018\n0.0169\nOurs\n0.0020\n0.0113\nTable 7. Using different NeRF backbones on ScanNet++. Our method is\nflexible to use an alternative NeRF representation, e.g., Instant NGP. The\ndefault approach builds on Nerfacto.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nKID \u2193\nInstant NGP [M\u00fcller et al. 2022]\n25.3\n0.844\n0.269\n0.0511\nOurs w/ Instant NGP\n25.6\n0.857\n0.173\n0.0148\nNerfacto [Tancik et al. 2023]\n25.6\n0.848\n0.245\n0.0398\nOurs\n26.1\n0.864\n0.161\n0.0113\nTable 8. Runtime comparison on ScanNet++. Our rendering time is compet-\nitive with Nerfacto; the training time is likely comparable to Mip-NeRF 360\nwhich was trained on 4 GPUs vs. our method was trained on a single GPU.\nMethod\nTraining Time \u2193\nRendering Time \u2193\nper frame (768x1152)\nMip-NeRF 360 [Barron et al. 2022]\n18 h 4x NVIDIA V100\n24 s 1x NVIDIA V100\nInstant NGP [M\u00fcller et al. 2022]\n3.8 h\n1x\nNVIDIA\nRTX\nA6000\n0.47 s\n1x\nNVIDIA\nRTX\nA6000\nNerfacto [Tancik et al. 2023]\n3.5 h\n0.87 s\nOurs\n2 d 10 h\n0.89 s\ntracking a column of pixels across a sequence of video frames: the\ninconsistency of Nerfacto + ControlNet is clearly visible as verti-\ncal stripe patterns. The full version of our method shows the best\ncombination of view consistency and visual quality in Fig. 8.\n4.5.2\nAlternative NeRF Backbone. We build our method on the\nNerfacto radiance field representation, however, it is flexible to\nbackpropagate gradients to a different NeRF backbone. Using In-\nstant NGP as alternative NeRF, leads to improvements of similar\nmagnitude across all metrics compared to using Nerfacto (Tab. 7).\nWith the Instant NGP backbone, we use the parameters as de-\nscribed in Sec. 4.2, except, due to higher memory consumption of the\nInstant NGP rendering, we reduce the patch size to 192 \u00d7 192, from\nwhich we crop four 64 \u00d7 64 patches for the discriminator. The loss\nweights are then set to \ud835\udf06N\nadv = 0.001, \ud835\udf06Nperc = 0.001 and \ud835\udf06Ngp = 0.1.\n4.5.3\nRuntime. Tab. 8 provides a comparison of training and ren-\ndering times. The rendering time of our method is dominated by the\nrendering time of the underlying NeRF, since the generator forward\npass is comparably fast, i.e., takes only 13ms. Hence, our method\nrenders novel views at effectively the same speed as Nerfacto and\nmuch faster than Mip-NeRF 360. Using Instant NGP as alternative\nNeRF backbone (Sec. 4.5.2) speeds up rendering towards the Instant\nNGP rendering time. Our training time is slower than Nerfacto or\nPixel column over time\nFirst Frame\nLast Frame\nfrom first to last frame |\u2192|\nNerfacto\nNerfacto + ControlNet\nOurs w/o discriminator\nOurs w/o generator\nOurs\nFig. 8. View consistency visualization. From a sequence of video frames,\nwe extract a column of pixels at the same position in each frame (left) and\nconcatenate them horizontally to visualize view consistency (right).\nInstant NGP; however, our method significantly improves visual\nquality. All methods are trained until convergence.\n4.5.4\nImpact of View Coverage on Visual Quality. Fig. 9-left shows\nthat PSNR of novel views increases with view coverage, and stag-\nnates for areas that have been observed by > 75 training views. The\nimprovement of our method over Nerfacto (Fig. 9-right) is twice as\nhigh in regions of low coverage, that are observed by < 25 training\n12\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\n0\n100\n200\nObserved by # train views\n17.5\n20.0\n22.5\n25.0\nPSNR\nOurs\n0\n100\n200\nObserved by # train views\n0.2\n0.3\n0.4\n0.5\n0.6\n\u2206 PSNR\nImprovement\nover Nerfacto\nFig. 9. Impact of view coverage on PSNR (left)/ improvement over Nerfacto\n(right) on a ScanNet++ scene. View coverage is computed using ground\ntruth depth and counting # train views that project to pixels in test views.\n\u0394 LPIPS: 0\n0.2\nNerfacto\nOurs\nGround truth overlayed\nw/ LPIPS improvement\nFig. 10. Perceptual improvement over Nerfacto. Our method particularly im-\nproves structured areas with objects or floor patterns, compared to texture-\nless surfaces. LPIPS is computed on patches.\nviews, compared to areas of higher coverage. Fig. 10 further visu-\nalizes the improvement in terms of LPIPS by calculating LPIPS on\nsmall 96 \u00d7 96 patches of test views in a convolutional manner. It\nshows that improvements particularly happen in richly structured\nareas, such as table-top objects or repetitive floor patterns, rather\nthan texture-less walls.\n4.5.5\nGAN Hallucination Effects. Using generative approaches for\nreconstruction comes with the danger of hallucinating content,\nwhich may not be view-consistent. Through backpropagation to\nthe NeRF representation and the generator conditioning, our novel\nOurs\nNearby training view\nFig. 11. GAN hallucination effects. Reconstruction of inscriptions is difficult,\nwhich can cause hallucinated characters (left).\nviews are highly view-consistent (Sec. 4.5.1) and closely match the\nground truth, e.g., difficult patterns in Fig. 6. We, however, observe\nthat reconstructing written text is very challenging, which can lead\nto hallucinated, incorrect characters as shown in Fig. 11.\n4.5.6\nDiscriminator on Unseen Views. Rendering fake patches from\nsampled poses rather than the training views, has potential to pro-\nvide additional discriminator feedback to the NeRF representation.\nOur attempt to sample poses from Gaussians around the training\nposes to render fake patches, however, left the average performance\nunchanged. Likely, a more sophisticated strategy is needed to sample\npatches that benefit from additional supervision.\n4.6\nLimitations\nOur results show that we can achieve significant improvements\ncompared to state-of-the-art methods. At the same time, we believe\nthere are still important limitations. For instance, at the moment\nour patch discriminator is trained per scene. However, it would be\nbeneficial to train a more generic prior that has access to a larger\nscene corpus to improve its capability. While feasible, a naive gen-\neralizable discriminator would tend to collapse to a scene classifier;\ni.e., it would primarily identify whether a patch belongs to the same\nscene or not. To overcome this issue, one possible solution is to\ntrain a NeRF representation for each training scene simultaneously,\nwhich would entail considerable computational expenses. Another\nlimitation is that we currently focus only on static scenes; however,\nwe believe it would be interesting to expand our approach to recent\ndeformable and dynamic NeRF approaches such as [I\u015f\u0131k et al. 2023;\nKirschstein et al. 2023; Park et al. 2021; Tretschk et al. 2021].\n5\nCONCLUSION\nWe introduce GANeRF, a new approach for adversarial optimiza-\ntion of neural radiance fields. The main idea behind our approach\nis to impose patch-based rendering constraints into the radiance\nfield reconstruction. By backpropagating gradients from a scene\npatch discriminator, we effectively address typical imperfections and\nrendering artifacts stemming from traditional NeRF methods. In par-\nticular, in regions with limited coverage, this significantly improves\nrendering quality, both qualitatively and quantitatively outperform-\ning state-of-the-art methods. At the same time, our method is only a\nstepping stone for combining rendering priors with high-end novel\nview synthesis. For instance, we believe that generalizing across\nscenes will offer numerous opportunities for leveraging similar ideas\nas those proposed in this work.\nGANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields\n\u2022\n13\nACKNOWLEDGMENTS\nThis work was funded by a Meta SRA. Matthias Nie\u00dfner was also\nsupported by the ERC Starting Grant Scan2CAD (804724). We thank\nAngela Dai for the video voice-over.\nREFERENCES\nSaeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz Khan, and\nAbdul Wahab Muzaffar. 2020. Image colorization: A survey and dataset. arXiv\npreprint arXiv:2008.10774 (2020).\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\nBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for\nAnti-Aliasing Neural Radiance Fields. ICCV (2021).\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\n2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. CVPR (2022).\nJonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman.\n2023. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. arXiv preprint\narXiv:2304.06706 (2023).\nMiko\u0142aj Bi\u0144kowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. 2018.\nDemystifying mmd gans. arXiv preprint arXiv:1801.01401 (2018).\nEric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon\nPark, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\n2023. GeNVS: Generative Novel View Synthesis with 3D-Aware Diffusion Models.\nIn arXiv.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. TensoRF:\nTensorial Radiance Fields. In European Conference on Computer Vision (ECCV).\nAnpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and\nHao Su. 2021. Mvsnerf: Fast generalizable radiance field reconstruction from multi-\nview stereo. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 14124\u201314133.\nKangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. 2022. Depth-supervised\nNeRF: Fewer Views and Faster Training for Free. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nOmar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, and Younes Akbari. 2020.\nImage inpainting: A review. Neural Processing Letters 51 (2020), 2007\u20132028.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In\nAdvances in Neural Information Processing Systems, Vol. 27.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron\nCourville. 2017. Improved Training of Wasserstein GANs. In Proceedings of the\n31st International Conference on Neural Information Processing Systems (Long Beach,\nCalifornia, USA) (NIPS\u201917). Curran Associates Inc., Red Hook, NY, USA, 5769\u20135779.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning\nfor Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). 770\u2013778. https://doi.org/10.1109/CVPR.2016.90\nWu Hecong. 2023. ControlLoRA: A Light Neural Network To Control Stable Diffusion\nSpatial Information. https://github.com/HighCWu/ControlLoRA\nPeter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul De-\nbevec. 2021. Baking neural radiance fields for real-time view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision. 5875\u20135884.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language\nModels. In International Conference on Learning Representations.\nMustafa I\u015f\u0131k, Martin R\u00fcnz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck,\nLourdes Agapito, and Matthias Nie\u00dfner. 2023. HumanRF: High-Fidelity Neural\nRadiance Fields for Humans in Motion. ACM Transactions on Graphics (TOG) 42, 4\n(2023), 1\u201312. https://doi.org/10.1145/3592415\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-Image\nTranslation with Conditional Adversarial Networks. CVPR (2017).\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual Losses for Real-Time\nStyle Transfer and Super-Resolution. In Computer Vision \u2013 ECCV 2016, Bastian Leibe,\nJiri Matas, Nicu Sebe, and Max Welling (Eds.). 694\u2013711.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. 2020. Analyzing and Improving the Image Quality of StyleGAN. In Proc. CVPR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization.\nCoRR abs/1412.6980 (2015).\nTobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nie\u00dfner.\n2023. NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads.\nhttps://doi.org/10.48550/arXiv.2305.03027 arXiv:2305.03027 [cs.CV]\nArno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and\ntemples: Benchmarking large-scale scene reconstruction. ACM Transactions on\nGraphics (ToG) 36, 4 (2017), 1\u201313.\nJeong-gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon Kim, David Han, and Hanseok\nKo. 2022. Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Ed-\nitable Portrait Image Synthesis. In European Conference on Computer Vision. Springer,\n236\u2013253.\nWei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-\nHsuan Yang. 2018. Learning Blind Video Temporal Consistency. In European Con-\nference on Computer Vision.\nChristian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham,\nAlejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang,\net al. 2017. Photo-realistic single image super-resolution using a generative adver-\nsarial network. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 4681\u20134690.\nQuan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and\nJingyi Yu. 2021. GNeRF: GAN-based Neural Radiance Field without Posed Camera.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).\nLars Mescheder, Sebastian Nowozin, and Andreas Geiger. 2018. Which Training Meth-\nods for GANs do actually Converge?. In International Conference on Machine Learning\n(ICML).\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields\nfor View Synthesis. In ECCV.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\nGraph. 41, 4, Article 102 (July 2022), 15 pages.\nhttps://doi.org/10.1145/3528223.\n3530127\nMichael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas\nGeiger, and Noha Radwan. 2022. RegNeRF: Regularizing Neural Radiance Fields\nfor View Synthesis from Sparse Inputs. In Proc. IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR).\nMichael Niemeyer and Andreas Geiger. 2021. GIRAFFE: Representing Scenes as Com-\npositional Generative Neural Feature Fields. In Proc. IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR).\nMichael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural\nimplicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision. 5589\u20135599.\nKeunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz,\nDan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021. HyperNeRF:\nA Higher-Dimensional Representation for Topologically Varying Neural Radiance\nFields. ACM Trans. Graph. 40, 6, Article 238 (dec 2021).\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image\nsynthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. 2337\u20132346.\nBarbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, and Matthias\nNie\u00dfner. 2022. Dense Depth Priors for Neural Radiance Fields from Sparse Input\nViews. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00c3\u00b6rn Om-\nmer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR).\nSara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and\nAngjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In\nCVPR.\nJohannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-Motion\nRevisited. In Conference on Computer Vision and Pattern Recognition (CVPR).\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020.\nGRAF:\nGenerative Radiance Fields for 3D-Aware Image Synthesis. In Advances in Neu-\nral Information Processing Systems 33, Vol. 25. Curran Associates, Inc., Red\nHook, NY, 20154\u201320166.\nhttps://papers.nips.cc/paper_files/paper/2020/hash/\ne92e1b476bb5262d793fd40931e0ed53-Abstract.html\nKaren Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks\nfor Large-Scale Image Recognition. In 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1409.1556\nIvan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. 2022. EpiGRAF:\nRethinking training of 3D GANs. In Advances in Neural Information Processing\nSystems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.).\nhttps://openreview.net/forum?id=TTM7iEFOTzJ\nMinjung Son, Jeong Joon Park, Leonidas Guibas, and Gordon Wetzstein. 2023. SinGRAF:\nLearning a 3D Generative Radiance Field for a Single Scene. In CVPR.\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization:\nSuper-fast Convergence for Radiance Fields Reconstruction. In CVPR.\nMatthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance\nWang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David\nMcAllister, and Angjoo Kanazawa. 2023. Nerfstudio: A Modular Framework for\nNeural Radiance Field Development. arXiv preprint arXiv:2302.04264 (2023).\nZachary Teed and Jia Deng. 2020. RAFT: Recurrent All-Pairs Field Transforms for\nOptical Flow. In European Conference on Computer Vision.\n14\n\u2022\nBarbara Roessle, Norman M\u00fcller, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner\nT. Tieleman and G. Hinton. 2012. Lecture 6.5 - rmsprop: Divide the gradient by a\nrunning average of its recent magnitude. (2012).\nEdgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00f6fer, Christoph Lass-\nner, and Christian Theobalt. 2021. Non-Rigid Neural Radiance Fields: Reconstruction\nand Novel View Synthesis of a Dynamic Scene From Monocular Video. In IEEE\nInternational Conference on Computer Vision (ICCV). IEEE.\nChaoyue Wang, Chang Xu, Chaohui Wang, and Dacheng Tao. 2018b. Perceptual\nAdversarial Networks for Image-to-Image Transformation. IEEE Transactions on\nImage Processing 27, 8 (2018), 4066\u20134079. https://doi.org/10.1109/TIP.2018.2836316\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\nWang. 2021. Neus: Learning neural implicit surfaces by volume rendering for\nmulti-view reconstruction. arXiv preprint arXiv:2106.10689 (2021).\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. 2018a. High-Resolution Image Synthesis and Semantic Manipulation\nwith Conditional GANs. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality\nassessment: from error visibility to structural similarity. IEEE transactions on image\nprocessing 13, 4 (2004), 600\u2013612.\nZhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and Liefeng Bo. 2022. 4K-NeRF:\nHigh Fidelity Neural Radiance Fields at Ultra High Resolutions. arXiv preprint\narXiv:2212.04701 (2022).\nJamie Wynn and Daniyar Turmukhambetov. 2023. DiffusioNeRF: Regularizing Neural\nRadiance Fields with Denoising Diffusion Models. In arxiv.\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang.\n2022. Sinnerf: Training neural radiance fields on complex scenes from a single\nimage. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part XXII. Springer, 736\u2013753.\nChandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and Angela Dai. 2023. Scan-\nNet++: A High-Fidelity Dataset of 3D Indoor Scenes. In Proceedings of the Interna-\ntional Conference on Computer Vision (ICCV).\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: Neural\nRadiance Fields from One or Few Images. In CVPR.\nZehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger.\n2022. Monosdf: Exploring monocular geometric cues for neural implicit surface\nreconstruction. arXiv preprint arXiv:2206.00665 (2022).\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. NeRF++: Analyzing\nand Improving Neural Radiance Fields. arXiv:2010.07492 (2020).\nLvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to Text-to-\nImage Diffusion Models. arXiv:2302.05543 [cs.CV]\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The\nunreasonable effectiveness of deep features as a perceptual metric. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition. 586\u2013595.\nXiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman,\nand Jonathan T Barron. 2021. Nerfactor: Neural factorization of shape and reflectance\nunder an unknown illumination. ACM Transactions on Graphics (TOG) 40, 6 (2021),\n1\u201318.\nKun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, and Jiangbo\nLu. 2023. NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-\nDriven Inter-viewpoint MiXer. In arxiv.\n"
  },
  {
    "title": "Multi-Modal Classifiers for Open-Vocabulary Object Detection",
    "link": "https://arxiv.org/pdf/2306.05493.pdf",
    "upvote": "4",
    "text": "Multi-Modal Classifiers for Open-Vocabulary Object Detection\nPrannay Kaul 1 Weidi Xie 1 2 3 Andrew Zisserman 1\nhttps://www.robots.ox.ac.uk/vgg/research/mm-ovod/\nAbstract\nThe goal of this paper is open-vocabulary object\ndetection (OVOD) \u2014 building a model that can\ndetect objects beyond the set of categories seen\nat training, thus enabling the user to specify cat-\negories of interest at inference without the need\nfor model retraining. We adopt a standard two-\nstage object detector architecture, and explore\nthree ways for specifying novel categories: via\nlanguage descriptions, via image exemplars, or\nvia a combination of the two. We make three\ncontributions: first, we prompt a large language\nmodel (LLM) to generate informative language\ndescriptions for object classes, and construct pow-\nerful text-based classifiers; second, we employ\na visual aggregator on image exemplars that can\ningest any number of images as input, forming\nvision-based classifiers; and third, we provide\na simple method to fuse information from lan-\nguage descriptions and image exemplars, yield-\ning a multi-modal classifier. When evaluating\non the challenging LVIS open-vocabulary bench-\nmark we demonstrate that: (i) our text-based clas-\nsifiers outperform all previous OVOD works; (ii)\nour vision-based classifiers perform as well as\ntext-based classifiers in prior work; (iii) using\nmulti-modal classifiers perform better than either\nmodality alone; and finally, (iv) our text-based and\nmulti-modal classifiers yield better performance\nthan a fully-supervised detector.\n1. Introduction\nIn this paper, we consider the problem of open-vocabulary\nobject detection (OVOD), which aims to localise and clas-\nsify visual objects beyond the categories seen at training\ntime. One may consider its usefulness from the perspec-\n1Visual Geometry Group, University of Oxford 2CMIC, Shang-\nhai Jiao Tong University 3Shanghai AI Lab. Correspondence to:\nPrannay Kaul <prannay@robots.ox.ac.uk>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nText Completion LLM\n\u2746\nWhat does a falcon look like?\nVisual Encoder\n\u2746\nText Encoder\n\u2746\nVisual Aggregator\n+\nOpen-\nVocabulary \nDetector\nA falcon is \na bird of \nprey with a \nhooked beak \nand sharp \ntalons.\nA falcon \nusually has a \ndark upper \nbody and light \ncoloured \nunderside.\nA falcon is a \nmedium-sized bird \nof prey with a \nlong tail, pointed \nwings, and a \nhooked beak.\nVector Mean Operator\nVision-based Classifier\nText-based Classifier\nMulti-Modal\nClassifier\nK Image exemplars of falcon\nTest Image\nCorrect falcon detection\nMulti-Modal Classifier used for OVD\nFigure 1. Overview of the architecture for generating text-based,\nvision-based, or multi-modal classifiers for OVOD. Vision (top\nleft): A frozen visual encoder ingests image exemplars of falcon\nproducing an embedding per exemplar. A trained aggregator takes\nthese embeddings as input and produces a vision-based classifier.\nText (top right): A text completion LLM is prompted to give de-\nscriptions of a falcon which are then encoded by a text encoder and\naveraged yielding a text-based classifier. Multi-Modal (middle):\nMulti-Modal classifiers are generated by adding the vision-based\nand text-based classifiers together. OVOD (bottom): The multi-\nmodal classifier is used to detect the falcon in a standard model.\nNote, all three types of classifier: vision-based, text-based and\nmulti-modal, can be used on the detector head for OVOD.\ntive of online inference, when users want to freely specify\ncategories of interest at inference time without the need or\nability to re-train models. To specify categories of interest,\nthree obvious ways exist, namely: (1) text-based, e.g. name\nthe category or describe it in text form; (2) vision-based,\ne.g. give image examples; (3) multi-modal, e.g. indicate the\ncategory jointly with text and image.\nExisting works (Bansal et al., 2018; Gu et al., 2022; Zareian\net al., 2021; Zhou et al., 2022; Feng et al., 2022) have ex-\nplored replacing the learnt classifiers in a traditional detector\nwith text embeddings, that are computed by passing the class\nname into a pre-trained text encoder with manual prompts,\nsuch as \u201ca photo of a dalmatian\u201d, however, this design can\n1\narXiv:2306.05493v1  [cs.CV]  8 Jun 2023\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n2\nbe sub-optimal from three aspects: first, the discriminative\npower of generated text embeddings relies entirely on the\ninternal representation of the pre-trained text encoder, po-\ntentially leading to lexical ambiguities \u2014 e.g. \u201cnail\u201d can\neither refer to \u201cthe hard surface on the tips of the fingers\u201d or\n\u201ca small metal spike with a flat tip hammered into wood to\nform a joint\u201d \u2014 simply encoding the class name will not be\nable to distinguish the two concepts; second, the class name\nfor objects of interest may be unknown to the user, while\nexemplar images can be easily acquired \u2014 e.g. \u201cdugong\u201d\nrefers to a herbivorous marine mammal with an adorable,\nplump appearance, a dolphin tail, round head and downward\nsnout (an example is in Section I of the Appendix); third, in\ncases where multi-modal information is preferable to spec-\nify the category of interest, e.g. a species of butterfly with\na distinctive wing pattern \u2014 language descriptions can be\nunsuitably long to capture all the intricacies of a given cate-\ngory, while an exemplar image can \u201ctell a thousand words\u201d\nand act as an effective complement to text.\nTo tackle these limitations, we propose a multi-modal open-\nvocabulary object detector, with the classifier of the de-\ntector for a particular category being constructed via nat-\nural language descriptions, image exemplars or a combi-\nnation of the two. Specifically, we establish an automatic\napproach for sourcing visual descriptions of the object cate-\ngories, by prompting a large language model with questions,\ne.g. \u201cWhat does a dalmatian look like?\u201d, yielding \u201cA dal-\nmatian is typically a large dog with a short coat of black\nspots on a white background\u201d. Such a description provides\nadditional visual cues to enhance the discriminative power\nof the classifier generated from a text encoder. For cases\nwhere collecting suitable informative language descriptions\nmay be difficult or require unnecessarily long descriptions\nto establish the differences between classes, e.g. the dog\nbreeds \u201cpug\u201d and \u201cbulldog\u201d have similar descriptions, we\ncan generate classifiers from image exemplars \u2014 RGB im-\nages of the class of interest. Finally, we suggest a simple\nmethod to fuse both language descriptions and image exem-\nplars, yielding multi-modal classifiers that perform better\nthan either modality individually.\nWe explore the issue of how best to combine the set of\nlanguage descriptions and the set of image exemplars into\na classifier, by comparing the performance of aggregation\nmethods on a standard detector architecture. By evaluat-\ning on the challenging LVIS (Gupta et al., 2019) open-\nvocabulary object detection benchmark we show that: (i)\nour automated method for sourcing rich natural language\ndescriptions yields text-based classifiers superior to those\nof previous work that rely entirely on the class name; (ii)\nvision-based classifiers can be effectively constructed by a\nvisual aggregator, enabling novel categories to be detected\nby specifying image exemplars; (iii) natural language de-\nscriptions and image exemplars can be simply combined to\nproduce multi-modal classifiers, which perform better than\neither modality individually, and achieve superior results to\nexisting approaches.\n2. Related Work\nClosed-Vocabulary Object Detection is one of the classi-\ncal computer vision problems, making a full overview here\nimpossible. Therefore, we outline some key milestones.\nIn general, modern object detection methods can be cast\ninto two sets: one-stage and two-(multi-)stage detectors.\nOne-stage detectors directly classify and regress bound-\ning boxes by either densely classifying a set of predefined\nanchor boxes (Redmon et al., 2016; Redmon & Farhadi,\n2018; Liu et al., 2016; Lin et al., 2017; Tan et al., 2020),\neach which may contain an object, or densely searching for\ngeometric entities of objects e.g. corners, centre points or\nboxes (Law & Deng, 2018; Zhou et al., 2019; Tian et al.,\n2019). Conversely, most two-stage detectors first propose\nclass-agnostic bounding boxes that are pooled to fixed size\nregion-of-interest (RoI) features and classified by a sub-\nnetwork in the second stage (Girshick, 2015; Ren et al.,\n2016; Li et al., 2019). Two-stage detectors are extended to\nmulti-stage detectors in which the additional stages refine\npredictions made by the previous stage (Cai & Vasconce-\nlos, 2018; Chen et al., 2019; Zhou et al., 2021). A unique\nwork in this area is that of (Carion et al., 2020) which uses\nthe Transformer architecture (Vaswani et al., 2017) to treat\nobject detection as a set prediction problem. Note that,\nthe classifiers in these object detectors are jointly learnt on\na training set, therefore only objects seen at training time\ncan be detected during inference time, thus termed closed-\nvocabulary object detection.\nOpen-Vocabulary Object Detection goes beyond closed-\nvocabulary object detection and enables users to ex-\npand/change the detector vocabulary at inference time, with-\nout the need for model re-training. Recently, OVOD has\nseen increased attention and progress primarily driven by the\nemergence of large-scale vision-language models (VLMs)\ne.g. CLIP and ALIGN (Radford et al., 2021; Jia et al., 2021),\nwhich jointly learn image and natural language representa-\ntions.\nViLD (Gu et al., 2022) distills representations from VLMs.\nFirst, groundtruth bounding boxes are used to crop an\nimage and an embedding for the box is sourced from a\nfrozen VLM image encoder. An object detection model\nis learnt by matching overlapping region-of-interest (RoI)\nfeatures with the embedding for the relevant box from the\nVLM image encoder using a L1 reconstruction loss. Re-\ngionCLIP (Zhong et al., 2022) uses image-caption data to\nconstruct region-wise pseudo-labels, followed by region-\ntext contrastive pre-training before transferring to detection.\nGLIP and MDETR (Li et al., 2022; Kamath et al., 2021)\nuse captions to cast detection as a phrase grounding task\n2\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n3\nand use early fusion between the image and text modal-\nities, increasing complexity. OVR-CNN (Zareian et al.,\n2021) uses large image-caption data to pre-train a detector\nto learn a semantic space and finetunes on smaller detec-\ntion data. OWL-ViT (Minderer et al., 2022) follows OVR-\nCNN but makes use of large transformer models and even\nlarger image-caption data. OV-DETR (Zang et al., 2022)\nmodifies the DETR framework for closed-vocabulary ob-\nject detection (Carion et al., 2020) to make it suitable for\nthe open-vocabulary setting. We note that OV-DETR can\ncondition object detection on image exemplars, but only\nprovides some qualitative examples, whereas we quantita-\ntively benchmark our method using vision-based classifiers.\nDetic and PromptDet (Zhou et al., 2022; Feng et al., 2022)\nimprove open-vocabulary detection by making use of image\nclassification data to provide weak supervision on a large\nset of categories. Our work uses Detic as a starting point\nin experiments, and we investigate different methods for\nconstructing the classifiers.\nLow-shot Object Detection. Despite garnering less atten-\ntion in recent literature, some low/few-shot object detec-\ntion works make use of image-conditioned object detec-\ntion (Kang et al., 2019; Hsieh et al., 2019; Osokin et al.,\n2020; Chen et al., 2021) in which image exemplars of novel\ncategories are encoded at inference time and used to de-\ntect novel category instances. These works focus on ar-\nchitectural advances usually leveraging attention between\nthe novel class image exemplars and the inference time\nimage (Chen et al., 2021; Hsieh et al., 2019).\nThere are an increasing number of low/few-shot object de-\ntection works based on finetuning detector parameters on\nlimited groundtruth data for novel categories (Wang et al.,\n2020; Sun et al., 2021; Qiao et al., 2021; Kaul et al., 2022).\nThese works finetune the detector on limited groundtruth\nnovel instances and so are not related to open-vocabulary\nobject detection using vision-based classifiers, where no\nnovel instances are available for re-training/finetuning.\nNatural Language for Classification. Natural language\nis a rich source of semantic information for classification.\nCLEVER (Choudhury et al., 2021) matches descriptions\nof images in simple text form with descriptions from ex-\npert databases e.g. Wikipedia to perform fine-grained clas-\nsification.\nZSLPP (Elhoseiny et al., 2017) extracts vi-\nsual information from large-scale text to identify parts of\nobjects and perform zero-shot classification. The work\nby (Menon & Vondrick, 2023) uses class descriptions from\nGPT-3 (Brown et al., 2020) for classification, analysing\nwhich parts of the description contribute to classification\ndecisions. CuPL (Pratt et al., 2022) uses a GPT-3 model\nto provide detailed descriptions enabling improved zero-\nshot image classification. The learnings from this work\ninform our use of natural language descriptions sourced\nfrom LLMs.\n3. Method\nIn this section, we start by providing background on open-\nvocabulary object detection (OVOD), then outline the pro-\nposed methods for constructing classifiers from language de-\nscriptions of a category (text-based classifiers, Section 3.2)\nand image exemplars (vision-based classifiers, Section 3.3).\nOur final method combines the classifiers found from lan-\nguage descriptions and image exemplars, yielding multi-\nmodal classifiers (Section 3.4).\n3.1. Preliminaries\nProblem Scenario. Given an image (I \u2208 R3\u00d7H\u00d7W ) fed\ninput to an open-vocabulary object detector, two outputs\nare generally produced: (1) classification, in which a class\nlabel, cj \u2208 CTEST, is assigned to the jth predicted object\nin the image, and CTEST refers to the category vocabulary\ndesired at inference time; (2) localisation, with bounding\nbox coordinates, bj \u2208 R4, denoting the location of the jth\npredicted object. In accordance with the setting introduced\nby Detic (Zhou et al., 2022), two datasets are used at training\ntime: a detection dataset, DDET, containing bounding box\ncoordinates, class labels and associated images, addressing\na category vocabulary, CDET; and an image classification\ndataset, DIMG, containing images with class labels only,\naddressing a category vocabulary, CIMG. In the most general\ncase there are no restrictions on the overlap or lack thereof\nbetween the sets CTEST, CDET and CIMG.\nArchitecture Overview. In this work, we make use of a\npopular multi-stage detector based on CenterNet2 (Zhou\net al., 2021) as done in Detic. This detector, with outputs\n{cj, bj}M\nj=1, can be formulated as (for simplicity we con-\nsider the two-stage variant below):\n{fj}M\nj=1 = \u03a6ROI \u25e6 \u03a6PG \u25e6 \u03a6ENC (I)\n(1)\n{bj, cj}M\nj=1 = {\u03a6BBOX (fj) , \u03a6CLS \u25e6 \u03a6PROJ (fj)}M\nj=1\n(2)\nwhere, each input image is first sequentially processed by a\nset of operations: an image encoder (\u03a6ENC); a proposal gen-\nerator (\u03a6PG); a region-of-interest (RoI) feature pooling mod-\nule (\u03a6ROI), yielding a set of RoI features, {fj}M\nj=1. The RoI\nfeatures are processed by a bounding box module (\u03a6BBOX)\nto infer position of objects, {bj}M\nj=1. Additionally, the RoI\nfeatures are processed by a classification module, consisting\nof a linear projection (\u03a6PROJ), and C classification vectors\nor classifiers (\u03a6CLS), yielding a set of class labels, {cj}M\nj=1\n(C is the size of the category vocabulary).\nIn closed-vocabulary object detection all parameters listed\nabove are learnt during training on DDET. While in the open-\nvocabulary scenario, the classifiers (\u03a6CLS) are not learnt\nduring training but are instead generated separately from\nan alternative source, e.g. a pre-trained text encoder. This\nallows CTEST \u0338= CDET, as the classifiers, \u03a6CLS, for a specific\n3\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n4\nWhat \ndoes a \ndog look \nlike?\nWhat \ndoes a \ncat look \nlike?\nText Completion\nLLM \u2746\nA dog is a four-legged \nmammal with pointed \nears, a long nose, and \na bushy tail.\nA cat typically has four \nlegs, a tail, fur, and \nwhiskers.\nVLM Text\nEncoder \u2746\nRich \ndescriptions \ngive explicit \nclass attributes \nand visual \nfeatures\nA dog is is a four-\nlegged mammal with \nfur.\nMost dogs have four \nlegs, a tail, and a head \nwith furry ears.\nA cat is a four-legged \nanimal with retractable \nclaws and a flexible \nspine.\nA cat has eyes are \nlarge and round, with a \ndistinctive vertical slit \nfor a pupil.\nVector Mean\nVector Mean\nMean \nembedding \nused as \ntext-based\nclassifier \nfor OVD\nFigure 2. Generating powerful text-based classifiers. A LLM (GPT-3)\nis used to generate multiple rich descriptions of the class of interest.\nThese descriptions are then encoded with the CLIP (Radford et al.,\n2021) VLM text encoder. The descriptions are more informative than\nthe simple phrases, such as \u201c(a photo of) a dog\u201d or \u201c(a photo of) a cat\u201d,\nused in previous work such as Detic and ViLD. Additional examples of\nclass descriptions are given in the Appendix (Section F).\n\u201cdalmatian\u201d\nexemplars\nTransformer Encoder\nN x\nLearnt\n[CLS]\ntoken\nImage\nEncoder\n\u2746\nOutput [CLS] \ntoken used as \nvision-based\nclassifier for \nOVOD\nOne \nEmbedding\nPer Image \nExemplar\nImage\nEncoder\n\u2746\nImage\nEncoder\n\u2746\nImage\nEncoder\n\u2746\nImage\nEncoder\n\u2746\nFigure 3. Generating an OVOD vision-based classifier from a\nset of image exemplars. A stack of transformer blocks is used\nto combine embeddings of multiple exemplars belonging to the\nsame category.\nset of user defined classes can be generated at inference time.\nIn the following sections, we describe different options for\nconstructing such classifiers: from natural language, from\nimage exemplars, or from a combination of the two.\n3.2. Text-based Classifiers from Language Descriptions\nExisting OVOD approaches, e.g. Detic (Zhou et al., 2022)\nand ViLD (Gu et al., 2022), make use of simple text-based\nclassifiers by encoding the category name with a man-\nual prompt, e.g. a photo of a(n) {class name}\nor a(n) {class name}, using an appropriate encoder\n\u2014 e.g. a CLIP text encoder, thereby yielding a set of clas-\nsifiers for CTEST. This method relies on the text encoder\nto produce a text-based classifier entirely from its internal\nunderstanding of class name.\nInstead, we make use of natural language descriptions of cat-\negories sourced from a large language model (LLM). Such\na design choice gives additional details like visual attributes,\nleading to increased discriminative information in the clas-\nsifier. This alleviates lexical confusion \u2014 class name\nmay have two different meanings, and effectively prevents\nthe need for human efforts to manually write descriptions\nor spend time searching external sources for them.\nFigure 2 outlines our method for generating informa-\ntive text-based classifiers.\nSpecifically, we start by\nprompting an autoregressive language model with a\nquestion: \u201cWhat does a(n) {class name} look\nlike?\u201d, and sample multiple descriptions per class. We\nuse OpenAI\u2019s API for GPT-3 (Brown et al., 2020) and gener-\nate 10 descriptions per class with temperature sampling (Fig-\nure 2 shows 3 descriptions per class for clarity, more exam-\nples can be found in Section F of the Appendix), yielding\nmultiple descriptions of the format {class name} is\na ... or similar. Given a set of M plain text descriptions\n{sc\ni}M\ni=1 for class c, we encode each element of the set with\na CLIP text encoder (Radford et al., 2021), fCLIP-T(\u00b7), and\nthe text-based classifier for class c is obtained from the mean\nof these text encodings:\nwc\nTEXT = 1\nM\nM\nX\ni=1\nfCLIP-T (sc\ni)\n(3)\nAt detector training time, text-based classifiers for cate-\ngories of interest (c \u2208 CDET and c \u2208 CIMG) are pre-computed\nand kept frozen, the rest of detector parameters are updated\ni.e. all parameters in Equations 1-2, except \u03a6CLS. At infer-\nence time, classifiers for testing categories are computed\nsimilarly to enable open-vocabulary object detection.\nDiscussion. In this paper, we only consider a single straight-\nforward question as a prompt to the LLM: \u201cWhat does\na(n) {class name} look like?\u201d. However, it is\nalso feasible to use alternative question prompts, e.g. \u201cHow\ncan you identify a(n) {class name}?\u201d\nor\n\u201cDescribe what a(n) {class name} looks\nlike?\u201d, and obtain visual descriptions with the same or\nsimilar concepts (Pratt et al., 2022).\nWe investigated using a transformer architecture to aggre-\ngate text embeddings from natural language descriptions,\nbut we found this was not beneficial to OVOD performance\nover simply using the mean vector. In general, each text\nembedding of a natural language description summarises\nthe category of interest very well and so the contrastive task,\n4\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n5\nwhich is used to train the aggregator (explained in detail\nfor the visual case below), is very easy with text embed-\ndings yielding no improvement in text-based classifiers for\nOVOD.\n3.3. Vision-based Classifiers from Image Exemplars\nIn addition to constructing classifiers with natural language\ndescriptions, another natural option is to use image exem-\nplars, especially in cases where a good description of the\ncategory is prohibitively long (such as the painted lady but-\nterfly or Vanessa cardui which has an intricate wing pattern),\nor occasionally the class name is not known beforehand,\ne.g. \u201cDeerstalker cap\u201d refers to the hat often worn by Sher-\nlock Holmes.\nIn such scenarios, we propose to construct classifiers by\nusing image exemplars, as shown in Figure 3. Specifically,\ngiven a set of K RGB image exemplars for category c,\n{xc\ni}K\ni=1, we encode each exemplar with a CLIP visual en-\ncoder, fCLIP-IM(\u00b7), yielding K image embeddings, which\nare then passed to a multi-layer Transformer (Vaswani et al.,\n2017) with learnable [CLS] token, tCLS:\nwc\nIMG = Transformer\n\u0010\n{fCLIP-IM (xc\ni)}K\ni=1 ; tCLS\n\u0011\n(4)\nThe Transformer architecture acts to best aggregate the K\nimage exemplars, and the output from the [CLS] token is\nused as the vision-based classifier for OVOD. When train-\ning the transformer aggregator, all exemplars are sourced\nfrom ImageNet-21k-P (Ridnik et al., 2021). When gener-\nating vision-based classifiers for OVOD, where possible,\nwe source our exemplars from ImageNet-21k (Deng et al.,\n2009) \u2014 where this is not possible, we use LVIS and/or\nVisualGenome training data to source image exemplars. Ad-\nditional details on how we collate exemplars for training\nand testing are provided in the Appendix (Section D). This\ntransformer architecture will be referred to as the visual\naggregator and its training procedure is described next.\nOffline Training. The visual aggregator is trained offline\ni.e. it is not updated during detector training. The training\nprocedure needs to learn an aggregator which combines\nmultiple image exemplars to produce effective vision-based\nclassifiers for OVOD \u2014 a classifier for a given class needs\nto be discriminative w.r.t. other classes. A CLIP image\nencoder is used to provide initial embeddings for each ex-\nemplar. We keep the CLIP image encoder frozen during\ntraining to improve training efficiency and prevent catas-\ntrophic forgetting in the CLIP representation. To provide\ndiscriminative vision-based classifiers, contrastive learning\nis utilised. For a given class, the output embedding from\nthe visual aggregator is trained to minimise similarity with\nthe output embedding from other classes and maximise the\nsimilarity with an output embedding from the same class.\nTo do this, the contrastive InfoNCE (van den Oord et al.,\n2018) loss is used. The visual aggregator should generalise\nwell and not be trained for a specific downstream OVOD\nvocabulary, therefore it is trained with the ImageNet-21k-P\ndataset (Ridnik et al., 2021) for image classification, which\ncontains \u223c11M images across \u223c11K classes. For category\nc during visual aggregator training, at each iteration, two\ndistinct sets of K exemplars are sampled, augmented and\nencoded by the frozen CLIP image encoder. The two sets\nare input separately to visual aggregator, outputting 2 em-\nbeddings from the learnable [CLS] token for class c. Given\na batch size B, the InfoNCE contrastive loss ensures sets\nformed from the same class have similar embeddings and\nthose of different classes are separated. Once trained, the\nvisual aggregator and visual encoder are frozen and provide\nvision-based classifiers for categories in CDET \u222a CIMG/CTEST\nduring detector training/testing. Additional details on how\nthe visual aggregator is trained are provided in the Appendix\n(Section C).\nDiscussion. Using image exemplars for open-vocabulary\ndetection may share some similarity to few-shot object detec-\ntion, however, there is a key distinction. In few-shot object\ndetection, the given \u201cnovel/rare\u201d annotations (albeit few) are\navailable for training, e.g. recent works have found that fine-\ntuning a pre-trained object detector on few-shot detection\ndata yields the best results (Wang et al., 2020; Qiao et al.,\n2021; Kaul et al., 2022), while in open-vocabulary detec-\ntion, there are no bounding box annotations for \u201cnovel/rare\u201d\ncategories. Image exemplars (i.e. the wholes image without\nbounding boxes) are used to specify the categories of inter-\nest; we do not update any parameters based on \u201cnovel/rare\u201d\ncategory bounding box data, unlike in few-shot object de-\ntection.\nDuring ablation experiments in Section A, we compare the\nvisual aggregator to a simple mean operator, when obtain-\ning a vision-based classifier from multiple image exemplar\nembeddings, and show the benefit of the trained aggregator\nin this case.\n3.4. Constructing Classifiers via Multi-Modal Fusion\nTo go one step further, a natural extension to the aforemen-\ntioned methods is to construct classifiers from multi-modal\ncues; intuitively, natural language descriptions and image\nexemplars may contain complementary information. For a\ngiven class, c, with text-based classifier, wc\nTEXT, and vision-\nbased classifier, wc\nIMG, the multi-modal classifier, wc\nMM, is\ncomputed by a simple fusion method based on addition:\nwc\nMM =\nwc\nTEXT\n\u2225wc\nTEXT\u22252\n+\nwc\nIMG\n\u2225wc\nIMG\u22252\n(5)\nFigure 1 demonstrates our entire pipeline for generating text-\nbased, vision-based and multi-modal classifiers, showing\nhow any of the three classifiers can be used with an open-\nvocabulary detector to detect a \u201cfalcon\u201d.\n5\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n6\nDiscussion. Section 3.3 provides details of the visual ag-\ngregator, which yields our vision-based classifiers, but for\nmulti-modal classifiers we simply compute the vector sum\nof our l2-normalised text-based and vision-based classi-\nfiers. We investigated using a unified multi-modal aggrega-\ntor which ingests both text and visual embeddings, sourced\nfrom class descriptions and image exemplars, respectively.\nSuch a model did not generate good multi-modal classifiers\nfor OVOD \u2014 distinguishing between sets of text and im-\nage embeddings for different classes becomes trivial as the\ntext embeddings alone are sufficient to solve the contrastive\nlearning task, thereby ignoring the visual embeddings alto-\ngether. Attempts to modify the training for a unified multi-\nmodal aggregator by using Dropout (Srivastava et al., 2014)\non the text embeddings were not fruitful.\n4. Experiments\nIn this section, we first introduce the standard dataset and\nbenchmark used in the literature (Gu et al., 2022; Zhou et al.,\n2022; Feng et al., 2022). Section 4.2 provides implementa-\ntion and training details for our OVOD models, which use\nclassifiers constructed from natural language descriptions,\nvisual exemplars or the combination of both. We compare\nour models with existing works in Section 4.3, demonstrat-\ning state-of-the-art performance. Additionally, Section 4.3\nprovides results for cross-dataset transfer. Section 4.4 pro-\nvides an ablation study regarding our design choices.\n4.1. Datasets and Evaluation Protocol\nStandard LVIS Benchmark. In this work, most experi-\nments are based on the LVIS object detection dataset (Gupta\net al., 2019), containing a large vocabulary and a long-\ntailed distribution of object instances.\nSpecifically, the\nLVIS dataset contains class, bounding box and mask annota-\ntions for 1203 classes across 100k images in the MS-COCO\ndataset (Lin et al., 2014). Annotations are collected in a\nfederated manner, i.e. manual annotations for a given image\nare not necessarily exhaustive. The classes are divided into\nthree sets \u2014 rare, common and frequent \u2014 based on the\nnumber of training images containing a given class.\nTraining Datasets. To develop open-vocabulary object de-\ntectors, we follow the same setting as proposed in ViLD (Gu\net al., 2022) and used in Detic (Zhou et al., 2022). Specif-\nically, the original LVIS training set (LVIS-all) is slightly\nchanged by removing all annotations belonging to the \u201crare\u201d\ncategories. This removes the annotations of 317 rare classes\nbut not the associated images, in other words, objects be-\nlonging to rare categories appear in the training set but are\nunannotated. This subset of LVIS training data containing\nonly \u201ccommon\u201d and \u201cfrequent\u201d annotations is referred to as\nLVIS-base. LVIS-base serves as DDET using notation from\nSection 3.1, unless stated otherwise. When using image\nclassification data, DIMG, as extra weak supervision, we use\nthe subset of categories in ImageNet-21K (Deng et al., 2009)\nthat overlap with the LVIS vocabulary and denote this subset\nas IN-L, as in Detic. IN-L covers 997 of the 1203 classes in\nthe LVIS vocabulary.\nEvaluation Protocol. For evaluation, previous work evalu-\nates OVOD models on the LVIS validation set (LVIS-val) for\nall categories \u2014 treating \u201crare\u201d classes as novel categories as\nit is guaranteed that no groundtruth box annotations whatso-\never are provided at the training stage. The main evaluation\nmetric is the standard mask AP metric averaged over the\n\u201crare\u201d classes and is denoted as APr. The mask AP averaged\nacross all classes is also reported, indicating overall class\nperformance and is denoted as mAP. The latter metric is an\nimportant consideration as a good model should improve\nboth APr and mAP; a model should not improve APr at the\ncost of worse performance in terms of mAP.\n4.2. Implementation Details\nObject Detector Architecture. The architecture we use is\nalmost identical to that in Detic, using the CenterNet2 (Zhou\net al., 2021) model with a ResNet-50 backbone (He et al.,\n2016) pre-trained on ImageNet-21k-P (Ridnik et al., 2021).\nIn addition to exploring different ways for constructing\nclassifiers (\u03a6CLS), as described in Section 3, we also add\na learnable bias before generating final confidence scores,\nthe effect of this bias term is investigated in the Appendix\n(Section A).\nDetector Training. The training recipe is the same as Detic\nfor fair comparison, using Federated Loss (Zhou et al., 2021)\nand repeat factor sampling (Gupta et al., 2019). While train-\ning our OVOD model on detection data only, DDET, we use a\n4\u00d7 schedule (\u223c58 LVIS-base epochs or 90k iterations with\nbatch size of 64). When using additional image-labelled\ndata (IN-L), we train jointly on DDET \u222a DIMG using a 4\u00d7\nschedule (90k iterations) with a sampling ratio of 1 : 4 and\nbatch sizes of 64 and 256, respectively. This results in \u223c15\nIN-L epochs and an additional \u223c11 LVIS-base epochs. For\nmini-batches containing images from DDET and DIMG we\nuse input resolutions of 6402 and 3202, respectively. We\nconduct our experiments on 4 32GB V100 GPUs.\nFor image-labelled data, DIMG, an image with class label is\ngiven, but no groundtruth bounding box is available. Fol-\nlowing Detic, the largest class-agnostic box proposal is used\nto produce an RoI feature for the given image, enabling\ndetector training. See the Detic paper for more details.\nText-based Classifier Construction. To generate plain text\nclass descriptions we use the GPT-3 DaVinci-002 model\navailable from OpenAI. For each class in LVIS, we generate\n10 descriptions and compute the classifier with the text\nencoder from a CLIP ViT-B/32 model (Radford et al., 2021),\nas detailed in Section 3.2. We follow the standard method\n6\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n7\nfrom CLIP and use the output embedding corresponding to\nthe final token in the input text.\nVision-based Classifier Construction. The visual aggre-\ngator, detailed in Section 3.3, should be general and not\nspecific to any class vocabulary. To fulfil this goal, we use\nthe curated ImageNet-21-P dataset (Ridnik et al., 2021) as\ntraining data for the aggregator. This dataset, designed\nfor pre-training visual backbones, filters out classes with\nfew examples from the original ImageNet-21k (Deng et al.,\n2009) dataset, leaving \u223c11M images across \u223c11K classes.\nTo generate a visual embedding from a given image ex-\nemplar, we use a CLIP ViT-B/32 visual encoder. For the\naggregator, we use N = 4 transformer blocks, with dimen-\nsion 512 (the same as the output dimension of the CLIP\nvisual encoder) and a multilayer perceptron dimension of\n2048. Comprehensive details of aggregator training is pro-\nvided in the Appendix (Section C). To test the effectiveness\nof our visual aggregator, we generate baseline vision-based\nclassifiers by taking the vector mean of the CLIP visual\nembeddings from the K image exemplars.\nWhen constructing vision-based classifiers for OVOD, we\nfind making use of test-time augmentation (TTA) improves\nperformance. Note, TTA here refers to augmentation of the\nimage exemplars used to build vision-based classifiers not\ntest-time augmentation of the test image on which OVOD\nis performed. In our work, each image exemplar is aug-\nmented 5 times and input separately to the visual encoder.\nTherefore, given K image exemplars, we generate 5K vi-\nsual embeddings to be ingested by our visual aggregator.\nMore details on the use of TTA in constructing vision-based\nclassifiers are found in the Appendix (Section A).\nFor total clarity regarding when datasets are used \u2014 the\nvisual aggregator is trained using ImageNet-21k-P (Ridnik\net al., 2021) and the vision-based classifiers for OVOD are\ngenerated from relevant image exemplars using the trained\naggregator (see Section D of the Appendix for details on\nsourcing the image exemplars). Detection data (e.g. LVIS-\nbase) is only used to train the open-vocabulary object detec-\ntor and image-level data (e.g. IN-L) may be used as an extra\nsource of weak supervision as in Detic (Zhou et al., 2022).\nMulti-Modal Classifier Construction. When computing\nmulti-modal classifiers, we simply compute the category-\nwise l2-normalised classifier from each modality, regardless\nof method used to compute them, and take the vector sum.\nIn all cases the text-based classifiers are sourced from class\ndescriptions as described in Section 3.2. We combine our\ntext-based classifiers with our vision-based classifiers, using\nthe trained visual aggregator. However, once again, to test\nthe effectiveness of our visual aggregator, we also combine\nour text-based classifiers with the baseline vision-based\nclassifiers described in the previous paragraph.\nModel\nBackbone\nExtra\nAPr\nmAP\nData\nViLD (Gu et al., 2022)\nResNet-50\n16.1\n22.5\nDetic (Zhou et al., 2022)\nResNet-50\n16.3\n30.0\nViLD-ens (Gu et al., 2022)\nResNet-50\n16.6\n25.5\nOV-DETR (Zang et al., 2022)\nResNet-50 + DETR\n17.4\n26.6\nF-VLM (Kuo et al., 2022)\nResNet-50\n\u2717\n18.6\n24.2\nOurs (Text-Based)\n19.3\n30.3\nOurs (Vision-Based)\n18.3\n29.2\nOurs (Multi-Modal)\nResNet-50\n\u2717\n19.3\n30.6\nRegCLIP (Zhong et al., 2022)\nResNet-50\nCC3M\n17.1\n28.2\nOWL-ViT (Minderer et al., 2022)\u2020\nViT-B/32\nLiT\n19.7\n23.3\nDetic (Zhou et al., 2022)\nResNet-50\nIN-L\n24.6\n32.4\nOurs (Text-Based)\n25.8\n32.7\nOurs (Vision-Based)\n23.8\n31.3\nOurs (Multi-Modal)\nResNet-50\nIN-L\n27.3\n33.1\nFully-Supervised (Zhou et al., 2022)\nResNet-50\n\u2717\n25.5\n31.1\nTable 1. Detection performance on the LVIS Open Vocabulary\nDetection Benchmark using our three types of classifier compared\nwith previous works. Best and second-best performing models are\ncoloured blue and red, respectively. We split models into those\nwhich only use LVIS-base as training data (top) and those which\nuse additional image-level data (bottom). Furthermore, we show\nresults for a fully-supervised model from Detic trained on LVIS-all\nin grey. \u2020 OWL-ViT reports bbox AP metrics and was trained\non Objects365 (Shao et al., 2019) and VisualGenome (Krishna\net al., 2017) not LVIS-base, therefore it is possible LVIS-defined\n\u201crare\u201d classes are contained in the detection training data of OWL-\nViT. Due to limited compute resources we present and compare\nto models which use ResNet-50 (He et al., 2016) backbones or\nsimilar. We report mask AP metrics except for \u2020.\n4.3. Open-Vocabulary Detection Results\nLVIS OVOD Benchmark. Table 1 shows results on LVIS-\nval for our work, which uses text-based, vision-based and\nmulti-modal classifiers, compared to a range of prior work.\nWe report overall mask AP performance and mask AP for\n\u201crare\u201d classes only. The latter metric is the key measure\nof OVOD performance. We separate the comparisons into\nthose models which do not use additional image-level data\n(top half of Table 1) and those which do (bottom half of\nTable 1). For a fair evaluation, we compare to models from\nprior works which use a ResNet-50 (He et al., 2016) back-\nbone. There are two exceptions: (1) OWL-ViT (Minderer\net al., 2022) which only investigates using Vision Trans-\nformers for OVOD (Dosovitskiy et al., 2021) \u2013 we compare\nResNet-50 models to the ViT-B/32 OWL-ViT model as it re-\nquires similar compute during inference in terms of GLOPs\n(141.5 and 139.6, respectively); (2) OV-DETR (Zang et al.,\n2022) which uses a DETR-style architecture (Carion et al.,\n2020) consisting of a ResNet-50 CNN backbone and modi-\nfied transformer encoder and decoder.\nIn the experiments without using extra data (DIMG = \u2205),\nour models with text-based or multi-modal classifiers obtain\nthe best performance on both APr and overall mAP, while\nF-VLM and Detic only performs strongly on APr and mAP,\n7\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n8\nModel\nExtra\nObjects365\nObjects365\nObjects365\nData\nmAP\nAP50\nAPr\nDetic (Zhou et al., 2022)\n\u2717\n13.9\n19.7\n9.5\nOurs (Text-Based)\n14.8\n21.0\n10.1\nDetic (Zhou et al., 2022)\nIN-L\n15.6\n22.2\n12.4\nOurs (Text-Based)\n16.6\n23.1\n13.1\nTable 2. Detection performance when training on LVIS-all (i.e. all\nLVIS training data) and evaluating on Objects365 (Shao et al.,\n2019), where the least frequent 1\n3 of classes are defined as \u201crare\u201d.\nBest performing models are coloured blue. Our text-based clas-\nsifiers outperform Detic when transferring to Objects365 across\nall classes and \u201crare\u201d classes only. Note the Detic (Zhou et al.,\n2022) models we compare to on Objects365 are not the same as\nthe models listed in the Detic paper, which use a large Swin-B\nbackbone and all of ImageNet-21k as extra data for weak supervi-\nsion. We use a fair comparison Detic model which uses the same\ntraining data (see text for more details). We report box AP metrics\nfor Objects365.\nrespectively. Our model with text-based classifiers is most\ndirectly comparable to Detic and our model outperforms\nDetic by 3.0 APr. When using extra data, we make use of a\nImageNet-21k subset as in Detic (IN-L). Models with text-\nbased and multi-modal classifiers outperform Detic (previ-\nous state-of-the-art) by 1.2 and 2.7 APr respectively. Note\nthat, our models trained with IN-L even outperform the\nfully-supervised baseline using CenterNet2, i.e. trained on\n\u201crare\u201d class box annotations. To the best of our knowl-\nedge, this is the first work on open-vocabulary detection that\noutperforms a comparable fully-supervised model on the\nchallenging LVIS benchmark. Note, some other works use\nlarger vision backbones, e.g. Swin-B (Liu et al., 2021), but\ndue to limited computation resources we only present and\ncompare to models with ResNet-50 backbones or similar.\nOur models with text-based and multi-modal classifiers sur-\npass state-of-the-art performance when using a ResNet-50\nbackbone or similar.\nCross-dataset Transfer. Table 2 shows results for cross-\ndataset transfer from LVIS to Objects365 (Shao et al., 2019)\nwhen using our text-based classifiers. We compare our\nwork to equivalent models from Detic, reporting box AP\nmetrics as standard in Objects365. In all cases, these mod-\nels are trained on LVIS-all and the models in the bottom\ntwo rows use IN-L as extra weak supervision. The trained\nopen-vocabulary detectors are evaluated on the Objects365\nvalidation set. Following Detic, we define \u201crare\u201d classes in\nObjects365 as the 1\n3 of classes with the lowest frequency\nin the Objects365 training set. Note the Detic models we\ncompare to in Table 2 are not the same as those listed in\nTable 4 of the Detic paper, which use a large Swin-B back-\nbone and all of ImageNet-21k as extra data. Instead, we\ncompare to other Detic models publicly available which\nare trained on LVIS-all (and IN-L). For ease, we provide\nlinks to the Detic configuration and checkpoint files used\nin this cross-dataset transfer evaluation 1 2. Evaluation on\nObjects365 after training on LVIS is easily done with Detic\nor with our text-based classifiers. In the case of Detic, the\nsimple classifiers based on LVIS class names are replaced\nwith equivalent simple classifiers based on Objects365 class\nnames. For our text-based classifiers, plain text descrip-\ntions are generated for each of the Objects365 classes and\nare encoded as described in Section 3.2 and replace the\ntext-based classifiers for LVIS. In both models, Detic and\nours, all other parameters of the open-vocabulary detector\nremain the same. In all cases, using our text-based classi-\nfiers gives performance improvements over the equivalent\nDetic model. Considering all classes, our method with extra\ndata outperforms Detic by 1.0 mAP and 0.9 AP50. For\n\u201crare\u201d classes as defined above, our method with extra data\noutperforms the equivalent Detic model by 0.7 APr. These\nresults demonstrate that our method, which uses text-based\nclassifiers generated from rich class descriptions, provides\nadditional information compared to using a simple classifier\nbased on the class name only, even when the training and\ntesting class vocabularies are disjoint.\n4.4. Ablation Study\nResults with Vision-Based Classifiers. Using vision-based\nclassifiers for OVOD is an under-explored area and so we\ncompare our method, detailed in Section 3.3, to baseline\nclassifiers in which the same visual encoder is used, but the\naction of the aggregator is replaced by performing simple\nvector mean. The orange rows of Table 3 compares the\nuse of our visual aggregator to performing simple vector\nmean across visual embeddings instead. When no additional\nimage-level data is used (top two orange rows) our aggrega-\ntor (Model B) boosts performance by 3.5 APr compared to\nthe vector mean baseline (Model A). For models which train\non additional image-level data (IN-L) our aggregator (Model\nG) boosts performance by 2.2 APr compared to the baseline\n(Model F). This comparison demonstrates the utility of our\nvisual aggregator in constructing better vision-based classi-\nfiers rather than na\u00a8\u0131vely averaging the K visual embeddings.\nNote our vision-based classifiers and the baseline classifiers\nboth utilise TTA as mentioned in Section 4.2 (see Section A\nof the Appendix for details on TTA). The results in Table 3\nuse K = 5. Further results for K = 1, 2, 10 are found in\nthe Appendix (Section E).\nResults with Multi-Modal Classifiers. To evaluate the\neffectiveness of multi-modal classifiers we perform similar\nexperiments as those using vision-based classifiers, except\nfor each model we combine the vision-based classifiers with\nthe text-based classifiers, as described in Section 3.4. Re-\nsults for multi-modal classifiers are shown in grey rows of\nTable 3. With no additional image-level data, the vector\n1Detic Configuration Checkpoint Extra Data: \u2717\n2Detic Configuration Checkpoint Extra Data: IN-L\n8\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n9\nvulture.n.01\npuffin.n.01\nferret.n.01\nbarbell.n.01\nfishbowl.n.02\nFigure 4. Some qualitative detection examples using our model with text-based classifiers, detecting \u201crare\u201d category instances in LVIS-val.\nOur text-based classifiers are sourced from rich natural language descriptions of a given class by prompting an GPT-3 LLM.\nVisual\nVisual\nText\nExtra\nModel\nMean?\nAgg.?\nCls.?\nData?\nAPr\nmAP\nA\n\u2713\n14.8\n28.8\nB\n\u2713\n\u2717\n18.3\n29.2\nC\n\u2713\n\u2717\n19.3\n30.3\nD\n\u2713\n\u2713\n20.7\n30.5\nE\n\u2713\n\u2713\n\u2717\n19.3\n30.6\nF\n\u2713\n21.6\n31.3\nG\n\u2713\nIN-L\n23.8\n31.3\nH\n\u2713\nIN-L\n25.8\n32.7\nI\n\u2713\n\u2713\n26.5\n32.8\nJ\n\u2713\n\u2713\nIN-L\n27.3\n33.1\nTable 3. Detection performance on the LVIS OVOD benchmark\ncomparing all three of our methods: (1) orange \u2014 vision-based\nclassifiers; (2) blue \u2014 text-based classifiers; (3) grey \u2014 multi-\nmodal classifiers. Results for models trained only on LVIS-base\nand LVIS-base+IN-L are shown in the top and bottom halves,\nrespectively. Visual Mean?: simple vector mean is used to combine\nvisual embeddings of image exemplars, Visual Agg.?: our visual\naggregator is used to combine visual embeddings, Text Cls.?:\ntext-based classifiers are used. Models which use text-based and\nvision-based classifiers represent our models with multi-modal\nclassifiers. We report mask AP metrics.\nmean baseline (Model D) outperforms the use of our ag-\ngregator (Model E) by 1.4 APr. However, for models with\nimage-level data (IN-L) our aggregator (Model J) boosts\nperformance by 0.8 APr compared to the baseline (Model I).\nFurthermore, comparing the multi-modal classifiers (grey\nrows in Table 3) with text-based classifiers (blue rows in\nTable 3) demonstrates that in all cases adding information\nfrom image exemplars yields improved OVOD performance\n\u2014 our best multi-modal model improves performance over\nour best text-based model by 1.5 APr confirming that com-\nbining the vision and text modalities utilises complementary\ninformation between the two.\nRelationship between IN-L and LVIS \u201crare\u201d classes. Sec-\ntion B of the Appendix splits the APr metric into two based\non the \u201crare\u201d LVIS categories contained in IN-L. One may\nexpect improvements in APr performance when training on\nIN-L to only come from \u201crare\u201d categories found in IN-L.\nOur evaluation finds this not to be the case. Detailed results\ncan be found in the Appendix (Section B) which breaks the\nAPr metric into \u201crare\u201d categories found in IN-L and those\nnot.\nAdditional Ablation Experiments. Section A of the Ap-\npendix presents ablation experiments which demonstrate:\n(1) applying a learnable bias before calculating the final de-\ntection score for a region improves OVOD performance; (2)\nimprovements in OVOD performance using our text-based\nclassifiers is orthogonal to applying this learnable bias; (3)\napplying TTA on image exemplars yield better vision-based\nclassifiers for OVOD; (4) comparisons between our text-\nbased classifiers and those generated from manual prompts.\nPlease refer to Section A of the Appendix for details and\nevaluation results for these experiments.\n5. Conclusion\nIn this paper, we tackle open-vocabulary object detection\nby investigating the importance of the method used to gen-\nerate classifiers for open-vocabulary detection. This work\ngoes beyond the very simple methods used in prior work\nto generate such classifiers \u2014 with the class name only.\nWe present a novel method which combines a large lan-\nguage model (LLM) and a visual-language model (VLM)\nto produce improved classifiers. Moreover, we investigate\nusing image exemplars to provide classifiers for OVOD\nand present a method for generating such classifiers us-\ning a large classification dataset and a simple transformer\nbased architecture. Finally, we combine our classifiers from\nthe two modalities to produce multi-modal classifiers for\nOVOD. Our experiments show that our method using natural\nlanguage only outperforms current state-of-the-art OVOD\nworks, especially in cases where no extra image-level data\nis used. Furthermore, our multi-modal classifiers set new\nstate-of-the-art performance with a large improvement over\nprior work.\nAcknowledgements\nWe thank Lauren Bates-Brownsword for help with proof-\nreading. This research was supported by the EPSRC CDT\nin AIMS EP/L015897/1 and EP/S515541/1, the EPSRC\nProgramme Grant VisualAI EP/T028572/1, and a Royal\nSociety Research Professorship. Weidi Xie would like to\nacknowledge the National Key R&D Program of China (No.\n2022ZD0161400).\n9\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n10\nReferences\nBansal, A., Sikka, K., Sharma, G., Chellappa, R., and Divakaran,\nA. Zero-shot object detection. In Proceedings of the European\nConference on Computer Vision, September 2018.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A.,\nAgarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child,\nR., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen,\nM., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.\nLanguage models are few-shot learners. In Advances in Neural\nInformation Processing Systems, 2020.\nCai, Z. and Vasconcelos, N. Cascade r-cnn: Delving into high\nquality object detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 6154\u20136162.\nIEEE Computer Society, 2018.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and\nZagoruyko, S. End-to-end object detection with transformers.\nIn Proceedings of the European Conference on Computer Vision,\npp. 213\u2013229. Springer, 2020.\nChen, D.-J., Hsieh, H.-Y., and Liu, T.-L. Adaptive image trans-\nformer for one-shot object detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\npp. 12242\u201312251, 2021.\nChen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng,\nW., Liu, Z., Shi, J., Ouyang, W., et al. Hybrid task cascade for\ninstance segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 4974\u20134983,\n2019.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple\nframework for contrastive learning of visual representations.\nIn Proceedings of the International Conference on Machine\nLearning, 2020.\nChoudhury, S., Laina, I., Rupprecht, C., and Vedaldi, A. The curi-\nous layperson: Fine-grained image recognition without expert\nlabels. In Proceedings of the British Machine Vision Conference,\n2021.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2009.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai,\nX., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G.,\nGelly, S., Uszkoreit, J., and Houlsby, N. An image is worth\n16x16 words: Transformers for image recognition at scale. Pro-\nceedings of the International Conference on Learning Repre-\nsentations, 2021.\nElhoseiny, M., Zhu, Y., Zhang, H., and Elgammal, A. Link the head\nto the \u201dbeak\u201d: Zero shot learning from noisy text description\nat part precision. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017.\nFeng, C., Zhong, Y., Jie, Z., Chu, X., Ren, H., Wei, X., Xie, W.,\nand Ma, L. Promptdet: Towards open-vocabulary detection\nusing uncurated images. 2022.\nGirshick, R. B. Fast R-CNN. In Proceedings of the International\nConference on Computer Vision, 2015.\nGu, X., Lin, T.-Y., Kuo, W., and Cui, Y. Open-vocabulary detection\nvia vision and language knowledge distillation. In Proceedings\nof the International Conference on Learning Representations,\n2022.\nGupta, A., Dollar, P., and Girshick, R. LVIS: A dataset for large\nvocabulary instance segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for\nimage recognition. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016.\nHsieh, T.-I., Lo, Y.-C., Chen, H.-T., and Liu, T.-L. One-shot object\ndetection with co-attention and co-excitation. In Advances in\nNeural Information Processing Systems. 2019.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q.,\nSung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-\nlanguage representation learning with noisy text supervision.\nIn Proceedings of the International Conference on Machine\nLearning, pp. 4904\u20134916, 2021.\nKamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., and\nCarion, N. Mdetr-modulated detection for end-to-end multi-\nmodal understanding. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 1780\u20131790,\n2021.\nKang, B., Liu, Z., Wang, X., Yu, F., Feng, J., and Darrell, T. Few-\nshot object detection via feature reweighting. In Proceedings\nof the International Conference on Computer Vision, pp. 8420\u2013\n8429, 2019.\nKaul, P., Xie, W., and Zisserman, A. Label, verify, correct: A\nsimple few-shot object detection method. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n2022.\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J.,\nChen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual\ngenome: Connecting language and vision using crowdsourced\ndense image annotations. International Journal of Computer\nVision, 123(1):32\u201373, 2017.\nKuo, W., Cui, Y., Gu, X., Piergiovanni, A., and Angelova, A. F-\nvlm: Open-vocabulary object detection upon frozen vision and\nlanguage models. arXiv preprint arXiv:2209.15639, 2022.\nLaw, H. and Deng, J. Cornernet: Detecting objects as paired\nkeypoints.\nIn Proceedings of the European Conference on\nComputer Vision, 2018.\nLi, L. H., Zhang*, P., Zhang*, H., Yang, J., Li, C., Zhong, Y., Wang,\nL., Yuan, L., Zhang, L., Hwang, J.-N., Chang, K.-W., and Gao,\nJ. Grounded language-image pre-training. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\n2022.\nLi, Y., Chen, Y., Wang, N., and Zhang, Z. Scale-aware trident net-\nworks for object detection. In Proceedings of the International\nConference on Computer Vision, 2019.\n10\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n11\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan,\nD., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft coco: Common\nobjects in context. In Proceedings of the European Conference\non Computer Vision, 2014.\nLin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll\u00b4ar, P. Focal loss\nfor dense object detection. In Proceedings of the International\nConference on Computer Vision, 2017.\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-\nY., and Berg, A. C. Ssd: Single shot multibox detector. In\nProceedings of the European Conference on Computer Vision,\npp. 21\u201337. Springer, 2016.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and\nGuo, B. Swin transformer: Hierarchical vision transformer\nusing shifted windows. In Proceedings of the International\nConference on Computer Vision, pp. 10012\u201310022, October\n2021.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regular-\nization. In Proceedings of the International Conference on\nLearning Representations, 2018.\nMenon, S. and Vondrick, C. Visual classification via description\nfrom large language models. Proceedings of the International\nConference on Learning Representations, 2023.\nMiller, G. A. Wordnet: A lexical database for english. Commu-\nnications of the Association for Computing Machinery, 38(11):\n39\u201341, nov 1995.\nMinderer, M., Gritsenko, A., Stone, A., Neumann, M., Weis-\nsenborn, D., Dosovitskiy, A., Mahendran, A., Arnab, A., De-\nhghani, M., Shen, Z., Wang, X., Zhai, X., Kipf, T., and Houlsby,\nN. Simple open-vocabulary object detection. In Proceedings\nof the European Conference on Computer Vision, pp. 728\u2013755,\n2022.\nOsokin, A., Sumin, D., and Lomakin, V. OS2D: One-stage one-\nshot object detection by matching anchor features. In Proceed-\nings of the European Conference on Computer Vision, 2020.\nPratt, S., Liu, R., and Farhadi, A. What does a platypus look like?\ngenerating customized prompts for zero-shot image classifica-\ntion. arXiv preprint arXiv:2209.03320, 2022.\nQiao, L., Zhao, Y., Li, Z., Qiu, X., Wu, J., and Zhang, C. Defrcn:\nDecoupled faster r-cnn for few-shot object detection. In Pro-\nceedings of the International Conference on Computer Vision,\npp. 8681\u20138690, October 2021.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agar-\nwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.\nLearning transferable visual models from natural language su-\npervision. In Proceedings of the International Conference on\nMachine Learning, pp. 8748\u20138763, 2021.\nRedmon, J. and Farhadi, A. Yolov3: An incremental improvement.\narXiv preprint arXiv:1804.02767, 2018.\nRedmon, J., Divvala, S. K., Girshick, R. B., and Farhadi, A. You\nonly look once: Unified, real-time object detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016.\nRen, S., He, K., Girshick, R., and Sun, J. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In\nAdvances in Neural Information Processing Systems, 2016.\nRidnik, T., Ben-Baruch, E., Noy, A., and Zelnik, L. Imagenet-21k\npretraining for the masses. In Proceedings of the Neural Infor-\nmation Processing Systems Track on Datasets and Benchmarks,\n2021.\nShao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J.,\nand Sun, J. Objects365: A large-scale, high-quality dataset\nfor object detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 8429\u20138438,\n2019. doi: 10.1109/ICCV.2019.00852.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and\nSalakhutdinov, R. Dropout: A simple way to prevent neural\nnetworks from overfitting. In Journal of Machine Learning\nResearch, 2014.\nSun, B., Li, B., Cai, S., Yuan, Y., and Zhang, C. Fsce: Few-\nshot object detection via contrastive proposal encoding. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, June 2021.\nTan, M., Pang, R., and Le, Q. V. Efficientdet: Scalable and efficient\nobject detection. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2020.\nTian, Z., Shen, C., Chen, H., and He, T. Fcos: Fully convolutional\none-stage object detection. In Proceedings of the International\nConference on Computer Vision, 2019.\nvan den Oord, A., Li, Y., and Vinyals, O.\nRepresentation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all\nyou need. In NIPS, 2017.\nWang, X., Huang, T. E., Darrell, T., Gonzalez, J. E., and Yu, F.\nFrustratingly simple few-shot object detection. In Proceedings\nof the International Conference on Machine Learning, 2020.\nZang, Y., Li, W., Zhou, K., Huang, C., and Loy, C. C. Open-\nvocabulary detr with conditional matching. 2022.\nZareian, A., Rosa, K. D., Hu, D. H., and Chang, S.-F. Open-\nvocabulary object detection using captions. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp. 14393\u201314402, 2021.\nZhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L. H., Zhou,\nL., Dai, X., Yuan, L., Li, Y., and Gao, J. Regionclip: Region-\nbased language-image pretraining. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp.\n16793\u201316803, 2022.\nZhou, X., Wang, D., and Kr\u00a8ahenb\u00a8uhl, P. Objects as points. arXiv\npreprint arXiv:1904.07850, 2019.\nZhou, X., Koltun, V., and Kr\u00a8ahenb\u00a8uhl, P. Probabilistic two-stage\ndetection. arXiv preprint arXiv:2103.07461, 2021.\nZhou, X., Girdhar, R., Joulin, A., Kr\u00a8ahenb\u00a8uhl, P., and Misra, I.\nDetecting twenty-thousand classes using image-level supervi-\nsion. In Proceedings of the European Conference on Computer\nVision, pp. 350\u2013368, 2022.\n11\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n12\nA. Ablation Studies\nWe now ablate some of the key components using the open-vocabulary LVIS benchmark without any extra image classi-\nfication data i.e. DIMG = \u2205, unless stated otherwise. All metrics have the standard LVIS definition and we report mask\nAP metrics in all cases. For reference APr, APc and APf represent mean average precision across \u201crare\u201d, \u201ccommon\u201d and\n\u201cfrequent\u201d classes, respectively, as defined in LVIS. Moreover, mAP, AP50 and AP75 represent mean average precision\nacross all classes but for all intersection-over-union (IoU) criteria, IoU= 0.5 and IoU= 0.75, respectively.\nBias?\nInit. Value\nmAP\nAP50\nAP75\nAPr\nAPc\nAPf\n\u2717\nN/A\n29.7\n43.7\n31.6\n15.6\n30.5\n35.0\n\u2713\n-2.0\n29.9\n43.5\n32.0\n17.7\n30.1\n35.0\nTable 4. The effect of including a learnable bias on detections scores for OVOD. The top row does not use a learnable bias, as in Detic.\nThe bottom row applies a learnable bias prior to computing final detection scores with logistic sigmoid. Applying a learnable bias\nimproves performance on novel/rare categories (APr). We report mask AP metrics.\nEffect of Detection Score Bias. Table 4 shows the effect of adding a learnable bias to the detection scores before applying a\nlogistic sigmoid to get a final detection score in the range [0, 1] To evaluate the effect of the learnable bias only, our proposed\ntext-based classifiers sourced from rich class descriptions, as described in Section 3.2, are not used and instead the same\nsimple text-based classifiers used in Detic, of form \u201ca(n) {class name}\u201d, are used in this comparison. We observe\nadding a learnable bias improves open-vocabulary detection by 2.1 AP on rare categories compared to not using a bias, as\ndone in Detic. Without the use of a bias, class-agnostic proposals are not biased towards being labelled as background. With\nrespect to a given class, a proposal is most likely to be negative, therefore use of a bias makes intuitive sense to reflect this\nand stabilises early training of the detector. Similar findings were found in RetinaNet (Lin et al., 2017).\nModel\nmAP\nAP50\nAP75\nAPr\nAPc\nAPf\nDetic\n30.2\n44.2\n32.1\n16.4\n31.0\n35.4\nOurs (w/o bias)\n30.4\n44.4\n32.3\n18.6\n30.8\n35.2\nOurs (w/ bias)\n30.3\n44.2\n32.2\n19.3\n30.5\n35.0\nTable 5. The effect of using our text-based classifiers sourced from rich descriptions. In contrast, Detic uses simple classifiers based on\nclass names only (top row). Results for a detector trained using our text-based classifiers but no learnable bias is shown in the middle row.\nOur proposed model makes use of text-based classifiers sourced from rich descriptions and a learnable bias (bottom row). Our method for\ntext-based classifiers improves performance on novel/rare categories (APr) by a large amount. We report mask AP metrics.\nNatural Language Descriptions. Table 5 shows the effect of using rich class descriptions, sourced from a LLM, rather than\nforming text-based classifiers from simple text prompts of the format \u201ca(n) {class name}\u201d as in Detic. To compare\nfairly to Detic with detection data only (top row), we report a set of results which do not make use of the learnable bias\non the detection scores as detailed above (middle row). Using our text-based classifiers without a learnable bias improves\nperformance on rare categories by 2.2 APr compared to the public Detic model. Using rich class descriptions, a learnable\nbias and our method (bottom row) further improves open-vocabulary detection on novel/rare categories by 2.9 and 0.7 APr\ncompared to the public Detic model and our method without a learnable bias, respectively.\nVisual Encoder\nTTA?\nmAP\nAPr\nAPc\nAPf\nCLIP ViT-B/32\n\u2717\n29.0\n16.3\n29.1\n34.4\n\u2713, harsh\n29.0\n17.2\n28.7\n34.6\n\u2713, gentle\n29.2\n18.3\n28.7\n34.4\nTable 6. The effect of using test-time augmentation (TTA) when generating classifier embeddings from image exemplars using a\nCLIP image encoder. Both TTA recipes use two common augmentations \u2014 ColorJitter and RandomHorizontalFlip. For\nharsh/gentle TTA \u2014 min scale of RandomResizedCrop = 0.5/0.8. Both harsh and gentle TTA perform better than no TTA in terms\nof performance on novel/rare categories (APr). We report mask AP metrics.\nTest-Time Augmentation on Image Exemplars for Vision-Based Classifiers. Table 6 shows the effect of using test-\ntime augmentation (TTA) on image exemplars to produce vision-based classifiers with our trained aggregator. For each\n12\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n13\nimage exemplar we generate 5 augmentations. As in the main paper, we use the case of K = 5 \u2014 for each class\nin the LVIS vocabulary we are have 5 RGB image exemplars. As mentioned in Section 4.2, we augment each exem-\nplar 5 times when using TTA. We consider two augmentation variations, with each containing ColorJitter and\nRandomHorizontalFlip. The \u2018harsh\u2019 variation uses RandomResizedCrop(scale=(0.5,1.0)) and the \u2018gen-\ntle\u2019 variation uses RandomResizedCrop(scale=(0.8,1.0)). We find adding \u2018gentle\u2019 TTA performs best, improv-\ning open-vocabulary detection by 2.0 AP on rare categories compared to no use of TTA. In the main paper, when using\nvision-based classifiers we utilise \u2018gentle\u2019 TTA on the image exemplars.\nPrompt\nmAP\nAPr\nAPc\nAPf\na/an class name\n29.9\n17.7\n30.1\n35.0\na photo of a/an class name\n29.5\n15.9\n30.0\n34.9\na photo of a/an class name in the scene\n29.4\n16.2\n29.7\n34.8\nOur LLM descriptions\n30.3\n19.3\n30.5\n35.0\nTable 7. The effect of using manually crafted prompts against our rich class descriptions sourced from LLMs. All models use a learnable\nbias on the detection scores and text-based classifiers. Using our class descriptions improves performance on \u201crare\u201d classes compared to\nmanually crafted prompts.\nComparing our LLM Descriptions to Manually Designed Text Prompts. Table 7 compares the detector performance\nbetween using simple manually crafted prompts (first three rows) and our rich class descriptions sourced from an LLM (final\nrow), for constructing text-based classifiers. We report results for the case where the detector is trained on LVIS-base only\n(i.e. no additional image-level data is used). In all cases we apply the learnable bias on the detection score. We note that\nof the manual prompts, the simplest one, of the form \u201ca(n) {class name}\u201d, performs best across all metrics. Using\nour text-based classifiers generated from LLM descriptions improves performance on rare classes by 1.6 APr and by 0.4\nmAP across all classes. Performance on \u201ccommon\u201d and \u201cfrequent\u201d classes is largely similar as the availability of labelled\ndetection data for these classes renders the quality of the classifier less important.\n13\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n14\nB. A Closer Look at \u201crare\u201d Class Performance\nModel\nDIMG\nmAP\nAPr\nAPr-w\nAPr-z\nDetic (Zhou et al., 2022)\n\u2717\n30.2\n16.3\n15.7\n19.7\nOurs (Text-Based)\n30.3\n19.3\n19.2\n19.4\nOurs (Multi-Modal)\n30.6\n19.2\n18.5\n22.2\nDetic (Zhou et al., 2022)\n\u2713\n32.4\n24.9\n25.4\n23.0\nOurs (Text-Based)\n32.6\n25.8\n26.7\n21.7\nOurs (Multi-Modal)\n33.1\n27.3\n27.8\n24.9\nTable 8. Comparison between Detic and our models using text-based classifiers and multi-modal classifiers on LVIS. As IN-L does not\ncover all classes in the LVIS vocabulary, we split the rare class metric APr into APr-w and APr-z which represent rare classes with and\nwithout weak annotations from IN-L, respectively. Best performing models are shown in bold. We report mask AP metrics.\nSection 4.1 detailed the data used to train our detector. IN-L contains images from the 997 classes from LVIS present in\nImageNet-21k (Deng et al., 2009).\nIN-L gives weak supervision during detector training. Out of the 337 \u201crare\u201d classes in LVIS, 277 are covered by IN-L and\nare therefore weakly supervised, leaving 60 classes for which no weak supervision is available.\nTo investigate the improvement in performance when using IN-L, we split the rare class metric (APr), which reports average\nprecision across rare classes, into APr-w which averages across rare classes present in IN-L and APr-z which averages\nacross rare classes not present in IN-L which are therefore truly zero-shot classes. Note that when no extra image-level data\nis used, i.e. DIMG = \u2205, all rare classes are truly zero-shot classes.\nTable 8 shows the result of using this breakdown. These results show training on IN-L improves performance on rare classes\nnot contained in IN-L, which may not be expected. The weak supervision from IN-L leads to a reduction in false positives\nfor all rare classes leading to improved performance across all metrics. Moreover, our multi-modal classifiers perform best\nacross all metrics.\n14\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n15\nC. Vision-based Classifier Pipeline Implementation Details\nFor the transformer architecture of the visual aggregator detailed in Section 3.3, we use 4 transformer encoder blocks, each\nwith a hidden dimension of 512 and MLP dimension 2048. As input to the first transformer block, we encode each image\nexemplar with a CLIP image encoder which remains frozen throughout training, yielding one embedding per exemplar. The\nset of embeddings are input with a learnable [CLS] token. The output [CLS] token is used as the final vision-based classifier.\nTo train the model we use the ImageNet-21k-P dataset (Ridnik et al., 2021) for 10 epochs. To speed up and improve\ntraining we store a dynamic queue of size 4096 \u00d7 K CLIP encoded embeddings, with 512 positions in the queue updated\neach iteration, using a last-in first-out policy. Each set of K represents encodings from K randomly sampled images for a\nsingle class. We use K = 5. For contrastive training, we use a temperature of 0.02 in the InfoNCE (van den Oord et al.,\n2018) loss function, the AdamW (Loshchilov & Hutter, 2018) optimiser with standard hyperparameters and a learning\nrate of 0.0002. Furthermore, during training we uniformly sample k \u2208 [1 : K] to simulate varying numbers of image\nexemplars being available for downstream OVOD when CTEST is defined. Therefore, for a given iteration, there may 1 \u2212 5,\nvisual embeddings input per class. Prior to input to the CLIP image ViT encoder, we apply random augmentations to\neach sampled image from ImageNet-21k-P. We use an augmentation policy similar to SimCLR (Chen et al., 2020), which\nincludes RandomResizedCrop, ColorJitter and RandomGrayscale. We discover that test-time augmentation\nof the image exemplars available for the vocabulary in CTEST improves downstream OVOD performance. For each image\nexemplar, we generate 5 test-time augmentations. Therefore if we have L image exemplars for a given class in CTEST, 5L\naugmented images are encoded using the CLIP image encoder and fused using the learnt transformer architecture \u2014 the\nvisual aggregator \u2014 as described in Section 3.3. Use of test-time augmentation is ablated in Section A.\n15\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n16\nD. Sourcing Image Exemplars\nIn this section, we detail how image exemplars are sourced when performing experiments using vision-based classifiers and\nmulti-modal classifiers, as described in Section 3.3 and 3.4, respectively. We start with an empty image exemplar dictionary\n(IED) for the 1203 LVIS classes.\nFrom constructing the image-level dataset IN-L, we know that ImageNet-21k (Deng et al., 2009) contains 997 out of the\n1203 classes in the LVIS vocabulary, using exact WordNet (Miller, 1995) synset matching. We add IN-L to our IED. The\nresult is 988 classes have more than 40 images in ImageNet-21k. This leaves 215 classes for which there are too few or no\nimage exemplars (< 40).\nNext, to try and fill this gap, we turn to LVIS itself. We add the LVIS training annotations with area greater than 322 to our\nIED. There are now 1095 LVIS classes with more than 40 examples, leaving 48 classes with at least 10 exemplars and 60\nclasses with less than 10 exemplars.\nThe final dataset we turn to is VisualGenome (Krishna et al., 2017), which provides bounding box annotations for 7605\nWordNet synsets. We include the annotations from VisualGenome with an exact WordNet synset match with the LVIS\nvocabulary to our IED. We now have 1110 LVIS classes with at least 40 exemplars and 1160 with at least 10 exemplars.\nReducing our minimum required number of exemplars per class from 40 to 10 leaves 43 classes with too few exemplars.\nAt this point, we inspect each of the remaining 43 classes by hand and find that all have other synsets present in ImageNet-21k\nwhich are visually identical or very similar. For example, \u201canklet\u201d is a \u201ccommon\u201d class in LVIS, for which LVIS gives a\ndefinition of \u201can ornament worn around the ankle\u201d and a WordNet synset of anklet.n.03. This synset is not found\nin ImageNet-21k but anklet.n.02, defined as \u201ca sock that reaches just above the ankle\u201d by WordNet, is present and\nvisual inspection shows these images to actually exactly match anklet.n.03. Therefore, we add ImageNet-21k images\nrelating to anklet.n.02 to our IED. As another example, penny.n.02 (as in the penny coin) is a \u201crare\u201d class in LVIS\nfor which exemplars could not be found automatically. However, ImageNet-21k contains images of coin.n.01 which is\na hypernym penny.n.02. The images for coin.n.01 are visually extremely similar and often identical to those one\nwould expect for penny.n.02 and so we add ImageNet-21k images relating to coin.n.02 to our IED.\nAfter applying some human effort as described above, our IED contains at least 40 image exemplars for 1110 (92% of LVIS\nclasses) and at least 10 image exemplars for all LVIS classes.\n16\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n17\n0\n2\n4\n6\n8\n10\nNumber of Image Exemplars per Class\n6\n8\n10\n12\n14\n16\n18\n20\nRare Class Mean Average Precision (mask APr)\nVision-based Classi\ufb01ers\nwith our Aggregator\nVision-based Classi\ufb01ers\nwithout our Aggregator\nOur Text-based Classi\ufb01ers\nusing Rich Class Descriptions\nFigure 5. Detection performance of our vision-based classifiers on the LVIS OVOD benchmark. We vary the number of image exemplars\navailable per class, K, to investigate the effect of the number of image exemplars on OVOD performance.\nE. Varying Number of Image Exemplars for Vision-based Classifiers\nIn this section we show results using vision-based classifiers varying the number of K image exemplars used per class.\nFigure 5 shows performance on the LVIS OVOD benchmark for rare classes using K = 1, 2, 5, 10, where K is the number\nof image exemplars per class used. We compare our method which makes use of our aggregator (red dashed), which has a\ntransformer architecture, with the simple vector mean of the embeddings (blue solid) for the K image exemplars. In both\ncases we apply the \u2018gentle\u2019 TTA detailed and ablated in Section A.\nThese results use LVIS-base as detection training data, no additional image-level labelled data i.e. DIMG = \u2205 and CLIP\nViT-B/32 as the pre-trained visual encoder to produce initial embeddings from each exemplar.\nFigure 5 shows that for each value of K, the use of our aggregator boosts performance on rare classes demonstrating the\nutility of our aggregator at combining the most useful information from the K given exemplars. Our method for K = 5\nsurpasses the performance of K = 10 with simple vector averaging. For K = 1, our method improves performance by 2.3\nAPr which further demonstrates the improved feature representation \u2014 K = 1 involves no aggregation as only 1 exemplar\nis available per class.\nFurthermore, we compare to the performance of our text-based classifiers which make use of rich class descriptions sourced\nfrom a GPT-3 model. Our vision-based classifiers cannot surpass the performance of our text-based classifiers demonstrating\nthe need for more research into using image exemplars for OVOD.\n17\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n18\nF. Additional Example Class Descriptions\nIn this section we show a selection of rich class descriptions sourced from the text-davinci-002 text completion\nmodel from OpenAI. For each class in the LVIS vocabulary we generate 10 rich descriptions. We also give the LVIS\nfrequency category \u2014 rare, common, frequent.\nF.1. Generated descriptions for \u201cbagpipe\u201d (rare)\n1. A bagpipe is a wind instrument with a bag that is filled with air.\n2. A bagpipe typically consists of a blowstick, a chanter, and one or more drones.\n3. A bagpipe is a musical instrument that has a bag, a blowpipe, and usually two drones.\n4. A bagpipe is a wind instrument with a bag that collects air, a reed pipe for each note,\nand a blowpipe.\n5. A bagpipe is a musical instrument that is played by blowing into a bag of air.\n6. Bagpipes vary in appearance, but most have a bag made from a animal skin, a blowpipe,\na chanter, and one or more drones.\n7. A bagpipe is a musical instrument that has a bag, a blowpipe, and usually drone pipes.\n8. A bagpipe is a musical instrument that is usually made out of wood.\n9. A typical Highland bagpipe has a chanter with a double reed, a blowstick, three drones\nwith single reeds, and a bag.\n10. A bagpipe consists of a blowing bag, a chanter, a drone, and usually one or more\ndrones.\nF.2. Generated descriptions for \u201ctrench coat\u201d (rare)\n1. A trench coat is a coat made of heavy cloth, sometimes waterproof, that hangs to about\nknee length.\n2. A trench coat looks like a long, military-style coat.\n3. A trench coat typically looks like a long, belted raincoat.\n4. A trench coat is a long, water-resistant coat that is typically worn over top of a\nsuit.\n5. A trench coat typically has a removable liner, a double-breasted front, and belted\ncuffs.\n6. A trench coat generally refers to a type of coat that is longer than waist length.\n7. A trench coat is a coat that is usually a little bit longer than waist length, has a\ntie or a belt around the waist, and has a collar.\n8. A trench coat is a coat made of waterproof material, typically hip-length or longer,\nwith a belt and a collar.\n9. A trench coat is a long, light coat with a belt.\n10. A trench coat is a raincoat made of heavy-duty fabric, typically poplin, gabardine, or\ndrill.\nF.3. Generated descriptions for \u201cwalrus\u201d (rare)\n1. A walrus is a large, flippered marine mammal with a bulky body, short limbs, and a\nlarge head with two long tusks protruding from the mouth.\n2. A walrus is a blubbery mammal with long tusks, whiskers, and a seal-like face.\n3. A walrus is a large, flippered marine mammal with a long, tusked head.\n4. A walrus is a stocky, rounded pinniped with small flippers, short fur, and long tusks.\n5. A walrus is a large, flippered marine mammal with a bulky body, short tail, and wide,\nflat head.\n6. A walrus is a large ocean mammal with two long tusks, a thick fur coat, and large\nflippers.\n7. A walrus is a large flippered marine mammal with a discontinuous distribution about\nthe North Pole in the Arctic Ocean and sub-Arctic seas of the Northern Hemisphere.\n8. A walrus is a large flippered marine mammal with a thick fur coat.\n9. A walrus is a large marine mammal with a body shaped somewhat like a seal.\n10. A walrus is a seal with a long face and large tusks.\n18\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n19\nF.4. Generated descriptions for \u201cbriefcase\u201d (common)\n1. A briefcase is a rectangular, portable case used to hold papers, documents, or other\nmaterials.\n2. A briefcase is a small case used to carry documents and other small items.\n3. A briefcase is a small, rectangular-shaped case that is used to carry important\ndocuments or other items.\n4. A briefcase is typically a rectangle shaped bag made of leather or synthetic\nmaterials.\n5. A briefcase generally looks like a small, rectangular case made out of a variety of\nmaterials, such as leather, canvas, or nylon.\n6. A briefcase is a narrow rectangular case used to carry documents and other valuables.\n7. A briefcase is a box-shaped bag typically used by businesspeople to transport\nimportant documents.\n8. A briefcase is a rectangular leather case with a handle.\n9. A briefcase is a small case used to carry documents and other small items.\n10. A typical briefcase is rectangular and has a handle on the top.\nF.5. Generated descriptions for \u201ccoin\u201d (common)\n1. A coin is a small, flat, round piece of metal or plastic that is used as money.\n2. A coin has a head side and a tail side.\n3. A coin is usually a small, flat, round piece of metal or plastic that is used as\nmoney.\n4. A coin has a round shape and is flat.\n5. A coin generally has a circular shape with a raised edge, and two faces --- one on\neach side.\n6. A coin is a small, flat, round piece of metal or plastic that is used as money.\n7. A coin is a round piece of metal with an image on one side and the words \u2018\u2018United\nStates of America\u2019\u2019 on the other.\n8. A coin is a small, round, flat piece of metal or plastic that is used as money.\n9. Sure, a coin is a small, round, flat piece of metal or plastic that is used as money.\n10. A coin is a small, round, metal disk with an image on one side and raised lettering on\nthe other.\nF.6. Generated descriptions for \u201cwaffle\u201d (common)\n1. A waffle is a pancake-like pastry that is cooked in a waffle iron and has a\ndistinctively grid-like pattern on the top and bottom.\n2. A waffle is a dish made from leavened batter or dough that is cooked between\ntwo plates that are patterned to give a characteristic size, shape, and surface\nimpression.\n3. A waffle is a thin, round, batter-based cake that is cooked in a waffle iron and is\nusually served with syrup.\n4. A waffle looks like a round, honeycomb-patterned cake that is cooked in a waffle iron.\n5. A waffle is typically a leavened batter or dough that is cooked between two plates\nthat are patterned to give it a characteristic size, shape, and surface impression.\n6. A waffle looks like an egg-shaped pancake with deep indentations.\n7. A waffle looks like a grid of squares.\n8. A waffle is a pancake-like pastry that is made from a leavened batter or dough and is\ncooked between two heated plates.\n9. A waffle is a pancake-like food that is cooked in a waffle iron.\n10. A waffle is a hotcake with different patterns on it.\n19\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n20\nF.7. Generated descriptions for \u201cavocado\u201d (frequent)\n1. An avocado looks like a pear-shaped fruit with green or blackish skin.\n2. It is a green fruit that has a dark brown or black seed in the center.\n3. An avocado is a pear-shaped green fruit with smooth, green skin and a large seed in\nthe center.\n4. An avocado is a fruit that is brown and bumpy on the outside and green and creamy on\nthe inside.\n5. An avocado is a fruit with a dark green or blackish skin and a soft, fleshy inside.\n6. An avocado is a green, pear-shaped fruit with a smooth, fleshy texture.\n7. An avocado is a pear-shaped fruit with smooth, green skin.\n8. An avocado is shaped like an egg and has a greenish-brownish skin.\n9. An avocado is typically a dark green or black color on the outside with a soft, light\ngreen or yellow color on the inside.\n10. An avocado is a pear-shaped fruit with smooth, green skin and a large, pit in the\ncenter.\nF.8. Generated descriptions for \u201csausage\u201d (frequent)\n1. A sausage is a cylindrical shape made of ground meat, typically pork, and a variety of\nspices and other ingredients.\n2. A sausage is typically a ground-up mix of pork, beef, or other meats with spices and\nsalt, encased in a thin skin.\n3. A sausage is typically a cylindrical shaped food made from ground meat, spices, and\nother ingredients.\n4. A sausage is a long, thin, cylindrical piece of meat.\n5. Sausages are typically long, cylindrical shaped foods made from ground meat and\nspices.\n6. A sausage is a tubular meat product typically made from ground pork, beef, or poultry.\n7. A sausage is a cylindrical casing filled with meat, typically pork, and various herbs\nand spices.\n8. A sausage looks like a tubular shape made of ground up meat that is usually encased in\na thin layer of intestine.\n9. A sausage is a cylindrical casing of meat that is typically filled with ground pork,\nalthough many other variations exist.\n10. When cooked, a sausage is typically cylindrical and can vary in length.\nF.9. Generated descriptions for \u201cspectacles\u201d (frequent)\n1. A spectacle is a pair of eyeglasses.\n2. A spectacles is a type of eyewear that consists of a frame that holds two lenses in\nfront of the eyes.\n3. Spectacles are a type of eyewear that helps people see more clearly.\n4. Spectacles are glasses that are worn in order to improve vision.\n5. A spectacles usually refers to a glass or plastic lens worn in front of the eye to\ncorrect vision, or protect the eye from debris, dust, wind, etc.\n6. A spectacles is a type of corrective lens used to improve vision.\n7. A spectacle is a lens worn in front of the eye to correct vision, for cosmetic reasons,\nor to protect the eye.\n8. A spectacles has a frame that goes around your head and two lenses in front of your\neyes.\n9. A spectacles has two glass or plastic lenses in metal or plastic frames that rest on\nthe ears.\n10. A pair of spectacles is a frame that holds two eyeglasses lenses in front of a\nperson\u2019s eyes.\n20\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n21\nG. Example Image Exemplars\nIn this section we show a selection of image exemplars, for LVIS classes, found using the process described in Section D.\nWe also give the LVIS frequency category \u2014 rare, common, frequent. For cases where the image exemplar comes from a\ndataset with bounding boxes (LVIS or VisualGenome) we show the bounding box in yellow.\nFigure 6. Image Exemplars for \u201cpuffin\u201d (rare).\nFigure 7. Image Exemplars for \u201capricot\u201d (rare).\nFigure 8. Image Exemplars for \u201cflamingo\u201d (common).\nFigure 9. Image Exemplars for \u201clantern\u201d (common).\n21\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n22\nFigure 10. Image Exemplars for \u201caerosol can\u201d (common).\nFigure 11. Image Exemplars for \u201cwineglass\u201d (frequent).\nFigure 12. Image Exemplars for \u201cbeanie\u201d (frequent).\nFigure 13. Image Exemplars for \u201cfire engine\u201d (frequent).\n22\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n23\nH. More Qualitative Results\nIn this section we show more rare category detections on the LVIS OVOD benchmark using our multi-modal classifier\ntrained with IN-L.\ndiary.n.01\ndinghy.n.01\nbeach_ball.n.01\nboom.n.01\nbowling_ball.n.01\nburrito.n.01\ncandy_bar.n.01\ncargo_ship.n.01\ncasserole.n.01\ncocoa.n.01\ncornbread.n.01\ncoverall.n.01\ncylinder.n.01\ndrumstick.n.01\neclair.n.01\nfunnel.n.01\nlab_coat.n.01\nmallet.n.01\nmilestone.n.01\nomelet.n.01\npapaya.n.01\nroller_skate.n.01\nsharpie.n.01\nsugar_bowl.n.01\ntrench_coat.n.01\nsatchel.n.01\nmartini.n.01\nFigure 14. Additional qualitative results on LVIS OVOD benchmark.\n23\nMulti-Modal Classifiers for Open-Vocabulary Object Detection\n24\nI. Dugong\nFigure 15. An example of a dugong \u2014 a visually distinctive marine species with a less well known name.\n24\n"
  },
  {
    "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
    "link": "https://arxiv.org/pdf/2306.05696.pdf",
    "upvote": "3",
    "text": "Embodied Executable Policy Learning with\nLanguage-based Scene Summarization\nJielin Qiu1,2\u2217, Mengdi Xu1,\u2217, William Han1,\u2217, Seungwhan Moon2, Ding Zhao1\n1Carnegie Mellon University, 2Meta Reality Labs\n{jielinq,mengdixu,wjhan,dingzhao}@andrew.cmu.edu, shanemoon@meta.com\nAbstract: Large Language models (LLMs) have shown remarkable success in\nassisting robot learning tasks, i.e., complex household planning. However, the\nperformance of pretrained LLMs heavily relies on domain-specific templated text\ndata, which may be infeasible in real-world robot learning tasks with image-based\nobservations. Moreover, existing LLMs with text inputs lack the capability to\nevolve with non-expert interactions with environments. In this work, we introduce\na novel learning paradigm that generates robots\u2019 executable actions in the form of\ntext, derived solely from visual observations, using language-based summarization\nof these observations as the connecting bridge between both domains. Our pro-\nposed paradigm stands apart from previous works, which utilized either language\ninstructions or a combination of language and visual data as inputs. Moreover, our\nmethod does not require oracle text summarization of the scene, eliminating the\nneed for human involvement in the learning loop, which makes it more practical for\nreal-world robot learning tasks. Our proposed paradigm consists of two modules:\nthe SUM module, which interprets the environment using visual observations and\nproduces a text summary of the scene, and the APM module, which generates\nexecutable action policies based on the natural language descriptions provided\nby the SUM module. We demonstrate that our proposed method can employ two\nfine-tuning strategies, including imitation learning and reinforcement learning\napproaches, to adapt to the target test tasks effectively. We conduct extensive\nexperiments involving various SUM/APM model selections, environments, and\ntasks across 7 house layouts in the VirtualHome environment. Our experimental\nresults demonstrate that our method surpasses existing baselines, confirming the\neffectiveness of this novel learning paradigm.\nKeywords: Decision-Making, Embodied Robot Learning, Large Language Model\n1\nIntroduction\nThere has been a surge of interest in building Large Language Models (LLMs) pretrained on large-\nscale datasets and exploring LLMs\u2019 capability in various downstream tasks. LLMs start from the\nTransformer model [1] and are first developed to solve different natural language processing (NLP)\napplications [2, 3, 4]. Recently, LLMs also show great potential for accelerating learning in many\nother domains by generating learned embeddings as meaningful representations for downstream tasks\nand encoding transferable knowledge in large pretraining datasets. Examples include transferring\nthe knowledge of LLM to, i.e., robotics control [5, 6], multimodal learning [7, 8], decision-making\n[9, 10], code generation [11], laws [12], computer vision (CV) [13], and so on.\nIn this paper, we focus on the problem of facilitating robot learning by having an LLM in the loop.\nThe robot generates actions according to its environment observations, which are, in general, sensory\ninformation in the format of images, point clouds, or kinematic states. We identify one key challenge\nin massively deploying LLMs to assist robots is that LLMs lack the capability to understand such\n\u2217Equal contribution\narXiv:2306.05696v1  [cs.RO]  9 Jun 2023\nFigure 1: The overall architecture of our approach, which includes a scene understanding module\n(SUM) and an action prediction module (APM). The agent takes pure visual observations and encodes\nthe information as language. Then the language is transferred to APM for action generation. APM\nfine-tuned on VirtualHome can generate executable action plans directly.\nnon-text-based environment observations. To solve this challenge, Liang et al. [5] utilize rule-based\nperception APIs to transform image-based observations into text formats, which then serve as inputs\nto the LLM. We instead propose to integrate the multimodal learning paradigm to transform images\ninto texts, which allows more principled and efficient transfer to novel robot learning tasks than\nrule-based APIs. Another key challenge is the widely-existing large distribution shifts between the\ntraining tasks of large pretrained models and testing tasks in the domain of robot learning. To close\nthe domain gap, Li et al. [9] adapt the pretrained LLM to downstream tasks via finetuning with\nobservations converted into text descriptions. In the presence of realistic visual observations, it is still\nbeing determined what is an appropriate method to co-adapt pretrained foundation models.\nTo address the above challenges, we propose a new visual-based robot learning paradigm that\ntakes advantage of embedded knowledge in both multimodal models and LLMs. To align different\nmodalities in the visual observations and text-based actions, we consider language as the bridge\ninformation. We build a scene-understanding model (SUM) with a pretrained image captioning model\nto grant the robot the ability to describe the surrounding environment with natural language. We then\nbuild an action prediction model (APM) with an LLM to generate execution actions according to\nthe scene caption in the format of natural language. To adapt pertained models in SUM and APM to\ndownstream robot learning tasks, we propose to finetune the multimodal model in SUM with pre-\ncollected domain-specific image-caption pairs and the language model in APM with corresponding\nlanguage-action pairs. Besides finetuning with expert demonstrations, we further propose a finetuning\nparadigm of APM based on the sparse environment feedbacks to endow APM\u2019s capability to evolute\nwith non-expert data. An illustration of the proposed framework is in Figure 1.\nOur contributions are summarised as follows:\n\u2022 We introduce a novel robot learning paradigm with LLM in the loop that handles multiple\nmodalities of visual observations and text-based actions in a principled manner. We bridge\nboth modalities with natural language generated by a pretrained multimodal model.\n\u2022 To adapt to target testing tasks, we propose two fine-tuning strategies, including imitation\nlearning and reinforcement learning approaches. We collect a new expert dataset for\nimitation learning-based finetuning.\n\u2022 We test the adaptation performance of multiple models of SUM and APM in seven house\nlayouts in the VirtualHome environment. Our experiments demonstrate that our proposed\nparadigm shows promising results. Our code is in the Supplementary Material.\n2\nRelated Work\nLanguage Models for Embodied Agents.\nRecently, several works have successfully combined\nLLMs with robot learning by taking advantage of the knowledge learned by LLMs, i.e., reasoning\n[5, 7, 8], planning [14, 15, 16, 9, 10], manipulation [17, 18, 19, 20, 21, 22, 23, 24], and navigation\n[25, 26, 27, 28, 29], which demonstrated the feasibility of using LLM to assist embodied learning.\nIn particular, in [15], language instructions are utilized to interpret the scene, whereas we rely on\nraw image observations. In [30], a predefined executable plan prompt is provided without learning\n2\nfrom visual observations, simplifying the generation of executable plans. In [31], both language\ninstructions and visual images are employed to fine-tune the VLM, which is subsequently used\nfor behavior cloning. However, the generated robot plan consists of high-level natural language\ninstructions rather than executable robot policies, as in our work. In [32], the proposed MINECLIP\nprimarily calculates the correlation between an open-vocabulary language goal string and a 16-frame\nvideo snippet. The correlation score serves as a learned dense reward function for training a robust\nmulti-task RL agent, which is distinct from our approach.\nPretraining and Finetuning Language Models.\nFine-tuning [33] has superseded the use of feature\nextraction of pretrained embeddings [34] while pretrained language models are favored over models\ntrained on many tasks due to their increased sample efficiency and performance [35]. The success of\nthese methods has led to the development of even larger models [2, 36]. But those large models may\nnot perform well on data that is different from what they were pretrained on. Under this case, fine-\ntuning pretrained contextual word embedding models to supervised downstream tasks has become\ncommonplace [37, 38]. More related works can be found in Appendix E.\n3\nMethod\nIn this section, we first introduce our focused problem, which is generating a visual-based policy by\nleveraging pretrained large models. We then introduce SUM, which learns language descriptions of\nthe surrounding environment, and APM, which predicts actions based on SUM\u2019s caption output. To\ngrant both SUM and APM the capability of making the correct understanding and decision in the\ntarget domain, we propose their finetuning algorithms.\nProblem Formulation.\nWe consider a general and realistic robot learning task where a robot\nagent receives a sequential visual observation V = [v1, v2, ..., vt], where t is the timestep, and\naims to generate a sequence of actions A = [a1, a2, ..., at] based on the pure visual observations V .\nTraditionally, the robot\u2019s policy is trained from scratch in the target tasks. Inspired by the success of\nlarge pretrained models, we aim to explore the benefit of utilizing pretrained LLMs and multimodal\nmodels for general robot learning tasks, where only visual observations are available as inputs. Given\nthe prevailing domain shift between the training domain of the pretrained models and the robot\nlearning tasks, we are motivated to develop a principled finetuning method.\nSUM: Learning Scene Descriptions from Visual Observations into Language.\nThe goal of the\nSUM (scene understanding module) is to transform visual observations into language descriptions\nthat contain an actionable trait to it. SUM shares similar functionalities of visual captioning models,\nwhich aim to automatically generate fluent and informative language descriptions of an image [39].\nFor the SUM to be capable of providing scene descriptions from visual observations, it needs to\ndistill representative and meaningful visual representations from an image, then generate coherent\nand intelligent language descriptions. In our framework, we adopt models with image captioning\nability as our SUM. Generally, image captioning models employ a visual understanding system and\na language model capable of generating meaningful and syntactically correct captions [40]. In a\nstandard configuration, the task can be defined as an image-to-sequence problem, where the inputs\nare pixels, which will be encoded as one or multiple feature vectors in the visual encoding step. Then\na language model will take the information to produce a sequence of words or subwords decoded\naccording to a given vocabulary in a generative way.\nWith the development of self-attention [41], the visual features achieved remarkable performance due\nto multimodal pretraining and early-fusion strategies [42, 43, 44, 45]. As for language models, the\ngoal is to predict the probability of a given sequence of words occurring in a sentence. As such, it is\na crucial component in image captioning, as it gives the ability to deal with natural language as a\nstochastic process. Formally, given a sequence of n words y1, . . . , yn, the language model component\nof an image captioning algorithm assigns a probability P (y1, y2, . . . , yn | X) to the sequence as:\nP (y1, y2, . . . yn | X) =\nn\nY\ni=1\nP (yi | y1, y2, . . . , yi\u22121, X) ,\n(1)\n3\nwhere X represents the visual encoding on which the language model is specifically conditioned.\nNotably, when predicting the next word given the previous ones, the language model is autoregressive,\nwhich means that each predicted word is conditioned on the previous ones. Additionally, the language\nmodel decides when to stop generating captions by outputting a special end-of-sequence token.\nAPM: Decoding Language Information into Executable Action Plans.\nThe goal of APM (action\nprediction module) is to transform latent language information from the SUM output into executable\naction plans. Since both latent language information and executable action plans are sequential data,\nan LLM with encoder-decoder architecture is a good option for APM in our framework. In addition,\nan LLM pretrained on a vast corpus of text already has adequate knowledge, which can be fine-tuned\non other tasks to improve learning efficiency.\nAn LLM with encoder-decoder architecture suits well for our setting. The encoder is responsible\nfor reading and understanding the input language information from SUM, which is usually based on\ntransformer architecture, and creates a fixed-length vector representation, called the context vector.\nThe decoder then takes the context vector as input and generates the output, in our case, the executable\naction plans. The decoder uses the context vector to guide its generation of the output and make sure\nit is coherent and consistent with the input information. However, due to the distribution change\nbetween the data that LLM was pretrained on and the new task, the LLM needs to be fine-tuned on\nthe task-specific data to transfer the knowledge. The fine-tuning strategies will be introduced in the\nfollowing sections. For LLMs, we use well-adopted pretrained architectures, including BERT [2],\nRoBERTa [3], and BART [46], as both the encoder and decoder. The goal of the LLM is to learn how\nto generate programmable, executable actions from the language descriptions outputted by SUM.\nTraining Pipeline.\nThe training pipeline contains two steps. First, we fine-tune SUM with the\ncurated VirtualHome observations (More details about data collection are introduced in Section 4).\nThis fine-tuning step is to familiarize SUM with the types of scenes present in the task-specific data.\nWe present pseudocode to fine-tune the SUM in Algorithm 1 in Appendix A.\nIn the second stage, we load the fine-tuned SUM and encode the outputs as latent language embed-\ndings. The embeddings are then fed into the APM, which is then fine-tuned using different fine-tuning\nloss objectives (supervised one or policy gradient, more details are introduced in Section 4), to\nachieve the optimal policy with maximum rewards. The pseudocode for finetuning APM with IL and\nREINFORCE are in Algorithms 2 and 3 in Appendix A, respectively.\nFine-tuning APM with IL and RL.\nThe output word from LLM is sampled from a learned\ndistribution over the vocabulary words. In most simple scenarios, i.e., the greedy decoding mechanism,\nthe word with the highest probability is output. The main drawback of this setting is that possible\nprediction errors quickly accumulate along the way. To alleviate this drawback, one effective strategy\nis to use the beam search algorithm [47, 48] that, instead of outputting the word with maximum\nprobability at each time step, maintaining k sequence candidates and finally outputs the most probable\none. For the training or fine-tuning strategies, most strategies are based on cross-entropy (CE) loss and\nmasked language model (MLM). But recently, RL-based learning objective has also been explored,\nwhich allows optimizing for captioning-specific non-differentiable metrics directly.\nImitation Learning with Cross-Entropy Loss.\nThe CE loss aims to minimize the negative log-\nlikelihood of the current word given the previous ground-truth words at each timestep. Given a\nsequence of target words y1:T , the loss is formally defined as:\nLXE(\u03b8) = \u2212\nn\nX\ni=1\nlog (P (yi | y1:i\u22121, X)) ,\n(2)\nwhere P is the probability distribution induced by LLM, yi is the ground-truth word at time i, y1:i\u22121\nindicate the previous ground-truth words, and X is visual encoding. The cross-entropy loss is\ndesigned to operate at the word level and optimize the probability of each word in the ground-\ntruth sequence without considering longer-range dependencies between generated words. Traditional\ntraining setting with cross-entropy suffers from exposure bias problems [49] caused by the discrepancy\nbetween the training data distribution as opposed to the distribution of its predicted words.\n4\nFigure 2: Top-down views of the seven different environ-\nments from VirtualHome [54].\nFigure 3: \u2018AUTO\u2019, \u2018FIRST PERSON\u2019,\n\u2018FRONT PERSON\u2019 views.\nReinforcement Learning with REINFORCE.\nGiven the limitations of word-level training strate-\ngies observed when using limited amounts of data, a significant improvement was achieved by\napplying the RL approach. Under this setting, the LLM is considered an agent whose parameters\ndetermine a policy. At each time step, the agent executes the policy to choose an action, i.e., the\nprediction of the next word in the generated sentence. Once the end-of-sequence is reached, the\nagent receives a reward, and the aim of the training is to optimize the agent parameters to maximize\nthe expected reward [40]. Similar to Ranzato et al. [49], for our policy gradient method, we use\nREINFORCE [50, 51], which uses the full trajectory, making it a Monte-Carlo method, to sample\nepisodes to update the policy. For fine-tuning LLMs using RL, we frame the problem into an Agent-\nEnvironment setting where the agent (policy) can interact with the environment to get the reward for\nits actions. This reward is then used as feedback to train the model. The mapping of the entities is\nfrom the agent (policy), which is an LLM, and the environment (the reward function, also named the\nmodel), which generates rewards. The reward function consumes the input as well as the output of\nthe LLM to generate the reward. The reward is then used in a loss function, and the policy is updated.\nFormally, to compute the loss gradient, beam search and greedy decoding are leveraged as follows:\n\u2207\u03b8L(\u03b8) = \u2212 1\nk\nk\nX\ni=1\n\u0010\u0010\nr\n\u0010\nwi\u0011\n\u2212 b\n\u0011\n\u2207\u03b8 log P\n\u0010\nwi\u0011\u0011\n,\n(3)\nwhere wi is the i-th sentence in the beam or a sampled collection, r(\u00b7) is the reward function, and b\nis the baseline, computed as the reward of the sentence obtained via greedy decoding [52], or as the\naverage reward of the beam candidates [53]. Note that since it would be difficult for a random policy\nto improve in an acceptable amount of time, the usual procedure entails pretraining with cross-entropy\nor masked language model first, and then fine-tuning stage with RL by employing a sequence level\nmetric as the reward. This ensures the initial RL policy is more suitable than the random one.\n4\nExperiments\nThis section introduces the environment we used in the experiments, the experimental settings,\nevaluations, and results. We would like to answer the following questions with experiments: (1)\nCan the proposed paradigm take pure visual observations to generate executable robot actions; (2)\nWhich SUMs can provide better scene descriptions for robot learning; (3) Which APMs show a\nbetter action decoding ability in generating executable actions; (4) Which fine-tuning strategies show\nbetter performances in this setting; (5) Can the model achieve consistent performance across different\nenvironments?\nEnvironments.\nWe build the experiment environments based on VirtualHome [55, 56], a multi-\nagent, virtual platform for simulating daily household activities. [54]. Puig et al. [55] provides a\ndataset of possible tasks in their respective environments. Each task includes a natural language\ndescription of the task (\u201cPut groceries in the fridge.\u201d), an elongated and more detailed natural language\ndescription of the task (\u201cI put my groceries into the fridge.\u201d), and the executable actions to perform\nthe task in the VirtualHome simulator ([[Walk] < groceries > (1), [Grab] < groceries > (1), ...\n[Close] < fridge > (1)]). We define the training and testing tasks based on the natural language\ndescriptions of the task due to their straightforwardness.\n5\nTable 1: Results from different SUMs fine-tuned by the imitation learning (IL) objective, where\nBERT serves as APM. The results are shown on 7 different environments in VirtualHome and also\nthe average performance. The best result in each environment and each SUM model is marked in\nblack and bold. The best SUM result with the highest average performance across 7 environments is\nmarked in orange and bold.\nSUM/Results(%)\nEnvironment\nBleu-1\nBleu-2\nBleu-3\nBleu-4\nROUGE-L\nMETEOR\nCIDEr\nSPICE\nExecution Rate\nOFA\n1\n55.1\u00b10.05\n45.4\u00b10.10\n36.5\u00b10.20\n23.0\u00b10.00\n60.0\u00b10.16\n33.4\u00b10.00\n30.2\u00b10.44\n49.9\u00b10.43\n78.0\u00b12.39\n2\n58.0\u00b10.20\n41.7\u00b10.19\n35.1\u00b11.01\n22.1\u00b10.73\n60.1\u00b10.50\n34.1\u00b10.52\n30.3\u00b10.71\n48.1\u00b10.41\n79.9\u00b12.37\n3\n55.3\u00b10.30\n42.3\u00b10.62\n34.9\u00b10.15\n23.0\u00b10.00\n60.5\u00b10.01\n34.8\u00b10.64\n31.2\u00b10.55\n48.4\u00b10.17\n80.0\u00b13.29\n4\n57.8\u00b10.73\n42.2\u00b10.31\n35.3\u00b10.38\n24.5\u00b10.67\n59.9\u00b10.45\n34.6\u00b10.54\n33.1\u00b10.63\n49.0\u00b10.66\n79.9\u00b14.14\n5\n59.4\u00b10.44\n40.3\u00b10.03\n34.8\u00b10.02\n24.2\u00b10.37\n59.7\u00b10.25\n35.1\u00b10.62\n32.7\u00b10.24\n38.0\u00b10.13\n77.4\u00b11.12\n6\n60.5\u00b10.01\n48.1\u00b10.53\n36.6\u00b10.07\n25.1\u00b10.15\n61.9\u00b10.13\n36.2\u00b10.60\n34.6\u00b11.07\n49.9\u00b10.77\n80.5\u00b11.13\n.\n7\n58.2\u00b10.30\n46.5\u00b10.58\n34.6\u00b10.04\n22.3\u00b10.08\n58.3\u00b10.92\n35.6\u00b10.62\n30.8\u00b10.37\n44.2\u00b10.33\n69.2\u00b12.31\nAverage\n57.8\u00b10.92\n43.8\u00b11.02\n35.4\u00b10.63\n23.5\u00b10.77\n60.1\u00b10.41\n34.8\u00b10.62\n31.8\u00b11.31\n46.8\u00b10.80\n77.8\u00b13.26\nBLIP\n1\n51.1\u00b10.50\n42.6\u00b10.41\n33.2\u00b10.34\n21.1\u00b10.63\n60.8\u00b10.73\n34.7\u00b10.63\n35.5\u00b100.09\n42.7\u00b10.91\n72.6\u00b11.99\n2\n50.5\u00b10.87\n41.8\u00b10.72\n30.5\u00b128\n22.3\u00b10.34\n60.3\u00b10.64\n33.6\u00b10.87\n30.0\u00b10.72\n42.8\u00b10.99\n66.1\u00b14.21\n3\n52.4\u00b10.54\n43.2\u00b10.65\n33.6\u00b10.13\n21.1\u00b10.52\n61.4\u00b10.29\n34.5\u00b10.12\n31.1\u00b10.00\n48.9\u00b10.80\n85.0\u00b13.32\n4\n51.0\u00b11.19\n42.1\u00b10.87\n33.8\u00b10.54\n22.8\u00b10.65\n60.6\u00b10.76\n34.4\u00b10.98\n35.1\u00b10.85\n46.0\u00b10.74\n73.0\u00b13.65\n5\n49.0\u00b10.53\n38.8\u00b10.43\n30.4\u00b10.72\n20.0\u00b10.47\n58.6\u00b10.65\n34.1\u00b10.75\n21.0\u00b10.66\n30.8\u00b10.69\n67.2\u00b10.93\n6\n52.6\u00b10.79\n44.5\u00b10.00\n31.0\u00b10.63\n24.8\u00b10.62\n62.0\u00b10.73\n35.3\u00b11.02\n31.0\u00b10.02\n42.4\u00b10.87\n84.1\u00b13.54\n7\n52.7\u00b10.50\n44.0\u00b10.21\n33.6\u00b10.18\n24.0\u00b10.52\n61.7\u00b10.08\n34.5\u00b10.60\n34.5\u00b10.81\n48.8\u00b10.28\n86.0\u00b14.92\nAverage\n51.3\u00b10.31\n42.4\u00b10.54\n32.3\u00b10.66\n22.3\u00b10.31\n60.7\u00b10.63\n34.4\u00b10.75\n31.2\u00b10.87\n43.2\u00b10.97\n76.3\u00b15.22\nGRIT\n1\n50.5\u00b10.99\n40.5\u00b10.86\n31.8\u00b11.82\n20.7\u00b11.02\n60.0\u00b11.44\n33.1\u00b10.97\n30.4\u00b11.42\n41.7\u00b10.85\n69.2\u00b15.57\n2\n52.1\u00b10.66\n41.8\u00b11.77\n31.7\u00b11.92\n20.1\u00b10.97\n59.9\u00b10.65\n32.1\u00b10.76\n29.4\u00b10.87\n42.0\u00b10.88\n71.4\u00b15.52\n3\n52.3\u00b10.88\n40.3\u00b10.82\n32.1\u00b10.77\n19.9\u00b11.53\n60.4\u00b10.68\n31.7\u00b10.66\n30.1\u00b12.52\n43.5\u00b11.64\n71.3\u00b15.98\n4\n51.9\u00b10.93\n39.8\u00b10.92\n31.8\u00b10.97\n21.3\u00b11.72\n59.7\u00b11.22\n32.0\u00b10.76\n30.0\u00b10.79\n42.8\u00b10.84\n72.8\u00b14.65\n5\n54.7\u00b10.93\n42.3\u00b11.02\n33.2\u00b11.25\n24.5\u00b10.93\n62.3\u00b11.42\n33.8\u00b11.77\n30.7\u00b11.32\n44.6\u00b11.23\n78.5\u00b15.07\n6\n54.6\u00b11.42\n44.7\u00b11.64\n34.1\u00b11.32\n25.8\u00b11.22\n65.8\u00b11.25\n30.1\u00b12.31\n34.5\u00b10.72\n44.0\u00b10.96\n78.4\u00b13.66\n7\n53.9\u00b10.88\n42.0\u00b11.79\n32.6\u00b12.00\n22.5\u00b10.90\n63.4\u00b11.00\n31.8\u00b11.23\n32.3\u00b11.31\n43.1\u00b11.41\n70.0\u00b13.99\nAverage\n52.9\u00b10.18\n41.6\u00b10.87\n32.4\u00b10.72\n22.1\u00b10.68\n61.6\u00b10.53\n32.1\u00b10.33\n31.1\u00b10.25\n43.1\u00b10.76\n73.1\u00b13.11\nIn VirtualHome, the agents are represented as 3D humanoid avatars that interact with given envi-\nronments through provided high-level instructions. Puig et al. [55] accumulated a knowledge base\nof instructions by using human annotators from AMT to first yield verbal descriptions of verbal\nactivities. These descriptions were further translated by AMT annotators into programs utilizing a\ngraphical programming language, thus amassing around 3,000 household activities in 50 different\nenvironments [55]. In this study, we evaluate our model\u2019s performance in seven unique environments\nin VirtualHome, which are shown in Figure 2. Each environment has a distinctive set of objects and\nactions that may be interacted with by agents.\nMetrics.\nWe used standard NLP evaluation metrics, i.e., BLEU [57], ROUGE [58], METEOR\n[59], CIDEr [60], and SPICE [61], for evaluating LLMs. In addition, we introduced the execution\nrate following Li et al. [9]. The execution rate is defined as the probability of the agent\u2019s success\nin performing the outputted action from APM over the whole trajectory. More experimental setup\ndetails about SUM and APM are listed in Appendix B.\nDatasets.\nTo fine-tune SUM and APM on task-specific robot learning scenarios, we collect data\nvia VirtualHome, including the agent\u2019s observations, language instructions, and action sequences.\nDuring data collection, a household activity program can be described as: [[actioni] < objecti >\n(idi), ... [actionn] < objectn > (idn)], where i denotes each step of the program, actioni and\nobjecti denotes the action performed on the object at step i, and idi symbolizes the unique identifier\nof objecti [55]. The original dataset was augmented by ResActGraph [56]. After augmentation,\nthe dataset contains over 30,000 executable programs, with each environment containing over 300\nobjects and 4,000 spatial relations. Additionally, we collect the image and text pairs separated by\nthe environments they were executed in. This is important due to the different objects and actions\navailable in each environment. However, as noted in Puig et al. [55] and Liao et al. [56], not all\nprograms were executable.\nDuring data collection, we observed that text descriptions often comprise just two words (i.e., walk\nbathroom, sitting chair, run treadmill). To have a more robust description, we prompt-engineered the\ntext with a fill-mask pipeline using BERT [2, 62]. For this study, we collect programs executed in\nthree different views: \u2018AUTO\u2019, \u2018FIRST PERSON\u2019, and \u2018FRONT PERSON\u2019 as shown in Figure 3.\nIn the \u2018AUTO\u2019 view, there are locked cameras in every scene through which the program randomly\niterates. The \u2018FIRST PERSON\u2019 view observes the agent\u2019s actions through the first-person point\nof view. The \u2018FRONT PERSON\u2019 view monitors the agent\u2019s actions through the front in a locked\nthird-person point of view. Therefore, the final count of image-text pairs for our dataset in the \u2018AUTO\u2019,\n\u2018FIRST PERSON\u2019, and \u2018FRONT PERSON\u2019 views are 26,600, 26,607, and 26,608, respectively.\n6\n5\nResults and Discussions\n5.1\nModel Performance with IL Fine-tuning\nFigure\n4:\nComparison\nof\nour\nap-\nproach with baseline methods in the\nimitation learning setting evaluated by the\nexecution rate.\nWe first want to show the benefit of the proposed frame-\nwork compared with other model architectures. Con-\ncretely, in the IL setting with expert data, we compare\nthe execution rate of our model with the MLP, MLP-1\nand LSTM baselines in Li et al. [9]. Our model has OFA\nin SUM and BART as APM. Note that all the baselines\nare not trained by datasets in other domains and have\nstructured text input instead of realistic visual inputs as\nour proposed model. In the LSTM baseline, the hidden\nrepresentation from the last timestep, together with the\ngoal and current observation, are used to predict the next\naction. MLP and MLP-1 both take the goal, histories, and the current observation as input and send\nthem to MLPs to predict actions. MLP-1 has three more average-pooling layers than MLP that\naverage the features of tokens in the goal, history actions, and the current observation, respectively,\nbefore sending them to the MLP layer. More details about the baselines can be found in Li et al. [9].\nAs shown in Figure 4, our approach outperforms baselines in Li et al. [9] in terms of a higher average\nexecution rate and a smaller standard deviation, though all the methods are trained on expert data\nwith imitation learning objectives. The results show that the pretrained embeddings and large model\narchitecture benefit the performance in downstream robot learning tasks.\n5.2\nModel Performance with RL Fine-tuning\nTable 2:\nResults from differ-\nent\nAPMs\nfine-tuned\nby\nthe\nREINFORCE loss objective. The\nresults are shown by the average\nof 7 different environments in\nVirtualHome. The best results are\nmarked in bold.\nAPM\nSUM\nExecution Rate (%)\nBERT\nOFA\n54.7\u00b11.15\nBLIP\n54.1\u00b11.37\nGRIT\n53.9\u00b13.00\nRoBERTa\nOFA\n55.6\u00b14.31\nBLIP\n55.2\u00b11.16\nGRIT\n54.8\u00b12.54\nBART\nOFA\n57.2\u00b12.43\nBLIP\n57.0\u00b13.12\nGRIT\n55.8\u00b10.99\nWe provide the model performance after fine-tuning SUM with\na frozen BERT in Table 1 for the IL setting with expert data\nand in Table 4 for the RL setting. We further provide the per-\nformance after fine-tuning APM with the fine-tuned SUM in\nTable 3 and Table 2. We can see that fine-tuning with expert\ndata in IL results in higher average and per-environment per-\nformance than fine-tuning with RL, which shows the benefit of\nhaving access to the expert datasets. However, fine-tuning with\nRL still brings performance improvement to 57.2% as in Ta-\nble 2. Note that without finetuning, the outputs of the LLMs in\nAPM are generally not executable, as shown in Figure 1. More-\nover, we consistently observe that the combination of having\nOFA in SUM and BART as APM achieves the best performance\nafter both IL (Table 3) and RL (Table 2) fine-tuning.\n5.3\nAblation Study\nTo better understand the importance of different components in\nour paradigm that affect the overall performance, we conduct ablation studies on different factors,\nincluding varying components in SUM, components in APM, and environmental variations.\nDifferent Components in SUM.\nThe performances of using different components in SUM for\nIL and RL fine-tuning are in Table 1 and Table 4, respectively. From Table 1, we see that with\nexpert data, OFA achieves better results than BLIP and GRIT on the average performance over 7\nenvironments. We conjecture that this may be due to OFA being pretrained on 20M image-text pairs,\nwhich is larger than the size of other models\u2019 pretraining data. While under REINFORCE fine-tuning\nloss, as in Table 4, BLIP slightly outperforms OFA in terms of average performance but has around 4\ntimes larger standard deviation than OFA.\nDifferent Components in APM.\nThe results of using different components in APM for IL and\nRL fine-tuning are presented in Table 3 and Table 2, respectively. We found that BART consistently\noutperforms other LLMs in both settings. We hypothesize that due to BART\u2019s architectural nature\n7\nTable 3: Results from different APMs fine-tuned by the imitation learning (IL) loss objective. The\nresults are shown by the average of 7 different environments in VirtualHome. The results are shown\nin %, and the best results are marked in bold.\nAPM\nSUM\nBleu-1\nBleu-2\nBleu-3\nBleu-4\nROUGE-L\nMETEOR\nCIDEr\nSPICE\nExecution Rate\nBERT\nOFA\n57.8\u00b10.92\n43.8\u00b11.02\n35.4\u00b10.63\n23.5\u00b10.77\n60.1\u00b10.41\n34.8\u00b10.62\n31.8\u00b11.31\n46.8\u00b10.80\n77.8\u00b13.26\nBLIP\n51.3\u00b10.31\n42.4\u00b10.54\n32.3\u00b10.66\n22.3\u00b10.31\n60.7\u00b10.63\n34.4\u00b10.75\n31.2\u00b10.87\n43.2\u00b10.97\n76.3\u00b15.22\nGRIT\n52.9\u00b10.18\n41.6\u00b10.87\n32.4\u00b10.72\n22.1\u00b10.68\n61.6\u00b10.53\n32.1\u00b10.33\n31.1\u00b10.25\n43.1\u00b10.76\n73.1\u00b13.11\nRoBERTa\nOFA\n57.7\u00b10.01\n43.2\u00b10.00\n35.6\u00b10.48\n24.1\u00b10.36\n59.9\u00b10.26\n34.7\u00b10.51\n31.4\u00b10.47\n47.3\u00b10.38\n75.4\u00b13.86\nBLIP\n50.5\u00b10.71\n41.1\u00b10.29\n32.0\u00b10.11\n23.5\u00b10.64\n61.1\u00b10.88\n33.0\u00b10.70\n31.8\u00b10.81\n42.9\u00b10.94\n77.7\u00b10.71\nGRIT\n53.1\u00b11.02\n42.0\u00b10.90\n34.1\u00b11.01\n23.1\u00b11.22\n60.4\u00b11.92\n31.5\u00b10.59\n31.5\u00b11.42\n42.8\u00b11.77\n75.4\u00b14.39\nBART\nOFA\n59.5\u00b10.09\n45.9\u00b10.31\n39.8\u00b10.37\n28.1\u00b10.72\n61.3\u00b10.65\n37.2\u00b10.69\n34.4\u00b10.78\n47.0\u00b10.88\n79.0\u00b11.91\nBLIP\n52.9\u00b10.80\n44.3\u00b10.52\n35.5\u00b10.49\n25.3\u00b10.62\n62.2\u00b11.12\n35.3\u00b11.62\n32.0\u00b10.97\n44.5\u00b10.88\n76.0\u00b11.98\nGRIT\n54.2\u00b11.68\n43.2\u00b11.85\n33.6\u00b11.60\n25.3\u00b10.93\n62.7\u00b11.85\n33.8\u00b10.62\n33.7\u00b10.74\n44.7\u00b11.12\n77.9\u00b11.77\nTable 4: Execution Rates by different SUM fine-tuned by REINFORCE, where BERT serves as APM.\nThe results are shown on 7 different environments in VirtualHome and also the average performance.\nThe best results are marked in bold.\nSUM\nEnv-1\nEnv-2\nEnv-3\nEnv-4\nEnv-5\nEnv-6\nEnv-7\nAverage\nOFA\n50.1\u00b10.65\n50.3\u00b10.52\n51.5\u00b10.48\n57.8\u00b10.88\n55.2\u00b10.00\n56.6\u00b10.37\n59.3\u00b10.48\n54.4\u00b10.55\nBLIP\n52.7\u00b10.78\n53.4\u00b11.00\n53.5\u00b10.92\n55.6\u00b10.68\n60.1\u00b10.49\n59.3\u00b10.91\n49.9\u00b10.90\n54.9\u00b11.99\nGRIT\n38.7\u00b11.02\n40.0\u00b11.11\n51.3\u00b10.99\n48.2\u00b10.90\n46.5\u00b10.85\n55.8\u00b10.70\n45.3\u00b11.08\n46.5\u00b12.01\nas a denoising autoencoder, it is more suitable for translating natural language descriptions into\nexecutable action programs for the VirtualHome simulator.\nDifferent Environments.\nTo test the performance variations under different environments, we\nconducted the experiments separately for each unique environment. The results are shown in Table 1\nand Table 4, for fine-tuning SUM under IL and RL settings, respectively. Due to image observation\nvariations having the most impact on SUM instead of APM, so we only test the performance of\nSUM under different environment settings. Through Table 1 and Table 4, we could find that the\nvariations exist among different environments. Generally, environment 6 seems to have the easiest\nenvironmental settings for the model to learn.\nStability.\nTo evaluate the stability of different models under different envs, we calculated the\nstandard deviation of the results across different trials. The results are shown in Tables 1,2,3,4, which\nshows that BART as APM and OFA as SUM seems more stable than the rest of the combinations.\n6\nConclusion\nIn this work, we introduce a novel robot learning paradigm with LLM in the loop that handles multiple\nmodalities of visual observations and text-based actions in a principled manner. We bridge both\nmodalities with natural language generated by a pretrained multimodal model. Our model contains\nSUM and APM, where SUM uses image observations as inputs taken by the robot to generate\nlanguage descriptions of the current scene, and APM predicts the corresponding actions for the\nnext step. We tested our method in the VirtualHome under 7 unique environments, and the results\ndemonstrated that our proposed paradigm outperforms the baselines in terms of execution rates and\nshows strong stability across environments.\nLimitations.\n(1) In our current study, we primarily focused on abstract high-level actions repre-\nsented by language commands, without taking into account low-level controls such as joint motor\ncontrol. This omission of the low-level control module may limit the overall effectiveness of the\nlearned policies and their ability to function in complex and dynamic environments. An interesting fu-\nture direction would be to consider the physical capabilities of embodied agents by learning universal\nlow-level controllers for various morphologies. (2) Our study might encounter challenges related to\nlong-tailed actions. In our collected dataset, there are actions that occur infrequently, and the current\nmethod may not have effectively learned policies for scenarios involving such actions that rarely\nappear in the collected dataset. This limitation could constrain the overall effectiveness of the learned\npolicies in real-world situations. (3) Given that we fine-tuned the model using a dataset collected in\nthe VirtualHome environment, the generalizability of the learned policies to other platforms might be\ninsufficient due to significant differences between various simulated platforms.\n8\nReferences\n[1] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL, 2019.\n[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV. Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692,\n2019.\n[4] T. B. Brown et al. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\n[5] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. R. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control. ArXiv, abs/2209.07753, 2022.\n[6] M. Ahn et al. Do as i can, not as i say: Grounding language in robotic affordances. ArXiv,\nabs/2204.01691, 2022.\n[7] A. Zeng, A. S. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. S. Ryoo,\nV. Sindhwani, J. Lee, V. Vanhoucke, and P. R. Florence. Socratic models: Composing zero-shot\nmultimodal reasoning with language. ArXiv, abs/2204.00598, 2022.\n[8] R. Zellers, A. Holtzman, M. E. Peters, R. Mottaghi, A. Kembhavi, A. Farhadi, and Y. Choi.\nPiglet: Language grounding through neuro-symbolic interaction in a 3d world. In ACL, 2021.\n[9] S. Li, X. Puig, Y. Du, C. J. Wang, E. Aky\u00a8urek, A. Torralba, J. Andreas, and I. Mordatch.\nPre-trained language models for interactive decision-making. ArXiv, abs/2202.01771, 2022.\n[10] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In ICML, 2022.\n[11] D. Fried, A. Aghajanyan, J. Lin, S. I. Wang, E. Wallace, F. Shi, R. Zhong, W. tau Yih, L. Zettle-\nmoyer, and M. Lewis. Incoder: A generative model for code infilling and synthesis. ArXiv,\nabs/2204.05999, 2022.\n[12] J. Kaplan, S. McCandlish, T. J. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,\nJ. Wu, and D. Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.\n[13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from\nnatural language supervision. In ICML, 2021.\n[14] D. Shah, B. Osinski, B. Ichter, and S. Levine. Lm-nav: Robotic navigation with large pre-trained\nmodels of language, vision, and action. ArXiv, abs/2207.04429, 2022.\n[15] W. Huang et al. Inner monologue: Embodied reasoning through planning with language models.\nArXiv, abs/2207.05608, 2022.\n[16] Y. Kant et al. Housekeep: Tidying virtual households using commonsense reasoning. ArXiv,\nabs/2205.10712, 2022.\n[17] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. D. Szlam. Clip-fields: Weakly\nsupervised semantic fields for robotic memory. ArXiv, abs/2210.05663, 2022.\n[18] Y. Jiang, A. Gupta, Z. V. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar,\nY. Zhu, and L. J. Fan. Vima: General robot manipulation with multimodal prompts. ArXiv,\nabs/2210.03094, 2022.\n9\n[19] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. ArXiv, abs/2209.05451, 2022.\n[20] A. F. C. Bucker, L. F. C. Figueredo, S. Haddadin, A. Kapoor, S. Ma, S. Vemprala, and R. Bonatti.\nLatte: Language trajectory transformer. ArXiv, abs/2208.02918, 2022.\n[21] A. Z. Ren, B. Govil, T.-Y. Yang, K. Narasimhan, and A. Majumdar. Leveraging language for\naccelerated learning of tool manipulation. ArXiv, abs/2206.13074, 2022.\n[22] A. C. Tam, N. C. Rabinowitz, A. K. Lampinen, N. A. Roy, S. C. Y. Chan, D. Strouse, J. X.\nWang, A. Banino, and F. Hill. Semantic exploration from language abstractions and pretrained\nrepresentations. ArXiv, abs/2204.05080, 2022.\n[23] A. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi. Simple but effective: Clip embeddings\nfor embodied ai. CVPR, pages 14809\u201314818, 2022.\n[24] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipula-\ntion. ArXiv, abs/2109.12098, 2021.\n[25] B. Lin, Y. Zhu, Z. Chen, X. Liang, J. zhuo Liu, and X. Liang. Adapt: Vision-language navigation\nwith modality-aligned action prompts. CVPR, pages 15375\u201315385, 2022.\n[26] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. K. Gupta. The unsurprising effectiveness of\npre-trained vision models for control. In ICML, 2022.\n[27] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Clip on wheels: Zero-shot\nobject navigation as object localization and exploration. ArXiv, abs/2203.10421, 2022.\n[28] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould. Vln-bert: A recurrent vision-and-\nlanguage bert for navigation. CVPR, pages 1643\u20131653, 2021.\n[29] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra. Improving\nvision-and-language navigation with image-text pairs from the web. ArXiv, abs/2004.14973,\n2020.\n[30] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv\npreprint arXiv:2209.11302, 2022.\n[31] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson.\nRobotic skill acquisition via instruction augmentation with vision-language models. arXiv\npreprint arXiv:2211.11736, 2022.\n[32] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale\nknowledge. arXiv preprint arXiv:2206.08853, 2022.\n[33] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. In ACL,\n2018.\n[34] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. In NAACL, 2018.\n[35] S. Ruder. Recent advances in language model fine-tuning. 2021.\n[36] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv,\nabs/1910.10683, 2019.\n[37] D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic, R. Krishnan, and D. X. Song. Pretrained\ntransformers improve out-of-distribution robustness. In ACL, 2020.\n10\n[38] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. A. Smith. Fine-tuning\npretrained language models: Weight initializations, data orders, and early stopping. ArXiv,\nabs/2002.06305, 2020.\n[39] L. Ke, W. Pei, R. Li, X. Shen, and Y.-W. Tai. Reflective decoding network for image captioning.\nICCV, pages 8887\u20138896, 2019.\n[40] M. Stefanini, M. Cornia, L. Baraldi, S. Cascianelli, G. Fiameni, and R. Cucchiara. From show to\ntell: A survey on deep learning-based image captioning. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 45:539\u2013559, 2021.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need, 2017.\n[42] H. H. Tan and M. Bansal. Lxmert: Learning cross-modality encoder representations from\ntransformers. ArXiv, abs/1908.07490, 2019.\n[43] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. In Neural Information Processing Systems, 2019.\n[44] X. Li, X. Yin, C. Li, X. Hu, P. Zhang, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, Y. Choi, and\nJ. Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.\n[45] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso, and J. Gao. Unified vision-language\npre-training for image captioning and vqa. ArXiv, abs/1909.11059, 2019.\n[46] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and\nL. Zettlemoyer.\nBart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In ACL, 2020.\n[47] K. Cho, B. van Merrienboer, C\u00b8 aglar G\u00a8ulc\u00b8ehre, D. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine\ntranslation. In EMNLP, 2014.\n[48] P. Koehn. Statistical machine translation. 2007.\n[49] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks. CoRR, abs/1511.06732, 2015.\n[50] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning, 8:229\u2013256, 1992.\n[51] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for rein-\nforcement learning with function approximation. In S. Solla, T. Leen, and K. M\u00a8uller, editors,\nNeurIPS, volume 12. MIT Press, 1999.\n[52] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. Self-critical sequence training for\nimage captioning. CVPR, pages 1179\u20131195, 2016.\n[53] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara. Meshed-memory transformer for image\ncaptioning. CVPR, pages 10575\u201310584, 2019.\n[54] X. Puig, K. K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating\nhousehold activities via programs. CVPR, pages 8494\u20138502, 2018.\n[55] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating\nhousehold activities via programs. arXiv:1806.07011 [cs], 06 2018.\n[56] Y.-H. Liao, X. Puig, M. Boben, A. Torralba, and S. Fidler. Synthesizing environment-aware\nactivities via activity sketches, 06 2019.\n11\n[57] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of\nmachine translation. In ACL, 2002.\n[58] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In ACL 2004, 2004.\n[59] S. Banerjee and A. Lavie. Meteor: An automatic metric for mt evaluation with improved\ncorrelation with human judgments. In IEEvaluation@ACL, 2005.\n[60] R. Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation.\narXiv:1411.5726 [cs], 06 2015.\n[61] P. Anderson, B. Fernando, M. Johnson, and S. Gould. SPICE: semantic propositional image\ncaption evaluation. CoRR, abs/1607.08822, 2016.\n[62] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv:1905.02450 [cs], 06 2019.\n[63] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Unifying\narchitectures, tasks, and modalities through a simple sequence-to-sequence learning framework.\narXiv:2202.03052 [cs], 02 2022.\n[64] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation. arXiv:2201.12086 [cs], 02 2022.\n[65] V.-Q. Nguyen, M. Suganuma, and T. Okatani.\nGrit: Faster and better image captioning\ntransformer using dual visual features. ArXiv, abs/2207.09666, 2022.\n[66] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 12 2015.\n[67] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable DETR: deformable transformers\nfor end-to-end object detection. CoRR, abs/2010.04159, 2020.\n[68] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language\nmodel pretraining with weak supervision. arXiv:2108.10904 [cs], 08 2021.\n[69] B. Yuhas, M. Goldstein, and T. Sejnowski. Integration of acoustic and visual speech signals using\nneural networks. IEEE Communications Magazine, 27:65\u201371, 11 1989. doi:10.1109/35.41402.\n[70] K. Tiippana. What is the mcgurk effect? Frontiers in Psychology, 5, 07 2014.\n[71] H. McGurk and J. MacDonald. Hearing lips and seeing voices. Nature, 264:746\u2013748, 1976.\n[72] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Revisiting\nvisual representations in vision-language models. In CVPR, pages 5579\u20135588, 2021.\n[73] J. Wang, X. Hu, P. Zhang, X. Li, L. Wang, L. Zhang, J. Gao, and Z. Liu. Minivlm: A smaller\nand faster vision-language model. CoRR, abs/2012.06946, 2020.\n[74] N. Gothoskar, M. L\u00b4azaro-Gredilla, A. Agarwal, Y. Bekiroglu, and D. George. Learning a\ngenerative model for robot control using visual feedback. ArXiv, abs/2003.04474, 2020.\n[75] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards universal\nvisual reward and representation via value-implicit pre-training. ArXiv, abs/2210.00030, 2022.\n[76] N. Strokina, W. Yang, J. Pajarinen, N. Serbenyuk, J.-K. K\u00a8am\u00a8ar\u00a8ainen, and R. Ghabcheloo. Visual\nrewards from observation for sequential tasks: Autonomous pile loading. Frontiers in Robotics\nand AI, 9, 2022.\n[77] A. Mohtasib, G. Neumann, and H. Cuay\u00b4ahuitl. A study on dense and sparse (visual) rewards in\nrobot policy learning. In TAROS, 2021.\n12\nA\nAlgorithms of Fine-tuning SUM and APM with Imitation Learning or\nREINFORCE\nWe provide the pseudo code for training SUM and APM in this section.\nAlgorithm 1 Fine-tuning SUM\nInitialize pretrained SUM model\nLoad VirtualHome dataset for fine-tuning\nfor n in num epochs do\nfor Imaget and Captiont in batchn do\n1.\n\u02c6\nCaptiont = SUM(Imaget)\n2.\nLossXEt(\u03b8t)\n=\nLXE(Captiont,\n\u02c6\nCaptiont)\n3.\n\u03b8t\n\u2190\n\u03b8t\n\u2212\n\u03b1\u2207\u03b8tL(Captiont,\n\u02c6\nCaptiont)\nend for\nrepeat\nSteps 1 through 3\nuntil max(num epochs) or convergence\nend for\nAlgorithm 2 Fine-tuning APM with Imitation Learn-\ning\nInitialize fine-tuned SUM and pretrained APM\nLoad VirtualHome dataset for fine-tuning\nfor n in num epochs do\nfor Imaget, Captiont Actiont in batchn do\n1.\n\u02c6\nCaptiont = SUM(Imaget)\n2.\n\u02c6\nActiont+1 = APM(\n\u02c6\nCaptiont, Actiont)\n3. LossXEt(\u03b8t) = LXE(Actiont,\n\u02c6\nActiont+1)\n4. \u03b8t \u2190 \u03b8t \u2212 \u03b1\u2207\u03b8tLXE(Actiont,\n\u02c6\nActiont+1)\nend for\nrepeat\nSteps 1 through 3\nuntil max(num epochs) or convergence\nend for\nAlgorithm 3 Fine-tuning APM with REINFORCE\nInitialize fine-tuned SUM, pretrained APM, and VirtualHome environment (env)\nLoad VirtualHome dataset for fine-tuning\nfor n in num epochs do\nTrajectoriest = [ ]\nstate = env.reset()\nfor Imaget, Captiont Actiont in batchn do\n1.\n\u02c6\nCaptiont = SUM(Imaget)\n2.\n\u02c6\nActiont = APM(\n\u02c6\nCaptiont, Actiont)\n3. Trajectoriest.append(\n\u02c6\nActiont)\nend for\nsort(Trajectoriest) by Task ID\nfor i in range(len(Trajectoriest)) do\n4.\n\u02c6\nActiont = sample action(Trajectoriest[i])\n5. Rewardt = env.step(Actiont,\n\u02c6\nActiont)\n6. Compute \u2207\u03b8t log P(\n\u02c6\nActiont|Actiont)\n7. \u03b8t \u2190 \u03b8t + \u03b1r\u2207\u03b8t log P(\n\u02c6\nActiont|Actiont)\nend for\nrepeat\nSteps 1 through 7\nuntil max(num epochs) or convergence\nend for\nB\nExperimental Setup\nSUM Setting\nFor SUM, we use the following image captioning models to serve as SUM: OFA\n[63], BLIP [64], and GRIT [65]. Both OFA and BLIP are pretrained on the same five datasets, while\nthe GRIT model [65] is pretrained on a different combination of datasets. For OFA, we adopted\nOFALarge due to its superior performance in five variations. OFALarge wields ResNet152 [66]\nmodules with 472M parameters and 12 encoders and decoder layers. For BLIP, we used ViT-L/16\nas the image encoder due to its better performance. For GRIP, we follow Nguyen et al. [65] which\nutilizes the Deformable DETR [67] framework. Note that in our study we want SUM to generate\ncaptions that not only describe the scene but also try to derive action from it. We observe that\nadding the prompt \u201da picture of \u201d following Wang et al. [68] causes the model to be biased in solely\n13\ndescribing the scene, which would in turn not be helpful for generating actionable captions. Therefore,\nwe remove prompts in the SUM setting. We load pretrained models and fine-tune them for 7 epochs\non our collected VirtualHome dataset. We keep the hyper-parameters consistent with the original\nimplementations [64, 63, 65].\nAPM Setting\nWe take LLM to act as the sole component in our APM. The goal of APM is to\ngenerate executable programs for the VirtualHome simulator. We deem the program outputted by\nthe APM executable if the agent in the VirtualHome simulator is able to understand and perform the\naction. When the action is executed by the agent, the simulator is then directed to output images and\ncaptions that are synonymous with the input of SUM. The output hidden layers of SUM acts as the\ninput embeddings to the APM, while the tokenized executable actions act as labels. The last hidden\nlayer of APM acts as input embeddings for the tokenizer and generates token identifiers. The token\nidentifiers are finally decoded into programmable actions that are fed into the VirtualHome simulator.\nC\nExperiment Parameters\nTable 5: Experiment parameters used in SUMs, where the best ones are marked in bold\nSUM\nBatch Size\nEncoder Layers\nAtt. Heads\nLearning Rate\nDropout\nEpochs\nOFA\n[4, 8, 16, 32]\n[24]\n[16]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nBLIP\n[8, 16, 32, 64]\n[12]\n[12]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nGRIT\n[4, 8, 16, 32]\n[6]\n[8]\n[1e-4, 1e-5, 1e-6]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nTable 6: Experiment parameters used in Supervised APMs, where the best ones are marked in bold\nAPM\nBatch Size\nEncoder Layers\nAtt. Heads\nLearning Rate\nDropout\nEpochs\nBERT\n[4, 8, 16, 32]\n[12]\n[12]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nBART\n[8, 16, 32, 64]\n[12]\n[16]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nRoBERTa\n[4, 8, 16, 32]\n[12]\n[12]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nTable 7: Experiment parameters used in REINFORCE APMs, where the best ones are marked in bold\nAPM\nBatch Size\nEncoder Layers\nAtt. Heads\nLearning Rate\nDropout\nEpochs\nBERT\n[4, 8, 16, 32]\n[12]\n[12]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nBART\n[8, 16, 32, 64]\n[12]\n[16]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nRoBERTa\n[4, 8, 16, 32]\n[12]\n[12]\n[1e-4, 1e-5, 1e-7]\n[0.1, 0.2, 0.3]\n[2, 5, 10, 20, 50]\nD\nMore Experimental Results\nFine-tuning performance on in-distribution tasks and unseen tasks\nTo further support our find-\nings, we conducted additional experiments that tested the fine-tuning performance on in-distribution\ntasks and unseen tasks in the VirtualHome environment following the setting in Li et al. [9]. Li et al.\n[9] used reinforcement learning to adapt to downstream tasks. It\u2019s important to note that Li et al. [9]\nused oracle text-based inputs that summarize the current observation, whereas we use raw image\ninputs and understand the scene with our fine-tuned SUM module. We measure the performance with\nthe episode success rate and summarize the main comparison results with Li et al. [9]) in Table 8. Our\nresults show that when fine-tuning with REINFORCE, our method outperforms Li et al. [9] in both\nin-distribution tasks and novel tasks. Additionally, when expert data is available in the downstream\ntasks, fine-tuning with imitation learning outperforms the REINFORCE approach.\nImportance and necessity of fine-tuning\nTo underscore the importance and necessity of fine-\ntuning, we present additional zero-shot testing performances without fine-tuning in Table 10 and\nTable 11. Our findings reveal that the episode success rate and action execution rates are significantly\nlower without fine-tuning in both methods, which highlights the crucial role that fine-tuning plays in\nimproving performance.\n14\nTable 8: Comparison of episode success rate.\nMethod\nIn-Distribution Tasks\nNovel Tasks\nLi et al. [9]\n53.7\n27.8\nOurs (REINFORCE)\n58.4\n33.7\nOurs (Imitation Learning)\n68.4\n44.8\nTable 9: Our fine-tuning results for different SUM/APM configurations in in-distribution and novel\ntasks, as well as using REINFORCE and imitation learning strategies. We measure the performance\nbased on the episode success rate.\nSUM\nAPM\nIn-Distribution REINFORCE\nNovel Tasks REINFORCE\nIn-Distribution Imitation\nNovel Tasks Imitation\nOFA\nBERT\n56.1\n31.4\n65.2\n40.7\nBART\n58.4\n33.7\n68.4\n44.8\nRoBERTa\n51.7\n32.3\n66.0\n42.8\nBLIP\nBERT\n53.7\n28.5\n61.1\n39.5\nBART\n55.2\n31.2\n64.3\n40.3\nRoBERTa\n50.6\n29.3\n62.8\n39.8\nGRIT\nBERT\n50.5\n28.8\n61.3\n40.4\nBART\n51.2\n30.0\n63.7\n39.6\nRoBERTa\n49.0\n27.1\n59.2\n38.7\nTable 10: Comparison action execution rates in zero-shot and fine-tuned settings using both REIN-\nFORCE and Imitation Learning.\nMethod\nAPM\nSUM\nREINFORCE\nImitation Learning\n1\nZero-shot\nZero-shot\n0.1\n0.1\n2\nZero-shot\nFine-tuned\n14.5\n21.4\n3\nFine-tuned\nZero-shot\n5.8\n6.9\n4\nFine-tuned\nFine-tuned\n57.2\n77.8\nTable 11: Comparison episode success rate in zero-shot and fine-tuned settings using both REIN-\nFORCE and Imitation Learning.\nMethod\nAPM\nSUM\nREINFORCE\nImitation Learning\n1\nZero-shot\nZero-shot\n0.7\n0.7\n2\nZero-shot\nFine-tuned\n16.7\n19.5\n3\nFine-tuned\nZero-shot\n7.7\n8.7\n4\nFine-tuned\nFine-tuned\n58.4\n76.8\nE\nMore Related Work\nMultimodal Learning\nFormalized multimodal learning research dates back to 1989 when [69]\nconducted an experiment that built off the McGurk Effect for audio-visual speech recognition using\nneural networks [70, 71]. Researchers in NLP and CV collaborated to make large and multimodal\ndatasets available, catering to specific downstream tasks, such as classification, translation, and\ndetection. In correlation, improvements in LLMs opened the gates to include other modalities of data,\nmost frequently visual data [63, 65, 64, 68, 14, 72, 73]. By utilizing the learned embeddings that have\nbeen pretrained on both language and image datasets, vision-language models are able to perform\nvery well. Within the above success, image captioning has been an important task in multimodal\nlearning, which aims at generating textual descriptions for the given images.\nVisual Feedback in Robot Learning\nVisual feedback is commonly used in robot learning.\nGothoskar et al. [74] learned a generative model from actions to image observations of features to\ncontrol a robot from visual feedback. Ma et al. [75] proposed a self-supervised pretrained visual\nrepresentation model which is capable of generating dense and smooth reward functions for unseen\nrobotic tasks. Strokina et al. [76] reviewed the methods of reward estimation and visual representa-\ntions used in learning-based approaches for robotics applications. Mohtasib et al. [77] studied the\nperformance of dense, sparse, visually dense, and visually sparse rewards in deep RL.\n15\n"
  },
  {
    "title": "Realistic Saliency Guided Image Enhancement",
    "link": "https://arxiv.org/pdf/2306.06092.pdf",
    "upvote": "2",
    "text": "http://yaksoy.github.io/realisticEditing/\nFind the project web page here:\nRealistic Saliency Guided Image Enhancement\nS. Mahdi H. Miangoleh1\nZoya Bylinskii2\nEric Kee2\nEli Shechtman2\nYa\u02d8g\u0131z Aksoy1\n1 Simon Fraser University\n2 Adobe Research\nFigure 1.\n(top) We develop a saliency-based image enhancement method that can be applied to multiple regions in the image to de-\nemphasize objects (steps 1, 2) or enhance subjects (steps 3, 4). (bottom) Our novel realism loss allows us to apply realistic edits to a wide\nvariety of objects while state-of-the-art methods [1,17] may generate less realistic editing results.\nAbstract\nCommon editing operations performed by profes-\nsional photographers include the cleanup operations: de-\nemphasizing distracting elements and enhancing subjects.\nThese edits are challenging, requiring a delicate balance\nbetween manipulating the viewer\u2019s attention while main-\ntaining photo realism. While recent approaches can boast\nsuccessful examples of attention attenuation or amplifica-\ntion, most of them also suffer from frequent unrealistic ed-\nits. We propose a realism loss for saliency-guided image en-\nhancement to maintain high realism across varying image\ntypes, while attenuating distractors and amplifying objects\nof interest.\nEvaluations with professional photographers\nconfirm that we achieve the dual objective of realism and ef-\nfectiveness, and outperform the recent approaches on their\nown datasets, while requiring a smaller memory footprint\nand runtime. We thus offer a viable solution for automating\nimage enhancement and photo cleanup operations.\n1. Introduction\nIn everyday photography, the composition of a photo\ntypically encompasses subjects on which the photographer\nintends to focus our attention, rather than other distracting\nthings. When distracting things cannot be avoided, photog-\nraphers routinely edit their photos to de-emphasize them.\nConversely, when the subjects are not sufficiently visible,\nphotographers routinely emphasize them. Among the most\ncommon emphasis and de-emphasis operations performed\nby professionals are the elementary ones: changing the sat-\nuration, exposure, or the color of each element. Although\nconceptually simple, these operations are challenging to ap-\nply because they must delicately balance the effects on the\nviewer attention with photo realism.\nTo automate this editing process, recent works use\nsaliency models as a guide [1,2,4,8,16,17]. These saliency\nmodels [3, 7, 10, 14, 19] aim to predict the regions in the\narXiv:2306.06092v1  [cs.CV]  9 Jun 2023\nimage that catch the viewer\u2019s attention, and saliency-guided\nimage editing methods are optimized to increase or decrease\nthe predicted saliency of a selected region.\nOptimizing\nsolely based on the predicted saliency, however, often re-\nsults in unrealistic edits, as illustrated in Fig. 1. This issue\nresults from the instability of saliency models under the im-\nage editing operations, as saliency models are trained on\nunedited images. Unrealistic edits can have low predicted\nsaliency even when they are highly noticeable to human ob-\nservers, or vice versa. This was also noted by Aberman et\nal. [1], and is illustrated in Fig. 2.\nPrevious methods tried to enforce realism using adver-\nsarial setups [2,4,8,17], GAN priors [1,8], or cycle consis-\ntency [2] but with limited success (Fig. 1). Finding the exact\npoint when an image edit stops looking realistic is challeng-\ning. Rather than focusing on the entire image, in this work,\nwe propose a method for measuring the realism of a local\nedit. To train our network, we generate realistic image ed-\nits by subtle perturbations to exposure, saturation, color or\nwhite balance, as well as very unrealistic edits by apply-\ning extreme adjustments. Although our network is trained\nwith only positive and negative examples at the extremes,\nwe successfully learn a continuous measure of realism for a\nvariety of editing operations as shown in Fig. 3.\nWe apply our realism metric to saliency-guided image\nediting by training the system to optimize the saliency of\na selected region while being penalized for deviations from\nrealism. We show that a combined loss allows us to enhance\nor suppress a selected region successfully while maintaining\nhigh realism. Our method can be also be applied to multiple\nregions in a photograph as shown in Fig. 1.\nEvaluations with professional photographers and photo\neditors confirm our claim that we maintain high realism and\nsucceed at redirecting attention in the edited photo. Further,\nour results are robust to different types of images including\nhuman faces, and are stable across different permutations\nof edit parameters. Taken together with our model size of\n26Mb and run-time of 8ms, these results demonstrate that\nwe have a more viable solution for broader use than the ap-\nproaches that are available for these tasks to date.\n2. Related Work\nVarious image enhancement methods have been intro-\nduced in the literature to amplify a region of interest\nor de-emphasise distracting regions, improve image aes-\nthetics, and redirect the viewer\u2019s attention.\nThis task\nhas been referred to as attention retargeting [15] or re-\nattentionizing [18] as well. Earlier methods [5,15,20,22,23]\nincorporated prior knowledge of saliency cues (saturation,\nsharpness, color, gamut, etc.) to guide the editing process to\nachieve the desired change in saliency. But, relying solely\non saliency cues both limits the diversity of generated ed-\nits, and creates unrealistic edits due to the lack of semantic\nOriginal image\nSaliency map\nEdited image\nSaliency map\nFigure 2. Predicted saliency maps [7] for the original images and\nedited versions, with extreme edits applied. Note that saliency\nmodels are typically trained with realistic images. This makes\nthem susceptible to inaccurate predictions for unrealistic inputs,\nas the green woman in the top row estimated to have low saliency.\nconstraints. As our experiments show, OHR [15] tends to\ngenerate unrealistic color changes that are semantically in-\ncorrect, and WRS [23] is limited to contrast and saturation\nadjustments with limited effectiveness.\nRecent works leverage saliency estimation networks [3,\n7, 10, 14, 19] to optimize for a desired saliency map in-\nstead of relying on prior saliency cues. Saliency models\nare trained to output a heatmap that represents where hu-\nman gaze would be concentrated in an image. These mod-\nels are not trained to respond to the realism of the input\nimage. Hence they might predict an inconsistent decrease\nor increase in the saliency of a region when unrealistic or\nsemantically implausible edits are applied, which would be\notherwise jarring to human viewers (Fig. 2). Using saliency\nas the only supervision can result in unrealistic images.\nTo prevent unrealistic edits, prior works enforce con-\nstraints on the allowable changes, use adversarial train-\ning [2, 4, 8, 17] or exploit learned priors from GAN-based\nmodels [1, 8]. For instance, Mechrez et al. [16] and Aber-\nman et al. (Warping) [1] constrain the result to match the in-\nput content in order to maintain its appearance. Aberman et\nal. (CNN and Recolorization) [1] use a regularization term\nthat limits the amount of change an image can undergo to\nmaintain the realism. Mejjati et al. [17] designed a global\nparametric approach to limit the editing operations to a set\nof common photographic ones. Chen et al. [2] exploit cycle\nconsistency to keep the output within the domain of the in-\nput image. Gatys et al. [4] use a texture loss alongside the\nVGG perceptual loss as a proxy for realism.\nLalonde et al. [11] argue that humans prefer color consis-\ntency within images, regardless of object semantics. They\nuse color statistics to measure realism and use it to recolor\nthe images to match the background in compositing task.\nZhu et al. [26] train a network to discriminate between nat-\nural images and computer-generated composites and use it\nas a realism measure for compositing task. Realism is also\na crucial factor in GANs, as highlighted by [9].\nTable 1. Parameter value ranges used to generate real and fake training images for the realism estimation network.\nExposure\nSaturation\nColor curve\nWhite balancing\nNumber of edits\nReal\n[0.85, 1.15]\n[0.85, 1.15]\n[0.85, 1.15]\nNot allowed\n[1, 3]\nFake\n[0.5, 0.75] \u222a [1.5, 2]\n[0, 0.5] \u222a [1.5 \u2212 2]\n[0.5, 2]\n[0.9, 1]\n[2, 4]\nFake(human specific)\n[0.5, 0.75] \u222a [1.25, 1.5]\n[0.5, 0.75] \u222a [1.25, 1.5]\n[0.5, 2]\nNot allowed\n[2, 3]\nFigure 3. The efficacy of the realism estimation network is illustrated over a range of exposure and saturation adjustments. Left is \u2206R\nplotted (vertical axis) for example images (second column) when selected regions (third column) are edited. Right, the edited images are\nshown with the corresponding change in estimated realism (inset numbers), and the value of the editing parameter applied (underneath).\nWe present a new method for estimating the realism of a\nlocal edit. Combining our realism loss with saliency guid-\nance, we show that we can successfully apply attention\nattenuation or amplification while keeping the final result\nrealistic without requiring data annotated with realism or\nbulky GAN priors to estimate realism.\n3. Realism Network\nWhen editing specific regions in an image, it is challeng-\ning to maintain the overall realism of the photograph. How\nquickly realism starts to degrade depends on the contents\nand size of the image regions, the overall composition of\nthe scene, as well as the type of edits being applied. This\nmakes the problem of defining precisely when an image edit\nstops looking realistic particularly challenging.\nIn this work, we propose to train a realism network using\nonly realistic and unrealistic examples at the extremes. We\ngenerate realistic edits by slightly perturbing image values,\nand unrealistic edits by applying aggressive edits. We show\nthat, despite being trained on binary data, our network can\nestimate continuous realism scores that can adapt to differ-\nent types of image regions and scenes. Our approach was\ninspired by the work of Zhu et al. [26], who similarly learn\ntheir realism from binary real and synthetic composites.\nTo generate real and fake samples, we exploit different\nparameter ranges for commonly used editing operations \u2013\nexposure, saturation, color curve, and white balancing (for-\nmal definitions in the Supplementary Material).\nFor in-\nstance, increasing the exposure of a region too much can\nresult in an unrealistic image, while a subtle increase to sat-\nuration will not signficantly affect the realism. Based on ex-\nperimentation, we determined the set of parameter ranges in\nTab. 1 to apply to image regions to create our training data.\nTo generate a training example, we first select a random\nnumber of edits (between 1-4), then an order for the edit op-\nerations (e.g., exposure, saturation, color curve, white bal-\nancing), and values for each of the operations, sampled uni-\nformly at random from the pre-specified ranges in Tab. 1.\nWe apply these edits in the selected order to a region seg-\nment in an MS-COCO image [12]. Fake examples are gen-\nerated by purposefully selecting extreme values. Real ex-\namples are generated by sampling subtle edits within nar-\nrower ranges. Because of the semantic importance of hu-\nman faces and increased sensitivity to edits in facial regions,\nwe enforce smaller parameter ranges when applying edits to\nfaces. Fig. 4 shows several examples.\nWe use the Pix2Pix [6] network architecture followed\nby two MLP layers to estimate the realism score R of the\ninput. For our samples in the training data, R is defined\nas 1 for real and 0 for fake samples. We also condition the\noutput on the input region by feeding the region\u2019s mask M\nas input to the network. We use squared error [13] as the\ncritic to compute the loss on the estimated value:\nLdisc = 1\n2R(Ifake, M)2 + 1\n2(R(Ireal, M) \u2212 1)2\n(1)\nwhere Ifake and Ireal are the generated fake and real sam-\nples. To measure the effect of the edit on the realism of\nthe image, we compute the difference between the scores\ninput image\nmask\nreal sample\nfake sample\nFigure 4. Examples of fake and real images that are used to train\nthe realism estimation network. See Section 3 for more details.\nestimated for the original image I and the edited image I\u2032:\n\u2206R(I\u2032, I, M) = R(I\u2032, M) \u2212 R(I, M),\n(2)\nwhere the edited region is defined by the mask M.\nAs Fig. 3 demonstrates \u2206R gives us continuous realism\nvalues for a range of edit parameters, despite the network\nbeing trained only on extreme cases. It also shows that the\nrange of edits that are considered realistic by our network\nis not the same for each image and depends on the subject\nand editing operation. We show more examples of edits that\nare classified realistic or unrealistic by our network in Fig. 9\nand the Supplementary Material.\n4. Saliency Guided Image Enhancement\nWe develop a saliency-guided image editing pipeline that\nenforces our realism loss to generate realistic and effective\nobject enhancement or distractor suppression results for a\ngiven mask. Our system can estimate a set of editing param-\neters for any permutation of 4 editing operators: exposure,\nsaturation, color curve, and white balancing.\nIn constructing our system, we borrow many ideas from\nthe existing saliency-guided image editing literature, and\nfocus our design improvements on improving the realism\nof the results, especially by including our proposed realism\nloss. Since these edit operations are non-linear, different\norderings of edits changes the end results. As a result, we\ncondition the regressed parameters on the permutation of\nthe edit operations by feeding the permutation as an input\nto the network. More details on the architecture of the net-\nwork and the embedding used to encode the permutation is\nincluded in the Supplementary Material.\nSaliency Loss\nA pretrained saliency model [7] (SalNet) is\nused as a proxy for the viewer attention that would be cap-\ntured by image regions before and after applying the edits,\nto supervise the image editing process.\nWe measure the change in the saliency of the region of\ninterest as the expected value of its relative change within\nthe masked region:\nS(I, I\u2032, M) = EM\n\u0014SalNet(I) \u2212 SalNet(I\u2032)\nSalNet(I)\n\u0015\n(3)\nwhere E denotes the expectation and M is the region mask.\nAs Fig. 2 shows, the predicted saliency heatmaps can\nchange drastically when applied to unrealistic edits. As a re-\nsult, relying on conventional metrics (e.g., absolute and rel-\native differences by [1,17], Binary cross entropy by [2] and\nKL-divergence by [4]) to measure the change in saliency\ncan cause large rewards or penalties during optimization.\nInfinitely large rewards for an unreal edit reduces the effec-\ntiveness of the realism term in the final loss function. To\ntackle this issue we define our saliency loss function as:\nLsal = exp (wsalS(I, I\u2032, M))\n(4)\nWhen saliency moves in the desired direction, the ex-\nponential squashes the loss, converging to the minimum\nand reducing the penalty quickly, acting as a soft margin.\nThis converging behaviour prevents large rewards that can\nbe generated by unrealistic edits during training. The expo-\nnential term imposes larger penalties when saliency moves\nin the wrong direction, providing robustness against outliers\nand faster convergence. wsal controls the absolute value of\nthe loss to balance the weight of saliency loss in our final\nloss, which we set to 5 and -1 for amplification and attenu-\nation, respectively.\nRealism Loss\nThe realism loss is defined as:\nLrealism = ReLU(\u2212\u2206R(I\u2032, I, M) \u2212 br)\n(5)\nThis loss is designed to penalize unrealistic edits, while\ngiving no rewards for edits that improve the estimated real-\nism score of the input. This prevents the network from being\npenalized by images that receive a low realism score even\nbefore any edits are applied. ReLU and offset br provide\na margin that allows a slight decrease in realism without a\npenalty which we set to 0.1 in our experiments.\nWe train two separate networks for each. The final net-\nwork objective is the product of the two loss functions:\nL = (1 + Lrealism) \u00d7 Lsal.\n(6)\nIn this formulation, the realism score acts as a weight for the\npenalty imposed on the change in the saliency. This allows\nus to balance the realism and saliency objectives.\nInput image\nOurs\nDeepsal [1]\nFigure 5. Saliency attenuation compared to Deepsal [1] on the\nimages provided by the authors on their project webpage. Our\nmethod is able to effectively attenuate the saliency of the target\nregion without applying an unrealistic camouflage.\nWe use an EfficientNet-lite3 [21] backbone and cascaded\nMLP layers as decoders to estimate parameters for each of\nthe edit operations. A detailed explanation of the architec-\nture specifics, datasets and training is provided in Supple-\nmentary Material.\n5. Experiments and Results\nWe compare our method against state of the art\nsaliency based image editing approaches \u2013 Deepsal [1],\nGazeshift [17] and MEC [16]. MEC provides results on\ntheir dataset alongside pre-computed results of WRS [23]\nand OHR [15] on the same dataset. We use this dataset to\ncompare against WRS and OHR as well as MEC. 1\nThe EfficientNet [21] backbone used in our architecture\nis known for its small size. Our results are thus generated\nsignificantly faster than the other state-of-the-art (SOTA)\nmethods with bulkier architectures and slower per-image\noptimizations. Based on speed measures reported in [17]\nTable 1c, MEC takes more than a day, OHR needs 30 sec-\nonds and Gazeshift takes 8 seconds to process each image,\nwhile our model requires only 8ms per image.\nWe present both qualitative and quantitative results.\nSince our method takes the permutation of the edits as input\nduring inference time we select the permutation at random\nfor the presented results unless mentioned otherwise.\n1Deepsal, WRS and MEC do not provide an open-source implementa-\ntion. Hence, we relied on the results included on their project pages. Also,\nDeepsal authors kindly provided us with results on Adobe Stock dataset\nfor their \u201cconvolutional network\u201d variation.\n5.1. Qualitative Comparison\nFigs. 5, 6, and 7 illustrate our results compared to the\nSOTA. They show our method performs different edits\nbased on the contents of the image. It can apply more sig-\nnificant color changes that camouflage the distractor (2nd\nand 4th rows of Fig. 6, 3rd row of Fig. 5) or very subtle ed-\nits for human faces (1st row of Fig. 6). The intensity and\ncharacteristics of the applied edits depends on semantics.\nThe use of adversarial loss in Gazeshift [17] and the\nregularization used in Deepsal [1] constrain the edits their\nmethods apply without taking realism explicitly into ac-\ncount. As results show, they often apply unrealistic edits\n(e.g., the camouflaged signs in Fig. 5 or the unattural skin\ntone and the color artifacts in Fig. 6) or very subtle edits\nwith lower effectiveness.\nMEC [16] reuses the color patterns and textures available\nin the image to update the target region. On the other hand\ndifferent regions and textures can correspond to different se-\nmantics. Consequently, as illustrated in Fig. 7a this method\ncan apply incompatible color and texture values to produce\nunrealistic edits (green crocodile eye, orange traffic sign) or\nineffective enhancements (brown bird). Fig. 7b provides a\ncomparison on their distractor suppression image set. Our\nmethod performs comparable in terms of effectiveness and\ngenerates realistic results consistently.\nOHR [15] tries to maximize the color distinction be-\ntween the masked region and the rest of the image for the\nimage enhancement task. Without explicit realism mod-\neling, it tends to generate unrealistic colors (e.g., blue\ncrocodile, bird, and horse in Fig. 7a). While incorrect colors\nincrease the saliency of these regions, they do so at the cost\nof realism. For similar reasons, this method is ineffective\nsuppressing distractors (Fig. 7b).\nWRS [23] does not generate unrealistic images, but also\nmakes edits that are hardly noticeable, and less effective at\nenhancing or suppressing the target regions. We believe this\nis due to the purposely limited range of allowed edit param-\neters (luminance, saturation and sharpness).\n5.2. What Do Photographers Think?\nTo include the perspective of professional photographers\nin comparing our results to others, we ran a user study. We\nreport our results using three choices of parameter order-\nings: choosing the one that achieves the Best Saliency, the\none that generates the Best Realism (according to our real-\nism estimation network), and a permutation of parameters\nselected at Random as used for the qualitative figures.\nUser Study\nWe recruited 8 professionals from UpWork,\nall of whom have multiple years of experience with photog-\nraphy, as well as using Photoshop to edit photos. We used\nthe Appen platform for hosting our rating tasks.\nAttenuation\nAmplification\nInput image\nOurs\nGazeshift [17]\nDeepsal [1]\nOurs\ngazeshift [17]\nFigure 6. Saliency modulation compared to GazeShift [17] and Deepsal [1] on Adobe Stock images from [17].\nInput image\nOHR [15]\nWRS [23]\nMechrez [16]\nOurs\n(a) Image Enhancement (Amplification)\nInput image\nOHR [15]\nWRS [23]\nMechrez [16]\nOurs\n(b) Distractor Suppression (Attenuation)\nFigure 7. Saliency modulation compared to MEC [16], WRS [23] and OHR [15] on the Mechrez dataset [16].\nOur study participants were presented with a panel of\n3 images: the original image, mask, and an edited result\nfrom one of methods evaluated. They were asked to \u201crate\neach image based on 2 criteria\u201d - effectiveness and real-\nism, with the following definitions provided for the atten-\nuate version of the task: \u201cThe images were edited to make\ncertain objects and regions less distracting. An image edit\nis effective if the masked objects/regions have indeed be-\ncome less distracting. An image edit is realistic if the photo\ndoes not look edited.\u201d For the amplify version of the task,\nthe wording for effectiveness was modified to: \u201cThe im-\nages were edited to make certain objects and regions pop-\nout (more attention-capturing, or salient). An image edit\nis effective if the masked objects/regions have indeed be-\ncome more attention-capturing.\u201d\nImages were randomly\nshuffled in each task, so the photographers rated the images\nand methods independently of each other.\nResults\nIn Tab. 2 we compare our approach to GazeShift\nand Deepsal on the 30 Adobe Stock images from [17]. We\nfind that our approach achieves significantly higher scores\nfor both effectiveness and realism compared to GazeShift in\nTable 2. Photographer ratings (on a 1 to 10 scale, higher is better)\nof effectiveness (i.e., achieve objective of attenuation or amplifi-\ncation of saliency) and realism (i.e., photo looks natural) on the\ndataset of 30 Adobe stock images. Numbers are the mean score\nacross 8 photographers, with standard deviation in parentheses.\nSaliency Attenuation\nSaliency Amplification\nMethod\nEffectiveness \u2191\nRealism \u2191\nEffectiveness \u2191\nRealism \u2191\nGazeShift [17]\n4.78 (2.89)\n5.93 (3.13)\n7.36 (2.37)\n7.07 (2.76)\nDeepSal [1]\n4.04 (2.90)\n8.49 (2.72)\n-\n-\nOurs - Best Realism\n6.56 (2.73)\n6.78 (2.70)\n7.39 (2.17)\n8.31 (1.89)\nOurs - Random\n6.36 (2.79)\n6.34 (2.88)\n7.36 (2.21)\n8.27 (1.94)\nOurs - Best Saliency\n6.64 (2.79)\n6.31 (2.70)\n7.50 (2.08)\n8.15 (2.10)\nTable 3. Photographer ratings as in Tab. 2 on (a) Mechrez [16]\ndataset and (b) the 14 images from DeepSal project webpage [1]\nSaliency Attenuation\nMethod\nEffectiveness \u2191\nRealism \u2191\nDeepsal [1]\n7.08 (2.84)\n5.82 (3.43)\nOurs - Random\n6.83 (2.52)\n7.41 (2.70)\n(a)\nSaliency Amplification\nMethod\nEffectiveness \u2191\nRealism \u2191\nMEC [16]\n7.06 (2.68)\n7.31 (2.93)\nWRS [23]\n5.41 (3.22)\n7.97 (2.70)\nOHR [15]\n7.04 (3.04)\n5.18 (3.76)\nOurs - Random\n6.24 (2.9)\n8.88 (1.74)\n(b)\nthe attenuation task. This matches our qualitative observa-\ntions that GazeShift is not successful at the task of attenuat-\ning distractor. GazeShift specializes in amplifying saliency\nin image regions, and we achieve similar performance on\nthis task, while also maintaining significantly higher real-\nism levels. In addition, results show a poor effectiveness\nscore for Deepsal as a result of subtle edits in Fig. 6. Subtle\nedits mean the realism score remains high since the results\nare almost identical to the original images.\nSince Deepsal was ineffective on Adobe Stock images,\nto provide a fair comparison we also compare to Deepsal\non 14 images they provided on their project page in Tab. 3a.\nWe achieve significantly higher realism scores while being\nsimilarly effective at reducing the saliency of the distractors.\nThis matches our qualitative observations that Deepsal edits\ncan be quite extreme and not always photo realistic.\nTab.\n3b\nshows\nuser\nstudy\nresults\non\nMechrez\ndataset [16].2\nWe used 77 images from the dataset to\nperform the user study.\nResults confirm that our results\nare superior in the realism while we achieve a comparable\neffectiveness compared MEC. WRS\u2019s low effectiveness\nyields a high realism score as its results are almost identical\nto the input; while the unrealistic color changes by OHR\nresult in low realism and effectiveness scores.\n5.3. Ablation Study\nWe trained a variation of our method in which instead of\na fixed realism score estimation model we used a discrim-\ninator as adversarial loss. We trained the discriminator as\npart of an adversarial training approach, similar to related\nwork [2,4,17]. We used the input image as the real sample\nand the generated image as the fake sample during training.\nFig. 8 illustrates sample results with this training strategy.\nSince the discriminator is trained to penalize \u201dany edits\u201d\n2Dataset has only 10 images for attenuation task, which is inadequate\nfor a meaningful user study. Hence we only provide amplification results.\nInput image\nOurs\nAdversarial training\nFigure 8. When the model trained via adversarial training pro-\nduces results that are effective at reducing saliency, the resulting\nimages are not realistic according to our user study.\nTable 4. Photographer ratings as in Tab. 2 comparing our main\nmethod to a variation with adversarial training instead of our fixed\nrealism network.\nSaliency Attenuation\nMethod\nEffectiveness \u2191\nRealism \u2191\nAdversarial Training\n5.06 (2.84)\n7.36 (3.07)\nOurs - Random\n6.36 (2.79)\n6.34 (2.88)\napplied in the previous steps of training it encourages the\nnetwork to apply subtle edits and hence a drop in effective-\nness of the method. On the other hand, due to the lack of\nexplicit realism training, the edits are unrealistic while the\neffectiveness is reasonable. Ratings reported in Tab. 4 also\nconfirm our findings.\n5.4. Diversity and Optimality of Estimated Edits\nFig. 9b illustrates the distribution of edit parameters es-\ntimated by our parameter estimation network for different\nimages on ADE20K [24,25] dataset. It shows that edit pa-\nrameters are different for each image and is based on its\ncontent. Also, it shows that the range of estimated edits is\nnot the same as the ranges used in Tab. 1 for real samples.\nTo evaluate if the estimated edits are close to optimal\nwith respect to realism, we provide Fig. 9a. In the figure we\nshow a realism heatmap obtained by adding a small additive\nconstant to the estimated edit parameter of saturation and\nexposure. Heatmaps shows the estimated edit parameters\n(center of the heatmap) are in the optimal realism region.\nChanging the edit parameters in each direction reduces the\nrealism of the end result.\n5.5. Generalization to Multiple Image Regions\nSince our model only modifies the region of interest, and\nperforms a forward pass efficiently, we can run it on mul-\ntiple regions and multiple masks by generating edit param-\neters for each region, in an iterative manner. Examples are\nprovided in Figs. 1 and 10. We used the same approach\nwith Gazeshift [17], which edits the whole image by esti-\nmating two sets of edit parameters, one for the region of\ninterest (foreground) and one for the background. This for-\nmulation of Gazeshift makes iterative editing impractical,\nsince there would be contradictory objectives between the\n(a) A heatmap visualizes the realism score achieved when we change the estimated satura-\ntion (x-axis) and exposure (y-axis). Our estimated values (center of the heatmap) achieve\nthe optimal realism while changing the parameters in any direction reduces the realism.\nSample edited images and their corresponding location in the heatmap are also visualized.\n(b) The diversity of estimated parameters on ADE20K [24,\n25] dataset. The x-axis is the range of each parameter. The\nattenuation task is blue, and amplification is labeled red.\nFigure 9. Visualizing diversity and optimality of edit pararmeters estimated by our method\nFigure 10. Given an input image and masks to attenuate and am-\nplify(left), Gazeshift when used iteratively on each object suffers\nfrom color artifacts (center top, faces, bowl and watermelons).\nOurs produces a notably more realistic and effective result (right).\nContradictory objective of edits applied to background and fore-\nground, Gazeshift fails to generalize to multiple regions and omit-\nting the background edits (center bottom) reduces the effectiveness\nof the edits. Image credit: @tysonbrand\niterations (what is foreground in one iteration becomes a\nbackground in the next iteration). For a more practical com-\nparison, we omit background edits when running Gazeshift.\nFigure 10 shows that Gazeshift performance suffers on an\niterative saliency enhancement task, but our method is able\nto generalize to multiple regions robustly.\n5.6. Limitations\nThe global edits (applying the same edits to every pixel\ninside a mask), used in both our method and Gazeshift [17]\nrequire an accurate mask of the target region. As shown in\nFig. 11 mask imperfections can cause unsmooth transitions\naround the boundaries. In these cases, pixel-wise optimiza-\ntion approaches likes Deepsal [1] and MEC [16] do not suf-\nfer from heavy artifacts due to mask imperfections.\nFigure 11. The effect of non-smooth mask boundaries. Left, an\ninput image has a mask with a sharp edge. Center, our method and\nGazeshift [17] produce strong boundary artifacts around the mask\nregion (see inset). Right, MEC [16] and Deepsal [1] do not exhibit\nthis problem because they operate in a pixel-wise manner.\n6. Conclusion and Future work\nWe describe a method to edit images using conventional\nimage editing operators to attenuate or amplify the attention\ncaptured by a target region while preserving image realism.\nRealism is achieved by introducing an explicit, and separate\nrealism network that is pre-trained to distinguish edited im-\nages. This strategy to achieve realism is distinct from pre-\nvailing approaches, including adversarial training schemes,\nas it introduces an additional form of weak supervision\u2014\nmanually specified ranges of parameter values that corre-\nspond to realistic and unrealistic (\u201cfake\u201d) edits. Training\nwith this realism critic makes it possible to estimate saliency\nmodulating image edits that are significantly more realistic\nand robust. Together with our millisecond-level inference\ntime, our approach offers a practical and deployable appli-\ncation of saliency guided image editing.\nReferences\n[1] Kfir Aberman, Junfeng He, Yossi Gandelsman, Inbar\nMosseri, David E. Jacobs, Kai Kohlhoff, Yael Pritch, and\nMichael Rubinstein. Deep saliency prior for reducing visual\ndistraction. In Proc. CVPR, 2021. 1, 2, 4, 5, 6, 7, 8\n[2] Yen-Chung Chen, Keng-Jui Chang, Yi-Hsuan Tsai, Yu-\nChiang Frank Wang, and Wei-Chen Chiu. Guide your eyes:\nLearning image manipulation under saliency guidance. In\nProc. BMVC, 2019. 1, 2, 4, 7\n[3] Camilo Fosco, Anelise Newman, Pat Sukhum, Yun Bin\nZhang, Nanxuan Zhao, Aude Oliva, and Zoya Bylinskii.\nHow much time do you have?\nmodeling multi-duration\nsaliency. In Proc. CVPR, 2020. 1, 2\n[4] Leon A. Gatys, Matthias K\u00a8ummerer, Thomas S. A. Wallis,\nand Matthias Bethge. Guiding human gaze with convolu-\ntional neural networks. arXiv:2109.01980 [cs.CV],\n2017. 1, 2, 4, 7\n[5] Aiko\nHagiwara,\nAkihiro\nSugimoto,\nand\nKazuhiko\nKawamoto.\nSaliency-based image editing for guiding\nvisual attention. In Proc. PETMEI, 2011. 2\n[6] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proc. CVPR, 2017. 3\n[7] Sen Jia and Neil D.B. Bruce. EML-NET: An Expandable\nMulti-layer NETwork for saliency prediction.\nImage Vis.\nComput., 95:103887, 2020. 1, 2, 4\n[8] Lai Jiang, Mai Xu, Xiaofei Wang, and Leonid Sigal.\nSaliency-guided image translation.\nIn Proc. CVPR, 2021.\n1, 2\n[9] Alexia Jolicoeur-Martineau. The relativistic discriminator:\na key element missing from standard gan. In Proc. ICLR,\n2019. 2\n[10] Matthias Kummerer, Thomas SA Wallis, Leon A Gatys, and\nMatthias Bethge. Understanding low-and high-level contri-\nbutions to fixation prediction. In Proc. CVPR, 2017. 1, 2\n[11] Jean-Francois Lalonde and Alexei A Efros. Using color com-\npatibility for assessing image realism. In Proc. ICCV, 2007.\n2\n[12] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nProc. ECCV, 2014. 3\n[13] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen\nWang, and Stephen Paul Smolley. Least squares generative\nadversarial networks. In Proc. ICCV, 2017. 3\n[14] R. Margolin, A. Tal, and L. Zelnik-Manor. What makes a\npatch distinct? In Proc. CVPR, 2013. 1, 2\n[15] Victor A. Mateescu and Ivan V. Baji\u00b4c. Attention retargeting\nby color manipulation in images. In Proc. PIPV, 2014. 2, 5,\n6, 7\n[16] Roey Mechrez, Eli Shechtman, and Lihi Zelnik-Manor.\nSaliency driven image manipulation.\nMach. Vis. Appl.,\n30(2):189\u2013202, 2019. 1, 2, 5, 6, 7, 8\n[17] Youssef Alami Mejjati, Celso F. Gomez, Kwang In Kim, Eli\nShechtman, and Zoya Bylinskii. Look here! a parametric\nlearning based approach to redirect visual attention. In Proc.\nECCV, 2020. 1, 2, 4, 5, 6, 7, 8\n[18] Tam Nguyen, Bingbing ni, Hairong Liu, Wei Xia, Jiebo\nLuo, Mohan Kankanhalli, and Shuicheng Yan.\nImage re-\nattentionizing.\nIEEE Trans. Multimed., 15(8):1910\u20131919,\n2013. 2\n[19] Junting Pan, Cristian Canton, Kevin McGuinness, Noel E.\nO\u2019Connor, Jordi Torres, Elisa Sayrol, and Xavier and Giro-\ni Nieto. Salgan: Visual saliency prediction with generative\nadversarial networks.\narXiv:1701.01081 [cs.CV],\n2017. 1, 2\n[20] Sara L. Su, Fr\u00b4edo Durand, and Maneesh Agrawala.\nDe-\nemphasis of distracting image regions using texture power\nmaps. In Proc. APGV, 2005. 2\n[21] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In Proc. ICML,\n2019. 5\n[22] Javier Vazquez-Corral and Marcelo Bertalm\u00b4\u0131o. Gamut map-\nping for visual attention retargeting. In Proc. CIC, 2017. 2\n[23] Lai-Kuan Wong and Kok-Lim Low. Saliency retargeting: An\napproach to enhance image aesthetics. In Proc. WACV, 2011.\n2, 5, 6, 7\n[24] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proc. CVPR, 2017. 7, 8\n[25] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. Int. J. Com-\nput. Vision, 127(3):302\u2013321, 2019. 7, 8\n[26] Jun-Yan Zhu, Philipp Kr\u00a8ahenb\u00a8uhl, Eli Shechtman, and\nAlexei A. Efros. Learning a discriminative model for the\nperception of realism in composite images. In Proc. ICCV,\n2015. 2, 3\n"
  }
]