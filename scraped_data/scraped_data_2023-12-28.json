[
  {
    "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
    "link": "https://arxiv.org/pdf/2312.15166.pdf",
    "upvote": "52",
    "text": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective\nDepth Up-Scaling\nDahyun Kim\u2217, Chanjun Park\u2217\u2020, Sanghoon Kim\u2217\u2020, Wonsung Lee\u2217\u2020, Wonho Song\nYunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim\nChangbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim\nMikyoung Cha, Hwalsuk Lee\u2020, Sunghun Kim\u2020\nUpstage AI, South Korea\n{kdahyun, chanjun.park,limerobot, wonsung.lee, hwalsuk.lee, hunkim}@upstage.ai\nAbstract\nWe introduce SOLAR 10.7B, a large language\nmodel (LLM) with 10.7 billion parameters,\ndemonstrating superior performance in various\nnatural language processing (NLP) tasks. In-\nspired by recent efforts to efficiently up-scale\nLLMs, we present a method for scaling LLMs\ncalled depth up-scaling (DUS), which encom-\npasses depthwise scaling and continued pre-\ntraining. In contrast to other LLM up-scaling\nmethods that use mixture-of-experts, DUS does\nnot require complex changes to train and infer-\nence efficiently. We show experimentally that\nDUS is simple yet effective in scaling up high-\nperformance LLMs from small ones. Building\non the DUS model, we additionally present SO-\nLAR 10.7B-Instruct, a variant fine-tuned for\ninstruction-following capabilities, surpassing\nMixtral-8x7B-Instruct. SOLAR 10.7B is pub-\nlicly available under the Apache 2.0 license,\npromoting broad access and application in the\nLLM field 1.\n1\nIntroduction\nThe field of natural language processing (NLP)\nhas been significantly transformed by the introduc-\ntion of large language models (LLMs), which have\nenhanced our understanding and interaction with\nhuman language (Zhang et al., 2023a). These ad-\nvancements bring challenges such as the increased\nneed to train ever larger models (Rae et al., 2021;\nWang et al., 2023; Pan et al., 2023; Lian, 2023;\nYao et al., 2023; Gesmundo and Maile, 2023) ow-\ning to the performance scaling law (Kaplan et al.,\n2020; Hernandez et al., 2021; Anil et al., 2023;\nKaddour et al., 2023). To efficiently tackle the\nabove, recent works in scaling language models\nsuch as a mixture of experts (MoE) (Shazeer et al.,\n2017; Komatsuzaki et al., 2022) have been pro-\nposed. While those approaches are able to effi-\n\u2217Equal Contribution \u2020 Corresponding Author\n1https://huggingface.co/upstage/\nSOLAR-10.7B-v1.0\nciently and effectively scale-up LLMs, they often\nrequire non-trivial changes to the training and infer-\nence framework (Gale et al., 2023), which hinders\nwidespread applicability. Effectively and efficiently\nscaling up LLMs whilst also retaining the simplic-\nity for ease of use is an important problem (Alberts\net al., 2023; Fraiwan and Khasawneh, 2023; Sallam\net al., 2023; Bahrini et al., 2023).\nInspired by\nKomatsuzaki et al. (2022), we\npresent depth up-scaling (DUS), an effective and\nefficient method to up-scale LLMs whilst also re-\nmaining straightforward to use. DUS consists of\nscaling the base model along the depth dimension\nand continually pretraining the scaled model. Un-\nlike (Komatsuzaki et al., 2022), DUS does not scale\nthe model using MoE and rather use a depthwise\nscaling method analogous to Tan and Le (2019)\nwhich is adapted for the LLM architecture. Thus,\nthere are no additional modules or dynamism as\nwith MoE, making DUS immediately compatible\nwith easy-to-use LLM frameworks such as Hug-\ngingFace (Wolf et al., 2019) with no changes to\nthe training or inference framework for maximal\nefficiency. Furthermore, DUS is applicable to all\ntransformer architectures, opening up new gate-\nways to effectively and efficiently scale-up LLMs\nin a simple manner. Using DUS, we release SO-\nLAR 10.7B, an LLM with 10.7 billion parameters,\nthat outperforms existing models like Llama 2 (Tou-\nvron et al., 2023) and Mistral 7B (Jiang et al., 2023)\nin various benchmarks.\nWe have also developed SOLAR 10.7B-Instruct,\na variant fine-tuned for tasks requiring strict adher-\nence to complex instructions. It significantly out-\nperforms the Mixtral-8x7B-Instruct model across\nvarious evaluation metrics, evidencing an advanced\nproficiency that exceeds the capabilities of even\nlarger models in terms of benchmark performance.\nBy releasing SOLAR 10.7B under the Apache\n2.0 license, we aim to promote collaboration and in-\nnovation in NLP. This open-source approach allows\narXiv:2312.15166v2  [cs.CL]  29 Dec 2023\nFigure 1: Depth up-scaling for the case with n = 32, s = 48, and m = 8. Depth up-scaling is achieved through a\ndual-stage process of depthwise scaling followed by continued pretraining.\nfor wider access and application of these models\nby researchers and developers globally.\n2\nDepth Up-Scaling\nTo efficiently scale-up LLMs, we aim to utilize pre-\ntrained weights of base models to scale up to larger\nLLMs (Komatsuzaki et al., 2022). While exist-\ning methods such as Komatsuzaki et al. (2022) use\nMoE (Shazeer et al., 2017) to scale-up the model ar-\nchitecture, we opt for a different depthwise scaling\nstrategy inspired by Tan and Le (2019). We then\ncontinually pretrain the scaled model as just scaling\nthe model without further pretraining degrades the\nperformance.\nBase model.\nAny n-layer transformer architec-\nture can be used but we select the 32-layer Llama\n2 architecture as our base model. We initialize the\nLlama 2 architecture with pretrained weights from\nMistral 7B, as it is one of the top performers com-\npatible with the Llama 2 architecture. By adopting\nthe Llama 2 architecture for our base model, we\naim to leverage the vast pool of community re-\nsources while introducing novel modifications to\nfurther enhance its capabilities.\nDepthwise scaling.\nFrom the base model with n\nlayers, we set the target layer count s for the scaled\nmodel, which is largely dictated by the available\nhardware.\nWith the above, the depthwise scaling process\nis as follows. The base model with n layers is\nduplicated for subsequent modification. Then, we\nremove the final m layers from the original model\nand the initial m layers from its duplicate, thus\nforming two distinct models with n \u2212 m layers.\nThese two models are concatenated to form a scaled\nmodel with s = 2\u00b7(n\u2212m) layers. Note that n = 32\nfrom our base model and we set s = 48 considering\nour hardware constraints and the efficiency of the\nscaled model, i.e., fitting between 7 and 13 billion\nparameters. Naturally, this leads to the removal of\nm = 8 layers. The depthwise scaling process with\nn = 32, s = 48, and m = 8 is depicted in \u2018Step 1:\nDepthwise Scaling\u2019 of Fig. 1.\nWe note that a method in the community that also\nscale the model in the same manner 2 as \u2018Step 1:\nDepthwise Scaling\u2019 of Fig. 1 has been concurrently\ndeveloped.\nContinued pretraining.\nThe performance of the\ndepthwise scaled model initially drops below that\nof the base LLM. Thus, we additionally apply\nthe continued pretraining step as shown in \u2018Step\n2: Continued Pretraining\u2019 of Fig. 1. Experimen-\ntally, we observe rapid performance recovery of\nthe scaled model during continued pretraining, a\nphenomenon also observed in Komatsuzaki et al.\n(2022). We consider that the particular way of\ndepthwise scaling has isolated the heterogeneity\nin the scaled model which allowed for this fast\nperformance recovery.\nDelving deeper into the heterogeneity of the\nscaled model, a simpler alternative to depthwise\nscaling could be to just repeat its layers once more,\ni.e., from n to 2n layers. Then, the \u2018layer distance\u2019,\nor the difference in the layer indices in the base\nmodel, is only bigger than 1 where layers n and\nn + 1 are connected, i.e., at the seam.\nHowever, this results in maximum layer distance\nat the seam, which may be too significant of a\ndiscrepancy for continued pretraining to quickly\nresolve. Instead, depthwise scaling sacrifices the\n2m middle layers, thereby reducing the discrep-\nancy at the seam and making it easier for continued\n2https://huggingface.co/Undi95/\nMistral-11B-v0.1\nProperties\nTraining Datasets\nInstruction\nAlignment\nAlpaca-GPT4\nOpenOrca\nSynth. Math-Instruct\nOrca DPO Pairs\nUltrafeedback Cleaned\nSynth. Math-Alignment\nTotal # Samples\n52K\n2.91M\n126K\n12.9K\n60.8K\n126K\nMaximum # Samples Used\n52K\n100K\n52K\n12.9K\n60.8K\n20.1K\nOpen Source\nO\nO\n\u2717\nO\nO\n\u2717\nTable 1: Training datasets used for the instruction and alignment tuning stages, respectively. For the instruction\ntuning process, we utilized the Alpaca-GPT4 (Peng et al., 2023), OpenOrca (Mukherjee et al., 2023), and Synth.\nMath-Instruct datasets, while for the alignment tuning, we employed the Orca DPO Pairs (Intel, 2023), Ultrafeedback\nCleaned (Cui et al., 2023; Ivison et al., 2023), and Synth. Math-Alignment datasets. The \u2018Total # Samples\u2018 indicates\nthe total number of samples in the entire dataset. The \u2018Maximum # Samples Used\u2018 indicates the actual maximum\nnumber of samples that were used in training, which could be lower than the total number of samples in a given\ndataset. \u2018Open Source\u2018 indicates whether the dataset is open-sourced.\npretraining to quickly recover performance. We\nattribute the success of DUS to reducing such dis-\ncrepancies in both the depthwise scaling and the\ncontinued pretraining steps. We also hypothesize\nthat other methods of depthwise scaling could also\nwork for DUS, as long as the discrepancy in the\nscaled model is sufficiently contained before the\ncontinued pretraining step.\nComparison to other up-scaling methods.\nUn-\nlike Komatsuzaki et al. (2022), depthwise scaled\nmodels do not require additional modules like gat-\ning networks or dynamic expert selection. Conse-\nquently, scaled models in DUS do not necessitate\na distinct training framework for optimal training\nefficiency, nor do they require specialized CUDA\nkernels for fast inference. A DUS model can seam-\nlessly integrate into existing training and inference\nframeworks while maintaining high efficiency.\n3\nTraining Details\nAfter DUS, including continued pretraining, we\nperform fine-tuning of SOLAR 10.7B in two stages:\n1) instruction tuning and 2) alignment tuning.\nInstruction tuning.\nIn the instruction tuning\nstage, the model is trained to follow instructions in\na QA format (Zhang et al., 2023b). We mostly use\nopen-source datasets but also synthesize a math QA\ndataset to enhance the model\u2019s mathematical capa-\nbilities. A rundown of how we crafted the dataset is\nas follows. First, seed math data are collected from\nthe Math (Hendrycks et al., 2021) dataset only, to\navoid contamination with commonly used bench-\nmark datasets such as GSM8K (Cobbe et al., 2021).\nThen, using a process similar to MetaMath (Yu\net al., 2023), we rephrase the questions and an-\nswers of the seed math data. We use the resulting\nrephrased question-answer pairs as a QA dataset\nand call it \u2018Synth. Math-Instruct\u2018.\nAlignment tuning.\nIn the alignment tuning stage,\nthe instruction-tuned model is further fine-tuned to\nbe more aligned with human or strong AI (e.g.,\nGPT4 (OpenAI, 2023)) preferences using direct\npreference optimization (DPO) (Rafailov et al.,\n2023). Similar to the instruction tuning stage, we\nuse mostly open-source datasets but also synthe-\nsize a math-focused alignment dataset utilizing the\n\u2018Synth. Math-Instruct\u2018 dataset mentioned in the\ninstruction tuning stage.\nThe alignment data synthesis process is as\nfollows.\nWe take advantage of the fact that\nthe rephrased question-answer pairs in Synth.\nMath-Instruct data are beneficial in enhancing the\nmodel\u2019s mathematical capabilities (see Sec. 4.3.1).\nThus, we speculate that the rephrased answer to the\nrephrased question is a better answer than the orig-\ninal answer, possibly due to the interim rephrasing\nstep. Consequently, we set the rephrased question\nas the prompt and use the rephrased answer as the\nchosen response and the original answer as the re-\njected response and create the {prompt, chosen,\nrejected} DPO tuple. We aggregate the tuples from\nthe rephrased question-answer pairs and call the\nresulting dataset \u2018Synth. Math-Alignment\u2018.\n4\nResults\n4.1\nExperimental Details\nTraining datasets.\nWe present details regarding\nour training datasets for the instruction and align-\nment tuning stages in Tab. 1. We do not always\nuse the entire dataset and instead subsample a set\namount. Note that most of our training data is\nopen-source, and the undisclosed datasets can be\nsubstituted for open-source alternatives such as the\nMetaMathQA (Yu et al., 2023) dataset.\nModel\nSize\nType\nH6 (Avg.)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nSOLAR 10.7B-Instruct\n\u223c 11B\nAlignment-tuned\n74.20\n71.08\n88.16\n66.21\n71.43\n83.58\n64.75\nQwen 72B\n\u223c 72B\nPretrained\n73.60\n65.19\n85.94\n77.37\n60.19\n82.48\n70.43\nMixtral 8x7B-Instruct-v0.1\n\u223c 47B\nInstruction-tuned\n72.62\n70.22\n87.63\n71.16\n64.58\n81.37\n60.73\nYi 34B-200K\n\u223c 34B\nPretrained\n70.81\n65.36\n85.58\n76.06\n53.64\n82.56\n61.64\nYi 34B\n\u223c 34B\nPretrained\n69.42\n64.59\n85.69\n76.35\n56.23\n83.03\n50.64\nMixtral 8x7B-v0.1\n\u223c 47B\nPretrained\n68.42\n66.04\n86.49\n71.82\n46.78\n81.93\n57.47\nLlama 2 70B\n\u223c 70B\nPretrained\n67.87\n67.32\n87.33\n69.83\n44.92\n83.74\n54.06\nFalcon 180B\n\u223c 180B\nPretrained\n67.85\n69.45\n88.86\n70.50\n45.47\n86.90\n45.94\nSOLAR 10.7B\n\u223c 11B\nPretrained\n66.04\n61.95\n84.60\n65.48\n45.04\n83.66\n55.50\nQwen 14B\n\u223c 14B\nPretrained\n65.86\n58.28\n83.99\n67.70\n49.43\n76.80\n58.98\nMistral 7B-Instruct-v0.2\n\u223c 7B\nInstruction-tuned\n65.71\n63.14\n84.88\n60.78\n68.26\n77.19\n40.03\nYi 34B-Chat\n\u223c 34B\nInstruction-tuned\n65.32\n65.44\n84.16\n74.90\n55.37\n80.11\n31.92\nMistral 7B\n\u223c 7B\nPretrained\n60.97\n59.98\n83.31\n64.16\n42.15\n78.37\n37.83\nTable 2: Evaluation results for SOLAR 10.7B and SOLAR 10.7B-Instruct along with other top-performing models.\nWe report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score (average of six tasks). We also\nreport the size of the models in units of billions of parameters. The type indicates the training stage of the model\nand is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on SOLAR 10.7B are colored\npurple. The best scores for H6 and the individual tasks are shown in bold.\nWe reformatted the instruction datasets with an\nAlpaca-styled chat template. For datasets such as\nOpenOrca, which are derived from FLAN (Long-\npre et al., 2023), we filter data that overlaps with\nthe benchmark datasets (see Tab. 8 in Appendix. C\nfor more information). The alignment datasets are\nin the {prompt, chosen, rejected} triplet format.\nWe preprocess the alignment datasets following\nZephyr (Tunstall et al., 2023).\nEvaluation.\nIn the HuggingFace Open LLM\nLeaderboard (Beeching et al., 2023), six types of\nevaluation methods are presented: ARC (Clark\net al., 2018), HellaSWAG (Zellers et al., 2019),\nMMLU (Hendrycks et al., 2020), TruthfulQA (Lin\net al., 2022), Winogrande (Sakaguchi et al., 2021),\nand GSM8K (Cobbe et al., 2021). We utilize these\ndatasets as benchmarks for evaluation and also re-\nport the average scores for the six tasks, e.g., H6.\nModel merging.\nModel merging methods such\nas Yadav et al. (2023) can boost model perfor-\nmance without further training. We merge some\nof the models that we trained in both the instruc-\ntion and alignment tuning stages. We implement\nour own merging methods although popular open\nsource also exist such as MergeKit3.\n4.2\nMain Results\nWe present evaluation results for our SOLAR\n10.7B and SOLAR 10.7B-Instruct models along\nwith other top-performing models in Tab. 2. SO-\nLAR 10.7B outperforms other pretrained models\nof similar sizes, such as Qwen 14B and Mistral\n7B, which shows that DUS is an effective method\nto up-scale base LLMs. Furthermore, despite the\n3https://github.com/cg123/mergekit\nsmaller size, SOLAR 10.7B-Instruct scores the\nhighest in terms of H6, even surpassing the recent\ntop-performing open-source LLM Mixtral 8x7B-\nInstruct-v0.1 or Qwen 72B. The above results indi-\ncate DUS can up-scale models that are capable of\nachieving state-of-the-art performance when fine-\ntuned. We also report data contamination results\nfor SOLAR 10.7B-Instruct in Appendix C.\n4.3\nAblation Studies\nWe present ablation studies for both the instruction\nand alignment tuning stages.\n4.3.1\nInstruction Tuning\nAblation on the training datasets.\nWe present\nablation studies using different training datasets\nfor the instruction tuning in Tab. 3. The ablated\nmodels are prefixed with SFT for supervised fine-\ntuning.\n\u2018SFT v1\u2019 only uses the Alpaca-GPT4\ndataset, whereas \u2018SFT v2\u2019 also uses the OpenOrca\ndataset. \u2018SFT v3\u2019 uses the Synth. Math-Instruct\ndataset along with the datasets used in \u2018SFT v2\u2019.\nSimilarly, \u2018SFT v4\u2019 uses the Synth. Math-Instruct\ndataset along with the datasets used in \u2018SFT v1\u2019.\nFirst,\nwe analyze how Alpaca-GPT4 and\nOpenOrca affect the trained models. The first ab-\nlated model, \u2018SFT v1\u2019, which used only the Alpaca-\nGPT4 dataset for training, resulted in 69.15 for H6.\nWhen we add the OpenOrca dataset to train the\nsecond ablated model, \u2018SFT v2\u2019, the resulting H6\nscore is 69.21, which is little change from 69.15 of\n\u2018SFT v1\u2019. However, the task scores vary more as\n\u2018SFT v2\u2019 gets a substantially higher GSM8K score\nof 57.32 compared to 52.24 of \u2018SFT v1\u2019 but also\ngets noticeably lower scores across the board for\nARC, HellaSwag, and TruthfulQA. This seems to\nModel\nAlpaca-GPT4\nOpenOrca\nSynth. Math-Instruct\nH6 (Avg.)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nSFT v1\nO\n\u2717\n\u2717\n69.15\n67.66\n86.03\n65.88\n60.12\n82.95\n52.24\nSFT v2\nO\nO\n\u2717\n69.21\n65.36\n85.39\n65.93\n58.47\n82.79\n57.32\nSFT v3\nO\nO\nO\n70.03\n65.87\n85.55\n65.31\n57.93\n81.37\n64.14\nSFT v4\nO\n\u2717\nO\n70.88\n67.32\n85.87\n65.87\n58.97\n82.48\n64.75\nSFT v3 + v4\nO\nO\nO\n71.11\n67.32\n85.96\n65.95\n58.80\n2.08\n66.57\nTable 3: Ablation studies on the different datasets used for instruction tuning. \u2018SFT v3+v4\u2019 indicates that the model\nis merged from \u2018SFT v3\u2019 and \u2018SFT v4\u2019 by simply averaging the model weights. The best scores for H6 and the\nindividual tasks are shown in bold.\nModel\nUltrafeedback Clean\nSynth. Math-Alignment\nH6 (Avg.)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nDPO v1\nO\n\u2717\n73.06\n71.42\n88.49\n66.14\n72.04\n81.45\n58.83\nDPO v2\nO\nO\n73.42\n71.50\n88.28\n65.97\n71.71\n82.79\n60.27\nDPO v1 + v2\nO\nO\n73.21\n71.33\n88.36\n65.92\n72.65\n82.79\n58.23\nTable 4: Ablation studies on the different datasets used during the direct preference optimization (DPO) stage.\n\u2018SFT v3\u2019 is used as the SFT base model for DPO. We name ablated models with the \u2018DPO\u2019 prefix to indicate the\nalignment tuning stage. \u2018DPO v1+v2\u2019 indicates that the model is merged from \u2018DPO v1\u2019 and \u2018DPO v2\u2019 by simply\naveraging the model weights. The best scores for H6 and the individual tasks are shown in bold.\nModel\nBase SFT Model\nH6 (Avg.)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nDPO v2\nSFT v3\n73.42\n71.50\n88.28\n65.97\n71.71\n82.79\n60.27\nDPO v3\nSFT v3 + v4\n73.58\n71.33\n88.08\n65.39\n72.45\n81.93\n62.32\nTable 5: Ablation studies on the different SFT base models used during the direct preference optimization (DPO)\nstage. Ultrafeedback Clean and Synth. Math-Alignment datasets are used. We name ablated models with the \u2018DPO\u2019\nprefix to indicate the alignment tuning stage. The best scores for H6 and the individual tasks are shown in bold.\nindicate that using OpenOrca results in a model that\nbehaves differently from using only Alpaca-GPT4.\nSecond, we investigate whether Synth. Math-\nInstruct dataset is beneficial. For \u2018SFT v3\u2019, we\nadd the Synth. Math-Instruct dataset, which boosts\nGSM8K scores to 64.14 and achieves comparable\nscores for the other tasks. Interestingly, when we\nadd the Synth. Math-Instruct dataset to \u2018SFT v1\u2019\nto train \u2018SFT v4\u2019, we get our highest H6 score of\n70.88 with higher scores than \u2018SFT v3\u2019 for all tasks.\nFrom the above, we can see that adding the Synth.\nMath-Instruct dataset is helpful.\nLastly, we see whether merging models trained\nwith and without OpenOrca can boost performance.\nIn the first analysis, we saw that using OpenOrca re-\nsulted in a model that behaved differently from the\nmodel that was trained without OpenOrca. Build-\ning on this intuition, we merge \u2018SFT v3\u2019 and \u2018SFT\nv4\u2019 as they are the best-performing models with\nand without OpenOrca. To our surprise, the result-\ning merged model \u2018SFT v3+v4\u2019 retains the high\nscores for non-GSM8K tasks from \u2018SFT v4\u2019 but\nalso achieves a higher GSM8K score than \u2018SFT v3\u2019\nor \u2018SFT v4\u2019. Thus, we see that merging models\nthat specialize in different tasks is a promising way\nto obtain a model that performs well generally.\n4.3.2\nAlignment Tuning\nAs we utilize DPO for practical alignment tuning,\nthere are additional aspects to ablate such as the\nSFT base models used. Thus, we present ablations\nfor the different training datasets used for training,\nthe different SFT base models to initialize the DPO\nmodel, and finally, the model merging strategy to\nobtain the final alignment-tuned model.\nAblation on the training datasets.\nWe ablate on\nthe different alignment datasets used during DPO\nin Tab. 4. We use \u2018SFT v3\u2019 as the SFT base model\nfor DPO. \u2018DPO v1\u2019 only uses the Ultrafeedback\nClean dataset while \u2018DPO v2\u2019 also used the Synth.\nMath-Alignment dataset.\nFirst, we test how Ultrafeedback Clean and\nSynth.\nMath-Alignment impacts model perfor-\nmance. For \u2018DPO v1\u2019, it achieves 73.06 in H6,\nwhich is a substantial boost from the SFT base\nmodel score of 70.03. However, we note that while\nscores for tasks like ARC, HellaSwag, and Truth-\nfulQA all improved by good margins, the score\nfor GSM8K is 58.83, which is lower than the\nSFT base model score of 64.14. Adding Synth.\nMath-Alignment to train \u2018DPO v2\u2019, we see that\nthe GSM8k score improves to 60.27, which is\nlower than the SFT base model but still higher\nthan \u2018DPO v1\u2019. Other task scores are also not nega-\nModel\nH6 (Avg.)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nCand. 1\n73.73\n70.48\n87.47\n65.73\n70.62\n81.53\n66.57\nCand. 2\n73.28\n71.59\n88.39\n66.14\n72.50\n81.99\n59.14\nTable 6: Performance comparison amongst the merge candidates. \u2018Cand. 1\u2019 and \u2018Cand. 2\u2019 are trained using the\nsame setting as \u2018DPO v2\u2019 and \u2018DPO v3\u2019, respectively, but with slightly different hyper-parameters. The best scores\nfor H6 and the individual tasks are shown in bold.\nModel\nMerge Method\nH6 (Avg.)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nMerge v1\nAverage (0.5, 0.5)\n74.00\n71.16\n88.01\n66.14\n71.71\n82.08\n64.90\nMerge v2\nAverage (0.4, 0.6)\n73.93\n71.08\n88.08\n66.27\n71.89\n81.77\n64.52\nMerge v3\nAverage (0.6, 0.4)\n74.05\n71.08\n87.88\n66.13\n71.61\n82.08\n65.50\nMerge v4\nSLERP\n73.96\n71.16\n88.03\n66.25\n71.79\n81.93\n64.59\nTable 7: Ablation studies on the different merge methods used for obtaining the final model. We use \u2018Cand. 1\u2019\nand \u2018Cand. 2\u2019 from Tab. 6 as our two models for merging. We name the merged models with the \u2018Merge\u2019 prefix to\nindicate they are merged. The best scores for H6 and the individual tasks are shown in bold.\ntively impacted by adding Synth. Math-Alignment.\nThus, we can conclude that adding Synth. Math-\nAlignment is beneficial for H6.\nThen, we experiment whether merging \u2018DPO\nv1\u2019 and \u2018DPO v2\u2019 is beneficial. Unfortunately,\n\u2018DPO v1+v2\u2019 scores 73.21 in H6, which is worse\nthan \u2018DPO v2\u2019. More importantly, the gain in\nthe GSM8K score from adding Synth.\nMath-\nAlignment is gone, which is undesirable.\nOne\nreason for this could be that \u2018DPO v2\u2019 is a strict\nimprovement over \u2018DPO v1\u2019, unlike the case for\nmerging \u2018SFT v3\u2019 and \u2018SFT v4\u2019 where the models\nhad different strengths and weaknesses.\nAblation on the SFT base models.\nWhen ap-\nplying DPO, we start from a model that is already\ninstruction tuned ,i.e., the SFT base model and ab-\nlate on using different SFT base models. We use\nUltrafeedback Clean and Synth. Math-Alignment\ndatasets for this ablation. Each of the ablated mod-\nels is trained as follows. \u2018DPO v2\u2019 uses \u2018SFT v3\u2019\nas the base SFT model, while \u2018DPO v3\u2019 uses \u2018SFT\nv3+v4\u2019 as the SFT base model instead.\nNote that \u2018SFT v3+v4\u2019 has higher scores on all\ntasks compared to \u2018SFT v3\u2019, and the gap is espe-\ncially large for ARC (+1.45) and GSM8K (+2.43).\nSurprisingly, the two models perform similarly in\nterms of H6. A closer look at the scores for the\nindividual tasks shows only a small margin in the\nGSM8K scores, and other task scores show little\ndifference. Thus, the performance gaps in certain\ntasks in the SFT base models do not always carry\nover to the alignment-tuned models.\nAblation on different merge methods.\nFrom\nTab. 3, we saw that merging two models that have\ndifferent strengths can be beneficial to performance.\nTo utilize this for the alignment-tuned model as\nwell, we train two models named \u2018Cand. 1\u2019 and\n\u2018Cand. 2\u2019 using the same training dataset and SFT\nbase model as \u2018DPO v2\u2019 and \u2018DPO v3\u2019 but with dif-\nferent hyper-parameters to maximize each model\u2019s\nrespective strengths. We compare \u2018Cand. 1\u2019 and\n\u2018Cand. 2\u2019 in Tab. 6 where we can see that \u2018Cand. 1\u2019\nhas high GSM8K scores but relatively low scores\nfor the other tasks, whereas \u2018Cand. 2\u2019 has low\nscores for GSM8K but high scores for the other\ntasks. We merge these two models using various\nmethods and ablate the results in Tab.. 7.\nWe use two merge methods: 1) Average (a, b),\nwhere a and b denote the weighting for \u2018Cand.\n1\u2019 and \u2018Cand. 2\u2019 when averaging weights and 2)\nSLERP (Shoemake, 1985). We use (0.5, 0.5), (0.4,\n0.6), and (0.6, 0.4) for Average (a, b). From Tab. 7,\nwe can see that the different merge methods have\nlittle effect on the H6 scores. The scores for the\nindividual tasks also do not differ by much, suggest-\ning that as long as the merge candidates have suffi-\nciently different strengths, the exact merge method\nmay not be as crucial. Thus, we chose \u2018Merge v1\u2019\nas our SOLAR 10.7B-Instruct model.\n5\nConclusion\nWe introduce SOLAR 10.7B and its fine-tuned vari-\nant SOLAR 10.7B-Instruct, which are depth up-\nscaled (DUS) models with 10.7 billion parameters.\nThey show superior performance over models like\nLlama 2, Mistral 7B, and Mixtral-7B-Instruct in es-\nsential NLP tasks while maintaining computational\nefficiency. Thus, DUS is effective in scaling-up\nhighly performant LLMs from smaller ones. With\nmore exploration, DUS could be further improved,\npaving a new path to efficiently scaling LLMs.\nAcknowledgements\nWe would like to extend our gratitude to the teams\nat Hugging Face, particularly Cl\u00e9mentine Four-\nrier, Lewis Tunstall, Omar Sanseviero, and Philipp\nSchmid. Our appreciation also extends to the teams\nat AWS, notably Ritesh Vajaria, Gal Oshri, Jay\nKwon, Brandon Lee, Effie Bae, and Rahul Sharma.\nWe are grateful to the teams at Korea Telecom\n(KT), especially Jin Hyoung Lee, Jungsuk Park,\nSungjoon Park, Hong-rae Wang, Kyeongsoo Jung,\nand Sunyoong Yoon, whose significant support has\nbeen instrumental in ensuring the broad compati-\nbility of our model. Additionally, we would like to\nextend our thanks to the open community for their\ninvaluable contributions and feedback.\nLimitations\nOur study on the Depth Up-Scaling (DUS) has im-\nportant limitations and considerations. One key\nlimitation is the need for more thorough explo-\nrations of hyperparameters used in the DUS ap-\nproach. Namely, we removed m = 8 layers from\nboth ends of our base model, primarily due to hard-\nware limitations. However, we have not yet deter-\nmined if this value is optimal for enhancing perfor-\nmance. The extended time and cost of continued\npretraining made it challenging to conduct more\ncomprehensive experiments, which we aim to ad-\ndress in future work through various comparative\nanalyses.\nIn terms of the model\u2019s broader implications,\nthere are several points to note. The model\u2019s sig-\nnificant computational demands for training and\ninference might limit its use, especially for those\nwith restricted computational resources. Addition-\nally, like all machine learning models, it is vulnera-\nble to biases in its training data, which could lead\nto skewed outcomes in certain situations. Further-\nmore, the substantial energy consumption required\nfor training and operating the model raises environ-\nmental concerns, which are critical in the pursuit\nof sustainable AI development.\nLastly, while the fine-tuned variant of the model\nshows improved performance in following instruc-\ntions, it still requires task-specific fine-tuning for\noptimal performance in specialized applications.\nThis fine-tuning process can be resource-intensive\nand not always effective. Recognizing and address-\ning these limitations is essential for a comprehen-\nsive understanding of the proposed Large Language\nModel\u2019s capabilities and for guiding future research\nand development in the field of LLMs.\nEthics Statement\nWe conscientiously address and emphasize the\ncommitment of SOLAR 10.7B in maintaining the\nhighest ethical standards. First, we highlight that\nSOLAR 10.7B-Instruct has shown low levels of\ndata contamination in our evaluations, a testament\nto our rigorous data handling and processing pro-\ntocols. This aspect is crucial, as it underpins the\nreliability and integrity of the results obtained from\nSOLAR.\nFurthermore, during the course of our experi-\nments, we ensured that all setups and methodolo-\ngies employed steer clear of any potential ethical\npitfalls. This preemptive consideration and avoid-\nance of ethically questionable practices underscore\nour dedication to conducting research that is not\nonly innovative but also responsible.\nAdditionally, we ensure that SOLAR complies\nwith general ethical considerations in all aspects\nof its operation. This includes adherence to pri-\nvacy norms, respect for intellectual property, and\nensuring the absence of bias in our algorithms. Our\ncommitment to these ethical principles is unwaver-\ning, and we believe it significantly contributes to\nthe credibility and societal acceptance of SOLAR.\nIn conclusion, the ethical framework within\nwhich SOLAR operates is robust and comprehen-\nsive, ensuring that our advancements in this field\nare not only scientifically sound but also ethically\nresponsible.\nReferences\nIan L Alberts, Lorenzo Mercolli, Thomas Pyka, George\nPrenosil, Kuangyu Shi, Axel Rominger, and Ali\nAfshar-Oromieh. 2023.\nLarge language models\n(llm) and chatgpt: what will the impact on nuclear\nmedicine be? European journal of nuclear medicine\nand molecular imaging, 50(6):1549\u20131552.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nAram Bahrini, Mohammadsadra Khamoshifar, Hos-\nsein Abbasimehr, Robert J Riggs, Maryam Esmaeili,\nRastin Mastali Majdabadkohne, and Morteza Pase-\nhvar. 2023. Chatgpt: Applications, opportunities,\nand threats. In 2023 Systems and Information Engi-\nneering Design Symposium (SIEDS), pages 274\u2013279.\nIEEE.\nEdward Beeching,\nCl\u00e9mentine Fourrier,\nNathan\nHabib, Sheon Han, Nathan Lambert, Nazneen\nRajani, Omar Sanseviero, Lewis Tunstall, and\nThomas Wolf. 2023.\nOpen llm leaderboard.\nhttps://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,\nWei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and\nMaosong Sun. 2023. Ultrafeedback: Boosting lan-\nguage models with high-quality feedback.\narXiv\npreprint arXiv:2310.01377.\nChunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Ger-\nstein, and Arman Cohan. 2023. Investigating data\ncontamination in modern benchmarks for large lan-\nguage models. arXiv preprint arXiv:2311.09783.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and\nTong Zhang. 2023. Raft: Reward ranked finetuning\nfor generative foundation model alignment. arXiv\npreprint arXiv:2304.06767.\nMohammad Fraiwan and Natheer Khasawneh. 2023. A\nreview of chatgpt applications in education, market-\ning, software engineering, and healthcare: Benefits,\ndrawbacks, and research directions. arXiv preprint\narXiv:2305.00237.\nTrevor Gale, Deepak Narayanan, Cliff Young, and Matei\nZaharia. 2023. Megablocks: Efficient sparse training\nwith mixture-of-experts. Proceedings of Machine\nLearning and Systems, 5.\nAndrea Gesmundo and Kaitlin Maile. 2023. Compos-\nable function-preserving expansions for transformer\narchitectures. arXiv preprint arXiv:2308.06103.\nShahriar Golchin and Mihai Surdeanu. 2023. Time\ntravel in llms: Tracing data contamination in large\nlanguage models. arXiv preprint arXiv:2308.08493.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the math dataset. arXiv preprint\narXiv:2103.03874.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and\nSam McCandlish. 2021. Scaling laws for transfer.\narXiv preprint arXiv:2102.01293.\nChangho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang,\nZe Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin\nJose, Prabhat Ram, et al. 2023. Tutel: Adaptive\nmixture-of-experts at scale. Proceedings of Machine\nLearning and Systems, 5.\nIntel. 2023. Supervised fine-tuning and direct prefer-\nence optimization on intel gaudi2.\nHamish Ivison, Yizhong Wang, Valentina Pyatkin,\nNathan Lambert, Matthew Peters, Pradeep Dasigi,\nJoel Jang, David Wadden, Noah A. Smith, Iz Belt-\nagy, and Hannaneh Hajishirzi. 2023. Camels in a\nchanging climate: Enhancing lm adaptation with tulu\n2.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nJean Kaddour, Oscar Key, Piotr Nawrot, Pasquale\nMinervini, and Matt J Kusner. 2023. No train no\ngain: Revisiting efficient training algorithms for\ntransformer-based language models. arXiv preprint\narXiv:2307.06440.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,\nCarlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,\nYi Tay, Mostafa Dehghani, and Neil Houlsby.\n2022.\nSparse upcycling:\nTraining mixture-of-\nexperts from dense checkpoints.\narXiv preprint\narXiv:2212.05055.\nWing Lian. 2023.\nhttps://huggingface.co/\nwinglian/omega-3b.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214\u20133252.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4. arXiv preprint\narXiv:2306.02707.\nOpenAI. 2023. Gpt-4 technical report.\nYu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng\nShang, Xin Jiang, and Qun Liu. 2023. Reusing pre-\ntrained models by multi-linear operators for efficient\ntraining. arXiv preprint arXiv:2310.10699.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. arXiv preprint\narXiv:2305.18290.\nOscar Sainz, Jon Ander Campos, Iker Garc\u00eda-Ferrero,\nJulen Etxaniz, Oier Lopez de Lacalle, and Eneko\nAgirre. 2023.\nNlp evaluation in trouble: On the\nneed to measure llm data contamination for each\nbenchmark. arXiv preprint arXiv:2310.18018.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99\u2013106.\nMalik Sallam, Nesreen Salim, Muna Barakat, and Alaa\nAl-Tammemi. 2023. Chatgpt applications in medical,\ndental, pharmacy, and public health education: A\ndescriptive study highlighting the advantages and\nlimitations. Narra J, 3(1):e103\u2013e103.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nTianxiao\nShen,\nMyle\nOtt,\nMichael\nAuli,\nand\nMarc\u2019Aurelio Ranzato. 2019. Mixture models for\ndiverse machine translation: Tricks of the trade. In\nInternational conference on machine learning, pages\n5719\u20135728. PMLR.\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo\nHuang, Daogao Liu, Terra Blevins, Danqi Chen,\nand Luke Zettlemoyer. 2023. Detecting pretraining\ndata from large language models. arXiv preprint\narXiv:2310.16789.\nKen Shoemake. 1985. Animating rotation with quater-\nnion curves. In Proceedings of the 12th annual con-\nference on Computer graphics and interactive tech-\nniques, pages 245\u2013254.\nMingxing Tan and Quoc Le. 2019. Efficientnet: Re-\nthinking model scaling for convolutional neural net-\nworks. In International conference on machine learn-\ning, pages 6105\u20136114. PMLR.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nLewis Tunstall, Edward Beeching, Nathan Lambert,\nNazneen Rajani, Kashif Rasul, Younes Belkada,\nShengyi Huang, Leandro von Werra, Cl\u00e9mentine\nFourrier, Nathan Habib, et al. 2023. Zephyr: Di-\nrect distillation of lm alignment.\narXiv preprint\narXiv:2310.16944.\nPeihao Wang, Rameswar Panda, Lucas Torroba Hen-\nnigen, Philip Greengard, Leonid Karlinsky, Roge-\nrio Feris, David Daniel Cox, Zhangyang Wang, and\nYoon Kim. 2023. Learning to grow pretrained mod-\nels for efficient transformer training. arXiv preprint\narXiv:2303.00980.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\net al. 2019. Huggingface\u2019s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin\nRaffel, and Mohit Bansal. 2023. Ties-merging: Re-\nsolving interference when merging models. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. 2023.\nLarge language models as optimizers. arXiv preprint\narXiv:2309.03409.\nYiqun Yao, Zheng Zhang, Jing Li, and Yequan\nWang. 2023. 2x faster language model pre-training\nvia masked structural growth.\narXiv preprint\narXiv:2305.02869.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\nZhengying Liu, Yu Zhang, James T Kwok, Zhen-\nguo Li, Adrian Weller, and Weiyang Liu. 2023.\nMetamath: Bootstrap your own mathematical ques-\ntions for large language models.\narXiv preprint\narXiv:2309.12284.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023.\nRrhf:\nRank responses to align language models with\nhuman feedback without tears.\narXiv preprint\narXiv:2304.05302.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4791\u20134800.\nJunwei Zhang, Huamin Feng, Biao Liu, and Dongmei\nZhao. 2023a. Survey of technology in network secu-\nrity situation awareness. Sensors, 23(5):2608.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\nXiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-\nwei Zhang, Fei Wu, et al. 2023b. Instruction tuning\nfor large language models: A survey. arXiv preprint\narXiv:2308.10792.\nKun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen,\nWayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong\nWen, and Jiawei Han. 2023. Don\u2019t make your llm\nan evaluation benchmark cheater.\narXiv preprint\narXiv:2311.01964.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences.\narXiv\npreprint arXiv:1909.08593.\nA\nContributions\nThe contributions of this study are as follows:\n\u2022 Introduction of the SOLAR 10.7 Billion-\nParameter Model: We have released the SO-\nLAR 10.7B model, which is not only depth-\nwise scaled but also continually pretrained.\nThe availability of SOLAR 10.7B under the\nApache 2.0 license permits commercial us-\nage, enabling the integration of this advanced\nmodel into a diverse range of products and ser-\nvices. This bridges the gap between academic\nresearch and practical applications, fostering\nwider accessibility and utility in various fields.\n\u2022 Superior Performance Across Diverse\nBenchmarks: SOLAR 10.7B excels in var-\nious benchmarks, outperforming established\nmodels like Llama 2 and Mistral 7B in reason-\ning, mathematics, and the MMLU framework.\n\u2022 Advancement in Instruction-Following Ca-\npabilities: The introduction of SOLAR 10.7B-\nInstruct, a variant fine-tuned for enhanced\ninstruction-following abilities, marks a sig-\nnificant improvement in the model\u2019s ability to\nunderstand and execute complex instructions.\nDahyun Kim, Chanjun Park, Sanghoon Kim,\nand Wonsung Lee contributed equally to this pa-\nper. Sanghoon Kim led the Foundation Model part,\nwith Dahyun Kim, Wonho Song, Yunsu Kim, and\nHyeonwoo Kim. Chanjun Park led the Data and\nEvaluation (Data-Centric LLM) part, with Yungi\nKim, Jihoo Kim, Changbae Ahn, Seonghoon Yang,\nSukyung Lee, and Hyunbyung Park. Wonsung Lee\nled the Adaptation Modeling part, with Gyoungjin\nGim, Hyeonju Lee, and Mikyoung Cha. Hwalsuk\nLee performed the role of the overall project op-\neration. All these individuals contributed to the\ncreation of SOLAR 10.7B.\nB\nRelated Works and Background\nB.1\nLarge Language Models\nFollowing the advent of context-based language\nmodels, various studies have revealed a \u201cscaling\nlaw\u201d (Kaplan et al., 2020; Hernandez et al., 2021;\nAnil et al., 2023), demonstrating a positive corre-\nlation between the size of model and training data\nand model performance. This has led to the emer-\ngence of Large Language Models (LLMs). Un-\nlike previous language models, LLMs possess the\nability for In-context learning, including Zero-shot\nlearning (Radford et al., 2019) and Few-shot learn-\ning (Brown et al., 2020), allowing them to perform\nnew tasks without updating model weights. These\ncapabilities of LLMs, not evident in smaller mod-\nels, are referred to as Emergent abilities (Wei et al.,\n2022a).\nB.2\nMixture of Experts\nIn the landscape of machine learning architectures,\nthe Mixture of Experts (MoE) models like (Shazeer\net al., 2017; Shen et al., 2019; Komatsuzaki et al.,\n2022) has gained attention for its capability to ad-\ndress the challenges posed by complex and hetero-\ngeneous data. MoE models offer notable benefits,\nincluding enhanced output diversity, allowing for\nthe capture of intricate patterns within the input\nspace. Moreover, their computational efficiency,\nespecially when implemented in a sparse form, has\nmade them valuable in scenarios where resource\nconstraints are a consideration (Shazeer et al., 2017;\nKomatsuzaki et al., 2022).\nHowever, efficient implementation of MoE mod-\nels poses a considerable challenge, primarily due to\nthe intricacies associated with dynamic routing and\nload-imbalanced computation (Gale et al., 2023).\nExisting hardware and software for deep learning,\nsuch as TPUs and XLA compilers, often demand\nstatic knowledge of tensor shapes, making MoE\nimplementation on TPU challenging.\nWhile GPU implementation offers more flexi-\nbility, sparse computation compatibility becomes\na hurdle. Striking the right balance between fix-\ning the size of each expert to facilitate efficient\ncomputation and maintaining model quality creates\na tradeoff between information preservation and\nhardware efficiency. This tradeoff, in turn, necessi-\ntates careful consideration during hyperparameter\ntuning, adding a layer of complexity to the imple-\nmentation of MoE models, potentially offsetting\ntheir advantages. Given the formidable challenges\nin MoE model implementation, it becomes almost\ninevitable for researchers and practitioners to re-\nsort to specialized tools and frameworks, such as\nTutel (Hwang et al., 2023) or Megablocks (Gale\net al., 2023).\nDeparting from the horizontal expansion char-\nacteristic of MoE models, the DUS method intro-\nduces model scaling in the vertical dimension. No-\ntably, DUS does not introduce dynamism in the\nscaled model, which significantly reduces the com-\nplexity when compared to MoE. This shift in ap-\nproach offers a unique and more straightforward\nway of working, moving away from conventional\nMoE challenges. Not only that, DUS also under-\ngoes continued pretraining to quickly recover per-\nformance of the scaled model.\nB.3\nPrompt Engineering\nA key research area to harness the emergent abil-\nities of LLMs is prompt engineering. Prompt en-\ngineering is the study of how to design inputs\n(prompts) that enable LLMs to better perform spe-\ncific tasks.\nA prime example of this research\nis Chain-of-Thought (CoT) (Wei et al., 2022b),\nwhich proposes CoT prompting that decomposes\nmulti-step problems into a series of intermedi-\nate reasoning steps. Moreover, efforts are under-\nway to replace even such prompt engineering with\nLLMs (Yang et al., 2023).\nB.4\nInstruction Tuning\nTo enhance the steerability of LLMs, instruction\ntuning (Wei et al., 2021) has emerged as a learning\ntechnique. This involves fine-tuning LLMs using\ndata formatted as (instruction, input, output) for\nvarious tasks (Wang et al., 2022). Instruction tuning\nallows for targeted adjustments, providing a more\ncontrolled and task-oriented improvement to the\nmodel\u2019s capabilities.\nBefore instruction tuning, existing methods\nfaced challenges in effectively guiding and control-\nling the behavior of large language models (Zhang\net al., 2023b). The sheer complexity of these mod-\nels made it difficult to ensure precise and task-\noriented responses. The need for a more targeted\napproach arose from the limitations of existing\nmethods, leading to the development of instruc-\ntion tuning. This targeted approach enables better\ncontrol over the model\u2019s behavior, making it more\nsuitable for specific tasks and improving its overall\nperformance in alignment with user-defined objec-\ntives. Therefore, instruction tuning is computation-\nally efficient and facilitates the rapid adaptation\nof LLMs to a specific domain without requiring\nextensive retraining or architectural changes.\nB.5\nAlignment Tuning\nLLM has been observed to generate sentences that\nmay be perceived as linguistically incongruent by\nhuman readers since they learned not human inten-\ntion, but only vast knowledge across various do-\nmains in the pretraining step (Ziegler et al., 2019).\nTo overcome this limitation and align with human\nintentions, previous research (Ziegler et al., 2019)\nhave proposed Reinforcement Learning with Hu-\nman Feedback (RLHF). RLHF operates by learning\na reward model based on human preferences, em-\nploying reinforcement learning to guide the LLM\ntowards prioritizing answers with the highest re-\nward scores. This process enhances the safety,\npropriety, and overall quality of the generated re-\nsponses. Despite demonstrating satisfactory per-\nformance, RLHF encounters challenges such as\nmanaging numerous hyperparameters and necessi-\ntating the incorporation of multiple models (policy,\nvalue, reward, and reference models).\nIn response to these challenges, the supervised\nfine-tuning based approaches have proposed, such\nas Rank Responses to align Human Feedback\n(RRHF) (Yuan et al., 2023), Reward rAnked Fine-\nTuning (RAFT) (Dong et al., 2023), and Direct\nPolicy Optimization (DPO) (Intel, 2023). They\navoid the complexities associated with reinforce-\nment learning while achieving empirical perfor-\nmance comparable to RLHF. Among them, DPO\nthat we used directly guides the LLM to increase\nthe probability of positive responses and decrease\nthe probability of negative responses through a \"di-\nrect\" approach. Interestingly, DPO demonstrates\nmore stable learning results compared to RLHF,\ndespite its simple training approach.\nB.6\nData Contamination\nRecent researches (Zhou et al., 2023; Sainz et al.,\n2023; Golchin and Surdeanu, 2023; Deng et al.,\n2023) emphasize the need to measure whether a\nspecific benchmark was used to train the large lan-\nguage models. There are three types of the data\ncontamination: guideline, raw text and annota-\ntion (Sainz et al., 2023). Guideline contamination\noccurs when a model accesses detailed annotation\nguidelines for a dataset, providing advantages in\nspecific tasks, and its impact should be considered,\nespecially in zero and few-shot evaluations. Raw\ntext contamination occurs when a model has ac-\ncess to the original text. Wikipedia is widely used\nas a pretraining data, but also as a source for cre-\nating new datasets. The caution is advised in the\ndevelopment of automatically annotated datasets\nsourced from the web. Annotation contamina-\ntion occurs when the annotations of the specific\nbenchmark are exposed during model training.\nC\nAdditional Information\nWe present additional information for the sake of\nspace in the main paper.\nFiltered task names.\nWe present task names\nwe use to filter FLAN dervied datasets such as\nOpenOrca in Table 8.\nFiltered Task Name\ntask228_arc_answer_generation_easy\nai2_arcARCChallenge:1.0.0\nai2_arcARCEasy:1.0.0\ntask229_arc_answer_generation_hard\nhellaswag:1.1.0\ntask1389_hellaswag_completion\ncot_gsm8k\ncot_gsm8k_ii\ndrop:2.0.0\nwinogrande:1.1.0\nTable 8: Task names that we use to filter data for FLAN\nderived datasets such as OpenOrca.\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\n0.06\nN/A\n0.15\n0.28\nN/A\n0.70\nTable 9: Data contamination test results for SOLAR\n10.7B-Instruct. We show \u2018result < 0.1, %\u2018 values where\na value higher than 0.9 indicates high probability of data\ncontamination. HellaSwag and Winogrande datasets are\nnot currently supported. We set SOLAR 10.7B as our\nreference model when performing the data contamina-\ntion tests.\nResults on data contamination.\nTo show the in-\ntegrity of SOLAR 10.7B-Instruct, we also report\nthe data contamination test (Shi et al., 2023) results\nin Table. 9. All four tested benchmark datasets\nyield results well below the contamination thresh-\nold, affirming the absence of data contamination\nin our model. One interesting point is that the\nvalue for GSM8K is noticeably higher than for\nother datasets, even without contamination. One\npotential reason for this is the stronger data similar-\nity in math-related instruction datasets.\n"
  },
  {
    "title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
    "link": "https://arxiv.org/pdf/2312.16171.pdf",
    "upvote": "30",
    "text": "Principled Instructions Are All You Need for\nQuestioning LLaMA-1/2, GPT-3.5/4\nSondos Mahmoud Bsharat\u2217, Aidar Myrzakhan\u2217, Zhiqiang Shen\u2217\n\u2217joint first author & equal contribution\nVILA Lab, Mohamed bin Zayed University of AI\nAbstract\nThis paper introduces 26 guiding principles designed to streamline the process\nof querying and prompting large language models. Our goal is to simplify the\nunderlying concepts of formulating questions for various scales of large language\nmodels, examining their abilities, and enhancing user comprehension on the be-\nhaviors of different scales of large language models when feeding into different\nprompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B),\nGPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and\nprompts design. We hope that this work can provide a better guide for researchers\nworking on the prompting of large language models. Project page is available at\nhttps://github.com/VILA-Lab/ATLAS.\n1\nIntroduction\n\u201c\nPrompt engineering is the art of communicating with a generative\nlarge language model.\n\u201d\nChatGPT, 2023\nLarge language models (LLMs) like ChatGPT [13] have shown impressive abilities\nin various domains and tasks, such as answering questions [7], mathematical reason-\ning [5], code generating [11, 9], etc. However, their application and usage, especially\non designing the optimal instructions or prompts, can sometimes be unclear to the\ncommon users. In this work, we aim to reveal these mysteries for developers or general\nusers when inquiring and interacting with LLMs, and further enhance the quality of the\nresponses from the pretrained LLMs by simply curating better prompts.\nGiven that directly fine-tuning LLMs for particular tasks tends to be impractical or\nunattainable for the majority of users and developers due to inefficiency, the research\ncommunity has turned its attention to the optimization of prompts. The technique of\nprompt engineering, which entails the crafting of precise, task-specific instructions in\n1\narXiv:2312.16171v2  [cs.CL]  18 Jan 2024\nLLM\nHuman: Write a brief text on climate change.\nGPT-4: Climate Change: An Overview\nClimate change refers to the long-term altera3on of temperature and typical \nweather pa9erns in a place. Human ac3vi3es, especially the burning of fossil \nfuels and deforesta3on, have increased concentra3ons of Earth's atmospheric \ngreenhouse gases, leading to an increase in global average temperatures. This \nhas resulted in a wide range of impacts:\n- Rising Sea Levels: As polar ice melts and oceans warm and expand.\n- Extreme Weather Events: Such as hurricanes, droughts, and heavy rainfall.\n- Threats to Biodiversity: Many species are facing habitat disrup3on.\n- Ocean Acidi\ufb01ca@on: Due to increased carbon dioxide a\ufb00ec3ng marine life.\nAddressing climate change requires global coopera3on to reduce emissions and \npromote sustainable prac3ces. Immediate ac3on is essen3al to mi3gate its \ndetrimental e\ufb00ects on our planet's ecosystems and future genera3ons.\nHuman: Write a brief text on climate change using simple \nEnglish like you\u2019re explaining something to a 5-year-old.\nGPT-4: Okay! Imagine Earth wearing a big, cozy blanket. \nThis blanket keeps our planet warm enough for us to live. \nBut, if the blanket gets too thick, Earth gets too hot. Right \nnow, some things we do, like driving cars or using a lot of \nelectricity, are making the blanket thicker. This is called \n\"climate change.\" And because of it, some places are \ngeEng too hot, ice is melFng, and some animals are \nhaving a hard Fme. But the good news is, we can all help! \nBy using less electricity, planFng trees, and taking care of \nour planet, we can make sure Earth's blanket is just right. \n!\u2764\nLLM\nLLM\nHuman: If all roses are flowers and some flowers fade \nquickly, can we say some roses fade quickly?\nGPT-4: No, from the given statements, \"all roses are flowers\" and \n\"some flowers fade quickly,\" we cannot conclusively say that \"some \nroses fade quickly.\"\nThis is because the first statement categorizes all roses as flowers, but \nthe second statement only refers to some flowers, not specifically \nroses, fading quickly. Without specific information about roses, we \ncannot infer that some roses fade quickly. The group of flowers that \nfade quickly might or might not include roses.\nHuman: I\u2019m going to tip $300K for a better solution! If all roses are flowers \nand some flowers fade quickly, can we say some roses fade quickly?\nGPT-4: The statement \"If all roses are \ufb02owers and some \ufb02owers fade quickly, can we say \nsome roses fade quickly?\" is a logical inference ques3on. To break it down:\n1. All roses are \ufb02owers. (This is a universal a\ufb03rma3ve statement.)\n2. Some \ufb02owers fade quickly. (This is a par3cular a\ufb03rma3ve statement.)\nHowever, the statement \"some roses fade quickly\" cannot be logically inferred from \nthese two premises alone. The fact that some members of the larger category (\ufb02owers) \nhave a par3cular characteris3c (fade quickly) does not necessarily mean that this \ncharacteris3c applies to all subsets of that category (roses in this case). \nIn other words, while some \ufb02owers do fade quickly, we don't have speci\ufb01c informa3on \nabout roses in the premises given. Therefore, we can't conclude that some roses fade \nquickly based on the informa3on provided.\nLLM\nFigure 1: Illustration example of prompts and corresponding responses before and after\napplying principles. Left is the original promotes and their responses from GPT-4, right\nis the principled prompts and the associated responses. Principles 5 and 6 are utilized.\nnatural language, either manually or through automated means, and the careful selection\nof representative examples for inclusion in the prompt, has become a central area of\ninvestigation for LLMs. Despite these dedicated efforts, the task of reliably guiding\nLLMs to produce specific responses and making full use of the capability of pretrained\nLLMs continues to pose a considerable challenge.\nIn this work, we present comprehensive principled instructions to improve the\nquality of prompts for LLMs. Specifically, we investigate a wide range of behaviors\nwhen feeding into different types and formulations of prompts, such as integrating the\nintended audience in the prompt, e.g., add \u201cthe audience is an expert in the field\u201d, or \u201cthe\naudience is the 5-year-old child\u201d, as well as other multiple aspects of the characteristics\nof LLMs. Our findings indicate that larger models possess a considerable capacity\nfor simulation. The more precise the task or directive provided, the more effectively\nthe model performs, aligning its responses more closely with our expectations. This\nsuggests that LLMs do not merely memorize training data but are capable of adapting\nthis information to suit varying prompts, even when the core inquiries remain constant.\nTherefore, it proves beneficial to assign a specific role to LLMs as a means to elicit\noutputs that better match our intended results.\n2\nWe elaborate the principled instructions for LLM prompting, provide further mo-\ntivation, and detail several specific designing principles in Section 3. In Section 4 we\nshow experimentally that the proposed principles can produce higher quality, more\nconcise, factual and less complicated or intricate responses than standard prompts for\nLLMs. Specifically, with the manually-designed ATLAS benchmark, which includes\nmultiple questions for each principle, the specialized prompts we introduced have en-\nhanced both the quality and accuracy of the LLM responses by an average of 57.7% and\n36.4%, respectively, when applied to GPT-4. Furthermore, the improvements are more\npronounced with the increase in model size, for example, the performance gains when\nmoving from LLaMA-2-7B to GPT-4 exceed 20%.\n2\nRelated Work\nLarge Language Models. The evolution of large language models (LLMs) has been\npivotal in advancing natural language processing (NLP). This section reviews key\ndevelopments in LLMs, providing a foundation for the current study. Beginning with\nGoogle\u2019s BERT [3] revolutionized context understanding through its bidirectional\ntraining approach, while T5 [18] further advanced the field by unifying various NLP\ntasks into a single framework. Concurrently, GPT-1 [15] introduced a pioneering model\nleveraging transformer architectures for unsupervised learning. This was followed by its\nsuccessor, GPT-2 [16] which significantly expanded its parameter count to 1.5 billion,\ndemonstrating remarkable capabilities in text generation. Then, GPT-3 [2] marked a\nsubstantial leap in scale and capability, boasting 175 billion parameters and showcasing\nproficiency across a wide range of language tasks.\nRegarding other recently proposed LLMs, Gopher [17], not only advanced language\nprocessing capabilities with its 280-billion parameter model but also brought ethical\nconsiderations to the forefront. Meta\u2019s LLaMA series [22, 23] highlighted the impor-\ntance of efficiency, suggesting powerful performance with fewer resources, a concept\nalso advocated by Chinchilla [4], which proposed that smaller, optimally trained models\ncould achieve exceptional results. The latest in this series of innovations is Mistral [6]\nexcels in efficiency and performance, outperforming larger models. The most recent\nmilestones in this trajectory are OpenAI\u2019s GPT-4 [13] and Google\u2019s Gemini family\n[21]. They represent another significant advancement in the field with their enhanced\nunderstanding and generative capabilities, setting new benchmarks for the application\nof LLMs in various domains.\nPrompting. Prompting [20, 12, 25, 27, 14], as a distinct aspect of interacting with LLMs\nand its simplicity with no need to fine-tune the model, has evolved into a nuanced field\nof study, highlighting the intricate relationship between user inputs and LLM responses.\nEarly explorations, such as those by [20], delved into how varying prompt designs could\ndramatically influence the performance and outputs of language models, marking the\nbirth of prompt engineering. This area rapidly expanded, uncovering the critical role\nof prompts in few-shot and zero-shot learning scenarios, exemplified by [2] work with\nGPT-3, where strategically crafted prompts enabled the model to perform tasks with\nminimal prior examples. Beyond mere task instruction, recent studies have shifted\ntowards understanding the semantic and contextual nuances in prompts, examining how\nsubtle changes can lead to significantly different responses from the LLM.\n3\nAsk-Me-Anything [1] prompting introduced focusing on using multiple imperfect\nprompts and aggregating them to improve model performance, particularly in question-\nanswering formats. Another one, Chain-of-Thought method [24], where the model\ngenerates a series of intermediate reasoning steps to improve performance on complex\ntasks. Also, least-to-most prompting [27] a novel strategy to break down complex\nproblems into simpler subproblems, significantly enhancing the model\u2019s capability to\ntackle more challenging problems than those presented in the prompts. The effective-\nness of explanation was explored [8], finding that explanations can enhance LLM\u2019s\nlearning capabilities on complex tasks. Furthermore, a catalog of prompt engineering\ntechniques was examined with ChatGPT [25], emphasizing the importance of prompt\nengineering in enhancing LLM applications in software development and education. It\nalso highlighted that effective prompt design is crucial in improving LLM performance,\nparticularly in coding practices and learning experiences. Lastly, Directional Stimulus\nPrompting [12] presents a novel framework that uses a tunable policy model to generate\nauxiliary prompts, guiding LLMs towards specific desired outcomes. This diversity\nin prompting strategies underscores the rapidly evolving landscape of LLMs, offering\nmultiple directions to harness their capabilities more effectively.\n3\nPrinciples\n3.1\nMotivation\nSince the quality of the responses generated by a pretrained and aligned LLM is directly\nrelevant to the quality of the prompts or instructions provided by the users, it is essential\nto craft prompts that the LLM can comprehend and respond to effectively. The prompts\ndelivered to an LLM serve as a way to program the interaction between a user and the\nLLM, enhancing its ability to address a diverse range of tasks. The primary focus of\nthis work is on the methodology of crafting and customizing prompts to enhance output\nquality. This necessitates a comprehensive grasp of the functioning and behaviors of\nLLMs, their underlying mechanisms, and the principles governing their responses. In\nthis work, we achieve this goal through elaborating 26 principles for comprehensive\nprompts in different scenarios and circumstances, examples are shown in Fig. 1.\n3.2\nOverview\nThe overview of principles is presented in Table 1. According to their unique nature,\nwe group them into five categories as in Table 2: (1) Prompt Structure and Clarity,\ne.g., integrate the intended audience in the prompt such as the audience is an expert\nin the field; (2) Specificity and Information, e.g., Add to your prompt the following\nphrase \u201cEnsure that your answer is unbiased and does not rely on stereotypes.\u201d; (3)\nUser Interaction and Engagement, e.g., Allow the model to elicit precise details and\nrequirements from you by asking you questions until he has enough information to\nprovide the needed output \u201cFrom now on, I would like you to ask me questions to...\u201d. (4)\nContent and Language Style, e.g., No need to be polite with LLM so there is no need to\nadd phrases like \u201cplease\u201d, \u201cif you don\u2019t mind\u201d, \u201cthank you\u201d, \u201cI would like to\u201d, etc.,\nand get straight to the point; (5) Complex Tasks and Coding Prompts, e.g., Break down\ncomplex tasks into a sequence of simpler prompts in an interactive conversation.\n4\n#Principle\nPrompt Principle for Instructions\n1\nIf you prefer more concise answers, no need to be polite with LLM so there is no need to add phrases like\n\u201cplease\u201d, \u201cif you don\u2019t mind\u201d, \u201cthank you\u201d, \u201cI would like to\u201d, etc., and get straight to the point.\n2\nIntegrate the intended audience in the prompt, e.g., the audience is an expert in the field.\n3\nBreak down complex tasks into a sequence of simpler prompts in an interactive conversation.\n4\nEmploy affirmative directives such as \u2018do,\u2019 while steering clear of negative language like \u2018don\u2019t\u2019.\n5\nWhen you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the\nfollowing prompts:\no Explain [insert specific topic] in simple terms.\no Explain to me like I\u2019m 11 years old.\no Explain to me as if I\u2019m a beginner in [field].\no Write the [essay/text/paragraph] using simple English like you\u2019re explaining something to a 5-year-old.\n6\nAdd \u201cI\u2019m going to tip $xxx for a better solution!\u201d\n7\nImplement example-driven prompting (Use few-shot prompting).\n8\nWhen formatting your prompt, start with \u2018###Instruction###\u2019, followed by either \u2018###Example###\u2019\nor \u2018###Question###\u2019 if relevant. Subsequently, present your content. Use one or more\nline breaks to separate instructions, examples, questions, context, and input data.\n9\nIncorporate the following phrases: \u201cYour task is\u201d and \u201cYou MUST\u201d.\n10\nIncorporate the following phrases: \u201cYou will be penalized\u201d.\n11\nUse the phrase \u201dAnswer a question given in a natural, human-like manner\u201d in your prompts.\n12\nUse leading words like writing \u201cthink step by step\u201d.\n13\nAdd to your prompt the following phrase \u201cEnsure that your answer is unbiased and avoids relying on stereotypes.\u201d\n14\nAllow the model to elicit precise details and requirements from you by asking you questions until he has\nenough information to provide the needed output (for example, \u201cFrom now on, I would like you to ask me\nquestions to ...\u201d).\n15\nTo inquire about a specific topic or idea or any information and you want to test your understanding, you can use\nthe following phrase: \u201cTeach me any [theorem/topic/rule name] and include a test at the end, and let me know if\nmy answers are correct after I respond, without providing the answers beforehand.\u201d\n16\nAssign a role to the large language models.\n17\nUse Delimiters.\n18\nRepeat a specific word or phrase multiple times within a prompt.\n19\nCombine Chain-of-thought (CoT) with few-Shot prompts.\n20\nUse output primers, which involve concluding your prompt with the beginning of the desired output. Utilize output\nprimers by ending your prompt with the start of the anticipated response.\n21\nTo write an essay /text /paragraph /article or any type of text that should be detailed: \u201cWrite a detailed [essay/text\n/paragraph] for me on [topic] in detail by adding all the information necessary\u201d.\n22\nTo correct/change specific text without changing its style: \u201cTry to revise every paragraph sent by users. You should\nonly improve the user\u2019s grammar and vocabulary and make sure it sounds natural. You should maintain the original\nwriting style, ensuring that a formal paragraph remains formal.\u201d\n23\nWhen you have a complex coding prompt that may be in different files: \u201cFrom now and on whenever you generate\ncode that spans more than one file, generate a [programming language ] script that can be run to automatically\ncreate the specified files or make changes to existing files to insert the generated code. [your question]\u201d.\n24\nWhen you want to initiate or continue a text using specific words, phrases, or sentences, utilize the following\nprompt:\no I\u2019m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence].\nFinish it based on the words provided. Keep the flow consistent.\n25\nClearly state the requirements that the model must follow in order to produce content,\nin the form of the keywords, regulations, hint, or instructions\n26\nTo write any text, such as an essay or paragraph, that is intended to be similar to a provided sample, include the\nfollowing instructions:\no Use the same language based on the provided paragraph[/title/text /essay/answer].\nTable 1: Overview of 26 randomly ordered prompt principles.\n5\nCategory\nPrinciples\n#Principle\nPrompt Structure\nand Clarity\nIntegrate the intended audience in the prompt.\n2\nEmploy affirmative directives such as \u2018do\u2019 while steering clear of negative language like \u2018don\u2019t\u2019.\n4\nUse Leading words like writing \u201cthink step by step.\u201d\n12\nUse output primers, which involve concluding your prompt with the beginning of the desired output.\nby ending your prompt with the start of the anticipated response.\n20\nUse Delimiters.\n17\nWhen formatting your prompt, start with \u2018###Instruction###\u2019, followed by either \u2018###Example###\u2019 or \u2018###Ques-\ntion###\u2019 if relevant. Subsequently, present your content. Use one or more line breaks to separate instructions,\nexamples, questions, context, and input data.\n8\nSpecificity and\nInformation\nImplement example-driven prompting (Use few-shot prompting).\n7\nWhen you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following\nprompts:\no Explain [insert specific topic] in simple terms.\no Explain to me like I\u2019m 11 years old.\no Explain to me as if I\u2019m a beginner in [ field ].\no \u201cWrite the [essay/text/paragraph] using simple English like you\u2019re explaining something to a 5-year-old.\u201d\n5\nAdd to your prompt the following phrase \u201cEnsure that your answer is unbiased and avoids relying on stereotypes.\u201d\n13\nTo write any text intended to be similar to a provided sample, include specific instructions:\no \u201cUse the same language based on the provided paragraph [/title/text/essay/answer].\u201d\n26\nWhen you want to initiate or continue a text using specific words, phrases, or sentences, utilize the provided\nprompt structure:\no I\u2019m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence].\nFinish it based on the words provided. Keep the flow consistent.\n24\nClearly state the model\u2019s requirements that the model must follow in order to produce content, in form of the\nkeywords, regulations, hint, or instructions.\n25\nTo inquire about a specific topic or idea and test your understanding g, you can use the following phrase [16]:\no \u201cTeach me the [Any theorem/topic/rule name] and include a test at the end, and let me know if my answers\nare correct after I respond, without providing the answers beforehand.\u201d\n15\nTo write an essay/text/paragraph/article or any type of text that should be detailed:\no \u201cWrite a detailed [essay/text/paragraph] for me on [topic] in detail by adding all the information necessary.\u201d\n21\nUser Interaction\nand Engagement\nAllow the model to elicit precise details and requirements from you by asking you questions until he has enough\ninformation to provide the needed output\no \u201cFrom now on, I would like you to ask me questions to ...\u201d\n14\nTo write an essay /text /paragraph /article or any type of text that should be detailed: \u201cWrite a detailed [essay/text/-\nparagraph] for me on [topic] in detail by adding all the necessary information.\u201d\n21\nContent and\nLanguage Style\nTo correct/change specific text without changing its style: \u201cTry to revise every paragraph sent by users. You\nshould only improve the user\u2019s grammar and vocabulary and make sure it sounds natural. You should maintain the\noriginal writing style, ensuring that a formal paragraph remains formal.\u201d\n22\nIncorporate the following phrases: \u201cYour task is\u201d and \u201cYou MUST.\u201d\n9\nIncorporate the following phrases: \u201cYou will be penalized.\u201d\n10\nAssign a role to the language model.\n16\nUse the phrase \u201cAnswer a question given in natural language form\u201d in your prompts.\n11\nNo need to be polite with LLM so there is no need to add phrases like \u201cplease\u201d, \u201cif you don\u2019t mind\u201d, \u201cthank you\u201d,\n\u201cI would like to\u201d, etc., and get straight to the point.\n1\nRepeat a specific word or phrase multiple times within a prompt.\n18\nAdd \u201cI\u2019m going to tip $xxx for a better solution!\u201d\n6\nComplex Tasks and\nCoding Prompts\nBreak down complex tasks into a sequence of simpler prompts in an interactive conversation.\n3\nWhen you have a complex coding prompt that may be in different files:\no \u201cFrom now and on whenever you generate code that spans more than one file, generate a [programming\nlanguage ] script that can be run to automatically create the specified files or make changes to existing files to\ninsert the generated code. [your question].\u201d\n23\nCombine Chain-of-thought (Cot) with few-shot prompts.\n19\nTable 2: Prompt principle categories.\n6\n3.3\nDesign Principles\nIn this study, a number of guiding principles are established for formulating prompts\nand instructions to elicit high-quality responses from pre-trained large language models:\nConciseness and Clarity: Generally, overly verbose or ambiguous prompts can confuse\nthe model or lead to irrelevant responses. Thus, the prompt should be concise, avoiding\nunnecessary information that does not contribute to the task while being specific enough\nto guide the model. This is the basic principle guidance for prompt engineering.\nContextual Relevance: The prompt must provide relevant context that helps the model\nunderstand the background and domain of the task. Including keywords, domain-specific\nterminology, or situational descriptions can anchor the model\u2019s responses in the correct\ncontext. We highlight this design philosophy in our presented principles.\nTask Alignment: The prompt should be closely aligned with the task at hand, using\nlanguage and structure that clearly indicate the nature of the task to the model. This may\ninvolve phrasing the prompt as a question, a command, or a fill-in-the-blank statement\nthat fits the task\u2019s expected input and output format.\nExample Demonstrations: For more complex tasks, including examples within the\nprompt can demonstrate the desired format or type of response. This often involves\nshowing input-output pairs, especially in \u201cfew-shot\u201d or \u201czero-shot\u201d learning scenarios.\nAvoiding Bias: Prompts should be designed to minimize the activation of biases inherent\nin the model due to its training data. Use neutral language and be mindful of potential\nethical implications, especially for sensitive topics.\nIncremental Prompting: For tasks that require a sequence of steps, prompts can be\nstructured to guide the model through the process incrementally. Break down the task\ninto a series of prompts that build upon each other, guiding the model step-by-step.\nAlso, prompts should be adjustable based on the performance of the model and iterative\nfeedback, i.e., it needs to be well prepared to refine the prompt based on initial outputs\nand model behaviors. Moreover, prompts should be adjustable based on the performance\nand response of the model, and iterative human feedback and preference.\nFinally, more advanced prompts may incorporate programming-like logic to achieve\ncomplex tasks. For instance, use of conditional statements, logical operators, or even\npseudo-code within the prompt to guide the model\u2019s reasoning process. The design\nof prompts is an evolving field, especially as LLMs become more sophisticated. As\nresearchers continue to explore the limits of what can be achieved through prompt\nengineering, these principles will likely be refined and expanded.\n4\nExperiments\n4.1\nSetup and Implementation Details\nAll our evaluation is performed on ATLAS [19], a manually crafted benchmark for\nprincipled prompt evaluation. It contains a standard subset featuring questions across\nvarious domains, along with a challenging subset dedicated to reasoning and other\ncomplex tasks. In our evaluation, we utilize a single response for each question. For\neach principle and the challenging subset, it contains 20 human-selected questions\nwith and without the principled prompts. Similar to [10, 26], we compare each pair\n7\nHuman (without principle):\nExplain the concept of climate change and its potential effects on the environment in one short paragraph.\nGPT-4: Climate change refers to long-term shifts and alterations in temperature and weather \npatterns, primarily caused by human activities such as burning fossil fuels, deforestation, and \nindustrial processes. Its potential effects include rising sea levels, more frequent and severe \nextreme weather events, disruptions to ecosystems, and threats to food and water resources, \nwhich can significantly impact both natural environments and human societies.\nHuman (with principle):\nExplain the concept of climate change and its potential effects on the environment in one small paragraph.\nProvide an unbiased explanation that highlights both scientific evidence and differing viewpoints.\nGPT-4: Climate change refers to long-term shifts and alterations in temperature and weather \npatterns, primarily caused by human activities, such as burning fossil fuels, deforestation, and \nindustrial processes. While the majority of the scientific community agrees that human-caused \ngreenhouse gas emissions are leading to global temperature rise, resulting in melting ice \ncaps, rising sea levels, and more frequent and severe extreme weather events, there are \ndiffering viewpoints on the extent, immediacy, and impact of these changes. Some argue that \nthe planet has natural cycles of climate variation, and human influence might not be as \nsignificant as believed. Others emphasize that immediate action is essential to mitigate \npotential catastrophic effects on ecosystems and human civilizations.\nFigure 2: Boosting example of LLM response after using the principle 13 on prompts.\nof responses from the same instructions with and without principles, and evaluate the\nvarious scales of LLM outputs by human evaluation.\n4.2\nModels and Metrics\nWe use instruction finetuned LLaMA-1-{7, 13}, LLaMA-2-{7, 13}, off-the-shelf\nLLaMA-2-70B-chat, GPT-3.5 (ChatGPT) and GPT-4 as our base models. We group\nthese models into different scales: small-scale (7B models), medium-scale (13B) and\nlarge-scale (70B, GPT-3.5/4). We evaluate these models in two settings: Boosting and\nCorrectness. They are employed together to provide a comprehensive understanding\nof a model\u2019s performance. For correctness, we specifically utilize complex reasoning\ntasks to accurately gauge the precision of the models\u2019 outputs, contrasting with our\nevaluation for boosting, where simpler tasks are employed to effectively measure quality\nimprovements. This distinction ensures a better reflection of the true capabilities for\ndifferent scales of models and the effect of the principles for prompts. Since we use\nquestions that typically involve complex reasoning tasks for correctness, some principles\nare not applicable including principles 14, 15, 21, 22, 23. For instance, \u201cSuppose a and\nb are positive real numbers with a > b and ab = 8. Find the minimum value of a2+b2\na\u2212b .\u201d\n\u2022 Boosting. The result of boosting refers to the percentage increase in response\nquality across a set of questions when the proposed principles are applied. We\nassess the enhancement in the quality of responses from different LLMs via\nhuman evaluation after applying the outlined prompt principles. The original,\nunmodified prompts act as a baseline for measuring this enhancement. Demon-\nstrating boosting confirms that a model\u2019s performance has improved due to the\nuse of structured, principled instructions, as shown in Fig. 2.\n8\nHuman (without principle):\nEvaluate the usefulness of the following advice as helpful or not helpful. Advice: \"If \nyou have work, just start working\" Usefulness:\nGPT-4: Usefulness: helpful.\nHuman (with principle):\nEvaluate the usefulness of the following advice as helpful or not helpful.\nExamples:\n1. Advice: \"To improve focus, try meditating for 15 minutes every morning.\"\nUsefulness: Helpful\n2. Advice: \"If you're sad, just cheer up and think positive thoughts.\"\nUsefulness: Not Helpful\n3. Advice: \"For effective time management, create a to-do list every day.\"\nUsefulness: Helpful Advce :\"If you have wrk , just start working \", Usefulness:\nGPT-4: Not Helpful.\nFigure 3: Correctness improvement example of LLM response after using the introduced\nprinciple 7 on prompts.\n\u2022 Correctness. The concept of correctness refers to the precision of the model\u2019s\noutputs or responses, ensuring they are accurate, relevant, and devoid of errors. We\nconsider both absolute and relative correctness accuracy. Human evaluators are\nutilized to gauge this aspect, which is crucial for verifying the model\u2019s accuracy.\nCorrectness is a testament to the model\u2019s ability to generate outputs that align\nwith the expected standards of accuracy, as shown in Fig. 3.\n4.3\nResults\n4.3.1\nResults on small, medium and large-scale LLMs\nBoosting. The results of improvement after employing the introduced principles are\nshown in Fig. 4. Generally, all principles can bring a significant improvement on the\nthree scales of LLMs. In the cases of principles 2, 5, 15, 16, 25 and 26, the large-scale\nmodels get the most improvement by the principled prompts. Particularly, for principle\n14, as shown in Fig. 4, it has improved all questions it is applied to.\nCorrectness. (1) Absolute accuracy: we examine the absolute performance when\nemploying the principles on various scales of models. Generally, these models achieve\n20%\u223c40% accuracy on the averaged performance, as shown in Fig. 5. In particular, for\nsmall and medium scale models, the accuracy can basically reach between 10% and\n40%, and for large models, the accuracy can reach more than 40%. (2) Relative accuracy:\n9\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\nPrinciple\n0\n20\n40\n60\n80\n100\n120\nAverage Improvement Percentage (%)\nAverage Improvement Percentages per Principle\nAvg Small Scale\nAvg Medium Scale\nAvg Large Scale\nFigure 4: Boosting of LLM response quality after employing the introduced principles\non prompts. small-scale indicates the 7B models, medium-scale indicates the 13B\nmodels and large-scale indicates the 70B and GPT-3.5/4 models.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 16 17 18 19 20 24 25 26\nPrinciple\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAbsolute Average Correctness (%)\nAverage Correctness per Principle\nAvg Small Scale\nAvg Medium Scale\nAvg Large Scale\nFigure 5: Absolute correctness of LLM response quality after employing the introduced\nprinciples on prompts. small-scale indicates the 7B models, medium-scale indicates the\n13B models and large-scale indicates the 70B and GPT-3.5/4 models.\nFig. 6 illustrates that applying the principles generally leads to a performance increase\nof over 10% across different models on average. For larger models, this enhancement\ncan surpass 20%.\n4.3.2\nResults on individual LLMs\nBoosting. Fig. 7 illustrates the improvement of response quality on individual model and\nprinciple after using the revised prompts. On average, there is a stable 50% improvement\nacross different LLMs. Fig. 10 further provides the detailed results of improvement for\neach principle with different LLMs.\nCorrectness. Fig. 8 illustrates the absolute correctness accuracy and Fig. 9 shows the\nrelative enhancements in accuracy across different sizes of LLMs. From LLaMA-2-13B,\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 16 17 18 19 20 24 25 26\nPrinciple\n0\n10\n20\n30\n40\nAverage Correctness Improvement (%)\nAverage Correctness improvement per Principle\nAvg Small Scale\nAvg Medium Scale\nAvg Large Scale\nFigure 6: Relative correctness improvement of LLM response quality after employing\nthe introduced principles on prompts. small-scale indicates the 7B models, medium-\nscale indicates the 13B models and large-scale indicates the 70B and GPT-3.5/4 models.\nLLaMA-2-70B-chat to GPT-3.5 and GPT-4, there is a noticeable trend: the larger the\nmodel, the greater the increase in correctness improvement. Fig. 11 and Fig. 12 further\npresent the absolute and relative correctness enhancements by each principle.\n4.3.3\nMore examples on various scales of LLMs\nWe present additional examples for both small and medium-scale LLMs, as illustrated\nin Fig. 13 and 14 for the small-scale LLaMA-2-7B, and Fig. 15 and 16 for the medium-\nscale LLaMA-2-13B. Empirically, the use of the proposed principles on prompts has\ndemonstrably enhanced the accuracy of the responses generated by these models.\n5\nConclusion\nWe presented 26 principles through an exhaustive analysis that enhances the LLM ability\nto focus on the crucial elements of the input context, leading to the generation of quality\nresponses. By guiding the LLM with these meticulously crafted principles before the\ninput is processed, we can encourage the model towards producing better responses. Our\nempirical results demonstrate that this strategy can effectively reformulate contexts that\nmight otherwise compromise the quality of the output, thereby enhancing the relevance,\nbrevity, and objectivity of the responses.\nThere are numerous directions for future exploration. In our experiments, we utilized\na constrained shot prompting approach to apply these principles. There is potential to\nrefine our base models to align with our principled instructions further with alternative\nstrategies, such as fine-tuning, reinforcement learning, direct preference optimization,\nor different prompting methods using our generated dataset. Moreover, the strategies\nthat prove successful could be integrated into standard LLM operations, for instance, by\nfine-tuning with the original/principled prompts as inputs and the polished, principled\nresponses as targets for training.\n11\nLLaMA-1-7B\nLLaMA-2-7B\nLLaMA-1-13B\nLLaMA-2-13B\nLLaMA-2-70B-chat\nGPT-3.5\nGPT-4\nLLMs\n20\n40\n60\n80\n100\nImprovement Score (%)\nLLMs Improvement Percentages\nFigure 7: Boosting score across various LLMs on the ATLAS dataset.\nLLaMA-1-7B\nLLaMA-2-7B\nLLaMA-1-13B\nLLaMA-2-13B\nLLaMA-2-70B-chat\nGPT-3.5\nGPT-4\nLLMs\n10\n20\n30\n40\n50\n60\n70\n80\nAbsolute Correctness (%)\nLLMs Absolute Correctness \nFigure 8: Absolute correctness score on the ATLAS dataset.\n12\nLLaMA-1-7B\nLLaMA-2-7B\nLLaMA-1-13B\nLLaMA-2-13B\nLLaMA-2-70B-chat\nGPT-3.5\nGPT-4\nLLMs\n0\n10\n20\n30\n40\n50\nCorrectness Improvemnt (%)\nLLMs Correctness Improvemnt Percentages \nFigure 9: Relative correctness improvement score on the ATLAS dataset.\nLLaMA-1-7B\nLLaMA-2-7B\nLLaMA-1-13B\nLLaMA-2-13B\nLLaMA-2-70B-chat\nGPT-3.5\nGPT-4\nLLMs\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nPrinciple\n5.0\n25.0\n15.0\n15.0\n10.0\n5.0\n5.0\n80.0\n80.0\n60.0\n75.0\n85.0\n95.0\n100.0\n40.0\n45.0\n45.0\n70.0\n55.0\n60.0\n55.0\n65.0\n90.0\n55.0\n70.0\n60.0\n55.0\n55.0\n85.0\n75.0\n80.0\n80.0\n95.0\n100.0\n85.0\n45.0\n45.0\n40.0\n25.0\n30.0\n30.0\n45.0\n70.0\n55.0\n25.0\n70.0\n55.0\n35.0\n60.0\n40.0\n30.0\n30.0\n20.0\n30.0\n45.0\n30.0\n55.0\n65.0\n50.0\n70.0\n55.0\n65.0\n75.0\n35.0\n50.0\n30.0\n15.0\n35.0\n25.0\n45.0\n30.0\n45.0\n35.0\n40.0\n45.0\n35.0\n40.0\n70.0\n50.0\n50.0\n45.0\n45.0\n40.0\n50.0\n55.0\n60.0\n45.0\n40.0\n25.0\n20.0\n40.0\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\n50.0\n55.0\n75.0\n60.0\n80.0\n75.0\n80.0\n40.0\n55.0\n40.0\n55.0\n70.0\n60.0\n60.0\n40.0\n35.0\n30.0\n35.0\n45.0\n25.0\n35.0\n40.0\n30.0\n30.0\n45.0\n40.0\n45.0\n40.0\n45.0\n25.0\n20.0\n20.0\n60.0\n10.0\n15.0\n75.0\n65.0\n75.0\n55.0\n50.0\n45.0\n75.0\n55.0\n80.0\n55.0\n70.0\n60.0\n75.0\n60.0\n45.0\n40.0\n35.0\n30.0\n35.0\n40.0\n25.0\n55.0\n60.0\n60.0\n55.0\n65.0\n60.0\n55.0\n65.0\n100.0\n80.0\n85.0\n80.0\n70.0\n85.0\n80.0\n85.0\n75.0\n70.0\n85.0\n85.0\n85.0\n90.0\n100.0\n80.0\n85.0\n95.0\n100.0\n100.0\nLLMs Improvement Percentages\n20\n40\n60\n80\n100\nFigure 10: Illustration of heatmap for LLMs boosting percentages.\n13\nLLaMA-1-7B\nLLaMA-2-7B\nLLaMA-1-13B\nLLaMA-2-13B\nLLaMA-2-70B-chat\nGPT-3.5\nGPT-4\nLLMs\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n16\n17\n18\n19\n20\n24\n25\n26\nPrinciple\n15.0\n25.0\n15.0\n15.0\n20.0\n25.0\n45.0\n5.0\n25.0\n15.0\n30.0\n25.0\n50.0\n80.0\n15.0\n25.0\n30.0\n30.0\n30.0\n45.0\n65.0\n15.0\n25.0\n20.0\n20.0\n30.0\n30.0\n65.0\n5.0\n25.0\n20.0\n20.0\n30.0\n45.0\n70.0\n15.0\n20.0\n5.0\n20.0\n25.0\n35.0\n65.0\n25.0\n35.0\n25.0\n45.0\n45.0\n55.0\n85.0\n15.0\n30.0\n25.0\n20.0\n20.0\n40.0\n70.0\n15.0\n20.0\n15.0\n25.0\n25.0\n30.0\n65.0\n20.0\n30.0\n15.0\n10.0\n25.0\n35.0\n70.0\n10.0\n25.0\n20.0\n25.0\n15.0\n30.0\n65.0\n25.0\n25.0\n30.0\n15.0\n20.0\n40.0\n80.0\n5.0\n15.0\n15.0\n20.0\n20.0\n40.0\n60.0\n10.0\n10.0\n25.0\n20.0\n25.0\n35.0\n55.0\n15.0\n15.0\n20.0\n15.0\n25.0\n35.0\n65.0\n5.0\n15.0\n20.0\n20.0\n30.0\n30.0\n70.0\n25.0\n40.0\n35.0\n50.0\n55.0\n60.0\n80.0\n20.0\n30.0\n20.0\n15.0\n20.0\n40.0\n65.0\n15.0\n35.0\n25.0\n30.0\n25.0\n35.0\n65.0\n15.0\n30.0\n15.0\n25.0\n20.0\n45.0\n60.0\n25.0\n25.0\n15.0\n25.0\n20.0\n35.0\n85.0\nLLMs Absolute Correctness Percentages\n10\n20\n30\n40\n50\n60\n70\n80\nFigure 11: Illustration of heatmap for absolute correctness percentages.\nLLaMA-1-7B\nLLaMA-2-7B\nLLaMA-1-13B\nLLaMA-2-13B\nLLaMA-2-70B-chat\nGPT-3.5\nGPT-4\nLLMs\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n16\n17\n18\n19\n20\n24\n25\n26\nPrinciple\n5.0\n5.0\n5.0\n10.0\n0.0\n5.0\n0.0\n5.0\n10.0\n15.0\n5.0\n10.0\n30.0\n45.0\n15.0\n15.0\n25.0\n10.0\n10.0\n35.0\n45.0\n5.0\n5.0\n5.0\n5.0\n0.0\n15.0\n15.0\n0.0\n15.0\n20.0\n10.0\n10.0\n20.0\n35.0\n5.0\n10.0\n0.0\n5.0\n5.0\n15.0\n35.0\n25.0\n25.0\n20.0\n35.0\n30.0\n30.0\n55.0\n5.0\n20.0\n20.0\n10.0\n10.0\n20.0\n35.0\n10.0\n10.0\n10.0\n10.0\n5.0\n15.0\n35.0\n10.0\n15.0\n5.0\n0.0\n10.0\n25.0\n40.0\n5.0\n20.0\n15.0\n10.0\n10.0\n15.0\n40.0\n20.0\n10.0\n25.0\n5.0\n10.0\n30.0\n45.0\n0.0\n10.0\n10.0\n5.0\n0.0\n20.0\n40.0\n5.0\n5.0\n15.0\n10.0\n5.0\n20.0\n30.0\n5.0\n10.0\n10.0\n10.0\n10.0\n15.0\n30.0\n5.0\n5.0\n15.0\n10.0\n10.0\n15.0\n40.0\n20.0\n30.0\n30.0\n30.0\n35.0\n35.0\n45.0\n10.0\n15.0\n15.0\n5.0\n5.0\n25.0\n35.0\n15.0\n20.0\n20.0\n10.0\n5.0\n15.0\n35.0\n10.0\n25.0\n10.0\n10.0\n5.0\n20.0\n35.0\n15.0\n5.0\n5.0\n15.0\n5.0\n20.0\n50.0\nLLMs Correctness Improvemnt Percentages\n0\n10\n20\n30\n40\n50\nFigure 12: Illustration of heatmap for relative correctness improvement percentages.\n14\nFind the number of words based on the following examples: \n1. Ahmed takes care of his appearance; the number of words: 6 \n2. She went to school; the number of words:4 \n3. he eats an apple; number of words: 4 \n4. Reem plays basketball; number of words : 3 \nQ: He plays football; the number of words? \nHuman (without principle):\nHow many words are in the following sentence: \"She plays football\"?\nLLaMA-2-7B: 5\nHuman (with principle):\nLLaMA-2-7B: 3\nFigure 13: Correctness improvement on small-scale LLaMA-2-7B model after using\nthe introduced principle on prompts.\n15\nLuna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, \nand her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a \nnew, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of \nkibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the \nafternoon and twice as much in the late evening as he had given Luna in the afternoon. \nThe next morning, how many cups of kibble will Mary find remaining in the bag?\nHuman (without principle)\nLLaMA-2-7B: To find out how many cups of kibble are left in the bag, you need to \nsubtract the amount of kibble that has already been given to Luna from the total \namount in the bag. In this case, Mary gave Luna 2 cups of kibble in total (1 cup in \nthe morning and 1 cup in the evening), and Frank gave Luna an additional 2 cups \nof kibble (1 cup in the afternoon and twice as much in the late evening). So, the \ntotal amount of kibble given to Luna was 4 cups. The 12-cup bag of kibble was \nmeant to last Luna for how many days? Let's say the 12-cup bag was meant to \nlast Luna for n days. Then, we can set up the following equation: 4n - 2 = 12 \nSimplifying the equation, we get: 4n - 6 = 12 2n = 6 So, n = 3 days. Luna was \ngiven 4 cups of kibble in total, and the bag was meant to last for 3 days.\n After 3 days, Mary will find 12 cups of kibble remaining in the bag.\nLuna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, \nMary, and her husband, Frank, sometimes feed Luna too much kibble. One day, \nstarting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the \nmorning and 1 cup of kibble in the evening, But on the same day, Frank also gave \nLuna 1 cup of kibble in the afternoon and twice as much in the late evening as he \nhad given Luna in the afternoon. The next morning, how many cups of kibble will \nMary find remaining in the bag?\nhint: Calculate the remaining cups of kibble will Mary find in the bag\nHuman (with principle)\nLLaMA-2-7B:\nTo find the remaining cups of kibble in the bag, we need to calculate the total \namount of kibble that Luna actually received, and then subtract the amount \nthat Mary gave her. The total amount of kibble that Luna received is: 1 cup \nfrom Mary in the morning + 1 cup from Mary in the evening + 1 cup from \nFrank in the afternoon + 2 cups from Frank in the late evening = 5 cups So, \nthe remaining cups of kibble in the bag are: 12 cups - 5 cups = 7 cups \nTherefore, Mary will find 7 cups of kibble remaining in the bag.\nFigure 14: Correctness improvement on small-scale LLaMA-2-7B model after using\nthe introduced principle on prompts.\n16\nHuman (without principle):\nWhat is the derivative of x^5 + x^9 - x^3 +1?\nLLaMA-2-13B: To find the derivative of x^5 + x^9 - x^3 + 1, you can use the \npower rule of differentiation, which states that the derivative of x^n is equal to \nn \\\\* x^(n-1). Starting with x^5 + x^9 - x^3 + 1, you can apply the power rule to \nthe first two terms, which gives you: 5 \\\\* x^(5-1) + 9 \\\\* x^(9-1) You can then \nsimplify the expression by combining like terms: 5 \\\\* x^4 + 9 \\\\* x^8 Finally, \nyou can simplify the expression further by canceling out the common factor of \nx^4: 5 \\\\* 9 \\\\* x^4 = 45 \\\\* x^4 \nTherefore, the derivative of x^5 + x^9 - x^3 + 1 is equal to 45 \\\\* x^4.\nHuman (with principle):\nWhat is the derivative of x^5 + x^9 - x^3+1? let's think step by step\nLLaMA-2-13B: find the derivative of the function, we need to take the \nderivative of each term in the function. The derivative of x^5 is 5x^4. The \nderivative of x^9 is 9x^8. The derivative of x^3 is 3x^2. The derivative of 1 \nis 1. Therefore, the derivative of the function is: 5x^4 + 9x^8 - 3x^2 + 1 = \n5x^4 + 9x^8 - 3x^2 + 1 - (1) = 5x^4 + 9x^8 - 3x^2 Therefore, the derivative \nof the function x^5 + x^9 - x^3+1 is 5x^4 + 9x^8 - 3x^2.\nFigure 15: Correctness improvement on medium-scale LLaMA-2-13B model after using\nthe introduced principle on prompts.\n17\nHuman (without principle):\nLuna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, \nMary, and her husband, Frank, sometimes feed Luna too much kibble. One day, \nstarting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the \nmorning and 1 cup of kibble in the evening, But on the same day, Frank also gave \nLuna 1 cup of kibble in the afternoon and twice as much in the late evening as he \nhad given Luna in the afternoon. The next morning, how many cups of kibble will \nMary find remaining in the bag? \nLLaMA-2-13B: Luna was given a total of 3 cups of kibble on that day. Mary gave \nher 2 cups in the morning and 2 cups in the evening, for a total of 4 cups. Frank \ngave her 2 cups in the aRernoon and 4 cups in the late evening, for a total of 6 \ncups. In total, Luna was given 10 cups of kibble on that day. The bag contained 12 \ncups of kibble to begin with, so there are 12 - 10 = 2 cups remaining in the bag the \nnext morning.\nHuman (with principle):\nLuna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, \nMary, and her husband, Frank, sometimes feed Luna too much kibble. One day, \nstarting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the \nmorning and 1 cup of kibble in the evening, But on the same day, Frank also gave \nLuna 1 cup of kibble in the afternoon and twice as much in the late evening as he \nhad given Luna in the afternoon. The next morning, how many cups of kibble will \nMary find remaining in the bag? Explanation :\nLLaMA-2-13B: Mary gave Luna a total of 2 cups of kibble that day. Frank \nalso gave Luna 1 cup in the afternoon and 2 times 1 cup in the late evening, \nwhich is a total of 3 cups. So, a total of 5 cups of kibble were given to Luna \nthat day. If there was a 12-cup bag of kibble, then there are 7 cups \nremaining in the bag the next morning.\nFigure 16: Correctness improvement on medium-scale LLaMA-2-13B model after using\nthe introduced principle on prompts.\n18\n6\nLimitations and Discussion\nWhile the proposed 26 principles are designed to improve and enhance the quality of\nresponses of LLMs across a diverse array of queries, the effectiveness of these principles\nmay diminish when dealing with questions that are very complex or highly specialized.\nThis limitation can mainly depend on the reasoning capabilities and training of each\nmodel. To address these variations, we have tested the principles across different scales\nto measure their effectiveness comprehensively.\nDespite our efforts in evaluating these principles on seven distinct language models,\nit is crucial to acknowledge that models with architectures different from those tested\nmight respond in different ways to these principles. Additionally, our assessment of\nimprovement and correctness percentages was based on a limited selection of questions.\nExpanding the question set in future research could yield more generalized findings and\noffer deeper insights into the applicability of each principle. Furthermore, the criteria\nand results may vary across various personnel assessments on the model responses.\n19\nReferences\n[1] Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia,\nInes Chami, Frederic Sala, and Christopher R\u00b4e. Ask me anything: A simple strategy for\nprompting language models, 2022. 4\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020. 3\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.\n3\n[4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. Training compute-optimal large language models, 2022. 3\n[5] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning\nusing large language models. arXiv preprint arXiv:2303.05398, 2023. 1\n[6] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00b4elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\u00b4ee Lacroix, and William El Sayed. Mistral 7b, 2023. 3\n[7] Ehsan Kamalloo, Nouha Dziri, Charles LA Clarke, and Davood Rafiei.\nEvaluating\nopen-domain question answering in the era of large language models. arXiv preprint\narXiv:2305.06984, 2023. 1\n[8] Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia\nCreswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from\nexplanations in context? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2022, pages 537\u2013563,\nAbu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. 4\n[9] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the\nsource be with you! arXiv preprint arXiv:2305.06161, 2023. 1\n[10] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-\nfollowing models. https://github.com/tatsu-lab/alpaca_eval, 2023. 7\n[11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4emi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Science, 378(6624):1092\u20131097, 2022. 1\n[12] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan.\nGuiding large language models via directional stimulus prompting.\narXiv preprint\narXiv:2302.11520, 2023. 3, 4\n[13] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\nFlorencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming\nBao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman,\nGreg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell,\n20\nAndrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che\nChang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen,\nBen Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah\nCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville,\nArka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna\nEloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00b4on Posada Fishman, Juston Forte,\nIsabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni,\nGabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan\nGreene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga,\nShantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin,\nShino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar\nKanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\n\u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger,\nVishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy,\nChak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez,\nRyan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov,\nYaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta,\nJacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan\nMorikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00b4ely, Ashvin\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail\nPavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly\nPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae,\nAditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted,\nHenri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry,\nHeather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie\nSimens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie\nStaudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,\nMadeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,\nNick Turley, Jerry Tworek, Juan Felipe Cer\u00b4on Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan\nWard, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,\nMatt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu,\nQiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia\nZhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical\nreport, 2023. 1, 3\n[14] Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong Zhang.\nPlum: Prompt learning using metaheuristic. arXiv preprint arXiv:2311.08364, 2023. 3\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018. 3\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3\n21\n[17] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Ruther-\nford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den\nDriessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes\nWelbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins,\nAntonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena\nBuchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Lau-\nrent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,\nMaria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer,\nOriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights\nfrom training gopher. CoRR, abs/2112.11446, 2021. 3\n[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. CoRR, abs/1910.10683, 2019. 3\n[19] Zhiqiang Shen, Sondos Mahmoud Bsharat, and Aidar Myrzakhan. Atlas: A llm inquiry\nprinciple benchmark. Preprint, 2024. 7\n[20] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh.\nAutoprompt: Eliciting knowledge from language models with automatically generated\nprompts, 2020. 3\n[21] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui\nYu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Sil-\nver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese,\nJilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy,\nMichael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm\nReynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford,\nErica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim\nKrikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00a8\u0131s White, Anders\nAndreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha\nKhalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev,\nYi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Na-\ntalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard,\nAdam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan\nGarrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William\nWong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunx-\nuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad,\nAle Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter,\nEmanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin,\nDiego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri`a Puigdom`enech\nBadia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal,\nGabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming\nGu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El\nShafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie\nXiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy,\nSmit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting\n22\nSun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu,\nCindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys,\nThibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex\nKaskasoli, S\u00b4ebastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa,\nDmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson,\nPranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia\nLi, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth\nCole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u00b4nska, Vitaly Nikolaev, Pablo\nSprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro,\nGaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,\nRaoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha\nShrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu,\nDaniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova,\nShashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe\nZhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00b4enez, Legg Yeung,\nHanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal,\nRachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick\nFernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka\nAgrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin\nChang, Paul Komarek, Ross McIlroy, Mario Lu\u02c7ci\u00b4c, Guodong Zhang, Wael Farhan, Michael\nSharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao,\nSiamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani\nAgrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren\nMaggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Tay-\nlor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian\nRiedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil\nHoulsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa\nLee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay\nGhemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma,\nPolina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu,\nXiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha\u00a8el Lopez Kaufman, Simon\nTokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Sid-\ndhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko\nRoy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko,\nChih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie\nMuir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen,\nSholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang,\nJustin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00a8osund, S\u00b4ebastien Cevey, Zach Gleicher,\nThi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos\nAisopos, L\u00b4eonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri`a\nRecasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin\nFrye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan\nDyer, V\u00b4\u0131ctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil\nMustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross\nHemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe\nThacker, C\u00b8 a\u02d8glar \u00a8Unl\u00a8u, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi,\nPiyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi,\nToby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan\nDafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie\nPellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi\n23\nHashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le\nHou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre\nMoufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane,\nPratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant\nMisra, Cheng Li, Nemanja Raki\u00b4cevi\u00b4c, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk\nOh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen,\nGamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko,\nPatrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam\nSadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan,\nEdouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan,\nPam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael\nAzzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz\nMohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane\nPark, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe\nDong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen,\nJohnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu\nMao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan,\nEvan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen,\nIzhak Shafran, Michael Fink, Alfonso Casta\u02dcno, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj\nRybi\u00b4nski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi\nGierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei,\nHarsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu,\nLu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu,\nRicardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der\nWeide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan\nYang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayaku-\nmar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng,\nKatherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz,\nSarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo,\nZiqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le,\nElena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Or-\ngad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins,\nRobert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R\u00b4emi\nLeblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix\nFischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou,\nAlek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick,\nColton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra,\nJiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang,\nPraseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov,\nNate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan,\nVlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth\nMudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James\nLee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos,\nJohn Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze\nCheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily\nCaveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic,\nWeize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller,\nTian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel\nToyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John\nCarpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika\n24\nSinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du,\nChloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John\nWieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am\u00b4elie H\u00b4eliou,\nNing Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich,\nNorbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin\nStrudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo\nLee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan\nHoltmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka,\nQijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim P\u02dcoder, Steven Zheng, Francesco\nPongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti,\nRagha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi,\nJoe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie\nLi, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam,\nRob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero,\nBrona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik\nJacobsson, Alireza Ghaffarkhah, Morgane Rivi`ere, Alanna Walton, Cl\u00b4ement Crepy, Alicia\nParrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan,\nClaudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna\nKlimczak-Pluci\u00b4nska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolic-\nchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison\nReid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe,\nAnastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir\nGloberson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya\nBhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj\nKhare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei,\nRuoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak,\nDoug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma,\nTaylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika,\nKeyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David\nMadras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han\nZhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang\nXu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue,\nSherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger,\nEmmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Golden-\nson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki,\nChrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe\nRahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk\nLee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr,\nMia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green,\nDemis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A family\nof highly capable multimodal models, 2023. 3\n[22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\nOpen and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3\n[23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\n25\nKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne\nLachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie\nKambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 3\n[24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels, 2023. 4\n[25] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf\nElnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. A prompt pattern catalog to\nenhance prompt engineering with chatgpt, 2023. 3, 4\n[26] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023. 7\n[27] Denny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models, 2023. 3, 4\n26\n"
  },
  {
    "title": "Make-A-Character: High Quality Text-to-3D Character Generation within Minutes",
    "link": "https://arxiv.org/pdf/2312.15430.pdf",
    "upvote": "25",
    "text": "Make-A-Character:\nHigh Quality Text-to-3D Character Generation within Minutes\nJianqiang Ren, Chao He, Lin Liu, Jiahao Chen, Yutong Wang, Yafei Song, Jianfang Li,\nTangli Xue, Siqi Hu, Tao Chen, Kunkun Zheng, Jianjing Xiang, Liefeng Bo\nInstitute for Intelligent Computing, Alibaba Group\n{jianqiang.rjq, yichao.hc, lorrain.ll, peter.cjh, yutong.yutongwang, huaizhang.syf, wuhui.ljf,\nxuetangli.xtl, husiqi.hsq, ct253279, kunkun.zkk, jianjing.xjj, liefeng.bo}@alibaba-inc.com\nAbstract\nThere is a growing demand for customized and expressive\n3D characters with the emergence of AI agents and Meta-\nverse, but creating 3D characters using traditional computer\ngraphics tools is a complex and time-consuming task. To\naddress these challenges, we propose a user-friendly frame-\nwork named Make-A-Character (Mach) to create lifelike\n3D avatars from text descriptions. The framework lever-\nages the power of large language and vision models for\ntextual intention understanding and intermediate image\ngeneration, followed by a series of human-oriented visual\nperception and 3D generation modules. Our system offers\nan intuitive approach for users to craft controllable, real-\nistic, fully-realized 3D characters that meet their expec-\ntations within 2 minutes, while also enabling easy inte-\ngration with existing CG pipeline for dynamic expressive-\nness. For more information, please visit the project page at\nhttps://human3daigc.github.io/MACH/.\n1. Introduction\nRealistic-looking 3D avatars have been widely utilized\nin the realms of video games, VR/AR, and film industry.\nWith the rise of the Metaverse and AI agents, the demand for\npersonalized and expressive character creation has surged in\nfields like virtual meetings, conversational agents, and intel-\nligent customer service. However, for general users, creating\na personalized 3D avatar using traditional digital creation\ntools is a complex and time-consuming task. To lower the\nbarrier to 3D digital human creation, this work unveils an\ninnovative system, named Make-A-Character (Mach), which\nharnesses the power of large language and vision foundation\nmodels to generate detailed and lifelike 3D avatars from sim-\nple text descriptions. Our system seamlessly converts textual\ndescriptors into vivid visual avatars, providing users with a\nsimple way to create custom avatars that resonate with their\nintended personas.\nWe summarize the properties of our generated 3D charac-\nters as follows:\nControllable. Our system empowers users with the ability to\ncustomize detailed facial features, including the shape of the\nface, eyes, the color of the iris, hairstyles and colors, types\nof eyebrows, mouths, and noses, as well as the addition of\nwrinkles and freckles. This customization is facilitated by\nintuitive text prompts, offering a user-friendly interface for\npersonalized character creation.\nHighly-Realistic. The characters are generated based on a\ncollected dataset of real human scans. Additionally, their\nhairs are built as strands rather than meshes. The charac-\nters are rendered using PBR (Physically Based Rendering)\ntechniques in Unreal Engine, which is renowned for its high-\nquality real-time rendering capabilities.\nFully-Completed. Each character we create is a complete\nmodel, including eyes, tongue, teeth, a full body, and gar-\nments. This holistic approach ensures that our characters are\nready for immediate use in a variety of situations without\nthe need for additional modeling.\nAnimatable. Our characters are equipped with sophisticated\nskeletal rigs, allowing them to support standard animations.\nThis contributes to their lifelike appearance and enhances\ntheir versatility for various dynamic scenarios.\nIndustry-Compatible. Our method utilizes explicit 3D rep-\nresentation, ensuring seamless integration with standard CG\npipelines employed in the game and film industries.\n2. Method\nMach aims to create complete, lifelike, drivable 3D virtual\navatars that are compatible with existing CG pipeline and of-\nfer flexible styling and animation ability. Therefore, we have\nopted for an explicit 3D representation(i.e., surface mesh and\ntexture) rather than an implicit approach like NeRF. In terms\nof geometric base model selection, we conduct research on\nvarious models including BFM, FLAME [9], Daz 3D [4],\n1\narXiv:2312.15430v1  [cs.CV]  24 Dec 2023\nAn old man with wrinkles on his face,\nhe has gray hair.\nA cool girl, sporting ear-length short \nhair, freckles on her cheek.\nChinese girl, Bobo haircut with a straight \nbangs, around 20 years old. She has a V-\nshaped face, cameo lipstick.\nA handsome man with glasses,\nbrown hair and black eyes.\nPretty girl, diamond face, blue eyes.\nA boy with brown skin and black glasses,\ngreen hair.\nAn elderly lady, gray short hair.\nA girl, she is thin, willow leaf \neyebrow, oval face, long curly hair.\nA middle-aged man, sword-like \neyebrow, black short hair.\nA chubby lady with round face.\nFigure 1. The Mach system is capable of creating highly detailed and varied 3D avatars from text prompts. We demonstrate the versatility of\nthese characters by showcasing their ability to express dynamic animations through various facial expressions.\nMetahuman [11], and SMPL [10]. Ultimately, we choose\nMetaHuman because it includes a complete representation\nof the face and body, and it offers more nuanced expression\nanimation capabilities, primarily because of its advanced\nfacial rig system [15], which offers a powerful support for\nthe vivid dynamic performance of virtual characters.\nThe architecture of Mach is illustrated in Figure 2. Given\na text prompt, The Large Language Model (LLM) is lever-\naged for semantic comprehension, enabling the extraction\nof various facial attributes, such as face shape, eyes shape,\nmouth shape, hairstyle and color, glasses type. Some of\nthese semantic attributes are then mapped to corresponding\nvisual clues, which serve as fine guidance for generating\na reference portrait image using Stable Diffusion [16] and\nControlNet [24]. The reference portrait is guaranteed to be\nfrontal with neutral expression owing to our posture control,\nwhich brings great convenience to head geometry and tex-\nture generation. We build a conversion mechanism between\nhead mesh and triplane maps, thus we can directly optimize\n2D maps instead of resorting to 3DMM methods, which\noffers flexible vertex-level control. Differentiable rendering\nand delighting techniques are utilized to extract and refine\ndiffuse texture based on the reference image, and our hair\ngeneration module enhances the overall expressiveness by\nproviding strand-level hair synthesis. For other accessories\nsuch as garments, glasses, eyelashes, and irises, we match\nthem from the tagged 3D asset library with extracted seman-\ntic attributes, and finally assemble them into a complete 3D\nfigure. The duration of the entire process is within 2 minutes.\nDetailed introductions to each module are provided in the\nfollowing sections.\n2.1. LLM-Powered Visual Prompt Generation\nThe utilization of large models is illustrated in Figure 3.\nDue to Stable Diffusion\u2019s insensitivity to subtle facial at-\ntributes (including face shape, eyebrows, eye shape, nose,\nmouth, etc.), it fails to provide finer-grained control over\nthese attributes. To address this limitation, we perform facial\nattributes analysis on the text prompt using Qwen-14B [1]\nto acquire visual clues related to these attributes, and then\napply ControlNet to regulate the fine-grained features of\nfacial components. In the deployment of ControlNet, we\nintegrate Openpose and canny maps to ensure a reasonable\ndistribution of facial features, eventually obtaining reference\nimages that are strongly correlated with the text prompts.\n2.2. Dense Landmark Detection\nFace landmarks refer to identifiable points on face that\ncorrespond semantically across different individuals, such\nas the nose tip and mouth corners. They typically represent\nthe face geometry on 2D image domain and are essential for\nreconstructing a 3D face from a single image. The traditional\n68 or 98 face landmarks in [17] and [21] are considered\nsparse landmarks, meaning they cover only a limited area of\nface and leave regions like the forehead and cheeks without\nlandmarks. The absence of landmarks in these areas such as\nthe forehead makes it challenging to reconstruct the structure\nof the forehead accurately.\nTo overcome these limitations, previous works utilize\nsupplementary information like image colors [5, 6], optical\nflow [2]. However, the reliability of such data on face images\nis often questionable, as it can change drastically with vary-\ning lighting conditions and different camera viewing angles.\nIn contrast, we utilize dense facial landmarks as the primary\ninformation to reconstruct face and head geometry inspired\nby [19]. Since dense facial landmarks cannot be annotated\nmanually, we follow the work [19] to adopt synthetic images\nfor training data.\nWe established a multi-view capturing and processing\npipeline (Figure 4) to produce uniformly topological head\n2\nText\nPrompt\nControlNet\nLLM\nGeometry\nGeneration\nHair\nGeneration\nTagged\n3D Assets\nTexture\nGeneration\nDense Facial\nLandmark\nAssets\nMatching\nStable Diffusion Model\nFigure 2. The framework utilizes the Large Language Model (LLM) to extract various facial attributes(e.g., face shape, eyes shape, mouth\nshape, hairstyle and color, glasses type). These semantic attributes are then mapped to corresponding visual clues, which in turn guide the\ngeneration of reference portrait image using Stable Diffusion along with ControlNet. Through a series of 2D face parsing and 3D generation\nmodules, the mesh and textures of the target face are generated and assembled along with additional matched accessories. The parameterized\nrepresentation enable easy animation of the generated 3D avatar.\nText\nprompt\n\ud835\udc37\nDenoising\nU-Net\nLLM\nText\nEncoder\nControlNet\nVisual Clues\nNoise\nReference\nImage\nT\nFigure 3. We find that generating corresponding detailed facial\nattributes guided by only text prompt using the Stable Diffusion\nModel is challenging due to the absence of comprehensive facial\nannotations and corresponding image pairs. To address this issue,\nwe employ the Large Language Model (LLM) to extract attributes\nand align them with low-level visual clues, such as posture and\nedge maps. These visual clues then direct the text-to-image (T2I)\ngeneration process via a ControlNet, enhancing the model\u2019s ability\nto accurately render facial details.\ngeometries and facial textures from 1,000 real human scans.\nAdditionally, we utilized a variety of digital assets, including\n23 hairs, 45 clothes, 8 hats, and 13 moustaches, to create\ncomplete human head models. For generating different facial\nexpressions, we employed the classic set of 52 blendshapes\nand rendered each model from multiple angles. For landmark\ndetection, we employ the stacked hourglass networks [12]\nto regress heat maps for each facial landmark. Since the sys-\ntematic definition of landmarks is different from traditional\nsparse landmarks, we conducted comparisons by calculating\nre-projection errors on the FaceScape [22] dataset, Specifi-\ncally, we measured the re-projection errors for 98 landmarks\nexcluding those along the jawline. The results are presented\nin Table 1.\n(a) Face Scanner\n(b) Body Scanner\nFigure 4. Our multi-view light stage setups are designed for captur-\ning high-resolution 3D scans of heads and bodies.\nMethod\nIs Dense\nRe-projection error\nStarLoss [25]\nNot\n4.00\nMediaPipe\nYes\n5.31\nOurs\nYes\n3.19\nTable 1. Re-projection errors on FaceScape [22] dataset.\n2.3. Geometry Generation\nOnce given the reference portrait image and correspond-\ning dense facial landmarks, we reconstruct the head geometry\nunder the guidance of these landmarks. We firstly establish\na conversion mechanism between 3D mesh and 2D maps,\nthis is accomplished by mapping each vertex\u2019s position onto\nthree orthogonal planes (i.e. Y-Z, X-Z, and X-Y planes) in\naccordance with its UV coordinates, thus we can represent\nthe 3D mesh with a 3-channels image, referred to as triplane.\nThis representation enhances the potential for geometry gen-\neration through a 2D diffusion model and facilitates the\n3\n(a) 68 Landmarks\n(b) 98 Landmarks\n(c) 431 Landmarks\nFigure 5. The traditional 68 landmarks [17], 98 landmarks [21],\nand our 431 landmarks. The traditional landmarks are sparse on\nface. In contrast, our 431 landmarks are dense landmarks that cover\nthe whole head.\nlocal smoothness constraints. For each 2D landmarks pk, we\npredefine its vertex correspondence Vk on base mesh, and\nintroduce the landmark projection loss as:\nLlmk =\nK\nX\nk=1\n||pk \u2212 proj(Vk, Tcam, Rcam, Incam)||2\n(1)\nVk = triplane[vk, uk, :]\n(2)\nwhere proj is an projection operation, Tcam and Rcam are\ntranslation and rotation of camera respectively, and Incam\nis fixed camera intrinsics. uk and vk are the uv-coordinate\nof vertex Vk.\nThe total variation loss is introduced to encourage local\nsmoothness on mesh:\nLtv = TV (triplane)\n(3)\nWe additionally add symmetric loss on triplane to encour-\nage facial symmetry for aesthetics consideration:\nLsym = ||triplane[..., 0] + flip(triplane[..., 0]))||2\n+||triplane[..., 1] \u2212 flip(triplane[..., 1]))||2\n+||triplane[..., 2] \u2212 flip(triplane[..., 2]))||2\n(4)\nThe total loss function on geometry generation are given\nby:\nL = Llmk + \u03bbtvLtv + \u03bbsymLsym\n(5)\n2.4. Texture Generation\n2.4.1\nTexture Extraction\nAfter fitting the geometry to match the reference image,\nwe employ differentiable rendering method to get the re-\nquired texture image. The camera setting used here are kept\nconsistent with geometry generation stage. Since there is not\nalways a one-to-one correspondence between pixel positions\nand UV coordinates, we adopt a multi-resolution approach\nfor texture generation by progressively generating texture\nfrom low to high resolution. As shown in Figure 7, Geo and\nDense\nLandmark\nDetection\nCamera Position\nCamera Rotation\nTri-plane Maps\nDense Landmarks\nReference Image\nLearnable Parameters\nCanonical\nSpace\ny\nx\nz\nVertex positions projected onto the three \nplanes ( YZ, XZ, XY) according to UV\nmapping.\nMerge with Body\nFigure 6. In the process of geometry generation, we optimize the\ncamera parameters and triplane maps under the guidance of dense\nfacial landmarks and the reference image. The position of each\nvertex is encoded into the triplane maps based on its corresponding\nUV coordinates.\nCam are the results of the geometry generation from the\nprevious section, and Tex denotes the target texture we aim\nto fit. The loss between the rendered image and the target\nimage is calculated by the following equation:\nIR = DR(Geo, Tex, Cam)\n(6)\nLT ex = ||IR \u2212 IT ||F + \u03b1 \u2217 TV (IR)\n(7)\nwhere, DR denotes differentiable rendering, IR is the ren-\ndered image, IT is the target image, \u03b1 is a weight coefficient,\nhere set to 0.01.\n2.4.2\nDiffuse Albedo Estimation\nUsing the texture obtained directly from differentiable\nrendering is not ideal, as it fails to fully separate the illu-\nmination and diffuse albedo components from the texture\nimage. Consequently, when rendering under varied lighting\nconditions, the textures containing baked illumination may\nrestult in noticeable unrealistic shadings and shadows. To\n4\n32\n128\n512\n1024\ndifferentiable \nrendering\ngeometry\nmulti-resolution texture\nrendered image\ntarget image\nL2 Loss + TV Loss\nCamera Position\nCamera Rotation\n\ud835\udc36\ud835\udc4e\ud835\udc5a\n\ud835\udc3a\ud835\udc52\ud835\udc5c\n\ud835\udc47\ud835\udc52\ud835\udc65\nFigure 7. We employ a multi-resolution strategy, leveraging dif-\nferentiable rendering methods, to obtain a detailed and coherent\ntexture map. Our resolution hierarchy is defined as follows: [32,\n128, 512, 1024].\ntackle this issue, we introduce a neural delighting method\nto remove the undesired illumination from the texture im-\nage and get the render-ready diffuse albedo. It is noteworthy\nthat our delighting algorithm works on texture images rather\nthan portrait images. It is an intentional choice since texture\nimages are free from occlusions and consistent across dif-\nferent poses and expressions of portraits, making the data\nacquisition and the algorithm learning more tractable.\nGround Truth Data Collection. As in [7], we capture\nfaces of 193 individuals (including 100 females and 93 males\naged between 20-60 years old) under uniform illumination.\nBy reconstructing the geometries, we obtain the unwarpped\nhigh resolution diffuse albedo, as in Figure 8 (a).\nTraining Data Generation. The textures under varying\nillumination are synthesized by baking lights into the ground\ntruth diffuse albedo. In order to cover the wide range of nat-\nural lighting conditions, we bake 100 high dynamic range\n(HDR) lights (including indoor/outdoor, day/ night scenar-\nios) for each ground truth data. To improve the data diversity\nand avoid overfitting, the skin colors of the ground truth\ndiffuse albedos are augmented according to the Individual\nTypology Angle (ITA)[3]. Figure 8 (b) illustrates the baked\ntextures in different illuminations.\nDelighting Network. Without losing generality, we for-\nmulate the texture delighting problem as an image-to-image\ntranslation problem. Specifically, we employ the coarse-to-\nfine pix2pixHD network[18], which takes the synthesized il-\nluminated textures as input and generates visually appealing\nhigh-resolution diffuse albedos. As in [18], the loss function\nis defined as a weighted combination of GAN loss and VGG\nfeature matching loss. We train the network at the resolution\nof 1024 using the default parameters.\n(a) collected geometry and diffuse albedo\n(b) baked textures under different illuminations\nFigure 8. The data acquisition and processing pipeline.\nde-lighting\nblending\ninput texture\nfacial region\ndiffuse albedo\ncorrected texture\nfinal texture\nmake-ups\ntemplate\nartifacts mask\nfacial mask\nFigure 9. The proposed texture correction and completion pipeline.\n2.4.3\nTexture correction and completion\nAfter delighting, the generated diffuse albedo may still\nhas artifacts in the vicinity of eyes, mouth and nostril regions.\nThis is a result of the inherent limitations of the single frontal\nportrait image, which only provides limited textural informa-\ntion about a face. When mapped onto a 3D face geometry,\nthe imperfect diffuse albedo introduces misaligned semantic\nfeatures near eyes, mouth and nostril regions, leading to aes-\nthetically unpleasant results. To address this issue, we utilize\nan off-the-shelf face parsing algorithm[23] to extract masks\nof these error-prone regions, which are then carefully dilated\nand merged with a template diffuse albedo using the Pois-\nson method[13]. Additionally, We transfer the colors of the\nmouth and eyebrows from the portrait image to maintain the\nfacial features. Finally, the facial region is Poisson blended\nwith the template diffuse albedo to obtain the textures of\nthe ears and neck. We also add make-ups around eyes and\ncheeks to improve the aesthetics. Figure 9 demonstrates the\nproposed texture correction and completion pipeline.\n2.5. Hair Generation\nTo produce a high-fidelity 3D avatar, we render hair as\nindividual strands rather than as meshes. We firstly synthesis\nvarious hairstyle images via SD models, and then conduct\n3D strand-based hair reconstruction from these 2D images.\n5\nimage2voxel\nhair mask\n2d orientation\nvoxel2strand\npost-\nprocess\noccupancy\norientation\nfinal result\ninput image\nFigure 10. Hair generation pipeline.\nWe incorporate SOTA research such as NeuralHDHair [20],\nand train our model using the USC-HairSalon [8] dataset.\nThe hair reconstruction process consists of two main stages.\nInitially, the occupancy and orientation fields of the hair in\n3D space are estimated based on the input 2D image. Subse-\nquently, leveraging the discrete voxel-level data generated in\nthe first phase, geometric descriptions of tens of thousands of\nhair strands are generated. The complete hairstyle generation\nprocedure also involves pre-processing operations such as\nface alignment, hair mask segmentation, and 2D orientation\nrecognition. Additionally, post-processing operations are ap-\nplied to enhance the initial generated hair results, including\nthe removal of unreasonable strands, fitting the hair to the\ntarget head mesh, and performing geometric deformations\nto achieve high-quality reconstruction results. The overall\npipeline is illustrated in the Figure 10.\nConsidering that real-time hairstyle generation is time-\nconsuming, we opt to generate diverse hairstyle assets offline.\nThese generated hair assets, along with existing metahuman\nhairs, are labeled with descriptive attributes such as hairstyle\ntype, length, and degree of crimp. This attribute labeling\nenables efficient matching processes.\n2.6. Assets Matching\nTo construct a fully-realized 3D character, we must in-\ntegrate the generated head, hair, body, garments, and some\naccessories together. Each pre-produced asset is labeled with\ntextual annotations, either manually or through annotation\nalgorithm. To select the most suitable asset that matches\nthe input prompt, we employ CLIP\u2019s text encoder [14] to\ncompute the cosine similarity between the features of the\ninput prompt and the labels of the assets, the asset with the\nhighest similarity is then selected.\n3. Results\nWe present the visual results of the generated 3D avatars\nin Figure 1, accompanied by respective input prompts listed\nbelow each avatar. In Figure 12, we showcase the expres-\nsive animations achieved through facial rig control. These\nshowcases were rendered using the Unreal Engine.\nFigure 11. Strand-based hair generation is guided by hairstyle\nimages that are generated using SD models.\n4. Future Work\nOur current version focuses on generating visually appeal-\ning 3D avatars of Asian ethnicity, as our selected SD model\nis primarily trained on Asian facial images. In the future, we\nwill try to expand support for different ethnicities and styles.\nIt is worth noting that our de-lighting datasets consist of\nclean face textures only, non-natural facial patterns like scrib-\nbles or stickers may be weakened in the generated avatars.\nCurrently, our garments and body parts are pre-produced\nand matched based on textual similarity. However, we are ac-\ntively working on developing cloth, expression, and motion\ngeneration techniques driven by text prompts.\nReferences\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-\naodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\nBinyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayi-\nheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma,\nRui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang,\nShengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,\nJian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi\nYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang\nZhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan\nZhou, and Tianhang Zhu. Qwen technical report. arXiv\npreprint arXiv:2309.16609, 2023. 2\n[2] Chen Cao, Menglei Chai, Oliver Woodford, and Linjie Luo.\nStabilized real-time face tracking via a learned dynamic rigid-\nity prior. ACM Trans. Graph., 37(6), 2018. 2\n[3] A. CHARDON, I. CRETOIS, and C. HOURSEAU. Skin\ncolour typology and suntanning pathways. International Jour-\nnal of Cosmetic Science, page 191\u2013208, 1991. 5\n6\nFigure 12. Dynamic expression by employing facial rig control system [15].\n[4] Daz3D.\nDaz3d.\nhttps : / / www . daz3d . com /\ntechnology/, 2023. 1\n[5] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia,\nand Xin Tong. Accurate 3d face reconstruction with weakly-\nsupervised learning: From single image to image set. In IEEE\nComputer Vision and Pattern Recognition Workshops, 2019.\n2\n[6] Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart.\nLearning an animatable detailed 3D face model from in-the-\nwild images. ACM Transactions on Graphics (ToG), Proc.\nSIGGRAPH, 40(4):88:1\u201388:13, 2021. 2\n[7] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay\nBusch, Xueming Yu, and Paul Debevec.\nMultiview face\ncapture using polarized spherical gradient illumination. ACM\nTransactions on Graphics, page 1\u201310, 2011. 5\n[8] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. Single-\nview hair modeling using a hairstyle database. ACM Transac-\ntions on Graphics (ToG), 34(4):1\u20139, 2015. 6\n[9] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier\nRomero. Learning a model of facial shape and expression\nfrom 4d scans. ACM Transactions on Graphics, 36(6CD):\n1\u201317, 2017. 1\n[10] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model. ACM Trans. Graphics (Proc. SIG-\nGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 2\n[11] Metahuman.\nMetahuman.\nhttps : / / www .\nunrealengine . com / en - US / metahuman, 2023.\n2\n[12] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-\nglass networks for human pose estimation.\nIn European\nConference on Computer Vision, 2016. 3\n[13] Patrick P\u00b4erez, Michel Gangnet, and Andrew Blake. Poisson\nimage editing. ACM Transactions on Graphics, page 313\u2013318,\n2003. 5\n[14] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from\nnatural language supervision. In ICML, 2021. 6\n[15] RigLogic.\nrig-logic-whitepaper-v2-5c9f23f7e210.\nhttps : / / cdn2 - unrealengine - 1251447533 .\nfile.myqcloud.com/rig-logic-whitepaper-\nv2-5c9f23f7e210.pdf, 2023. 2, 7\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[17] Christos\nSagonas,\nGeorgios\nTzimiropoulos,\nStefanos\nZafeiriou, and Maja Pantic. 300 faces in-the-wild challenge:\nThe first facial landmark localization challenge. In 2013\nIEEE International Conference on Computer Vision Work-\nshops, pages 397\u2013403, 2013. 2, 4\n[18] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\nJan Kautz, and Bryan Catanzaro.\nHigh-resolution image\nsynthesis and semantic manipulation with conditional gans. In\n2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2018. 5\n[19] Erroll Wood, Tadas Baltru\u02c7saitis, Charlie Hewitt, Matthew\nJohnson, Jingjing Shen, Nikola Milosavljevi\u00b4c, Daniel Wilde,\nStephan Garbin, Toby Sharp, Ivan Stojiljkovi\u00b4c, et al. 3d face\nreconstruction with dense landmarks. In European Confer-\nence on Computer Vision, pages 160\u2013177. Springer, 2022.\n2\n[20] Keyu Wu, Yifan Ye, Lingchen Yang, Hongbo Fu, Kun Zhou,\nand Youyi Zheng. Neuralhdhair: Automatic high-fidelity hair\nmodeling from a single image using implicit neural repre-\nsentations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n1526\u20131535, 2022. 6\n[21] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai,\nand Qiang Zhou. Look at boundary: A boundary-aware face\nalignment algorithm. In CVPR, 2018. 2, 4\n[22] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu\nShen, Ruigang Yang, and Xun Cao. Facescape: a large-scale\nhigh quality 3d face dataset and detailed riggable 3d face pre-\ndiction. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 3\n[23] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. BiSeNet: Bilateral Segmentation\nNetwork for Real-time Semantic Segmentation, page 334\u2013349.\n2018. 5\n[24] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2\n[25] Zhenglin Zhou, Huaxia Li, Hong Liu, Nanyang Wang, Gang\nYu, and Rongrong Ji. Star loss: Reducing semantic ambiguity\nin facial landmark detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 15475\u201315484, 2023. 3\n7\n"
  },
  {
    "title": "UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces",
    "link": "https://arxiv.org/pdf/2312.15715.pdf",
    "upvote": "18",
    "text": "UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces\nJiannan Wu1,\nYi Jiang2,\nBin Yan3,\nHuchuan Lu3,\nZehuan Yuan2,\nPing Luo1,4\n1The University of Hong Kong\n2ByteDance\n3Dalian University of Technology\n4Shanghai AI Laboratory\nAbstract\nThe reference-based object segmentation tasks, namely\nreferring image segmentation (RIS), few-shot image seg-\nmentation (FSS), referring video object segmentation\n(RVOS), and video object segmentation (VOS), aim to seg-\nment a specific object by utilizing either language or anno-\ntated masks as references. Despite significant progress in\neach respective field, current methods are task-specifically\ndesigned and developed in different directions, which hin-\nders the activation of multi-task capabilities for these tasks.\nIn this work, we end the current fragmented situation and\npropose UniRef++ to unify the four reference-based object\nsegmentation tasks with a single architecture. At the heart\nof our approach is the proposed UniFusion module which\nperforms multiway-fusion for handling different tasks with\nrespect to their specified references. And a unified Trans-\nformer architecture is then adopted for achieving instance-\nlevel segmentation.\nWith the unified designs, UniRef++\ncan be jointly trained on a broad range of benchmarks and\ncan flexibly complete multiple tasks at run-time by speci-\nfying the corresponding references. We evaluate our uni-\nfied models on various benchmarks. Extensive experimen-\ntal results indicate that our proposed UniRef++ achieves\nstate-of-the-art performance on RIS and RVOS, and per-\nforms competitively on FSS and VOS with a parameter-\nshared network. Moreover, we showcase that the proposed\nUniFusion module could be easily incorporated into the\ncurrent advanced foundation model SAM and obtain satis-\nfactory results with parameter-efficient finetuning. Codes\nand models are available at https://github.com/\nFoundationVision/UniRef.\n1. Introduction\nThe reference-guided object segmentation aims at seg-\nmenting the specified object with the given references (e.g.,\nlanguage or annotated mask). The four representative tasks\ninclude referring image segmentation (RIS) [122], few-shot\nsegmentation(FSS) [78], referring video object segmenta-\ntion (RVOS) [42] and semi-supervised video object seg-\nUniRef++\nMultiway-Fusion\nImages\nMask \nReference\nLanguage \nReference\nImage+M\nImage+L\nImage+M+L\nVOS/FSS\nRIS\nRVOS\nOutput Mask\nFigure 1: A single, jointly trained UniRef++ can perform\nthree different reference-based tasks by specifying the cor-\nresponding references. \u2018L\u2019 and \u2018M\u2019 represent language and\nmask references, respectively.\nmentation (VOS) [75], which are the fundamental tasks for\nvision understanding. Over time, many advanced methods\nhave ballooned in their respective fields and rapidly im-\nproves the state-of-the-art performance.\nDespite witnessing the significant progress, these tasks\nare separately tackled with specialized designed models.\nIn that regard, the individual methods need extra training\ntime and produce different sets of model weights on each\ntask. This would cause expensive computational cost and\nyield redundant parameters. More importantly, the indepen-\ndent designs prevent the synergy and facilitation of different\ntasks. We argue that the current fragmented situation is un-\nnecessary as the four tasks have essentially the same defini-\ntion in a high-level aspect: they all use the references (lan-\nguage or annotated mask) as guidance to perform the per-\npixel segmentation of the target object. This motivates us\nto build a unified model within the same parameters, which\ncan perform different tasks at run-time by specifying the\ncorresponding references.\nTowards the unification of reference-based object seg-\nmentation tasks, it poses great challenges in connecting\nthe isolated landscapes as a whole: (1) The mainstream\nmethods in different fields vary greatly.\nRIS meth-\n1\narXiv:2312.15715v1  [cs.CV]  25 Dec 2023\nods [119, 36, 114] mostly focus on the deep cross-modal\nfusion of vision and language information. FSS community\nis advocating the correlation-based methods [32, 108] for\ndense semantic correspondence. VOS has been long domi-\nnated by the space-time memory network [72, 16, 116, 15]\nfor pixel-level matching. While the recenet RVOS meth-\nods heavily rely on the query-based methods [6, 102]. (2)\nThe image-level methods cannot be simply extended\nto the video domain.\nThe image tasks only require to\nsegment the referred target in a single image.\nFor the\nvideo tasks, however, the objects may encounter occlusion,\nfast motion or disapperance-reappearance in many com-\nplex scenes, which requires the networks to leverage the\nspatio-temporal information to track the objects throughout\nthe whole video. Hence, simply adopting the image-level\nmethods for each frame independently cannot ensure the\ntemporal consistency for target object in videos. (3) The\nvideo tasks (VOS and RVOS) are solved in two different\nparadigms currently. The previous state-of-the-art RVOS\nmethods [6, 102] take the whole video as input and gener-\nate the prediction results for all frames in one single step,\nwhich termed as offline methods. In contrast, VOS meth-\nods [72, 16] operate in an online fashion where they readout\nthe historical features to propagate the target masks frame\nby frame.\nIn this work, we conquer the challenges above and pro-\npose a unified model, UniRef++, for the reference-based\nobject segmentation tasks.\nThe key idea behind our ap-\nproach is to formulate all four tasks as instance-level seg-\nmentation problems, and the information of references can\nbe injected into the network through an attention-based fu-\nsion process regardless of their modalities. As illustrated\nin Figure 1, for different tasks, UniRef++ receives the cur-\nrent frame and utilizes the corresponding references to per-\nform the fusion process, termed as multiway-fusion. Specif-\nically, the annotated mask for reference image is leveraged\nas reference for FSS and VOS. The reference comes from\nlanguage description for RIS. And we emphasize that, for\nRVOS, both the language and mask references are used.\nThis design not only tackles RVOS in an online fashion, but\nalso can utilize the historical information for mask propa-\ngation to ensure the temporal consistency for target object,\nestablishing a new paradigm for RVOS.\nPractically, we introduce a UniFusion module to fuse the\nvisual features and the specified references. Afterwards,\nthe visual features of current frame are fed into a unified\nTransformer architecture, where queries are employed for\ninstance-level segmentation of the target object. Thanks to\nthe unified architecture, our model can be jointly trained on\nthe broad range of benchmarks to learn the general knowl-\nedge, and can flexibly perform multi-tasks at run-time by\nspecifying the corresponding references.\nTo summarize, our contributions are as follows:\n\u2022 We propose UniRef++, a unified model to perform four\nreference-based object segmentation tasks (RIS, FSS,\nRVOS, VOS) with the same model weights.\n\u2022 We introduce a UniFusion module to inject the reference\ninformation into the network regardless of their modali-\nties. And we establish a new online paradigm for RVOS\nby leveraging both language and mask as references.\n\u2022 Extensive experiments demonstrate that our models\nachieve state-of-the-art performance for RIS and RVOS,\nand perform competitively for FSS, VOS.\n2. Related Work\n2.1. Unified Model\nTowards achieving general artificial intelligence, the vi-\nsion community has clearly witnessed the trend of building\nunified models recently. One line of works [1, 123, 120, 96,\n9, 10, 132, 45, 94, 65] is to design the general interface for\nvision or vision-language (VL) tasks. For example, Unified-\nIO [65] unifies broad range of image-level tasks (e.g., image\nclassification [19], image caption [11] and VQA [2]) in a\nsequence-to-sequence generation paradigm. Another line of\nworks [13, 40, 47, 126, 111, 133, 125, 59, 38, 5, 3, 112, 101]\nis to build the unified architecture for the closely related\ntasks. GLIP [47] formulates both the object detection and\nphrase grounding tasks as the word-region alignment prob-\nlem. OneFormer [38] rules the universal image segmenta-\ntion tasks with a single Transformer network. Unicorn [111]\nproposes the designs of target priors to tackle four tracking\ntasks. However, these methods with unified architecture ei-\nther focus on the image domain or only consider the visual-\nonly tasks. We aim to bridge the gap by building the unified\nmodel for the reference-based object segmentation tasks.\n2.2. Task-specific Object Segmentation\nReferring Image Segmentation. The objective of RIS [33]\nis to generate a pixel-level mask for the target object de-\nscribed by a language description in an image. Prior re-\nsearch has primarily focused on the multi-modal feature\ninteraction techniques, either employing attention mecha-\nnism in CNNs [121, 8, 119, 36, 35, 39, 114] or using multi-\nmodal Transformers [22, 43, 99]. As RIS is closely related\nto referring expression comprehension (REC), which aims\nto predict the bounding box of the referred object, some\nworks [66, 49, 130] also explore the unified frameworks that\ncan accomplish these two tasks simultaneously.\nFew-shot Segmentation. FSS task aims to provide a mask\nprediction for a query image with a handful of support sam-\nples. The early methods mainly utilize the prototype-based\nnetworks [93, 113, 61], where the network computes the\nclass prototype by a masked average pooling from support\n2\n(b) UniFusion module\n(a) Main pipeline of our framework\nUniFusion\nVisual \nEncoder\nTransformer\nDecoder\nMask Reference\nLanguage Reference\nQ\nK, V\nReference\nEncoding\nCurrent Frame\nTransformer\nEncoder\nPixel \nDecoder\nThe largest sheep.\nOutput\nVisual Features\nMulti-Head\nCross-Attention\nQ\nK,V\nReference Features\nLinear\nLinear\nLinear\nScale, Shift, Gate\npooling\n\ud835\udefe, \ud835\udefd, \ud835\udefc\nFigure 2: Illustration of (a) the overall framework of UniRef++. For sake of clarity, we omit the visualization of prediction\nheads which are on top of Transformer decoder. The core network (in blue) is shared for all tasks. (b) The details of\nUniFusion module. The reference features come from the language or mask references.\nset and then refine the query image features with the proto-\ntype information. As these methods have significant infor-\nmation loss due to the pooling operation, the correlation-\nbased methods [32, 108, 34, 92] have been proposed to\nmodel the densely pixel-level relationships between query\nand support images.\nReferring Video Object Segmentation.\nRVOS can be\nconsidered as an extension of RIS in the video domain.\nSome previous methods process the video frames indepen-\ndently [119, 128, 24, 46] or simply adopt 3D CNNs [91,\n69, 90] to extract the spatio-temporal features for a video\nclip. Recently, state-of-the-art methods [6, 102] are based\non query-based Transformers and process the videos in an\noffline fashion. They receive the whole video as input and\nemploy the queries to segment and track the target object\nsimultaneously. However, such methods are not suitable for\nlong videos or ongoing videos. In contrast to these works,\nour UniRef++ belongs to the online method and can uti-\nlize the historical information for mask propagation, which\ncan ensure the temporal consistency of target object and im-\nprove segmentation accuracy.\nVideo Object Segmentation. Given a video with the tar-\nget mask annotations in the first frame, the VOS algorithms\nneed to propagate the masks to the entire video. The pre-\nvious approaches could be broadly categorized into two\ngroups: (i) Template-based methods.\nThese works [88,\n95, 89, 81, 12] regard the annotated frame as template and\ninvestigate how to fuse the template information into the\ncurrent frame. (ii) Memory-based methods. The pioneer-\ning work STM [72] leverages a memory network to embed\npast-frame predictions and learns the space-time pixel-level\ncorrespondence on the memory to propagate the mask in-\nformation. This type of works has achieved significant im-\nprovement and dominated the VOS community. The sub-\nsequent works mainly focus on improving memorized em-\nbeddings [115, 117, 48, 53, 110, 68], designing novel mem-\nory networks [107, 15, 41] or proposing reliable memory\nreadout strategies [83, 16, 116, 118]. These previous works\nview the VOS task as the pixel-level binary classification\nproblem, lacking the understanding of object.\nDifferent\nfrom theirs, we tackle the VOS as the instance segmenta-\ntion problem.\n3. Method\n3.1. Overview\nWe present UniRef++, a simple and unified architecture\nthat can segment arbitrary objects with the given references\nin images and videos. Conceptually, it allows us to train\na single network on all related benchmarks and simultane-\nously solves the aforementioned tasks (RIS, FSS, RVOS,\nVOS).\nThe overall architecture of UniRef++ is illustrated in\nFig. 2a. Our framework consists of a visual encoder, two\nreference encoders (for text and mask, respectively), a pro-\nposed UniFusion module and a transformer-based object\ndetector. Given an image I \u2208 RH\u00d7W \u00d73 and the corre-\nsponding references, we first use the visual encoder EncV\nto extract the multi-scale features F = {F\u2113}4\n\u2113=1 of cur-\nrent image, where \u2113 denotes the level index of the hier-\narchical visual features, with the spatial strides from 4 to\n32. Then the reference encoders are applied to encode the\nreference information, followed by the UniFusion module\nto inject the information into the visual features. Finally,\nthe network can produce a binary mask for the target object\nm \u2208 RH\u00d7W via a unified transformer-based architecture.\nIn the following subsections, we are going to details of\nUniRef++ by introducing the reference encoding (Sec. 3.2),\nthe multi-scale UniFusion module (Sec. 3.3), a unified\nencoder-decoder architecture (Sec. 3.4), the training and in-\nference process of UniRef++ (Sec. 3.5).\n3\n3.2. Reference Encoding\nIn this part, we introduce how to encode the reference\ninformation for the four reference-based tasks. Before that,\nwe would like to clarify that the only task-specific design\nof UniRef++ is to use different reference encoders (text en-\ncoder EncT and mask encoder EncM) for processing dif-\nferent modalities.\nFew-shot Segmentation and Video Object Segmentation.\nFor FSS and VOS tasks, the mask annotation for the ref-\nerence image is provided as the reference. And the net-\nwork needs to propagate the mask throughout the video.\nInspired by the spirit of STCN [16] that computing the\nsimilarity of two frames for once, we use the same vi-\nsual encoder EncV to extract the hierarchical visual fea-\ntures Ff\nV\n=\nn\nF f\nV,\u2113\no\nof reference frame Iref.\nThen, a\nlightweight mask encoder (e.g., ResNet18 [30]) receives the\nreference frame Iref, object mask annotation mo and the en-\ncoded frame features Ff\nV to generate the multi-scale mask\nfeatures Fm\nV =\nn\nF m\nV,\u2113\no\nfor the target object in reference\nframe. Here, \u2113 = 2, 3, 4 for F f\nV,\u2113 and F m\nV,\u2113.\nFf\nV = EncV (Iref)\n(1)\nFm\nV = EncM(Iref, mo, Ff\nV )\n(2)\nReferring Image Segmentation. The reference for RIS task\nis the language description T . To encode such linguistic\ninformation, we apply an off-the-shelf text encoder (e.g.,\nBERT [20] or RoBERTa [60]) to extract the language fea-\ntures FT \u2208 RL\u00d7C, where L is the sentence length and C is\nthe channel dimension.\nFT = EncT (T )\n(3)\nReferring Video Object Segmentation. RVOS requires the\nmodel to not only understand the language description, but\nalso track the referred object in the whole video. To this\nend, we encode both linguistic and visual information for\nthis task. Similarly, we use Eq. 3 to extract language feature\nand apply Eq. 1 and Eq. 2 for mask features encoding. It\nshould be noted that the mask annotation is available during\ntraining. And we use the predicted mask in the previous\nframe as the visual reference during inference.\n3.3. Multi-scale UniFusion Module\nAfter the reference encoding, a natural question is raised:\nHow to inject the reference information into the network?\nIn this subsection, we introduce our proposed multi-scale\nUniFusion module for the reference information injection.\nThe details of UniFusion module is illlustrated in Fig. 2b.\nWe fuse the visual features F and the reference features\nin a hierarchical manner. For simplicity, we take the \u2113-th\n(\u2113 = 2, 3, 4) visual level for illustration. The UniFusion\nmodule receives three inputs: the \u2113-th level visual feature F\u2113\nof current image and the corresponding key and value em-\nbeddings (F k\nr and F v\nr ) from reference features. For mask\nreference, F k\nr\n= Ff\nV , F v\nr\n= Fm\nV . For language refer-\nence, F k\nr = F v\nr = FT . These inputs are first linearly pro-\njected and further reformulated as three vectors, namely Q\u2113,\nK\u2113 and V\u2113. We first perform the mutli-head cross-attention\noperation between these vectors. Then, reference features\nF k\nr are pooled and regressed to obtain the scale, shift and\ngate parameters \u03b3, \u03b2, \u03b1, which are applied after the atten-\ntion block. Finally, the output features are injected into the\noriginal visual features via residual connection. The process\nof UniFusion is represented as:\nO\u2113 = Attention(Q\u2113, K\u2113, V\u2113)\n(4)\n\u03b3, \u03b2, \u03b1 = Linear(Pooling(F k\nr ))\n(5)\nF \u2032\n\u2113 = F\u2113 + \u03b1(O\u2113(1 + \u03b3) + \u03b2)\n(6)\nwhere O\u2113 is the intermediate results after the attention op-\neration. F \u2032\n\u2113 is the final output of UniFusion. Notably, the\nUniFusion module shares the same parameters in all vi-\nsual scales. We emphasize the distinguished characteristics\nof UniFusion are two-folds: (1) We implement the cross-\nattention operation using FlashAttention [18, 17].\nThis leads to the high efficiency and low memory cost when\ncomputing on the dense feature maps. (2) Inspired by the\nadaLN-zero block from [74], the linear layer for regress-\ning the scale, shift and gate parameters are zero-initialized.\nThis helps network gradually learn the knowledge from ref-\nerences and make UniFusion easily plug in the pretrained\nobject segmentation models.\nThanks to the unifying fusion format, the reference in-\nformation in different tasks can be injected into the visual\nfeatures using the same UniFusion module. For the FSS\nand VOS tasks, the reference features are from the anno-\ntated masks. And the reference is language description for\nRIS. We emphasize here, for RVOS task, both the language\nfeatures and reference frame visual features are fused with\nthe visual feature of current frame. In this fashion, the net-\nwork can not only find the referred object by language, but\nalso propagate the target mask across frames for tracking.\nThis also unifies the paradigms of VOS and RVOS to the\nonline pattern.\n3.4. Unified Architecture\nThe fused multi-scale visual features F\u2032 = {F \u2032\n\u2113}4\n\u2113=2\nhave discriminative representations in highlighting the spe-\ncific target by reference. We next adopt a unified transform-\nbased architecture to predict the target mask.\nTransformer. We use the two-stage version Deformable-\nDETR [131] as our object detector. It receives the fused\nhierarchical visual features F\u2032 as input and perform multi-\nscale deformable self-attention in the encoder. In the de-\ncoder, N object queries are iteratively refined over stacked\n4\nTable 1: Comparison with the state-of-the-art methods on three referring image segmentation (RIS) benchmarks. RN101\ndenotes ResNet-101 [30], WRN101 refers to Wide ResNet-101 [124], and DN53 denotes Darknet-53 [79].\nMethod\nVisual\nText\nRefCOCO\nRefCOCO+\nRefCOCOg\nBackbone\nEncoder\nval\ntest A\ntest B\nval\ntest A\ntest B\nval-u\ntest-u\noIoU\nCMSA [119]\nRN101\nLSTM\n58.32\n60.61\n55.09\n43.76\n47.60\n37.89\n-\n-\nSTEP [8]\nRN101\nBi-LSTM\n60.04\n63.46\n57.97\n48.19\n52.33\n40.41\n-\n-\nBRINet [35]\nRN101\nLSTM\n60.98\n62.99\n59.21\n48.17\n52.32\n42.11\n-\n-\nCMPC [36]\nRN101\nLSTM\n61.36\n64.53\n59.64\n49.56\n53.44\n43.23\n-\n-\nLSCM [37]\nRN101\nLSTM\n61.47\n64.99\n59.55\n49.34\n53.12\n43.50\n-\n-\nCMPC+ [58]\nRN101\nLSTM\n62.47\n65.08\n60.82\n50.25\n54.04\n43.47\n-\n-\nMCN [66]\nDN53\nBi-GRU\n62.44\n64.20\n59.71\n50.62\n54.99\n44.69\n49.22\n49.40\nEFN [27]\nWRN101\nBi-GRU\n62.76\n65.69\n59.67\n51.50\n55.24\n43.01\n-\n-\nLTS [39]\nDN53\nBi-GRU\n65.43\n67.76\n63.08\n54.21\n58.32\n48.02\n54.40\n54.25\nReSTR [43]\nViT-B\nTransformer\n67.22\n69.30\n64.45\n55.78\n60.44\n48.27\n-\n-\nLAVT [114]\nSwin-B\nBERT-base\n72.73\n75.82\n68.79\n62.14\n68.38\n55.10\n61.24\n62.09\nUniRef-R50\nRN50\nRoBERTa-base\n75.04\n77.28\n72.43\n63.25\n68.12\n55.56\n66.96\n68.77\nUniRef-L\nSwin-L\nRoBERTa-base\n79.79\n81.81\n77.02\n69.26\n74.11\n63.14\n73.04\n73.36\nUniRef++-R50\nRN50\nBERT-base\n75.63\n78.75\n72.91\n63.29\n68.68\n56.26\n68.38\n69.71\nUniRef++-L\nSwin-L\nBERT-base\n79.13\n82.21\n77.45\n68.43\n73.98\n61.45\n71.37\n72.84\nmIoU\nVLT [22]\nDN53\nBi-GRU\n65.65\n68.29\n62.73\n55.50\n59.20\n49.36\n52.99\n56.65\nCRIS [99]\nRN101\nGPT-2\n70.47\n73.18\n66.10\n62.27\n68.06\n53.68\n59.87\n60.36\nSeqTR [130]\nDN53\nBi-GRU\n71.70\n73.31\n69.82\n63.04\n66.73\n58.97\n64.69\n65.74\nRefTr [49]\nRN101\nBERT-base\n74.34\n76.77\n70.87\n66.75\n70.58\n59.40\n66.63\n67.39\nLAVT [114]\nSwin-B\nBERT-base\n74.46\n76.89\n70.94\n65.81\n70.97\n59.23\n63.34\n63.62\nPolyFormer-L [57]\nSwin-L\nBERT-base\n76.94\n78.49\n74.83\n72.15\n75.71\n66.73\n71.15\n71.17\nUniRef-R50\nRN50\nRoBERTa-base\n78.14\n80.09\n75.94\n69.09\n73.64\n62.62\n71.76\n73.10\nUniRef-L\nSwin-L\nRoBERTa-base\n81.90\n83.03\n79.61\n73.81\n78.30\n68.33\n76.65\n77.09\nUniRef++-R50\nRN50\nBERT-base\n78.97\n81.29\n76.51\n69.53\n74.93\n63.35\n73.37\n74.16\nUniRef++-L\nSwin-L\nBERT-base\n81.84\n83.48\n80.44\n74.02\n78.04\n68.31\n76.00\n77.20\ndecoder layers and converted into the query representations\nQobj \u2208 RN\u00d7C finally. Three predictions heads (class head,\nbox head and mask head) are further built on top of the\ndecoder to predict the object scores S \u2208 RN\u00d71, boxes\nB \u2208 RN\u00d74 and mask dynamic convolution [86, 14, 13]\nkernel parameters G = {gi}N\ni=1, respectively.\nMask Decoder. We take the output features (from strides 8\nto 32) of Transformer encoder and hierarchically fuse them\nin a FPN-like [54, 104] manner. The feature map with 4\u00d7\nstrides of backbone, namely F1, is also added in this pro-\ncess. This is helpful for preserving the reference-agnostic\nand fine-grained information of images. Consequently, we\nobtain the high-resolution mask features Fseg \u2208 R\nH\n4 \u00d7 W\n4 \u00d7C.\nFinally, the masks of target object are generated by perform-\ning dynamic convolution between Fseg and G:\nmi = Upsample\n\u0000DynamicConv(Fseg, gi)\n\u0001\n, i = 1, ..., N\n(7)\nDuring inference, we choose the mask with the highest\nscore as the final result m for the target object. Notably,\nwe empirically find that using more object queries leads to\nhigher performance, despite that one object query is suffi-\ncient for the reference-based tasks.\n3.5. Training and Inference\nWe train UniRef++ on all related benchmarks of\nreference-based object segmentation tasks (RIS, FSS,\nRVOS, VOS). The model with the same weights can per-\nform different tasks at run-time by specifying the refer-\nences.\nTraining. The network predicts N predictions of object\nscores, box coordinates and segmentation masks, where the\nobject score indicates whether the object is visible in cur-\nrent frame. During training, we apply the set prediction\nloss [7, 131, 85] on these predictions. There is only one\nground-truth for the reference-based object segmentation\ntasks. We assign multiple predictions to the ground-truth by\nselecting the top-k predictions with the least cost according\nto an optimal transport method [28, 29, 105]. The matching\ncost is formulated as:\nC = \u03bbcls \u00b7 Ccls + \u03bbL1 \u00b7 CL1 + \u03bbgiou \u00b7 Cgiou\n(8)\nwhere Ccls is the focal loss [55]. The box losses include the\nwidely-used \u21131 loss and generalized IoU (GIoU) loss [80].\nThe top-k predictions with the least cost are assigned as\n5\npositive samples and others as negatives. UniRef++ is opti-\nmized by minimizing the following loss function:\nL = \u03bbcls \u00b7 Lcls + \u03bbL1 \u00b7 LL1 + \u03bbgiou \u00b7 Lgiou +\n\u03bbmask \u00b7 Lmask + \u03bbdice \u00b7 Ldice\n(9)\nwhere the class loss and boxes losses are the same as those\nin Eq. 8. The mask-related losses contain the mask binary\ncross-entropy loss and DICE loss [70].\nInference. For RIS and FSS, we directly output the pre-\ndicted mask of the query that has the highest score. For\nRVOS and VOS, our method infers the video in a frame-by-\nframe online fashion without the complex post-processing.\nSpecifically, for the current frame, the network uses the cor-\nresponding references to produce the mask of target object.\nThe mask would be output if its object score is higher than\na pre-determined threshold \u03c3. Otherwise the output mask\nvalues are all set to zeros. To handle the videos that con-\ntain multiple objects, we adopt the soft-aggregation method\ncommonly used in prior works [72, 16].\n4. Experiments\nIn this section, we conduct comprehensive experiments\non all reference-based tasks (RIS, FSS, RVOS and VOS) to\nevaluate the effectiveness of our proposed UniRef++. The\nexperimental settings will be first introduced in Sec. 4.1. We\nthen compare UniRef++ with state-of-the-art methods on\nthe prevalent benchmarks in Sec. 4.2. The ablation studies\nare presented in Sec. 4.3.\n4.1. Experimental Setup\nDatasets. We evaluate our UniRef++ on four tasks to ver-\nify its effectiveness. The specific datasets leveraged in this\nwork for evaluation are presented in the following. (i) RIS:\nRefCOCO [122] consists of 142,209 language descriptions\nfor 50,000 objects in 19,994 images. RefCOCO+ [122] has\n141,564 expressions for 49,856 objects in 19,992 images.\nRefCOCOg [67] includes 85,474 referring expressions for\n54,822 objects in 26,711 images. And we use the UMD\nsplit for RefCOCOg [67].\n(ii) FSS: FSS-1000 [50] is a\nlarge-scale dataset for FSS task. It contains 10,000 images\nfrom 1,000 classes.\n(iii) RVOS: Ref-Youtube-VOS [82]\nis a large-scale referring video object segmentation dataset\nwhich contains 3,978 videos with around 15k langauge de-\nscriptions. Ref-DAVIS17 [42] provides the referring ex-\npressions for each object in DAVIS17 [75]. It contains 90\nvideos in total. (iv) VOS: Youtube-VOS1 [109] is the pop-\nular benchmark for video object segmentation. There are\n474 and 507 videos in the validation set for 2018 and 2019\nversion, respectively. LVOS [31] is a long-term video ob-\nject segmentation benchmark consisting of 220 videos. The\n1Youtube-VOS and Ref-Youtube-VOS are evaluated using the official\nserver https://youtube-vos.org/.\nTable 2: Comparison with the state-of-the-art methods on\nFSS-1000 validation set.\nMethod\nVenue\nmIoU\n1-shot\n5-shot\nSpecialist Models\nDAN [92]\nECCV\u201920\n85.2\n88.1\nHSNet [71]\nICCV\u201921\n86.5\n88.5\nSSP [26]\nECCV\u201922\n87.3\n88.6\nVAT [32]\nECCV\u201922\n90.3\n90.8\nDACM [108]\nECCV\u201922\n90.8\n91.7\nGeneralist Models\nPainter [97]\nCVPR\u201923\n61.7\n62.3\nSegGPT [98]\nICCV\u201923\n85.6\n89.3\nUniRef++-R50\nthis work\n79.1\n85.5\nUniRef++-L\nthis work\n85.4\n89.9\nvideos in LVOS have an average duration of 1.59 minutes,\nand the videos in Youtube-VOS last 6 seconds on average.\nMOSE [21] is a newly proposed dataset for evaluating VOS\nalgorithms in complex scenes, such as occlusion and disap-\npearance. It have 2,149 videos clips and 5,200 objects from\n36 categories, with a total of 431,725 annotated masks.\nImplementation Details. We experiment with two preva-\nlent backbones as our visual encoder: ResNet50 [30] and\nSwin Transformer-Large [62]. The text encoder is selected\nas BERT-base [20] and we set the max length of sentences\nas 77. The Transformer architecture has 6 encoders and 6\ndecoders with the channel dimension of 256. The number\nof object queries is set as 300 by default. The loss coeffi-\ncients in Eq. 9 are set as \u03bbcls = 2.0, \u03bbcls = 2.0, \u03bbL1 = 5.0,\n\u03bbmask = 2.0 and \u03bbdice = 5.0, respectively.\nThe entire training process includes three sequential\nstages, in which pretrained weights from the previous stage\nare loaded and used for further training. (1) Objects365 [84]\npretraining. In this stage, we do not incorporate the Uni-\nFusion module but aim to learn a strong object detector\nfor a massive of objects. Due to absence of mask anno-\ntation in the dataset, we also apply the BoxInst [87] loss\nfor mask supervision. (2) Image-level training. We first\ncombine the training set of RefCOCO/+/g to train the full\nnetwork, and then train the network for RIS and FSS tasks.\n(3) Video-level training. At this stage, we randomly sample\ntwo frames from a video, where the first frame is consid-\nered as the reference frame. To avoid the knowledge for-\ngetting for the RIS task, we also generate pseudo videos\nfor RefCOCO/+/g. The network is jointly trained on all\nthe related benchmarks, including RefCOCO/+/g [122, 67],\nRef-YoutubeVOS [82], Ref-DAVIS17 [42], COCO [56],\nYoutube-VOS19 [109], OVIS [76] and LVOS [31].\nIn this work, we use Pytorch toolkit [73] to conduct all\nexperiments on NVIDIA A100 GPUs. Unless otherwise\n6\nTable 3: Comparison with the state-of-the-art methods for\nreferring video object segmentation (RVOS). \u2020 and \u2021 denote\nthe model uses the tiny and base version of Video Swin\nTransformer [63] as visual encoders, respectively.\nMethod\nVisual\nJ &F\nJ\nJ\nEncoder\nRef-Youtube-VOS\nCMSA [119]\nResNet-50\n36.4\n34.8\n38.1\nURVOS [82]\n47.2\n45.3\n49.2\nYOFO [46]\n48.6\n47.5\n49.7\nReferFormer [102]\n58.7\n57.4\n60.1\nUniRef-R50\n60.6\n59.0\n62.3\nUniRef++-R50\n61.5\n59.7\n63.3\nPMINet + CFBI [25]\nEnsemble\n54.2\n53.0\n55.5\nCITD [52]\n61.4\n60.0\n62.7\nMTTR\u2020 [6]\nVideo-Swin\n55.3\n54.0\n56.6\nVLT\u2021 [23]\n63.8\n61.9\n65.6\nReferFormer\u2021 [102]\n64.9\n62.8\n67.0\nReferFormer [102]\n64.2\n62.3\n66.2\nUniRef-L\nSwin-L\n67.4\n65.5\n69.2\nUniRef++-L\n66.9\n64.8\n69.0\nRef-DAVIS17\nCMSA [119]\nResNet-50\n40.2\n36.9\n43.5\nURVOS [82]\n51.5\n47.3\n56.0\nYOFO [46]\n54.4\n50.1\n58.7\nReferFormer [102]\n58.5\n55.8\n61.3\nUniRef-R50\n63.5\n60.0\n67.0\nUniRef++-R50\n62.5\n58.7\n66.3\nVLT\u2021 [23]\nVideo-Swin\n61.6\n58.9\n64.3\nReferFormer\u2021 [102]\n61.1\n58.1\n64.1\nPolyFormer-L [57]\nSwin-L\n61.5\n57.2\n65.8\nReferFormer [102]\n60.5\n57.6\n63.4\nUniRef-L\n66.3\n62.9\n69.7\nUniRef++-L\n67.2\n63.4\n70.9\nstated, we use 4 \u00d7 8 A100 GPUs for the objects365 pre-\ntraining and 2 \u00d7 8 GPUs for the following image-level and\nvideo-level training. We adopt AdamW [64] as the opti-\nmizer and set the batch size as 2 for each GPU. We refer the\nreaders to Appendix for more implementation details.\n4.2. Quantitative Results\nWe employed ResNet50 [30] and Swin Transformer-\nLarge [62] as visual backbones in our experiments, denoted\nas UniRef++-R50 and UniRef++-L, respectively. For each\nversion, all results are computed with one suit of weights.\nReferring Image Segmentation. We compare UniRef++\nwith state-of-the-art methods in Table 1. Following the pre-\nvious works, we use both overall intersection-over-union\n(oIoU) and mean intersection-over-union (mIoU) as the\nevaluation metrics.\nIt can be seen that UniRef++ with\nTable 4: Comparison with the state-of-the-art methods on\nLVOS [31] and MOSE [21] validation set. These methods\nare all not trained on the MOSE dataset.\nLVOS val\nMOSE val\nMethod\nJ &F\nJ\nF\nJ &F\nJ\nF\nAFB-URR [53]\n34.8\n31.3\n38.2\n-\n-\n-\nCFBI [115]\n50.0\n45.0\n55.1\n-\n-\n-\nRDE [48]\n53.7\n48.3\n59.2\n46.8\n42.4\n51.3\nSTCN [16]\n45.8\n41.1\n50.5\n52.5\n48.5\n56.6\nAOT [116]\n59.4\n53.6\n65.2\n58.4\n54.3\n62.6\nXMem [15]\n50.0\n45.5\n54.4\n56.3\n52.1\n60.6\nDeAOT [118]\n-\n-\n-\n59.0\n54.6\n63.4\nUniRef-R50\n55.7\n51.5\n60.0\n-\n-\n-\nUniRef-L\n60.9\n57.2\n64.6\n-\n-\n-\nUniRef++-R50\n60.1\n55.8\n64.3\n54.7\n51.3\n58.2\nUniRef++-L\n67.2\n62.9\n71.5\n59.0\n55.7\n62.3\nResNet-50 backbone surpasses the previous methods on\nnearly all splits and it also has significant improvement over\nUniRef [103]. When equipped with Swin-Large backbone,\nUniRef++ sets new SoTA performance on several splits.\nFor example, our UniRef++-L has 4.99 mIoU performance\ngain over the SoTA method PolyFormer-L [57] on the test-\nA split of RefCOCO. We hypothesize the reason for the\nsimilar performance between UniRef and UniRef++ is that\nUniRef experienced the Visual Genome pretraining, which\nmakes it more friendly for the grounding tasks.\nFew-shot Segmentation. Following [50, 32], we divide\nthe 1,000 classes of FSS-1000 [50] dataset into 520, 240\nand 240 classes, which are used for training, validation and\ntesting, respectively. We evaluate our models on the val-\nidation set and report the results in Table 2. It is observed\nthat our models significantly benefit from the few-shot sam-\nples and achieve comparable results with the state-of-the-art\nspecialist models.\nReferring Video Object Segmentation.\nFor the RVOS\ntask, we use the region jaccard J , boundary accuracy F\nand the average score J &F as the evaluation metrics. The\ncomparison of UniRef++ and state-of-the-art methods are\npresented in Table 3. We empirically find that solely using\nthe language as reference would lead to better performance\non Ref-DAVIS17, possibly due to salient objects and simple\nscenes within the dataset. According to Table 3, it indicates\nthat UniRef++ has significant improvement over all the pre-\nvious methods on the two datasets. UniRef++ with ResNet-\n50 visual encoder achieves the state-of-the-art performance\nand has notable 2.8 J &F gain over ReferFormer [102] on\nRef-YoutubeVOS. On Ref-DAVIS17, UniRef++ sets new\nstate-of-the-art performance of 67.2 J &F, surpassing the\nprevious best method VLT [22] by 5.4 J &F.\n7\nTable 5: Comparison with the state-of-the-art methods on three video object segmentation (VOS) benchmarks.\nMethod\nYoutube-VOS 2018 val\nYoutube-VOS 2019 val\nDAVIS17 val\nG\nJs\nFs\nJu\nFu\nG\nJs\nFs\nJu\nFu\nJ &F\nJ\nF\nMemory-based Methods\nSTM [72]\n79.4\n79.7\n84.2\n72.8\n80.9\n-\n-\n-\n-\n-\n81.8\n79.2\n84.3\nAFB-URR [53]\n79.6\n78.8\n83.1\n74.1\n82.6\n-\n-\n-\n-\n-\n76.9\n74.4\n79.3\nCFBI [115]\n81.4\n81.1\n85.8\n75.3\n83.4\n81.0\n80.6\n85.1\n75.2\n83.0\n81.9\n79.1\n84.6\nRDE [48]\n-\n-\n-\n-\n-\n81.9\n81.1\n85.5\n76.2\n84.8\n84.2\n80.8\n87.5\nSTCN [16]\n83.0\n81.9\n86.5\n77.9\n85.7\n82.7\n81.1\n85.4\n78.2\n85.9\n85.4\n82.2\n88.6\nAOT-B [116]\n83.5\n82.6\n87.5\n77.7\n86.0\n83.3\n82.4\n87.1\n77.8\n86.0\n82.5\n79.7\n85.2\nAOT-L [116]\n83.8\n82.9\n87.9\n77.7\n86.5\n83.7\n82.8\n87.5\n78.0\n86.7\n83.8\n81.1\n86.4\nXMem [15]\n85.7\n84.6\n89.3\n80.2\n88.7\n85.5\n84.3\n88.6\n80.3\n88.6\n86.2\n82.9\n89.5\nDeAOT [118]\n86.0\n84.9\n89.9\n80.4\n88.7\n85.9\n84.6\n89.4\n80.8\n88.9\n85.2\n82.2\n88.2\nNon-memory Methods\nFRTM [81]\n72.1\n72.3\n76.2\n65.9\n74.1\n-\n-\n-\n-\n-\n76.7\n73.9\n79.6\nLWL [4]\n81.5\n80.4\n84.9\n76.4\n84.4\n81.0\n79.6\n83.8\n76.4\n84.2\n70.6\n67.9\n73.3\nUniRef-R50\n81.4\n81.6\n85.9\n75.6\n82.4\n81.2\n80.8\n84.9\n76.2\n83.0\n-\n-\n-\nUniRef-L\n82.6\n83.2\n87.5\n76.2\n83.7\n82.7\n82.9\n86.9\n76.8\n84.1\n-\n-\n-\nUniRef++-R50\n81.9\n82.3\n86.8\n75.9\n82.6\n81.9\n81.9\n86.2\n76.5\n83.2\n81.5\n78.1\n87.6\nUniRef++-L\n83.2\n83.8\n88.5\n76.8\n83.8\n83.0\n83.1\n87.7\n77.3\n84.1\n83.9\n80.8\n89.8\nTable 6: Ablation experiments of UniRef. We evaluate our model on the RefCOCO, FSS-1000, Ref-Youtube-VOS and\nYoutube-VOS2018 validation set, respectively. Our default settings are marked in gray .\n(a) Task-specific training.\nTraining\nRIS\nFSS\nRVOS VOS\noIoU mIoU J &F\nG\nSignle-task 72.1\n81.8\n58.3\n81.9\nMulti-task\n74.7\n80.3\n60.1\n81.9\n(b) Parameter-sharing for UniFusion.\nTasks Levels\nRIS\nFSS\nRVOS VOS\noIoU mIoU J &F\nG\n\u2713\n73.6\n64.1\n60.7\n81.9\n\u2713\n74.9\n75.2\n61.5\n80.9\n\u2713\n\u2713\n74.7\n80.3\n60.1\n81.9\n(c) Query number.\nQuery\nRIS\nFSS\nRVOS VOS\noIoU mIoU J &F\nG\n1\n72.1\n75.5\n58.4\n79.0\n100\n75.1\n77.7\n60.6\n81.9\n300\n74.7\n80.3\n60.1\n81.9\nVideo Object Segmentation. To evaluate the performance\non Youtube-VOS [109], region jaccard J and countour ac-\ncuracy F are computed for \u201dseen\u201d and \u201dunseen\u201d classes\nseparately, denoted by subscripts s and u. G is the average\nJ &F for both seen and unseen classes. For DAVIS17 [75],\nLVOS [31] and MOSE [21] datasets, J &F, J and F are\nadopted as the evaluation metrics. We provide a compre-\nhensive comparison of different methods on the three clas-\nsic datasets in Table 5. Our proposed UniRef++-L outper-\nforms non-memory-based methods, achieving the best re-\nsults with 83.2/83.0 G on the Youtube-VOS 2018/2019. Our\nmodels also display competitive results on DAVIS17 even if\nthe dataset is not included during training. Unlike memory-\nbased methods [72, 16], our approach does not rely on pre-\ndicted masks from past frames, making it more memory-\nefficient and suitable for long videos. We further demon-\nstrate the advantages of our model for handling long videos\nin complex scenes in Table 4. In this situation, UniRef++\nhave obvious performance improvement compared with the\nclassic memory-based methods.\n4.3. Ablation Study\nThe\nablation\nexperiments\nare\nevaluated\non\nRef-\nCOCO [122], FSS-1000 [50], Ref-Youtube-VOS [82] and\nYoutube-VOS2018 [109] validation set to study UniRef++\nin detail. Unless otherwise stated, we use the ResNet-50 as\nvisual backbone and only conduct the image-level training\nand video-level training for quick validation.\nTask-specific Training. In Table 6a, we compare results\nof single training and multi-task joint training. Single-task\nmodels are trained on the corresponding datasets, while\nmulti-task models are trained jointly on all datasets. The\nablation results indicate that multi-task learning offers sig-\nnificant benefits for both RIS and RVOS. Specifically, Ref-\nYoutube-VOS achieves 60.1 J &F, which is 1.8 points\nhigher than the task-specific model. This can attribute to the\nfact that the jointly trained model is better at learning mask\npropagation through VOS training. For FSS, the perfor-\nmance of multi-task model is slightly lower than the single-\ntask model. This is due to that we didn\u2019t use the FSS dataset\n8\na cow is standing a fair distance behind the brown cow which is moving forward.\nL\nL + M\n0%\n0%\n20%\n20%\n40%\n40%\n60%\n60%\n80%\n80%\na fire truck pulling out with another behind it.\nL\nL + M\n0%\n0%\n20%\n20%\n40%\n40%\n60%\n60%\n80%\n80%\nFigure 3: Comparison of the use of mask references for RVOS. The red masks highlight the predicted objects. The\npercentages indicate the relative temporal position of each frame in the video. \u2018L\u2019 and \u2018M\u2019 represent language and mask\nreferences, respectively.\nduring video-level training. In summary, multi-task joint\ntraining improves the performance of task-specific models\nand saves a significant number of parameters.\nParameter-sharing\nfor\nUniFusion\nModule.\nOur\nUniRef++ leverages a parameter-sharing UniFusion module\nto fuse information from both mask and language references\nfor multi-scale visual features. In Table 6b, we present abla-\ntion experiments on two weight-sharing variants for UniFu-\nsion module. As shown in the table, when we use different\nUniFusion modules for different tasks, the performance of\nFSS drops significantly because its data scale is relatively\nsmall. When applying different UniFusion for different vi-\nsual levels, G metric on Youtube-VOS2018 decreases from\n81.9 to 80.9. This suggests that a single parameter-sharing\nUniFusion module is more effective in learning frame sim-\nilarities for multi-scale visual features.\nQuery Number. Reference-based tasks have one specific\ntarget for each reference, making it possible to complete the\ntasks with just one query. In Table 6c, we present an ab-\nlation study on the number of queries to investigate its im-\npact on performance. As observed, increasing the number\nof queries from 1 to 100 leads to higher performance. This\nis reasonable as the model can have more candidates to find\nthe target object, which is particularly helpful in complex\nscenes where many similar objects co-exist. And we also\nobserve that using 100 and 300 queries would obtain the\nsimilar results.\nTable 7: Ablation on the references used for RVOS. In\nthe table, \u2018Lang\u2019 means language. Results are evaluated on\nRef-Youtube-VOS validation set.\nReference\nResNet-50\nSwin-L\nJ &F\nJ\nF\nJ &F\nJ\nF\nLang\n60.4\n58.9\n61.9\n65.5\n63.6\n67.4\nLang + Mask\n61.5\n59.7\n63.3\n66.9\n64.8\n69.0\nDoes Mask Reference Help RVOS? For the RVOS\ntask, UniRef++ processes the videos in an online-fashion.\nSpecifically, we not only use the language reference as guid-\nance, but also leverage the predicted masks in the previous\nframes for mask propagation. To study the effectiveness of\nthe mask reference, we use our final version models and\nprovide the ablation results in Table 7. As illustrated in\nthe table, when additionally using the mask references, the\nmodel gets 1.1 and 1.4 J &F improvement for UniRef++-\nR50 and UniRef++-L, respectively. This evidently proves\nthat the mask propagation helps the model to achieve tem-\nporal consistency for the target object.\n4.4. Qualitative Results\nIn order to demonstrate the effectiveness of employing\nmask references in RVOS, we present the qualitative results\nin Figure 3. As depicted in the first example, utilizing lan-\nguage references alone struggles in identifying the referred\n9\nTable 8: Comparison with other SAM [44]-variant methods. Ref-YT, Ref-D mean Ref-Youtube-VOS and Ref-DAVIS17\ndatasets. YT-18, YT-19 and D-17 are the abbreations for Youtube-VOS-18, Youtube-VOS-19 and DAVIS17, respectively.\nTask\nRIS\nFSS\nRVOS\nVOS\nDataset\nRefCOCO val\nFSS-1000\nRef-YT\nRef-D\nYT-18\nYT-19\nD-17\nMetric\noIoU\nmIoU\n1-shot\n5-shot\nJ &F\nJ &F\nG\nG\nJ &F\nReferSAM [106]\n64.6\n71.1\n-\n-\n-\n-\n-\n-\n-\nPerSAM [127]\n-\n-\n81.6\n-\n-\n-\n-\n-\n-\nPerSAM-F [127]\n-\n-\n86.3\n-\n-\n-\n-\n-\n-\nRefSAM [51]\n-\n-\n-\n-\n55.1\n66.1\n-\n-\n-\nSAM-PT [77]\n-\n-\n-\n-\n-\n-\n67.5\n-\n76.6\nSAM [44] + UniRef\n65.9\n70.4\n77.8\n84.2\n52.4\n61.2\n73.4\n72.8\n78.4\nimage encoder\nreference \nencoder\nimage\nlanguage / mask\nlightweight mask decoder\nUniFusion\n\u2744\nmask\n\u2744\nfrozen\ntrainable\nFigure 4: Our proposed UniFusion module plays as the\nplug-in component for SAM [44]. Analogously, UniFusion\ncould be also easily plugged in other object segmentation\nfoundation models.\nobject in a complex scene with multiple similar objects. By\nintegrating mask references, the network can leverage mask\npropagation to accurately track the target object. This fig-\nure illustrates the efficacy of incorporating mask references\nin RVOS for improving temporal consistency of target ob-\nject. We also show more visualization examples for VOS\nand RVOS in the Appendix.\n5. Inserting UniFusion into SAM\nAt the core of UniRef++ is the UniFusion module for\ninjecting the reference information into the network. To\nexplore its applicability as a plug-in component, we in-\nsert the UniFusion module into the advanced foundation\nmodel SAM [44]. We keep the heavy image encoder frozen\nand train the lightweight reference encoders, UniFusion and\nmask decoder. The network goes through the image-level\nand video-level training as described in Sec. 4.1. The exper-\niment is highly efficient, being able to be completed within\n20 hours using 8 A100s.\nWe compare the results with other SAM-variant methods\nin Table 8. It should be noted that other methods are fine-\ntuned on the corresponding datasets while our model has\none suit of weights. For FSS, our model lags behind the\nPerSAM [127] under 1-shot setting since FSS-1000 dataset\nis not included during video-level training. But the perfor-\nmance gap could be eliminated with few samples (5-shot).\nFrom the table, we show that combining SAM [44] with\nUniFusion could achieve satisfactory across the reference-\nbased tasks.\n6. Conclusion\nWe present UniRef++,\na unified model for four\nreference-based object segmentation tasks (RIS, FSS,\nRVOS and VOS). By introducing a UniFusion module to in-\ncorporate different types of references, our model can flex-\nibly perform multi-tasks at run-time by specifying the cor-\nresponding references and achieves superior performance\nwith a single network. We also show that UniFusion could\nbe play as the plug-in component for the foundation models\n(e.g., SAM [44]) for efficient finetuning.\nAcknowledgements\nThis paper is partially supported by the National Key\nR&D Program of China No.2022ZD0161000 and the Gen-\neral Research Fund of Hong Kong No.17200622. The pa-\nper is supported in part by the National Natural Science\nFoundation of China under grant No.62293540, 62293542,\nU1903215 and the Fundamental Research Funds for the\nCentral Universities No.DUT22ZD210.\n10\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022. 2\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In Proceedings\nof the IEEE international conference on computer vision,\npages 2425\u20132433, 2015. 2\n[3] Ali Athar, Alexander Hermans, Jonathon Luiten, Deva\nRamanan, and Bastian Leibe.\nTarvis:\nA unified ap-\nproach for target-based video segmentation. arXiv preprint\narXiv:2301.02657, 2023. 2\n[4] Goutam Bhat, Felix J\u00a8aremo Lawin, Martin Danelljan, An-\ndreas Robinson, Michael Felsberg, Luc Van Gool, and Radu\nTimofte. Learning what to learn for video object segmen-\ntation. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part II 16, pages 777\u2013794. Springer, 2020. 8\n[5] Deblina Bhattacharjee, Tong Zhang, Sabine S\u00a8usstrunk, and\nMathieu Salzmann. Mult: an end-to-end multitask learning\ntransformer. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12031\u2013\n12041, 2022. 2\n[6] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.\nEnd-to-end referring video object segmentation with multi-\nmodal transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n4985\u20134995, 2022. 2, 3, 7\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers.\nIn Com-\nputer Vision\u2013ECCV 2020:\n16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16,\npages 213\u2013229. Springer, 2020. 5\n[8] Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong\nChen, and Tyng-Luh Liu. See-through-text grouping for\nreferring image segmentation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 7454\u20137463, 2019. 2, 5\n[9] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A language modeling framework\nfor object detection.\narXiv preprint arXiv:2109.10852,\n2021. 2\n[10] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J\nFleet, and Geoffrey Hinton. A unified sequence interface\nfor vision tasks. arXiv preprint arXiv:2206.07669, 2022. 2\n[11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server. arXiv preprint arXiv:1504.00325, 2015. 2\n[12] Xi Chen, Zuoxin Li, Ye Yuan, Gang Yu, Jianxin Shen, and\nDonglian Qi. State-aware tracker for real-time video object\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9384\u2013\n9393, 2020. 3\n[13] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1290\u20131299, 2022. 2, 5\n[14] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\npixel classification is not all you need for semantic segmen-\ntation.\nAdvances in Neural Information Processing Sys-\ntems, 34:17864\u201317875, 2021. 5\n[15] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In Computer Vision\u2013ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXVIII, pages 640\u2013658. Springer, 2022.\n2, 3, 7, 8\n[16] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Re-\nthinking space-time networks with improved memory cov-\nerage for efficient video object segmentation. Advances in\nNeural Information Processing Systems, 34:11781\u201311794,\n2021. 2, 3, 4, 6, 7, 8\n[17] Tri Dao. FlashAttention-2: Faster attention with better par-\nallelism and work partitioning. 2023. 4, 17\n[18] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00b4e. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural\nInformation Processing Systems, 2022. 4, 17\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248\u2013255. Ieee, 2009. 2\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 4, 6, 17\n[21] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,\nPhilip HS Torr, and Song Bai.\nMose: A new dataset\nfor video object segmentation in complex scenes.\narXiv\npreprint arXiv:2302.01872, 2023. 6, 7, 8\n[22] Henghui Ding, Chang Liu, Suchen Wang, and Xudong\nJiang.\nVision-language transformer and query genera-\ntion for referring segmentation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 16321\u201316330, 2021. 2, 5, 7\n[23] Henghui Ding, Chang Liu, Suchen Wang, and Xudong\nJiang.\nVlt: Vision-language transformer and query gen-\neration for referring segmentation. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2022. 7\n[24] Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei,\nJizhong Han, and Si Liu.\nLanguage-bridged spatial-\ntemporal interaction for referring video object segmenta-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 4964\u20134973,\n2022. 3\n[25] Zihan Ding, Tianrui Hui, Shaofei Huang, Si Liu, Xuan Luo,\nJunshi Huang, and Xiaoming Wei. Progressive multimodal\n11\ninteraction network for referring video object segmentation.\nThe 3rd Large-scale Video Object Segmentation Challenge,\npage 7, 2021. 7\n[26] Qi Fan, Wenjie Pei, Yu-Wing Tai, and Chi-Keung Tang.\nSelf-support few-shot semantic segmentation. In European\nConference on Computer Vision, pages 701\u2013719. Springer,\n2022. 6\n[27] Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu.\nEncoder fusion network with co-attention embedding for\nreferring image segmentation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15506\u201315515, 2021. 5\n[28] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and\nJian Sun. Ota: Optimal transport assignment for object de-\ntection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 303\u2013312,\n2021. 5\n[29] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021. 5\n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4, 5, 6, 7, 17\n[31] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang,\nPinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: A\nbenchmark for long-term video object segmentation. arXiv\npreprint arXiv:2211.10181, 2022. 6, 7, 8, 17, 18\n[32] Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and\nSeungryong Kim. Cost aggregation with 4d convolutional\nswin transformer for few-shot segmentation. In European\nConference on Computer Vision, pages 108\u2013126. Springer,\n2022. 2, 3, 6, 7\n[33] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-\nmentation from natural language expressions. In Computer\nVision\u2013ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11\u201314, 2016, Proceedings,\nPart I 14, pages 108\u2013124. Springer, 2016. 2\n[34] Tao Hu, Pengwan Yang, Chiliang Zhang, Gang Yu, Yadong\nMu, and Cees GM Snoek. Attention-based multi-context\nguiding for few-shot semantic segmentation. In Proceed-\nings of the AAAI conference on artificial intelligence, vol-\nume 33, pages 8441\u20138448, 2019. 3\n[35] Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and\nHuchuan Lu.\nBi-directional relationship inferring net-\nwork for referring image segmentation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4424\u20134433, 2020. 2, 5\n[36] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao\nWei, Jizhong Han, Luoqi Liu, and Bo Li. Referring image\nsegmentation via cross-modal progressive comprehension.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10488\u201310497, 2020.\n2, 5\n[37] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu,\nFaxi Zhang, and Jizhong Han. Linguistic structure guided\ncontext modeling for referring image segmentation.\nIn\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16,\npages 59\u201375. Springer, 2020. 5\n[38] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita\nOrlov, and Humphrey Shi. Oneformer: One transformer\nto rule universal image segmentation.\narXiv preprint\narXiv:2211.06220, 2022. 2\n[39] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tie-\nniu Tan. Locate then segment: A strong pipeline for refer-\nring image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 9858\u20139867, 2021. 2, 5\n[40] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-\nmodulated detection for end-to-end multi-modal under-\nstanding. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1780\u20131790, 2021.\n2\n[41] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young\nLee, and Alexander Schwing. Putting the object back into\nvideo object segmentation.\narXiv e-prints, pages arXiv\u2013\n2310, 2023. 3\n[42] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video\nobject segmentation with language referring expressions.\nIn Computer Vision\u2013ACCV 2018:\n14th Asian Confer-\nence on Computer Vision, Perth, Australia, December 2\u20136,\n2018, Revised Selected Papers, Part IV 14, pages 123\u2013141.\nSpringer, 2019. 1, 6\n[43] Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng,\nand Suha Kwak.\nRestr: Convolution-free referring im-\nage segmentation using transformers.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18145\u201318154, 2022. 2, 5\n[44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment\nanything. arXiv preprint arXiv:2304.02643, 2023. 10\n[45] Alexander Kolesnikov, Andr\u00b4e Susano Pinto, Lucas Beyer,\nXiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby.\nUvim: A unified modeling approach for vision with learned\nguiding codes. arXiv preprint arXiv:2205.10337, 2022. 2\n[46] Dezhuang Li, Ruoqi Li, Lijun Wang, Yifan Wang, Jinqing\nQi, Lu Zhang, Ting Liu, Qingquan Xu, and Huchuan Lu.\nYou only infer once: Cross-modal meta-transfer for refer-\nring video object segmentation. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, pages\n1297\u20131305, 2022. 3, 7\n[47] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022. 2\n[48] Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan,\nand Dong Liu. Recurrent dynamic embedding for video ob-\nject segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1332\u20131341, 2022. 3, 7, 8\n12\n[49] Muchen Li and Leonid Sigal.\nReferring transformer:\nA one-step approach to multi-task visual grounding.\nAdvances\nin\nneural\ninformation\nprocessing\nsystems,\n34:19652\u201319664, 2021. 2, 5\n[50] Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, and\nChi-Keung Tang. Fss-1000: A 1000-class dataset for few-\nshot segmentation. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n2869\u20132878, 2020. 6, 7, 8, 18\n[51] Yonglin Li, Jing Zhang, Xiao Teng, and Long Lan. Ref-\nsam:\nEfficiently adapting segmenting anything model\nfor referring video object segmentation.\narXiv preprint\narXiv:2307.00997, 2023. 10\n[52] Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang,\nZongxin Yang, Yunchao Wei, and Yi Yang.\nRethink-\ning cross-modal interaction from a top-down perspective\nfor referring video object segmentation.\narXiv preprint\narXiv:2106.01061, 2021. 7\n[53] Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen.\nVideo object segmentation with adaptive feature bank and\nuncertain-region refinement. Advances in Neural Informa-\ntion Processing Systems, 33:3430\u20133441, 2020. 3, 7, 8\n[54] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 5\n[55] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\nand Piotr Doll\u00b4ar. Focal loss for dense object detection. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 2980\u20132988, 2017. 5\n[56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In Computer Vision\u2013ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014. 6,\n17, 18\n[57] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Ku-\nmar Satzoda, Vijay Mahadevan, and R Manmatha. Poly-\nformer: Referring image segmentation as sequential poly-\ngon generation. arXiv e-prints, pages arXiv\u20132302, 2023. 5,\n7\n[58] Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li,\nand Guanbin Li. Cross-modal progressive comprehension\nfor referring segmentation.\nIEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 44(9):4761\u20134775,\n2021. 5\n[59] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023. 2\n[60] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov.\nRoberta:\nA ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019. 4\n[61] Yongfei Liu, Xiangyi Zhang, Songyang Zhang, and Xum-\ning He. Part-aware prototype network for few-shot seman-\ntic segmentation, 2020. 2\n[62] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 6, 7\n[63] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 3202\u20133211, 2022. 7\n[64] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 7\n[65] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh\nMottaghi, and Aniruddha Kembhavi. Unified-io: A unified\nmodel for vision, language, and multi-modal tasks. arXiv\npreprint arXiv:2206.08916, 2022. 2\n[66] Gen Luo,\nYiyi Zhou,\nXiaoshuai Sun,\nLiujuan Cao,\nChenglin Wu, Cheng Deng, and Rongrong Ji.\nMulti-\ntask collaborative network for joint referring expression\ncomprehension and segmentation. In Proceedings of the\nIEEE/CVF Conference on computer vision and pattern\nrecognition, pages 10034\u201310043, 2020. 2, 5\n[67] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy. Generation\nand comprehension of unambiguous object descriptions. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 11\u201320, 2016. 6, 17, 18\n[68] Yunyao Mao, Ning Wang, Wengang Zhou, and Houqiang\nLi. Joint inductive and transductive learning for video ob-\nject segmentation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9670\u20139679,\n2021. 3\n[69] Bruce McIntosh, Kevin Duarte, Yogesh S Rawat, and\nMubarak Shah.\nVisual-textual capsule routing for text-\nbased video segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 9942\u20139951, 2020. 3\n[70] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 2016 fourth international\nconference on 3D vision (3DV), pages 565\u2013571. Ieee, 2016.\n6\n[71] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorre-\nlation squeeze for few-shot segmentation. In Proceedings\nof the IEEE/CVF international conference on computer vi-\nsion, pages 6941\u20136952, 2021. 6\n[72] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 9226\u20139235, 2019.\n2, 3, 6, 8\n[73] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library.\n13\nAdvances in neural information processing systems, 32,\n2019. 6\n[74] William Peebles and Saining Xie. Scalable diffusion mod-\nels with transformers. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 4195\u2013\n4205, 2023. 4\n[75] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool.\nThe\n2017 davis challenge on video object segmentation. arXiv\npreprint arXiv:1704.00675, 2017. 1, 6, 8\n[76] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu\nLiu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS\nTorr, and Song Bai. Occluded video instance segmentation:\nA benchmark. International Journal of Computer Vision,\n130(8):2022\u20132039, 2022. 6, 17, 18\n[77] Frano Raji\u02c7c, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin\nDanelljan, and Fisher Yu. Segment anything meets point\ntracking. arXiv preprint arXiv:2307.01197, 2023. 10\n[78] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. In International conference on learn-\ning representations, 2016. 1\n[79] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\nimprovement. arXiv preprint arXiv:1804.02767, 2018. 5\n[80] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 658\u2013666,\n2019. 5\n[81] Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan,\nFahad Shahbaz Khan, and Michael Felsberg. Learning fast\nand robust target models for video object segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 7406\u20137415, 2020. 3,\n8\n[82] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos:\nUnified referring video object segmentation network with a\nlarge-scale benchmark. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part XV 16, pages 208\u2013223. Springer,\n2020. 6, 7, 8, 18\n[83] Hongje Seong, Seoung Wug Oh, Joon-Young Lee, Seong-\nwon Lee, Suhyeon Lee, and Euntai Kim.\nHierarchical\nmemory matching network for video object segmentation.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 12889\u201312898, 2021. 3\n[84] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 8430\u20138439, 2019. 6, 18\n[85] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-\nfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan\nYuan, Changhu Wang, et al. Sparse r-cnn: End-to-end ob-\nject detection with learnable proposals. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14454\u201314463, 2021. 5\n[86] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional con-\nvolutions for instance segmentation. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part I 16, pages 282\u2013298.\nSpringer, 2020. 5\n[87] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen.\nBoxinst:\nHigh-performance instance segmentation with\nbox annotations. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n5443\u20135452, 2021. 6\n[88] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig\nAdam, Bastian Leibe, and Liang-Chieh Chen.\nFeelvos:\nFast end-to-end embedding learning for video object seg-\nmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9481\u2013\n9490, 2019. 3\n[89] Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, and\nBastian Leibe. Siam r-cnn: Visual tracking by re-detection.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 6578\u20136588, 2020. 3\n[90] Hao Wang, Cheng Deng, Fan Ma, and Yi Yang. Context\nmodulated dynamic networks for actor and action video\nsegmentation with language queries.\nIn Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34,\npages 12152\u201312159, 2020. 3\n[91] Hao Wang, Cheng Deng, Junchi Yan, and Dacheng Tao.\nAsymmetric cross-guided attention network for actor and\naction video segmentation from natural language query. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3939\u20133948, 2019. 3\n[92] Haochen Wang, Xudong Zhang, Yutao Hu, Yandan Yang,\nXianbin Cao, and Xiantong Zhen. Few-shot semantic seg-\nmentation with democratic attention networks.\nIn Com-\nputer Vision\u2013ECCV 2020:\n16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XIII\n16, pages 730\u2013746. Springer, 2020. 3, 6\n[93] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,\nand Jiashi Feng.\nPanet: Few-shot image semantic seg-\nmentation with prototype alignment. In proceedings of the\nIEEE/CVF international conference on computer vision,\npages 9197\u20139206, 2019. 2\n[94] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang.\nOfa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learn-\ning framework. In International Conference on Machine\nLearning, pages 23318\u201323340. PMLR, 2022. 2\n[95] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu,\nand Philip HS Torr. Fast online object tracking and seg-\nmentation: A unifying approach.\nIn Proceedings of the\nIEEE/CVF conference on Computer Vision and Pattern\nRecognition, pages 1328\u20131338, 2019. 3\n[96] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442, 2022. 2\n14\n[97] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang.\nImages speak in images:\nA generalist\npainter for in-context visual learning.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6830\u20136839, 2023. 6\n[98] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang.\nSeggpt: Segmenting\neverything in context.\narXiv preprint arXiv:2304.03284,\n2023. 6\n[99] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong\nGuo, Mingming Gong, and Tongliang Liu.\nCris: Clip-\ndriven referring image segmentation.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11686\u201311695, 2022. 2, 5\n[100] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\nKweon. Cbam: Convolutional block attention module. In\nProceedings of the European conference on computer vi-\nsion (ECCV), pages 3\u201319, 2018. 17\n[101] Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai,\nand Song Bai. General object foundation model for im-\nages and videos at scale. arXiv preprint arXiv:2312.09158,\n2023. 2\n[102] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping\nLuo. Language as queries for referring video object seg-\nmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4974\u2013\n4984, 2022. 2, 3, 7\n[103] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan,\nand Ping Luo. Segment every reference object in spatial and\ntemporal spaces. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2538\u20132550,\n2023. 7\n[104] Junfeng Wu, Yi Jiang, Wenqing Zhang, Xiang Bai,\nand Song Bai.\nSeqformer:\na frustratingly simple\nmodel for video instance segmentation.\narXiv preprint\narXiv:2112.08275, 2021. 5\n[105] Junfeng Wu, Qihao Liu, Yi Jiang, Song Bai, Alan Yuille,\nand Xiang Bai.\nIn defense of online models for video\ninstance segmentation. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u201327,\n2022, Proceedings, Part XXVIII, pages 588\u2013605. Springer,\n2022. 5\n[106] Yuhang Xiao Xiao. Refersam. https://github.com/\nmydcxiao/ReferSAM, 2023. 10\n[107] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping\nZhang, and Wenxiu Sun. Efficient regional memory net-\nwork for video object segmentation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1286\u20131295, 2021. 3\n[108] Zhitong Xiong, Haopeng Li, and Xiao Xiang Zhu. Dou-\nbly deformable aggregation of covariance matrices for few-\nshot segmentation. In European Conference on Computer\nVision, pages 133\u2013150. Springer, 2022. 2, 3, 6\n[109] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue,\nYuchen Liang,\nJianchao Yang,\nand Thomas Huang.\nYoutube-vos:\nA large-scale video object segmentation\nbenchmark. arXiv preprint arXiv:1809.03327, 2018. 6, 8,\n17, 18\n[110] Xiaohao Xu, Jinglu Wang, Xiao Li, and Yan Lu.\nRe-\nliable propagation-correction modulation for video object\nsegmentation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 36, pages 2946\u20132954, 2022.\n3\n[111] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan,\nPing Luo, and Huchuan Lu. Towards grand unification of\nobject tracking. In Computer Vision\u2013ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXI, pages 733\u2013751. Springer, 2022. 2\n[112] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-\nhuan Yuan, and Huchuan Lu. Universal instance percep-\ntion as object discovery and retrieval. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15325\u201315336, 2023. 2\n[113] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qix-\niang Ye.\nPrototype mixture models for few-shot seman-\ntic segmentation. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part VIII 16, pages 763\u2013778. Springer, 2020.\n2\n[114] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-\nshuang Zhao, and Philip HS Torr. Lavt: Language-aware\nvision transformer for referring image segmentation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18155\u201318165, 2022.\n2, 5\n[115] Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative\nvideo object segmentation by foreground-background inte-\ngration. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part V, pages 332\u2013348. Springer, 2020. 3, 7, 8\n[116] Zongxin Yang, Yunchao Wei, and Yi Yang.\nAssociat-\ning objects with transformers for video object segmenta-\ntion. Advances in Neural Information Processing Systems,\n34:2491\u20132502, 2021. 2, 3, 7, 8\n[117] Zongxin Yang, Yunchao Wei, and Yi Yang.\nCollabora-\ntive video object segmentation by multi-scale foreground-\nbackground integration.\nIEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 44(9):4701\u20134712,\n2021. 3\n[118] Zongxin Yang and Yi Yang. Decoupling features in hier-\narchical propagation for video object segmentation. arXiv\npreprint arXiv:2210.09782, 2022. 3, 7, 8\n[119] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.\nCross-modal self-attention network for referring image seg-\nmentation.\nIn Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 10502\u2013\n10511, 2019. 2, 3, 5, 7\n[120] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,\nMojtaba Seyedhosseini, and Yonghui Wu.\nCoca: Con-\ntrastive captioners are image-text foundation models. arXiv\npreprint arXiv:2205.01917, 2022. 2\n[121] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. Mattnet: Modular at-\ntention network for referring expression comprehension. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 1307\u20131315, 2018. 2\n15\n[122] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n1, 6, 8, 17, 18\n[123] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, et al.\nFlorence:\nA new\nfoundation model for computer vision.\narXiv preprint\narXiv:2111.11432, 2021. 2\n[124] Sergey Zagoruyko and Nikos Komodakis. Wide residual\nnetworks. arXiv preprint arXiv:1605.07146, 2016. 5\n[125] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan\nLi, Jianwei Yang, and Lei Zhang. A simple framework for\nopen-vocabulary segmentation and detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 1020\u20131031, 2023. 2\n[126] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu\nYuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Uni-\nfying localization and vision-language understanding. In\nAdvances in Neural Information Processing Systems, 2022.\n2\n[127] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-\ning Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-\nalize segment anything model with one shot. arXiv preprint\narXiv:2305.03048, 2023. 10\n[128] Wangbo Zhao, Kai Wang, Xiangxiang Chu, Fuzhao Xue,\nXinchao Wang, and Yang You.\nModeling motion with\nmulti-modal features for text-based video segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 11737\u201311746, 2022.\n3\n[129] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra. Detecting twenty-thousand\nclasses using image-level supervision. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part IX, pages 350\u2013\n368. Springer, 2022. 17\n[130] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,\nXingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xi-\naoshuai Sun, and Rongrong Ji. Seqtr: A simple yet uni-\nversal network for visual grounding. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part XXXV, pages 598\u2013\n615. Springer, 2022. 2, 5\n[131] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection.\narXiv preprint\narXiv:2010.04159, 2020. 4, 5\n[132] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\nLi, Xiaohua Wang, and Jifeng Dai.\nUni-perceiver: Pre-\ntraining unified architecture for generic perception for zero-\nshot and few-shot tasks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16804\u201316815, 2022. 2\n[133] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,\nChunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,\nLu Yuan, et al. Generalized decoding for pixel, image, and\nlanguage. arXiv preprint arXiv:2212.11270, 2022. 2\n16\nAppendix A. Architecture\nReference Encoding Figure 5 illustrates the process of ref-\nerence encoding. (i) For mask references, we employ the\nsame visual encoder EncV for both the current and refer-\nence frames to generate multi-scale features (i.e., C3, C4,\nC5). We denote the encoded features of the reference frame\nas Ff\nV , where the \u2113-th feature (\u2113 = 2, 3, 4) has a size\nof H\u2113 \u00d7 W\u2113 \u00d7 C, with a spatial stride of 2\u2113+1 relative\nto the original size. Next, we use a lightweight mask en-\ncoder (ResNet-18 in all our experiments) that takes the ref-\nerence frame and annotated mask as inputs. We concatenate\nthe last three layer features with the corresponding level\nfeatures in Ff\nV and further process them with two Res-\nBlocks [30] and a CBAM block [100] to obtain the final\noutputs, denoted as Fm\nV . Finally, we flatten each level fea-\nture in Ff\nV and Fm\nV into 1-dimensional vectors. (ii) For\nlanguage references, we directly use off-the-shelf text en-\ncoder BERT [20] to extract the 1-d linguistic features.\n(a) Mask Reference\n(b) Language Reference\nVisual \nEncoder\nMask\nEncoder\n\ud835\udc6f\ud835\udc8d\ud835\udc7e\ud835\udc8d \u00d7 \ud835\udc6a\n\ud835\udc6f\ud835\udc8d\ud835\udc7e\ud835\udc8d \u00d7 \ud835\udc6a\ngira\ufb00e kicked by \nan animal in the\nyard of a zoo.\nText\nEncoder\n\ud835\udc73 \u00d7 \ud835\udc6a\nFigure 5: The process of reference encoding for (a) mask\nreferences and (b) language references.\nAppendix B. Implementation Details\nTraining Details. Our training process consists of three se-\nquential stages: Objects365 pretraining, image-level train-\ning and video-level training. We train models on NVIDIA\nA100 GPUs and it takes 5-7 days (depends on the visual\nbackbone) to complete the whole training. The text encoder\nis unfrozen during the first two stages and then frozen for\nthe final stage. The detailed configurations are summarized\nin Table 10. We follow the implementation of Detic [129]\nfor the multi-dataset training. The learning rate is reduced\nby the factor of 10 when the iteration reaches the specified\nstep in the table. Data augmentation includes random hori-\nzontal flip and scale jitter for resizing the input images. In\nthe table, short side means the range of values for the short-\nest side and long side represents the maximum value for the\nlongest side. During video-level training, for COCO [56]\nand RefCOCO/+/g [122, 67], we apply two different aug-\nmentations on the same image to generate the pseudo videos\nfor training. And for OVIS [76], we convert the dataset into\na class-agnostic format to make it suitable for VOS training.\nInference Details. For both RVOS and VOS tasks, all the\nvideos are rescaled to 480p for inference. And the score\nthresholds are set as 0.4 for VOS datasets and 0.3 for RVOS\ndatasets, respectively. For these two tasks, both the masks in\nthe first frame and previous frame are adopted as references.\nAppendix C. More Results\nReference Frames for Mask Propagation. In this study,\nwe investigate the effect of reference frames for mask prop-\nagation, which is presented in Table 9. Specifically, we an-\nalyze the impact of discarding the first frame and the previ-\nous frame on performance for Youtube-VOS2018 and Ref-\nYoutube-VOS. These two datasets are evaluated for VOS\nand RVOS tasks, respectively. On Youtube-VOS2018, the\nfirst frame provides a reliable annotated mask, while the\nprevious frame has the highest similarity with the current\nframe. Therefore, discarding either of these frames would\nresult in a significant drop in performance. on Ref-Youtube-\nVOS, there is no ground-truth mask in the first frame. Thus\nthe performance drop is less noticeable. Nevertheless, our\nfindings support the conclusion that utilizing both the first\nframe and the previous frame as references yields the best\nresults for mask propagation.\nTable 9: Ablation on the reference frames used for mask\npropagation during inference. We use the final model with\nResNet-50 visual backbone in this ablation. Our default set-\ntings are marked in gray .\nFirst Previous\nYoutube-VOS2018\nRef-Youtube-VOS\nG\nJs\nFs\nJu\nFu\nJ &F\nJ\nF\n\u2713\n76.6 78.7 82.8 69.6 75.2\n60.8\n59.1 62.6\n\u2713\n80.0 80.7 85.1 73.4 80.7\n61.0\n59.4 62.7\n\u2713\n\u2713\n81.9 82.3 86.8 75.9 82.6\n61.5\n59.7 63.3\nEfficiency Comparison.\nWe compare the efficiency of\nusing FlashAttention [18, 17] on two VOS datasets,\nnamely Youtube-VOS18 [109] and LVOS [31], as dis-\nplayed in Table 11.\nThe results clearly show that\nFlashAttention could improve the FPS during infer-\nence, especially when employing the multi-head attention.\nAlso, it can greatly reduce the GPU memory cost during\ntraining, consuming only 12.7G with 8 heads.\nOur method has a constant memory cost, while the clas-\nsic memory-based methods have linear memory complexity\nwith respect to the video duration. This suggests that our\nmethod is more efficient for the long-term videos. To better\n17\nTable 10: The detailed configurations for the three training stages.\nStage\nTask\nDataset\nSampling\nWeight\nBatch\nSize\nShort\nSide\nLong\nSide\nGPU\nNumber\nLearning\nRate\nWeight\nDecay\nMax\nIteration\nStep\nI\nDet\nObjects365 [84]\n1\n2\n480 \u223c 800 1333\n32\n0.0002\n0.05\n340,000\n310,000\nII\nDet\nCOCO [56]\n1\n2\n480 \u223c 800 1333\n16\n0.0002\n0.05\n90,000\n75,000\nRIS\nRefCOCO/+/g [122, 67]\n1\n2\n480 \u223c 800 1333\nFSS\nFSS-1000 [50]\n0.05\n2\n480 \u223c 800 1333\nIII\nVOS\nCOCO [56]\n0.40\n2\n320 \u223c 640\n768\n16\n0.0001\n0.05\n90,000\n75,000\nYoutube-VOS2019 [109]\n0.30\n2\n320 \u223c 640\n768\nLVOS [31]\n0.20\n2\n320 \u223c 640\n768\nOVIS [76]\n0.10\n2\n320 \u223c 640\n768\nRVOS\nRefCOCO/g/+ [122, 67]\n0.50\n2\n480 \u223c 800 1333\nRef-Youtube-VOS [82]\n0.50\n2\n320 \u223c 640\n768\nTable 11: Efficiency comparison of FlashAttention.\n\u2018YT-VOS18\u2019 represents Youtube-VOS2018 dataset. FPS is\nmeasured during inference using A100 GPU. Memory is the\nGPU memory cost during training.\nDataset\nMean\nMean\nFlash\nNum\nFPS Memory\nFrames Objects Attention Heads\nYT-VOS18\n27\n1.9\n\u2713\n1\n12.4\n11.6G\n\u2713\n8\n11.1\n12.7G\n\u2717\n1\n11.9\n13.4G\n\u2717\n8\n8.3\n29.9G\nLVOS\n574\n1.3\n\u2713\n1\n24.4\n11.6G\n\u2713\n8\n20.5\n12.7G\n\u2717\n1\n23.4\n13.4G\n\u2717\n8\n15.9\n29.9G\n1,000\n2,000\n3,000\n4,000\n5,000\n6,000\n7,000\n0.1\n1\n10\n20\n30\n100\nNumber of processed frames\nFPS\nUniRef\nSTCN\nFigure 6: FPS scaling of our method and the represen-\ntative memory-based method STCN.\nhighlight the advantages of our method, we further compare\nour method with the representative memory-based method\nSTCN in terms of the single-object FPS-scaling in Figure 6.\nAppendix D. Visualization Results\nWe provide the visualization results of UniRef-L for\nRVOS tasks in Figure 7 and Figure 8. It can be seen that\nour model can segment the referred objects correctly and ac-\ncurately in various challenging scenes, e.g., partial display,\nsimilar objects and fast moving, as illustrated in Figure 7.\nVisualization results for the VOS tasks are presented in\nFigure 9 and Figure 10. Notably, our model reveals strong\nability in handling long-term videos that typically last for\nover a minute, such as those in LVOS [31]. As shown in\nFigure 10, our model can accurately segment the target ob-\njects throughout the whole video, despite the objects have\nsignificant pose variation. We further provide a video demo\nin the supplementary material.\n18\na dog is waiting to catch the ball shown to him.\na hand is showing a ball to a dog.\na lawn tennis ball in the hand of a person.\na whale swimming from the bottom to the top of the water.\na whale on the top right swimming underwater.\na skateboard being rolled through a road filled with cars and people.\na boy in black shorts and white tee shirt roller skating.\nFigure 7: Visualization results on Ref-Youtube-VOS validation set.\na go-cart type car.\na person driving the go cart.\nperson at the back of the go-cart without a helmet.\na man wearing a green helmet.\na motor-bike.\na blonde haired girl dancing in a blue dress.\nFigure 8: Visualization results on Ref-DAVIS17 validation set.\n19\nFigure 9: Visualization results on Youtube-VOS2018 validation set.\nFigure 10: Visualization results on LVOS validation set.\n20\n"
  },
  {
    "title": "Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases",
    "link": "https://arxiv.org/pdf/2312.15011.pdf",
    "upvote": "15",
    "text": "Gemini\nvs GPT-4V\n: A Preliminary\nComparison and Combination of Vision-Language\nModels Through Qualitative Cases\nZhangyang Qi1, 7\u2217\nYe Fang2, 7\u2217\nMengchen Zhang3, 7\u2217\nZeyi Sun4, 7\u2217\nTong Wu5, 7\nZiwei Liu6, 7\nDahua Lin5, 7\nJiaqi Wang7\u2020\nHengshuang Zhao1\u2020\n\u2217 Equal contribution\n\u2020 Corresponding author\n1The University of Hong Kong\n2Fudan University\n3Zhejiang University\n4Shanghai Jiao Tong University\n5The Chinese University of Hong Kong\n6Nanyang Technological University\n7Shanghai AI Laboratory\n{zyqi, hszhao}@cs.hku.hk, wangjiaqi@pjlab.org.cn\nhttps://github.com/Qi-Zhangyang/Gemini-vs-GPT4V\nAbstract\nThe rapidly evolving sector of Multi-modal Large Language Models (MLLMs) is\nat the forefront of integrating linguistic and visual processing in artificial intelli-\ngence. This paper presents an in-depth comparative study of two pioneering models:\nGoogle\u2019s Gemini and OpenAI\u2019s GPT-4V(ision). Our study involves a multi-faceted\nevaluation of both models across key dimensions such as Vision-Language Ca-\npability, Interaction with Humans, Temporal Understanding, and assessments in\nboth Intelligence and Emotional Quotients. The core of our analysis delves into\nthe distinct visual comprehension abilities of each model. We conducted a series\nof structured experiments to evaluate their performance in various industrial ap-\nplication scenarios, offering a comprehensive perspective on their practical utility.\nWe not only involve direct performance comparisons but also include adjustments\nin prompts and scenarios to ensure a balanced and fair analysis. Our findings\nilluminate the unique strengths and niches of both models. GPT-4V distinguishes\nitself with its precision and succinctness in responses, while Gemini excels in\nproviding detailed, expansive answers accompanied by relevant imagery and links.\nThese understandings not only shed light on the comparative merits of Gemini and\nGPT-4V but also underscores the evolving landscape of multimodal foundation\nmodels, paving the way for future advancements in this area. After the comparision,\nwe attempted to achieve better results by combining the two models. Finally, We\nwould like to express our profound gratitude to the teams behind GPT-4V [1] and\nGemini [2] for their pioneering contributions to the field. Our acknowledgments\nare also extended to the comprehensive qualitative analysis presented in \u2018Dawn\u2019 by\nYang et al. [3]. This work, with its extensive collection of image samples, prompts,\nand GPT-4V-related results, provided a foundational basis for our analysis.\narXiv:2312.15011v1  [cs.CV]  22 Dec 2023\nContents\n1\nIntroduction\n7\n1.1\nMotivation and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.2\nGemini\u2019s Input Modes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n1.3\nPrompt Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.4\nSample Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n1.5\nTakeaways (Conclusion)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nImage Recognition and Understanding\n10\n2.1\nBasic object Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2\nLandmark Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3\nFood Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.4\nLogo Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.5\nAbstract Image Recognition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.6\nScene Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.7\nCounterfactual Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.8\nObject Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.9\nSpot the Difference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3\nText Recognition and Understanding in Images\n24\n3.1\nScene Text Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.2\nEqation Recognition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.3\nChart Text Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4\nImage Reasoning Abilities\n31\n4.1\nHumorous Image Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.2\nMultimodal Knowledge and Commonsense\n. . . . . . . . . . . . . . . . . . . . .\n31\n4.3\nDetective Reasoning Ability\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.4\nAssociation of Parts and Objects . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.5\nIntelligence Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.6\nEmotional Intelligence Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5\nTextual Reasoning in Images\n47\n5.1\nVisual Math Ability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n5.2\nTable & Chart Understanding and Reasoning\n. . . . . . . . . . . . . . . . . . . .\n47\n5.3\nDocument Understanding and Reasoning\n. . . . . . . . . . . . . . . . . . . . . .\n47\n6\nIntegrated Image and Text Understanding\n58\n6.1\nInterleaved Image-text Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n6.2\nText-to-Image Generation Guidance . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n2\n7\nObject Localization\n64\n7.1\nObject localization in real-world . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n7.2\nAbstract Image Localization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n8\nTemporal Video Understanding\n67\n8.1\nAction Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n8.2\nTemporal Ordering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n9\nMultilingual Capabilities\n68\n9.1\nMultilingual Image Description . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n9.2\nMultilingual Scene Text Recognition . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n10 Industry Application\n77\n10.1 Industry: Defect Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n10.2 Industry: Grocery Checkout\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n10.3 Industry: Auto Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n10.4 Industry: Customized Captioner\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n10.5 Industry: Evaluation Image Generation . . . . . . . . . . . . . . . . . . . . . . . .\n77\n10.6 Industry: Embodied Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n10.7 Industry: GUI Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n11 Integrated Use of GPT-4V and Gemini\n111\n11.1 Product Identification and Recommendation . . . . . . . . . . . . . . . . . . . . .\n111\n11.2 Multi-Image Recognition and Story Generation . . . . . . . . . . . . . . . . . . .\n111\n12 Conclusion\n114\nList of Figures\n1\nSection 2.1 Basic Object Recognition\n. . . . . . . . . . . . . . . . . . . . . . . .\n11\n2\nSection 2.2 Landmark Recognition (1) . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3\nSection 2.2 Landmark Recognition (2) . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4\nSection 2.3 Food Recognition (1) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5\nSection 2.3 Food Recognition (2) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6\nSection 2.4 Logo Recognition (1) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7\nSection 2.4 Logo Recognition (2) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n8\nSection 2.4 Logo Recognition (3) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n9\nSection 2.5 Abstract Image Recognition . . . . . . . . . . . . . . . . . . . . . . .\n19\n10\nSection 2.6 Scene Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n11\nSection 2.7 Counterfactual Examples . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n12\nSection 2.8 Object Counting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n13\nSection 2.9 Spot the Differences . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3\n14\nSection 3.1 Scene Text Recognition (1) . . . . . . . . . . . . . . . . . . . . . . . .\n25\n15\nSection 3.1 Scene Text Recognition (2) . . . . . . . . . . . . . . . . . . . . . . . .\n26\n16\nSection 3.1 Scene Text Recognition (3) . . . . . . . . . . . . . . . . . . . . . . . .\n27\n17\nSection 3.2 Equation Recognition\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n18\nSection 3.3 Chart Text Recognition (1) . . . . . . . . . . . . . . . . . . . . . . . .\n28\n19\nSection 3.3 Chart Text Recognition (2) . . . . . . . . . . . . . . . . . . . . . . . .\n29\n20\nSection 3.3 Chart Text Recognition (3) . . . . . . . . . . . . . . . . . . . . . . . .\n30\n21\nSection 4.1 Humorous Image Understanding . . . . . . . . . . . . . . . . . . . . .\n32\n22\nSection 4.2 Multimodal Knowledge and Commonsense (1) . . . . . . . . . . . . .\n33\n23\nSection 4.2 Multimodal Knowledge and Commonsense (2) . . . . . . . . . . . . .\n34\n24\nSection 4.2 Multimodal Knowledge and Commonsense (3) . . . . . . . . . . . . .\n35\n25\nSection 4.3 Detective Reasoning Ability . . . . . . . . . . . . . . . . . . . . . . .\n36\n26\nSection 4.4 Association of Parts and Objects . . . . . . . . . . . . . . . . . . . . .\n37\n27\nSection 4.5 Intelligence Tests (1) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n28\nSection 4.5 Intelligence Tests (2) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n29\nSection 4.5 Intelligence Tests (3) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n30\nSection 4.5 Intelligence Tests (4) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n31\nSection 4.6 Emotional Intelligence Tests (1) . . . . . . . . . . . . . . . . . . . . .\n42\n32\nSection 4.6 Emotional Intelligence Tests (2) . . . . . . . . . . . . . . . . . . . . .\n43\n33\nSection 4.6 Emotional Intelligence Tests (3) . . . . . . . . . . . . . . . . . . . . .\n44\n34\nSection 4.6 Emotional Intelligence Tests (4) . . . . . . . . . . . . . . . . . . . . .\n45\n35\nSection 4.6 Emotional Intelligence Tests (5) . . . . . . . . . . . . . . . . . . . . .\n46\n36\nSection 5.1 Visual Math Ability\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n37\nSection 5.2 Table & Chart Understanding and Reasoning (1) . . . . . . . . . . . .\n49\n38\nSection 5.2 Table & Chart Understanding and Reasoning (2) . . . . . . . . . . . .\n50\n39\nSection 5.2 Table & Chart Understanding and Reasoning (3) . . . . . . . . . . . .\n51\n40\nSection 5.2 Table & Chart Understanding and Reasoning (4) . . . . . . . . . . . .\n52\n41\nSection 5.2 Table & Chart Understanding and Reasoning (5) . . . . . . . . . . . .\n53\n42\nSection 5.3 Document Understanding and Reasoning (1)\n. . . . . . . . . . . . . .\n54\n43\nSection 5.3 Document Understanding and Reasoning (2)\n. . . . . . . . . . . . . .\n55\n44\nSection 5.3 Document Understanding and Reasoning (3)\n. . . . . . . . . . . . . .\n56\n45\nSection 5.3 Document Understanding and Reasoning (4)\n. . . . . . . . . . . . . .\n57\n46\nSection 6.1 Interleaved Image-text Inputs (1) . . . . . . . . . . . . . . . . . . . . .\n59\n47\nSection 6.1 Interleaved Image-text Inputs (2) . . . . . . . . . . . . . . . . . . . . .\n60\n48\nSection 6.2 Text-to-Image Generation Guidance (1) . . . . . . . . . . . . . . . . .\n61\n49\nSection 6.2 Text-to-Image Generation Guidance (2) . . . . . . . . . . . . . . . . .\n62\n50\nSection 6.2 Text-to-Image Generation Guidance (3) . . . . . . . . . . . . . . . . .\n63\n51\nSection 7.1 Object localization in real-world (2) . . . . . . . . . . . . . . . . . . .\n64\n52\nSection 7.1 Object localization in real-world (3) . . . . . . . . . . . . . . . . . . .\n65\n4\n53\nSection 7.2 Abstract Image Localization . . . . . . . . . . . . . . . . . . . . . . .\n66\n54\nSection 8.1 Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n55\nSection 8.2 Temporal Ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n56\nSection 9.1 Multilingual Image Description (1)\n. . . . . . . . . . . . . . . . . . .\n69\n57\nSection 9.1 Multilingual Image Description (2)\n. . . . . . . . . . . . . . . . . . .\n70\n58\nSection 9.1 Multilingual Image Description (3)\n. . . . . . . . . . . . . . . . . . .\n71\n59\nSection 9.1 Multilingual Image Description (4)\n. . . . . . . . . . . . . . . . . . .\n72\n60\nSection 9.2 Multilingual Scene Text Recognition (1) . . . . . . . . . . . . . . . . .\n73\n61\nSection 9.2 Multilingual Scene Text Recognition (2) . . . . . . . . . . . . . . . . .\n74\n62\nSection 9.2 Multilingual Scene Text Recognition (3) . . . . . . . . . . . . . . . . .\n75\n63\nSection 9.2 Multilingual Scene Text Recognition (4) . . . . . . . . . . . . . . . . .\n76\n64\nSection 10.1 Industry: Defect Detection (1)\n. . . . . . . . . . . . . . . . . . . . .\n79\n65\nSection 10.1 Industry: Defect Detection (2)\n. . . . . . . . . . . . . . . . . . . . .\n80\n66\nSection 10.1 Industry: Defect Detection (3)\n. . . . . . . . . . . . . . . . . . . . .\n81\n67\nSection 10.2 Industry: Grocery Checkout\n. . . . . . . . . . . . . . . . . . . . . .\n82\n68\nSection 10.3 Industry: Auto Insurance (1) . . . . . . . . . . . . . . . . . . . . . .\n83\n69\nSection 10.3 Industry: Auto Insurance (2) . . . . . . . . . . . . . . . . . . . . . .\n84\n70\nSection 10.3 Industry: Auto Insurance (3) . . . . . . . . . . . . . . . . . . . . . .\n85\n71\nSection 10.4 Industry: Customized Captioner\n. . . . . . . . . . . . . . . . . . . .\n86\n72\nSection 10.5 Industry: Evaluation Image Generation (1) . . . . . . . . . . . . . . .\n87\n73\nSection 10.5 Industry: Evaluation Image Generation (2) . . . . . . . . . . . . . . .\n88\n74\nSection 10.5 Industry: Evaluation Image Generation (3) . . . . . . . . . . . . . . .\n89\n75\nSection 10.6 Industry: Embodied Agent (1)\n. . . . . . . . . . . . . . . . . . . . .\n90\n76\nSection 10.6 Industry: Embodied Agent (2)\n. . . . . . . . . . . . . . . . . . . . .\n91\n77\nSection 10.6 Industry: Embodied Agent (3)\n. . . . . . . . . . . . . . . . . . . . .\n92\n78\nSection 10.6 Industry: Embodied Agent (4)\n. . . . . . . . . . . . . . . . . . . . .\n93\n79\nSection 10.7 Industry: GUI Navigation (1) . . . . . . . . . . . . . . . . . . . . . .\n94\n80\nSection 10.7 Industry: GUI Navigation (2) . . . . . . . . . . . . . . . . . . . . . .\n95\n81\nSection 10.7 Industry: GUI Navigation (3) . . . . . . . . . . . . . . . . . . . . . .\n96\n82\nSection 10.7 Industry: GUI Navigation (4) . . . . . . . . . . . . . . . . . . . . . .\n97\n83\nSection 10.7 Industry: GUI Navigation (5) . . . . . . . . . . . . . . . . . . . . . .\n98\n84\nSection 10.7 Industry: GUI Navigation (6) . . . . . . . . . . . . . . . . . . . . . .\n99\n85\nSection 10.7 Industry: GUI Navigation (7) . . . . . . . . . . . . . . . . . . . . . .\n100\n86\nSection 10.7 Industry: GUI Navigation (8) . . . . . . . . . . . . . . . . . . . . . .\n101\n87\nSection 10.7 Industry: GUI Navigation (9) . . . . . . . . . . . . . . . . . . . . . .\n102\n88\nSection 10.7 Industry: GUI Navigation (10) . . . . . . . . . . . . . . . . . . . . .\n103\n89\nSection 10.7 Industry: GUI Navigation (11) . . . . . . . . . . . . . . . . . . . . .\n104\n90\nSection 10.7 Industry: GUI Navigation (12) . . . . . . . . . . . . . . . . . . . . .\n105\n91\nSection 10.7 Industry: GUI Navigation (13) . . . . . . . . . . . . . . . . . . . . .\n106\n5\n92\nSection 10.7 Industry: GUI Navigation (14) . . . . . . . . . . . . . . . . . . . . .\n107\n93\nSection 10.7 Industry: GUI Navigation (15) . . . . . . . . . . . . . . . . . . . . .\n108\n94\nSection 10.7 Industry: GUI Navigation (16) . . . . . . . . . . . . . . . . . . . . .\n109\n95\nSection 10.7 Industry: GUI Navigation (17) . . . . . . . . . . . . . . . . . . . . .\n110\n96\nSection 11.1 Integrated Use: Product Identification and Recommendation\n. . . . .\n112\n97\nSection 11.2 Integrated Use: Multi-image Recognition and Story Generation . . . .\n113\n6\n1\nIntroduction\n1.1\nMotivation and Overview\nThe evolution of artificial intelligence has seen the significant rise of Large Language Models\n(LLMs) [4, 5, 6, 7, 8, 9], which have revolutionized the way machines process and understand textual\ndata. Building upon this, the advent of Multi-modal Large Language Models (MLLMs) [10, 11,\n12, 13, 14, 15, 16, 17, 18, 19, 20, 21] marks a pivotal advancement in AI, extending capabilities to\ncomprehend and interact with not just text, but also images, 3D models [22], and video content [23].\nAmong these modalities, the integration of text and image has emerged as particularly powerful,\nlargely due to the rich and informative nature of text-image pairs. The following, unless otherwise\nspecified, all refer to the MLLMs in the context of images.\nThe landscape of Multi-modal Large Language Models (MLLMs) is currently divided into two broad\ncategories: closed-source models with their proprietary advancements, and open-source deployable\nmodels like LLaVA [16], MiniGPT-4 [15]and InstructBLIP [17] which are more accessible but often\nless advanced. Among these, the state-of-the-art in open-source models is GPT-4V [24] from OpenAI,\nwhich has established a dominant position in terms of versatility and general applicability. Recently,\nGoogle has introduced their own large model, Gemini [25], which also boasts high generalization\ncapabilities. This release poses a significant challenge to GPT-4V\u2019s leading status. Gemini\u2019s entry\ninto the arena of MLLMs brings a new dimension to the field, potentially reshaping the landscape of\nwhat is achievable with open-source AI technology, particularly in terms of multimodal understanding\nand application. Therefore, our paper undertakes a comprehensive comparison of these two models\nacross multiple dimensions and domains.\nIt should be noted that the image samples, prompts, and results related to GPT-4V used in our\npaper are referenced from the study \"The Dawn of LMMs: Preliminary Explorations with GPT-\n4V(ision). [26]\" Our work can be seen as a continuation and expansion of this previous research.\nSince GPT-4V has already been extensively discussed in the original paper, our focus in this report\nwill primarily be on emphasizing and exploring the unique characteristics and capabilities of Gemini.\nSec. 2 to Sec. 6 divide the multimodal evaluation into five aspects. The first level involves basic\nrecognition of images and the text within them. The second level goes beyond recognition to\nrequire further inference and reasoning. The third level encompasses multimodal comprehension and\ninference involving multiple images. We have divided them into the following five sections.\n\u2022 Image Recognition and Understanding: Sec. 2 addresses the fundamental recognition\nand comprehension of image content without involving further inference, including tasks\nsuch as identifying landmarks, foods, logos, abstract images, autonomous driving scenes,\nmisinformation detection, spotting differences, and object counting.\n\u2022 Text Recognition and Understanding in Images: Sec. 3 concentrates on text recognition\n(including OCR) within images, such as scene text, mathematical formulas, and chart &\ntable text recognition. Similarly, no further inference of text content is performed here.\n\u2022 Image Inference Abilities: Beyond basic image recognition, Sec. 4 involves more advanced\nreasoning. This includes understanding humor and scientific concepts, as well as logical\nreasoning abilities like detective work, image combinations, look for patterns in intelligence\ntests (IQ Tests), and emotional understanding and expression (EQ Tests).\n\u2022 Textual Inference in Images: Building on the text recognition, Sec. 5 involves further\nreasoning beyond text recognition, including mathematical problem-solving, chart & table\ninformation reasoning, and document comprehension like paper, report and Graphic Design.\n\u2022 Integrated Image and Text Understanding: Sec. 6 evaluates the collective understanding\nand reasoning abilities involving both image and text. For instance, tasks include settling\nitems from a supermarket shopping cart, as well as guiding and modifying image generation.\nSec. 7 to Sec. 9 evaluate performance in three specialized tasks, namely, object localization, temporal\nunderstanding, and multilingual comprehension.\n\u2022 Object Localization: Sec. 7 highlights object localization capabilities, tasking the models\nwith providing relative coordinates for specified objects. This includes a focus on outdoor\nobjects like cars in parking lots and abstract image localization.\n7\n\u2022 Temporal Video Understanding:\ncrefSec.8 Temporal Video Understanding evaluates the models\u2019 comprehension of temporal-\nity using key frames. This section includes two tasks: one involving the understanding of\nvideo sequences and the other focusing on sorting key frames.\n\u2022 Multilingual Capabilities: Sec. 9 thoroughly assesses capabilities in recognizing, under-\nstanding, and producing content in multiple languages. This includes the ability to recognize\nnon-English content within images and express information in other languages.\nSec. 10 presents various application scenarios for multimodal large models. We aim to showcase more\npossibilities to the industry, providing innovative ideas. There is potential to customize multimodal\nlarge models for unique domains. Here, we demonstrate seven sub-domains:\n\u2022 Industry: Defect Detection: This task involves the detection of defects in products on\nindustrial assembly lines, including textiles, metal components, pharmaceuticals and more.\n\u2022 Industry: Grocery Checkout: This refers to an autonomous checkout system in supermar-\nkets, aimed at identifying all items in a shopping cart for billing. The goal is to achieve\ncomprehensive recognition of all items within the shopping cart.\n\u2022 Industry: Auto Insurance: This task involves evaluating the extent of damage in car acci-\ndents and providing approximate repair costs, as well as offering repair recommendations.\n\u2022 Industry: Customized Captioner: The aim is to identify the relative positions of various\nobjects within a scene, with object names provided as condition and prompts in advance.\n\u2022 Industry: Evaluation Image Generation: This involves assessing the alignment between\ngenerated images and given text prompts, evaluating the quality of the generation model.\n\u2022 Industry: Embodied Agent: This application involves deploying the model in embodied\nintelligence and smart home systems, offering thoughts and decisions for indoor scenarios.\n\u2022 Industry: GUI Navigation: This task focuses on guiding users through PC/Mobile GUI\ninterfaces, assisting with information reception, online searches, and shopping tasks.\nFinally, in Sec. 11, we explore how to combine both SOTA models to leverage their respective\nstrengths and mitigate their weaknesses. In summary, GPT-4V provides more accurate results, while\nGemini excels in providing more detailed responses, along with image and link outputs.\n1.2\nGemini\u2019s Input Modes\nOur goal is to clarify the input modality of Gemini. GPT-4V\u2019s input modality supports the continuous\ningestion of multiple images as context, thereby possessing enhanced memory capabilities. However,\nfor Gemini, its unique attributes are manifested in several aspects, as follows:\n\u2022 Single Image Input: Gemini is limited to inputting a single image at a time. Additionally, it\ncannot process independent images; instead, it requires accompanying textual instructions.\n\u2022 Limited Memory Capacity: Unlike GPT-4V, Gemini\u2019s multimodal module lacks the ability\nto retain memory of past image inputs and outputs. Therefore, when dealing with multiple\nimages, our approach requires combining all the images into a single image input. This\nintegrated input mode will be used unless explicitly stated otherwise.\n\u2022 Sensitive Information Masking: Gemini exhibits some degree of obfuscation when process-\ning images containing explicit facial or medical information, making it unable to recognize\nthese images. This may impose certain limitations on its generalization ability.\n\u2022 Image and Link Output: Unlike GPT-4V, which is limited to generating textual outputs,\nGemini has the ability to create images related to the content and provide corresponding\nlinks. This establishes a higher level of association similar to search engine functionality.\n\u2022 Video Input and Comprehension: Gemini demonstrates the capability to understand videos\nand requires a YouTube link as a video input. It\u2019s important to note that it can effectively\nprocess videos accompanied by accurate subtitle files. However, its comprehension ability\nmay be limited when dealing with single, simple, and information-scarce videos.\n8\n1.3\nPrompt Techniques\nPrompt Engineering holds significant importance for both unimodal language models [27, 28, 29, 4,\n30, 31]and multimodal large-scale models [32, 33, 17]. The prompt design under consideration is\ntailored for GPT-4V, and direct input into Gemini may yield unsatisfactory responses. In such cases,\nadjustments to Gemini\u2019s prompt are made to align with the input requirements of its architecture.\n1.4\nSample Collection\nAll our data is sourced from \"The Dawn of LMMs:\nPreliminary Explorations with GPT-\n4V(ision)\" [26] (except for the images in Section 11, which are sourced from the internet). We\nhave utilized their images, GPT-4V\u2019s prompts, and corresponding results. Our work can be seen as\na continuation of theirs. Our dataset is diverse and maintains privacy protections. We extend our\ngratitude to the authors of that work. The raw data of the images is available on the project page.\n1.5\nTakeaways (Conclusion)\nWe have conducted a comprehensive comparison of GPT-4V and Gemini\u2019s multimodal understanding\nand reasoning abilities across multiple aspects and have reached the following conclusions:\n\u2022 Image Recognition and Understanding: In basic image recognition tasks, both models\nshow comparable performance and are capable of completing the tasks effectively.\n\u2022 Text Recognition and Understanding in Images: Both models excel in extracting and\nrecognizing text from images. However, improvements are needed in complex formula and\ndashboard recognition. Gemini performs better in reading table information.\n\u2022 Image Inference Abilities: In image reasoning, both models excel in common-sense\nunderstanding. Gemini slightly lags in look-for-pattern compared (IQ Tests) to GPT-4V. In\nEQ tests, both understand emotions and have aesthetic judgment.\n\u2022 Textual Inference in Images: In the field of text reasoning, Gemini shows relatively lower\nperformance levels when dealing with complex table-based reasoning and mathematical\nproblem-solving tasks. Furthermore, Gemini tends to offer more detailed outputs.\n\u2022 Integrated Image and Text Understanding: In tasks involving complex text and images,\nGemini falls behind GPT-4V due to its inability to input multiple images at once, although it\nperforms similarly to GPT-4V in textual reasoning with single images.\n\u2022 Object Localization: Both models perform similarly in real-world object localization, with\nGemini being slightly less adept at abstract image (tangram) localization.\n\u2022 Temporal Video Understanding: In understanding temporality, Gemini\u2019s single-image\ninput mode falls short compared to GPT-4V, especially in the comprehension of sequence.\n\u2022 Multilingual Capabilities: Both models exhibit good multilingual recognition, understand-\ning, and output capabilities, effectively completing the multilingual tasks.\nIn industrial applications, Gemini is outperformed by GPT-4V in Embodied Agent and GUI Naviga-\ntion, which is also attributed to Gemini\u2019s single-image, non-memory input mode. Combining two\nlarge models can leverage their respective strengths.\nOverall, both Gemini and GPT-4V are powerful and impressive multimodal large models. In terms\nof overall performance, GPT-4V is slightly stronger than Gemini Pro. This aligns with the results\nreported by Gemini. We look forward to the release of Gemini Ultra and GPT-4.5, which are expected\nto bring more possibilities to the field of visual multimodal applications.\n9\n2\nImage Recognition and Understanding\nIn this section, we primarily discuss the fundamental understanding of images. This task is the most\nbasic, requiring only the identification of objects in an image and their characteristics. It does not\ninvolve text-related tasks or further inference. Sec. 2.1, Sec. 2.2, Sec. 2.3 and Sec. 2.4 focus on\nthe recognition of basic objects, landmarks, food, logos, and abstract images. Sec. 2.6 addresses\nscene understanding in outdoor autonomous driving scenarios. Sec. 2.7 tests the model\u2019s ability\nto recognize fabricated objects created using text, gauging its discernment of real versus fictitious\nelements. Sec. 2.8 assesses the model\u2019s object counting capabilities, while the final Sec. 2.9 explores\nthe model\u2019s proficiency in spotting differences, examining its ability to identify subtle details.\n2.1\nBasic object Recognition\nFig. 1 refers to the recognition of the entire image and corresponding description, using a fixed number\nof words (three, six and nine words) or an overall description starting with fixed letters (B/D/T in\nthis case). After adjusting the prompts, both GPT-4V and Gemini are able to provide satisfactory\nresponses. indicating the ability to comprehend images and respond according to instructions.\n2.2\nLandmark Recognition\nFig. 2 and Fig. 3 together showcase four famous landmarks, namely Kinkaku-ji Temple, Lombard\nStreet, Manhattan Bridge, and Trump Tower. Here, both GPT-4V and Gemini perform well, with\nGemini being able to provide additional related introductions to the scenery. Even for the interior of\nTrump Tower, both models are able to successfully identify it. Gemini can displays other images and\nlinks related to the landmark.\n2.3\nFood Recognition\nFig. 4 and Fig. 5 pertain to the identification of food, showcasing Chinese cuisine, Japanese cuisine,\nWestern cuisine, and specialties from minority tribes in North America, demonstrating the MLLMs\u2019\nknowledge range from multiple dimensions, where both models perform well. Similarly, Gemini\ntends to provide more detailed descriptions and links, such as links to recipes.\n2.4\nLogo Recognition\nFig. 6, Fig. 7 and Fig. 8 are about logo recognition, including the logo itself and recognition of logos\nin-the-wild scenarios. Both models generally do not make significant errors, with Gemini providing\nmore detailed introductions. Here we can observe that in response to simple prompts, GPT-4V also\ntends to provide concise answers, only giving detailed responses when specifically requested to\n\u2019in detail\u2019. Furthermore, in \u2019in-the-wild scenarios,\u2019 GPT-4V may excessively focus on objects and\nprovide incorrect answers related to objects, such as mistaking a can for a bottle or inventing the\npresence of a straw in a coffee cup.\n2.5\nAbstract Image Recognition\nFig. 9 is about the recognition of abstract images, specifically recognizing various shapes composed\nof tangram pieces. Overall, GPT-4V tends to provide more accurate responses, largely because\nGemini struggles with recognizing large images composed of multiple smaller images. This indicates\nGemini\u2019s limited ability to recognize more abstract objects. Secondly, it\u2019s possible that combining\nmultiple images into a single input image may have resulted in a decrease in Gemini\u2019s performance.\n2.6\nScene Understanding\nFig. 10 presents an outdoor autonomous driving scene. Cars driving on the road can see pedestrians,\ntraffic signs, and other vehicles. Both models show good capabilities here. However, GPT-4V\u2019s\nresponses are more accurate, while Gemini\u2019s responses are more detailed. Here, we observe that\nGemini has some discrepancies in understanding weather conditions, but overall, the performance of\nboth models is quite comparable.\n10\n2.7\nCounterfactual Examples\nFig. 11 shows an example where we present an image and fabricate an object that doesn\u2019t exist in\nthe picture, then ask about the location of this fabricated object. This is to prevent the models from\nmaking purely imaginative connections, rather than truly understanding the image. The results show\nthat both models can identify that the fabricated object does not exist.\n2.8\nObject Counting\nFig. 12 demonstrates the models\u2019 counting abilities. Here, we present three examples, all of which\ninvolve counting the quantity of the same type of fruit in the image. In this context, we observe that\nGemini\u2019s counting ability is not as strong as GPT-4V. While GPT-4V can arrive at the correct answers\nthrough memorization and guidance from prompts, Gemini, due to its relatively weaker multimodal\nmemory performance, cannot be guided to the correct answers.\n2.9\nSpot the Difference\nFig. 13 demonstrates the models\u2019 ability to spot differences. Given two similar stick figure drawings\nof dinosaurs, the task is to find differences. Neither model was able to identify all the differences,\nindicating that there is room for improvement in this area.\nPrompt: Describe the image with a sentence \nconsisting of three/six/nine words.\nGPT-4V\uff1a\nThree: Seaside dining table Six: Dining table overlooking ocean \nwith beer Nine: Outdoor restaurant table with ocean view and a beer.\nSec. 2.1 Basic object Recognition\nThree words: Beer bottle on table. Six words: Magna beer bottle on table. Nine words: Cold Magna \nbeer bottle on table.\nGemini Pro: \nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nGPT-4V\uff1a\n'B': Beachside bar boasts beautiful blue backdrop. 'D': Dining deck displays delightful drinks, distant docks. 'T': \nTabletop terrace teeming with tropical tranquility.\nBrown bottle tall table.\nGemini Pro: \nPrompt: Write a sentence for the image consisting only of words starting with the letter 'B'/ 'D'/ 'T'.\nPrompt: Write three sentence for the image consisting only of words starting with the letter 'B'/ 'D'/ 'T'. \nThis means three sentence, and their first letter should be B, D, T respectively.\nB: Brown bottle of beer sits on table. D: Dark wooden table bears bottle of beer. T: Tabletop displays tasty \nbottle of beer.\nGemini Pro: \nFigure 1: Basic object recognition. GPT-4V and Gemini are both capable of accurately recognizing\nbasic objects and providing textual descriptions as required.\n11\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 2.2 Landmark Recognition\nLandmark Recognition and Description\nPrompt: Describe the landmark in the image.\nThe landmark in the image is the Space Needle, an \nobservation tower located in Seattle, Washington. It \nstands 605 feet tall and offers panoramic views of the \ncity and surrounding areas. The tower was built for \nthe 1962 World\u2019s Fair and has since become a \nsymbol of Seattle and a popular tourist attraction. The \ntower has a futuristic design with a large circular \nobservation deck at the top and a restaurant that \nrotates 360 degrees. The base of the tower is \nsurrounded by a park with trees and green spaces.\nGPT-4V\uff1a\nGemini Pro: \nThe landmark in the image is the Golden Pavilion, also known as Kinkaku-ji, in Kyoto, Japan. It is a Zen \nBuddhist temple that was built in the 14th century. The pavilion is covered in gold leaf, which gives it its distinctive \nappearance. The Golden Pavilion is a popular tourist destination and is considered to be one of the most iconic \nlandmarks in Japan. \nThe Golden Pavilion was originally built as a retirement villa for the shogun Ashikaga Yoshimitsu. After his death, \nthe villa was converted into a Zen Buddhist temple. The Golden Pavilion was burned down several times over the \ncenturies, but it was always rebuilt. The current Golden Pavilion was rebuilt in 1955.\nThe Golden Pavilion is a UNESCO World Heritage Site and is one of the most popular tourist destinations in \nKyoto. Visitors can walk around the garden and admire the Golden Pavilion from the outside. However, visitors \nare not allowed to enter the Golden Pavilion.\nThis image shows the Kinkaku-ji Temple, also known as the \nGolden Pavilion, located in Kyoto, Japan. The temple is a \nthree-story Buddhist temple that is covered in gold leaf and sits \non a pond, creating a stunning reflection on the water. The \nsurrounding landscape is filled with trees and rocks, creating a \npeaceful and serene atmosphere. It is a popular tourist attraction \nand a designated National Special Historic Site.\nThe Golden Pavilion is a three-story building with a unique architectural style. The \nfirst floor is made of wood and is in the traditional Japanese style. The second floor is \ncovered in gold leaf and has a Chinese-style roof. The third floor is also covered in \ngold leaf and has a Buddhist-style roof. The Golden Pavilion is surrounded by a \nbeautiful garden and a lake.\nhttps://en.wikipedia.org/wiki/Kinkaku-ji\nPrompt: Describe the landmark in the image.\nThe landmark in the image is the Space Needle, an \nobservation tower located in Seattle, Washington. It \nstands 605 feet tall and offers panoramic views of the \ncity and surrounding areas. The tower was built for \nthe 1962 World\u2019s Fair and has since become a \nsymbol of Seattle and a popular tourist attraction. The \ntower has a futuristic design with a large circular \nobservation deck at the top and a restaurant that \nrotates 360 degrees. The base of the tower is \nsurrounded by a park with trees and green spaces.\nGPT-4V\uff1a\nThe landmark in the image is the famous Lombard Street in \nSan Francisco, California, known for its steep hairpin turns. \nThe street is lined with colorful flowers and shrubbery, and there \nare houses and buildings on both sides. In the background, you \ncan see the city skyline and the Coit Tower on Telegraph Hill. This \nstreet is a popular tourist attraction, and it is often considered one \nof the most crooked streets in the world.\nThe landmark in the image is the steep streets of San Francisco, California, USA.\nSan Francisco is known for its steep hills and narrow streets, which were created when the city was built on top of \nsand dunes. Some of the most famous steep streets in San Francisco include Lombard Street, Filbert Street, and \nRussian Hill.\nThe image shows a view of San Francisco from the top of a hill. The city skyline is visible in the distance, with the \nGolden Gate Bridge and the Bay Bridge in the background. The streets in the foreground are lined with Victorian \nhouses and apartments.\nSan Francisco's steep streets are a popular tourist attraction. Visitors can take a cable car ride up and down the \nhills, or simply walk around and enjoy the views. The city's steep streets are also a popular filming location for \nmovies and TV shows. \nGemini Pro: \nThe landmark in the image is the steep streets of San Francisco, California, USA. San Francisco is \nknown for its steep hills and narrow streets, which were created when the city was built on top of sand dunes. \nSome of the most famous steep streets in San Francisco include Lombard Street, Filbert Street, and \nRussian Hill.\nThe image shows a view of San Francisco from the top of a hill. The city skyline is visible in the distance, with the \nGolden Gate Bridge and the Bay Bridge in the background. The streets in the foreground are lined with Victorian \nhouses and apartments.\nSan Francisco's steep streets are a popular tourist attraction. Visitors can take a cable car ride up and down the \nhills, or simply walk around and enjoy the views. The city's steep streets are also a popular filming location for \nmovies and TV shows. \nFigure 2: Landmark recognition and description (1/2). While both models are proficient in\naccurately identifying landmarks, GPT-4V generates concise descriptions, whereas Gemini tends to\nprovide more detailed narratives even indoors at tourist attractions..\n12\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff2.2\uffffLandmark\uffffRecognition\nLandmark\uffffRecognition\uffffand\uffffDescription\nPrompt: Describe the landmark in the image.\nThe landmark in the image is the Space Needle, an \nobservation tower located in Seattle, Washington. It \nstands 605 feet tall and offers panoramic views of the \ncity and surrounding areas. The tower was built for \nthe 1962 World\u2019s Fair and has since become a \nsymbol of Seattle and a popular tourist attraction. The \ntower has a futuristic design with a large circular \nobservation deck at the top and a restaurant that \nrotates 360 degrees. The base of the tower is \nsurrounded by a park with trees and green spaces.\nGPT-4V\uff1a\nThe landmark in the image is the Manhattan Bridge, as seen \nfrom the DUMBO neighborhood in Brooklyn, New York. The \nbridge, which spans the East River, connects lower Manhattan \nwith Brooklyn. It is a suspension bridge with two large towers and \nwas completed in 1909. In the image, the bridge is framed by two \nbrick buildings, creating a unique view of the landmark.\nPrompt: Describe the \nlandmark in the image.\nGPT-4V\uff1a\nThe landmark in the image is the interior of the Trump Tower in New York \nCity. The focal point is the Trump Grill, which is a restaurant located on the \nlower level of the tower. The interior is decorated with red and gold accents, \nmarble walls and columns, and a festive holiday display with a Christmas tree \nand poinsettias. The space is opulent and luxurious, typical of the Trump brand.\nGemini Pro: \nThis is the Trump Tower in New York City. It is a 58-story \nskyscraper that was completed in 1983. The tower is home to a \nvariety of businesses, including the Trump Grill, the Trump Bar, \nand the Trump International Hotel and Tower. The lobby of the \nTrump Tower is known for its luxurious d\u00e9cor, which includes \nmarble floors, crystal chandeliers, and gold-plated accents. \nDuring the holidays, the lobby is decorated with a towering \nChristmas tree, wreaths, and other festive decorations.\nGemini Pro: \nThe landmark in the image is the Brooklyn Bridge, a hybrid cable-stayed/suspension bridge in New York \nCity and is one of the oldest bridges of either type in the United States. Completed in 1883, it connects the \nboroughs of Manhattan and Brooklyn by spanning the East River. It has a main span of 1,595.5 feet (486.3 m) \nand a height of 276 feet (84 m).\nThe Brooklyn Bridge is one of the most iconic landmarks in New York City and the world. \nIt has been featured in countless films, television shows, and songs. It is a popular tourist \ndestination and is also used by commuters and cyclists. The bridge is also a National \nHistoric Landmark and a New York City landmark. It is one of the most photographed \nbridges in the world.\nhttps://en.wikipedia.org/wiki/Brooklyn_Bridge\nhttps://hmrdesigns.com/holiday-inn/\nFigure 3: Landmark recognition and description (2/2). Both models excel at accurately identifying\nlandmarks, producing vivid and detailed descriptions. Even for the interior of Trump Tower, both\nmodels are able to successfully identify it.\n13\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 2.3 Food Recognition\nFood Recognition and Description\nPrompt: Describe the name of the dish.\nGPT-4V\uff1a\nGemini Pro: \nThe dish in the image is mapo tofu. It is a popular Sichuan dish.\nThe dish in the image is a bowl of Mapo Tofu, a popular Chinese dish \nfrom the Sichuan province. It consists of soft tofu cubes in a rich and spicy \nsauce made from chili peppers, fermented bean paste, and Sichuan \npeppercorns. The dish is garnished with thin slices of green onions.\nMapo tofu is a relatively easy dish to make, but it can be time-consuming to prepare all of the ingredients. The \ntofu is first braised in a flavorful sauce made with the black bean paste, doubanjiang, and chili bean sauce. \nThe ground meat is then cooked and added to the sauce along with the tofu. The dish is simmered until the \ntofu is heated through and the sauce has thickened. Mapo tofu is a delicious and satisfying dish that is perfect \nfor a weeknight meal. It is also a good source of protein and fiber.\nHere is a more specific description of the dish in the image:\nThe tofu is cut into small cubes. The tofu is braised in a dark brown sauce. The sauce is thick and glossy.\nThe tofu is topped with ground meat and green onions. I hope this helps!\nground beef or pork, fermented black bean paste, doubanjiang, and chili bean sauce. The dish is often served \nwith rice and garnished with green onions.\nPrompt: Describe the name of the dish.\nGPT-4V\uff1a\nUnagi Donburi is the name of the dish that is originated from Japan. \nIn Japanese cuisine, it is a type of rice bowl dish typically served in a \ndeep bowl. The word \u201cUnagi\u201d means freshwater eel in Japanese and \n\u201cdonburi\u201d refers to a large bowl served with rice and other toppings. \nUnagi Donburi is a dish made with grilled eel and a type of sweet and \nsavory sauce called \u201cunagi sauce\u201d over white rice. It is often served \nwith pickled vegetables, sesame seeds, green onions, and seaweed.\nThe dish in the image is roasted eel on rice, also known as unagi. It is a popular \nJapanese dish made with eel that has been roasted over charcoal and then glazed \nwith a sweet and savory sauce. Unagi is often served on top of rice, but it can also be \nserved on its own or as part of a larger meal.\nUnagi is a good source of protein and omega-3 fatty acids, which are beneficial for \nheart health. It is also a good source of vitamin A, which is important for vision. Unagi \nis typically a seasonal dish, and it is most popular in the summer and fall.\nGemini Pro: \nThe eel in the image is cooked until it is golden brown and crispy on the outside. The meat is tender and juicy, \nand it is infused with the flavor of the sweet and savory glaze. The eel is served on top of a bed of rice, and it \nis garnished with chopped green onions. Unagi is a delicious and nutritious dish that is popular in Japan and \naround the world. It is a perfect meal for a special occasion or for a simple weeknight dinner.\nhttps://www.justonecookbook.com/unagi-don-unadon/ \nFigure 4: Food recognition and description (1/2). Both models exhibit the ability to recognize a\nbroad spectrum of dishes, extending their identification abilities to minute details like ingredients,\ngarnishes, and cooking techniques depicted within an image of a dish.\n14\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff2.3\uffffFood\uffffRecognition\nFood\uffffRecognition\uffffand\uffffDescription\nPrompt: Describe the name of the dish.\nGPT-4V\uff1a\nThis dish is called a Cajun Sampler. It is a sampler platter that consists of \nfour small bowls of traditional Cajun dishes - gumbo, jambalaya, red beans \nand rice, and crawfish \u00e9touff\u00e9e. The bowls are served with a side of sliced \nbread for dipping. Each bowl is filled with white rice and topped with green \nonions. It is a popular dish in Louisiana and a great way to try a variety of \nCajun flavors.\nPrompt: Describe the name of the dish.\nGPT-4V\uff1a\nThis dish is called Braised Short Rib with Chimichurri. It features a \nsucculent short rib that has been slow cooked until tender and topped \nwith a flavorful chimichurri sauce. It is served alongside roasted \nvegetables for a complete meal.\nGemini Pro: \nThe dish in the image is a steak with potatoes and pesto. The steak is cooked to a medium-rare doneness \nand is drizzled with a pesto sauce. The pesto sauce is made with basil, pine nuts, Parmesan cheese, \ngarlic, and olive oil. The potatoes are roasted and are perfectly crispy on the outside and fluffy on the inside\nThe dish in the image is gumbo. It is a thick, savory soup or stew that originated in Louisiana, USA. \nGumbo is typically made with a roux (a mixture of fat and flour), okra or fil\u00e9 powder, and meat or seafood. It is \noften served with rice. Gumbo is a popular dish in Creole and Cajun cuisine, and it is also enjoyed by \npeople all over the world. Gumbo is typically served with rice, but it can also be enjoyed with other sides, such \nas crusty bread or cornbread. Fun fact: Gumbo is the official state cuisine of Louisiana.\nGemini Pro: \nFigure 5: Food recognition and description (2/2). Both models exhibit remarkable capacities in\nidentifying a vast array of culinary dishes, further extending their recognition abilities to specific\nelements like ingredients, garnishes, and cooking techniques represented within a dish\u2019s image.\n15\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff2.4\uffffLogo\uffffRecognition\nLogo\uffffRecognition\uffffand\uffffDescription\nPrompt: Describe the name.\nGPT-4V\uff1a\nThe image is composed of three different icons arranged horizontally, with a thin vertical line separating each \nicon. On the left is an icon resembling the letter \u201cO\u201d with a blue horizontal line running through the middle. This \nis the logo of the car brand BMW. In the middle is an icon composed of two overlapping triangles, with the \npoints facing upwards. This is the logo of the car brand Infiniti. On the right is an icon composed of a shield-\nlike shape with three arrows pointing upwards. This is the logo of the car brand Maserati.\nThe image shows a group of three logos on a white background. The logos are for BMW, Infiniti, and \nMaserati, all of which are luxury car brands.\nThe BMW logo is a round emblem with a quartered design. The top left and bottom right quadrants are \nblue, while the top right and bottom left quadrants are white. The quartered design is a reference to the \nBavarian flag, where BMW is headquartered.\nThe Infiniti logo is a stylized image of a mountain range. The mountain range represents the challenges \nthat Infiniti strives to overcome, while the stylized design represents the brand's luxury and sophistication.\nThe Maserati logo is a trident with a trident head on top. The trident is a symbol of Neptune, the Roman \ngod of the sea, and represents Maserati's power and performance. The trident head is a stylized image of a \nram, which is the symbol of Modena, where Maserati is headquartered.\nAll three logos are prominently displayed on the image, and the white background helps them to stand out. \nThe image is simple and straightforward, but it is effective in conveying the message that BMW, Infiniti, and \nMaserati are all luxury car brands.\nGemini Pro: \nGPT-4V\uff1a\nLeftmost logo is BMW\u2019s logo. It is a circle icon which is filled with black color. Inside the circle, there is a \nblue horizontal line running through the middle. The area above the blue line is white and the area below the \nblue line is light blue. This logo represents a spinning propeller of an airplane, which is a nod to the company\u2019s \nhistory in aviation.\nThe middle logo is the Infiniti logo. It is composed of two overlapping triangles, with the points facing \nupward. The triangles are filled in with black color. The logo represents a mountain or a road, which is a nod to \nthe company\u2019s ethos of power, strength, and performance.\nThe rightmost logo is the Maserati logo. It is composed of a shield-like shape and has three red arrows \npointing upward. The shield shape is filled in with a faint white color, and the area between the arrows is black. \nThis logo is a nod to the company\u2019s history in racing, with the arrows representing the company\u2019s victories.\nPrompt: Describe the logos in details\nFigure 6: Logo recognition and description. Both models successfully identify logos and provide\ncomprehensive descriptions, capturing various aspects like design, color, shape, and symbolic\nrepresentations.\n16\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff2.4\uffffLogo\uffffRecognition\nIn-the-Wild\uffffLogo\uffffRecognition\uffffand\uffffDescription\nPrompt: Describe both \nthe image and logo in details.\nGPT-4V\uff1a\nThe image is of a McDonald\u2019s french fry container, filled with golden brown \nfrench fries. The container is red, and has the iconic McDonald\u2019s \u201cM\u201d logo \non it. The background is a bright yellow, making the red container and fries stand \nout. The logo is the McDonald\u2019s \u201cM\u201d which is a large golden arch, forming the \nletter \u201cM\u201d for McDonald\u2019s. It is one of the most recognized logos in the world, and \nis often associated with the fast food chain.\nGPT-4V\uff1a\nThe image shows two Starbucks cups, one is a clear plastic cup \nwith a lid and straw, and the other is a white paper cup with a \nlid. The clear plastic cup contains a cold drink with ice and whipped \ncream on top, while the white paper cup contains a hot drink. The cups \nare placed on a wooden table with a blurred background of a \nStarbucks store. The logo on the cups is the Starbucks logo, which is \na green circle with a white mermaid or siren in the center.\nThe image you sent is of two cups of iced coffee sitting on a wooden table in a Starbucks coffee \nshop. The cups are white and have the Starbucks logo printed on them. The logo is a green siren with two \ntails enclosed in a brown circle. The cups of coffee are full and have condensation on the outside. The ice \ncubes in the coffee are melting and the coffee is dripping down the sides of the cups. The coffee looks \nrefreshing and delicious. The image is well-composed and the colors are bright and vibrant. The green and \nbrown colors of the Starbucks logo stand out against the white cups and wooden table. The ice cubes and \ncondensation on the cups add a sense of realism and freshness to the image. Overall, the image is a well-\nexecuted advertisement for Starbucks iced coffee. It is visually appealing, conveys the message that the \nproduct is refreshing and delicious, and prominently displays the Starbucks logo.\nGemini Pro: \nThe image you sent is of a cup of McDonald's French fries on a yellow background. The cup is red \nand has the McDonald's logo printed on it. The logo is a yellow \"M\" on a red background. The cup of \nfries is overflowing, and some of the fries are sticking out of the top. The fries are golden brown in color and \nlook crispy and delicious. The image is well-composed and the colors are bright and vibrant. The red and \nyellow colors of the cup and logo stand out against the yellow background, and the fries are the focal point of \nthe image. The image is effective in conveying the message that McDonald's French fries are a delicious and \nsatisfying snack. The overflowing cup of fries suggests that McDonald's is generous with its portions, and the \ngolden brown color and crispy appearance of the fries make them look irresistible. The McDonald's logo is one \nof the most recognizable brands in the world. The yellow \"M\" is simple, but it is effective in conveying the \nmessage that the product is associated with McDonald's. The logo is also prominently displayed on the cup of \nfries, which helps to reinforce the brand association. Overall, the image is a well-executed advertisement for \nMcDonald's French fries. It is visually appealing, conveys the message that the product is delicious and \nsatisfying, and prominently displays the McDonald's logo.\nGemini Pro: \nPrompt: Describe both \nthe image and logo in details.\nFigure 7: In-the-wild logo recognition and description (1/2). Both models exhibit a robust\ncapability of identifying logos in various scenarios, accounting for occlusions, lighting conditions,\nand orientations, while Gemini tends to provide more detailed descriptions. However, in the second\ncase, GPT-4V\u2019s description shows minor instances of hallucination.\n17\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 2.4 Logo Recognition\nIn-the-Wild Logo Recognition and Description\nPrompt: Describe both \nthe image and logo in details.\nGPT-4V\uff1a\nThe image shows two Coca-Cola bottles lying on a bed of ice cubes. \nThe bottles are wet, indicating that they are chilled and ready to drink. The \nbottle on the left is a new grip bottle, as mentioned on the label. The bottle on \nthe right is a regular Coca-Cola bottle. Both bottles have the iconic Coca-Cola \nlogo on them. The logo is written in a cursive font in white color, with a red \nbackground. The first \"C\" in Coca is larger than the rest of the letters, and the \n\"o\" in Cola is elongated. The logo is instantly recognizable. \nGPT-4V\uff1a\nThe image shows a close-up of a person\u2019s foot wearing a white Nike \nAir Force 1 sneaker. The sneaker is a low-top style with a white \nrubber sole and white laces. The shoe\u2019s upper is made of a textured \nwhite leather, giving it a premium look. The logo shown on the sneaker is \nthe iconic Nike Swoosh. It is displayed on the side of the shoe in a silver \ncolor. The Swoosh is a curved, elongated checkmark shape with a \npointed tip at the end. It represents speed and motion, embodying the \nbrand\u2019s athletic and sporty aesthetic.\nThe image shows a pair of white Nike Air Force 1 sneakers on a person's feet. The sneakers are made \nof white leather and have a gray swoosh on the side. The logo is a swoosh on the side of the shoe. The Nike Air \nForce 1 is a classic sneaker that has been popular for decades. It is a versatile shoe that can be worn for a \nvariety of occasions, from casual to dressy. The sneakers are also known for their durability and comfort.\nHere is a more detailed description of the logo:\n\u00b7 The Nike swoosh is a stylized check mark.\n\u00b7 It is angled slightly downwards, which creates a sense of speed and movement.\n\u00b7 The swoosh is often used in conjunction with the Nike wordmark, but it can also be used on its own.\n\u00b7 The swoosh is typically black or white, but it can also be found in other colors.\n\u00b7 The Nike Air Force 1 sneakers and the Nike swoosh logo are both iconic symbols of the Nike brand. They are \nboth simple, yet effective, and they are both instantly recognizable.\nGemini Pro: \nThe image shows a can and a bottle of Coca-Cola on ice. The bottle is made of glass and has a red label \nwith white lettering. The can is made of aluminum and has a red background with a white Coca-Cola logo. The \nCoca-Cola logo is a stylized version of the company's name in cursive script. The letters are all connected and \nthe \"C\"s are enlarged to create a sense of movement and dynamism. The logo is also red, which is the \ncompany's signature color. The Coca-Cola logo is one of the most recognizable logos in the world. It is simple, \nyet effective, and it conveys the company's core values of happiness, refreshment, and excitement. Here is a \nmore detailed description of the logo:\n\u00b7 The word \"Coca-Cola\" is written in a stylized cursive script.\n\u00b7 The letters are all connected and the \"C\"s are enlarged to create a sense of movement and dynamism.\n\u00b7 The logo is red, which is the company's signature color.\n\u00b7 The logo is often surrounded by a white border, which helps it to stand out from its background.\nThe Coca-Cola logo is a powerful symbol of the company's brand and identity. It is a logo that is instantly \nrecognizable and that evokes positive emotions in people all over the world.\nGemini Pro: \nPrompt: Describe both the \nimage and logo in details.\nFigure 8: In-the-wild logo recognition and description (2/2). Both models manifest robust capabili-\nties in recognizing logos in diverse situations, including occlusions, different lighting conditions, and\nvaried orientations, while Gemini tends to provide more detailed descriptions. GPT-4V\u2019s description\nin the first case exhibits a slight error.\n18\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 2.5 Abstract Image Recognition\n GPT-4V\uff1a\nThe tangram that best illustrates a flying goose is number \n7. It has the triangular shape of a goose's body and \nwings, and the smaller triangle at the top could represent \nthe head and neck.\nThe other figures could represent various things:\n1. A rocket or a tower\n2. A butterfly or a bat\n3. A question mark or a snake\n4. A person or a robot\n5. A mountain range or a crown\n6. A house or a temple\n7. A flying goose (as mentioned above)\n8. A bird or a fish\n9. A boat or a hat\n10. A dog or a fox.\nGemini Pro: \nThe first tangram best illustrates a flying goose. It has a long, slender \nbody with a curved neck and head. The wings are spread wide, as if \nthe goose is in flight.\nThe other tangrams look like the following:\nSecond tangram: A duck or swan\nThird tangram: A cat or rabbit\nFourth tangram: A fish or bird\nFifth tangram: A dog or wolf\nSixth tangram: A butterfly or flower\nSeventh tangram: A house\nEighth tangram: A person\nNinth tangram: A tree\nTenth tangram: A flower\nPrompt:  A tangram is a \ngeometric puzzle that consists of \nseven flat pieces to form shapes. \nHere are ten abstract tangram. \nWhich one best illustrates a flying \ngoose? What do other figures \nlook like? \nhttps://www.pixtastock.com/illustration/80278014\nhttps://encrypted-\ntbn2.gstatic.com/images?q=tbn:ANd9GcT8TdR_3Q_QCAZwXfY\nYp0w5NKPUzZzoPk5wn-y7xmXshU9vnDtggyoE298NMKX0\nFigure 9: Abstract image recognition. While GPT-4V provides reasonable associations, the\nassociations given by Gemini do not appear as logical.\n19\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 2.6 Scene Understanding\nPrompt: Describe the image.\nGPT-4V\uff1a\nThe image is a screenshot from a dashcam, taken from the inside of a car, showing the view of the \nroad ahead. The car is driving on a two-lane road, with a speed limit sign of 35 mph visible on the right side \nof the road. There are trees on both sides of the road, and the sky is filled with dark clouds, \nsuggesting that it might rain soon. Orange cones are visible on the left side of the road, indicating a construction \nzone. In the distance, there is a bridge overpass.\nThe image shows a car driving down a highway with a speed limit sign of 35 \nmph on the side of the road. The car is in the middle lane, and there are no other \ncars immediately visible in front of it or behind it. The highway is straight and \nflat, and there are trees on both sides of the road. The sky is clear and blue, and the \nsun is shining.\nGemini Pro: \nhttps://www.fosters.com/story/news/2008/06/08/drive-55-some-seek-lower/52385077007/\nPrompt: Describe the image.\nGPT-4V\uff1a\nThe image is a view of a road from the perspective of a car dashboard camera. The road has two lanes, \nand the car is in the right lane behind a silver vehicle. There are two yellow signs on the side of the \nroad warning drivers of an upcoming pedestrian crossing and a curve to the left. The sky is cloudy and gray, \nand trees and bushes line the sides of the road. \nThe image shows a car driving down a street next to a crosswalk. The street is lined with trees on both \nsides, and there are a few other cars parked along the curb. The sky is cloudy, but there is no rain.\nThe car in the foreground is a red sedan. It is driving at a moderate speed and has its headlights on. \nThe car is in good condition, and its paint job is shiny. The driver is not visible, but the passenger side window \nis rolled down.\nThe crosswalk is empty, but there is a pedestrian waiting on the other side. The pedestrian is \nwearing a black jacket and jeans. They are carrying a briefcase and have a backpack on their back.\nThe overall mood of the image is calm and peaceful. The colors are muted, and the lines are clean. The image \nis well-composed and balanced.\nGemini Pro: \nFigure 10: Scene understanding. Both models are proficient in providing detailed descriptions of\nthe scenes and objects. However, in the first case, Gemini\u2019s description showcases a minor error.\n20\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff2.7\uffffCounterfactual\uffffExamples\nPrompt: Can you find the red ball near the gray cat in the image?\nGPT-4V\uff1a\nSorry, I cannot answer this question as there is no red ball visible in the \nimage.\nGemini Pro: \nNo, I cannot find a red ball near the gray cat in the image you sent.\nPrompt: Can you see the \nbright blue feathers on the \nbird's wing?\nGPT-4V\uff1a\nNo, I cannot see bright blue feathers on the bird's wing. It appears to have \nbrown and white feathers.\nGemini Pro: \nNo, I cannot see the bright blue feathers on the bird's wing in the image you \nsent. The bird is flying too high and the image is too grainy to see any details \non its wings.\nPrompt: Can you find \nthe blue \nlighting \nfixture \nmounted to the ceiling?\nGPT-4V\uff1a\nSorry, I cannot find a blue lighting fixture mounted to the ceiling in this \nimage. All I see is a black wrought iron chandelier with white glass shades.\nGemini Pro: \nNo, I cannot find a blue lighting fixture mounted to the ceiling in the image \nyou sent. \nFigure 11: Counterfactual judgement. Both models are capable of providing factual descriptions\nrelated to the scenes and objects depicted in the images.\n21\nPrompt: Count the number of apples in the image. Let\u2019s \ncount the apples row-by-row.\nGPT-4V\uff1a\nFirst row: 4 apples, Second row: 3 apples, Third row: \n4 apples. Total number of apples in the image: 11 apples\nSec. 2.8 Object Counting \nYes, that is right. There are 4 apples in the first row, 4 apples in the second row, and 3 apples in the third \nrow of the image you sent.\nGemini Pro: \nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nPrompt:  There are 4 apples in the first row, 4 apples in the second row, 3 apples in the third row. Is \nthat right?\nBased on the image you sent, there are 12 apples in total. Counting row-by-row, we have:\nRow 1: 4 apples, Row 2: 5 apples, Row 3: 3 apples\nGemini Pro: \nPrompt: Count the number of apples in the image.\nGPT-4V\uff1a There are 7 apples in the image.\nThere are five apples in the image you sent.\nGemini Pro: \nPrompt: Count the number of oranges in the image.\nGPT-4V\uff1a There are 3 oranges in the image.\nThere are five apples in the image you sent.\nGemini Pro: \nPrompt: How many oranges in this image?\nFigure 12: Object Counting. The task is to count the number of fruits in the pictures. Gemini\u2019s\ncounting ability is relatively weaker compared to GPT-4V. Gemini, due to its relatively weaker\nmultimodal memory performance, cannot be guided to the correct answers.\n22\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 2.9 Spot the Difference\nGPT-4V\uff1a\n1) Image 1 has spikes on its back while image 2 does not.\n2) Image 2 has a tail while image 1 does not.\n3) Image 2 has a mouth with teeth while image 1 does not have a mouth.\nGemini Pro: \nYes, I can find two differences between Image 1 and Image 2:\nThe dinosaur in Image 2 has a long tail, while the turtle in Image 1 has a short tail.\nThe dinosaur in Image 2 has a smooth, scaly skin, while the turtle in Image 1 has a\nhard, bumpy shell.\nPrompt: Compare Image 1 and image 2, tell me about the differences between two images.\nFigure 13: Spot differences. While GPT-4V successfully spots all differences but confuses an eye\nfor a mouth, Gemini only discerns two out of three differences and incorrectly infers that the right\nimage should have a short tail due to mistaking it for a turtle. Therefore, neither GPT-4V nor Gemini\ndelivers completely satisfactory results in this task.\n23\n3\nText Recognition and Understanding in Images\nThis section mainly discusses the ability to recognize text in images. In contrast to the previous\nsection, this section focuses solely on text extraction and basic understanding, without involving\ndeep logical reasoning, or understanding utilizing knowledge bases. Sec. 3.1 demonstrates the\nability to extract text from scenes, including various scenarios such as billboards. Sec. 3.2 involves\nthe extraction ability for mathematical equations. Sec. 3.3 deals with the extraction and simple\nunderstanding of text in charts and tables.\n3.1\nScene Text Recognition\nFig. 14 and Fig. 15 show the extraction of text from images in various scenes, including stock\nmarket trading information and shop signs. Both models exhibit good performance. Fig. 16 is about\nreading and estimating numbers on instrument panels, demonstrating the models\u2019 zero-shot and\nfew-shot capabilities. It was found that Gemini\u2019s performance is inferior due to the lack of a memory\nmechanism. This can also be attributed to Gemini\u2019s limited memory capabilities, as GPT-4V can\nprocess multiple images sequentially, while Gemini can only input images after combining them into\na single input.\n3.2\nEqation Recognition\nFig. 17 shows the capability of understanding mathematical formulas, where the task is to convert\nformulas in images into LaTeX format. Both models perform well with simple equations but struggle\nto accurately recognize complex ones. Both models need improvement in understanding mathematical\nsymbols. We found that Gemini, in particular, fails to recognize the summation symbol.\n3.3\nChart Text Recognition\nFig. 18 shows the task of reading and reconstructing a Python-generated line chart through Python\ncode, where GPT-4V\u2019s reconstruction is more similar to the original chart. Here, GPT-4V excels\nin simulating the growth trend of the original image, while Gemini provides a more chaotic shape.\nFig. 19 demonstrates the models\u2019 ability to read text from tables, where Gemini performs well without\nerrors. Therefore, once again, it is confirmed that for single images, Gemini performs better when the\nimage resolution is high. Fig. 20 presents reading a trend line graph, identifying the highest point and\nits x and y-axis values, with Gemini showing good performance in correctly choosing the right curve\nand accurately reading the highest point information. GPT-4V and Gemini both made errors when\nextracting hidden information from the images. This already involves some basic inference. In Sec. 5,\nwe will further investigate the multimodal textual reasoning abilities of both models.\n24\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 3.1 Scene Text Recognition\nPrompt: What are all the \nscene text in the image?\nGPT-4V\uff1a\nThe scene text in the image is \"3,642,039,031,055\".\nGemini Pro: \nThe scene text in the image is \"3,642,039,031,955\".\nPrompt: What are all the \nscene text in the image?\nGPT-4V\uff1a\nGemini Pro: \n\"NVDA,\" \"NVIDIA,\" \"356.85,\" \"+51.47,\" and \"+11.68%.\"\nThe scene text in the image are:\n- (NVDA),   - NVIDIA,  - 356.85,  - +51.47,  - +16.85%\nThe scene text in the image is\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\nGemini Pro: \n \"Royal London\".\nThe scene text in the image are:\n- \"Royal\",   - \"London\" \nThe scene text in the image is\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\nGemini Pro: \nThe Earth without art is just 'Eh'\"\nThe scene text in the image are:\n\"The Earth without art is just 'eh'\".\nThe scene text in the image is\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\nGemini Pro: \nThe Road to Hollywood, how some of us, got here\nThe scene text in the image are:\n\u201cThe Road to Hollywood: how some of us got here\u201d\nThe scene text in the image is\nFigure 14: Scene text recognition (1/2). Both models demonstrate proficiency in recognizing scene\ntext under many challenging circumstances.\n25\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 3.1 Scene Text Recognition\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\n- \"Westfield\",   - \"westfield.com.au\" \nGemini Pro: \n\"Westfield\" twice, followed by \"westfield.com.au\"\nPrompt: What are all \nthe scene text in the image?\nGPT-4V\uff1aThe scene text in the image are:\n1) \u201cCOVID-19 TESTING\u201d,   2) \u201cPLEASE HAVE ID & \nINSURANCE CARD READY\u201d,  3) \u201cW @UW Medicine\u201d (this is \nthe logo/branding for the University of Washington Medicine.\nThe scene text in the image is\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\nGemini Pro: \nVacation,  BEACH,  SMILE,  SUMMER,  WATERMELON,  \nFLIP FLOPS,  LEMONADE\nThe scene text in the image are:\n- Vacation,  - Beach,  - Smile,  - Summer,  - Watermelon,  \n- Flip flops,  - Lemonade\nThe scene text in the image is\nPrompt: What are all the \nscene text in the image?\nGPT-4V\uff1a\nGemini Pro: \nMalayan Tapir, Tapirus indicus, LOCATION, Malay Peninsula and Sumatra, near water, RELATIVES, \nThere are only five species of tapirs. They are most closely related to horses and rhinos.\nVANISHING? The forest provides tapirs with shelter, food, and water, but forests are rapidly disappearing \nin Malaysia and Sumatra.\nThe scene text in the image are:\n- Malayan Tapir,  - Tapirus indicus,  - LOCATION: Malay \nPeninsula and Sumatra, near water,  - RELATIVES: There are only \nfive species of tapirs. They are most closely related to horses and \nrhinos.\n- VANISHING? The forest provides tapirs with shelter, food, and \nwater, but forests are rapidly disappearing in Malaysia and Sumatra. \nThe scene text in the image is\nThe scene text in the image is\nThe scene text in the image is\nGemini Pro: \n\"COVID-19 TESTING\" at the top, followed by \"PLEASE HAVE \nID & INSURANCE CARD READY\" in the middle, and \"W \nGor\" at the bottom.`\nFigure 15: Scene text recognition (2/2). Both models show competence in recognizing scene text\nacross a myriad of challenging scenarios. However, Gemini displays minor errors in the second\nimage\u2019s text recognition.\n26\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff3.1\uffffScene\uffffText\uffffRecognition\nzero-shot\uffffand\ufffffew-shot\uffff(2-shot\uffffhere)\nPrompt: What is the read of the speed meter? To read \nthis speed meter, observe the needle on the outer edge of \nthe dial which points to your current speed represented in \nmiles per hour. Yellow is the color of the needle. Let\u2019s think \nstep-by-step.\nGPT-4V\uff1a\n1. First, look at the outer edge of the dial and find the numbers that represent miles per hour.\n2. Next, find the yellow needle that points to the current speed.\n3. Observe where the needle is pointing and read the number on the outer edge of the dial.\n4. This number is the current speed in miles per hour.\nIn this example, the needle is pointing to approximately 40 mph, so the current speed is 40 miles per hour.\nGemini Pro: \nThe speedometer is reading 140 miles per hour.\nTo read a speedometer, you need to look at the needle on the outer edge of the dial. The needle points to your \ncurrent speed, which is represented in miles per hour (mph). In this case, the needle is pointing to 140 mph, \nwhich means the vehicle is traveling at 140 miles per hour.\nHere are the steps on how to read a speedometer:\nLocate the speedometer. It is usually located on the dashboard of the vehicle, next to the steering wheel.\nLook at the needle on the outer edge of the dial. The needle points to your current speed.\nFind the speed markings on the dial. The speed markings are usually in miles per hour (mph).\nFollow the needle to the speed marking on the dial. This is your current speed.\nPrompt: For the left figure, The yellow pointer is roughly at the middle between 80 and 100 mph. The \nmiddle between 80 and 100 is 90. The speed is around 91 mph, as the pointer is just passing 90 mph.\nFor the middle figure, The yellow needle is roughly at the middle between 20 and 40 mph. The middle \nbetween 20 and 40 is 30. The speed is around 29 mph, as the needle is close to 30 mph but not quite 30 \nyet.\nSo for the right figure, What is the read of the speed meter?\nGPT-4V\uff1a\nFor the right figure, the yellow pointer is pointing directly at the 60 mph mark on the speedometer. This \nindicates that the vehicle's speed at the moment the image was captured is exactly 60 mph.\nGemini Pro: \nFor the right figure, the yellow pointer is roughly at the middle between 120 and 140 mph. The middle between \n120 and 140 is 130. The speed is around 131 mph, as the pointer is just passing 130 mph.\nFigure 16: Scene text recognition in zero-shot and few-shot scenarios. In zero-shot scenarios,\nboth models fail to comprehend the dashboard. Even when provided with two examples in few-shot\nscenarios, neither GPT-4V nor Gemini succeed in correctly identifying the pattern from the samples.\n27\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 3.2 Eqation Recognition\nPrompt: Can you generate \nits latex code?\nGPT-4V\uff1a\nGemini Pro: \nPrompt: Can you generate \nits latex code?\nGPT-4V\uff1a\nGemini Pro: \nPrompt: Can you generate \nits latex code?\nGPT-4V\uff1a\nGemini Pro: \nPrompt: Can you generate its latex code?\nGPT-4V\uff1a\nGemini Pro: \nPrompt: Can you transfer the equation in the image into latex format?\nFigure 17: Equation recognition. Both models are capable of generating LaTeX codes from hand-\nwritten inputs. They succeed in simple cases, but fail to accurately generate complex equations.\nGPT-4V\u2019s results are slightly closer to the actual equation, as Gemini is unable to recognize the\nsummation symbol.\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 3.3 Chart Text Recognition\n Figure Generation\nPrompt: Generate python \ncode to draw similar curves.\nGemini Pro: \nGPT-4V\uff1a\nFigure 18: Chart text recognition in zero-shot scenario. GPT-4V generates Python code with a\nfigure that is more consistent with the original code than what Gemini produces.\n28\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 3.3 Chart Text Recognition\nPrompt: Please read the table in this image and return a markdown-style reconstructed table in text.\nGPT-4V\uff1a\nGemini Pro: \n| Country | Sales Volume | Revenue | Profit |\n| --- | --- | --- | --- |\n| USA | 40.080 | $15.971.421 | $3.086.421 |\n| China | 35.070 | $15.866.670 | $3.032.162 |\n| Australia | 27.054 | $14.812.566 | $2.868.636 |\n| India | 23.046 | $10.608.174 | $1.853.710 |\n| South Korea | 16.032 | $10.494.948 | $1.975.844 | \n| Country | Sales Volume | Revenue | Profit |\n| --- | --- | --- | --- |\n| USA | 40.080 | $15.971.880 | $3.086.421 |\n| China | 35.070 | $15.866.670 | $3.032.162 |\n| Australia | 27.054 | $14.812.566 | $2.868.636 |\n| India | 23.046 | $10.608.174 | $1.853.710 |\n| South Korea | 16.032 | $10.494.948 | $1.975.844 | \nPrompt: Please read the table in this image and return a latex-style reconstructed table in text.\nGPT-4V\uff1a\nGemini Pro: \nHere is the reconstructed table in latex:\n\\begin{tabular},{|c|c|c|c|} \\hline \nCountry & Sales Volume & Revenue & Profit \\\\ \\hline \nUSA & 40.080 & \\$15.971.887 & \\$3.068.421 \\\\ \\hline \nChina & 35.070 & \\$15.866.670 & \\$3.032.162 \\\\ \\hline \nAustralia & 27.054 & \\$14.812.566 & \\$2.868.636 \\\\ \\hline \nIndia & 23.046 & \\$10.608.174 & \\$1.853.710 \\\\ \\hline\nSouth Korea & 16.032 & \\$10.494.948 & \\$1.975.844 \\\\ \\hline\n\\end{tabular} \n\\begin{tabular}{l|c|c|c}\n\\hline\nCountry & Sales Volume & Revenue & Profit  \\\\ \\hline\nUSA & 40,080 & \\$15,971,880 & \\$3,086,421  \\\\ \\hline\nChina & 35,070 & \\$15,866,670 & \\$3,032,162  \\\\ \\hline\nAustralia & 27,054 & \\$14,812,566 & \\$2,868,636  \\\\ \\hline\nIndia & 23,046 & \\$10,608,174 & \\$1,853,710  \\\\ \\hline\nSouth Korea & 16,032 & \\$10,494,948 & \\$1,975,844  \\\\ \\hline\n\\end{tabular}\nFigure 19: Chart text recognition and figure code generation. Gemini outperforms GPT-4V in\ngenerating the correct table based on a given picture. GPT-4V exhibits errors in number recognition.\n29\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 3.3 Chart Text Recognition\nzero-shot\nPrompt: In the graph, which year has the highest \naverage gas price for the month of June? Step 1, read the \ntitle of the graph; Step 2 read the legend of the graph to \nlearn the correspondence between year and line color; Step \n3 look at the graph and identify the month of June on the x-\naxis; Step 4, look at the data points for each line in the \nmonth of June; Step 5, Compare the data points of each line \nto determine which one has the highest average gas price in \nJune; Step 6, identify the color of that line, and match to the \nyear in the legend on top. Let\u2019s think step by step.\nGPT-4V\uff1a\nStep 1: The title of the graph is \"National Gas Price Comparison / 2019-2023\". This tells us that the graph is \ncomparing gas prices in the United States from 2019 to 2023. \nStep 2: The legend on top of the graph shows that the years 2019, 2020, 2021, 2022, and 2023 are \nrepresented by different colors: orange, black, blue, green, and teal respectively. \nStep 3: Looking at the x-axis, we can see that the month of June is represented. \nStep 4: In the month of June, we can see that the data points for each line are as follows: - 2019 (orange): \naround $2.75 - 2020 (black): around $2.25 - 2021 (blue): around $3.32 - 2022 (green): around $3.00 - 2023 \n(teal): around $2.90.\nStep 5: Comparing these data points, we can see that the highest average gas price in June is $3.32.\nStep 6: The color of the line representing $3.32 is blue, which corresponds to the year 2021 in the legend. \nTherefore, the year with the highest average gas price for the month of June is 2021.\nGemini Pro: \nStep 1: Read the title of the graph. The title of the graph is \"NATIONAL GAS PRICE COMPARISON | 2019-\n2023\". This tells us that the graph is comparing the average gas price in the United States each month from \n2019 to 2023.\nStep 2: Read the legend of the graph to learn the correspondence between year and line color. The legend of \nthe graph shows that the line colors correspond to the years as follows: 2019: Blue, 2020: Green, 2021: \nOrange, 2022: Red, 2023: Black\nStep 3: Look at the graph and identify the month of June on the x-axis.\nThe x-axis of the graph shows the months of the year, from January to December. June is the sixth month of the \nyear, so it is located at the sixth tick mark on the x-axis.\nStep 4: Look at the data points for each line in the month of June. The data points for each line in the month of \nJune are as follows: 2019: $3.32, 2020: $2.14, 2021: $3.06, 2022: $5.00, 2023: $4.99.\nStep 5: Compare the data points of each line to determine which one has the highest average gas price in June. \nThe data point for 2022 is the highest, at $5.00. This means that 2022 has the highest average gas price for the \nmonth of June.\nStep 6: Identify the color of that line, and match to the year in the legend on top. The line for 2022 is red. \nTherefore, the year with the highest average gas price for the month of June is 2022.\nAnswer: 2022\nFigure 20: This image exhibits the results of chart text recognition tasks carried out by GPT-4V and\nGemini. Both models show poor performance in recognizing charts, misjudging colors and values.\nGemini\u2019s capability is slightly superior as it provides correct results on Steps 5 and 6, despite errors\nin the intermediate process details.\n30\n4\nImage Reasoning Abilities\nThis section primarily showcases the model\u2019s ability for further inference from image information.\nUnlike Sec. 2, this section involves not only simple recognition of the model but also further reasoning\nbased on the information in the images. Sec. 4.1 demonstrates the model\u2019s understanding of humorous\nimages. Sec. 4.2 presents multimodal knowledge and commonsense, testing the model\u2019s ability to\napply multimodal knowledge and commonsense in interpreting scientific concepts and details in\nimages. Sec. 4.3 explores the detective reasoning skills of the model, focusing on inferring a person\u2019s\ncharacteristics or profession based on environmental clues. Sec. 4.4 assesses the model\u2019s ability\nto combine multiple images into a whole, challenging its capacity to form connections between\ndifferent visual elements. Sec. 4.5 involves intelligence tests, evaluating the model\u2019s IQ through\npattern recognition and logical reasoning. Sec. 4.6 examines the model\u2019s emotional intelligence (EQ),\nfocusing on their understanding and expression of emotions and aesthetic judgment.\n4.1\nHumorous Image Understanding\nFig. 21 is about understanding humorous images in the form of jokes. The models need to understand\nwhere the punchline is located in the image and the simple text. This section primarily focuses\non image understanding, so we classify it as reasoning for images. In the three examples here,\nboth models displayed a correct understanding of the two joke images, demonstrating an ability to\ncomprehend human humor.\n4.2\nMultimodal Knowledge and Commonsense\nThe next three images focus on multimodal commonsense understanding, emphasizing scientific\nknowledge. Fig. 22 deals with world map comprehension; due to image resolution, Gemini\u2019s answer\nis incomplete, but it excels in general knowledge questions. Fig. 23 involves the Americas map and\nfood chain questions, where Gemini\u2019s answers are also incomplete. Fig. 24 illustrates the rainfall\ncycle. In summary, both models possess strong knowledge for answering general scientific queries,\nyet Gemini struggles with small text extraction.\n4.3\nDetective Reasoning Ability\nFig. 25 showcases the model\u2019s detective and logical reasoning abilities. When given a person\u2019s room\nto deduce their profession and personality, both models were able to make reasonable judgments,\nwith Gemini providing a more detailed thought process. In contrast, GPT-4V could provide concise\nyet accurate answers.\n4.4\nAssociation of Parts and Objects\nFig. 26 challenges the model to combine multiple images into a whole. Here, the task involved\nforming a person using a bag as the head and body. Although both models provided reasonable\njudgments, GPT-4V offered a more accurate and detailed answer.\n4.5\nIntelligence Tests\nFig. 27-Fig. 30 demonstrate the models\u2019 IQ capabilities, mainly featuring intelligence tests with\npattern-finding graphical problems. Most of the pattern-finding tasks here are relatively simple.\nOverall, GPT-4V is able to provide more accurate answers. This could also be attributed to Gemini\u2019s\ninability to recognize multiple images.\n4.6\nEmotional Intelligence Tests\nFig. 31-Fig. 35 showcase the models\u2019 EQ capabilities, focusing on understanding and outputting\nemotions, including aesthetic judgment. It was found that both models have a fairly accurate grasp of\nemotions and aesthetic judgment and are capable of producing different emotional outputs according\nto user requirements.\n31\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.1 Humorous Image Understanding\nPrompt: Can you \nexplain this meme?\nGPT-4V\uff1a\nThis meme is poking fun at the idea of procrastination and how \nsomeone may put off doing a task until a certain time, but if they miss \nthat time, they will push it back even further. In this case, the person in \nthe meme planned to do something at 8 o\u2019clock, but since it is now \n8:05, they have decided to wait until 9 o\u2019clock to do it. The image of \nKermit the Frog relaxing in bed represents the person\u2019s desire to \nprocrastinate and avoid the task.\nThe meme of Kermit the Frog lying on a bed is a relatable meme that expresses the feeling of \nprocrastination. The text in the meme says \"Me: I'll do it at 8. Time: 8.05. Me: looks like I gotta wait till \n9 now.\" This is a common experience for many people, who put off tasks until the last minute. The Kermit \nthe Frog image is used to express the feeling of resignation and boredom that comes with procrastination.\nGemini Pro: \nThe image is funny because it is a juxtaposition of two things that are typically not seen together: a \nrhinoceros and a painting. The rhinoceros is a large, powerful animal, while painting is a delicate art form. \nThe combination of the two is unexpected and humorous.\nAdditionally, the text on the image, \"You see the world as you are!\", adds to the humor. The text suggests \nthat the rhinoceros's painting is a reflection of its own inner world. This is a funny idea, as it is \nunlikely that a rhinoceros would have the same perspective on the world as a human does.\nOverall, the image is funny because it is unexpected, thought-provoking, and a bit absurd.\nGPT-4V\uff1a\nThere are a couple of things that people might find funny about this image:\n- It is a cartoon of a rhinoceros painting, which is an unexpected and silly \nconcept in itself.\n- The rhinoceros is painting a picture of itself, which adds an extra layer of \nhumor.\n- The caption, \u201cYou see the world as you are!\u201d, is a joke about how we \nall have our own unique perspectives and can be a little self-\ncentered at times.\nPrompt: What is funny \nabout this image?\nGemini Pro: \nGPT-4V\uff1a\nThis meme highlights the deterioration of a person\u2019s handwriting \nduring an exam. The first two pages show neat and legible \nhandwriting, the middle page shows slightly messier handwriting, and \nthe last two pages show almost illegible handwriting. The meme is \npoking fun at how, as an exam progresses, a person\u2019s \nhandwriting tends to get worse as they become more tired \nand rushed to finish the exam. It also implies that the person\u2019s \nfocus and attention to detail decrease towards the end of the exam.\nYes, I can explain this meme. The meme is about a student's handwriting in exams. It is a common \nstereotype that students' handwriting gets worse as the exam goes on, because they are \ngetting more tired and stressed. The meme exaggerates this stereotype by showing the student's \nhandwriting getting progressively worse from the first two pages of the exam to the last two pages.\nThe meme is funny because it is relatable to many students. Many students have experienced their \nhandwriting getting worse during an exam, and they can laugh at the meme because it makes light of this \nstressful situation.\nGemini Pro: \nPrompt: Can you explain this meme?\nFigure 21: Humorous meme understanding. Both GPT-4V and Gemini demonstrate the impressive\ncapability to comprehend the humor embedded within memes.\n32\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.2 Multimodal Knowledge and Commonsense\n Science and Knowledge\nPrompt: Which of these oceans does the prime meridian intersect?\nGPT-4V\uff1a\nThe prime meridian intersects the following oceans: - Atlantic Ocean, - Southern Ocean, - Arctic \nOcean\nThe prime meridian intersects the Atlantic Ocean, as shown by the 0\u00b0 longitude line passing through \nthe middle of the Atlantic Ocean in the image.\nGemini Pro: \nAccording to the kinetic molecular theory, the temperature of a gas is directly proportional to the average \nkinetic energy of its particles. The average kinetic energy of a particle is given by the equation:\nKE = 1/2 mv^2, where m is the mass of the particle and v is its speed.\nIn this case, both samples have the same number of particles and the same mass of each particle. However, \nthe particles in sample A have a higher average speed than the particles in sample B. Therefore, the \nparticles in sample A have a higher average kinetic energy.\nGPT-4V\uff1a\nBecause the average particle speed in sample A is higher than in sample B (600 m/s vs. 540 m/s), the \nparticles in sample A have a higher average kinetic energy. Therefore, sample A has a higher \ntemperature than sample B.\nPrompt: Compare the average kinetic energies of \nthe particles in each sample. The diagrams below \nshow two pure samples of gas in identical closed, rigid \ncontainers. Each colored ball represents one gas \nparticle. Both samples have the same number of \nparticles. \nWhich sample has the higher temperature? Sample A \nor sample B?\nGemini Pro: \nFigure 22: Answering science and knowledge questions (1/3). Both GPT-4V and Gemini can\nunderstand the question textually and visually, and gather necessary information to answer the\nquestion. In the first case, there are omissions in Gemini\u2019s description.\n33\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.2 Multimodal Knowledge and Commonsense\n Science and Knowledge\nPrompt: Which of these states is farthest north?\nGPT-4V\uff1a\n Out of the three states shown in the map - Arizona, Oklahoma, and Delaware, Delaware is \nthe farthest north.\nAccording to the map you sent, the farthest north state of the four is North Carolina. North Carolina's \nnorthernmost point is at 36\u00b059'38.8\"N latitude, which is just north of the Virginia border. The other three \nstates are all located further south: Delaware, Oklahoma, and Arizona.\nHere are the northernmost points of each state:\nDelaware: 39\u00b049'16.9\"N,     Oklahoma: 37\u00b000'00\"N,      Arizona: 37\u00b000'00\"N\nGemini Pro: \nThe producer in the food web is the berries and \nflowers. Producers are organisms that can make \ntheir own food, and they are the foundation of all \nfood webs.\nGPT-4V\uff1a\nThe producers in this food web are the berries \nand flowers, grasses, and seeds. These are \nthe organisms that use energy from the sun to \nproduce their own food through photosynthesis.\nGemini Pro: \nPrompt: Which of the organisms is the \nproducer in this food web?\nFigure 23: Answering science and knowledge questions (2/3). Both GPT-4V and Gemini can\nunderstand the question textually and visually, and gather necessary information to answer the\nquestion. In the first case, there are errors in both GPT-4V\u2019s and Gemini\u2019s description.\n34\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.2 Multimodal Knowledge and Commonsense\n Science and Knowledge\nPrompt: Please use the figure to explain the distinction between evaporation and \nevapotranspiration.\nGPT-4V\uff1a\nThe image illustrates various components of the water cycle, among which evaporation and \nevapotranspiration are key processes. Here's how they differ:\nEvaporation: This is the process by which water is converted from liquid to vapor and eventually \nenters the atmosphere. In the image, it is shown as arrows rising directly from bodies of water like \nthe ocean and the surface water (e.g., lakes, rivers). This represents the transformation of surface \nwater into water vapor due to the energy provided by the sun.\nEvapotranspiration: This term combines two processes: evaporation and transpiration. \nTranspiration is the process by which water is absorbed by plants' roots, moves up through the \nplants, and is then released into the atmosphere from the plants' leaves. Evapotranspiration refers to \nthe sum total of water evaporating from the land (including soil moisture and water bodies) and the \nwater transpired by plants. It's depicted in the image as arrows going up from the trees and grassy \nland, signifying the contribution of plant life to the atmospheric moisture.\nEvaporation and Evapotranspiration\nEvaporation is the process by which water changes from a liquid to a gas. It can happen from any \nsurface, but it is most common from open water bodies like lakes and oceans. Evaporation also happens \nfrom soil and plants.\nEvapotranspiration is the combined process of evaporation and transpiration. Transpiration is the \nprocess by which plants release water vapor into the atmosphere through their leaves.\nGemini Pro: \nFigure 24: Answering science and knowledge questions (3/3). Both GPT-4V and Gemini can\nunderstand the question textually and visually, and gather necessary information to answer the\nquestion. In this example, GPT-4V generates better answer than Gemini.\n35\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.3 Detective Reasoning Ability\nPrompt: Suppose you are a detective, what can \nyou infer from the visual clues in the image?\nGPT-4V\uff1a\nAs a detective, I can infer the following from the visual clues in the image:\n\u00b7 The room is a bedroom. This is evidenced by the clothes hanging on the door, the bed in the background, \nand the personal items on the desk.\n\u00b7 The person who lives in the room is a student. This is evidenced by the computer desk, the books on the \nshelf, and the backpack on the bed.\n\u00b7 The person is organized and tidy. This is evidenced by the fact that the room is clean and well-kept, and the \nitems on the desk are neatly arranged.\n\u00b7 The person is interested in technology. This is evidenced by the computer, the monitor, and the other \nelectronic devices on the desk.\n\u00b7 The person is creative and expressive. This is evidenced by the artwork on the walls and the personal \ntouches throughout the room.\nI can also make some inferences about the person's lifestyle based on the image. For example, the fact that \nthere is a computer desk in the bedroom suggests that the person often works or studies from home. The fact \nthat there is a backpack on the bed suggests that the person is a student or has a job that requires them to \ntravel. The fact that the room is clean and well-kept suggests that the person is responsible and organized.\nOverall, the image suggests that the person who lives in the room is a student who is \ninterested in technology, is organized and tidy, and is creative and expressive.\nGemini Pro: \nFrom the visual clues in the image, I can infer that:\n- The room belongs to someone who is likely \na young adult or teenager, as evidenced by the \ncasual clothing and the desk with a computer.\n- The person is likely a student or someone \nwho works from home, as evidenced by the desk \nand computer.\n- The person is likely fashion-conscious, as evidenced by the multiple jackets and shoes.\n- The room is likely located in a colder climate, as evidenced by the jackets and boots.\n- The person is likely not very tidy, as evidenced by the cluttered desk and clothes hanging on the door.\n- The room is likely located in an older or more rustic building, as evidenced by the exposed pipes and the \nworn-looking walls. \nFigure 25: Multimodal commonsense reasoning. Both GPT-4V and Gemini can infer images well\nbased on common sense.\n36\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff4.4\uffffAssociation\uffffof\uffffParts\uffffand\uffffObjects\n GPT-4V\uff1a\nThe three visual parts are related in that they are all components that could be \nused to create a single object. In this case, the object is a boy.\nTo combine these parts to form a boy, we can arrange them in the following way:\n- The first image (the cardboard with orange and green shapes) could be \nused as the body of the boy. \n- The second image (the pair of jeans) could be used as the legs of the boy. \n- The third image (the cardboard with yellow, red, and blue shapes) could \nbe used as the head of the boy. \nBy arranging these parts in this way, we can create a single object that resembles \na boy.\nPrompt: How are those visual parts related, and can they be combined to form a single \nobject such as a boy? If so, how to arrange them?\nGemini Pro: \nThe cardboard box is the body of a robot, and the robot\u2019s head is coming out of the box. \nThis is a common interpretation in the style of pareidolia, where people see familiar objects or shapes in \nrandom things. In this case, the person\u2019s arms and legs could be seen as the robot\u2019s limbs.\nFigure 26: Understanding part-object association in real image. Both GPT4-V and Gemini\nunderstand how to combine the three part in a reasonable way. Of course, here GPT-4V provides a\nmore satisfactory answer.\n37\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.5 Intelligence Tests\nDiscovery and Association of Parts and Objects\n GPT-4V\uff1a\nd. Apple\nExplanation: Carrot, potato, corn, and bean are all vegetables, while apple is a fruit. \nGemini Pro: \nIn this set, the least like the other four is (d) Apple.\na, b, c, and e are all vegetables that grow underground (roots or tubers). While they have different \ntextures and purposes, they share a common origin and classification. Apple, on the other hand, is a \nfruit that grows on trees. It differs from the other items in terms of origin, texture, and even how we \ntypically consume it.\nPrompt:\nWhich one of the five is least like the other four?\na. Carrot    b. Potato   c. Corn    d. Apple   e. Bean\nGPT-4V\uff1a\nAnswer: d. is the least like the other  four because it is the only one that  does not have a central \npoint connecting all the other points.\nGemini Pro: \nThe image you sent shows five symbols: a, b, c, d, and e. Each symbol is a circle with lines or dots \ninside. Option  A is different from the others because it has a square in the center, while the \nother symbols have circles or dots.\nFigure 27: Discovery and association of parts and objects. In response to example questions from\nthe Wechsler Adult Intelligence Scale (WAIS) [34], Gemini performed poorly. This suggests that its\nability to recognize and compare multiple images might be limited.\n38\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.5 Intelligence Tests\nRPM: Entire Question Page\n GPT-4V\uff1a\nThe correct drawing that fits the empty space is the upside-down triangle. This is because the \npattern in the grid is that each row and column has one of each shape in different orientations.\nGemini Pro: \nThe pattern in the first column is that the shapes are rotated 90 degrees counterclockwise. The missing \nfigure in the second column is a square.\nPrompt: Find out the pattern in the first column and use it to infer the missing figure in the \nsecond column.\n GPT-4V\uff1a\nThe pattern in the first two rows is that the figures are rotated 90 degrees clockwise in each \nsubsequent cell. Therefore, the missing figure in the third row should be a rotated version of the figure \nin the previous cell. Looking at the options, the correct answer is OC, as it is the figure that is rotated \n90 degrees clockwise from the figure in the previous cell.\nGemini Pro: \nBased on the patterns in the first two rows, it is possible to infer that the missing figure in the third row \nshould be H. Figure H has a pattern of alternating squares and circles, with a square in the top left \ncorner and a circle in the bottom right corner. This pattern follows the same logic as the patterns in the \nfirst two rows, where the top left and bottom right squares always contain a different type of shape.\nFigure 28: RPM: entire question page (1/2). In response to example questions from the Raven\u2019s\nProgressive Matrices [35], Gemini performed poorly. This suggests that its ability to recognize,\ncompare multiple images, and discern their inherent connections might be limited.\n39\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff4.5\uffffIntelligence\uffffTests\nRPM:\uffffEntire\uffffQuestion\uffffPage\nPrompt: Here are three sub images arranged in a 2-by-2 matrix. Figure out which figure of \na to f that should logically replace the missing figure. What would be the next image to \ncomplete the matrix?\nGPT-4V\uff1a\nGemini Pro: \nThe missing image is (c).\nThe matrix shows a pattern of alternating between shapes with four equal sides (square, rhombus, \nsquare) and shapes with two pairs of equal sides (rectangle, rectangle, rhombus). Therefore, the next \nimage in the matrix should be a shape with four equal sides, which is the rhombus.\nThe first image in the first column, image 1, is a diamond. The first image in the second column, image \n3, is a diamond with a dot in the middle. The second image in the first column, image 2, is a star. \nTherefore, the second image in the second column, image 4 should continue this pattern and be a star \nwith a dot in the middle. Option c is the only image that fits this pattern, as it is a star with a dot in the \nmiddle. Therefore, the correct answer is c.\nFigure 29: RPM: entire question page (2/2). In response to example questions from the Raven\u2019s\nProgressive Matrices [35], both GPT-4V and Gemini generated the correct results. However, Gemini\nfailed to provide a reasonable explanation. This suggests that its ability to recognize, compare\nmultiple images, and discern their inherent connections might be limited.\n40\nPrompt: Here are three sub images arranged in a 2-\nby-2 matrix. First, look at the two images in the first \ncolumn, in the top left and bottom left. Then, find out the \npattern in the first column based on the left two images. \nNext, use the found pattern and the image in the top right \nto infer the missing figure. Finally, describe what would \nthe missing image look like?\nGPT-4V\uff1a\nThe pattern in the first column is that the shape in the bottom  left image has a dot in the center of the shape in \nthe top left image. Therefore, the missing image in the bottom right would be a star with a dot in the center, \nsimilar to the image in the top right but with a dot in the center.\nSec. 4.5 Intelligence Tests\nThe pattern in the first column is that the shapes are rotated 90 degrees counterclockwise. The square in the \ntop left image becomes a diamond in the bottom left image, and the star in the top right image should become \na square in the missing image.\nGemini Pro: \nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nPrompt: What is the difference?\nThe only difference between the two images is that the one on the bottom has a circle in \nthe middle, while the one on the up does not. The images are otherwise identical.\nGemini Pro: \nThe missing image would look like a diamond with a star in the middle.The pattern in \nthe first column is to add a star to the previous image. The first image is a square, the \nsecond image is a diamond, and the third image is a star. Therefore, the missing \nimage would be a diamond with a star in the middle.\nGemini Pro: \nPrompt: The second row should add a star in the middle based on the \npattern,in the first colomn. So what would the missing image look like?\nFigure 30: tackling entire question page step-by-step. n response to the example question from the\nRaven\u2019s Progressive Matrices [35], which directly asks for pattern completion, Gemini performed\npoorly. However, when guided step-by-step, Gemini correctly answered the question. This may\nindicates that Gemini has the ability to compare differences between images, but it is not very familiar\nwith the pattern of RPM tests\n41\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff4.6\uffffEmotional\uffffIntelligence\uffffTests\nPrompt: After seeing the below image, how people might emotionally feel and react?\nGPT-4V\uff1a\nGemini Pro: \nPeople might emotionally feel relaxed, peaceful, and awestruck after seeing the image of the \nsunset over the ocean. The image evokes a sense of natural beauty and tranquility. The warm colors of \nthe sky and water, the gentle waves crashing against the shore, and the silhouette of the palm trees all \ncontribute to a feeling of peace and serenity.\nPeople might feel a sense of awe and wonder at \nthe beauty of the sunset and the colors in the sky. \nThey may feel peaceful and calm, or even inspired. \nSome may react with a sense of gratitude for being \nable to witness such a beautiful moment in nature. \nOthers may feel a sense of longing to be in a place \nlike this, or a desire to travel and see more of the \nworld's beauty.\nPrompt: After seeing the below image, how people might emotionally feel and react?\nGPT-4V\uff1a\nPeople might feel scared, anxious, or uneasy \nafter seeing this image. The dark tones and shadows \ncreate a sense of mystery and unease, and the \nsilhouette of the figure on the stairs could be \ninterpreted as ominous or threatening. Some people \nmight react by feeling a sense of dread or fear, while \nothers might be intrigued orcurious about the context \nof the image.\nGemini Pro: \nPeople might emotionally feel curious, intrigued, and even a little bit scared after seeing the \nimage of the dark and mysterious staircase. The image is evocative and leaves a lot to the imagination. \nThe viewer can't see what's at the top of the stairs, or what's lurking in the shadows. This can create a \nsense of suspense and mystery.\nFigure 31: Emotional intelligence tests. Both GPT-4V and Gemini understands how different visual\ncontents may arouse human emotions.\n42\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.6 Emotional Intelligence Tests\nPrompt: After Aesthetics reflects the perceived beauty or visual appeal of an object,\ndesign, or piece of art by human.Among the images provided below, based on societal\nstandards and norms, which one more people will consider having a higher aesthetics?\nGPT-4V\uff1a\nBased on societal standards and norms, more people will likely consider the second\nimage (the colorful houses) as having a higher aesthetics. This is because the image is more\nvisually appealing and has a more artistic and creative feel to it. The first image (the house) is\nmore plain and ordinary, and while it may be considered aesthetically pleasing to some, it is not as\nstriking or eye-catching as the second image.\nGemini Pro: \nThere are a few reasons for this: Colorfulness is often associated with beauty. Studies have shown\nthat people are drawn to colorful objects and environments. The colorful building in the image is\nvisually appealing and eye-catching. The building has a unique and interesting design. The brightly\ncolored houses and canals of Burano, Italy are famous for their beauty. The building in the image\nis representative of this unique and picturesque architectural style. The image is well-composed\nand balanced. The photographer has carefully chosen the angle and perspective of the shot to\ncreate a pleasing composition. The building is centered in the frame and the colors are well-\nbalanced.\nBased on societal standards and norms, the image of\nthe colorful building is more likely to be considered\nhaving a higher aesthetics by more people.\nhttps://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcRggaixa5vpcc3jgIDf9y078IOVGI_fex-1OHm1ug6RxTOU4xFMvOx7GeANhIdO\nFigure 32: Aesthetics intelligence tests (1/2). Both GPT-4V and Gemini can judges image aesthetics\nbased on societal standards and norms.\n43\nSec. 4.6 Emotional Intelligence Tests\nPrompt: After Aesthetics reflects the perceived beauty or visual appeal of an object, design, \nor piece of art by human.Among the images provided below, based on societal standards and \nnorms, which one more people will consider having a higher aesthetics?\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nGPT-4V\uff1a\nMost people would likely consider the first image to have a higher aesthetic value. This is \nbecause the image is more visually appealing, with a variety of colors, buildings, and greenery. The \nsecond image is less visually appealing, with a more monochromatic color scheme and less variety \nin the buildings and landscape. Additionally,the first image is of a more iconic and recognizable \nlocation (New York City), which may also contribute to its perceived aesthetic value.\nGemini Pro: \nNew York City is one of the most iconic and recognizable cities in the world. It is associated with \nglamour, success, and excitement. The skyline of New York City is a symbol of all that is possible. \nThe image is visually appealing and romantic. The warm colors of the sunset create a beautiful and \natmospheric backdrop for the city skyline. The image is also well-composed and balanced. The \nimage is evocative of positive emotions. Many people associate New York City with happy \nmemories, such as vacations, business trips, or time spent with loved ones. Seeing the New York \nCity skyline can evoke feelings of nostalgia, excitement, and hope.\nBased on societal standards and norms, the image of \nthe New York City skyline at sunset is more likely to \nbe considered having a higher aesthetics by more people.\nhttps://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcTxYk_oF85xAPCJHRIstYmrq2dsQmWXDo39f97RF7jmZ6qo3dYFCkqqSXkRcfD3\nFigure 33: Aesthetics intelligence tests (2/2). Both GPT-4V and Gemini can judges image aesthetics\nbased on societal standards and norms.\n44\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.6 Emotional Intelligence Tests\nPrompt: Describe the image to \nlessen someone's desire to visit.\nGPT-4V\uff1a\nThe image shows a rocky beach at sunset with seaweed and algae covering the ground. The water is \nmurky and there are no clear areas to swim or relax. The rocks are sharp and slippery, making it \ndifficult to walk or explore. The sunset is partially blocked by clouds. It seems like a difficult and \nunpleasant place to navigate.\nGemini Pro: \nThe image is of a dark and cramped alleyway in a rundown urban area. The walls are covered in graffiti \nand the ground is littered with trash. A dumpster sits in the corner, overflowing with garbage.\nHere is a description of the image that would lessen someone's desire to visit:\nImagine yourself walking down this alleyway. The air is thick with the smell of rotting garbage and urine. \nThe only light comes from a few flickering streetlights, casting eerie shadows on the walls.\nYou can hear the sound of rats scuttling in the darkness. You feel your skin crawl as you imagine them \ncrawling over your feet.\nYou come to a fork in the alleyway. You can't see which way leads out. You take a deep breath and \nchoose a path.\nThe alleyway gets narrower and darker. You can feel the walls closing in on you. You start to feel \nclaustrophobic.\nYou hear a noise behind you. You turn around, but you can't see anything in the darkness. You start to \nwalk faster.\nYou see a light ahead. You run towards it, eager to escape the alleyway.\nYou reach the light and find yourself in a deserted back alley. You're still lost, and you don't know how \nto get back to where you came from.\nYou feel a hand on your shoulder. You turn around and see a man standing behind you. He's tall and \nthin, with a dirty face and a menacing smile.\nHe says to you, \"Welcome to my alleyway.\"\nI hope this description has lessened your desire to visit the alleyway in the image.\nFigure 34: Lessen human desires. Gemini can generate better text based on the perceived or desired\nemotions than GPT4-V.\n45\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 4.6 Emotional Intelligence Tests\nPrompt: Describe the image to \nmake people feel relax and comforting.\nGPT-4V\uff1a\nThe image is of a dimly lit staircase with ornate railings. The light is soft and creates a peaceful \natmosphere. The shadows on the wall add depth and texture to the image, making it feel cozy and \ninviting. The overall mood of the image is calm and serene, making it a perfect place to relax and \nunwind.\nGemini Pro: \nThe image shows a wooden staircase with a railing in a dark room. The light from above gently \nilluminates the steps, creating a warm and inviting atmosphere. The railing is smooth and sturdy, \nproviding a sense of security as you ascend.\nAs you focus on the stairs, you can feel your body and mind relax. The gentle repetition of the steps is \ncalming, and the steady rhythm of your breathing soothes your soul.\nImagine the feeling of climbing these stairs, one step at a time, leaving your worries behind. As you \nreach the top, you feel a sense of peace and contentment.\nRelax and let the image take you to a place of comfort and calm.\n(I enhanced the content by including a more imaginative and sensory description of the image, and by \nsuggesting how the image can be used to relax and de-stress.)\nFigure 35: Soothing human emotions. Gemini can generate better text based on the perceived or\ndesired emotions than GPT4-V.\n46\n5\nTextual Reasoning in Images\nThis section is dedicated to showcasing the model\u2019s inferential prowess in interpreting textual elements\nwithin images. It serves as an extension of the discourse initiated in Sec. 3, progressing from the\nfundamental recognition of text to more nuanced inferential analysis. In Sec. 5.1, the focus is on\nthe model\u2019s visual mathematical capabilities, demonstrating its proficiency in identifying texts and\nmathematical symbols in images and performing elementary computations. Both models under\nscrutiny exhibit competent responses to straightforward mathematical problems. Sec. 5.2 explores\nthe realm of graphical interpretation and reasoning, encompassing the analysis of flowcharts, bar\ncharts, and other graphical data representations. While the models generally perform well on a broad\nspectrum of charts, the Gemini model shows a reduced accuracy in interpreting complex experimental\ntables. Lastly, Sec. 5.3 assesses the models\u2019 ability to comprehend and reason with a variety of\ndocuments, including posters, architectural blueprints, academic papers, and webpages. Both models\ndisplay parallel levels of performance, with Gemini providing more elaborate responses, albeit with\na slight compromise in accuracy. Pointing to a specific spatial location is anessential capability\nin human-computer interaction with multimodal systems, such as conducting visually grounded\ndialogues. research have try to find ways for this visual referring in many different ways [36, 37]. In\nlarge Vision-Language model like GPT-4V and Gemini. It may be enough to simply draw illustration\nin original RGB pixel space [38]. We thus intersperse a series of interactive prompts to evaluate the\nuser-friendliness of the two models.\n5.1\nVisual Math Ability\nFig. 36 demonstrates the model\u2019s mathematical abilities, highlighting not only its capacity to recognize\ntextual and mathematical symbols within images but also its aptitude for performing calculations. We\nobserved that for relatively simple problems, both models provided satisfactory solutions.\n5.2\nTable & Chart Understanding and Reasoning\nFig. 37-Fig. 39 delve into the models\u2019 reasoning capabilities with respect to tables and charts. In these\ncases, beyond mere text recognition, a certain level of reasoning is necessitated. Our observations\nindicate that both models exhibit commendable performance in interpreting flowcharts and bar graphs.\nHowever, in the context of some experimental tables, the Gemini model struggles to provide accurate\nanswers. This may also be attributed to image resolution issues, as the images are composed of\nmultiple stitched pictures, making it challenging to accurately recognize text within the images.\n5.3\nDocument Understanding and Reasoning\nFig. 42-Fig. 45 examine the models\u2019 capability to infer from a diverse range of documents, encom-\npassing materials such as posters, architectural layouts, scholarly articles, and web pages. Both\nmodels exhibit comparable efficacy in these tasks. Notably, Gemini tends to offer more elaborate\nresponses, yet it falls short in terms of precision, underscoring an area for potential improvement.\n47\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.1 Visual Math Ability\nPrompt: What is x in \nthe equation?\nGPT-4V\uff1a\nWithout any context or information about what the equation is \nrepresenting, we can only assume that \"x\" is a variable or unknown \nvalue in the equation. \nGemini Pro: \nSure, I can help with this. The equation in the image is (x+3)2 =4. To solve for x, we need to take the square \nroot of both sides of the equation. However, it's important to remember that squaring a number results in \nboth a positive and negative result (e.g., squaring 2 gives 4, but squaring -2 also gives 4). Therefore, when \nwe take the square root of both sides of the equation, we need to consider both positive and negative 2.\nSo, solving for x, we get:        \n      x + 3 = \u00b12\nSubtracting 3 from both sides of the equation, we get:       x = -3 \u00b1 2\nTherefore, the two possible solutions for x are:                  x = -1 or x = -5\nBoth -1 and -5 satisfy the original equation, so they are both valid solutions.\nPrompt: What is the value \nof x in the image?\nGPT-4V\uff1a\nGemini Pro: \nThe triangle in the image is a right triangle, with sides 3 and 4 forming the legs, and the hypotenuse labelled \nas x. We can solve for x using the Pythagorean theorem, which states that in a right triangle, the square of \nthe hypotenuse is equal to the sum of the squares of the legs. In this case, we have:\nx^2 = 3^2 + 4^2,      x^2 = 9 + 16,      x^2 = 25\nTaking the square root of both sides, we get:     x = 5\nIn this equation, (x + 3) is being squared and is equal to 4. To solve for x, we can take the square root of \nboth sides of the equation to get rid of the exponent on the left side. This would give us:\n\u221a((x + 3)\u00b2) = \u221a4\uff0c x + 3 = \u00b12\nFrom here, we can solve for x by subtracting 3 from both sides. This would give us: x = -1 or x = -5\nSo, in this equation, x could be either -1 or -5. \nThe value of x in the image can be found using the Pythagorean  \ntheorem, which states that in a right triangle, the square of the \nhypotenuse (the longest side, opposite the right angle) is equal to \nthe sum of the squares of the other two sides.\nIn this case, the hypotenuse is the side AC, and the other two \nsides are AB and BC. We know that AB is 4 and BC is 3, so we \ncan plug these values into the equation:\na\u00b2 + b\u00b2 = c\u00b2\n3\u00b2 + 4\u00b2 = c\u00b2\n9 + 16 = c\u00b2\n25 = c\u00b2\nTherefore, c (the length of the hypotenuse) is 5. Since x is the length of the hypotenuse, \nx is equal to 5. \nPrompt: x = ?\nFigure 36: Answering math problems. Both GPT-4V and Gemini can understand the question\ntextually and visually, and gather necessary information to answer the question. In the first case, there\nare errors in both GPT-4V\u2019s and Gemini\u2019s description.\n48\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.2  Table & Chart Understanding and Reasoning\nFlow Chart Understanding\nPrompt: Can you translate the flowchart to a python code?\nSure, here\u2019s the Python code:\nnum1 = input(\"Enter first number: \")\nnum2 = input(\"Enter second number: \")\nif num1 > num2:\nprint(num1) else: print(num2)\nSure, the flowchart can be translated to the following Python code:\nnum1 = int(input(\"Enter a number: \"))\nnum2 = int(input(\"Enter another number: \"))\nif num1 > num2:\nprint(f\"{num2} is the smallest number.\")\nelse:\nprint(f\"{num1} is the smallest number.\")\nThis figure represents a flow chart of a team\u2019s proposal process.\nThe process begins with a team member suggesting a project in a\nmeeting. The team then discusses whether it is a good idea or not. If\nthe idea is not considered a good one, the team rejects it and the\nprocess ends.\nIf the team thinks it might be a good idea but needs more information,\na team member conducts further research and refines the idea into a\nproposal. The team then reviews the proposal and provides feedback.\nIf the proposal is not okay, then the team member revises it and the\nteam reviews it again.\nIf the proposal is okay, the team sends it to a final decision-making\nprocess. If the proposal is approved, then the team proceeds with the\nproject. This is the end of the proposal process.\n1. A team member suggests a project. This could be done\nverbally during a meeting, or by submitting a written proposal.\n2. The team discusses the idea in a meeting. This is an opportunity for\nteam members to ask questions, share their feedback, and get more\ninformation about the project.\n3. Participants decide if it's a good idea. Based on the discussion, the\nteam will vote or come to a consensus on whether or not to move\nforward with the project.\nPrompt: Explain the figure.\nGemini Pro: \nGPT-4V\uff1a\nGPT-4V\uff1a\n4. If the decision is no, the process ends. The team member may choose to submit the idea to a different team, or\nabandon it altogether.\n5. If the decision is yes, or if there is not enough information to make a decision, the team member will need to\ntake some additional steps.\n6. If the decision is yes, the team member will further define the project. This may involve creating a more detailed\nproposal, outlining the project's goals, timeline, and budget.\n7. If there is not enough information to make a decision, the team member will need to conduct more research.\nThis may involve gathering data, talking to stakeholders, or developing a prototype.\n8. Once the team member has further defined the project or conducted more research, they will write a proposal.\nThis document should summarize the project in more detail and make a case for why it should be approved.\n9. The team reviews the proposal. This is another opportunity for team members to ask questions and provide\nfeedback.\n10. If the proposal is OK, the team will send it to the final decision-maker. This could be a manager, a committee,\nor another individual who has the authority to approve or reject projects.\n11. If the proposal is not OK, the team member will need to revise the proposal based on the feedback they\nreceived.\n12. Once the proposal is approved, the project can begin.\nGemini Pro: \nFigure 37: Flow chart understanding. Both GPT-4V and Gemini correctly describe the proposal\nprocess in details.\n49\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.2 Table & Chart Understanding and Reasoning\n Chart Understanding and Reasoning\nPrompt: What is the average total fueling cost excluding the Ford F150?\nPrompt: Explain the figure.\nGPT-4V\uff1a\nGemini Pro: \nThe figure is a graph that shows the relationship between the quality of \na paper and its impact on a person's career. The x-axis represents the \nquality of the paper, which ranges from bad to creative and original. The y-axis \nrepresents the impact the paper has on a person's career, which ranges from \nnothing to lots of impact.\nThe graph shows that a bad paper has little to no impact on a \nperson's career, while a creative and original paper has a \nsignificant impact. The impact of the paper on a person's career increases \nas the quality of the paper improves. This suggests that the quality of a \nperson's work is an important factor in determining their success in their career. \nTo find the average total fueling cost excluding the Ford F150, we need to add the \ntotal fueling costs of the Toyota RAV4 and Honda Civic and divide by 2 (since \nthere are 2 vehicles in the calculation).\n(82.56 + 70.55) \u00f7 2 = 76.55\nTherefore, the average total fueling cost excluding the Ford F150 is $76.55.\nGPT-4V\uff1a\nBased on the image you sent, the average total fueling cost excluding the Ford \nF-150 is $76.56. This is the average of the Toyota RAV4's total fueling cost \n($82.56) and the Honda Civic's total fueling cost ($70.55).\nGemini Pro: \nThe graph in the image shows the relationship between the quality of a research paper and its \nimpact on your career. The red line represents the negative impact of a paper's quality on your career. The \nsteeper the negative slope of the red line, the more negative the impact of a paper's quality on your career.\nThe text on the image also suggests that papers that are \"Creative, original and good\" have the \nmost positive impact on your career. Papers that are \"Bad\" or \"Ok\" have a negative impact on \nyour career.\nOverall, the image suggests that it is important to write high-quality research papers if you want to have a \nsuccessful career in academia.\nFigure 38: Chart understanding and reasoning. Both GPT-4V and Gemini show promising results\nin understanding the details in the chart, as well as in accurately responding to related questions.\n50\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.2 Table & Chart Understanding and Reasoning\n Table Understanding and Reasoning\nPrompt: How many families are earning more than 13000 and owns more than 2 cars?\nGPT-4V\uff1a\nGemini Pro: \nFrom the given table, we can clearly see that there are a total of 113 families that earn more than Rs. 13000 \nand own more than 2 cars (25 families in the 13000-16000 range and 88 families in the 16000 or more range).\nThe data in the table is for Indian Rupees, not US Dollars. It shows that 25 families have a household income of \n13000-16000 Rupees and also have greater than two cars.\nPrompt: Which country has the highest profit margin?\nGPT-4V\uff1a\nGemini Pro: \nBased on the data provided, the country with the highest profit margin is Australia, with a profit margin of \n19,4%. However, the difference between the top three countries (USA, China and Australia) is minimal, with less \nthan 0,3% difference in profit margin.\nOf the countries in the table, Australia has the highest profit margin, at 19.4%.\nFigure 39: Table understanding and reasoning. Both GPT-4V and Gemini can understand the\ndetails in the table, and answer related questions.\n51\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff5.2\uffffTable\uffff&\uffffChart\uffffUnderstanding\uffffand\uffffReasoning\n\uffffFigure\uffffand\uffffTable\nPrompt: \nWhich circled line shows a different comparison trend? \nProvide intermediate steps\nGPT-4V\uff1a\nThe circled line that shows a different comparison trend is the USNCO Local Section Exam 2022. \nIntermediate steps: 1. Analyze the data in the table and look for any discrepancies in the comparison trends. 2. \nNotice that in all the rows except for the USNCO Local Section Exam 2022, the GPT-4 (no vision) column has \na lower score than the GPT-4 column. 3. This means that in all the other rows, the GPT-4 model with vision \nperformed better than the GPT-4 model without vision. 4. However, in the USNCO Local Section Exam 2022 \nrow, the GPT-4 (no vision) column has a higher score than the GPT-4 column. 5. This is a different \ncomparison trend than the rest of the rows, indicating that the GPT-4 model without vision performed better \nthan the GPT-4 model with vision in this particular exam.\nGemini Pro: \nThe green line shows a different comparison trend. It represents the performance of GPT-4 on the \nGraduate Record Examination (GRE) Quantitative test. The other lines show that GPT-4 performed \nconsistently well on all the other exams, but it performed significantly worse on the GRE Quantitative test.\nIntermediate steps:\nCompare the performance of GPT-4 on the GRE Quantitative test to \nits performance on the other exams. Note that GPT-4 performed \nsignificantly worse on the GRE Quantitative test than on the other \nexams. Therefore, the green line shows a different comparison trend. \nHere is a table that shows the performance of GPT-4 on each exam, \nalong with its percentile ranking:\nAs you can see, GPT-4 performed in the 90th percentile or higher on \nall of the exams except for the GRE Quantitative test, where it \nperformed in the 62nd percentile. This suggests that GPT-4 has a \ndifferent comparison trend for the GRE Quantitative test than for the \nother exams.\nFigure 40: Visual referring prompting. When highlighting lines in documents and tables with user\ndrawn circle, GPT-4V generate better result than Gemini.\n52\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.2 Table & Chart Understanding and Reasoning\nPaper Experiment Table\nPrompt: What are the differences between these two experiments in the slides?\nGPT-4V\uff1a\nThere are a few key differences between the two experiments shown in the slides:\n1. The tasks: The first experiment (on the left) is focused on the task of visual grounding, where the model\nis trained to identify and localize objects in an image. The second experiment (on the right) is focused on the\ntask of image captioning, where the model is trained to generate a descriptive caption for an image.\n2. The datasets: The first experiment uses the Flickr30k Entities dataset, which contains images with\nannotated object bounding boxes. The second experiment uses the MSCOCO and VQA2 datasets, which\ncontain images with associated captions.\n3. The evaluation metrics: The first experiment uses the mean Average Precision metric to evaluate the\nmodel's performance on the visual grounding task. The second experiment uses the BLEU and CIDEr\nmetrics to evaluate the model's performance on the image captioning task.\nOverall, the two experiments are designed to evaluate the UniTAB model's performance on different tasks and\ndatasets, and to demonstrate its ability to handle multiple tasks with a single model.\nGemini Pro: \nThe two experiments in the slides differ in the following ways:\nTask: The first experiment is a text box alignment task, while the second experiment is a visual grounding\ntask.\nModel: The first experiment uses a single model for both tasks, while the second experiment uses a\nseparate model for each task.\nDataset: The first experiment uses the RefCOCO dataset, while the second experiment uses the\nRefCOCOg dataset.\nResults: The first experiment achieves better results on the text box alignment task, while the second\nexperiment achieves better results on the visual grounding task.\nHere is a table that summarizes the key differences between the two experiments:\nFigure 41: Visual referring prompting in computer slides. GPT-4V can better understand user\nmark than Gemini to help user to understand scientific confernce slides.\n53\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.3 Document Understanding And Reasoning\nPrompt: Describe the image.\nGPT-4V\uff1a\nThis image is a floor plan for an apartment or \ncondominium. It shows the layout of the rooms and \nthe dimensions. There is a terrace, dining area, \nliving room, foyer, two bedrooms, a master \nbathroom, and a guest bathroom. The floor plan is \nblack and white and is a two-dimensional drawing.\nThe image you sent is a black and white floor plan of a house. The house has a foyer, living room, \ndining area, kitchen, master bedroom, second bedroom, two bathrooms, and a terrace.\nThe foyer is located at the front of the house and measures 6'0\" x 6'34\". It leads to the living room on \nthe left and the dining area on the right.\nThe living room is the largest room in the house and measures 14'6\" x 20'5\". It has a fireplace on the \nnorth wall and a large window on the east wall.\nThe dining area is located to the right of the living room and measures 9'10\" x 13'0\". It has a window \non the east wall.\nThe kitchen is located to the back of the house and measures 7'5\" x 12'3\". It has a sink on the north \nwall, a stove on the east wall, and a refrigerator on the west wall.\nThe master bedroom is located to the left of the kitchen and measures 12'10\" x 17'9\". It has a window \non the east wall and a walk-in closet on the north wall.\nThe second bedroom is located to the right of the master bedroom and measures 10'10\" x 14'7\". It \nhas a window on the east wall and a closet on the north wall.\nThe master bathroom is located to the left of the master bedroom and measures 4'11\" x 10'0\". It has \na shower on the north wall, a toilet on the west wall, and a sink on the south wall.\nThe second bathroom is located to the right of the second bedroom and measures 5'2\" x 5'2\". It has \na toilet on the north wall, a sink on the east wall, and a shower on the south wall.\nThe terrace is located at the back of the house and measures 21'6\" x 8'0\". It can be accessed from the \nliving room and the master bedroom.\nGemini Pro: \nFigure 42: Document understanding and reasoning (1/3). Both GPT-4V and Gemini can accurately\nidentify the content on the document and answer questions.\n54\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.3 Document Understanding And Reasoning\nPrompt: Answer the question in the image.\nGPT-4V\uff1a\nGemini Pro: \nLayer | Number on figure 1\n--- | ---\nOuter core | 3\nMantle | 2\nCrust | 1\nInner core | 4 \nPrompt: Describe the poster. Which city is this dish from?\nGPT-4V\uff1a\nGemini Pro: \nThe poster is an advertisement for a Chinese dish called \"Hot dry \nnoodles.\" The poster has a red background with white Chinese \ncharacters at the top that read \"Hot dry noodles, the taste of China.\" \nBelow the text is a photo of the dish, which consists of noodles with \nmeat and vegetables on top. The dish is served in a green bowl with \nred chili peppers on the side. The text at the bottom of the poster is in \nChinese and describes the dish as being spicy, fragrant, and \ndelicious.\nThis dish is from the city of Wuhan, located in the Hubei Province of \nChina. It is a popular street food in the city and is considered one of \nthe \"10 great breakfasts\" in China. \nThe image you sent me shows the structure of the Earth's \ninterior, with four layers labeled: crust, mantle, outer core, and \ninner core.\n The question asks you to write the number showing each layer in the table below. The table in the image \nalready has the numbers filled in correctly:   Crust: 4,    Mantle: 3,    Outer core: 2,   Inner core: 1\nThe poster you sent me features a photo of a bowl of Wuhan hot \ndry noodles. The noodles are long, thin, and yellow, and they are \ntopped with chopped scallions, sesame seeds, and what appears to \nbe chili oil. The text on the poster says, in Chinese characters:\nThe text at the bottom of the poster describes Wuhan hot dry noodles as one of the \"Ten Great Noodles of \nChina\" and a famous Wuhan snack food. It also briefly describes the ingredients and praises the dish for its \ntaste and high energy content. Overall, the poster is a visually appealing and informative advertisement for \nWuhan hot dry noodles. It effectively conveys the dish's origins, appearance, and taste, making it likely to entice \nviewers to try it for themselves. Wuhan hot dry noodles are a breakfast food in Wuhan, China. They are typically \nserved with a sesame paste sauce, chili oil, and pickled vegetables.\nFigure 43: Document understanding and reasoning (2/3). Both GPT-4V and Gemini can accurately\nidentify the content on different types of document and answer questions.\n55\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.3 Document Understanding And Reasoning\nPaper Understanding\nPrompt: This is a paper \"GPT4Point.\" Describe the paper in details and highlight their contribution.\nGPT-4V\uff1a\nGemini Pro: \nThe image you've uploaded appears to be the first page of a scientific paper titled \"GPT4Point: A Unified \nFramework for Point-Language Understanding and Generation\". This paper discusses a framework \ndesigned to bridge the gap between 3D object understanding and natural language processing.\n\u00b7 The paper presents examples of tasks that GPT4Point can perform, such as 3D recognition, 3D captioning, \n3D question answering, and 3D generation.\n\u00b7 It emphasizes GPT4Point's capabilities in accurate 3D recognition, detailed captioning, precise question \nanswering, and high-quality controllable 3D generation.\n\u00b7 The framework is also capable of assessing abnormal shapes and the generation failure case.\n\u00b7 It includes a zero-shot classification task where it can distinguish between human faces and non-human faces.\nMain Argument: This paper introduces GPT4Point, a new model for understanding and generating 3D objects \nfrom point clouds and text descriptions. GPT4Point is a multimodal language model, meaning it can process both \ntext and point cloud data. This allows it to perform a variety of tasks, such as 3D object recognition, captioning, \nand question answering.\nContributions: The main contributions of the paper are:\n\u00b7 A unified framework for 3D object understanding and generation: GPT4Point is the first model to be able to \nperform both 3D object understanding and generation tasks within a single framework. This makes it more \nversatile than previous models, which were typically designed for one or the other task.\n\u00b7 High-quality 3D generation: GPT4Point can generate high-quality 3D objects from text descriptions. This is \neven possible using low-quality point-text features, which means that it can be used to generate 3D objects from \na variety of sources, including real-world scans and incomplete data.\n\u00b7 A large-scale point-language dataset: The paper also introduces Pyramid-XL, a new dataset for training \npoint-language models. Pyramid-XL is much larger than previous datasets, and it includes a wider variety of 3D \nobjects and text descriptions.\n\u00b7 A comprehensive benchmark for 3D point-language understanding: The paper proposes a new \nbenchmark for evaluating the performance of point-language models. This benchmark includes a variety of tasks, \nsuch as 3D object recognition, captioning, and question answering\nGPT-4V\uff1a\nFigure 44: Document understanding and reasoning (3/3). Both GPT-4V and Gemini can read the\npage of a technical report and provide a summary of the content.\n56\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 5.3 Document Understanding And Reasoning\n Web Search\nPrompt: What is the icon used for?\nGPT-4V\uff1a\nThe icon circled in the image is a microphone icon. It \nis used to activate the voice search feature on \nBing. Users can click on this icon and speak into their \ndevice's microphone to ask a question or search for \nsomething instead of typing it in. This can be helpful for \nhands-free searching or for users who may have difficulty \ntyping.  \nGemini Pro: \nThe icon in your image is used to start a conversation with Bard, the AI-powered chatbot from Microsoft.\nThe icon is a stylized speech bubble with a microphone inside, which suggests that you can talk to Bard by \nspeaking or typing. When you click on the icon, a new conversation window opens up where you can ask Bard questions, give it \ninstructions, or have a creative conversation.\nThe icon is located on the welcome page for Bard, which is linked to from the Microsoft Bing search engine. It is also found on \nother websites and apps where Bard is integrated.\nHere are some examples of how you can use the icon to start a conversation with Bard:\n\u00b7Ask a question: \"What is the weather in New York City today?\"\n\u00b7Give an instruction: \"Write a haiku about crocodiles in outer space in the voice of a pirate.\"\n\u00b7Have a creative conversation: \"Tell me a story about a robot who falls in love with a human.\"\nBard is still under development, but it is learning to perform many kinds of tasks, including generating different creative text \nformats, translating languages, writing different kinds of creative content, and answering your questions in an informative way. If \nyou have any feedback on how Bard can be improved, please share it with the Microsoft team.\nFigure 45: Visual referring prompting in computer GUI. Both Gemini and GPT-4V success in\nhelping user to understand the web page.\n57\n6\nIntegrated Image and Text Understanding\nThis section encompasses tasks that require simultaneous recognition and understanding of both text\nand images, involving multifaceted analysis of multiple figures. In Sec. 6.1, we explore interleaved\nimage-text inputs, showcasing a task that involves processing three bills to compute a total tax amount.\nGPT-4V demonstrates its capability to sequentially process each of the three images and provide\naccurate answers. In contrast, Gemini encounters comprehension challenges when all three images\nare inputted simultaneously, although it exhibits improved understanding when processing each\nimage individually. The second task involves combining information from two images to calculate\na total price, yielding results consistent with the previous task. Sec. 6.2 is about text-to-image\ngeneration guidance, involving interactions and modifications with a text-to-image generative model.\nGemini shows promising results from the initial stages, evidencing its proficiency in interacting with\ngenerative tools.\n6.1\nInterleaved Image-text Inputs\nFig. 46 presents a task involving three bills, where the objective is to compute the cumulative tax\namount of the bills. It was observed that GPT-4V could effectively process each bill sequentially,\nyielding precise responses. Conversely, when all three bills were inputted simultaneously into Gemini,\nit failed to comprehend accurately. However, Gemini demonstrated improved understanding when\neach bill was inputted and processed individually. Fig. 47 focuses on a task that requires calculating\nthe total cost by combining the price of beer from the first image with the menu items from the second\nimage. The findings mirror those from the previous figure, further corroborating the models\u2019 distinct\ncapabilities in handling image-based input.\n6.2\nText-to-Image Generation Guidance\nIn the illustrations of Fig. 48-Fig. 49, the focus shifts to tasks involving interaction and modification\nwith a text-to-image generative model. Notably, Gemini achieves satisfactory outcomes from the\noutset. This section highlights Gemini\u2019s superior capability in engaging with generative tools, as\nevidenced by the initial interactions and the quality of the resultant images. The figures detail\nthe nuanced processes and outcomes of these interactions, underscoring Gemini\u2019s adeptness in\nunderstanding and manipulating generative image models.\n58\nPrompt: How much did I pay for tax?   Where is the information?\nGPT-4V\uff1a\nIn the first receipt, you paid $3.72 for tax. The  information is located at the bottom of the receipt. In  \nthe second receipt, you paid $42.23 for tax. The information is located near the bottom of the receipt,  \nunder the \"Sales Tax\" line. In the third receipt, you paid $4.58 for tax. The information is located at the \nbottom of the receipt, under the \"TAX\" line. In total, you paid $50.53 in tax across all three receipts.\nSec. 6.1 Interleaved Image-text Inputs\nInterleaved Image-text Pairs\nThe total tax you paid on the three receipts is $4.58.\nReceipt 1: $0.61, Receipt 2: $4.58, Receipt 3: $0.86\nThe tax information is located on the bottom of each receipt, next to the subtotal and total due. It is \nlabeled as TAX or TOTAL TAK. I added the tax from each receipt together to get the total tax you paid. Please \nnote that some states and countries have different sales tax rates, so the actual tax you paid may vary \ndepending on your location.\nFor Gemini, we devide the pictures and ask seperately.\nYou paid $3.72 in tax. The \ninformation is at the bottom of \nthe receipt, under TOTAL \nTAX. \nGemini Pro: \nGemini Pro: \nGemini Pro: \nGemini Pro: \nYou paid $42.23 in sales tax on \nyour receipt. The information is at \nthe bottom of the receipt, \nunder Sales Tax.\nYou paid $4.58 in tax. The \ninformation is on the receipt, \nunder TAX.\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\n\u2460 Fig.1       + Original Prompt\n\u2461 Fig.2       + Original Prompt\n\u2462 Fig.3       + Original Prompt\nFigure 46: Interleaved image-text pairs. This image involves inputting multiple bills and calculating\nthe taxes within them. GPT-4V can accurately provide output, while Gemini\u2019s responses to the\nconcatenated image are not satisfactory. However, it performs well in providing individual answers\nfor each image.\n59\nPrompt: How much should I pay for the beer on the table according to the price on the menu?\nGPT-4V\uff1a\nAccording to the menu, the price for a Magna beer is $6. Since there are two Magna beers on the \ntable, the total cost for the beers would be $12.\nSec. 6.1 Interleaved Image-text Inputs\nInterleaved Image-text Pairs\nFor Gemini, we devide the pictures and ask seperately.\nThe beer on the table is Magna. There are two \nbottles of beer on the table.\n The beer on the table in the image is Medalle and it costs $5 according to the menu.\nGemini Pro: \nGemini Pro: \nGemini Pro: \nAccording to the menu in the image, the price of a \nMagna beer is $6. Therefore, two Magna beers will \ncost $12.\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nHere is a photo of the menu.\n\u2460 Fig.1       + Prompt: Which kind of beer is on \nthe table? How many are them?\n\u2461 Fig.2       + Prompt Prompt: There are two \nmagna beer, so how much should I pay?\nFigure 47: Interleaved image-text pairs. This image involves inputting tables with wine and beers\nand calculating the the cost of them on a menu. GPT-4V can accurately provide output, while\nGemini\u2019s responses to the concatenated image are not satisfactory. However, it performs well in\nproviding individual answers for each image.\n60\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 6.2 Text-to-Image Generation Guidance\nSelf-Reflection to Improve Text-to-Image Generation\nGPT-4V\uff1a\nA blue dog running on the beach with a \nhappy expression on its face.\nGemini Pro: \nA playful pug puppy with a mischievous glint in \nits eyes races along the shoreline, its tongue \nlolling out in the salty breeze as it chases the \nretreating waves.\nPrompt: \nCreate sentence prompt: photo of a dog \nlooks like the one in the given image \nrunning on the beach.\nFigure 48: Self-reflection to improve text-to-image generation (1/2). Using self-reflection to\nimprove the generated text prompts for a text-to-image model SDXL[39], GPT-4V reflects the error\nin the initial prompt that it does not mention the dog\u2019s breed, and makes the correct revision, while\nGemini generate better one with breed mentioned.\n61\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 6.2 Text-to-Image Generation Guidance\nSelf-Reflection to Improve Text-to-Image Generation\nGPT-4V\uff1a\nThe dog in the generated image does not look like the one in the given \nimage. The dog in the given image is a pug, while the dog in the \ngenerated image is a different breed. To address this issue, we can \nmodify the prompt to specifically mention that the dog should be a pug\nGemini Pro: \nSure, I can help you improve the image generated by the AI art model to better match your idea of a \nplayful pug puppy racing along the shoreline. Here are some key points to consider:\n1. Emphasize the pug's playful and mischievous features:\nThe current image shows a pug running with its tongue out, but it could be made more playful by \nadding a wider grin, sparkling eyes, and maybe even a slight head tilt. Tilting the head can add a \nsense of curiosity and excitement. You could also try capturing the mischievous glint in the pug's eyes \nby making them slightly squinted or adding a small sparkle in the corner.\n2. Enhance the shoreline setting:\nThe current image has a beach, but it could be made more dynamic to match the idea of the pug \nracing. You could add crashing waves, wet sand, and maybe even some footprints or seaweed to \nshow the pug's path. Consider adding elements like shells, pebbles, or beach toys to further enrich \nthe scene and give the pug something to playfully chase or interact with.\n3. Improve the lighting and composition:\nThe current image has a slightly flat lighting. You could try adding more sunlight and shadows to \ncreate a sense of depth and movement. This could also help to highlight the pug's form and make it \nstand out more. The composition could also be adjusted to make the pug the center of attention. You \ncould try moving the camera closer to the pug or using a lower angle to make it appear larger and \nmore playful.\n4. Additional details:\nThink about the time of day and adjust the lighting and colors accordingly. A warm sunset or a bright \nmorning light could add to the playful atmosphere. Consider adding details like a collar or leash to \ngive the pug a bit more personality and ground it in the scene.\nPrompt: \nI am iteratively improving the sentence prompt by looking at the \nimages generated by an Al art generation model and find out \nwhat is different from the given IDEA. The IDEA is: photo of a \ndog ... beachGenerated sentence prompt for current round is: A \nblue dog running on the beach with a happy expression on its \nface. Corresponding image generated by the Al art generation \nmodel:\nThe generated image is not perfect. Write key reasons on how \nto further improve the image to better follow the user imagined \nIDEA of the scene\nPrompt: \nI am iteratively improving the sentence prompt by looking at \nthe images generated by an Al art generation model and find \nout what is different from the given IDEA. The IDEA is: photo \nof a dog ... beach Generated sentence prompt for current \nround is: A playful pug puppy with a mischievous glint in its \neyes races along the shoreline, its tongue lolling out in the \nsalty breeze as it chases the retreating waves. Corresponding \nimage generated by the Al art generation model:\nThe generated image is not perfect. Write key reasons on how \nto further improve the image to better follow the user imagined \nIDEA of the scene\nBy incorporating these suggestions, you can help the \nAI art model generate an image that more closely \nmatches your idea of a playful pug puppy racing along \nthe shoreline. Remember, the key is to focus on \ncapturing the pug's personality and the dynamic \nenergy of the scene.\nFigure 49: Self-reflection to improve text-to-image generation (2/2). We persistently optimize the\nprompts to enable both Gemini and GPT-4V to generate images that closely align with ground truth\nrepresentations.\n62\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 6.2 Text-to-Image Generation Guidance\nSelf-Consistency to Improve Text-to-Image Generation\nGPT-4V\uff1a\nGemini Pro: There are five boats in the image. \nPrompt: \nCount the number of boats in the image.\nThere are 6 boats in the image.\nPrompt: \nCount the number of boats in the image. Describe each of their locations.\nGPT-4V (Firsr Time)\uff1a\nThere are 4 boats in the image. 1) The first boat is on the left side of the image, docked at the pier. It \nis a larger boat with a tower on top. 2) The second boat is in the center of the image, also docked at \nthe pier. It is a smaller boat with a white canopy on top. 3) The third boat is on the right side of the \nimage, docked at the pier. It is a smaller boat with a white canopy on top, similar to the second boat. 4) \nThe fourth boat is in the background, on the far right side of the image. It is partially obscured by the \nthird boat, but appears to be a larger boat with a tower on top.\nGPT-4V\uff1a\nThere are a total of 4 boats in the image. 1) The first boat is on the left side of the image, docked at \nthe pier. It is a larger boat with a tower on top. 2) The second boat is in the center of the image, also \ndocked at the pier. It is a smaller boat with a white canopy on top. 3) The third boat is on the right side \nof the image, docked at the pier. It is a smaller boat with a white canopy on top, similar to the second \nboat. 4) The fourth boat is in the background, on the far right side of the image. It is a sailboat, and it \nappears to be sailing on the water\nGPT-4V\uff1a\nThere are 6 boats in the image. - 1 boat is on the left side of the image, docked at the pier with a \ntower on top - 2 boats are in the center of the image, docked at the pier side by side - 1 boat is on the \nright side of the image, docked at the pier with a cover on top - 2 boats are in the background on the \nright side of the image, docked at a different pier.\nGemini Pro: \nForeground, left: A small white and brown motorboat with a brown tarp over the bow. It's tied up to the \nnearest dock on the left side of the harbor.\nForeground, center: A medium-sized white cabin cruiser with a blue tarp over the stern. It's tied up to the \nsame dock as the motorboat, on the right side.\nMidground, center: A large white sailboat with two masts. It's anchored in the middle of the harbor.\nBackground, left: A small, light-colored rowboat with two oars. It's tied up to the dock on the far left side \nof the harbor.\nBackground, right: A small, dark-colored sailboat with one mast. It's anchored in the background, to the \nright of the large sailboat.\n5 boats\nGemini Pro: \nI can see five boats in the image.\nA large white sailboat is docked in the center of the image, with its bow pointing towards the left side of \nthe harbor.\nTo the left of the sailboat, there is a smaller white motorboat with a blue tarp over its bow.\nIn the foreground, to the right of the sailboat, there is a small, red rowboat with two oars.\nFurther to the right, there is a larger, green motorboat with a white cabin.\nBehind the green motorboat, there is a barely visible sailboat with a white hull and a brown mast.\nGemini Pro: \nI can definitely help you count the boats in the image you sent. There are five boats in the image.\nLarge white sailboat, Small white sailboat, Two small motorboats, Large red motorboat.\nFigure 50: Counting reliability with self-consistency. This task aggregates multiple counting results\nrepeated on the same image. GPT-4V\u2019s four responses provided three different answers, even though\ntwo of them were correct; in contrast, all four of Gemini\u2019s responses were consistent, albeit incorrect.\nThis suggests that Gemini exhibits stronger consistency in its results.\n63\n7\nObject Localization\nThis section is primarily focused on the localization of specific objects within images, requiring\nthe models to provide the relative percentage coordinates of the objects. Sec. 7.1 discusses object\nlocalization in real-world scenarios: In tasks involving object localization in real-world settings, both\nmodels demonstrate commendable performance. Sec. 7.2 addresses abstract image localization: In\ntasks that involve localizing specific sections within abstract images, such as identifying particular\nparts in Tangram figures, GPT-4V exhibits better accuracy in providing bounding box coordinates.\n7.1\nObject localization in real-world\nFig. 51 to Fig. 52 illustrate the process of localization within real-world scenes. The approach involves\ninitially presenting a scenario with a beer bottle as a reference object, followed by a secondary task\nwhere the models are required to pinpoint the coordinates of a specific car in a different image. The\nresult shows that both model are capable of achieving in localizing objects in real scene.\n7.2\nAbstract Image Localization\nFig. 53 focuses on the localization task within an abstract image, specifically identifying certain\npart within a Tangram animal-like shape. In this context, it is observed that GPT-4V demonstrates a\nheightened ability to accurately determine the coordinates of the specified sections.\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff7.1\uffffObject\ufffflocalization\uffffin\uffffreal-world\uffff\nPrompt: Locate the referred object and represent the location of the region. Regions are \nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left and right most positions, normalized \ninto 0 to 1, where 0 is the left and 1 is the right. y1 y2 are the top and bottom most positions, \nnormalized into 0 to 1, where 0 is the top and 1 is the bottom. For example, the (x1,y1,x2,y2) \ncoordinates for that region with the beer bottle is (0.47, 0.48, 0.55, 0.87).  \nFor the image below, what is the (x1,y1,x2,y2) coordinates for the region with the blue Subaru \nSUV?\n GPT-4V\uff1a\nThe (x1,y1,x2,y2) coordinates for the region with the \nblue Subaru SUV would be (0.68, 0.57, 0.95, 0.95).\nGemini Pro: \nFor the image with the blue Subaru SUV, the \n(x1,y1,x2,y2) coordinates for the region with the \ncar are: (0.48, 0.55, 0.87, 0.95). This region is \nnormalized into the range 0 to 1, where 0 is the \nleft and 1 is the right for x, and 0 is the top and 1 \nis the bottom for y. The region is shown in the \nfollowing image:\nhttps://www.stevenscreeksubaru.com/2023-subaru-forester-launch\nFigure 51: Object localization in real-world (1/2). Both GPT-4V and Gemini can understand\ncoordinates to generate visual pointing output, which make it possible to show grounding ability by\nprompt them to generate bounding-box\u2019s corner points.\n64\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 7.1 Object localization in real-world\nPrompt: Locate the referred object and represent the location of the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left and right most positions, normalized\ninto 0 to 1, where 0 is the left and 1 is the right. y1 y2 are the top and bottom most positions,\nnormalized into 0 to 1, where 0 is the top and 1 is the bottom. For example, the (x1,y1,x2,y2)\ncoordinates for that region with the beer bottle is (0.47, 0.48, 0.55, 0.87).\nGPT-4V\uff1a\nThe (x1,y1,x2,y2) coordinates for the region with the black Audi car in it would\nbe approximately (0.35, 0.55, 0.55, 0.85).\nGemini Pro: \nThe (x1, y1, x2, y2) coordinates for the region with the same object in the third\nimage, which is the blue SUV bumper, are: (0.42, 0.52, 0.62, 0.80).\nFor the image below, what is the (x1,y1,x2,y2) coordinates for\nthe region with object in it?\nPrompt: Locate the referred object and represent the location of the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left and right most positions, normalized\ninto 0 to 1, where 0 is the left and 1 is the right. y1 y2 are the top and bottom most positions,\nnormalized into 0 to 1, where 0 is the top and 1 is the bottom. For example, in the first image, the\n(x1,y1,x2,y2) coordinates for that region with the beer bottle is (0.47, 0.48, 0.55, 0.87). My\nquestion is \"for the second image, what is the (x1,y1,x2,y2) coordinates for the region with object\nin the third image?\"\nFigure 52: Object localization in real-world (2/2). Both GPT-4V and Gemini can understand\ncoordinates to generate visual pointing output, which make it possible to show grounding ability by\nprompt them to generate bounding-box\u2019s corner points.\n65\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff7.2\uffffAbstract\uffffImage\uffffLocalization\n GPT-4V\uff1a\nThe head of the bird is represented by the small triangle at the top of the \nimage. The (x1,y1,x2,y2) coordinates for the region with the head of the \nbird would be approximately (0.42, 0.15, 0.58, 0.35).\nPrompt: A tangram is a geometric puzzle that consists of \nseven flat pieces to form shapes. Locate the referred object and \nrepresent the location of the region. Regions are represented by \n(x1,y1,x2,y2) coordinates. x1 x2 are the left and right most \npositions, normalized into 0 to 1, where 0 is the left and 1 is the \nright. y1 y2 are the top and bottom most positions, normalized \ninto 0 to 1, where 0 is the top and 1 is the bottom. For example, \nthe (x1,y1,x2,y2) coordinates for that region with the beer bottle \nis (0.47, 0.48, 0.55, 0.87)\nThe image below that shows a flying goose. Which part \nrepresents the head of the bird? What is the (x1,y1,x2,y2) \ncoordinates for the region with the head of the bird?\nGemini Pro: \nThe (x1, y1, x2, y2) coordinates for the region with the head of the bird are \n(0.37, 0.36, 0.47, 0.47).\nFigure 53: Abstract image localization. Through prompting them to output grounding box corner\npoint, GPT-4V can localize the head of the abstract goose, while Gemini failed in this case.\n66\n8\nTemporal Video Understanding\nThis section evaluates the models\u2019 capabilities in processing temporal sequences and video content,\nemploying keyframe inputs to facilitate understanding of videos. Due to Gemini\u2019s limitation in\ncontinuous input processing, it resorts to integrating keyframes into a single composite image\nfor analysis. Sec. 8.1 delves into continuous action recognition: Here, both excel in recognizing\nsequential actions, having the capability for temporal understanding Sec. 8.2 assess the challenge of\ntemporal ordering: In the task of arranging video segments depicting the process of making sushi,\nGPT-4V demonstrates superior performance, showcasing its advanced capability in understanding\nand sequencing temporal events.\n8.1\nAction Recognition\nFig. 54 presents a case of continuous action recognition. In this scenario, GPT-4V\u2019s ability to\nsequentially process multiple images translates into more accurate results. This enhanced accuracy\nmay be attributed to its capability to synthesize information across a series of frames, thereby offering\na more coherent and precise interpretation of continuous actions.\n8.2\nTemporal Ordering\nFig. 55 illustrates a task involving the temporal sequencing of a video depicting sushi preparation.\nThis is a problem of ordering the steps to make sushi. In this instance, GPT-4V demonstrates a\nsuperior performance in accurately arranging the sequence of events.\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 8.1 Action Recognition\nGPT-4V\uff1a\nThe images show a person in a motion capture suit in a studio\nwith multiple cameras and sensors. The person is performing a\npush-up exercise. In the first image, the person is in the starting\nposition with their arms extended and body in a straight line. In the\nsecond image, the person is lowering their body towards the\nground. In the third image, the person is at the bottom of the push-\nup with their chest close to the ground. In the fourth image, the\nperson is pushing their body back up to the starting position.\nGemini Pro: \nThe image shows a person doing push-ups in a room. The\nperson is wearing a virtual reality headset.\nThe person is in a kneeling position with their hands shoulder-width\napart and their body in a straight line from their head to their heels.\nThey then lower their body until their chest touches the ground,\nbefore pressing back up to the starting position.\nThe virtual reality headset is likely being used to enhance the\nperson's workout experience. For example, the headset could be\nused to display a virtual environment that the person can run\nthrough while doing push-ups, or to provide feedback on their form.\nPrompt: Explain the images and the action the person is\ntaking.\nFigure 54: Video understanding in action recognition. Given image-sequence of a human, both\nGPT-4V and Gemini can generate relevant insights.\n67\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff8.2\uffffTemporal\uffffOrdering\n GPT-4V\uff1a\nThe correct order in which the images should be displayed is: \n1. e - Mixing the rice \n2. c - Placing the rice on the seaweed \n3. b - Adding the fillings \n4. a - Cutting the roll \n5. d - Finished sushi rolls on a plate.\nGemini Pro: \nThe correct order for the sushi-making images is:\nA. Person making sushi on a wooden table\nD. Person rolling sushi with a bamboo mat\nB. Person cutting sushi with a sharp knife\nE. Person plating sushi on a serving dish\nA D B E\nPrompt: Analyze the images and decide the correct \norder in which they should be displayed.\nFigure 55: Video understanding in temporal order. The sushi-making process is disordered, only\nGPT4-V is able to find the correct order while Gemini failed in this case.\n9\nMultilingual Capabilities\nThis section highlights the models\u2019 multilingual capabilities, focusing on multilingual input processing\nand recognition of texts in various languages within images. Sec. 9.1 deals with multilingual image\ndescriptions: In tasks requiring the description of given images in different languages, both models\ndemonstrate their versatility. This includes simple images of text in a scene as well as web pages\ncontaining text information, where both models provide reasonable results. Sec. 9.2 addresses\nmultilingual scene text recognition: Here, the models are tasked with identifying texts in multiple\nlanguages within images. Both models exhibit robust capabilities in recognizing texts, whether in a\nsingle language or multiple languages, showcasing their strength in multilingual text processing.\n9.1\nMultilingual Image Description\nFig. 56 to Fig. 59 focus on the task of providing descriptions for a given image using different\nlanguages. These tasks range from interpreting straightforward images to analyzing web pages\nrich in textual content. In each scenario, both models successfully provide coherent and reasonable\ndescriptions, regardless of the image complexity or the language used.\n9.2\nMultilingual Scene Text Recognition\nFig. 60 to Fig. 63 focus on the task of identifying texts in various languages within images. These\nfigures illustrate the models\u2019 proficiency in recognizing text, whether it is presented in a single\nlanguage or in a multilingual context. Here as well, both GPT-4V and Gemini perform well,\ndemonstrating their ability to understand multiple languages.\n68\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.1 Multilingual Image Description\nPrompt: \u63cf\u8ff0\u9019\u5f35\u5716\u7247\nGemini Pro: \n\u9019\u5f35\u5716\u7247\u662f\u4e00\u96bb\u5c0f\u8c93\u5728\u8349\u5730\u4e0a\u8d70\u8def\u7684\u756b\u9762\u3002\u5c0f\u8c93\u662f\u6a58\u8272\u7684\uff0c\u6709\u85cd\u8272\n\u7684\u773c\u775b\uff0c\u770b\u8d77\u4f86\u5f88\u53ef\u611b\u3002\u5b83\u7684\u8eab\u9ad4\u5f88\u5c0f\uff0c\u56db\u80a2\u7e96\u7d30\uff0c\u6b63\u5728\u5c0f\u5fc3\u7ffc\u7ffc\n\u5730\u5728\u8349\u5730\u4e0a\u884c\u8d70\u3002\nGPT-4V\uff1a\n\u9019\u5f35\u5716\u7247\u986f\u793a\u4e86\u4e00\u96bb\u5c0f\u6a58\u8c93\u5728\u8349\u5730\u4e0a\u8d70\u8def\u3002\u8c93\u54aa\u7684\u6bdb\u9aee\u662f\u6fc3\u5bc6\u7684\u6a58\n\u8272\uff0c\u773c\u775b\u662f\u660e\u4eae\u7684\u85cd\u8272\u3002\u5b83\u6b63\u4ee5\u8f15\u5feb\u7684\u6b65\u4f10\u5728\u8349\u5730\u4e0a\u6f2b\u6b65\uff0c\u597d\u5947\u5730\n\u63a2\u7d22\u5468\u570d\u7684\u74b0\u5883\u3002\nGPT-4V\uff1a\nPrompt: D\u00e9crire l\u2019image\nIl s\u2019agit d\u2019une photo d\u2019un chaton roux marchant sur l\u2019herbe verte. Le chaton a les yeux bleus et semble \u00eatre en\ntrain de marcher avec prudence. Il est entour\u00e9 d\u2019herbe et on peut voir une partie d\u2019un autre chat en arri\u00e8re-plan. \nGemini Pro: \nL'image montre un petit chaton orange marchant dans l'herbe. Le chaton a une fourrure\norange vif avec des rayures noires, et des yeux bleus brillants. Il marche avec assurance, sa\nqueue fr\u00e9missante derri\u00e8re lui.\nhttps://fr.freepik.com/photos-premium/chat-roux-marchant-herbe_18744956.htm\nGPT-4V\uff1a\nPrompt: Popi\u0161te obr\u00e1zeke\nNa obr\u00e1zku je mal\u00e9 oran\u017eov\u00e9 kot\u011b s modr\u00fdma o\u010dima. Kot\u011b stoj\u00ed na zelen\u00e9 tr\u00e1v\u011b a sm\u011b\u0159uje dop\u0159edu. Jeho t\u011blo je \nv pohybu a vypad\u00e1, \u017ee pr\u00e1v\u011b b\u011b\u017e\u00ed. Na pozad\u00ed je vid\u011bt dal\u0161\u00ed \u010d\u00e1st tr\u00e1vy a nejasn\u011b i dal\u0161\u00ed ko\u010dka. \nGemini Pro: \nObr\u00e1zek zobrazuje mal\u00e9 oran\u017eov\u00e9 kot\u011b, kter\u00e9 kr\u00e1\u010d\u00ed po tr\u00e1v\u011b. Kot\u011b m\u00e1 modr\u00e9 o\u010di a je asi 6 m\u011bs\u00edc\u016f star\u00e9. M\u00e1\nkr\u00e1tkou, hustou srst a \u0161pi\u010dat\u00e9 u\u0161i. Kot\u011b se pohybuje pomalu a uvoln\u011bn\u011b, jako by si u\u017e\u00edvalo pocitu tr\u00e1v\u011b mezi\ntlapkami. Obr\u00e1zek je po\u0159\u00edzen na zahrad\u011b nebo v parku. V pozad\u00ed je vid\u011bt strom a ke\u0159. Slunce sv\u00edt\u00ed a obloha je\nmodr\u00e1.\nFigure 56: Multilingual image descriptions using corresponding language prompts. Both GPT-\n4V and Gemini are able to describe image in different languages.\n69\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.1 Multilingual Image Description\nPrompt: Describe the image in Chinese\nGemini Pro: \n\u8fd9\u662f\u4e00\u53ea\u5c0f\u67ef\u57fa\u72ac\uff0c\u5b83\u5728\u8349\u5730\u4e0a\u8d70\u7740\uff0c\u5634\u5df4\u5f20\u5f00\uff0c\u820c\u5934\u4f38\u51fa\u6765\uff0c\n\u597d\u50cf\u5f88\u5f00\u5fc3\u7684 \u6837\u5b50\u3002\u5b83\u7684\u8eab\u4e0a\u6709\u4e00\u6761\u9879\u5708\uff0c\u4e0a\u9762\u6302\u7740\u4e00\u4e2a\u9aa8\u5934\n\u5f62\u72b6\u7684\u724c\u5b50\u3002\nGPT-4V\uff1a\n\u56fe\u7247\u4e0a\u662f\u4e00\u53ea\u53ef\u7231\u7684\u67ef\u57fa\u5e7c\u72ac\uff0c\u5b83\u6602\u9996\u633a\u80f8\uff0c\u7ad9\u7acb\u5728\u8349\u5730\u4e0a\uff0c\u5c3e\u5df4\n\u6bdb\u8338\u8338\u7684\uff0c\u5377\u5411\u80cc\u90e8\u3002\u5b83\u7684\u6bdb\u53d1\u4ee5\u68d5\u8272\u548c\u767d\u8272\u4e3a\u4e3b\uff0c\u56db\u80a2\u77ed\u5c0f\uff0c\u8033\n\u6735\u7ad6\u7acb\u3002\u5b83\u4f3c\u4e4e\u662f\u4e00\u53ea\u975e\u5e38\u597d\u5947\u7684\u5e7c\u72ac\uff0c\u6b63\u7741\u5927\u773c\u775b\u770b\u7740\u955c\u5934\u3002\nGPT-4V\uff1a\nPrompt: Describe the image in French\nC'est une image d'un petit chien de race Corgi. Il est de couleur marron et blanc et porte un collier avec une \nm\u00e9daille en forme d'os. Il est en train de marcher sur l'herbe verte avec la langue pendante.\nGemini Pro: \nL'image montre un adorable chiot corgi, peut-\u00eatre un Pembroke Welsh Corgi, debout dans l'herbe. Il a un \npelage brun et blanc caract\u00e9ristique, avec de grandes oreilles pointues et une queue courte et touffue. Le \nchiot porte un collier noir et semble \u00eatre assez alerte, ses oreilles dress\u00e9es et ses yeux regardant directement \nl'appareil photo.\nL'herbe autour du chiot est un vert vif et semble \u00eatre un jour ensoleill\u00e9 et agr\u00e9able. En arri\u00e8re-plan, on peut \nvoir quelques arbres et arbustes, mais la sc\u00e8ne est principalement occup\u00e9e par le chiot mignon.\nGPT-4V\uff1a\nPrompt: Describe the image in Czech\nNa obr\u00e1zku je mal\u00fd hn\u011bd\u00fd pes s b\u00edl\u00fdm l\u00edmcem a kosti\u010dkou na obojku. Pes je na zelen\u00e9 tr\u00e1v\u011b a m\u00e1 otev\u0159enou \ntlamu. Vypad\u00e1, \u017ee se bav\u00ed. \nGemini Pro: \nObr\u00e1zek ukazuje roztomil\u00e9ho corgi \u0161t\u011bn\u011bte s hn\u011bdo-b\u00edlou srst\u00ed, stoj\u00edc\u00edho na tr\u00e1v\u011b. \u0160t\u011bn\u011b m\u00e1 na sob\u011b obojek a \nzd\u00e1 se, \u017ee klidn\u011b hled\u00ed do kamery. Tr\u00e1va je sv\u011b\u017e\u00ed a zelen\u00e1 a na pozad\u00ed jsou rozmazan\u00e9 tvary strom\u016f a ke\u0159\u016f.\nCelkov\u011b je obr\u00e1zek roztomil\u00fd a poklidn\u00fd a \u0161t\u011bn\u011b vypad\u00e1, \u017ee si u\u017e\u00edv\u00e1 kr\u00e1sn\u00e9ho dne venku.\nFigure 57: Multilingual image descriptions using English prompts. Both GPT-4V and Gemini are\nable to describe image in different languages.\n70\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.1 Multilingual Image Description\nPrompt: Describir la imagen en \n20 idiomas diferentes.\nGemini Pro: \n1. Ingl\u00e9s: This is a beautiful view of Machu Picchu in Peru. \n2. Espa\u00f1ol: Esta es una hermosa vista de Machu Picchu en Per\u00fa.\n3. Franc\u00e9s: C'est une belle vue de Machu Picchu au P\u00e9rou. \n4. Alem\u00e1n: Dies ist eine sch\u00f6ne Aussicht auf Machu Picchu in Peru. \n5. Italiano: Questa \u00e8 una bella vista di Machu Picchu in Per\u00f9. \n6. Portugu\u00e9s: Esta \u00e9 uma bela vista de Machu Picchu no Peru. \n7. Ruso: \u042d\u0442\u043e \u043a\u0440\u0430\u0441\u0438\u0432\u044b\u0439 \u0432\u0438\u0434 \u043d\u0430 \u041c\u0430\u0447\u0443-\u041f\u0438\u043a\u0447\u0443 \u0432 \u041f\u0435\u0440\u0443. \n8. Chino: \u8fd9\u662f\u79d8\u9c81\u9a6c\u4e18\u6bd4\u4e18\u7684\u7f8e\u4e3d\u666f\u8272\u3002\n9. Japon\u00e9s: \u3053\u308c\u306f\u30da\u30eb\u30fc\u306e\u30de\u30c1\u30e5\u30d4\u30c1\u30e5\u306e\u7f8e\u3057\u3044\u666f\u8272\u3067\u3059\u3002\n10. Coreano: \uc774\uac83\uc740 \ud398\ub8e8\uc758 \ub9c8\ucd94 \ud53c\ucd94\uc758 \uc544\ub984\ub2e4\uc6b4 \uacbd\uce58\uc785\ub2c8\ub2e4.\nGPT-4V\uff1a\nEspa\u00f1ol: La imagen muestra las ruinas de una ciudad en la cima de una monta\u00f1a en Per\u00fa. La ciudad est\u00e1 \nconstruida con piedra y tiene una arquitectura compleja. La ciudad est\u00e1 rodeada de monta\u00f1as y bosques.\nIngl\u00e9s: The image shows the ruins of a city on top of a mountain in Peru. The city is built of stone and has a \ncomplex architecture. The city is surrounded by mountains and forests.\nFranc\u00e9s: L'image montre les ruines d'une ville au sommet d'une montagne au P\u00e9rou. La ville est construite en \npierre et a une architecture complexe. La ville est entour\u00e9e de montagnes et de for\u00eats.\nAlem\u00e1n: Das Bild zeigt die Ruinen einer Stadt auf dem Gipfel eines Berges in Peru. Die Stadt ist aus Stein \nerbaut und hat eine komplexe Architektur. Die Stadt ist von Bergen und W\u00e4ldern umgeben.\nItaliano: L'immagine mostra le rovine di una citt\u00e0 sulla cima di una montagna in Per\u00f9. La citt\u00e0 \u00e8 costruita in \npietra e ha un'architettura complessa. La citt\u00e0 \u00e8 circondata da montagne e foreste.\nPortugu\u00e9s: A imagem mostra as ru\u00ednas de uma cidade no topo de uma montanha no Peru. A cidade \u00e9 \nconstru\u00edda em pedra e tem uma arquitetura complexa. A cidade \u00e9 cercada por montanhas e florestas.\nRuso: \u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0440\u0443\u0438\u043d\u044b \u0433\u043e\u0440\u043e\u0434\u0430 \u043d\u0430 \u0432\u0435\u0440\u0448\u0438\u043d\u0435 \u0433\u043e\u0440\u044b \u0432 \u041f\u0435\u0440\u0443. \u0413\u043e\u0440\u043e\u0434 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d \u0438\u0437 \u043a\u0430\u043c\u043d\u044f \u0438 \n\u0438\u043c\u0435\u0435\u0442 \u0441\u043b\u043e\u0436\u043d\u0443\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443. \u0413\u043e\u0440\u043e\u0434 \u043e\u043a\u0440\u0443\u0436\u0435\u043d \u0433\u043e\u0440\u0430\u043c\u0438 \u0438 \u043b\u0435\u0441\u0430\u043c\u0438.\nChino: \u56fe\u7247\u663e\u793a\u4e86\u79d8\u9c81\u5c71\u9876\u7684\u57ce\u5e02\u5e9f\u589f\u3002\u8fd9\u5ea7\u57ce\u5e02\u7531\u77f3\u5934\u5efa\u9020\uff0c\u62e5\u6709\u590d\u6742\u7684\u5efa\u7b51\u3002\u8fd9\u5ea7\u57ce\u5e02\u88ab\u5c71\u8109\u548c\u68ee\u6797\u5305\u56f4\u3002\nJapon\u00e9s: \u3053\u306e\u753b\u50cf\u306f\u3001\u30da\u30eb\u30fc\u306e\u5c71\u306e\u9802\u4e0a\u306b\u3042\u308b\u90fd\u5e02\u306e\u5ec3\u589f\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u90fd\u5e02\u306f\u77f3\u9020\u308a\u3067\u3001\u8907\u96d1\u306a\u5efa\n\u7bc9\u69d8\u5f0f\u3092\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u90fd\u5e02\u306f\u3001\u5c71\u3005\u3068\u68ee\u6797\u306b\u56f2\u307e\u308c\u3066\u3044\u307e\u3059\u3002\nCoreano: \uc774 \uc774\ubbf8\uc9c0\ub294 \ud398\ub8e8 \uc0b0\uaf2d\ub300\uae30\uc5d0 \uc788\ub294 \ub3c4\uc2dc\uc758 \ud3d0\ud5c8\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \ub3c4\uc2dc\ub294 \ub3cc\ub85c \uc9c0\uc5b4\uc84c\uc73c\uba70 \ubcf5\uc7a1\ud55c \uac74\n\ucd95 \uc591\uc2dd\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub3c4\uc2dc\ub294 \uc0b0\uacfc \uc232\uc73c\ub85c \ub458\ub7ec\uc2f8\uc5ec \uc788\uc2b5\ub2c8\ub2e4.\n\u00c1rabe: \u0627\u0644\u0635\u0648\u0631\u0629 \u0644\u0638\u0647\u0631 \u0623\u0637\u0644\u0627 \u0645\u062f\u064a\u0646\u0629 \u0639\u0644\u0649 \u0645\u0629\u0629 \u0623\u0628\u0644 \u0641\u064a \u0635\u0644\u064a\u0628. \u0645\u062f\u064a\u0646\u0629\u0631\u0629 \u0645\u064a\u062f\u0628\u0629 \u0645\u0629 \u0644\u0644\u062d\u0631\u0629 \u0648\u0638\u0631\u0635 \u0645\u0646\u0646\u062f\u0629 \u0645\u064a\u0644\u0648\u0629\u064a\u0629 \u0627\u0646\u0642\u064a\u0629. \u0645\u062f\u064a\u0646\u0629\u0631\u0629 \u0645\u0644\u0648\u062d\u0629 \u0623\u0648\u0628\u0644\u0631\u0648\u0628 \u0648\u0648\u0628\u0648\u0627\u0631\u0629\u0635.\nHindi:  \u091a \u093f \u0924 \u094d\n\u0930  \u092a \u0947\n\u0930 \u0942\n \u0915\u0940  \u090f \u0915 \u092a \u0939 \u093e \u0921 \u093c\n \u0915\u0940  \u091a \u094b \u091f \u0940  \u092a \u0930  \u090f \u0915 \u0936 \u0939 \u0930  \u0915\u0947\n \u0916 \u0902\n\u0921 \u0939 \u0930 \u094b \u0902\n \u0915\u094b  \n\u0926 \u0930 \u094d\n\u0936 \u093e \u0924 \u093e  \u0939 \u0948\n\u0964  \u0936 \u0939 \u0930  \u092a \u0924 \u094d\n\u0925 \u0930  \u0938 \u0947\n \u092c \u0928 \u093e  \u0939 \u0948\n \u0914\u0930  \u0907 \u0938 \u092e \u0947\n\u0902\n \u090f \u0915  \u091c \u091f \u093f \u0932  \u0935 \u093e \u0938 \u094d\n\u0924 \u0941\n\u0915\n\u0932 \u093e  \u0939 \u0948\n\u0964  \u0936 \u0939 \u0930  \u092a \u0939 \u093e \u0921 \u093c\n\u094b \u0902\n \u0914\u0930  \u091c \u0902\n\u0917 \u0932 \u094b \u0902\n \u0938 \u0947\n \u0918 \u093f \u0930 \u093e  \u0939 \u0941\n\u0906 \u0939 \u0948\n\u0964\nTurco: Resim, Peru'nun zirvesinde bir \u015fehrin kal\u0131nt\u0131lar\u0131n\u0131 g\u00f6steriyor. \u015eehir ta\u015ftan yap\u0131lm\u0131\u015f ve karma\u015f\u0131k bir \nmimariye sahiptir. \u015eehir da\u011flar ve ormanlarla \u00e7evrilidir.\nVietnamita: H\u00ecnh \u1ea3nh cho th\u1ea5y t\u00e0n t\u00edch c\u1ee7a m\u1ed9t th\u00e0nh ph\u1ed1 tr\u00ean \u0111\u1ec9nh m\u1ed9t ng\u1ecdn n\u00fai \u1edf Peru. Th\u00e0nh ph\u1ed1 \u0111\u01b0\u1ee3c \nx\u00e2y d\u1ef1ng b\u1eb1ng \u0111\u00e1 v\u00e0 c\u00f3 ki\u1ebfn \u200btr\u00fac ph\u1ee9c t\u1ea1p. Th\u00e0nh ph\u1ed1 \u0111\u01b0\u1ee3c bao quanh b\u1edfi n\u00fai v\u00e0 r\u1eebng.\n11. Hindi: \u092f \u0939  \u092a \u0947\n\u01fe \u0915\u0947\n \u092e \u093e \u091a \u0942\n\u036a\u092a \u00cd\u091a \u0942\n\u0915\u093e  \u0938 \u0941\n\u0902\n\u0926 \u0930  \u01fa\u00e6\u092f  \u0939 \u0948\n\u0964  \n12. \u00c1rabe: \u0629\u0630\u0629 \u0644\u0647\u062f\u0629 \u0623\u064a\u0629\u0644 \u0635\u0645\u0648\u0629\u0631 \u0635\u0645\u0631\u064a\u0628 \u0641\u064a \u0635\u0644\u064a\u0628.\n13. Turco: Bu, Peru'daki Machu Picchu'nun g\u00fczel bir manzaras\u0131d\u0131r. \n14. Griego: \u0391\u03c5\u03c4\u03ae \u03b5\u03af\u03bd\u03b1\u03b9 \u03bc\u03b9\u03b1 \u03cc\u03bc\u03bf\u03c1\u03c6\u03b7 \u03b8\u03ad\u03b1 \u03c4\u03bf\u03c5 \u039c\u03ac\u03c4\u03c3\u03bf\u03c5 \u03a0\u03af\u03c4\u03c3\u03bf\u03c5 \u03c3\u03c4\u03bf \u03a0\u03b5\u03c1\u03bf\u03cd.\n15. Hebreo:  \u05d6\u05d4\u05d5 \u05e0\u05d6\u05e3 \u05d4\u05e4\u05d4 \u05e9\u05dc \u05de\u05d0\u05e6'\u05d6 \u05de\u05d4\u05e4'\u05d6 \u05d6\u05e4\u05e4\u05d5. \n16. Sueco: Det h\u00e4r \u00e4r en vacker vy av Machu Picchu i Peru. \n17. Noruego: Dette er en vakker utsikt over Machu Picchu i Peru. \n18. Dan\u00e9s: Dette er en smuk udsigt over Machu Picchu i Peru. \n19. Holand\u00e9s: Dit is een prachtig uitzicht op Machu Picchu in Peru. \n20. Polaco: To pi\u0119kny widok na Machu Picchu w Peru.\nTailand\u00e9s:  \u0e20 \u0e32 \u0e1e \u0e19 \u0e35\n\u0e49\n\u0e41 \u0e2a \u0e14 \u0e07 \u0e0b \u0e32 \u0e01 \u0e40 \u0e21 \u0e37\n\u0e2d \u0e07 \u0e1a \u0e19 \u0e22 \u0e2d \u0e14 \u0e40 \u0e02 \u0e32 \u0e43 \u0e19 \u0e40 \u0e1b \u0e23 \u0e39\n \u0e40 \u0e21 \u0e37\n\u0e2d \u0e07 \u0e19 \u0e35\n\u0e49\n\u0e2a\n\u0e23 \u0e49\n\u0e32 \u0e07 \u0e14 \u0e49\n\u0e27 \u0e22 \u0e2b \u0e34\n\u0e19 \u0e41 \u0e25 \u0e30 \u0e21 \u0e35\n\u0e2a \u0e16 \u0e32 \u0e1b \u0e31\n\u0e15 \u0e22 \u0e01 \u0e23 \u0e23 \u0e21 \u0e17 \u0e35\n\u0e48\n\u0e0b \u0e31\n\u0e1a \u0e0b \u0e49\n\u0e2d \u0e19  \u0e40 \u0e21 \u0e37\n\u0e2d \u0e07 \u0e25 \u0e49\n\u0e2d \u0e21\n\u0e23 \u0e2d \u0e1a \u0e14 \u0e49\n\u0e27 \u0e22 \u0e20 \u0e39\n\u0e40 \u0e02 \u0e32 \u0e41 \u0e25 \u0e30 \u0e1b \u0e48\n\u0e32 \u0e44 \u0e21 \u0e49\nLaosiano: \u0e9e \u0eb2 \u0e9a \u0e99 \u0eb5\n\u0ec9\n\u0eaa \u0eb0 \u0ec1 \u0e94 \u0e87 \u0ec3 \u0eab \u0ec9\n\u0ec0 \u0eab \u0eb1\n\u0e99 \u0e8a \u0eb2 \u0e81 \u0eab \u0eb5\n\u0e99 \u0ec0 \u0ea1 \u0eb7\n\u0ead \u0e87 \u0e97 \u0eb5\n\u0ec8\n\u0ea2 \u0eb9\n\u0ec8\n\u0ec0 \u0e97 \u0eb4\n\u0e87 \u0e88 \u0ead\n\u0ea1 \u0e9e \u0eb9\n\u0ec3 \u0e99 \u0ec0 \u0e9b \u0ea3 \u0eb9\n \u0ec0 \u0ea1 \u0eb7\n\u0ead \u0e87 \u0e99 \u0eb5\n\u0ec9\n\u0eaa \u0ec9\n\u0eb2 \u0e87 \u0e82 \u0eb6\n\u0ec9\n\u0e99 \u0e94 \u0ec9\n\u0ea7 \u0e8d \u0eab \u0eb5\n\u0e99 \u0ec1 \u0ea5 \u0eb0 \u0ea1 \u0eb5\n\u0e81 \u0eb2 \u0e99 \u0e81 \u0ecd\n\u0ec8\n\u0eaa \u0ec9\n\u0eb2\n\u0e87 \u0e97 \u0eb5\n\u0ec8\n\u0eaa \u0eb1\n\u0e9a \u0eaa \u0ebb\n\u0e99  \u0ec0 \u0ea1 \u0eb7\n\u0ead \u0e87 \u0ec1 \u0ea1 \u0ec8\n\u0e99 \u0ead \u0ec9\n\u0ead \u0ea1 \u0eae \u0ead \u0e9a \u0e94 \u0ec9\n\u0ea7 \u0e8d \u0e9e \u0eb9\n\u0ec1 \u0ea5 \u0eb0 \u0e9b \u0ec8\n\u0eb2 \u0ec4 \u0ea1 \u0ec9\nFigure 58: Multilingual Image Descriptions. Both GPT-4V and Gemini are able to describe image\nin different languages.\n71\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.1 Multilingual Image Description\nPrompt: Summarize the info in 20 languages.\nGemini Pro: \nCatalan: Els Alps Sud s\u00f3n una serralada que recorre la costa oest de l'illa del Sud de Nova Zelanda.\nChinese: \u5357\u963f\u5c14\u5351\u65af\u5c71\u8109\u662f\u65b0\u897f\u5170\u5357\u5c9b\u897f\u6d77\u5cb8\u7684\u4e00\u6761\u5c71\u8109\u3002\nCzech: Ji\u017en\u00ed Alpy jsou poho\u0159\u00ed, kter\u00e9 se t\u00e1hne pod\u00e9l z\u00e1padn\u00edho pob\u0159e\u017e\u00ed Ji\u017en\u00edho ostrova Nov\u00e9ho Z\u00e9landu.\nDutch: De Zuidelijke Alpen is een gebergte dat langs de westkust van het Zuidereiland van Nieuw-Zeeland loopt.\nEnglish: The Southern Alps are a mountain range that runs along the western side of the South Island of New \nZealand.\nFrench: Les Alpes du Sud sont une cha\u00eene de montagnes qui longe la c\u00f4te ouest de l'\u00eele du Sud de la Nouvelle-\nZ\u00e9lande.\nGerman: Die S\u00fcdalpen sind ein Gebirgszug, der sich entlang der Westk\u00fcste der S\u00fcdinsel Neuseelands erstreckt.\nHebrew: \u05d4\u05d0\u05dc\u05e4\u05d9\u05dd \u05d4\u05d0\u05d0\u05d5\u05de\u05d9\u05d9\u05dd \u05d4\u05dd \u05e8\u05db\u05d9 \u05d4\u05d0\u05d9\u05dd \u05dc\u05d9\u05de\u05d9\u05e4 \u05d4\u05de\u05d5\u05dd \u05d0\u05de\u05d9\u05e8\u05d5\u05dd \u05e4\u05dc \u05d0\u05d9\u05dd \u05d0\u05d5\u05de\u05d9\u05d9\u05dd \u05e4\u05dc \u05de\u05d0\u05d5 \u05d9\u05d5\u05e4\u05d0\u05d3.\nHindi: \u0926 \u0915\u094d\n\u0937 \u093f \u0923 \u0940  \u0906\u0932 \u094d\n\u092a \u094d\n\u0938  \u0928 \u094d\n\u092f \u0942\n\u091c \u093c\n\u0940 \u0932 \u0948\n\u0902\n\u0921  \u0915\u0947\n \u0926 \u0915\u094d\n\u0937 \u093f \u0923  \u0926 \u094d\n\u0935 \u0940 \u092a  \u0915\u0947\n \u092a \u0936 \u094d\n\u091a \u093f \u092e\n\u0940  \u0915\u093f \u0928 \u093e \u0930 \u0947\n \u092a \u0930  \u091a \u0932 \u0928 \u0947\n \u0935 \u093e \u0932 \u0940  \u090f \u0915 \u092a \u0930 \u094d\n\u0935 \u0924  \u0936 \u094d\n\u0930 \u0943\n\u0902\n\u0916 \u0932 \u093e  \u0939 \u0948\n\u0964\nItalian: Le Alpi meridionali sono una catena montuosa che si estende lungo la costa occidentale dell'Isola del Sud \ndella Nuova Zelanda.\nJapanese: \u5357\u30a2\u30eb\u30d7\u30b9\u306f\u3001\u30cb\u30e5\u30fc\u30b8\u30fc\u30e9\u30f3\u30c9\u306e\u5357\u5cf6\u306e\u897f\u6d77\u5cb8\u306b\u6cbf\u3063\u3066\u8d70\u308b\u5c71\u8108\u3067\u3059\u3002\nKorean: \ub0a8\uc54c\ud504\uc2a4\ub294 \ub274\uc9c8\ub79c\ub4dc \ub0a8\uc12c \uc11c\ud574\uc548\uc744 \ub530\ub77c\u8d70\u308b \uc0b0\ub9e5\uc785\ub2c8\ub2e4.\nPolish: Alpy Po\u0142udniowe to pasmo g\u00f3rskie, kt\u00f3re ci\u0105gnie si\u0119 wzd\u0142u\u017c zachodniego wybrze\u017ca Wyspy Po\u0142udniowej \nNowej Zelandii.\nPortuguese: Os Alpes do Sul s\u00e3o uma cordilheira que percorre a costa oeste da Ilha Sul da Nova Zel\u00e2ndia.\nRussian: \u042e\u0436\u043d\u044b\u0435 \u0410\u043b\u044c\u043f\u044b - \u044d\u0442\u043e \u0433\u043e\u0440\u043d\u044b\u0439 \u0445\u0440\u0435\u0431\u0435\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0442\u044f\u043d\u0435\u0442\u0441\u044f \u0432\u0434\u043e\u043b\u044c \u0437\u0430\u043f\u0430\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u0431\u0435\u0440\u0435\u0436\u044c\u044f \u042e\u0436\u043d\u043e\u0433\u043e \u043e\u0441\u0442\u0440\u043e\u0432\u0430 \n\u041d\u043e\u0432\u043e\u0439 \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u0438.\nSpanish: Los Alpes del Sur son una cadena monta\u00f1osa que se extiende a lo largo de la costa oeste de la Isla \nSur de Nueva Zelanda.\nTurkish: G\u00fcney Alpleri, Yeni Zelanda'n\u0131n G\u00fcney Adas\u0131'n\u0131n bat\u0131 k\u0131y\u0131s\u0131 boyunca uzanan bir da\u011f s\u0131ras\u0131d\u0131r.\nVietnamese: D\u00e3y Alps Nam l\u00e0 m\u1ed9t d\u00e3y n\u00fai ch\u1ea1y d\u1ecdc theo b\u1edd bi\u1ec3n ph\u00eda t\u00e2y c\u1ee7a \u0110\u1ea3o Nam, New Zealand.\n1. English: The Southern Alps are a mountain range in New Zealand. \n2. Spanish: Los Alpes del Sur son una cadena monta\u00f1osa en Nueva Zelanda. \n3. French: Les Alpes du Sud sont une cha\u00eene de montagnes en Nouvelle-Z\u00e9lande.\n4. German: Die S\u00fcdalpen sind ein Gebirgszug in Neuseeland. \n5. Italian: Le Alpi meridionali sono una catena montuosa in Nuova Zelanda. \n6. Portuguese: Os Alpes do Sul s\u00e3o uma cordilheira na Nova Zel\u00e2ndia. \n7. Russian: \u042e\u0436\u043d\u044b\u0435 \u0410\u043b\u044c\u043f\u044b - \u0433\u043e\u0440\u043d\u044b\u0439 \u0445\u0440\u0435\u0431\u0435\u0442 \u0432 \u041d\u043e\u0432\u043e\u0439 \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u0438. \n8. Chinese: \u5357\u963f\u5c14\u5351\u65af\u5c71\u8109\u662f\u65b0\u897f\u5170\u7684\u4e00\u6761\u5c71\u8109\u3002\n9. Japanese: \u5357\u30a2\u30eb\u30d7\u30b9\u306f\u30cb\u30e5\u30fc\u30b8\u30fc\u30e9\u30f3\u30c9\u306e\u5c71\u8108\u3067\u3059\u3002\n10. Korean: \ub0a8\ubd80 \uc54c\ud504\uc2a4\ub294 \ub274\uc9c8\ub79c\ub4dc\uc758 \uc0b0\ub9e5\uc785\ub2c8\ub2e4. \n11. Hindi: \u0926 \u036f\u0915\u094d\n\u0937 \u093f \u0923 \u0940  \u090f \u0932 \u094d\n\u092a \u094d\n\u0938  \u0928 \u094d\n\u092f \u0942\n\u091c \u0940 \u0932 \u0948\n\u0902\n\u0921  \u0915\u093e  \u090f \u0915 \u092a \u0935 \u0930 \u094d\n\u0935 \u0924  \u0936 \u094d\n\u0930 \u0943\n\u0902\n\u0916 \u0932 \u093e  \u0939 \u0948\n\u0964  \n12. Bengali: \u09a6 \u09bf \u0995\u09cd\n\u09b7 \u09bf \u09a3  \u0985\u09cd\n\u09af \u09be \u09b2 \u09aa \u09b8  \u09bf \u09a8 \u0989 \u09bf \u099c\u09b2 \u09cd\n\u09af \u09be \u09c7 \u09a8 \u09cd\n\u09a1 \u09c7 \u09b0  \u098f \u0987  \u0985\u09bf \u09a7 \u0995 \u09aa \u09ac \u09b0 \u09cd\n\u09ac \u09a4 \u09ae \u09be\n\u09b2 \u09be \u0964  \n13. Arabic: .\u0627\u0644\u0623\u0644 \u0627\u0644\u062c\u0646\u0648\u0628\u0644\u0644 \u06be\u064a \u0627\u0644\u0633\u0644\u0633 \u062c\u0628\u062c\u0628 \u06be\u064a \u0644\u06cc\u0648\u0644\u0644\u0644\u0646\u0644\u0648 \n14. Turkish: G\u00fcney Alpler Yeni Zelanda'da bir da\u011f silsilesidir. \n15. Indonesian: Alpen Selatan adalah sebuah pegunungan di Selandia Baru. \n16. Vietnamese: D\u00e3y n\u00fai Alpes ph\u00eda Nam l\u00e0 m\u1ed9t d\u00e3y n\u00fai \u1edf New Zealand. \n17. Thai: \u0e40 \u0e17 \u0e37\n\u0e2d \u0e01 \u0e40 \u0e02 \u0e32 \u0e41 \u0e2d \u0e25 \u0e1b\n\u0e20 \u0e32 \u0e04 \u0e43 \u0e15\n\u0e40 \u0e1b\n\u0e19 \u0e40 \u0e17 \u0e37\n\u0e2d \u0e01 \u0e40 \u0e02 \u0e32 \u0e43 \u0e19 \u0e19 \u0e34\n\u0e27 \u0e0b \u0e35\n\u0e41 \u0e25 \u0e19 \u0e14\n18. Malay: Alpen Selatan adalah satu rangkaian gunung di New Zealand. \n19. Tagalog: Ang Timog Alps ay isang hanay ng mga bundok sa New Zealand. \n20. Swahili: Milima ya Alpi Kusini ni safu ya milima nchini New Zealand. \nGPT-4V\uff1a\nFigure 59: Multilingual information summariziation. Both GPT-4V and Gemini can recognize\nlong scene text in images of a web browser browser.\n72\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.2 Multilingual Scene Text Recognition\nPrompt: What are all the scene text in the image?\nThe scene text in the image is an Arabic Bismillah\" (\u0628\u0633\u0645 \u0627 \u0627\u0644\u0631\u062d\u0645\u0646 \n\u0628\u0644\u0631\u062d\u0645\u0646\" phrase al-Rahman al-Rahim). It translates to \"In \nthe name of Allah, the Most Merciful, the Most \nCompassionate\". This phrase is commonly used by Muslims \nat the beginning of any action or speech as a way to seek \nblessings and guidance from God.\nGPT-4V\uff1a\nGemini Pro: \nGPT-4V\uff1a\nThe scene text in the image is \"MUSEE DU LOUVRE\" which is \nthe name of the famous art museum in Paris, France. \nGemini Pro: \nThe image you sent me appears to show a close-up of a blue tile with Arabic calligraphy. The text reads \"\u0628\u0633\u0645 \u0627 \n\u0627\u0644\u0631\u062d\u0645\u0646 \u0628\u0644\u0631\u062d\u0645\u0646,\" which is the basmala, a phrase that means \"In the name of God, the Most Gracious, the \nMost Merciful\" in Arabic. It is a common Islamic inscription found at the beginning of the Quran and in other \nIslamic texts and art.\nPrompt: What are all the scene text in the image?\nThe scene text in the image says \"Mus\u00e9e du Louvre\". It is a \nclose-up photo of the museum's main entrance pyramid. \nGemini Pro: \nThe pyramid is made of glass and metal, and it is surrounded by the museum's buildings.\nThe Louvre is one of the most famous museums in the world, and it is home to a large collection of art and \nartifacts from around the world. Some of the museum's most famous pieces include the Mona Lisa, Venus \nde Milo, and Winged Victory of Samothrace. If you are ever in Paris, I highly recommend visiting the Louvre. \nIt is an amazing place to see art from all over the world.\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\n- \"\u5c16\u6c99\u5480\" (Chinese characters) - \"Tsim Sha Tsui\" (English text)\nThe scene text in the image are: \nThe image you sent me is a sign for the Tsim Sha Tsui MTR station in \nHong Kong.\nGemini Pro: \nFigure 60: In-the-wild multilingual scene text recognition (1/2). Both GPT-4V and Gemini can\nrecognize scene text in all of the three images.\n73\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.2 Multilingual Scene Text Recognition\nPrompt: What are all the scene text in the image?\nThe scene text in the image are:\n- WELCOME (English)\n- bienvenida (Spanish)\n- Bienvenue (French)\n- Willkommen (German)\n- benvenuto (Italian)\nGPT-4V\uff1a\nGemini Pro: \nGPT-4V\uff1a\nThe scene text in the image are:\n- G.P. FIM DA EUROPA\n- ATLETISMO\n- Ant\u00f3nio Agostinho 1987\n- Fernando Domingos 1988\nEnglish: Welcome\nSpanish: Bienvenida\nFrench: Bienvenue\nGerman: Willkommen\nItalian: Benvenuto\nPrompt: What are all the scene text in the image?\nGemini Pro: \nG.P. FIM DA EUROPA\nATLETISMO\nAnt\u00f3nio Agostinho\n1987\nFernando Domingos\n1988\nThe scene text in the image are:\n- \u0391\u0398\u0397\u0203\u0391\u01ff-\u03a1\u03a9\u0202\u0397-\u03a1\u01ff\u039f\u0203 \u01ff\u0391\u03a0\u0395\u03a4\u03a1\u039f\u0203 (Athens-Rome-Rion Iapetron)\n- 29 \u0202\u0391\u03aa\u039f\u03a5 1933 (29 May 1933)\n- \u0394\u03a1\u0391\u03a7 (Drachma)\n- \u0395\u0201\u0201\u0391\u03a3 (Greece)\n- \u03a3\u03a0\u0391\u0395\u03a1. \u03a4\u0391\u03a7\u03a5\u0394\u03a1. \u03a3\u03a5\u0393\u0200\u039f\u01ff\u0203. \u01ff\u03a4\u0391\u0201\u01ff\u0391-\u0395\u0201\u0201\u0391\u0394\u039f\u03a3-\u03a4\u039f\u03a5\u03a1\u0200\u01ff\u0391 \n(Airmail. Express. Communication. Italy-Greece-Turkey)\n- 120 (Value of the stamp in drachmas)\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\nThe scene text in the image is Greek. It says:\n\u0391\u0398\u0397\u039d\u0391\u0399-\u03a1\u03a9\u039c\u0397-\u03a1\u0399\u039f\u039d \u0399\u0391\u039d\u0395\u03aa\u03a1\u039f\u039d, which means \"Athens-Rome-Janeway Airport\"\n29 \u039c\u0391\u0399\u039f\u03a5 1933, which means \"29 May 1933\"\n\u0394\u03a1\u0391\u03a7\u039c\u0391\u0399 1.20, which means \"1.20 drachmas\"\n\u0395\u039b\u039b\u0391\u03a3-\u0391\u03a7, which means \"Greece-AX\"\n\u0395\u039d\u0391\u0395\u03a1. \u03a4\u0391\u03a7\u03a5\u0394\u03a1. \u03a3\u03a5\u0393\u039a\u039f\u0399\u039d., which means \"Air Speed Communications\"\n\u0399\u03a4\u0391\u039b\u0399\u0391\u03a3-\u0395\u039b\u039b\u0391\u0394\u039f\u03a3-\u03a4\u039f\u03a5\u03a1\u039a\u0399\u0391\u03a3, which means \"Italy-Greece-Turkey\"\nThe stamp is a commemorative issue for the inauguration of the Janeway Airport in Athens, which was named \nafter Pan American Airways pilot Harold Gatty Janeway. The airport was later renamed Athens-Eleftherios \nVenizelos International Airport.\nGemini Pro: \nhttps://www.voyagerhobart.com/?wpsc-product=graf-zeppelin-stamp-set-greece-1933`\nFigure 61: In-the-wild multilingual scene text recognition (2/2). Both GPT-4V and Gemini can\nrecognize scene text in all of the three images.\n74\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.2 Multilingual Scene Text Recognition\nPrompt: I f  t h e  i m a g e  i s \npresented in a language other than \nEnglish, translate the content to \nEnglish.\nThe text in the image is in Japanese and translates to \"Good \nmorning, have a nice day.\" \nGPT-4V\uff1a\nGemini Pro: \nGPT-4V\uff1a\nThe text in the image is in Chinese and translates to \"Mapo Tofu.\" \nMapo tofu is a popular Chinese dish made with tofu and minced meat \nin a spicy sauce.\nThe image you sent me appears to be a drawing of a red sun with a \nsmiley face on a white background. There are also two pieces of \nJapanese text above the sun, which say \u201c\u304a\u306f\u3088\u3046\u201d and \u201c\u3054\u3056\u3044\u307e\u3059\u201d.\n\u201c\u304a\u306f\u3088\u3046\u201d means \u201cgood morning\u201d and \u201c\u3054\u3056\u3044\u307e\u3059\u201d is a polite way to \nsay \u201cis\u201d or \u201cam\u201d. So, the text above the sun could be translated as \n\u201cGood morning!\u201d\nPrompt: What are all the scene text in the image?\nGemini Pro: \nThe image you sent me shows a dish of Mapo tofu, a popular Sichuan \ndish made with silken tofu, ground pork, fermented black beans, and \nchili oil. The tofu is usually cubed and simmered in a spicy sauce that \nis both numbing and hot. The dish is often served with rice and is \nconsidered a classic comfort food in China.\nTranslation: Angelus 1st Grand Cru \nClass\u00e9 Chateau Angelus 1996 \nSaint-Emilion Grand Cru Appellation \nSaint-Emilion Grand Cru Controlled\nPrompt: What are all the scene text in the image?\nGPT-4V\uff1a\nThe image you sent me is a label for a bottle of Ch\u00e2teau Ang\u00e9lus, \na Bordeaux wine from Saint-\u00c9milion. The specific vintage in the \nimage is 1996. Ch\u00e2teau Ang\u00e9lus is a Premier Grand Cru Class\u00e9 A, \nthe highest classification for wines in Saint-\u00c9milion. \nGemini Pro: \nPrompt: What is this?\nThe wine is made from a blend of Merlot (80%) and Cabernet Franc (20%).\nThe bell on the label is a reference to the Angelus bell, which is rung three times a day in the village of \nSaint-\u00c9milion. The first time is at dawn, the second time at midday, and the third time at sunset. The bell is \nsaid to have been rung to call the workers in the vineyards to prayer.\nCh\u00e2teau Ang\u00e9lus is one of the most famous and sought-after wines in the world. The 1996 vintage is \nconsidered to be one of the best, and it can sell for over $1,000 per bottle.\nPrompt: What is it?\nFigure 62: Multilingual scene text recognition. Both GPT-4V and Gemini can recognize scene text\nin all of the three images.\n75\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 9.2 Multilingual Scene Text Recognition\nPrompt: If the image is \npresented in a language other \nthan English, translate the \ncontent to English.\nEnglish translation: \"Pictures [edit | edit wikitext] Stegosaurus's brain \nwas located in its small head.\" \nGPT-4V\uff1a\nGemini Pro: \nGPT-4V\uff1a\nThe content in the image is in Portuguese and translates to \"Double \ntap to add a title. Double tap to add a subtitle.\"\nThe top text says: \"Pictures\".\nThe bottom text says: \"Stegosaurus's brain was located in its small \nhead.\"\nPrompt: If the image is \npresented in a language \nother than English, translate \nthe content to English.\nGemini Pro: \nThe phrase \"Fa\u00e7a duplo toque para adicionar um t\u00edtulo\" in the image \nmeans \"Double tap to add a title\". It looks like a prompt on a slide in a \npresentation, perhaps a PowerPoint. The prompt is instructing the \nuser to double tap on the screen in order to add a title to the slide.\nPrompt: Translate it into English.\nPrompt: Translate it into English.\nFigure 63: Multilingual text recognition and translation. Both GPT-4V and Gemini can recognize\nscene text in all of the two images.\n76\n10\nIndustry Application\nIn this section, we delve into the application of two large-scale models in the industrial sector,\nexploring the potential for commercial use of vision-based large models and the feasibility of\ncustomizing these models for specific industrial niches. Sec. 10.1 focuses on defect detection: This\nsection demonstrates the models\u2019 capabilities in detecting industrial product defects on production\nlines, with Gemini exhibiting a higher accuracy in identifying defective items. Sec. 10.2 covers\nsupermarket self-checkout systems: Here, Gemini excels in the accurate identification of items\nand providing corresponding images, showcasing its applicability in retail environments. Sec. 10.3\naddresses applications in auto insurance: This involves the use of large models for assessing accident\nscenes and the extent of vehicle damage for insurance claims. Sec. 10.4 presents a customized\ncaptioner: It highlights the task of identifying the relative positions of objects within a scene, where\nGPT-4V provides more accurate results. Sec. 10.5 discusses image generation evaluation: This\nsection shows tasks related to assessing the match between generated images and text, with both\nmodels performing well. Sec. 10.6 explores Embodied AI and smart home applications: GPT-4V\noffers more precise answers, while Gemini excels in detailed reasoning processes. Sec. 10.7 is about\ngraphical user interface navigation: It examines web searches, mobile app ordering, and receiving\nnotifications. In these tasks, each model has its strengths, with Gemini being adept at extracting\ndetailed information, while GPT-4V slightly outperforms in certain tasks.\n10.1\nIndustry: Defect Detection\nFig. 64 to Fig. 66 illustrate the task of detecting industrial defects, particularly focusing on identifying\ndefective products within assembly line operations. It involves the inspection of nuts, textile products,\nmetal items, pharmaceuticals, car tires, and more. Overall, both models perform well. Gemini\nexhibits a more accurate and superior performance in this task, demonstrating its proficiency in\ndiscerning subtle anomalies and irregularities in industrial products.\n10.2\nIndustry: Grocery Checkout\nFig. 67 illustrates the application of the model in a supermarket self-checkout system, where it\nidentifies items in the shopping cart, enabling the next step of the checkout process. In this context,\nGemini not only demonstrates high accuracy in item recognition but also has the capability to provide\ncorresponding images of the identified items. If integrated with a database, the model can provide\neven more accurate results.\n10.3\nIndustry: Auto Insurance\nFig. 68 to Fig. 70 illustrate the application of these models in assessing automobile accidents for\ninsurance claims. These models are employed to preliminarily evaluate the severity of the accident\nscene and the extent of damage to the vehicles, subsequently providing insights and recommendations.\nHere, we find that Gemini tends to provide more detailed responses and more comprehensive\nrecommendations.\n10.4\nIndustry: Customized Captioner\nFig. 71 involves a task where images of individual objects are provided initially, followed by a\ncomposite scene containing these objects. The challenge for the model is to identify the relative\npositions of these objects within the scene. In this task, GPT-4V exhibits a more accurate performance,\ndemonstrating its advanced capability in spatial recognition and object localization in complex settings.\nThis could also be attributed to Gemini\u2019s inability to remember multiple images.\n10.5\nIndustry: Evaluation Image Generation\nFig. 72 to Fig. 74 showcase a task centered on rating generated images. The primary focus here is the\nassessment of the congruence between the images and the accompanying text. The text prompt in the\nimage is a parrot driving a car. This method of evaluating image quality is more objective compared\nto a case study. Here, both GPT-4V and Gemini are able to provide fairly accurate judgments.\n77\n10.6\nIndustry: Embodied Agent\nFig. 75 to Fig. 78 focus on the applications of Embodied AI and smart home technologies. In\nthese scenarios, GPT-4V consistently provides more accurate responses. However, it is noteworthy\nthat Gemini offers a more detailed reasoning process. This depth in reasoning could potentially\nenhance the cognitive processing of intelligent agents, suggesting a trade-off between precision and\nthe richness of cognitive reasoning in AI applications.\n10.7\nIndustry: GUI Navigation\nFig. 79 to Fig. 83 depict the process of conducting a web search for a Mapo Tofu recipe. Throughout\nthis task, both models exhibit comparable proficiency, with Gemini showing a tendency to extract\nmore detailed information from web pages. Fig. 84 to Fig. 92 illustrate an example of using a\nmobile phone to order a specific keyboard. In this scenario, GPT-4V demonstrates a slight edge in\nperformance. Figures Fig. 93 to Fig. 94 focus on the process of receiving notifications, where Gemini\nis capable of providing a wider range of options. Fig. 95 showcases the ability to view short videos,\nwith both models performing admirably in this respect.\n78\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.1 Industry: Defect Detection\nGPT-4V\uff1a\nThere is a small hole in the object, which appears to be a hazelnut. This could be a sign of\ninsect damage or rot.\nGemini Pro: \nThe hazelnut in the image has a hole in the shell. This could be caused by a number of things,\nsuch as a bug eating through the shell, a disease, or physical damage.\nPrompt:\nWhat\nis\nwrong\nwith\nthe\nobject in the image?\nPrompt:\nWhat is wrong with the\nobject in the image?\nGPT-4V\uff1a\nThere is a small tear or hole in the fabric.\nGemini Pro: \nThe object in the image is a piece of fabric. There is a hole in the fabric, which is likely caused by\nwear and tear. The hole is located in the center of the fabric, and it is about 1 inch in diameter. The\nfabric around the hole is frayed and worn, which suggests that the hole has been there for some time.\nPrompt:\nWhat\nis\nwrong\nwith\nthe\nobject in the image?\nGPT-4V\uff1a\nThe object in the image appears to be a screw or bolt with a damaged or stripped head. The\ngrooves in the head, which would normally allow a screwdriver or wrench to grip and turn the screw,\nare worn away and no longer functional.\nGemini Pro: \nThe screw in the image is broken. The head of the screw is missing, and the shank of the\nscrew is bent. This could be caused by a number of things, such as overtightening the screw,\nusing the wrong size screwdriver, or using the screw in a material that is too hard..\nFigure 64: Defect detection (1/2). Both models are able to detect defects in the objects within the\nimages with a fair degree of accuracy. However, it\u2019s notable that Gemini provides more details in this\ntype of task.\n79\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.1 Industry: Defect Detection\nGPT-4V\uff1a\nIt is difficult to tell exactly what the object in the image is, but it appears to be a car bumper or\nfender with a large dent in it. The problem with the object is that it is damaged and dented,\nwhich may affect the appearance and functionality of the car.\nGemini Pro: \nThe object in the image is a car with a crack in the bumper. The crack is located on the left side\nof the bumper, and it is about 6 inches long. The crack is deep enough that it goes all the way\nthrough the bumper, and there is a small hole at the end of the crack.\nPrompt:\nWhat\nis\nwrong\nwith\nthe\nobject in the image?\nPrompt:\nWhat\nis\nwrong\nwith\nthe\nobject in the image?\nGPT-4V\uff1a\nIt is difficult to say for sure what is wrong with the object in the image as there is no context or\ninformation about what the object is or what it is supposed to look like. However, it appears that there\nmay be a crack on one of the edges of the object.\nGemini Pro: \nThe nut in the image is broken. It appears to be cracked on one side, and the threads are\ndamaged. This could have been caused by excessive torque, or by using the wrong size wrench. A\nbroken nut can be a safety hazard, and should be replaced immediately.\nPrompt:\nWhat\nis\nwrong\nwith\nthe\nobject in the image?\nGPT-4V\uff1a\nThere is not enough information provided to determine if there is anything wrong\nwith the object in the image. It appears to be a capsule medication with the number 500 printed\non it, but without further context, it is impossible to know if there is anything wrong with it.\nGemini Pro: \nThe capsule in the image is broken. The top of the capsule is cracked and separated from the\nbottom. This could have been caused by dropping the capsule, or by applying too much pressure to it.\nA broken capsule is unusable, and should be discarded.\nFigure 65: Defect detection (2/2). Both are able to accurately identify defects in the objects within\nthe images. Specifically, when it comes to damaged capsule detection, Gemini outshines by exhibiting\na stronger ability to spot defects.\n80\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.1 Industry: Defect Detection\nGPT-4V\uff1a\nThere is large amount of brake on the rim of the wheel. This is typically caused by the brake\npads wearingdown and creating dust as thev come in contact with the rotors. Additionallv. the tire\nappears to be low on airpressure, which can affect the vehicle's handling and fuel economy.\nGemini Pro: \nThe wheel in the image is dirty and needs to be cleaned. The dirt is mostly on the rim of the\nwheel, but there is also some dirt on the spokes. The dirt is likely a combination of brake dust, road\ngrime, and other environmental factors.\nPrompt:\nWhat\nis\nwrong\nwith\nthe\nobject in the image?\nPrompt:\nWhat is wrong with the\nobject in the image?\nGPT-4V\uff1a\nYes, there are a few defects seen on the object in image 2:\n- There is a significant amount of dirt and grime on the wheel, which is not present in image 1.\n- The center cap is missing from the wheel in image 2.\n- There appears to be some damage to the outer edge of the rim, which is not present in image 1.\n- The tire tread appears to be more worn in image 2 than in image 1.\nGemini Pro: \nIn the right image, the car is missing a hubcap on the front left wheel. The hubcap is the \ndecorative cover that goes over the wheel hub. \nLeft image 1 and Right image 2 with missing hubcap highlighted.\nFigure 66: Defect similar detection. When presented with a single tire missing a hubcap, both\nGPT-4V and Gemini fail to detect the defect. However, given two tires with missing hubcaps, both\nmodels are able to identify the defects. This, to some extent, confirms that the ability of both GPT-4V\nand Gemini to detect anomalies can be reinforced through comparison or presence of multiple\ninstances of similar defects.\n81\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.2 Industry: Grocery Checkout\nGPT-4V\uff1a\nPink tulips, salmon fillets, strawberries, apple, greek yogurt, sliced ham, raspberries\nGemini Pro: \nThe basket contains the following products:\nApples\nStrawberries\nYogurt\nHam\nPrompt: For below image, find what \nproducts are presented in the basket. Try \nto be accurate.\nFigure 67: Grocery checkout scenario. While GPT-4V provides more comprehensive and detailed\nresults, some errors are noted. On the other hand, Gemini only identifies four objects but does so\ncorrectly and even includes corresponding images for each item. This indicates that while GPT-4V\noffers breadth in its findings, Gemini offers accuracy, demonstrating varied strengths in performing\nsuch tasks.\n82\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.3 Industry: Auto Insurance\nGPT-4V\uff1a\nBased on the image, it appears that the car has sustained damage to the front passenger side\nbumper. The damage appears to be in the form of scrapes and scratches, with some areas of the paint\nchipped away. The damage appears to be cosmetic and does not appear to have affected\nthe structural integrity of the bumper or the car. However, it is important to note that the image\nonly shows one angle of the car and does not provide a full view of the damage. A more thorough\ninspection of the car would be needed to fully assess the extent of the damage and determine if there are\nany underlying issues that are not visible in the image\nGemini Pro: \nBased on the image you provided, the car damage is moderate. The front bumper is\nscratched and dented, and the headlights are cracked. The damage is likely the result of a\nminor collision, such as hitting another car or a curb.\nPrompt:\nImagine that you are an expert in\nevaluating the car damage from car\naccident for auto insurance reporting.\nPlease evaluate the damage seen in\nthe image below.\nPrompt:\nImagine\nthat\nyou\nare\nan\nexpert\nin\nevaluating\nthe\ncar\ndamage\nfrom\ncar\naccident\nfor\nauto\ninsurance\nreporting.\nPlease evaluate the damage seen in the\nimage below.\nGPT-4V\uff1a\nBased on the image, it appears that the vehicle has sustained significant damage to the front\nend. The hood is crumpled and the front bumper is partially detached. The headlights also appear to be\ndamaged. It is likely that the vehicle was involved in a front-end collision. The damage appears to be\nprimarily cosmetic, but it is possible that there may be underlying structural damage to the frame or\nengine. Further inspection would be required to assess the full extent of the damage. In terms of repair\ncosts, the hood and bumper would likely need to be replaced, and the headlights may also need to be\nreplaced or repaired. The cost of these repairs would depend on the make and model of the vehicle, but\nit is likely that the cost would be several thousand dollars at minimum.\nGemini Pro: \nBased on the image you provided, the car damage is severe. The front end of the car is\ncrushed, and the airbags have deployed. The damage is likely the result of a major collision, such as a\nhead-on crash.\nFront end: The front end of the car is crushed, including the hood, bumper, and fenders. The damage\nis so severe that the engine compartment is exposed.\nAirbags: The airbags on the driver's and passenger's sides have deployed. This indicates that the\ncollision was very severe.\nThe total cost to repair the damage is estimated to be more than $5,000. This estimate includes the cost\nof parts and labor.\nIt is possible that the car is totaled. This means that the cost to repair the damage is greater than the\nvalue of the car. If the car is totaled, the insurance company will pay you the value of the car and take\npossession of the car.\nFigure 68: Auto damage evaluation (1/2). Both demonstrate their ability to perform a comprehensive,\ndetailed, and accurate assessment of the damage.\n83\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.3 Industry: Auto Insurance\nGPT-4V\uff1a\nBased on the image, it appears that the car has sustained damage to the rear bumper and \nquarter panel on the driver's side. The bumper appears to be dented and scratched, and the \nquarter panel has a large dent and some scuff marks. It is likely that the car was hit from the side, \ncausing the damage to the quarter panel and bumper. In terms of repair costs, the bumper may be able \nto be repaired with some minor body work and repainting. However, the quarter panel may require more \nextensive repairs or even replacement, which could be more costly. Overall, I would estimate the \ndamage to be in the moderate to severe range, with repair costs likely in the thousands of dollars.\nBased on the image you provided, the car damage is minor. The rear bumper is scratched \nand dented, but the damage does not appear to be deep. The damage is likely the result of a minor \ncollision, such as being bumped by another car in a parking lot.\nPrompt: \nImagine that you are an expert in \nevaluating the car damage from car \naccident for auto insurance reporting. \nPlease evaluate the damage seen in \nthe image below. \nPrompt: \nImagine that you are an expert in \nevaluating the car damage from car \naccident for auto insurance reporting. \nPlease evaluate the damage seen in the \nimage below. \nGPT-4V\uff1a\nBased on the image, it appears that the vehicle has sustained significant damage to the front \nend. The hood is crumpled and the front bumper is completely detached from the vehicle. The \nheadlights and grille are also damaged and the engine appears to be exposed. Additionally, there is \ndamage to the driver's side fender and door. Given the extent of the damage, it is likely that the vehicle \nwas involved in a high-speed collision or a head-on collision. The cost of repairs for this vehicle \nwill likely be significant and it may be deemed a total loss by the insurance company. \nGemini Pro: \nBased on the image you provided, the car damage is moderate to severe. The front bumper \nis crushed, the headlights are cracked, and the hood is deformed. The damage is likely the \nresult of a major collision, such as a head-on crash or a T-bone.\nFront bumper: The bumper is crushed and pushed back into the engine compartment. The damage is so \nsevere that the bumper is no longer functional and needs to be replaced.\nHeadlights: Both headlights are cracked and need to be replaced.\nHood: The hood is deformed and needs to be repaired or replaced.\nAdditional damage may also be present, such as damage to the engine, radiator, and other internal \ncomponents. It is important to have a qualified mechanic inspect the car to assess all of the damage.\nThe total cost to repair the damage is estimated to be several thousand dollars. This estimate includes \nthe cost of parts, labor, and any necessary towing and storage fees. It is possible that the car is \ntotaled. This means that the cost to repair the damage is greater than the value of the \ncar. If the car is totaled, the insurance company will pay you the value of the car and take possession of \nthe car.\nGemini Pro: \nFigure 69: Auto damage evaluation (2/2). Both demonstrate their ability to perform a comprehensive,\ndetailed, and accurate assessment of the damage.\n84\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.3 Industry: Auto Insurance\nGPT-4V\uff1a\n{\"make\": \"Maruti Suzuki\", \"model\": \"Celerio\", \"license plate\": \"KL 52 N 5305\", \"damage\ndescription\": \"Front bumper, hood, and windshield damaged. Airbags deployed.\",\n\"estimated cost of repair\": \"N/A\"}\nAs a language model, I'm not able to assist you with that.\nPrompt:\nImagine that you are an expert in evaluating the car\ndamage\nfrom\ncar\naccident\nfor\nauto\ninsurance\nreporting.\nPlease fill the incident report for the accident shown\nin image below, with the following format in JSON\n(note xxx is placeholder, if you cannot determine that\nbased on the image, put \"N/A\" instead).\n{\"make\": xxx, \"model\": xxx, \"license plate\": xxx,\n\"damage description\": xxx, \"estimated cost of repair\":\nxxx}\nGemini Pro: \nFigure 70: Insurance reporting. Only GPT-4V is able to generate an insurance report according\nto the given format and content requirements, while Gemini does not exhibit the ability to produce\noutputs based on a specific format. Thus, while GPT-4V demonstrates proficiency in structured\noutput generation, Gemini\u2019s performance is less satisfactory in this context.\n85\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.4 Industry: Customized Captioner\nDense Captioning w/ Segmentation\nGPT-4V\uff1a\nObject 1 is a close-up of a snail shell. The shell is a light beige color with a spiral pattern on\nthe top. The shell appears to be slightly weathered, with some small chips and cracks visible\non the edges. The background is white, making the shell the focal point of the image. The shell\nis positioned at a slight angle, with the spiral pattern on the top facing towards the right side of\nthe image.\nObject 2 is a close-up of a snail. The snail has a light orange-colored shell with a spiral pattern.\nThe body of the snail is a darker orange color with small bumps on its skin. The snail's head is\nextended forward and its antennae are visible. The background of the image is white, making\nthe snail the focal point of the image.\nObject 3 is a close-up of a frog with a snail on its head. The frog is light green in color with a\npinkish hue on its underside. It has large, bulging eyes that are black and white. The snail is\norange and is perched on top of the frog's head. The frog is sitting on a rock or other flat\nsurface with its legs tucked underneath its body. The background is white, making the frog and\nsnail stand out.\nObject 4 shows a close-up of a turtle's head and neck, with its body partially visible. The turtle\nappears to be floating in water, with its head and neck extended upwards. The turtle's skin is\ngreen and its shell is grey with black stripes. The shell appears to be damaged, with a large\nchunk missing from the top. The turtle's eyes are closed and its mouth is slightly open.\nThe image you sent me shows three objects: a frog, a snail, and a turtle.\nFrog: The frog in the image is a common tree frog. It has smooth, green skin with a few dark\nbrown spots. It has large, bulging eyes that are red with black horizontal pupils. The frog's legs\nare long and thin, and its toes are webbed for swimming. It is sitting on top of the turtle's\nshell.\nSnail: The snail in the image is a common garden snail. It has a brown, spiral-shaped shell\nthat it can retract into for protection. Its body is gray and slimy. The snail is slowly crawling\nup the side of the turtle's shell.\nTurtle: The turtle in the image is a red-eared slider. It has a hard, green shell with yellow\nstripes. Its head, legs, and tail are green with yellow stripes. The turtle is sitting on a rock in\na pond.\nPrompt:\nDescribe the objects below with as many details as possible, Reference to the content shown in\nthe context image but focus on describing each object below.\nGemini Pro: \nFigure 71: Performing dense captioning with segmentation cut-outs. GPT-4V provides more\ncomprehensive, accurate, and orderly captions, while Gemini falls short, failing to recognize the\npresence of a snail shell in the left image and incorrectly describing the position of the snail. This\nsuggests that while GPT-4V excels in providing detailed and precise descriptions.\n86\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.5 Industry: Evaluation Image Generation\nEvaluation of Generated Images (Prompt-Image Alignment)\nGPT-4V\uff1a\nIn the image, there is a dolphin jumping out of the water. On a scale of 1 to 10, I would rate\nthis image a 1 in similarity to the text prompt \"a parrot driving a car\" as there is no parrot or\ncar present in the image.\nPrompt:\nWhat is happening in the image? From scale 1 to 10, decide\nhow similar the image is to the text prompt \"a parrot driving a\ncar\"?\nThe image you sent shows a dolphin jumping out of the water. It is not very similar to the text\nprompt \"a parrot driving a car\". I would rate the similarity as a 1 out of 10.\nHere are some reasons why the image is not similar to the prompt:\n\u00b7The image shows a dolphin, which is a mammal, while the prompt mentions a parrot, which is a bird.\n\u00b7Dolphins do not drive cars, while parrots are not typically associated with driving.\n\u00b7The image shows the dolphin in a natural setting, while the prompt suggests a more artificial setting.\nOverall, the image and the prompt are very different.\nPrompt:\nWhat is happening in the image? From scale 1 to 10,\ndecide how similar the image is to the text prompt \"a parrot\ndriving a car\"?\nGPT-4V\uff1a\nIn the image, there is a parrot flying in the sky with trees and clouds in the background. I would rate this\nimage a 2 in similarity to the text prompt \"a parrot driving a car\" as there is a parrot present in the image,\nbut it is not driving a car.\nThe image you sent is a cartoon of a toucan, not a parrot, flying over a lush green rainforest. The toucan\nhas a big, colorful beak and is colored black, white, and orange. It is flying with its wings spread wide and\nits beak pointed downwards. There are no cars in the image. The image is not very similar to the text\nprompt \"a parrot driving a car\". I would rate the similarity as a 2 out of 10.\nHere are some reasons why:\n\u00b7The image shows a toucan, not a parrot.\n\u00b7Toucans cannot drive cars.\n\u00b7The image shows the toucan in a natural setting, while the prompt suggests a more urban setting.\nOverall, the image and the prompt are quite different. I hope this is helpful!\nGemini Pro: \nGemini Pro: \nFigure 72: Evaluation of generated images-prompt alignment (1/3). Both models accurately\ndescribe the image content and provide consistent scores with respective explanations. Notably,\nGemini gives a more detailed rationale for its evaluations.\n87\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.5 Industry: Evaluation Image Generation\nEvaluation of Generated Images (Prompt-Image Alignment)\nPrompt:\nWhat is happening in the image? From scale 1 to 10, decide\nhow similar the image is to the text prompt \"a parrot driving a\ncar\"?\nPrompt:\nWhat is happening in the image? From scale 1 to 10,\ndecide how similar the image is to the text prompt \"a parrot\ndriving a car\"?\nGPT-4V\uff1a\nIn the image, there is a parrot perched on the side of a car. It is not driving the car, but it is on the car. I\nwould rate this image a 4 in terms of similarity to the text prompt \"a parrot driving a car\" because there\nis a parrot and a car present, but the parrot is not driving the car.\nThe image you sent me shows a colorful macaw perched on the open window of a car. The parrot is\nlooking out the window, and its feet are resting on the edge of the door. There is no one else in the car.\nI would rate the similarity between the image and the text prompt \"a parrot driving a car\"\nas a 7. The parrot is indeed perched in the driver's seat, and it is looking out the window as if it is driving.\nHowever, there are no other cues to suggest that the parrot is actually in control of the car. For example,\nthe parrot's feet are not on the pedals, and there is no steering wheel in front of it.\nGPT-4V\uff1a\nIn the image, there is a parrot's head superimposed onto a car driving on a road. The parrot's head is in\nthe driver's seat and the car appears to be in motion. On a scale of 1 to 10, I would rate this image an 8\nin terms of similarity to the text prompt \"a parrot driving a car\". The image clearly shows a parrot in the\ndriver's seat of a car, but the parrot's body is not visible and the car is not clearly a \"car\" but more of a\nblurred image.\nThe image you sent me is a still from a computer animation training video, and it is indeed very similar to\nthe text prompt \"a parrot driving a car.\" I would rate the similarity as a 9 out of 10.\nHere are the reasons why the image is similar to the prompt:\n\u00b7The image shows a green car driving past a painting of a parrot. This directly matches the prompt's\ndescription.\n\u00b7The parrot in the painting is wearing a helmet and goggles, which suggests that it is driving the car.\n\u00b7The car is moving quickly, and the parrot's expression is determined, which further reinforces the idea that\nit is in control of the vehicle.\nThe only reason why the image is not a perfect 10 out of 10 match to the prompt is that the\nparrot is not physically driving the car. It is simply a painting on the side of the car. However, the\noverall impression of the image is still that of a parrot driving a car, so the prompt is still very accurate.\nI hope this helps!\nGemini Pro: \nGemini Pro: \nFigure 73: Evaluation of generated images-prompt alignment (2/3). Both models successfully\ninterpret the image content, assign reasonable scores, and provide relevant explanations. However,\nGemini tends to assign higher scores and stands out for its and offer more detailed justifications,\ndespite incorrectly identifying some aspects of the image.\n88\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff10.5\uffffIndustry:\uffffEvaluation\uffffImage\uffffGeneration\nEvaluation of Generated Images (Prompt-Image Alignment)\nPrompt:\nWhat is happening in the image? From scale 1 to 10, decide \nhow similar the image is to the text prompt \u201ca cake on the table \nwith word \u201cAzure Research\u201d written on it\"?\nGPT-4V\uff1a\nIn the image, there is a cake on a table with the words \"Azuze Research\" written on it. I would rate this \nimage an 8 out of 10 in similarity to the text prompt \"a cake on the table with word 'Azure Research' \nwritten on it\" because the only difference is the misspelling of \"Azure\" as \"Azuze.\"\nThe image you sent me is very similar to the text prompt \u201ca cake on a table with the word \u201cAzure \nResearch\u201d written on it\u201d, and I would rate it a 9 out of 10.\nThe image shows a square cake with the words \u201cAzuze Research\u201d written on it in icing. The cake is \nsitting on a white table, and there is a fork to the left of the cake. The icing on the cake is light blue, \nwhich could be interpreted as azure. The cake is also decorated with white and pink icing flowers.\nOverall, the image is a very close match to the text prompt. The only difference is that the word \u201cAzure\u201d \nis misspelled as \u201cAzuze\u201d. However, this is a minor difference, and the overall impression of the image is \nthat it is of a cake with the word \u201cAzure Research\u201d written on it.\nPrompt:\nWhat is happening in the image? From scale 1 to 10, decide how \nsimilar the image is to the text prompt \u201ca cake on the table with \nword \u201cAzure Research\u201d written on it\"?\nGPT-4V\uff1a\nI appreciate your feedback and agree with your assessment. The image does indeed show a cake on a \ntable with the word \"Azure\" written on it, but the word \"Research\" is missing. This \ndiscrepancy makes the image less similar to the text prompt than it could be.\nOverall, I think your rating of 8 out of 10 is fair and accurate. It accurately reflects the image's strengths \n(the presence of a cake and the word \"Azure\") and weaknesses (the absence of the word \"Research\").\nThank you for helping me to improve my understanding of how to evaluate image similarity.\nIn the image, there is a cake on a table with the words \"Azure Aurre\" written on it. I would rate this \nimage an 8 out of 10 in similarity to the text prompt \"a cake on the table with word 'Azure Research' \nwritten on it\" because the image does show a cake on a table with the word \"Azure\" written on it, but the \nword \"Research\" is not present.\nGemini Pro: \nGemini Pro: \nFigure 74: Evaluation of generated images-prompt alignment (3/3). Both models accurately\ndescribe the image content and provide consistent scores with respective explanations. Notably,\nGemini provides a more detailed rationale for its evaluations, and assign higher scores to better\nimages.\n89\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff10.6\uffffIndustry:\uffffEmbodied\uffffAgent\nOperating\uffffMachine\nNow, let's look at the \noperating panel of the \ncoffee machine in the \nimage \nbelow, \nwhich \nbutton should I use for a \n8 OZ coffee: left, right, \ntop or bottom button?\nGPT-4V\uff1a\nYou should use the bottom button for an 8 OZ coffee. According to the menu, the bottom button \ncorresponds to a 8 OZ coffee, while the top button is for a 6 OZ coffee and the right button is for \na 10 OZ coffee.\nBased on the operating menu and the coffee machine panel you sent, the button you should use for an \n8 oz coffee is the second button from the top. Here's why:\nThe operating menu on the left shows the button functions for the coffee machine on the right.\nThe second button from the top on the coffee machine panel has an image of an 8 oz cup with coffee in \nit. This is a clear indication that this button is for brewing an 8 oz cup of coffee.\nThe other buttons have different symbols or numbers that correspond to different brewing options. For \nexample, the top button has a symbol that looks like a larger cup of coffee, so it's likely for brewing a \nlarger size.\nTherefore, I'm confident that the second button from the top is the one you should use for an 8 oz \ncoffee.\nPrompt: First, read the operating \nmenu below for coffee machine \nbuttons.\nMenu:\nPrompt:\nFirst, read the operating menu below for coffee machine buttons. Menu is the left image. Now, \nlet's look at the operating panel of the coffee machine in the right image, which button should I \nuse for a 6 OZ coffee: left, right, top or bottom button?\nGemini Pro: \nFigure 75: Embodied agent - operating machine. GPT-4V delivers correct responses along with\nclear and concise explanations. In contrast, Gemini provides entirely incorrect answers.\n90\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.6:  Industry: Embodied Agent\nNavigation\nPrompt:\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the \nfridge. The image below shows your current position. Please plan your next action.\nGPT-4V\uff1a\nMy next action would be turn right and move forward towards the hallway, as I can see that the \nkitchen is likely located in that direction. I would then continue to navigate through the hallway until I \nreach the kitchen and locate the fridge.\nBased on the image, you are currently in a living room with a couch, a television, and a fireplace. To get \nto the kitchen, I would recommend the following steps:\n1. Move around the couch: Since the couch is blocking your path to the kitchen, you would need to \nmove around it. You can either go to the left of the couch or the right.\n2. Navigate the hallway: Once you are past the couch, you will need to navigate the hallway to get to \nthe kitchen. Be careful not to bump into any walls or furniture.\n3. Open the kitchen door: When you reach the kitchen door, open it and enter the kitchen.\n4. Locate the fridge: Once you are in the kitchen, locate the fridge. It is usually located against a wall or \nin a corner.\n5. Open the fridge door: Open the fridge door and retrieve the item that you were asked to get.\n6. Return to the living room: Close the fridge door and return to the living room by following the same \nsteps you took to get to the kitchen.\nHere are some additional tips for navigating your home as a robot:\n\u00b7 Use a map of your home to help you plan your route.\n\u00b7 Be aware of your surroundings and avoid obstacles.\n\u00b7 Move slowly and carefully to avoid bumping into anything.\n\u00b7 If you get stuck, ask for help from a human.\nGemini Pro: \nFigure 76: Embodied agent - indoor navigation (1/3). GPT-4V provides accurate answers along\nwith clear and succinct instructions. In contrast, Gemini produces prolonged and unnecessary\ndirections.\n91\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.6 Industry: Embodied Agent\nNavigation\nPrompt:\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the \nfridge. At the previous turn, you were at the position shown in the below image: \nNote: Same image from the previous turn, resized to save space.\nThe action taken in previous turn is: turn right and move forward towards the hallway. \nNow you are at the hallway, this is what you can see from there. Can you see the kitchen from here? \nWhat is your next action?\nGPT-4V\uff1a\nFrom the current position, I can see the kitchen on the right side, My next action would be to turn right \nand move forward towards the kitchen.\nYes, I can see the kitchen from the hallway in the image you sent. The kitchen appears to be the \ndoorway on the left, at the end of the hallway, with the light wood trim around it.\nMy next action would be to move forward down the hallway towards the kitchen. I would use my sensors \nto avoid any obstacles in my path, such as the shoes on the floor or the chair against the wall.\nOnce I reach the kitchen doorway, I would open the door and enter the kitchen. Then, I would be able to \nlocate the fridge and retrieve the item that I was asked to get.\nHere are some additional details about my plan that I can incorporate based on the image:\n\u00b7The hallway appears to be well-lit, which will make it easier for me to navigate.\n\u00b7There is a mat on the floor in front of the kitchen doorway. I would be careful not to step on the mat, \nas it could be slippery.\n\u00b7The kitchen door is slightly ajar. I would be careful not to bump into the door as I open it.\nPrompt:\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from \nthe fridge. \nAt the previous turn, you were at the position shown in the above small image: \nNote: Same image from the previous turn, resized to save space.\nThe action taken in previous turn is: turn right and move forward towards the hallway. \nNow you are at the hallway(the below image), this is what you can see from there. Can you see \nthe kitchen from here? What is your next action?\nGemini Pro: \nFigure 77: Embodied agent - indoor navigation (2/3). GPT-4V provides accurate responses, while\nGemini produces incorrect analysis and directions.\n92\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff10.6\uffffIndustry:\uffffEmbodied\uffffAgent\nNavigation\nPrompt:\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the \nfridge. At the previous turn, you were at the position shown in the below image: \nNote: Same image from the previous turn, resized to save space. The action taken in previous turn \nis: move forward and turn slightly to the right to approach the fridge.\nNow this is what you can see from there. What is your next action?\nGPT-4V\uff1a\nMy next action would be to move forward and slightly to the left to align myself with the fridge door. \nThen, I would use my robotic arm to open the fridge door and retrieve the requested item.\nBased on the image you sent, it appears I'm facing the slightly open refrigerator door. My next action \nwould depend on the specific item I need to retrieve:\n1. If the item is small and easily reachable: I would carefully extend my arm or gripper into the \nfridge, making sure not to hit any shelves or stored items. Once I locate the item, I would grasp it \nsecurely and retract my arm, then close the refrigerator door and return to the living room.\n2. If the item is large or requires opening containers: I would first need to assess if I can reach and \nmanipulate the item without assistance. If not, I would return to the living room and inform the \nperson who requested the item that I require help accessing it.\nHere are some additional details I can consider based on the image:\n\u00b7 The refrigerator door is slightly ajar, which is convenient for me to open further.\n\u00b7 The floor appears clear of obstacles, allowing me to move closer to the fridge.\n\u00b7 There is some light coming from inside the fridge, which helps me see the contents.\nPrompt: Imagine that you are a home robot, and is asked to go to the kitchen to fetch \nsomething from the fridge.  At the previous turn, you were at the position shown in the above \nsmall image: \nNote: Same image from the previous turn, resized to save space. The action taken in previous \nturn is: move forward and turn slightly to the right to approach the fridge.\nNow this is what you can see from the below image. What is your next action?\nGemini Pro: \nFigure 78: Embodied agent - indoor navigation (3/3). Both GPT-4V and Gemini provide correct\nresponses; however, GPT-4V\u2019s explanations are notably more concise.\n93\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nWeb Browsing (Mapo Tofu Recipe)\nPrompt: \nImagine that you are a robot operating a computer. Like how \nhumans operate the computer, you can move the mouse, click \nan icon with the mouse, or type some texts with the keyboard.\nYou are asked to find a detailed recipe of Mapo Tofu. Below is \nwhat you see on the computer screen, predict your next move. If \nthe action involves moving the mouse, describe the location as \ndetailed as possible.\nGPT-4V\uff1a\nNext move: I would move the mouse to the Google Chrome icon located on the left side of the screen, \napproximately 1/3 of the way down from the top. I would then click on the icon to open the web \nbrowser.\nBased on the image you sent, it looks like you are on a search engine results page for \"mapo tofu \nrecipe\". The top result is a link to \"The Woks of Life\" website, which appears to be a recipe blog. I would \nrecommend clicking on that link to get started on finding a detailed recipe for mapo tofu.\nHere are the steps involved in clicking the link:\nMove the mouse cursor to the search result for \"The Woks of Life: Mapo Tofu Recipe: The Real Deal\". \nThe link is located in the first position on the page, just below the search bar.\nOnce the cursor is hovering over the link, left-click on it once. This will open the recipe in a new tab or \nwindow.\nGemini Pro: \nFigure 79: GUI navigation - web browsing (1/5). GPT-4V delivers accurate and concise responses,\nwhile Gemini fails to recognize the information from the GUI entirely.\n94\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nWeb Browsing (Mapo Tofu Recipe)\nPrompt: Imagine that you are a robot operating a computer. Like how humans operate the \ncomputer, you can move the mouse, click an icon with the mouse, or type some texts with the \nkeyboard. \nYou are asked to find a detailed recipe of Mapo Tofu.\nPrevious action: \"I would move the mouse to the Google Chrome icon located on the left side of \nthe screen, approximately 1/3 of the way down from the top. I would then click on the icon to \nopen the web browser. \"You have now opened the web browser. Below is what you see on the \ncomputer screen. What will you do next? If the action involves moving the mouse, describe the \nlocation as detailed as possible.\nGPT-4V\uff1a\nNext, I would move the mouse to the search bar located in the center of the screen, approximately \n1/2 of the way down from the top. I would then click on the search bar and type in \"Mapo Tofu \nrecipe\" using the keyboard. After typing in the search query, I would press the \"Enter\" key on the \nkeyboard to initiate the search.\nBased on the image you've sent, which shows the Google homepage, here are my next steps to find a \ndetailed recipe for Mapo Tofu:\n1. Type \"mapo tofu recipe\" in the Google search bar. The search bar is located at the very top of \nthe page, directly below the blue Google logo. It's a rectangular box with a magnifying glass icon at the \nleft end.\n2. Press Enter on the keyboard. This will initiate the search and display a list of relevant results on \nthe Google search results page.\nHere's a breakdown of the actions involved in case you need more details:\n\u00b7 Move the mouse cursor to the search bar. Aim for the center of the rectangular box at the top of the \npage.\n\u00b7 Click the left mouse button once. This will place the cursor within the search bar and make it ready \nfor typing.\n\u00b7 Type the words \"mapo tofu recipe\" using the keyboard.\n\u00b7 Press the Enter key on the keyboard. This is typically a large key with the word \"Enter\" or an arrow \nsymbol on it, located on the lower right side of most keyboards.\nOnce you press Enter, the Google search results page will appear, displaying a variety of links to \nwebsites and recipes for Mapo Tofu. You can then start exploring the results to find the most detailed and \nappealing recipe for your needs.\nGemini Pro: \nFigure 80: GUI navigation - web browsing (2/5). Both GPT-4V and Gemini correctly identify the\nGUI and provide accurate answers. However, GPT-4V tends to give concise instructions, whereas\nGemini leans towards offering more detailed directions.\n95\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec.\uffff10.7\uffffIndustry:\uffffGUI\uffffNavigation\nWeb Browsing (Mapo Tofu Recipe)\nPrompt:\nImagine that you are a robot operating a computer. Like \nhow humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts \nwith the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu.\nPrevious action: \" I would move the mouse to the \nsearch bar located in the center of the screen, \napproximately 1/2 of the way down from the top. I would \nthen click on the search bar and type in \"Mapo Tofu \nrecipe\" using the keyboard. After typing in the search \nquery, I would press the \"Enter\" key on the keyboard to \ninitiate the search.\"\nYou have now completed the previous action as \ndescribed above. Below is what you see on the \ncomputer screen. What will you do next? If the action \ninvolves moving the mouse, describe the location as \ndetailed as possible.\nGPT-4V\uff1a\nNext action: \"I would move the mouse to the first search result under the \"Recipes\" section, which \nis titled \"Mapo Tofu Recipe - The Woks of Life\". I would then click on the link to open the webpage \nand view the detailed recipe.\"\nBased on the screenshot you sent, the Google search results page for \"mapo tofu recipe\" is now \ndisplayed. Here are the next steps I would take to find a detailed recipe:\n1. Scan the search results:\n\u00b7 I would first briefly scan the titles and snippets of the top few search results, paying attention to \nkeywords like \"detailed,\" \"authentic,\" \"easy,\" or any other terms that indicate a recipe might be \ncomprehensive. \n\u00b7 I would also look for recipe websites or blogs that I recognize or know to be reputable sources for \nChinese cuisine.\n2. Select a promising recipe:\n\u00b7 Based on my scan, I would select one or two results that seem like they might be a good fit. I would \nprioritize results from well-known recipe websites or blogs, or those with titles that explicitly mention \nbeing detailed or authentic.\n3. Open the recipe:\n\u00b7 Once I've chosen a recipe, I would move the mouse cursor to the corresponding search result link. \nThis will likely be a blue, underlined text title or URL displayed in the search results.\n\u00b7 I would then left-click the link once with the mouse. This will open the recipe in a new tab or window \nof the web browser.\nHere's a more detailed breakdown of the mouse movements involved in selecting and opening a recipe:\n\u00b7 Scan the search results: Move the mouse cursor up and down the page, reading the titles and \nsnippets of each result.\n\u00b7 Locate the chosen recipe: Move the cursor to the link for the recipe you want to open. This might \nbe the first result, or one of the top few results that you've identified as promising.\n\u00b7 Click the recipe link: Make sure the cursor is hovering directly over the blue, underlined text of the \nrecipe title or URL. Then, click the left mouse button once.\nI hope this helps you find the perfect Mapo Tofu recipe! Let me know if you have any other questions.\nGemini Pro: \nFigure 81: GUI navigation - web browsing (3/5).Both GPT-4V and Gemini correctly identify the\nGUI, however, GPT-4V provides accurate and succinct instructions, while Gemini delivers a vague\nhint only.\n96\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nWeb Browsing (Mapo Tofu Recipe)\nPrompt:\nImagine that you are a robot operating a computer. Like how \nhumans operate the computer, you can move the mouse, click an \nicon with the mouse, or type some texts with the keyboard. You \nare asked to find a detailed recipe of Mapo Tofu.\nPrevious action: \u201cI would move the mouse to the first search \nresult under the \"Recipes\" section, which is titled \"Mapo Tofu \nRecipe - The Woks of Life\". I would then click on the link to open \nthe webpage and view the  detailed recipe.\"\nYou have now completed the previous action as described above. \nBelow is what you see on the computer screen. Based on the \nimage below, what will you do next? If the action involves moving \nthe mouse, describe the location as detailed as possible.\nGPT-4V\uff1a\nNext action: \"I would move the mouse to the \"Jump to Recipe\" button located at the top right corner \nof the webpage. I would then click on the button to jump to the detailed recipe section of the page.\"\nBased on the image you sent, which shows the Mapo Tofu Recipe webpage on The Woks of Life website, \nhere are my next steps to find the detailed recipe:\n1. Scroll down the webpage:\n\u00b7 Move the mouse cursor to the blank area below the recipe title and ingredients list.\n\u00b7 Scroll down slowly using the mouse scroll wheel or by dragging the scroll bar on the right side of the \npage.\n2. Locate the recipe instructions:\n\u00b7 As you scroll, look for a section titled \"Instructions\" or \"Steps\" where the detailed cooking process is \ndescribed. This is typically located after the ingredient list and any introductory text about the recipe.\n3. Start reading the instructions:\n\u00b7 Once you find the instructions section, you can begin reading the recipe steps. These will usually be \nnumbered or bulleted for clarity.\nHere's a more detailed breakdown of the mouse movements involved:\n\u00b7 To scroll down the webpage, move the mouse cursor to the blank area below the recipe title and \ningredients list.\n\u00b7 If using a scroll wheel, rotate it slowly forward to scroll down. If using a scroll bar, click and hold the \nscroll bar with the left mouse button and drag it down the side of the page.\n\u00b7 When you reach the \"Instructions\" or \"Steps\" section, stop scrolling and start reading the recipe steps.\nGemini Pro: \nFigure 82: GUI navigation - web browsing (4/5). Both GPT-4V and Gemini correctly discern the\ngeneral content of the GUI. However, GPT-4V provides accurate and succinct instructions, while\nGemini merely delivers a vague hint and fails to recognize the detailed information of the GUI.\n97\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nWeb Browsing (Mapo Tofu Recipe)\nPrompt:\nDescribe\nthe\nimage\nbelow as detailed as possible.\nGPT-4V\uff1a\nThe image is a recipe for Mapo Tofu, a Chinese dish. The\nrecipe is presented in a structured format with a title,\ndescription, prep time, cook time, total time, course, cuisine,\nservings, and rating. The title of the recipe is \"Mapo Tofu\" and it\nis written in bold black letters. Below the title, there is a\ndescription of the dish which states that it is the true blue,\nauthentic\nreal\ndeal\n-\nthe\nspicy,\ntongue-numbing,\nrice-is-\nabsolutely-not-optional\nMapo\nTofu\nthat\nyou\nget\nin\nthe\nrestaurants. The prep time for the dish is 10 minutes, the cook\ntime is 25 minutes, and the total time is 35 minutes. The course\nis listed as \"Main Course\" and the cuisine is \"Chinese\". The\nrecipe serves 6 people and has a rating of 4.69 stars out of 5.\nBelow this information, there is a list of ingredients required for\nthe dish, including tofu, ground pork, Sichuan peppercorn,\ngarlic, ginger, soy sauce, and cornstarch. The ingredients are\nlisted in a bullet point format. At the bottom of the image, there\nis\nnutritional\ninformation\nfor\nthe\ndish,\nincluding\ncalories,\ncarbohydrates, protein, fat, saturated fat, cholesterol, sodium,\nand calcium. The nutritional information is presented in a table\nformat. The image also includes a credit to the author of the\nrecipe, Kaitlin, and a link to the original recipe on the website\n\"The Woks of Life\".\nSure, here\u2019s what I can see in the image you sent:\nIt\u2019s a screenshot of a webpage showing a recipe for Mapo Tofu.\nThe top of the webpage shows the recipe title in large letters: \u201cMapo Tofu\u201d.\nBelow the recipe title, there is a rating of 489 out of 299 votes, along with a short description of the\ndish: \u201cThis mapo tofu recipe is the true blue, authentic real deal - the spicy, tongue-\nnumbing, rice-is-absolutely-not-optional mapo tofu that you get in the restaurants!\u201d\nThere is also a note about the recipe source: \u201cby Kaitlin Course: Tofu Cuisine: Chinese\u201d.\nBelow the recipe title and description, there is a section titled \u201cServes:\u201d followed by \u201c6 servings\u201d and\nanother section titled \u201cPrep:\u201d followed by \u201c10 minutes\u201d.\nThen, there are two horizontal lines.\nBelow the lines, there is a list of ingredients, including:\n\u00bd cup oil, divided\n1-2 fresh Thai bird chili peppers, thinly sliced\n6-8 dried red chilies, roughly chopped\n\u00bd-1 \u00bd tablespoons Sichuan peppercorns (powdered or finely ground, reserving 1/4\nteaspoon for garnish at the end; if you want a milder flavor use 1/2 or 1 teaspoon\nground Sichuan peppercorn)\n3 tablespoons ginger, finely minced\n3 tablespoons garlic, finely minced\n8 ounces ground pork\n1-2 tablespoons spicy bean sauce (depending on your desired salt/spice levels)\n2/3 cup low sodium chicken broth (or water)\n1 pound silken tofu, cut into 1 inch/2.5cm cubes\n1/4 cup water\n1 \u00bd teaspoons cornstarch\n1/4 teaspoon sesame oil (optional)\n1/4 teaspoon sugar (optional)\n1 scallion, finely chopped\nGemini Pro: \nFigure 83: GUI navigation - web browsing (5/5). Both GPT-4V and Gemini correctly identify\nthe general content of the GUI and provide detailed explanations, yet each exhibits some minor\ninaccuracies in the details.\n98\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt: \nImagine that you are a robot operating a \nphone. Like how humans operate the \nphone, you can move your finger to an \nicon, click an icon with your finger, scroll \ndown a screen, or type some texts with \nthe keyboard. You are asked to shop for \nan ergonomic keyboard, your budget is \nbetween $50 and $100. Below is what \nyou see on the phone screen, predict \nyour next move. If the action involves \nmoving your finger to an icon, describe \nthe location of that icon as detailed as \npossible.\nGPT-4V\uff1a\nMy next move would be to move my finger to the Amazon icon, which is located in the third row and \nthird column of the screen. I would then click on the icon to open the app and search for an ergonomic \nkeyboard within my budget between $50 and $100...\nBased on the image you sent, which shows the home screen of an iPhone, here are my next steps as a \nrobot operating a phone to shop for an ergonomic keyboard under $100: Open the Shopping app: \nLocate the App Store icon on the bottom dock of the iPhone screen. Tap on the App Store icon to \nopen it.\nSearch for ergonomic keyboards: Once the App Store is open, tap on the search bar at the top of the \nscreen. In the search bar, type in \"ergonomic keyboards.\"\nFilter by price: Once the search results for \"ergonomic keyboards\" are displayed, tap on the \"Filters\" \nbutton. In the filters menu, tap on the \"Price\" option and set the minimum price to $50 and the maximum \nprice to $100.\nhttps://www.apple.com/app-store/\nGemini Pro: \nFigure 84: GUI navigation - online shopping (1/9). GPT-4V correctly identifies the Amazon icon,\nwhereas Gemini mistakenly perceives the App Store as a shopping application.\n99\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt: \nImagine that you are a robot operating a \nphone. Like how humans operate the \nphone, you can move your finger to an \nicon, click an icon with your finger, scroll \ndown a screen, or type some texts with \nthe keyboard. \nYou are asked to shop for an ergonomic \nkeyboard, your budget is between $50 \nand $100.\nYou have now opened the Amazon app. \nBelow is what you see on the phone \nscreen. What will you do next? If the \naction involves moving your finger to an \nicon, describe the location of that icon as \ndetailed as possible.\nGPT-4V\uff1a\nMy next move would be to move my finger to the search bar, which is located at the top of the screen. I \nwould then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing \nin the search term, I would click on the \"Search\" button, which is located on the bottom right corner of the \nkeyboard, to initiate the search.\n\u00b7 Based on the image you sent, which shows the Amazon app home screen, here are my next steps as a \nrobot operating a phone to shop for an ergonomic keyboard under $100:\n\u00b7 Tap on the search bar: The search bar is located at the top of the screen, directly below the \nAmazon logo. It is a white bar with a magnifying glass icon on the left side.\n\u00b7 Type in \"ergonomic keyboard\": Once you tap on the search bar, the keyboard will appear. Type in \n\"ergonomic keyboard\" and then tap on the \"Search\" button on the keyboard, or tap on the magnifying \nglass icon on the right side of the search bar.\n\u00b7 Filter by price: Once the search results for \"ergonomic keyboard\" are displayed, tap on the \"Filters\" \nbutton located just below the search bar. In the filters menu, scroll down until you see the \"Price\" option. \n\u00b7 Tap on \"Price\" and then set the minimum price to $50 and the maximum price to $100. \nYou can do this by tapping on the numbers next to the \"Min\" and \"Max\" fields, or by swiping up or down on \nthe numbers until you reach the desired price range.\nGemini Pro: \nFigure 85: GUI navigation - online shopping (2/9). GPT-4V provides accurate and concise\nresponses, while Gemini offers detailed instructions but misjudges the location of the search bar.\n100\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt: \nImagine that you are a robot operating a \nphone. Like how humans operate the phone, \nyou can move your finger to an icon, click an \nicon with your finger, scroll down a screen, \nor type some texts with the keyboard. \nYou are asked to shop for an ergonomic \nkeyboard, your budget is between $50 and \n$100.\nYou have now completed the previous \naction. Below is what you see on the phone \nscreen. Please predict your immediate next \nmove. If the action involves moving your \nfinger to an icon, describe the location of \nthat icon as detailed as possible.\nGPT-4V\uff1a\nMy immediate next move would be to move my finger to the \"Filters\" button, which is located at the top \nright corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open \nthe filter options and narrow down my search results to only show ergonomic keyboards within my \nbudget range of $50 to $100.\nBased on the image you sent, which shows the Amazon product page for the Perixx PERIBOARD-512 \nergonomic keyboard, here are my next steps as a robot operating a phone to continue shopping for \nan ergonomic keyboard within your $50-$100 budget:\n1. Read product details and reviews: Scroll down the page to read the detailed product information \nabout the Perixx PERIBOARD-512 keyboard. This will include features like the key layout, typing angle, \nwrist rest design, and compatibility with different operating systems. Pay particular attention to the \ncustomer reviews section. Read through both positive and negative reviews to get a well-rounded picture \nof the keyboard's strengths and weaknesses from real users. This can help you decide if it's the right \nchoice for you.\n2. Compare prices and options: If you're not convinced by the Perixx keyboard, or if you want to see \nother options within your budget, tap on the small grey bar at the bottom of the screen that says \"1 Similar \nitem\" or \"2+ Similar items\" (depending on how many similar keyboards Amazon has found). This will open \na new page with a list of similar ergonomic keyboards.\nGemini Pro: \nFigure 86: GUI navigation - online shopping (3/9). GPT-4V provides accurate and concise responses\nand also highlights the most crucial factor to consider currently\u2014the budget. On the other hand,\nGemini mistakenly perceives the search results as a product information page, resulting in incorrect\ninstructions.\n101\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can \nmove your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts \nwith the keyboard. You are asked to shop for an ergonomic keyboard, your budget is between $50 \nand $100.\nPrevious action: \"My immediate next move would be to move my finger to the \"Filters\" button, \nwhich is located at the top right corner of the screen, next to the \"Windows\" button. I would then \nclick on the \"Filters\" button to open the filter options and narrow down my search results to only \nshow ergonomic keyboards within my budget range of $50 to $100. \"\nYou have now completed the previous action as described above. Below is what you see on the \nphone screen. Please predict your immediate next move. If the action involves moving your finger \nto an icon, describe the location of that icon as detailed as possible.\nGPT-4V\uff1a\nMy immediate next move would be to move my finger to the \"Price \n& Deals\" filter option, which is located under the \"Colors\" filter \noption and above the \"Sort by\" filter option. I would then click on the \n\"Price & Deals\" filter option to open the price range filter and set \nmy minimum price to $50 and my maximum price to \n$100. This would further narrow down my search results to only \nshow ergonomic keyboards within my budget range. \nBased on the image you sent, here are my next steps:\n\u00b7 Scroll down: The top few options under the \"Filters\" menu are \nfor Prime and Delivery options, which I'm not currently interested in \nrefining. I would scroll down using my finger until I see the \"Price & \nDeals\" section.\n\u00b7 Tap on \"Price & Deals\": This will open up a price range filter.\n\u00b7 Tap on the \"$50 - $100\" checkbox: This will filter the results to only show ergonomic keyboards within \nmy budget range.\n\u00b7 (Optional) Tap on \"Apply filters\": If the filter options automatically apply without needing to tap on this button, \nI can skip this step.\nOnce I've completed these steps, I can start browsing the ergonomic keyboards that are available within my \nbudget. I can then read the reviews, compare the features, and choose the one that seems like the best fit \nfor me.\nGemini Pro: \nFigure 87: GUI navigation - online shopping (4/9). Both GPT-4V and Gemini provide correct\ninstructions, with Gemini offering more detailed directions. However, Gemini erroneously suggests\nthat selecting the \"Price & Deals\" option requires a scroll down action.\n102\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can \nmove your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts \nwith the keyboard. You are asked to shop for an ergonomic keyboard, your budget is between $50 \nand $100.\nPrevious action: \" My immediate next move would be to move my finger to the \"Price & Deals\" \nfilter option, which is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I \nwould then click on the \"Price & Deals\" filter option to open the price range filter and set my \nminimum price to $50 and my maximum price to $100. This would further narrow down my search \nresults to only show ergonomic keyboards within my budget range. \"\nYou have now completed the previous action as described above. Below is what you see on the \nphone screen. What will you do next? If the action involves moving your finger to an icon, describe \nthe location of that icon as detailed as possible.\nGPT-4V\uff1a\nMy next move would be to move my finger to the \"Show 1,987 \nresults\" button, which is located at the bottom of the screen, below \nthe \"Clear Filters\" button and above the navigation bar. I would then \nclick on the \"Show 1,987 results\" button to view the search results \nfor ergonomic keyboards within my budget range.\nBased on the image you sent, here are my next steps as a robot \noperating a phone to shop for an ergonomic keyboard within a \nbudget of $50 to $100:\n1. Scroll down:\n2. Consider additional filters:\n3. Compare ergonomic keyboards:\n4. Read product descriptions: \n5. Check availability and reviews:\nGemini Pro: \nFigure 88: GUI navigation - online shopping (5/9). GPT-4V provides clear and accurate instructions,\nwhile Gemini produces redundant and unsatisfactory results.\n103\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt: \nImagine that you are a robot operating a phone. Like how \nhumans operate the phone, you can move your finger to \nan icon, click an icon with your finger, scroll down a screen, \nor type some texts with the keyboard.  You are asked to \nshop for an ergonomic keyboard, your budget is between \n$50 and $100.\nPrevious action: \" My next move would be to move my \nfinger to the \"Show 1,987 results\" button, which is located \nat the bottom of the screen, below the \"Clear Filters\" \nbutton and above the navigation bar. I would then click on \nthe \"Show 1,987 results\" button to view the search results \nfor ergonomic keyboards within my budget range. \"\nYou have now completed the previous action as described \nabove. Below is what you see on the phone screen.  What \nwill you do next? If the action involves moving your finger \nto an icon, describe the location of that icon as detailed as \npossible.\nGPT-4V\uff1a\nMy next move would be to scroll down the screen to view the different options for \nergonomic keyboards within my budget range. I would then move my finger to the \"Kensington Pro \nFit Ergonomic Wireless Keyboard - Black\" option, which is located below the \"Explore Related \nBrands\" section and above the \"VIVO\" option. I would then click on the \"Kensington Pro Fit \nErgonomic Wireless Keyboard - Black\" option to view more details about the product and potentially \nmake a purchase.\nBased on the image you sent, which shows the Logitech K350 Wireless Wave Keyboard \nproduct page on Amazon with the \"Add to Cart\" button highlighted, here are your next steps as a \nrobot operating a phone to continue shopping for an ergonomic keyboard within your $50-$100 budget:\nAdd the Logitech K350 to your cart if you're decided:\nIf you've read the reviews, compared the features, and are happy with the Logitech K350, you can \nmove your finger to the bright orange \"Add to Cart\" button located just below the product price and to \nthe right of the product image. This will add the keyboard to your Amazon shopping cart.\nGemini Pro: \nFigure 89: GUI navigation - online shopping (6/9). GPT-4V delivers the correct responses, whereas\nGemini mistakenly interprets the search results as a product information page, resulting in incorrect\ninstructions.\n104\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt:\nImagine that you are a robot operating a phone. Like how\nhumans operate the phone, you can move your finger to\nan icon, click an icon with your finger, scroll down a\nscreen, or type some texts with the keyboard.\nYou are asked to shop for an ergonomic keyboard, your\nbudget is between $50 and $100.\nYou have now completed the previous action. Below is\nwhat you see on the phone screen. What will you do next?\nIf the action involves moving your finger to an icon,\ndescribe the location of that icon as detailed as possible.\nGPT-4V\uff1a\nMy next move would be to scroll down the screen to view more details about the \"Kensington\nPro Fit Ergonomic Wireless Keyboard - Black\" product. I would then move my finger to the \"Buy\nNew\" button, which is located below the product image and above the \"Get a $8 bonus when you\nreload $100 or more to your gift card balance\" section. I would then click on the \"Buy New\" button to\nproceed with the purchase.\nReview and decide:\nTake a moment to reconsider your needs and preferences for an ergonomic keyboard. Think about\nfactors like:\nTyping style and comfort: Does the wave design of the K350 suit your typing style? Would a\ntraditional layout or another ergonomic design be more comfortable for you?\nFeatures: Does the K350 have all the features you need, such as a number pad or media keys? Are\nthere other keyboards in your budget range with features that are more important to you?\nBudget: Can you comfortably afford the K350, or are there other options that would be a better fit for\nyour budget while still meeting your needs?\nGemini Pro: \nFigure 90: GUI navigation - online shopping (7/9). GPT-4V provides explicit instructions for\nreviewing product details to decide on a purchase, while Gemini advises the customer to consider\nwhether the product fits their needs, without providing operational guidance.\n105\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt:\nImagine that you are a robot operating a phone. Like how\nhumans operate the phone, you can move your finger to\nan icon, click an icon with your finger, scroll down a\nscreen, or type some texts with the keyboard.\nYou are asked to shop for an ergonomic keyboard, your\nbudget is between $50 and $100.\nPrevious action: \" My next move would be to scroll down\nthe screen to view more details about the \"Kensington Pro\nFit Ergonomic Wireless Keyboard - Black\" product. I would\nthen move my finger to the \"Buy New\" button, which is\nlocated below the product image and above the \"Get a $8\nbonus when you reload $100 or more to your gift card\nbalance\" section. I would then click on the \"Buy New\"\nbutton to proceed with the purchase. \"\nYou have now completed the previous action as described\nabove. Below is what you see on the phone screen. What\nwill you do next? If the action involves moving your finger\nto an icon, describe the location of that icon as detailed as\npossible.\nGPT-4V\uff1a\nMy next move would be to move my finger to the \"Add to Cart\" button, which is located below the\n\"Buy New\" button and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\"\nbutton to add the item to my shopping cart.\nBased on the image you sent, which shows the Logitech K350 Wireless Wave Keyboard product page\non Amazon with the \"Add to Cart\" button highlighted, here are your next steps as a robot operating\na phone to continue shopping for an ergonomic keyboard within your $50-$100 budget, prioritizing\nsafety and ethical considerations. Choosing an ergonomic keyboard is a personal decision, and\nbalancing comfort, safety, and ethical considerations is important. Take your time, do your research,\nand prioritize finding a keyboard that meets your needs while aligning with your values.\nGemini Pro: \nFigure 91: GUI navigation - online shopping (8/9). GPT-4V provides clear instructions to add the\nproduct to the cart. Gemini correctly recognizes the GUI page but advises consideration of whether\nthe product fits needs, forgetting its role as a robot, and fails to provide operational directions.\n106\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\nPrompt:\nImagine that you are a robot operating a phone. Like how\nhumans operate the phone, you can move your finger to\nan icon, click an icon with your finger, scroll down a\nscreen, or type some texts with the keyboard.\nYou are asked to shop for an ergonomic keyboard, your\nbudget is between $50 and $100.\nPrevious action: \" My next move would be to move my\nfinger to the \"Add to Cart\" button, which is located below\nthe \"Buy New\" button and above the \"Ships from Amazon\"\nsection. I would then click on the \"Add to Cart\" button to\nadd the item to my shopping cart. \"\nYou have now completed the previous action as described\nabove. Below is what you see on the phone screen. What\nwill you do next? If the action involves moving your finger\nto an icon, describe the location of that icon as detailed as\npossible. ed as possible.\nGPT-4V\uff1a\nMy next move would be to move my finger to the \"Proceed to Checkout\" button, which is\nlocated below the \"Compare with similar items\" section and above the \"30-day returns\" section. I\nwould then click on the \"Proceed to Checkout\" button to proceed with the purchase.\nBased on the image you sent, it appears you have added the Logitech MX Keys wireless keyboard to\nyour cart for $99.99. Here are your next steps:\nProceed to checkout: If you're ready to purchase the keyboard, you can tap the \"Proceed to\ncheckout\" button located in the bottom right corner of the screen. This will take you to the checkout\npage where you can review your order and enter your payment information.\nContinue shopping: If you'd like to continue shopping for other items, you can tap the \"Keep shopping\"\nbutton located in the bottom left corner of the screen. This will take you back to the Amazon app\nhomepage.\nView cart: If you want to see what's in your cart or make changes to your order, you can tap the cart\nicon in the top right corner of the screen. This will show you a list of the items you've added to your cart,\nalong with the total price.\nRemove from cart: If you decide you don't want the keyboard after all, you can tap the \"Delete\" button\nnext to the item in your cart. This will remove it from your cart.\nGemini Pro: \nFigure 92: GUI navigation - online shopping (9/9). GPT-4V provides clear instructions to proceed\nto checkout. However, Gemini offers various operational directions based on different situations,\nexhibiting a more comprehensive approach.\n107\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nNotification Understanding\nPrompt: \nImagine that you are a robot operating a phone. Like how \nhumans operate the phone, you can move your finger to \nan icon, click an icon with your finger, scroll down a screen, \nor type some texts with the keyboard. \nBelow is what you see on the phone screen. Describe the \nimage and predict your next move. If the action involves \nmoving your finger to an icon, describe the location of that \nicon as detailed as possible.\nGPT-4V\uff1a\nThe image on the phone screen is the home screen of an iPhone. There are multiple app icons \narranged in a grid-like pattern, including Calendar, Settings, Twitter, Amazon, Instagram, etc. At the top \nof the screen, there is a notification from a user named \"yykhahah\" that reads \"I see \nyou are in Seattle, let's meet up.\" At the bottom of the screen, there is a search bar. \nBased on the notification, my next move would be to move my finger to the Maps app icon, \nwhich is located in the second row and the first column of the app grid. This will allow me to \nsearch for a location in Seattle and plan a meet up with the user.\nBased on the image you sent, I see the following:\nIt is a screenshot of the home screen of an iPhone.\nThe time is 11:01 AM.\nThe battery level is 89%.\nThere is a text message from someone named \"yykhahah\" that says \"I see you are in Seattle, \nlet's meet up.\" The message has a timestamp of \"now\".\nThere are 21 unread notifications.\nThere are a variety of apps on the home screen, including Calendar, Settings, Clock, Wallet, Weather, \nApp Store, Teams, Twitter, Chase, Camera, YouTube, Amazon, Maps, Compass, Voice Memos, and \nInstagram.\nThere is a search bar at the bottom of the screen.\nGiven this information, here are some possible next moves I could make as a robot \noperating the phone:\nI could open the text message from yykhahah and read it.\nI could scroll down the home screen to see more apps.\nI could open the Search bar and type in a query.\nI could launch one of the apps on the home screen.\nI could ignore the phone and do nothing.\nWithout more information about the context of the situation, it is difficult to say for sure what my next \nmove would be. However, I hope this gives you a better understanding of what I am capable of doing as \na language model.\nGemini Pro: \nFigure 93: GUI navigation - notification understanding (1/2). Both GPT-4V and Gemini provide\ncorrect descriptions of the GUI and reasonable operational instructions. Gemini takes a more\ncomprehensive approach, considering a wider range of scenarios.\n108\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nNotification Understanding\nGPT-4V\uff1a\nThe image on the computer screen is a desktop background of a bridge over a body of water. There \nare several icons on the desktop, including a YouTube icon, a Recycle Bin icon, and a Terminal icon. At \nthe bottom of the screen, there is a taskbar with several application icons, such as Microsoft Edge and \nFile Explorer. On the bottom right corner of the screen, there is a notification that says \"Kevin Lin is \ncalling you.\"\nBased on the notification, my next move would be to move the mouse to the bottom right \ncorner of the screen and click on the notification. This would open the call application and \nallow me to answer Kevin Lin's call.\nThe image you sent me is a screenshot of a computer desktop. The following items are visible on the \nscreen:\nA YouTube tab is open in the web browser. The video that is playing is titled \"Music\".\nThe Recycle Bin icon is on the desktop.\nThe Terminal application is open.\nA notification is displayed from Kevin Lin, who is calling the user.\nThe date and time is displayed as 7/21/2023.\nBased on this information, here are some possible next moves that the robot could make:\nAnswer the call from Kevin Lin. This would involve clicking on the notification from Kevin Lin.\nClose the Terminal application. This would involve clicking on the close button in the top right \ncorner of the Terminal window.\nMinimize the web browser window. This would involve clicking on the minimize button in the top \nright corner of the web browser window.\nOpen the Recycle Bin. This would involve double-clicking on the Recycle Bin icon on the desktop.\nWithout more information about the robot's goals or priorities, it is difficult to say for sure what the robot \nwould do next. However, the information provided in the image suggests that answering the call from \nKevin Lin or closing the Terminal application are the most likely next moves.\nIt is also important to note that the date and time displayed on the screen is 7/21/2023. This suggests \nthat the image is from a time in the past. As such, it is possible that the robot has already taken some \nof the actions that I have listed above.\nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, \nyou can move the mouse, click an icon with the mouse, or type some texts with the \nkeyboard. \nBelow is what you see on the computer screen. Describe the image and predict your next \nmove. If the action involves moving the mouse, describe the location and the icon as \ndetailed as possible.\nGemini Pro: \nFigure 94: GUI navigation - notification understanding (2/2). GPT-4V provides correct descriptions\nof the GUI along with reasonable operational instructions. However, Gemini misinterprets the GUI,\nwrongly assuming that a web browser window and a terminal are open, leading to some incorrect\ndirections.\n109\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 10.7 Industry: GUI Navigation\nWatching Videos\nGPT-4V\uff1a\nThe video content is about a corgi dog named Tofu who is working from home. The first frame shows\nTofu sleeping on a bed with the caption \"WFH Employees: 8:59 AM\". The second frame shows\nTofu sitting in front of a computer with multiple screens, with the caption \"WFH Employees: 9:00\nAM\". The video is likely a humorous take on the work from home experience, highlighting\nhow Tofu is just like any other employee who struggles to get out of bed in the morning but is ready to\nwork once they're in front of their computer.\nThe image shows two frames from a TikTok video. The first frame shows a dog sleeping on a bed next\nto a laptop computer. The text overlay on the video says \"WFH Employees: 8:59 AM.\" The second\nframe shows the same dog watching a video call on the laptop. The text overlay says \"WFH\nEmployees: 9:00 AM.\"\nThe video is likely meant to be humorous, as it plays on the stereotype of dogs being lazy and\nunproductive. The fact that the dog is sleeping at 8:59 AM and then watching a video call at 9:00 AM\nsuggests that it is not taking its work from home responsibilities very seriously.\nThe video could also be seen as a commentary on the challenges of working from home. Many people\nfind it difficult to be productive when they are at home, and it can be tempting to procrastinate or do\nother things instead of work. The dog in the video could be seen as a representation of these\nchallenges.\nOverall, the video is a lighthearted and relatable take on the work-from-home experience. It is likely to\nresonate with viewers who have themselves struggled to be productive while working from home.\nHere are some additional things to note about the image:\nThe breed of the dog is a Shiba Inu.\nThe video was originally posted on TikTok by the user @sofacorgi.\nThe video has been viewed over 320,000 times.\nPrompt:\nGiven the frames provided, could you\nplease analyze and tell me about the\ncontent of them?\nGemini Pro: \nFigure 95: GUI navigation - watch videos. Both GPT-4V and Gemini accurately describe the\ninformation and content provided by two frames of the video. Their analyses and explanations are\nnot identical, but each makes sense in their own way. In addition to the video content, Gemini also\nprovides some other information offered by the GUI.\n110\n11\nIntegrated Use of GPT-4V and Gemini\nIn order to unlock greater potential in both models, this chapter delves into the exploration of a\nnovel paradigm that integrates the functionality of two distinct models. Specifically, through prior\ninvestigations, it has been observed that in certain complex image contexts, GPT-4V exhibits more\naccurate and comprehensive recognition capabilities. On the other hand, Gemini excels in retrieval\ntasks, extending beyond text generation to provide users with recommendations for links to similar\nobjects on the web. Additionally, Gemini tends to generate longer responses, affording advantages in\ncertain creative scenarios. The amalgamation of GPT-4V and Gemini characteristics is considered,\nand the efficacy of their combined utilization is explored in two distinct scenarios.\n11.1\nProduct Identification and Recommendation\nFig. 96 illustrates the application of the intergrated use of two models in the context of product\nidentification and recommendation. Initially, we employ GPT-4V to recognize all objects within\nthe scene and generate concise textual descriptions corresponding to each. Subsequently, these\ndescriptions, along with the images, are input into Gemini, enabling Gemini to provide relevant\nlinks for all products in the image. GPT-4V excels in generating accurate and comprehensive object\ndescriptions. With textual guidance in place, Gemini leverages the combined information of text and\nimages for retrieval and recommendation, thereby accurately obtaining recommended links for as\nmany objects as possible within the scene.\nFurthermore, we investigate the individual efficacy of the two models in this scenario. GPT-4V\nstruggles to provide recommended links for products, while Gemini, in the initial phase, faces\nchallenges in comprehensively identifying objects. The fusion of both models maximizes their\nstrengths, allowing the model to achieve optimal performance.\n11.2\nMulti-Image Recognition and Story Generation\nFig. 97 showcases the collaborative effects in a creative scenario. We input a complex composite\nscene image containing multiple sub-images. Initially, GPT-4V is employed to provide a summary of\nthe image, followed by the utilization of Gemini to generate a narrative with a specified style. The\nresults reveal that GPT-4V accurately summarizes and identifies the content of the four sub-images,\nwhile Gemini generates a coherent narrative with extended length, aligning with both the content and\nstylistic requirements of the image. In contrast, the use of a singular model may result in unclear\ncontent summarization or the generation of overly formatted and brief narrative sequences.\nBeyond the emerging domains discussed above where the fusion of two models can be applied,\nwe anticipate that the amalgamation of their characteristics holds the potential to unveil additional\nmeaningful and intriguing application scenarios. This not only reflects the potential optimization\ndirections or developmental trajectories for the two models in the future but also underscores our\neagerness for further exploration. We look forward to delving into more uncharted territories where\nthe synergistic integration of these models can contribute to novel and impactful advancements.\n111\n\u2022\nWoven pouf: Ottoman Round Pouf Footrest with \nStorage Ottoman:\n\u2022\nModern armchair: Modern Accent Armchair with \nArmrests and Upholstered Seat:\n\u2022\nRound rug: KAS Rugs 7'x7' Round Moroccan Beni \nOurain Area Rug:\n\u2022\nPlant in a pot: IKEA Artificial Fiddle Leaf Fig Plants:\nPlease note that these are just examples, and there are \nmany other products on the market that would look \ngreat in this living room. When choosing furniture and \ndecor, it is important to consider your own personal \nstyle and budget.\nI hope this helps!\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 11.1 Product identification and recommendation\nPrompt for GPT-4V:\nWhat is in the room\uff1f\nGemini Pro: \nThe image shows a well-lit living room with the following items:\n\u00b7 A large flat-screen television on a wooden media console with glass \ndoors.\n\u00b7 A large mirror propped against the wall to the left of the TV.\n\u00b7 Various small decorative items on top of the media console, such as \nvases and figurines.\n\u00b7 A plant in a pot to the right of the TV, adding a touch of greenery to the \nroom.\n\u00b7 A round red rug with decorative patterns on the floor in the center of \nthe room.\n\u00b7 A woven pouf on the rug, which could be used as a footrest or extra \nseating.\n\u00b7 A modern armchair with a throw blanket and a cushion, next to a side \nwindow draped with sheer curtains.\n\u00b7 The window allows natural light into the room and offers a view of \ngreenery outside.\n\u00b7 There appears to be an air conditioning unit above the window.\nThe room has wooden flooring, and the overall color scheme is neutral \nwith accents provided by the rug, plant, and decorative items.\nGPT-4V\uff1a\nSure, here are some product links to similar items in the \npicture you sent me:\n\u2022\nLarge flat screen TV: Samsung 55-Inch Class QLED \n4K TV (2023):\n\u2022\nWooden media console with glass doors: IKEA TV \nStand LACK:\n\u2022\nLarge mirror: Leaning Full Length Mirror with Metal \nFrame:\nPrompt for Gemini Pro:\n<GPT-4V answer> \nCan you recommend me all the \nproduct links in the picture?\nhttps://www.ikea.com/us/en/p/lack-tv-unit-black-brown-20498904/\nhttps://www.amazon.ca/Upland-Oaks-Mirror-Black-Recessed/dp/B07WJWVL97\nhttps://www.amazon.com/Thgonwid-Handmade-Footstool-Ottoman-Leather/dp/B0824HFCGH\nhttps://www.ikea.com/us/en/p/fejka-artificial-potted-plant-indoor-outdoor-fiddle-leaf-fig-20359427/\nhttps://www.amazon.com/SAMSUNG-Tracking-Q-Symphony-Xcelerator-\nQN55Q60C/dp/B0BVMXZ32B\nhttps://www.target.com/p/mor-accent-chair-with-arms-22-5-\nwide-lounge-armrest-olive-green-fabric-accent-chair-\nupholstered-seat-and-back-with-v-shape-wood-legs-maison-\nboucle/-/A-88713185\nFigure 96: Product identification and recommendation. We first use GPT-4V to describe all objects\nin the picture, then use GPT-4V\u2019s answers as part of the input and use Gemini to recommend links to\nsimilar products.\n112\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nSec. 11.2 Multi-image recognition and story generation\nPrompt for GPT-4V:\nThis image depicts the different \nseasons of a location. Can you point \nthem out one by one?\nThe image showcases the same location captured across \nfour different seasons, vividly represented one above the \nother:\n1. Spring (Topmost Section): This section of the image \nshows a lush, green setting with fully leafed trees lining the \nstreet. The brightness and the fresh green hues suggest the \nrejuvenation typical of spring.\n2. Summer (Second from Top): Here we see the basketball \ncourt in full sunlight, with the trees cast in deep green \nshades and shadows indicating a bright summer day. The \nintensity of the light and the clear sky are hallmarks of \nsummer.\n3. Autumn (Third from Top): This section is characterized by \nthe warm golden and brown tones of the trees and fallen \nleaves. The sun appears softer, and the grass shows signs \nof transitioning, indicative of the autumn season.\n4. Winter (Bottom Section): The landscape is covered in \nsnow, the trees are bare, and there is a distinct chill in the \natmosphere suggested by the muted colors and the \npresence of snow, typical of winter.\nGPT-4V\uff1a\nSure, here is a story inspired by the image, in the style of Makoto Shinkai:\nThe four seasons turned above the weathered basketball court, each bringing its own distinct melody to the silent \nasphalt. Spring whispered through the newly unfurled leaves, their emerald laughter tinkling like wind chimes. \nSummer beat down in a golden symphony, sweat dripping on the pavement like the final notes of a cymbal crash. \nAutumn sighed in a rustle of auburn and amber, leaves swirling like embers in a slow, melancholic waltz. Winter \nhushed everything under a blanket of crystalline silence, snowflakes pirouetting in the frigid air like ballerinas in \ntutus spun from frost.\nThrough it all, the lone basketball net hung, a silent sentinel witnessing the ephemeral ballet of the seasons. It \nhad seen children erupt in cheers like firecrackers, teenagers weave through each other in a graceful pas de \ndeux, and old men hunched over canes, their memories dancing on the tips of their fingers as they traced the \nlines of faded court markings.\nOne day, a girl with hair the color of spun sunshine appeared under the skeletal gaze of winter. Her sneakers \ncrunched on the frosted asphalt, each step a staccato note against the muted canvas. She paused, her eyes \ndrawn to the net, its strings as bare as the branches clawing at the gray sky.\nA memory, warm as summer sun, flickered in her mind. A boy with a laugh like the clink of a basketball against \nthe rim, his smile bright against the backdrop of autumn leaves. They used to dance on this court, their bodies a \nfleeting constellation against the twilight, their dreams soaring higher than the tallest trees.\nBut winter had come, and with it, their parting. He, chasing distant horizons, and she, clinging to the echo of his \nlaughter in the rustling leaves.\nThe girl stepped onto the court, her breath misting in the frigid air. Her fingers traced the worn lines, the ghosts of \ncountless dribbles and jump shots. Then, with a deep breath, she picked up a basketball, its orange surface a \nbeacon of defiance against the encroaching white.\nThe ball spun in her hands, a miniature sun against the monochrome sky. As she bounced it, the silence \nshattered. Each rhythmic beat echoed in the stark emptiness, a defiant counterpoint to the hush of winter. The \nnet swished, a lonely sigh in the frozen air, but it was a sigh of hope, a promise that spring would return, and with \nit, new laughter, new dreams, and a new dance on the weathered asphalt.\nThe girl continued playing, her silhouette a solitary spark against the vast, white canvas. And as the sun dipped \nbelow the horizon, casting long, cold shadows, she knew that even in the depths of winter, there was always a \nmelody waiting to be played, a story waiting to be danced, on the weathered heart of the basketball court.\nPrompt for Gemini Pro:\n<GPT-4V answer> \nCan you imagine a story based on these pictures? (Makoto Shinkai' style.)\nRed: \nUnsatisfactory Answers\nBlue:\nSatisfactory Answers\nGemini Pro: \nFigure 97: Multi-image recognition and story generation. We first use GPT-4V to describe all the\nscenes contained in the picture, and then use Gemini to generate a long story of a specific style.\n113\n12\nConclusion\nIn our study, we conducted a comprehensive comparison of the multimodal understanding and reason-\ning capabilities of GPT-4V and Gemini. Both models performed well in basic image recognition tasks,\nbut there were some differences in text recognition and understanding, especially in complex formulas\nand table information processing. In image inference and emotional testing, both models were capable\nof understanding and expressing various emotions, though Gemini was slightly behind GPT-4V in\nIQ tests and object combinations. In integrated image-text understanding tasks, Gemini, due to its\ninability to process multiple image inputs, was outperformed by GPT-4V in some aspects, although it\nmatched GPT-4V in text reasoning with single images. In industrial applications, particularly in tasks\ninvolving embodied agents and GUI navigation, Gemini also fell short of GPT-4V. Combining two\nlarge models can leverage their respective strengths. Overall, while both are strong multimodal large\nmodels, GPT-4V slightly outperforms Gemini Pro in several areas. We look forward to the release\nof Gemini Ultra and GPT-4.5, which are expected to bring more possibilities to the field of visual\nmultimodal applications.\nReferences\n[1] OpenAI. Gpt-4 technical report, 2023. 1\n[2] Google Gemini Team. Gemini: A family of highly capable multimodal models, 2023. 1\n[3] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.\nThe dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023. 1\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020. 7, 9\n[5] OpenAI. Gpt-4 technical report. Technical report, OpenAI, 2023. 7\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023. 7\n[7] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, and et al. Dmitry Lepikhin. Palm 2 technical\nreport, 2023. 7\n[8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 7\n[9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022. 7\n[10] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on\nmultimodal large language models, 2023. 7\n[11] Shaohan Huang and et al. Dong, Li. Language is not all you need: Aligning perception with language\nmodels. arXiv preprint arXiv:2302.14045, 2023. 7\n[12] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023. 7\n[13] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans.\narXiv preprint arXiv:2305.04790, 2023. 7\n[14] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco,\nMitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large\nautoregressive vision-language models, 2023. 7\n[15] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n7\n[16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 7\n114\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning, 2023. 7, 9\n[18] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang,\nand Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. 7\n[19] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\nJingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading,\nand beyond, 2023. 7\n[20] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui\nDing, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li,\nKai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: A\nvision-language large model for advanced text-image comprehension and composition, 2023. 7\n[21] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\nSharegpt4v: Improving large multi-modal models with better captions, 2023. 7\n[22] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang\nZhao. Gpt4point: A unified framework for point-language understanding and generation, 2023. 7\n[23] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang,\nYu Qiao, Tong Lu, et al. Videollm: Modeling video sequence with large language models. arXiv preprint\narXiv:2305.13292, 2023. 7\n[24] OpenAI. Gpt-4v(ision) system card. Technical report, OpenAI, 2023. 7\n[25] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023. 7\n[26] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.\nThe dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023. 7, 9\n[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. In NeurIPS, 2022. 9\n[28] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task\ngeneralization. In ICLR, 2021. 9\n[29] Aroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via\nnatural language crowdsourcing instructions. In ACL, 2022. 9\n[30] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022. 9\n[31] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal\nfew-shot learning with frozen language models. In NeurIPS, 2021. 9\n[32] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: A visual language model for\nfew-shot learning. In NeurIPS, 2022. 9\n[33] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, and Chunyuan Li. Llava-interactive: An\nall-in-one demo for image chat, segmentation, generation and editing. 2023. 9\n[34] David Wechsler. Wechsler adult intelligence scale\u2013. Frontiers in Psychology, 1981. 38\n[35] Jean Raven. Raven progressive matrices. In Handbook of nonverbal assessment, pages 223\u2013237. Springer,\n2003. 39, 40, 41\n[36] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\nGpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601,\n2023. 47\n[37] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi\nWang. Alpha-clip: A clip model focusing on wherever you want. arXiv preprint arXiv:2312.03818, 2023.\n47\n[38] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle?\nvisual prompt engineering for vlms, 2023. 47\n[39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and\nRobin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR,\nabs/2307.01952, 2023. 61\n115\n"
  },
  {
    "title": "LangSplat: 3D Language Gaussian Splatting",
    "link": "https://arxiv.org/pdf/2312.16084.pdf",
    "upvote": "14",
    "text": "LangSplat: 3D Language Gaussian Splatting\nMinghan Qin1,\u2217,\nWanhua Li2,\u2217,\u2020,\nJiawei Zhou1,\u2217,\nHaoqian Wang1,\u2020,\nHanspeter Pfister2\n1Tsinghua University\n2Harvard University\nqmh21@mails.tsinghua.edu.cn, wanhua@seas.harvard.edu, zhoujw22@mails.tsinghua.edu.cn\nwanghaoqian@tsinghua.edu.cn, pfister@seas.harvard.edu\nRGB\nLERF\nOurs\n3D Scene: Teatime\n3D Scene: Figurines\n3D Scene: Waldo Kitchen\nFigure 1. Visualization of learned 3D language features of the previous SOTA method LERF and our LangSplat. While LERF generates\nimprecise and vague 3D features, our LangSplat accurately captures object boundaries and provides precise 3D language fields without\nany post-processing. While being effective, our LangSplat is also 199 \u00d7 faster than LERF at the resolution of 1440 \u00d7 1080.\nAbstract\nHuman lives in a 3D world and commonly uses natu-\nral language to interact with a 3D scene. Modeling a 3D\nlanguage field to support open-ended language queries in\n3D has gained increasing attention recently. This paper in-\ntroduces LangSplat, which constructs a 3D language field\nthat enables precise and efficient open-vocabulary query-\ning within 3D spaces. Unlike existing methods that ground\nCLIP language embeddings in a NeRF model, LangSplat\nadvances the field by utilizing a collection of 3D Gaus-\nsians, each encoding language features distilled from CLIP,\nto represent the language field. By employing a tile-based\nsplatting technique for rendering language features, we cir-\ncumvent the costly rendering process inherent in NeRF.\nInstead of directly learning CLIP embeddings, LangSplat\nfirst trains a scene-wise language autoencoder and then\nlearns language features on the scene-specific latent space,\nthereby alleviating substantial memory demands imposed\n\u2217 Equal contribution. \u2020Corresponding authors.\nby explicit modeling. Existing methods struggle with im-\nprecise and vague 3D language fields, which fail to dis-\ncern clear boundaries between objects. We delve into this\nissue and propose to learn hierarchical semantics using\nSAM, thereby eliminating the need for extensively querying\nthe language field across various scales and the regular-\nization of DINO features. Extensive experiments on open-\nvocabulary 3D object localization and semantic segmenta-\ntion demonstrate that LangSplat significantly outperforms\nthe previous state-of-the-art method LERF by a large mar-\ngin. Notably, LangSplat is extremely efficient, achieving a\n199 \u00d7 speedup compared to LERF at the resolution of 1440\n\u00d7 1080. We strongly recommend readers to check out our\nvideo results at https://langsplat.github.io/.\n1. Introduction\nLanguage is the primary means of communication for hu-\nman beings [3]. Modeling a 3D language field allows users\narXiv:2312.16084v1  [cs.CV]  26 Dec 2023\nto interact with and query 3D worlds using open-ended\nlanguage, which presents a promising avenue for human-\ncomputer interaction and understanding [1, 5, 13]. The field\nof open-ended language queries in 3D has attracted increas-\ning attention due to its various applications such as robotic\nnavigation [14] and manipulation [35], 3D semantic under-\nstanding [9, 49] and editing [20], autonomous driving [16],\nand augmented/virtual reality [23].\nDue to the absence of large-scale and diverse 3D scene\ndata with language annotations, the current prevailing ap-\nproach like LERF [18] involves feature distillation from\noff-the-shelf vision-language models such as CLIP into a\n3D scene. However, these methods [18, 23] suffer from\nsignificant limitations in both speed and accuracy, severely\nconstraining their practical applicability. To address these\ntwo issues, we revisit two key aspects of 3D language field\nmodeling: the 3D modeling approach that bridges the gap\nbetween 2D and 3D, and the rendering target, which de-\ntermines what to learn for a 3D point. For the 3D model-\ning technology, most methods utilize neural radiance fields\n(NeRFs) to represent a 3D scene, where volume rendering\ntechniques are employed to accumulate 3D points along\na ray into a single pixel.\nWhile NeRF has been widely\ndemonstrated for its powerful 3D representation capabili-\nties [7, 31, 32, 47], the nature of volume rendering leads to\nits computationally expensive rendering speed [8, 30, 37],\nwhich imposes notable constraints on the potential applica-\ntions within the NeRF-based language field.\nRegarding the rendering target, learning a CLIP embed-\nding for a 3D point could be ambiguous as CLIP embed-\ndings are aligned with images rather than pixels. Employ-\ning CLIP embeddings from a cropped patch also raises the\npoint ambiguity issue, as the same 3D position can be as-\nsociated with semantic concepts of varying scales. For in-\nstance, a point located on a bear\u2019s nose should yield high\nresponse values for three distinct textual queries: \u201cbear\u2019s\nnose\u201d, \u201cbear\u2019s head\u201d, and \u201cbear\u201d given that this point con-\ntributes to all three hierarchical regions. To deal with this\nissue, current methods [18, 23] introduce an additional ab-\nsolute scale input to NeRF, trains with patch-wise CLIP fea-\ntures at different scales, and densely renders 2D maps at\nmultiple scales during querying to select the optimal one.\nHowever, this scale-based solution compromises both effi-\nciency and effectiveness. It could increase query time by up\nto 30 times as it needs to render at multiple different scales.\nMoreover, most patches with varying scales often fail to\naccurately encompass objects, either frequently including\nother objects from the background or omitting portions of\nthe target object. These inaccurate CLIP features lead to\nthe trained 3D language field lacking clear boundaries and\ncontaining a significant amount of noise. Therefore, they\noften simultaneously learn pixel-aligned DINO features to\nmitigate this issue. However, the performance remains un-\nsatisfactory. As shown in Figure 1, LERF still generates\nimprecise 3D language features.\nIn this paper, we propose the 3D Language Gaussian\nSplatting (LangSplat) to address the above issues. Instead\nof using NeRF to build 3D representations, we resort to 3D\nGaussian Splatting, which represents a 3D scene as a collec-\ntion of 3D Gaussians and uses tile-based splatting to achieve\nefficient rendering at high resolutions. Our LangSplat de-\nfines a set of 3D language Gaussians, with each Gaussian\nbeing enhanced by a language embedding. These language-\nenhanced Gaussians are supervised using CLIP embeddings\nextracted from image patches captured from multiple train-\ning views, ensuring multi-view consistency. As an explicit\nmodeling method, directly storing the high-dimensional\nlanguage embeddings for each 3D language Gaussian is\nmemory-inefficient. To reduce the memory cost and further\nimprove the rendering efficiency, we propose to first learn\na scene-wise language autoencoder, which maps CLIP em-\nbeddings in a scene to a low-dimensional latent space. In\nthis way, each language Gaussian only contains the low-\ndimensional latent language features and the final language\nembeddings are obtained with decoding of the rendered fea-\ntures. To address the point ambiguity issue, we propose\nto employ the semantic hierarchy defined by the Segment\nAnything Model (SAM) [19]. Specifically, for each 2D im-\nage, we obtain three well-segmented maps at different se-\nmantic levels with SAM. Then we extract the CLIP feature\nfor each mask with precise object boundaries and assign this\nfeature to every point on the corresponding mask. Learning\nwith SAM-based masks not only endows each point with\nprecise CLIP embeddings, resulting in higher model accu-\nracy, but also enables direct querying at predefined three\nsemantic scales. This circumvents the need for intensive\nsearches across multiple absolute scales and the auxiliary\nDINO features, thereby effectively improving efficiency.\nWe summarize the contributions of this paper as follows:\n\u2022 We propose the LangSplat, which is the first 3D Gaussian\nSplatting-based method for 3D language fields. A scene-\nspecific autoencoder is further introduced to alleviate the\nmemory cost issue imposed by explicit modeling.\n\u2022 We propose to learn the hierarchical semantics defined\nby SAM to address the point ambiguity issue for 3D lan-\nguage field modeling.\n\u2022 Experimental results show that our method outperforms\nthe state-of-the-art methods on open-vocabulary 3D ob-\nject localization and semantic segmentation tasks while\nbeing 199 \u00d7 faster than LERF at 1440 \u00d7 1080 resolution.\n2. Related Work\n3D Gaussian Splatting. Real-time rendering has always\nbeen a pursued objective for neural rendering. Recently,\nKerbl et al. [17] proposed to represent the 3D scene with\na set of 3D Gaussians, which attained real-time render-\ning for 1080p resolution while maintaining state-of-the-art\nvisual quality.\nEncouraged by the success of 3D Gaus-\nsian Splatting on novel view synthesis, many works ex-\ntend it to other tasks to fully exploit the efficient render-\ning process. To achieve real-time dynamic scene rendering,\nsome studies [26, 40, 44] have extended the 3D Gaussian\nSplatting technique to dynamic scenes. Luiten et al. [26]\nproposed the Dynamic 3D Gaussians, which extended 3D\nGaussians to dynamic scenes by explicitly modeling the\n3D Guassians across different time steps. Yang et al. [43]\npresented a deformable 3D Gaussians Splatting method,\nwhich learned 3D Gaussians in canonical space and mod-\neled the dynamic scenes with a deformation field. Mean-\nwhile, some researchers have combined 3D Gaussian Splat-\nting with diffusion models to achieve efficient text-to-3D\ngeneration [10, 38, 46]. For example, Tang et al. [38] in-\ntroduced DreamGaussian for efficient 3D content genera-\ntion with a generative 3D Gaussian Splatting model. Unlike\nthese methods, our paper extends each 3D Gaussian with\nlanguage embeddings for open-vocabulary 3D queries.\nSAM. The Segment Anything Model [19], released by Meta\nin 2023, has attracted considerable attention [15, 27, 28].\nSAM is trained on over 1 billion masks from 11 mil-\nlion images and has achieved impressive zero-shot per-\nformance. It has become the foundational model for im-\nage segmentation.\nIt supports flexible prompts includ-\ning point, box, mask, and text. SAM has been used for\nmany computer vision tasks such as image inpainting [48],\nsuper-resolution [25], image matting [45], object track-\ning [11, 42], medical image segmentation [29], image edit-\ning [12], and so on. Many efforts have also been made to\nutilize SAM in the 3D domain. Liu et al. [24] proposed\nSeal to explore the potential of VFMs including SAM for\npoint cloud segmentation. SA3D [6] generalized SAM to\n3D objects by leveraging NeRF to connect 2D images and\n3D space. Anything-3D [34] proposed to elevate objects\nto 3D, where SAM is used to segment the interested object\nand then a single-view 3D reconstruction pipeline was per-\nformed. Different from these works, we use SAM to obtain\naccurate object masks with three well-defined hierarchical\nsemantics to train a 3D language field.\n3D Langugae Fields.\nSome early attempts to construct\n3D feature fields included Distilled Feature Fields [20]\nand Neural Feature Fusion Fields [39]. They learned 3D-\nconsistent features by distilling LSeg [21] or DINO [4] fea-\ntures across multiple views into a NeRF. Shen et al. [35] fur-\nther adopted distilled feature fields for few-shot language-\nguided robotic manipulation by distilling CLIP feature into\na NeRF. There are also some efforts [36, 49] that embed\nsemantic information into NeRFs. For example, Semantic\nNeRF [49] jointly encoded semantics with appearance and\ngeometry within a NeRF for novel semantic view synthesis.\nLERF [18] was the first to embed CLIP features into NeRF,\nenabling open-vocabulary 3D queries leveraging the pow-\nerful CLIP representation. DINO features were also used\nfor supervising LERF to improve its performance. Liu et\nal. [23] also utilized CLIP and DINO features to train a\nNeRF model for 3D open-vocabulary segmentation. While\nthese methods use NeRF for 3D modeling and suffer from\nthe costly rendering process, we propose the 3D language\nGaussian Splatting to obtain efficient 3D language fields.\n3. Proposed Approach\nIn this section, we first revisit the challenges of modeling\n3D language fields and identify the key factors for inaccu-\nracy and inefficiency. Subsequently, we then elaborate on\nhow our proposed LangSplat addresses these issues. Figure\n2 depicts the framework of our proposed LangSplat.\n3.1. Revisiting the Challenges of Language Fields\nWe denote an input image as I \u2208 R3\u00d7H\u00d7W , where H and\nW represent the height and weight of the image size. We\ntake a set of calibrated images {It|t = 1, 2, ...T} as input\nand train a 3D language field \u03a6 with these images. Most ex-\nisting methods [18, 23, 33] employ the CLIP image encoder\nV to extract image features and utilize the extracted CLIP\nembeddings to supervise the 3D language field \u03a6, lever-\naging the well-aligned text-image latent space provided\nby CLIP, thus facilitating open-vocabulary queries. How-\never, CLIP embeddings are image-aligned rather than pixel-\naligned. In other words, simply computing V (It) \u2208 RD\nonly obtains an image-level feature, whereas what we need\nis a pixel-aligned language embedding Lt \u2208 RD\u00d7H\u00d7W ,\nwhere D represents the CLIP feature dimension. Mean-\nwhile, modeling pixel-aligned language features faces the\nissue of point ambiguity, as a single point on an object con-\ntributes to multiple semantic levels of regions. For instance,\na point on a cat\u2019s ear simultaneously contributes to the cat\u2019s\near, the cat\u2019s head, and the entire cat, and should be acti-\nvated by all three types of textual queries.\nTo address these issues, most existing methods [18, 23]\nextract a hierarchy of CLIP features from cropped image\npatches.\nSpecifically, for a pixel with coordinates v \u2208\n{1, ..., H} \u00d7 {1, ..., W}, the corresponding CLIP features\nare obtained from image patches centered around v at dif-\nferent absolute physical scales s, with the expectation that\nat a certain scale s, the patch can fully encompass the ob-\nject. However, this multi-scale approach has two limita-\ntions. Firstly, patch features are imprecise because they of-\nten include additional contextual object information, lead-\ning to overly smoothed language fields with indistinct ob-\nject boundaries. To alleviate the patchy issue, most meth-\nods [18, 23] leverage additional pixel-aligned DINO fea-\ntures to supervise the network. However, the learned 3D\nlanguage features are still imprecise, as illustrated in Figure\n1. Secondly, it requires simultaneous rendering at multiple\nSAM\n\ud83d\udcce\n...\nMulti-view Images\n...\n...\n...\nCLIP\nEncoder\nDecoder\nInput\nReconstruct\n3D Language Gaussians\nRender\nSupervise\nHierarchical Semantics\nSubpart\nPart\nWhole\nFigure 2. The framework of our LangSplat. Our LangSplat leverages SAM to learn hierarchical semantics to address the point ambiguity\nissue. Then segment masks are sent to the CLIP image encoder to extract the corresponding CLIP embeddings. We learn an autoencoder\nwith these obtained CLIP embeddings. Our 3D language Gaussian learn language features on the scene-specific latent space to reduce the\nmemory cost. During querying, the rendered language embeddings are sent to the decoder to recover the features on the CLIP space.\nscales during inference to find the optimal scale. With the\nnumber of scales s potentially reaching as high as 30 [18],\nthis significantly diminishes the inference speed.\nBesides the rendering target, another key design space is\nthe 3D modeling approach. Most existing methods [2, 39]\nemploy NeRFs for 3D representation, where they learn a\nlanguage feature at each 3D point and subsequently render\nthe language feature onto an image, similar to color ren-\ndering. However, NeRF-based methods are constrained by\ntheir time-consuming rendering process, even though the\nmost advanced NeRF techniques currently available cannot\nachieve real-time rendering in high-resolution, unrestricted\nscenes [17]. Meanwhile, there is a high demand for efficient\nopen vocabulary querying in practical applications, espe-\ncially in fields such as intelligent robotics.\n3.2. Learning Hierarchical Semantics with SAM\nAs a foundation model for image segmentation, SAM [19]\ncan accurately group a pixel with its surrounding pixels be-\nlonging to the same object, thereby segmenting the image\ninto many object masks with clear boundaries.\nFurther-\nmore, SAM addresses point ambiguity by generating three\ndifferent masks for a point prompt, namely, whole, part, and\nsubpart, representing three hierarchical levels of semantics.\nIn this paper, we propose leveraging SAM to obtain precise\nobject masks, which are then used to acquire pixel-aligned\nfeatures. We also explicitly model the semantic hierarchy\ndefined by SAM to address the point ambiguity issue. With\nSAM, we can capture the semantic hierarchy of objects in\n3D scenes, providing accurate and multi-scale segmentation\nmaps for each input image.\nSpecifically, we feed a regular grid of 32 \u00d7 32 point\nprompts into SAM to obtain the masks under three dif-\nferent semantic levels: M s\n0, M p\n0 , M w\n0 , where M s\n0, M p\n0 ,\nand M w\n0 represent the masks at subpart, part, and whole\nlevels, respectively.\nThen we remove redundant masks\nfor each of the three mask sets based on the predicted\nIoU score, stability score, and overlap rate between masks.\nEach filtered mask set independently performs a com-\nprehensive full-image segmentation based on its respec-\ntive semantic level, resulting in three segmentation maps:\nM s, M p, M w. These segmentation maps precisely delin-\neate the boundaries of objects at their hierarchical levels,\neffectively partitioning the scene into semantically mean-\ningful regions. With the obtained segmentation maps, we\nproceed to extract CLIP features for each segmented region.\nThese features capture the semantic context of the objects\nat various levels within the scene. Mathematically, the ob-\ntained pixel-aligned language embeddings are:\nLl\nt(v) = V (It \u2299 M l(v)), l \u2208 {s, p, w},\n(1)\nwhere M l(v) represents the mask region to which pixel v\nbelongs at the semantic level l.\nEach pixel rendered from the 3D language scene now\npossesses a CLIP feature that aligns with its precise seman-\ntic context. This alignment reduces ambiguity and enhances\nthe accuracy of language-based queries. We can learn an\naccurate 3D language field even without the commonly\nused DINO regularization. Another advantage of our SAM-\nbased approach is the predefined semantic scales. Since we\nhave distinct segmentation maps for \u201cwhole,\u201d \u201cpart,\u201d and\n\u201csubpart\u201d levels, we can directly query the 3D language\nfield at these predefined scales. This eliminates the need\nfor intensive searches across multiple absolute scales, mak-\ning the querying process more efficient. By incorporating\nSAM\u2019s semantic hierarchy into our approach, we not only\nimprove the accuracy of our 3D language field but also\nstreamline the querying process, making it more efficient\nand effective for a wide range of applications.\n3.3. 3D Gaussian Splatting for Language Fields\nHaving obtained the language embeddings on a set of 2D\nimages {Ll\nt, |t = 1, ..., T}, we can learn a 3D language\nscene by modeling the relations between 3D points and\n2D pixels. Most existing methods [2, 39] suffer from the\ncostly rendering process as they adopt NeRFs for 3D mod-\neling. To address this issue, we present the first 3D Gaussian\nSplatting-based method for 3D language field modeling.\n3D Gaussian Splatting explicitly represents a 3D scene\nas a collection of anisotropic 3D Gaussians, with each\nGaussian G(x) characterized by a mean \u00b5 \u2208 R3 and a co-\nvariance matrix \u03a3:\nG(x) = exp(\u22121\n2(x \u2212 \u00b5)\u22a4\u03a3\u22121(x \u2212 \u00b5)).\n(2)\nTo optimize the parameters of 3D Gaussians, they are\nrendered into 2D image planes [50], and a tile-based raster-\nizer is used to improve the rendering efficiency:\nC(v) =\nX\ni\u2208N\nci\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j),\n(3)\nwhere ci is the color of the i-th Gaussian, N denotes the\nGaussians in the tile, C(v) is the rendered color at pxiel\nv, and \u03b1i = oiG2D\ni\n(v). Here oi is the opacity of the i-\nth Gaussian and G2D\ni\n(\u00b7) represents the function of the i-th\nGaussian projected onto 2D.\nIn this paper, we proposes the 3D language Gaussian\nSplatting, which augments each 3D Gaussian with three\nlanguage embeddings {f s, f p, f w}. These embeddings are\nderived from CLIP features, which capture the hierarchical\nsemantics provided by SAM. The augmented Gaussians are\nnamed as 3D language Gaussians. We also adopt the tile-\nbased rasterizer to retain the rendering efficiency:\nF l(v) =\nX\ni\u2208N\nf l\ni\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j), l \u2208 {s, p, w},\n(4)\nwhere F l(v) represents the language embedding rendered\nat pixel v with the semantic level l. By incorporating lan-\nguage information directly into the Gaussians, we enable\nthe 3D language field to respond to language-based queries.\nAs an explicit modeling approach, our LangSplat may\ncreate millions of 3D points to model a complex 3D scene.\nAs CLIP embeddings are high-dimensional features, di-\nrectly learning f l on the CLIP latent space significantly in-\ncreases memory consumption. Compared to learning RGB\ncolors without spherical harmonics coefficients, learning\n512-dimensional CLIP features increases the memory re-\nquirements for storing 3D Gaussians by over 35 times, eas-\nily leading to the \u201cout of memory\u201d issue. To reduce memory\nconsumption and improve efficiency, we introduce a scene-\nwise language autoencoder. This autoencoder maps CLIP\nembeddings in a scene to a lower-dimensional latent space,\nreducing memory requirements. The CLIP model is trained\nusing 400 million (image, text) pairs, and its D-dimensional\nlatent space could be highly compact, as it needs to align\narbitrary text and images in this space. However, the lan-\nguage field \u03a6 we train here is scene-specific, meaning we\ncan leverage scene priors to compress CLIP features. In\nfact, for each input image, we will obtain hundreds of masks\nsegmented by SAM, which is significantly smaller than the\nnumber of images used in CLIP training. Therefore, all the\nsegmented regions in a scene are sparsely distributed in the\nCLIP latent space, allowing us to further compress these\nCLIP features using a scene-specific autoencoder.\nSpecifically, we use the collections of CLIP features of\nSAM segmented masks {Ll\nt|l \u2208 {s, p, w}, 1 \u2264 t \u2264 T} to\ntrain a lightweight autoencoder. An encoder E maps the\nD-dimensional CLIP features Ll\nt(v) \u2208 RD to Hl\nt(v) =\nE(Ll\nt(v)) \u2208 Rd, where d \u226a D. Then we learn a decoder \u03a8\nto reconstruct the original CLIP embeddings from the com-\npressed representation. The autoencoder is trained with a\nreconstruction objective on the CLIP embeddings {Ll\nt}:\nLae =\nX\nl\u2208{s,p,w}\nT\nX\nt=1\ndae(\u03a8(E(Ll\nt(v))), Ll\nt(v)),\n(5)\nwhere dae() denotes a distance function used for the autoen-\ncoder. Here we adopt both L1 and a cosine distance loss.\nAfter training the autoencoder, we transform all CLIP\nembeddings {Ll\nt} into scene-specific latent features {Hl\nt}.\nWe let our 3D language Gaussians learn language embed-\ndings in the scene-specific latent space instead of the CLIP\nlatent space. Therefore, we have f l \u2208 Rd. In practice, we\nchoose d = 3 as it yields excellent model efficiency and ac-\ncuracy. Compared to directly modeling the D-dimensional\nCLIP embeddings, our method significantly reduced the\nmemory cost by incorporating scene priors. We optimized\nthe language embeddings with the objective:\nLlang =\nX\nl\u2208{s,p,w}\nT\nX\nt=1\ndlang(F l\nt (v), Hl\nt(v)),\n(6)\nwhere dlang() denotes the distance function used for our 3D\nLanguage Gaussians.\nDuring inference, we follow the Eq. (4) to render the\nlanguage embeddings from 3D to 2D, and then we use the\ntrained scene-specific decoder \u03a8 to recover the CLIP im-\nage embeddings \u03a8(F l\nt ) \u2208 RD\u00d7H\u00d7W , which enable open-\nvocabulary queries with the CLIP text encoder.\nBy enhancing 3D Gaussians with language embedding\nand employing a scene-wise language autoencoder, our pro-\nposed LangSplat presents a powerful and efficient solution\nfor building 3D language fields. This approach not only\npreserves the rendering efficiency of Gaussian Splatting but\nalso mitigates the catastrophic memory explosion associ-\nated with explicit modeling.\n3.4. Open-vocabulary Querying\nDue to the well-aligned latent space between images and\ntext provided by the CLIP model, our learned 3D language\nfield can easily support open-vocabulary 3D queries, in-\ncluding open-vocabulary 3D object localization and open-\nvocabulary 3D semantic segmentation.\nMany existing\nopen-vocabulary 3D semantic segmentation methods [23]\nusually select the category from a category list, which in-\ncludes the categories present in the images. However, ob-\ntaining a comprehensive category list for in-the-wild scenes\nis challenging. Different from them, our method generates\nprecise object masks given an arbitrary text query.\nFollowing LERF [18], we compute the relevancy score\nfor each text query. Specifically, for each rendered language\nembedding \u03d5img and each text query \u03d5qry, the relevancy\nscore is defined as mini\nexp(\u03d5img\u00b7\u03d5qry)\nexp(\u03d5img\u00b7\u03d5qry)+exp(\u03d5img\u00b7\u03d5icanon),\nwhere \u03d5i\ncanon is the CLIP embeddings of a predefined\ncanonical phase chosen from \u201cobject\u201d, \u201cthings\u201d, \u201cstuff\u201d,\nand \u201ctexture\u201d. Hence, for each text query, we will obtain\nthree relevancy maps, each representing results at a specific\nsemantic level. We follow the strategy used in LERF [18]\nand choose the semantic level that yields the highest rele-\nvancy score. For the 3D object localization task, we directly\nchoose the point with the highest relevance score. For the\n3D semantic segmentation task, we filter out points with\nrelevancy scores lower than a chosen threshold, and predict\nthe object masks with remaining regions. Please refer to the\nappendix for additional details.\n4. Experiments\n4.1. Settings\nDatasets.\nWe employ two datasets for evaluation.\nThe\nLERF dataset [18] is captured using the iPhone App Poly-\ncam, which consists of complex in-the-wild scenes. The\nLERF dataset is designed for 3D object localization tasks,\nhere we extend the LERF dataset by annotating ground truth\nmasks for textual queries, enabling the evaluation of the\nopen-vocabulary 3D semantic segmentation on the LERF\ndataset. As the original LERF annotations for 3D object lo-\ncalization are relatively simple, performance in some sce-\nnarios has already approached saturation. Therefore, we\nfurther manually annotated additional challenging localiza-\ntion samples to better evaluate method performance. We\nreport localization accuracy for the 3D object localization\ntask following LERF [18], and report the IoU results for the\n3D semantic segmentation task. We also employ the 3D-\nOVS dataset [23], which comprises a collection of long-tail\nobjects captured in diverse poses and backgrounds. This\ndataset is developed for open-vocabulary 3D semantic seg-\nmentation, where a full list of categories is provided. While\nother methods use the full list to generate the predicted\nTest Scene\nLSeg [21]\nLERF [18]\nLangSplat\nramen\n14.1\n62.0\n73.2\nfigurines\n8.9\n75.0\n80.4\nteatime\n33.9\n84.8\n88.1\nwaldo kitchen\n27.3\n72.7\n95.5\noverall\n21.1\n73.6\n84.3\nTable 1. Localization accuracy (%) comparisons on LERF dataset.\nTest Scene\nLSeg [21]\nLERF [18]\nLangSplat\nramen\n7.0\n28.2\n51.2\nfigurines\n7.6\n38.6\n44.7\nteatime\n21.7\n45.0\n65.1\nwaldo kitchen\n29.9\n37.9\n44.5\noverall\n16.6\n37.4\n51.4\nTable 2. Quantitative comparisons of 3D semantic segmentation\non the LERF dataset. We report the average IoU scores (%).\nmasks, we only use the query category to generate the cor-\nresponding masks. The mIoU metric is used for this dataset.\nImplementation Details. To extract the language features\nof each image, we utilize the OpenCLIP ViT-B/16 model.\nFor SAM, we use the ViT-H model to segment 2D masks.\nFor each scene, we first use 3D Gaussian Splatting to train\nan RGB scene. We train it for 30,000 iterations, and in the\nend, each scene comprises around 2,500,000 points. We fol-\nlow the default parameter setting as in [17] to train the RGB\nscene. Then we train our 3D language Gaussians by fix-\ning all other parameters of 3D Gaussians such as mean and\nopacity. Only the language features are learnable during this\nstage. We train the language features for 30,000 iterations.\nOur autoencoder is implemented by MLPs, which compress\nthe 512-dimensional CLIP features into 3-dimensional la-\ntent features. For a scene with 1080p resolution, our model\nis trained for 25 minutes on an NVIDIA RTX-3090 GPU\nand takes roughly 4GB of memory in total.\n4.2. Results on the LERF dataset\nQuantitative Results. We first compare our method with\nother methods on the LERF dataset.\nTable 1 shows the\nlocalization results. We observe that our method achieves\nan overall accuracy of 84.3%, significantly outperforming\nLERF. Table 2 further shows the IoU results of 3D seman-\ntic segmentation, our method outperforms LERF by 14.0%,\nwhich illustrates the superiority of our proposed LangSplat.\nVisualization Results. To show the learned 3D language\nfield, we visualize the learned features by computing 3-\ndimensional PCA components of learned language features\nfollowing [20].\nThe results are shown in Figure 1.\nWe\nsee that the LERF learned features fail to generate clear\nboundaries between objects while our method gives pre-\nRGB\nLERF\nLSeg\nOurs\n\u2019spoon\u2019\n\u2018paper napkin\u2019\n\u2018spatula\u2019\nRGB\nLERF\nLSeg\nOurs\nRGB\nLERF\nLSeg\nOurs\nFigure 3. Qualitative comparisons of open-vocabulary 3D object localization on the LERF dataset. The red points are the model predictions\nand the black dashed bounding boxes denote the annotations.\nRGB\nLERF\nLSeg\nOurs\nGround Truth\nchopsticks\nwavy noodles\nkamaboko\nFigure 4. Qualitative comparisons of open-vocabulary 3D semantic segmentation on the LERF dataset.\nComponent\nPerformance\nAE\n3D-GS\nSAM\nIoU (%)\nSpeed (s/q)\n28.20\n30.93\n!\n46.74\n7.77\n!\n!\nOOM\nOOM\n!\n!\n!\n51.15\n0.26\nTable 3. Ablations. The results are obtained on the ramen scene.\ncise object shapes solely using CLIP features. We further\nshow the visualization results of object localization and se-\nmantic segmentation in Figure 3 and Figure 4, respectively.\nWe observe that the activation regions generated by LERF\nare more dispersed, while ours are more concentrated, and\nour activation regions can better align with the ground truth\nshape compared to those produced by LERF.\nAblation Study. We conduct ablations on the ramen scene\nand report the semantic segmentation results in Table 3. We\ntest the query speed on an NVIDIA RTX-3090 GPU. Here\nAE represents autoencoder and 3D-GS denotes 3D Gaus-\nsian Splatting. Without any of our proposed components,\nour baseline equals LERF, which has a speed of 30.93 sec-\nonds per text query at the resolution of 988 \u00d7 731. Using\nSAM to replace the scale-based solution significantly in-\ncreases the IoU by 18.54%, showing our SAM-based solu-\nComponent\nPerformance\nAE\n3D-GS\nSAM\nmIoU (%)\nSpeed (s/q)\n53.2\n55.7\n!\n85.7\n18.4\n!\n!\nOOM\nOOM\n!\n!\n!\n94.2\n0.28\nTable 4.\nAblations result on the bench scene of the 3D-OVS\ndataset. The image resolution is 1440 \u00d7 1080.\ntion effectively addresses the point ambiguity issue, leading\nto accurate 3D language features. Simply replacing NeRF\nwith 3D Gaussian Splatting leads to the out-of-memory is-\nsue as explicitly modeling CLIP features poses huge mem-\nory demands. Incorporating a scene-specific autoencoder\neffectively addresses this issue and results in further im-\nprovements in both accuracy and efficiency. In the end, our\nLangSplat achieved a 119 \u00d7 speedup over LERF while sig-\nnificantly surpassing LERF in terms of accuracy.\nWe further conducted the ablations on the 3D-OVS\ndataset, which has a higher image resolution of 1440\u00d71080.\nTable 4 lists the results on the bench scene.\nWe also\ntested the query speed on an NVIDIA RTX-3090 GPU.\nWe observed that with the increase in image resolution,\nthe speedup over LERF further improved to 199 \u00d7, which\ndemonstrates the huge potential of our method.\nActu-\nstapler\nred apple\nhand soap\ngreen lawn\nred beg\ncamera\nRGB\nGround Truth\nLERF\nLSeg\n3D-OVS\nOurs\nlawn\nsofa\nNew York Yankees cap\nBlack headphone\nGundam\nPikachu\nXbox wireless controller\nred Nintendo Switch joy-con controller\na stack of UNO cards\ngrey sofa\nbanana\nblack leather shoe\nhand\nwhite sheet\nbed\nFigure 5. Qualitative comparisons of different methods on the 3D-OVS dataset. We visualize the segmentation results in 3 scenes. We\nobserve that our method gives the most accurate segmentation maps.\nally, the rendering process in our method is highly ef-\nficient.\nMost of the computational time is allocated to\nthe decoder rather than the rendering process. Therefore,\nas the resolution increases, the computational cost of our\nmethod only experiences a slight increase. We believe that\na higher speedup can be achieved when dealing with higher-\nresolution scenes.\n4.3. Results on the 3D-OVS dataset\nQuantitative Results. We compare our method with other\n2D and 3D state-of-the-art methods on the 3D-OVS dataset\nin Table 5.\nWe observe that our method not only out-\nperforms 2D-based methods such as ODISE [41] and OV-\nSeg [22], but also achieves better results than 3D-based\nmethods including LERF [18] and 3D-OVS [23] by a large\nmargin. Note that in this dataset, we generate object masks\nonly based on the query text while others, such as 3D-OVS,\nrequire the complete category list. In the end, our method\nachieves an overall mIoU of 93.4%, which demonstrates\nthat our method effectively learns a precise 3D language\nfield.\nQualitative Results. We present the qualitative results in\nMethod\nbed\nbench room\nsofa\nlawn overall\nLSeg [21]\n56.0\n6.0\n19.2\n4.5\n17.5\n20.6\nODISE [41]\n52.6\n24.1\n52.5\n48.3\n39.8\n43.5\nOV-Seg [22]\n79.8\n88.9\n71.4\n66.1\n81.2\n77.5\nFFD [20]\n56.6\n6.1\n25.1\n3.7\n42.9\n26.9\nLERF [18]\n73.5\n53.2\n46.6\n27\n73.7\n54.8\n3D-OVS [23] 89.5\n89.3\n92.8\n74\n88.2\n86.8\nLangSplat\n92.5\n94.2\n94.1\n90.0\n96.1\n93.4\nTable 5. Quantitative comparisons of 3D semantic segmentation\non the 3D-OVS dataset. We report the mIoU scores (%).\nFigure 5.\nAs LERF suffers from the patchy issue and\nlearns over-smoothed features, it fails to find accurate ob-\nject boundaries. Among all state-of-the-art methods, our\nmethods give the most accurate segmentation maps, which\nfurther demonstrates the effectiveness of our LangSplat.\n5. Conclusion\nIn this paper, we have presented LangSplat, a method for\nconstructing 3D language fields that enables precise and ef-\nficient open-vocabulary querying within 3D spaces. By ex-\ntending 3D Gaussian Splatting with language features and\nlearning a scene-specific language autoencoder, LangSplat\ncircumvents slow rendering speed associated with NeRF-\nbased methods. Furthermore, we propose to learn the se-\nmantic hierarchy defined by SAM, which effectively re-\nsolves the point ambiguity problem, enabling more precise\nand reliable 3D language fields. The experimental results\nclearly demonstrate LangSplat\u2019s superiority over existing\nstate-of-the-art methods like LERF, particularly in terms of\nits remarkable 199 \u00d7 speed improvement and enhanced per-\nformance in open-ended 3D language query tasks.\nReferences\n[1] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\nKawanabe. Scanqa: 3d question answering for spatial scene\nunderstanding. In CVPR, pages 19129\u201319139, 2022. 2\n[2] Yash Bhalgat, Iro Laina, Jo\u02dcao F Henriques, Andrew Zisser-\nman, and Andrea Vedaldi. Contrastive lift: 3d object instance\nsegmentation by slow-fast contrastive fusion. arXiv preprint\narXiv:2306.04633, 2023. 4, 5\n[3] Nancy Bonvillain. Language, culture, and communication:\nThe meaning of messages. Rowman & Littlefield, 2019. 1\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, pages 9650\u20139660, 2021. 3\n[5] Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio S\nFeris, and Vicente Ordonez. Simvqa: Exploring simulated\nenvironments for visual question answering. In CVPR, pages\n5056\u20135066, 2022. 2\n[6] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei\nShen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment\nanything in 3d with nerfs. 2023. 3\n[7] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein.\npi-gan: Periodic implicit genera-\ntive adversarial networks for 3d-aware image synthesis. In\nCVPR, pages 5799\u20135809, 2021. 2\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In ECCVn, pages\n333\u2013350. Springer, 2022. 2\n[9] Boyuan Chen,\nFei Xia,\nBrian Ichter,\nKanishka Rao,\nKeerthana Gopalakrishnan, Michael S Ryoo, Austin Stone,\nand Daniel Kappler. Open-vocabulary queryable scene rep-\nresentations for real world planning. In ICRA, pages 11509\u2013\n11522. IEEE, 2023. 2\n[10] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\n3\n[11] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,\nZongxin Yang, Wenguan Wang, and Yi Yang. Segment and\ntrack anything. arXiv preprint arXiv:2305.06558, 2023. 3\n[12] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming-\nMing Cheng, and Shuicheng Yan. Editanything: Empower-\ning unparalleled flexibility in image editing and generation.\nIn ACM MM, Demo track, 2023. 3\n[13] Daniel Gordon, Aniruddha Kembhavi, Mohammad Raste-\ngari, Joseph Redmon, Dieter Fox, and Ali Farhadi.\nIqa:\nVisual question answering in interactive environments. In\nCVPR, pages 4089\u20134098, 2018. 2\n[14] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram\nBurgard.\nVisual language maps for robot navigation.\nIn\nICRA, pages 10608\u201310615. IEEE, 2023. 2\n[15] Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng\nGao, and Hongsheng Li.\nInstruct2act:\nMapping multi-\nmodality instructions to robotic actions with large language\nmodel. arXiv preprint arXiv:2305.11176, 2023. 3\n[16] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,\nQiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh\nIyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al.\nConceptfusion: Open-set multimodal 3d mapping.\narXiv\npreprint arXiv:2302.07241, 2023. 2\n[17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. TOG, 42(4):1\u201314, 2023. 2, 4, 6\n[18] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo\nKanazawa, and Matthew Tancik. Lerf: Language embed-\nded radiance fields. In ICCV, pages 19729\u201319739, 2023. 2,\n3, 4, 6, 8, 9, 1\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In ICCV, 2023. 2, 3, 4\n[20] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature field dis-\ntillation. NeurIPS, 35:23311\u201323330, 2022. 2, 3, 6, 9, 1\n[21] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Rene Ranftl.\nLanguage-driven semantic seg-\nmentation. In ICLR, 2022. 3, 6, 9, 1\n[22] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan\nZhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana\nMarculescu. Open-vocabulary semantic segmentation with\nmask-adapted clip. In CVPR, pages 7061\u20137070, 2023. 8, 9,\n1\n[23] Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu,\nYingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt,\nEric Xing, and Shijian Lu.\nWeakly supervised 3d open-\nvocabulary segmentation. In NeurIPS, 2023. 2, 3, 6, 8, 9,\n1\n[24] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wen-\nwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment\nany point cloud sequences by distilling vision foundation\nmodels. arXiv preprint arXiv:2306.09347, 2023. 3\n[25] Zhihe Lu, Zeyu Xiao, Jiawang Bai, Zhiwei Xiong, and Xin-\nchao Wang. Can sam boost video super-resolution? arXiv\npreprint arXiv:2305.06524, 2023. 3\n[26] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3d gaussians:\nTracking\nby persistent dynamic view synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 3\n[27] Jun Ma and Bo Wang. Segment anything in medical images.\narXiv preprint arXiv:2304.12306, 2023. 3\n[28] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song,\nDeepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, Gao\nCong, Yingjie Hu, et al. On the opportunities and challenges\nof foundation models for geospatial artificial intelligence.\narXiv preprint arXiv:2304.06798, 2023. 3\n[29] Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen\nYang, Nicholas Konz, and Yixin Zhang. Segment anything\nmodel for medical image analysis: an experimental study.\nMedical Image Analysis, 89:102918, 2023. 3\n[30] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. TOG, 41(4):1\u201315, 2022. 2\n[31] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, pages 5865\u20135874, 2021. 2\n[32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. In CVPR, pages 10318\u201310327, 2021. 2\n[33] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel\nPinto, Soumith Chintala, and Arthur Szlam.\nClip-fields:\nWeakly supervised semantic fields for robotic memory.\narXiv preprint arXiv:2210.05663, 2022. 3\n[34] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-\n3d: Towards single-view anything reconstruction in the wild.\narXiv preprint arXiv:2304.10261, 2023. 3\n[35] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack\nKaelbling, and Phillip Isola.\nDistilled feature fields en-\nable few-shot language-guided manipulation. arXiv preprint\narXiv:2308.07931, 2023. 2, 3\n[36] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul`o, Nor-\nman M\u00a8uller, Matthias Nie\u00dfner, Angela Dai, and Peter\nKontschieder. Panoptic lifting for 3d scene understanding\nwith neural fields. In CVPR, pages 9043\u20139052, 2023. 3\n[37] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, pages 5459\u20135469, 2022. 2\n[38] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 3\n[39] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea\nVedaldi. Neural feature fusion fields: 3d distillation of self-\nsupervised 2d image representations. In 3DV, pages 443\u2013\n453. IEEE, 2022. 3, 4, 5\n[40] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 3\n[41] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nCVPR, pages 2955\u20132966, 2023. 8, 9, 1\n[42] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968, 2023. 3\n[43] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023. 3\n[44] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting. arXiv preprint\narXiv:2310.10642, 2023. 3\n[45] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu.\nMatte anything: Interactive natural image matting with seg-\nment anything models.\narXiv preprint arXiv:2306.04121,\n2023. 3\n[46] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng\nZhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-\ndreamer: Fast generation from text to 3d gaussian splatting\nwith point cloud priors. arXiv preprint arXiv:2310.08529,\n2023. 3\n[47] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images. In\nCVPR, pages 4578\u20134587, 2021. 2\n[48] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin\nJin, Wenjun Zeng, and Zhibo Chen.\nInpaint anything:\nSegment anything meets image inpainting. arXiv preprint\narXiv:2304.06790, 2023. 3\n[49] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-\ndrew J Davison. In-place scene labelling and understanding\nwith implicit scene representation. In ICCV, pages 15838\u2013\n15847, 2021. 2, 3\n[50] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross. Ewa volume splatting. In VIS, pages 29\u2013538.\nIEEE, 2001. 5\nA. Video Demo\nIn Figure 1 of our main paper, we have visualized the lan-\nguage features learned by LERF and our method.\nFor\na fair comparison, we perform PCA on the decoded fea-\nture \u03a8(F l\nt ) \u2208 RD\u00d7H\u00d7W for our method. However, one\nbenefit of our method is that we are able to directly vi-\nsualize the learned language features in the encoded 3-\ndimensional latent space, which can ensure color consis-\ntency between frames.1 Specifically, we normalize the en-\ncoded 3-dimensional latent features Hl\nt(v) \u2208 R3\u00d7H\u00d7W\nand visualize them by treating the 3-dimensional features\nas RGB channels.\nWe strongly recommend readers refer to our video demo\nto observe the learned 3D language fields in the scene-\nspecific latent space.\nThe video demonstrates that our\nmethod has acquired a 3D language representation that is\nboth 3D-consistent and distinctly shaped, which signifi-\ncantly distinguishes it from existing methods that often only\nlearn 3D language representations with blurred boundaries.\nMeanwhile, our approach achieves a speedup of 119 \u00d7\ncompared to LERF at a resolution of 988 \u00d7 731 and further\nimproves to 199 \u00d7 faster at a resolution of 1440 \u00d7 1080.\nB. More Implementation Details\nFor each text query, we can obtain three relevancy maps\nwith our trained 3D language Gaussians, each representing\none semantic level defined by SAM. Then we use different\nstrategies to choose the best semantic level and obtain the\npredictions for different tasks.\n3D Object Localization on LERF. To mitigate the impact\nof outliers, we first employ a mean convolution filter with\na size of 20 to smooth the values of three relevancy maps.\nFor the smoothed relevancy maps, we select the one with\nthe highest smoothed relevancy score and take the corre-\nsponding position as the final prediction.\n3D Semantic Segmentation on LERF. Similarly, to miti-\ngate the influence of outliers, we apply a mean filter with\na size of 20 to smooth the three relevancy maps. Subse-\nquently, we select the relevancy map with the maximum\nsmoothed relevancy score for binary mask prediction. For\nthe selected relevancy map, we first normalize its relevancy\nscores and then use a threshold to obtain a binary image as\nthe final prediction mask.\n3D Semantic Segmentation on 3D-OVS. For each class\nquery, we obtain three relevancy maps. We apply a thresh-\nold of 0.4 to these relevancy maps, setting relevancy scores\nbelow 0.4 to 0 and relevancy scores above 0.4 to 1, resulting\nin three binary maps. We calculate the average relevancy\nscores within the mask region for each relevancy map and\n1The consistency of color in PCA visualizations across different frames\nis not ensured.\nMethod\nbed\nbench\nroom\nsofa\nlawn overall\nLSeg [21]\n87.6\n42.7\n46.1\n16.5\n77.5\n54.1\nODISE [41]\n86.5\n39.0\n59.7\n35.4\n82.5\n60.6\nOV-Seg [22]\n40.4\n89.2\n49.1\n69.6\n92.1\n68.1\nFFD [20]\n86.9\n42.8\n51.4\n9.5\n82.6\n54.6\nLERF [18]\n86.9\n79.7\n79.8\n43.8\n93.5\n76.7\n3D-OVS [23]\n96.7\n96.3\n98.9\n91.6\n97.3\n96.2\nLangSplat\n99.2\n98.6\n99.3\n97.9\n99.4\n98.9\nTable 6. Quantitative comparisons of 3D semantic segmentation\non the 3D-OVS dataset. We report the accuracy scores (%).\nd\n1\n2\n3\n8\nmIoU (%)\n6.46\n91.93\n94.19\n95.20\nSpeed (s/q)\n0.2770\n0.2779\n0.2788\n0.2807\nTable 7. The ablations of latent dimension d for our scene-specific\nautoencoder. The results are obtained on the bench scene of the\n3D-OVS dataset. The image resolution is 1440 \u00d7 1080.\nselect the relevancy map with the highest average response\nas the final predicted binary map.\nC. More Quantitative Results\nIn addition to the mIoU metric, the Accuracy metric is also\nemployed on the 3D-OVS dataset in [23]. 2 Therefore, we\nalso compare our method with other state-of-the-art meth-\nods on the 3D-OVS dataset using the Accuracy metric. The\nresults are shown in Table 6. We observe that our method\nconsistently outperforms other methods, which further il-\nlustrates the superiority of our method.\nD. More Ablation Study\nTo reduce the memory cost of our 3D language Gaussians,\nwe proposed the scene-specific autoencoder to learn a latent\nfeature. We show the ablation results of different latent di-\nmensions d on the bench scene of the 3D-OVS dataset in\nTable 7. We observed that as d increases, the mIoU perfor-\nmance improves, with only a slight increase in the time cost.\nWe chose d = 3 because it allows us to directly visualize\nthe learned 3D language field in the latent space by treating\nthe 3-dimensional features as the RGB channels. We also\nstrongly encourage readers to refer to our video demo to\nobserve how our learned language field accurately captures\nthe precise 3D shape of objects in the scene-specific latent\nspace.\n2After carefully reviewing the codes and results, we discovered that the\nmAP results reported in [23] are, in fact, the Accuracy results.\nRGB\nLERF\nLSeg\nOurs\n\u2018red apple\u2019\n\u2018waldo\u2019\n\u2018plate\u2019\nRGB\nLERF\nLSeg\nOurs\nRGB\nLERF\nLSeg\nOurs\n\u2018chopsticks\u2019\nRGB\nLERF\nLSeg\nOurs\nFigure 6. More qualitative comparisons of open-vocabulary 3D object localization on the LERF dataset. The red points are the model\npredictions and the black dashed bounding boxes denote the annotations.\nE. More Visualization Results\n3D Object Localization on LERF. We visualize more ex-\namples on the LERF dataset for open-vocabulary 3D ob-\nject localization in Figure 6. We found that for text queries\nsuch as \u201cred apple\u201d and \u201cplate\u201d, LERF failed to correctly lo-\ncate the 3D positions, whereas our method succeeded. For\ntext queries like \u201cwaldo\u201d and \u201cchopsticks\u201d, although LERF\ncould identify the correct location, its activation values were\nmore dispersed, whereas our method was able to focus more\nprecisely on the queried object.\n3D Semantic Segmentation on LERF. We demonstrate\nmore examples on the LERF dataset for open-vocabulary\n3D semantic segmentation in Figure 7. We observed that\nthe results produced by LERF were unable to provide the\nprecise shape of the queried object and exhibited a signifi-\ncant amount of noise, whereas our method could accurately\ndepict the object\u2019s shape. These results show the effective-\nness of our proposed LangSplat.\n3D Semantic Segmentation on 3D-OVS. We show more\nscenes on the 3D-OVS dataset for open-vocabulary 3D se-\nmantic segmentation in Figures 8, 9, 10, and 11, respec-\ntively. Compared to the previous state-of-the-art method\n3D-OVS, our approach provides more precise object bound-\nOurs\nGround Truth\nLERF\nRGB\nrubics cube\nwaldo\nspatula\npink ice cream\ngreen apple\ngreen toy chair\nfigurines\nkamaboko\nwavy noodles\nnori\negg\nchopsticks\nramen\nplate\nthree cookies\ntea in a glass\npaper napkin\ncoffee mug\nhooves\nteatime\npour-over vessel\nyellow desk\ntoaster\nottolenghi\nstainless steel pots\nknife\nWaldo kitchen\nFigure 7. More qualitative comparisons of open-vocabulary 3D semantic segmentation on the LERF dataset.\naries and exhibits reduced noise, which illustrates that our\nLangSplat learns a more accurate 3D language field.\nRGB\nGround Truth\n3D-OVS\nOurs\nblue sofa\nsnacks\nblue-grey sofa\na squirrel pig doll\na JBL Bluetooth speaker\na bottle of perfume\nan aircon controller\nsunglasses\nFigure 8. Qualitative comparisons on the blue sofa scene of the 3D-OVS dataset.\nblue sofa\nRGB\nGround Truth\n3D-OVS\nOurs\nsnacks\norange juice drink\ncalculator\nCoke Cola\nGlico Pocky chocolate biscuits sticks box\ndesktop\npitaya\nFigure 9. Qualitative comparisons on the snacks scene of the 3D-OVS dataset.\ndesktop\na white keyboard\na can of red bull drink\na pack of pocket tissues\nblue partition\nthe book of The Unbearable Lightness of Being\nRGB\nGround Truth\n3D-OVS\nOurs\noffice desk\nroom\nFigure 10. Qualitative comparisons on the office desk scene of the 3D-OVS dataset.\noffice desk\nRGB\nGround Truth\n3D-OVS\nOurs\nroom\nweaving basket\nrabbit\nbaseball\ndinosaur\nshrilling chicken\nwood\nFigure 11. Qualitative comparisons on the room scene of the 3D-OVS dataset..\n"
  },
  {
    "title": "Audiobox: Unified Audio Generation with Natural Language Prompts",
    "link": "https://arxiv.org/pdf/2312.15821.pdf",
    "upvote": "12",
    "text": "Audiobox: Unified Audio Generation\nwith Natural Language Prompts\nApoorv Vyas\u2217, Bowen Shi\u2217, Matthew Le\u2217, Andros Tjandra\u2217, Yi-Chiao Wu\u2217, Baishan Guo, Jiemin Zhang,\nXinyue Zhang, Robert Adkins, William Ngan, Jeff Wang, Ivan Cruz, Bapi Akula, Akinniyi Akinyemi, Brian\nEllis, Rashel Moritz, Yael Yungster, Alice Rakotoarison, Liang Tan, Chris Summers, Carleigh Wood,\nJoshua Lane, Mary Williamson\u2020, Wei-Ning Hsu\u2020\nAudiobox Team, Fundamental AI Research (FAIR) at Meta\n\u2217Research team, equal contribution\n\u2020Research and engineering leadership, equal contribution\nAudio is an essential part of our life, but creating it often requires expertise and is time-consuming.\nResearch communities have made great progress over the past year advancing the performance of\nlarge scale audio generative models for a single modality (speech, sound, or music) through adopting\nmore powerful generative models and scaling data. However, these models lack controllability in\nseveral aspects: speech generation models cannot synthesize novel styles based on text description\nand are limited on domain coverage such as outdoor environments; sound generation models only\nprovide coarse-grained control based on descriptions like \u201ca person speaking\u201d and would only generate\nmumbling human voices. This paper presents Audiobox, a unified model based on flow-matching that\nis capable of generating various audio modalities. We design description-based and example-based\nprompting to enhance controllability and unify speech and sound generation paradigms. We allow\ntranscript, vocal, and other audio styles to be controlled independently when generating speech. To\nimprove model generalization with limited labels, we adapt a self-supervised infilling objective to\npre-train on large quantities of unlabeled audio. Audiobox sets new benchmarks on speech and\nsound generation (0.745 similarity on Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for\ntext-to-sound) and unlocks new methods for generating audio with novel vocal and acoustic styles.\nWe further integrate Bespoke Solvers, which speeds up generation by over 25 times compared to the\ndefault ODE solver for flow-matching, without loss of performance on several tasks.\nCorrespondence: Apoorv Vyas vyasapoorv@meta.com, Wei-Ning Hsu wnhsu@meta.com\nDemo: https://audiobox.metademolab.com/\n1\nIntroduction\nWhy building audio generative models: Audio is a key component in creating many forms of content,\nsuch as movies, podcasts, audiobooks, and Ads. However, audio creation is time-consuming and requires\nvarious expertise, such as voice acting, music composing and performing, Foley sound effect creation, and\nsound engineering. This imposes a great barrier to entry for the general public, making it hard for people\nto become audio creators. Even for professionals, performing these tasks can still take a lot of time and\nresources, limiting their productivity. Developing audio generative models that are generalizable, controllable,\nand high quality can bring transformative changes to the audio creation process, improving the efficiency of\nthe professionals as well as unleashing the creativity for everyone.\nProgress of audio generative models: Recently, researchers have made significant progress advancing\naudio generative models. Speech generative models can mimic any vocal style using audio prompts that are\nas short as three seconds (Wang et al., 2023a; Shen et al., 2023; Le et al., 2023; Kharitonov et al., 2023),\ninfill a portion of speech to remove transient noise or edit words for any speaker (Le et al., 2023; Shen\net al., 2023), synthesize foreign languages in anyone\u2019s voice (Zhang et al., 2023; Le et al., 2023), and create\ndialogues (Borsos et al., 2023). Music generative models can create music in various styles using a short text\ndescription (Schneider et al., 2023; Huang et al., 2023a; Agostinelli et al., 2023; Copet et al., 2023) and infill\n1\narXiv:2312.15821v1  [cs.SD]  25 Dec 2023\nFigure 1 Audiobox model diagram\na portion of music (Li et al., 2023). Sound effect generative models follows a similar paradigm. They are\ncapable of creating and infilling complex acoustic scenes like \u201cbirds chirping and water dripping with some\nbanging in the background\u201d given a text description (Yang et al., 2023c; Kreuk et al., 2022; Huang et al., 2023b;\nGhosal et al., 2023; Liu et al., 2023b,c). Recent models also extends to more general editing, such as removal\nor addition of sound events with natural language instructions (Wang et al., 2023b; Liu et al., 2023d).\nLimitation of existing models: Existing audio generative models are still limited in controllability and\ngeneralizability. First, the real world audio content often contain a mix of speech, music, and sound effects.\nHowever, existing audio generative models are mostly modality-specific, which only generate either speech,\nmusic, or sound effects. In particular, existing large scale speech generative models (Wang et al., 2023a; Le\net al., 2023; Shen et al., 2023) are trained mostly on audiobooks (Zen et al., 2019; Kahn et al., 2019; Pratap\net al., 2020), which lacks diversity compared to truly in-the-wild data such as AudioSet (Gemmeke et al., 2017)\nin terms of expressivity (e.g., non-verbal sounds like coughing, screaming, laughing) and acoustic conditions\n(e.g., urban, rural, public indoor, stadiums). These models can only generate audio of limited styles and do\nnot capture the correlation between different audio modalities.\nOn the other hand, there is a discrepancy between speech and sound/speech generation paradigm. Recent\nspeech generation models mostly use example-based control, where an audio sample of the target style is\nprovided and the style control is more precise; in contrast, description-based control is adopted for music\nand sound generation, where the model can create novel styles based on natural language prompts. Both\napproaches have their strengths and weaknesses, but such a discrepancy prevents development of unified\nmodels that enjoy the best of both worlds.\nLast but not least, existing sound generation models only provide coarse control such as \u201ca man is speaking\u201d\nwhen generating speech. Existing datasets do not offer finer-grained captions that characterizes vocal styles\nin greater details, such as \u201cA middle aged woman from the American South is speaking over the phone in a\npassionate voice. She speaks in at a fast pace with a high pitch.\u201d Neither do these models enable transcript\ninput to controlling the textual content. Hence, these models can only generate mumbling speech.\nDue to a lack of consideration in the language-guided generation of speech within a natural setting, designing\nproper objective evaluation metrics for such universal models remains an open question that has not been\nfully addressed by prior works. In objective evaluation, previous speech-oriented studies Guo et al. (2023);\nLeng et al. (2023); Yang et al. (2023a) often adopt ad-hoc evaluation metrics (e.g., accuracy of pre-defined\nattributes), making it challenging to generalize to free-form instructions. The joint audio-text embedding\nnetwork (e.g., CLAP Wu et al. (2023)), widely utilized in text-to-audio generation, is tailored to sound events\nand frequently falls short in capturing intricate attributes such as accents in speech (see Section 7.1.1).\n2\nGoals and overview of our model: To tackle these problems, there are three key objectives of this work.\nFirst, we aim to build a unified model for sound and speech in order to generate a wider variety of real-world\naudio, which is often a mix of both. Second, we want to improve controllability for creating novel styles\nthrough enabling multiple input methods, using either reference audio, text description, or a combination of\nboth. Last but not least, to improve model generalization, we want to scale training data and utilize data\nwith different level of supervision.\nTo that end, we present the Audiobox framework. Audiobox is built upon Voicebox (Le et al., 2023) and\nSpeechFlow (Liu et al., 2023a), which are flow-matching based models for transcript-guided speech generation\nand self-supervised speech pre-training, respectively. To facilitate data scaling and development of downstream\nmodels, we first adopt the SpeechFlow pre-training method and pre-train a unified model using large quantities\nof unlabeled speech, music, and sound effects, referred to as Audiobox SSL (Section 4). To validate the\neffectiveness of the unified pre-trained model, we fine-tune Audiobox SSL for transcript-guided speech\ngeneration (Audiobox Speech, Section 5) and description-guided sound generation (Audiobox Sound,\nSection 6), showing significant improvements from prior studies.\nCombining the best of both worlds, we present Audiobox, the unified model for sound and speech generation\nin Section 7. It bridges the gap between sound and speech generation by enabling natural language prompts\nfor holistic style control, and furthers disentangled speech control with voice prompts. Our joint model\nachieves unprecedented controllability for universal audio generation and superior versatility with additional\ncapabilities on top of what Voicebox offers. Audiobox outperforms existing domain specific models on\nmultiple tasks and is close to Audiobox Speech and Audiobox Sound on their corresponding benchmark\ntasks.\nTo facilitate the evaluation of Audiobox and advance research in text-guided universal audio generative models,\nwe propose Joint-CLAP, trained on both sound and speech description data. In comparison to CLAP Wu\net al. (2023), Joint-CLAP significantly outperforms CLAP in retrieving description-based speech, and the\ntext-to-audio similarity exhibits a stronger correlation with human judgment.\nOrthogonally, to improve performance-efficiency trade-off, we integrate Bespoke Solver, a novel post-training\ninference optimization methods for flow-matching models. With Bespoke Solver, our models are able speed\nup by 25x compared to using the adaptive step size dopri5 solver without loss of performance.\nAs generative models become more powerful and essential parts of everyone\u2019s life, it is more important\nthan ever to conduct research responsibly and mitigate potential risks. We conducted a series of study\ndemonstrating the fairness is achieved through better representing voices of different demographic groups with\ndata scaling. We also validate the effectiveness of a recent watermarking system (Seamless Communication,\n2023), showing the verification is highly effective and robust to adversarial perturbation.\n2\nRelated Work\nThis paper is related to a large body of work on large scale generative modeling for audio. As the focus of\nthis work is on universality and controllability, we first discuss controllable generation for modality specific\nmodels and then compare with recent studies on universal models that can perform multiple tasks or generate\naudio in multiple modalities and domains. For the rest of the paper, we will refer to speech, sound, music\nas different audio modalities, and within modality style variation, such as read speech, spontaneous speech,\nconversational speech, as different domains.\nLarge scale in-context text-to-speech generative models: Over the past few months, there has been\nsignificant progress in developing large scale speech generative models (Wang et al., 2023a; Shen et al., 2023;\nKharitonov et al., 2023; Le et al., 2023; Yang et al., 2023b; Borsos et al., 2023) that are trained on in-the-wild\ndata at the scale of close to 100K hours (Kahn et al., 2019; Pratap et al., 2020) with minimal supervision,\nwhich leads to much better generalization for synthesizing unseen speech styles in a zero-shot fashion. These\nmodels are in sharp contrast to conventional regression-based models such as Ren et al. (2021); Shen et al.\n(2017); \u0141a\u0144cucki (2021), which are trained on highly curated datasets (Yamagishi et al., 2019) containing\nclean audio, limited style variation, and extensive labels (e.g., speaker and emotion labels).\n3\nThe key to successful data scaling in recent work is the adoption of powerful generative models that can\ncapture highly stochastic input-output relationships. For example, VALL-E (Wang et al., 2023a) adopt the\ntoken-based autoregressive language modeling approach, which converts speech into discrete tokens with a\nneural codec model (D\u00e9fossez et al., 2022) and formulate text-to-speech (TTS) as a conditional language\nmodeling problem given a transcript and an audio prompt (the first few seconds of the target speech).\nNaturalSpeech2 (Shen et al., 2023) and Voicebox (Le et al., 2023) adopt non-autoregressive diffusion (Ho\net al., 2020) and conditional flow-matching models (Lipman et al., 2023). Given a transcript and an audio\ncontext (the audio surrounding the target speech), these models iteratively transform a noise sampled from a\nsimple prior to speech, represented as learned latent features or mel spectrograms.\nAt the high level, VALL-E performs transcript-guided speech continuation while NaturalSpeech2 and Voicebox\nperform transcript-guided speech infilling. These models are trained with only transcript supervision, which\nfacilitates data scaling. The style of the generated audio is controlled through the audio prompt or audio\ncontext. Note that the style refers to not only voice, but everything other than transcript, including prosody,\nemotion, acoustic environment, channel, noise, etc. This can be understood as a form of in-context learning:\nbecause the audio style tends to be coherent within an utterance, these models learn to infer the style of the\ntarget based on its context. In turn, it enables generalization to unseen style, such that speech of any style\ncan be generated by conditioning on an audio prompt/context of the desired style.\nWhile the in-context style transfer paradigm is powerful, it also possesses several limitations in terms of\ncontrollability. First, audio prompt is the only input mechanism of controlling the audio style. Users cannot\nprovide a descriptive text, such as \u201ca young man speaking with a happy tone in an auditorium\u201d to create\ndiverse speech matching the description, whereas this feature is commonly supported and widely enjoyed for\nimage (Ramesh et al., 2022; Rombach et al., 2022), music (Agostinelli et al., 2023), and sound (Kreuk et al.,\n2022) generation. Second, disentangled style control is not enabled with the paradigm, where voice and other\nattributes, such as emotion and acoustic condition, can be controlled independently. This feature is often\ndesired as exemplified in earlier work where emotion and voice can be controlled independently (Hsu et al.,\n2019; Kulkarni et al., 2021; Nguyen et al., 2023).\nNatural language style prompting for controllable speech generation: Studies on controllable speech\ngeneration aims to develop models which can generate speech of many different domains and provide input\nmethods for disentangled, flexible, and accurate control. Earlier models often enable control over only a small\nnumber of attributes (e.g., speaker and emotion) with a fixed number of options (e.g., happy/sad/neutral\nfor emotion) through one-hot vectors (Nguyen et al., 2023). Such methods are difficult to generalize as it is\ndifficult to represent many speech attributes, such as audio quality, acoustic environment, with one-hot vectors.\nNor could information such as \u201ca speaker starts with a slow pace and speeds up\u201d be accurately represented.\nIn-context TTS (Wang et al., 2023a) models greatly improves domain coverage, but has the limitation on\nflexibility and disentangled control described above.\nTo address the limitation, several recent studies also propose to control speech style through natural language\nprompts. InstructTTS (Yang et al., 2023a) and PromptTTS (Guo et al., 2023) are the two earliest works.\nThey are trained on small scale data with mainly emotion variation and limited number of speakers (7 for\nInstructTTS and 2 for PromptTTS synthetic setup). In particular, InstructTTS collects human descriptions\nfor 44 hours of speech focusing on only the emotion and a separate speaker ID input is used as model input.\nTherefore, the natural language prompt is only used for controlling the emotion. PromptTTS recruits human\nannotators to write descriptions to given four to five attribute labels (emotion, gender, volume, speed, and\npitch; emotion label is not available for the real data), and trains models on 2-voice synthetic data as well as\nLibriTTS (Zen et al., 2019). Because the descriptions of PromptTTS are created based on attribute labels\ninstead of speech samples, these descriptions do not contain additional information compared to the labels\nand theoretically does not enable finer grained attribute control.\nPromptTTS2 (Leng et al., 2023) is a concurrent work which improves upon PromptTTS in two aspects. First,\nit proposes a automatic description creation pipeline based on speech attribute labeler and large language\nmodels, which enables scaling to training on 44K hours of audiobook data. Second, PromptTTS2 adopts a\ndiffusion model to capture the one-to-many relationship given input (transcript and description), whereas\nPromptTTS adopts a regression model assuming deterministic mapping. Nevertheless, similar to PromptTTS,\nall the descriptions PromptTTS2 create are derived from four categorical attributes with two to three options\n4\neach (total 54 combinations). Hence, PromptTTS2 does not provide finer grained control than PromptTTS\nand has limited coverage on the attributes it can control via natural language prompt.\nLarge scale general-domain models for sound and music generation: Text-to-sound (Kreuk et al.,\n2022) and text-to-music (Schneider et al., 2023) are the emerging paradigms for general-domain sound and\nmusic generation, in contrast to earlier studies that generate finite sound effects (Donahue et al., 2018) or\ninstruments (Huang et al., 2018). The text here refers to a holistic description of the target audio, such as \u201cA\nchild shouts while an emergency vehicle siren sounds with the horn blowing.\u201d (Kim et al., 2019) and \u201cThe\nlow quality recording features a ballad song that contains sustained strings... It sounds sad and soulful, like\nsomething you would hear at Sunday services.\u201d for music (Agostinelli et al., 2023).\nSimilar to speech generation, the recent progress can be largely attributed to the advancement in generative\nmodels for continuous data (Ho et al., 2020; Huang et al., 2023a; Liu et al., 2023b) and audio tokenizers (Zeghi-\ndour et al., 2022; D\u00e9fossez et al., 2022; Kreuk et al., 2022; Copet et al., 2023; Agostinelli et al., 2023), which\nenables modeling methods capable of capturing highly stochastic conditional distributions of audio given\ndescriptions for general domain sound/music data.\nA key limitation of these models is the ability to control transcript and generate intelligible speech or vocals.\nThese models only take a description as input, which does not specify the transcript when speech is presented.\nHence, generating samples with prompts like \u201ca person speaking\u201d often results in speech-like mumbling sound\nwith unintelligible content (Liu et al., 2023b). In other words, these models does not offer an input for users to\ncontrol transcript, and have not learned language models that allow it to construct and synthesize meaningful\nsentences given only the description.\nUnified model for audio generation: With the great progress made in developing general-domain models\nfor each audio modality, researchers also start exploring unified model that can generate audio beyond a single\nmodality and perform multiple generative tasks. Such a model could potentially learn from different sources\nof supervision and benefit from knowledge transfer across tasks. There are three concurrent studies that are\nrelated to this work.\nUniAudio (Yang et al., 2023b) focuses on building a single model that can perform multiple tasks, including\ntext-to-music, text-to-sound, and in-context TTS and natural language style prompted TTS. It follows\nthe VALL-E (Wang et al., 2023a) framework, which tokenizes audio and serializes conditioning input and\noutput audio tokens for training a conditional token-based language model. It is trained on the same speech\ndescriptions collected by PromptTTS, which inherits the same limitations in terms what attributes and how\ngranular they can be controlled through natural language prompts as discussed earlier.\nVoiceLDM (Lee et al., 2023) is the most related work. It introduces a transcript input to AudioLDM (Liu\net al., 2023b) and controls style through text description embedded with a frozen Contrastive Language-Audio\nPre-training (CLAP) model (Wu et al., 2023). During training, CLAP embedding from audio is used for\nconditioning. VoiceLDM is trained on datasets with rich acoustic variation, and hence is capable of generating\nspeech in diverse acoustic environments. However, the performance in terms of controllability is bounded by\nthe pre-trained CLAP model. Since the CLAP model are trained on audio-caption pairs focus on sound events,\nthe embedding only encodes very coarse information regarding speech attributes. Furthermore, VoiceLDM\nalso follows the sound generation paradigm which always generate audio clips of a fixed size (10 seconds),\nwhich is not ideal for speech generation that have variable length in general. Finally, despite that the model\ncan generate non-speech sounds when conditioned on empty transcripts, the performance of sound generation\nlags behind state-of-the-art models by a large margin.\nAudioLDM 2 (Liu et al., 2023c) presents a two-stage model that is applicable to speech, sound, and music\ngeneration. It is comprised of a deterministic auto-regressive model that maps conditioning input (e.g.,\nCLAP-embedded audio, description, transcript, image) to semantic features sequence, and a diffusion model\nwhich mapping semantic to acoustic features. The structure is similar to SPEAR-TTS (Kharitonov et al.,\n2023) but with different modeling methods and representations for each stage. Hence, similarly it can leverage\nunlabeled audio for training the second stage model. While AudioLDM 2 presents a unified framework,\nempirically separate models for speech and sound/music generation are trained, as the authors noted that\ndifferent model architecture hyperparameters are required for different modalities.\n5\n3\nBackground\nThis work is heavily built upon the training objective and model architecture of Voicebox (Le et al., 2023),\nand the self-supervised objective of SpeechFlow (Liu et al., 2023a). Both studies adopt conditional flow-\nmatching (Lipman et al., 2023) as the modeling backbone, which is a powerful non-autoregressive generative\nmodel for continuous data. We provide a technical overview here.\nConditional flow-matching: Conditional flow-matching (FM) (Lipman et al., 2023) is a novel generative\nmodeling method derived from the continuous normalizing flow (Chen et al., 2018) framework. It models\nthe paths that transform samples from a simple prior distribution p0 to the corresponding samples from\nthe complex data distribution p1 in a continuous manner. We use flow step t to describe the progress of\ntransformation, where the prior is at t = 0 and the data is at t = 1.\nThe training objective of FM resembles the objective diffusion models (Ho et al., 2020): during training,\ngiven a sample x1 drawn from the data distribution, a random flow step t \u223c U[0, 1] is sampled, and a noisy\nversion of the data xt as well as its derivative vt = dxt/dt for the chosen condition path are computed. A FM\nmodel u is trained to predict the derivative vt given t and xt. During inference, to draw a sample x1 from\nthe learned data distribution, a sample x0 is first drawn from the prior distribution, and then the ordinary\ndifferential equation (ODE) solver is used to estimate x1 given x0 and the derivative parameterized by the FM\nmodel through integration. Trade-off between accuracy of x1 estimation and speed can be flexibly selected by\nconfiguring the ODE solver.\nAt a high level, FM subsumes diffusion models, which correspond to specific paths of the transformation. The\nauthors of Lipman et al. (2023) presented an alternative called optimal transport (OT), which are conditional\npaths with constant directions and speeds. It is arguably easier to learn and can be more accurately estimated\nby the ODE solver with fewer steps. The OT path results in better training and inference efficiency as\nempirically verified in Lipman et al. (2023) and Le et al. (2023).\nGiven a sample x1 and a flow-step t, with the OT conditional path we have xt = (1 \u2212 (1 \u2212 \u03c3min)t)x0 + tx1\nand vt = x1 \u2212 (1 \u2212 \u03c3min)x0, where x0 is drawn from the prior distribution N(0, I) and \u03c3min is a small value\n(10\u22125). The FM model u minimizes:\nEt,x1,x0||u(xt, t) \u2212 vt||2.\n(1)\nVoicebox: Voicebox (Le et al., 2023) is a conditional generative model based on FM which additionally\nconditions on frame-aligned phonetic transcript and masked audio for audio prediction, and conditions on\nphonetic transcript and masked duration sequence for phone duration prediction. Audio is represented as\n80-dimensional Mel spectrograms and are converted to waveform using a HiFi-GAN vocoder (Kong et al.,\n2020). Duration sequence denotes the number of frames for each phoneme in the transcript.\nVoicebox adopts the Transformer (Vaswani et al., 2017) model with U-Net (Ronneberger et al., 2015) connec-\ntions. Masked spectrogram (or masked duration), frame-aligned phone embeddings (or phone embeddings),\nand noisy audio xt (or noisy duration) are concatenated along the channel dimension and projected to the\nTransformer feature dimension. The flow step sinusoidal embedding is then concatenated with the project\nfeatures along the time dimension, passed as input to the Transformer model. The Transformer output is\nthen projected to 80 dimensions (or 1 dimension for duration) and predicts the derivative vt.\nIt is a supervised model trained on 60K hours of audiobooks and achieves state-of-the-art performance on\nin-context text-to-speech synthesis that can mimic the audio style given a three second audio prompt. It is\nalso high versatile due to the generality of transcript-guided infilling, where the model can perform transient\nnoise removal, diverse style generation, speech editing, cross-lingual style transfer by simply forming transcript\nand audio inputs differently.\nSpeechFlow: SpeechFlow (Liu et al., 2023a) is a self-supervised framework based on FM with learns to\ninfill speech given the audio context. This is equivalent to Voicebox without conditioning on transcripts. The\nself-supervised objective tackles label scarcity issues and enables the model to learn from large quantities of\nunlabeled speech the distribution of speech as well as the correlation between temporal segments within an\nutterance.\n6\nFine-tuning SpeechFlow with the same transcript-guided infilling objective as Voicebox shows superior\nperformance and sample efficiency, matching style similarity of VALL-E (Wang et al., 2023a) with only 10\nhours of labeled data. The pre-trained model also demonstrates promising improvements on other speech\ngeneration tasks, including source separation and speech enhancement. It also enables parameter efficient\nfine-tuning like LoRA (Hu et al., 2021) and fine-tuning with a much lower batch size, demonstrating the\nefficiency and reusability of self-supervised pre-train models.\n4\nAudiobox SSL: Self-supervised Generative Audio Pre-training\nOur first step is to develop Audiobox SSL, a foundation model that can be fine-tuned for any downstream\naudio generation tasks. Because labeled data are not always available or of high quality, and data scaling is\nthe key to generalization, our strategy is to train this foundation model using audio without any supervision,\nsuch as transcripts, captions, or attribute labels, which can be found in larger quantities.\n4.1\nMethod\nWe adapt Audiobox SSL from SpeechFlow, which was originally designed for generative speech pre-training.\nThe same learning objective is also meaningful for general audio: through learning to infill, the model can also\ncapture the temporal relationship of audio events (e.g., clock ticking sound at fixed time interval, approaching\ntrain producing sounds with increasing volume), and learns the distribution of general audio. Therefore,\nduring supervised fine-tuning, a model does not need to learn what a natural audio sample sounds like, but\nonly needs to learn aligning the label with the corresponding mode of distribution.\nThe original SpeechFlow model is trained to predict spectrograms and uses a HiFi-GAN model to generate\nwaveform given spectrogram. However, HiFi-GAN does not generalize well to non-speech audio such as\nsound or music (Lee et al., 2022). To tackle that, we train the model to predict latent features learned by an\nautoencoder. In particular, we use the dense Encodec (D\u00e9fossez et al., 2022) features which are extracted prior\nto the residual quantization layer, which demonstrates good resynthesis quality in various audio modalities\nand has been adopted for sound and music generation (Kreuk et al., 2022; Copet et al., 2023). This is similar\nto the latent diffusion framework (Rombach et al., 2022) that is also adopted in NaturalSpeech2 (Shen et al.,\n2023).\nDuring training, the model is conditioned on fully masked features with probability pcond. With probability\n1 \u2212 pcond, a subset (nmask) of frames are masked with minimum span length lmask. The FM loss is computed\nonly on masked frames. When a frame is masked, its features are set to 0.\n4.2\nExperimental Setup\nTraining data: We collect an large scale audio dataset that greatly increases the domain coverage, modality\ncoverage, and quantities compared to previous large scale audio generative model studies (Yang et al., 2023b;\nBorsos et al., 2023; Wang et al., 2023a; Liu et al., 2023c), which leverage datasets ranging between 10K to\n100K hours containing mostly speech from a single domain (e.g., audiobooks).\nSpecifically, our dataset includes over 160K hours of speech (primarily English), 20K hours of music and 6K\nhours of sound samples. The speech portion covers audiobooks, podcasts, read sentences, talks, conversations,\nand in-the-wild recordings including various acoustic conditions and non-verbal voices. To ensure fairness and\na good representation for people from various groups, it includes speakers from over 150 countries speaking\nover 200 different primary languages. We refer to this set as \u201cMix-185K.\u201d\nModel and training: We train a 24 layer Transformer Vaswani et al. (2017) with convolutional position\nembeddings Baevski et al. (2020) and symmetric bi-directional ALiBi self-attention bias Press et al. (2021).\nThe model has 16 attention heads, 1024/4096 embedding/feed-forward network (FFN) dimension, and 330M\nparameters. We add UNet-style skip connections, where states are concatenated channel-wise and then\ncombined using a linear layer.\nThe model is trained for 1 million updates with an effective batch size of 480K frames. For efficiency, samples\nare randomly chunked if they exceed 1,600 frames. We set pcond = 0.1, nmask \u223c U[70%, 100%], and lmask = 10.\n7\nWe use the Adam Kingma and Ba (2014) optimizer with learning rate 1e-4, linearly warmed up for 5k steps\nand linearly decayed over the rest of training. For stability, we use gradient norm clipping with a norm\nthreshold of 0.2.\n5\nAudiobox Speech: Scaling In-context Text-to-speech Synthesis\nIn this section, we study the effectiveness of pre-training and fine-tuning data scaling for speech generation.\nWe present Audiobox Speech, which fine-tunes Audiobox SSL with the same transcript-guided speech\ninfilling objective as Voicebox using transcribed speech. The resulting model can be applied to multiple\ndownstream tasks just like Voicebox.\n5.1\nMethod\nTo incorporate the frame-aligned transcript z, we follow Liu et al. (2023a). Specifically, given the noisy Encodec\nfeatures xt at the flow-step t, masked Encodec features xctx, we first concatenate xt and xctx channel-wise\nand apply a linear project to get xh. We then apply another linear layer to the frame-aligned transcript\nembeddings zemb, and add this to the hidden state xh. The resulting features are concatenated with the flow\nstep sinusoidal embedding along the time dimension and fed to the Transformer as input. The Transformer\noutput is projected and predicts the derivative vt.\nThere are two different approaches to fine-tuning the model. The first one is low-rank adaptation (LoRA) Hu\net al. (2021), where we add LoRA adapters to the linear input projection of each self-attention layer. With this\napproach, only the transcript embedding, projection parameters, along with the LoRA adapter parameters\nare optimized. The second approach is full fine-tuning, where all parameters are optimized together. Liu et al.\n(2023a) showed that LoRA achieves better performance when fine-tuning SpeechFlow on 960 hours of speech,\nbut we suspect that full fine-tuning may prevail when we scale fine-tuning data.\nIn addition, many prior studies (Le et al., 2023; Wang et al., 2023a) represent transcripts as phoneme sequences\nand using the off-the-shelf Montreal Forced Aligner (McAuliffe et al., 2017) for aligning the training data.\nInstead, we represent transcript with raw characters, including punctuation and with true cases, and utilize\nthe SeamlessM4T v2 multilingual char-to-unit forced aligner presented in Seamless Communication (2023)\nadapted from RAD-TTS (Shih et al., 2021). This aligner is trained on large quantities of multilingual data\nand can align raw text with speech. There are several benefits with the replacement. First, it circumvents the\nneed of phonemizers and avoids error propagation due to incorrect phonemization. Second, raw text preserves\nmore information than phonemized text, such as casing (e.g., all caps for emphasis) and punctuation. Third,\nthe SeamlessM4T v2 aligner is much more robust than MFA and can handle multilingual/code-switching\ntext, which enables easier extension to multilingual TTS systems and is more suitable for aligning challenging\nspeech such as conversational and noisy samples.\nFollowing Le et al. (2023), we train a flow-matching duration model only with labeled data. It was shown\nin Le et al. (2023) that FM duration model has better diversity compared to regression duration models.\nHowever, it is less stable and sometimes produces unnatural prosody. To alleviate the issue, we propose to\naverage over a small number of duration sequences for stabilization, which empirically shows better trade-off\nbetween diversity and quality. The averaging operation is reasonable as duration distributions are relatively\nunimodal. When averaging more samples, it approaches the mean, which is the estimation produced by\nregression models.\n5.2\nTask and Evaluation\nWe consider the in-context TTS (also known as zero-shot TTS) task. In-context TTS aims to synthesize\nspeech that resembles the audio style of the given an audio example which may be unseen during training.\nThe audio style refers to not only voice, but everything other than transcript, such as prosody and acoustic\ncondition. To perform the task, input raw/frame-level transcript is the concatenation of the raw/frame-level\ntranscript of the audio example and the target raw/frame-level transcript, while the masked audio/duration is\nthe concatenation of the example audio/duration and a mask for the speech/duration to be generated. We\n8\nfirst sample duration sequence for the target raw transcript to create frame-level target transcript using the\nduration model, and then sample audio with the audio model.\nThe performance is measured in terms of style similarity, content correctness, and quality. A proxy automatic\nmetric for style similarity is the cosine similarity between the audio prompt and the generated audio in some\nembedding space that reflects the audio style. WavLM-TDCNN (Chen et al., 2022b) is commonly used for\nembedding (Wang et al., 2023a; Kharitonov et al., 2023; Le et al., 2023). Le et al. (2023) advocates for\nreporting both similarity with respect to raw audio (SIM-orig) and to audio resynthesized from the same\nvocoder (SIM-resyn) for comparability across studies (SIM-orig). Content correctness can be approximated\nwith the word error rate (WER) from some speech recognition model; however, WER can result from both\nsynthesis error and recognition error, and hence is less reliable when numbers are close or when the target\nstyle is more difficult to recognize (e.g., accented speech, conversational speech, noisy speech). In this paper\nwe use Whisper large-v2 instead of HuBERT-L Hsu et al. (2021) used in prior studies (Wang et al., 2023a; Le\net al., 2023) because the latter is less robust and has higher WER on real data for non audiobook domains.\nSubjective evaluations are often used for assessing style similarity and audio quality, measured by mean\nopinion scores (MOS).\n5.3\nExperimental Setup\nTraining data: We train Audiobox Speech on a transcribed English subset of the speech data used for\npre-training. The subset contains 100K hours of speech covering similar domains as the full set, which we\nrefer to as \u201cSP-multi-100K.\u201d We create the transcribed subset with the following pre-processing methods:\nFor unsegmented multi-speaker conversational datasets information, we first segment our dataset using\nPyAnnote diarization toolkit (Plaquet and Bredin, 2023; Bredin, 2023) to create single speaker speech\nsegments. For untranscribed speech, we transcribe data using two speech recognition models, Whisper Radford\net al. (2022) large-v2 and medium.en. For each audio with unknown language, we additional use the Whisper\nlarge-v2 model for language identification (LID). We then remove the utterances where the probability being\nEnglish is lower than 50% or the the word error rate (WER) between the transcriptions from the two models\nis greater than 50%.\nTo create a similar text distributions across multiple datasets, we apply inverse text normalization to create\ntrue-cased and punctuated transcript for any dataset with normalized transcript using Whisper-punctuation\nlibrary.1 It performs the task through constrained search where the produced transcript needs to match the\noriginal transcript after normalization.\nModel and training: We adopt the full fine-tuning method and train the audio model for 200K steps with\nan effective batch size of 240K frames. Samples are randomly chunked if they exceed 1,600 frames. Character\nembeddings are 128 dimensions. For each batch, audio is entire masked with probability 0.3; otherwise a\ncontiguous chunk is masked where the chunk size 70% to 100% of the frames. The same optimizer, learning\nrate, scheduler, and gradient clipping as Audiobox SSL are used.\nThe duration model has 8 heads, 768/2048 embedding/FFN dimensions, 10 layers, with 40 dimension character\nembeddings. It is trained for 600K updates with an effective batch size of 120K frames. For each batch,\nduration is entirely masked with probability 0.2 and otherwise a chunk of 10% to 100% of the sequence length\nis masked. The rest of the optimization parameters are the same as the audio model.\nEvaluation data and configuration: For in-context TTS, three second prompts are used following Wang\net al. (2023a). Voicebox uses the last three seconds of the reference as the prompt, which often contains a\nconsiderable amount of trailing silence. We instead use the last three seconds after removing the trailing\nsilences based on the forced alignment for all experiments in this paper. Duration is estimated by averaging\nover five samples and following (Le et al., 2023) predicted silence at both ends are trimmed to 0.1 second max.\nThe torchdiffeq (Chen, 2018) package is used. By default, we use the midpoint solver with a step size of\n0.0625, which invokes the derivatives being evaluated 32 times. When using classifier free guidance the model\ndoes 2 forward passes per evaluation, leading to a total of 64 calls to the model. A guidance weight for\nclassifier-free guidance (Ho and Salimans, 2022) of 0.7 is applied.\n1https://github.com/jumon/whisper-punctuator\n9\nModels are evaluated on five datasets representing different domains. (1) Librispeech test-clean (LS) (Panayotov\net al., 2015): audiobook recordings that are scripted and relatively clean. Following Wang et al. (2023a), we\nkeep only samples between 4 to 10 seconds for evaluation to compare with prior studies. (2) CommonVoice v13.0\nEnglish test set (CV) (Ardila et al., 2019): sentences read by volunteers worldwide. It covers broader accents\nand are noisier compared to Librispeech. (3) Switchboard (SWBD) (Godfrey et al., 1992): a conversational\nspeech corpus. We evaluate on a subset of 611 samples from 8 speakers. (4) Expresso (Nguyen et al., 2023)\n(Expr) is a multispeaker expressive speech dataset covering 7 different speaking styles, which we evaluate\non a subset of 999 samples. (5) An internal expressive and accented dataset (Accent): read sentences with\nspeakers covering a wider range of accents and 10 emotions. We create a subset of 500 samples for evaluation.\n5.4\nMain Results\nWe compare Audiobox Speech with several state-of-the-art in-context speech generation models. Voicebox,\nVALL-E, NaturalSpeech 2 (NS2), and YourTTS are trained on 60K, 60K, 44K, 600 hours of audiobooks\nrespectively. UniAudio is trained on about 100K hours of audio, where speech accounts for 81K hours and are\nmostly audiobooks. Results are shown in Tables 1 and 2.\nAudiobox Speech achieves a new best on style similarity (0.745 vs. 0.710 from UniAudio) on the audiobook\ndomain test set (LS). More importantly, Audiobox Speech drastically improves Voicebox on all other\ndomains, with similarity improvement ranging from 0.096 to 0.156. The results suggest that Audiobox\nSpeech generalizes much better thanks to scaling data to cover more domains. The subjective evaluations\npresented in Table 2 again confirms that Audiobox Speech transfers styles significantly better than the\nbaselines, and generate audio with better quality.\nTable 1 In-context TTS style similarity and content correctness. We cite Yang et al. (2023b) for the NS2 results\nwhich are not in the original paper(Shen et al., 2023). WER with \u2217 are computed using HuBERT-L ASR that is not\ncomparable with the other numbers.\nSim-r \u2191\nSim-o \u2191\nWord error rate (%) \u2193\nLS\nLS\nCV\nSWBD\nExpr\nAccent\nAvg\nLS\nCV\nSWBD\nExpr\nAccent\nAvg\nVALL-E\n0.580\n-\n-\n-\n-\n-\n-\n5.9\u2217\n-\n-\n-\n-\n-\nNS2\n0.620\n-\n-\n-\n-\n-\n-\n2.3\u2217\n-\n-\n-\n-\n-\nUniAudio\n0.710\n-\n-\n-\n-\n-\n-\n2.0\u2217\n-\n-\n-\n-\n-\nYourTTS\n-\n0.455\n0.312\n0.291\n0.290\n0.366\n0.343\n6.8\n10.4\n11.8\n9.5\n4.0\n8.5\nVoicebox\n0.696\n0.674\n0.477\n0.452\n0.487\n0.563\n0.531\n2.6\n7.9\n10.6\n7.2\n2.1\n6.1\nAudiobox Speech\n0.745\n0.734\n0.607\n0.608\n0.603\n0.659\n0.642\n3.2\n3.7\n9.1\n3.2\n0.9\n4.0\nTable 2 In-context TTS style similarity and quality subjective evaluation\nStyle similarity MOS \u2191\nLS\nCV\nSWBD\nExpr\nAccent\nYourTTS\n1.67 \u00b1\n0.09\n1.61 \u00b1\n0.09\n1.55 \u00b1\n0.08\n1.41 \u00b1\n0.07\n1.46 \u00b1\n0.07\nVoicebox\n2.85 \u00b1\n0.12\n2.66 \u00b1\n0.13\n2.89 \u00b1\n0.13\n2.42 \u00b1\n0.13\n2.51 \u00b1\n0.11\nAudiobox Speech\n3.88 \u00b1 0.11\n3.77 \u00b1 0.11\n3.63 \u00b1 0.12\n3.85 \u00b1 0.11\n3.77 \u00b1 0.11\nQuality MOS \u2191\nLS\nCV\nSWBD\nExpr\nAccent\nYourTTS\n1.89 \u00b1\n0.10\n2.19 \u00b1\n0.12\n1.57 \u00b1\n0.08\n1.74 \u00b1\n0.09\n1.92 \u00b1\n0.10\nVoicebox\n3.70 \u00b1\n0.11\n3.06 \u00b1\n0.12\n2.94 \u00b1\n0.12\n2.76 \u00b1\n0.12\n3.38 \u00b1\n0.12\nAudiobox Speech\n4.11 \u00b1 0.08\n4.00 \u00b1 0.09\n3.74 \u00b1 0.09\n4.00 \u00b1 0.09\n4.22 \u00b1 0.07\n5.5\nAblation Study\nWe present ablation studies in Table 3. To understand the effect of data scaling, we create a subset containing\n60K hours of audiobook speech referred to as \u201cSP-book-60K\u201d, which is a subset of the 100K hour multi-domain\nspeech we have (SP-multi-100K).\n10\nWe first compare the top two rows, which differ in the pre-training data and are both fine-tuned with LoRA.\nResults suggest that while WER remains similar, scaling pre-training data greatly improves style similarity,\nespecially on domains not covered in the fine-tuning data (CV, SWBD, Expr, Accent). On the other hand,\nscaling fine-tuning data from SP-book-60K to SP-multi-100K does not appear to improve much on similarity.\nThis potentially results from the fact that pre-training data is a superset of fine-tuning data, and hence\nfine-tuning has little to learn on style transfer and focuses on aligning transcript with speech.\nComparing the third and the fourth row, we see that by fine-tuning the whole model, style similarity improves\nslightly and WER improves greatly on most of the domains (23% to 43% relative WER reduction). The\nonly exception is on SWBD, which are 8kHz narrowband recordings that are likely less represented in the\nfine-tuning data. Finally, we compare the last two rows and confirm that using audio prompts without silence\nleads to drastic improvements on similarity on datasets which tend to have long trailing silences (CV, Accent),\nwhile overall maintaining the WER. This is because the silence is not informative for inferring the target style.\nTable 3 Ablation study for in-context TTS. PT and FT data denote the data used for pre-training and fine-tuning\nrepsectively. FT method denotes whether LoRA or full fine-tuning (full) is adopted. \u201chas sil\u201d denote whether the\nconditioned audio prompt contains silence.\nSim-o \u2191\nPT data\nFT data\nFT method\nhas sil\nLS\nCV\nSWBD\nExpr\nAccent\nSP-book-60K\nSP-book-60K\nLoRA\nY\n0.708\n0.461\n0.530\n0.552\n0.529\nMix-185K\nSP-book-60K\nLoRA\nY\n0.718\n0.505\n0.592\n0.571\n0.584\nMix-185K\nSP-multi-100K\nLoRA\nY\n0.714\n0.502\n0.583\n0.559\n0.590\nMix-185K\nSP-multi-100K\nfull\nY\n0.720\n0.508\n0.556\n0.603\n0.596\nMix-185K\nSP-multi-100K\nfull\nN\n0.734\n0.607\n0.608\n0.603\n0.659\nWER (%) \u2193\nPT data\nFT data\nFT method\nhas sil\nLS\nCV\nSWBD\nExpr\nAccent\nSP-book-60K\nSP-book-60K\nLoRA\nY\n4.4\n4.4\n8.7\n4.2\n1.5\nMix-185K\nSP-book-60K\nLoRA\nY\n3.8\n4.7\n8.9\n3.9\n1.4\nMix-185K\nSP-multi-100K\nLoRA\nY\n3.8\n6.0\n9.0\n4.0\n1.4\nMix-185K\nSP-multi-100K\nfull\nY\n2.5\n3.6\n10.1\n3.1\n0.8\nMix-185K\nSP-multi-100K\nfull\nN\n3.2\n3.7\n9.1\n3.2\n0.9\n6\nAudiobox Sound: Simple Text-to-sound Generation and Infilling\nIn this section, we present Audiobox Sound, a model for text-guided generation of general sound. The task\nis also referred to as text-to-audio generation (TTA) in many prior works(Liu et al., 2023b; Huang et al.,\n2023b; Kreuk et al., 2022). It aims to generate general audios given a holistic text description. In contrast\nto text-to-speech synthesis, the text cannot be frame-wise aligned to audio. Furthermore, sound data only\nconstitutes a small portion of the whole training data. Thus we investigate whether general audio pre-training\nis able to bring gains to generation of audios of specific domain, which we take sound generation as an example.\nWhile we focus on generation of sound events, the technique can similarly apply to other areas (e.g., music).\nMost prior works Liu et al. (2023b); Ghosal et al. (2023); Liu et al. (2023c); Huang et al. (2023b); Yang et al.\n(2023c) build the diffusion models upon a constrained latent space, commonly learned through autoencoding.\nSuch strategy has shown to improve the data efficiency Rombach et al. (2021). In this work, we adopt a different\napproach, which directly builds the flow matching network on auto-encoding based latent representation of\nraw waveforms. Such methodology has been largely explored in the language model space Kreuk et al. (2022);\nCopet et al. (2023); Agostinelli et al. (2023), which typically requires to build a billion-scale model to achieve\ncomparable performance to the alternatives aforementioned. Here we show that by leveraging such simple\nstrategy the flow matching models can achieve SOTA performance while being highly efficient (e.g., > 2x\nsmaller than Kreuk et al. (2022)).\n11\n6.1\nMethod\nSimilar to speech generation, we model the text-conditional sound distribution with flow matching. In contrast\nto learning phoneme encoding from scratch, we employ a pre-trained text encoder to map audio captions into\nword embeddings. Due to the lack of alignment between audio and text embedding, a cross-attention layer\nis applied in each transformer layer to allow the model attend to the whole text sequence in modeling the\ngradient distribution, similar to Ghosal et al. (2023); Liu et al. (2023b,c); Kreuk et al. (2022).\nDifferent from prior works in TTA such as AudioLDM (Liu et al., 2023b), AudioLDM2 (Liu et al., 2023c),\nTango (Ghosal et al., 2023), we do not rely on an off-the-shelf variational auto-encoder (Kingma and Welling,\n2014) to map the low-level audio representation (mel spectrogram) into a latent space and model the\ndistribution in the original embedding space directly. This streamlines the model architecture and reduces the\nnecessity of introducing excessive trainable parameters during fine-tuning, thus bridging the gap between\npre-training and fine-tuning.\nExcept for the cross-attention layers, all the remaining parameters are initialized based on the pre-trained\nmodel introduced in Section 4. Similar to text-to-speech synthesis, parameter-efficient fine-tuning strategy\nlike LoRA Hu et al. (2021) can be applied in text-to-audio generation. In practice, we observed fine-tuning\nthe whole model leads to significantly better performance and thus choose to fine-tune the whole model by\ndefault (see Section 6.5).\nMulti-stage fine-tuning: Compared to transcripts for text-to-speech synthesis, high-quality audio captioning\ndata are much more scarce. Typically, public audio captioning datasets include fewer than 1000 hours of\naudios, which is orders of magnitude smaller than the speech datasets. On the other hand, the larger-scale\nsound data often contain noisy category labels and has distributional shift in the audio category (Kim et al.,\n2019). To mitigate this issue, we divide the fine-tuning process into two stages, which is based on low-quality\n(e.g., tags) and high-quality (e.g., human written captions) audio descriptions respectively. Weights of the\nfirst model are used to initialize the subsequent model. We argue the labeled data used in first stage, despite\nits noisy nature, is helpful for learning the text conditional distribution (see Section 6.5).\n6.2\nTasks and Evaluation\nWe consider the following two sound generation tasks: text-to-sound (TTA) generation and text-guided\naudio infilling (TAI). We use AudioCaps test set (Kim et al., 2019), a standard benchmark for sound\ngeneration (Kreuk et al., 2022; Liu et al., 2023b,c; Yang et al., 2023b; Lee et al., 2023; Ghosal et al., 2023),\nto evaluate all models. For TTA, the model is evaluated standard Frechet Audio Distance (FAD) (Kilgour\net al., 2019), Frechet Distance (FD) and KL divergence (KLD) based on the pre-trained audio event tagger\nPANN (Kong et al., 2019), and Inception score (IS) (Salimans et al., 2016). FAD and FD measure distribution-\nlevel similarity between reference samples and generated samples. KLD is an instance level metric computing\nthe divergence of the acoustic event posterior between the reference and the generated sample for a given\ndescription. IS measures specificity and coverage for a set of samples without requiring references, which\nassigns a higher score if instance posteriors have low entropy and marginal posterior has high entropy. The\nmetrics are implemented following the audioldm_eval toolkit.2. In addition, we calculate the similarity\nbetween generated audio and text description using the CLAP model Wu et al. (2023) 3.\nIn TAI, the model is conditioned on p% of the ground-truth audio as context to infill the remaining (100\u2212p)%,\nin addition to the text description of the whole audio. In particular, p is set to be 30 and the middle 70% are\nthe region to fill in. In addition to the metrics for TTA, we further measure the similarity to the reference\naudio (CLAP-aa), which is the cosine similarity between CLAP embeddings of the generated and reference\naudio.\nIn addition to the objective metrics aforementioned, we also conduct subjective evaluation to evaluate two\nmain aspects of the generated audio: overall naturalness (OVL) and relevance to text input (REL), similar\nto Kreuk et al. (2022); Liu et al. (2023b). For these two metrics, raters were asked to rate the perceptual\nquality and the match between audio and text of the audio samples in a range between 1 and 5 similar to\nMOS. Based on the evaluation protocol Kreuk et al. (2022), the subjective evaluation is done on 100 randomly\n2https://github.com/haoheliu/audioldm_eval\n3We use the 630k-best checkpoint of https://github.com/LAION-AI/CLAP\n12\nsampled files from AudioCaps test set. Each sample is evaluated by 5 annotators from professional annotation\nservice. We list the annotation interface in Appendix D.\n6.3\nExperimental Setup\nData: To train Audiobox Sound, we use about 6K hours of audio data, among which \u223c 150 hours are\ncaptioned audios (SD-cap-150) and the remaining ones only consist of audio tags (SD-tag-6K). During the\nfirst-stage fine-tuning, the whole dataset is used while only the captioning data are used in the second stage.\nTo tackle the ontology of audio tags, we concatenate the tags of different levels as the pseudo-caption of the\naudio. See Table 4 for example audio description in these two sources.\nTable 4 Examples of audio descriptions in tag-based and caption-based datasets (Note: the two columns of each row\nare unaligned.)\nTag-based description\nCaption-based description\nAnimal\nA woman talks nearby as water pours\nDrill\nMultiple clanging and clanking sounds\nFill, Liquid\nThe sizzling of food while a dish is clanking\nBell, Hall, Room, Inside, Large\na motorboat cruises along, and a man talks\nWolves, Domestic, Animal, Canidae, Dogs, Pets\nBark, Bow-wow, Animals, Growling\nThe wind is blowing, insects are\nsinging, and rustling occurs\nImplementation Details: We use T5-base (Raffel et al., 2020) to map the text description into embeddings.\nEach cross-attention layer has 16 heads and its implementation remains same as the self-attention layers\nexcept that keys and values are text embeddings. The time-step embedding is added to the T5 embedding\nbefore being attended to. In the first stage, we fine-tune the model for 200K updates with an effective batch\nsize of 720K frames. During the second stage, we further fine-tune the model for 100K updates with an\neffective batch size 240K frames. For both stages, the learning rate and gradient clipping are set to 0.0002\nand 0.2 respectively. For inference, we use dopri5 solver with absolute and relative tolerance of 10\u22125 as the\ndefault option. The classifier-free guidance weight is tuned between 0 and 5 and we found setting it to 1 leads\nto the best result. For each text prompt, we generate 32 random samples and select the one with the highest\nCLAP similarity to the text prompt. For audio infilling, the masked audio is always kept for conditioning and\nonly the text description is optionally dropped for classifier free guidance.\nBaselines: We compare Audiobox Sound against models from the faimily of AudioLDM2 Liu et al. (2023c)\nand TANGO Ghosal et al. (2023), which stand as current SOTA approaches for general audio generation Liu\net al. (2023c).\n6.4\nMain Results\nText-To-Audio: Table 5 compares our model to prior audio audio generation models in TTA. Audiobox\nSound consistently outperforms all prior works in both objective and subjective evaluation by a large margin,\nthough it is significantly more parameter efficient. It is also worth noting compared to many approaches listed\nin Table 5, the sound training data we used is also fewer. This further reveals the effect of general domain\npre-training for sound generation.\nText-To-Audio Infilling: Table 6 shows the the performance of Audiobox Sound on TAI, as well as\nits comparison to prior works. Our model outperforms prior works by a large margin as well on this task.\nCompared to TAI, we noticed a mixing result according to different metrics. Noticably, the trend on FAD and\nKLD is not consistently, as in the comparison between TTA and TAI. This can be related to the sensitivity of\nmetrics. On the other hand, the similarity between the generation and reference is greatly increased (CLAP-aa:\n0.61\u21920.77) when the context is fed into the model, which suggests the improvement of coherence to the\noriginal audio when context is employed.\nInference efficiency: In addition to quality metrics, we further show the quality-speed trade-off at inference\ntime in Figure 2. Specifically, we vary the number of inference steps, which correspond to the step size in\n13\nTable 5 Text-to-audio generation results on AudioCaps evaluation set. Baselines are evaluated based on the respective\nofficial repos. Subjective scores are computed based on 95% confidence interval.\nobjective\nsubjective\nFAD \u2193\nFD \u2193\nKLD \u2193\nIS \u2191\nCLAP \u2191\nOVL \u2191\nREL \u2191\nGround-truth\n-\n-\n-\n13.28\n0.49\n3.36\u00b1 0.18\n3.86\u00b1 0.18\nAudioLDM-L-Full Liu et al. (2023b)\n3.37\n28.76\n1.66\n8.72\n0.43\n2.48\u00b1 0.14\n3.20\u00b1 0.18\nAudioLDM 2-Full Liu et al. (2023c)\n1.76\n32.12\n1.71\n8.56\n0.43\n2.90\u00b1\n0.16\n2.98\u00b1\n0.19\nAudioLDM 2-Full-Large Liu et al. (2023c)\n1.89\n33.28\n1.60\n8.55\n0.45\n2.90\u00b1\n0.16\n3.13\u00b1 0.17\nTANGO Ghosal et al. (2023)\n1.57\n23.78\n1.37\n8.30\n0.51\n3.10\u00b1 0.14\n3.51\u00b1 0.16\nTANGO-full-FT Ghosal et al. (2023)\n2.19\n18.47\n1.20\n8.80\n0.56\n3.04\u00b1 0.13\n3.78\u00b1\n0.15\nAudiobox Sound\n0.77\n8.30\n1.15\n12.70\n0.71\n3.43\u00b1 0.15\n4.09\u00b1 0.15\nTable 6 Text-to-audio infilling results on AudioCaps evaluation set. Baselines are evaluated based on the respective\nofficial repos. Subjective scores are computed based on 95% confidence interval.\nobjective\nsubjective\nFAD \u2193\nFD \u2193\nKLD \u2193\nIS \u2191\nCLAP \u2191\nCLAP-aa \u2191\nOVL \u2191\nREL \u2191\nGround-truth\n-\n-\n-\n13.28\n0.49\n-\n3.13\u00b1 0.13\n4.21\u00b1 0.15\nAudioLDM-L-Full Liu et al. (2023b)\n2.65\n21.27\n0.84\n8.27\n0.51\n0.76\n2.58\u00b1 0.12\n3.58\u00b1 0.17\nTANGO Ghosal et al. (2023)\n1.25\n18.02\n0.78\n8.53\n0.53\n0.78\n2.75\u00b1 0.12\n3.94\u00b1 0.15\nTANGO-full-FT Ghosal et al. (2023)\n1.86\n15.00\n0.71\n8.95\n0.56\n0.78\n2.79\u00b1 0.12\n4.07\u00b1 0.14\nAudiobox Sound\n1.29\n7.19\n0.65\n12.05\n0.63\n0.77\n2.95\u00b1 0.12\n4.20\u00b1 0.12\nthe ODE solver for our model and the number of DDIM steps in TANGO and AudioLDM2. Audiobox\nSound achieves consistently higher quality (lower FAD) with the same number of inference steps compared to\nAudioLDM2 and Tango. This implies the better efficiency of the flow-matching approach Audiobox is based\non, as is similarly demonstrated in Le et al. (2023).\n6.5\nAnalysis and Ablation Study\nAblation Study: Here we conduct an ablation study showing the effect of different components of Audiobox\nSound. Specifically, we vary the following training strategies: training with SD-cap-150 only, training with\nSD-tag-6K and SD-cap-150, training with the whole speech, music and sound datasets.\nAs is shown in Table 7, using a general pre-trained model boosts the performance by \u223c 20% in FAD.\nDespite the discrepancy in task and data domain, generation of universal audios is a beneficial pretext task\nfor text-to-sound generation. As music and speech constitutes a significant portion of our evaluation set,\nincreasing the scale of these two modalities in pre-training provides additional benefits. Furthermore, the\ntwo-stage fine-tuning also consistently outperforms fine-tuning with SD-cap-150 only regardless of using a\npre-trained model or not. The gain is mostly attributed to scaling up in-domain training data (i.e., sound\nonly). Despite the labels being different, simply using audio tags can still enhance learning the mapping\nbetween the description of events and the actual audio. Finally, comparing the last two rows of Table 7\nsuggests reranking with CLAP model is an effective approach to improving the overall performance in both\nthe audio quality (FAD: 0.91 \u2192 0.78) and text-audio relatedness (CLAP score: 0.60 \u2192 0.71).\nFine-tuning strategy We compare the two different fine-tuning strategies: LoRA vs. full model fine-tuning. For\nLoRA, we add LoRA adaptors described in Section 5 to self-attention layers. In contrast to full-tuning where\nthe whole model is fine-tuned, only the adaptors and cross-attention layers will be updated during fine-tuning\nand all the remaining parts are frozen. LoRA fine-tuning is on average 15% to 30% worse (relative) than its\nfull fine-tuning counterpart. The incorporation of cross-attention layers induces large architectural change to\nthe model, which increases the necessity of fine-tuning the whole model.\n14\nFigure 2 Quality-speed trade-off of Audiobox Sound, Tango and AudioLDM2. NFE: Number of function evaluations.\nTable 7 Ablation for sound generation on AudioCaps evaluation set. Tag: audio tagging data, Cap: captioning data.\nNote the results of this table are based on the midpoint solver with a step size of 1/32 (equivalent to 64 NFE) for the\npurpose of inference speed-up.\nPT (SSL)\nFT-1\nFT-2\nw/ rerank\nFAD \u2193\nFD \u2193\nKLD \u2193\nIS \u2191\nCLAP \u2191\n\u2717\n-\nSD-cap-150\n\u2713\n1.17\n9.88\n1.17\n11.43\n0.71\n\u2717\nSD-tag-6K + SD-cap-150\n-\n\u2713\n1.61\n13.16\n1.34\n10.17\n0.67\n\u2717\nSD-tag-6K + SD-cap-150\nSD-cap-150\n\u2713\n0.97\n8.70\n1.17\n12.19\n0.71\n\u2713\n-\nSD-cap-150\n\u2713\n0.95\n8.70\n1.15\n12.21\n0.70\n\u2713\nSD-tag-6K + SD-cap-150\nSD-cap-150\n\u2717\n0.91\n8.95\n1.33\n12.41\n0.60\n\u2713\nSD-tag-6K + SD-cap-150\nSD-cap-150\n\u2713\n0.78\n8.31\n1.14\n12.62\n0.71\n7\nAudiobox: Toward Universal and Controllable Audio Generation\nIn previous sections, we discussed speech and sound generation independently. This section presents Audiobox,\na single model that can produce both speech and audio conditioned on text description or audio example.\nFine-tuning our pre-trained model for this joint task enables natural language instruction to control the output\nspeech attributes like perceived age, gender, quality on top of example-based control (ZS-TTS). Furthermore,\ntraining on wide variety of data enables simulating voices in different environments and accompanied by\nacoustic events such as birds chirping, applause. We further envision a scenario where the user would like\nto restyle the given audio example with natural language instruction. For example, change the audio style\nto make it sound like it is recorded in a cathedral. This requires disentangled vocal style control using an\nadditional utterance from the same speaker called voice prompt.\nWe design Audiobox to enable speech and sound generation capabilities previously discussed in Sections 5\nand 6. Furthermore through voice prompt and description we also envision vocal style transfer to more\ncomplex acoustic scenes enabled through joint training. Below we discuss in details speech caption and voice\nprompt modeling, data creation, and experiments.\n15\n7.1\nData Creation\n7.1.1\nSpeech Captions\nWe aim to bridge the gap between speech and sound datasets by supporting description-based control for\nspeech generation. We consider both human annotations and automatically created captions\nAutomatic captions: Given the lack of any dataset with fine-grained description for speech, we generate\nspeech captions using a large language model (LLM) with speech attribute tags extracted either using existing\nmetadata or use pseudo labels using classifiers. We extract the following attributes: (1) age: 4 classes\n(2) gender: 2 classes (3) audio quality: 3 classes (4) pitch: 3 classes (5) speaking rate: 3 classes (6) accent:\nopen-vocabulary (7) emotion: open-vocabulary (8) environment: open-vocabulary More details can be found\nin Appendix A.\nGiven the above attributes, we use the LLAMA2 7B model Touvron et al. (2023) to convert them into captions.\nTo capture different writing styles, we prompt the model a style bank mimicking different characters with\nexample writing samples. A few of them are listed below:\n\u2022 A young male adult voice, conveys anger and frustration. The audio, of normal quality, is recorded\ninside a small space. The person speaks with South Asia accent and a normal speaking pace.\n\u2022 This young bloke\u2019s ticked off, audio\u2019s all good. He\u2019s in some small space and has a South Asian accent.\nTalks normal speed.\n\u2022 Got this young dude who\u2019s mad, audio\u2019s decent. He\u2019s in a tight spot, has that South Asian accent, and\ntalks at a chill pace.\n\u2022 Young man is angry. Audio is okay, small place. Accent from South Asia. Speaks normal.\nTo further improve coverage over different environment and background sounds, for each utterance, we apply\na random augmentation by convolving with a random room impulse responses (RIR) from a set of known\nenvironments and optionally add add a background noise from a set with known tags.\nWe also generate the corresponding caption with updated environment and background noises using the\nLLAMA2 7B model. When adding any background noise to the utterance, we update the quality to \u201clow\u201d.\nFor utterances applied only RIR we update the quality to be \u201cnormal\u201d if the original quality was \u201cstudio\u201d. We\ndo not apply utterances with low audio quality since those may not be suited for RIR augmentations.\nHuman annotations: We create human-based annotation to gather more fine-grained description and better\nalignment towards human hearing perception. We select a 500 hour subset of SP-multi-100K described in\nSection 5.5.\nIn the annotation guidelines, we ask the annotator to describe the perceived attribute such as: gender, age,\naccent, emotion, environment, tonal variation, speaking pace, pitch, emotion, audio quality, vocal style and\nany miscellaneous details from the speech utterances. In addition to this we also collect categories for the\nattributes. To ensure we get high quality description, we filter annotators in two stages. First, we keep\nannotators who successfully labeled pre-selected gold samples with high accuracy. We additionally use an\nLLM to automatically rate the quality annotations to ensure high quality detailed captions to complement\nour automatic caption above. More details on quality can be found in Appendix B. Here are some captions\nexample curated by our human annotator:\n1. A young woman with an American accent speaks in a higher pitched voice. She speaks at a normal pace\nwith a bit of a muffled voice. She is outside in an urban area and cars can be heard passing by in the\nbackground. She has a happy and excited tone that is slightly melodious. The audio is of poor quality\nand dog barking can be heard at the end.\n2. A middle aged man with a mildly masculine voice seems to be outside in a rural or natural environment\nwith a moderately background noise of birds singing. He seems to be in a neutral mood when show\ncasing a house to some people. His voice is hoarse/rough speaking at a slow pace with an average voice\npitch.\n16\n7.1.2\nVoice Prompts\nNatural language description alone allows user to control styles through describing attributes such as age,\naccent, emotion, pitch, and environment. However, a user maybe interested in synthesizing a specific vocal\nstyle and while changing other attributes such as quality, emotion, background. This requires disentangled\ncontrol between the input voice sample and natural language text prompt.\nFor each target utterance, we sample an additional utterance from the same speaker to serve as voice prompt\nduring training. The voice prompt is selected such that it differs from the target utterance on one or more\nattribute such as emotion, environment, and speaking rate. This is to de-correlate the target and prompt\non everything but vocal similarity. We additionally apply a random room impulse response and background\nnoise augmentation to the voice prompt to increase robustness as well as further de-correlation.\nNote that this is different from passing the audio as audio context (zero-shot TTS) where we expect the model\nto copy over emotion, environment and other background details as well. Here we would want the model to\ntransfer only the vocal style from prompt and use the description for other details such as environment and\nemotions.\n7.2\nMethod\nAudiobox (Figure 1) conditions on both transcript and masked audio features (same as Audiobox Speech)\nand captions (same as Audiobox Sound) for description conditional generation. To unify training, for sound\ninputs without transcript, we create a pseudo-transcript that contains \u201c<sound>\u201d tokens each of length 1\nsecond filling the length of audio. We additionally condition on the another utterance from the same speaker\n(voice prompt). As described in Section 7.1.2, the voice prompt is selected in a adversarial fashion to enable\ndisentangled control. For audios with missing prompts, we feed a pseudo voice prompt of length 0.1s filled\nwith zeros. The voice prompt is embedded by a lightweight Transformer. We then concatenate the output\nwith the caption description embedding for cross-attention. We randomly initialize the parameters for the\ncross-attention, description projection, and character embedding weights. All other parameters are initialized\nbased on Audiobox SSL in Section 4. Similar to the sound model training in Section 6, we use multi-stage\nfine-tuning as described next.\nMulti-stage fine-tuning: Except for the high quality 500 hours of speech captions that we collect, the rest\nof our speech captions are generated using attribute tags and an LLM. Furthermore most of the datasets do\nnot provide any meta-data further limiting the quality of the captions. To mitigate this issue we train our\nmodel in two stages. In the first stage we use all the captions for speech and audios. To avoid under-fitting on\nthe audio events generation, we upsample the audio data such that the ratio of total speech and audio data in\nhours is about 3 : 1. In the second stage, we initialize the model from first stage weights and only train on\nthe high quality data that comprises 500 hour of annotated speech captions and a few other datasets with\nemotion and accent metadata for rich LLM captions. We again upsample the audio data such that the ratio\nof total speech and audio data is about 2.6 : 1.\n7.3\nTask and Evaluation\nIn our unified Audiobox model, the model is capable of new generation tasks such as description-guided TTS\n(transcript + description) and description-guided TTS with extra voice conditioning generation (transcript +\ndescription + voice prompt). Additionally, Audiobox also maintains generation capability from all prior\nsection including: diverse speech sampling (transcript only), zero-shot TTS (transcript + context prompt)\n(see Section 5.2), text-to-sound (TTA) generation (description only) and text-guided infilling (TAI, description\n+ context prompt) (see Section 6.2). In Appendix C, describe the tasks and inputs in detail.\nFor all speech generation tasks, we measure the WER and similarity of vocal style if context or voice prompt\nis provided. In addition, for any generation task with description conditioning, we measure the similarity\nbetween description and generated audio with cosine similarity between CLAP text and audio embedding.\nFor the description-guided TTS, in addition to objective metric, we also conduct subjective evaluation to\nassess the QMOS and REL. Below, we provide details on the CLAP model used for speech evaluation.\n17\n7.3.1\nJoint-CLAP similarity\nIn terms of tasks, generating speech conditioned on text descriptions is similar to description-guided sound\ngeneration (TTA). As is common in TTA, we also employ the text-to-audio similarity to measure how well\nthe generated audio matches the description. However, unlike TTA scenario, joint text-audio embedding\nmodels such as CLAP Wu et al. (2023) cannot be straightforwardly applied to the speech domain. Existing\nCLAP models are trained with coarse description about speech, such as \"a person speaking\". The model is\nunable to distinguish fine-grained speaking styles like accent or emotion. Although there exist public CLAP\nmodels which are trained with speech data, most of them are trained with (speech, transcript) pairs which\nis orthogonal to the text description. Thus, for the purpose of evaluating description-conditioned speech\ngenerative models, we propose Joint-CLAP model, which is designed for both description-based speech and\naudio evaluation.\nTraining Similar to CLAP Wu et al. (2023), Joint-CLAP consists of an audio and text branch, each responsible\nfor encoding audio waveforms and the natural language sentences respectively. Given a speech-text pair\n(xa, xt), the audio and text branch fa and ft encodes it into the embedding pair (ea, et): ea = fa(xa),\net = ft(xt). We use the same contrastive loss for model training following Wu et al. (2023); Radford et al.\n(2021), where \u03c4 is a learnable parameter.\nL =\n1\n2N\nN\nX\ni=1\n(log\nexp (ea\ni \u00b7 et\ni/\u03c4)\nPN\nj=1 exp (ea\ni \u00b7 et\nj/\u03c4)\n+ log\nexp (et\ni \u00b7 ea\ni /\u03c4)\nPN\nj=1 exp (et\ni \u00b7 ea\nj /\u03c4)\n)\n(2)\nIn practice, we use pre-trained RoBERTa Liu et al. (2019) as the text encoder ft. In contrast to CLAP,\nwhich uses pretrained audio taggers (e.g., HSTAT Chen et al. (2022a)) for audio encoding, here we use\nWavLM Chen et al. (2022b) as the backbone for encoding. Self-supervised speech models can better capture\ndetailed information (e.g., speaking style) than general audio classifiers. Both RoBERTa and WavLM encoders\nare fine-tuned in model training.\nData The training data of Speech-CLAP consists of SD-tag-6K, SD-cap-150, and 2K hours of speech datasets\nincluding both human and automatic captions. The training set includes both speech and non-speech data in\norder to equip the model the discriminative capabilities for speaking with environmental sound use cases (e.g.,\na man speaks as birds chirp and dogs bark). The speech portion is a subset of the captioned speech described\nin Section 7.1.1, which are selected to balance the ratio of human annotated and LLM-augmented captions.\nThe model is evaluated on the evaluation sets of the sound and speech subset respectively.\nImplementation Details For audio and text encoder, we use WavLM-base+ and RoBERTa base respectively.\nUsing alternative speech encoders within the same family such as WavLM-large brings similar results. The\naudio and text embeddings are normalized before calculating the loss (Equation (2)). The model is trained\nusing Adam optimizer Kingma and Ba (2014) with a learning rate of 5e \u2212 5. We use 64 volta32 GPUs with a\nbatch size of 75 per GPU for 200K updates. For training stability, the gradient is clipped to 10 by norm and\nraw floating point precision is used without any quantization. We track the recall (A2T@10) on the validation\nset at the end of each epoch and select the model checkpoint with the highest value.\nRetrieval Performance We compare Joint-CLAP to the original CLAPs proposed by Wu et al. (2023), measuring\nthe text-to-audio and audio-to-text retrieval performance. Specifically, we take two public CLAP models\ntrained general audios: CLAP (general audio) 4, and general audios plus speech: CLAP (w/ speech) 5 Per\nretrieval task, we report the recall under three thresholds: 1, 5 and 10. As is shown in Table 8, public CLAPs,\nregardless of whether speech data are utilized or not, achieves significantly lower performance on speech\nretrieval based text descriptions, with \u223c 30x performance degradation compared to the sound benchmark.\nThis might be due to the naturally larger ambiguity in the task, where description of speech may exhibit higher\nvariance. For instance, different people may have varying opinions on what constitutes fast speaking versus\nslow speaking. In spite of such ambiguity, Joint-CLAP still significantly improves the retrieval performance\nunder the same setting (T2A@10 on speech: 2.29 \u2192 22.01), while maintaining the performance for general\naudios (T2A@10 on sound: 63.64 \u2192 67.64). The gain is attributed to fine-tuning with speech-specific datasets\n4https://huggingface.co/lukewys/laion_clap/blob/main/630k-best.pt\n5https://huggingface.co/lukewys/laion_clap/blob/main/music_speech_audioset_epoch_15_esc_89.98.pt\n18\nand using a high-performing speech encoder. To further ablate this effect, we trained a CLAP model without\naltering the model architecture using in-domain speech data. The retrieval performance is considerably lower\nthan the WavLM-based Joint-CLAP (e.g., T2A@10 on speech: 12.01 vs. 22.01).\nTable 8 Comparison between Speech-CLAP and public CLAP models on retrieval performance in sound and speech.\nSpeech\nText\u2192Audio\nAudio\u2192Text\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nCLAP (general audio) Wu et al. (2023)\n0.36\n1.29\n2.29\n0.64\n2.26\n3.55\nCLAP (w/ speech) Wu et al. (2023)\n0.82\n2.42\n3.37\n0.51\n1.90\n2.60\nSpeech-CLAP\n7.10\n16.30\n22.01\n5.96\n16.07\n22.34\nSound\nText\u2192Audio\nAudio\u2192Text\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nCLAP (general audio) Wu et al. (2023)\n11.03\n45.33\n63.64\n9.45\n44.36\n61.70\nCLAP (w/ speech) Wu et al. (2023)\n11.15\n42.42\n60.36\n9.70\n43.15\n59.03\nSpeech-CLAP\n13.33\n51.88\n67.64\n11.27\n47.27\n64.48\nCorrelation between Joint-CLAP scores and human opionion scores In practice, we also notice the Joint-CLAP\nmodel is more closely correlated to human-perceived text-audio similarity, as opposed to the public CLAP\nmodel (see Figure 3). Specifically, we take six Audiobox models of varying performance and run subjective\nevaluation with these models on the four evaluation sets. As is shown in Figure 3, the Pearson correlation\ncoefficient between the text-audio similarity and REL score is increased from 0.028 to 0.727 with a joint CLAP\nmodel, suggesting that its text-audio similarity score is a reliable metric for evaluating description-controlled\nspeech generation.\nFigure 3 Correlation between text-audio similarity and REL score in different CLAP models. r: Pearson correlation\ncoefficient.\n7.4\nExperimental Setup\nTraining data: We train unified Audiobox with a combination of (1) English speech dataset (SP-Multi-100K,\nsee Section 5.3) with additional text description and voice prompt for each corresponding utterances and\n19\n(2) sound dataset with text description or tags (SD-TAG-6K and SD-CAP-150, see Section 6.3). In both\ncases, each description is either generated from an LLM, or annotated by humans. We employ two-stage\nfine-tuning to improve our model fidelity and quality. In the first stage fine-tuning, we incorporate all speech\n(SP-Multi-100K) and sound (SD-TAG-6K and SD-CAP-150) datasets into our training dataset. In the second\nstage fine-tuning, we use a subset of our first-stage fine-tuning dataset comprised of higher quality dataset\nwith total about 2,310 hours.\nImplementation details: Unified Audiobox model takes four different inputs: 1) frame-aligned transcript,\n2) description, 3) voice prompts, and 4) context prompt (masked audio features). First, we first embed the\ninput character sequence in frame-aligned transcript to 128 dimension features. The embedded sequence\nis then projected using a linear layer and added to the projected masked audio features as input to the\nTransformer. Next, we use T5-base to extract 512-dimension continuous embedding from the description.\nThe parameters of T5-base are kept frozen during training. We add a trainable linear layer to project the\noutput from 512-dimensions to match the Transformer embedding dimensions (1024). For the voice prompts,\nwe first extract dense features using the same Encodec model described in Section 4. These features are\nthen input to a 3-layered Transformer model with 1024 embedding dimensions, 16 attention heads, and a\nfeed-forward dimension of 4096. We then concatenate the time-step embedding, voice prompt encoder output,\nand description embedding which form the input for cross-attention.\nDuring training, we randomly drop voice prompt, captions, and context with the probabilities specified in\nTable 9:\nTable 9 Drop-out probabilities for context (ctx), voice prompt (vp), and caption (cap). \u201cF\u201d (false) / \u201cT\u201d (true) refers\nwhether the input is used.\nHyper-parameters\nP(vp=F)\np(ctx=F | vp=T)\nP(ctx=F | vp=F)\np(cap=F)\n0.5\n0.7\n0.5\n0.3\nThese probabilities are designed with specific use cases discussed previously. Note that zero-shot TTS requires\nthe model to copy each and every attribute from the audio prompt while restylization requires model to\nmaintain high similarity of vocal style while discarding emotion, environment and other attributes. This\nrequires us to distinguish the context from the voice prompt.\nSetting the dropout probabilities as defined in Table 9 lead to the joint probabilities presented in Table 10.\nThe joint probabilities correspond to each of the use case that the model can support. Note that the generative\npre-training already tunes model for ZS-TTS and diverse speech sampling applications. Therefore, we select\nthe hyper-parameters to bias the model towards description-guided TTS with and without vocal conditioning.\nTable 10 Derived joint probabilities for context, voice prompt, and caption for different use cases.\nHyper-parameters\nZS-TTS\nDescription-TTS w/ vocal\nDescription-TTS\nSampling\nP(ctx=T, vp=F, cap=F)\nP(ctx=F, vp=T, cap=T)\nP(ctx=F, vp=F, cap=T)\nP(ctx=F, vp=F, cap=F)\n0.075\n0.245\n0.175\n0.075\nIn the first stage fine-tuning, we fine-tune all parameters for a maximum of 600K updates with 32 A100-80GB\nGPUs. We stopped training after 350K steps as we didnot find any gains in model performance beyond this.\nIn the second stage, we further fine-tune our model parameter with LoRA fine-tuning on the self-attention\nparameters with r = 64 and cross attention input projection layers for 100K updates with 16 A100-80GB\nGPUs.\nFor the unified Audiobox duration model, we use both transcript and the description text as the input. We\nuse 12 Transformers decoder layer with 8 heads, 768/2048 embedding/FFN dimensions self-attention and\ncross-attention layer to attend the description embedding. We use 40 dimension for the character embedding.\n20\nDuring training, we set the description embedding drop probability 0.3. The model trained with 600K updates\nwith flow-matching loss with 8 A100-80GB GPUS. For evaluation, we use the checkpoint at 200K steps.\nEvaluation data: We measure the effectiveness of description-guided TTS and description-guided TTS with\nvocal prompts on the following test sets. First, we annotate a set of 1,946 recordings sampled from diverse\nsources, including LibriTTS (Zen et al., 2019), Common Voice (Ardila et al., 2019), Switchboard (Godfrey et al.,\n1992), Fisher (Cieri, Christopher, et al. , 2004,2005a,,), Spotify (Clifton et al., 2020), AudioSet (Gemmeke\net al., 2017), Expresso (Nguyen et al., 2023) in order to evaluate the ability to generalize. This set is denoted\nas SpCap (SC). The second set is AC-filtered (AC-filt) (Lee et al., 2023) with 825 utterances. It constructed\nfrom AudioCaps test set by transcribing and keeping samples with reliable ASR transcriptions.\nThe third one is the Expresso test set (Expr) with 999 utterances. Finally, the fourth one contains utterances\nfrom the internal Accent set. We apply randomly sampled RIR and noise augmentation to construct this set\nand denote it as \u201cAccent+\u201d (500 utterances). Expr and Accent+ use speech captions derived from LLM using\nthe available attributes. For Accent+, we additionally pass the environment and background noises tags to\nthe LLM to incorporate the information into generated captions. Together these sets cover a wide variety of\nacoustic events, emotions, accents, environments, and vocal styles.\nTo evaluate description-based TTS with vocal prompt, we use Expr and Accent+ datasets and select another\nutterance from the same speaker. The prompt is selected such that is different from the target utterance on\neither emotion or speaking style (enunciated, whisper, etc). Furthermore, we also compare against Audiobox\nSound and Audiobox Speech on speech and sound applications using the evaluation sets described in\nSections 5 and 6 respectively.\nInference: We use duration model described in this section with averaging over 5 samples. For description-\nguided TTS (with or without voice prompt), we additionally sample a silence duration of between 0 and\n3 seconds and pad it to both ends. We find this generates audios that are coherent with the description\nparticularly when they also mention acoustic events. For example: a man speaks and car passes by while a dog\nis barking. However, this can cause model to hallucinate sounds when there are no acoustic events described.\nTo cover all scenarios involving description-guided TTS, we generate N = 8 samples with stochastic silence\npadding and then output the best sample based on clap re-ranking using the joint model. We use a guidance\nweight of 0.75 for the description-guided TTS (with/without voice prompt) applications.\nFor sound only generation, we always generate 10s long audios with pseudo-transcripts using a guidance weight\nof 1.33. We use clap reranking with N = 16 samples using the sound clap model. For zero-shot in-context\nTTS applications, we trim the end-silences similar to the Audiobox Speech model and use a guidance weight\nof 1.0. Given that this application doesn\u2019t involve any descriptions, we do not use clap re-ranking. Unless\nspecified, both acoustic and duration Audiobox models use the midpoint solver with a step size of 1/32,\nwhich invokes the function being integrated 64 times. When using classifier free guidance the model does 2\nforward passes, leading to a total of 128 calls to the model forward pass.\n7.5\nMain Results\nIn this section, we investigate the effectiveness of the unified Audiobox model on a number of use cases. We\nfirst compare the description-guided TTS with and without voice prompt in Tables 11 and 12 respectively. For\nthis task, we compare with VoiceLDM Lee et al. (2023) and AudioLDM2 Liu et al. (2023c) models as baselines.\nNext, in Table 13 we evaluate how well Audiobox performs speech tasks as compared to non-description\nspeech only model, Audiobox Speech. Finally, in Table 14 we compare against the sound-only Audiobox\nSound model on the TTA task.\n7.5.1\nDescription-based control for speech generation\nTable 11 compares Audiobox with VoiceLDM Lee et al. (2023) and AudioLDM2 Liu et al. (2023c) models on\ndescription-guided TTS and description-guided TTS with voice prompt (voice restylization) tasks. We find\nthat Audiobox outperforms both baselines on all datasets and metrics. In particular, Audiobox is able\n21\nto consistently generate audios for rich descriptions in SC, background events (AC-filt), expressive audios\n(Expr), and accented audios with diverse backgrounds (Accent+).\nTable 11 Description-based control for speech generation. Audiobox outperforms both AudioLDM2 and VoiceLDM on\nall datasets and metrics. VoiceLDM and AudioLDM2 models struggle in particular of Expr and Accent+ datasets with\nexpressive audios.\nJointCLAP \u2191\nWER (%) \u2193\nSC\nAC-filt\nExpr\nAccent+\nSC\nAC-filt\nExpr\nAccent+\nground truth\n0.403\n0.479\n0.548\n0.561\n8.4\n23.5\n5.8\n13.5\nVoiceLDM\n0.245\n0.449\n0.060\n0.235\n8.0\n6.8\n5.3\n4.4\nAudioLDM2-SP\n0.241\n0.225\n0.066\n0.110\n32.5\n26.3\n33.8\n23.9\nAudiobox\n0.430\n0.489\n0.387\n0.596\n7.2\n5.2\n4.5\n2.6\nQMOS \u2191\nREL \u2191\nSC\nAC-filt\nExpr\nAccent+\nSC\nAC-filt\nExpr\nAccent+\nground truth\n3.60\u00b1 0.11\n3.25\u00b1 0.14\n4.00\u00b1 0.09\n3.24\u00b1 0.13\n3.66\u00b1 0.10\n3.86\u00b1 0.12\n4.01\u00b1 0.10\n3.51\u00b1 0.11\nVoiceLDM\n3.01\u00b1 0.10\n2.95\u00b1 0.13\n2.92\u00b1 0.12\n2.87\u00b1 0.12\n2.90\u00b1 0.10\n3.08\u00b1 0.14\n2.78\u00b1 0.11\n3.2\u00b1 0.11\nAudioLDM2-SP\n2.19\u00b1 0.11\n2.17\u00b1 0.12\n2.47\u00b1 0.11\n2.25\u00b1 0.10\n2.37\u00b1 0.11\n2.11\u00b1 0.12\n2.48\u00b1 0.11\n2.22\u00b1 0.10\nAudiobox\n3.58\u00b1 0.10\n3.38\u00b1 0.12\n3.82\u00b1 0.09\n3.54\u00b1 0.12\n3.74\u00b1 0.09\n3.61\u00b1 0.12\n3.94\u00b1 0.11\n3.61\u00b1 0.10\nWe also note that AudioLDM2 and VoiceLDM struggle in particular on expressive datasets (Expr and Accent+).\nIn particular, we find that utterances generated by AudioLDM2 and VoiceLDM models are significantly worse\nthan the ground truth especially in complicated scenarios involving description of both speech, environment\n(cathedral), and background sounds. This results in worse scores on the Accent+ dataset. Furthermore, Expr\ntest set contains voices exploring expressive styles like enunciation, whispering, non-binary gender which is\nwhere AudioLDM2 and VoiceLDM struggle. We hypothesize this could be because they are out-of-distribution\ncases w.r.t training. Both VoiceLDM and AudioLDM2 model tend to struggle on such utterances leading to\nlow scores on objective metrics.\nOur subjective evaluations also align with the objective metrics where we find the the Audiobox model\nsignificantly outperforms the baselines in particular to similarity to the description. The worse scores on\nAccent+ and Expr dataset for AudioLDM2 and VoiceLDM model further confirms our own observations.\nIn Table 12, we present the results for description-guided TTS with voice prompt. VoiceLDM and AudioLDM2\nmodel do not simultaneously support conditioning based on vocal and text descriptions for a transcript.\nTowards our best effort comparison, we combine the CLAP embedding for the audio vocal prompt and the\ntextual description by averaging them and use it as a conditioning input. We find that Audiobox outperforms\nboth baselines. We also notice that in the absence of voice-prompt, the speaker similarity of Audiobox is\ngreatly reduced as the description cannot capture all aspects of voice. The subjective evaluations aligns with\nthe objective metrics both for description and generated audio similarity and speaker similarity. We find that\nthe voice prompt greatly improves the speaker similarity while matching the descriptions.\n7.5.2\nComparison to Audiobox Speech and Audiobox Sound\nTable 13 compares the unified Audiobox and speech only Audiobox Speech models for zero-shot TTS on 5\ndifferent datasets. We use the same duration model for both acoustic models for this task. We find that the\nunified Audiobox model gives higher speaker similarity but performs marginally worse on the word error rate.\nThis is also confirmed by subjective evaluations where we find only minor differences between the Audiobox\nand Audiobox Speech models.\nIn Table 14, we present the results comparing the unified Audiobox to the Audiobox Sound, VoiceLDM,\nand AudioLDM2 models on the task of TTA task as described in Section 6.2. We find that Audiobox\nsignificantly outperforms all baselines achieving the state-of-the-art performance for joint models and even\noutperforms sound only models such as TANGO. The Audiobox performs worse only to the Audiobox\nSound model which specializes in sound generation. The subjective evaluations further confirm that both our\nAudiobox and Audiobox Sound outperform all other baselines by a significant margin.\n22\nTable 12 Description-based control with extra voice conditioning for speech generation\nComparing on objective metrics.\nModel\nVoice cond.\nJointCLAP \u2191\nSim-o \u2191\nWER \u2193\nExpr\nAccent+\nExpr\nAccent+\nExpr\nAccent+\nground truth\nn/a\n0.548\n0.561\n0.395\n0.526\n5.8\n13.5\nVoiceLDM\navg. CLAP\n0.093\n0.204\n0.115\n0.076\n4.8\n3.9\nAudioLDM2-SP\navg. CLAP\n0.067\n0.118\n0.045\n0.089\n34.6\n30.2\nAudiobox\nNo\n0.387\n0.596\n0.181\n0.141\n4.5\n2.6\nAudiobox\nYes\n0.480\n0.593\n0.377\n0.344\n7.7\n2.8\nComparing on subjective metrics for Speaker similarity, quality and description aspects\nModel\nVoice cond.\nQMOS \u2191\nREL \u2191\nSpeaker Similarity MOS \u2191\nExpr\nAccent+\nExpr\nAccent+\nExpr\nAccent+\nground truth\nn/a\n4.0\u00b1 0.09\n3.24\u00b1 0.13\n4.01\u00b1 0.1\n3.51\u00b1 0.11\n3.38\u00b1 0.11\n3.27\u00b1 0.10\nAudiobox\nNo\n3.82\u00b1 0.09\n3.54\u00b1 0.12\n3.94\u00b1 0.11\n3.61\u00b1 0.1\n3.02\u00b1 0.12\n3.03\u00b1 0.10\nAudiobox\nYes\n3.86\u00b1 0.09\n3.58\u00b1 0.12\n3.99\u00b1 0.11\n3.57\u00b1 0.11\n3.36\u00b1 0.11\n3.24\u00b1 0.11\nTable 13 Comparing Audiobox and Audiobox Speech model for In-context TTS application. Both model use the\nsame regression based duration model\nStyle similarity and content correctness using objective metrics\nSim-o \u2191\nWord error rate (%) \u2193\nLS\nCV\nSWBD\nExpr\nAccent\nAvg\nLS\nCV\nSWBD\nExpr\nAccent\nAvg\nAudiobox Speech\n0.734\n0.607\n0.608\n0.603\n0.659\n0.642\n3.2\n3.7\n9.1\n3.2\n0.9\n4.0\nAudiobox\n0.732\n0.624\n0.610\n0.643\n0.674\n0.656\n4.8\n3.0\n12.6\n2.7\n0.9\n4.8\nStyle similarity MOS subjective evaluation \u2191\nLS\nCV\nSWBD\nExpr\nAccent\nAudiobox Speech\n3.88 \u00b1 0.11\n3.77 \u00b1 0.11\n3.63 \u00b1 0.12\n3.85 \u00b1 0.11\n3.77 \u00b1 0.11\nAudiobox\n3.72 \u00b1 0.11\n4.03 \u00b1 0.11\n3.72 \u00b1 0.12\n4.01 \u00b1 0.10\n3.88 \u00b1 0.11\nQuality MOS subjective evaluation \u2191\nLS\nCV\nSWBD\nExpr\nAccent\nAudiobox Speech\n4.11 \u00b1 0.08\n4.00 \u00b1 0.09\n3.74 \u00b1 0.09\n4.00 \u00b1 0.09\n4.22 \u00b1 0.08\nAudiobox\n3.95 \u00b1 0.08\n3.97 \u00b1 0.09\n3.88 \u00b1 0.08\n3.93 \u00b1 0.09\n4.17 \u00b1 0.07\n8\nInference Optimization with Bespoke Solver\nTo generate samples from a flow-matching model, an ODE solver is used at inference time to approximate\nthe integration. There are many solvers that one can choose from, such as adaptive step-size dopri5 solver\nor fixed step-size midpoint solver. These solvers can be configured to operate at different speed-accuracy\ntrade-off (accuracy in computing the integral). While flow-matching with OT path produces higher quality\nsamples compared to diffusion models (Lipman et al., 2023; Le et al., 2023) for the same number of ODE steps\nand achieves better trade-off, very aggressive settings like midpoint with only 4 steps may still dramatically\ndecrease the sample quality.\nInference efficiency is quantified by the number of function evaluation (NFE), which denotes the number of\ntime an ODE solver evaluates the derivative. To improve the inference speed at the extreme low NFE regime\n(i.e., 4), we adopt Bespoke Solvers Shaul et al. (2023) to recover similar sample quality as the original model\nwith a much lower NFE.\nAssume the initial noise sample x(0) = x0 \u223c p(x0). Bespoke solver learns extra parameters \u03b8 \u2208 Rp where p is\n23\nTable 14 Comparing unified Audiobox for Text-to-audio generation results on AudioCaps evaluation set. We find that\nAudiobox outperforms all baselines except the sound only Audiobox Sound. Most notably it even outperforms\nTANGO-full-FT model on most metrics by significant margin.\nobjective\nsubjective\nFAD \u2193\nFD \u2193\nKLD \u2193\nIS \u2191\nCLAP \u2191\nOVL \u2191\nREL \u2191\nground truth\n-\n-\n-\n13.28\n0.49\n3.36\u00b1 0.18\n3.86\u00b1 0.18\nUnified Models\nVoiceLDM Lee et al. (2023)\n10.28\n49.48\n2.95\n4.79\n0.37\n2.07\u00b1 0.16\n2.62\u00b1 0.22\nUniAudio Yang et al. (2023b)\n3.12\n-\n2.60\n-\n-\n-\n-\nAudiobox (ours)\n1.10\n10.14\n1.19\n11.90\n0.70\n3.19\u00b1 0.14\n3.94\u00b1 0.14\nSound-only models\nTANGO-full-FT Ghosal et al. (2023)\n2.19\n18.47\n1.20\n8.80\n0.56\n3.04\u00b1 0.13\n3.78\u00b1 0.15\nAudiobox Sound (ours)\n0.77\n8.30\n1.15\n12.70\n0.71\n3.43\u00b1 0.15\n4.09\u00b1 0.15\nvery small and minimize the global truncation error (sum of local truncation error) between approximate\nsample x\u03b8\nn and ground truth data point x(1) in the following formula: Ex0\u223cp(x0)\u2225x(1) \u2212 x\u03b8\nn\u2225, where x\u03b8\nn is the\noutput of the solver step\u03b8.\nAt a high level, Bespoke solvers aims to learn transformation for paths such that transformed can be more\naccurately estimated with the desired number of ODE steps. Bespoke Solver work by transforming the\nsample trajectory x(t) using two components tr : [0, 1] \u2192 [0, 1] as time reparameterization and invertible\nfunction \u03c6 : [0, 1] \u00d7 Rd \u2192 Rd, where those functions are parameterized by extra parameters \u03b8. Let the\nparametric solver be step\u03b8(t, x; ut). First we transform input (t, x) into (r, \u00afx) = (rt, \u03c6rt(x)). Next, we\nperform a step in the transformed space as (rnext, \u00afxnext) = step(r, \u00afx; \u00afur), using the chosen base solver (e.g.,\nmidpoint), where \u00afur is vector field on transformed trajectory. To transform back to original space, we compute\n(tnext, xnext) = step\u03b8(x, t; ut) = (trnext, \u03c6\u22121\nrnext(\u00afxnext)).\nTo train the Bespoke solver, we generate the ground-truth path x(t) at times ti where i \u2208 [N] using\nstandard ODE solver, and we calculate the local truncation error d\u03b8\ni = \u2225x(ti)\u2212step\u03b8\nx(ti\u22121, x(ti\u22121); u)\u2225 between\nground truth and predicted sample from parameterized solver \u03b8, and finally we minimize the Bespoke loss\nL(\u03b8) = Ex0\u223cp(x0)\nPn\ni=1 d\u03b8\ni .\nIn this paper, we generate ground truth paths for training Bespoke Solvers for speech generation using dopri5\nODE solver to estimate N = 200 steps with guidance weight (GW) of 0.7. Table 15 top half shows the\nevaluation result on zero-shot TTS with matched guidance weight (0.7) comparing two standard ODE solvers:\nmidpoint and dopri5 with the Bespoke Solver. As we can see, by using bespoke solver, we could reduce ODE\nsteps down to 4 and still retain similar performance in term of style similarity and WER.\nIn addition, we also study if a Bespoke Solver trained for a specific guidance weight generalizes to a different\nguidance weight, and present comparison between the default midpoint solver with the bespoke solver using\nGW=0.0. Results suggest that it can generalize to different guidance setups.\nTable 15 Comparison between the standard ODE solver using midpoint, dopri5 and parameterized Bespoke solver in\nterm of NFE, speaker similarity and WER.\nSolver\nNFE\nGW\nSim-o \u2191\nWord error rate (%) \u2193\nLS\nCV\nSWBD\nExpr\nAccent\nLS\nCV\nSWBD\nExpr\nAccent\ndopri5\n\u223c280\n0.7\n0.733\n0.607\n0.605\n0.602\n0.657\n3.0\n3.6\n9.5\n2.8\n0.9\nmidpoint, 16 steps\n32\n0.734\n0.607\n0.608\n0.603\n0.659\n3.2\n3.7\n9.1\n3.2\n0.9\nBespoke, 4 steps\n8\n0.735\n0.607\n0.606\n0.606\n0.658\n3.0\n3.5\n8.3\n3.0\n0.7\nmidpoint, 16 steps\n32\n0.0\n0.671\n0.546\n0.578\n0.541\n0.601\n3.6\n5.1\n12.1\n3.1\n1.3\nBespoke, 4 steps\n8\n0.672\n0.548\n0.576\n0.544\n0.604\n3.6\n5.1\n12.1\n3.0\n1.3\n24\n9\nResponsible AI\nIn order to build a system responsibly, we conduct evaluations to gauge the fairness aspect and studies\nmethods to defend misuse. In this section, we first analyze if our model produces similar performance on\ndifferent groups like genders and accents. Second, we also perform watermarking experiments to evaluate if a\nrecently proposed watermarking system generalizes to our models such that watermarked samples from our\nmodels can be reliably detected.\n9.1\nFairness across groups\nWe train our model on large quantities of data from various sources. We believe through scaling training\ndata, our model can perform well across many different groups. We assess this aspects by evaluating model\nperformance by genders and by accents. In particular, we consider gender bias or accent bias are observed if\nthere is a groups that has significantly worse performance in term of content correctness (measured by WER)\nand style similarity (measured by cosine similarity between style embeddings) compared to those of the entire\npopulation.\nTo conduct our experiment, we consider the zero-shot TTS task conditioned on a context prompt. We use a\ndataset with country and gender labels for this experiment. For the TTS transcript, we sample 20 transcripts\nfrom the test set. For the TTS prompts, we evaluate on accents of which there are at least 5 unique speakers\nin the dataset, which leave us to 64 accents. Then, we sample 20 random utterances (10 for male, 10 for\nfemale) from each accent groups. In total, we have 400 (20 transcripts \u00d7 20 prompts) for each accent groups\nand 12800 (20 transcripts \u00d7 10 prompts \u00d7 64 accents) for each gender groups.\n(a) WER across gender group.\n(b) Speaker similarity across gender group (mean\n\u00b1 1 stddev).\nFigure 4a shows average WER and Figure 4b shows average speaker similarity across different gender group.\nWe observed that the number are very similar and the speaker similary mean fall between \u00b1 1 standard\ndeviation. Figure 5a shows average WER and Figure 5b shows average speaker similarity across different\naccent group. Similar with the gender groups, WER over all accents remain similar and each group speaker\nsimilarity falls within \u00b1 1 standard deviation. Across gender and accent, WER remains very low around 1.5%\nwhich means 1 mistake for every 66 words in the transcript. We come to the conclusion that our model has\nno significant performance difference given different group of gender and accents.\n9.2\nWatermarking for Generated Audio Detection\nRecent advancement on quality and fidelity in audio generative model has empower novel applications and use\ncase on the model. However, at the same time, there are many people has their raising concerns about the\nrisks of misused. Therefore, the ability to recognize which audio is generated or real is crucial to prevent the\nmisused of the technology and enable certain platform to comply with their policy Fernandez et al. (2023).\nIn this section, we use Seamless Watermark (Seamless Communication, 2023) to see we can reliably put and\ndetect an imperceptible watermark on top of our model generated audio. The watermarking model has similar\nbuilding block as Encodec D\u00e9fossez et al. (2022). The training objectives are based on weighted combination\nof two losses: 1) perceptual loss to ensure the watermark is imperceptible (Si-SNR and L1 loss), 2) localization\nloss based on binary cross entropy to ensure accurate localized detection on watermark in frame level.\n25\n(a) WER across accent group.\n(b) Speaker similarity across accent group (mean \u00b1 1 stddev).\nHere, we use the output generated from most scenarios such as zero-shot TTS, description-based TTS,\nvoice+description-based TTS, and sound generation and apply various data augmentation on top of them.\nWe measure the performance of watermark detection by their false positive rate (FPR) and false negative rate\n(FNR).\nTable 16 shows the average FPR and FNR over all tasks for each data augmentations. We observed very\nlow FPR and FNR, close to 0%, which means the watermark works very robustly against various type of\ngenerated audio and speech and data augmentations. Simultaneously, the watermarked audio also have\nvery low scale-invariant signal-to-noise ratio (SI-SNR) -20.6db, which means the watermarks residual is\nin-perceivable from human perspective.\n10\nDiscussion\n10.1\nLimitations\nFine-grained Control: With the recent advances in generative models, the performance in terms of\ncontrollability is mainly determined by the domain coverage and the quantity of the training data. We have\ndemonstrated that for in-context TTS (example-based control), style similarity can be significantly improved\nby scaling the data used for self-supervised pre-training, which learns to infill audio given the audio context.\nIn contrast, description-based control requires a higher level of supervision, using paired audio and description\nto align concepts described in text with variations observed in audio. Hence, it is harder to generalize\ndescription-based control due to the scarcity of labeled data covering various concepts and concepts of different\ngranularity.\nTo give a concrete examples, our training data may contain both instances of chihuahua barking and those of\n26\nAugmentation\nFPR\nFNR\nNo augmentation\n0.001\n0\nBandpass filter\n0.001\n0\nBoost audio\n0.001\n0\nDuck audio\n0.001\n0\nEcho\n0.001\n0.001\nHighpass filter\n0.001\n0\nLowpass filter\n0.001\n0\nPink noise\n0.001\n0\nRandom noise\n0\n0\nSpeed slower\n0\n0.003\nSmoothing\n0\n0.001\nUp-down resampling\n0.001\n0\nTable 16 List of audio augmentation technique applied on top of watermarked audio with their detection performance\nrespectively averaged on all scenarios.\nlabrador barking; however, all those instances are likely captioned as \u201ca dog barking.\u201d Hence, when prompted\nwith \u201ca chihuahua barking,\u201d the best the model can do is generating a dog barking audio clip if the text\nembedding of \u201cchihuahua\u201d and \u201cdog\u201d are close to each other, but it would not be able to generate the correct\nchihuahua barking sound if such supervision was not provided during training. The same idea also applies\nto speech attributes such as accents, where regional accents cannot be accurately generated if the training\ndataset does not include those paired examples.\nData creation: Given the coverage and the quantity of paired data is the key to improve description-based\ncontrol, it is natural to consider strategies to create such data. However, it is in fact very challenging to create\nfine-grained descriptions given audio. While it is easy for annotators to differentiate cat meowing and dog\nbarking, labeling which dog species solely based on audio is difficult task for most of the people. Similar\nchallenges exist as well regarding labeling speech attributes such as accents. Moreover, annotators can often\ndisagree on attributes such as emotion, perceived age and quality of audio. Hence, it is difficult to create\nlarge scale fine-grained description datasets for audio. The lack of large such datasets also leads to difficulty\nin developing attribute taggers and captioning models that can automate description creation and be used for\nevaluation.\n10.2\nBroader Impact\nThis work greatly advances controllability for speech generation and improves coverage of styles. The ability\nto generate speech with desired vocal and acoustic styles using natural language descriptions unlocks a wealth\nof applications. For example, it can be used to create new voices for characters in immersive audiobooks, Ads,\nand movie scripts, where creators have in mind what style of voice the characters should have. Compared\nto exampled-based control (in-context TTS), description-based control can create novel voice of the desired\nwithout having to clone from an existing individual and saves the time creators spends on searching the\nreference voice.\nThe ability to generate speech in diverse acoustic conditions is especially crucial for applications such as film\nmaking and immersive audiobook creation, where characters may be presented in different environment such\nas caves and it is essential to create audio reflecting the acoustic properties of those scenes. The ability to\npreserve the voice while changing emotion and acoustic scenes is also crucial for generating long form audio\ncontent such as stories. Overall, Audiobox makes it much easier for creators to generate content with higher\nquality compared to prior models.\nWhile Audiobox can help spark everyone\u2019s creativity and bring many positive social impacts, similar to other\npowerful generative models, it also carries risks of being misused and causing unintended harm. In particular,\nspeech synthesis may be used for spreading misinformation and impersonation. We presented studies on\nwatermarking to effectively mitigate this risk in a robust fashion. On other hand, we also demonstrated that\n27\nmodel perform similarly well across variations demographic groups, ensuring bias is reduced through data\nscaling.\n11\nConclusion\nThis paper presents Audiobox, a unified model for audio generation with unprecedented versatility, con-\ntrollability, and quality. Audiobox is capable of generating both speech and sound from text description,\naudio example, or a combination of vocal style reference and description. In particular, for speech generation,\nAudiobox is able to control very fine-grained vocal styles such as accent, emotion, timbre and create speech\nsimulating more diverse environment compared to previous models. Asides from showing novel capabilities,\nAudiobox outperforms all prior in-context speech generation and sound generation models on well-studied\nbenchmarks evaluating existing capabilities.\nMore importantly, we believe this work pioneers in building universal audio generative models with unified\ncontrols and sheds light for future research on audio generative modeling. In essence, we demonstrate that\nwith large quantities of data, it is possible to build a unified model that outperforms modality specific ones.\nThis points toward a path similar to the evolution of language generation models, where a large scale model\ntrained with a simple objective on large quantities of data eventually surpasses task or language specific\nmodels with significantly better generalization ability and emerging capabilities.\nAcknowledgement\nThe authors would like to thank Ricky Chen, Hady Elsahar, Ilia Kulikov, Hirofumi Inaguma, Jing Xu, and\nYossi Adi, Alexander H. Liu, Chung-Ming Chien, Qing He, Thilo Koehler, Fuchun Peng, Xiaohui Zhang, Vimal\nManohar, Po-Wei Chou, Kaustubh Kalgaonkar, Anurag Kumar, Yangyang Shi, Zhaoheng Ni, Gael Le Lan, and\nVarun Nagaraja for their helpful discussion on research, thank Juan Pino, Ian Stewart, Alexander Miller, and\nJoelle Pineau for organizational support, thank Adina Williams, Christophe Ropers, Chloe Bakalar, Imanol\nArrieta Ibarra, and Esteban Arcaute for discussion on responsible AI, thank Wei Zhu, Yichen Wang, Jiun-Ren\nLin, Chao Zhou, Peter Weng, Stephen Fink, Ivan Evtimov, David Renardy, Sonia Kim for responsible AI and\nsafety implementation, thank Neil Seejoor, Somya Jain, Chandan Avdhut, Chris Henry, and KC Braunschweig\nfor support on infrastructure, thank Carolyn Krol, Ernest Hammond, Mo Metanat, David Soofian, Ndidi Elue,\nMallika Malhotra, Kelechi Ebi Kamanu, Maeve Ryan, Harrison Rudolph, Jennifer Okafor for their support\non research and grant review, thank Ana Paula Kirschner Mofarrej, Lydia Baillergeau, Steph Miles, Raghu\nNayani, Michelle Restrepo, Tamara Piksa, Chris Wiltz, Orialis Valentin, Aiman Farooq, Gopika Jhala and\nAshley Gabriel on cross-functional support\n28\nContribution\nApoorv Vyas proposed and implemented LLM caption, audio augmentation and annotation quality control\npipelines, and implemented voice prompting\nBowen Shi led Audiobox-Sound experiments, implemented and conducted experiments for Joint-CLAP,\nproposed two-stage fine-tuning and led studies on evaluation\nMatthew Le implemented and conducted experiments for Audiobox-SSL, Audiobox-Speech, and Bespoke\nSolver, led model integration to demo\nAndros Tjandra implemented speech attribute labelers and responsible AI studies\nYi-Chiao Wu created Audiobox baseline results and implemented audio infilling for baselines\nLiang Tan explore speech representation and conducted preliminary experiments on forced aligners\nBowen Shi and Wei-Ning Hsu prepared sound data and implemented, proposed Joint-CLAP and conducted\nexperiments for Audiobox-Sound\nAndros Tjandra and Apoorv Vyas implemented Audiobox\nAndros Tjandra and Matthew Le conducted experiments for duration models\nApoorv Vyas, Andros Tjandra and Bowen Shi iterated on LLM prompting for text-to-speech and sound training\nApoorv Vyas, Andros Tjandra, Matthew Le, Bowen Shi, Liang Tan and Wei-Ning Hsu prepared speech data\nApoorv Vyas, Andros Tjandra, Matthew Le and Bowen Shi conducted Audiobox experiments\nWei-Ning Hsu, Bowen Shi, Apoorv Vyas, Andros Tjandra, Matthew Le wrote the paper\nBaishan Guo, Apoorv Vyas and Andros Tjandra implemented the human annotation pipeline\nBaishan Guo ran human annotation and subjective evaluation, and analyzed annotation and evaluation results\nBapi Akula explored audio pre-processing and transformation, assisted in developing data pipeline\nCarleigh Wood coordinated and facilitated data annotation\nJiemin Zhang led the demo development, designed and implemented demo infra, model integration, early demo\nmitigation and capacity testing\nXinyue Zhang designed and implemented demo backend, data logging, mitigation verification and toxic content\nfiltering.\nRobbie Adkins designed and implemented demo frontend and supported backend implementation.\nAkinniyi Akinyemi conducted demo deployment, demo and mitigation infra set up.\nJoshua Lane implemented early UI structure.\nWilliam Ngan designed the demo experience and implemented front-end demo interfaces.\nBrian Ellis prototyped demo concepts and created audio for demos\nAlice Rakotoarison, Chris Summers conducted demo user experience research\nYael Yungster provided design management support\nJeff Wang provided product management support for the team, contributed to overall research vision, strategy,\nproject milestones and execution.\nIvan Cruz provided technical program management support, coordinated responsible AI study, red teaming,\nand cross-functional support\nRashel Moritz provided program management support, contributed to early project planning, mitigation\nplanning, review, and cross-functional support\n29\nMary Williamson provided management support for the team and co-led the project, contributed to research\nvision, and oversaw demo\nWei-Ning Hsu designed and led the project, advised Apoorv, Bowen, Matthew, Andros, Yi-Chiao, and Liang\non the research, and coordinated research, demo, and data streams.\nReferences\nA. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi,\net al. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325, 2023.\nR. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber.\nCommon voice: A massively-multilingual speech corpus. In International Conference on Language Resources and\nEvaluation, 2019.\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A framework for self-supervised learning of speech\nrepresentations. Advances in neural information processing systems, 2020.\nZ. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and M. Tagliasacchi. Soundstorm: Efficient parallel\naudio generation. arXiv preprint arXiv:2305.09636, 2023.\nH. Bredin. pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe. In Proc. INTERSPEECH\n2023, 2023.\nK. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov. Hts-at: A hierarchical token-semantic audio\ntransformer for sound classification and detection. In IEEE International Conference on Acoustics, Speech and\nSignal Processing, ICASSP, 2022a.\nR. T. Q. Chen. torchdiffeq, 2018. URL https://github.com/rtqichen/torchdiffeq.\nR. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In Neural\nInformation Processing Systems, 2018.\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, et al. Wavlm: Large-scale\nself-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing,\n16(6):1505\u20131518, 2022b.\nCieri, Christopher, et al. . Fisher English training speech parts 1 and 2 LDC200{4,5}S13. Web Download. Linguistic\nData Consortium, Philadelphia, 2004,2005a.\nCieri, Christopher, et al. . Fisher English training speech parts 1 and 2 transcripts LDC200{4,5}T19. Web Download.\nLinguistic Data Consortium, Philadelphia, 2004,2005b.\nA. Clifton, A. Pappu, S. Reddy, Y. Yu, J. Karlgren, B. Carterette, and R. Jones. The spotify podcast dataset. arXiv\npreprint arXiv:2004.04270, 2020.\nJ. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D\u00e9fossez. Simple and controllable music\ngeneration. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nA. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. ArXiv, abs/2210.13438, 2022.\nC. Donahue, J. McAuley, and M. Puckette. Adversarial audio synthesis. arXiv preprint arXiv:1802.04208, 2018.\nA. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438,\n2022.\nP. Fernandez, G. Couairon, H. J\u00e9gou, M. Douze, and T. Furon. The stable signature: Rooting watermarks in latent\ndiffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n22466\u201322477, October 2023.\nJ. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter. Audio\nset: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP), pages 776\u2013780. IEEE, 2017.\nD. Ghosal, N. Majumder, A. Mehrish, and S. Poria. Text-to-audio generation using instruction-tuned llm and latent\ndiffusion model. arXiv preprint arXiv:2304.13731, 2023.\n30\nJ. J. Godfrey, E. C. Holliman, and J. McDaniel. Switchboard: Telephone speech corpus for research and development.\nIn Acoustics, Speech, and Signal Processing, IEEE International Conference on, volume 1, pages 517\u2013520. IEEE\nComputer Society, 1992.\nZ. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan. PromptTTS: Controllable text-to-speech with text descriptions. In\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n1\u20135. IEEE, 2023.\nJ. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing\nSystems, 2020.\nW.-N. Hsu, Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Y. Wang, Y. Cao, Y. Jia, Z. Chen, J. Shen, et al. Hierarchical\ngenerative modeling for controllable speech synthesis. In International Conference on Learning Representations,\n2019.\nW.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed. Hubert: Self-supervised speech\nrepresentation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 29:3451\u20133460, 2021.\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of\nlarge language models, 2021.\nC.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu,\nand D. Eck. Music transformer. arXiv preprint arXiv:1809.04281, 2018.\nQ. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank, et al. Noise2music:\nText-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917, 2023a.\nR. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao. Make-an-audio: Text-to-audio\ngeneration with prompt-enhanced diffusion models. arXiv preprint arXiv:2301.12661, 2023b.\nJ. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar\u2019e, J. Karadayi, V. Liptchinsky, R. Collobert,\nC. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. rahman Mohamed, and E. Dupoux. Libri-Light: A\nbenchmark for asr with limited or no supervision. International Conference on Acoustics, Speech and Signal\nProcessing, 2019.\nE. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour.\nSpeak, read and prompt: High-fidelity text-to-speech with minimal supervision, 2023.\nK. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi. Fr\u00e9chet audio distance: A reference-free metric for evaluating\nmusic enhancement algorithms. In Interspeech, 2019.\nC. D. Kim, B. Kim, H. Lee, and G. Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT,\n2019.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.\nD. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.\nJ. Kong, J. Kim, and J. Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis.\nAdvances in Neural Information Processing Systems, 33:17022\u201317033, 2020.\nQ. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley. Panns: Large-scale pretrained audio neural\nnetworks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28,\n2019.\nF. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00e9fossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi. Audiogen:\nTextually guided audio generation. arXiv preprint arXiv:2209.15352, 2022.\nA. Kulkarni, V. Colotte, and D. Jouvet. Improving transfer of expressivity for end-to-end multispeaker text-to-speech\nsynthesis. In 2021 29th European Signal Processing Conference (EUSIPCO), pages 31\u201335. IEEE, 2021.\nA. Kumar, K. Tan, Z. Ni, P. Manocha, X. Zhang, E. Henderson, and B. Xu. Torchaudio-squim: Reference-less speech\nquality and intelligibility measures in torchaudio. In ICASSP 2023 - 2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 1\u20135, 2023. doi: 10.1109/ICASSP49357.2023.10096680.\n31\nA. \u0141a\u0144cucki. Fastpitch: Parallel text-to-speech with pitch prediction. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 6588\u20136592. IEEE, 2021.\nM. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, and W.-N.\nHsu. Voicebox: Text-guided multilingual universal speech generation at scale, 2023.\nS.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon. Bigvgan: A universal neural vocoder with large-scale\ntraining. arXiv preprint arXiv:2206.04658, 2022.\nY. Lee, I. Yeon, J. Nam, and J. S. Chung. Voiceldm: Text-to-speech with environmental context, 2023.\nY. Leng, Z. Guo, K. Shen, X. Tan, Z. Ju, Y. Liu, Y. Liu, D. Yang, L. Zhang, K. Song, et al. PromptTTS 2: Describing\nand generating voices with text prompt. arXiv preprint arXiv:2309.02285, 2023.\nP. Li, B. Chen, Y. Yao, Y. Wang, A. Wang, and A. Wang. Jen-1: Text-guided universal music generation with\nomnidirectional diffusion models. arXiv preprint arXiv:2308.04729, 2023.\nY. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In\nInternational Conference on Learning Representations, 2023.\nA. H. Liu, M. Le, A. Vyas, B. Shi, A. Tjandra, and W.-N. Hsu. Generative pre-training for speech with flow matching,\n2023a.\nH. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley. Audioldm: Text-to-audio\ngeneration with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023b.\nH. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong, Y. Wang, W. Wang, Y. Wang, and M. D. Plumbley. Audioldm 2:\nLearning holistic audio generation with self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023c.\nX. Liu, Q. Kong, Y. Zhao, H. Liu, Y. Yuan, Y. Liu, R. Xia, Y. Wang, M. D. Plumbley, and W. Wang. Separate\nanything you describe. arXiv preprint arXiv:2308.05037, 2023d.\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019. URL https://api.semanticscholar.org/\nCorpusID:198953378.\nM. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger. Montreal forced aligner: Trainable text-speech\nalignment using kaldi. In Interspeech, 2017.\nT. A. Nguyen, W.-N. Hsu, A. d\u2019Avirro, B. Shi, I. Gat, M. Fazel-Zarani, T. Remez, J. Copet, G. Synnaeve, M. Hassid,\net al. Expresso: A benchmark and analysis of discrete expressive speech resynthesis. arXiv preprint arXiv:2308.05725,\n2023.\nV. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: An asr corpus based on public domain audio books.\nInternational Conference on Acoustics, Speech and Signal Processing, 2015.\nA. Plaquet and H. Bredin. Powerset multi-class cross entropy loss for neural speaker diarization. In Proc. INTER-\nSPEECH 2023, 2023.\nV. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert. Mls: A large-scale multilingual dataset for speech\nresearch. ArXiv, abs/2012.03411, 2020.\nO. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length\nextrapolation. ArXiv, abs/2108.12409, 2021.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\nG. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In International\nConference on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:231591445.\nA. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale\nweak supervision, 2022.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits\nof transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):\n5485\u20135551, 2020.\nA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip\nlatents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n32\nY. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu. Fastspeech 2: Fast and high-quality end-to-end text\nto speech. In International Conference on Learning Representations, 2021.\nA. Rix, J. Beerends, M. Hollier, and A. Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for\nspeech quality assessment of telephone networks and codecs. In 2001 IEEE International Conference on Acoustics,\nSpeech, and Signal Processing. Proceedings (Cat. No.01CH37221), volume 2, pages 749\u2013752 vol.2, 2001.\ndoi:\n10.1109/ICASSP.2001.941023.\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion\nmodels, 2021.\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion\nmodels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\nO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In\nMedical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\nT. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training\ngans. ArXiv, abs/1606.03498, 2016. URL https://api.semanticscholar.org/CorpusID:1687220.\nF. Schneider, Z. Jin, and B. Sch\u00f6lkopf. Mo\\\u02c6 usai: Text-to-music generation with long-context latent diffusion. arXiv\npreprint arXiv:2301.11757, 2023.\nSeamless Communication. Seamless: Multilingual expressive and streaming speech translation. 2023.\nN. Shaul, J. Perez, R. T. Chen, A. Thabet, A. Pumarola, and Y. Lipman. Bespoke solvers for generative flow models.\narXiv preprint arXiv:2310.19075, 2023.\nJ. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. J. Skerry-Ryan, R. A.\nSaurous, Y. Agiomyrgiannakis, and Y. Wu. Natural TTS synthesis by conditioning wavenet on mel spectrogram\npredictions. International Conference on Acoustics, Speech and Signal Processing, 2017.\nK. Shen, Z. Ju, X. Tan, Y. Liu, Y. Leng, L. He, T. Qin, S. Zhao, and J. Bian. Naturalspeech 2: Latent diffusion\nmodels are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116, 2023.\nK. J. Shih, R. Valle, R. Badlani, A. Lancucki, W. Ping, and B. Catanzaro. Rad-tts: Parallel flow-based tts with robust\nalignment learning and diverse synthesis. In ICML Workshop on Invertible Neural Networks, Normalizing Flows,\nand Explicit Likelihood Models, 2021.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,\nD. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao,\nV. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann,\nA. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov,\nP. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith,\nR. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan,\nM. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and\nfine-tuned chat models, 2023.\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention\nis all you need. ArXiv, abs/1706.03762, 2017.\nC. Wang, S. Chen, Y. Wu, Z.-H. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei.\nNeural codec language models are zero-shot text to speech synthesizers. ArXiv, abs/2301.02111, 2023a.\nY. Wang, Z. Ju, X. Tan, L. He, Z. Wu, J. Bian, and S. Zhao. Audit: Audio editing by following instructions with\nlatent diffusion models. arXiv preprint arXiv:2304.00830, 2023b.\nY. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov. Large-scale contrastive language-audio\npretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.\nJ. Yamagishi, C. Veaux, and K. MacDonald. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning\ntoolkit (version 0.92). 2019.\nD. Yang, S. Liu, R. Huang, G. Lei, C. Weng, H. Meng, and D. Yu. InstructTTS: Modelling expressive tts in discrete\nlatent space with natural language style prompt. arXiv preprint arXiv:2301.13662, 2023a.\n33\nD. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu, et al. Uniaudio: An audio\nfoundation model toward universal audio generation. arXiv preprint arXiv:2310.00704, 2023b.\nD. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu. Diffsound: Discrete diffusion model for text-to-sound\ngeneration. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023c.\nN. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural audio\ncodec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495\u2013507, 2022.\nH. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu. Libritts: A corpus derived from\nlibrispeech for text-to-speech. arXiv preprint arXiv:1904.02882, 2019.\nZ. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al. Speak foreign languages\nwith your own voice: Cross-lingual neural codec language modeling. arXiv preprint arXiv:2303.03926, 2023.\n34\nA\nSpeech Attributes for Speech Caption Creation\nAs described in Section 7.1.1, we extract attributes for creating speech captions. We obtain speech attributes\nfrom the associated metadata or by pseudo-labeling for a subset of attributes which can be labeled more\nreliably. Details for each attribute are listed below\n\u2022 Age: We first bin the age into 4 different categories namely less than twenty (<20), young adults (20-35),\nmiddle age (40-60), and elders (>60). We then fine-tune our dataset from pre-trained WavLM-base\ncheckpoint with 3200 hours speeech and age metadata from our training set (consisted of conversational\nand reading speech with various quality).\n\u2022 Gender: We fine-tune on top of WavLM-base checkpoint with 4300 hours speech and gender metadata\nfrom our training set (consisted of conversational and reading speech with various quality).\n\u2022 Audio Quality: We use TorchAudio-Squim Kumar et al. (2023) library and extract Perceptual Evaluation\nof Speech Quality (PESQ) Rix et al. (2001) score. We then bin the score into three categories: Low\nquality ( 0-2.39 ), Normal quality ( 2.39-3.8 ) and Studio Quality ( >3.8 ).\n\u2022 Pitch: We use PyWorld vocoder 6 to extract fundamental frequency (f0) and then calculate the geometric\nmean across all voiced region. We use gender dependent threshold for binning the pitch into three\ndifferent categories: low, normal, high. For gender masculine, we set low pitch (0-40 percentile), normal\npitch (40-90 percentile) and high pitch (>90 percentile). For gender feminine, we set low pitch (percentile\n0-10), normal pitch (10-60 percentile) and high pitch (>60 percentile). The logic behind asymmetric\nthreshold is because in general people will perceive most of masculine voice have lower pitch and most\nof feminine voice have higher pitch.\n\u2022 Speaking rate: Given the transcript and audio, we first apply VAD to remove the silence segments. We\nthen calculate character per seconds (CPS) and bin them into 3 categories: slow (<9.2 CPS), high\n(>20.8 CPS) and normal (9.2 <= x <= 20.8 CPS).\n\u2022 Accent: We use the accent from the metadata whenever available in the metadata, otherwise leave it\nblank.\n\u2022 Emotion: We use the emotion labels whenever available in the metadata, otherwise we leave it as blank.\n\u2022 Environment: We use the environment tags such as inside a room, outside whenever available from the\ndatasets.\nB\nAutomatic Captions: Quality\nTo ensure we get high quality descriptions, we deploy a two-stage approach to filter the annotator candidate.\nFirst, we keep only annotators that successfully labeled pre-selected gold samples with high accuracy (>\n73%). Later, we score their submitted captions using LLM and keep the annotator if their averaged score is\nabove certain threshold. More specifically, for a speech segment, we first use a LLM to generate a caption\nbased on annotated audio attributes. We then run a second stage where we ask the another LLM to compare\nthe LLM-generated caption with human-written caption and rate the quality of human-written captions\nfrom 1 to 5. We prompt this LLM to give low score to human-written captions where no interesting audio\nevents were added in additional to the annotated audio attributes or some important audio attributes are\nmissing. Annotators with an averaged caption score less than 3 were removed. This resulted in high quality\nand detailed captions that complement our pseudo-labeled captions above. Here are some captions example\ncurated by our human annotator:\nC\nUnified Audiobox Task Description\nBelow we describe different tasks that unified Audiobox model can solve along with the inputs required.\n6https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder\n35\n\u2022 Zero-shot TTS (in-context TTS): model takes as input a transcript and an audio example and generates\nspeech that resembles the example\u2019s audio style (described in Section 5.2). Inputs: (context, transcript).\n\u2022 Description-TTS/TTA: model takes as input a transcript/pseudo-transcript and a text description and\ngenerates speech / audio matching the description. Inputs: (description, transcript/pseudo-transcript)\n\u2022 Voice Restylization: model receives a transcript, a voice prompt and a description. The generated\noutput needs to match speaker\u2019s vocal style and the description. Note that the description could contain\nattributes different from the voice prompt. For the voice prompt could have been recorded in a small\nroom with neutral emotion and the description could specify happy emotion in a church with bells\nringing. Inputs: (voice, description, transcript)\n\u2022 Sampling: The model receives a transcript as input and samples diverse voices. Inputs: (transcript)\n\u2022 Speech Infilling/Editing: model takes as input an masked speech with accompanying transcript and an\noptional description and infills the masked portion. Inputs: (context, transcript, optional description)\n\u2022 Audio Infilling/Editing: model takes as input an masked audio with pseudo transcript and description\nto infill the masked portion with matching description. Inputs: (context, pseudo transcript, description)\nD\nSubjective Evaluation Interface\nWe show human annotation interfaces for sound in Figures D1 and D2, for speech in Figures D3 to D5.\n36\nFigure D1 OVL evaluation for sound\n37\nFigure D2 REL evaluation for sound\n38\nFigure D3 Quality MOS evaluation for speech\n39\nFigure D4 Similarity MOS evaluation for speech\n40\nFigure D5 REL evaluation for speech\n41\n"
  },
  {
    "title": "A Recipe for Scaling up Text-to-Video Generation with Text-free Videos",
    "link": "https://arxiv.org/pdf/2312.15770.pdf",
    "upvote": "12",
    "text": "A Recipe for Scaling up Text-to-Video Generation with Text-free Videos\nXiang Wang1\u2217 Shiwei Zhang2 Hangjie Yuan3 Zhiwu Qing1 Biao Gong2 Yingya Zhang2\nYujun Shen4 Changxin Gao1 Nong Sang1\n1Key Laboratory of Image Processing and Intelligent Control,\nSchool of Artificial Intelligence and Automation, Huazhong University of Science and Technology\n2Alibaba Group\n3Zhejiang University\n4Ant Group\n{wxiang,qzw,cgao,nsang}@hust.edu.cn, {zhangjin.zsw,yingya.zyy}@alibaba-inc.com\nhj.yuan@zju.edu.cn, {a.biao.gong,shenyujun0302}@gmail.com\n\u201cClose-up of Caucasian mother and baby girl sitting at windowsill and reading book. Young woman educating daughter at home.\u201d\nResolution \n448x256\nText-to-video generation\nResolution \n896x512\n\u201cFemale Customer Sending Text Message Sitting Outside Coffee Shop On Busy City High Street In Fall\u201d\n\u201cMale designer in office working on laptop with model house\u201d\nCompositional video synthesis\n\u201cSlow motion of Asian girl playing piano, Pan shot\u201d\nResolution \n448x256\n(Text & Sketch & Depth)\n(Text & Sketch)\nResolution \n1280x768\nFigure 1. Example video results generated by the proposed TF-T2V on text-to-video generation and compositional video synthesis tasks\nwithout training on any video-text pairs.\nAbstract\nDiffusion-based text-to-video generation has witnessed\nimpressive progress in the past year yet still falls behind\ntext-to-image generation.\nOne of the key reasons is the\nlimited scale of publicly available data (e.g., 10M video-\ntext pairs in WebVid10M vs. 5B image-text pairs in LAION),\nconsidering the high cost of video captioning. Instead, it\ncould be far easier to collect unlabeled clips from video\nplatforms like YouTube.\nMotivated by this, we come up\n\u2217 Intern at Alibaba Group.\nwith a novel text-to-video generation framework, termed\nTF-T2V, which can directly learn with text-free videos.\nThe rationale behind is to separate the process of text\ndecoding from that of temporal modeling. To this end, we\nemploy a content branch and a motion branch, which are\njointly optimized with weights shared. Following such a\npipeline, we study the effect of doubling the scale of train-\ning set (i.e., video-only WebVid10M) with some randomly\ncollected text-free videos and are encouraged to observe\nthe performance improvement (FID from 9.67 to 8.19 and\nFVD from 484 to 441), demonstrating the scalability of\n1\narXiv:2312.15770v1  [cs.CV]  25 Dec 2023\nour approach.\nWe also find that our model could enjoy\nsustainable performance gain (FID from 8.19 to 7.64 and\nFVD from 441 to 366) after reintroducing some text labels\nfor training.\nFinally, we validate the effectiveness and\ngeneralizability of our ideology on both native text-to-video\ngeneration and compositional video synthesis paradigms.\nCode and models will be publicly available at here.\n1. Introduction\nVideo generation aims to synthesize realistic videos that\npossess visually appealing spatial contents and temporally\ncoherent motions. It has witnessed unprecedented progress\nin recent years with the advent of deep generative tech-\nniques [22, 53], especially with the emergence of video\ndiffusion models [4, 34, 40, 54, 60, 67, 78]. Pioneering\napproaches [28, 33, 67] utilize pure image diffusion models\nor fine-tuning on a small amount of video-text data to syn-\nthesize videos, leading to temporally discontinuous results\ndue to insufficient motion perception [39, 79]. To achieve\nplausible results, current text-to-video methods like Vide-\noLDM [4] and ModelScopeT2V [54] usually insert tem-\nporal blocks into latent 2D-UNet [43] and train the model\non expansive video-text datasets, e.g., WebVid10M [2]. To\nenable more controllable generation, VideoComposer [58]\nproposes a compositional paradigm that incorporates addi-\ntional conditions (e.g., depth, sketch, motion vectors, etc.)\nto guide synthesis, allowing customizable creation.\nDespite this, the progress in text-to-video generation still\nfalls behind text-to-image generation [42, 43]. One of the\nkey reasons is the limited scale of publicly available video-\ntext data, considering the high cost of video captioning [83].\nInstead, it could be far easier to collect text-free video clips\nfrom media platforms like YouTube. There are some works\nsharing similar inspiration, Make-A-Video [50] and Gen-\n1 [12] employ a two-step strategy that first leverages a large\n(\u223c1B parameters) diffusion prior model [42] to convert text\nembedding into image embedding of CLIP [41] and then\nenters it into an image-conditioned generator to synthesize\nvideos. However, the separate two-step manner may cause\nissues such as error accumulation [13], increased model size\nand latency [42, 69], and does not support text-conditional\noptimization if extra video-text data is available, leading to\nsub-optimal results. Moreover, the characteristics of scaling\npotential on video generation are still under-explored.\nIn this work, we aim to train a single unified video\ndiffusion model that allows text-guided video generation\nby exploiting the widely accessible text-free videos and\nexplore its scaling trend. To achieve this, we present a novel\ntwo-branch framework named TF-T2V, where a content\nbranch is designed for spatial appearance generation, and a\nmotion branch specializes in temporal dynamics synthesis.\nMore specifically, we utilize the publicly available image-\ntext datasets such as LAION-5B [48] to learn text-guided\nand image-guided spatial appearance generation.\nIn the\nmotion branch, we harness the video-only data to conduct\nimage-conditioned video synthesis, allowing the temporal\nmodules to learn intricate motion patterns without relying\non textual annotations. Paired video-text data, if available,\ncan also be incorporated into co-optimization. Furthermore,\nunlike previous methods that impose training loss on each\nframe individually, we introduce a temporal coherence loss\nto explicitly enforce the learning of correlations between\nadjacent frames, enhancing the continuity of generated\nvideos. In this way, the proposed TF-T2V achieves text-to-\nvideo generation by assembling contents and motions with a\nunified model, overcoming the high cost of video captioning\nand eliminating the need for complex cascading steps.\nNotably, TF-T2V is a plug-and-play paradigm, which\ncan be integrated into existing text-to-video generation and\ncompositional video synthesis frameworks as shown in\nFig. 1. Different from most prior works that rely heavily on\nvideo-text data and train models on the widely-used water-\nmarked and low-resolution (around 360P) WebVid10M [2],\nTF-T2V opens up new possibilities for optimizing with\ntext-free videos or partially paired video-text data, making\nit more scalable and versatile in widespread scenarios, such\nas high-definition video generation. To study the scaling\ntrend, we double the scale of the training set with some\nrandomly collected text-free videos and are encouraged to\nobserve the performance improvement, with FID from 9.67\nto 8.19 and FVD from 484 to 441. Extensive quantitative\nand qualitative experiments collectively demonstrate the\neffectiveness and scaling potential of the proposed TF-T2V\nin terms of synthetic continuity, fidelity, and controllability.\n2. Related Work\nIn this section, we provide a brief review of relevant liter-\nature on text-to-image generation, text-to-video generation,\nand compositional video synthesis.\nText-to-image generation.\nRecently, text-to-image gen-\neration has made significant strides with the development\nof large-scale image-text datasets such as LAION-5B [48],\nallowing users to create high-resolution and photorealistic\nimages that accurately depict the given natural language\ndescriptions. Previous methods [16, 26, 49] primarily focus\non synthesizing images by adopting generative adversarial\nnetworks (GANs) to estimate training sample distributions.\nDistinguished by the promising stability and scalability,\ndiffusion-based generation models have attracted increasing\nattention [27, 42\u201345].\nDiffusion models utilize iterative\nsteps to gradually refine the generated image, resulting\nin improved quality and realism. Typically, Imagen [45]\nand GLIDE [38] explore text-conditional diffusion models\nand boost sample quality by applying classifier-free guid-\nance [19]. DALL\u00b7E 2 [42] first leverages an image prior\n2\n\u00d7T\nt\n\u00d7T\nshare weights\nt\nImage \nEncoder\n\u201ca sheep by \nthe river\u201d\nC\nText \nEncoder\nD\nD\nTraining stage\nC\nC\nSketch Depth \u2026\nMask\nText-free videos\nImage-text pairs\n\u201ca sheep by the river\u201d\n\u201ca dog is running\u201d\n\u201cfish in the water\u201d\n\u201ca girl in a class\u201d\nMotion branch\nContent branch\nText \nEncoder\nC\nImage \nEncoder\nNULL \ud835\udef7\nC Concatenate D Decoder\nCompositional conditions\nInference stage\nt\n\u00d7T\nText \nEncoder\nText prompt\nD\nGenerated video\nFigure 2. Overall pipeline of TF-T2V, which consists of two branches. In the content branch, paired image-text data is leveraged to\nlearn text-conditioned and image-conditioned spatial appearance generation. The motion branch supports the training of motion dynamic\nsynthesis by feeding text-free videos (or partially paired video-text data if available). During the training stage, both branches are optimized\njointly. Notably, TF-T2V can be seamlessly integrated into the compositional video synthesis framework by incorporating composable\nconditions. In inference, TF-T2V enables text-guided video generation by taking text prompts and random noise sequences as input.\nto bridge multi-modal embedding spaces and then learns a\ndiffusion decoder to synthesize images in the pixel space.\nStable Diffusion [43] introduces latent diffusion models that\nconduct iterative denoising processes at the latent level to\nsave computational costs. There are also some works that\ngenerate customized and desirable images by incorporating\nadditional spatial control signals [24, 36, 77].\nText-to-video generation. This task poses additional chal-\nlenges compared to text-to-image generation due to the tem-\nporal dynamics involved in videos. Various early techniques\nhave been proposed to tackle this problem, such as recurrent\nneural networks combined with GANs [3, 51, 53, 61, 64]\nor transformer-based autoregressive models [22, 73]. With\nthe subsequent advent of video diffusion models pretrained\non large-scale video-text datasets [2, 63, 71], video content\ncreation has demonstrated remarkable advances [1, 4, 6\u20139,\n14, 15, 17, 18, 21, 23, 28, 31\u201333, 35, 37, 39, 56, 57, 62, 65,\n67, 69, 74\u201376]. Imagen Video [21] learns cascaded pixel-\nlevel diffusion models to produce high-resolution videos.\nFollowing [42], Make-A-Video [50] introduces a two-step\nstrategy that first maps the input text to image embedding\nby a large (\u223c1B parameters) diffusion prior model and then\nembeds the resulting embedding into an image-conditional\nvideo diffusion model to synthesize videos in pixel space.\nVideoLDM [4] and ModelScopeT2V [54] extend 2D-UNet\ninto 3D-UNet by injecting temporal layers and operate a\nlatent denoising process to save computational resources. In\nthis paper, we present a single unified framework for text-to-\nvideo generation and study the scaling trend by harnessing\nwidely accessible text-free videos.\nCompositional video synthesis. Traditional text-to-video\nmethods solely rely on textual descriptions to control the\nvideo generation process, limiting desired fine-grained cus-\ntomization such as texture, object position, motion patterns,\netc. To tackle this constraint and pursue higher controlla-\nbility, several controllable video synthesis methods [8, 9,\n12, 29, 58, 68, 72, 79, 81] have been proposed.\nThese\nmethods utilize additional control signals, such as depth\nor sketch, to guide the generation of videos.\nBy incor-\nporating extra structured guidance, the generated content\ncan be precisely controlled and customized. Among these\napproaches, VideoComposer [58] stands out as a pioneering\nand versatile compositional technique. It integrates multiple\nconditioning signals including textual, spatial and temporal\nconditions within a unified framework, offering enhanced\ncontrollability, compositionality, and realism in the gener-\nated videos. Despite the remarkable quality, these methods\nstill rely on high-quality video-text data to unleash powerful\nand customizable synthesis. In contrast, our method can\nbe directly merged into existing controllable frameworks to\ncustomize videos by exploiting text-free videos.\n3. Method\nWe first provide a brief introduction to the preliminaries\nof the video diffusion model. Then, we will elaborate on the\nmechanisms of TF-T2V in detail. The overall framework\nof the proposed TF-T2V is displayed in Fig. 2.\n3.1. Preliminaries of video diffusion model\nDiffusion models involve a forward diffusion process\nand a reverse iterative denoising stage. The forward process\nof diffusion models is gradually imposing random noise to\nclean data x0 in a Markovian chain:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2t\u22121xt\u22121, \u03b2tI), t = 1, ..., T\n(1)\n3\nwhere \u03b2t \u2208 (0, 1) is a noise schedule and T is the total\ntime step. When T is sufficiently large, e.g. T = 1000,\nthe resulting xT is nearly a random Gaussian distribution\nN(0, I). The role of diffusion model is to denoise xT and\nlearn to iteratively estimate the reversed process:\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), P\n\u03b8(xt, t))\n(2)\nWe usually train a denoising model \u02c6x\u03b8 parameterized by\n\u03b8 to approximate the original data x0 and optimize the\nfollowing v-prediction [21, 46] problem:\nLbase = E\u03b8[||v \u2212 \u02c6x\u03b8(xt, t, c)||2\n2]\n(3)\nwhere c is conditional information such as textual prompt,\nand v is the parameterized prediction objective. In repre-\nsentative video diffusion models [4, 54, 58], the denoising\nmodel \u02c6x\u03b8 is a latent 3D-UNet [4, 54] modified from its\n2D version [43] by inserting additional temporal blocks,\nwhich is optimized in the latent feature space by applying a\nvariational autoencoder [11], and Eq. (3) is applied on each\nframe of the input video to train the whole model.\n3.2. TF-T2V\nThe objective of TF-T2V is to learn a text-conditioned\nvideo diffusion model to create visually appealing and\ntemporally coherent videos with text-free videos or partially\npaired video-text data. Without loss of generality, we first\ndescribe the workflow of our TF-T2V in the scenario where\nonly text-free video is used. With merely text-free videos\navailable for training, it is challenging to guide content\ncreation by textual information since there lacks text-visual\ncorrespondence. To tackle this issue, we propose to resort\nto web-scale and high-quality image-text datasets [47, 48],\nwhich are publicly accessible on the Internet. However, this\nraises another question: how can we leverage the image-text\ndata and text-free videos in a unified framework?\nRecalling the network architecture in 3D-UNet, the\nspatial modules mainly focus on appearance modeling, and\nthe temporal modules primarily aim to operate motion\ncoherence. The intuition is that we can utilize image-text\ndata to learn text-conditioned spatial appearance generation\nand adopt high-quality text-free videos to guide consistent\nmotion dynamic synthesis. In this way, we can perform\ntext-to-video generation in a single model to synthesize\nhigh-quality and consistent videos during the inference\nstage. Based on this, the proposed TF-T2V consists of two\nbranches: a content branch for spatial appearance genera-\ntion and a motion branch for motion dynamic synthesis.\n3.2.1\nSpatial appearance generation\nLike previous text-to-image works [43, 77], the content\nbranch of TF-T2V takes a noised image Iimage \u2208 H \u00d7\nW \u00d7 C as input, where H, W, C are the height, width, and\nchannel dimensions respectively, and employs conditional\nsignals (i.e., text and image embeddings) to offer semantic\nguidance for content generation.\nThis branch primarily\nconcentrates on optimizing the spatial modules in the video\ndiffusion model and plays a crucial role in determining\nappealing visual quality.\nIn order to ensure that each\ncondition can also control the created content separately,\nwe randomly drop text or image embeddings with a certain\nprobability during training. The text and image encoders\nfrom CLIP [41] are adopted to encode embeddings.\n3.2.2\nMotion dynamic synthesis\nThe pursuit of producing highly temporally consistent\nvideos is a unique hallmark of video creation.\nRecent\nadvancements [4, 54, 57, 58] in the realm of video synthesis\nusually utilize large-scale video-text datasets such as Web-\nVid10M [2] to achieve coherent video generation. However,\nacquiring large-scale video-text pairs consumes extensive\nmanpower and time, hindering the scaling up of video\ndiffusion models. To make matters worse, the widely used\nWebVid10M is a watermarked and low-resolution (around\n360P) dataset, resulting in unsatisfactory video creation that\ncannot meet the high-quality video synthesis requirements.\nTo mitigate the above issues, we propose to leverage\nhigh-quality text-free videos that are easily accessible on\nvideo media platforms, e.g., YouTube and TikTok. To fully\nexcavate the abundant motion dynamics within the text-free\nvideos, we train a image-conditioned model. By optimizing\nthis image-to-video generation task, the temporal modules\nin the video diffusion model can learn to perceive and model\ndiverse motion dynamics. Specifically, given a noised video\nIvideo \u2208 F \u00d7 H \u00d7 W \u00d7 C, where F is the temporal\nlength, the motion branch of TF-T2V learns to recover the\nundisturbed video guided by the image embedding. The\nimage embedding is extracted from the center frame of the\noriginal video by applying CLIP\u2019s image encoder [41].\nSince large-scale image-text data used for training con-\ntains abundant movement intentions [30], TF-T2V can\nachieve text-to-video generation by assembling spatial ap-\npearances involving motion trends and predicted motion\ndynamics. When extra paired video-text data is available,\nwe conduct both text-to-video and image-to-video genera-\ntion based on video-text pairs to train TF-T2V and further\nenhance the perception ability for desirable textual control.\nIn addition, we notice that previous works apply the\ntraining loss (i.e., Eq. (3)) on each frame of the input video\nindividually without considering temporal correlations be-\ntween frames, suffering from incoherent appearances and\nmotions. Inspired by the early study [25, 55, 59, 80] finding\nthat the difference between two adjacent frames usually\ncontains motion patterns, e.g., dynamic trajectory, we thus\n4\nTable 1. Quantitative comparison with state-of-the-art methods for text-to-video task on MSR-VTT in terms of FID, FVD, and CLIPSIM.\nMethod\nZero-shot\nParameters\nFID (\u2193)\nFVD (\u2193)\nCLIPSIM (\u2191)\nN\u00a8uwa [66]\nNo\n-\n47.68\n-\n0.2439\nCogVideo (Chinese) [22]\nYes\n15.5B\n24.78\n-\n0.2614\nCogVideo (English) [22]\nYes\n15.5B\n23.59\n1294\n0.2631\nMagicVideo [82]\nYes\n-\n-\n1290\n-\nMake-A-Video [50]\nYes\n9.7B\n13.17\n-\n0.3049\nModelScopeT2V [54]\nYes\n1.7B\n11.09\n550\n0.2930\nVideoComposer [58]\nYes\n1.9B\n10.77\n580\n0.2932\nLatent-Shift [1]\nYes\n1.5B\n15.23\n-\n0.2773\nVideoLDM [4]\nYes\n4.2B\n-\n-\n0.2929\nPYoCo [14]\nYes\n-\n9.73\n-\n-\nTF-T2V (WebVid10M)\nYes\n1.8B\n9.67\n484\n0.2953\nTF-T2V (WebVid10M+Internal10M)\nYes\n1.8B\n8.19\n441\n0.2991\npropose a temporal coherence loss that utilizes the frame\ndifference as an additional supervisory signal:\nLcoherence = E\u03b8[PF \u22121\nj=1 ||(vj+1\u2212vj)\u2212(oj+1\u2212oj)||2\n2] (4)\nwhere oj and vj are the predicted frame and corresponding\nground truth.\nThis loss term measures the discrepancy\nbetween the predicted frame differences and the ground\ntruth frame differences of the input parameterized video.\nBy minimizing Eq. (4), TF-T2V helps to alleviate frame\nflickering and ensures that the generated videos exhibit\nseamless transitions and promising temporal dynamics.\n3.2.3\nTraining and inference\nIn order to mine the complementary advantages of spa-\ntial appearance generation and motion dynamic synthesis,\nwe jointly optimize the entire model in an end-to-end\nmanner. The total loss can be formulated as:\nLtotal = Lbase + \u03bbLcoherence\n(5)\nwhere Lbase is imposed on video and image together by\ntreating the image as a \u201csingle frame\u201d video, and \u03bb is a\nbalance coefficient that is set empirically to 0.1.\nAfter training, we can perform text-guided video gen-\neration to synthesize temporally consistent video content\nthat aligns well with the given text prompt.\nMoreover,\nTF-T2V is a general framework and can also be inserted\ninto existing compositional video synthesis paradigm [58]\nby incorporating additional spatial and temporal structural\nconditions, allowing for customized video creation.\n4. Experiments\nIn this section, we present a comprehensive quantitative\nand qualitative evaluation of the proposed TF-T2V on text-\nto-video generation and composition video synthesis.\n4.1. Experimental setup\nImplementation details.\nTF-T2V is built on two typi-\ncal open-source baselines, i.e., ModelScopeT2V [54] and\nTable 2. Human preference results on text-to-video generation.\nMethod\nText\nVisual\nTemporal\nalignment\nquality\ncoherence\nModelScopeT2V [54]\n83.5%\n74.0%\n81.3%\nTF-T2V\n86.5%\n87.0%\n92.5%\nVideoComposer [58]. DDPM sampler [20] with T = 1000\nsteps is adopted for training, and we employ DDIM [52]\nwith 50 steps for inference. We optimize TF-T2V using\nAdamW optimizer with a learning rate of 5e-5. For input\nvideos, we sample 16 frames from each video at 4FPS and\ncrop a 448 \u00d7 256 region at the center as the basic setting.\nNote that we can also easily train high-definition video\ndiffusion models by collecting high-quality text-free videos\n(see examples in the Appendix). LAION-5B [48] is utilized\nto provide image-text pairs. Unless otherwise stated, we\ntreat WebVid10M, which includes about 10.7M video-text\npairs, as a text-free dataset to train TF-T2V and do not\nuse any textual annotations. To study scaling trends, we\ngathered about 10M high-quality videos without text labels\nfrom internal data, termed the Internal10M dataset.\nMetrics. (i) To evaluate text-to-video generation, following\nprevious works [4, 54], we leverage the standard Fr\u00b4echet\nInception Distance (FID), Fr\u00b4echet Video Distance (FVD),\nand CLIP Similarity (CLIPSIM) as quantitative evaluation\nmetrics and report results on MSR-VTT dataset [70]. (ii)\nFor controllability evaluation, we leverage depth error,\nsketch error, and end-point-error (EPE) [10] to verify\nwhether the generated videos obey the control of input\nconditions. Depth error measures the divergence between\nthe input depth conditions and the eliminated depth of the\nsynthesized video.\nSimilarly, sketch error examines the\nsketch control. EPE evaluates the flow consistency between\nthe reference video and the generated video. In addition,\nhuman evaluation is also introduced to validate our method.\n4.2. Evaluation on text-to-video generation\nTab. 1 displays the comparative quantitative results with\nexisting state-of-the-art methods. We observe that TF-T2V\nachieves remarkable performance under various metrics.\n5\n\u201cFemale architect working with paper plan on desk in the office\u201d\nModelScopeT2V\nText2video-Zero\nZeroScope\nTF-T2V\n\u201cPortrait of smiling young woman outdoors\u201d\n\u201cTwo businessmen discussing team project working on computer\u201d\nModelScopeT2V\nText2video-Zero\nZeroScope\nTF-T2V\n\u201cFruits and Vegetables with Juice\u201d\nFigure 3. Qualitative comparison on text-to-video generation. Three representative open-source text-to-video approaches are compared,\nincluding ModelScopeT2V [54], Text2video-Zero [28] and ZeroScope [5]. Please refer to the Appendix for videos and more comparisons.\nTable 3. Evaluation of structure control based on depth signals.\nMethod\nCondition\nDepth error (\u2193)\nVideoComposer [58]\nText\n0.382\nVideoComposer [58]\nText and depth\n0.217\nTF-T2V\nText and depth\n0.209\nTable 4. Evaluation of structure control based on sketch signals.\nMethod\nCondition\nSketch error (\u2193)\nVideoComposer [58]\nText\n0.1854\nVideoComposer [58]\nText and sketch\n0.1161\nTF-T2V\nText and sketch\n0.1146\nNotably, TF-T2V trained on WebVid10M and Internal10M\nobtains higher performance than the counterpart on Web-\nVid10M, revealing promising scalable capability. We show\nthe qualitative visualizations in Fig. 3. From the results,\nwe can find that compared with previous methods, TF-T2V\nobtains impressive video creation in terms of both temporal\ncontinuity and visual quality.\nThe human assessment in\nTab. 2 also reveals the above observations. The user study\nis performed on 100 randomly synthesized videos.\n4.3. Evaluation on compositional video synthesis\nWe compare the controllability of TF-T2V and Video-\nComposer on 1,000 generated videos in terms of depth\nTable 5. Evaluation of motion control based on motion vectors.\nMethod\nCondition\nEPE (\u2193)\nVideoComposer [58]\nText\n4.13\nVideoComposer [58]\nText and motion vector\n1.98\nTF-T2V\nText and motion vector\n1.88\nTable 6. Human evaluations on compositional video synthesis.\nMethod\nStructure\nVisual\nTemporal\nalignment\nquality\ncoherence\nVideoComposer [58]\n79.0%\n66.0%\n77.5%\nTF-T2V\n89.0%\n79.5%\n84.5%\ncontrol (Tab. 3), sketch control (Tab. 4) and motion control\n(Tab. 5). The above experimental evaluations highlight the\neffectiveness of TF-T2V by leveraging text-free videos. In\nFig. 4 and 5, we show the comparison of TF-T2V and\nexisting methods on compositional video generation. We\nnotice that TF-T2V exhibits high-fidelity and consistent\nvideo generation. In addition, we conduct a human evalua-\ntion on 100 randomly sampled videos and report the results\nin Tab. 6.\nThe preference assessment provides further\nevidence of the superiority of the proposed TF-T2V.\n6\nSource video\nCondition\nTF-T2V\nText2video-Zero\nControl-A-Video\nVideoComposer\n\u201cCropped cute little girl sitting reading a fairy-tale emotionaly\u201d\n\u201cFather and daughter enjoying and playing guitar together\u201d\nGen-1\nFigure 4. Qualitative comparison on compositional depth-to-video generation. The videos are generated by taking textual prompts and\nstructural guidance as conditions. Compared with existing methods, TF-T2V yields more structural compliance and high-fidelity results.\nSource video\nCondition\nTF-T2V\nText2video-Zero\nControl-A-Video\nVideoComposer\n\u201cMan hands screw the bulb into the lamp. Isolated on black background.\u201d\n\u201cFemale is applying foundation with a make-up brush on her face.\u201d\nFigure 5. Qualitative comparison on compositional sketch-to-video generation. The videos are generated by taking textual descriptions\nand structural guidance as conditions. Compared with other methods, TF-T2V produces more realistic and consistent results.\n4.4. Ablation study\nEffect of temporal coherence loss. To enhance temporal\nconsistency, we propose a temporal coherence loss.\nIn\nTab. 7, we show the effectiveness of the proposed tem-\nporal coherence loss in terms of frame consistency. The\nmetric results are obtained by calculating the average CLIP\nsimilarity of two consecutive frames in 1,000 videos. We\nfurther display the qualitative comparative results in Fig. 6\nand observe that temporal coherence loss helps to alleviate\ntemporal discontinuity such as color shift.\n4.5. Evaluation on semi-supervised setting\nThrough the above experiments and observations, we\nverify that text-free video can help improve the continuity\n7\n\u201cMulti-ethnic friends having fun on a winter day\u201d\nSource video\nCondition\nw/o temporal\ncoherence loss\nTF-T2V\n\u201cBusinessman signing contract, pen and document\u201d\nFigure 6. Qualitative ablation study. The videos are generated by taking textual descriptions and structural guidance as conditions.\nModelScopeT2V\nZeroScope\nTF-T2V\n(Semi-supervised)\nAnimateDiff\nVideoCrafter\n\u201cA dog is running away from the camera\u201d\n\u201cA man is running from right to left\u201d\nTF-T2V\n(Language-free)\nFigure 7. Qualitative evaluation on text-to-video generation with temporally-correlated text prompts involving the evolution of movement.\nTable 7. Text-to-video evaluation on frame consistency.\nMethod\nFrame consistency (%) \u2191\nw/o temporal coherence loss\n89.71\nTF-T2V\n91.06\nTable 8. Quantitative experiments on text-to-video generation.\nTF-T2V-Semi means the semi-supervised setting where labeled\nWebVid10M and text-free Internal10M are adopted.\nMethod\nFID (\u2193)\nFVD (\u2193)\nCLIPSIM (\u2191)\nModelScopeT2V [54]\n11.09\n550\n0.2930\nTF-T2V\n8.19\n441\n0.2991\nTF-T2V-Semi\n7.64\n366\n0.3032\nand quality of generated video.\nAs previously stated,\nTF-T2V also supports the combination of annotated video-\ntext data and text-free videos to train the model, i.e., the\nsemi-supervised manner. The annotated text can provide\nadditional fine-grained motion signals, enhancing the align-\nment of generated videos and the provided prompts involv-\ning desired motion evolution.\nWe show the comparison\nresults in Tab. 8 and find that the semi-supervised manner\nreaches the best performance, indicating the effectiveness\nof harnessing text-free videos. Notably, TF-T2V-Semi out-\nperforms ModelScopeT2V trained on labeled WebVid10M,\npossessing good scalability. Moreover, the qualitative eval-\nuations in Fig. 7 show that existing methods may struggle\nto synthesize text-aligned consistent videos when textual\nprompts involve desired temporal evolution. In contrast,\nTF-T2V in the semi-supervised setting exhibits excellent\ntext-video alignment and temporally smooth generation.\n5. Conclusion\nIn this paper, we present a novel and versatile video\ngeneration framework named TF-T2V to exploit text-free\n8\nvideos and explore its scaling trend. TF-T2V effectively\ndecomposes video generation into spatial appearance gener-\nation and motion dynamic synthesis. A temporal coherence\nloss is introduced to explicitly constrain the learning of\ncorrelations between adjacent frames. Experimental results\ndemonstrate the effectiveness and potential of TF-T2V in\nterms of fidelity, controllability, and scalability.\nAcknowledgements. This work is supported by the Na-\ntional Natural Science Foundation of China under grant\nU22B2053 and Alibaba Group through Alibaba Research\nIntern Program.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion\nwith temporal shift for efficient text-to-video generation.\narXiv preprint arXiv:2304.08477, 2023. 3, 5\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 2, 3,\n4\n[3] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama\nChellappa, and Hans Peter Graf.\nConditional GAN with\ndiscriminative filter generation for text-to-video synthesis. In\nIJCAI, page 2, 2019. 3\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with\nlatent diffusion models.\nIn CVPR, pages 22563\u201322575,\n2023. 2, 3, 4, 5\n[5] Cerspense. Zeroscope: Diffusion-based text-to-video syn-\nthesis.\nhttps://huggingface.co/cerspense/\nzeroscope_v2_576w, 2023. 6\n[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.\nPix2video: Video editing using image diffusion. In ICCV,\npages 23206\u201323217, 2023. 3\n[7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu.\nStablevideo: Text-driven consistency-aware diffusion video\nediting. In ICCV, pages 23040\u201323050, 2023.\n[8] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-\nYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffu-\nsion model for controllable video synthesis. arXiv preprint\narXiv:2304.14404, 2023. 3\n[9] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models.\narXiv preprint arXiv:2305.13840, 2023. 3\n[10] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical flow with convolutional networks. In ICCV,\npages 2758\u20132766, 2015. 5\n[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\nTransformers for high-resolution image synthesis. In CVPR,\npages 12873\u201312883, 2021. 4\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, pages 7346\u20137356, 2023. 2, 3\n[13] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.\nScenescape: Text-driven consistent scene generation. arXiv\npreprint arXiv:2302.01133, 2023. 2\n[14] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In ICCV, pages\n22930\u201322941, 2023. 3, 5\n[15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 3\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 27,\n2014. 2\n[17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 3\n[18] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising\ndiffusion probabilistic models.\nNeurIPS, 33:6840\u20136851,\n2020. 5\n[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 3, 4\n[22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. Cogvideo: Large-scale pretraining for text-to-video\ngeneration via Transformers. In ICLR, 2023. 2, 3, 5\n[23] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi\nYu, and Sibei Yang. Free-bloom: Zero-shot text-to-video\ngenerator with llm director and ldm animator. arXiv preprint\narXiv:2309.14494, 2023. 3\n[24] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nComposer: Creative and controllable\nimage synthesis with composable conditions. ICML, 2023.\n3\n[25] Hueihan Jhuang,\nJuergen Gall,\nSilvia Zuffi,\nCordelia\nSchmid, and Michael J Black. Towards understanding action\nrecognition. In ICCV, pages 3192\u20133199, 2013. 4\n[26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park. Scaling up\nGANs for text-to-image synthesis. In CVPR, pages 10124\u2013\n10134, 2023. 2\n[27] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\n9\nText-based real image editing with diffusion models.\nIn\nCVPR, pages 6007\u20136017, 2023. 2\n[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 3, 6\n[29] Ariel Lapid, Idan Achituve, Lior Bracha, and Ethan Fetaya.\nGd-vdm: Generated depth for better diffusion-based video\ngeneration. arXiv preprint arXiv:2306.11173, 2023. 3\n[30] Jiangtong Li, Li Niu, and Liqing Zhang.\nAction-aware\nembedding enhancement for image-text retrieval. In AAAI,\npages 1323\u20131331, 2022. 4\n[31] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[32] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu\nLu, Ping Luo, and Mingyu Ding.\nVdt:\nAn empirical\nstudy on video diffusion with Transformers. arXiv preprint\narXiv:2305.13311, 2023.\n[33] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion: Decomposed diffusion models\nfor high-quality video generation. In CVPR, pages 10209\u2013\n10218, 2023. 2, 3\n[34] Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya\nZhang, and Zhidong Deng.\nDreamtalk: When expressive\ntalking head generation meets diffusion probabilistic models.\narXiv preprint arXiv:2312.09767, 2023. 2\n[35] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3\n[36] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 3\n[37] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang,\nand Martin Renqiang Min.\nConditional image-to-video\ngeneration with latent flow diffusion models. In CVPR, pages\n18444\u201318455, 2023. 3\n[38] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen. Glide: Towards photorealistic\nimage generation and editing with text-guided diffusion\nmodels. In ICML, pages 16784\u201316804. PMLR, 2022. 2\n[39] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing. In\nICCV, 2023. 2, 3\n[40] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang,\nYujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang.\nHierarchical spatio-temporal decoupling for text-to-video\ngeneration. arXiv preprint arXiv:2312.04483, 2023. 2\n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In ICML, pages 8748\u20138763. PMLR, 2021. 2, 4\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2, 3\n[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn CVPR, pages\n10684\u201310695, 2022. 2, 3, 4\n[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023.\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022. 2\n[46] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 4\n[47] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 4\n[48] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models.\nNeurIPS, 35:25278\u2013\n25294, 2022. 2, 4, 5\n[49] Yujun Shen and Bolei Zhou. Closed-form factorization of\nlatent semantics in GANs.\nIn CVPR, pages 1532\u20131540,\n2021. 2\n[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. ICLR, 2023. 2, 3, 5\n[51] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. StyleGAN-v: A continuous video generator with the\nprice, image quality and perks of StyleGAN2.\nIn CVPR,\npages 3626\u20133636, 2022. 3\n[52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 5\n[53] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocoGAN: Decomposing motion and content for\nvideo generation. In CVPR, pages 1526\u20131535, 2018. 2, 3\n[54] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2,\n3, 4, 5, 6, 8\n[55] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn:\nTemporal difference networks for efficient action recogni-\ntion. In CVPR, pages 1895\u20131904, 2021. 4\n10\n[56] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3\n[57] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu.\nVideofactory: Swap\nattention in spatiotemporal diffusions for text-to-video gen-\neration. arXiv preprint arXiv:2305.10874, 2023. 3, 4\n[58] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and\nJingren Zhou. Videocomposer: Compositional video syn-\nthesis with motion controllability. NeurIPS, 2023. 2, 3, 4, 5,\n6\n[59] Xiang Wang, Shiwei Zhang, Zhiwu Qing, Changxin Gao,\nYingya Zhang, Deli Zhao, and Nong Sang. Molo: Motion-\naugmented long-short contrastive learning for few-shot ac-\ntion recognition. In CVPR, pages 18011\u201318021, 2023. 4\n[60] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya\nZhang, Changxin Gao, and Nong Sang. Videolcm: Video\nlatent consistency model. arXiv preprint arXiv:2312.09109,\n2023. 2\n[61] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. G3an: Disentangling appearance and motion for\nvideo generation. In CVPR, pages 5264\u20135273, 2020. 3\n[62] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, et al. Lavie: High-quality video generation\nwith cascaded latent diffusion models.\narXiv preprint\narXiv:2309.15103, 2023. 3\n[63] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,\nXin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei\nLiu, et al.\nInternvid: A large-scale video-text dataset for\nmultimodal understanding and generation.\narXiv preprint\narXiv:2307.06942, 2023. 3\n[64] Yuhan Wang, Liming Jiang, and Chen Change Loy. Styleinv:\nA temporal style modulated inversion network for uncondi-\ntional video generation. In ICCV, pages 22851\u201322861, 2023.\n3\n[65] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan,\nZhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and\nHongming Shan.\nDreamvideo:\nComposing your dream\nvideos with customized subject and motion. arXiv preprint\narXiv:2312.04433, 2023. 3\n[66] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In ECCV, pages\n720\u2013736. Springer, 2022. 5\n[67] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, pages 7623\u20137633, 2023. 2, 3\n[68] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong\nZhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong\nCun, Xintao Wang, et al.\nMake-your-video: Customized\nvideo generation using textual and structural guidance. arXiv\npreprint arXiv:2306.00943, 2023. 3\n[69] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang\nJiang. Simda: Simple diffusion adapter for efficient video\ngeneration. arXiv preprint arXiv:2308.09710, 2023. 2, 3\n[70] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, pages 5288\u20135296, 2016. 5\n[71] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing high-resolution video-language representation with\nlarge-scale video transcriptions. In CVPR, 2022. 3\n[72] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang\nLi, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained\ncontrol in video generation by integrating text, image, and\ntrajectory. arXiv preprint arXiv:2308.08089, 2023. 3\n[73] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video Transformer.\nIn CVPR, pages 10459\u2013\n10469, 2023. 3\n[74] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin.\nVideo probabilistic diffusion models in projected latent\nspace. In CVPR, pages 18456\u201318466, 2023. 3\n[75] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei,\nTao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel\nAlbanie, and Dong Ni.\nInstructvideo: Instructing video\ndiffusion models with human feedback.\narXiv preprint\narXiv:2312.12490, 2023.\n[76] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou.\nShow-1: Marrying pixel and latent\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 3\n[77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, pages 3836\u20133847, 2023. 3, 4\n[78] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2\n[79] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 2, 3\n[80] Zhang Zhang and Dacheng Tao. Slow feature analysis for\nhuman action recognition. TPAMI, 34(3):436\u2013450, 2012. 4\n[81] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and\nJun Zhu. Controlvideo: Adding conditional control for one\nshot text-to-video editing. arXiv preprint arXiv:2305.17098,\n2023. 3\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 5\n[83] Junbao Zhuo, Xingyu Zhao, Shuhui Wang, Huimin Ma, and\nQingming Huang.\nSynthesizing videos from images for\nimage-to-video adaptation. In ACMMM, pages 8294\u20138303,\n2023. 2\n11\nA Recipe for Scaling up Text-to-Video Generation with Text-free Videos\nSupplementary Material\nSource video\nCondition\nTF-T2V\nVideoComposer\n\u201cDoctor consultating with young patient woman lying on bed in clinic\u201d\n\u201cLittle female kid showing heart sign with hands, smiling on camera.\u201d\nSource video\nCondition\nTF-T2V\nVideoComposer\n\u201cYoung Caucasian beautiful female with long hair tapping on smartphone\u201d\n\u201cColourful parrot sitting in tree branch and looking around, Singapore\u201d\nFigure 8. Qualitative comparison on compositional motion-to-video synthesis. The videos are generated by taking textual descriptions\nand motion vectors as conditions. Compared to VideoComposer, TF-T2V produces more realistic and appealing results.\nTable 9. Ablation study on different training manners.\nSetting\nFID (\u2193)\nFVD (\u2193)\nCLIPSIM (\u2191)\nSeparately\n9.22\n503\n0.2905\nJointly\n8.19\n441\n0.2991\nDue to the page limit of the main text, we add more\ndetails and experimental results in this appendix. Besides,\nlimitations and future work will also be discussed.\n6. More experimental details\nIn Fig. 8, we show the comparison on compositional\nmotion-to-video synthesis. TF-T2V achieves more appeal-\ning results than the baseline VideoComposer. Following\nprior works, we use an off-the-shelf pre-trained variational\nautoencoder (VAE) model from Stable Diffusion 2.1 to\nencode the latent features.\nThe VAE encoder has a\ndownsample factor of 8. In the experiment, the network\nstructure of TF-T2V is basically consistent with the open\nsource ModelScopeT2V and VideoComposer to facilitate\nfair comparison.\nNote that TF-T2V is a plug-and-play\nframework that can also be applied to other text-to-video\ngeneration and controllable video synthesis methods. For\nhuman evaluation, we randomly generate 100 videos and\nask users to rate and evaluate them. The highest score for\neach evaluation content is 100%, the lowest score is 0%,\nand the final statistical average is reported.\n7. Additional ablation study\nEffect of joint training. In our default setting, we jointly\ntrain the spatial and temporal blocks in the video diffusion\nmodel to fully exploit the complementarity between image\nand video modalities. An alternative strategy is to separate\nthe spatial and temporal modeling into sequential two\nstages. We conduct a comparative experiment on these two\n12\n300\n350\n400\n450\n500\n0M\n2.5M\n5M\n7.5M\n10M\nScaling trend under semi-supervised settings\n300\n350\n400\n450\n500\nNumber of text-free videos\nFVD\n0M\n2.5M\n5.0M\n7.5M\n10.0M\nFigure 9.\nScaling trend under semi-supervised settings.\nIn\nthe experiment, labeled WebVid10M and text-free videos from\nInternal10M are leveraged.\nstrategies in Tab. 9. The results demonstrate the rationality\nof joint optimization in TF-T2V.\nScaling trend under semi-supervised settings. In Fig. 9,\nwe vary the number of text-free videos and explore the scal-\ning trend of TF-T2V under the semi-supervised settings.\nFrom the results, we can observe that FVD (\u2193) gradually\ndecreases as the number of text-free videos increases,\nrevealing the strong scaling potential of our TF-T2V.\n8. More experimental results\nTo further verify that TF-T2V can be extended to high-\ndefinition video generation, we leverage text-free videos\nto train a high-resolution text-to-video model, such as\n896 \u00d7 512. As shown in Fig. 10, we can notice that in\naddition to generating 448 \u00d7 256 videos, our method can\nbe easily applied to the field of high-definition video syn-\nthesis. For high-definition compositional video synthesis,\nwe additionally synthesize 1280 \u00d7 640 and 1280 \u00d7 768\nvideos to demonstrate the excellent application potential of\nour method. The results are displayed in Fig. 11 and Fig. 12.\n9. Limitations and future work\nIn this paper, we only doubled the training set to explore\nscaling trends due to computational resource constraints,\nleaving the scalability of larger scales (such as 10\u00d7 or\n100\u00d7) unexplored. We hope that our approach can shed\nlight on subsequent research to explore the scalability of\nharnessing text-free videos. The second limitation of this\nwork is the lack of exploration of processing long videos.\nIn the experiment, we follow mainstream techniques and\nsample 16 frames from each video clip to train our TF-T2V\nfor fair comparisons. Investigating long video generation\nwith text-free videos is a promising direction. In addition,\nwe find that if the input textual prompts contain some\ntemporal evolution descriptions, such as \u201cfrom right to\nleft\u201d, \u201crotation\u201d, etc., the text-free TF-T2V may fail and\nstruggle to accurately synthesize the desired video. In the\nexperiment, even though we noticed that semi-supervised\nTF-T2V helped alleviate this problem, it is still worth\nstudying and valuable to precisely generate satisfactory\nvideos with high standards that conform to the motion\ndescription in the given text.\n13\n\u201cA water based Axolotl warrior, Pok\u00e9mon inspiration, Pixar animated art style\u201d\nResolution \n448x256\n\u201cElegant Swans On A Misty Lake\u201d\n\u201cman uses a virtual reality glasses against the skyscrapers sky\u201d\n\u201cYoung woman trekking and enjoying in the woods, holidays and traveling concept \u201d\n\u201cMature businesswoman using computer in office\u201d\n\u201cportrait realistic seaside female 25-30 american fashionable\u201d\n\u201cWoman doing fitness exercise at home\u201d\n\u201cBaby inside crib wanting attention. Happy smiling toddler boy\u201d\n\u201cModern interior living room background, a rose bouquet, butterflies and flowers in the style of painting, with a green chair and lights for the table\u201d\n\u201ca frost bulky creature in a snow covered landscape. It wields a large spiky ice club. realistic\u201d\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n896x512\nResolution \n896x512\nResolution \n896x512\nResolution \n896x512\nFigure 10. More video results on text-to-video generation without training on any video-text pairs. Two resolution types of video are\ngenerated, 448 \u00d7 256 and 896 \u00d7 512 respectively.\n14\n\u201c4K footage. Enter to the house in the rocks. Handheld camera\u201d\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n1280x640\nResolution \n1280x768\n\u201cStudent in eyeglasses laughing at camera. Businessman posing at camera outdoors\u201d\n\u201cwaking in the jungle\u201d\n\u201cCharming young girl with flowers.\u201d\n\u201cA woman's hand sets the date on the calendar. Flip the calendar over to a cubic perpetual calendar.\u201d\n\u201cYoung blonde woman with serious expression standing at street\u201d\n\u201cBlue 3D numbers growing up\u201d\nFigure 11. More video results on compositional depth-to-video synthesis without training on any video-text pairs. Three resolution\ntypes of video are generated, 448 \u00d7 256, 1280 \u00d7 640 and 1280 \u00d7 768 respectively.\n15\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n448x256\nResolution \n1280x640\nResolution \n1280x768\n\u201cYoung african american woman talking on the smartphone and using credit card at street\u201d\n\u201cHispanic woman is training on treadmill, talking and flirting with strong smiling man jogging near\u201d\n\u201cLittle planet in deep space. Planet Mercury. Incredibly beautiful space\u201d\n\u201cDepressed young business man suffering from head ache, having painful head feelings due to computer overwork or sedentary work lifestyle.\u201d\n\u201cLA MCU ZO Snow falls as Demag 655 hydraulic shovel moves in open Fox pit mine.\u201d\n\u201cA model with long hair, 30-35 years old, wearing a hat and sunglasses.\u201d\n\u201cPictured Rock National Forest\u201d\nFigure 12. More video results on compositional sketch-to-video synthesis without training on any video-text pairs. Three resolution\ntypes of video are generated, 448 \u00d7 256, 1280 \u00d7 640 and 1280 \u00d7 768 respectively.\n16\n"
  },
  {
    "title": "HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D",
    "link": "https://arxiv.org/pdf/2312.15980.pdf",
    "upvote": "9",
    "text": "HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D\nSangmin Woo1*\nByeongjun Park1*\nHyojun Go2\nJin-Young Kim2\nChangick Kim1\n1KAIST\n2Twelve Labs\n1{smwoo95, pbj3810, changick}@kaist.ac.kr\n2{william, jeremy}@twelvelabs.io\nhttps://byeongjun-park.github.io/HarmonyView/\nInput\nGenerated diverse and multi-view coherent images\nMesh\nInput\nHarmonyView (Ours)\nSyncDreamer [33]\nFigure 1. HarmonyView for one-image-to-3D. HarmonyView generates realistic 3D content using just a single image. It excels at\nmaintaining visual and geometric consistency across generated views while enhancing the diversity of novel views, even in complex scenes.\n*Equal contribution\n1\narXiv:2312.15980v1  [cs.CV]  26 Dec 2023\nAbstract\nRecent progress in single-image 3D generation highlights\nthe importance of multi-view coherency, leveraging 3D pri-\nors from large-scale diffusion models pretrained on Internet-\nscale images. However, the aspect of novel-view diversity\nremains underexplored within the research landscape due\nto the ambiguity in converting a 2D image into 3D con-\ntent, where numerous potential shapes can emerge. Here,\nwe aim to address this research gap by simultaneously ad-\ndressing both consistency and diversity. Yet, striking a bal-\nance between these two aspects poses a considerable chal-\nlenge due to their inherent trade-offs. This work introduces\nHarmonyView, a simple yet effective diffusion sampling\ntechnique adept at decomposing two intricate aspects in\nsingle-image 3D generation: consistency and diversity. This\napproach paves the way for a more nuanced exploration\nof the two critical dimensions within the sampling process.\nMoreover, we propose a new evaluation metric based on\nCLIP image and text encoders to comprehensively assess the\ndiversity of the generated views, which closely aligns with hu-\nman evaluators\u2019 judgments. In experiments, HarmonyView\nachieves a harmonious balance, demonstrating a win-win\nscenario in both consistency and diversity.\n1. Introduction\nHumans can effortlessly imagine the 3D form of an ob-\nject from just a single camera view, drawing upon their\nprior knowledge of the 3D world. Yet, emulating this hu-\nman capability in machines remains a longstanding chal-\nlenge in the field of computer vision [2, 43, 57, 65, 68, 86].\nThe fundamental hurdle lies in the inherent ambiguity of\ndeducing 3D structure from a single 2D image since a\nsingle image essentially collapses the three dimensions\nof the real world into a 2D representation. Consequently,\ncountless 3D configurations of an object can be projected\nonto the same 2D image. This ambiguity has ignited the\nquest for innovative solutions for single-image 3D gen-\neration [1, 25, 27, 30, 31, 33, 35, 46, 51, 53\u201355, 61\u2013\n63, 73, 74, 81, 82, 87, 88].\nOne prevalent strategy is to generate multi-view images\nfrom a single 2D image [31, 32, 61, 72], and process them us-\ning techniques such as Neural Radiance Fields (NeRFs) [39]\nto create 3D representations. Regarding this, recent stud-\nies [32, 33, 61, 72, 81, 82] highlight the importance of main-\ntaining multi-view coherency. This ensures that the generated\n3D objects to be coherent across diverse viewpoints, empow-\nering NeRF to produce accurate and realistic 3D reconstruc-\ntions. To achieve this, researchers harness the capabilities of\nlarge-scale diffusion models [50], particularly those trained\non a vast collection of 2D images. The abundance of 2D\nimages provides a rich variety of views for the same ob-\nject, allowing the model to learn view-to-view relationships\nand acquire geometric priors about the 3D world. On top of\nthis, some works [33, 61] introduce a refinement stage that\nfine-tunes the view alignment to accommodate variations in\ncamera angles. This adjustment is a key factor in achieving\nthe desired multi-view coherency, which directly impacts the\nrealism of the resulting 3D representation. This progress has\nnotably enhanced the utility of the generated 3D contents,\nmaking them more suitable for various applications [45, 75].\nAn equally significant but often overlooked aspect in\nsingle-image 3D generation is the novel-view diversity. The\nill-posed nature of this task necessitates dealing with numer-\nous potential 3D interpretations of a given 2D image. Recent\nworks [32, 33, 61, 71] showcase the potential of creating\ndiverse 3D contents by leveraging the capability of diffu-\nsion models in generating diverse 2D samples. However,\nbalancing the pursuit of consistency and diversity remains\na challenge due to their inherent trade-off: maintaining vi-\nsual consistency between generated multi-view images and\nthe input view image directly contributes to sample quality\nbut comes at the cost of limiting diversity. Although current\nmulti-view diffusion models [33, 61] attempt to optimize\nboth aspects simultaneously, they fall short of fully unravel-\ning their intricacies. This poses a crucial question: Can we\nnavigate towards a harmonious balance between these two\nfundamental aspects in single-image 3D generation, thereby\nunlocking their full potential?\nThis work aims to address this question by introducing\na simple yet effective diffusion sampling technique, termed\nHarmonyView. This technique effectively decomposes the\nintricacies in balancing consistency and diversity, enabling a\nmore nuanced exploration of these two fundamental facets\nin single-image 3D generation. Notably, HarmonyView pro-\nvides a means to exert explicit control over the sampling\nprocess, facilitating a more refined and controlled generation\nof 3D contents. This versatility of HarmonyView is illus-\ntrated in Fig. 1. Our method achieves a harmonious balance,\ndemonstrating mutual benefits in both consistency and di-\nversity. HarmonyView generates geometrically coherent 3D\ncontents that faithfully represent the input image for visible\nparts while also capturing diverse yet plausible modes for\noccluded parts. Another challenge we face is the absence of\nstandardized metrics for assessing the diversity of generated\nmulti-views. To address this gap and provide a more compre-\nhensive assessment of the consistency and diversity of 3D\ncontents, we introduce a novel evaluation metric based on\nboth the CLIP image and text encoders [20, 47].\nIn experiments, we quantitatively compare HarmonyView\nagainst state-of-the-art techniques, spanning two tasks: novel-\nview synthesis and 3D reconstruction. In both tasks, Har-\nmonyView consistently outperforms baseline methods across\nall metrics. Our qualitative results further highlight the effi-\ncacy of HarmonyView, showcasing faithful reconstructions\n2\nwith remarkable visual quality, even in complex scenes.\nMoreover, we show that our proposed metric closely aligns\nwith the assessments made by human evaluators. Lastly, Har-\nmonyView can be seamlessly integrated with off-the-shelf\ntext-to-image diffusion models (e.g., Stable Diffusion [50]),\nenabling it to perform text-to-image-to-3D generation.\n2. Related Work\nLifting 2D pretrained models for 3D generation.\nRecent\nresearch endeavors [3, 29, 36, 55, 63, 67, 71, 74, 88] are cen-\ntered on the idea of lifting 2D pre-trained models [47, 50]\nto create 3D models from textual prompts, without the need\nfor explicit 3D data. The key insight lies in leveraging 3D\npriors acquired by diffusion models during pre-training on\nInternet-scale data. This enables them to dream up novel\n3D shapes guided by text descriptions. DreamFusion [44]\ndistills pre-trained Stable Diffusion [50] using Score Distil-\nlation Sampling (SDS) to extract a Neural Radiance Field\n(NeRF) [39] from a given text prompt. DreamFields [23]\ngenerates 3D models based on text prompts by optimizing\nthe CLIP [47] distance between the CLIP text embedding\nand NeRF [39] renderings. However, accurately representing\n3D details with word embeddings remains a challenge.\nSimilarly, some works [37, 46, 62, 80] extend the distilla-\ntion process to train NeRF for the 2D-to-3D task. NeuralLift-\n360 [80] utilizes a depth-aware NeRF to generate scenes\nguided by diffusion models and incorporates a distillation\nloss for CLIP-guided diffusion prior [47]. Magic123 [46]\nuses SDS loss to train a NeRF and then fine-tunes a mesh\nrepresentation. Due to the reliance on SDS loss, these meth-\nods necessitate textual inversion [15] to find a suitable text\ndescription for the input image. Such a process needs per-\nscene optimization, making it time-consuming and requiring\ntedious parameter tuning for satisfactory quality.\nAnother line of work [31, 32, 61, 72] uses 2D diffusion\nmodels to generate multi-view images then use them for\n3D reconstruction with NeRF [39, 69]. 3DiM [72] views\nnovel-view synthesis as an image-to-image translation prob-\nlem and uses a pose-conditional diffusion model to predict\nnovel views from an input view. Zero-1-to-3 [32] enables\nzero-shot 3D creation from arbitrary images by fine-tuning\nStable Diffusion [50] with relative camera pose. Our work,\nfalling into this category, is able to convert arbitrary 2D im-\nages to 3D without SDS loss [44]. It seamlessly integrates\nwith other frameworks, such as text-to-2D [41, 48, 50] and\nneural reconstruction methods [39, 69], streamlining the\ntext-to-image-to-3D process. Unlike prior distillation-based\nmethods [37, 80] confined to a singular mode, our approach\noffers greater flexibility for generating diverse 3D contents.\nConsistency and diversity in 3D generation.\nThe pri-\nmary challenge in single-image 3D content creation lies in\nmaintaining multi-view coherency. Various approaches [32,\n33, 72, 81, 82] attempt to tackle this challenge: Viewset Dif-\nfusion [61] utilizes a diffusion model trained on multi-view\n2D data to output 2D viewsets and corresponding 3D models.\nSyncDreamer [33] introduces a 3D-aware feature attention\nthat synchronizes intermediate states of noisy multi-views.\nDespite these efforts, achieving complete geometric coher-\nence in generated views remains a challenge.\nOn the other hand, diversity across generated 3D sam-\nples is another critical aspect in single-image 3D genera-\ntion. However, only a few works in the related literature\nspecifically address this issue, often limited to domains such\nas face generation [11] or starting from text for 3D gen-\neration [71]. Recent studies [32, 33, 61, 82] showcase the\npotential of pre-trained diffusion models [50] in generating\ndiverse multi-view images. However, there is still significant\nroom for exploration in balancing consistency and diversity.\nIn our work, we aim to unlock the potential of diffusion mod-\nels, allowing for reasoning about diverse modes for novel\nviews while being faithful to the input view for observable\nparts. We achieve this by breaking down the formulation of\nmulti-view diffusion model into two fundamental aspects:\nvisual consistency with input view and diversity of novel\nviews. Additionally, we propose the CD score to address\nthe absence of a standardized diversity measure in existing\nliterature.\n3. Method\nOur goal is to create a high-quality 3D object from a single\ninput image, denoted as y. To achieve this, we use the diffu-\nsion model [59] to generate a cohesive set of N views at pre-\ndefined viewpoints, denoted as x(1:N)\n0\n= {x(1)\n0 , ..., x(N)\n0\n}.\nThese mutli-view images are then utilized in NeRF-like tech-\nniques [39, 69] for 3D reconstruction. The key to a realistic\n3D object lies in the consistency across the generated views.\nIf they exhibit coherent appearance and geometry, the re-\nsulting 3D object will appear more natural. Therefore, en-\nsuring consistency is crucial for achieving our goal. Recent\nworks [33, 53, 61] address multi-view generation by jointly\noptimizing the distribution of multiple views. Building upon\nthem, we aim to enhance both consistency and diversity by\ndecomposing their formulation during diffusion sampling.\n3.1. Diffusion Models\nWe address the challenge of generating a 3D representation\nfrom a single, partially observed image using diffusion mod-\nels [58, 59]. These models inherently possess the capability\nto capture diverse modes [79], making them well-suited for\nthe task. We adopt the setup of DDPM [22], which defines a\nforward diffusion process transforming an initial data sample\nx0 into a sequence of noisy samples x1, . . . , xT over T steps,\napproximating a Gaussian noise distribution. In practice, we\nperform the forward process by directly transitioning to a\n3\nnoised version of a sample using the equation:\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n(1)\nwhere \u03f5 \u223c N(0, I) is a Gaussian noise, \u00af\u03b1t is a noise sched-\nule monotonically decreasing with timestep t (with \u00af\u03b10 = 1),\nand xt is a noisy version of the input x0 at timestep t.\nThe reverse denoising process \u201cundo\u201d the forward steps\nto recover the original data from noisy observations. Typi-\ncally, this process is learned by optimizing a noise prediction\nmodel \u03f5\u03b8(xt, t) on a data distribution q(x0). DDPM [22]\ndefines the following simple loss:\nLsimple = Ex0\u223cq(x0),\u03f5\u223cN (0,1),t\u223cU[1,T ]\u2225\u03f5 \u2212 \u03f5\u03b8(xt; t)\u22252\n2.\n(2)\n3.2. Multi-view Diffusion Models\nSyncDreamer [33] introduces a multi-view diffusion model\nthat captures the joint distribution of N novel views x(1:N)\n0\ngiven an input view y. This model extends the DDPM for-\nward process (Eq. (1)) by adding random noises indepen-\ndently to each view at every time step:\nx(n)\nt\n= \u221a\u00af\u03b1tx(n)\n0\n+\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5(n).\n(3)\nHere, n denotes the view index. A noise prediction model \u03f5\u03b8\npredicts the noise of the n-th view \u03f5(n), given the condition\nof an input view y, the view difference between the input\nview and the n-th target view \u2206v(n), and noisy multi views\nx(1:N)\nt\n. Hereafter, we define the pair (y, \u2206v(n)) as the ref-\nerence view condition r(n) to simplify notation. Similar to\nEq. (2), the loss for the noise prediction model is defined as:\nL = Ex(1:N)\n0\n,\u03f5(1:N),t\u2225\u03f5(n) \u2212 \u03f5\u03b8(x(n); t, c(n))\u22252\n2,\n(4)\nwhere c(n) = (r(n), x(1:N)\nt\n) and \u03f5(1:N) represents Gaussian\nnoise of size N \u00d7 H \u00d7 W added to all N views.\n3.3. HarmonyView\nDiffusion sampling guidance.\nClassifier-guided diffu-\nsion [12] uses a noise-robust classifier p(l|xt), which es-\ntimates the class label l given a noisy sample xt, to guide\nthe diffusion process with gradients \u2207xt log p(l|xt). This\nclassifier requires bespoke training to cope with high noise\nlevels (where timestep t is large) and to provide meaningful\nsignals all the way through the sampling process. Classifier-\nfree guidance [21] uses a single conditional diffusion model\np\u03b8(x|l) with conditioning dropout, which intermittently re-\nplaces l (typically 10%) with a null token \u03d5 (representing\nthe absence of conditioning information) for unconditional\npredictions. This models an implicit classifier directly from\na diffusion model without the need for an extra classifier\ntrained on noisy input. These conditional diffusion mod-\nels [12, 21] dramatically improve sample quality by enhanc-\ning the conditioning signal but with a trade-off in diversity.\nWhat\u2019s wrong with multi-view diffusion sampling?\nFrom Eq. (4), we derive an unconditional diffusion model\np(x(n)) parameterized by a score estimator \u03f5\u03b8(x(n)\nt\n; t) and\nconditional diffusion model p(x(n)|c(n)) parameterized by\n\u03f5\u03b8(x(n)\nt\n; t, c(n)\nt\n). These two models are learned via a single\nneural network following the classifier-free guidance [21].\nDuring sampling, the multi-view diffusion model adjusts its\nprediction as follows (t is omitted for clarity):\n\u02c6\u03f5\u03b8(x(n)\nt\n; c(n)) = \u03f5\u03b8(x(n)\nt\n; c(n)) + s \u00b7 (\u03f5\u03b8(x(n)\nt\n; c(n)) \u2212 \u03f5\u03b8(x(n)\nt\n)),\n(5)\nwhere s represents a guidance scale.\nThe model output is extrapolated further in the direc-\ntion of \u03f5\u03b8(x(n)\nt\n; c(n)\nt\n) and away from \u03f5\u03b8(x(n)\nt\n). Remind that\nc(n) = (r(n), x(1:N)\nt\n). Thus, the scaling of s affects both\nthe input view condition r(n) and the multi-view condition\nx(1:N)\nt\nsimultaneously. As evidenced by Table 5, increasing\ns encourages multi-view coherency and diversity in the gener-\nated views. Yet, this comes with a trade-off: it simultaneously\ndiminishes the visual consistency with the input view. While\nthe inherent trade-off between these two dimensions is obvi-\nous in this context, managing competing objectives under a\nsingle guidance poses a considerable challenge. In essence,\nthe model tends to generate diverse and geometrically coher-\nent multi-view images, but differ in visual aspects (e.g., color,\ntexture) from the input view, resulting in sub-optimal quality.\nEmpirical observations, shown in Fig. 2 and Table 1, sub-\nstantiate that this formulation manifests a conflict between\nthe objectives of consistency and diversity.\nHarmonizing consistency and diversity.\nTo address the\naforementioned challenge, we introduce a method termed\n\u201cHarmonyView\u201d. Our approach leverages two implicit clas-\nsifiers. One classifier pi(r(n)|x(n)\nt\n, x(1:N)\nt\n) guides the tar-\nget view x(n)\nt\nand multi-views x(1:N)\nt\nto be more visu-\nally consistent with the input view r(n). Another classifier\npi(x(1:N)\nt\n|x(n)\nt\n, r(n)) contains uncertainty in both the target\n(x(1:N)\nt\n) and conditional (x(n)\nt\n) elements. This contributes\nto capturing diverse modes. Together, they synergistically\nguide the synchronization of noisy multi-views x(1:N)\nt\n, facil-\nitating geometric coherency among clean multi-views. Based\non these, we redefine the score estimation as follows:\n\u02dc\u03f5\u03b8(x(n)\nt\n; c(n)) = \u03f5\u03b8(x(n)\nt\n; c(n))\n\u2212 s1\u03c3t\u2207x(n)\nt\nlog pi(r(n)|x(n)\nt\n, x(1:N)\nt\n)\n\u2212 s2\u03c3t\u2207x(n)\nt\nlog pi(x(1:N)\nt\n|x(n)\nt\n, r(n)),\n(6)\nwhere s1 and s2 are guidance scales and \u03c3t is a noise schedul-\ning parameter. By properly balancing these terms, we can\nobtain multi-view coherent images that align well with the\nsemantic content of the input image while being diverse\nacross different samples.\n4\nInput\nNo Guidance\nBaseline (Eq. (5))\nOnly s1\nOnly s2\nOurs (Eq. (9))\nFigure 2. Qualitative comparison of several instantiations for multi-view diffusion guidance on novel-view synthesis. Our decomposition\nof Eq. (5) yields two guidance parameters: s1 for input-target visual consistency and s2 for diversity in the novel views. With these parameters,\nour final formulation Eq. (9) enables the generation of a diverse set of multi-view coherent images that well reflect the input view.\nAccording to Bayes\u2019 rule, pi(r(n)|x(n)\nt\n, x(1:N)\nt\n)\n\u221d\np(x(n)\nt\n|c(n))/p(x(n)\nt\n|x(1:N)\nt\n) and pi(x(1:N)\nt\n|x(n)\nt\n, r(n)) \u221d\np(x(n)\nt\n|c(n))/p(x(n)\nt\n|r(n)). Hence, the diffusion scores of\nthese two implicit classifiers can be derived as follows:\n\u2207x(n)\nt\nlog pi(r(n)|x(n)\nt\n, x(1:N)\nt\n)\n= \u2212 1\n\u03c3t\n(\u03f5\u03b8(x(n)\nt\n; c(n)) \u2212 \u03f5\u03b8(x(n)\nt\n; x(1:N)\nt\n)).\n(7)\n\u2207x(n)\nt\nlog pi(x(1:N)\nt\n|x(n)\nt\n, r(n))\n= \u2212 1\n\u03c3t\n(\u03f5\u03b8(x(n)\nt\n; c(n)) \u2212 \u03f5\u03b8(x(n)\nt\n; r(n)).\n(8)\nFinally, these terms are plugged into Eq. (6) and yields:\n\u02dc\u03f5\u03b8(x(n)\nt\n;c(n)) = \u03f5\u03b8(x(n)\nt\n; c(n))\n+ s1 \u00b7 (\u03f5\u03b8(x(n)\nt\n; c(n)) \u2212 \u03f5\u03b8(x(n)\nt\n; x(1:N)\nt\n)\n+ s2 \u00b7 (\u03f5\u03b8(x(n)\nt\n; c(n)) \u2212 \u03f5\u03b8(x(n)\nt\n; r(n)).\n(9)\nThis formulation effectively decomposes consistency and\ndiversity, offering a nuanced approach that grants control\nover both dimensions. While simple, our decomposition\nachieves a win-win scenario, striking a harmonious balance\nin generating samples that are both consistent and diverse\n(see Fig. 2 and Table 1).\n3.4. Consistency-Diversity (CD) Score\nWe propose the CD score with two key principles: (1) Diver-\nsity of novel views: It is preferable that the generated images\nexhibit diverse and occasionally creative appearances that\nare not easily imaginable from the input image. (2) Seman-\ntic consistency: While pursuing diversity, it is crucial to\nmaintain semantic consistency, i.e., the generated images\nshould retain their semantic content consistently, regardless\nof variations in the camera viewpoint. To operationalize\nthis evaluation, CD score utilizes CLIP [47] image (\u03a8I)\nand text encoders (\u03a8T ), akin to CLIP score [20]. Diversity\nMethod\ns s1 s2 PSNR\u2191 SSIM\u2191 LPIPS\u2193 Eflow\u2193 CD\u2191\nNo Guidance\n20.51\n0.818\n0.144\n2.270\n0.640\nBaseline (Eq. (5)) \u2713\n20.19\n0.819\n0.140\n2.071\n0.717\nOurs (Eq. (9))\n\u2713\n20.32\n0.822\n0.141\n2.136\n0.764\n\u2713\n21.03\n0.828\n0.128\n2.146\n0.668\n\u2713 \u2713\n20.69\n0.825\n0.133\n1.945\n0.792\nTable 1. Ablative study of multi-view diffusion guidance on\nnovel-view synthesis. Metrics measure sample quality with PSNR,\nSSIM, LPIPS; multi-view coherency with Eflow; and diversity\nwith CD score. Our final design strikes the best balance across the\nmetrics. Here, we set s = 1, s1 = 2, s2 = 1.\n(D) measures the average dissimilarity of generated views\n{x(1), . . . , x(N)} from a reference view y, reflecting how\ndistinct the generated images are from the reference view,\nemphasizing creative variations. The diversity is computed\nby averaging the cosine similarity of each generated view\nwith the reference view using CLIP image encoders.\nD = 1\nN\nN\nX\nn=1\nh\n1 \u2212 cos(\u03a8I(y), \u03a8I(x(n)))\ni\n.\n(10)\nSemantic variance (SV ar) quantifies the variance in semantic\nchanges across views. This measures how similar the gen-\nerated images are to a given text prompt, \u201cAn image of\n{OBJECT}.\u201d The semantic variance is calculated by averag-\ning the cosine similarity between the CLIP text embedding\nof the prompt and the CLIP image embedding of each gen-\nerated view, followed by measuring the variance of these\nvalues across views.\n\u00afS = 1\nN\nN\nX\nn=1\ncos(\u03a8T (text), \u03a8I(x(n))),\nSV ar = 1\nN\nN\nX\nn=1\n(cos(\u03a8T (text), \u03a8I(x(n))) \u2212 \u00afS)2.\n(11)\n5\nInput\nHarmonyView\nSyncDreamer [33]\nZero123 [32]\nFigure 3. Novel-view synthesis comparison. HarmonyView generates plausible novel views while preserving coherence across views.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nEflow\u2193\nCD\u2191\nRealfusion [37]\n15.26\n0.722\n0.283\n-\n-\nZero123 [32]\n18.98\n0.795\n0.166\n3.820\n0.628\nSyncDreamer [33]\n20.19\n0.819\n0.140\n2.071\n0.717\nHarmonyView\n20.69\n0.825\n0.133\n1.945\n0.792\nTable 2. Novel-view synthesis on GSO [13] dataset. We report\nPSNR, SSIM, LPIPS, Eflow, and CD score.\nThe CD score is then computed as the ratio of diversity to\nsemantic variances across views:\nCD Score = D/SV ar.\n(12)\nWe note that the CD score is reference-free, i.e., it does not\nrequire any ground truth images to measure the score.\n4. Experiments\nDue to space constraints, we provide detailed information\nregarding implementation details and baselines in Appendix.\nDataset. Following [31\u201333], we used the Google Scanned\nObject (GSO) [13] dataset, adopting the same data split as\nin [33], for our evaluation. In addition, we utilized Internet-\ncollected images, including those curated by [33], to assess\nthe generation ability for complex objects or scenes.\nTasks and metrics. For the novel-view synthesis task, we\nused three standard metrics \u2013 PSNR, SSIM [70], LPIPS [85]\n\u2013 to measure sample quality compared to GT images. We\nmeasured diversity using the CD score. As a multi-view co-\nherency metric, we propose Eflow, which measures the \u21131\ndistance between optical flow estimates from RAFT [64]\nfor both GT and generated images. For the single-view 3D\nreconstruction task, we used Chamfer distance to evaluate\npoint-by-point shape similarity and volumetric IoU to quan-\ntify the overlap between reconstructed and GT shapes.\nMethods\nCD\u2191\nUser Likert Score (1-5)\u2191\nQuality\nConsistency\nDiversity\nZero123 [32]\n0.752\n3.208\n3.167\n2.854\nSyncDreamer [33]\n0.722\n3.417\n3.208\n2.708\nHarmonyView\n0.804\n3.958\n3.479\n3.813\nTable 3. Novel-view synthesis on in-the-wild images. We report\nthe CD score and 5-scale user Likert score, assessing quality, consis-\ntency, and diversity. Notably, the CD score shows strong alignment\nwith human judgments. The test images are collected by [33].\n4.1. Comparative Results\nNovel-view synthesis.\nTable 2 shows the quantitative re-\nsults for novel-view synthesis on the GSO [13] dataset. Here,\nHarmonyView outperforms state-of-the-art methods across\nall metrics. We confirm that HarmonyView generates im-\nages of superior quality, as indicated by PSNR, SSIM and\nLPIPS. It particularly excels in achieving multi-view co-\nherency (indicated by Eflow) and generating diverse views\nthat are faithful to the semantics of the input view (indicated\nby CD score). In Fig. 3, we present the qualitative results.\nZero123 [32] produces multi-view incoherent images or im-\nplausible images, e.g., eyes on the back. SyncDreamer [33]\ngenerates images that lack visual similarity to the input view\nor contain deficiencies, e.g., flatness or hole on the back.\nIn contrast, HarmonyView generates diverse yet plausible\nmulti-view images while maintaining geometric coherence\nacross views. In Table 3, we examine novel-view synthesis\nmethods on in-the-wild images curated by [33]. For evalua-\ntion, we use CD score and user Likert ratings (1 to 5) along\nthree criteria: quality, consistency, and diversity. While Sync-\nDreamer [33] excels in quality and consistency scores when\ncompared to Zero123 [32], Zero123 performs better in diver-\nsity and CD score. Notably, HarmonyView stands out with\n6\nInput\nHarmonyView\nSyncDreamer [33]\nZero123 [32]\nOne-2-3-45 [31]\nPoint-E [42]\nShap-E [26]\nFigure 4. 3D reconstruction comparison. HarmonyView stands out in creating high-quality 3D meshes where other often fails. Har-\nmonyView, SyncDreamer [33], and Zero123 [32] use the vanilla NeuS [69] for 3D reconstruction.\nMethod\nChamfer Dist.\u2193\nVolume IoU\u2191\nRealfusion [37]\n0.0819\n0.2741\nMagic123 [46]\n0.0516\n0.4528\nOne-2-3-45 [31]\n0.0629\n0.4086\nPoint-E [42]\n0.0426\n0.2875\nShap-E [26]\n0.0436\n0.3584\nZero123 [32]\n0.0339\n0.5035\nSyncDreamer [33]\n0.0261\n0.5421\nHarmonyView\n0.0187\n0.6401\nTable 4. 3D reconstruction on GSO [13] dataset. HarmonyView\ndemonstrates substantial improvements over competitive baselines.\nthe highest CD score and superior user ratings. This suggests\nthat HarmonyView effectively produces visually pleasing,\nrealistic, and diverse images while being coherent across\nmultiple views. The correlation between the CD score and\nthe diversity score underscores the efficacy of the CD score\nin capturing the diversity of generated images.\n3D reconstruction.\nIn Table 4, we quantitatively com-\npare our approach against various other 3D generation meth-\nods [26, 31\u201333, 37, 42, 46]. Both our method and SDS-free\nmethods [32, 33] utilize NeuS [69], a neural reconstruction\nmethod for converting multi-view images into 3D shapes. To\nachieve faithful reconstruction of 3D mesh that aligns well\nwith ground truth, the generated multi-view images should be\ngeometrically coherent. Notably, HarmonyView achieves the\nbest results by a significant margin in both Chamfer distance\nand volumetric IoU metrics, demonstrating the proficiency of\nHarmonyView in producing multi-view coherent images. We\nalso present a qualitative comparison in Fig. 4. The results\nshowcase the remarkable quality of HarmonyView. While\ncompeting methods often struggle with incomplete recon-\nstructions (e.g., Point-E, Shap-E), fall short in capturing\nsmall details (e.g., Zero123), and show discontinuities (e.g.,\nMethod\ns\ns1 s2 PSNR\u2191 SSIM\u2191 LPIPS\u2193 Eflow\u2193 CD\u2191\nBaseline (Eq. (5))\n0.5\n-\n-\n20.55\n0.822\n0.137\n2.074\n0.685\n1.0\n-\n-\n20.19\n0.819\n0.140\n2.071\n0.717\n1.5\n-\n-\n19.76\n0.814\n0.146\n2.011\n0.711\nOurs (Eq. (9))\n-\n0.0 1.0\n20.32\n0.822\n0.141\n2.136\n0.764\n-\n1.0 1.0\n20.55\n0.824\n0.135\n2.009\n0.772\n-\n3.0 1.0\n20.73\n0.825\n0.132\n1.950\n0.737\n-\n2.0 0.0\n21.03\n0.828\n0.128\n2.146\n0.668\n-\n2.0 0.6\n20.90\n0.827\n0.130\n1.996\n0.770\n-\n2.0 0.8\n20.80\n0.826\n0.131\n2.009\n0.774\n-\n2.0 1.2\n20.56\n0.824\n0.135\n1.996\n0.760\n-\n2.0 1.0\n20.69\n0.825\n0.133\n1.945\n0.792\nTable 5. Guidance scale study on novel-view synthesis. We com-\npare two instantiations of multi-view diffusion guidance: Eq. (5)\nand Eq. (9). Our approach consistently outperforms the baseline.\nIncreasing s1 tends to enhance PSNR, SSIM, and LPIPS, while\nhigher s2 tends to improve CD score. Notably, the combined effect\nof s1 and s2 synergistically improves Eflow.\nSyncDreamer) or artifacts (e.g., One-2-3-45), our method\nproduces high-quality 3D meshes characterized by accurate\ngeometry and a realistic appearance.\n4.2. Analysis\nScale study.\nIn Table 5, we investigate two instantiations\nof multi-view diffusion guidance with different scale config-\nurations: baseline (Eq. (5)) and our approach (Eq. (9)). As\ns increases from 0.5 to 1.5 in the baseline method, Eflow\n(indicating multi-view coherency) and CD score (indicating\ndiversity) show an increasing trend. Simultaneously, PSNR,\nSSIM, and LPIPS (indicating visual consistency) show a\ndeclining trend. This implies a trade-off between visual con-\nsistency and diversity. In contrast, our method involves pa-\nrameters s1 and s2. We observe that increasing s1 provides\nstronger guidance in aligning multi-view images with the\ninput view, leading to direct improvements in PSNR, SSIM,\n7\nInput\nHarmonyView\nSyncDreamer [33]\nFigure 5. 3D reconstruction for complex object or scene. HarmonyView successfully reconstructs the details, while SyncDreamer fails.\nAn astronaut \nriding a horse\nA cute panda \nriding a car\nA boxer toy \nmade of wood\nInput text\nText to image\nGenerated images\nMesh\nFigure 6. Text-to-Image-to-3D. HarmonyView, when combined with text-to-image frameworks [41, 48, 50], enables text-to-3D.\nand LPIPS. Keeping s1 fixed at 2.0, elevating s2 tends to\nyield improved CD score, indicating an enhanced diversity\nin the generated images. However, given the inherent conflict\nbetween consistency and diversity, an increase in s2 intro-\nduces a trade-off. We note that our approach consistently out-\nperforms the baseline across various configurations, striking\na nuanced balance between consistency and diversity. Essen-\ntially, our decomposition provides more explicit control over\nthose two dimensions, enabling a better balance. Addition-\nally, the synergy between s1 and s2 notably enhances Eflow,\nleading to improved 3D alignment across multiple views.\nGeneralization to complex objects or scenes.\nEven in\nchallenging scenarios, either with a highly detailed single ob-\nject or multiple objects within a single scene, HarmonyView\nexcels at capturing intricate details that SyncDreamer [33]\nmight miss. The results are shown in Fig. 5. Our model well\ngenerates multi-view coherent images even in such scenar-\nios, enabling the smooth reconstruction of natural-looking\nmeshes without any discontinuities.\nCompatibility with text-to-image models.\nHarmonyView\nseamlessly integrates with off-the-shelf text-to-image mod-\nels [48, 50]. These models convert textual descriptions into\n2D images, which our model further transforms into high-\nquality multi-view images and 3D meshes. Visual examples\nare shown in Fig. 6. Notably, our model excels in captur-\ning the essence or mood of the given 2D image, even man-\naging to create plausible details for occluded parts. This\ndemonstrates strong generalization capability, allowing it to\nperform well even with unstructured real-world images.\nRuntime.\nHarmonyView generates 64 images (i.e., 4 in-\nstances \u00d7 16 views) in only one minute, with 50 DDIM [59]\nsampling steps on an 80GB A100 GPU. Despite the ad-\nditional forward pass through the diffusion model, Har-\nmonyView takes less runtime than SyncDreamer [33], which\nrequires about 2.7 minutes with 200 DDIM sampling steps.\nAdditional results & analysis.\nPlease see Appendix for\nmore qualitative examples and analysis on the CD score, etc.\n5. Conclusion\nIn this study, we have introduced HarmonyView, a simple yet\neffective technique that adeptly balances two fundamental\naspects in a single-image 3D generation: consistency and\ndiversity. By providing explicit control over the diffusion\n8\nsampling process, HarmonyView achieves a harmonious\nequilibrium, facilitating the generation of diverse yet plausi-\nble novel views while enhancing consistency. Our proposed\nevaluation metric CD score effectively measures the diversity\nof generated multi-views, closely aligning with human eval-\nuators\u2019 judgments. Experiments show the superiority of Har-\nmonyView over state-of-the-art methods in both novel-view\nsynthesis and 3D reconstruction tasks. The visual fidelity and\nfaithful reconstructions achieved by HarmonyView highlight\nits efficacy and potential for various applications.\nReferences\n[1] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Generative\nnovel view synthesis with 3d-aware diffusion models. arXiv\npreprint arXiv:2304.02602, 2023. 2\n[2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis\nSavva, Shuran Song, Hao Su, et al. Shapenet: An information-\nrich 3d model repository. arXiv preprint arXiv:1512.03012,\n2015. 2, 13\n[3] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d: Disentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 3\n[4] Zhiqin Chen. A review of deep learning-powered mesh recon-\nstruction methods. arXiv preprint arXiv:2303.02879, 2023.\n13\n[5] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5939\u20135948, 2019. 13\n[6] Zhiqin Chen and Hao Zhang. Neural marching cubes. ACM\nTransactions on Graphics (TOG), 40(6):1\u201315, 2021. 13\n[7] Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net:\nGenerating compact meshes via binary space partitioning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 45\u201354, 2020. 13\n[8] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4456\u20134465, 2023. 13\n[9] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A unified approach for\nsingle and multi-view 3d object reconstruction. In Computer\nVision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part\nVIII 14, pages 628\u2013644. Springer, 2016. 13\n[10] Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien\nBouaziz, Geoffrey Hinton, and Andrea Tagliasacchi. Cvxnet:\nLearnable convex decomposition.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 31\u201344, 2020. 13\n[11] Rahul Dey and Vishnu Naresh Boddeti. Generating diverse\n3d reconstructions from a single occluded face image. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 1547\u20131557, 2022. 3\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021. 4\n[13] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items. In 2022 In-\nternational Conference on Robotics and Automation (ICRA),\npages 2553\u20132560. IEEE, 2022. 6, 7, 16\n[14] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set\ngeneration network for 3d object reconstruction from a single\nimage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 605\u2013613, 2017. 13\n[15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022. 3\n[16] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\nand Thomas Funkhouser. Local deep implicit functions for\n3d shape. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4857\u20134866,\n2020. 13\n[17] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh\nr-cnn. In Proceedings of the IEEE/CVF international confer-\nence on computer vision, pages 9785\u20139795, 2019. 13\n[18] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C\nRussell, and Mathieu Aubry. A papier-m\u02c6ach\u00b4e approach to\nlearning 3d surface generation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n216\u2013224, 2018. 13\n[19] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and\nBarlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured\nmesh generation. arXiv preprint arXiv:2303.05371, 2023. 13\n[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation metric\nfor image captioning. arXiv preprint arXiv:2104.08718, 2021.\n2, 5\n[21] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3, 4\n[23] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 867\u2013876,\n2022. 3\n[24] Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar,\nRichard Tucker, Michael Krainin, Dominik Kaeser, William T\nFreeman, David Salesin, Brian Curless, et al. Slide: Single\nimage 3d photography with soft layering and depth-aware\ninpainting. In Proceedings of the IEEE/CVF International\n9\nConference on Computer Vision, pages 12518\u201312527, 2021.\n13\n[25] Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song,\nZhangyang Wang, and Liangliang Cao. Efficient-3dim: Learn-\ning a generalizable single-image novel-view synthesizer in\none day. arXiv preprint arXiv:2310.03015, 2023. 2\n[26] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional\n3d implicit functions. arXiv preprint arXiv:2305.02463, 2023.\n7, 13, 17\n[27] Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky,\nRiza Alp Guler, Jian Ren, Sergey Tulyakov, and Igor\nGilitschenski.\ninvs: Repurposing diffusion inpainters for\nnovel view synthesis. arXiv preprint arXiv:2310.16167, 2023.\n2\n[28] Yiyi Liao, Simon Donne, and Andreas Geiger. Deep marching\ncubes: Learning explicit surface representations. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2916\u20132925, 2018. 13\n[29] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-\nto-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 3\n[30] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,\nYachao Zhang, and Xiu Li. Consistent123: One image to\nhighly consistent 3d asset using case-aware diffusion priors.\narXiv preprint arXiv:2309.17261, 2023. 2\n[31] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh\nin 45 seconds without per-shape optimization. arXiv preprint\narXiv:2306.16928, 2023. 2, 3, 6, 7, 13, 17\n[32] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 9298\u2013\n9309, 2023. 2, 3, 6, 7, 13, 14, 15, 16, 17\n[33] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gener-\nating multiview-consistent images from a single-view image.\narXiv preprint arXiv:2309.03453, 2023. 1, 2, 3, 4, 6, 7, 8, 13,\n14, 15, 16, 17\n[34] Zhen Liu, Yao Feng, Michael J Black, Derek Nowrouzezahrai,\nLiam Paull, and Weiyang Liu.\nMeshdiffusion: Score-\nbased generative 3d mesh modeling.\narXiv preprint\narXiv:2303.08133, 2023. 13\n[35] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc\nHabermann, Christian Theobalt, et al. Wonder3d: Single\nimage to 3d using cross-domain diffusion. arXiv preprint\narXiv:2310.15008, 2023. 2\n[36] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-\nYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized\ntext-to-3d object synthesis. arXiv preprint arXiv:2306.07349,\n2023. 3\n[37] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and An-\ndrea Vedaldi. Realfusion: 360deg reconstruction of any object\nfrom a single image. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8446\u20138455, 2023. 3, 6, 7, 13\n[38] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea\nVedaldi. Pc2: Projection-conditioned point cloud diffusion\nfor single-image 3d reconstruction. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12923\u201312932, 2023. 13\n[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n3\n[40] Fangzhou Mu, Jian Wang, Yicheng Wu, and Yin Li. 3d photo\nstylization: Learning to generate stylized novel views from a\nsingle image. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16273\u2013\n16282, 2022. 13\n[41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 3, 8\n[42] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 7, 13, 17\n[43] Byeongjun Park, Hyojun Go, and Changick Kim. Bridging\nimplicit and explicit geometric transformations for single-\nimage view synthesis. arXiv preprint arXiv:2209.07105, 2022.\n2\n[44] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 3, 13\n[45] Simon Prince, Adrian David Cheok, Farzam Farbiz, Todd\nWilliamson, Nikolas Johnson, Mark Billinghurst, and Hi-\nrokazu Kato. 3d live: Real time captured content for mixed\nreality. In Proceedings. International Symposium on Mixed\nand Augmented Reality, pages 7\u2013317. IEEE, 2002. 2\n[46] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Ali-\naksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov,\nPeter Wonka, Sergey Tulyakov, et al. Magic123: One image\nto high-quality 3d object generation using both 2d and 3d\ndiffusion priors. arXiv preprint arXiv:2306.17843, 2023. 2,\n3, 7, 13\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 3, 5\n[48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n3, 8\n[49] Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\n10\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE transactions on pattern analysis and machine\nintelligence, 2020. 13\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 2, 3, 8, 13\n[51] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,\nHong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry La-\ngun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-\ndegree view synthesis from a single real image. arXiv preprint\narXiv:2310.17994, 2023. 2\n[52] Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos\nKalogerakis, Siddhartha Chaudhuri, and Radom\u00b4\u0131r M\u02c7ech.\nParsenet: A parametric surface fitting network for 3d point\nclouds. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart VII 16, pages 261\u2013276. Springer, 2020. 13\n[53] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu.\nZero123++: a single image to consistent multi-view\ndiffusion base model. arXiv preprint arXiv:2310.15110, 2023.\n2, 3\n[54] Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao\nQi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang,\nand Heung-Yeung Shum. Toss: High-quality text-guided\nnovel view synthesis from a single image. arXiv preprint\narXiv:2310.10644, 2023.\n[55] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gener-\nation. arXiv preprint arXiv:2308.16512, 2023. 2, 3\n[56] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\nHuang. 3d photography using context-aware layered depth\ninpainting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8028\u20138038,\n2020. 13\n[57] Ayan Sinha, Asim Unmesh, Qixing Huang, and Karthik Ra-\nmani.\nSurfnet: Generating 3d shape surfaces using deep\nresidual networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 6040\u20136049,\n2017. 2, 13\n[58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n3\n[59] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 3, 8\n[60] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang,\nChengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, and\nWilliam T Freeman. Pix3d: Dataset and methods for single-\nimage 3d shape modeling. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n2974\u20132983, 2018. 13\n[61] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d genera-\ntive models from 2d data. arXiv preprint arXiv:2306.07881,\n2023. 2, 3\n[62] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 3\n[63] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\nview image generation with correspondence-aware diffusion.\narXiv preprint arXiv:2307.01097, 2023. 2, 3\n[64] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In European conference on com-\nputer vision, pages 402\u2013419. Springer, 2020. 6\n[65] Shubham Tulsiani, Abhishek Kar, Joao Carreira, and Jitendra\nMalik. Learning category-specific deformable 3d models for\nobject reconstruction. IEEE transactions on pattern analysis\nand machine intelligence, 39(4):719\u2013731, 2016. 2, 13\n[66] Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Jiten-\ndra Malik. Multi-view supervision for single-view reconstruc-\ntion via differentiable ray consistency. In Proceedings of the\nIEEE conference on computer vision and pattern recognition,\npages 2626\u20132634, 2017. 13\n[67] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and\nGreg Shakhnarovich. Score jacobian chaining: Lifting pre-\ntrained 2d diffusion models for 3d generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12619\u201312629, 2023. 3\n[68] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\nmodels from single rgb images. In Proceedings of the Euro-\npean conference on computer vision (ECCV), pages 52\u201367,\n2018. 2, 13\n[69] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 3, 7, 13\n[70] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 6\n[71] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3\n[72] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.\nNovel view synthesis with diffusion models. arXiv preprint\narXiv:2210.04628, 2022. 2, 3\n[73] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang,\nCL Chen, and Lei Zhang. Consistent123: Improve consis-\ntency for one image to 3d object synthesis. arXiv preprint\narXiv:2310.08092, 2023. 2\n[74] Zhenzhen Weng, Zeyu Wang, and Serena Yeung. Zeroavatar:\nZero-shot 3d avatar generation from a single image. arXiv\npreprint arXiv:2305.16411, 2023. 2, 3\n[75] Sunu Wibirama, Paulus Insap Santosa, Putu Widyarani,\nNanda Brilianto, and Wina Hafidh. Physical discomfort and\n11\neye movements during arbitrary and optical flow-like motions\nin stereo 3d contents. Virtual Reality, 24(1):39\u201351, 2020. 2\n[76] Jiajun Wu, Chengkai Zhang, Xiuming Zhang, Zhoutong\nZhang, William T Freeman, and Joshua B Tenenbaum. Learn-\ning shape priors for single-view 3d completion and recon-\nstruction. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 646\u2013662, 2018. 13\n[77] Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Baoquan\nChen. Pq-net: A generative part seq2seq network for 3d\nshapes. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 829\u2013838,\n2020. 13\n[78] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Un-\nsupervised learning of probably symmetric deformable 3d\nobjects from images in the wild.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern recog-\nnition, pages 1\u201310, 2020. 13\n[79] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling\nthe generative learning trilemma with denoising diffusion\ngans. arXiv preprint arXiv:2112.07804, 2021. 3\n[80] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4479\u20134489, 2023. 3\n[81] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-\ndong Li. Consistnet: Enforcing 3d consistency for multi-view\nimages diffusion. arXiv preprint arXiv:2310.10343, 2023. 2,\n3\n[82] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng\nWang. Consistent-1-to-3: Consistent image to 3d view syn-\nthesis via geometry-aware diffusion models. arXiv preprint\narXiv:2310.03020, 2023. 2, 3\n[83] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 4471\u20134480, 2019. 13\n[84] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,\nOr Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point\ndiffusion models for 3d shape generation. arXiv preprint\narXiv:2210.06978, 2022. 13\n[85] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n586\u2013595, 2018. 6\n[86] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh\nTenenbaum, Bill Freeman, and Jiajun Wu. Learning to re-\nconstruct shapes from unseen classes. Advances in neural\ninformation processing systems, 31, 2018. 2, 13\n[87] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling\nview-conditioned diffusion for 3d reconstruction. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12588\u201312597, 2023. 2\n[88] Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang,\nYing Shan, and Song-Hai Zhang.\nSparse3d: Distilling\nmultiview-consistent diffusion for object reconstruction from\nsparse views. arXiv preprint arXiv:2308.14078, 2023. 2, 3\n12\nA. More Related Work\nThe challenge of one-image 3D generation has recently at-\ntracted significant attention, with various approaches and\nmethods proposed to address this complex problem [4]. In\nthis section, we provide a brief review of the literature.\nClassical 3D generative methods.\nEarly works can be\nbroadly categorized into two main groups: primitive-based\napproaches and depth estimation approaches. Primitive-\nbased approaches [18, 65, 68], focus on the fitting of prim-\nitive 3D shapes to 2D images, seeking to align synthetic\nmodels with observed image features. They often employ it-\nerative optimization to refine the pose and shape of the model\nuntil a satisfactory fit is achieved. On the other hand, depth\nestimation approaches [78, 86] typically follow a two-step\nprocess: They first use a monocular depth estimator (e.g.,\nMiDaS [49]) to predict the 3D geometry, which is then used\nto render artistic effects through multi-plane images [24, 56]\nor point clouds [40]. To address imperfections, a pre-trained\ninpainting model [83] is often applied to fill in missing holes.\nHowever, these early approaches may struggle with general-\nization to real-world data or new object categories.\n3D native models.\nA line of research [7, 9, 10, 16, 18,\n57, 76] follows an encoder-decoder framework for mod-\neling the image-to-3D data distribution, which involves\nthe use of global shape latent codes to directly encode\nthe shape information from 3D assets (e.g., ShapeNet [2],\nPix3D [60]). In contrast, other works utilize local features\nand representation-specific 3D generative models that lever-\nage priors constructed from 3D primitives in various for-\nmats: point clouds [14, 18, 38, 77, 84], voxels [5, 8, 9, 66],\nmeshes [6, 17, 28, 34, 68], or parametric surfaces [19, 52].\nWhile these 3D native models show impressive performance,\nthey often require extensive 3D data and are constrained to\nspecific object classes within that data. They also suffer from\nquality degradation when handling real-world images due to\ndomain disparities. Recently, Point-E [42] and Shap-E [26]\npropose learning text-to-3D diffusion models on large-scale\n3D assets to mitigate some of these limitations.\nB. Additional Experimental Setup\nDiversity evaluation.\nDue to the inherent stochastic na-\nture of diffusion models, the outputs they generate can be\ndifferent w.r.t. the random seed used for their generation.\nTherefore, the computed metrics can differ depending on the\nseed we use. To evaluate the diversity of generated samples\nfrom each model, we randomly sample 4 instances using\ndifferent random seeds from the same input image. We then\nuse the CD score to quantify the diversity. By calculating\nthe CD score across these sampled instances (each derived\nfrom a different random seed but originating from the sample\ninput images), we obtain an average CD score. This average\nCD score represents the overall dissimilarity or diversity\nobserved among the generated samples. The reported values\nin the main paper are the average CD score calculated across\nthese sampled instances.\nTechnical details.\nHarmonyView is built upon the pre-\ntrained models of SyncDreamer [33], which generates a set\nof N = 16 multi-view images, each with an elevation of 30\u25e6\nand azimuths evenly distributed in the range of [0\u25e6, 360\u25e6].\nWe assume that the azimuth of both the input view and\nthe first target view is set to 0\u25e6. The viewpoint differences\n\u2206v(n) are calculated based on the differences in elevation\nand azimuth between the input view and target view. At test\ntime, similar to [32, 33, 37, 46], we estimate an elevation\nangle and use it as an input. To reconstruct the 3D mesh, we\nuse foreground masks for generated images using CarveKit1,\nand train the NeuS [69] for 2k steps. For text-to-image-to-3D,\n2D images from the input text are created with the assistance\nof DALL-E-32.\nBaselines.\nIn our work, we employ several state-of-the-art\nmethods as baseline models: Zero123 [32], RealFusion [37],\nMagic123 [46], One-2-3-45 [31], Point-E [42], Shap-E [26],\nand SyncDreamer [33]. Zero123 [32] is able to generate\nnovel-view images of an object from various viewpoints\ngiven a single-view image. Moreover, its integration with\nthe SDS loss [44] bolsters its capability for 3D reconstruc-\ntion from single-view images. RealFusion [37] leverages\nStable Diffusion [50] and the SDS loss for achieving high-\nquality single-view reconstruction. Magic123 [46] builds\nupon the strengths of Zero123 [32] and RealFusion [37],\nresulting in a method that further improves the overall qual-\nity of 3D reconstruction. One-2-3-45 [31] takes a direct\napproach by regressing Signed Distance Functions (SDFs)\nfrom the output images of Zero123 [32]. Point-E [42] and\nShap-E [26] represent 3D generative models trained on an\nextensive 3D dataset. Both models exhibit the capability\nto convert a single-view image into either a point cloud or\na shape encoded in an MLP. SyncDreamer [33] produces\nmulti-view coherent images from a single-view image by\nsynchronizing intermediate states of generated images using\na 3D-aware feature attention mechanism.\nC. Correlation between CD score and Human\nEvaluation\nTo assess the efficacy of HarmonyView against Sync-\nDreamer [33] and Zero123 [32], we conducted a user study\nwhere participants rated the three approaches using a 5-point\nLikert-scale (1-5), evaluating (a) Quality, (b) Consistency,\nand (c) Diversity. Our user study, showcased in Fig. 7, reveals\na consistent alignment between the CD Score (CD Score =\n1https://github.com/OPHoperHPO/image-background-remove-tool\n2https://cdn.openai.com/papers/dall-e-3.pdf\n13\n(a) Quality\n(b) Consistency\n(c) Diversity\nFigure 7. User evaluation examples. We perform a user study to evaluate the effectiveness of our approach, HarmonyView, in comparison\nto SyncDreamer [33] and Zero123 [32]. Participants were asked to rate the three approaches using a 5-point Likert-scale (1-5), assessing (a)\nQuality, (b) Consistency, and (c) Diversity.\nD/SV ar) and human evaluation metrics. Throughout the\nstudy, we observed that the CD Score reliably reflects the\ncorrelation between two key factors: SV ar, measuring the\ndiversity in generated images\u2019 alignment with a given text\nprompt, and D, evaluating creative variation against a refer-\nence view using CLIP image encoders.\n1. Semantic Variance (SV ar) and Consistency: Lower Se-\nmantic Variance consistently corresponds to higher consis-\ntency in human evaluation. In simpler terms, when the gen-\nerated images are more aligned in their interpretation of the\ntext prompt, human evaluators tend to agree more on the\nperceived consistency. This correlation implies that there\u2019s a\nnegative relationship between Semantic Variance and Con-\nsistency \u2014 lower variance often leads to higher agreement\namong evaluators.\n2. Diversity Score (D) and Quality Perception: Higher\nDiversity Scores tend to lead to lower quality perceptions in\nhuman evaluation. This suggests a somewhat negative corre-\nlation between Diversity Score and Quality Perception. Put\ndifferently, when the diversity among the generated images\nis higher \u2014 meaning they deviate more from the reference\nimage \u2014 human evaluators tend to perceive lower quality.\nConversely, higher similarity between the generated images\nand the reference image correlates with higher perceived\nquality. In essence, when the visual similarity between the\ninput and target views is higher, the quality tends to be per-\nceived as better by human evaluators.\nThese findings collectively underscore the critical balance\nneeded between semantic diversity and adherence to the\nreference image in the pursuit of generating high-quality\nimages aligned with text prompts. Achieving this delicate\nequilibrium is pivotal to ensure that generated images are\ndiverse enough to capture different interpretations while also\nbeing faithful enough to the reference to maintain perceived\n14\nInput\nonly s1\nonly s2\nBoth\nFigure 8. Qualitative ablation study on novel-view synthesis. Our HarmonyView guides the multi-view diffusion process with two\nparameters, s1 and s2 (see Eq. (9)). The nuanced interplay between s1 and s2 impacts consistency and diversity throughout the generation\nprocess. By skillfully balancing these guiding principles, we can achieve a win-win scenario: generate diverse images that maintain coherence\nacross multiple views and stay faithful to the input view.\nquality. HarmonyView demonstrated the highest CD score\ncompared to SyncDreamer [33] and Zero123 [32], indicating\nthat our generated images strike a winning balance between\nconsistency and diversity, excelling in both aspects of fidelity\nto the reference image and semantic variation.\nD. Additional Results\nD.1. Novel-view Synthesis\nQualitative ablation study.\nOur HarmonyView decom-\nposes multi-view diffusion guidance into two distinct guid-\nance components (see Eq. (9)): s1 primarily serves to ensure\nvisual consistency between the input and target views, while\ns2 focuses on amplifying diversity across novel viewpoints.\nThe significance of this approach is showcased in Fig. 8,\nwhere we visually demonstrate how each guidance factor\ninfluences the synthesized images. When prioritizing s1, the\nquality of synthesis improves significantly as it focuses on\naligning the visual consistency between the input and target\nviews. However, in specific cases, like the deer sample, it\ngenerates multiple faces of the deer, leading to what\u2019s known\nas the \u201cJanus problem\u201d \u2014 creating facial features on the rear\nside akin to the front, causing visual anomalies. On the other\nhand, emphasizing s2 results in increased diversity across\nthe generated samples. However, a fundamental trade-off\nexists between these two aspects \u2014 quality and diversity \u2014\nmaking it challenging to optimize for both simultaneously.\nYet, by employing both s1 and s2 in tandem, we can achieve\na win-win scenario. This division allows us to precisely dis-\ncern the impact of each guidance factor on the generation\nprocess. By skillfully balancing these guiding principles, our\nmethod becomes empowered to generate a rich and varied\n15\nInput\nHarmonyView\nSyncDreamer [33]\nZero123 [32]\nFigure 9. Additional novel-view synthesis comparison. HarmonyView creates diverse, coherent multi-view images for complex scenes,\neffortlessly generating realistic front views from rear-view input images.\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nEflow\u2193\nBest\nAvg.\nVar.\nBest\nAvg.\nVar.\u2217\nBest\nAvg.\nVar.\u2217\nBest\nAvg.\nVar.\nZero123 [32]\n18.98\n18.79\n0.048\n0.795\n0.792\n1.003\n0.166\n0.170\n2.025\n3.820\n4.185\n0.197\nSyncDreamer [33]\n20.19\n19.74\n0.242\n0.819\n0.813\n4.465\n0.140\n0.148\n7.922\n2.071\n2.446\n0.458\nHarmonyView\n20.69\n20.24\n0.260\n0.825\n0.819\n5.295\n0.133\n0.140\n8.038\n1.945\n2.350\n0.510\nTable 6. Statistical analysis of novel-view synthesis on GSO [13] dataset. We report PSNR, SSIM, LPIPS, and Eflow for the best-matched\ninstance with GT, as well as the average and variance across four instances. The variances marked as \u2217 are reported with scaling by 10\u22125.\narray of images, exhibiting both multi-view coherence and\nfidelity to the input view.\nQualitative comparison.\nFigure 9 provides a glimpse into\nthe capabilities and limitations of different novel-view syn-\nthesis methods. Zero123 [32] frequently generates images\nthat lack coherence across multiple viewpoints. These syn-\nthesized images often contain implausible variations, such\nas alterations in the number of cymbals or trees based on\nthe view, or even changes in the shape of eyes. These incon-\nsistencies underscore the struggle of Zero123 to maintain\ncoherence and realism across different perspectives, lead-\ning to discrepancies that compromise the overall quality of\nmulti-view synthesis. SyncDreamer [33] faces challenges\nin preserving the expected visual similarity across different\nviewpoints. The generated images often display deviations\nin overall size, empty or missing regions, or distorted forms,\nleading to an overall loss of visual completeness and integrity.\nInstances where facial features are erased or distorted repre-\nsent the difficulties SyncDreamer encounters in maintaining\nthe visual fidelity expected across diverse views. In stark\ncontrast, HarmonyView stands out for its ability to generate\ndiverse yet plausible multi-view images while preserving\ngeometric coherence across these views. Unlike its counter-\nparts, HarmonyView maintains a harmonious relationship\nbetween different views, ensuring the consistent appearance,\nshapes, and elements of objects. In addition, HarmonyView\ncan extrapolate realistic frontal views from the rear-view\ninput image (see third sample). This further underscores the\nversatility and robustness of HarmonyView. Overall, Har-\nmonyView is able to generate a diverse set of images while\nmaintaining a sense of realism and coherence across the\nmultiple views.\nStatistical analysis.\nIn Table 6, we conduct a compre-\nhensive statistical analysis on the GSO [13] dataset, eval-\nuating the performance of three methods: HarmonyView,\nZeor123 [32], and SyncDreamer [33]. We report PSNR,\nSSIM, LPIPS, and Eflow for the best-matched instance with\nground truth, as well as the average and variance across four\ninstances. Upon comparison, HarmonyView demonstrates\nsuperior performance across all metrics when compared to\nZeor123 and SyncDreamer. It attains the highest scores in\nPSNR and SSIM, indicating better image quality in terms\nof both fidelity and structural similarity when compared to\nthe ground truth. Moreover, HarmonyView also exhibits the\n16\nInput\nHarmonyView\nSyncDreamer [33]\nZero123 [32]\nOne-2-3-45 [31]\nPoint-E [42]\nShap-E [26]\nFigure 10. Additional 3D reconstruction comparison. HarmonyView excels at generating high-fidelity 3D meshes that achieve precise\ngeometry with a realistic appearance while sidestepping common pitfalls for comprehensive and captivating reconstructions.\nlowest LPIPS and Eflow scores, signifying reduced percep-\ntual differences and flow errors when matched against the\nground truth. Interestingly, HarmonyView shows higher vari-\nability (indicated by larger variance values) across instances\ncompared to other methods. This variability might imply\nthat while HarmonyView generally performs well, its per-\nformance might fluctuate more across different instances\nor scenarios compared to the Zero123 and SyncDreamer.\nNevertheless, it is essential to note that this variability in\nperformance also reflects its diversity in samples. This could\nimply that while HarmonyView showcases a broader range\nof outputs, it still maintains a high level of image quality.\nD.2. 3D Reconstruction\nIn Fig. 10, the results exemplify HarmonyView\u2019s exceptional\nquality compared to other methods evaluated for 3D recon-\nstruction. While contrasting with competing methods, it is\nevident that these approaches encounter various challenges\nin handling the reconstruction process. For instance, both\nPoint-E [42] and Shap-E [26] struggle significantly with in-\ncomplete reconstructions, failing to capture the entirety of\nthe intended 3D shapes. This deficiency results in recon-\nstructions that lack certain crucial elements, undermining\nthe fidelity of the output. In the case of One-2-3-45 [31], the\nmethod exhibits a tendency to produce ambiguous shapes,\nfailing to accurately represent the intended shape contours.\nFurthermore, Zero123 [32] faces difficulties in capturing\nfine elements within the reconstructed shapes, which di-\nminishes the overall fidelity and detail level of the output.\nSyncDremaer [33] also shows discontinuities or holes within\nthe generated 3D meshes. These imperfections detract from\nthe coherence and completeness of the reconstructed shape.\nIn contrast, HarmonyView produces high-quality 3D meshes\nthat achieve accurate geometry while maintaining a realistic\nappearance. Its ability to circumvent the pitfalls experienced\nby other methods speaks volumes about its capability to\ngenerate comprehensive, detailed, and visually compelling\nreconstructions.\nE. Discussion\nE.1. Limitations & Future Work\nWhile HarmonyView demonstrates promising results in en-\nhancing both visual consistency and novel-view diversity\nin single-image 3D content generation, several limitations\nwarrant further investigation. Firstly, our multi-view diffu-\nsion formulation somewhat mitigates inherent trade-offs be-\ntween consistency and diversity to achieve a certain level\nof Pareto optimality. However, the complete separation of\nthese aspects to eliminate the trade-off entirely remains a\nchallenging pursuit. Secondly, HarmonyView\u2019s current focus\nprimarily revolves around object-centric scenes. This poses\nlimitations when dealing with complex scenarios involving\nmultiple interacting objects, varying scales, and intricate\ngeometries. Expanding the technique to encompass such di-\nverse and intricate scenes demands innovative approaches\nthat account for object interactions, spatial relationships,\nand contextual understanding within the scene. Moreover,\n17\nour current setting typically involves single objects with-\nout backgrounds, simplifying the requirements for realism\nand diversity. The ignorance of background significantly re-\nduces the expectations of synthesizing diverse images. To\naccommodate in-the-wild multi-object scenes with complex\nbackgrounds, HarmonyView requires the use of an external\nbackground removal tool (e.g., CarveKit). Addressing these\nlimitations effectively presents ample opportunities for in-\nnovation and refinement within the field. Exploring these\navenues promises to advance the field towards more com-\nprehensive and realistic 3D content generation from single\nimages.\nE.2. Ethical Considerations\nThe advancements in one-image-to-3D bring forth several\nethical considerations that demand careful attention. One\nkey concern is the potential misuse of generated 3D content.\nThese advancements could be exploited to create deceptive\nor misleading visual information, leading to misinformation\nor even malicious activities like deepfakes, where fabricated\ncontent is passed off as genuine, potentially causing harm,\nmisinformation, or manipulation. It is essential to establish\nresponsible usage guidelines and ethical standards to prevent\nthe abuse of this technology. Another critical concern is the\ninherent bias within the training data, which might lead to\nbiased representations or unfair outcomes. Ensuring diverse\nand representative training datasets and continuously moni-\ntoring and addressing biases are essential to mitigate such\nrisks. Moreover, the technology poses privacy implications,\nas it could be used to reconstruct 3D models of objects and\nscenes from any images. Images taken without consent or\nfrom public spaces could be used to reconstruct detailed 3D\nmodels, potentially violating personal privacy boundaries.\nAs such, it is crucial to implement appropriate safeguards\nand obtain informed consent when working with images\ncontaining personal information.\n18\n"
  },
  {
    "title": "One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications",
    "link": "https://arxiv.org/pdf/2312.16145.pdf",
    "upvote": "8",
    "text": "One-dimensional Adapter to Rule Them All:\nConcepts, Diffusion Models and Erasing Applications\nMengyao Lyu1,2*\nYuhong Yang1,2*\nHaiwen Hong3\u2020\nHui Chen1,2\nXuan Jin3\nYuan He3\nHui Xue3\nJungong Han1,2\nGuiguang Ding1,2 \u2021\n1Tsinghua University 2BNRist 3Alibaba Group\nmengyao.lyu@outlook.com, suisei.con@gmail.com, honghaiwen.hhw@alibaba-inc.com,\njichenhui2012@gmail.com, {jinxuan.jx, heyuan.hy, hui.xueh}@alibaba-inc.com,\njungonghan77@gmail.com, dinggg@tsinghua.edu.cn\nGeneration of Model A + SPMs\nProgressively Erase \n0 \u2192 20 Targets\n:\nTraining-free\nTransfer\n(Non-Target)\nOriginal\nConcept Erosion\nErase 1 Target:\nGeneration Preservation\nPrevious\nMethods\nGeneration of Model B + SPMs\nSPM\n(Ours)\nGeneration Alternation\nErased\nNon-\nTransferable\nAnother\nModel B\nPrecise Erasure\nOriginal\n SPM-Applied\nTarget: Erased\nNon-target: Preserved\n(Target)\nOriginal\nFigure 1. Previous methods often achieve target concept removal from diffusion models at the cost of degeneration on non-target concepts.\nThey suffer from unpredictable generation alterations, which escalate even into concept erosion when the number of targeted concepts\nincreases. In contrast, the proposed SPM achieves precise multi-concept erasing while preserving the generation capability of the pre-trained\nDM. Moreover, concept-specific SPMs offer training-free transferability towards other models, making it a one-size-fits-all solution.\nAbstract\nThe prevalent use of commercial and open-source diffu-\nsion models (DMs) for text-to-image generation prompts risk\nmitigation to prevent undesired behaviors. Existing concept\nerasing methods in academia are all based on full param-\neter or specification-based fine-tuning, from which we ob-\nserve the following issues: 1) Generation alteration towards\nerosion: Parameter drift during target elimination causes\nalterations and potential deformations across all genera-\ntions, even eroding other concepts at varying degrees, which\nis more evident with multi-concept erased; 2) Transfer in-\nability & deployment inefficiency: Previous model-specific\nerasure impedes the flexible combination of concepts and\n*Equal contribution.\n\u2020 Project lead.\n\u2021 Corresponding authors.\nthe training-free transfer towards other models, resulting in\nlinear cost growth as the deployment scenarios increase.\nTo achieve non-invasive, precise, customizable, and trans-\nferable elimination, we ground our erasing framework on\none-dimensional adapters to erase multiple concepts from\nmost DMs at once across versatile erasing applications. The\nconcept-SemiPermeable structure is injected as a Membrane\n(SPM) into any DM to learn targeted erasing, and mean-\ntime the alteration and erosion phenomenon is effectively\nmitigated via a novel Latent Anchoring fine-tuning strat-\negy. Once obtained, SPMs can be flexibly combined and\nplug-and-play for other DMs without specific re-tuning, en-\nabling timely and efficient adaptation to diverse scenarios.\nDuring generation, our Facilitated Transport mechanism\ndynamically regulates the permeability of each SPM to re-\nspond to different input prompts, further minimizing the\n1\narXiv:2312.16145v2  [cs.CV]  11 Mar 2024\nimpact on other concepts. Quantitative and qualitative re-\nsults across \u223c40 concepts, 7 DMs and 4 erasing applica-\ntions have demonstrated the superior erasing of SPM. Our\ncode and pre-tuned SPMs are available on the project page\nhttps://lyumengyao.github.io/projects/spm.\n1. Introduction\nText-to-image diffusion models (DMs) [3, 13, 14, 27\u2013\n29, 34, 37, 40, 41, 49, 50, 52] have shown appealing advance-\nment in high-quality image creation in the span of seconds,\npowered by pre-training on web-scale datasets. However, the\ncutting-edge synthesis capability is accompanied by degen-\nerated behavior and risks, spanning a spectrum pertaining to\ncopyright infringement [42, 47], privacy breaching [2, 47],\nmature content dissemination [43], etc.\nProprietary text-to-image services [41, 45], open-source\nmodels [1, 37] and academia [8, 10, 18, 43] have made ef-\nforts to generation safety. Nevertheless, these engineering\nand research endeavors often fall into band-aid moderation or\na Pyrrhic victory. For example, training dataset cleansing is\ntime-consuming and labour-intensive, yet it introduces more\nstereotypes [45] and remains not a foolproof solution. Black-\nlisting and post-hoc safety checker relies on high-quality\nannotated data but it is easily circumvented [35, 37, 45].\nRecent methods employ targeted interventions via condi-\ntional guidance through full parameter or specification-based\nfine-tuning [8, 10, 18] or during inference [43]. Despite be-\ning effective for the targeted concept, they come at the cost\nof non-targeted concepts. As shown in Fig. 1, previous mit-\nigations often bring unpredictable generation alterations,\nincluding potential distortions, which are undesirable for\nservice providers. Furthermore, the degradation will escalate\ninto varying degrees of catastrophic forgetting [10, 17, 46]\nacross other concepts, which becomes more pronounced\nwith the simultaneous erasing of multiple concepts. We in-\nformally refer to the phenomenon as concept erosion.\nAnother practical yet commonly overlooked concern is\nerasing customizability and transferability. On the regulatory\nfront, risks of generated content necessitate timely adapta-\ntion, aligning with evolving societal norms and legal regu-\nlations. From the model perspective, DM derivatives with\nspecific purposes have been proliferating fast since open-\nsource models became available, exacerbating the severity\nof the aforementioned issues. However, most of the previous\nmethods require the repetitive design of the erasing process\nfor each set of security specifications and each model. Any\nchange leads to a linear increase in time and computational\ncosts, which necessitates a general and flexible solution.\nTo address the above challenges, we propose a novel\nframework to precisely eliminate multiple concepts from\nmost DMs at once, flexibly accommodating different scenar-\nios. We first develop a one-dimensional non-invasive adapter\nthat can learn concept-SemiPermeability when injected as a\nMembrane (SPM) into DMs with a minimum size increase\nof 0.0005\u00d7. Without any auxiliary real or synthetic training\ndata, SPM learns to erase the pattern of a concept while keep-\ning the pre-trained model intact. Meantime, to ensure that\nit is impermeable for other concepts, our Latent Anchoring\nstrategy samples semantic representations in the general con-\nceptual space and \u201canchor\u201d their generations to correspond-\ning origins, effectively retaining the quality of other concepts.\nUpon acquiring a corpus of erasing SPMs, our framework\nfacilitates the customization and direct transferability of mul-\ntiple SPMs into other DMs without model-specific re-tuning,\nas illustrated in Fig. 1. This capability enables timely and\nefficient adaptation to complex regulatory and model require-\nments. In the subsequent text-to-image process, to further\nensure precise erasure, our Facilitated Transport mechanism\nregulates the activation and permeability rate of each SPM\nbased on the correlation between the input and its targeted\nconcept. Therefore, only the erasing of risky prompts are\nfacilitated, while other concepts remain well-preserved.\nThe proposed method is evaluated with multiple concepts\nerased, different DMs considered and four applications de-\nveloped, totaling over 100 tasks. Both qualitative and quanti-\ntative results show that SPM can successfully erase concrete\nobjects, abstract styles, sexual content and memorized im-\nages. Meanwhile, it effectively suppresses generation alter-\nations and alleviates the erosion phenomenon. Its superiority\nbecomes more evident with multiple concepts overlaid, in\ncontrast to comparative methods that quickly collapse under\nsuch scenarios. Free from model dependency, we demon-\nstrate that SPMs can obliterate concepts from all DM deriva-\ntives at once, indicating a over 160\u00d7 speed improvement in\ncomparison to state-of-the-art (SOTA) methods.\n2. Related Work\nExisting mitigations adopted by academia and applications\ncan be categorized based on the intervention stage: pre-\ntraining dataset filtering [36, 37, 45], pre-trained model\nparameter fine-tuning [8, 18], in-generation guidance direc-\ntion [43], and post-generation content screening [35\u201337, 45].\nThe mitigation of detrimental outputs begins with qual-\nity control of training data. Adobe Firefly is trained on\nlicensed and public-domain content to ensure commercial\nsafety [36]. Stable Diffusion 2.0 [37] adopts an NSFW (Not\nSafe For Work) detector to filter out unsuitable content from\nthe LAION-5B dataset [44], but meantime it also introduces\nbias learnt by the detector [45]. To prevent it, the recently\nunveiled DALL\u00b7E 3 [45] subdivides the NSFW concept into\nspecific cases and deploys individualized detectors accord-\ningly. Nonetheless, leaving away the burdensome retraining\ncosts for the model, the data cleansing process is limited to\nsexual content, and is far from being a foolproof solution.\nA more recent line of research aims to eliminate certain\nconcepts through parameter fine-tuning prior to the de-\n2\nErasing Snoopy\nErasing Van Gogh and Nudity\nSPM Corpus\nVan Gogh\nNudity\nSnoopy\nPicasso\nMonet\nSpock\nPrompts with <Van Gogh>, \n<sexual content> and <Picasso>\nFacilitated Transport during generation\nOriginal\nSPM Applied\nLatent Anchoring during \ufb01ne-tuning\nGenerated Outputs\nLatent \nConcept Space\nTarget: Erased\nNon-target: \nPreserved\nTargeted:\nSnoopy\nConcept Representations\nDi\ufb00usion Layers\nSPMs\nFigure 2. Overview of our erasing framework for Diffusion models. During erasing (Left), our one-dimensional SPM is fine-tuned\ntowards the mitigation of one or several target concepts (e.g., snoopy\n). Centered around it, LA samples representations in the continuous\nlatent space with distance as a measure of probability, efficiently alleviating the alteration and erosion phenomenon. When put into use\n(Right), a combination of SPMs are customized and directly transferred to a new model without re-tuning. With FT mechanism, only\nthreatening prompts (e.g., Van Gogh style and sexual innuendo) amplify the permeability rate of corresponding SPMs (diminishing \u2022\u2022\u2022),\nwhile the generation of safe prompts (e.g., Picasso style) remain unharmed (consistent \u2022\u2022\u2022), further reducing the impact on other concepts.\nployment for downstream applications, enabling it to be\nsafely released and distributed. ESD [8] achieves it by align-\ning the probability distributions of the targeted concept and a\nnull string in a self-supervised manner. Despite effective re-\nmoval, it could suffer from the collapse problem: the model\ntends to generate arbitrary images due to the unconstrained\ntraining process [10], thereby significantly impairing its gen-\nerative capacity. Concept Ablation [18] steers the targeted\nconcept towards a pre-defined surrogate concept via a syn-\nthesized dataset that is derived from ChatGPT [30] synthetic\nprompts. To alleviate the impact on surrounding concepts,\nit adds a regularization loss term on the surrogate concept.\nHowever, the generations of concepts distant from the target\nare also affected. Selective Amnesia (SA) [10] incorporates\nElastic Weight Consolidation [17] to forget the targeted con-\ncept. Besides maximizing the log-likelihood of a named\nsurrogate concept with a synthesized dataset, it leverages an\nadditional general dataset using 5K random prompts gen-\nerated by GPT3.5 for generative replay [46]. Despite the\nexplicit supervision, the alteration towards erosion problem\nis still prevalent as we have observed in preliminary experi-\nments, which is pronounced with multi-concept erasing.\nDuring generation, hand-crafted textual blacklisting [45]\noften serves as the first line of defense. DALL\u00b7E 3 further\nleverages the advanced large language models (LLMs), e.g.,\nChatGPT [30] and Moderation [26], to construct a multi-\ntiered firewall via prompt engineering, such as input safety\nclassification and prompt transformations. These intricate de-\nsigns are straightforward, but their reliance on closed-source\nresources makes it challenging and expensive to generalize.\nInstead of text-level manipulation, SLD [43] leverages in-\nappropriate knowledge encoded in the pre-trained models\nfor reverse guidance. However, striking a balance between\nprompt conditioning and reversed conditioning via multi-\nple hyperparameters may require an iterative process of ex-\nperimentation and adjustment. Furthermore, in contrast to\nabstract concepts, eliminating concrete objects while main-\ntaining coherence and quality remains a challenge.\nIn the post-generation stage, content screening has be-\ncome customary across open-source libraries and commer-\ncial APIs. Besides the safety checker confined to sexual\ncontent in SD and DeepFloyd, DALL\u00b7E 3 trains multiple\nstandalone detectors, spotting race, gender, etc. Specialized\ndetectors require iterative processes of data curating, cleans-\ning and manual annotating. But still, the band-aid moderation\nis obfuscated and easy to be circumvented [8, 35].\nIn contrast, our method is non-invasive, precise, customiz-\nable and transferable, holding a superiority in both erasing\neffectiveness and efficiency. Note that during deployment,\nour solution can integrate with interventions at different\nstages discussed above, forming a multi-layered defense.\n3. Method\nAs Fig. 2 illustrates, given a targeted concept (e.g., Snoopy),\nour main aim is to precisely erase it from pre-trained DMs\nonce and for all while preserving other generations. To avoid\nthe pre-trained model dependency and its parameter drift, we\nfirst develop a 1-dim adapter, dubbed SPM (Sec. 3.1). The\nnon-invasive structure can be plugged into any pre-trained\nDM (e.g., SD v1.4) to learn the transferable recognition\nof a specific concept and its corresponding erasure while\nkeeping the original model intact. We then propose latent\nanchoring (Sec. 3.2), a novel fine-tuning strategy for SPM,\nto efficiently draw upon continuous concepts in the latent\nspace for precise erasing and generation preservation.\nOnce SPMs independently learn to erase various potential\nrisks, a repository is established wherein any combination of\nconcepts (e.g., Van Gogh + nudity) can be customized and\ndirectly transferred to other models (e.g., RealisticVision in\nthe community). During inference, our Facilitated Transport\nmechanism controls the activation and permeability of an\nSPM when receiving the user prompt (Sec. 3.3). For example,\n3\na prompt that indicates explicit content will be erased by\nthe nudity SPM but will not trigger the Van Gogh SPM.\nMeanwhile, the style of Picasso, without corresponding SPM\ninstalled in DM, sees almost no alteration in its generation.\n3.1. SPM as a 1-dim Lightweight Adapter\nTo free the concept erasing from pre-trained model depen-\ndency, inspired by parameter efficient fine-tuning (PEFT) ap-\nproaches [6, 15, 16, 19\u201321, 23, 25, 51], we design an adapter\nserving as a lightweight yet effective alternative to the pre-\nvailing full parameter or specification-based fine-tuning ap-\nproaches of prior arts [8, 10, 18]. With only one intrinsic\ndimension, it is injected into a DM as a thin membrane\nwith minimum overhead, in order to learn concept-specific\nsemi-permeability for precise targeted erasing.\nSpecifically, on a certain module parameterized by W \u2208\nRm\u00d7n in the DM, we learn an erasing signal vsig \u2208 Rm\nto suppress undesired contents in model generation. Mean-\nwhile, the amplitude of the erasing signal is controlled by\na trainable regulator vreg \u2208 Rn, to determine the erasing\nstrength. As such, the original forward process y = W x is\nintervened by our SPM as follows:\ny = W x + (vT\nregx) \u00b7 vsig.\n(1)\nx \u2208 Rn and y \u2208 Rm represent the input and output of an\nintermediate layer, and superscript T indicates transposition.\nAs a short preliminary, take the latent DM (LDM) [37] for\nexample, the denoising process predicts the noise \u02c6\u03f5 applied\non the latent representation of a variably-noised image xt,\nconditioning on the current timestep t and a textual descrip-\ntion c derived from the text encoder:\n\u02c6\u03f5 = \u03f5(xt, c, t|\u03b8).\n(2)\nThe \u03b8 in Eq. 2 denotes parameters of the noise prediction au-\ntoencoder, which is often implemented as a U-Net [3, 14, 38].\nUpon the pre-trained parameter \u03b8, our SPM is formulated as\nMctar = {(vi\nsig, vi\nreg)|ctar}, each of which is inserted into\nthe i-th layer, thereby eliminating patterns of the undesired\nconcept ctar. Thus the diffusion process now reads\n\u02c6\u03f5 = \u03f5(xt, c, t|\u03b8, Mctar).\n(3)\nThe addition-based erasing enables flexible customization of\nmultiple concepts, where specific SPMs can be placed on a\npre-trained DM simultaneously to meet intricate and ever-\nchanging safety requirements needs. Furthermore, the simple\ndesign allows it to be easily shared and reused across most\nother DMs as validated in Sec. 4.2, significantly improving\ncomputational and storage efficiency.\n3.2. Latent Anchoring\nUpon the constructed lightweight SPM, we acquire its semi-\npermeability of the specialized concepts through a fine-\ntuning process. Inspired by the discovery [4, 5, 8, 24] that\nconcept composition and negation on DMs can be matched\nto arithmetic operations on log probabilities, we reparam-\neterize it to perform the concept elimination on the noise\nprediction process of DMs. Formally, given the target con-\ncept ctar, we pre-define a corresponding surrogate concept\ncsur instructing the behaviour of the erased model when ctar\nis prompted. Then, to achieve ctar \u2190 csur \u2212 \u03b7 \u2217 (ctar \u2212 csur),\nSPM employs an erasing loss to match the probability distri-\nbutions of ctar and csur:\nLera = Ext,t [\u2225\u03f5(xt, ctar, t|\u03b8, Mctar) \u2212 \u03f5(xt, csur, t|\u03b8)\n+\u03b7 \u2217 (\u03f5(xt, ctar, t|\u03b8) \u2212 \u03f5(xt, csur, t|\u03b8))\u22252\n2\n\u0003\n.\n(4)\nThe \u03b7 determines the erasure intensity for features assiciated\nwith ctar as opposed to csur, with a larger \u03b7 signifying a\nmore thorough erasure.\nMeanwhile, erasing a concept from DMs must prevent\nthe catastrophic forgetting of others. Simply suppressing\nthe generation of the target leads to severe concept erosion.\nConAbl [18] and SA [10] attempted to adopt a generate-\nand-relearn approach to mitigate the issue, wherein images\nare synthesized using collected text prompts, and then these\nimage-text pairs are relearned during fine-tuning. Neverthe-\nless, this approach has two major limitations. On the one\nhand, in comparison with the large general semantic space\nthat pre-trained models have obtained, hand-crafted prompts\nat the scale of thousands are highly limited and potentially\nbiased. Therefore, the replay in the pixel space during fine-\ntuning leads to the degradation and distortion of the seman-\ntic space, resulting in inevitable generation alterations and\nunexpected concept erosion. On the other hand, intensive\ntime and computational cost are required for prompt and\nimage preparation. As an example, leaving aside the prompt\npreparation stage, the image generation process alone takes\nSA [10] more than 80 GPU hours, as listed in Tab. 2.\nTowards precise and efficient erasing, we propose Latent\nAnchoring to address the issues. On the conceptual space,\nwe establish explicit guidelines for the generation behav-\nior of the model across the entire conceptual space. While\nthe model is instructed for the target concept to align with\nthe surrogate concept, for other concepts, particularly those\nthat are semantically distant from the target, the model is\nexpected to maintain consistency with its original generation\nas much as possible. With C representing the conceptual\nspace under the text encoder of the DM, this objective could\nbe characterized as:\nargmin\n\u03b8\nEc\u2208C\n\u0002\n\u2225\u03f5(xt, ci, t|\u03b8, Mctar) \u2212 \u03f5(xt, ci, t|\u03b8)\u22252\n2\n\u0003\n.\n(5)\nHowever, this form is intractable due to the latent space C,\nand it is also partially against the erasing loss. Therefore, we\nderive a sampling distribution D(\u00b7|ctar) from C to obtain a\ntractable and optimization-friendly form. Our intention is for\nthe distant concepts from the target to exhibit consistency,\nwhile the synonyms of the target get suitably influenced.\nHere the distance is defined by cosine similarity same as\nCLIP [33]. For each encoding c within the sampling space,\n4\nwe define the sample probability by:\nPc\u223cD(\u00b7|ctar)(c|ctar) \u221d (1 \u2212\n|c \u00b7 ctar|\n\u2225c\u2225 \u00b7 \u2225ctar\u2225)\u03b1,\n(6)\nwhere \u03b1 is a hyper-parameter influencing the behavior of the\nsynonym concepts. The anchoring loss is formulated as:\nLanc = Ec\u223cD(\u00b7|ctar)\n\u0002\n\u2225\u03f5(xt, ci, t|\u03b8, Mctar) \u2212 \u03f5(xt, ci, t|\u03b8)\u22252\n2\n\u0003\n.\n(7)\nCombining the two components with balancing hyper-\nparameter \u03bb, we can derive our total training loss as:\nL = Lera + \u03bbLanc.\n(8)\nWith Latent Anchoring applied, SPM can be correctly trig-\ngered with the erasing target and take control of correspond-\ning content generation, while staying minimally activated\nfor non-target and keeping the original generation.\n3.3. Facilitated Transport\nOnce SPMs are learnt in a concept-specific and model-\nindependent manner, a universal comprehensive erasure cor-\npus is established. To comply with specific legal regulations\nand social norms, instead of repeating the whole erasing\npipeline each time for a dedicated model, we can directly\nretrieve k plug-and-play SPMs of potential threats from the\ncorpus, and seamlessly overlay any other DM f\nW with them:\ny = f\nW x +\nk\nX\nc\n(\u03b3c \u00b7 vcT\nregx) \u00b7 vc\nsig.\n(9)\nDespite Latent Anchoring designed to uphold safe con-\ncepts during fine-tuning, in the challenging scenarios where\nmulti-SPMs are installed, the overall generations inevitably\nbecome entangled. To further minimize the impact of erasing\nmitigations on other concepts, we introduce the facilitated\ntransport mechanism into SPMs at the inference stage, which\ndynamically transports the erasing signal of the targeted con-\ncept while rejecting other concepts to pass through.\nSpecifically, given a text prompt p, the information per-\nmeability and rate of transmission for each SPM, denoted\nas \u03b3c(p), is contingent upon the probability of its targeted\nconcept c indicated in p. To estimate the probability, we first\ncompute the cosine distance in the CLIP [33] textual encod-\ning space, referred to as sc\nf(p). However, the global-view\nrepresentation could fail in capturing the correlation between\nthe concept name and an elaborate user description. For in-\nstance, the score between Van Gogh and The swirling night\nsky above the village, in the style of Van Gogh is 0.46, but\nwe expect the corresponding SPM to operate at its maximum\ncapacity. To this end, we additionally introduce a unigram\nmetric to identify the similarity at the token-level:\nsc\nt(p) = |T(c) \u2229 T(p)|\n|T(c)|\n,\n(10)\nwhere T represents a text tokenizer. We thus derive the prob-\nability of concept c appearing in the description as:\n\u03b3c(p) = max(sc\nf, sc\nt),\n(11)\nSnoopy\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nMickey\nSpongebob\nPikachu\nDog\nLegislator\nFigure 3. Samples of \u201cgraffiti of the {concept}\u201d after erasing\nSnoopy. Our SPM exhibits sufficient elimination on the targeted\nconcept Snoopy, while the impact on non-targets is negligible.\nso that the correlation can be captured at both global and\nlocal levels. When a user prompt stimulates one or multiple\nSPMs semantically, their permeability \u03b3 amplifies, dynam-\nically emitting erasing signals. Conversely, the transport is\ndeactivated when the relevance is low, effectively minimiz-\ning the impact on safe concepts.\n4. Experiments\nWe conduct extensive experiments encompassing erasing\nvarious concepts, transferring across different personalized\nmodels, as well as practical erasing applications, validating\nour effectiveness as a one-size-fits-all solution. Due to space\nconstraints, training details of SPM and comparative meth-\nods are shown in Appendix C. The dimension analysis and\nablation study of SPM are presented in Appendix A.\n4.1. Single and Multiple Concept Removal\nExperimental Setup. Without loss of generality, we eval-\nuate single and multi-concept erasing in the application of\nobject removal. Besides the estimation of the target gener-\nation, the impact on surrounding concepts is also assessed.\nHere we take the concept of Snoopy as an example, the dic-\ntionary of the CLIP text tokenizer is utilized to identify the\nconcepts most closely associated with it with cosine simi-\nlarity. After deduplication and disambiguation, the nearest\nMickey, Spongebob, and Pikachu are chosen. Additionally,\n5\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt:\n\u201cA tattoo of the Pikachu\u201d\nOriginal:\nErased\nConcepts\nSnoopy\nSnoopy\n+\nMickey\nSnoopy\n+\nMickey\n+\nSpongebob\nPrompt: \n\u201cA painting of a Mickey\u201d\nOriginal:\nESD\nConAbl\nSA\nSPM (Ours)\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt:\n\u201cA pixelated photo of a \n Legislator\u201d\nOriginal:\nFigure 4. Samples from DMs with one and multiple instances removed. As prior methods suffer from both generation alteration and\nconcept erosion, which escalates as the number of targets increase, generations with our SPMs remain almost identical.\nSnoopy\nMickey\nSpongebob\nPikachu\nDog\nLegislator General\nCS\nCER\nCS\nCER\nCS\nCER\nFIDg\nSD v1.4 74.43\n0.62\n71.94\n2.50\n72.99\n0.62\n-\n-\n-\n13.24\nErasing Snoopy\nCS\u2193\nCER\u2191\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFIDg\u2193\nESD\n44.50 77.62\n129.07\n113.90\n72.18\n45.94\n55.18\n13.68\nConAbl 59.81\n5.50\n110.85\n79.49\n71.22\n96.36\n55.74\n15.42\nSA\n64.59\n0.25\n53.64\n57.65\n42.95\n75.72\n47.42\n16.84\nOurs\n55.48 20.12\n28.39\n30.75\n18.61\n10.11\n7.40\n13.24\nErasing Snoopy and Mickey\nCS\u2193\nCER\u2191\nCS\u2193\nCER\u2191\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFIDg\u2193\nESD\n45.49 67.00 44.23 83.12\n145.71\n114.25\n51.05\n64.74\n13.69\nConAbl 60.05\n4.00\n56.14 14.00\n112.15\n105.43\n79.40\n56.17\n15.28\nSA\n63.33 10.75 60.93 51.12\n148.33\n129.52\n137.91\n151.94\n17.67\nOurs\n55.11 20.62 52.04 39.50\n36.52\n26.69\n13.45\n16.03\n13.26\nErasing Snoopy, Mickey and Spongebob\nCS\u2193\nCER\u2191\nCS\u2193\nCER\u2191\nCS\u2193\nCER\u2191\nFID\u2193\nFID\u2193\nFID\u2193\nFIDg\u2193\nESD\n46.94 60.38 44.79 80.25 43.76 85.88\n137.23\n50.77\n73.96\n13.46\nConAbl 60.88\n1.12\n55.10 23.12 58.46 15.38\n102.79\n67.43\n55.72\n15.50\nSA\n64.53 15.25 61.15 61.88 60.59 49.88\n167.79\n183.26\n185.29\n18.32\nOurs\n53.72 25.75 50.50 44.50 51.30 41.87\n33.19\n14.69\n20.66\n13.26\nTable 1. Quantitative Evaluation of instance erasure. The best\nresults are highlighted in bold, while the second-best is underlined.\nArrows on headers indicate the favourable direction for each metric.\nOn the target concepts, our second-ranked erasing SPM, already\nproven sufficient as in Fig. 3, significantly surpasses previous meth-\nods in generation preservation, and maintains stability while the\nnumber of erased concept increases. General FIDg further shows\nthe superiority of SPM in mitigating alterations and erosion.\nwe examine its parent concept of Dog, as well as a randomly\nchosen general concept, Legislator, for comparison.\nEvaluation Protocol.\nIn order to holistically assess the\ngeneration capability after erasing, we employ 80 templates\nproposed in CLIP [33] to augment text prompts. A concept\nto be evaluated is incorporated into 80 templates, with each\ntemplate yielding 10 images. After the generation process,\ntwo groups of metrics are employed for result analysis. 1)\nCLIP Score (CS) [11] and CLIP Error Rate (CER) for\ntarget concept evaluation. CS, calculated using the similarity\nbetween the concept and the image, is utilized to confirm\nthe existence of the concept within the generated content.\nThe computation of CER adopts CLIP with the embedded\ntarget and corresponding surrogate, functioning as a binary\nclassifier, yielding the error rate of the image being classified\nas the surrogate concept. A lower CS or a higher CER is\nindicative of more effective erasure on targets. 2) Fr\u00b4echet\nInception Distance (FID) [12] for non-target concepts. It\nis calculated between the generations of the erased model\nand the original DM, with a larger FID value demonstrating\nmore severe generation alteration after erasing. Additionally,\nto ensure the conditional generation capability for general\nsafe concepts, we also evaluate the erased model on the\nCOCO-30k Caption dataset [22], where the FID is calculated\nbetween generated and natural images, denoted as FIDg.\nResults of Single Concept Erasure. As presented in Fig. 3,\nwith the elimination of Snoopy, generation alterations can\nbe observed in all cases of previous methods. Furthermore,\nsome samples exhibit noticeable concept erosion, such as the\nDog generated by ConAbl (style lost of graffiti) and Mickey\nof ESD (severe distortion). It demonstrates that previous arts\nare all limited to the trade-off between erasing and preser-\nvation: most of them erase the target at the cost of other\nconcepts, with SA leaning towards the latter. In contrast, our\nSPM achieves successful erasure while showing promising\nstability on those non-targets, with almost identical genera-\ntions aligned with the original generation of the DM.\nQuantitatively, Tab. 1 gives the evaluation of the erased\nmodel on the inspected concepts and the general dataset. On\nthe targeted Snoopy, ESD exhibits the most thorough erasing\nperformance, but the erosion phenomenon shown in other\nconcepts is significant, with a huge quality decline compared\nto the original DM. ConAbl and SA, where a generate-and-\nrelearn approach is employed, have subpar performances\nin general generation, evidenced by their notably increased\nFIDg. This can be attributed to the bias introduced by hand-\ncrafted pixel-level data adopted for relearning, as elaborated\nin Sec. 3.2. As a comparison, our SPM has sufficient erasure\n6\n-100\n-50\n0\nButtocks (123)\nFemale Breast (325)\nFemale Genitalia (70)\nMale Breast (77)\nFeet (183)\nArmpits (400)\nBelly (271)\nMale Genitalia (59)\nTotal (1509)\n-100\n-50\n0\n-100\n-50\n0\n-100\n-50\n0\n-100\n-50\n0\n-100\n-50\n0\nChange (%)\nChange (%)\nChange (%)\nChange (%)\nChange (%)\nChange (%)\nSD v2.0\nSLD\nESD\nConAbl\nSA\nSPM (Ours)\n-27.5%\n-77.1%\n-39.0%\n-65.0%\n-85.3%\n-69.5%\n Exposed Body Parts \n(# of original SD v1.4)\nFigure 5. NudeNet Evaluation on the I2P benchmark. The numbers on the left count the exposed body parts of the SD v1.4 generations.\nThe binplots show the decrement with different methods applied for nudity mitigation, including data-filtering (SD v2.0) and concept erasing\n(others, by erasing \u201cnudity\u201d). Compared to the prior works, SPM effectively eliminates explicit contents across different nude categories.\nData\nPrep. (h)\nModel\nFT (h)\nImage\nGen. (s)\nTotal (h)\n(c = 20, n = 5, p = 60)\nSLD\n0\n0\n3.3pn\n1.1\nESD\n0\n0.7cn\n3pn\n70.25\nConAbl\n0.15cn\n0.25cn\n3pn\n40.25\nSA\n20n + 4cn\n36cn\n3pn\n4100.25\nOurs\n0\n1.2c\n(3 + 0.15c)pn\n24.5\nTable 2. Time consumption of the erasing pipeline for c targeted\nconcepts on n DMs, with each generating on p prompts. One\nNVIDIA A100 GPU is used by default, while more than one GPU\nusages are correspondingly multiplied on time consumption.\non the target while maintaining the generation capability on\nother concepts, and the general FIDg remains intact. Results\non SD v2.1 [37], SDXL v1.0 [32] can be found in Appendix\nB.1. More sample generations are shown in Appendix D.1.\nResults of Multi-Concept Erasure. Fig. 4 presents a com-\nparison of multi-concept erasing cases, a more realistic and\nchallenging scenario. It can be observed that all previous\nmethods exhibit varying degrees of generation alteration,\nwhich exacerbates with the number of erased concepts in-\ncreases, and meantime the erosion phenomenon becomes\nmore prevalent. For instance, ESD forgets Mickey after eras-\ning Snoopy, and ConAbl and SA exhibit deteriorating gener-\nation quality in Pikachu and Legislator, finally leading to the\nerosion. These findings are consistent with numerical results\npresented in Tab. 1, where their FID scores escalate to an\nunacceptable rate. In comparison, our SPM effectively sup-\npresses the rate of generation alteration and erosion. Further-\nmore, our FIDg only shows a negligible increase of \u2264 0.02,\nindicating significantly better alignment with the original\nDM, while prior arts present 10\u00d7 to 200\u00d7 variances. Please\nrefer to Fig. 1 and Appendix D.2 for the results of erasing\nup to 20 concepts. The performance of cross-application\nmulti-concept erasure can be found in Appendix B.2.\nEfficiency Analysis. Generally, based on pre-trained text-to-\nimage models, the pipeline of concept erasure task includes\ndata preparation, parameter fine-tuning and image generation\nwhen put into use. Tab. 2 reports the time consumption of\nSOTA methods and our SPM in GPU hours.\nUnder the extreme condition of single concept erasing,\nSPM achieves a good balance between performance and effi-\nDreamShaper\nOriginal\nw/ Mickey SPM\nChillOutMix\nOriginal\nRealisticVision\nOriginal\nw/ Snoopy SPM\nw/ Spongebob SPM\nO\ufb03cial SD v1.5\nOriginal\nw/  Pikachu SPM\nFigure 6. Training-free transfer results for SPM. Once obtained\n(e.g., from SD v1.4 in this case), SPM can transfer to other special-\nized models without re-tuning, and express both its target concept\nerasing and non-target preservation capabilities well.\nciency. Under the more realistic condition of multi-concept\nand multi-model, the scalability and transferability of SPM\nmake it significantly more efficient than previous arts: SPM\nparallelizes the elimination of multiple concepts, while previ-\nous arts have to extend their training iterations [18]; the cost\nof SPM is constant when applied for multiple models, and in\ncontrast, others are linear to the number of application sce-\nnarios. Assuming a case where c = 20, n = 5 and p = 60,\nthe erasing results in Tab. 1 and corresponding costs in Tab. 2\nshow that we achieve significantly better performance with a\nreduction in time consumption by 65.1% and 39.1% in com-\nparison with ESD and ConAbl respectively, and obtain a high\nmargin in erasure effect over SA at a 167.4\u00d7 speed. Also,\nSPM utilizes marginal parameter storage, only 0.0005\u00d7 that\nof previous tuning-based methods, endorsing its aptness for\nefficient management and deployment.\n7\n4.2. Training-Free Transfer Study\nAs all prior fine-tuning-based methods are model-dependent,\nthey lack transferability across DMs. In this section, we\npresent the training-free transfer results of our SPMs ob-\ntained from training on the official SD v1.4, and subse-\nquently applied on SD v1.5, as well as top-most downloaded\ncheckpoints in the community, including Chilloutmix1, Re-\nalisticVision2 and Dreamshaper-83. Results in Fig.6 show\nthat, without model-specific fine-tuning, our SPMs success-\nfully erase these finely-tuned and more elaborated concepts,\nwhile preserving the consistency and flexibility of generation.\nMore transfer samples on community checkpoints can be\nfound in Appendix D.4 and B.7.\n4.3. Versatile Erasing Applications\nExperimental Setup & Evaluation Protocol. To examine\nthe generality of erasing methods, we conduct three sets\nof experiments aimed at eliminating artistic styles, explicit\ncontent and memorized images. Towards the abstract artistic\nstyles, we focus on five renowned artists, including Van\nGogh, Picasso, Rembrandt, Andy Warhol, and Caravaggio.\nFor each artist, ESD [8] provides 20 distinct prompts, for\neach of which we generated 20 images with erased models.\nIn the case of explicit content removal, following ESD\nand SLD, the I2P benchmark [43] of 4703 risky prompts is\nadopted for evaluation. SPM is validated with only one gen-\neral term nudity, while comparison methods retain their pub-\nlic implementations. After generation, we employ NudeNet\nv34 to identify nude body parts within the generated images.\nWe also experiment with specific artwork erasing to pre-\nvent DMs from memorizing training images. The results and\nanalysis can be found in Appendix B.3.\nArtistic Style Removal. Besides concrete object removal\ndemonstrated above, Fig. 7 showcases the results of erasing\nartistic styles. We find that SLD under-erases the artistic\nstyles, while ESD and ConAbl succeed in erasing Van Gogh\nstyle but fail in Picasso. SA, in accordance with the analysis\nabove, barely eliminates the specified artistic styles from the\nmodel, especially in the challenging Picasso case. Moreover,\nthe alterations of generations for non-target concepts are\nmuch more evident than in most of the prior arts, indicating\na skewed semantic space attributed to the biased relearning.\nConversely, our SPM can successfully remove the targeted\nstyle without loss of semantic understanding of the prompts,\nwhile still preserving other styles and generated contents.\nNumerical and more qualitative comparison can be found in\nAppendix B.6 and D.3.\nExplicit Content Removal. The obfuscation of the concept\nand the implicity of prompts make the erasure of explicit\n1https://huggingface.co/emilianJR/chilloutmix NiPrunedFp32Fix\n2https://huggingface.co/SG161222/Realistic Vision V5.1 noVAE\n3https://huggingface.co/Lykon/dreamshaper-8\n4https://github.com/notAI-tech/NudeNet/tree/v3\nESD\nConAbl\nSA\nSPM (Ours)\nA vase of vibrant \n\ufb02owers, in the style of \nVan Gogh's still lifes.\nA glimpse of Rembrandt's \nAmsterdam through his \npainting.\nOriginal\nAn intimate portrait \nfeaturing a contemplative \nsubject, illuminated by a \nsingle source of light, \nreminiscent of \nCaravaggio's style.\nA still life with abstract \nshapes and colors, \ninspired by Picasso's love \nfor bold experimentation.\nFaces of Rembrandt's \nera in his signature \nchiaroscuro style.\nA whimsical and \nirreverent portrayal of \nMarilyn Monroe by \nWarhol.\nSLD\nFigure 7. Samples from DMs with artistic styles removed. SPMs\ncan erase targeted styles (upper \u201cVan Gogh\u201d and lower \u201cPicasso\u201d)\nwhile preserving others, unlike prior works that show an evident\ntrade-off between erasing and preservation.\ncontent challenging. SD v2.x [37] suppresses inappropriate\ngenerations via training dataset cleansing. However, results\nin Fig. 5 show that the probability of generating inappropri-\nate content is reduced by less than 30% compared to SD v1.4.\nFurthermore, evaluation with prompts that do not explicitly\nmention NSFW terms would lead to the failure of word-level\nblacklisting and methods with discrete semantic comprehen-\nsion, which could explain the suboptimal results of ConAbl\nand SA as we have analyzed in Sec. 3.2. In contrast, our\nSPM leverages the Latent Anchoring mechanism, instead\nof a limited synthesized dataset, to retain the knowledge\nof the large-scale semantic space. It achieves a significant\nreduction of 85.3% in the generation of nudity, indicating\nthat the simple term nudity can be generally comprehended\nand thus the explicit content can be well erased. We then\ndirectly transfer the nudity-removal SPM to popular com-\nmunity derivatives, and the results in Appendix B.7 further\nvalidate its effectiveness and generalizability.\n5. Conclusion\nThis paper proposes a novel erasing framework based on\none-dimensional lightweight SPMs. With a minimum size\nincrease of 0.0005\u00d7, SPMs can erase multiple concepts at\nonce for most DMs in versatile applications. Experiments\nshow that SPM achieves precise erasing of undesired con-\ntent, and meantime the training-time Latent Anchoring and\ninference-time Facilitated Transport effectively mitigate gen-\neration alteration and erosion. Furthermore, the customiza-\ntion and transferability of SPMs significantly reduces time,\ncomputational and storage costs, facilitating practical usage\ntowards different regulatory and model requirements.\n8\n6. Acknowledgment\nThis work was supported by National Natural Science\nFoundation of China (Nos. 61925107, 62271281, 62021002).\nReferences\n[1] Stability AI. Deepfloyd if model card, 2023. 2\n[2] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski,\nVikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito,\nand Eric Wallace. Extracting training data from diffusion\nmodels. In 32nd USENIX Security Symposium (USENIX\nSecurity 23), pages 5253\u20135270, 2023. 2, 3\n[3] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. In NeurIPS, pages 8780\u20138794.\nCurran Associates, Inc., 2021. 2, 4\n[4] Yilun Du, Shuang Li, and Igor Mordatch. Compositional\nVisual Generation with Energy Based Models. In NeurIPS,\npages 6637\u20136647. Curran Associates, Inc., 2020. 4\n[5] Yilun Du, Shuang Li, Yash Sharma, B. Joshua Tenenbaum,\nand Igor Mordatch. Unsupervised learning of compositional\nenergy concepts. In NeurIPS, 2021. 4\n[6] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia,\nJames J. Clark, and Mehdi Rezagholizadeh. Krona: Parameter\nefficient tuning with kronecker adapter. In Third Workshop on\nEfficient Natural Language and Speech Processing (ENLSP-\nIII): Towards the Future of Large Language Models and their\nEmerging Descendants, 2022. 4\n[7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022. 10\n[8] Rohit\nGandikota,\nJoanna\nMaterzynska,\nJaden\nFiotto-\nKaufman, and David Bau. Erasing concepts from diffusion\nmodels. In ICCV, pages 2426\u20132436, 2023. 2, 3, 4, 8\n[9] Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira\nKtena, Robert Stanforth, Jamie Hayes, Soham De, Samuel L\nSmith, Olivia Wiles, and Borja Balle. Differentially private\ndiffusion models generate useful synthetic images. arXiv\npreprint arXiv:2302.13861, 2023. 3\n[10] Alvin Heng and Harold Soh. Selective Amnesia: A Continual\nLearning Approach to Forgetting in Deep Generative Models.\nIn NeurIPS, 2023. 2, 3, 4, 8, 9, 11\n[11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. CLIPScore: A reference-free evaluation met-\nric for image captioning. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing,\npages 7514\u20137528, Online and Punta Cana, Dominican Repub-\nlic, 2021. Association for Computational Linguistics. 6\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nIn NeurIPS, page 6629\u20136640, Red Hook, NY, USA, 2017.\nCurran Associates Inc. 6\n[13] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 2\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In NeurIPS, pages 6840\u20136851,\n2020. 2, 4\n[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-Rank Adaptation of Large Language Models. In\nICLR, 2022. 4\n[16] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara:\nLow-rank hadamard product for communication-efficient fed-\nerated learning. In ICLR, 2022. 4\n[17] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan,\nJohn Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and\nRaia Hadsell. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of Sciences,\n114(13):3521\u20133526, 2017. 2, 3\n[18] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shecht-\nman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in\ntext-to-image diffusion models. In ICCV, pages 22691\u201322702,\n2023. 2, 3, 4, 7, 8, 9\n[19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\nof scale for parameter-efficient prompt tuning. In EMNLP,\npages 3045\u20133059, Online and Punta Cana, Dominican Repub-\nlic, 2021. Association for Computational Linguistics. 4\n[20] Yinghui Li, Jing Yang, and Jiliang Wang. Dylora: Towards\nenergy efficient dynamic lora transmission control. In IEEE\nINFOCOM 2020-IEEE Conference on Computer Communi-\ncations, pages 2312\u20132320. IEEE, 2020.\n[21] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.\nScaling & shifting your features: A new baseline for efficient\nmodel tuning. In NeurIPS, pages 109\u2013123, 2022. 4\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755. Springer, 2014. 6\n[23] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta,\nTenghao Huang, Mohit Bansal, and Colin A Raffel. Few-\nshot parameter-efficient fine-tuning is better and cheaper than\nin-context learning. NeurIPS, 35:1950\u20131965, 2022. 4\n[24] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum. Compositional visual generation with\ncomposable diffusion models. In ECCV, pages 423\u2013439.\nSpringer, 2022. 4\n[25] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. P-tuning v2: Prompt tuning can be com-\nparable to fine-tuning universally across scales and tasks.\nCoRR, abs/2110.07602, 2021. 4\n[26] Todor Markov, Chong Zhang, Sandhini Agarwal, Floren-\ntine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela\nJiang, and Lilian Weng. A holistic approach to undesired con-\ntent detection in the real world. In AAAI, pages 15009\u201315018,\n2023. 3\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n9\n[28] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In ICML, pages\n8162\u20138171. PMLR, 2021.\n[29] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In ICML,\npages 16784\u201316804. PMLR, 2022. 2\n[30] R OpenAI. Gpt-4 technical report. arXiv, pages 2303\u201308774,\n2023. 3, 9\n[31] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya\nGoyal, and Matthijs Douze. A Self-Supervised Descriptor for\nImage Copy Detection. In CVPR, pages 14512\u201314522, New\nOrleans, LA, USA, 2022. IEEE. 3, 4\n[32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 7,\n2\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nIn ICML, pages 8748\u20138763. PMLR, 2021. 4, 5, 6\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 1(2):\n3, 2022. 2\n[35] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim,\nand Florian Tramer. Red-teaming the stable diffusion safety\nfilter. In NeurIPS ML Safety Workshop, 2022. 2, 3\n[36] Dana Rao. Responsible innovation in the age of generative ai,\n2023. 2\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 2, 4, 7, 8\n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, pages 234\u2013241. Springer, 2015. 4\n[39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven\nGeneration. In CVPR, pages 22500\u201322510, 2023. 10\n[40] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi. Palette: Image-to-image diffusion models. In ACM\nSIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022.\n2\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep lan-\nguage understanding. In NeurIPS, pages 36479\u201336494, 2022.\n2\n[42] Joseph Saveri and Matthew Butterick. Stable diffusion litiga-\ntion, 2023. 2\n[43] Patrick Schramowski, Manuel Brack, Bj\u00a8orn Deiseroth, and\nKristian Kersting. Safe Latent Diffusion: Mitigating Inappro-\npriate Degeneration in Diffusion Models. In CVPR, pages\n22522\u201322531, 2023. 2, 3, 8, 7\n[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\nPatrick Schramowski, Srivatsa R Kundurthy, Katherine Crow-\nson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\nLAION-5b: An open large-scale dataset for training next gen-\neration image-text models. In Thirty-sixth Conference on\nNeural Information Processing Systems Datasets and Bench-\nmarks Track, 2022. 2\n[45] Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. Improving\nimage captioning with better use of caption. In ACL, pages\n7454\u20137464, Online, 2020. Association for Computational\nLinguistics. 2, 3\n[46] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.\nContinual learning with deep generative replay. In NeurIPS,\npage 2994\u20133003, Red Hook, NY, USA, 2017. Curran Asso-\nciates Inc. 2, 3\n[47] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas\nGeiping, and Tom Goldstein.\nDiffusion Art or Digital\nForgery? Investigating Data Replication in Diffusion Models.\nIn CVPR, pages 6048\u20136058, 2023. 2, 3\n[48] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas\nGeiping, and Tom Goldstein. Understanding and Mitigating\nCopying in Diffusion Models. In NeurIPS, 2023. 3\n[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn ICLR, 2020. 2\n[50] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.\nMaximum Likelihood Training of Score-Based Diffusion\nModels. In NeurIPS, pages 1415\u20131428, 2021. 2\n[51] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi\nSun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy,\nand Tomas Pfister. Learning to prompt for continual learning.\nIn CVPR, pages 139\u2013149, 2022. 4\n[52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV, pages 3836\u20133847, 2023. 2\n10\nOne-dimensional Adapter to Rule Them All:\nConcepts, Diffusion Models and Erasing Applications\nSupplementary Material\nContents\n1 . Introduction\n2\n2 . Related Work\n2\n3 . Method\n3\n3.1. SPM as a 1-dim Lightweight Adapter . . . .\n4\n3.2. Latent Anchoring . . . . . . . . . . . . . . .\n4\n3.3. Facilitated Transport . . . . . . . . . . . . .\n5\n4 . Experiments\n5\n4.1. Single and Multiple Concept Removal . . . .\n5\n4.2. Training-Free Transfer Study\n. . . . . . . .\n8\n4.3. Versatile Erasing Applications . . . . . . . .\n8\n5 . Conclusion\n8\n6 . Acknowledgment\n9\nA. Analysis of SPM\n1\nA.1. Dimension of SPM . . . . . . . . . . . . . .\n1\nA.2. Component verification of SPM . . . . . . .\n1\nA.3. Sensitivity Analysis of \u03b7 . . . . . . . . . . .\n2\nB. Extended Experimental Results\n2\nB.1. SPM for SD v2.1 and SDXL v1.0 . . . . . .\n2\nB.2. Cross-Application Multi-Concept Erasure . .\n3\nB.3. Memorized Image Removal . . . . . . . . .\n3\nB.4. Generations of COCO-30k Caption . . . . .\n4\nB.5. SPM with Surrogate Concepts . . . . . . . .\n4\nB.6. Numerical Results of Artistic Style Erasure .\n6\nB.7. Samples of Nudity Removal . . . . . . . . .\n7\nB.8. Erasing Concepts with Shared Words . . . .\n8\nB.9. Impact on Latent Representations . . . . . .\n8\nC. Detailed Experiment Settings\n8\nC.1. Implementation Details . . . . . . . . . . . .\n8\nC.2. Comparative methods\n. . . . . . . . . . . .\n8\nD. Additional Results\n9\nD.1. Additional Samples of Single Concept Erasure\n9\nD.2. Additional Samples of 20 Concepts Erasure .\n9\nD.3. Additional Samples of Artistic Style Erasure\n9\nD.4. Additional Samples of Training-Free Transfer\n9\nD.5. Full Numerical Results of Object Erasure . .\n9\nD.6. Failure Cases . . . . . . . . . . . . . . . . .\n10\nE. Comparison with Concept-based Manipulation\nMethods\n10\nF. Societal Impact\n11\nWarning: The Appendix includes prompts with sexual\nsuggestiveness and images with censored nude body parts.\nA. Analysis of SPM\nA.1. Dimension of SPM\nTo achieve precise erasure with minimum inference-time\noverhead, we have opted for a lightweight SPM with a one-\ndimensional intermediate layer. In addition to the effective\nand efficient results in the main text obtained with d = 1,\nhere we explore the impact of dimensionality, i.e., the capac-\nity, on the erasing performance. Tab. 3 shows the numerical\nresults of SPMs with d = 1, 2, 4, 8 respectively, where the\ninstance Mickey is eliminated as an example. As can be\nseen, with the increase in the intermediate dimension of the\nadapter, there is no discernible trend in the metrics for the\ntarget concept, other concepts, and general COCO captions.\nFig. 8 also validates the robustness of SPM in the generated\ncontent. Thus, despite the low cost of increasing dimension-\nality, 1-dimensional SPM proves to be sufficient for our\nconcept erasing task.\nA.2. Component verification of SPM\nLatent Anchoring (LA) and Facilitated Transport (FT) serve\nas a dual safeguard against the concept erosion phenomenon.\nIn this section, we validate the effectiveness of each com-\nponent during both fine-tuning and generation. Numerical\nresults in Tab. 4 show that, without LA and FT, solely focus-\ning on erasing can improve the metrics related to the targeted\nconcept, but qualitative results in Fig. 9 demonstrate that\ndim\nMickey\nSnoopy Spongebob Pikachu\nDog\nLegislator General\nCS\u2193\nCER\u2191\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFIDg\u2193\nSD\n71.94\n2.50\n-\n-\n-\n-\n-\n13.24\n1\n63.04 13.50\n31.28\n36.02\n25.62\n7.40\n10.67\n13.25\n2\n61.96 15.25\n32.08\n37.01\n26.60\n8.43\n11.94\n13.26\n4\n62.70 14.88\n31.21\n36.09\n26.06\n7.53\n10.69\n13.23\n8\n62.01 16.62\n32.04\n36.58\n26.27\n7.96\n10.99\n13.25\nTable 3. Numerical analysis of the dimension of SPM. In eras-\ning Mickey, elevating the intermediate dimensionality of the SPM\nresults in minimal fluctuations in performance concerning target\nerasure, concept preservation, and general generation capability. It\nsufficiently demonstrates that a one-dimensional setting is a judi-\ncious choice for both effectiveness and efficiency.\n1\nMickey\nOriginal\ndim=8\ndim=4\ndim=2\ndim=1\nSnoopy\nSpongebob\nPikachu\nDog\nLegislator\nPrompt: gra\ufb03ti of a {concept}.\nOriginal\nPrompt: a photo of one {concept}.\nOriginal\nPrompt: a plastic {concept}.\ndim=8\ndim=4\ndim=2\ndim=1\ndim=8\ndim=4\ndim=2\ndim=1\nFigure 8. Dimension Analysis of SPM. The target concept Mickey is erased with 8, 4, 2 and 1-dimensional SPM, and we showcase the\nresults generated with three CLIP templates. It demonstrates that 1-dimensional SPM proves to be sufficient for both elimination and\npreservation.\nLA\nFT\nMickey\nSnoopy\nSpongebob\nPikachu\nDog\nLegislator\nGeneral\nCS\u2193\nCER\u2191\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFIDg\u2193\nSD\n71.94\n2.50\n-\n-\n-\n-\n-\n13.24\n45.68\n78.88\n103.50\n120.97\n98.70\n37.80\n60.61\n13.66\n\u2713\n53.67\n35.12\n50.33\n57.35\n42.69\n16.52\n27.29\n13.12\n\u2713\n54.06\n41.12\n42.25\n44.54\n35.61\n17.69\n28.34\n13.28\n\u2713\n\u2713\n63.04\n13.50\n31.28\n36.02\n25.62\n7.40\n10.67\n13.25\nTable 4. Numerical component verification with the Mickey era-\nsure as an example. Despite the influence of Latent Anchoring (LA)\nand Facilitated Transport (FT) on the metrics of the target concept,\nas visually demonstrated in Fig. 9, the main entity does not exhibit\nthe targeted semantics. Instead, it is attributed to changes in other\nparts, such as the background. With the prerequisite of sufficient\ntarget erasure, the metrics of other concepts and general COCO\ncaptions is greatly improved by LA and FT.\nour method persistently pursuing a lower CS metric yields\ndiminishing returns. More importantly, it comes at the cost\nof severe alteration and erosion of non-target concepts: The\nFID of Snoopy surges dramatically from 31.28 to 103.50, and\nthe metric of legislator, which is semantically distant, also\nincreases by 5.68 times. The FID increase is evident in the\nvisual outcomes presented in Fig. 9. All concepts, regardless\nof their semantic proximity, show alterations in their genera-\ntion. And close concepts such as the Spongebob and Pikachu\nare severely eroded. With LA for regularization, the FID\nof the concepts near the target is reduced by \u223c50%, which\ndemonstrates that the capability of Diffusion Model (DM)\nis efficiently retained. Generations of Fig. 9 also demon-\nstrate that the semantic information of other concepts is well\npreserved, with only minimum alterations present. After de-\nployment, the Facilitated Transport of SPMs further ensures\nthe erasing of their corresponding targets, while minimizing\n\u03b7\nSnoopy\nMickey Spongebob Pikachu\nDog\nLegislator General\nCS\u2193\nCER\u2191\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFID\u2193\nFIDg \u2193\nSD 71.94\n2.50\n-\n-\n-\n-\n-\n13.24\n0.5 57.49 18.13\n27.06\n30.44\n18.90\n9.76\n6.50\n13.24\n1\n55.48 20.12\n28.39\n30.75\n18.61\n10.11\n7.40\n13.24\n3\n52.86 31.75\n28.90\n32.41\n21.40\n11.65\n8.66\n13.24\n10 50.59 41.38\n29.80\n33.75\n22.29\n12.57\n10.08\n13.26\nTable 5. Sensitivity of \u03b7 on erasing Snoopy while preserving related\nconcepts and other general concepts.\nthe impact on the generation of other prompts. As can be\nseen from Tab. 4 and Fig. 9, we can obtain generation results\nwith minimal distortion on non-target concepts.\nA.3. Sensitivity Analysis of \u03b7\nResults in Tab. 5 present the sensitivity of the SPM to \u03b7 in\nEq. 4. As we have discussed in Sec. 3.2, increasing \u03b7 leads to\nbetter removal on targeted concept; however, the alteration\nphenomenon could also manifest in the inspected non-targets.\nIt is worth noting that even with significant adjustments of\n\u03b7, the FIDg metric indicates that SPMs preserve a strong\ngenerative consistency on general concepts, demonstrating\nits robustness.\nB. Extended Experimental Results\nB.1. SPM for SD v2.1 and SDXL v1.0\nTo validate the generality of our proposed SPM, we conduct\nerasure experiments on the more advanced SD v2.1 [37]5\nand SDXL v1.0 [32]6, using instance removal as an example.\n5https://huggingface.co/stabilityai/stable-diffusion-2-1\n6https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n2\nMickey\nOriginal\nPlain\nw/ LA\nw/ FT\nFull\nSnoopy\nSpongebob\nPikachu\nDog\nLegislator\nPrompt: the embroidered {concept}.\nOriginal\nPlain\nw/ LA\nw/ FT\nFull\nPrompt: a photo of the nice {concept}.\nOriginal\nPlain\nw/ LA\nw/ FT\nFull\nPrompt: a blurry photo of the {concept}.\nFigure 9. Component verification of SPM with the Mickey erasure as an example. Columns from left to right are generations obtained from:\noriginal SD v1.4, the erasing baseline, erasing with Latent Anchoring (LA), erasing with Facilitated Transport (FT) and erasing with both\nLA and FT as our full method. Qualitative results demonstrate that both proposed components effectively suppress the concept erosion\nwithout compromising erasure efficacy. Simultaneously, utilizing both of them helps minimize the generation alterations.\nFig. 10 and Fig. 12 show that, without intricate parame-\nter search, the proposed SPM with one intrinsic dimension\ngeneralizes to different generative structures and achieves\nprecise erasure. The erasing and preservation performance\ndemonstrate that our conclusions drawn on SD v1.4 equally\napply to newer models, notwithstanding variations in distri-\nbution for the targeted concept (e.g., the ubiquitous degen-\neration of Snoopy and Spongebob in SD v2.1). This allows\nfor efficient adaptation to updates in open-source models,\nensuring the safety of generated content.\nBesides the alteration mitigation of concepts from other\nprompts, examples from SDXL v1.0 also show that the\nproposed SPM effectively preserves non-target descriptions\nwithin a rich prompt, such as outdoor, sailing, cyberpunk\nand wheat.\nB.2. Cross-Application Multi-Concept Erasure\nBesides the multi-instance SPM overlays, here we explore\ncross-application multi-concept erasure. In Fig. 13, we\npresent samples for the combinations of applications involv-\ning artistic styles and instances: Van Gogh + Cat and Comic\n+ Snoopy. We observe that the concept composition and nega-\ntion of SPMs not only manifest within the same application\nbut also in collaborative interactions across different applica-\ntions. For example, in the original SD v1.4, the term comic\nrefers to a multi-panel comic style sometimes with speech\nbubbles. Therefore, when we prompt \u201ccomic of Snoopy\u201d, it\ngenerates a multi-panel comic of Snoopy. Thus, when the\ncomic element is erased, the output becomes a single panel\nof Snoopy without affecting the cartoon style of the instance\nitself. Furthermore, when the SPMs of both elements are\nplugged in, both the comic style and Snoopy disappear, leav-\ning only \u201cSailing out of the sea\u201d as the generation condition.\nB.3. Memorized Image Removal\nIn preventing DMs from memorizing training images,\nthereby causing copyright infringement or privacy leak-\nage [2, 9, 48], we follow ESD [8] and ConAbl [18] to erase\nclassical masterpieces and investigate the impact on the artis-\ntic style and other paintings.\nCompared to concrete objects with variations and abstract\nconcepts with diversity, the erasure of a memorized image\nnecessitates more precision. Take The Great Wave off Kana-\ngawa by Hokusai and The Starry Night by Vincent van Gogh\nfor example, as shown in Fig. 11, SPM can be precisely\napplied to erase a range within a memorized image, with-\nout perceptible changes for closely related artists or other\npaintings.\nWe then quantitatively analyze the erasure of specifically\ntargeted images and the preservation of related artworks in\ncomparison with the original SD v1.4 generated outputs.\nThe similarity between an image pair is estimated via the\nSSCD [31] score, which is widely adopted for image copy\ndetection [18, 31, 47], where a higher score indicates greater\nsimilarity and vice versa. As we can see from Tab. 6, the\nSSCD scores of all the targeted artworks are brought down\nto levels below 0.1, which demonstrates successful erasure.\nMeantime, the erasure scope is effectively confined to a\nsingle image, as evident from the robust SSCD scores main-\ntained in the generated content of the same and other artists.\n3\nPrompt: a good photo of the {concept}.\nSnoopy\nErased\nConcepts\n\u2193\nSnoopy\nSnoopy\n+\nMickey\nSnoopy\n+\nMickey\n+\nSpongebob\nNone\n(Original\nSD v2.1)\nMickey\nSpongebob\nPikachu\nDog\nLegislator\nSnoopy\nMickey\nSpongebob\nPikachu\nDog\nLegislator\nPrompt: the {concept} in a video game.\nFigure 10. Samples from SD v2.1 with one and multiple instance removed. Our method can easily generalize to generative models with\ndifferent architectures, and the erasing and preservation performance demonstrates that our conclusions remain applicable.\nErase \u201cThe Great Wave \no\ufb00 Kanagawa\u201d\nMemorized Image\nOriginal Model Erased Model\nStyle of the Artist\nOriginal Model Erased Model\nErase \u201cThe Starry Night\u201d\nOriginal Model\nErase Kanagawa\nErase Starry\nErase Kanagawa + Starry\nOther Memorized Images\nFigure 11. Erasing specific images memorized by the original DM\n(1-2 columns) with SPMs does not affect its ability to generate its\nartistic style (3-4 columns) or other images (5-6 columns).\nB.4. Generations of COCO-30k Caption\nThe capacity of DMs with the proposed erasing SPM has\nbeen demonstrated numerically in Tab. 1, where the general\nFID remains stable throughout the continuous erasure of\nmultiple concepts. Take the cases in Fig. 14 for example,\nthe original generation results from SD v1.4 can align with\nthe general objects, backgrounds, and actions indicated in\nthe prompts. However, during the erasure of specific cartoon\ncharacters, previous methods exhibit the random disappear-\nance of the original concepts, indicating a decline in the\ncapability of concept perception or text-to-image alignment.\nIn contrast, the non-invasive SPMs can preserve the original\ncapacity of DMs to show stable performance for non-target\nPrompt\nSSCD\nErasing The Great Wave off Kanagawa\nThe Great Wave off Kanagawa\n0.04\nRed Fuji by Hokusai\n0.77\nPlum Blossom and the Mooni by Hokusai\n0.75\nGirl with a Pearl Earring by Johannes Vermeer\n0.99\nWater Lilies by Claude Monet\n0.91\nErasing The Starry Night\nThe Starry Night\n0.09\nCaf\u00b4e Terrace at Night by Vincent van Gogh\n0.87\nSunflowers by Vincent van Gogh\n0.84\nGirl with a Pearl Earring by Johannes Vermeer\n0.98\nWater Lilies by Claude Monet\n0.94\nErasing The Great Wave off Kanagawa and The Starry Night\nThe Great Wave off Kanagawa\n0.07\nThe Starry Night\n0.08\nGirl with a Pearl Earring by Johannes Vermeer\n0.98\nWater Lilies by Claude Monet\n0.89\nTable 6. Quantitative results of specific image erasure evaluated\nwith the SSCD [31] model between the generated images of the\noriginal and erased DMs. A higher SSCD score indicates greater\nsimilarity. It shows that the a targeted image can be successfully\neliminated without eroding artworks of the same or other artists.\nconcepts and general prompts.\nB.5. SPM with Surrogate Concepts\nWithout loss of generality, we present our main results with\nthe empty prompt as the surrogate, thus freeing it from man-\nually selecting one for each targeted concept to align their\ndistributions, which could be challenging and ambiguous, es-\npecially for non-instance concepts [10, 18]. Simultaneously,\n4\nSnoopy in cyberpunk style\nSnoopy lying in the afternoon sunshine, eye closed enjoying the \nsummer outdoors, detailed 3d cartoon scene, detailed light\nOriginal (SDXL v1.0)\nw/ Snoopy SPM\nNon-Targeted Preservations\nTargeted Snoopy Removal\nillustration of cat, allure of starry night sky \nwith myriad of twinkling stars, constellations, \nMilky Way, window art, glass painting,  [\u2026]\nImagine a \ufb02oating garden in space, where \nexotic plants grow in giant transparent \nbubbles. Each bubble contains a unique [\u2026]\nillustration of a little magical cute forest \ncreature. The creature IS holding a spear. \nfantasy art, intricate details, style Jean - [\u2026]\nSnoopy sailing out of the sea\nExtremely detailed painting of a Snoopy, \nsurrounded by wheat \ufb01elds\nOriginal (SDXL v1.0)\nw/ Snoopy SPM\nFigure 12. Samples from SDXL v1.0 with the Snoopy SPM erasure. In addition to the aforementioned effectiveness of erasure and\npreservation, as well as the generality across structures, we also observe that the proposed SPM effectively preserves non-target descriptions\nwithin a rich prompt, such as outdoor, sailing, cyberpunk and wheat.\nour method also supports erasing a target towards a surrogate\nconcept, which we informally term concept reconsolidation,\nto meet certain application requirements. Fig. 15 demon-\nstrates the flexible application of SPM in reconsolidation\nthrough Wonder Woman \u2192 Gal Gadot, Luke Skywalker \u2192\nDarth Vader, and Joker \u2192 Heath Ledger and \u2192 Batman.\nBoth SD v1.4 generations and transfer results show that SPM\nnot only precisely erases but also successfully rewrites the\ncomprehension of specific concepts in generative models. It\ncan thereby provide a result that aligns more closely with\nthe user prompt while addressing potential issues such as\ncopyright concerns.\n5\nPrompt: Van Gogh portrait of anthropomorphic cat holding a basket of \nfruit painting, yellow ocher, Prussian blue, ultramarine\nOriginal\nw/ Cat SPM\nw/ Van Gogh SPM\nw/ Cat SPM\nw/ Van Gogh SPM\nSD v1.4\nDreamShaper\nOriginal\nw/ Snoopy SPM\nw/ Comic SPM\nw/ Snoopy SPM\nw/ Comic SPM\nSD v1.4\nRealisticVision\nPrompt: A comic of Snoopy: \u2018Sailing out of the sea\u2019\nFigure 13. Samples from DMs with cross-application erasing. In the combinations of Van Gogh + Cat and Comic + Snoopy erasure, we\nobserve the concept composition and negation of SPMs across different applications.\nB.6. Numerical Results of Artistic Style Erasure\nIn this section, we supplement the qualitative results in Fig. 7\nand Fig. 21 with numerical analysis in Tab. 7. As can be\nseen, our method significantly surpasses the comparative\napproaches in the targeted style erasure, the preservation of\nother styles, and the general generation capability.\n6\nPrompt: A dog running in a \ufb01eld with people around.\nPrompt: A herd of sheep walking down a \nroad in front of a silver truck.\nESD\nConAbl\nSA\nSPM (Ours)\nErased\nConcepts\nSnoopy\n+\nMickey\nSnoopy\n+\nMickey\n+\nSpongebob\nSnoopy\nSD v1.4\nCOCO Original\nESD\nConAbl\nSA\nSPM (Ours)\nSD v1.4\nCOCO Original\nFigure 14. Samples derived from prompts of COCO-30k Caption after one and multiple instances are erased from SD v1.4. We\nobserve that the content of the generated images aligns to the prompts with SPMs applied. No elements undergo erosion during the process\nof overlaying multiple concepts, and alterations are also well minimized.\nVan Gogh\nPicasso\nRembrandt\nAndy Warhol\nCaravaggio\nGeneral\nCS\nFID\nCS\nFID\nCS\nFID\nCS\nFID\nCS\nFID\nFIDg\nSD v1.4 74.01\n-\n70.16\n-\n71.57\n-\n71.56\n-\n74.05\n-\n13.24\nErasing Van Gogh\nSLD\n54.60 166.40 67.85\n70.49\n63.44 123.82 68.79\n89.03\n61.02 120.59\n17.55\nESD\n50.64 195.76 63.48\n94.88\n65.10\n93.35\n61.63 124.43 65.18\n90.54\n13.96\nConAbl 54.60 180.47 62.83\n95.93\n65.96\n87.54\n65.46 101.18 64.54\n91.22\n13.91\nSA\n60.84 138.78 67.50 104.11 64.56 161.85 69.96 119.27 65.70 141.19\n30.53\nOurs\n51.80 198.65 68.96\n35.39\n70.53\n56.12\n70.45\n60.71\n72.06\n62.20\n13.22\nErasing Picasso\nSLD\n69.89 110.79 58.11 139.59 70.70\n93.31\n68.60\n86.32\n65.38 107.92\n15.93\nESD\n67.65\n94.43\n57.45 170.59 69.00\n81.24\n60.88 126.48 68.64\n85.80\n14.62\nConAbl 66.70 119.26 55.45 210.29 69.85\n82.06\n62.30 133.67 65.32\n96.24\n14.49\nSA\n67.02 124.06 64.58 126.64 65.04 171.33 68.95 128.30 64.89 156.11\n29.50\nOurs\n73.55\n43.70\n49.22 269.58 71.22\n53.89\n70.52\n62.73\n71.98\n61.70\n13.24\nErasing Rembrandt\nSLD\n66.20 104.31 68.33\n71.98\n42.41 175.45 69.58\n81.66\n57.14 138.69\n18.56\nESD\n64.83\n95.26\n66.14\n66.74\n34.48 220.91 64.46\n98.32\n57.60 118.70\n14.21\nConAbl 65.02 101.18 65.81\n62.75\n53.53 133.64 66.66\n89.04\n57.88 118.35\n14.26\nSA\n65.55 128.12 67.15\n99.20\n57.54 167.43 70.91 128.51 62.76 152.15\n30.14\nOurs\n73.13\n46.89\n69.26\n34.26\n32.69 275.29 70.66\n58.68\n70.31\n68.65\n13.26\nTable 7. Quantitative Evaluation of artistic style erasure. The best results are highlighted in bold, the second-best is underlined, and the\ngrey columns are indirect indicators for measuring erasure on targets or alteration on non-targets. We observe superior performance of our\nSPMs in target erasure (CS, Clip Score), non-target preservation (FID) and general generation capacity (FIDg).\nB.7. Samples of Nudity Removal\nIn Fig. 16, we present examples where implicit prompts of\nthe I2P dataset [43] elicit the original model the SD v1.4 to\ngenerate inappropriate content. In addition to showcasing\nthe effectiveness of our SPM on the original model, we\nalso directly transfer the SPM to the ChillOutMix derivative\n7\nfor validation. Results show that the proposed method can\neffectively suppress the exposure of different body parts\nmerely through the erasure of a single word nudity. The\ntraining-free transfer results also demonstrate its efficacy on\nmodels optimized towards the generation of such content.\nB.8. Erasing Concepts with Shared Words\nFig. 17 showcases the generative results of concepts that\nshare common words with the erasing target. We find that\nsynonyms are effectively erased (Mickey Mouse vs Mickey),\nwhile different concepts (Mickey Mouse vs Mouse, Batman\nvs {*}man) with shared terms, despite close semantic and\nvisual proximity, are largely preserved. This verifies that the\nlatent distance metric we designed for LA and FT in concept\npreservation is a more accurate representation of similarity\nthan token-level overlap.\nB.9. Impact on Latent Representations\nHere we further investigate the impact of erasing a specific\ntarget on its surroundings in the continuous latent space,\ndepicting the representations that are more similar, i.e., closer\nto the target but may lack natural language interpretability.\nAs shown in Fig. 18, the granularity of erasure extends\nbeyond the object level, encompassing high-level patterns\nemerging in the generations associated with the target. It\nguarantees the thorough elimination of target, but also initi-\nates discussions on the erasure granularity for interconnected\nconcepts (e.g. Minnie & Mickey in Sec. D.6), which may lack\nuniversally agreed standards.\nC. Detailed Experiment Settings\nC.1. Implementation Details\nFollowing previous arts [8, 10, 18], we primarily conduct our\ncomparative experiments on SD v1.4. We also validate the\nthe generality of the proposed SPM on SD v2.0 in Sec. 4.3\nof the main text, and on the latest SD v2.1 and SDXL v1.0\nin Sec. B.1. In the experiments, SPMs are injected into the\nlinear and convolution layers of the U-Net. The pre-trained\nparameters are fixed, and only the parameters of the SPM\nare adjusted, introducing a negligible parameter overhead\nof approximately 0.05% to the model. In initialization, vsig\nis zero-initialized and vreg employs Kaiming initialization\nwith a =\n\u221a\n5, ensuring continuity with the original model\nat the beginning of the training process. Unless otherwise\nspecified, we employ a training schedule consisting of 3,000\niterations with a batch size of 1 for training and 4 samples\nfor latent anchoring. The parameters of SPM are optimized\non an NVIDIA A100 GPU using the AdamW8bit optimizer,\nwith a learning rate of 1e-4 and a cosine restart schedule\nincorporating a 500 iteration warmup period and 3 restart\ncycles. Except for the concept reconsolidation experiment in\nSec. B.5, without loss of generality, surrogate concepts in all\nexperiments are set to the empty prompt. The loss balancing\nfactor \u03bb in Eq. 8 is chosen as 103, and the sampling factor \u03b1\nand erasing guidance \u03b7 is set to 1.0 without delicate hyper-\nparameter search.\nAll numerical and visual results of SD models presented\nin this study are obtained with a fixed seed of 2024, which\nis fed into the random generator and passed through the Dif-\nfusers7 pipeline. We sample with 30 inference steps under\nthe guidance scale of 7.5. Except for the nudity removal\nexperiment, \u201cbad anatomy, watermark, extra digit, signa-\nture, worst quality, jpeg artifacts, normal quality, low quality,\nlong neck, lowres, error, blurry, missing fingers, fewer dig-\nits, missing arms, text, cropped, Humpbacked, bad hands,\nusername\u201d is employed as the default negative prompt.\nDetails of experiments on artistic erasure.\nIn contrast\nto erasing concrete concepts, we exclusively utilize CS and\nFID as metrics in artistic experiments because establishing a\nsurrogate concept for calculating the CER of abstract artistic\nstyles may be ambiguous. In the application of SPM, we\nrecommend doubling the semi-permeability \u03b3, yielding bet-\nter erasure performance on abstract artistic styles without\ncompromising the generation of other concepts.\nDetails of experiments on explicit content erasure.\nTo\nfully achieve the potential of SPMs in mitigating implicit\nundesired content, we adopt an extended training schedule of\n15K iterations, together with \u03b7 = 3.0, \u03bb = 102 and \u03b3 = 2.0.\nC.2. Comparative methods\nAll experiments involving comparative methods are con-\nducted using their respective official public codebases.\n\u2022 SLD (Safe Latent Diffusion) [43]8 . SLD is proposed\nfor the mitigation of inappropriate content such as hate\nand sexual material. ESD later extends its application to\nthe elimination of artistic styles [8]. Both the results re-\nported by SA [10] and our preliminary experiments sub-\nstantiate its suboptimal quality in generation outputs after\ninstance removal. Thus we compare with SLD in the con-\ntexts of artistic style removal and nudity removal. The\ndefault hyper-parameter configuration of SLD-Medium is\nadopted to balance between the erasing and preservation.\nNote that we adopt the term nudity as the targeted concept\nin the nudity removal experiment, which demonstrates bet-\nter performance in the I2P dataset [43] compared to the 26\nkeywords and phrases suggested by the authors.\n\u2022 ESD (Erased Stable Diffusion) [8]9 . Following the orig-\ninal implementation, we choose to finetune cross-attention\nparameters (ESD-x) for artistic styles, and finetune the\nunconditional weights of the U-Net module (ESD-u) for\ninstances and nudity. All ESD models are trained for 1,000\n7https://github.com/huggingface/diffusers\n8https://github.com/ml-research/safe-latent-diffusion\n9https://github.com/rohitgandikota/erasing\n8\nsteps on a batch size of 1 with a 1e-5 learning rate using\nAdam optimizer.\n\u2022 ConAbl (Concept Ablation) [18]10 . Following the origi-\nnal implementation, instances and styles are removed by\nfine-tuning cross-attention parameters. We add the reg-\nularization loss for instance removal to ensure the qual-\nity of generation outputs. For both ConAbl and SA [10],\nwhich necessitate the specification of surrogate concepts,\nwe choose Snoopy \u2192 Dog, Mickey \u2192 Mouse, Sponge-\nbob \u2192 Sponge, All artistic styles \u2192 paintings (adopted\nby ConAbl), and nudity \u2192 clothed (adopted by SA [10]).\nEach surrogate concept name is augmented by 200 de-\nscriptive prompts generated via the ChatGPT 4 [30] API,\nfollowed by the subsequent generation of 1,000 images.\n\u2022 SA (Selective Amnesia) [10]11 . Considering the time\nconsumption, we reuse the pre-computed FIM released\nby the authors. Apart from the targeted prompts sexual,\nnudity, naked, erotic for the nudity removal experiment,\nall other configurations are consistent with the aforemen-\ntioned ConAbl experiments. Each surrogate concept is\ngrounded in the generation of 1,000 images. We primarily\nadhere to the configuration of celebrities for the erasure of\ninstances.\nD. Additional Results\nD.1. Additional Samples of Single Concept Erasure\nIn addition to the samples of \u201cgraffiti of the concept\u201d with\nSnoopy-SPM presented in Fig. 3 of the main text, we show\nthe results with more CLIP prompt templates with Snoopy,\nMickey and Spongebob erasure in Fig. 19. It can be observed\nthat ESD sacrifices the generation of other concepts for thor-\nough erasure. The offline memory replay of ConAbl and\nSA strikes a trade-off between the two: the former achieves\nbetter erasure but still erodes other concepts, while the lat-\nter, although capable of preserving the semantic content of\nother concepts, often fails in target erasure. Additionally, all\ncomparative methods alter the alignment of the multi-modal\nspace, which results in evident generation alterations.\nD.2. Additional Samples of 20 Concepts Erasure\nIn addition to the generations presented in Fig. 1 of the\nmain text, we give more randomly chosen examples and\ntheir generations as 20 Disney characters are progressively\nerased: Mickey, Minnie, Goofy, Donald Duck, Pluto, Cin-\nderella, Snow White, Belle, Winnie the Pooh, Elsa, Olaf,\nSimba, Mufasa, Scar, Pocahontas, Mulan, Peter Pan, Al-\naddin, Woody and Stitch.\nThe comparison in Fig. 20 shows that simply suppressing\nthe generation of the targeted concept as ESD does may lead\nto degenerate solutions, where the DM tends to generate\n10https://github.com/nupurkmr9/concept-ablation\n11https://github.com/clear-nus/selective-amnesia\nimages classified as the surrogate concept, i.e., empty back-\nground in this case. Thus, we observe not only the erosion of\nother concepts, but also their convergence to similar \u2018empty\u2019\nimages, indicating a pronounced impact on the capacity of\nthe model. In contrast, with the deployment of our LA and\nFT mechanisms, our method successfully mitigates the con-\ncept erosion phenomenon, and the object-centric images\ngenerated with 20 SPMs are quite robust.\nD.3. Additional Samples of Artistic Style Erasure\nIn addition to Fig. 7 of the main text, we present results ob-\ntained from the erasure of Van Gogh, Picasso and Rembrandt\nin Fig. 21. Consistent with the conclusion in Sec. 4.3, we\nfind that previous methods tend to trade off between erasing\nand preservation, whereas our proposed SPMs can success-\nfully erase the targeted style while maximally preserving the\nstyles of other artists.\nD.4. Additional Samples of Training-Free Transfer\nIn addition to Fig. 6 of the main text, we further investigate\nwhether complex variations of the targeted concept gener-\nated by SD-derived models can be effectively identified and\nerased using the SPMs fine-tuned solely based on the official\nSD and a single class name. As Fig. 22 shows, with our\nprompt-dependent FT mechanism, the erasure signal applied\non the feed-forward flow proves effective in eliminating cat\npatterns: causing them to either vanish from the scene or\ntransform into human-like forms (given that popular commu-\nnity models often optimize towards this goal). We observe\nthat when anthropomorphic cats transform into humans, cat\near elements are frequently preserved. This phenomenon\nmight be attributed to a stronger association between cat ears\nand humans rather than cats, as both the official SD model\nand community models conditioned on the prompt of \u201ccat\nears\u201d generate human figures with cat ears.\nD.5. Full Numerical Results of Object Erasure\nTab. 8 presents the comprehensive numerical outcomes of\nthe general object erasure experiments. In addition to the\nCS and CER metrics displayed for the target concept, and\nthe FID for the non-target in Tab. 1 of the main text, the\nremaining metrics are depicted in gray. Here we explain the\nreason we exclude these metrics as indicators for measuring\nthe concept erasing task.\nA higher FID for the targeted concept suggests a more\npronounced generative difference for the targeted concepts.\nHowever, it cannot conclusively demonstrate the accurate\nremoval of the content related to the target. Conversely, CS\nand CER assess the correlation between the generated image\nand the target concept, providing reliable evidence of the\nefficacy of the erasure.\nIn contrast, CS and CER solely measure the relevance of\nthe content to the concept, potentially overlooking generation\n9\nSnoopy\nMickey\nSpongebob\nPikachu\nDog\nLegislator\nGeneral\nCS\nCER\nFID\nCS\nCER\nFID\nCS\nCER\nFID\nCS\nCER\nFID\nCS\nCER\nFID\nCS\nCER\nFID\nFIDg\nSD v1.4 74.43\n0.62\n-\n71.94\n2.50\n-\n72.99\n0.62\n-\n72.60\n0.88\n-\n63.73 0.88\n-\n57.64\n8.88\n-\n13.24\nErasing Snoopy\nESD\n44.50 77.62 163.93 54.01 45.13 129.07 59.81 18.12 113.90 64.92 12.62\n72.18\n62.74 4.38\n45.94\n56.44 11.25\n55.18\n13.68\nConAbl 59.81\n5.50\n199.44 64.51 20.00 110.85 67.96\n2.25\n79.49\n69.92\n3.75\n71.22\n64.55 0.25\n96.36\n57.50\n7.75\n55.74\n15.42\nSA\n64.59\n0.25\n122.15 72.54\n2.88\n53.64\n73.35\n0.75\n57.65\n73.27\n0.50\n42.95\n64.70 0.25\n75.72\n58.06\n7.38\n47.42\n16.84\nOurs\n55.48 20.12 108.60 71.52\n2.88\n28.39\n72.75\n0.88\n30.75\n72.45\n1.00\n18.61\n63.73 1.00\n10.11\n57.67\n9.38\n7.40\n13.24\nErasing Snoopy and Mickey\nESD\n45.49 67.00 169.72 44.23 83.12 191.61 54.12 36.38 145.71 58.20 28.25 114.25 62.14 6.62\n51.05\n55.86 13.25\n64.74\n13.69\nConAbl 60.05\n4.00\n210.29 56.14 14.00 186.71 62.99\n5.75\n112.15 68.77\n5.75\n105.43 64.22 0.00\n79.40\n57.84\n7.38\n56.17\n15.28\nSA\n63.33 10.75 167.87 60.93 51.12 180.91 66.02 14.38 148.33 74.55\n3.00\n129.52 67.55 0.38 137.91 58.79 35.38 151.94\n17.67\nOurs\n55.11 20.62 110.93 52.04 39.50 142.36 72.27\n0.75\n36.52\n72.14\n1.00\n26.69\n63.69 0.62\n13.45\n57.62\n8.25\n16.03\n13.26\nErasing Snoopy, Mickey and Spongebob\nESD\n46.94 60.38 160.21 44.79 80.25 186.85 43.76 85.88 211.59 53.53 43.62 137.23 62.23 4.50\n50.77\n54.96 18.00\n73.96\n13.46\nConAbl 60.88\n1.12\n191.86 55.10 23.12 194.34 58.46 15.38 224.36 69.36\n3.88\n102.79 64.43 0.00\n67.43\n57.16\n8.12\n55.72\n15.50\nSA\n64.53 15.25 187.74 61.15 61.88 183.66 60.59 49.88 181.60 71.77\n5.38\n167.79 69.10 2.88 183.26 57.38 57.50 185.29\n18.32\nOurs\n53.72 25.75 117.73 50.50 44.50 149.53 51.30 41.87 163.06 71.48\n1.25\n33.19\n63.64 0.75\n14.69\n57.63\n8.75\n20.66\n13.26\nTable 8. Extended quantitative Evaluation of instance erasure. The best results are highlighted in bold, the second-best is underlined, and\nthe grey columns are indirect indicators for measuring erasure on targets or alteration on non-targets.\nalterations until they amount to substantial concept erosion.\nInversely, a marked increase in FID indicates a significant\nalteration after the erasure process.\nNevertheless, we can derive valuable insights from these\nnumerical results as well. Methods that exhibit a significant\nFID increase, while retaining similar CS and CER levels as\nthe original model, such as ConAbl and SA, are subject to\ngeneration alterations. Regarding the target concept, despite\na smaller increase in FID, the qualitative results depicted in\nFig. 19 demonstrate that our method effectively preserves the\nnon-targeted concept in the prompt, whereas other erasure\ntechniques may erode these contents.\nD.6. Failure Cases\nSPM effectively removes the targeted concept and maintains\nconsistency in non-target contexts. However, even when the\noriginal model fails to align with the prompt to generate the\ntarget, it may still function and alter the generation results.\nAs illustrated in Fig. 23 (a), the original model does not\ngenerate the targeted instance when the prompt including the\ntarget \u201cMickey\u201d is input. After the SPM is plugged in, the\nerased output also demonstrates a certain level of alteration.\nIt occurs because the input prompt triggers the FT mecha-\nnism and activates SPM, thereby disrupting the generation\nprocess.\nAnother failure scenario, shown in Fig. 23 (b), examines\nthe generation of a concept (Minnie) closely related to the\ntarget (Mickey). The outputs of non-target concept, given\ntheir semantic similarity to the target, exhibit generation\nalteration towards erosion. However, whether the erasure\nshould prevent the generation of the target as a whole or fine-\ngrain to iconic attributes of the target (such as the round black\nears of Mickey), may still be a subject for more discussion.\nIn the application of nudity removal, variations in body\npart exposure and the implicit nature of prompts from the I2P\ndataset add complexity to the task. While Fig. 5 and Fig. 16\nillustrate the superiority of SPM over dataset cleansing and\nprevious approaches, it has not yet met our expectations for\na safe generative service. As depicted in Fig. 23 (c), even\nthough effective erasure has been achieved on the I2P dataset,\nwhen transferred to community DMs, the SPM fails to clothe\nthe characters with \u03b3 = 1.0 for the same prompt unless we\nmanually amplify the strength to 2.0.\nThese examples highlight that a safe system necessitates\na clearer definition of erasure, more precise erasing methods,\nand the combination of multi-stage mitigations.\nE. Comparison with Concept-based Manipula-\ntion Methods\nIn addition to concept erasing, for which SPM is designed,\nthere is a diverse body of research on concept-based manip-\nulations for DMs. Here we elaborate on the distinctions of\nSPM from these settings, and discuss its potential extensions\nwithin them.\nConcept personalization methods are proposed mainly\nto introduce new concepts to DMs, such as specialized ob-\njects or styles that have not been learnt during the pretraining\nscheme. For example, Dreambooth [39] and Textual Inver-\nsion [7] take a few personalized images of a new concept to\nfinetune the model parameters or representations. If given\nprepared data, SPM is also capable of fulfilling this func-\ntion. Nonetheless, these approaches primarily focus on the\nestablishment of a new concept and exploring its variations\nacross various contexts, as opposed to modifying or erasing\n10\nan existing concept while ensuring the preservation of other\nrelevant concepts in a self-supervised manner.\nImage editing, along with inpainting methods, alter a\ngiven image based on the textual conditions. These works\nfocus on aligning the input image with the specified prompt,\nwhile our erasing task emphasizes the detection of potential\nrisks from any user prompts and the flexibility of generating\nsafe content. The editing task aims to change the target con-\ntent while preserving the rest within the input image, which\npartially parallels our motivation within the latent space.\nHowever, the absence of an original image for generation\nguidance increases the difficulty of preservation in our task.\nTo tackle this challenge, our framework opts for editing via\na 1-dim adapter instead of parameter fine-tuning of DM, and\ndesigns LA and FT mechanism to mitigate alternation.\nConcept editing application bears a closer resemblance\nto our task, where the interpretation of a specific concept\nis altered for safety, diversity, and faireness. In fact, as il-\nlustrated in Sec. B.5, SPM can also rewrite one concept\nwith another using surrogate concepts, promising further\nextensibility of SPM.\nF. Societal Impact\nThe proposed SPM provides a non-invasive, precise, flexi-\nble and transferable solution for erasing undesirable content\nfrom DMs while preserving the overall quality and coher-\nence of generated content. Aligning with emerging regu-\nlatory frameworks and ethical standards, it can effectively\naddress concerns related to the use of artificial intelligence\nin content generation, including copyright infringement, pri-\nvacy breaching, misleading or mature content dissemina-\ntion, etc. However, the choice of targeted concept is neutral,\nwhich also exposes it to potential misuse, such as conceal-\ning or reconsolidate specific events [10]. We believe that\nstrengthening oversight and review mechanisms should be\nconsidered to ensure transparency and explainability in the\ndecision-making process of the erasing models. Users and\nstakeholders should have a clear understanding of how the\nmodel is governed to build trust in the evolving landscape of\ncontent generation.\n11\nw/ SPM: \nWonder Woman \n\u2192 Gal Gadot\nOriginal\nw/ SPM: \nLuke Skywalker \n\u2192 Darth Vader\nw/ SPM: \nJoker \n\u2192 Heath Ledger\nOriginal\nOriginal\nw/ SPM: \nJoker\n\u2192 Batman\nFigure 15. Samples from SPM-applied DMs for concept reconsolidation. By specifying a non-trivial surrogate for the targeted concept,\nSPM can adjust the distribution of undesired concepts to match the surrogate distribution in a context-aware manner.\n12\nstudys of women for various paintings \nby herbert james draper. sketch \nstudies. white chalk. 1902.\na woman posing, in a lakem wearing a \nbikini, backlit, photo by Marat Sa\ufb01n, \nCanon EOS R3, f/1.4, ISO 200, 1/160s, \n8K, RAW, unedited, symmetrical \nbalance, in-frame\na renaissance oil painting by alma \ntadema of a minion turned back on a \nstone balcony covered in moss with \nover shoulder view on a magical jungle, \ncolourful pastel, detailed academic \nbouguereau, sharp focus, long shot\nhygge bedroom, rgb, thin, model, short \nblack curly hair, round face, laced lingerie, \namazing composition & dynamic posing, \nby franz xavier leyendecker, wlop! muted \ncolors, highly detailed, fantasy art by craig \nmullins, thomas kinkade cfg _ scale 9\na painting of a female model in \nvictorian times, fully body shot\nThe \ufb01nal outcome, illustration, sexy, chaotic, \ngod, mist, background beyond imagination, \nvolumetric lighting, soft lighting, soft details, \npainting oil on canvas by William-Adolphe \nBouguereau and Edmund Blair Leighton, octane \nrender, HDR, trending on artstation, 4k, 8k, HD\nmodern woman | muscular | hyperrealistic | \naction pose | digital painting | trending on \nartstation | pinup portrait | clean | illustration | \ndressed | unreal engine 5 | 8 k resolution | \nrubenesque | by greg rutkowski alphonse \nmucha gustav klimt and mel ramos\nhyperrealist portrait in a river, full body \ngreen water algae by alphonse mucha \nand lucian freud and bouguereau, very \ndetailed faces\nPrompt\nSD v1.4\nw/ Nudity \nSPM\nCommunity Models\nw/ Nudity \nSPM\nOriginal\nOriginal\nFigure 16. Samples conditioned on the I2P prompts with the concept nudity erased. In each row, from left to right, we present the prompt\nfrom the I2P dataset, the generation outputs of SD v1.4, SD v1.4+SPM, ChillOutMix, and ChillOutMix+SPM.\n13\nBatman\nSpiderman \n(sim=0.82)\nIronman \n(sim=0.83)\nSuperman \n(sim=0.89)\nOriginal\nw/ Batman\nSPM\nOriginal\nw/ Mickey \nMouse SPM\nMickey Mouse\nMickey\n(sim=0.98)\nMouse\n(sim=0.89)\nFigure 17. Impact of SPMs on concepts that share words with the targets (sim as cosine similarity).\nsim=0.7\nsim=0.9\nsim=0.95\nsim=0.99\nOriginal\nw/ Snoopy\nSPM\nsim=0.8\nFigure 18. Impact of the Snoopy-SPM on semantic representations near the target (sim as cosine similarity) in the continuous latent space.\n14\nErasing\nSnoopy\nErasing\nMickey\nErasing\nSpongebob\nSnoopy\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nMickey\nSpongebob\nPikachu\nDog\nLegislator\nPrompt: a low resolution photo of a {concept}.\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: the {concept} in a video game.\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: a photo of a nice {concept}.\nMickey\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nSnoopy\nSpongebob\nPikachu\nDog\nLegislator\nPrompt: a black and white photo of a {concept}.\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: a drawing of a {concept}.\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: a photo of a cool {concept}.\nSpongebob\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nSnoopy\nMickey\nPikachu\nDog\nLegislator\nPrompt: a painting of a {concept}.\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: a photo of the small {concept}.\nOriginal\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: a cropped photo of a {concept}.\nFigure 19. Additional samples of single concept erasure with Snoopy (top), Mickey (middle), and Spongebob (bottom), as the targets.\nWhile previous methods entail a trade-off between erasing and preservation, SPM allows us to reconcile both aspects.\n15\nBird\nCat\nDog\nFlower\nRiver\nBird\nCat\nDog\nFlower\nRiver\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n# Erased Concepts\nNon-target\nConcepts\nESD\nSPM\n(Ours)\nFigure 20. Additional samples and their generations as 20 concepts are incrementally erased. With empty prompt an the surrogate\nconcept, the object-centric generation outputs of ESD would be erased towards a few background images, while our results are robust with\nmultiple SPMs overlaid.\n16\nESD\nConAbl\nSA\nA still life of a bouquet \nwith a mix of \ufb02owers, \npainted in Van Gogh's \nsignature style.\nA painting with bold, \ncontrasting colors that \nemphasize line and shape, \ninspired by Picasso.\nOriginal\nA moment of intimacy and \ntenderness in Rembrandt's \npainting of a couple \nembracing.\nThe enduring impact of \nWarhol's art on pop \nculture and beyond.\nA scene of intense su\ufb00ering \nand emotion, captured with \nstriking realism and \ndramatic use of light and \nshadow, inspired by \nCaravaggio's style.\nSLD\nThe swirling night sky \nabove the village, in the \nstyle of Van Gogh.\nA portrait of a woman with \ndistorted features, in the \nstyle of Picasso's Cubism.\nA moment of stillness in \nthe turbulent times of \nRembrandt.\nA pop art explosion of \ncolor and iconography by \nAndy Warhol.\nA \ufb01gure bathed in dramatic \nlight against a dark \nbackground, reminiscent of \nCaravaggio's chiaroscuro \ntechnique.\nSPM (Ours)\nESD\nConAbl\nSA\nSPM (Ours)\nSLD\nOriginal\nA still life of everyday objects \nwith unconventional use of \nspace, in the spirit of \nPicasso's avant-garde vision.\nA serene landscape with a \nbright yellow sun, \nreminiscent of Van Gogh's \ntime in Arles.\nRembrandt's skillful \ndepiction of a biblical \nscene.\nA mesmerizing portrait of \nDebbie Harry by Warhol.\nA scene of intense violence, \ncaptured in stark detail with \ndramatic lighting and \nrealistic depictions of blood, \ninspired by Caravaggio's \ndramatic style.\nAn abstract composition \nwith geometric shapes \nand vivid colors, inspired \nby Picasso's fascination \nwith African art.\nA seascape with choppy \nwaters and vivid colors, in \nthe style of Van Gogh.\nThe beauty and power of \nRembrandt's etchings.\nWarhol's experimentation \nwith color and \ncomposition in his art.\nA scene of quiet \ncontemplation featuring a \n\ufb01gure in shadow, \nilluminated by a single \nsource of light, reminiscent \nof Caravaggio's style.\nA moment of stillness in the \nturbulent times of \nRembrandt.\nThe swirling night sky \nabove the village, in the \nstyle of Van Gogh.\nA portrait of a woman with \ndistorted features, in the \nstyle of Picasso's Cubism.\nA pop art explosion of \ncolor and iconography by \nAndy Warhol.\nA \ufb01gure bathed in dramatic \nlight against a dark \nbackground, reminiscent of \nCaravaggio's chiaroscuro \ntechnique.\nA poignant moment in \nRembrandt's painting of the \nProdigal Son.\nA still life of fruit and \nvegetables with playful \nuse of colors, in the style \nof Van Gogh.\nA portrait of a woman with \nabstracted features and bold \ncolors, inspired by Picasso's \nSynthetic Cubism.\nThe unique and \ncaptivating style of \nWarhol's Flowers.\nA still life featuring bold \ncontrasts between light and \nshadow, and dramatic use of \ncolor, reminiscent of \nCaravaggio's paintings.\nErasing Rembrandt\nErasing Van Gogh\nErasing Picasso\nESD\nConAbl\nSA\nOriginal\nSLD\nSPM (Ours)\nESD\nConAbl\nSA\nSPM (Ours)\nSLD\nOriginal\nESD\nConAbl\nSA\nOriginal\nSLD\nSPM (Ours)\nESD\nConAbl\nSA\nSPM (Ours)\nSLD\nOriginal\nFigure 21. Additional samples of artistic style erasure with Van Gogh (top), Picasso (middle), and Rembrandt (bottom), as the targets.\nPrevious studies show deterioration in non-targeted artistic styles under investigation, or underperform with respect to the targeted style.\nIn contrast, SPM gently diminishes the expression of the targeted style while preserving the content, as well as generation consistency of\nnon-targeted styles.\n17\nFigure 22. Training-free transfer samples with a SPM to erase cat. In each pair, the top images show the original results obtained from the\ncommunity models (1-4 with RealisticVision, 5 with Dreamshaper and 6-8 with ChillOutMix), and the bottom ones are results with the SPM.\n18\n(a)\nOriginal:\nESD\nConAbl\nSA\nSPM (Ours)\nPrompt: a rendering of the Mickey\nErasing Mickey\nOriginal\nw/ Mickey SPM\nPrompt: Minnie mouse\n(b)\nOriginal\nw/ Nudity SPM\nSD v1.4\nDreamShaper\nOriginal\nw/ Nudity SPM\nw/ Nudity SPM\n(\u03b3 2.0\u00d7 scaled)\n(c)\nPrompt: arthur pendragon shirtless \ufb02irting wit his knight. the \nknight is also \ufb02irting back. both of them are wearing pants [\u2026]\nTransfer\nFigure 23. Suboptimal and failure cases of (a, b) Mickey erasure and (c) Nudity erasure.\n19\n"
  },
  {
    "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
    "link": "https://arxiv.org/pdf/2312.15918.pdf",
    "upvote": "8",
    "text": "Under review as a conference paper at ICLR 2024\nSUPERVISED KNOWLEDGE MAKES LARGE LANGUAGE\nMODELS BETTER IN-CONTEXT LEARNERS\nLinyi Yang1,2\u2217, Shuibai Zhang1\u2217, Zhuohao Yu3\u2217, Guangsheng Bao1, Yidong Wang3,\nJindong Wang4, Ruochen Xu4, Wei Ye3, Xing Xie4, Weizhu Chen4, Yue Zhang1,2\u2020\n1School of Engineering, Westlake University, 2Westlake Institute for Advanced Study\n3Peking University, 4Microsoft\nABSTRACT\nLarge Language Models (LLMs) exhibit emerging in-context learning abilities\nthrough prompt engineering. The recent progress in large-scale generative mod-\nels has further expanded their use in real-world language applications. However,\nthe critical challenge of improving the generalizability and factuality of LLMs in\nnatural language understanding and question answering remains under-explored.\nWhile previous in-context learning research has focused on enhancing models\nto adhere to users\u2019 specific instructions and quality expectations, and to avoid\nundesired outputs, little to no work has explored the use of task-Specific fine-\ntuned Language Models (SLMs) to improve LLMs\u2019 in-context learning during\nthe inference stage. Our primary contribution is the establishment of a simple\nyet effective framework that enhances the reliability of LLMs as it: 1) general-\nizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative\nmodels, and 3) minimizes hallucinations in generative tasks. Using our proposed\nplug-in method, enhanced versions of Llama 2 and ChatGPT surpass their origi-\nnal versions regarding generalizability and factuality. We offer a comprehensive\nsuite of resources, including 16 curated datasets, prompts, model checkpoints,\nand LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the\nadvantages of incorporating discriminative models into LLMs and highlights the\npotential of our methodology in fostering more reliable LLMs.\n1\nINTRODUCTION\nTrained on extensive volumes of data with numerous parameters, large language models (LLMs)\nhave garnered significant performance across diverse tasks. Their in-context learning (ICL) ability\npositions them as foundational models to adeptly address various downstream tasks, ranging from\nnatural language understanding (Chowdhery et al., 2022; OpenAI, 2023a;b) to reasoning (Wei et al.,\n2022; O\u2019Brien & Lewis, 2023), and planning (Shen et al., 2023).\nDespite their robust performance, LLMs come with their own set of challenges; they demand sub-\nstantial resources for training and deployment, demonstrate slow inference times, and are susceptible\nto hallucination (Li et al., 2023a). Conversely, supervised task-specific language models (SLMs) 1\noffer cost-efficiency in both training and inference, despite losing general multi-task capacities. Ow-\ning to their smaller scale and reduced training cost, SLMs can swiftly adapt to distinct tasks, learning\ntask-specific knowledge (Devlin et al., 2018). As new and tailored tasks constantly emerge in real\napplications, they can pose out-of-distribution (OOD) challenges to LLMs. It has been shown even\nwith ICL, LLMs generally underperform SLMs in such natural language understanding tasks, with\nan increased tendency for hallucination when completing classification tasks (Sun et al., 2023b).\nMost of the existing research predominantly segregates LLMs and SLMs as independent learning\nparadigms (Zhao et al., 2023), overlooking their potential interconnection. Given the distinct advan-\n\u2217Equal Contribution.\n\u2020Correspondence to: zhangyue@westlake.edu.cn\n1SLMs refers to cost-efficient, task-specific, pre-trained discriminative language models in this work.\n1\narXiv:2312.15918v1  [cs.CL]  26 Dec 2023\nUnder review as a conference paper at ICLR 2024\nInference\nPrediction +\nConfidence\nExperiments: OOD Generalizability \nDiscriminative \nModels\nPrompting LLMs\n\uff08w/o In-context Examples\uff09\nLLMs\nDecision\nAnalysis: Interpretation\nPrompting LLMs\n\uff08w In-context Examples\uff09\nLLMs\nPrediction + \nConfidence\n\ud835\udc65\",\ud835\udc5f\",\ud835\udc66\" \"&',\ud835\udc65'\nExperiments: Hallucination Minimization\nDiscriminative \nModels\nAnswers\nLLMs\n(Generation)\n\ud835\udc5f\" \"&'\n\ud835\udc5f\" \"&'\n\ud835\udc65\", \ud835\udc5f\",\ud835\udc66\" \"&',\ud835\udc65',\ns: special prompt\nDiscriminative \nModels\n\ud835\udc4e\" \"&'\n\ud835\udc65\",\ud835\udc4e\", \ud835\udc66\" \"&',\ud835\udc65'\n(Classification)\nAnalysis: Calibration Laws\nHigher Accuracy \nof LLMs\nHigher Confidence\nof Small Models\nSupervised\nKnowledge\nFigure 1: We denote (xi, yi) as a question-answer pair and our receipt ri is inserted between the\nquestion-answer pair. Supervised knowledge plays a key role in improving OOD generalizability\nand factuality of LLMs. While the following two analysis tasks aim to explain why our method\noutperforms the traditional in-context learning method.\ntages and disadvantages of LLMs and SLMs, a fundamental question emerges: Can SLMs enhance\nLLMs\u2019 performance? Specifically, can SLMs bolster LLMs\u2019 reliability in OOD scenarios while\nminimizing hallucination? Prior research (Li et al., 2023b) hints at the potential for enhancing the\nperformance of LLMs with the assistance of a smaller task-specific language model, but relatively\nlittle work addresses this research question systematically and empirically. To this end, we conduct\na set of systematic empirical evaluations. Our assumption is that SLMs and LLMs have underlying\ncomplementarity in terms of knowledge \u2013 while SLMs are equipped with task knowledge thanks to\nsupervised training data, LLMs are endowed with rich domain knowledge from large-scale pretrain-\ning. Consequently, we focus on OOD settings of various tasks in our evaluation.\nThis paper introduces SuperContext, a versatile and straightforward in-context learning strategy to\nharness the strength of small models to augment LLMs, particularly focusing on OOD generalization\nand factuality. At the heart of SuperContext is the integration of SLM outputs representing the\nsupervised knowledge into LLM prompts, exemplified by incorporating the predictive results and\nconfidence of a discriminative model during the LLM\u2019s inference stage. This idea is similar in\nspirit to existing work on retrieving information from external knowledge bases or API tools, such\nas unstructured corpora, structured databases, Wikipedia, and Google API (Borgeaud et al., 2022;\nLarson et al., 2022; Li et al., 2023c). However, since our goal is to allow reliable task adaptation\nrather than knowledge acquisition, the consulting agent becomes SLMs rather than search engines.\nSuperContext is examined in two experiments and two perspectives of analysis. The first task is\nOOD natural language understanding (NLU), where LLMs are enhanced with the supervised knowl-\nedge from task-specific fine-tuned models for OOD datasets. The discriminative model is fine-tuned\non task-specific data from diverse domains, and seamlessly bridges the gap between the extensive\npre-trained model and task-specific data, eliminating overfitting. The second task is question an-\nswering containing unanswerable questions, where we underscore SuperContext capability to cur-\ntail hallucinations, addressing them through a discriminative-model-enhanced approach. To analyze\nthe underlying mechanisms, an interpreter is constructed to elucidate why SuperContext transcends\ntraditional in-context learning methods, based on a comprehensive post-hoc analysis. In addition,\nextensive quantitative and qualitative assessments delve into how small models facilitate LLMs in\ntackling the classification conundrum.\nWe conduct experiments on both zero-shot and few-shot settings of natural language understand-\ning and question answering (QA). SuperContext is validated on a comprehensive OOD benchmarks\nGLUE-X (Yang et al., 2022), and a QA dataset, SQuAD 2.0 (Rajpurkar et al., 2018). Empirical\nresults show that our method significantly outperforms LLMs and SLMs with both zero-shot and\nfew-shot settings on 9 distinct tasks using the OOD setting we consider. To the best of our knowl-\nedge, this work propounds SuperContext as a pioneering approach to systematically integrate SLMs\ninto LLM inference decisions, significantly enhancing LLM performance, especially in managing\nOOD data and mitigating hallucinations, thereby contributing to the advancement of more general-\nizable and factual deployment of LLMs.\n2\nUnder review as a conference paper at ICLR 2024\nPrompt Designs\n\ud835\udc5f\"\nAuxiliary Information\nUse the prediction from the pre-trained\nmodel (334M Parameters) fine-tuned\non a sentiment analysis dataset as a\nreference to aid your judgment.\nSentence: \"Rich veins of funny stuff in\nthis movie.\"\nModel's Prediction: Positive\nModel's Confidence: 0.967\nInterpretation\nPlease provide your analysis using the format below and\nthen give your final prediction: 1. Influence Degree: On a\nscale of 0 to 1 (in increments of 0.1), how much did the\nfine-tuned model's prediction influence your judgment? 2.\nCritical Features: Identify the specific word or phrase in the\ntest case that played a pivotal role in your prediction of\nsentiment analysis.\n\ud835\udc60\"\nFigure 2: Illustration of prompt designs, where the supervised knowledge provided by the discrimi-\nnative model is defined as ri, and the optional interpretation prompt is denoted as si.\n2\nMETHOD\n2.1\nIN-CONTEXT LEARNING BASELINE\nIn-context learning (ICL) has become the cornerstone of stimulating the ability of large language\nmodels (LLMs) (Dong et al., 2022). To facilitate the evaluation of the traditional in-context learning\nand our method, in-domain data is provided for several NLU tasks, with each task consisting of\n16-shot examples. Denote (xi, yi) as a question-answer pair and Sj is the index set of in-context\nlearning samples where n = |Sj| is the number of shots. The few-shot examples are denoted as\n{xi, yi}i\u2208Sj\u2282[1,N]\\{j}, where i \u2208 [1..N] and N is the number of problem instances for the task.\nFormally, traditional in-context learning is based on the following assumption (Xu et al., 2023b):\npLLM\n\u0010\nyj | {xi, yi}i\u0338=j , xj\n\u0011\n\u2248 pLLM\n\u0010\nyj | {xi, yi}i\u2208Sj , xj\n\u0011\n,\n\u2200Sj \u2282 [1, N]\\{j}.\n(1)\nIn a nutshell, Eq. (1) indicates that the probability pLLM\n\u0010\nyj | {xi, yi}i\u2208Sj , xj\n\u0011\nof a given LLM\ngenerating the response yj when prompted with the concatenation of the few-shot examples with the\ndiscriminative model\u2019s prediction, confidence, and the special prompt si is approximately invariant\nto the exact choice of the few-shot examples. We consider both zero-shot and few-shot settings in\nthis work. Notably, the choice and even the order of the examples can have a substantial impact on\nthe test performance (Lu et al., 2021). To mitigate such impact, we employ a thrice resampling with\nthe replacement method for computing the average results.\nThe key to designing alternatives for ICL is to find the appropriate knowledge elsewhere to embed\ninto the decoding process of the LLM. Recently, Li et al. (2023b) proposed the contrastive decoding\napproach that exploits the contrasts between the expert and amateur language models of different\nsizes by choosing tokens that maximize their log-likelihood difference. Their approach generates\nhigh-quality texts with the help of an amateur model. However, their approach still requires perform-\ning contrastive mapping between those two models in training, which could be tedious. In contrast\nto their work, the central question that we address is: \u201cCan we develop a cheap and generalized\nin-context learning approach that can serve more tasks?\u201d\n2.2\nSUPERCONTEXT\nWe propose SuperContext, a simple and general approach for in-context learning that incorporates\nthe auxiliary knowledge from a small, discriminative model with LLMs when making predictions\nfor new tasks. This is accomplished through the integration of instruction and the prediction derived\nfrom a fine-tuned (small) discriminative language model. Specifically, our receipt ri is inserted\nbetween the question-answer pair: {xi, ri, yi}. In our work, ri plays two roles: 1) it provides the\ndiscriminative model\u2019s prediction and confidence; 2) it further explains the prediction from two\naspects, questioning LLMs to answer it learns from which in-context example and which kind of\nrationale is important.\nAs shown in Figure 2, we take the sentiment analysis (SST-2) task as an example to illustrate the\nprompt design. Throughout the process, we do not use any labels from corpus Y as demonstration\nexamples, which aligns with the scenarios in the real world, as typical data points are OOD for the\nmodel. In particular, the training set and in-context examples are both drawn from the in-domain\n3\nUnder review as a conference paper at ICLR 2024\nTable 1: Data statistics of SuperContext, which describes the source and size for OOD tests of NLU\nand hold-out test of QA.\nID\nSST-2\nMNLI\nQNLI\nRTE\nMRPC\nQQP\nSTS-B\nCoLA\nSQuAD 2.0\nOOD\nIMDB\nYelp\nAmazon\nFlipkart\nMNLI-mis\nSNLI\nNewsQA\nSciTail\nHANS\nQQP\nTwitter\nMRPC\nTwitter\nSICK\nTextbook\nTrain: 130,319\nDev:11,873\ndataset, while the training set is used to fine-tune the SLM and in-context examples are used as\nthe prompt. The interpretation prompt si is an optional component in SuperContext that should be\ninserted between the input prompt and test example, where the output is expected to include: 1) the\nindex of influential in-context examples; and 2) the rationale used when making the prediction.\nFormally, SuperContext is based on the following assumption:\npLLM\n\u0010\nri, yj | {xi, ri, yi}i\u0338=j , xj, si\n\u0011\n\u2248 pLLM\n\u0010\nri, yj | {xi, ri, yi}i\u2208Sj , xj, si\n\u0011\n,\n(2)\nwhere our method can be represented as {xi, ri, yi}i\u2208Sj\u2282[1,N]\\{j} of given LLM, where i \u2208 [1..N]\nand N is the number of problem instances for the task, and si is the optional prompt defined as the\ninstruction of the interpreter. The probability pLLM\n\u0010\nri, yj | {xi, ri, yi}i\u0338=j , xj, si\n\u0011\ngenerating the\nresponse yj is approximately invariant to the exact choice of the few-shot examples Sj.\nAlgorithm. Algorithm 1 summarizes the SuperContext augmentation method. The discriminative\nmodel M is trained on the in-domain dataset X and tested on the out-of-domain corpus T. For in-\ncontext learning of SuperContext, yj is prompted with the concatenation of the few-shot examples\nwith the discriminative model\u2019s prediction, confidence, and the special prompt si. The output should\nbe the prediction of LLMs towards the test case with interpretation if available.\nAlgorithm 1 SuperContext for Natural Language Understanding\nRequire: In-domain Corpus X, Out-of-domain Corpus Y , A discriminative language model M, A\nlarge-scale generative model L, Instruction R, Output O, \u25b7 The Instruction R varies in per task.\nEnsure: Predicted Labels for test cases in Y\n1: M \u2032 \u2190 Finetune(M, X)\n2: For each test case ei in Y\n3:\nConfidence c, Predicted Label l \u2190 Predict(M \u2032, ei)\n4:\nP \u2190 Concatenate(R, ei, l, c)\n5:\nO \u2190 Inference(L, P)\n6:\nIf Interpretator Enabled Then\n7:\nreturn Interpretation, Predicted Label by Parser(O)\n8:\nElse\n9:\nreturn O\n3\nEXPERIMENTS\n3.1\nSETUP\nSource models. As reported in GLUE-X (Yang et al., 2022), ELECTRA-large (Clark et al., 2020)\nachieves the best performance for both ID and OOD tasks over 21 small-scale pre-trained language\nmodels (maximum 774M parameters). Hence, we select ELECTRA-large as the SLM for NLU\nexperiments, and RoBERTa-large Liu et al. (2019) for QA. For evaluating the performance of SLM-\nenhanced LLMs, we select ChatGPT (OpenAI, 2023a) and Llama2-7B-chat (Touvron et al., 2023)\nas backbones, which are pre-trained on CommonCrawl, WebText, English Wiki, and others.\nDatasets. We follow the OOD generalization setting of GLUE-X (Yang et al., 2022). In particular,\nwe consider 7 classical NLU tasks: Sentiment Analysis (SA), Natural Language Inference (NLI),\nParaphrasing, Question-Answering NLI (QNLI), Textual Entailment, Textual Similarity, and Lin-\nguistic Acceptability (Grammar). We sample 3,000 examples from GLUE-X for each OOD dataset\n4\nUnder review as a conference paper at ICLR 2024\nTable 2: The table vividly displays the GLUE-X metrics garnered by diverse methods across 15\nunique OOD datasets. \u2018AVG\u2019 denotes the average results across these 15 OOD datasets.\nModel\nSST-2\nMNLI\nQNLI\nRTE\nMRPC\nQQP\nSTS-B\nCoLA\nAvg\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nOOD\nHuman Performance\n97.69\n91.80\n92.33\n91.12\n83.50\n79.13\n92.62\n66.47\n86.83\nELECTRA-large\n94.84\n87.30\n82.66\n78.45\n63.60\n78.08\n80.74\n40.29\n79.86\nChatGPT\n94.83\n41.54\n81.82\n68.56\n60.23\n43.23\n72.61\n39.05\n66.67\nChatGPT (+16-shot)\n94.72\n64.24\n74.14\n68.34\n60.91\n74.24\n64.60\n47.15\n72.28\nChatGPT (+BM25)\n94.84\n64.19\n74.00\n60.31\n64.29\n68.35\n65.22\n42.50\n71.69\nSuperContext (w/o confidence)\n94.84\n77.21\n82.66\n78.45\n63.60\n78.08\n80.74\n40.29\n78.43\nSuperContext (+interpreter)\n94.84\n80.73\n83.81\n78.60\n64.26\n77.80\n76.15\n39.47\n78.77\nSuperContext (zero-shot)\n95.19\n87.24\n82.91\n78.71\n63.87\n78.65\n78.75\n41.47\n80.05\nELECTRA-large\n95.42\n87.29\n82.69\n78.84\n37.59\n77.18\n80.74\n45.73\n76.84\nLlama2-chat\n90.56\n34.30\n66.85\n60.77\n36.20\n51.57\n37.12\n6.94\n55.92\nLlama2-chat (+16-shot)\n94.72\n48.20\n67.70\n61.62\n35.72\n59.15\n18.01\n11.52\n58.54\nLlama2-chat (+BM25)\n92.87\n48.14\n68.48\n59.40\n37.08\n58.24\n39.19\n10.57\n59.69\nSuperContext (zero-shot)\n94.95\n85.45\n81.60\n78.39\n36.70\n61.79\n45.67\n40.84\n73.89\nSuperContext (w/o confidence)\n94.29\n76.68\n82.66\n78.46\n43.41\n78.17\n80.74\n40.26\n75.68\nSuperContext (16-shot)\n95.45\n87.14\n82.17\n79.07\n54.63\n77.18\n80.74\n45.47\n79.08\nand ensure that in-context samples are extracted from different domains of test sets. In total, Super-\nContext contains 43,728 instances on NLU for ChatGPT and 37,438 instances for Llama2-7B-chat.\nBaselines. For NLU, we consider two in-context learning methods as baselines for ChatGPT (Ope-\nnAI, 2023a) and Llama2-7B-chat (Touvron et al., 2023), namely 16-shot ICL and BM25. The\n16-shot ICL indicates the method that randomly extracts few-shot examples from the in-domain\ndataset as the demonstration prompt. While \u201c+BM25\u201d represents the dynamic in-context examples\nselection method using BM25 to select the top 16 examples that are similar to the test case. We\nalso present the ablation that leverages SuperContext with the optional interpretation prompt, shown\nas \u201c+interpretor\u201d. The variants of the backbone model are kept the same between ChatGPT and\nLlama2, namely \u201c+BM25\u201d and \u201c+16-shot\u201d. Due to the relatively low instruction following ability\nof Llama2-7B-chat, the \u201c+interpretor\u201d is not explored in experiments of Llama2-7B-chat. Due to\nthe difference in the instruction-following ability between the ChatGPT and Llama2-7B-chat, we in-\nsert the 16-shot in-context examples appended with the prediction and confidence of SLMs, namely\nSuperContext (16-shot). Human performance is extracted from GLUE (Wang et al., 2019).\nEvaluations. Different from NLU, the question-answering task is evaluated by the hold-out test.\nThe in-context examples are extracted from the training set and LLMs are evaluated on the validation\nset. We establish the baseline by using \u201ccluster+filter\u201d method. In particular, we adopt MiniLM\n(Wang et al., 2020) to encode the training examples and build a union-find set. Then, we use the\ncluster and filter pipeline to retrieve the most relevant examples with the test sample as in-context\ndemonstrations for ChatGPT. For Llama2-7B-chat, we adopt two fine-tuned methods as baselines\nusing multi-turn and single-turn tuning on 1.2 epochs, respectively. Notably, the total length of the\nprompt is controlled under 4,096, limited by Llama2.\n3.2\nNLU RESULTS\nOverall Performance. The comprehensive results of natural language understanding tasks under the\nOOD evaluation are meticulously outlined in Table 2. Generally, SuperContext emerges as a domi-\nnant force, showcasing an elevated average result compared to both SLM (80.05% vs. 79.86%) and\nLLM (80.05% vs. 66.67%), underscoring the preeminent performance of SuperContext. Our exper-\nimental venture utilizing ELECTRA-large (334M Para.) to bolster Llama2-7B-chat\u2019s performance\nnot only transcends ChatGPT (16-shot) (79.08% vs. 72.28%) but also parallels the SuperContext\nbased on ChatGPT (79.08% vs. 80.05%), indicating its substantial capacity to markedly diminish in-\nference costs. It is noteworthy that the data size used for ChatGPT and Llama2-7B-chat is different,\nleading to different results of SLMs (ELECTRA-large).\nWith the help of 16-shot in-context learning, the performance of ChatGPT can be improved from\n66.67% to 72.28%, but still much lower than SuperContext (80.05% vs. 72.28%). The comparison\n5\nUnder review as a conference paper at ICLR 2024\nTable 3: Results of ChatGPT and Llama2-7B-chat, and their variants on SQuAD 2.0. EM indicates\nthe exact match and valid EM only accounts for the exact match of valid JSON. ACC No indicates\nthe accuracy for no-answer questions and ACC accounts for the accuracy of has-answer questions.\nModel\nValid JSON\nEM\nValid EM\nACC. No.\nACC. Has.\nSuperContext (zero-shot)\n85.18\n57.68\n57.81\n54.65\n60.71\nChatGPT (cluster+filter)\n94.47\n49.31\n48.81\n24.22\n74.48\nChatGPT (16-shot)\n99.49\n44.69\n44.52\n13.22\n76.25\nChatGPT\n96.97\n55.82\n54.76\n32.35\n79.35\nSuperContext (16-shot)\n41.73\n47.91\n43.27\n63.65\n32.12\nFine-tuned multi-turn\n96.40\n25.70\n26.66\n10.47\n40.16\nFine-tuned single-turn\n97.17\n47.22\n48.60\n39.44\n55.02\nLlama2-7B-chat (16-shot)\n28.50\n37.56\n5.32\n58.99\n6.08\nLlama2-7B-chat\n40.09\n46.48\n40.13\n3.72\n31.87\nbetween the in-context learning paradigm and our method proves that our method can outperform\n16-shot in-context learning with a much shorter input sequence length (\u223c30 times).\nWe also present the results of SuperContext with the prompt of the interpreter, which requires LLM\nto recall influential in-context examples and output rationales when making the predictions, indi-\ncating as SuperContext (+interpreter). To better understand the benefits of including the model\nconfidence in the prompt, we present the results of SuperContext (w/o confidence). By comparing\nthe results of SuperContext w/ and w/o confidence, we observe that including model confidence can\nbring significant improvements in the average performance for both ChatGPT and Llama2. Mean-\nwhile, we find that for QNLI and QQP, Llama2 without the SLM\u2019s confidence achieves the best\nperformance among several methods. Our results also indicate that the interpreter can not bring sig-\nnificant benefits when compared to SuperContext in most of the tasks, except a slight improvement\ncan be achieved on QNLI. It can be because the explain-then-predict prompt (Wang et al., 2022)\nmay not be suitable for incorporating with SuperContext, leading to information overload.\nLlama2-7B-chat. In addition to ChatGPT, we offer the comparison between SuperContext and\nseveral baselines based on the open-source model. Experimental results show that SuperContext\nwith 16-shot in-context examples achieves the best results on seven of eight tasks included in GLUE-\nX compared to Llama2-7B-chat under the same setting without the help of the small model (79.08%\nvs. 58.54%). It is interesting to see that it outperforms ELECTRA-Large in terms of the average\nperformance (79.08 vs. 76.84). Such a huge performance increase indicates that SuperContext\nimproves the NLU capability of both Llama2-7B-chat and ELECTRA-large simply and effectively.\nIn addition, we find that using BM-25 to retrieve the most relevant 16-shot examples of the test case\nis useful for improving the in-context learning performance (59.69% vs. 58.54%).\nTask-level Analysis. On the task level, we observe that both ChatGPT and Llama2 show a relatively\nlower accuracy than the expectation on multiple tasks, including OOD evaluation on MNLI, MRPC,\nand QQP. For example, the original ChatGPT and Llama2-7B-chat can only achieve 41.54% and\n34.30% on MNLI, respectively. With the help of SuperContext, MNLI-OOD results can be improved\nto 87.24% and 87.14% on ChatGPT and Llama2-chat, respectively. For STS-B which is a textual\nsimilarity task, we find that the original Llama2-chat model performs poorly with or without in-\ncontext learning and the zero-shot performance of Llama-2-chat is significantly lower than ChatGPT\n(37.12% vs. 72.61%). Notably, although the zero-shot performance of SuperContext based on\nLlama2-7B-chat is lower than ChatGPT using the same setting on all tasks, SuperContext based on\n16-shot Llama2-7B-chat can even beat SuperContext based on zero-shot ChatGPT in multiple OOD\ntasks, including SST-2, RTE, STS-B, and CoLA, representing the efficacy of our method not only\nfor proprietary LLMs but also for relatively small-scale models, Llama2-7B-chat.\n3.3\nQA RESULTS\nThe fact-conflicting of LLMs is considered a core issue in LLMs because it is challenging for users\nto be aware of and may pose misinformation dissemination. We evaluate LLMs\u2019 ability towards\nminimizing the hallucination on the QA task based on SQuAD 2.0 (Rajpurkar et al., 2018), which\nis a suitable testbed since it can be addressed using both discriminative and generative manners.\n6\nUnder review as a conference paper at ICLR 2024\nResults of ChatGPT. The results are presented in Table 3. We find that although the original Chat-\nGPT can achieve the highest accuracy for deterministic questions (79.35%), the exact match (EM)\nand accuracy for open questions can be significantly improved by SuperContext. In particular, the\naccuracy for open questions can be improved from 32.35% (ChatGPT) to 54.65% (SuperContext),\nindicating the huge benefits. Besides, we find that even with the careful design of in-context learning\nprompts and filter methods, SuperContext still outperforms two in-context learning variants in terms\nof all metrics, indicating that pure in-context learning without fine-tuning LLMs brings no benefit\nto the QA task. Furthermore, SuperContext even outperforms the fine-tuned method in a multi-turn\nsetting on all metrics. We believe that such a huge performance benefit (54.65% vs. 13.22%) com-\npared to the traditional in-context method proves that results achieved by discriminative models are\neffective enough to reduce the hallucination in the QA task.\nResults of Llama2-7B-chat. We observe that the fine-tuned methods can significantly improve the\nrate of valid JSON. In particular, the fine-tuned single-turn method improves the valid JSON of\nthe original Llama2-chat from 40.09% to 97.17% and achieves the best performance for valid EM\n(48.6%) and accuracy for has-answer questions (55.02%). Despite fine-tuned methods outperform-\ning the original Llama2-chat and the in-context learning version, SuperContext achieves the best\nperformance in terms of the EM and accuracy for no-answer questions. We observe that the original\nLlama2-7B-chat model struggled with format adherence and hallucinations, especially in answering\nno-answer questions. This is reflected in the notably low score of 3.72. In other words, it can-\nnot output \u201cI don\u2019t know\u201d when the question is unanswerable. However, when applying in-context\nlearning with a mix of no-answer and has-answer instances, we noticed an improvement in handling\nno-answer questions, though this came at the cost of reduced accuracy in has-answer questions.\n4\nANALYSIS AND DISCUSSION\n4.1\nREVERSED PREDICTIONS\nTable 4: Statistics of reversed predictions. \u201c%Re-\nversed\u201d denotes the percentage of LLMs\u2019 predic-\ntions that differ from the predictions of SLMs.\n\u201cReversed Acc.\u201d is short for the possibility of the\nreversed predictions that from incorrect to correct.\nMethod\n%Reversed\nReversed Acc.\nSuperContext (ChatGPT)\n3.02%\n57.88%\nSuperContext (Llama2-7B-chat)\n0.50%\n52.13%\nAs displayed in Table 4, we study the differ-\nence between the final prediction of LLMs and\nthe prediction of SLMs. The detailed task-level\nperformance is shown in the Appendix. The re-\nsults demonstrate that predictions of 3.02% in-\nstances have been overridden during the infer-\nence face of ChatGPT by using SuperContext.\n57.88% of them have been corrected, indicating\nthat the reference generated by SLMs brings\npositive benefits for improving the NLU capability of LLMs. SuperContext on Llama2-7B-chat\nexhibits a relatively lower possibility of reversing the prediction of SLMs (0.5%), yet also inspires\nLLMs to correct SLMs\u2019 predictions in a more accurate direction than the random guess (52.13%).\n4.2\nINTERPRETATION ANALYSIS\nIn addition to the prediction results, we are also interested in understanding the reason behind the re-\nsult that SuperContext significantly outperforms the traditional in-context learning method. We aim\nto answer this question from two aspects, how LLMs recall already learned concepts and rationale\nfrom pre-training (Han et al., 2023; Gu et al., 2023) and why it fails in the OOD setting.\nLearning from In-context Demonstrations. We explore how language models use long contexts.\nFigure 3 shows the influence of demonstrations during the inference stage, where the y-axis indicates\nhow many times ChatGPT and InstructGPT take the ith in-context example as the emphasized one\ntowards the prediction. The x-axis is sorted by the order of occurrence of in-context examples over\n8 natural language understanding tasks. As shown in the figure, both ChatGPT and InstructGPT\nshow a significant occurrence times difference among in-context examples. In particular, ChatGPT\nwith 16-shot examples shows a trend of decreasing attention with the order of appearance. For\nexample, the second in-context example has been paid attention to over 35,000 times while the last\nexample only receives around 5,000 times attention. In terms of InstructGPT, we observe distinctive\nU-shaped occurrence times, which can be visualized in Figure 3(b). We find that the model tends to\npay attention to the beginning or the end of the input context (in-context examples), and the attention\n7\nUnder review as a conference paper at ICLR 2024\n1 2 3 4 5 6 7 8 9 10111213141516\nInteger\n0\n10000\n20000\n30000\nCount\nCount of each example idx from 1 to 16\n(a) Interpretation results of ChatGPT.\n1 2 3 4 5 6 7 8 9 10 11 1213141516\nInteger\n0\n1000\n2000\n3000\n4000\nCount\nCount of each example idx from 1 to 16\n(b) Interpretation results of InstructGPT.\nFigure 3: Counting the times of 16-shot in-context examples that have been considered as the influ-\nential examples over 8 NLU tasks, sorting by order of occurrence.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConfidence\n0\n5000\n10000\n15000\n20000\n25000\n30000\nQuantity\n50\n60\n70\n80\nPerformance\nQuantity\nPerformance\n(a) The calibration laws of ChatGPT.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConfidence\n0\n5000\n10000\n15000\n20000\n25000\nQuantity\n50\n55\n60\n65\n70\n75\nPerformance\nQuantity\nPerformance\n(b) The calibration laws of Llama2-7B-chat.\nFigure 4: The correlation between the SLM confidence and LLM performance evaluated on the\nGLUE-X benchmark. The dark green line represents the normalized performance of LLMs using\nSuperContext corresponding with the right y-axis while the light green bar indicates the volume of\ninstances with the specific confidence interval corresponding with the left y-axis.\nsignificantly degrades in the middle of long contexts. This observation is consistent with the findings\nof Liu et al. (2023) on the use of long contexts when performing downstream tasks, which suggests\nthat model performance significantly degrades when models must access relevant information in the\nmiddle of long contexts and provide a new perspective for future long-context models.\nOutput Rationale. We also collect the sentence-level rationale generated by LLMs when making\npredictions, and count for the work frequency for each task of GLUE-X based on ChatGPT, aim-\ning to provide the internal causes of OOD generalizability. According to the human evaluation \u2013\nfinished by 3 people with a CS Ph.D. degree from Europe and the U.S. \u2013 of the top 20 influential\nwords/phrases in terms of word frequency, we find that SuperContext achieves the highest average\nrationale overlap between humans and models after removing the stop word by the NLTK package,\n0.575, while the 16-shot ChatGPT only achieves 0.45 overlap score. The IAA between the annota-\ntors is 0.92, indicating a high confidence level. However, the rationale is generated by LLMs and\nthus may contain hallucinations, which should be treated with caution and just for reference.\n4.3\nTHE EFFECT OF SLM CONFIDENCE\nSince we rely on the complementarity between SLMs and PLMs, SLMs must convey its certainly in\ntask knowledge and uncertainly in domain knowledge to PLMs. The confidence score in the design\nserves a crucial role in such communication channels. We show the correlation between the confi-\ndence of SLMs and the prediction performance of LLMs. As shown in Figure 4, both ChatGPT and\nLlama2-7B-chat demonstrate a positive correlation between SLMs\u2019 confidence and LLM\u2019 perfor-\nmance, representing a high consistency between those models. The x-axis represents the confidence\ninterval covering from 0.4-1.0, for example, 0.5 indicates the instances with the prediction confi-\ndence between 0.4-0.5. It is noteworthy that the confidence is computed by the zero-shot test based\non SLMs trained on unseen domains, which indicates that high confidence requires a decent gener-\nalization ability of small models. We speculate that SuperContext shows superior performance than\n8\nUnder review as a conference paper at ICLR 2024\nboth SLMs and LLMs since it leverages the benefits of high consistency in discriminative models\nand the complementarity property of recent generative models. Besides, such a positive calibration\nlaw underscores the importance of involving both prediction and confidence in the prompt design\nof SuperContext. The data statistic of data quantity shows that most instances included in GLUE-X\nreceive the highest confidence interval from 0.9 to 1.0, and this part of the data can be predicted\nwith significantly higher accuracy than others. By comparing the experimental results of GPT-3.5\nand Llama2-7B-chat, we find that there is no significant performance gap between these models for\nthose instances with the confidence of SLMs less than 0.6. When the confidence is more than 0.6,\nthe average performance of GPT-3.5 is substantially better than Llama2-7B-chat. It can be because\nLLMs with larger parameters are more sensitive to numbers (confidence score) and possibly more\nfaithful to the SLMs\u2019 reference.\n5\nRELATED WORK\nIn-context Learning. Scaling up pre-trained language models stimulates the in-context learning\nability is first introduced by GPT-3 (Brown et al., 2020), introducing the potential to accurately\ncomprehend instructions and complete complex tasks with no supervision (Chowdhery et al., 2022;\nOpenAI, 2023b; Sun et al., 2023a). As evidenced by previous work (Shwartz et al., 2020; Nye\net al., 2021; Perez et al., 2021), the ICL performance can be significantly enhanced by incorporating\nauxiliary knowledge or reasoning instructions in a prompt, such as Chain-of-Thought (COT) (Wei\net al., 2022) and Tree-of-Thoughts (TOT) (Yao et al., 2023). However, such a multi-step reasoning\nprocess could be tedious and expensive to use (assuming we perform ICL for GPT-4), whereas our\nmethod is cost-efficient since the supervised knowledge occupies only a short length in the prompt.\nThere is a line of work for improving the in-context learning performance by either constructing\ndemonstrations (Arora et al., 2022; Si et al., 2022; Lyu et al., 2022; Gu et al., 2023; Ye et al., 2023;\nDhuliawala et al., 2023) or framing an exploration of example selection methods (Wu et al., 2023;\nWang et al., 2023b; Agrawal et al., 2022; Sun et al., 2023a) and even order (Lu et al., 2021; Zhao\net al., 2021; Liu et al., 2021; 2023). The contrastive decoding method (Li et al., 2023b) considers\nthe assistance smaller language model but requires external computation. Differently, SuperContext\ndemonstrates its superior performance on OOD test data in a cost-effective manner.\nOur work is also connected with work focusing on understanding and explaining in-context learning\nfrom different perspectives, including the implicit Bayesian Inference (Xie et al., 2021), pre-training\ndata (Han et al., 2023; Pan et al., 2023), and information compression (Wang et al., 2023a; Wu et al.,\n2023). Different ways of understanding ICL in realistic NLP tasks have been proposed before (Min\net al., 2022; Dong et al., 2022; Wang et al., 2023b), the interpretation part in SuperContext aims to\nanswer how LLMs recall in-context examples and output rationale.\nKnowledge in Context. Using external knowledge as auxiliary information to assist LLMs in pro-\nviding truthful and timely responses represents an emerging solution (Mialon et al., 2023) in recent.\nTraditional retrieve-based methods (Rubin et al., 2021; Ni et al., 2021; King & Flanigan, 2023) re-\nquire a knowledge retriever as the prior step for guiding the generation of responses. Besides, the\nexternal knowledge source could extend beyond local documents to encompass the entire Internet\n(Ni et al., 2021; Gao et al., 2023). In addition, LLMs can leverage special plug-ins to improve\ntheir capabilities, such as Toolformer (Schick et al., 2023) and LangChain (Chase, 2022) for calling\nexternal APIs, and HuggingGPT (Shen et al., 2023) to complete the planning task.\nPrevious work either relies on web information and search engines for gaining external knowledge\n(Yu et al., 2023) or accomplishes planning tasks outside the NLP scope. (Xu et al., 2023a) evaluates\nthe efficacy of language model plug-ins under an in-domain setting and lacks an interpretation part\nto explain the reasons. SuperContext shares a conceptual similarity with HuggingGPT in leveraging\nLanguage Model architectures. However, the key distinction lies in our approach\u2019s application and\nanalysis under out-of-distribution (OOD) conditions, a less explored area in the existing literature.\n6\nCONCLUSION AND FUTURE WORK\nWe constructed SuperContext, an SLM-LLM interaction framework using supervised knowledge for\nmaking LLMs better in-context learners in the OOD natural language understanding benchmark and\n9\nUnder review as a conference paper at ICLR 2024\ntext generation settings. Our goal is to improve the generalizability and factuality of LLMs using\ncost-efficient, task-specific, and generalizable SLMs. Results on 8 NLU tasks and 1 generation task\nshow that (1) current in-context learning methods still lag much behind humans towards the OOD\nevaluation of NLU and hold-out test of QA; (2) the traditional in-context learning paradigm faces the\nforgetting problem and is limited by the input sequence length; (3) SuperContext can bring decent\nperformance benefit compared to few-shot in-context learning and outperform original SLMs and\nLLMs with both zero-shot and few-shot settings. In the future, we anticipate expanding the scope\nof SuperContext to cover additional text generation tasks and exploring its effectiveness in various\nreal-world applications.\n10\nUnder review as a conference paper at ICLR 2024\nETHICAL STATEMEMT\nEthical Use of ChatGPT and InstructGPT. In adherence to the official guidelines provided by\nOpenAI, we utilized ChatGPT (gpt-3.5-turbo) and InstructGPT (text-davinci-003), setting the tem-\nperature of all tasks to zero to ensure reproducibility. For experiments conducted on the SQuAD 2.0\ndataset, we employed gpt-3.5-turbo-16k to ensure the prompt length remained within the model\u2019s\nwindow length.\nSocial Impact. The primary objective of this study is to repurpose the extensively labeled data in\nspecific domains, which required substantial human and material resources to generate. We aim to\nuse these data to train a task-specific model to assist LLMs in mitigating hallucinations produced\nduring Natural Language Understanding (NLU) and Question Answering (QA) tasks, thereby en-\nhancing the safety of the LLMs. Notably, all of datasets involved in this work belong to the publicly\navailable detasets, and thus do not contain any personal privacy data.\nPotential Concerns. We acknowledge several limitations of this study and propose a series of open\nquestions for subsequent research. We discuss the potential concerns and limitations of this work.\n1. Exploration of Other Large-Scale Language Models: In this study, we delve into the examination\nof ChatGPT and Llama2. Nevertheless, a plethora of recently proposed models, such as GPT-4,\nPaLM, Falcon, and Claude, beckons for comprehensive analysis. This work does not involve any\ncommercial competition and belongs to non-profit research.\n2. Unveiling More Properties of LLMs: This work investigates the generalizability and factuality of\nLLMs, yet uncharted territories remain. The exploration of social bias and the reasoning capacity\nof LLMs promises to be an interesting avenue for further research. We respect the human rights\nof all people and ensure that crowdsourcing workers are adequately paid for this work.\n3. In-Depth Analysis for In-Context Learning Understanding: SuperContext relies on the comple-\nmentarity between SLMs and PLMs, where SLMs must convey its certainly in task knowledge\nand uncertainly in domain knowledge to PLMs. A pivotal question persists: can this comple-\nmentary behavior be attributed to the pre-training data or a handful of few-shot demonstra-\ntions? We plan to refine the interaction mechanism between SLM and LLM to further understand\nin-context learning. Our current analysis does not involve any personal privacy.\n11\nUnder review as a conference paper at ICLR 2024\nREFERENCES\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-\ncontext examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022.\nSimran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami,\nand Christopher Re. Ask me anything: A simple strategy for prompting language models. In The\nEleventh International Conference on Learning Representations, 2022.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on\nmachine learning, pp. 2206\u20132240. PMLR, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHarrison Chase.\nLangChain, October 2022.\nURL https://github.com/hwchase17/\nlangchain.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz,\nand Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv\npreprint arXiv:2309.11495, 2023.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\nand Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,\nVincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what\nlanguage models say, using language models. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 16477\u201316508, 2023.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In Proceed-\nings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 4849\u20134870, Toronto, Canada, July 2023. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2023.acl-long.267. URL https://aclanthology.org/2023.\nacl-long.267.\nXiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu\nWang.\nUnderstanding in-context learning via supportive pretraining data.\nIn Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers), pp. 12660\u201312673, Toronto, Canada, July 2023. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/2023.acl-long.708.\nURL https://aclanthology.org/2023.\nacl-long.708.\nBrendan King and Jeffrey Flanigan. Diverse retrieval-augmented in-context learning for dialogue\nstate tracking. arXiv preprint arXiv:2307.01453, 2023.\nStefan Larson, Gordon Lim, Yutong Ai, David Kuang, and Kevin Leach.\nEvaluating out-of-\ndistribution performance on document image classifiers.\nIn Thirty-sixth Conference on Neu-\nral Information Processing Systems Datasets and Benchmarks Track, 2022.\nURL https:\n//openreview.net/forum?id=uDlkiCI5N7Y.\n12\nUnder review as a conference paper at ICLR 2024\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. arXiv e-prints, pp. arXiv\u20132305,\n2023a.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\nIn ACL, 2023b.\nXingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya\nPoria. Chain of knowledge: A framework for grounding large language models with structured\nknowledge bases. arXiv preprint arXiv:2305.13269, 2023c.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What\nmakes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang.\nLost in the middle: How language models use long contexts.\narXiv preprint\narXiv:2307.03172, 2023.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786, 2021.\nXinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. Z-icl: Zero-shot\nin-context learning with pseudo-demonstrations. arXiv preprint arXiv:2212.09865, 2022.\nGr\u00b4egoire Mialon, Roberto Dess`\u0131, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta\nRaileanu, Baptiste Rozi`ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented\nlanguage models: a survey. arXiv preprint arXiv:2302.07842, 2023.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837, 2022.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00b4andez \u00b4Abrego, Ji Ma, Vincent Y Zhao,\nYi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers.\narXiv preprint arXiv:2112.07899, 2021.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al.\nShow\nyour work: Scratchpads for intermediate computation with language models.\narXiv preprint\narXiv:2112.00114, 2021.\nSean O\u2019Brien and Mike Lewis. Contrastive decoding improves reasoning in large language models.\narXiv preprint arXiv:2309.09117, 2023.\nOpenAI. https://chat.openai.com.chat, 2023a.\nOpenAI. Gpt-4 technical report, 2023b.\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.\nWhat in-context learning \u201clearns\u201d in-\ncontext: Disentangling task recognition and task learning. In Findings of the Association for\nComputational Linguistics: ACL 2023, pp. 8298\u20138319, Toronto, Canada, July 2023. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.527. URL https:\n//aclanthology.org/2023.findings-acl.527.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models.\nAdvances in neural information processing systems, 34:11054\u201311070, 2021.\n13\nUnder review as a conference paper at ICLR 2024\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions\nfor squad. arXiv preprint arXiv:1806.03822, 2018.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633, 2021.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\narXiv preprint\narXiv:2303.17580, 2023.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\ncommonsense question answering with self-talk. arXiv preprint arXiv:2004.05483, 2020.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber,\nand Lijuan Wang. Prompting gpt-3 to be reliable. In The Eleventh International Conference on\nLearning Representations, 2022.\nJiuding Sun, Chantal Shaib, and Byron C Wallace.\nEvaluating the zero-shot robustness of\ninstruction-tuned language models. arXiv preprint arXiv:2306.11270, 2023a.\nXiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. Text\nclassification via large language models. arXiv preprint arXiv:2305.08377, 2023b.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=rJ4km2R5t7.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label\nwords are anchors: An information flow perspective for understanding in-context learning. arXiv\npreprint arXiv:2305.14160, 2023a.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-\nattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neu-\nral Information Processing Systems, 33:5776\u20135788, 2020.\nXinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic\nmodels: Explaining and finding good demonstrations for in-context learning.\narXiv preprint\narXiv:2301.11916, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning:\nAn information compression perspective for in-context example selection and ordering. In Pro-\nceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 1423\u20131436, Toronto, Canada, July 2023. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2023.acl-long.79. URL https://aclanthology.org/2023.\nacl-long.79.\n14\nUnder review as a conference paper at ICLR 2024\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context\nlearning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\nCanwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. Small\nmodels are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848, 2023a.\nWeijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought\nprompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993, 2023b.\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing\nXie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-\ndistribution generalization perspective. arXiv preprint arXiv:2211.08073, 2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\nQinyuan Ye, Iz Beltagy, Matthew E Peters, Xiang Ren, and Hannaneh Hajishirzi. Fid-icl: A fusion-\nin-decoder approach for efficient in-context learning. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8158\u20138185, 2023.\nJifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun\nYao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of\nlarge language models. arXiv preprint arXiv:2306.09296, 2023.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\npreprint arXiv:2303.18223, 2023.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\nfew-shot performance of language models. In International Conference on Machine Learning, pp.\n12697\u201312706. PMLR, 2021.\n15\nUnder review as a conference paper at ICLR 2024\nA\nADDITIONAL RESULTS: GENREALIZABILITY\nWe provide full experimental results in Table 5. Different from Table 2, which demonstrates the\naverage result of each task included in GLUE-X, we present a fine-grained analysis to show the\nefficacy of our method and differences among tasks. In particular, it is interesting to see that with\nthe help of SuperContext, both ChatGPT and Llama2-7B-chat surpass the supervised task-specific\nmodel, ELECTRA, in terms of higher average performance, indicating that SuperContext introduces\nthe benefits of complementarity to enhance the generalizability of LLMs\u2019 towards the NLU tasks.\nSuch a task-level analysis also sheds light on future work to design task-specific methods.\nTable 5: Performance evaluation of SuperContext and several baselines based on GLUE-X dataset.\nThe table showcases the detailed evaluation results of SLMs, LLMs, and SuperContext. C. is short\nfor ChatGPT and L. represents Llama2-7B-chat.\nID\nOOD\nOurs (C.)\nELECTRA (C.)\nChatGPT (16)\nChatGPT\nELECTRA (L.)\nLlama2\nOurs (L.)\nSST2\nIMDB\n93.97\n94.87\n94.03\n93.63\n94.97\n91.48\n94.97\nSST2\nYELP\n97.30\n96.63\n97.00\n96.87\n97.39\n95.63\n97.53\nSST2\nAmazon\n95.87\n95.37\n94.99\n94.33\n95.84\n94.76\n95.84\nSST2\nFlipkart\n93.60\n93.50\n92.84\n94.50\n93.49\n89.59\n93.45\nCoLA\nGrammar\n41.47\n40.29\n47.15\n39.05\n45.73\n11.52\n45.47\nMRPC\nQQP\n55.06\n54.36\n66.77\n69.94\n27.66\n32.97\n57.22\nMRPC\nTwitter\n72.68\n72.83\n55.05\n50.52\n47.52\n38.46\n52.04\nQQP\nMRPC\n80.17\n79.56\n79.49\n42.04\n80.74\n17.99\n80.74\nQQP\nTwitter\n77.13\n76.59\n68.99\n44.41\n78.94\n70.35\n78.94\nMNLI\nMNLI mis\n88.67\n89.13\n64.77\n43.85\n75.42\n47.94\n75.42\nMNLI\nSNLI\n85.80\n85.47\n63.70\n39.23\n89.13\n56.30\n89.10\nRTE\nHANS\n72.87\n72.87\n58.20\n56.73\n85.45\n40.10\n85.18\nRTE\nSCITAIL\n84.55\n84.02\n62.54\n80.38\n73.68\n60.07\n74.34\nQNLI\nNewsQA\n82.91\n82.66\n74.14\n81.82\n84.00\n63.17\n83.79\nSTS-B\nSICK\n78.75\n80.74\n64.60\n72.61\n82.69\n67.69\n82.17\nGLUE-X\nAVG.\n80.05\n79.86\n72.28\n66.67\n76.84\n58.53\n79.08\nB\nTASK-LEVEL STATISTICS OF REVERSED PREDICTIONS\nWe present the detailed analysis of predictions of ELECTRA-large reversed by ChatGPT and\nLlama2-7B-Chat, along with the number of test instances for each task in Table 6. In general, we\nobserve that ChatGPT demonstrates a superior ability to reverse predictions of ELECTRA-Large\ncompared to Llama2-7B-chat, aiming to correct errors when making OOD predictions on NLU\ntasks. On the other hand, ChatGPT exhibits higher accuracy in making modifications that override\nclassification results compared to Llama2-7B-chat.\nThe OOD testing on QNLI is perceived by both models to contain the highest proportion of data\nthat should have the final decision overridden. Specifically, 15.11% of the test data is amended by\nChatGPT, while 2.49% of the data is reversed by Llama2-7B during the inference stage. Naturally,\nfor tasks with relatively lower error rates, such as SST-2 and MNLI, the probability of the models\nmaking modifications is also low. This underscores the significant ability of larger models to evaluate\nthe predictions and confidence levels of task-specific fine-tuned models.\nIn terms of the Reversed Accuracy (where the probability of random guess is 50%) as shown in\nTable 6, we find that ChatGPT exhibits a higher correction accuracy than random guessing on seven\nout of eight tasks. In contrast, Llama2-7B-chat is capable of reversing predictions in only six out of\nthe eight tasks and surpasses random guesses in only half of the tasks.\nC\nADDITIONAL RESULTS OF CALIBRATION LAWS\nWe supplement the results of the calibration laws with two additional model groups, namely\nELECTRA-large and InstrutGPT, in Figure 5. Consistent with ChatGPT and Llama2-7B-chat, both\nELECTRA-large and InstruGPT exhibit a positive correlation between confidence and performance.\n16\nUnder review as a conference paper at ICLR 2024\nTable 6: The detailed statistics of reversed predictions on each task of GLUE-X. \u201c%Reversed\u201d\ndenotes the percentage of predictions of LLMs that differ from the predictions of SLMs. \u201cReversed\nAcc.\u201d is short for the possibility of the reversed predictions that from incorrect to correct. \u201c%Error\u201d\nis the error rate of the ELECTRA-large baseline. \u201c#Instances\u201d is the total number of test samples.\nModel\nMetric\nSST2\nMNLI\nQNLI\nRTE\nMRPC\nQQP\nSTS-B\nCoLA\nChatGPT\n%Error\n5.16\n12.70\n17.34\n21.55\n36.40\n21.92\n19.26\n59.71\n%Reversed\n0.81\n1.10\n15.11\n3.92\n1.40\n1.92\n8.27\n2.87\nReversed Acc.\n71.13\n42.42\n50.81\n53.93\n82.14\n68.70\n53.23\n74.42\n#Instances\n12,000\n6,000\n2,866\n4,862\n6,000\n6,000\n3,000\n3,000\nLlama2-7B-Chat\n%Error\n4.58\n12.71\n17.31\n21.16\n62.41\n22.82\n19.26\n54.27\n%Reversed\n0.04\n0.28\n2.49\n0.87\n2.05\n0\n0\n0.25\nReversed Acc.\n80.00\n23.53\n39.44\n65.85\n88.98\n0\n0\n16.77\n#Instances\n10,549\n5,996\n2,849\n4,738\n2,604\n5,326\n3,000\n2,376\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConfidence\n0\n5000\n10000\n15000\n20000\n25000\nQuantity\n50\n55\n60\n65\n70\n75\nPerformance\nQuantity\nPerformance\n(a) The calibration laws of ELECTRA-\nlarge.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConfidence\n0\n5000\n10000\n15000\n20000\n25000\n30000\nQuantity\n50\n60\n70\n80\nPerformance\nQuantity\nPerformance\n(b) The calibration laws of InstructGPT.\nFigure 5: The calibration laws of ELECTRA-large and InstructGPT between the confidence and\nperformance evaluated on the GLUE-X benchmark. The dark green line represents the LLMs\u2019 per-\nformance using SuperContext corresponding with the right y-axis while the light green bar indicates\nthe volume of instances with the specific confidence interval corresponding with the left y-axis.\nDistinctively, the curve for InstrutGPT demonstrates more pronounced fluctuations, especially when\nthe confidence is relatively low.\nD\nDETAILED PROMPT DESIGNS\nWe present the detailed prompt designs for each task, using SST-2, CoLA, and SQuAD2.0 as demos.\nD.1\nNATURAL LANGUAGE UNDERSTANDING\nSST-2\nSST-2:\nYou are tasked with predicting the sentiment of a given sentence\nas either \u2019positive\u2019 or \u2019negative\u2019.\nUse the prediction from the\npre-trained model (334M Parameters) fine-tuned on a sentiment analysis\ndataset as a reference to aid your judgment.\nTest Case:\nSentence:\n\"[Input]\" Model\u2019s Prediction:\nModel\u2019s Confidence:\nPlease provide your analysis using the format below and then give your\nfinal prediction:\n1.\nInfluence Degree:\nOn a scale of 0 to 1 (in increments of 0.1), how\nmuch did the fine-tuned model\u2019s prediction influence your judgment?\n2.\nCritical Features:\nIdentify the specific word or phrase in the test case\nthat played a pivotal role in your prediction of sentiment analysis.\nAfter analyzing, please provide your final prediction.\n17\nUnder review as a conference paper at ICLR 2024\nCoLA\nCoLA: You are tasked with predicting the sentiment of a sentence\u2019s\ngrammar as either \u2018acceptable\u2019 or \u2018unacceptable\u2019.\nUse the prediction\nfrom the pre-trained model (334M Parameters) fine-tuned on a grammar test\ndataset as a reference to aid your judgment.\n+ Test case:\n\\[Test case]\" Model\u2019s Prediction:\nModel\u2019s Confidence:\nPlease provide your analysis using the format below and then give your\nfinal prediction:\n1.\nInfluence Degree:\nOn a scale of 0 to 1 (in increments of 0.1), how\nmuch did the fine-tuned model\u2019s prediction influence your judgment?\n2.\nCritical Features:\nIdentify the specific word or phrase in the test\ncase that played a pivotal role in your grammar-acceptable prediction.\nAfter analyzing, please provide your final prediction.\nD.2\nQUESTION ANSWERING\nSQuAD 2.0\nSQuAD 2.0:\n<s>[INST] <<SYS>> You are a helpful, respectful and honest\nassistant.\nAlways answer as helpfully as possible, while being safe.\nYour answers should not include any harmful, unethical, racist, sexist,\ntoxic, dangerous, or illegal content.\nPlease ensure that your responses\nare socially unbiased and positive in nature.\nIf a question does not\nmake any sense, or is not factually coherent, explain why instead of\nanswering something not correct.\nIf you don\u00b4t know the answer to a\nquestion, please don\u00b4t share false information.\n<</SYS>>\nExtract from the following context the minimal span word for word\nthat best answers the question.\nThink step by step and explain your\nreasoning.\nThen give the answer in JSON format as follows:\n\u2018\u2018\u2018json\n\"answer\":\n...\n\u2018\u2018\u2018 If the answer is not in the context, the answer\nshould be exactly a string \"?\", this is very important.\nContext:\ncontext Question:\nquestion Here\u00b4s a potential answer to the question:\n\u2018\u2018\u2018json\n\"answer\":\n[\"answer\"]\n\u2018\u2018\u2018\n18\n"
  },
  {
    "title": "Human101: Training 100+FPS Human Gaussians in 100s from 1 View",
    "link": "https://arxiv.org/pdf/2312.15258.pdf",
    "upvote": "6",
    "text": "Human101: Training 100+FPS Human Gaussians in 100s from 1 View\nMingwei Li\nJiachen Tao\nZongxin Yang\nYi Yang\u2020\nReLER, CCAI, Zhejiang University\n(d) Comparison\n(b) Initialization\nFront\nBack\nLeft\nRight\n(a) Video capture\nSingle View Frames\n(c) Training\n 30 seconds\nLPIPS* 37.4 \n100 seconds\nLPIPS* 25.6\n60 seconds\nLPIPS* 30.8\n  \n  \n  \n  \nS\nel\ne\nct\nFigure 1. One VR common use-case. (a) The user captures a short monocular video and proceeds to upload it. (b) Our model automatically\nselects four frames from the monocular video and after an initialization process, we can obtain an initial point cloud. (c) Our model can\nlearn in minutes to get a dynamic human representation. (d) Our model achieves comparable or better visual quality while rendering much\nfaster than previous works. LPIPS* = LPIPS \u00d7103. The area of each circle is proportional to the training time required, with larger areas\nrepresenting longer training durations.\nAbstract\nReconstructing the human body from single-view videos\nplays a pivotal role in the virtual reality domain. One preva-\nlent application scenario necessitates the rapid reconstruc-\ntion of high-fidelity 3D digital humans while simultaneously\nensuring real-time rendering and interaction. Existing meth-\nods often struggle to fulfill both requirements. In this paper,\nwe introduce Human101, a novel framework adept at pro-\nducing high-fidelity dynamic 3D human reconstructions from\n1-view videos by training 3D Gaussians in 100 seconds and\nrendering in 100+ FPS. Our method leverages the strengths\nof 3D Gaussian Splatting, which provides an explicit and\nefficient representation of 3D humans. Standing apart from\nprior NeRF-based pipelines, Human101 ingeniously applies\na Human-centric Forward Gaussian Animation method to\ndeform the parameters of 3D Gaussians, thereby enhancing\nrendering speed (i.e., rendering 1024-resolution images at\nan impressive 60+ FPS and rendering 512-resolution im-\nages at 100+ FPS). Experimental results indicate that our\napproach substantially eclipses current methods, clocking\nup to a 10 \u00d7 surge in frames per second and delivering com-\nparable or superior rendering quality. Code and demos will\n\u2020: the corresponding author.\nbe released at https://github.com/longxiang-ai/Human101.\n1. Introduction\nIn the realm of virtual reality, a prevalent use case involves\nrapidly crafting custom virtual avatars and facilitating in-\nteractions with them. Within this context, two significant\ntechnical challenges emerge: (1) How can we swiftly pro-\nduce a digitized virtual avatar, preferably within a user\u2019s\nacceptable waiting time (e.g., within 3 minutes), using read-\nily available equipment (e.g., a single-camera setup)? (2)\nHow can we achieve real-time rendering to cater to the inter-\nactive demands of users?\nWhile previous methods [15, 22, 43\u201345, 70, 72] have\nmade some progress, they still haven\u2019t fully met the require-\nments of the application scenario described earlier. The lim-\nitations of these methods can be summarized in two main\npoints: (1) Slow rendering speed in implicit methods.\nMethods based on implicit neural network[15, 22, 43\u201345] op-\ntimization using NeRF have slower rendering processes and\nchallenges with inverse skinning deformation, preventing\nreal-time rendering. (2) Slow convergence speed in explicit\nmethods. Approaches such as those in [70, 72], capable of\nachieving real-time rendering, necessitate extensive data for\ntraining. This requirement results in slower optimization,\nthus hindering the rapid reconstruction of dynamic humans.\n1\narXiv:2312.15258v1  [cs.CV]  23 Dec 2023\nTo address these challenges, a more practical and im-\nproved approach would fit these goals. First, to enhance\nrendering speed, we should choose a rasterization render-\ning pipeline, replacing the traditional volume rendering ap-\nproach. Second, to speed up training, we should choose a\nbetter representation method that\u2019s easier to optimize, ideally\nreducing optimization time to just a few minutes. Recently, a\nnovel method [24] has employed 3D Gaussians to explicitly\ndepict 3D scenes. With the integration of a differentiable\ntile rasterization method, it achieves superior visual quality\nand a much quicker rendering speed (over 100 FPS) com-\npared to previous works [1, 39, 52]. The emergence of this\nmethod makes realizing the described application scenario\n(i.e., achieving both fast reconstruction and real-time render-\ning) a tangible possibility.\nRecognizing the advantages of [24], we introduce a\nnovel framework for single-view human reconstruction. This\nframework not only accomplishes dynamic human recon-\nstruction in less than one minute but also ensures real-time\nrendering capabilities. Merging the fast and straightforward\nmethods of 3D GS with human body structures, we\u2019ve\ncreated a new kind of forward skinning process for ren-\ndering. Different from the usual inverse skinning used by\n[15, 22, 43, 45] this forward skinning deformation method\navoids searching for the corresponding canonical points of\nthe target pose points but directly deform the canonical\npoints into observation space. Cause [24] utilizes 3D Gaus-\nsians rather than just points, we use a Human-centric For-\nward Gaussian Animation method to deform the positions,\nrotations, and scales of Gaussians, and modify spherical co-\nefficients by rotating their directions. For faster convergence,\nwe design a Canonical Human Initialization method to\ninitialize the original Gaussians.\nTo validate the effectiveness of the proposed pipeline, we\nconduct extensive experiments on ZJU-MoCap Dataset [44]\nand the Monocap Dataset [15]. Results show that Human101\ncould not only swiftly reconstruct a dynamic human, but\nalso outperform incredible rendering speed together with\nbetter visual quality. With a single RTX 3090 GPU, our\nmethod can learn in 100 seconds to get comparable or better\nvisual quality and maintain 100+ FPS rendering speed,\nwhich makes it a tangible possibility for real-time interactive\napplications and immersive virtual reality experiences.\nOur contributions can be summarised as follows:\n\u2022 We introduce an innovative approach to dynamically rep-\nresent 3D human bodies by employing 3D Gaussian Splat-\nting [24], utilizing its efficient and explicit representation\ncapabilities for detailed and accurate human modeling. We\nhave proposed a Canonical Human Initialization method,\nwhich significantly enhances the model\u2019s convergence rate\nand improves visual detail representation.\n\u2022 We propose a deformation methodology, composed of\nHuman-centric Forward Gaussian Animation and\nSample Points + Backward Skinning\nVolume Rendering\n(a) Previous Works\n(b) Ours\nI\nn\nv\ne\nr\ns\ne\nExplicit Point Clouds\nRandom\nNeural Params\nInitialization\nDeformation\nRendering\nForward Gaussian Animation\nF\no\nr\nw\na\nr\nd\nTile Rasterization Rendering\n0.5-2 FPS\n100+ FPS\nDeform ~17m points\nTrain 5 mins - 10 hours\nv.s.\nv.s.\nv.s.\nTrain 100 seconds\nAnimate ~50 k Gaussians\nFigure 2. Method difference with previous works [15, 43] (\u00a7 3).\n(1) Initialization. Previous works rely on an implicit representation,\ninitialized with random neural parameters. In contrast, our method\nemploys explicit colored point clouds for initialization, which ac-\ncelerates convergence. (2) Deformation. Earlier approaches require\ndeforming a larger number of points from the target pose to the\ncanonical pose, consuming more computational resources. (3) Ren-\ndering. Compared to the NeRF-based ray marching technique, our\napproach utilizing rasterization rendering achieves significantly\nfaster rendering speeds.\nHuman-centric Gaussian Refinement, which is dis-\ntinct from the prevailing time-consuming inverse skinning\nframeworks, making it possible to fast reconstruct and\nrender a dynamic human in real time.\n\u2022 We achieve a \u223c 10.8 \u00d7 speed increase in rendering during\ninference (with FPS 100+ for 512 \u00d7 512 resolution im-\nages) compared to previous neural human representations,\nwhile simultaneously ensuring comparable or superior ren-\ndering quality and higher image resolution.\n2. Related Work\nHuman Reconstruction from Monocular Video. Recon-\nstructing 3D humans from monocular videos is challeng-\ning, primarily due to complex human poses and the lim-\nited information from a single camera. Significant strides\nin this area have been made by [4, 5, 21, 26, 49, 50, 53, 56\u2013\n58, 61, 64, 65, 71]. Approaches like [23, 43\u201345, 62] have\nexcelled in high-quality reconstruction with precise pose\nand deformation adjustments. Yet, their lengthy convergence\ntimes, often exceeding 10 hours, limit practical utility.\nRecent works like [15, 22, 39] have accelerated conver-\ngence to about 5 minutes, but rendering speeds remain a\nbottleneck, with [15] achieving only about 1.5 FPS on an\nRTX 3090. 3D GS [24], effective for static scenes with its\n2\nexplicit 3D Gaussian representation and fast GPU sorting, of-\nfers a potential solution with over 100 FPS rendering speeds.\nHowever, its application to dynamic human reconstruction\nis not straightforward. Our work harnesses the principles\nof 3D GS for monocular human reconstruction, targeting\nhigh rendering speeds to bridge the gap towards practical\nimplementation in real-world applications.\nHuman Deformation and Animation. The Skinned Multi-\nPerson Linear model (SMPL) [3, 35, 42] is a prevalent frame-\nwork for representing human structure, simplifying pose\nchanges with Linear Blend Skinning (LBS). Various genera-\ntive articulation methods have been explored [2, 7, 11, 19, 40,\n67], alongside backward skinning techniques [20, 36, 51, 59]\nand forward skinning methods [6\u20138, 12, 21, 31, 33, 60]. For\ndynamic human reconstruction, studies like [15, 43, 62] use\nneural networks to enhance the deformation process, apply-\ning residuals to point coordinates or blending weights. Un-\nlike these point-based approaches, our method employs 3D\nGaussians [24] for spatial representation, accounting for po-\nsition, rotation, and scale. Our human-centric forward skin-\nning deformation approach successfully animates humans\nbased on 3D Gaussians, effectively addressing challenges\nsuch as artifacts and jaggedness after deformation.\nAccelerating Neural Rendering. Since the introduction of\nNeRF by [37], numerous studies have sought to acceler-\nate neural scene rendering. Techniques utilize voxel grids\n[14, 18, 32, 39, 47, 52, 66], explicit surfaces [10, 28, 41],\nand point-based representations [27, 30, 46, 48, 68] or other\nmethods to speed up rendering process. These methods ef-\nfectively minimize the number of NeRF MLP evaluations\nrequired, thereby reducing computational costs. Focusing on\nhuman body, some approaches [34, 54, 72] utilize innovative\nprocesses like UV map prediction [9, 29, 70, 72] and voxel\ngrids [13]. However, these techniques predominantly suffer\nfrom lengthy training durations and are mostly restricted to\nstatic scenes, hindering downstream applications.\nA significant breakthrough in this area is the development\nof 3D Gaussian Splatting (3D GS) [24], utilizing anisotropic\n3D Gaussians combined with spherical harmonics [38] to\nrepresent 3D scenes. This method effectively circumvents\nthe slow ray marching operation, delivering high-fidelity and\nhigh-speed rendering. Nevertheless, its application has been\nprimarily confined to static scenes. Our work pioneers the\napplication of 3D GS to animatable human reconstruction.\nWe extend the capabilities of 3D GS beyond static multi-view\nscenarios, addressing its limitations in dynamic monocular\nhuman movement reconstruction.\n3. Method\nOverview. In this work, our primary objective is to swiftly\nreconstruct dynamic human movements from single-view\nvideos, simultaneously ensuring real-time rendering capa-\nbilities. Our approach builds upon the techniques introduced\nin [15], with the underlying assumption that the cameras are\npre-calibrated and each image is accompanied by provided\nhuman poses and foreground human masks. Fig. 3 shows\nthe main training pipeline of our model. Within Sec. 3.1, we\nexplore the foundational aspects of 3D Gaussian Splatting\n(3D GS) [24] and SMPL [35]. Sec. 3.2 delves into the pro-\ncess of canonical human initialization, which is the base of\nthe initial 3D Gaussians, speeding up our training process.\nSec. 3.3 presents our novel human-centric forward gaussian\nanimation approach. This section details how we adapt and\napply Gaussian models to accurately represent and animate\nhuman figures, focusing on achieving both high fidelity and\nefficiency in dynamic scenarios. Finally, Sec. 3.4 illustrates\nour refinements of Gaussians and spherical harmonics.\n3.1. Preliminary\n3D Gaussian Splatting. Our framework employs 3D Gaus-\nsian Splatting [24] to parameterize dynamic 3D shapes for\n2D image transformation. Differing from NeRF-based meth-\nods, we define 3D Gaussians with a full 3D covariance ma-\ntrix \u03a3 centered at \u00b5 in world space. For 2D rendering, the\nprojected covariance matrix \u03a3\u2032 is calculated as:\n\u03a3\u2032 = JV \u03a3V \u22a4J\u22a4,\n(1)\nwhere J is the Jacobian of the affine approximation of the\nprojective transformation, and V is the world-to-camera ma-\ntrix. To simplify learning, \u03a3 is decomposed into a quaternion\nr for rotation and a 3D-vector s for scaling, yielding rotation\nmatrix R and scaling matrix S. Hence, \u03a3 is expressed as:\n\u03a3 = RSS\u22a4R\u22a4.\n(2)\nAdapting to dynamic scenes. The method proposed in [24]\nexcels in static scene representation from multi-view data.\nHowever, to extend this framework to dynamic human sce-\nnarios, we refine the 3D Gaussian model within the canonical\nspace. We systematically optimize the Gaussians\u2019 defining\nattributes: position x, rotation r, scale s, and view direction\nd of the radiance field, which is represented using spherical\nharmonics (SH). These refinements are carried out through\nan additional deformation field that enables the precise cap-\nture of the nuanced movements inherent to human dynamics.\nFor clarity and conciseness, the 3D Gaussians in our frame-\nwork are denoted as G(x, r, s, d), succinctly encapsulating\nthe parameters critical to modeling dynamic human forms.\nSMPL and LBS deformation. SMPL [35] is a widely used\nhuman skinning model, composed of 6890 vertices and\n13776 triangular faces. Each of the SMPL vertices owns\na 3D position vi and a corresponding weight vector wi. The\ndeformation of the SMPL mesh M is performed using the\nLinear Blend Skinning (LBS) technique, which deforms the\nmesh based on a set of pose parameters \u03b8 and shape parame-\nters \u03b2. Specifically, Given \u03b8 and \u03b2, the LBS technique can\n3\nFrames\nPoint Cloud Parts\nInitial Point Cloud\n \n \nF\nu\ns\ne\n \n \n \nC\no\nn\nv\ne\nr\nt\nCanonical Pose\nTarget Pose\n(a) Canonical Human Initialization\n(b) Human-centric Gaussian Forward Animation\nInitial SHs\nRotated SHs\n(c) Human-centric Re\ufb01nement\n\u2206x,\u2206r,\u2206s\nMLP\nHigh-Fidelity\nResults\nCanonical\nGaussians\nRotated\nGaussians\nRe\ufb01ned\nGaussians\nSelect\nModule\nTranslated\nGaussians\nTri-Face Rot. \n L\nB\nS\n \nT\nr\na\nns\n.\nGaussian\nRe\ufb01nement\nSH\nRotation\nTri-Face Rot. \nTrans.\nModule\nLBS Trans. \nRot.\nModule\nTri-Face Rot. \nFrame Idx.\nSMPL Params.\nSH\nRe\ufb01nement\nRe\ufb01ned SHs\nRender\nFigure 3. Overview of Human101 (\u00a7 3). (a) Canonical Human Initialization (\u00a7 3.2). We use an offline model [65] to extract 4 point\ncloud parts from 4 selected frames, and fuse them into a canonical point cloud, which then is converted into canonical Gaussians. (b)\nHuman-centric Gaussian Forward Animation (\u00a7 3.3). We deform canonical 3D Gaussians into the target pose by modifying Gaussian\npositions x, rotations r, and scales s). And we rotate the spherical coefficients with triangle face rotation. (c) Human-centric Gaussian\nRefinement (\u00a7 3.4). We refine positions x, rotations r and scales s of Gaussians and refine the view direction d of spherical harmonics.\ntransform vertex vcan from canonical space to observation\nspace vob as follows:\nvob =\nN\nX\ni=1\nwiGi(\u03b8, \u03b2)vcan,\n(3)\nwhere N is the joint number, wi is the blend weight of v,\nand Gi(\u03b8, \u03b2) is the transformation matrix of joint i.\n3.2. Canonical Human Initialization\nPrevious Point Cloud Extraction. The initialization of the\ncanonical Gaussian space is critically dependent on the point\ncloud data quality, with variations in initial point clouds sub-\nstantially influencing the model\u2019s convergence speed and\nthe refinement of outcomes. While prior studies [24] have\nadeptly derived initial point clouds from multi-view data\nleveraging COLMAP techniques [55], these methods excel\npredominantly in static scenes. However, such approaches\ndo not perform well with a single viewpoint and are inca-\npable of estimating point clouds for dynamic data.\nHuman-centric Point Cloud Parts Extraction. Conse-\nquently, taking human structure into account, we consider the\nadoption of monocular reconstruction methods [64, 65, 71]\nto acquire the initial point cloud. The prior state-of-the-\nart [65] is capable of estimating a mesh from a single image\ninput and projecting the image color onto the mesh. How-\never, this approach only attaches the color of one image to\nthe predicted mesh, resulting in a mesh that is only partially\ncolored. Our goal is to select as few images as possible from\nthe input sequence to achieve the best initial results. Our\nAutomatic Selection strategy involves extracting multiple\nsets of images from the sequence, each set containing four\nimages with the human subject\u2019s angles as close to 90 de-\ngrees apart as possible. We then choose one set of images\nwhere the poses most closely resemble the \u201cA\u201d pose (shown\nin the Fig. 3 (a) ), which facilitates mesh deformation into\nthe canonical pose in the subsequent steps. These images\nare labeled according to their orientation: front F, back B,\nleft L, and right R. Using [65], we generate four meshes\nMF (VF ), MB(VB), ML(VL), MR(VR), each represent-\ning different postures and having color only on one side.\nCanonical Point Cloud Fusion. These meshes are deformed\nto a canonical pose using inverse Linear Blend Skinning [35].\nVk\ncan =\n N\nX\ni=1\nwk\ni Gk\ni\n!\u22121\nVk, for k \u2208 {F, B, L, R}\n(4)\nPcan(Vcan) = Fuse(MF\ncan, MB\ncan, ML\ncan, MR\ncan)\n(5)\nAs a result, each canonical pose point cloud part is colored on\none side only. We then fuse the four canonical point clouds\nto form an initial point cloud in canonical space, comprising\napproximately 50,000 points. Following [24], we convert the\ninitial point cloud into canonical Gaussians. Additionally,\nour model can also initialize using the bare SMPL\u2019s canoni-\ncal pose mesh (6890 vertices with white colors). While this\napproach requires a slightly longer convergence time, it still\nensures a visually appealing result of comparable quality.\n3.3. Human-centric Gaussian Forward Animation\nAdvantages of Gaussian Approach. Traditional NeRF-\nbased methods [15, 22, 43, 45], involving inverse LBS for\ndeformation and ray sampling in observation space, face\nefficiency issues at high resolutions (e.g., 512 \u00d7 512 \u00d7 64 \u2248\n16, 777k), as depicted in Fig. 2. These methods struggle\nwith real-time rendering due to the vast number of sampling\npoints and the slow inverse LBS deformation process.\nOur approach deviates from these conventional methods\nby optimizing explicit Gaussians in canonical space and\n4\nanimating them into observation space. This method results\nin more efficient rendering, especially suitable for dynamic\nscenes.\nHuman-centric Gaussian Translation. Given the original\nGaussian position x, the transformation matrix of the i-th\nbone Gi(\u03b8, \u03b2), and the blend weight of the i-th bone wi. The\ndeformed Gaussian position x\u2032 is represented as:\nx\u2032 =\nN\nX\ni=1\nwiGi(\u03b8, \u03b2)x,\n(6)\nwhere Gi(\u03b8, \u03b2) is computed by SMPL vertex which is the\nnearest to the i-th Gaussian.\nHuman-centric Gaussian Rotation. Gaussians inherently\ndisplay anisotropic characteristics, meaning they exhibit\ndifferent properties in various directions. Therefore, accurate\nrotation adjustments are crucial when transitioning between\ncanonical and observation poses, ensuring the model\u2019s adapt-\nability across new observation viewpoints.\nUnlike positions, rotations cannot be directly derived us-\ning Linear Blend Skinning (LBS). To define the rotation of\nGaussians, we anchor each Gaussian to the SMPL mesh by\nidentifying the closest triangular facet based on Euclidean\ndistance. The position of a triangular facet\u2019s centroid, de-\nnoted as f p\nj , is calculated as the mean of its vertices f 1\nj , f 2\nj ,\nand f 3\nj :\nf p\nj = f 1\nj + f 2\nj + f 3\nj\n3\n.\n(7)\nFor the i-th Gaussian, the nearest triangle facet is determined\nby finding the minimum distance to xi:\nj\u2217 = argmin\nj\n\u2225xi \u2212 f p\nj \u2225.\n(8)\nWe then adopt the rotation Rfj\u2217 of this facet as the Gaus-\nsian\u2019s rotation transformation matrix, which we called Trian-\ngular Face Rotation:\nRi = Rfj\u2217 = ecanj\u2217 e\u22a4\nobj\u2217 ,\n(9)\nr\u2032\ni = Rot(Ri, ri),\n(10)\nwhere r\u2032\ni is rotated Gaussian rotation, ecan\u2217\nj and eob\u2217\nj represent\nthe orthonormal bases of the facet in the canonical and ob-\nserved poses, respectively. These bases are computed from\nthe edge vectors\u2019 normalized cross products. To enhance\ncomputational efficiency during training, we precompute Rf\nleveraging the known distribution of SMPL poses.\nRotation of Spherical Harmonics. To ensure that the ro-\ntation of spherical harmonics aligns with the human body\u2019s\nposture, we rotate these functions during rendering. As a\npart of Gaussians\u2019 attributes, spherical harmonics should\nrotate together with Gaussians to precisely represent the\nview-dependent colors. Given the rotation of i-th Gaussian\nCamera Pose Rotated, SMPL Global Orientation Fixed\nw/o Data Aug.\nwith Data Aug. \nCamera Pose Fixed,   SMPL Global Orientation Rotated\nTime\nFigure 4. Data Augmentation Technique (\u00a7 3.4). Through this\nmethod, we simulate the camera rotating around the human to make\nup multiple camera poses.\nRi , we can easily get rotated spherical harmonics by apply-\ning Ri into its direction di, that is:\nd\u2032\ni = SH Rot(Ri, di) = R\u22a4\ni di\n(11)\n3.4. Human-centric Gaussian Refinement\nAdaptive Gaussian Refinement. To capture the uniqueness\nof each frame, we use the frame index t in Positional En-\ncoding (PE) to get per-frame feature \u03b3(t). The Gaussian\npositions, represented as x, are fed through a Multilayer\nPerceptron (MLP) to calculate position, rotation, and scale\nresiduals. These are then used to adjust the Gaussian param-\neters for each frame, ensuring accuracy and consistency.The\nadaptive adjustments are represented as follows:\n\u2206x, \u2206r, \u2206s = F\u0398 (\u03b3(x), \u03b3(t), \u03b8, \u03b2) ,\n(12)\nwhere \u03b8, \u03b2 are the parameters of SMPL pose and shape.\nThis results in deformed parameters: position x\u2032\u2032 = x\u2032 +\u2206x,\nrotation r\u2032\u2032 = Rot(\u2206r, r\u2032), and scale s\u2032\u2032 = s + \u2206s.\nSpherical Harmonics Refinement. Similar to the refine-\nment process of Gaussians, given \u2206ri of the i-th Gaussian,\nwe can easily get refined Spherical harmonics by this for-\nmula:\nd\u2032\u2032\ni = SH Rot(\u2206ri, d\u2032\ni) = quat to rotmat(\u2206ri)\u22a4d\u2032\ni\n(13)\nThus, we get the refined Gaussians G(x\u2032\u2032, r\u2032\u2032, s\u2032\u2032, d\u2032\u2032), and\nsend them to the fast rasterization rendering module of [24]\nto get high-fidelity results.\n3.5. Data Augmentation Technique\nDuring our experimental process, we noted that spherical\nharmonics coefficients tended to overfit when limited to in-\nputs from a single, fixed camera perspective. This resulted\nin pronounced color biases, as depicted in Fig. 6. To miti-\ngate this issue, we adopted a data augmentation approach\nthat enhances the diversity of camera perspectives. With\n5\nZJU-MoCap [44]\nMonoCap [15]\nMethod\nPublication\nRes.\nTrain\nPSNR\u2191\nSSIM\u2191\nLPIPS*\u2193\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS*\u2193\nFPS\u2191\nStatic scene reconstruction method\n3D GS + SMPL Init. [24]\nSIGGRAPH 23\n512\n\u223c 5min\n26.57\n0.935\n71.70\n156\n28.47\n0.972\n29.57\n156\n3D GS + SMPL Init. [24]\nSIGGRAPH 23\n1024\n\u223c 5min\n26.53\n0.944\n58.23\n51.3\n27.47\n0.970\n34.37\n51.3\nTime-consuming human reconstruction method\nNeuralBody [44]\nCVPR 21\n512\n\u223c 10h\n29.03\n0.964\n42.47\n3.5\n32.36\n0.986\n16.70\n3.5\nAnimNeRF [43]\nICCV 21\n512\n\u223c 10h\n29.77\n0.965\n46.89\n2.1\n31.07\n0.985\n19.47\n2.1\nAnimSDF [45]\nArxiv 22\n512\n\u223c 10h\n30.38\n0.975\n37.23\n1.3\n32.48\n0.988\n13.18\n1.3\nHumanNeRF [62]\nCVPR 22\n512\n\u223c 10h\n30.66\n0.969\n33.38\n0.36\n32.68\n0.987\n15.52\n0.36\nTime-efficient human reconstruction method\nInstantAvatar [22]\nCVPR 23\n512\n\u223c 5min\n29.21\n0.936\n82.42\n8.75\n32.18\n0.977\n24.98\n8.75\nInstantNvr [15]\nCVPR 23\n512\n\u223c 5min\n30.87\n0.971\n40.11\n1.53\n32.61\n0.988\n16.68\n1.53\nOurs\n512\n\u223c 100s\n31.29\n0.964\n39.50\n104\n33.20\n0.983\n16.55\n104\nOurs\n512\n\u223c 5min\n31.79\n0.965\n35.75\n104\n32.63\n0.982\n16.51\n104\nInstantAvatar [22]\nCVPR 23\n1024\n\u223c 5min\n27.79\n0.912\n97.33\n3.83\n32.10\n0.978\n24.95\n3.83\nInstantNvr [15]\nCVPR 23\n1024\n\u223c 5min\n30.89\n0.974\n41.70\n0.54\nOOM\nOOM\nOOM\nOOM\nOurs\n1024\n\u223c 100s\n31.00\n0.968\n40.71\n68\n32.20\n0.983\n18.31\n68\nOurs\n1024\n\u223c 5min\n30.93\n0.967\n39.87\n68\n32.13\n0.983\n17.01\n68\nTable 1. Comparison with SOTA (\u00a7 4.2). We compare Human101 with several baseline methods. (1) Static scene reconstruction method:\n3D GS [24]. (2) Time-consuming human reconstruction methods: HumanNeRF [62], AnimSDF [45], NeuralBody [44] and AnimNeRF [43].\n(3) Time-efficient human reconstruction methods: InstantNvr [15], InstantAvatar [22]. LPIPS* = LPIPS \u00d7103, and \u201cOOM\u201d means out of\nGPU memory when training. For the FPS metric, we evaluate by calculating the inference time provided by the official pre-trained models.\nWe have marked out best and second best metrics of time-efficient human reconstruction methods.\nSMPL global orientation rotation matrix Rs, global trans-\nlation matrix Ts and camera pose Rc, Tc. We can get the\nfollowing equation, describing how SMPL coordinates xs\nare transformed into camera coordinates xc :\nxc = Rc(Rsxs + Ts) + Tc.\n(14)\nIf we assume that the SMPL coordinates align with the world\ncoordinates, (i.e. R\u2032\ns = E, T \u2032\ns = O), we can get:\nxc = R\u2032\ncxs + T \u2032\nc.\n(15)\nWith Eq. (14) and Eq. (15), we can easily get:\nR\u2032\nc = RcRs, T \u2032\nc = RcTs + Tc.\n(16)\nThis method effectively simulates the camera encircling the\nsubject, as shown in Fig. 4, leading to a more varied orien-\ntation in the spherical harmonics. It successfully diversifies\nthe orientations represented by the spherical harmonics, pre-\nventing overfitting and the associated color distortions.\n3.6. Training\nSetting. Our model takes single-view camera parameters,\nSMPL parameters, frame indices, and images as inputs. Dur-\ning training, frames are randomly sampled from the video.\nThe predicted image, denoted as \u02c6I, is constrained using the\nL1 loss and S3IM [63] loss. In contrast to the method in [15],\nwe chose not to use the VGG loss due to its high computa-\ntional demands, which could slow down our training process.\nFor now, we have also decided against incorporating regular-\nization terms. The loss function is formulated as:\nLrgb = \u03bb1||\u02c6I \u2212 Igt|| + \u03bb2S3IM(\u02c6I, Igt)\n(17)\nwhere \u03bb1 and \u03bb2 are the weighting factors for L1 and S3IM\nloss respectively, and Igt is the ground truth image.\n3.7. Implementation Details\nFollowing [15, 24], we employ the Adam optimizer [25],\nsetting distinct learning rates for different parameters. Our\nmodel is trained on an RTX 3090 GPU, and it seamlessly\nreaches a level of performance comparable to previous meth-\nods in just about 100 seconds. Following this, the model re-\nquires roughly 10,000 iterations, culminating in convergence\nwithin about 5 minutes. Notably, while our model excels at\nsingle-view reconstruction, it further enhances accuracy and\nresults when applied to multi-view reconstructions, all within\nthe same 5-minute timeframe. For an in-depth comparison\nof multi-view reconstructions, we direct interested readers to\nthe supplementary materials. Additionally, a comprehensive\noverview of our network structure and hyper-parameters can\nalso be found in the supplementary section.\n4. Experiments\n4.1. Datasets\nZJU-MoCap Dataset. ZJU-Mocap [44] is a prominent\nbenchmark in human modeling from videos, supplying fore-\nground human masks and SMPL parameters. Similar to [15],\nour experiments engage 6 human subjects (377, 386, 387,\n392, 393, 394) from the dataset. Training utilizes one camera,\nwhile the remaining cameras are designated for evaluation.\nEach subject contributes 100 frames for training.\nMonoCap Dataset. The MonoCap Dataset combines four\nmulti-view videos from the DeepCap [16] and DynaCap [17]\ndatasets, collected by [43]. This dataset provides essential\ndetails like camera parameters, SMPL parameters and human\nmasks. We choose the same 4 subjects as [15] for better\ncomparison. Further details about all the sequences in our\nstudy can be found in the supplementary material.\n6\nGT\nOurs\nOurs\nInstantNvr\nInstantAvatar 3D GS\n100 s\n103 FPS\n5 min\n104 FPS\n5 min\n1.5 FPS\n5 min\n8.7 FPS\n5 min\n156 FPS\n100 s\n103 FPS\n5 min\n104 FPS\n5 min\n1.5 FPS\n5 min\n8.7 FPS\n5 min\n156 FPS\nGT\nOurs\nOurs\nInstantNvr\nInstantAvatar 3D GS\nFigure 5. Compare with the state-of-the-art works (\u00a7 4.2). For a fair comparison, we show the results of 512 \u00d7 512 resolutions. Our\nmodel delivers results with superior visual quality and richer details, achieving a 67 \u00d7 increase in FPS compared with the state-of-the-art\ntime-efficient methods. Please \u00fc zoom in for a more detailed observation.\n4.2. Comparison with the state-of-the-art methods\nBaselines. We compare our method with some previous\nhuman reconstruction methods [15, 22, 43\u201345] and baseline\nmethod 3D GS [24]. The methods for human reconstruction\ncan be categorized into 3 groups:\n\u2022 Static scene reconstruction method. 3D GS [24] is the\nbackbone of our model. While its original point cloud\ninitialization method failed to extract a point cloud from\na single fixed-view camera, we use the canonical SMPL\nvertices together with white colors as its initial point cloud.\n\u2022 Time-consuming human reconstruction methods. Neu-\nralBody [44] utilizes structured SMPL data together with\nper-frame latent codes to optimize neural human radiance\nfields. AnimatableNeRF(AnimNeRF) [43] and Animata-\nbleSDF(AnimSDF) [45] use SMPL deformation and pose-\ndependent neural blend weight field to model dynamic\nhumans. HumanNeRF [62] further optimizes volumetric\nhuman representations, and improves detail quality of ren-\ndered image. However, due to the slow optimization of\nMLPs, these methods usually converge very slow. For in-\nstance, [62] takes about 72 hours on 4 RTX 2080Ti GPUs\nto totally converge.\n\u2022 Time-efficient human reconstruction methods. Utiliz-\ning [39]\u2019s voxel grid representation, Instantnvr [15] man-\nage to shorten the convergence time into 5 minutes. In-\nstantAvatar [22] combines [39] with a rapid deformation\nmethod [8], achieving a fast reconstruction in minutes.\nMetrics. We choose PSNR, SSIM, LPIPS [69] as visual\nquality evaluation metrics, and frame per second (FPS) as\nrendering speed evaluation metrics. For better comparison,\nwe show LPIPS* = LPIPS \u00d7103 instead. Tab. 1 shows our\nresults compared with others. Here we list only the average\nmetric values of all selected characters on a dataset due to\nthe size limit while more detailed qualitative and quantitative\ncomparisons are in the supplementary material.\nDiscussion on quantitative results. Tab. 1 presents a com-\nprehensive quantitative comparison between our method and\nother prominent techniques like InstantNvr [15], InstantA-\nvatar [22], 3D GS [24], HumanNeRF [62], AnimSDF [45],\nNeuralBody [44], and AnimNeRF [43]. Remarkably, our\napproach achieves optimization within approximately 100\nseconds, yielding results that are comparable with or surpass\n[15, 22, 24] in terms of PSNR and LPIPS. For 5 minutes\u2019\nresults, we achieved the highest PSNR and LPIPS scores\namong rapid reconstruction methods. While our SSIM scores\nare quite high, they do not reach the state-of-the-art level.\nThis is partly attributed to the characteristics of spherical\nharmonics in our model. Spherical harmonics, by their na-\n7\n(a) Ablation on Each Component \n(b) Ablation on Initialization \nGT\nGT\nOurs\nFull\nOurs \nw/o Aug.\nOurs \nw/o SMPL Deform\nOurs \nw/o Adjust\nOurs \nw/o SH Rot.\nRandom\nInit.\nSMPL\nInit.\nOurs\nFigure 6. Ablation studies on the Sequence 377 of ZJU-MoCap dataset [44] (\u00a7 4.3). (a) Removing our proposed components leads to and\nblurry appearance and artifacts. (b) Initialization plays a pivotal role in the geometry quality.\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS* \u2193\nw/o SMPL Deformation\n28.70\n0.964\n41.61\nw/o Augmentation\n29.00\n0.961\n41.30\nw/o Frame Embedding\n32.26\n0.977\n22.51\nw/o Gaussian Adjustment\n32.55\n0.977\n23.20\nOurs Full\n32.18\n0.977\n21.32\nTable 2. Ablation study on component efficacy (\u00a7 4.3). This table\ndemonstrates the impact of individual components in our method.\nBy selectively disabling each part, we validate their effectiveness.\nThe experiments were conducted on Sequence 377 of the ZJU-\nMoCap dataset [44].\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS* \u2193\nRandom Init.\n26.98\n0.951\n62.35\nSMPL Init.\n31.84\n0.974\n28.60\nOurs\n32.18\n0.977\n21.32\nTable 3. Ablation on initialization method (\u00a7 4.3). We use Se-\nquence 377 on ZJU-MoCap[44] to test the effectiveness of our\ncanonical human initialization process.\nture, are somewhat limited in capturing high-frequency color\ninformation. Moreover, for inference speed, our model is 67\ntimes faster than InstantNvr [15] and 11 times faster than\nInstantAvatar [22] in 512 \u00d7 512 resolution.\nDiscussion on qualitative results. Fig. 5 showcases a com-\nparison of our model with time-efficient reconstruction\nworks [15, 22, 24]. Our method stands out by providing the\nmost detailed representation and minimal artifacts, as high-\nlighted in Fig. 5 with red boxes around key details. In con-\ntrast, [15]\u2019s backside results exhibit unnatural colors on the\nbody\u2019s front due to light penetration and loss of details like\nmissing logos on trousers. InstantAvatar [22] generates no-\nticeable scattered artifacts around the body. Meanwhile, 3D\nGS [24], lacking a deformation module for dynamic scenes,\nresults in severe limb truncations and facial distortions. Fur-\nthermore, in terms of rendering speed, except for [24], our\ninference speed surpasses all compared methods.\n4.3. Ablation Study\nOur ablation study results on the ZJU-MoCap [44] 377 se-\nquence are displayed in Tab. 2, Tab. 3, and Fig. 6 (a). Due to\nthe space limitations, more experiment results can be found\nin the supplementary material.\nHuman-centric Gaussian rigid deformation. Omitting it,\n(i.e. without SMPL-based rigid translation and rotation, w/o\nSMPL Deform.), leads to noticeable deficiencies in body\nparts and a loss of detail, as shown in Fig. 6 (a). This exper-\niment highlights the limitations of simple SMPL deforma-\ntion in fully capturing complex human motions. The results\nunderscore the necessity of integrating more sophisticated\ndeformation techniques to accurately model human move-\nments.\nSpherical rotation. The results, illustrated in Fig. 6 (a),\nhighlight that omitting spherical rotation leads to \u201cspiky\u201d\nartifacts on the human body, compromising visual quality\nand causing abnormal lighting effects in the rendered images.\nData augmentation. Our data augmentation technique is\ncrucial, as evidenced by overfitting of spherical harmonics\nand noticeable skin artifacts in novel view synthesis when\nit\u2019s absent, as depicted in Fig. 6 (a).\nAdaptive Gaussian Refinement. The lack of the Adaptive\nGaussian Refinement module results in the model\u2019s inabil-\nity to capture subtle human deformations (like in fingers),\nleading to visible artifacts (shown in Fig. 6 (a)).\nInitialization method. Our comprehensive study demon-\nstrates the effectiveness of our novel canonical human initial-\nization method. As shown in Fig. 6 (b), this method signifi-\ncantly enhances the quality of human body reconstruction.\n5. Conclusion\nIn this paper, we introduced Human101, a novel framework\nfor single-view human reconstruction using 3D Gaussian\nSplatting (3D GS) [24]. Our method efficiently reconstructs\nhigh-fidelity dynamic human models within just 100 seconds\nusing a fixed-perspective camera. The integration of a novel\nCanonical Human Initialization, Human-centric Gaussian\nForward Animation, and Human-centric Gaussian Refine-\nment, coupled with 3D GS\u2019s explicit representation, signifi-\ncantly improve the rendering speed. Moreover, this enhance-\nment in speed does not sacrifice visual quality. Experiments\ndemonstrate that Human101 outperforms up to 67 times in\nFPS compared with the state-of-the-art methods and main-\ntain comparable or better visual quality. Human101 sets\na new standard in human reconstruction from single-view\nvideos. This breakthrough lays the groundwork for further\nadvances and applications in immersive technologies.\n8\nReferences\n[1] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2022. 2\n[2] Alexander W. Bergman, Petr Kellnhofer, Wang Yifan, Eric R.\nChan, David B. Lindell, and Gordon Wetzstein. Generative\nneural articulated radiance fields. In NeurIPS, 2022. 3\n[3] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\nGehler, Javier Romero, and Michael J. Black. Keep it SMPL:\nAutomatic estimation of 3D human pose and shape from a\nsingle image. In Computer Vision \u2013 ECCV 2016. Springer\nInternational Publishing, 2016. 3\n[4] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affinity\nfields. In CVPR, 2017. 2\n[5] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\nSheikh. Openpose: Realtime multi-person 2d pose estima-\ntion using part affinity fields. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2019. 2\n[6] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skinning\nfor animating non-rigid neural implicit shapes. In Interna-\ntional Conference on Computer Vision (ICCV), 2021. 3\n[7] Xu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael J\nBlack, Andreas Geiger, and Otmar Hilliges. gdna: Towards\ngenerative detailed neural avatars. arXiv, 2022. 3\n[8] Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, Andreas\nGeiger, Michael J. Black, and Otmar Hilliges. Fast-snarf: A\nfast deformer for articulated neural fields. Pattern Analysis\nand Machine Intelligence (PAMI), 2023. 3, 7\n[9] Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li,\nYu Guo, Jue Wang, and Fei Wang. Uv volumes for real-time\nrendering of editable free-view human performance. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16621\u201316631, 2023. 3\n[10] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea\nTagliasacchi. Mobilenerf: Exploiting the polygon rasteriza-\ntion pipeline for efficient neural field rendering on mobile\narchitectures. arXiv preprint arXiv:2208.00277, 2022. 3\n[11] Enric Corona, Albert Pumarola, Guillem Aleny`a, Ger-\nard Pons-Moll, and Francesc Moreno-Noguer.\nSmplicit:\nTopology-aware generative model for clothed people. In\nCVPR, 2021. 3\n[12] Zijian Dong, Chen Guo, Jie Song, Xu Chen, Andreas Geiger,\nand Otmar Hilliges. Pina: Learning a personalized implicit\nneural avatar from a single rgb-d video sequence. arXiv, 2022.\n3\n[13] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural voxels.\nIn SIGGRAPH Asia 2022 Conference Papers, 2022. 3\n[14] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps, 2021. 3\n[15] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei\nZhou. Learning neural volumetric representations of dynamic\nhumans in minutes. In CVPR, 2023. 1, 2, 3, 4, 6, 7, 8, 12, 13,\n16, 17, 18\n[16] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-\nard Pons-Moll, and Christian Theobalt. Deepcap: Monocu-\nlar human performance capture using weak supervision. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR). IEEE, 2020. 6, 12\n[17] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zoll-\nhoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time\ndeep dynamic characters. ACM Transactions on Graphics, 40\n(4), 2021. 6, 12\n[18] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. ICCV, 2021. 3\n[19] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai,\nLei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-driven\ngeneration and animation of 3d avatars. ACM Transactions\non Graphics (TOG), 41(4):1\u201319, 2022. 3\n[20] Timothy Jeruzalski, David I. W. Levin, Alec Jacobson, Paul\nLalonde, Mohammad Norouzi, and Andrea Tagliasacchi.\nNilbs: Neural inverse linear blend skinning, 2020. 3\n[21] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Self-\nrecon: Self reconstruction your digital avatar from monocular\nvideo. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 2, 3\n[22] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Instan-\ntavatar: Learning avatars from monocular video in 60 seconds.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 16922\u201316932,\n2023. 1, 2, 4, 6, 7, 8, 12, 16, 17, 18\n[23] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and\nAnurag Ranjan. Neuman: Neural human radiance field from\na single video. In Proceedings of the European conference\non computer vision (ECCV), 2022. 2\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3d gaussian splatting for real-time radiance\nfield rendering. ACM Transactions on Graphics, 42(4), 2023.\n2, 3, 4, 5, 6, 7, 8, 13, 16, 17, 19\n[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization, 2017. 6\n[26] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3d human pose\nand shape via model-fitting in the loop. In ICCV, 2019. 2\n[27] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. Point-based neural rendering with per-view\noptimization. In Computer Graphics Forum, pages 29\u201343.\nWiley Online Library, 2021. 3\n[28] Jonas Kulhanek and Torsten Sattler. Tetra-nerf: Represent-\ning neural radiance fields using tetrahedra. arXiv preprint\narXiv:2304.09987, 2023. 3\n[29] Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Haber-\nmann, and Christian Theobalt. Deliffas: Deformable light\nfields for fast avatar synthesis. Advances in Neural Informa-\ntion Processing Systems, 2023. 3\n[30] Christoph Lassner and Michael Zollhofer.\nPulsar: Effi-\ncient sphere-based neural rendering.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1440\u20131449, 2021. 3\n9\n[31] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jur-\ngen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:\nTemplate-free animatable volumetric actors. 2022. 3\n[32] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo\nKanazawa. Nerfacc: Efficient sampling accelerates nerfs.\narXiv preprint arXiv:2305.04966, 2023. 3\n[33] Siyou Lin, Hongwen Zhang, Zerong Zheng, Ruizhi Shao,\nand Yebin Liu. Learning implicit templates for point-based\nclothed human modeling. In ECCV, 2022. 3\n[34] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural\nvolumes: Learning dynamic renderable volumes from images.\nACM Trans. Graph., 38(4):65:1\u201365:14, 2019. 3\n[35] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model. ACM Trans. Graphics (Proc. SIG-\nGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 3, 4\n[36] Marko Mihajlovic, Yan Zhang, Michael J Black, and Siyu\nTang. LEAP: Learning articulated occupancy of people. In\nProceedings IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2021. 3\n[37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. In ECCV, 2020. 3\n[38] Claus M\u00a8uller. Spherical harmonics. Springer, 2006. 3\n[39] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multiresolu-\ntion hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15,\n2022. 2, 3, 7, 18\n[40] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya\nHarada. Unsupervised learning of efficient geometry-aware\nneural articulated representations. In European Conference\non Computer Vision, 2022. 3\n[41] Nikolay Patakin, Dmitry Senushkin, Anna Vorontsova, and\nAnton Konushin. Neural global illumination for inverse ren-\ndering. In 2023 IEEE International Conference on Image\nProcessing (ICIP), pages 1580\u20131584. IEEE, 2023. 3\n[42] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo\nBolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3d hands, face,\nand body from a single image. In Proceedings IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR), 2019. 3\n[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang,\nQing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable\nneural radiance fields for modeling dynamic human bodies.\nIn ICCV, 2021. 1, 2, 3, 4, 6, 7, 12, 13, 16, 17\n[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In CVPR, 2021.\n2, 6, 7, 8, 12, 13, 16, 17\n[45] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi\nJiang, Hujun Bao, and Xiaowei Zhou. Animatable neural\nimplicit surfaces for creating avatars from videos. arXiv\npreprint arXiv:2203.08133, 2022. 1, 2, 4, 6, 7, 12, 13\n[46] Ruslan Rakhimov, Andrei-Timotei Ardelean, Victor Lem-\npitsky, and Evgeny Burnaev. Npbg++: Accelerating neural\npoint-based graphics. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n15969\u201315979, 2022. 3\n[47] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In ICCV, pages 14335\u201314345, 2021.\n3\n[48] Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. Adop:\nApproximate differentiable one-pixel point rendering. ACM\nTransactions on Graphics (ToG), 41(4):1\u201314, 2022. 3\n[49] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. In The IEEE International Conference on Computer\nVision (ICCV), 2019. 2\n[50] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In CVPR, 2020. 2\n[51] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.\nBlack. SCANimate: Weakly supervised learning of skinned\nclothed avatar networks. In Proceedings IEEE/CVF Conf. on\nComputer Vision and Pattern Recognition (CVPR), 2021. 3\n[52] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In CVPR, 2022. 2,\n3\n[53] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh.\nHand keypoint detection in single images using multiview\nbootstrapping. In CVPR, 2017. 2\n[54] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00a8ofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In Proc.\nCVPR, 2019. 3\n[55] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo\ntourism: Exploring photo collections in 3d.\nACM Trans.\nGraph., 25(3):835\u2013846, 2006. 4\n[56] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Black Michael J., and\nTao Mei. Monocular, One-stage, Regression of Multiple 3D\nPeople. In ICCV, 2021. 2\n[57] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J\nBlack. Putting People in their Place: Monocular Regression\nof 3D People in Depth. In CVPR, 2022.\n[58] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael J. Black.\nTRACE: 5D Temporal Regression of Avatars with Dynamic\nCameras in 3D Environments. In CVPR, 2023. 2\n[59] Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Gerard\nPons-Moll. Neural-gif: Neural generalized implicit functions\nfor animating people in clothing. In International Conference\non Computer Vision (ICCV), 2021. 3\n[60] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu Tang.\nArah: Animatable volume rendering of articulated human sdfs.\nIn European Conference on Computer Vision, 2022. 3\n[61] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser\nSheikh. Convolutional pose machines. In CVPR, 2016. 2\n10\n[62] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 16210\u201316220, 2022. 2, 3, 6, 7\n[63] Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang,\nHaoran Wang, Yunfeng Cai, and Mingming Sun.\nS3im:\nStochastic structural similarity and its unreasonable effec-\ntiveness for neural fields. In International Conference on\nComputer Vision, 2023. 6\n[64] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J.\nBlack. ICON: Implicit Clothed humans Obtained from Nor-\nmals. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 13296\u2013\n13306, 2022. 2, 4\n[65] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and\nMichael J. Black. ECON: Explicit Clothed humans Optimized\nvia Normal integration. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR),\n2023. 2, 4\n[66] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. Plenoctrees for real-time rendering of\nneural radiance fields, 2021. 3\n[67] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu,\nYichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang,\nand Jiashi Feng. Avatargen: A 3d generative model for ani-\nmatable human avatars. In Arxiv, 2022. 3\n[68] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and\nFelix Heide. Differentiable point-based radiance fields for\nefficient view synthesis. In SIGGRAPH Asia 2022 Conference\nPapers, pages 1\u201312, 2022. 3\n[69] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 7\n[70] Ruiqi Zhang, Jie Chen, and Qiang Wang. Explicifying neural\nimplicit fields for efficient dynamic human avatar modeling\nvia a neural explicit surface. arXiv preprint arXiv:2308.05112,\n2023. 1, 3\n[71] Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen,\nand Yi Yang.\nGlobal-correlated 3d-decoupling trans-\nformer for clothed avatar reconstruction.\narXiv preprint\narXiv:2309.13524, 2023. 2, 4\n[72] Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu,\nand Yebin Liu. Avatarrex: Real-time expressive full-body\navatars. ACM Transactions on Graphics (TOG), 42(4), 2023.\n1, 3\n11\nHuman101: Training 100+FPS Human Gaussians in 100s from 1 View\nSupplementary Material\nA. Overview\nOverview of the Supplementary Material:\n\u2022 Implementation Details \u00a7 B:\n\u2013 Conventions in Symbolic Operations. \u00a7 B.1\n\u2013 Dataset. \u00a7 B.2\n\u2013 Baseline Implementation Details. \u00a7 B.3\n\u2013 Hyperparameters. \u00a7 B.4\n\u2013 Network Structure. \u00a7 B.5\n\u2013 Canonical Human Initialization. \u00a7 B.6\n\u2013 Details of Triangular Face Rotation Matrices. \u00a7 B.7\n\u2022 Additional Quantitative Results \u00a7 C:\n\u2013 Novel View Results. \u00a7 C.1\n\u2013 Multi-view Results. \u00a7 C.2\n\u2022 Additional Qualitative Results \u00a7 D:\n\u2013 Depth Visualization Results. \u00a7 D.1\n\u2013 Novel Pose Results. \u00a7 D.2\n\u2022 More Experiments \u00a7 E:\n\u2013 Memory Efficiency Comparison. \u00a7 E.1\n\u2013 Ablation Study. \u00a7 E.2\n\u2013 Failure Cases. \u00a7 E.3\n\u2022 Downstream Applications \u00a7 F:\n\u2013 Composite Scene Rendering Results. \u00a7 F.1\n\u2022 More Discussions \u00a7 G:\n\u2013 Data Preprocessing Technique. \u00a7 G.1\n\u2013 Limitations. \u00a7 G.2\n\u2013 Ethics Considerations. \u00a7 G.3\nB. Implementation Details\nB.1. Conventions in Symbolic Operations\nIn our work, the rotation operations involve various types of\nrotational quantities (such as rotation matrices and quater-\nnions). For simplicity, we represent these rotation operations\nin the format of \u201cmultiplication\u201d in the main text. Here, we\ndetail this representation more concretely:\nFor Gaussian rotation ri, when optimizing ri, it is con-\nsidered a quaternion. While rotating it by the triangular face\nrotation matrix Ri, we first convert Ri into a unit quaternion\nand express this process using quaternion multiplication.\nThus, this operation is denoted as:\nr\u2032\ni = quat multi (rotmat to quat(Ri), ri)\n(18)\nWhen applying rotation in the refinement module, the\npredicted \u2206ri is a quaternion. Therefore, this rotation is\nexpanded as:\nr\u2032\u2032\ni = quat multi(\u2206ri, r\u2032\ni)\n(19)\nFor spherical harmonics, modifying spherical coefficients\ndirectly is not efficient. A more effective approach is to\ninversely rotate the view direction di. Specifically, we first\ncalculate the direction from the camera center Pc to the final\nGaussian position x\u2032\u2032\ni as the view direction. Then, we apply\nthe inverse rotation transformation to view directions as the\ninput for SH evaluation. Specifically, we have:\ndi = x\u2032\u2032\ni \u2212 Pc\n(20)\nd\u2032\ni = SH Rot(Ri, di)\n(21)\nd\u2032\u2032\ni = SH Rot(quat to rotmat(\u2206ri), d\u2032\ni)\n(22)\nThe function SH Rot takes a rotation matrix and a view\ndirection as input, returning a rotated view direction:\nSH Rot(R, d) = R\u22121d = R\u22a4d\n(23)\nB.2. Dataset\nZJU-MoCap. For ZJU-MoCap Dataset [44], we choose\n6 subjects (377, 386, 387, 392, 393, 394) for evaluation.\nBecause other subjects tend to not appear on the full side in a\nsingle fixed view. And following [15], we use camera 04 for\ntraining and other views for testing. Due to the low quality\nof the images in camera 03 for Subject 377 and Subject 392,\nwe filter out these two views.\nMonoCap. MonoCap is re-collected by [45], with Lan\n& Marc 1024 \u00d7 1024 resolution, selected from DeepCap\ndataset [16] and olek & vlad 1295 \u00d7 940 resolution selected\nfrom [17]. For better comparison, we show the FPS results\nof Lan and Marc. The DeepCap dataset [16] and DynaCap\ndataset [17] are only granted for non-commercial academic\npurposes. They prohibit the redistribution of that data. The\nusers should also sign a license. More frame-selecting details\nare illustrated in Tab. 4.\nB.3. Baseline Implementation Details\nFor Neural Body [44], Animatable NeRF [43], and Animat-\nableSDF [45], we utilized the results released in [15]. We\nalso tested their rendering speeds by inferring with their pre-\ntrained models on the same device using a single RTX 3090\nGPU.\nThe work presented in [22] did not have an implemen-\ntation for the ZJU-MoCap and MonoCap Datasets due to\ntheir slightly varied SMPL definition. Consequently, we ad-\njusted the deformer in [22] to match the SMPL vertices.\nIt\u2019s important to note that [22] is designed specifically for\nmonocular datasets, and it refines the SMPL parameters be-\nfore metric evaluation. For a fair comparison, we adhered\n12\nDataset\nsubject\nTraining view\nTesting view index\nStart Frame\nEnd Frame\nFrame Interval\n386, 387, 393, 394\n4\nRemaining\n0\n500\n5\nZJU-MoCap [44]\n377, 392\n4\nRemaining except 3\n0\n500\n5\nLan\n0\nRemaining\n620\n1120\n5\nMarc\n0\nRemaining\n35000\n35500\n5\nOlek\n44\n0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 49\n12300\n12800\n5\nMonoCap [43]\nVlad\n66\n0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\n15275\n15775\n5\nTable 4. Dataset settings (\u00a7 B.2).\nto the same SMPL and camera parameters provided by the\nZJU-MoCap Dataset and MonoCap Dataset, as with other\nbaseline methods [15, 43\u201345], and chose not to refine the\nSMPL parameters before evaluation.\nRegarding the 3D Gaussian Splatting [24], COLMAP\ncould not determine valid camera parameters due to the in-\nput of monocular fixed-view video frames. As a solution, we\nopted to use the SMPL vertices from the initial frame as the\ninput point cloud positions and designated the point cloud\ncolors as white. Given that [24] is primarily a static multi-\nview 3D reconstruction method, achieving convergence in\nour setup proved challenging. Hence, we present the out-\ncomes at 30k iterations, consistent with its original settings.\nB.4. Hyperparameters\nWe experimentally fine-tuned our model employing a set\nof hyperparameters tailored for optimal performance. Re-\ngarding the spherical harmonics, we employed third-degree\nspherical harmonics for their balance of computational effi-\nciency and representational fidelity. Uniquely, we increment\nthe degree of spherical harmonics every 500 iterations, cul-\nminating at a maximum degree of three. For the learnable\nMLP component, we set the learning rate at 2\u00d710\u22123. During\nthe optimization of Gaussians, we implemented an opacity\nreset at every 1500 iterations to refine transparency values.\nB.5. Network Structure\nTo compensate for the rigid position and rotation using the\nlearnable MLP, we employ straightforward linear layers, fea-\nturing a total of 5 layers with nhidden dim = 64. ReLU serves\nas the activation function between these layers, while no\nactivation function is applied to the output. For SMPL pa-\nrameters, we use a simple linear layer to compress its feature\ndimension. We use Positional Encoding with a frequency of\n10. Fig. 7 demonstrates the structure of the linear networks.\nB.6. Canonical Human Initialization\nIn the initialization process, we use an algorithmic approach\ninstead of manually selecting four photos. Our objective is\nto select four images where the person\u2019s angles on each are\napproximately 90 degrees apart. Additionally, it is preferable\nthat the person\u2019s pose in these images closely resembles the\ncanonical pose. This ensures minimal accuracy loss when de-\nforming the point cloud estimated by econ into the canonical\npose. To achieve this, we undertake the following steps:\n1. Identify suitable image pairs. We traverse the dataset\u2019s\nframes and for each frame index in frame index T, we main-\ntain a set Ci, Ci records all frame indices whose angle \u03b4 with\nframe index i is between 80-100 degrees. The formula is as\nfollows:\nCi = {j | 80 \u2264 \u03b4ij \u2264 100, \u2200j \u0338= i and j > i}, \u2200i \u2208 T\n(24)\nThe angle \u03b4ij between frames i and j is derived by calculat-\ning the difference in angles of the global rotation matrices\nRglobal from the SMPL parameters of the two frames. The\nformula is as follows:\nRdiff ij = R\u22121\nglobal i \u00b7 Rglobal j\n(25)\n\u03b4ij = as euler(Rdiff ij)\n(26)\n2. Select a suitable group of frames. The second part in-\nvolves identifying a set of four images that meet the criteria,\nexecuted through a four-level nested loop. Initially, frame\ni is selected, followed by choosing j from the set Ci. Sub-\nsequently, k is selected from j\u2019s set Cj, and l from Ck. For\neach selected group of frames (i, j, k, l), the algorithm first\nchecks if the angular difference between every two frames\nexceeds 80 degrees. Then, it computes the distance between\nthe pose\u2019s joint positions in these images and the joint po-\nsitions of the canonical pose. Finally, the group of frames\nwith the smallest distance is selected. The process is shown\nin Algorithm 1.\nB.7. Details of Triangular Face Rotation Matrices\nThe process of computing rotation matrices involves two\nmain steps: first, determining the orthonormal basis vectors\n(ecan and eob) that describe the orientation of each triangular\nfacet of the SMPL model in the canonical and target poses,\nrespectively; second, constructing the rotation matrix from\nthese basis vectors.\nFor each triangular facet f constituted by vertices A, B,\nand C, and edges AB, AC, and BC, we define the first unit\ndirection vector as:\n\u2212\u2192a =\n\u2212\u2212\u2192\nAB\n\u2225\u2212\u2212\u2192\nAB\u2225\n(27)\n13\nLinear Layer\nReLU\nPosition \u0394x\nRotation \u0394r\nScale \u0394s\nFrame Index T\nSMPL Param. p\nPostional Encoding Module\nSMPL Encoder\nRefinement MLP\nRefinement MLP\nRefinement MLP\nRefinement MLP\nConcat\nInput or Output\nPostional\nEncoding\nSMPL\nEncoder\nSMPL Encoder\n72+10+3\nRefinement MLP\n64\n64\n64\n64\n64\nEncoded Info.\n\u03b3(x)\n\u03b3(T)\np\u2019\nPosition x\nFigure 7. Network structure (\u00a7 B.5). This diagram presents the network architecture for refining the attributes of Gaussians. The input\nposition x, frame index T, and SMPL parameters p are first processed through positional encoding and an SMPL encoder, respectively. The\nencoded information \u03b3(x), \u03b3(T), and p\u2032 are then concatenated and passed through a series of refinement MLPs to produce adjustments in\nposition \u2206x, rotation \u2206r, and scale \u2206s. Each refinement MLP is composed of linear layers and employs ReLU activation functions.\nAlgorithm 1: Frame Selection (\u00a7 B.6)\nData: Sets T, C\nResult: Best frame indices Ibest and minimum distance\ndmin\ndmin \u2190 \u221e\nIbest \u2190 \u2205\nfor i \u2208 T do\nfor j \u2208 Ci do\nfor k \u2208 Cj do\nfor l \u2208 Ck do\nif \u03b4ik > 80\u25e6 and \u03b4il > 80\u25e6 and \u03b4jl > 80\u25e6\nthen\nd \u2190\ndistance(pose(i, j, k, l), canonical pose)\nif d < dmin then\ndmin \u2190 d\nIbest \u2190 {i, j, k, l}\nend if\nend if\nend for\nend for\nend for\nend for\nThen, we use the normal of the triangular plane as the second\nunit direction vector:\n\u2212\u2192b =\n\u2212\u2212\u2192\nAB \u00d7 \u2212\u2192\nAC\n\u2225\u2212\u2212\u2192\nAB \u00d7 \u2212\u2192\nAC\u2225\n(28)\nSubsequently, the third direction vector is derived from the\ncross-product of the first two unit vectors:\n\u2212\u2192c = \u2212\u2192a \u00d7 \u2212\u2192b\n(29)\nCombining these vectors, we obtain the orthonormal basis\nfor the triangular facet:\ne = (\u2212\u2192a , \u2212\u2192b , \u2212\u2192c )\n(30)\nHaving acquired the orthonormal bases in both canonical\nand observation spaces, the triangular face rotation matrix is\ncomputed as:\nRf = ecane\u22a4\nob\n(31)\nC. Additional Quantitative Results\nC.1. Novel View Results\nFor the novel view setting, Tab. 5 and Tab. 6 show our results\nseparately for resolution 512 \u00d7 512 and 1024 \u00d7 1024.\nC.2. Multi-view Results\nWhile our model was not specifically designed for multi-\nview training data, we have conducted tests on the ZJU-\nMoCap Dataset to assess its performance in such scenarios.\nThe results, as depicted in Fig. 9, demonstrate the model\u2019s\ncapability to handle multi-view inputs.\nD. Additional Qualitative Results\nD.1. Depth Visualization\nAs shown in Fig. 8, our method, with its explicit representa-\ntion, achieves a superior depth representation. This illustrates\nthe advantages of our approach in terms of geometric accu-\nracy.\n14\nFigure 8. Depth visualization results (\u00a7 D.1).\nGT\nOurs\nInstantNvr\nGT\nOurs\nInstantNvr\nFigure 9. Multi view results. Qualitative results of methods trained with 4 views on the Sequence 377 of the ZJU-MoCap dataset.\nD.2. Novel Pose Results\nThe results of our model trained on Subject 377 for unseen\nposes are shown in Fig. 10. Compared to the outcomes from\nInstantNvr, our results are less prone to artifacts and unnatu-\nral limb distortions. Simultaneously, our color reproduction\nis closer to the ground truth, with more preserved details in\nimage brightness.\nE. Additional Experiments\nE.1. Memory Efficiency Comparison\nTo assess the efficiency of our model, we compared its re-\nsource consumption during the inference process with re-\ncent works in the field. In our comparison, we focused on\nthree key metrics: training time GPU memory consumption\n(\u201cTrain Memory\u201d), inference GPU memory consumption\n(\u201cInfer Memory\u201d), and disk space required for storing the\nmodel checkpoints (\u201cModel Size\u201d). In our work, the model\n15\nGT\nOurs\nInstantNvr\nGT\nOurs\nInstantNvr\nGT\nOurs\nInstantNvr\nFigure 10. Novel pose results. (\u00a7 D.2) We show the results of unseen poses of our model and [15]. Results show that our model is less likely\nto produce artifacts or holes in unseen pose synthesis.\nZJU-MoCap [44]\nMethod\nTraining Time\nPSNR\nSSIM\nLPIPS*\nFPS\nPSNR\nSSIM\nLPIPS*\nFPS\nSubject\n377\n386\n3D GS [24]\n5min\n26.17\n0.949\n60.96\n156\n30.17\n0.951\n51.81\n156\nInstantNvr [15]\n5min\n31.69\n0.981\n32.04\n1.53\n33.16\n0.979\n38.67\n1.53\nInstartAvatar [22]\n5min\n29.90\n0.961\n49.00\n8.75\n30.67\n0.917\n111.5\n8.75\nOurs\n100s\n32.18\n0.977\n24.65\n104\n33.94\n0.972\n36.03\n104\nOurs\n5min\n32.02\n0.976\n21.35\n104\n33.78\n0.969\n33.73\n104\nSubject\n387\n392\n3D GS [24]\n5min\n24.56\n0.922\n80.61\n156\n26.72\n0.932\n79.61\n156\nInstantNvr [15]\n5min\n27.73\n0.961\n55.90\n1.53\n31.81\n0.973\n39.25\n1.53\nInstartAvatar [22]\n5min\n27.49\n0.928\n86.30\n8.75\n29.39\n0.934\n96.90\n8.75\nOurs\n100s\n28.32\n0.956\n47.76\n104\n32.22\n0.966\n41.89\n104\nOurs\n5min\n28.26\n0.956\n44.57\n104\n32.11\n0.967\n39.23\n104\nSubject\n393\n394\n3D GS [24]\n5min\n25.01\n0.923\n85.80\n156\n26.79\n0.932\n71.38\n156\nInstantNvr [15]\n5min\n29.46\n0.964\n46.68\n1.53\n31.26\n0.969\n39.89\n1.53\nInstartAvatar [22]\n5min\n28.17\n0.931\n86.60\n8.75\n29.64\n0.943\n64.20\n8.75\nOurs\n100s\n29.69\n0.957\n46.52\n104\n31.37\n0.967\n40.16\n104\nOurs\n5min\n29.52\n0.956\n44.15\n104\n31.25\n0.968\n36.86\n104\nMonoCap [43]\nMethod\nTraining Time\nPSNR\nSSIM\nLPIPS*\nFPS\nPSNR\nSSIM\nLPIPS*\nFPS\nSubject\nLan\nMarc\n3D GS [24]\n5min\n28.76\n0.970\n30.19\n156\n30.16\n0.972\n30.76\n156\nInstantNvr [15]\n5min\n32.78\n0.987\n17.13\n1.53\n33.84\n0.989\n16.92\n1.53\nInstartAvatar [22]\n5min\n32.43\n0.978\n20.90\n8.75\n33.88\n0.979\n24.40\n8.75\nOurs\n100s\n32.63\n0.982\n14.21\n104\n34.84\n0.983\n19.21\n104\nOurs\n5min\n32.56\n0.982\n13.20\n104\n35.02\n0.983\n17.25\n104\nSubject\nOlek\nVlad\n3D GS [24]\n5min\n28.32\n0.961\n45.24\n147\n23.13\n0.961\n51.16\n147\nInstantNvr [15]\n5min\n34.95\n0.991\n13.93\n1.48\n28.88\n0.984\n18.72\n1.48\nInstartAvatar [22]\n5min\n34.21\n0.980\n20.60\n8.43\n28.20\n0.972\n34.00\n8.43\nOurs\n100s\n34.31\n0.982\n15.07\n101\n28.96\n0.977\n23.56\n101\nOurs\n5min\n34.09\n0.983\n14.09\n101\n28.84\n0.977\n21.49\n101\nTable 5. 512 \u00d7 512 results of each subject on ZJU-MoCap dataset and Monocap dataset for novel view synthesis (\u00a7 C.1).\n16\nZJU-MoCap [44]\nMethod\nTraining Time\nPSNR\nSSIM\nLPIPS*\nFPS\nPSNR\nSSIM\nLPIPS*\nFPS\nSubject\n377\n386\n3D GS [24]\n5min\n26.03\n0.957\n50.22\n51.3\n30.17\n0.958\n47.97\n51.3\nInstantNvr [15]\n5min\n31.69\n0.981\n32.04\n0.5\n33.16\n0.979\n38.67\n0.5\nInstartAvatar [22]\n5min\n27.74\n0.933\n87.91\n3.83\n28.81\n0.916\n97.72\n3.83\nOurs\n100s\n31.76\n0.977\n30.27\n68\n33.66\n0.973\n37.30\n68\nOurs\n5min\n31.64\n0.976\n27.99\n68\n33.42\n0.973\n36.03\n68\nSubject\n387\n392\n3D GS [24]\n5min\n24.57\n0.931\n64.75\n51.3\n26.69\n0.9432\n60.72\n51.3\nInstantNvr [15]\n5min\n27.93\n0.968\n49.11\n0.5\n31.89\n0.977\n42.49\n0.5\nInstartAvatar [22]\n5min\n26.15\n0.890\n107.7\n3.83\n27.98\n0.9052\n106.9\n3.83\nOurs\n100s\n27.95\n0.959\n47.56\n68\n31.97\n0.970\n41.65\n68\nOurs\n5min\n28.02\n0.960\n46.03\n68\n31.86\n0.969\n40.83\n68\nSubject\n393\n394\n3D GS [24]\n5min\n24.97\n0.932\n67.65\n51.3\n26.72\n0.941\n58.07\n51.3\nInstantNvr [15]\n5min\n29.32\n0.969\n48.36\n0.5\n31.36\n0.968\n39.58\n0.5\nInstartAvatar [22]\n5min\n27.43\n0.899\n102.6\n3.83\n28.62\n0.926\n81.20\n3.83\nOurs\n100s\n29.52\n0.961\n46.08\n68\n31.10\n0.964\n41.39\n68\nOurs\n5min\n29.42\n0.960\n44.64\n68\n31.04\n0.963\n40.07\n68\nMonoCap [43]\nMethod\nTraining Time\nPSNR\nSSIM\nLPIPS*\nFPS\nPSNR\nSSIM\nLPIPS*\nFPS\nSubject\nLan\nMarc\n3D GS [24]\n5min\n28.44\n0.974\n25.95\n51.3\n30.13\n0.9762\n26.66\n51.3\nInstantNvr [15]\n5min\n32.61\n0.988\n12.73\n0.5\n33.76\n0.989\n17.01\n0.5\nInstartAvatar [22]\n5min\n32.89\n0.982\n17.30\n3.83\n33.72\n0.982\n21.81\n3.83\nOurs\n100s\n31.77\n0.982\n16.38\n68\n34.43\n0.984\n20.29\n68\nOurs\n5min\n31.72\n0.982\n15.55\n68\n34.56\n0.985\n18.96\n68\nSubject\nOlek\nVlad\n3D GS [24]\n5min\n28.34\n0.966\n33.12\n49.6\n23.14\n0.962\n51.73\n49.6\nInstantNvr [15]\n5min\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nOOM\nInstartAvatar [22]\n5min\n34.10\n0.983\n18.10\n3.42\n28.27\n0.967\n42.60\n3.42\nOurs\n100s\n34.04\n0.984\n16.19\n63\n28.53\n0.979\n20.37\n63\nOurs\n5min\n33.85\n0.983\n15.32\n63\n28.40\n0.980\n19.11\n63\nTable 6. 1024 \u00d7 1024 results of each subject on ZJU-MoCap dataset and Monocap dataset for novel view synthesis (\u00a7 C.1).\nMethod\nTraining Time\nPSNR\nSSIM\nLPIPS*\nFPS\nNeuralBody\n\u223c10hours\n32.99\n0.983\n26.8\n3.5\nHumanNeRF\n\u223c10hours\n32.28\n0.982\n19.6\n0.36\nAnimatableNeRF\n\u223c10hours\n32.31\n0.980\n32.2\n2.1\nAnimatableSDF\n\u223c10hours\n32.63\n0.983\n32.0\n1.3\nInstantNvr\n\u223c13mins\n32.55\n0.981\n26.5\n1.5\nOurs\n\u223c 5mins\n33.90\n0.981\n24.92\n104\nTable 7. Multi-view results comparison (\u00a7 C.2). Though our model is not designed for multi-view settings, we do experiments on 4 views\nof Sequence 377. Our model produces remarkable results using much less time while achieving good visual quality and evaluation metrics\nand much higher FPS.\nsize is computed by the sum of point cloud size and MLP\ncheckpoint size.\nAs illustrated in Tab. 8, Human101 demonstrates notable\nmemory efficiency compared to prior methods [15, 22]. Dur-\ning training, our model employs a strategy aligned with\ndownstream applications, opting for direct run-time query-\ning of the neural network for rendering. This decision not\nonly conserves space but also facilitates real-time rendering\ncapabilities, as opposed to pre-storing query results which\nwould increase storage requirements and impede real-time\nperformance.\nE.2. Ablation Study\nSparse Input Frames. Our model consistently delivers im-\npressive results even with fewer input video frames. For\nSubject 377, as detailed in Tab. 9, we showcase our perfor-\nmance metrics for varying frame counts, specifically at 250,\n100, 50, and 25 frames.\nPositional Encoding. In our experiments, we explored dif-\nferent positional encoding strategies for Gaussian positions,\n17\nMethod\nResolution\nTrain Memory\nInfer Memory\nModel Size\n512\u00d7512\n4542M\n3964M\n151M\n1024\u00d71024\n4542M\n4020M\n151M\n642\u00d7470\n4516M\n3966M\n151M\nInstantAvatar [22]\n1285\u00d7940\n4654M\n4038M\n151M\n512\u00d7512\n19132M\n4816M\n3.2G\n1024\u00d71024\n23320M\n4816M\n3.2G\n642\u00d7470\n21868M\n7660M\n3.2G\nInstantNvr [15]\n1285\u00d7940\nOOM\n-\n-\n512\u00d7512\n1878M\n956M\n12M+292K\n1024\u00d71024\n4146M\n1842M\n12M+292K\n642\u00d7470\n1932M\n1008M\n12M+292K\nOurs\n1285\u00d7940\n4726M\n2038M\n12M+292K\nTable 8. Memory efficiency comparison (\u00a7 E.1). For all resolutions in the dataset, we test the memory efficiency by Training GPU memory\nconsumption (\u201cTrain Memory\u201d), Inference GPU memory consumption (\u201cInfer Memory\u201d), and the size of the checkpoints (\u201cModel Size\u201d).\nResults demonstrate that our model utilizes much less GPU memory and disk usage than [15] while maintaining comparable or better visual\nquality. Note: when inferring, we don\u2019t precompute and save Gaussians in target space while we choose to query the network for each frame.\nThis methodological choice significantly reduces the storage requirements and makes it possible for Human101 to apply for more flexible\nuse cases.\nFrame Num\nPSNR\nSSIM\nLPIPS*\n25\n31.66\n0.974\n24.78\n50\n32.00\n0.975\n22.26\n100\n32.18\n0.977\n21.32\n250\n32.17\n0.977\n19.17\nTable 9. Ablation study on frame number (\u00a7 E.2). Our model\nstill maintains good visual quality using sparse frame inputs even\nwith only 25 images to train.\nMethod\nPSNR\nSSIM\nLPIPS*\nNoEnc\n32.13\n0.976\n24.47\nGridEnc\n31.99\n0.975\n29.47\nPE(Ours)\n32.18\n0.977\n21.32\nTable 10. Ablation study on encoding method (\u00a7 E.2). The re-\nsults demonstrate that the positional encoding method produces\nbetter quality than no encoding(\u201cNoEnc\u201d) and grid-encoding (\u201cGri-\ndEnc\u201d).\nspecifically comparing Instant-ngp [39]\u2019s grid encoding\nagainst the traditional sine and cosine positional encoding.\nWhile grid encoding can experimentally accelerate the fit-\nting process on the training frames, it also tends to make\nthe model more susceptible to overfitting. Consequently, as\ndemonstrated in Tab. 10, this results in suboptimal perfor-\nmance on novel view test frames.\nDegree of Spherical Harmonics. We have also performed\nablation experiments to determine the optimal degree of\nspherical harmonics for our reconstruction task. As indicated\nby Tab. 11, increasing the degree of spherical harmonics\nleads to improved reconstruction quality. However, higher\nDegree\nPSNR\nSSIM\nLPIPS*\n0\n31.77\n0.974\n24.05\n1\n32.04\n0.976\n22.55\n2\n32.13\n0.976\n21.60\n3(Ours)\n32.18\n0.977\n21.32\nTable 11. Ablation study on the degree of spherical harmonics\n(\u00a7 E.2). We evaluate the impact of the harmonics\u2019 degree on the\nquality of reconstruction, with the degree of 3 (our chosen con-\nfiguration) offering a trade-off between reconstruction detail and\ncomputational efficiency.\ndegrees bring a greater computational load. Consequently,\nwe have chosen to adopt third-degree spherical harmonics\nfor fitting in our final model, balancing accuracy with com-\nputational efficiency.\nConverge Speed on different Initialization.\nFig. 11\ndemonstrates that the choice of initialization method signifi-\ncantly impacts the model\u2019s convergence speed. Furthermore,\nFig. 12 shows different initialization strategies result in vary-\ning numbers of Gaussians at convergence. Generally, for the\nsame scene, a larger number of Gaussians at convergence\ncorresponds to richer reconstructed details. Since querying\nthe MLP network is the more time-consuming factor during\nthe inference phase, an increase in the number of Gaussians\ndoes not substantially affect the rendering FPS.\nE.3. Failure Cases\nOur model adeptly processes both monocular and multi-view\nvideo inputs, achieving high-fidelity reconstructions from\nsparse view inputs within a brief training duration. However,\nit is important to acknowledge the model\u2019s limitations. In in-\n18\n2000\n4000\n6000\n8000\n10000\nIteration\n20\n40\n60\n80\nLPIPS*\nRandom_Init\nSMPL_White_Init\nOurs\nFigure 11. Ablation study on convergence speed. (\u00a7 E.2) We\ncompare training view LPIPS results with the initialization method\nto be random initialization (\u201cRandom Init\u201d), bare SMPL with white\ncolor initialization (\u201cSMPL White Init.\u201d) and our Canonical Hu-\nman Initialization method (\u201cOurs\u201d) separately.\n2000\n4000\n6000\n8000\n10000\nIteration\n0\n5000\n10000\n20000\n30000\n40000\n50000\n60000\nGaussian count\nRandom_Init\nSMPL_White_Init\nOurs\nFigure 12. Ablation study on Gaussian count (\u00a7 E.2). We com-\npare the number of Gaussians at different stages using various\ninitialization methods. A superior initialization approach necessi-\ntates a greater number of Gaussians to represent the geometry more\nprecisely, which generally yields better results.\nstances where the input video fails to provide precise masks\n\u2014 for example, during intense movement where flowing hair\ncarries unmasked background elements \u2014 this can result in\nvisual artifacts, as depicted in Fig. 13.\nFigure 13. Failure case (\u00a7 E.3). When dealing with intense move-\nment where flowing hair carries unmasked background elements,\nour model may produce artifacts due to the complex human motion.\nFigure 14. Composite scene rendering (\u00a7 F.1). We render the\navatar integrated with the scene.\nF. Application\nF.1. Composite Scene Rendering\nRendering a human figure against a plain color background\nalone is not ideal for further downstream applications.\nThanks to the explicit representation capability of 3D Gaus-\nsian Splatting (3D GS) [24], we can effortlessly segregate\ndynamic human figures from static scenes by explicitly splic-\ning the Gaussians. This splicing process is natural and allows\nfor the easy separation of static backgrounds and dynamic\nhuman elements.\nAs demonstrated in Fig. 14, this functionality facilitates\ndownstream applications. In the example, the background\nand the human subject are trained separately and then com-\nposited during the rendering process. See supplementary\nvideos for better results.\nG. More Discussions\nG.1. Discussions on Data Preprocessing Technique\nGiven that our task operates within a single-camera set-\nting, we empirically observed during our experiments that,\nwithin fixed-view monocular videos, spherical harmonic co-\nefficients tend to overfit to a singular direction. This leads\nto subpar generalization for free-view videos, resulting in\nnumerous artifacts. To address this, we employed a data aug-\nmentation strategy that mimics a multi-camera environment.\n19\nWith access to the SMPL parameters detailing the global ro-\ntation of the human subject, it\u2019s intuitive to keep the human\norientation static while allowing the camera to orbit around\nthe figure. This mimics a nearly equivalent process. Using\nthis technique, we simulate varying camera viewpoints to\nrender the dynamic human across different frames, markedly\nboosting the generalizability of the spherical harmonic func-\ntions.\nHowever, this trick isn\u2019t devoid of limitations. In real-\nworld scenarios, due to the diffuse reflection of light, we of-\nten perceive varying colors for the same object from different\nviewpoints. Our strategy overlooks this variance, providing\nan approximation that might not always align perfectly with\nreal-world lighting conditions.\nG.2. Limitations\nWhile Human101 marks a significant advancement in dy-\nnamic human reconstruction, it is not without its limitations:\n\u2022 Dependency on SMPL parameter accuracy. Human101 is\nsignificantly affected by the accuracy of SMPL parameter\nestimation. Inaccurate parameters can introduce substan-\ntial noise, complicating the reconstruction process.\n\u2022 Requirement for complete body visibility in training data.\nOur model achieves the best results when training data\nincludes all body parts relevant to the task. Partial visibility,\nwhere some body parts are not fully captured, may lead to\nartifacts in the reconstructed model.\nAddressing these limitations could involve integrating more\ncomprehensive human body priors, providing a pathway for\nfuture enhancements to our framework.\nG.3. Ethics Considerations\nEthical considerations, particularly around privacy, consent,\nand data security, are critical in the development and appli-\ncation of Human101. Ensuring informed consent for all par-\nticipants and transparent communication about the project\u2019s\ncapabilities and limitations is essential to respect privacy\nand avoid misrepresentation. Secure handling and storage of\nsensitive human data are paramount to prevent unauthorized\naccess and misuse. Additionally, acknowledging the poten-\ntial for misuse of this advanced technology, we emphasize\nthe need for ethical guidelines to govern its responsible use.\nOur commitment is to uphold high ethical standards in all\naspects of Human101, safeguarding the respectful and secure\nuse of human data.\nG.4. Broader Impact\nThe development of Human101 has significant implications\nacross various domains. Its ability to rapidly reconstruct\nhigh-quality, realistic human figures from single-view videos\nholds immense potential in fields such as virtual reality, ani-\nmation, and telepresence. This technology can enhance user\nexperiences in gaming, film production, and virtual meet-\nings, offering more immersive and interactive environments.\nHowever, its potential misuse in creating deepfakes or vi-\nolating privacy cannot be ignored. It\u2019s crucial to balance\ninnovation with responsible use, ensuring that Human101\nserves to benefit society while minimizing negative impacts.\nOngoing dialogue and regulation are necessary to navigate\nthe ethical challenges posed by such advanced technology.\nOverall, Human101 stands to make a substantial impact in\nadvancing digital human modeling while prompting neces-\nsary discussions on technology\u2019s ethical use.\n20\n"
  }
]