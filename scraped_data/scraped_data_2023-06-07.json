[
  {
    "title": "Recognize Anything: A Strong Image Tagging Model",
    "link": "https://arxiv.org/pdf/2306.03514.pdf",
    "upvote": "9",
    "text": "Recognize Anything: A Strong Image Tagging Model\nYoucai Zhang\u22171, Xinyu Huang\u22171, Jinyu Ma\u22171, Zhaoyang Li\u22171, Zhaochuan Luo1, Yanchun Xie1,\nYuzhuo Qin1, Tong Luo1, Yaqian Li1, Shilong Liu2, Yandong Guo3, Lei Zhang2\n1OPPO Research Institute, 2International Digital Economy Academy (IDEA), 3AI2 Robotics\n\u2217Equal Contribution\n(zhangyoucai,huangxinyu2,majinyu,lichaoyang1)@oppo.com\nAbstract\nWe present the Recognize Anything Model (RAM): a\nstrong foundation model for image tagging. RAM makes\na substantial step for large models in computer vision,\ndemonstrating the zero-shot ability to recognize any com-\nmon category with high accuracy. RAM introduces a new\nparadigm for image tagging, leveraging large-scale image-\ntext pairs for training instead of manual annotations.\nThe development of RAM comprises four key steps.\nFirstly, annotation-free image tags are obtained at scale\nthrough automatic text semantic parsing. Subsequently, a\npreliminary model is trained for automatic annotation by\nunifying the caption and tagging tasks, supervised by the\noriginal texts and parsed tags, respectively. Thirdly, a data\nengine is employed to generate additional annotations and\nclean incorrect ones. Lastly, the model is retrained with the\nprocessed data and fine-tuned using a smaller but higher-\nquality dataset.\nWe evaluate the tagging capabilities of RAM on numer-\nous benchmarks and observe impressive zero-shot perfor-\nmance, significantly outperforming CLIP and BLIP. Re-\nmarkably, RAM even surpasses the fully supervised man-\nners and exhibits competitive performance with the Google\ntagging API. We are releasing the RAM at https:\n//recognize-anything.github.io/ to foster the\nadvancements of large models in computer vision.\n1. Introduction\nLarge language models (LLM) trained on large-scale\nweb datasets have sparked a revolution in nature language\nprocessing (NLP). These models[20, 5] exhibit impressive\nzero-shot generalization, enabling them to generalize to\ntasks and data distributions beyond their training domain.\nWhen it comes to computer vision (CV), Segment Anything\nModel (SAM) [12] has also demonstrated remarkable zero-\nshot localization abilities through data scaling-up.\nHowever, SAM lacks the capability to output semantic\nSAM\nLocalization\nRecognition\n(Accuracy & Scope)\nSEEM\nGrounding-DINO\nODISE\nCLIP\nBLIP\nML-Decoder\nAccuracy\nScope\nTag2Text\nRAM\nNo Recognition\nFigure 1. SAM excels in providing strong localization capabilities,\nwhile it falls short when it comes to recognition tasks. In contrast,\nRAM exhibits exceptional recognition abilities, surpassing exist-\ning models in terms of both accuracy and scope.\nlabels, which is another foundational task on par with lo-\ncalization. Multi-label image recognition, also known as\nimage tagging, aims to provide semantic labels by recog-\nnizing multiple labels of a given image.\nImage tagging\nis a significant and practical computer vision task, as im-\nages inherently contain multiple labels encompassing ob-\njects, scenes, attributes, and actions. Regrettably, existing\nmodels in multi-label classification, detection, segmenta-\ntion, and vision-language approaches have exhibited defi-\nciency in tagging, characterized by limited scopes or poor\naccuracy, as illustrated in Figure 1.\nTwo core components impede the progress of image tag-\nging. 1) The difficulty lies in collecting large-scale high-\nquality data. Specifically, there is a lack of a universal and\nunified label system and an efficient data annotation engine,\ncapable of semi-automatic or even automatic annotation of\nlarge-scale images with a vast number of categories.\n2)\nThere is a lack of efficient and flexible model design that can\nleverage large-scale weakly-supervised data to construct an\nopen-vocabulary and powerful model.\nTo address these key bottlenecks, this paper introduces\narXiv:2306.03514v3  [cs.CV]  9 Jun 2023\nliving room, dog, blanket, carpet, couch, desk, furniture, pillow, plant, \nsit, wood floor, lamp\nliving room, dog, sit on, blanket, couch, plant, modern\nMissing: lamp, carpet\nliving room, lamp, houseplant, cushion, throw pillow, picture frame\nBad: property, design, throw\nMissing: dog, couch, carpet, blanket\nliving room, dog, sit, couch\nMissing: lamp, blanket, carpet\ncouch, picture frame,lamp, houseplant, wood floor, flowerpot, carpet\nBad: event, property, television\nMissing: living room, dog, blanket\nRAM\nChristmas market, Christmas tree, stall, market square, snow, \npeople, stroll, town, building\nChristmas market, Christmas tree, snow, town, people\nMissing: building\nChristmas decoration, town square, market, snow, building\nBad: human hair, human head, mixed-use\nChristmas market, winter, town, people\nMissing: Christmas tree, snow, building\nPerson, Building\nMissing: Christmas tree, snow, market\nTag2Text\nML-Decoder\nGoogle\nTagging API\nBLIP\nFigure 2. The comparison of recognition capability among tagging models. RAM recognize more valuable tags than other models without\nmissing important part. ML-Decoder and Google tagging API tend to output redundant tags (e.g., \u201chuman head\u201d) or less relevant tags\n(e.g., \u201cproperty\u201d) tags. BLIP\u2019s tag recall is limited as it relies on caption generation. Note: borderline tags are not listed here.\nthe Recognize Anything Model (RAM), a strong foundation\nmodel for image tagging. RAM overcomes the challenges\nrelated to data, including label system, dataset and data en-\ngine, as well as the limitations in model design.\nLabel System: We begin by establishing a universal and\nunified label system. We incorporate categories from popu-\nlar academic datasets (classification, detection, and segmen-\ntation) as well as commercial tagging products (Google,\nMicrosoft, Apple). Our label system is obtained by merg-\ning all the public tags with the common tags from texts, thus\ncovering most of common labels with a moderate amount of\n6,449. The remaining open-vocabulary labels can be iden-\ntified through open-set recognition.\nDataset: How to automatically annotate large-scale im-\nages with the label system is another challenge [30]. Draw-\ning inspiration from CLIP [22] and ALIGN [11], which\nleverage publicly available image-text pairs at scale to train\npowerful visual models, we adopt similar datasets for im-\nage tagging. To utilize these large-scale image-text data\nfor tagging, following [9, 10], we parse the texts and ob-\ntain the image tags through automatic text semantic pars-\ning. This process allows us to obtain a diverse collection of\nannotation-free image tags in accordance with image-text\npairs.\nData Engine: However, the image-text pairs from the\nweb are inherently noisy, often containing missing or incor-\nrect labels. To enhance the quality of annotations, we de-\nsign a tagging data engine. In addressing missing label, we\nleverage existing models to generate additional tags. With\nregards to incorrect labels, we first localize specific regions\ncorresponding to different tags within the image. Subse-\nquently, we employ region clustering techniques to identify\nand eliminate outliers within the same class. Furthermore,\nwe filter out tags that exhibit contrary predictions between\nwhole images and their corresponding regions, ensuring a\ncleaner and more accurate annotation.\nModel: Tag2Text [10] has demonstrated superior image\ntagging capabilities by the integration of image tagging and\ncaption, employing a lightweight recognition decoder [18]\nin conjunction with the original image encoder. However,\nthe effectiveness of Tag2Text is limited to recognizing fixed\nand predefined categories. In contrast, RAM enable gener-\nalization to previously unseen categories by incorporating\nsemantic information into label queries. This model design\n2\nParse\n(Offline)\nTagging\nImage\nImage-Tag \nInteraction Encoder\nImage-Tag\nRecognition Decoder\nA cat laying in a suitcase\nnext to a pillow.\ncat, lay, suitcase, pillow\nEmbedding\n(Offline)\nImage-Tag-Text\nGeneration Decoder \nCross\nAttention\nTag\nGeneration\nText\nImage\nEncoder\nCLIP Text\nEncoder\nTextual \nLabel Queries\ncat, lay, suitcase, \npillow, dog, person, \u2026\nTag List\nPrompt\nFigure 3. Illustration of RAM\u2019s model architecture. Large-scale image tags are obtained from image-text pairs through automatic text\nsemantic parsing. With image-tag-text triplets, RAM unifies the captioning and tagging tasks. Furthermore, RAM introduces an off-\nthe-shelf text encoder to encoder tags into textual label queries with semantically-rich context, empowering the generalization to unseen\ncategories in training stage.\nallows RAM to empower the recognition capabilities of any\nvisual dataset, underlining its potential for diverse applica-\ntions.\nBenefitting from the large-scale, high-quality image-tag-\ntext data and the synergistic integration of tagging with cap-\ntion, we develop a strong recognize anything model (RAM).\nRAM represents a new paradigm for image tagging, demon-\nstrating that a general model trained on noisy, annotation-\nfree data can outperform fully supervised models. The ad-\nvantages of RAM are summarized as follows:\n\u2022 Strong and general. RAM exhibits exceptional image\ntagging capabilities with powerful zero-shot general-\nization as illustrated in Figure 2;\n\u2022 Reproducible and affordable. RAM requires Low re-\nproduction cost with open-source and annotation-free\ndataset. Moreover, the strongest version of RAM only\nrequires 3-days 8 A100 GPUs training;\n\u2022 Flexible and versatile. RAM offers remarkable flexi-\nbility, catering to various application scenarios. By se-\nlecting specific classes, RAM can be directly deployed\nto address specific tagging needs. Furthermore, when\ncombined with localization model (Grounding DINO\nand SAM), RAM forms a strong and general pipeline\nfor visual semantic analysis.\n2. Recognize Anything Model\n2.1. Model Architecture\nAs illustrated in Figure 3, we extract image tags through\ntext semantic parsing, providing a large-scale of tags with-\nout expensive manual annotations. The overall architecture\nof RAM is similar to that of Tag2Text[10], which consists of\nthree key modules: an image encoder for feature extraction,\nfollowing a image-tag recognition decoder [18] for tagging,\nand a text generation encoder-decoder for captioning. The\nimage features interact with tags by the cross-attention lay-\ners in the image-tag interaction encoder and recognition de-\ncoder.\nIn the training stage, the recognition head learns\nto predict the tags parsed from text, while in the inference\nstage, it serves as a image-to-tags bridge by predicting tags\nwhich provide a more explicit semantic guidance to image\ncaptioning.\nCompared with Tag2Text [10], RAM\u2019s core advance-\nment in model design is the introduction of open-vocabulary\nrecognition.\nTag2Text can only recognize the categories\nthat it has seen during training, while RAM can recognize\nany category.\n2.2. Open-Vocabulary Recognition\nTextual Label Queries. Inspired by [23, 28], the pivotal\nenhancement lies in the incorporation of semantic infor-\nmation into the label queries of the recognition decoder,\nwhich facilitates generalization to previously unseen cate-\ngories in training stage. To achieve this, we utilize an off-\nthe-shelf text encoder to encode the individual tags from the\ntag list, consequently providing textual label queries with\nsemantically-rich context. In contrast, the label queries em-\nployed in the original recognition decode [10, 18] are ran-\ndomly learnable embeddings, lacking the semantic relation-\nship with unseen categories, thus are confined to predefined\nseen categories.\nImplementation Details. We adopt Swin-transformer [19]\nas the image encoder, as it demonstrated better performance\n3\nTop-10k frequent\n(10,000)\nCOCO \n(80)\nImageNet\n(1,000)\nCOCO\n(80)\nCOCO\n(133)\nApple\n(1,301)\nCIFAR100\n(100)\nOpenImages V6\n(19,982)\nNUS-WIDE\n(81)\nObjects365\n(365)\nOpenImages V6 \n(603)\nVOC\n(20)\nADE20k\n(150)\nLVIS\n(1,212)\nMicrosoft\n(3,464)\nGoogle\n(6,792)\nV-COCO\n(29)\nHICO\n(117)\nOpenImages V6\nTag2Text\nRAM-seen\nClassification\nTagging Products\nHOI\nSegmentation\nDetection\nRAM\nFigure 4. Recognition Scopes of different tagging models. Tag2Text recognizes 3,400+ fixed tags. RAM upgrades the number to 6,400+,\ncovering more valuable categories than OpenImages V6. With open-set capability, RAM is feasible to recognize any common category.\nthan naive ViT in both vision-language [10] and tagging do-\nmains [18]. The encoder-decoder used for text generation\nare 12-layer transformers, and the tag recognition decoder\nis a 2-layer transformer. We utilize the off-the-shelf text en-\ncoder from CLIP [22] and perform prompt ensembling [22]\nto obtain textual label queries.\nWe also adopt the CLIP\nimage encoder to distill image feature, which further im-\nproves the model\u2019s recognition ability for unseen categories\nvia image-text feature alignment.\n2.3. Model Efficiency\nTraining Phase. RAM is pretrained on large-scale datasets\nwith a resolution of 224 and fine-tuning at a resolution of\n384 using small and high-quality datasets. Empirical evi-\ndence suggests that RAM converges rapidly, often with con-\nvergence achieved after a minimal number of epochs (typ-\nically less than 5 epochs). This accelerated convergence\nenhances the reproducibility of RAM with limited compu-\ntational resources. To illustrate, the version of RAM pre-\ntrained on 4 millions necessitate 1-day of computation, and\nthe strongest version of RAM pretrained on 14 million im-\nages necessitate a mere 3-days of computation on 8 A100\nGPUs.\nInference Phase. The lightweight image-tag recognition\ndecoder effectively ensures the inference efficiency of RAM\non image tagging.\nFurthermore, we eliminate the self-\nattention layers from the recognition decoder, which not\nonly further improves efficiency but also circumvents po-\ntential interference between label queries. Consequently,\ninstead of fixed categories and quantities, RAM allows cus-\ntomization of label queries for any category and quantity\nwhich want to automatically recognize, enhancing its util-\nity across various visual tasks and datasets.\n3. Data\n3.1. Label System\nThis work adopts three guiding principles for the formu-\nlation of the label system: 1) Tags that frequently appear in\nimage-text pairs are more valuable due to their representa-\ntional significance in image description. 2) A variety of do-\nmains and contexts should be represented in the tags. Our\nconception of a tag includes objects, scenes, attributes, and\nactions from a range of sources, which aids model gener-\nalization to complex, unseen scenarios. 3) The quantity of\ntags needs to be moderate. Excessive tag numbers can incur\nheavy annotation costs.\nInitially, we parse 14 million sentences from our pre-\ntraining datasets into tags by utilizing a SceneGraph-\nParser [25] with minor modifications. We then hand-pick\ntags from the top-10k most frequently occurring tags. Our\nselection intentionally covers tags from numerous popu-\nlar datasets for classification, detection, and segmentation,\nas illustrated in Figure 4. While most are fully covered,\n4\nexceptions include ImageNet and OpenImages V6, due to\ntheir unusual tag presence. Additionally, we partially cover\ntags from leading tagging products, which were obtained\nvia public APIs [2, 3, 1] using open-source images. Conse-\nquently, RAM can recognize up to 6449 fixed tags, which\nis substantially more than Tag2Text [10], and includes a\nhigher proportion of valuable tags. To reduce redundancy,\nwe collected synonyms via various methodologies includ-\ning manual checks, referring to WordNet [7], translating\nand merging tags, etc. Tags within the same synonym group\nare assigned the same tag ID, resulting in 4585 tag IDs in\nthe label system.\n3.2. Datasets\nSimilar to BLIP [15] and Tag2Text [10], we pre-\ntrain our model on widely-used open-source datasets.\n4\nmillion (4M) image and 14 million (14M) image set-\ntings are adopted. The 4M setting includes two human-\nannotated datasets, COCO [16] (113K images, 557K cap-\ntions) and Visual Genome [13] (101K images, 822K cap-\ntions), along with two large-scale web-based datasets, Con-\nceptual Captions [6] (3M images, 3M captions) and SBU\nCaptions [21] (849K images, 849K captions). The 14M set-\nting builds upon the 4M, with the addition of Conceptual\n12M [6] (10M images, 10M captions).\n3.3. Data Engine\nGiven the predominantly open-source nature of our train-\ning datasets, which are largely crawled from the Internet, we\nencounter a non-negligible amount of missing and incorrect\nlabels. To mitigate this, we design an automatic data engine\nto generate additional tags and clean erroneous ones.\nGeneration. Our initial step involves training a baseline\nmodel using the captions and tags parsed from these cap-\ntions, similar to the approach used in Tag2Text [10]. We\nthen leverage this baseline model to supplement both the\ncaptions and tags, utilizing its generative and tagging capa-\nbilities, respectively. The original captions and tags, in con-\njunction with the generated captions, corresponding parsed\ntags, and generated tags, are merged to form a temporary\ndataset. This step significantly expands the number of tags\nin the 4M image dataset from 12 million to 39.8 million.\nCleaning. To address the issue of incorrect tags, we initially\nemploy Grounding-Dino [29] to identify and crop regions\ncorresponding to a specific category within all images. Sub-\nsequently, we cluster the regions from this category based\non K-Means++ [4] and eliminate the tags associated with\nthe outlier 10%. Simultaneously, we also remove tags with-\nout the prediction of this specific category using the baseline\nmodel. The motivation is that the precision of tagging mod-\nels can be improved by predicting regions rather than whole\nimages.\n4. Experiment\n4.1. Experimental Setting\nTest Benchmarks. We conducted a comprehensive evalu-\nation of the models on various popular benchmark datasets\nacross different computer vision tasks, including classifica-\ntion, detection, and segmentation, as summarized in Table 1\nFor classification, we adopt the OpenImages V6 [14], which\ncontains 9605 categories. However, due to issues of miss-\ning labels and incorrect annotations within the OpenImages\ndataset, we curated two high-quality subsets: OpenImages-\ncommon, comprising 214 well-annotated common cate-\ngories, and OpenImages-rare, consisting of 200 categories\nnot included in our label system for open-set experiments.\nAdditionally, to facilitate better zero-shot evaluation, we\nemployed an internal test set known as OPPO-common,\nwhich exhibits high annotation quality.\nFor detection and segmentation datasets, we selected\nthe widely recognized COCO [16] and ADE20k [31, 32]\ndatasets. In these datasets, we focused solely on seman-\ntic labels as image-level tagging ground-truth, disregard-\ning bounding boxes and masks. It is important to note that\nADE20k contains plenty of very small ground-truth anno-\ntations and ambiguous categories that deviate from main-\nstream concepts, e.g., \u201cbuffet\u201d. Thus, we created a subset\nof ADE20k called ADE20k-clean by removing a few small\ntargets and ambiguous categories.\nEvaluation Metrics.\nTo assess the performance of the\nmodels, we employed various evaluation metrics.\nMean\nAverage Precision (mAP) was used for reporting results in\nablation experiments and comparisons with other classifica-\ntion models. For models where mAP was not available, we\nutilized Precision/Recall metrics and manually adjusted the\nthreshold of different models to ensure comparability across\nevaluations.\nTable 1. Details of test benchmarks.\nType\nDataset\n#Category\n#Image\nCls.\nOPPO-common\n200\n44,606\nOpenImages-common [14]\n214\n57,224\nOpenImages-rare [14]\n200\n21,991\nDet.\nCOCO-80 [16]\n80\n5,000\nSeg.\nCOCO-133 [16]\n133\n5,000\nADE20k [31, 32]\n150\n2,000\nADE20k-clean [31, 32]\n143\n2,000\n4.2. Comparison with SOTA Models\nComparison with Multi-Label Classification Models.\nWe compare RAM with state-of-the-art (SOTA) models\nin multi-label classification, as show in Table 2. Gener-\nally, a generalist model typically lacks expertise in spe-\ncific domains, whereas an expert model struggles to gen-\n5\nTable 2. Comparison with classification models in mAP. Cells marked with \u2717 means unable to evaluate on such setting. Cell background\ncolor: Green means fully supervised learning; Blue means zero-shot performance; Yellow denotes that the model has seen the cor-\nresponding training images, but not the annotations. Notably, RAM\u2019s zero-shot generalization to OpenImages-common is superior to\nML-Decoder\u2019s full supervision. RAM can also recognize categories in OpenImages-rare, even though it has not seen them during training.\nMethods\nTags\u2021\nMulti-label Classification\nDetection\nSegmentation\nOPPO\nOpenImages\nOpenImages\nCOCO-80\nCOCO-133\nADE20k\nADE20k\n-common\n-common\n-rare\n-clean\nML-Decoder [23]\n33.9M\n82.4\u2020\n85.8\n79.5\n72.8\u2020\n\u2717\n\u2717\n\u2717\nMKT [8]\n0.6M\n78.2\n77.8\n63.5\n62.9\n51.0\n37.1\n38.4\nTag2Text-4M [10]\n11.4M\n83.0\n82.9\n\u2717\n78.3\u2020\n66.9\u2020\n\u2717\n\u2717\nTag2Text-14M [10]\n33.6M\n85.4\n83.4\n\u2717\n78.2\u2020\n67.1\u2020\n\u2717\n\u2717\nRAM-4M\n39.3M\n85.6\n86.0\n66.7\n79.0\n68.3\n51.5\n53.2\nRAM-14M\n119.9M\n86.9\n86.5\n69.2\n80.6\n69.4\n55.4\n56.9\n\u2020 A few categories that are not supported by the model are excluded when calculating mAP.\n\u2021 The total number of common tags that co-occur in the training set and the top-10k parsed tags.\nTable 3. Comparison with detection, segmentation and vision-language models in Precision/Recall. Cells marked with \u2731 means poor\nperformance in large-sized categories, or long inference time due to the high image resolution, e.g., 1024 for ODISE. Notably, RAM\noutperforms CLIP and BLIP with large margins in common categories.\nMethods\nBackbone\nMulti-label Classification\nDetection\nSegmentation\nOPPO\nOpenImages\nOpenImages\nCOCO-80\nCOCO-133\nADE20k\nADE20k\n-common\n-common\n-rare\n-clean\nGrounding-DINO [17]\nSwin-B\n\u2731\n\u2731\n\u2731\n83.1 / 86.9\n66.4 / 48.3\n34.3 / 24.7\n35.6 / 26.0\nODISE [26]\nDiffusion-v3\n\u2731\n\u2731\n\u2731\n78.5 / 85.9\n71.1 / 80.2\n47.4 / 48.0\n48.2 / 50.3\nSEEM [33]\nFocalNet-L\n\u2717\n\u2717\n\u2717\n75.7 / 67.8\n71.8 / 61.0\n\u2717\n\u2717\nCLIP-400M [22]\nViT-B\n76.6 / 54.1\n77.9 / 52.9\n67.5 / 46.5\n64.0 / 38.7\n47.8 / 36.4\n30.3 / 5.3\n31.0 / 5.5\nBLIP-129M [15]\nViT-B\n76.7 / 57.5\n78.6 / 55.1\n65.2 / 46.5\n67.0 / 39.0\n53.8 / 34.6\n28.5 / 8.8\n29.1 / 9.3\nTag2Text-4M [10]\nSwin-B\n76.6 / 74.8\n75.9 / 71.9\n\u2717\n80.5 / 66.1\u2020\n71.2 / 54.0\u2020\n\u2717\n\u2717\nTag2Text-14M [10]\nSwin-B\n77.9 / 79.4\n76.4 / 73.3\n\u2717\n80.1 / 64.5\u2020\n71.2 / 53.2\u2020\n\u2717\n\u2717\nRAM-4M\nSwin-B\n78.4 / 75.2\n79.2 / 73.7\n53.9 / 48.4\n81.8 / 66.1\n74.3 / 54.0\n47.0 / 47.6\n47.8 / 50.3\nRAM-14M\nSwin-L\n78.8 / 79.4\n80.3 / 75.7\n53.8 / 54.3\n82.9 / 66.4\n74.3 / 54.1\n53.2 / 50.0\n53.7 / 52.2\n\u2020 A few categories that are not supported by the model are excluded when calculating precision and recall.\neralize beyond its specialized field.\nSpecifically, the su-\npervised expert model ML-Decoder [23] excels in its des-\nignated domain of expertise, OpenImages, but faces chal-\nlenges in generalizing to other domains and unseen cate-\ngories. MKT [8] is a generalist model in tagging by trans-\nferring the knowledge from CLIP, fails to achieve satisfac-\ntory accuracy across all domains. Tag2Text [10] is powerful\nat zero-shot tagging, but it lacks the ability to handle open-\nset scenarios.\nRAM exhibits impressive tagging abilities, showcasing\nimpressive accuracy and broad coverage. Particularly note-\nworthy is the performance of RAM-4M, which surpasses\nML-Decoder on the OpenImages-common dataset. While\nML-Decoder relies on 9 million annotated images from\nOpenImages, our RAM-4M achieves higher accuracy with\na training set of 4 million annotation-free image-text data.\nThis improvement is attributed to the utilization of 39.3\nmillion common tags derived from the 4 million images,\noutperforming ML-Decoder trained with only 33.9 million\ncommon tags from 9 million images. Moreover, RAM can\nrecognize any common category by leveraging a vast range\nof 6,400+ seen common categories, coupled with its open-\nvocabulary ability.\nComparison with Detection and Segmentation Models.\nThe comparison in Table 3 reveals that supervised detection\nand segmentation models excel in specific domains such\nas COCO datasets, which encompass a limited number of\ncategories. However, these models face challenges when it\ncomes to recognizing a larger number of categories. On the\none hand, they take much more computational overheads\nas they requires more complex network and larger input im-\nage sizes for extra localization task. Especially, ODISE [26]\ntakes long inference time due to its adoption of the diffusion\nmodel and large input image resolution. On the other hand,\nthe scalability of training data for detection and segmenta-\ntion is limited, resulting in poor generalization performance\nfor these models. Although Grounding-DINO [17] serve\nas a generalist model, it struggles to achieve satisfactory\n6\nTable 4. Ablation study of RAM model based on Tag2Text baselines.\n\u201cSeen Categories\u201d refers to the number of training cate-\ngories.\u201cCaptioning\u201d refers to the joint training of captioning and tagging tasks. \u201cTextual Queries\u201d refers to using a text encoder to\ngenerate label queries possessing semantic information. \u201cDistillation\u201d refers to image feature distillation using CLIP\u2019s image encoder.\nCase\nSeen\nCategories\nCaptioning\nTextual\nQueries\nDistillation\nOPPO\nOpenImages\n-common\n-common\n-rare\nTag2Text\n3,429\n80.60\n83.52\n\u2717\n3,429\n\u2713\n81.37\n84.04\n\u2717\n(a)\n3,429\n\u2713\n\u2713\n81.22\n84.09\n60.99\n(b)\n3,429\n\u2713\n\u2713\n\u2713\n81.70\n84.16\n61.88\n(c)\n6,449\n\u2713\n\u2713\n\u2713\n80.27\n83.09\n63.54\nTable 5. Ablation study of data engine. \u201cParsing\u201d means the training tags parsed from the captions. \u201cGeneration\u201d means the supplemen-\ntation of captions and tags. \u201cCleaning\u201d refers to data cleaning. \u201cFine-tuning\u201d refers to fine-tuning the pre-trained model with COCO.\nBackbone\nPre-train\nParsing\nGeneration\nCleaning\nFine-tuning\nOPPO\nOpenImages\n#Images\n#Tags\n-common\n-common\n-rare\nSwin-Base\n4M\n12.0M\n\u2713\n80.27\n83.09\n63.54\n4M\n41.7M\n\u2713\n\u2713\n82.50\n84.27\n67.17\n4M\n39.8M\n\u2713\n\u2713\n\u2713\n82.83\n84.94\n66.88\n4M\n39.8M\n\u2713\n\u2713\n\u2713\n\u2713\n85.56\n86.01\n66.74\n14M\n121.5M\n\u2713\n\u2713\n\u2713\n83.52\n85.39\n68.54\n14M\n121.5M\n\u2713\n\u2713\n\u2713\n\u2713\n86.47\n86.50\n68.79\nSwin-Large\n14M\n121.5M\n\u2713\n\u2713\n\u2713\n83.26\n84.94\n68.60\n14M\n121.5M\n\u2713\n\u2713\n\u2713\n\u2713\n86.92\n86.46\n69.21\nperformance for large-sized categories. In contrast, RAM\ndemonstrates impressive open-set ability, surpassing exist-\ning detection and segmentation models. RAM showcases\nits capability to generalize across a broader range of cate-\ngories, providing a robust solution for the challenges faced\nby conventional detection and segmentation models.\nCompared with Vision-Language Models.\nDespite\nthe open-set recognition capabilities of CLIP [22] and\nBLIP [15], these models suffer from subpar accuracy. Fur-\nthermore, their interpretability is limited, as they rely on\ncosine similarity computations of dense embeddings for\nimage-text pairs. In contrast, RAM exhibits superior per-\nformance, surpassing CLIP and BLIP by a significant mar-\ngin, with accuracy increases of over 20% observed across\nalmost all datasets. However, it is worth noting that RAM\nperforms slightly worse than CLIP and BLIP in the case of\nOpenImages-rare dataset. We attribute this discrepancy to\nthe smaller training dataset utilized for RAM and the rela-\ntively less emphasis placed on rare classes during training.\n4.3. Model Ablation Study\nIn Table 4, we study the impact of various model im-\nprovements to RAM based on Tag2Text [10] and make\nthe following key observations.\n1) The training integra-\ntion of captioning and tagging can promote the tagging abil-\nity. 2) The open-set recognition capability can be achieved\nthrough textual queries by CLIP [22], but has little impact\non the seen categories in training. 3) The expansion of the\nlabel system introduces minimal impact on existing cate-\ngories, which can be attributed to the additional categories\nincreases the difficulty of model training. However, this\nexpansion concurrently enhances the model\u2019s coverage and\nenhances the open-set ability of unseen categories.\n4.4. Data Engine Ablation Study\nWe present an ablation study of the data engine in Table\n5. The findings are summarized as follows: 1) Adding more\ntags from 12.0M to 41.7M significantly improves model\nperformance across all test sets, indicating the severe miss-\ning label problem in original datasets. 2) Further cleaning\nthe tags of some categories results in a slight increase in per-\nformance on the OPPO-common and OpenImages-common\ntest sets. Limited by the inference speed of Grounding-\nDino, we only conduct cleaning process for 534 categories.\n3) Scaling up the training images from 4M to 14M brings\nremarkable improvements across all test sets. 4) Employ-\ning a larger backbone network leads to a slight improve-\nment on OpenImages-rare and even slightly inferior per-\nformance on common categories.\nWe attribute this phe-\nnomenon to our insufficient resources available for conduct-\ning hyper-parameter search. 5) Fine-tuning with tags parsed\nfrom the COCO Caption dataset [16] demonstrates remark-\nable increases in performance on the OPPO-common and\n7\nOpenImages-common test sets. The COCO Caption dataset\nprovides five descriptive sentences for each image, offering\na comprehensive description that approximates a complete\nset of tag labels.\n5. Conclusion\nWe present the Recognize Anything Model (RAM), a\nstrong foundation model designed for image tagging, which\nheralds a novel paradigm in this field. RAM demonstrate\nthe zero-shot ability to recognize any category with high\naccuracy, surpassing the performance of both fully super-\nvised models and existing generalist approaches like CLIP\nand BLIP. RAM represents a considerable advancement for\nlarge-scale models in the field of computer vision, holding\nthe potential to empower the recognition capabilities of any\nvisual tasks or datasets.\nThere still exists room for further refinement of RAM.\nFor example, scaling up the training dataset beyond 14 mil-\nlion images to better cover diverse domains, multiple rounds\nof data engine, increasing the backbone parameters to en-\nhance the model capacity.\nLimitations.\nSimilar to CLIP, the current version of\nRAM efficiently recognizes common objects and scenes,\nyet struggles with abstract tasks like object counting. More-\nover, zero-shot RAM\u2019s performance lags behind task-\nspecific models in fine-grained classifications, such as dif-\nferentiating between car models or identifying specific\nflower or bird species. It is also noteworthy that RAM is\ntrained on open-source datasets and could potentially reflect\ndataset biases.\nReferences\n[1] Apple Developer. https://developer.apple.com/\ndocumentation/vision.\n[2] Google Cloud vision API. https://cloud.google.\ncom/vision.\n[3] Microsoft\nAzure\ncognitive\nservice.\nhttps:\n//azure.microsoft.com/zh-cn/products/\ncognitive-services/vision-services/.\n[4] David Arthur and Sergei Vassilvitskii. K-means++ the ad-\nvantages of careful seeding.\nIn Proceedings of the eigh-\nteenth annual ACM-SIAM symposium on Discrete algo-\nrithms, pages 1027\u20131035, 2007.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020.\n[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3558\u20133568, 2021.\n[7] Christiane Fellbaum.\nWordNet:\nAn Electronic Lexical\nDatabase. Bradford Books, 1998.\n[8] Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Bo Ren, and\nShu-Tao Xia. Open-vocabulary multi-label classification via\nmulti-modal knowledge transfer.\nCoRR, abs/2207.01887,\n2022.\n[9] Xinyu Huang, Youcai Zhang, Ying Cheng, Weiwei Tian,\nRuiwei Zhao, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong\nGuo, and Xiaobo Zhang.\nIdea: Increasing text diversity\nvia online multi-label recognition for vision-language pre-\ntraining. In Proceedings of the 30th ACM International Con-\nference on Multimedia, pages 4573\u20134583, 2022.\n[10] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui\nFeng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei\nZhang. Tag2text: Guiding vision-language model via image\ntagging. arXiv preprint arXiv:2303.05657, 2023.\n[11] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nConference on Machine Learning, pages 4904\u20134916. PMLR,\n2021.\n[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n[13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32\u201373, 2017.\n[14] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,\nand Vittorio Ferrari. The open images dataset v4: Unified\nimage classification, object detection, and visual relationship\ndetection at scale. IJCV, 2020.\n[15] Junnan Li,\nDongxu Li,\nCaiming Xiong,\nand Steven\nHoi.\nBLIP: Bootstrapping Language-Image Pre-training\nfor Unified Vision-Language Understanding and Generation.\narXiv:2201.12086 [cs], Feb. 2022. arXiv: 2201.12086.\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014.\n[17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023.\n[18] Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, and Jun Zhu.\nQuery2label: A simple transformer way to multi-label clas-\nsification. arXiv preprint arXiv:2107.10834, 2021.\n[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\n8\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021.\n[20] OpenAI. Gpt-4 technical report, 2023.\n[21] Vicente Ordonez,\nGirish Kulkarni,\nand Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. Advances in neural information processing sys-\ntems, 24, 2011.\n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\n[23] Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben\nBaruch, and Asaf Noy. Ml-decoder: Scalable and versatile\nclassification head. In IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, WACV 2023, Waikoloa, HI,\nUSA, January 2-7, 2023, pages 32\u201341. IEEE, 2023.\n[24] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,\nZhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,\nHongsheng Li, Xiaogang Wang, and Yu Qiao.\nInternim-\nage: Exploring large-scale vision foundation models with\ndeformable convolutions. CoRR, abs/2211.05778, 2022.\n[25] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei\nLi, Weiwei Sun, and Wei-Ying Ma. Unified visual-semantic\nembeddings: Bridging vision and language with structured\nmeaning representations. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6609\u20136618, 2019.\n[26] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xi-\naolong Wang, and Shalini De Mello.\nOpen-Vocabulary\nPanoptic Segmentation with Text-to-Image Diffusion Mod-\nels. arXiv preprint arXiv:2303.04803, 2023.\n[27] Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.\nFocal modulation networks. In NeurIPS, 2022.\n[28] Yuhang Zang, W ei Li, Kaiyang Zhou, Chen Huang, and\nChen Change Loy. Open-vocabulary detr with conditional\nmatching. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceed-\nings, Part IX, pages 106\u2013122. Springer, 2022.\n[29] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M. Ni, and Heung-Yeung Shum. DINO: DETR\nwith improved denoising anchor boxes for end-to-end object\ndetection. CoRR, abs/2203.03605, 2022.\n[30] Youcai Zhang, Yuhao Cheng, Xinyu Huang, Fei Wen, Rui\nFeng, Yaqian Li, and Yandong Guo. Simple and robust loss\ndesign for multi-label learning with missing labels. arXiv\npreprint arXiv:2112.07368, 2021.\n[31] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017.\n[32] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 127:302\u2013321, 2019.\n[33] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. CoRR, abs/2304.06718, 2023.\n9\n"
  },
  {
    "title": "Emergent Correspondence from Image Diffusion",
    "link": "https://arxiv.org/pdf/2306.03881.pdf",
    "upvote": "5",
    "text": "Emergent Correspondence from Image Diffusion\nLuming Tang\u2217\nMenglin Jia\u2217\nQianqian Wang\u2217\nCheng Perng Phoo\nBharath Hariharan\nCornell University\nAbstract\nFinding correspondences between images is a fundamental problem in computer\nvision. In this paper, we show that correspondence emerges in image diffusion\nmodels without any explicit supervision. We propose a simple strategy to extract this\nimplicit knowledge out of diffusion networks as image features, namely DIffusion\nFeaTures (DIFT), and use them to establish correspondences between real images.\nWithout any additional fine-tuning or supervision on the task-specific data or\nannotations, DIFT is able to outperform both weakly-supervised methods and\ncompetitive off-the-shelf features in identifying semantic, geometric, and temporal\ncorrespondences. Particularly for semantic correspondence, DIFT from Stable\nDiffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points\nrespectively on the challenging SPair-71k benchmark. It even outperforms the\nstate-of-the-art supervised methods on 9 out of 18 categories while remaining on\npar for the overall performance. Project page: https://diffusionfeatures.\ngithub.io.\n1\nIntroduction\nDrawing correspondences between images is a critical primitive in 3D reconstruction [73], object\ntracking [22, 90], video segmentation [92], image and video editing [102, 58, 98]. This problem of\ndrawing correspondence is easy for humans: we can match object parts not only across different\nviewpoints, articulations and lighting changes, but even across drastically different categories (e.g.,\nbetween cats and horses) or different modalities (e.g., between photos and cartoons). Yet, we rarely if\never get explicit correspondence labels for training. The question is, can computer vision systems\nsimilarly learn accurate correspondences without any labeled data at all?\nThere is indeed some evidence that contrastive self-supervised learning techniques produce good\ncorrespondences as a side product of learning on unlabeled data [10, 28]. However, in this paper,\nwe look to a new class of self-supervised models that has been attracting attention: diffusion-based\ngenerative models [32, 79]. While diffusion models are primarily models for image synthesis, a\nkey observation is that these models produce good results for image-to-image translation [53, 85]\nand image editing [8, 80]. For instance, they can convert a dog to a cat without changing its pose\nor context [61]. It would appear that to perform such editing, the model must implicitly reason\nabout correspondence between the two categories (e.g., the model needs to know where the dog\u2019s\neye is in order to replace it with the cat\u2019s eye). We therefore ask, do image diffusion models learn\ncorrespondences?\nWe answer the question in the affirmative by construction: we provide a simple way of extracting\ncorrespondences on real images using pre-trained diffusion models. These diffusion models [41] have\nat the core a U-Net [71, 17, 70] that takes noisy images as input and produces clean images as output.\nAs such they already extract features from the input image that can be used for correspondence.\nUnfortunately, the U-Net is trained to de-noise, and so has been trained on noisy images. Our strategy\n*Equal contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.03881v2  [cs.CV]  6 Dec 2023\nSource Point\nDIFT Predicted Target Points\ncross-instance\ncross-category\ncross-domain\nFigure 1: Given a red source point in an image (far left), we would like to develop a model that\nautomatically finds the corresponding point in the images on the right. Without any fine-tuning\nor correspondence supervision, our proposed diffusion features (DIFT) could establish semantic\ncorrespondence across instances, categories and even domains, e.g., from a duck to a penguin, from a\nphoto to an oil-painting. More results are in Figs. 15 and 16 of Appendix E.\nfor handling this issue is simple but effective: we add noise to the input image (thus simulating the\nforward diffusion process) before passing it into the U-Net to extract feature maps. We call these\nfeature maps (and through a slight abuse of notation, our approach) DIffusion FeaTures (DIFT).\nDIFT can then be used to find matching pixel locations in the two images by doing simple nearest\nneighbor lookup using cosine distance. We find the resulting correspondences are surprisingly robust\nand accurate (Fig. 1), even across multiple categories and image modalities.\nWe evaluate DIFT with two different types of diffusion models, on three groups of visual cor-\nrespondence tasks including semantic correspondence, geometric correspondence, and temporal\ncorrespondence. We compare DIFT with other baselines, including task-specific methods, and other\nself-supervised models trained with similar datasets and similar amount of supervision (DINO [10]\nand OpenCLIP [36]). Although simple, DIFT demonstrates strong performance on all tasks without\nany additional fine-tuning or supervision, outperforms both weakly-supervised methods and other\nself-supervised features, and even remains on par with the state-of-the-art supervised methods on\nsemantic correspondence.\n2\nRelated Work\nVisual Correspondence. Establishing visual correspondences between different images is crucial\nfor various computer vision tasks such as Structure-from-Motion / 3D reconstruction [2, 73, 60, 74],\nobject tracking [22, 97], image recognition [63, 81, 9] and segmentation [50, 47, 72, 28]. Traditionally,\ncorrespondences are established using hand-designed features, such as SIFT [51] and SURF [6].\nWith the advent of deep learning, methods that learn to find correspondences in a supervised-learning\nregime have shown promising results [46, 14, 42, 35]. However, these approaches are difficult to scale\ndue to the reliance on ground-truth correspondence annotations. To overcome difficulties in collecting\na large number of image pairs with annotated correspondences, recent works have started looking into\nhow to build visual correspondence models with weak supervision [91] or self-supervision [92, 37].\nMeanwhile, recent works on self-supervised representation learning [10] has yielded strong per-pixel\nfeatures that could be used to identify visual correspondence [84, 3, 10, 28]. In particular, recent\nwork has also found that the internal representation of Generative Adversarial Networks (GAN) [23]\ncould be used for identifying visual correspondence [99, 62, 57] within certain image categories.\nOur work shares similar spirits with these works: we show that diffusion models could generate\nfeatures that are useful for identifying visual correspondence on general images. In addition, we\nshow that features generated at different timesteps and different layers of the de-noising process\n2\nencode different information that could be used for determining correspondences needed for different\ndownstream tasks.\nDiffusion Model [78, 32, 79, 41] is a powerful family of generative models. Ablated Diffusion\nModel [17] first showed that diffusion could surpass GAN\u2019s image generation quality on Ima-\ngeNet [15]. Subsequently, the introduction of classifier-free guidance [33] and latent diffusion\nmodel [70] made it scale up to billions of text-image pairs [75], leading to the popular open-sourced\ntext-to-image diffusion model, i.e., Stable Diffusion. With its superior generation ability, recently\npeople also start investigating the internal representation of diffusion models. For example, previous\nworks [85, 31] found that the intermediate-layer features and attention maps of diffusion models are\ncrucial for controllable generations; other works [5, 94, 101] explored adapting pre-trained diffusion\nmodels for various downstream visual recognition tasks. Different from these works, we are the first\nto directly evaluate the efficacy of features inherent to pre-trained diffusion models on various visual\ncorrespondence tasks.\n3\nProblem Setup\nGiven two images I1, I2 and a pixel location p1 in I1, we are interested in finding its corresponding\npixel location p2 in I2. Relationships between p1 and p2 could be semantic correspondence (i.e.,\npixels of different objects that share similar semantic meanings), geometric correspondence (i.e.,\npixels of the same object captured from different viewpoints), or temporal correspondence (i.e., pixels\nof the same object in a video that may deform over time).\nThe most straightforward approach to obtaining pixel correspondences is to first extract dense image\nfeatures in both images and then match them. Specifically, given a feature map Fi for image Ii, we\ncan extract a feature vector Fi(p) for pixel location p through bilinear interpolation. Then given a\npixel p1 in image I1, we can obtain the corresponding pixel in image I2 as:\np2 = arg min\np\nd(F1(p1), F2(p))\n(1)\nwhere d is a distance metric and we use cosine distance by default in this work.\n4\nDiffusion Features (DIFT)\nIn this section, we first review what diffusion models are and then explain how we extract dense\nfeatures on real images using pre-trained diffusion models.\n4.1\nImage Diffusion Model\nDiffusion models [32, 79] are generative models that aim to transform a Normal distribution to an\narbitrary data distribution. In our case, we use image diffusion models, thus the data distribution and\nthe Gaussian prior are both over the space of 2D images.\nDuring training, Gaussian noise of different magnitudes is added to clean data points to obtain noisy\ndata points. This is typically thought of as a \u201cdiffusion\u201d process, where the starting point of the\ndiffusion x0 is a clean image from the training dataset and xt is a noisy image obtained by \u201cmixing\u201d\nx0 with noise:\nxt = \u221a\u03b1tx0 + (\n\u221a\n1 \u2212 \u03b1t)\u03f5\n(2)\nwhere \u03f5 \u223c N(0, I) is the randomly-sampled noise, and t \u2208 [0, T] indexes \u201ctime\u201d in the diffusion\nprocess with larger time steps involving more noise. The amount of noise is determined by {\u03b1t}T\n1 ,\nwhich is a pre-defined noise schedule. We call this the diffusion forward process.\nA neural network f\u03b8 is trained to take xt and time step t as input and predict the input noise \u03f5. For\nimage generation, f\u03b8 is usually parametrized as a U-Net [71, 17, 70]. Once trained, f\u03b8 can be used\nto \u201creverse\u201d the diffusion process. Starting from pure noise xT sampled from a Normal distribution,\nf\u03b8 can be iteratively used to estimate noise \u03f5 from the noisy data xt and remove this noise to get a\ncleaner data xt\u22121, eventually leading to a sample x0 from the original data distribution. We call this\nthe diffusion backward process.\n3\nSource Point\nPredicted Target Point\nSource Point\nPredicted Target Point\nFigure 2: Given a Stable Diffusion generated image, we extract its intermediate layer activations\nat a certain time step t during its backward process, and use them as the feature map to predict the\ncorresponding points. Although simple, this method produces correct correspondences on generated\nimages already not only within category, but also cross-category, even in cross-domain situations,\ne.g., from a photo to an oil painting.\n4.2\nExtracting Diffusion Features on Real Images\nWe hypothesize that diffusion models learn correspondence implicitly [85, 61] in Sec. 1, but how\ncan we extract this correspondence? Consider first generated images, where we have access to the\ncomplete internal state of the network throughout the entire backward process. Given a generated\nimage from Stable Diffusion [70] , we extract the feature maps of its intermediate layers at a specific\ntime step t during the backward process, which we then utilize to establish correspondences between\ntwo different generated images as described in Sec. 3. As illustrated in Fig. 2, this straightforward\napproach allows us to find correct correspondences between generated images, even when they belong\nto different categories or domains.\nReplicating this approach for real images is challenging because of the fact that the real image itself\ndoes not belong to the training distribution of the U-Net (which was trained on noisy images), and\nwe do not have access to the intermediate noisy images that would have been produced during the\ngeneration of this image. Fortunately, we found a simple approximation using the forward diffusion\nprocess to be effective enough. Specifically, we first add noise of time step t to the real image\n(Eq. (2)) to move it to the xt distribution, and then feed it to network f\u03b8 together with t to extract the\nintermediate layer activations as our DIffusion FeaTures, namely DIFT. As shown in Figs. 1 and 3,\nthis approach yields surprisingly good correspondences for real images.\nMoving forward, a crucial consideration is the selection of the time step t and the network layer from\nwhich we extract features. Intuitively we find that a larger t and an earlier network layer tend to yield\nmore semantically-aware features, while a smaller t and a later layer focus more on low-level details.\nThe optimal choices of t and layer depend on the specific correspondence task at hand, as different\ntasks may require varying trade-offs between semantic and low-level features. For example, semantic\ncorrespondence likely benefits from more semantic-level features, whereas geometric correspondence\nbetween two views of the same instance may perform well with low-level features. We therefore\nuse a 2D grid search to determine these two hyper-parameters for each correspondence task. For a\ncomprehensive list of the hyper-parameter values used in this paper, please refer to Appendix C.\nLastly, to enhance the stability of the representation in the presence of random noise added to the\ninput image, we extract features from multiple noisy versions with different samples of noise, and\naverage them to form the final representation.\n5\nSemantic Correspondence\nIn this section, we investigate how to use DIFT to identify pixels that share similar semantic meanings\nacross images, e.g., the eyes of two different cats in two different images.\n5.1\nModel Variants and Baselines\nWe extract DIFT from two commonly used, open-sourced image diffusion models: Stable Diffusion\n2-1 (SD) [70] and Ablated Diffusion Model (ADM) [17]. SD is trained on the LAION [75] whereas\nADM is trained on ImageNet [15] without labels. We call these two features DIFTsd and DIFTadm\nrespectively.\nTo separate the impact of training data on the performance of DIFT, we also evaluate two other\ncommonly used self-supervised features as baselines that share basically the same training data:\n4\nSource Image\nDIFTadm\nDINO\nOpenCLIP\nDIFTsd\nTarget Image\nFigure 3: Visualization of semantic correspondence prediction on SPair-71k using different features.\nThe leftmost image is the source image with a set of keypoints; the rightmost image contains the\nground-truth correspondence for a target image whereas any images in between contain keypoints\nfound using feature matching with various features. Different colors indicate different keypoints. We\nuse circles to indicate correctly-predicted points under the threshold \u03b1bbox = 0.1 and crosses for\nincorrect matches. DIFT is able to establish correct correspondences under clustered scenes (row 3),\nviewpoint changes (row 2 and 4), and occlusions (row 5). See Fig. 17 in Appendix E for more results.\nOpenCLIP [36] with ViT-H/14 [18] trained on LAION, as well as DINO [10] with ViT-B/8 trained\non ImageNet [15] without labels. Note that for both DIFT and other self-supervised features, we do\nnot fine-tune or re-train the models with any additional data or supervision.\n5.2\nBenchmark Evaluation\nDatasets. We conduct evaluation on three popular benchmarks: SPair-71k [55], PF-WILLOW [27]\nand CUB-200-2011 [89]. SPair-71k is the most challenging semantic correspondence dataset,\ncontaining diverse variations in viewpoint and scale with 12,234 image pairs on 18 categories for\ntesting. PF-Willow is a subset of PASCAL VOC dataset [20] with 900 image pairs for testing. For\nCUB, following [58], we evaluate 14 different splits of CUB (each containing 25 images) and report\nthe average performance across all splits.\nEvaluation Metric. Following prior work, we report the percentage of correct keypoints (PCK).\nThe predicted keypoint is considered to be correct if they lie within \u03b1 \u00b7 max(h, w) pixels from the\nground-truth keypoint for \u03b1 \u2208 [0, 1], where h and w are the height and width of either the image\n(\u03b1img) or the bounding box (\u03b1bbox). To find a suitable time step and layer feature to use for DIFT\nand other self-supervised features, we grid search the hyper-parameters using SPair-71k and use the\nsame hyper-parameter settings for PF-WILLOW and CUB.\nWe observed inconsistencies in PCK measurements across prior literature1. Some works [35, 42, 14]\nuse the total number of correctly-predicted points in the whole dataset (or each category split) divided\n1ScorrSAN [35] and GANgealing [62]\u2019s evaluation code snippets, which calculate PCK per image and PCK per point respectively.\n5\nTable 1: PCK(\u03b1bbox = 0.1) per image on SPair-71k. All the DIFT results have gray background\nfor easy lookups. Methods are grouped into 3 groups: (a) fully supervised with correspondence\nannotations, (b) weakly supervised with in-domain image collections, (c) no supervision. Best\nnumbers in group (a) are bolded. Among groups (b) and (c) taken together, we annotate best\nand second-best results. Without any supervision, both DIFTsd and DIFTadm outperform previous\nweakly-supervised methods and self-supervised techniques by a large margin.\nSup.\nMethod\nSPair-71K Category\nAero\nBike\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDog\nHorse\nMotor\nPerson\nPlant\nSheep\nTrain\nTV\nAll\n(a)\nCATs [14]\n52.0\n34.7\n72.2\n34.3\n49.9\n57.5\n43.6\n66.5\n24.4\n63.2\n56.5\n52.0\n42.6\n41.7\n43.0\n33.6\n72.6\n58.0\n49.9\nMMNet [100]\n55.9\n37.0\n65.0\n35.4\n50.0\n63.9\n45.7\n62.8\n28.7\n65.0\n54.7\n51.6\n38.5\n34.6\n41.7\n36.3\n77.7\n62.5\n50.4\nTransforMatcher [42]\n59.2\n39.3\n73.0\n41.2\n52.5\n66.3\n55.4\n67.1\n26.1\n67.1\n56.6\n53.2\n45.0\n39.9\n42.1\n35.3\n75.2\n68.6\n53.7\nSCorrSAN [35]\n57.1\n40.3\n78.3\n38.1\n51.8\n57.8\n47.1\n67.9\n25.2\n71.3\n63.9\n49.3\n45.3\n49.8\n48.8\n40.3\n77.7\n69.7\n55.3\n(b)\nNCNet [69]\n17.9\n12.2\n32.1\n11.7\n29.0\n19.9\n16.1\n39.2\n9.9\n23.9\n18.8\n15.7\n17.4\n15.9\n14.8\n9.6\n24.2\n31.1\n20.1\nCNNGeo [67]\n23.4\n16.7\n40.2\n14.3\n36.4\n27.7\n26.0\n32.7\n12.7\n27.4\n22.8\n13.7\n20.9\n21.0\n17.5\n10.2\n30.8\n34.1\n20.6\nWeakAlign [68]\n22.2\n17.6\n41.9\n15.1\n38.1\n27.4\n27.2\n31.8\n12.8\n26.8\n22.6\n14.2\n20.0\n22.2\n17.9\n10.4\n32.2\n35.1\n20.9\nA2Net [76]\n22.6\n18.5\n42.0\n16.4\n37.9\n30.8\n26.5\n35.6\n13.3\n29.6\n24.3\n16.0\n21.6\n22.8\n20.5\n13.5\n31.4\n36.5\n22.3\nSFNet [45]\n26.9\n17.2\n45.5\n14.7\n38.0\n22.2\n16.4\n55.3\n13.5\n33.4\n27.5\n17.7\n20.8\n21.1\n16.6\n15.6\n32.2\n35.9\n26.3\nPMD [48]\n26.2\n18.5\n48.6\n15.3\n38.0\n21.7\n17.3\n51.6\n13.7\n34.3\n25.4\n18.0\n20.0\n24.9\n15.7\n16.3\n31.4\n38.1\n26.5\nPSCNet [38]\n28.3\n17.7\n45.1\n15.1\n37.5\n30.1\n27.5\n47.4\n14.6\n32.5\n26.4\n17.7\n24.9\n24.5\n19.9\n16.9\n34.2\n37.9\n27.0\nPWarpC [83]\n37.4\n28.8\n60.8\n22.9\n40.5\n29.4\n22.8\n60.1\n19.5\n37.8\n38.4\n27.9\n32.1\n29.7\n29.2\n20.2\n44.5\n50.0\n35.3\n(c)\nDINO [10]\n43.6\n27.2\n64.9\n24.0\n30.5\n31.4\n28.3\n55.2\n16.8\n40.2\n37.1\n32.9\n29.1\n41.1\n22.0\n26.8\n36.4\n26.9\n33.9\nDIFTadm (ours)\n49.7\n39.2\n77.5\n29.3\n40.9\n36.1\n30.5\n75.5\n23.7\n63.7\n52.8\n49.3\n34.1\n52.3\n39.3\n37.3\n59.6\n45.4\n46.3\nOpenCLIP [36]\n51.7\n31.4\n68.7\n28.4\n31.5\n34.9\n36.1\n56.4\n21.1\n44.5\n41.5\n41.2\n41.2\n51.8\n21.7\n28.6\n46.3\n20.7\n38.4\nDIFTsd (ours)\n61.2\n53.2\n79.5\n31.2\n45.3\n39.8\n33.3\n77.8\n34.7\n70.1\n51.5\n57.2\n50.6\n41.4\n51.9\n46.0\n67.6\n59.5\n52.9\nTable 2: PCK(\u03b1bbox = 0.1) per point of various methods on SPair-71k. The groups and colors\nfollow Tab. 1. \u201cMean\" denotes the PCK averaged over categories. Same as in Tab. 1, without any\nsupervision, both DIFTsd and DIFTadm outperform previous weakly-supervised methods with a\nlarge margin, and also outperform their contrastive-learning counterparts by over 14 points.\nSup.\nMethod\nSPair-71K Category\nAero\nBike\nBird\nBoat\nBottle\nBus\nCar\nCat\nChair\nCow\nDog\nHorse\nMotor\nPerson\nPlant\nSheep\nTrain\nTV\nMean\nAll\n(b)\nNBB [1, 26]\n29.5\n22.7\n61.9\n26.5\n20.6\n25.4\n14.1\n23.7\n14.2\n27.6\n30.0\n29.1\n24.7\n27.4\n19.1\n19.3\n24.4\n22.6\n27.4\n-\nGANgealing [62]\n-\n37.5\n-\n-\n-\n-\n-\n67.0\n-\n-\n23.1\n-\n-\n-\n-\n-\n-\n57.9\n-\n-\nNeuCongeal [58]\n-\n29.1\n-\n-\n-\n-\n-\n53.3\n-\n-\n35.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\nASIC [26]\n57.9\n25.2\n68.1\n24.7\n35.4\n28.4\n30.9\n54.8\n21.6\n45.0\n47.2\n39.9\n26.2\n48.8\n14.5\n24.5\n49.0\n24.6\n36.9\n-\n(c)\nDINO [10]\n45.0\n29.5\n66.3\n22.8\n32.1\n36.3\n31.7\n54.8\n18.7\n43.1\n39.2\n34.9\n31.0\n44.3\n23.1\n29.4\n38.4\n27.1\n36.0\n36.7\nDIFTadm (ours)\n51.6\n40.4\n77.6\n30.7\n43.0\n47.2\n42.1\n74.9\n26.6\n67.3\n55.8\n52.7\n36.0\n55.9\n46.3\n45.7\n62.7\n47.4\n50.2\n52.0\nOpenCLIP [36]\n53.2\n33.4\n69.4\n28.0\n33.3\n41.0\n41.8\n55.8\n23.3\n47.0\n43.9\n44.1\n43.5\n55.1\n23.6\n31.7\n47.8\n21.8\n41.0\n41.4\nDIFTsd (ours)\n63.5\n54.5\n80.8\n34.5\n46.2\n52.7\n48.3\n77.7\n39.0\n76.0\n54.9\n61.3\n53.3\n46.0\n57.8\n57.1\n71.1\n63.4\n57.7\n59.5\nTable 3: Comparison with state-of-the-art methods on PF-WILLOW PCK per image (left) and CUB\nPCK per point (right). The groups follow Tab. 1. Colors of numbers indicate the best, second-best\nresults. All the DIFT results have gray background for better reference. DIFTsd achieves the best\nresults without any fine-tuning or supervision with in-domain annotations or data.\nSup.\nMethod\nPCK@\u03b1bbox\n\u03b1 = 0.05\n\u03b1 = 0.10\n(a)\nSCNet [29]\n38.6\n70.4\nDHPF [56]\n49.5\n77.6\nPMD [48]\n-\n75.6\nCHM [54]\n52.7\n79.4\nCATs [14]\n50.3\n79.2\nTransforMatcher [42]\n-\n76.0\nSCorrSAN [35]\n54.1\n80.0\n(b)\nWarpC [82]\n49.0\n75.1\n(b)\nPWarpC [83]\n45.0\n75.9\nGSF [39]\n49.1\n78.7\n(c)\nDINO [10]\n30.8\n51.1\nDIFTadm (ours)\n46.9\n67.0\nOpenCLIP [36]\n34.4\n61.3\nDIFTsd (ours)\n58.1\n81.2\nSup.\nMethod\nPCK@\u03b1img = 0.1\n(b)\nGANgealing [62]\n56.8\nNeuCongeal [58]\n65.6\n(c)\nDINO [10]\n66.4\nDIFTadm (ours)\n78.0\nOpenCLIP [36]\n67.5\nDIFTsd (ours)\n83.5\nby the total number of predicted points as the final PCK, while some works [62, 58, 26] first calculate\na PCK value for each image and then average it across the dataset (or each category split). We denote\nthe first metric as PCK per point and the second as PCK per image. We calculate both metrics for\nDIFT and self-supervised features, and compare them to methods using that metric respectively.\nQuantitative Results. We report our results in Tabs. 1 to 3. In addition to feature matching using\nDINO and OpenCLIP, we also report state-of-the-art fully-supervised and weakly-supervised methods\nin the respective tables for completeness. Across the three datasets, we observe that features learned\nvia diffusion yield more accurate correspondences compared to features learned using contrastive\napproaches (DIFTsd vs. OpenCLIP, DIFTadm vs. DINO).\n6\nSource patch\nTop-5 nearest neighbor cross-category target patches predicted by DIFTsd\nFigure 4: Given image patch specified in the leftmost image (red rectangle), we use DIFTsd to retrieve\nthe top-5 nearest patches in images from different categories in the SPair-71k test set. DIFT is able to\nfind correct correspondence for different objects sharing similar semantic parts, e.g., the wheel of an\nairplane vs. the wheel of a bus. More results are in Fig. 18 of Appendix E.\n0\n200\n400\n600\n800\nTime step t\n20\n40\n60\nSPair-71k all PCK@0.1\nDINO\nOpenCLIP\nDIFTsd with different time step t\nFigure 5: PCK per point of DIFTsd on SPair-71k. It maintains high accuracy with a wide range of\nt, outperforming other off-the-shelf self-supervised features.\nFurthermore, even without any supervision (be it explicit correspondence or in-domain data), DIFT\noutperforms all the weakly-supervised baselines on all benchmarks by a large margin. It even\noutperforms the state-of-the-art supervised methods on PF-WILLOW, and for 9 out of 18 categories\non SPair-71k.\nQualitative Results. To get a better understanding of DIFT\u2019s performance, we visualize a few\ncorrespondences on SPair-71k using various off-the-shelf features in Fig. 3. We observe that DIFT is\nable to identify correct correspondences under cluttered scenes, viewpoint changes, and instance-level\nappearance changes.\nIn addition to visualizing correspondence within the same categories in SPair-71k, we also visualize\nthe correspondence established using DIFTsd across various categories in Fig. 4. Specifically, we\nselect an image patch from a random image and query the image patches with the nearest DIFT\nembedding in the rest of the test split but from different categories. DIFT is able to identify correct\ncorrespondence across various categories.\nSensitivity to the choice of time step t. For DIFTsd, we plot how its PCK per point varies with\ndifferent choices of t on SPair-71k in Fig. 5. DIFT is robust to the choice of t on semantic correspon-\ndence, as a wide range of t outperforms other off-the-shelf self-supervised features. Appendix B\nincludes more discussion on how and why does t affect the nature of correspondence.\n7\nDIFTsd\nOpenCLIP\nDIFTsd\nOpenCLIP\nFigure 6: Edit propagation. The first column shows the source image with edits (i.e., the overlaid\nstickers), and the rest columns are the propagated results on new images from different instances,\ncategories, and domains, respectively. Compared to OpenCLIP, DIFTsd propagates edits much more\naccurately. More results are in Fig. 20 of Appendix E.\n5.3\nApplication: Edit Propagation\nOne application of DIFT is image editing: we can propagate edits in one image to others that share\nsemantic correspondences. This capability is demonstrated in Fig. 6, where we showcase DIFT\u2019s\nability to reliably propagate edits across different instances, categories, and domains, without any\ncorrespondence supervision.\nTo achieve this propagation, we simply compute a homography transformation between the source\nand target images using only matches found in the regions of the intended edits. By applying this\ntransformation to the source image edits (e.g., an overlaid sticker), we can integrate them into the\ncorresponding regions of the target image. Figure 6 shows the results for both OpenCLIP and DIFTsd\nusing the same propagation techniques. OpenCLIP fails to compute reasonable transformation due\nto the lack of reliable correspondences. In contrast, DIFTsd achieves much better results, further\njustifying the effectiveness of DIFT in finding semantic correspondences.\n6\nOther Correspondence Tasks\nWe also evaluate DIFT on geometric correspondence and temporal correspondence. As in Sec. 5, we\ncompare DIFT to other off-the-shelf self-supervised features as well as task-specific methods.\n6.1\nGeometric Correspondence\nIntuitively, we find when t is small, DIFT focuses more on low-level details, which makes it useful as\na geometric feature descriptor.\nSetup. We evaluate DIFT for homography estimation on the HPatches benchmark [4]. It con-\ntains 116 sequences, where 57 sequences have illumination changes and 59 have viewpoint\nchanges. Following [91], we extract a maximum of 1,000 keypoints from each image, and use\ncv2.findHomography() to estimate the homography from mutual nearest neighbor matches. For\nDIFT, we use the same set of keypoints detected by SuperPoint [16], as in CAPS [91].\nFor evaluation, we follow the corner correctness metric used in [91]: the four corners of one image\nare transformed into the other image using the estimated homography and are then compared with\nthose computed using the ground-truth homography. We deem the estimation correct if the average\nerror of the four corners is less than \u03f5 pixels. Note that we do this evaluation on the original image\nresolution following [91], unlike [16].\nResults. We report the accuracy comparison between DIFT and other methods in Tab. 4. Visualization\nof the matched points can be found in Fig. 7. Though not trained using any explicit geometry\nsupervision, DIFT is still on par with the methods that utilize explicit geometric supervision signals\n8\nTable 4: Homography estimation accuracy [%] at 1, 3, 5 pixels on HPatches. Colors of numbers\nindicate the best, second-best results. All the DIFT results have gray background for better reference.\nDIFT with SuperPoint keypoints achieves competitive performance.\nMethod\nGeometric\nSupervision\nAll\nViewpoint Change\nIllumination Change\n\u03f5 = 1\n\u03f5 = 3\n\u03f5 = 5\n\u03f5 = 1\n\u03f5 = 3\n\u03f5 = 5\n\u03f5 = 1\n\u03f5 = 3\n\u03f5 = 5\nSIFT [51]\nNone\n40.2\n68.0\n79.3\n26.8\n55.4\n72.1\n54.6\n81.5\n86.9\nLF-Net [59]\nStrong\n34.4\n62.2\n73.7\n16.8\n43.9\n60.7\n53.5\n81.9\n87.7\nSuperPoint [16]\n36.4\n72.7\n82.6\n22.1\n56.1\n68.2\n51.9\n90.8\n98.1\nD2-Net [19]\n16.7\n61.0\n75.9\n3.7\n38.0\n56.6\n30.2\n84.9\n95.8\nDISK [86]\n40.2\n70.6\n81.5\n23.2\n51.4\n67.9\n58.5\n91.2\n96.2\nContextDesc [52]\n40.9\n73.0\n82.2\n29.6\n60.7\n72.5\n53.1\n86.2\n92.7\nR2D2 [66]\n40.0\n74.4\n84.3\n26.4\n60.4\n73.9\n54.6\n89.6\n95.4\nw/ SuperPoint kp.\nCAPS [91]\nWeak\n44.8\n76.3\n85.2\n35.7\n62.9\n74.3\n54.6\n90.8\n96.9\nDINO [10]\nNone\n38.9\n70.0\n81.7\n21.4\n50.7\n67.1\n57.7\n90.8\n97.3\nDIFTadm (ours)\n43.7\n73.1\n84.8\n26.4\n57.5\n74.3\n62.3\n90.0\n96.2\nOpenCLIP [36]\n33.3\n67.2\n78.0\n18.6\n45.0\n59.6\n49.2\n91.2\n97.7\nDIFTsd (ours)\n45.6\n73.9\n83.1\n30.4\n56.8\n69.3\n61.9\n92.3\n98.1\nViewpoint Change\nIllumination Change\nFigure 7: Sparse feature matching using DIFTsd on HPatches after removing outliers with\ncv2.findHomography(). Left are image pairs under viewpoint change, and right are ones un-\nder illumination change. Although never trained with correspondence labels, it works well under\nboth challenging changes. More results are in Fig. 21 of Appendix E.\ndesigned specifically for this task, such as correspondences obtained from Structure-from-Motion [73]\npipelines. This shows that not only semantic-level correspondence, but also geometric correspondence\nemerges from image diffusion models.\n6.2\nTemporal Correspondence\nDIFT also demonstrates strong performance on temporal correspondence tasks, including video\nobject segmentation and pose tracking, although never trained or fine-tuned on such video data.\nSetup. We evaluate DIFT on two challenging video label propagation tasks: (1) DAVIS-2017 video\ninstance segmentation benchmark [65]; (2) JHMDB keypoint estimation benchmark [40].\nFollowing evaluation setups in [49, 37, 10, 95], representations are used as a similarity function:\nwe segment scenes with nearest neighbors between consecutive video frames. Note that there is\nno training involved in this label propagation process. We report region-based similarity J and\ncontour-based accuracy F [64] for DAVIS, and PCK for JHMDB.\nResults. Table 5 reports the experimental results, comparing DIFT with other self-supervised\nfeatures (pre-)trained with or without video data. DIFTadm outperforms all the other self-supervised\nlearning methods on both benchmarks, even surpassing models specifically trained on video data by\na significant margin. DIFT also yields the best results within the same pre-training dataset.\n9\nTable 5: Video label propagation results on DAVIS-2017 and JHMDB. Colors of numbers indicate\nthe best, second-best results. All the DIFT results have gray background for better reference. DIFT\neven outperforms other self-supervised learning methods specifically trained with video data.\n(pre-)Trained\non Videos\nMethod\nDataset\nDAVIS\nJHMDB\nJ &Fm\nJm\nFm\nPCK@0.1\nPCK@0.2\n\u2717\nInstDis [93]\nImageNet [15]\nw/o labels\n66.4\n63.9\n68.9\n58.5\n80.2\nMoCo [30]\n65.9\n63.4\n68.4\n59.4\n80.9\nSimCLR [12]\n66.9\n64.4\n69.4\n59.0\n80.8\nBYOL [25]\n66.5\n64.0\n69.0\n58.8\n80.9\nSimSiam [13]\n67.2\n64.8\n68.8\n59.9\n81.6\nDINO [10]\n71.4\n67.9\n74.9\n57.2\n81.2\nDIFTadm (ours)\n75.7\n72.7\n78.6\n63.4\n84.3\nOpenCLIP [36]\nLAION [75]\n62.5\n60.6\n64.4\n41.7\n71.7\nDIFTsd (ours)\n70.0\n67.4\n72.5\n61.1\n81.8\n\u2713\nVINCE [24]\nKinetic [11]\n65.2\n62.5\n67.8\n58.8\n80.4\nVFS [95]\n68.9\n66.5\n71.3\n60.9\n80.7\nUVC [49]\n60.9\n59.3\n62.7\n58.6\n79.6\nCRW [37]\n67.6\n64.8\n70.2\n58.8\n80.3\nColorization [88]\n34.0\n34.6\n32.7\n45.2\n69.6\nCorrFlow [44]\nOxUvA [87]\n50.3\n48.4\n52.2\n58.5\n78.8\nTimeCycle [92]\nVLOG [21]\n48.7\n46.4\n50.0\n57.3\n78.1\nMAST [43]\nYT-VOS [96]\n65.5\n63.3\n67.6\n-\n-\nSFC [34]\n71.2\n68.3\n74.0\n61.9\n83.0\nDINO\nDIFTadm\nDINO\nDIFTadm\nFigure 8: Video label propagation results on DAVIS-2017. Colors indicate segmentation masks for\ndifferent instances. Blue rectangles show the first frames. Compared to DINO, DIFTadm produces\nmasks with more accurate and sharper boundaries. More results are in Fig. 22 of Appendix E.\nWe also show qualitative results in Fig. 8, presenting predictions of video instance segmentation in\nDAVIS, comparing DIFTadm with DINO. DIFTadm produces masks with clearer boundaries when\nsingle or multiple objects are presented in the scene, even attends well to objects in the presence of\nocclusion.\n7\nConclusion\nThis paper demonstrates that correspondence emerges from image diffusion models without explicit\nsupervision. We propose a simple technique to extract this implicit knowledge as a feature extractor\nnamed DIFT. Through extensive experiments, we show that although without any explicit supervision,\nDIFT outperforms both weakly-supervised methods and other off-the-shelf self-supervised features\nin identifying semantic, geometric and temporal correspondences, and even remains on par with the\nstate-of-the-art supervised methods on semantic correspondence. We hope our work inspires future\nresearch on how to better utilize these emergent correspondence from image diffusion, as well as\nrethinking diffusion models as self-supervised learners.\n10\nAcknowledgement. This work was partially funded by NSF 2144117 and the DARPA Learning with\nLess Labels program (HR001118S0044). We would like to thank Zeya Peng for her help on the edit\npropagation section and the project page, thank Kamal Gupta for sharing the evaluation details in\nthe ASIC paper, thank Aaron Gokaslan, Utkarsh Mall, Jonathan Moon, Boyang Deng, and all the\nanonymous reviewers for valuable discussion and feedback.\nA\nSocietal Impact\nAlthough DIFT can be used with any diffusion model parameterized with a U-Net, the dominant\npublicly available model is the one trained on LAION [75]. The LAION dataset has been identified\nas having several issues including racial bias and stereotypes [7]. Diffusion models trained on these\ndatasets inherit these issues. While these issues may a priori seem less important for estimating\ncorrespondences, it might lead to differing accuracies for different kinds of images. One could obtain\nthe benefit of good correspondences without the associated issues if one could trained a diffusion\nmodel on a curated dataset. Unfortunately, the huge computational cost also prohibits the training of\ndiffusion models in academic settings on cleaner datasets. We hope that our results encourage efforts\nto build more carefully trained diffusion models.\nB\nDiscussion\nWhy does correspondence emerge from image diffusion? One conjecture is that the diffusion\ntraining objective (i.e., coarse-to-fine reconstruction loss) requires the model to produce good,\ninformative features for every pixel. This is in contrast to DINO and OpenCLIP that use image-level\ncontrastive learning objectives. In our experiments, we have attempted to evaluate the importance\nof the training objective by specifically comparing DIFTadm and DINO in all our evaluations: two\nmodels that share exactly the same training data, i.e., ImageNet-1k without labels.\nHow and why does t affect the nature of correspondence? In Fig. 9, for the same clean image,\nwe first add different amount of noise to get different xt following Eq. (2), then feed it into SD\u2019s de-\nnoising network \u03f5\u03b8 together with time step t to get the predicted clean image \u02c6xt\n0 = xt\u2212(\u221a1\u2212\u03b1t)\u03f5\u03b8(xt,t)\n\u221a\u03b1t\n.\nWe can see that, with the increase of t, more and more details are removed and only semantic-level\nfeatures are preserved, and when t becomes too large, even the object structure is distorted. Intuitively,\nthis explains why we need a small t for correspondences that requires details and a relatively large t\nfor semantic correspondence.\ninput clean image\npredicted clean images at different time step t\nt=1\nt=51\nt = 261\nt = 501\nt = 51\nt = 1\nFigure 9: Within a reasonable range, when t gets larger, the predicted clean images remain the overall\nstructure but have less details, suggesting DIFT contains more semantic-level information and less\nlow-level features with the increase of t.\nHow long does it take to run DIFT? Since we only perform a single inference step when extracting\nDIFT, it actually takes similar running time compared to competing self-supervised features with\nthe same input image size. For example, when extracting features for semantic correspondence as\nin Sec. 5, on one single NVIDIA A6000 GPU, DIFTsd takes 203 ms vs. OpenCLIP\u2019s 231 ms on\none single 768\u00d7768 image; DIFTadm takes 110 ms vs. DINO\u2019s 154 ms on one single 512\u00d7512\nimage. In practice, as mentioned in the last paragraph of Sec. 4.2, since there is randomness when\n11\nextracting DIFT, we actually use a batch of random noise to get an averaged feature map for each\nimage to slightly boost stability and performance, which would increase the running time shown\nabove. But if computation is a bottleneck, one can remove this optimization at the cost of a tiny loss\nin performance: e.g., on SPair-71k, DIFTsd: PCK 59.5\u219257.9; DIFTadm: PCK 52.0\u219251.1.\nWould diffusion inversion help? Another way to get xt from a real input image is diffusion inversion.\nUsing DDIM inversion [77] to recover input image\u2019s corresponding xt and then feeding into f\u03b8 to\nget diffusion feature yielded similar results. At the same time, inversion makes the inference process\nseveral times slower. We leave how to utilize diffusion inversion to get better correspondence to\nfuture work.\nDoes correspondence information exist in SD\u2019s encoder? We also evaluated SD\u2019s VAE encoder\u2019s\nperformance on all benchmarks and found that its performance was lower by an order of magnitude.\nSo DIFTsd\u2019s correspondence only emerges inside its U-Net and requires diffusion-based training.\nWould task-specific adaptation lead DIFT to better results? More sophisticated mechanisms\ncould be applied to further enhance the diffusion features, e.g., concatenating and re-weighting\nfeatures from different time step t and different network layers, or even fine-tuning the network with\ntask-specific supervision. Some recent works [5, 94, 101] fine-tune either the U-Net or the attached\nhead for dense prediction tasks and yield better performance. However, task-specific adaptation\nentangles the quality of the features themselves with the efficacy of the fine-tuning procedure. To\nkeep the focus on the representation, we chose to avoid any fine-tuning to demonstrate the quality\nof the off-the-shelf DIFT. Nevertheless, our preliminary experiments suggest that such fine-tuning\nwould indeed further improve performance on correspondence. We\u2019ll leave how to better adapt DIFT\nto downstream tasks to future work.\nC\nImplementation Details\nThe total time step T for both diffusion models (ADM and SD) is 1000. U-Net consists of downsam-\npling blocks, middle blocks and upsampling blocks. We only extract features from the upsampling\nblocks. ADM\u2019s U-Net has 18 upsampling blocks and SD\u2019s U-Net has 4 upsampling blocks (the\ndefinition of blocks are different between these two models). Feature maps from the n-th upsampling\nblock output are used as the final diffusion feature. For a fair comparison, we also grid-search which\nlayer to extract feature for DINO and OpenCLIP for each task, and report the best results among the\nchoices.\nAs mentioned in the last paragraph of Sec. 4.2, when extracting features for one single image using\nDIFT, we use a batch of random noise to get an averaged feature map. The batch size is 8 by default.\nWe shrink it to 4 when encountering GPU memory constraints.\nThe input image resolution varies across different tasks but we always keep it the same within the\ncomparison vs. other off-the-shelf self-supervised features (i.e., DIFTadm vs. DINO, DIFTsd vs.\nOpenCLIP) thus the comparisons are fair. For DIFT, feature map size and dimension also depend on\nwhich U-Net layer features are extracted from.\nThe following sections list the time step t and upsampling block index n (n starts from 0) we used\nfor each DIFT variant on different tasks as well as input image resolution and output feature map\ntensor shape.\nC.1\nSemantic Correspondence\nWe use t = 101 and n = 4 for DIFTadm on input image resolution 512\u00d7512 so feature map size is\n1/16 of input and dimension is 1024; we use t = 261 and n = 1 for DIFTsd on input image resolution\n768\u00d7768 so feature map size is 1/16 of input and dimension is 1280. These hyper-parameters are\nshared on all semantic correspondence tasks including SPair-71k, PF-WILLOW, and CUB, as well as\nthe visualizations in Figs. 1, 15 and 16.\nWe don\u2019t use image-specific prompts for DIFTsd. Instead, we use a general prompt \u201ca photo of\na [class]\u201d where [class] denotes the string of the input images\u2019 category, which is given by\nthe dataset. For example, for the images of SPair-71k under cat class, the prompt would be \u201ca\nphoto of a cat\u201d. For CUB, the same prompt is used for all images: \u201ca photo of a bird\u201d. Changing\n12\nper-class prompt to a null prompt (empty string \"\") will only lead a very small performance drop,\ne.g., DIFTsd\u2019s PCK per point on SPair-71k: 59.5\u219257.6.\nC.2\nGeometric Correspondence\nOn HPatches, the input images are resized to 768\u00d7768 to extract features for both DIFTadm and\nDIFTsd. We use t = 41, n = 11 for DIFTadm so feature map size is 1/2 of input and dimension is\n512; we use t = 0, n = 2 for DIFTsd so feature map size is 1/8 of input and dimension is 640.\nIn addition, for DIFTsd, each image\u2019s prompt is a null prompt, i.e., an empty string \"\".\nFor all the methods listed in Tab. 4, when doing homography estimation, we tried both co-\nsine and L2 distance for mutual nearest neighbor matching, and both RANSAC and LMEDS for\ncv2.findHomography(), and eventually we report the best number among these choices for each\nmethod.\nC.3\nTemporal Correspondence\nThe configurations we use for DIFTadm and DIFTsd are:\nDataset\nMethod\nTime step\nt\nBlock index\nn\nTemperature\nfor softmax\nPropagation\nradius\nk for\ntop-k\nNumber of\nprev. frames\nDAVIS-2017\nDIFTadm\n51\n7\n0.1\n15\n10\n28\nDAVIS-2017\nDIFTsd\n51\n2\n0.2\n15\n15\n28\nJHMDB\nDIFTadm\n101\n5\n0.2\n5\n15\n28\nJHMDB\nDIFTsd\n51\n2\n0.1\n5\n15\n14\nFor experiments on DAVIS, we use the same original video frame size (480p version of DAVIS,\nspecific size varies across different videos) as in DINO\u2019s implementation [10], for both DIFTadm and\nDIFTsd. n=7 for DIFTadm so feature map size is 1/8 of input and dimension is 512. n=2 for DIFTsd\nso feature map size is 1/8 of input and dimension is 640. For experiments on JHMDB, following\nCRW\u2019s implementation [37], we resize each video frame\u2019s smaller side to 320 and keep the original\naspect ratio. n=5 for DIFTadm so feature map size is 1/8 of input and dimension is 1024. n=2 for\nDIFTsd so feature map size is 1/8 of input and dimension is 640.\nIn addition, for DIFTsd, each image\u2019s prompt is a null prompt, i.e., an empty string \"\".\nD\nAdditional Quantitative Results\nD.1\nSemantic Correspondence on PF-PASCAL\nWe didn\u2019t do evaluation on PF-PASCAL [27] in the main paper because we found over half of the\ntest images (i.e., 302 out of 506) actually also appear in the training set, which makes the benchmark\nnumbers much less convincing, and also partially explains why the previous supervised methods tend\nto have much higher test accuracy on PF-PASCAL vs. PF-WILLOW (e.g., over 90 vs. around 70)\neven using exactly the same trained model. And this duplication issue of train/test images also gives\nhuge unfair disadvantage to the methods that are never adapted (either supervised or unsupervised)\non the training set before evaluation.\nHowever, even in this case, as shown in Tab. 6, DIFT still demonstrates competitive performance\ncompared to the state-of-the-art weakly-supervised method PWarpC [83], as well as huge gains vs.\nother off-the-shelf self-supervised features.\nD.2\nFeature Matching on HPatches\nFollowing CAPS [91], for the given image pair, we extract SuperPoint [16] keypoints in both images\nand match them using our proposed feature descriptor, DIFTsd. We follow the evaluation protocol as\nin [19, 91] and use Mean Matching Accuracy (MMA) as the evaluation metric, where only mutual\nnearest neighbors are considered as matched points. The MMA score is defined as the average\npercentage of correct matches per image pair under a certain pixel error threshold. Fig. 10 shows the\n13\nTable 6: PCK per image on PF-PASCAL. The groupings and colors follow Tab. 1.\nSup.\nMethod\nPCK@\u03b1img\n\u03b1 = 0.05\n\u03b1 = 0.10\n(b)\nPWarpC [83]\n64.2\n84.4\n(c)\nDINO [10]\n36.9\n53.6\nDIFTadm (ours)\n56.5\n72.5\nOpenCLIP [36]\n39.8\n61.1\nDIFTsd (ours)\n69.4\n84.6\n1 2 3 4 5 6 7 8 9 1011\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMMA\nOverall\nSuperPoint + DIFTsd\nSuperPoint + CAPS\nSuperPoint\nDISK (2k)\nHes. Aff. + Root-SIFT\nHAN + HN++\nDELF\nLF-Net\nD2-Net MS\nSIFT + ContextDesc\nR2D2\n1 2 3 4 5 6 7 8 9 1011\nthreshold [px]\nIllumination\n1 2 3 4 5 6 7 8 9 1011\nViewpoint\nMethods\n# Feature\n# Match\nHesAff + Root-SIFT\n6.7K\n2.8K\nHAN + HN++\n3.9K\n2.0K\nLF-Net\n0.5K\n0.2K\nSuperPoint\n1.7K\n0.9K\nDELF\n4.6K\n1.9K\nSIFT + ContextDesc\n4.4K\n1.9K\nD2-Net MS\n8.3K\n2.8K\nR2D2\n5.0K\n1.8K\nDISK (2k)\n2.0K\n1.0K\nSuperPoint + CAPS\n1.7K\n1.0K\nSuperPoint + DIFTsd\n1.7K\n0.9K\nFigure 10: Mean Matching Accuracy (MMA) on HPatches [4]. For each method, we show the MMA\nwith varying pixel error thresholds. We also report the mean number of detected features and mutual\nnearest neighbor matches. Although not trained with any explicit geometric correspondence labels,\nDIFTsd is able to achieves competitive performance compared to other feature descriptors that are\nspecifically design or trained for this task.\ncomparison between DIFT and other feature descriptors that are specially designed or trained for\ngeometric correspondence. We report the average results for the whole dataset, as well as subsets on\nillumination and viewpoint changes respectively. For each method, we also present the mean number\nof detected features per image and mutual nearest neighbor matches per image pair. We can see that,\nalthough not trained with any explicit geometry supervision, DIFT is still able to achieve competitive\nperformance.\nD.3\nAnalysis on Hyper-parameters\nHere we analyze how the choice of time step and U-Net layer would affect DIFT\u2019s performance on\ndifferent correspondence tasks.\nAblation on time step. Similar to Fig. 5, we plot how HPatches homography estimation accuracy\nand DAVIS video label propagation accuracy vary with different choices of t, in Figs. 11 and 12\nrespectively. Both curves have smooth transitions and there\u2019s a large range of t where DIFT gives\ncompetitive performance.\nAblation on U-Net layer. Compared to the definition of 4 block choices in Appendix C, here we make\na more fine-grained sweep over SD\u2019s 15 layers inside U-Net upsampling blocks. The transition from\nblock index n to layer index i is 0/1/2/3 to 3/7/11/14 respectively and both start from 0. We evaluate\nPCK per point on SPair-71k using DIFTsd with different layer index i. As shown in Fig. 13, the\naccuracy varies but there are still multiple choices of i that lead to good performance.\nE\nAdditional Qualitative Results\nPCA visualization of DIFT. In Fig. 14, for each pair of images, we extract DIFTsd from the\nsegmented instances, then compute PCA and visualize the first 3 components, where each component\nserves as a color channel. We can see the same object parts share similar embeddings, which also\ndemonstrates the emergent correspondence.\nCorrespondence on diverse internet images. Same as Fig. 1, in Figs. 15 and 16 we show more\ncorrespondence prediction on various image groups that share similar semantics. For each target\nimage, the DIFTsd predicted point will be displayed as a red circle, together with a heatmap showing\n14\n0\n50\n100\n150\n200\n250\n300\n350\ntime step t\n40\n50\n60\n70\n80\nHomography estimation accuracy\nDIFTsd with different time step t\nepsilon=5\nepsilon=3\nepsilon=1\nFigure 11: Homography estimation accuracy [%] at 1, 3, 5 pixels on HPatches using DIFTsd with\ndifferent time step t. Intuitively, as t gets larger, DIFT contains more semantic information and less\nlow-level details, so the accuracy decreases when using larger t.\n0\n50\n100\n150\n200\n250\n300\ntime step t\n68\n70\n72\n74\n76\nVideo label propagation\naccuracy\nDIFTadm with different time step t\nFigure 12: Video label propagation accuracy (J &Fm) on DAVIS using DIFTadm with different time\nstep t. There\u2019s a wide range of t, where DIFT maintains a stable and competitive performance.\n0\n2\n4\n6\n8\n10\n12\n14\nSD U-Net layer i\n10\n20\n30\n40\n50\n60\nSPair-71k PCK@0.1\nDIFTsd with features from different U-Net layer i\nFigure 13: PCK per point on SPair-71k using DIFTsd with different layer i inside U-Net\u2019s 15\nupsampling layers in total. The transition from block index n to layer index i is 0/1/2/3 to 3/7/11/14\nrespectively. We can see there are multiple choices of i leading to good performance.\n15\nSame Instance\nCross Instance\nCross Category\nFigure 14: Visualization of the first three PCA components of DIFTsd on the segmented instance\npairs (same instance, cross instance, cross category). Each component matches a color channel. We\ncan see the same object parts share similar DIFT embeddings.\nthe per-pixel cosine distance calculated using DIFTsd. We can see it works well across instances,\ncategories, and even image domains, e.g., from an umbrella photo to an umbrella logo.\nSemantic correspondence comparison among off-the-shelf features on SPair-71k. Same as\nFig. 3, we show more comparison in Fig. 17, where we can see DIFT works well under challenging\nocclusion, viewpoint change and intra-class appearance variation.\nCross-category semantic correspondence. Same as Fig. 4, in Fig. 18 we select an interesting image\npatch from a random source image and query the image patches with the nearest DIFTsd features in\nthe rest of the test split but with different categories. We see that DIFT is able to identify reasonable\ncorrespondence across various categories.\nFailure Cases on SPair-71k. In Fig. 19, we select four examples with low PCK accuracy and\nvisualize DIFTsd\u2019s predictions along with ground-truths. We can see that, when the semantic\ndefinition of key points is ambiguous, or the appearance/viewpoint change between source and target\nimages is too dramatic, DIFT fails to give correct predictions.\nImage editing propagation. Similar to Fig. 6, Fig. 20 shows more examples on edit propagation\nusing our proposed DIFTsd. It further demonstrates the effectiveness of DIFT on finding semantic\ncorrespondence, even when source image and target image are from different categories or domains.\nGeometric correspondence. Same as Fig. 7, in Fig. 21 we show the sparse feature matching results\nusing DIFTsd on HPatches. Though not trained using any explicit geometry supervision, DIFT still\nworks well under large viewpoint change and challenging illumination change.\nTemporal correspondence. Similar to Fig. 8, Fig. 22 presents additional examples of video instance\nsegmentation results on DAVIS-2017, comparing DINO, DIFTadm and Ground-truth (GT). We can\nsee DIFTadm could create instance masks that closely follow the silhouette of instances.\n16\nSource Point\nDIFTsd Predicted Target Points\nFigure 15: DIFT can find correspondences on real images across instances, categories, and even\ndomains, e.g., from a photo of statue of liberty to a logo.\n17\nSource Point\nDIFTsd Predicted Target Points\nFigure 16: DIFT can find correspondences on real images across instances, categories, and even\ndomains, e.g., from a photo of an aeroplane to a sketch.\n18\nsource image\nDINO\nOpenCLIP\nDIFT_adm\nDIFT_sd\ntarget image\nFigure 17: Semantic correspondence using various off-the-shelf features on SPair-71k. Circles\nindicates correct predictions while crosses for incorrect ones.\n19\nSource patch\nTop-5 nearest neighbor cross-category target patches predicted by DIFTsd\nFigure 18: Given the image patch specified in the leftmost image (red dot), we use DIFTsd to query\nthe top-5 nearest image patches from different categories in the SPair-71k test set. DIFT is still able\nto find correct correspondence for object parts with different overall appearance but sharing the same\nsemantic meaning, e.g., the leg of a bird vs. the leg of a dog.\nsource image\nDIFTsd\ntarget image\nsource image\nDIFTsd\ntarget image\nFigure 19: Failure cases of semantic correspondence on SPair-71k. Circle denotes correct predictions\nwhile cross for wrong ones. When the semantic definition of key points is ambiguous, or the\nappearance/viewpoint change between source and target images is too dramatic, DIFTsd fails to\npredict the correct corresponding points.\n20\nFigure 20: Edit propagation using DIFTsd. Far left column: edited source images. Right columns:\ntarget images with the propagated edits. Note that despite the large domain gap in the last row,\nDIFTsd still manages to establish reliable correspondences for correct propagation.\n21\nFigure 21: Sparse feature matching using DIFTsd on HPatches after removing outliers. Left are\nimage pairs under viewpoint change, and right are ones under illumination change. Although never\ntrained with geometric correspondence labels, it works well under both challenging changes.\n22\nDINO\nDIFTadm\nDINO\nDIFTadm\nGT\nDINO\nDIFTadm\nGT\nGT\nFigure 22: Additional video label propagation results on DAVIS-2017. Colors indicate segmentation\nmasks for different instances. Blue rectangles show the first frames. GT is short for \"Ground-Truth\".\n23\nReferences\n[1] K. Aberman, J. Liao, M. Shi, D. Lischinski, B. Chen, and D. Cohen-Or. Neural best-buddies: Sparse\ncross-domain correspondence. ACM Transactions on Graphics (TOG), 37(4):1\u201314, 2018. 6\n[2] S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. M. Seitz, and R. Szeliski. Building rome\nin a day. Communications of the ACM, 54(10):105\u2013112, 2011. 2\n[3] S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel. Deep vit features as dense visual descriptors. arXiv\npreprint arXiv:2112.05814, 2(3):4, 2021. 2\n[4] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. Hpatches: A benchmark and evaluation of\nhandcrafted and learned local descriptors. In CVPR, 2017. 8, 14\n[5] D. Baranchuk, I. Rubachev, A. Voynov, V. Khrulkov, and A. Babenko. Label-efficient semantic segmenta-\ntion with diffusion models. arXiv preprint arXiv:2112.03126, 2021. 3, 12\n[6] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. Lecture notes in computer\nscience, 3951:404\u2013417, 2006. 2\n[7] A. Birhane, V. U. Prabhu, and E. Kahembwe. Multimodal datasets: misogyny, pornography, and malignant\nstereotypes. arXiv preprint arXiv:2110.01963, 2021. 11\n[8] T. Brooks, A. Holynski, and A. A. Efros. Instructpix2pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 1\n[9] K. Cao, M. Brbic, and J. Leskovec.\nConcept learners for few-shot learning.\narXiv preprint\narXiv:2007.07375, 2020. 2\n[10] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties\nin self-supervised vision transformers. In ICCV, pages 9650\u20139660, 2021. 1, 2, 5, 6, 9, 10, 13, 14\n[11] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In\nCVPR, pages 6299\u20136308, 2017. 10\n[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual\nrepresentations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020. 10\n[13] X. Chen and K. He. Exploring simple siamese representation learning. In CVPR, pages 15750\u201315758,\n2021. 10\n[14] S. Cho, S. Hong, S. Jeon, Y. Lee, K. Sohn, and S. Kim. Cats: Cost aggregation transformers for visual\ncorrespondence. NeurIPS, 34:9011\u20139023, 2021. 2, 5, 6\n[15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255. Ieee, 2009. 3, 4, 5, 10\n[16] D. DeTone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-supervised interest point detection and\ndescription. In CVPRW, pages 224\u2013236, 2018. 8, 9, 13\n[17] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:8780\u20138794,\n2021. 1, 3, 4\n[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929, 2020. 5\n[19] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and T. Sattler. D2-net: A trainable cnn\nfor joint detection and description of local features. arXiv preprint arXiv:1905.03561, 2019. 9, 13\n[20] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes\n(voc) challenge. IJCV, 88:303\u2013338, 2010. 5\n[21] D. F. Fouhey, W.-c. Kuo, A. A. Efros, and J. Malik. From lifestyle vlogs to everyday interactions. In\nCVPR, pages 4991\u20135000, 2018. 10\n[22] S. Gao, C. Zhou, C. Ma, X. Wang, and J. Yuan. Aiatrack: Attention in attention for transformer visual\ntracking. In ECCV, pages 146\u2013164. Springer, 2022. 1, 2\n[23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020. 2\n[24] D. Gordon, K. Ehsani, D. Fox, and A. Farhadi. Watching the world go by: Representation learning from\nunlabeled videos. arXiv preprint arXiv:2003.07990, 2020. 10\n[25] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D.\nGuo, M. G. Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv\npreprint arXiv:2006.07733, 2020. 10\n[26] K. Gupta, V. Jampani, C. Esteves, A. Shrivastava, A. Makadia, N. Snavely, and A. Kar. Asic: Aligning\nsparse in-the-wild image collections. arXiv preprint arXiv:2303.16201, 2023. 6\n24\n[27] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal flow. In CVPR, 2016. 5, 13\n[28] M. Hamilton, Z. Zhang, B. Hariharan, N. Snavely, and W. T. Freeman. Unsupervised semantic segmenta-\ntion by distilling feature correspondences. In ICLR, 2022. 1, 2\n[29] K. Han, R. S. Rezende, B. Ham, K.-Y. K. Wong, M. Cho, C. Schmid, and J. Ponce. Scnet: Learning\nsemantic correspondence. In ICCV, 2017. 6\n[30] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation\nlearning. In CVPR, pages 9729\u20139738, 2020. 10\n[31] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3\n[32] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.\n1, 3\n[33] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[34] Y. Hu, R. Wang, K. Zhang, and Y. Gao. Semantic-aware fine-grained correspondence. In ECCV, pages\n97\u2013115. Springer, 2022. 10\n[35] S. Huang, L. Yang, B. He, S. Zhang, X. He, and A. Shrivastava. Learning semantic correspondence with\nsparse annotations. arXiv preprint arXiv:2208.06974, 2022. 2, 5, 6\n[36] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\nH. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. 2, 5, 6,\n9, 10, 14\n[37] A. Jabri, A. Owens, and A. A. Efros. Space-time correspondence as a contrastive random walk. NeurIPS,\n2020. 2, 9, 10, 13\n[38] S. Jeon, S. Kim, D. Min, and K. Sohn. Pyramidal semantic correspondence networks. IEEE TPAMI,\n44(12):9102\u20139118, 2021. 6\n[39] S. Jeon, D. Min, S. Kim, J. Choe, and K. Sohn. Guided semantic flow. In European Conference on\nComputer Vision, 2020. 6\n[40] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black. Towards understanding action recognition. In\nICCV, pages 3192\u20133199, 2013. 9\n[41] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative\nmodels. arXiv preprint arXiv:2206.00364, 2022. 1, 3\n[42] S. Kim, J. Min, and M. Cho. Transformatcher: Match-to-match attention for semantic correspondence. In\nCVPR, pages 8697\u20138707, 2022. 2, 5, 6\n[43] Z. Lai, E. Lu, and W. Xie. Mast: A memory-augmented self-supervised tracker. In CVPR, pages\n6479\u20136488, 2020. 10\n[44] Z. Lai and W. Xie.\nSelf-supervised learning for video correspondence flow.\narXiv preprint\narXiv:1905.00875, 2019. 10\n[45] J. Lee, D. Kim, J. Ponce, and B. Ham. Sfnet: Learning object-aware semantic correspondence. In CVPR,\npages 2278\u20132287, 2019. 6\n[46] J. Y. Lee, J. DeGol, V. Fragoso, and S. N. Sinha. Patchmatch-based neighborhood consensus for semantic\ncorrespondence. In CVPR, pages 13153\u201313163, 2021. 2\n[47] W. Li, O. Hosseini Jafari, and C. Rother. Deep object co-segmentation. In ACCV, pages 638\u2013653.\nSpringer, 2019. 2\n[48] X. Li, D.-P. Fan, F. Yang, A. Luo, H. Cheng, and Z. Liu. Probabilistic model distillation for semantic\ncorrespondence. In CVPR, pages 7505\u20137514, 2021. 6\n[49] X. Li, S. Liu, S. De Mello, X. Wang, J. Kautz, and M.-H. Yang. Joint-task self-supervised learning for\ntemporal correspondence. NeurIPS, 32, 2019. 9, 10\n[50] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu. Unsupervised part segmentation through disentangling\nappearance and shape. In CVPR, pages 8355\u20138364, 2021. 2\n[51] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91\u2013110, 2004. 2, 9\n[52] Z. Luo, T. Shen, L. Zhou, J. Zhang, Y. Yao, S. Li, T. Fang, and L. Quan. Contextdesc: Local descriptor\naugmentation with cross-modality context. In CVPR, pages 2527\u20132536, 2019. 9\n[53] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image synthesis and\nediting with stochastic differential equations. In ICLR, 2021. 1\n[54] J. Min and M. Cho. Convolutional hough matching networks. In CVPR, 2021. 6\n25\n[55] J. Min, J. Lee, J. Ponce, and M. Cho. Spair-71k: A large-scale benchmark for semantic correspondence.\narXiv prepreint arXiv:1908.10543, 2019. 5\n[56] J. Min, J. Lee, J. Ponce, and M. Cho. Learning to compose hypercolumns for visual correspondence. In\nECCV, 2020. 6\n[57] J. Mu, S. De Mello, Z. Yu, N. Vasconcelos, X. Wang, J. Kautz, and S. Liu. Coordgan: Self-supervised\ndense correspondences emerge from gans. In CVPR, pages 10011\u201310020, 2022. 2\n[58] D. Ofri-Amar, M. Geyer, Y. Kasten, and T. Dekel. Neural congealing: Aligning images to a joint semantic\natlas. In CVPR, 2023. 1, 5, 6\n[59] Y. Ono, E. Trulls, P. Fua, and K. M. Yi. Lf-net: Learning local features from images. NeurIPS, 31, 2018.\n9\n[60] O. \u00d6zye\u00b8sil, V. Voroninski, R. Basri, and A. Singer. A survey of structure from motion. Acta Numerica,\n26:305\u2013364, 2017. 2\n[61] G. Parmar, K. K. Singh, R. Zhang, Y. Li, J. Lu, and J.-Y. Zhu. Zero-shot image-to-image translation.\narXiv preprint arXiv:2302.03027, 2023. 1, 4\n[62] W. Peebles, J.-Y. Zhu, R. Zhang, A. Torralba, A. Efros, and E. Shechtman. Gan-supervised dense visual\nalignment. In CVPR, 2022. 2, 5, 6\n[63] Y. Peng, X. He, and J. Zhao. Object-part attention model for fine-grained image classification. IEEE TIP,\n27(3):1487\u20131500, 2017. 2\n[64] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark\ndataset and evaluation methodology for video object segmentation. In CVPR, pages 724\u2013732, 2016. 9\n[65] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel\u00e1ez, A. Sorkine-Hornung, and L. Van Gool. The 2017 davis\nchallenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 9\n[66] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon, and M. Humenberger. R2d2:\nrepeatable and reliable detector and descriptor. arXiv preprint arXiv:1906.06195, 2019. 9\n[67] I. Rocco, R. Arandjelovic, and J. Sivic. Convolutional neural network architecture for geometric matching.\nIn CVPR, pages 6148\u20136157, 2017. 6\n[68] I. Rocco, R. Arandjelovi\u00b4c, and J. Sivic. End-to-end weakly-supervised semantic alignment. In CVPR,\npages 6917\u20136925, 2018. 6\n[69] I. Rocco, M. Cimpoi, R. Arandjelovi\u00b4c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus\nnetworks. NeurIPS, 31, 2018. 6\n[70] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with\nlatent diffusion models, 2021. 1, 3, 4\n[71] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image seg-\nmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th\nInternational Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241.\nSpringer, 2015. 1, 3\n[72] J. C. Rubio, J. Serrat, A. L\u00f3pez, and N. Paragios. Unsupervised co-segmentation through region matching.\nIn CVPR, pages 749\u2013756. IEEE, 2012. 2\n[73] J. L. Sch\u00f6nberger and J.-M. Frahm. Structure-from-motion revisited. In CVPR, 2016. 1, 2, 9\n[74] J. L. Sch\u00f6nberger, E. Zheng, M. Pollefeys, and J.-M. Frahm. Pixelwise view selection for unstructured\nmulti-view stereo. In ECCV, 2016. 2\n[75] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta,\nC. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation\nimage-text models. arXiv preprint arXiv:2210.08402, 2022. 3, 4, 10, 11\n[76] P. H. Seo, J. Lee, D. Jung, B. Han, and M. Cho. Attentive semantic alignment with offset-aware correlation\nkernels. In ECCV, pages 349\u2013364, 2018. 6\n[77] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 12\n[78] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS,\n32, 2019. 3\n[79] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative\nmodeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 3\n[80] L. Tang, N. Ruiz, Q. Chu, Y. Li, A. Holynski, D. E. Jacobs, B. Hariharan, Y. Pritch, N. Wadhwa,\nK. Aberman, et al. Realfill: Reference-driven generation for authentic image completion. arXiv preprint\narXiv:2309.16668, 2023. 1\n26\n[81] L. Tang, D. Wertheimer, and B. Hariharan. Revisiting pose-normalization for fine-grained few-shot\nrecognition. In CVPR, pages 14352\u201314361, 2020. 2\n[82] P. Truong, M. Danelljan, F. Yu, and L. Van Gool. Warp consistency for unsupervised learning of dense\ncorrespondences. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n10346\u201310356, 2021. 6\n[83] P. Truong, M. Danelljan, F. Yu, and L. Van Gool. Probabilistic warp consistency for weakly-supervised\nsemantic correspondences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8708\u20138718, 2022. 6, 13, 14\n[84] N. Tumanyan, O. Bar-Tal, S. Bagon, and T. Dekel. Splicing vit features for semantic appearance transfer.\nIn CVPR, pages 10748\u201310757, 2022. 2\n[85] N. Tumanyan, M. Geyer, S. Bagon, and T. Dekel. Plug-and-play diffusion features for text-driven\nimage-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 1, 3, 4\n[86] M. Tyszkiewicz, P. Fua, and E. Trulls. Disk: Learning local features with policy gradient. Advances in\nNeural Information Processing Systems, 33:14254\u201314265, 2020. 9\n[87] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W. Smeulders, P. H. Torr, and E. Gavves.\nLong-term tracking in the wild: A benchmark. In ECCV, pages 670\u2013685, 2018. 10\n[88] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy. Tracking emerges by colorizing\nvideos. In ECCV, pages 391\u2013408, 2018. 10\n[89] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset.\n2011. 5\n[90] Q. Wang, Y.-Y. Chang, R. Cai, Z. Li, B. Hariharan, A. Holynski, and N. Snavely. Tracking everything\neverywhere all at once. arXiv preprint arXiv:2306.05422, 2023. 1\n[91] Q. Wang, X. Zhou, B. Hariharan, and N. Snavely. Learning feature descriptors using camera pose\nsupervision. In ECCV, pages 757\u2013774. Springer, 2020. 2, 8, 9, 13\n[92] X. Wang, A. Jabri, and A. A. Efros. Learning correspondence from the cycle-consistency of time. In\nCVPR, pages 2566\u20132576, 2019. 1, 2, 10\n[93] Z. Wu, Y. Xiong, S. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance-level\ndiscrimination. arXiv preprint arXiv:1805.01978, 2018. 10\n[94] J. Xu, S. Liu, A. Vahdat, W. Byeon, X. Wang, and S. De Mello. Open-vocabulary panoptic segmentation\nwith text-to-image diffusion models. arXiv preprint arXiv:2303.04803, 2023. 3, 12\n[95] J. Xu and X. Wang. Rethinking self-supervised correspondence learning: A video frame-level similarity\nperspective. arXiv preprint arXiv:2103.17263, 2021. 9, 10\n[96] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang. Youtube-vos: A large-scale video object\nsegmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. 10\n[97] B. Yan, Y. Jiang, P. Sun, D. Wang, Z. Yuan, P. Luo, and H. Lu. Towards grand unification of object\ntracking. In ECCV, pages 733\u2013751. Springer, 2022. 2\n[98] E. Yu, K. Blackburn-Matzen, C. Nguyen, O. Wang, R. Habib Kazi, and A. Bousseau. Videodoodles:\nHand-drawn animations on videos with scene-aware canvases. ACM Transactions on Graphics (TOG),\n42(4):1\u201312, 2023. 1\n[99] Y. Zhang, H. Ling, J. Gao, K. Yin, J.-F. Lafleche, A. Barriuso, A. Torralba, and S. Fidler. Datasetgan:\nEfficient labeled data factory with minimal human effort. In CVPR, 2021. 2\n[100] D. Zhao, Z. Song, Z. Ji, G. Zhao, W. Ge, and Y. Yu. Multi-scale matching networks for semantic\ncorrespondence. In ICCV, pages 3354\u20133364, 2021. 6\n[101] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu. Unleashing text-to-image diffusion models for visual\nperception. arXiv preprint arXiv:2303.02153, 2023. 3, 12\n[102] Y. Zhou, C. Barnes, E. Shechtman, and S. Amirghodsi. Transfill: Reference-guided image inpainting\nby merging multiple color and spatial transformations. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2266\u20132276, 2021. 1\n27\n"
  },
  {
    "title": "Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis",
    "link": "https://arxiv.org/pdf/2306.03504.pdf",
    "upvote": "3",
    "text": "Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis\nZhenhui Ye * # 1 2 Ziyue Jiang * # 1 2 Yi Ren * 2 Jinglin Liu 2 Chen Zhang # 1 2 Xiang Yin 2 Zejun Ma 2\nZhou Zhao 1\nAbstract\nWe are interested in a novel task, namely low-\nresource text-to-talking avatar. Given only a few-\nminute-long talking person video with the audio\ntrack as the training data and arbitrary texts as the\ndriving input, we aim to synthesize high-quality\ntalking portrait videos corresponding to the input\ntext. This task has broad application prospects\nin the digital human industry but has not been\ntechnically achieved yet due to two challenges:\n(1) It is challenging to mimic the timbre from out-\nof-domain audio for a traditional multi-speaker\nText-to-Speech system. (2) It is hard to render\nhigh-fidelity and lip-synchronized talking avatars\nwith limited training data. In this paper, we in-\ntroduce Adaptive Text-to-Talking Avatar (Ada-\nTTA), which (1) designs a generic zero-shot multi-\nspeaker TTS model that well disentangles the text\ncontent, timbre, and prosody; and (2) embraces\nrecent advances in neural rendering to achieve\nrealistic audio-driven talking face video genera-\ntion. With these designs, our method overcomes\nthe aforementioned two challenges and achieves\nto generate identity-preserving speech and realis-\ntic talking person video. Experiments demon-\nstrate that our method could synthesize realis-\ntic, identity-preserving, and audio-visual synchro-\nnized talking avatar videos.\n1. Introduction\nRecent years have witnessed an emergence of generative\nartificial intelligence in various domains, for instance, with a\nlarge language model (LLM)-based chatbot (Adamopoulou\n& Moussiades, 2020), we can obtain high-quality, natural,\nand realistic dialogue text content. Using an advanced text-\nto-speech (TTS) system (Kim et al., 2021; Ren et al., 2021;\nWang et al., 2023; Ye et al., 2023b), we can synthesize\n*Equal contribution #Interns at ByteDance 1Zhejiang Uni-\nversity\n2ByteDance.\nCorrespondence\nto:\nZhou\nZhao\n<zhaozhou@zju.edu.cn>.\nProceedings of the 40 th International Conference on Machine\nLearning, 2023. Copyright 2023 by the author(s).\npersonalized speech given reference audio and plain texts.\nThe development of neural rendering techniques also makes\nit possible to achieve high-fidelity and realistic talking face\nvideo generation (TFG) (Prajwal et al., 2020; Guo et al.,\n2021; Ye et al., 2023c) with only a few training samples. It\nis natural to think of combining the TTS model and TFG\nmethod so that the joint system could allow users to create\na talking video with only text input. This joint system\nhas broad potential applications such as news broadcasting,\nvirtual lectures, and talking chatbots considering the recent\nadvance of ChatGPT (Ouyang et al., 2022).\nHowever, previous TTS and TFG system typically requires\na large amount of identity-specific data to produce satisfying\npersonalized results(Ren et al., 2020; Suwajanakorn et al.,\n2017), which raises challenges to real scenarios in which\ntypically only a few-minute-long video of a target person is\navailable. Motivated by this observation, we are interested\nin a novel task named low-resource text-to-talking avatar\n(TTA). Given only a few-minute-long talking person video\nwith transcribed audio track as the training data, we aim to\nsynthesize identity-preserving and audio-lip synchronized\ntalking portrait videos given the driving input text.\nWe first consider the challenges in TTS and TFG respec-\ntively. As for the TTS stage, the main challenge is how\nto properly preserve the timbre identity of the input au-\ndio (Kharitonov et al., 2023). A naive solution is to fine-\ntune a pre-trained TTS model on the given text-audio pairs.\nHowever, fine-tuning induces a large latency, and due to the\nlimited amount of data, generalizability, the performance is\nnot guaranteed; Another solution is to extract a speaker em-\nbedding of the input audio with an off-the-shelf toolkit (Jia\net al., 2018), which is known to introduce information loss\nand the identity-preserving quality is not satisfying. As for\nthe TFG stage, the main challenge is to achieve high-fidelity\nand audio-visual synchronization given the limited amount\nof audio-video pairs. Some zero-shot methods (Prajwal\net al., 2020; Zhou et al., 2020; 2021) achieve good lip syn-\nchronization by training the model on big data, but the video\nquality is not as good. Some recent neural rendering-based\nmethods (Guo et al., 2021; Tang et al., 2022; Liu et al., 2022)\nachieve the goal of high fidelity, yet lip synchronization is\npoor due to the small amount of audio-lip training pairs.\narXiv:2306.03504v2  [cs.CV]  2 Aug 2023\nAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis\nIn this paper, we propose Ada-TTA to handle the afore-\nmentioned problems. Ada-TTA is a joint system of TTS\nand TFG, which takes advantage of the most recent ad-\nvances in each sub-task. To improve the identity-preserving\npower of the TTS model, we introduce a well-designed\nzero-shot multi-speaker TTS model trained on a 20,000-\nhour-long TTS dataset, which can synthesize high-quality\npersonalized speech with only one short recording of an un-\nseen speaker. To achieve high-fidelity and lip-synchronized\ntalking face generation, we utilize the recently proposed\nGeneFace++ (Ye et al., 2023a) as the TFG system, since it\nimproves the lip-synchronization and system efficiency of\nthe previous neural rendering-based methods while main-\ntaining high fidelity. Combining the advantages of these two\nadvanced systems in TTS and TFG, Ada-TTA achieves low-\nresource but high-quality text-to-talking avatar synthesis.\nThe experiment shows good performance of our Ada-TTA\nin terms of synthesized speech and video. It also shows\nAda-TTA outperforms the baseline from the perspective of\nobjective and subjective metrics.\n2. Related works\nOur work is majorly related to a low-resource personal-\nized text-to-speech and talking face generation. We discuss\nrelated works from these two fields respectively.\nPrevious personalized speech generation approaches, also\nknown as zero-shot multi-speaker TTS, can be categorized\ninto speaker adaptation and speaker encoding methods. Tra-\nditional works (Ren et al., 2019; Casanova et al., 2022; Ye\net al., 2022) are typically trained on small reading-style\ndatasets and cannot generalize well for unseen speakers.\nSome recent works trained on large-scale multi-domain\ndatasets demonstrate the effectiveness in zero-shot scenar-\nios. Among them, some works (Wang et al., 2023; Shen\net al., 2023) utilize the neural audio codec models to convert\naudio waveform into latent and consider it as the interme-\ndiate representation for speech generation, which ignores\nthe intrinsic properties of speech attributes and may lead\nto inferior or uncontrollable results (e.g, degraded timbre\nidentity similarity and uncontrollable prosody). By contrast,\nthe proposed zero-shot multi-speaker TTS system disentan-\ngles the speech into different attributes and models each of\nthem using architectures with appropriate inductive biases,\nwhich improves the identity-preserving power and prosody\nnaturalness.\nTraditional works in zero-shot/low-resource TFG (Prajwal\net al., 2020; Zhou et al., 2021; 2020) typically adopt a\nGAN-based renderer given a reference image of the iden-\ntity, which fails to generate realistic and identity-preserving\nvideo. Recent works have embraced the neural radiance field\n(NeRF) (Mildenhall et al., 2020; Guo et al., 2021) as it en-\nables realistic 3D pose control and could achieve good video\nVideo Frames\nAudio Track\nTimbre\nProsody\nSynthesized Speech\nInput Text\n\u201cHello everyone \u2026\u201d\nTrain the \nrenderer\nA short video\nfor training\nReference\nAppearance\nGeometry\nSynthesized Video\nZero-shot\nMulti-Speaker TTS\nTalking Face \nGeneration\nFigure 1: The overall pipeline of Ada-TTA. The dotted line\ndenotes that the process is only executed during the training\nphase.\nquality with limited data of an identity. Then Some works\nexplore sample-efficient and time-efficient training (Liu\net al., 2022; Shen et al., 2022). Recently, GeneFace++ (Ye\net al., 2023a), a recently proposed method improves lip\nsynchronization to OOD audio and achieves real-time infer-\nence, which promotes the NeRF-based TFG applicable to\nreal-world scenarios.\n3. Proposed method\nThe proposed system consists of two main modules: a zero-\nshot multi-speaker TTS module and a TFG module. First,\nthe TTS model obtains a reference audio clip of the target\nidentity and extracts the timbre and prosody pattern from it\nin an in-context-learning manner, then transforms the input\ntext to the speech audio. Subsequently, the TFG module\nsynthesizes the talking person portrait video synchronized\nwith the input speech. The overall pipeline of our system is\nshown in Figure 1.\n3.1. Zero-Shot Multi-Speaker TTS\nWe use an internal zero-shot multi-speaker TTS model. As\nshown in Figure 2, it is a VQGAN-based (Esser et al., 2021)\nTTS model which comprises a timbre encoder, a text en-\ncoder, a vector quantization (VQ) prosody encoder, and\na mel decoder to disentangles the mel-spectrogram into\nspeech attributes (e.g., prosody and timbre). We disentan-\ngle the mel-spectrogram with a carefully designed informa-\ntion bottleneck: 1) we use the text encoder to encode the\nphoneme sequence into the content representation; 2) we\nfeed the reference mel-spectrogram sampled from a differ-\nent speech of the same speaker to disentangle the timbre and\ncontent information and temporally average the output of\nthe timbre encoder to get a one-dimensional global timbre\nAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis\nTimbre \nEncoder\nMel\nDecoder\nText\nReference Mel\nVQ-Prosody\nEncoder\n\"hello everyone ...\"\nText\nEncoder\nP-LLM\ninfer\ntrain\nFigure 2: The overall structure of our internal zero-shot\nmulti-speaker TTS model.\nvector. 3) we feed the first 20 bins in each mel-spectrogram\nframe into the prosody encoder, as it contains almost com-\nplete prosody and less timbre/content information. We also\nintroduce a carefully-tuned vector quantization (VQ) layer\nand a phoneme-level downsampling layer to the prosody\nencoder to constrain the information flow. The correctly-\ndesigned bottleneck will learn to remove the content infor-\nmation and the global timbre information from the output\nof the prosody encoder, which ensures the performance of\ndisentanglement. During training the GT mel-spectrogram\nis used to extract the prosody sequence. And during the\ninference phase, we need to predict the prosody sequence\ngiven the input text. To this end, leveraging the powerful\nin-context learning capability of LLMs, we learn a prosody\nlarge language model (P-LLM) that fits the distribution of\nprosody in an auto-regressive manner. To be specific, the\nP-LLM is a decoder-only transformer-based architecture\nthat uses prosody codes from the reference speech as the\nprompt to generate the prosody codes for the target speech.\nOnce the P-LLM is trained, during inference, we propose to\nuse the content from the given text sequence, the timbre ex-\ntracted from the prompt speech, and the prosody predicted\nby our P-LLM to generate the target speech for zero-shot\npersonalized speech synthesis.\n3.2. Talking Face Generation\nWe adopt GeneFace++ (Ye et al., 2023a) as the talking face\ngeneration model, which is a low-resource TFG method\nthat could render lip-synchronized and high-fidelity talking\nface video in real-time. It compromises an audio-to-motion\nmodule to transform the input speech into facial landmarks,\nand a motion-to-video render to synthesize video frames\ngiven the landmark. We extract pitch contours and HuBERT\nfeatures from the raw waveform as the audio representa-\ntion, and select 68 facial landmarks from the reconstructed\n3DMM mesh as the motion representation. The audio-to-\nmotion module consists of a generic HuBERT-conditioned\nVAE and an identity-specific postnet to produce the audio-\nsynchronized and identity-preserving facial landmark. The\nmotion-to-video renderer is a landmark-conditioned NeRF\nthat can freely control the facial expression and head pose\nby adjusting the facial landmark and the camera pose. To\nimprove the system efficiency, the audio-to-motion module\nis built with a non-autoregressive network structure that pro-\ncesses the whole audio sequence in one forward, and the\nmotion-to-video renderer adopts recent advances in Grid-\nbased NeRF, which replaces previous computationally ex-\npensive dense MLP query with simple bi-linear indexing in\na learnable grid.\n3.3. Training and Inference\nThe training procedure of the zero-shot multi-speaker\nTTS model has two stages. In the first stage, we train\nthe VQGAN-based TTS model to reconstruct the mel-\nspectrogram given the input text, GT prosody code, and\nthe reference mel-spectrogram. The training loss of the\nVQGAN-based TTS is as follows:\nL = ||y \u2212 \u02c6y||2\n2 + LV Q + LAdv\n(1)\nwhere ||y \u2212 \u02c6y||2\n2 is the L2 loss on mel-spectrogram, LV Q is\nthe VQVAE loss function (Van Den Oord et al., 2017; Esser\net al., 2021) and LAdv is the LSGAN-styled adversarial\nloss (Mao et al., 2017). Then in the second stage, we train\nthe P-LLM to predict the prosody code given the input text\nand previous prosody code sequence. The P-LLM is trained\nin a teacher-forcing mode in the training stage via the cross-\nentropy loss.\nIn training our talking face generation model, we use the pre-\ntrained audio-to-motion module in GeneFace++ and train\nthe NeRF-based renderer from scratch. To further improve\nthe image quality, we adopt VGG perceptual loss on the lip\npart of the predicted image.\nDuring inference, the zero-shot multi-speaker TTS model\nsynthesizes identity-preserving speech given the input text,\nthen the talking face generation model takes the synthesized\naudio as input and generates the talking portrait images. Fi-\nnally, the synthesized audio and video frames are integrated\ninto the final video.\n4. Experiments\nTraining Details.\nAs for the TTS model, to improve the\ntimbre and prosody generalizability, we scale up the TTS\nmodel to 222.5M parameters and adopt GigaSpeech (Chen\net al., 2021), a 10,000-hour-long English TTS dataset for\ntraining the model. We train this large-scale TTS model on\n8 NVIDIA A100 GPUs with a batch size of 30 sentences on\neach GPU. It takes 420k steps for convergence and no fur-\nAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis\nuniversity\neveryone\n<silence>\ntogether\nAda-TTA\nYourTTS+\nWav2Lip\nFigure 3: The video frames generated by the TTA systems.\nther fine-tuning is needed. As for the talking face generation,\nwe use the pre-trained audio-to-motion module provided by\nGeneFace++ (Ye et al., 2023a) and train the NeRF-based\nrenderer for each specific identity for 320k steps, which\ntakes about 6 hours on 1 NVIDIA A100 GPU. The talking\nperson videos used to train the renderer are about 3 minutes\nin length.\nComparison baseline.\nSince there is no publicly available\nlow-resource text-to-avatar system, we construct a baseline\nby combining YourTTS (Casanova et al., 2022), a recently\nproposed zero-shot multi-speaker TTS, and Wav2Lip (Pra-\njwal et al., 2020), a state-of-the-art few-shot talking face\ngeneration method from the perspective of lip synchroniza-\ntion. We denote the baseline as YourTTS+Wav2Lip. Note\nthat during inference Wav2Lip takes the whole training\nvideo as input and only regenerates the lip part. By contrast,\nour model renders the whole frame.\nEvaluation Metrics.\nWe conduct the CMOS (compara-\ntive mean opinion score) test to evaluate the performance\nof the text-to-avatar systems. We analyze the CMOS in\nthree aspects: CMOS-A (Only analyze the audio, including\nspeaker similarity, prosody, and audio quality), CMOS-V\n(Only analyze the video, including image fidelity, 3D real-\nness, and identity preserving), CMOS (the overall opinion\nscore of the synthesized video). We tell the tester to focus\non one corresponding aspect and ignore the other aspect\nwhen scoring. As for the subjective evaluation, we use the\nWavLM (Chen et al., 2022) model fine-tuned for speaker\nverification to compute the cosine similarity score between\nthe ground-truth speech and synthesized speech; we use FID\nto measure the image quality of the synthesized video.\nExperiment Results.\nAs shown in Table 1, our method\nachieves a higher speaker similarity than YourTTS, proving\nthe effectiveness of our zero-shot multi-speaker TTS model\nto preserve the identity of the input reference audio. We\nalso notice our method achieves better FID than Wav2Lip,\nwhich denotes a better image quality of the rendered video\nframes. We further perform the CMOS test to evaluate the\nperformance in terms of human perception. The results are\nTable 1: Objective evaluation of the TTA systems. Spk-Sim\ndenotes cosine speaker similarity.\nMethod\nSpk-Sim\u2191\nFID\u2193\nYourTTS + Wav2Lip\n0.9392\n55.43\nAda-TTA (Ours)\n0.9854\n28.36\nTable 2: CMOS Results of the TTA systems. The error bar\nis standard deviation. Y+W denotes YourTTS+Wav2Lip.\nMethod\nCMOS-A\nCMOS-V\nCMOS\nY+W\n0.00\n0.00\n0.00\nAda-TTA\n+0.84 \u00b1 0.50\n+0.76 \u00b1 0.42\n+0.74 \u00b1 0.31\nlisted in Table 2. We find that prefer the synthesized videos\nby our method in terms of audio quality (CMOS-A), video\nquality (CMOS-V), and overall quality (CMOS). To make\na better qualitative comparison, we recommend the reader\nto watch a demo video 1. We also provide some keyframes\nin Figure 3 to compare the performance of our Ada-TTA\nand the baseline. We can observe that compared with the\nbaseline, Ada-TTA produces (1) better speech in terms of\nhigh timbre similarity and good audio quality; and (2) better\nvideo with high lip-synchronization, good image fidelity,\nand free head pose control.\n5. Conclusions\nIn this paper, we present Ada-TTA, an adaptive high-quality\ntext-to-talking avatar synthesis system. Given only a few-\nminute-long talking person video as training data, with Ada-\nTTA we can synthesize identity-preserving speech given\narbitrary input text, and generate the lip-synchronized video.\nWe describe a zero-shot multi-speaker TTS model and a\nhigh-quality talking face generation method used to con-\nstruct the Ada-TTA system. Our experiments demonstrate\nour system\u2019s ability to synthesize realistic speech and video\nat a limited data scale.\nReferences\nAdamopoulou, E. and Moussiades, L. Chatbots: History,\ntechnology, and applications. Machine Learning with\nApplications, 2:100006, 2020.\nCasanova, E., Weber, J., Shulby, C. D., Junior, A. C., G\u00a8olge,\nE., and Ponti, M. A. Yourtts: Towards zero-shot multi-\nspeaker tts and zero-shot voice conversion for everyone.\n1The\nlink\nfor\ndemo\nvideo\nis\nhttps://\ngenefaceplusplus.github.io/GeneFace++/ada_\ntta.mp4\nAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis\nIn International Conference on Machine Learning, pp.\n2709\u20132720. PMLR, 2022.\nChen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q.,\nWeng, C., Su, D., Povey, D., Trmal, J., Zhang, J.,\net al. Gigaspeech: An evolving, multi-domain asr corpus\nwith 10,000 hours of transcribed audio. arXiv preprint\narXiv:2106.06909, 2021.\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li,\nJ., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: Large-\nscale self-supervised pre-training for full stack speech\nprocessing. IEEE Journal of Selected Topics in Signal\nProcessing, 16(6):1505\u20131518, 2022.\nEsser, P., Rombach, R., and Ommer, B. Taming transformers\nfor high-resolution image synthesis. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 12873\u201312883, 2021.\nGuo, Y., Chen, K., Liang, S., Liu, Y.-J., Bao, H., and Zhang,\nJ. Ad-nerf: Audio driven neural radiance fields for talking\nhead synthesis. In ICCV, pp. 5784\u20135794, 2021.\nJia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., Ren,\nF., Nguyen, P., Pang, R., Lopez Moreno, I., Wu, Y.,\net al. Transfer learning from speaker verification to mul-\ntispeaker text-to-speech synthesis. Advances in neural\ninformation processing systems, 31, 2018.\nKharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Gir-\ngin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and\nZeghidour, N. Speak, read and prompt: High-fidelity\ntext-to-speech with minimal supervision. arXiv preprint\narXiv:2302.03540, 2023.\nKim, J., Kong, J., and Son, J. Conditional variational au-\ntoencoder with adversarial learning for end-to-end text-\nto-speech. In ICML. PMLR, 2021.\nLiu, X., Xu, Y., Wu, Q., Zhou, H., Wu, W., and Zhou,\nB. Semantic-aware implicit neural audio-driven video\nportrait generation. arXiv preprint arXiv:2201.07786,\n2022.\nMao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., and\nPaul Smolley, S. Least squares generative adversarial\nnetworks. In ICCV, 2017.\nMildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T.,\nRamamoorthi, R., and Ng, R. Nerf: Representing scenes\nas neural radiance fields for view synthesis. In ECCV, pp.\n405\u2013421. Springer, 2020.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. NeurIPS, 2022.\nPrajwal, K., Mukhopadhyay, R., Namboodiri, V. P., and\nJawahar, C. A lip sync expert is all you need for speech\nto lip generation in the wild. In ACM MM, pp. 484\u2013492,\n2020.\nRen, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and\nLiu, T.-Y. Fastspeech: Fast, robust and controllable text\nto speech. Advances in neural information processing\nsystems, 32, 2019.\nRen, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., and\nLiu, T.-Y. Fastspeech 2: Fast and high-quality end-to-end\ntext to speech. arXiv preprint arXiv:2006.04558, 2020.\nRen, Y., Liu, J., and Zhao, Z. Portaspeech: Portable and\nhigh-quality generative text-to-speech. NIPS, 34:13963\u2013\n13974, 2021.\nShen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin,\nT., Zhao, S., and Bian, J. Naturalspeech 2: Latent diffu-\nsion models are natural and zero-shot speech and singing\nsynthesizers. arXiv preprint arXiv:2304.09116, 2023.\nShen, S., Li, W., Zhu, Z., Duan, Y., Zhou, J., and Lu, J.\nLearning dynamic facial radiance fields for few-shot talk-\ning head synthesis. In ECCV, 2022.\nSuwajanakorn, S., Seitz, S. M., and Kemelmacher-\nShlizerman, I. Synthesizing obama: learning lip sync\nfrom audio. ACM Transactions on Graphics (ToG), 36\n(4):1\u201313, 2017.\nTang, J., Wang, K., Zhou, H., Chen, X., He, D., Hu, T.,\nLiu, J., Zeng, G., and Wang, J. Real-time neural radiance\ntalking portrait synthesis via audio-spatial decomposition.\narXiv preprint arXiv:2211.12368, 2022.\nVan Den Oord, A., Vinyals, O., et al. Neural discrete rep-\nresentation learning. Advances in neural information\nprocessing systems, 30, 2017.\nWang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S.,\nChen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec\nlanguage models are zero-shot text to speech synthesizers.\narXiv preprint arXiv:2301.02111, 2023.\nYe, Z., Zhao, Z., Ren, Y., and Wu, F.\nSyntaspeech:\nsyntax-aware generative adversarial text-to-speech. arXiv\npreprint arXiv:2204.11792, 2022.\nYe, Z., He, J., Jiang, Z., Huang, R., Huang, J., Liu, J.,\nRen, Y., Yin, X., Ma, Z., and Zhao, Z. Geneface++:\nGeneralized and stable real-time audio-driven 3d talking\nface generation. arXiv preprint arXiv:2305.00787, 2023a.\nYe, Z., Huang, R., Ren, Y., Jiang, Z., Liu, J., He, J., Yin,\nX., and Zhao, Z. Clapspeech: Learning prosody from\ntext context with contrastive language-audio pre-training.\narXiv preprint arXiv:2305.10763, 2023b.\nAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis\nYe, Z., Jiang, Z., Ren, Y., Liu, J., He, J., and Zhao, Z.\nGeneface: Generalized and high-fidelity audio-driven 3d\ntalking face synthesis. In ICLR, 2023c.\nZhou, H., Sun, Y., Wu, W., Loy, C. C., Wang, X., and Liu,\nZ. Pose-controllable talking face generation by implicitly\nmodularized audio-visual representation. In CVPR, pp.\n4176\u20134186, 2021.\nZhou, Y., Han, X., Shechtman, E., Echevarria, J., Kaloger-\nakis, E., and Li, D. Makelttalk: speaker-aware talking-\nhead animation. ACM Transactions on Graphics (TOG),\n39(6):1\u201315, 2020.\n"
  },
  {
    "title": "Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias",
    "link": "https://arxiv.org/pdf/2306.03509.pdf",
    "upvote": "3",
    "text": "Mega-TTS: Zero-Shot Text-to-Speech at Scale\nwith Intrinsic Inductive Bias\nZiyue Jiang \u2217\u2020 \u2660\u2661\nYi Ren \u2217\u2661\nZhenhui Ye \u2217\u2020 \u2660\u2661\nJinglin Liu \u2661\nChen Zhang \u2020 \u2661\u2660\nQian Yang \u2660\nShengpeng Ji \u2660\nRongjie Huang \u2660\nChunfeng Wang \u2661\nXiang Yin \u2661\nZejun Ma \u2661\nZhou Zhao \u2021 \u2660\n\u2660Zhejiang University & \u2661ByteDance\nziyuejiang@zju.edu.cn, ren.yi@bytedance.com, zhaozhou@zju.edu.cn\nAbstract\nScaling text-to-speech to a large and wild dataset has been proven to be highly ef-\nfective in achieving timbre and speech style generalization, particularly in zero-shot\nTTS. However, previous works usually encode speech into latent using audio codec\nand use autoregressive language models or diffusion models to generate it, which\nignores the intrinsic nature of speech and may lead to inferior or uncontrollable\nresults. We argue that speech can be decomposed into several attributes (e.g.,\ncontent, timbre, prosody, and phase) and each of them should be modeled using\na module with appropriate inductive biases. From this perspective, we carefully\ndesign a novel and large zero-shot TTS system called Mega-TTS, which is trained\nwith large-scale wild data and models different attributes in different ways: 1)\nInstead of using latent encoded by audio codec as the intermediate feature, we still\nchoose spectrogram as it separates the phase and other attributes very well. Phase\ncan be appropriately constructed by the GAN-based vocoder and does not need to\nbe modeled by the language model. 2) We model the timbre using global vectors\nsince timbre is a global attribute that changes slowly over time. 3) We further use\na VQGAN-based acoustic model to generate the spectrogram and a latent code\nlanguage model to fit the distribution of prosody, since prosody changes quickly\nover time in a sentence, and language models can capture both local and long-range\ndependencies. We scale Mega-TTS to multi-domain datasets with 20K hours of\nspeech and evaluate its performance on unseen speakers. Experimental results\ndemonstrate that Mega-TTS surpasses state-of-the-art TTS systems on zero-shot\nTTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, ro-\nbustness, and speaker similarity due to the proper inductive bias of each module.\nAudio samples are available at https://mega-tts.github.io/demo-page.\n1\nIntroduction\nText-to-speech (TTS) synthesis [53, 2, 49, 35, 48, 45, 29, 66, 43, 28] aims to generate human-like\nspeech from text and has gained significant attention in the field of machine learning. Traditional\nTTS systems [13, 11, 60, 8, 21] are usually trained on limited datasets, which impairs their models\u2019\nability to produce diverse and generalizable results. In contrast, large-scale TTS systems [58, 67, 27]\nare trained on tens of thousands of hours of speech data, which significantly improves their zero-shot\ncapability [58, 67]. Current large-scale TTS systems typically encode the speech waveform into latent\n\u2217Equal contribution.\n\u2020Interns at ByteDance.\n\u2021Corresponding author.\nPreprint. Under review.\narXiv:2306.03509v1  [eess.AS]  6 Jun 2023\nTable 1: The illustration of intrinsic properties for different components in human speech.\nModality\nComponents\nIntrinsic Properties\nSuitable for LM\nHuman speech\nPhase\nHighly dynamic, irrelevant to semantics\n%\nTimbre\nGlobal and stable\n%\nProsody\nLong-term dependencies\n!\nRapid changes\nWeak relation with text\nContent\nMonotonic alignment\n%\nwith neural codec models [14] as the intermediate representation and model it with autoregressive\nlanguage models (LM) [58] or diffusion models [50].\nAs presented in Table 1, human speech can be decoupled into several attributes: content, timbre,\nprosody, phase, etc. However, current large-scale TTS systems directly use neural audio codec\nmodels to encode the entire speech into latent and ignore the following intrinsic nature of speech: 1)\nphase is highly dynamic and irrelevant to semantics, which means people are much less sensitive\nto perceive phase than to prosody and timbre, especially for monaural audio. Therefore, only one\nreasonable phase is needed for waveform reconstruction, and it is not necessary to model all possible\nphases. Modeling phase with LM or diffusion model can waste a lot of model parameters since they\nmodel the full distribution of phase4. 2) Timbre should remain stable within the sentence as a global\nvector. Modeling timbre with time-varying latent is costly5. 3) Prosody typically has both local and\nlong-term dependencies and changes rapidly over time with a weak correlation to text, which makes\nconditional phoneme-level LLMs inherently ideal for generating prosody sequences. 4) Content has\nmonotonic alignment with speech while the autoregressive language model cannot guarantee that,\nwhich can lead to repeating or missing word issues [59, 58, 67].\nTo make use of the large and wild training datasets while matching the inductive bias of the model\nand the intrinsic nature of speech, we propose a zero-shot text-to-speech model called Mega-TTS.\nSpecifically, 1) considering the limitations of neural audio codec models, we select mel-spectrogram\nas the intermediate representation to separate the phase and other attributes. We adopt a GAN-based\nvocoder to reconstruct the phase information to improve our model\u2019s efficiency. 2) To model timbre\ninformation, we employ global vectors since timbre is a global attribute that changes slowly over\ntime. We extract the global information from a different speech of the same speaker with the global\nspeaker encoder to decompose the timbre and content information. 3) To capture prosody information\nin a sentence, we adopt a VQGAN-based acoustic model to generate the mel-spectrogram and a\nlatent code language model called P-LLM to fit the distribution of prosody. The P-LLM is capable of\ncapturing both local and long-range dependencies for prosody modeling.\nTo evaluate the zero-shot performance of Mega-TTS, we perform experiments on VCTK [57],\nAISHELL-3 [51] and LibriSpeech test-clean [42] datasets. All of the test speakers are unseen in\nthe training corpus. Our Mega-TTS surpasses the state-of-the-art zero-shot TTS systems [8, 58] in\nterms of speaker similarity, speech naturalness, and generation robustness, which demonstrates the\nsuperiority of introducing appropriate inductive biases. Moreover, Mega-TTS outperforms state-of-\nthe-art models on speech editing [52, 3] and cross-lingual TTS [67] tasks. The main contributions of\nthis work are summarized as follows:\n\u2022 We propose Mega-TTS, a zero-shot text-to-speech system that considers intrinsic inductive biases.\nInstead of using latent encoded by audio codec as the intermediate representation [64, 14, 58], we\ndecompose mel-spectrogram into content, timbre, prosody, and phase attributes and model each of\nthem according to their intrinsic properties.\n4That is why GAN-based vocoders [31] are popular.\n5Our method retains a small portion of time-varying timbre information in the latent code, while the majority\nis represented as the global vector.\n2\n\u2022 We train Mega-TTS on a multi-domain and multi-lingual dataset that contains 20k hours of speech\ndata. It is worth noting that existing large-scale TTS systems [58, 50] are typically trained with\nspeech corpora from audiobooks, while our system is trained on multi-domain speech corpora.\n\u2022 We evaluate Mega-TTS on 3 down-stream speech generation tasks (i.e., zero-shot TTS, speech\nediting, and cross-lingual TTS), demonstrating that Mega-TTS can be applied to various speech\ngeneration tasks. We also propose a novel sampling strategy for speech editing via the discrete\nprosody tokens extracted by Mega-TTS.\n2\nBackground\nIn this section, we briefly overview the background of this work, including zero-shot text-to-speech\n(TTS) and generative models for speech synthesis.\nZero-shot text-to-speech.\nText-to-speech models usually generate mel-spectrogram from text [59,\n2, 35, 48, 29, 47, 36, 22] and then synthesize speech waveform from the generated mel-spectrogram\nusing a separately pre-trained vocoder [41, 31, 62, 20], or directly generate waveform from text in\nan end-to-end manner [45, 15, 30, 37]. For decades, the increasing demand for personalized speech\ngeneration in various applications has posed challenges for TTS models [53], especially in zero-shot\nmulti-speaker scenarios regarding domain shifts. Previous approaches can be categorized into speaker\nadaptation [13, 11, 60, 23] and speaker encoding [25, 1, 26, 61] methods. Traditional works are\ntypically trained on small datasets [11, 23, 21, 8], while some recent works [4, 58, 27, 67] are trained\non large-scale datasets and demonstrate the effectiveness in zero-shot scenarios. These systems utilize\nthe neural audio codec models [64, 14] to convert audio waveform into latent and consider it as the\nintermediate representation for speech generation. Among them, SPEAR-TTS [27] splits the TTS\ntask into two sequence-to-sequence tasks, which enables the training using abundant audio-only data.\nNaturalSpeech 2 [50] uses a text-conditioned diffusion model to generate the latent vectors of the\nneural audio codec model. VALL-E [58, 67] proposes the first neural codec language model for\ntext-to-speech, exhibiting strong in-context learning abilities to overcome challenges in zero-shot\nspeech generation. However, these methods ignore the intrinsic property of speech and may lead to\ninferior or uncontrollable results (e.g., word skipping, repeating, and collapse [58, 67]). Considering\nthe nature of different speech attributes, the autoregressive language model is ideally suitable for\nprosody modeling. ProsoSpeech [46] has proposed to improve the prosody modeling for TTS with\nlatent prosody vectors predicted by a language model. Nevertheless, it lacks the in-context learning\ncapacity, which restricts its application scenarios.\nGenerative models for speech synthesis.\nGenerative models, like language models [4, 33],\nVAE [34, 47], GAN [31, 30], Normalizing flow [39, 29], and diffusion model [32, 24, 43, 22],\nhave been applied to speech or audio synthesis for years. Previous works of autoregressive generative\nmodel mainly aim at waveform generation [41, 18] and continuous acoustic feature generation [59, 49].\nRecently, speech generation systems like AudioLM [4] and VALL-E [58] propose to utilize neural\naudio codec models [64, 14] to convert audio waveform into discrete codes as the intermediate\nrepresentation and design LLMs to generate these codes to achieve speech synthesis. Although good\nreconstruction quality can be achieved by neural audio codec models, they ignore the intrinsic nature\nof speech [14] and may not be suitable to serve as the generator of intermediate representation for\nspeech generation. The encoded latent contains the phase, content, and timbre attributes and language\nmodels are not suitable for predicting these due to the error propagation problem.\n3\nMethod\nTo introduce proper inductive biases into large-scale TTS systems, we propose Mega-TTS, a zero-shot\nTTS system for natural and robust speech generation in various scenarios (i.e., zero-shot prompt-\nbased TTS, speech editing, and cross-lingual TTS). As shown in Figure 1, Mega-TTS consists of a\nVQGAN-based [16] TTS model and a prosody large language model (P-LLM). We carefully model\ndifferent speech attributes in different ways. First, we choose the mel-spectrogram as the intermediate\nrepresentation as it separates the phase from other attributes very well. Secondly, we extract the\nglobal vector from the random previous sentence of the same speaker with the global timbre encoder\nto disentangle the timbre and content information. Finally, we further use a VQGAN-based acoustic\n3\nConv\nStacks\nConv\nStacks\nVector\nQuantization\nPhoneme-level\nPooling\nP-LLM Training\nContent \nEncoder\nMel-Spectrogram\nProsody\nEncoder\nMel\nDecoder\nText\nProsody Code\nTimbre\nEncoder\nReference Mel\nTimbre Latent\nGAN\nDiscriminator\nDP & LR\nP-LLM\nTimbre Latent\n2\n3\nP-LLM\nPrevious\nMel-Spectrogram\n1\n2\n3\n4\nCurrent\nMel-Spectrogram\n5\n6\n7\n...\n1\n4\n5\n6\n7\n8\n...\nContent Latent\n1\n2\n3\n4\n5\n6\n7\n8\n...\nProsody Code\nProsody\nEncoder\nProsody\nEncoder\n(a)  Mega-TTS\n(b)  P-LLM Training\n(c)  Prosody Encoder\nFigure 1: The overall architecture for Mega-TTS. In subfigure (a), P-LLM denotes the prosody\nlarge language model; DP & LR denote the duration predictor and length regulator proposed in\nFastSpeech [48]. In subfigure (b), P-LLM autoregressively predicts the discrete prosody codes.\nmodel to generate the mel-spectrogram and propose a latent code language model called P-LLM to fit\nthe distribution of prosody, since language models are capable of capturing both local and long-range\ndependency. During inference, we propose to use the content from the given text sequence, the timbre\nextracted from the prompt speech, and the prosody predicted by our P-LLM to generate the target\nspeech, which is a novel TTS decoding mechanism called prosody-oriented speech decoding. Finally,\nto demonstrate that our model can be applied to various scenarios, we design inference strategies for\ndownstream tasks. We describe these designs and the training and inference procedures in detail in\nthe following subsections.\n3.1\nDisentangling speech into different components\nTo introduce appropriate inductive biases into different speech attributes, we need to separately\nexpress these attributes and carefully design different architectures for them. The overall model\narchitecture of Mega-TTS is shown in Figure 1. We use three types of encoders to separately encode\ncontent, prosody, and timbre representations. Then we adopt a GAN-based mel-spectrogram decoder\nto generate mel-spectrograms with these representations. We describe the disentangling strategy and\ndetailed design of the proposed encoders as follows.\nDisentangling strategy.\nWe disentangle the mel-spectrogram into content, prosody, and timbre rep-\nresentations with the reconstruction loss of the autoencoder and a carefully designed bottleneck [44]:\n1) we feed the mel-spectrogram into the prosody encoder, and we also introduce carefully-tuned\ndimension reduction and phoneme-level downsampling to the prosody encoder to constrain the infor-\nmation flow; 2) the content encoder encodes the phoneme sequence into the content representation;\n3) we feed the reference mel-spectrogram sampled from a different speech of the same speaker to\ndisentangle the timbre and content information and temporally average the output of the timbre\nencoder to get a one-dimensional global timbre vector. The correctly-designed bottleneck will learn\nto remove the content information and the global timbre information from the output of the prosody\nencoder, which ensures the performance of disentanglement. Due to the limited page space, we put\nmore details about the hyperparameter selection for the information bottleneck in Appendix D.\nArchitecture design of encoders.\n1) The prosody encoder consists of two convolution stacks, a\nphoneme-level pooling layer, and a vector quantization (VQ) bottleneck. The first convolution stacks\ncompress mel-spectrograms into phoneme-level hidden states according to the phoneme boundary\nand the second stacks capture phoneme-level correlations. The vector quantization layer [54] then\nutilizes these hidden states to obtain phoneme-level prosody codes u = {u1, u2, ..., uT } and hidden\nstates Hprosody. To ease the difficulty of disentanglement, only the low-frequency band of the\nmel-spectrogram (the first 20 bins in each mel-spectrogram frame) is used as input, as it contains\nalmost complete prosody and significantly less timbre/content information compared to the full\nband [46]; 2) The content encoder is composed of several feed-forward Transformer layers. To\nachieve the monotonic alignment between the speech content and generated speech, we adopt\n4\nthe duration predictor and length regulator following common practice in non-autoregressive TTS\nsystems [48, 50]. Differently, we feed the prosody information extracted by the prosody encoder to\nthe duration predictor in order to ease the one-to-many mapping problem [48, 45]; 3) The timbre\nencoder is designed to extract a global vector Htimbre that contains the speaker identity of the given\nspeech. The timbre encoder consists of several stacks of convolution layers. To ensure the stability of\ntimbre information across the time axis, we temporally average the output of the timbre encoder to\nget a one-dimensional timbre vector Htimbre.\nTo keep good perceptual quality, we introduce a GAN-based mel-spectrogram decoder. We adopt the\nmulti-length discriminator [10, 63] based on random windows of different lengths as the discriminator.\nOverall, the first-stage training loss L of Mega-TTS can be formulated as:\nLVQ = \u2225yt \u2212 \u02c6yt\u22252 + \u2225sg[E(yt)] \u2212 zq\u22252\n2 + \u2225sg [zq] \u2212 E(yt)\u22252\n2 ,\n(1)\nL = E [LVQ + LAdv] ,\n(2)\nwhere yt is the target speech and \u02c6yt is the generated speech. Lrec = \u2225yt \u2212 \u02c6yt\u22252 is the reconstruction\nloss, sg[\u00b7] denotes the stop-gradient operation, and zq is the temporal collection of codebook entries.\nLVQ is the VQVAE loss function [54, 16] and LAdv is the LSGAN-styled adversarial loss [38] whose\nobjective is to minimize the distribution distance between the predicted mel-spectrograms and the\nground truth mel-spectrograms.\n3.2\nP-LLM\nThe P-LLM is a latent code language model that captures local and long-range dependency for\nprosody modeling. We describe the prosody-oriented speech decoding mechanism and details of the\nP-LLM as follows.\nProsody-oriented speech decoding.\nDenote (yp, xp) and (yt, xt) as the prompt and target speech-\ntranscription pairs. Our goal is to synthesize the high-quality target speech yt given an unseen speech\nprompt yp. During inference, the timbre of the target speech \u02dcHtimbre is expected to be the same as\nthat of the prompt speech. Therefore, to generate the target speech yt, we only need the prosody\ninformation \u02dcu of the target speech. Therefore, the prosody-oriented speech decoding procedure can\nbe formulated as follows:\nEncode : u = Eprosody(yp), Hcontent = Econtent(xp), \u02dcHtimbre = Etimbre(yp),\n\u02dcHcontent = Econtent(xt) ,\nProsody prediction : \u02dcu = f(\u02dcu|u, Hcontent, \u02dcHtimbre, \u02dcHcontent; \u03b8) ,\nDecode : \u02c6yt = D(\u02dcu, \u02dcHtimbre, \u02dcHcontent) ,\n(3)\nwhere Eprosody, Etimbre, Econtent, and D denote the prosody encoder, timbre encoder, content\nencoder, and mel decoder. u is the prosody tokens of the prompt speech, \u02dcu is the predicted prosody\ntokens of the target speech, f is the prosody prediction function, and \u03b8 is the parameter of the P-LLM.\n\u02c6yt is the generated speech.\nGenerating prosody codes.\nThe proposed prosody-oriented speech decoding mechanism requires\nthe predicted prosody codes \u02dcu of the target speech. Leveraging the powerful in-context learning\ncapability of LLMs, we design the P-LLM module to predict \u02dcu. The P-LLM is a decoder-only\ntransformer-based architecture [7] for prosody modeling, which uses prosody codes u from yp as the\nprompt and Hcontent, \u02dcHcontent, and \u02dcHtimbre as the condition. The autoregressive prosody prediction\nprocess of P-LLM can be formulated as:\np\n\u0010\n\u02dcu | u, Hcontent, \u02dcHtimbre, \u02dcHcontent; \u03b8\n\u0011\n=\nT\nY\nt=0\np\n\u0010\n\u02dcut | \u02dcu<t, u, Hcontent, \u02dcHtimbre, \u02dcHcontent; \u03b8\n\u0011\n,\n(4)\nwhere \u03b8 is the parameter of our P-LLM. Since the discrete prosody sequence u is phoneme-level, we\ndirectly concatenate it with Hcontent, \u02dcHcontent, and \u02dcHtimbre as the input. The P-LLM is trained in a\nteacher-forcing mode in the training stage via the cross-entropy loss.\n5\n3\n4\n5\n3\n4\n5\nP-LLM\nPrompt\nMel-Spectrogram\n1\n2\n3\n4\n5\n6\n7\n...\n1\n2\n3\n4\nProsody\nEncoder\n(a)  Zero-Shot TTS Mode\n3\nP-LLM\nCurrent\nMel-Spectrogram\n1\n2\n6\n7\n...\n4\n5\n1\n2\n3\n4\n5\n6\n7\n8\n...\nProsody\nEncoder\n(b)  Speech Editing Mode\nMask\nPrompt\nProbability\nCalculation\nCandidate\nPaths\nTimbre Latent\nContent Latent\nProsody Latent\nMasked Region\nFigure 2: The inference modes for Mega-TTS. In subfigure (a), P-LLM takes information from the\nprompt speech to generate prosody codes for the target speech; In subfigure (b), P-LLM utilizes the\ncontextual information from the masked speech to perform speech editing.\n3.3\nSpeech prompting for inference\nTo facilitate in-context learning for various speech generation tasks, we design different speech\nprompting mechanisms to encourage Mega-TTS to follow the information in the speech prompt.\nInference for TTS.\nFor zero-shot TTS, P-LLM uses u, Hcontent, \u02dcHtimbre, \u02dcHcontent to generate\nthe target prosody codes \u02dcu for the target speech according to Equation 4. We use the top-k random\nsampling scheme [17] to sample the results since we observe that the sampling-based method\ncould increase the diversity of the generated speech. Then, we concatenate the content \u02dcHcontent,\ntimbre \u02dcHtimbre, and prosody \u02dcu information to generate the target speech yt using the mel decoder.\nLeveraging the proper inductive biases and powerful in-context learning capability of our P-LLM,\nthe generated speech can retain not only similar timbre but also the rhythmic habits of the prompt\nspeech. For cross-lingual TTS, u, Hcontent, \u02dcHtimbre, \u02dcHcontent are extracted from the prompt speech\nin a foreign language, and the subsequent procedure keeps the same as that of zero-shot TTS.\nInference for speech editing.\nIn speech editing, the predicted prosody codes should achieve\nsmooth transitions at both the left and right boundaries of the masked region. Previous works like\nEditSpeech [52] propose to perform left and right autoregressive inferences separately and concat\nthe mel-spectrogram at the least L2-norm difference fusion point. However, the L2-norm difference\nof the mel-spectrogram is far from human perception, leading to poor audio naturalness. Since the\nprosody representations in Mega-TTS is discrete, we can solve the transition problem by operating\non discrete prosody representations. First, we regard the area on the left side of the mask as a prompt\nto generate N candidate paths with top-k random sampling strategy. Secondly, the N generated paths\nare used as new prompts to generate the probability matrix of the area on the right side of the mask\nand the ground-truth prosody codes are used to obtain the probabilities of each decoding step from\nthe probability matrix. In the third stage, we sum up the log probabilities of each decoding step for\nthe candidate paths. Finally, we choose the path that achieves the maximum probability in the second\nstep as the predicted result. The decoding strategy for speech editing can be formulated as follows:\nMax\ni\u2208[1,N] Likelihood = Max\ni\u2208[1,N]\nR\nY\nt=L\np\n\u0010\nui\nt | ui\n<t, Hcontent, \u02dcHtimbre, \u02dcHcontent; \u03b8\n\u0011\n\u00b7\nT\nY\nt=R\np\n\u0010\nugt\nt | ui\n<t, Hcontent, \u02dcHtimbre, \u02dcHcontent; \u03b8\n\u0011\n,\n(5)\nwhere L and R are the left and right boundaries of the mask. T is the length of the mel-spectrogram.\nui is the prosody code in the i-th candidate path. ugt\nt is the ground-truth prosody codes. Since our\n6\ndecoding strategy considers the prosody information of the boundaries on both sides, the edited\nregion can achieve smooth transitions.\n4\nExperiments\nIn this section, we present the evaluation results of Mega-TTS and the comparison with baselines in\nterms of the objective and subjective metrics.\n4.1\nExperimental setup\nTraining datasets.\nWe use GigaSpeech [9] and WenetSpeech [65] as the training corpora, which\ncontains 20k hours of multi-domain speeches in English and Chinese in total. Since the speech in\nGigaSpeech and WenetSpeech does not have speaker identities and multiple speakers may appear in a\nspeech clip, we process the datasets with an open-source automatic speaker diarization model6 [6, 5].\nWe also extract the phoneme-level alignments with the external alignment tool7. More information\ncan be found in Appendix A.3.\nEvaluation datasets.\nWe employ two datasets for evaluation: 1) VCTK dataset [57], an English\ndataset that contains 108 speakers; 2) LibriSpeech [42] test-clean, an English dataset that contains 40\nspeakers. For each of these datasets, we randomly sample 10 utterances for each of the 40 speakers,\nresulting in a subset of 400 utterances for evaluation; Specifically, to synthesize each sample, we\nrandomly select a different utterance of the same speaker to form the speech prompt. Note that all\nspeakers in the evaluation datasets are unseen during training.\nModel configuration.\nOur Mega-TTS consists of three encoders, a prosody large language model,\na mel decoder, and a discriminator. The prosody encoder, timbre encoder, and mel generator consist\nof 5 convolutional blocks with 320 hidden size, 5 convolution 1D kernel size. The content encoder is\na 4-layer Transformer [56] with 2 attention heads, 320 embedding dimensions, 1280 1D convolution\nfilter size, and 5 convolution 1D kernel size. The duration predictor is a 3-layer 1D convolution with\nReLU activation and layer normalization, which have 320 hidden size. The discriminator follows\nthe architecture proposed in SyntaSpeech [63]. The P-LLM model is a decoder-only architecture\nthat contains 8 Transformer layers with 8 attention heads, 512 embedding dimensions, 2048 1D\nconvolution filter size, and 5 convolution 1D kernel size. The overall number of model parameters is\n222.5M. We add more detailed model configurations in Appendix A.1.\nTraining and inference.\nIn the training stage, we train Mega-TTS on 8 NVIDIA A100 GPUs, with\na batch size of 30 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98,\n\u03f5 = 10\u22129 and follow the same learning rate schedule in [56]. It takes 320k steps for the VQ-GAN\nTTS model\u2019s training and 100K steps for the P-LLM\u2019s training until convergence. The predicted\nmel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V18 [31]. In the\ninference stage, we use the top-5 random sampling scheme [17] to sample diverse results.\nObjective metrics.\nWe evaluate the pitch distance and speaker similarity for zero-shot TTS. In\nterms of the pitch distance, we compute the average dynamic time warping (DTW) [40] distances\nbetween the pitch contours of ground-truth speech and synthesized speech. And for the cosine speaker\nsimilarity, we use the WavLM model [12] finetuned for speaker verification9 to compute the cosine\nspeaker similarity score between the ground-truth speech and synthesized speech. The similarity\nscore is in the range of [\u22121, 1], where a larger value indicates a higher similarity of input samples. In\naddition, we also evaluate the word error rate (WER) for cross-lingual TTS. We use the ASR system\nfrom the released HuBERT-Large model [19] to transcribe the generated speech into text. Then, the\nWER between the transcribed text and the original target text is measured. We use all samples in the\ntest set for the objective evaluation. We put more information in Appendix A.4 and Appendix A.5.\n6https://huggingface.co/pyannote/speaker-diarization\n7https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\n8https://github.com/jik876/hifi-gan\n9https://huggingface.co/microsoft/wavlm-base-plus-sv\n7\nTable 2: The objective and subjective comparisons for zero-shot text-to-speech synthesis. We evaluate\nthe audio quality, speech prosody, and speaker similarity of different systems on the VCTK and\nLibriSpeech test-clean datasets with 95% confidence intervals.\nDataset\nMethod\nSubjective\nObjective\nMOS-Q (\u2191)\nMOS-P (\u2191)\nMOS-S (\u2191)\nPitch (\u2193)\nSpeaker (\u2191)\nVCTK\nGround Truth\n4.35 \u00b1 0.11\n4.48 \u00b1 0.10\n4.33 \u00b1 0.13\n-\n0.915\nYourTTS [8]\n4.04 \u00b1 0.10\n4.18 \u00b1 0.09\n3.76 \u00b1 0.12\n32.43\n0.847\nMega-TTS\n4.27 \u00b1 0.09\n4.32 \u00b1 0.11\n4.27 \u00b1 0.10\n17.45\n0.877\nLibriSpeech\nGround Truth\n4.23 \u00b1 0.13\n4.49 \u00b1 0.11\n4.29\u00b10.16\n-\n0.956\nYourTTS [8]\n3.83 \u00b1 0.12\n4.06 \u00b1 0.13\n3.22 \u00b1 0.21\n44.05\n0.909\nMega-TTS\n4.08 \u00b1 0.17\n4.21\u00b10.17\n3.90 \u00b1 0.18\n35.46\n0.936\nTable 3: The comparison between Mega-TTS and VALL-E.\nMethod\nCMOS-Q\nCMOS-P\nMOS-S (\u2191)\nVALL-E [58]\n-0.23\n-0.27\n4.06 \u00b1 0.22\nMega-TTS\n0.00\n0.00\n4.11 \u00b1 0.21\nTable 4: The MOS evaluation (\u2191) for speech quality, speech prosody, and speaker similarity on speech\nediting task on the VCTK dataset with 95% confidence intervals.\nMethod\nMOS-Q (\u2191)\nMOS-P (\u2191)\nMOS-S (\u2191)\nEditSpeech [52]\n3.57 \u00b1 0.12\n3.87 \u00b1 0.14\n3.93 \u00b1 0.14\nA3T [3]\n3.73 \u00b1 0.13\n3.96 \u00b1 0.14\n3.97 \u00b1 0.12\nMega-TTS\n3.81 \u00b1 0.14\n4.11 \u00b1 0.14\n4.36 \u00b1 0.16\nSubjective metrics.\nWe conduct the MOS (mean opinion score) and CMOS (comparative mean\nopinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical\nTurk. We keep the text content and prompt speech consistent among different models to exclude\nother interference factors. We randomly choose 50 samples from the test set of each dataset for the\nsubjective evaluation and each audio is listened to by at least 20 testers. We analyze the MOS in\nthree aspects: MOS-Q (Quality: clarity, high-frequency, and original timbre reconstruction), MOS-P\n(Prosody: naturalness of pitch, energy, and duration), and MOS-S (Speaker similarity). We also\nanalyze the CMOS in terms of audio quality and speech prosody. We tell the tester to focus on one\ncorresponding aspect and ignore the other aspect when scoring. We put more information about the\nsubjective evaluation in Appendix A.2.\n4.2\nResults of zero-shot synthesis\nWe compare the zero-shot synthesis performance of Mega-TTS with baseline systems, including:\n1)YourTTS [8], a powerful zero-shot TTS model trained on 1k hours of speech dataset. We use the\nofficial code and released checkpoint10; 2) VALL-E, a large-scale zero-shot TTS model using the\naudio codec model to generate discrete speech codes and LLM to generate them. For VALL-E, we\ndirectly download the first 16 utterances from the VALL-E demo page. The audio samples consist\nof 8 samples from LibriSpeech and 8 samples from VCTK11. As shown in Table 2, Mega-TTS\nsignificantly outperforms YourTTS in terms of audio quality and speech prosody. And in terms of\nspeaker similarity, Mega-TTS significantly outperforms YourTTS with +0.51 MOS-S on VCTK and\n+0.68 MOS-S on LibriSpeech, demonstrating the effectiveness of Mega-TTS in zero-shot scenarios.\nBesides, as shown in Table 3, Mega-TTS outperforms VALL-E in all metrics. It can be seen that\nMega-TTS is able to generate more natural speeches than VALL-E, demonstrating the effectiveness\nof introducing intrinsic inductive biases. To further investigate the performance of disentanglement,\nwe also visualize the distribution of the timbre and prosody representations in Appendix C.\n10https://github.com/Edresson/YourTTS\n11VALL-E does not release its code officially. The unofficial implementations and our implementation are\ndeficient, which would make us difficult to fairly compare our system with VALL-E.\n8\nTable 5: The comparisons for cross-lingual text-to-speech synthesis with 95% confidence intervals.\nWe also measure the word error rate (WER) and speaker similarity score for the objective evaluations.\nMethod\nSubjective\nObjective\nMOS-Q (\u2191)\nMOS-P (\u2191)\nMOS-S (\u2191)\nWER (\u2193)\nSpeaker (\u2191)\nYourTTS [8]\n3.65 \u00b1 0.21\n3.92 \u00b1 0.18\n3.32 \u00b1 0.27\n7.59%\n0.883\nVALL-E X [67]\n3.73 \u00b1 0.17\n3.97 \u00b1 0.18\n3.81 \u00b1 0.16\n-\n-\nMega-TTS\n3.85 \u00b1 0.17\n4.08 \u00b1 0.19\n3.86 \u00b1 0.18\n3.04%\n0.919\nTable 6: The comparison of robustness between Mega-TTS and other systems on the 50 particularly\nhard sentences. Each kind of word error is counted at once per sentence.\nMethod\nRepeats\nSkips\nError Sentences\nError Rate\nTacotron [59]\n10\n16\n22\n44%\nVALL-E [58]\n8\n11\n14\n28%\nFastSpeech [48]\n0\n0\n0\n0%\nMega-TTS\n0\n0\n0\n0%\n4.3\nResults of zero-shot speech editing\nWe compare the quality of generated audio samples of our Mega-TTS with SOTA speech editing\nbaselines, including 1) EditSpeech [52]; 2) A3T [3]. Since the text content of the generated speech has\nbeen edited in the speech editing evaluation, the ground truth is missing. Therefore, we only conduct\nthe subjective evaluation. We manually define modification operations (i.e., insertion, replacement,\nand deletion) of the test samples. We then conduct the experiments on the VCTK dataset. We evaluate\nthe audio quality, speech prosody, and speaker similarity for each audio sample. The results are\npresented Table 4. It can be seen that Mega-TTS achieves the highest perceptual quality, prosody,\nand speaker similarity score, which demonstrates the effectiveness of our proposed speech prompting\nmechanism for speech editing and the powerful in-context learning capability of Mega-TTS.\n4.4\nResults of zero-shot cross-lingual TTS\nTo compare Mega-TTS with the zero-shot cross-lingual TTS models VALL-E X [67], we directly\ndownload the utterances from the VALL-E X demo page, which consists of 6 speech pairs from\nLibriSpeech, EMIME, and AISHELL-3. Since YourTTS [8] is built only for English TTS, we\nevaluate the performance of English TTS with Chinese samples as prompts. The results are listed\nin Table 5. It can be seen that Mega-TTS surpasses VALL-E X in terms of audio quality, speech\nprosody, and speaker similarity scores, which further demonstrates the superiority of introducing\nproper inductive biases to different speech attributes. For objective evaluations, we use all of the text\nsamples in the LibriSpeech test-clean set as the target sentences and randomly select one audio from\nAISHELL-3 as the speech prompt for each target sentence. The results show that Mega-TTS achieves\na significantly lower WER than YourTTS, demonstrating the effectiveness of our method.\n4.5\nResults of robustness evaluation\nTo further evaluate the robustness of the proposed model, we adopt the 50 particularly hard sentences\nfollowing FastSpeech [48]. As shown in Table 6, Tacotron [59] and VALL-E [58] show poor\nrobustness on these complicated sentences. As a comparison, our Mega-TTS shows equivalent\nrobustness to the non-autoregressive models, such as FastSpeech [48], without any repeat or skip\nissues. It can be seen that directly modeling the discrete speech tokens with LLMs like VALL-E [58]\nwould cause robustness issues. As a comparison, Mega-TTS not only leverages the in-context learning\ncapability of LLMs, but also maintains good robustness by introducing the proper inductive bias to\neach speech component.\n9\n5\nConclusion\nIn this paper, we proposed Mega-TTS, which aims to introduce proper inductive biases into large-scale\nzero-shot TTS systems. We disentangle speech into different attributes (i.e., content, timbre, prosody,\nand phase) and model different attributes in different ways. We train Mega-TTS with 20K hours of\nmulti-domain speech data and evaluate its performance on unseen datasets. Our experimental results\non three speech synthesis tasks show that Mega-TTS outperforms state-of-the-art zero-shot TTS\nmodels regarding audio quality, speech prosody, speaker similarity, and robustness. Due to limited\npage space, we discuss the limitations and future works in Appendix F and the broader impacts in\nAppendix G.\nReferences\n[1] Sercan Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning with\na few samples. Advances in neural information processing systems, 31, 2018.\n[2] Sercan \u00d6 Ar\u0131k, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yong-\nguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al. Deep voice: Real-time\nneural text-to-speech. In International conference on machine learning, pages 195\u2013204. PMLR,\n2017.\n[3] He Bai, Renjie Zheng, Junkun Chen, Mingbo Ma, Xintong Li, and Liang Huang.\nA3t:\nAlignment-aware acoustic and text pretraining for speech synthesis and editing. In International\nConference on Machine Learning, pages 1399\u20131411. PMLR, 2022.\n[4] Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt\nSharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a\nlanguage modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022.\n[5] Herv\u00e9 Bredin and Antoine Laurent.\nEnd-to-end speaker segmentation for overlap-aware\nresegmentation. In Proc. Interspeech 2021, 2021.\n[6] Herv\u00e9 Bredin, Ruiqing Yin, Juan Manuel Coria, Gregory Gelly, Pavel Korshunov, Marvin\nLavechin, Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and Marie-Philippe Gill. pyan-\nnote.audio: neural building blocks for speaker diarization. In ICASSP 2020, IEEE International\nConference on Acoustics, Speech, and Signal Processing, 2020.\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[8] Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00f6lge,\nand Moacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion\nfor everyone. In International Conference on Machine Learning, pages 2709\u20132720. PMLR,\n2022.\n[9] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan\nSu, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr\ncorpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021.\n[10] Jiawei Chen, Xu Tan, Jian Luan, Tao Qin, and Tie-Yan Liu. Hifisinger: Towards high-fidelity\nneural singing voice synthesis. arXiv preprint arXiv:2009.01776, 2020.\n[11] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, and Tie-Yan Liu.\nAdaspeech: Adaptive text to speech for custom voice. arXiv preprint arXiv:2103.00993, 2021.\n[12] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li,\nNaoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-\ntraining for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing,\n16(6):1505\u20131518, 2022.\n10\n[13] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan\nWang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample efficient adaptive text-to-speech.\narXiv preprint arXiv:1809.10460, 2018.\n[14] Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\n[15] Jeff Donahue, Sander Dieleman, Mikolaj Binkowski, Erich Elsen, and Karen Simonyan. End-\nto-end adversarial text-to-speech. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12873\u201312883, 2021.\n[17] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 889\u2013898, 2018.\n[18] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It\u2019s raw! audio generation with\nstate-space models. In International Conference on Machine Learning, pages 7616\u20137633.\nPMLR, 2022.\n[19] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed.\nHubert: Self-supervised speech representation learning by\nmasked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 29:3451\u20133460, 2021.\n[20] Rongjie Huang, Max WY Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. Fast-\ndiff: A fast conditional diffusion model for high-quality speech synthesis. arXiv preprint\narXiv:2204.09934, 2022.\n[21] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao.\nGenerspeech: To-\nwards style transfer for generalizable out-of-domain text-to-speech synthesis. arXiv preprint\narXiv:2205.07211, 2022.\n[22] Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. Prodiff:\nProgressive fast diffusion model for high-quality text-to-speech. In Proceedings of the 30th\nACM International Conference on Multimedia, pages 2595\u20132605, 2022.\n[23] Sung-Feng Huang, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, and Hung-yi Lee. Meta-tts:\nMeta-learning for few-shot speaker adaptive text-to-speech. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 30:1558\u20131571, 2022.\n[24] Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim.\nDiff-tts: A denoising diffusion model for text-to-speech. arXiv preprint arXiv:2104.01409,\n2021.\n[25] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming\nPang, Ignacio Lopez Moreno, Yonghui Wu, et al. Transfer learning from speaker verification to\nmultispeaker text-to-speech synthesis. Advances in neural information processing systems, 31,\n2018.\n[26] Minki Kang, Dongchan Min, and Sung Ju Hwang.\nAny-speaker adaptive text-to-speech\nsynthesis with diffusion models. arXiv preprint arXiv:2211.09383, 2022.\n[27] Eugene Kharitonov, Damien Vincent, Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Sertan Girgin, Olivier\nPietquin, Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt:\nHigh-fidelity text-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023.\n[28] Heeseung Kim, Sungwon Kim, and Sungroh Yoon. Guided-tts: A diffusion model for text-\nto-speech via classifier guidance. In International Conference on Machine Learning, pages\n11119\u201311133. PMLR, 2022.\n11\n[29] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-tts: A generative flow\nfor text-to-speech via monotonic alignment search. Advances in Neural Information Processing\nSystems, 33:8067\u20138077, 2020.\n[30] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. In International Conference on Machine Learning,\npages 5530\u20135540. PMLR, 2021.\n[31] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks\nfor efficient and high fidelity speech synthesis. Advances in Neural Information Processing\nSystems, 33:17022\u201317033, 2020.\n[32] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.\n[33] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet,\nDevi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation.\narXiv preprint arXiv:2209.15352, 2022.\n[34] Yoonhyung Lee, Joongbo Shin, and Kyomin Jung. Bidirectional variational inference for\nnon-autoregressive text-to-speech. In International Conference on Learning Representations,\n2021.\n[35] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis\nwith transformer network. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 33, pages 6706\u20136713, 2019.\n[36] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice\nsynthesis via shallow diffusion mechanism. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 11020\u201311028, 2022.\n[37] Yanqing Liu, Ruiqing Xue, Lei He, Xu Tan, and Sheng Zhao. Delightfultts 2: End-to-end speech\nsynthesis with adversarial vector-quantized auto-encoders. arXiv preprint arXiv:2207.04646,\n2022.\n[38] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.\nLeast squares generative adversarial networks. In Proceedings of the IEEE international\nconference on computer vision, pages 2794\u20132802, 2017.\n[39] Chenfeng Miao, Shuang Liang, Minchuan Chen, Jun Ma, Shaojun Wang, and Jing Xiao. Flow-\ntts: A non-autoregressive network for text to speech based on flow. In ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n7209\u20137213. IEEE, 2020.\n[40] Meinard M\u00fcller. Dynamic time warping. Information retrieval for music and motion, pages\n69\u201384, 2007.\n[41] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n[42] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an\nasr corpus based on public domain audio books. In 2015 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), pages 5206\u20135210. IEEE, 2015.\n[43] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-\ntts: A diffusion probabilistic model for text-to-speech. In International Conference on Machine\nLearning, pages 8599\u20138608. PMLR, 2021.\n[44] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. Autovc:\nZero-shot voice style transfer with only autoencoder loss. In International Conference on\nMachine Learning, pages 5210\u20135219. PMLR, 2019.\n12\n[45] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2:\nFast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020.\n[46] Yi Ren, Ming Lei, Zhiying Huang, Shiliang Zhang, Qian Chen, Zhijie Yan, and Zhou Zhao.\nProsospeech: Enhancing prosody with quantized vector pre-training in text-to-speech. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7577\u20137581. IEEE, 2022.\n[47] Yi Ren, Jinglin Liu, and Zhou Zhao. Portaspeech: Portable and high-quality generative text-to-\nspeech. Advances in Neural Information Processing Systems, 34, 2021.\n[48] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech:\nFast, robust and controllable text to speech. Advances in neural information processing systems,\n32, 2019.\n[49] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,\nZhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by\nconditioning wavenet on mel spectrogram predictions. In 2018 IEEE international conference\non acoustics, speech and signal processing (ICASSP), pages 4779\u20134783. IEEE, 2018.\n[50] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and\nJiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and\nsinging synthesizers. arXiv preprint arXiv:2304.09116, 2023.\n[51] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. Aishell-3: A multi-speaker mandarin tts\ncorpus and the baselines. arXiv preprint arXiv:2010.11567, 2020.\n[52] Daxin Tan, Liqun Deng, Yu Ting Yeung, Xin Jiang, Xiao Chen, and Tan Lee. Editspeech: A\ntext based speech editing system using partial inference and bidirectional fusion. In 2021 IEEE\nAutomatic Speech Recognition and Understanding Workshop (ASRU), pages 626\u2013633. IEEE,\n2021.\n[53] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv\npreprint arXiv:2106.15561, 2021.\n[54] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\n[55] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[57] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Superseded-cstr vctk corpus:\nEnglish multi-speaker corpus for cstr voice cloning toolkit. 2016.\n[58] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen,\nYanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text\nto speech synthesizers. arXiv preprint arXiv:2301.02111, 2023.\n[59] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly,\nZongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end\nspeech synthesis. arXiv preprint arXiv:1703.10135, 2017.\n[60] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying\nXiao, Ye Jia, Fei Ren, and Rif A Saurous. Style tokens: Unsupervised style modeling, control\nand transfer in end-to-end speech synthesis. In International Conference on Machine Learning,\npages 5180\u20135189. PMLR, 2018.\n[61] Yihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, and Tie-Yan Liu.\nAdaspeech 4: Adaptive text to speech in zero-shot scenarios. arXiv preprint arXiv:2204.00436,\n2022.\n13\n[62] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel wavegan: A fast waveform gen-\neration model based on generative adversarial networks with multi-resolution spectrogram. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6199\u20136203. IEEE, 2020.\n[63] Zhenhui Ye, Zhou Zhao, Yi Ren, and Fei Wu. Syntaspeech: syntax-aware generative adversarial\ntext-to-speech. arXiv preprint arXiv:2204.11792, 2022.\n[64] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi.\nSoundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 30:495\u2013507, 2021.\n[65] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu,\nXiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: A 10000+ hours multi-domain mandarin\ncorpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 6182\u20136186. IEEE, 2022.\n[66] Chen Zhang, Yi Ren, Xu Tan, Jinglin Liu, Kejun Zhang, Tao Qin, Sheng Zhao, and Tie-Yan Liu.\nDenoispeech: Denoising text to speech with frame-level noise modeling. In ICASSP 2021-2021\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n7063\u20137067. IEEE, 2021.\n[67] Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen,\nYanqing Liu, Huaming Wang, Jinyu Li, et al. Speak foreign languages with your own voice:\nCross-lingual neural codec language modeling. arXiv preprint arXiv:2303.03926, 2023.\n14\nAppendices\nA\nDetailed Experimental Settings\nA.1\nModel Configurations\nWe list the model hyper-parameters of Mega-TTS in Table 7.\nTable 7: Hyperparameters of Mega-TTS models.\nHyper-parameter\nValue\nProsody Encoder\nEncoder Layers\n5\nHidden Size\n320\nConv1D Kernel\n5\nVQ Embedding Size\n2048\nVQ Embedding Channel\n256\nContent Encoder\nPhoneme Embedding Size\n320\nEncoder Layers\n4\nHidden Size\n320\nKernel Size\n5\nFilter Size\n1280\nTimbre Encoder\nEncoder Layers\n5\nHidden Size\n320\nConv1D Kernel\n31\nMel Decoder\nDecoder Layers\n5\nHidden Size\n320\nConv1D Kernel\n5\nP-LLM\nDecoder Layers\n8\nHidden Size\n512\nDecoder Kernel Size\n5\nDecoder channel size\n2048\nProsody Code Embedding Size\n2050\nAttention Headss\n8\nNumber of Contextual Sentences\n7\nMulti-Length Discriminator\nNumber of Discriminators\n3\nWindow Size\n32, 64, 128\nConv2D Layers\n3\nHidden Size\n192\nTotal Number of Parameters\n222.5M\nA.2\nDetails in Subjective Evaluation\nWe perform the audio quality, speech prosody, and speaker similarity evaluations on Amazon\nMechanical Turk (MTurk). For each dataset, we randomly select 50 samples from the test set and\nuse the TTS systems to generate the audio samples. Each audio has been listened to by at least\n20 listeners. For MOS, each tester is asked to evaluate the subjective score of a sentence on a 1-5\nLikert scale. For CMOS, listeners are asked to compare pairs of audio generated by systems A and B,\nindicating which of the two audio they prefer, and choose one of the following scores according to\nthe degree of superiority: 0 indicating no difference, 1 indicating source slightly better, 2 indicating\nsource mostly better and 3 indicating source completely better. For audio quality evaluation (MOS-Q\nand CMOS-Q), we tell listeners to \u201cPlease focus on the audio quality and ignore other factors\u201d. For\nprosody evaluations (MOS-P and CMOS-P), we tell listeners to \u201cPlease focus on the prosody and\nstyle, and ignore the differences of grammar, audio quality, or other factors. \u201d. For speaker similarity\n15\n(a) Screenshot of MOS-Q testing.\n(b) Screenshot of MOS-P testing.\n(c) Screenshot of MOS-S testing.\n(d) Screenshot of CMOS-Q testing.\n(e) Screenshot of CMOS-P testing.\nFigure 3: Screenshots of MOS and CMOS testings for audio quality, speech prosody, and speaker\nsimilarity evaluations.\n16\nevaluations (MOS-S), we tell listeners to \u201c Please focus only on the similarity of the speaker to the\nreference, and ignore the differences of content, grammar, prosody, audio quality, or other factors.\u201d.\nThe screenshots of instructions for testers are shown in Figure 3. We paid $12 to participants hourly\nand totally spent about $1000 on participant compensation. We tell the participants that the data will\nbe used in scientific research.\nA.3\nDetails of Speaker Diarization Model\nTo obtain the speaker information from GigaSpeech and WenetSpeech, we use a released automatic\nspeaker diarization model called pyannote.audio12, which achieves DER=11.24% on the VoxConverse\ndataset and DER=14.09% on the AISHELL-4 dataset. We only assign the speaker ID to the audio\nclip when its activation score is higher than 70% and abandon other audio clips. We also abandon the\naudio clips that contain multiple speakers speaking simultaneously.\nA.4\nDetails of Speaker Similarity Model\nTo measure the speaker similarity, we use the WavLM [12] model finetuned for speaker verification\nfrom https://huggingface.co/microsoft/wavlm-base-plus-sv to extract the speaker em-\nbedding. Then the cosine similarity between the synthesized speech\u2019s speaker embedding and the\nground-truth speech\u2019s speaker embedding is calculated as the speaker similarity score. The WavLM\nmodel is pretrained on 94,000 hours of speech data and finetuned on the VoxCeleb1 dataset using an\nX-Vector head with an Additive Margin Softmax loss, which achieves 0.84%, 0.928%, and 1.758%\nEER (Equal Error Rate) on the Vox1-O, Vox1-E, and Vox1-H trial lists.\nA.5\nDetails of ASR Model\nTo measure the audio quality and speech intelligibility for cross-lingual TTS systems, we evaluate\nthe word error rate (WER) metric. We use the finetuned HuBERT-Large model to transcribe the\nsynthesized speech into text and calculate the WER between the transcribed text and the original\ntarget text. The finetuned HuBERT-Large model from https://huggingface.co/facebook/\nhubert-large-ls960-ft is finetuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and\n3.3% WER on the dev-clean, dev-other, test-clean, and test-other set of Librispeech.\nA.6\nError Bars and Random Seeds\nFor the subjective evaluations, we report confidence intervals of the results of MOS tests in Table 2,\nTable 3, Table 4, and Table 5. For the objective evaluations, we ran the experiments 10 times with 10\ndifferent random seeds ([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained\nthe averaged results.\nB\nVisualizations of Mel-Spectrograms\nWe put more visualizations of mel-spectrograms with different random seeds in Figure 4. We can see\nthat with different random seeds, Mega-TTS can generate diverse results that have different prosody\nand frequency details.\nC\nVisualization of Representations\nTo validate the effectiveness of disentanglement for speech components in Section 3.1, we adopt\nT-SNE [55] to visualize timbre embedding and prosody embedding for unseen speakers on the VCTK\ndataset. We randomly select 10 speakers and directly use the encoders proposed in Section 3.1 to\nextract the timbre and prosody information from their audio samples. The results are shown in\nFigure 5 and Figure 6. It can be seen that the timbre embeddings are ideally located according to\nthe speaker ID. However, the prosody embeddings of different speakers have similar distributions.\nIt shows that our proposed prosody and timbre encoders are able to disentangle the corresponding\nrepresentations from the mel-spectrograms, which further ensures the effectiveness of our P-LLM.\n12https://huggingface.co/pyannote/speaker-diarization\n17\n(a) S=1234\n(b) S=1334\n(c) S=1734\n(d) S=3234\n(e) S=4999\n(f) S=9222\nFigure 4: Visualizations of the mel-spectrograms generated with different random seeds S.\nFigure 5: The T-SNE visualization of timbre embeddings for 10 unseen speakers on the VCTK\ndataset.\nTable 8: The disentanglement performance evaluation for VQGAN-based TTS model with different\nVQ hyperparameters.\nChannel Size * Embedding Size\nPitch (\u2193)\nSpeaker (\u2191)\n64*512\n73.82\n0.719\n256*2048\n49.30\n0.941\n1024*4096\n78.84\n0.707\nD\nHyperparameter Selection for the Information Bottleneck\nIn this section, we describe the details of the hyperparameter selection for the information bottleneck\nproposed in Section 3.1. The information bottleneck of Mega-TTS mainly contains two key hyperpa-\nrameters: the channel size and the embedding size of the vector quantization (VQ) layer. When the\nchannel size and the embedding size are too small or large, the performance of disentanglement will\nbe poor. Therefore, we should carefully select these hyperparameters. We train the VQGAN-based\nTTS models with different VQ hyperparameters and evaluate their pitch distance and speaker similar-\nity following Section 4. Differently, we use the proposed encoders to extract the timbre, content, and\nprosody embeddings of the test samples. Then, we randomly shuffle the timbre embedding sequence\n18\nFigure 6: The T-SNE visualization of prosody embeddings for 10 unseen speakers on the VCTK\ndataset.\nTable 9: The performance of Mega-TTS using different sizes of the training data.\nDataset Usage\nTotal Time (hours)\nPitch (\u2193)\nSpeaker (\u2191)\nDuration (\u2193)\nGiGaSpeech\n10K\n36.50\n0.935\n62.61\nLibriSpeech\n960\n43.90\n0.915\n69.85\nVCTK\n44\n81.33\n0.828\n82.39\nTable 10: The performance of Mega-TTS with different hidden sizes of P-LLM.\nHidden Size of P-LLM\nPitch (\u2193)\nSpeaker (\u2191)\n128\n82.24\n0.917\n256\n71.74\n0.920\n512\n35.46\n0.936\nand reconstruct the mel-spectrogram with the original content, original prosody, and shuffled timbre\ninformation. We calculate the pitch distance between the ground-truth speech and the generated\nspeech, but we calculate the speaker similarity between the shuffled ground-truth speech and the\ngenerated speech. As shown in Table 8, when the channel size is 256 and the embedding size is\n2048, the VQGAN-based TTS model shows the best pitch accuracy and speaker similarity, i.e., the\ndisentanglement performance is the best.\nE\nAblation Studies of Dataset Size and Model Size\nIn this section, we evaluate the influences of the training dataset size and model size on the zero-shot\nTTS task for Mega-TTS. We evaluate the pitch distance, speaker similarity, and the average absolute\nduration error in milliseconds on the LibriSpeech test-clean set. As shown in Table 9, when the\ndataset size grows, the zero-shot performance of Mega-TTS is significantly improved. Moreover,\nfrom Table 10, we can see that when the hidden size of P-LLM grows, the pitch distance significantly\ndrops, demonstrating that the in-context learning capability of P-LLM can be greatly improved by\nthe size of the model.\n19\nF\nLimitations and Future Works\nAlthough achieving superior performance on various zero-shot speech synthesis tasks, Mega-TTS\nstill suffers from two main limitations.\nData coverage.\nAlthough we use 20K hours of multi-domain data for training, our model still\ncannot cover everyone\u2019s voice. Especially for some speakers with extremely heavy accents, our\nmodel cannot imitate their speaking style very well. In the future, we will further scale up the training\ndata to 200K hours to further improve the performance of the model.\nReconstruction Robustness.\nAlthough the reconstruction quality of the proposed VQGAN-based\nTTS model is satisfying on the clean dataset, it will be influenced by the background music or the\nextremely loud reverberation. In future work, we will explore a new model structure that is more\nrobust against the acoustic environment noises.\nG\nBroader Impacts\nMega-TTS improves the quality and efficiency of zero-shot speech synthesis, which makes it easier\nfor people to synthesize personalized speeches. In most cases, people will utilize this technique to\nfacilitate movies, games, podcasts, and other services only. However, it may carry potential risks\nin misuse of the model, such as spoofing voice or other deepfake-related usages. To handle this,\npotential solutions like building a corresponding deepfake detection model should be considered. We\nalso plan to include restrictions in the open-source license of the Mega-TTS project to prevent the\nmisuse of the model.\n20\n"
  },
  {
    "title": "A Static Evaluation of Code Completion by Large Language Models",
    "link": "https://arxiv.org/pdf/2306.03203.pdf",
    "upvote": "3",
    "text": "A Static Evaluation of Code Completion by Large Language Models\nHantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski,\nXiaopeng Li, Murali Krishna Ramanathan, Baishakhi Ray,\nParminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang\nAWS AI Labs\n{dhantian, kuvrun, tiayuche, zijwan, robkwiat, xiaopel\nmkraman, rabaisha, parmib, sudipta, drot, bxiang}@amazon.com\nAbstract\nLarge language models trained on code have\nshown great potential to increase productiv-\nity of software developers. Several execution-\nbased benchmarks have been proposed to eval-\nuate functional correctness of model-generated\ncode on simple programming problems. Never-\ntheless, it is expensive to perform the same eval-\nuation on complex real-world projects consid-\nering the execution cost. On the contrary, static\nanalysis tools such as linters, which can detect\nerrors without running the program, haven\u2019t\nbeen well explored for evaluating code genera-\ntion models. In this work, we propose a static\nevaluation framework to quantify static errors\nin Python code completions, by leveraging Ab-\nstract Syntax Trees. Compared with execution-\nbased evaluation, our method is not only more\nefficient, but also applicable to code in the\nwild. For experiments, we collect code context\nfrom open source repos to generate one mil-\nlion function bodies using public models. Our\nstatic analysis reveals that Undefined Name\nand Unused Variable are the most common er-\nrors among others made by language models.\nThrough extensive studies, we also show the im-\npact of sampling temperature, model size, and\ncontext on static errors in code completions.\n1\nIntroduction\nAutomatic code completion by large language mod-\nels trained on numerous code repositories has\ndemonstrated great potential in accelerating soft-\nware development. Code assistant services pow-\nered by these models provide developers with code\nsuggestions following the current context in real-\ntime. However, it has been shown that about 70%\nof the suggestions are discarded by users in a recent\nstudy (Ziegler et al., 2022). Even worse, mislead-\ning recommendations can lead to failure in complet-\ning programming tasks (Vaithilingam et al., 2022).\nTherefore, it is important to understand the weak-\nness of current code generation models through\ncomprehensive evaluation and analysis.\nFigure 1: A function completion example, with an Un-\nused Variable error (gray) in context, and an Undefined\nName error (red) in completion.\nRecently, execution-based evaluation has be-\ncome increasingly popular, where model-generated\ncode is executed with unit tests to check functional\ncorrectness. Several benchmarks have been pro-\nposed along this direction, such as HumanEval\n(Chen et al., 2021), MBPP (Austin et al., 2021),\nMBXP (Athiwaratkun et al., 2022), CodeContests\n(Li et al., 2022), and DS-1000 (Lai et al., 2022).\nAlthough these benchmarks are highly reliable and\naccurate, they only focus on well-defined algorith-\nmic and data science problems, which do not reflect\nthe need in general software development. Running\nexecution-based evaluation with real-world code-\nbases is, however, prohibitively expensive because\neach project requires a different setup and the com-\nputation cost is potentially unbounded.\nIn contrast to the execution-based approach,\nstatic program analysis (or static analysis) can an-\nalyze programs without executing them. Although\nstatic analysis is usually unable to determine func-\ntional correctness, it covers a large collection of\narXiv:2306.03203v1  [cs.CL]  5 Jun 2023\nstatic error types, such as undefined names or un-\nused variables that are illustrated in Figure 1. More\nimportantly, the analysis can be very fast and does\nnot require any project specific environment setup,\nwhich allows us to evaluate model completions for\ncomplex real-world code at large scale. Static anal-\nysis tools such as linters have been widely used, for\nexample in code editors, to examine human-written\ncode, but their value in evaluating code generation\nmodels has not been well explored yet.\nIn this work, we propose a static evaluation\nframework for Python language. Code snippets\nare first parsed into Abstract Syntax Trees (ASTs)\nand then analyzed by Pyflakes1, a popular static\nanalysis tool for Python. To simulate real-world\nuse cases of auto completion, we collect code from\npublic Github repositories to build a function com-\npletion dataset of 100K problems. In each problem,\nwe randomly mask out a function body in a Python\nfile and ask the model to complete it given the pre-\nceding context up until the function header. We\nthen evaluate public models by sampling 10 com-\npletions for each problem, resulting in one million\ngenerations for each model and sampling tempera-\nture, which will be examined by our static evalua-\ntion pipeline.\nDuring AST parsing, we find most of the errors\narise from incomplete generations that hit the max\nlength limit. Otherwise, models of all sizes perform\nquite well in producing parsable codes. Moving\nforward, Pyflakes analysis reveals that Undefined\nName and Unused Variable are the most prominent\nstatic errors in model-generated code. We also ob-\nserve higher temperatures consistently lead to more\nerrors. Scaling up the model, while able to reduce\nerrors of many types, do not show a clear benefit\nfor preventing undefined names. Through a more\nfine-grained classification, we find larger models\ngenerate fewer undefined variables but more un-\ndefined methods, which add up to a mixed result.\nFinally, we demonstrate that errors in context can\nlead to errors of the same type in generation, which\nis likely a consequence of large language models\u2019\nin context learning capability.\nIn summary, our main contributions include the\nfollowing. (1) We propose a static evaluation frame-\nwork for code completion. (2) Our evaluation on\npublic models reveals common static errors and\nhow they are impacted by various factors such as\ntemperature, model size, and context.\n1https://github.com/PyCQA/pyflakes\n2\nBackground\nCode Generation with Transformers Over re-\ncent years, it has become increasingly popular\nto train Transformer-based language models on\nsource code (Feng et al., 2020; Ahmad et al., 2021;\nWang et al., 2021; Lu et al., 2021; Guo et al.,\n2022) to support software engineering tasks (Iyer\net al., 2018; Tufano et al., 2019). In particular, sev-\neral decoder-only transformer models have been\ndeveloped to facilitate code generation, such as\nCodex (Chen et al., 2021), CodeGen (Nijkamp\net al., 2022), Incoder (Fried et al., 2022), and Al-\nphaCode (Li et al., 2022). These pretrained causal\nlanguage models can be used to predict the contin-\nuation of input code without any finetuning.\nAbstract Syntax Tree An Abstract Syntax Tree\n(a.k.a., AST) is used to represent a source code\nin a concise tree form. By discarding unnecessary\ndetails of the underlying code and its corresponding\nparsed tree, AST only presents the main structural\ncontent of the source code following the language\ngrammar (Aho et al., 2007).\nStatic Analysis Static analysis is a common way\nto detect software bugs without executing the pro-\ngram (Ayewah et al., 2008; Chess and McGraw,\n2004; Chess and West, 2007; Zheng et al., 2006).\nStatic analyzers tend to detect bugs by analyzing\nthe static code text, its AST, documentation, etc.\nThe users usually need to specify the error patterns\nand static analyzers use different AST, graph, and\npath analysis to find those patterns in the code.\nThere are a plethora of static analysis tools and\nthey can detect a wide range of errors depending\non the specified patterns (Emanuelsson and Nils-\nson, 2008). For example, Linter is a popular tool\nthat checks for coding style errors and thus, tries to\nenforce a coding standard (Van Oort et al., 2021).\n3\nThe Function Completion Dataset\nWe introduce the function completion task, which\nis one of the most important use cases of auto com-\npletion services. Given an input code snippet that\nends with a function signature plus an optional doc-\nstring, the model is asked to generate the function\nbody. Previous works on code completion (Lu et al.,\n2021; Svyatkovskiy et al., 2019) have mainly fo-\ncused on single-line completion. However, a single\nline is often too short to reveal models\u2019 capability\nin writing syntactically correct code. We believe\nfunction, as the fundamental building block in most\nprogramming languages, better serves this purpose.\nFigure 2: Evaluation pipeline. Left: We parse [context] and [context + generation] into ASTs. If [context] is not\nparsable, we stop without reporting any error on generation. If [context] is parsable, but [context + generation] is\nnot, we report the AST error in generation. Right: If both are parsable, we run Pyflakes on the trees, which reports\nerrors in [context] and errors in [context + generation]. Taking the difference gives us errors in generation.\nSoftware developers use code generation models\nas black-box services on a diverse set of coding\nprojects. To better simulate the real-world scenario,\nwe build an evaluation set by sampling from pub-\nlic Github repositories. Specifically we collected\npermissively licensed Python code in repositories\nthat were created between April, 2022 and August,\n2022. The selection criterion precludes any chrono-\nlogical overlap between our evaluation data and the\ntraining data of models to be tested in this work.2\nThe collected Python codes are reformatted as\nfunction completion problems. We first use tree-\nsitter3 to parse the whole file to identify all the func-\ntions. Then a function that contains a docstring is\nrandomly selected. The code from the beginning\nof the file up until the end of the docstring is used\nas the context, and the function body is considered\nas the groundtruth. The rest of the file is discarded.\nAt test time, we prompt the model with the con-\ntext part as input, and let the model generate the\nfunction body. We choose only functions with doc-\nstrings so that context is well-defined and model\ncan generate meaningful code completions. We\nfurther select test samples whose context length is\nbetween 64 and 768 tokens, and groundtruth length\nis shorter than 256 tokens, to match our model gen-\neration setting. Our final evaluation set consists of\n100K function completion problems.\n4\nStatic Error Analysis\nWe propose an evaluation pipeline to detect errors\nin function completions generated by models, illus-\ntrated in Figure 2. Suppose the model generates a\ncompletion x given the input context c. We cannot\n2CodeGen models were trained on data up until Oct, 2021.\n3https://tree-sitter.github.io/tree-sitter/\ndirectly analyze x which is partial code without\ncontext. Meanwhile, c may also contain errors es-\npecially in real-world cases. Therefore, we perform\nour analysis in two passes. We first check c for any\nerrors in the input that need to be excluded, and\nthen do another pass on the full code (c, x), the\nconcatenation of the context and model completion.\nAny error that is identified in (c, x) but not in c\nmust arise from x, or in other words, be generated\nby the model. More specifically, we conduct the\nfollowing two steps of analysis for Python code.\n4.1\nAST parsing\nIn the first step, we parse both c and (c, x) into\nabstract syntax trees using Python\u2019s native ast mod-\nule. If the code is parsable, an AST will be returned.\nOtherwise, a syntax error is captured. Based on the\nparsing outcomes, we take the following actions:\n1. If c is not parsable, we are unable to conclude\nany error in generation. Empirically this rarely\nhappens, as we will show in the next section.\n2. If c is parsable but (c, x) is not, then we can\nconfirm the reported syntax error is caused by\nmodel generation. However, notice that only\none error will be returned even if there are mul-\ntiple, due to the nature of AST parsing.\n3. If both c and (c, x) are parsable, there\u2019s no AST\nerror in model generation. The ASTs will be\nused for static analysis in the next step.\n4.2\nStatic analysis with Pyflakes\nIf both c and (c, x) can be parsed into ASTs, we\nperform static analysis using Pyflakes. Pyflakes is a\nstatic analysis tool that checks a Python source file\nfor errors by examining the AST. One advantage\nis that the analysis does not rely on dependencies\nof the source file, which is important given the\ndiversity of packages used in real-world code. We\nrun Pyflakes on c and (c, x) to identify errors in\ncontext and in full code. Errors that are detected in\n(c, x) but not in c are considered as introduced by\nmodel completion.\n5\nExperiments\nWith the proposed pipeline we conduct error analy-\nsis for CodeGen models (Nijkamp et al., 2022) on\nthe test set described in Section 3, and present the\nanalysis results.\n5.1\nExperiment Setup\nWe evaluate CodeGen-mono models of all sizes,\nranging from 350M to 16B. We generate function\ncompletions using nucleus sampling with top-p\n0.95. Sampling temperature is varied between 0.2\nand 0.8 for the 2B model, and fixed to 0.4 for the\nrest models. We sample 10 generations for each\nproblem, which results in one million code comple-\ntions for each model and temperature. The maxi-\nmum generation length is 256 tokens. Generated\ncode completions are then passed to our static eval-\nuation pipeline built with Python 3.8 and Pyflakes\n3.0.1. Evaluating one million generations takes\nonly a few hours on a single CPU thread, and can\nbe fully parallelized for acceleration.\n5.2\nValidation of Model Output\nWhile we mainly focus on static errors in this study,\nit is also important to validate that the models do\ngenerate relevant code. A counter-example would\nbe to generate a single line of \"return\" for every\nfunction signature, which is syntactically correct\nbut not meaningful at all. Towards this end, we\ncalculate the edit similarity between model genera-\ntion and groundtruth, and compare against Pass@1\nfrom HumanEval (Chen et al., 2021) which is a pop-\nular execution-based benchmark to evaluate code\ngeneration models. Specifically, for both datasets\nwe generate 10 samples per problem, and report the\naveraged edit similarity or pass rate over all gen-\nerations. As shown in Table 1, models of all sizes\nand temperatures are able to achieve reasonable\nedit similarity on the function completion dataset,\nwhich means the generations are semantically rel-\nevant. Moreover, edit similarity and HumanEval\nPass@1 both improve as the model scales up, high-\nlighting that model scale is crucial for accurate\nModel\nTemp\nEdit\nSimilarity\nHumanEval\nPass@1\nCodeGen-16B\n0.4\n72.07\n31.83\nCodeGen-6B\n68.76\n26.46\nCodeGen-2B\n64.83\n23.72\nCodeGen-350M\n56.47\n12.62\nCodeGen-2B\n0.2\n65.10\n25.06\n0.4\n64.83\n23.72\n0.6\n64.09\n21.28\n0.8\n62.62\n17.56\nTable 1: Edit similarity on function completion dataset\nand Pass@1 on HumanEval, of CodeGen models across\ndifferent sizes and temperatures. (1) Edit similarity\nand HumanEval Pass@1 are positively correlated across\ndifferent settings, which justifies edit similarity can be\nused as an alternative metric for model evaluation. (2)\nAs expected, larger models have better edit similarity (a\nproxy to accuracy) on function completion task.\ncode generation. Finally, the strong positive corre-\nlation between the last two columns shows that edit\nsimilarity on the function completion dataset can be\nused as an alternative metric for model comparison.\n5.3\nAST Results\nWe run AST parsing and find there are only 0.42%\ncases with unparsable context that need to be dis-\ncarded. For the rest, we report percentage of gen-\nerations with AST errors in Table 2. A full list of\nerror types is included in Appendix A. For each\ntype, we also show a code example in Appendix B.\nWhile there are about 7-8% of unparsable gener-\nations, most of the parsing errors happen at the end\nof file (EOF), which means the generated code is\nincomplete due to the 256 max token limit. Extend-\ning generation length may help reduce EOF errors,\nbut will require more computation and increase the\nperceived latency of the auto-completion service.\nOn the other hand, non-EOF errors only account\nfor a tiny fraction, usually around 0.1-0.2%, which\nindicates CodeGen models can generally follow\nthe abstract syntax grammar to produce parsable\ncodes, regardless of model size and temperature.\nFinding 1. Codes generated by models, unless in-\ncomplete, are mostly parsable into ASTs, regardless\nof model size or temperature.\nWe also show the top-3 non-EOF error types\nranked by frequency, which are Invalid syntax,\nPrint Missing Parentheses, and Keyword Argu-\nment Repeated. Notably, the first two categories\nare often related to Python\u2019s interpreter version.\nTo illustrate, Python2-style print like print \"abc\"\nModel\nTemp\nTotal\nEOF\nNon EOF\nInvalid\nSyntax\n\"print\"\nMissing\nParentheses\nKeyword\nArgument\nRepeated\nCodeGen-16B\n0.4\n7.330%\n7.236%\n0.094%\n0.042%\n0.041%\n0.004%\nCodeGen-6B\n7.446%\n7.253%\n0.193%\n0.081%\n0.094%\n0.006%\nCodeGen-2B\n7.272%\n7.177%\n0.095%\n0.052%\n0.018%\n0.008%\nCodeGen-350M\n8.703%\n8.593%\n0.110%\n0.041%\n0.016%\n0.028%\nCodeGen-2B\n0.2\n8.067%\n7.982%\n0.085%\n0.045%\n0.018%\n0.008%\n0.4\n7.272%\n7.177%\n0.095%\n0.052%\n0.018%\n0.008%\n0.6\n6.823%\n6.713%\n0.110%\n0.060%\n0.020%\n0.008%\n0.8\n7.496%\n7.337%\n0.159%\n0.085%\n0.029%\n0.014%\nTable 2: Percentages of AST errors across different model sizes and temperatures. We show (1) total AST errors;\n(2) errors at the end of file (EOF); (3) errors not at EOF; (4) top 3 non-EOF errors. Models generally perform well\nat AST level except for EOF errors caused by max generation length limit.\nFigure 3: Number of undefined variables versus unde-\nfined functions. Larger models generate more undefined\nfunctions but fewer undefined variables.\nwill lead to Print Missing Parentheses in Python3.\nAnother example is that using async as a variable\nname will cause Invalid Syntax because async has\nbecome a reserved word since Python3.7. Models\nlearn to make such errors from their training data\nwhich consists of code written for different Python\nversions. In many cases, it is difficult for a model to\ninfer the intended interpreter version directly from\nthe limited context. An interesting future direction\nis to guide models to generate version-compatible\ncode given the target environment.\nFinding 2. Interpreter version mismatch is one of\nthe major reasons for non-EOF AST errors.\n5.4\nPyflakes Results\nWe present frequencies of top 6 linter errors from\nPyflakes in Table 3, with code examples in Ap-\npendix B. While Pyflakes also finds other problems\nin code, most of them are very sparse and thus less\nimportant, which we leave to Appendix A. Notice\nthat one code snippet may contain multiple errors.\nWe count each type only once in every test sample.\nAmong all errors, Undefined Name and Un-\nused Variable are the most common ones, where\nthe model either calls a variable that is not defined,\nor defines a variable but never uses it. Closely\nrelated are Unused Import, Redefined While Un-\nused and Undefined Local, which can be consid-\nered as special cases of the first two. Models also\nsometimes unnecessarily use f-strings by not giv-\ning any placeholder. It is worth pointing out that\nnot all Pyflakes errors will impact execution. In\nfact among the six types, only Undefined Name\nand Undefined Local may cause runtime problems.\nHowever, all these errors can harm readability and\nmaintenance which are critical for software devel-\nopment. Hence, it is important to address them to\nimprove the quality of auto code completion.\nAcross sampling temperatures, we observe in\nevery column that more errors are generated under\nhigher temperatures, which is expected because\ngenerations in such cases are less confident.\nFinding 3. Higher temperature always leads to\nmore errors of every type.\nThe impact of model size on error rate is less\nconsistent though. For Unused Variable, Unused\nImport, and Undefined Local, error rate does de-\ncrease as the model scales up. However, the other\nthree categories do not manifest such correlation.\nWe investigate the underlying reason for this mixed\nresult particularly in the case of Undefined Name.\nNotice that if an undefined name is a function call,\nit can potentially be defined afterwards outside the\ncurrent function completion scope. While not guar-\nanteed, the model might be able to fix this error by\nitself if we allow generating longer code instead\nof only one function. In contrast, using a vari-\nable without first defining it is usually a mistake.\nEven in some rare cases where the variable defi-\nnition is made up correctly after the usage, such\nordering is often less preferred in terms of coding\nModel\nTemp\nUndefined\nName\nUnused\nVariable\nFString\nMissing\nPlaceholders\nUnused\nImport\nRedefined\nWhile\nUnused\nUndefined\nLocal\nCodeGen-16B\n0.4\n4.323%\n1.729%\n0.135%\n0.107%\n0.131%\n0.047%\nCodeGen-6B\n4.374%\n1.775%\n0.089%\n0.149%\n0.126%\n0.055%\nCodeGen-2B\n4.364%\n1.810%\n0.147%\n0.150%\n0.146%\n0.065%\nCodeGen-350M\n4.472%\n2.032%\n0.151%\n0.173%\n0.155%\n0.095%\nCodeGen-2B\n0.2\n4.206%\n1.751%\n0.125%\n0.139%\n0.139%\n0.067%\n0.4\n4.364%\n1.810%\n0.147%\n0.150%\n0.146%\n0.065%\n0.6\n4.711%\n2.000%\n0.188%\n0.170%\n0.159%\n0.076%\n0.8\n5.377%\n2.490%\n0.240%\n0.247%\n0.184%\n0.086%\nTable 3: Percentages of Pyflakes errors across different model sizes and temperatures. Higher temperatures always\nlead to more errors in every category. On the other hand, larger models do not necessarily generate fewer errors.\nstyle. In Figure 3, we break down the undefined\nnames into variables and functions. We find that\nlarger models yield fewer undefined variables, but\nmore undefined functions, which demonstrates that\nthe correlation between error count and model size\nvaries for different errors types.\nFinding 4. While larger models are more accurate\ncode generators (Nijkamp et al., 2022), scaling\nup model size does not lead to reduction in error\ncounts for all error categories.\n5.5\nCorrelation with Errors in Context\nWe further study the correlation between errors in\ncontext and in generation. Denote by c the input\ncontext, x the model generation, e the error type.\nWe write e \u2208 c to mean c contains an error of type e.\nFor every e,4 we calculate P(e \u2208 x|e \u2208 c), the gen-\neration error rate when context contains the same\ntype of error(s). We also report the relative ratio\nP(e\u2208x|e\u2208c)\nP(e\u2208x|e/\u2208c) to measure the impact of context. From\nTable 4, if the model observes errors in context, it\nis more likely to produce the same type of errors in\ngeneration, and the error rate can be amplified by\n7\u223c200 times depending on the type. This is pos-\nsibly an undesired consequence of the in-context\nlearning capability of large language models.\nWe also calculate P(e \u2208 c|e \u2208 x) to show how\nmany of the generation errors co-occur with con-\ntext errors. As indicated by the last column of Ta-\nble 4, even though context errors can significantly\namplify generations errors, the co-occurrences of\ntwo do not account for a large fraction. This im-\nplies problematic context is not the only factor for\nproblematic generation, and it is often the case for\nmodels to produce errors even with correct context.\n4We omit Unused Import from Table 3 because it is valid to\nhave unused imports in the context that is yet to be completed.\nError type\nP(e\u2208x|e\u2208c) P(e\u2208x|e\u2208c)\nP(e\u2208x|e/\u2208c) P(e\u2208c|e\u2208x)\nUndefined Name\n26.33%\n7.80\n25.99%\nUnused Variable\n14.13%\n8.45\n8.56%\nFString Missing\nPlaceholders\n20.63%\n215.50\n35.08%\nRedefined\nWhile Unused\n2.44%\n21.16\n22.30%\nUndefined Local\n7.00%\n108.68\n1.08%\nTable 4: Correlation between errors in context and in\ngeneration for the 2B model. First two columns indicate\nerrors in context can amplify errors in generation; the\nlast column shows not all generations errors can be\nattributed to context. Other models have similar results.\nFinding 5. Errors in context generally lead to more\nerrors in generation.\n6\nDiscussion\nWe present a static evaluation framework for code\ncompletions generated by large language models.\nBy utilizing the proposed framework, we conduct\nerror analysis of CodeGen models on a large scale\nreal-world Python evaluation set. Our experiment\nreveals common static errors made by pretrained\nmodels, as well as their frequency trend across\nmodel sizes and sampling temperatures. By point-\ning out weaknesses of existing models, we hope our\nstudy also sheds light on future directions towards\nmore accurate code generation.\nThere are a few limitations of this study. First,\nwe focus on left-to-right code generation without\nconsidering right-side and cross-file context, which\ncan be used to determine broader categories of\nerrors with improved precision. Second, each static\nanalysis tool has its own limitations. Thus, the\npresented analysis is limited by Pyflakes\u2019s accuracy\nand coverage to detect certain code issues.\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655\u20132668,\nOnline. Association for Computational Linguistics.\nAlfred V Aho, Ravi Sethi, and Jeffrey D Ullman. 2007.\nCompilers: principles, techniques, and tools, vol-\nume 2. Addison-wesley Reading.\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang,\nXiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin\nAhmad, Shiqi Wang, Qing Sun, Mingyue Shang, Su-\njan Kumar Gonugondla, Hantian Ding, Varun Ku-\nmar, Nathan Fulton, Arash Farahani, Siddhartha Jain,\nRobert Giaquinto, Haifeng Qian, Murali Krishna Ra-\nmanathan, Ramesh Nallapati, Baishakhi Ray, Parmin-\nder Bhatia, Sudipta Sengupta, Dan Roth, and Bing\nXiang. 2022. Multi-lingual evaluation of code gener-\nation models. CoRR, abs/2210.14868.\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le,\nand Charles Sutton. 2021. Program synthesis with\nlarge language models. CoRR, abs/2108.07732.\nNathaniel Ayewah, William Pugh, David Hovemeyer,\nJ David Morgenthaler, and John Penix. 2008. Using\nstatic analysis to find bugs. IEEE software, 25(5):22\u2013\n29.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nBrian Chess and Gary McGraw. 2004. Static analysis\nfor security. IEEE security & privacy, 2(6):76\u201379.\nBrian Chess and Jacob West. 2007. Secure program-\nming with static analysis. Pearson Education.\nP\u00e4r Emanuelsson and Ulf Nilsson. 2008. A comparative\nstudy of industrial static analysis tools. Electronic\nnotes in theoretical computer science, 217:5\u201321.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536\u20131547, Online. Association for Computational\nLinguistics.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\nCoRR, abs/2204.05999.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. UniXcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212\u20137225, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1643\u20131652, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang,\nRuiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih,\nDaniel Fried, Sida I. Wang, and Tao Yu. 2022. DS-\n1000: A natural and reliable benchmark for data sci-\nence code generation. CoRR, abs/2211.11501.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de Mas-\nson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with alpha-\ncode. CoRR, abs/2203.07814.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-\ndaresan, Shao Kun Deng, Shengyu Fu, and Shujie\nLiu. 2021. Codexglue: A machine learning bench-\nmark dataset for code understanding and generation.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\narXiv preprint.\nAlexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel\nSundaresan. 2019. Pythia: Ai-assisted code com-\npletion system. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, KDD 2019, Anchorage,\nAK, USA, August 4-8, 2019, pages 2727\u20132735. ACM.\nMichele Tufano, Cody Watson, Gabriele Bavota, Massi-\nmiliano Di Penta, Martin White, and Denys Poshy-\nvanyk. 2019. An empirical study on learning bug-\nfixing patches in the wild via neural machine transla-\ntion. ACM Trans. Softw. Eng. Methodol., 28(4).\nPriyan Vaithilingam, Tianyi Zhang, and Elena L. Glass-\nman. 2022. Expectation vs. experience: Evaluating\nthe usability of code generation tools powered by\nlarge language models. In Extended Abstracts of the\n2022 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA \u201922, New York, NY, USA.\nAssociation for Computing Machinery.\nBart Van Oort, Lu\u00eds Cruz, Maur\u00edcio Aniche, and Arie\nVan Deursen. 2021. The prevalence of code smells\nin machine learning projects. In 2021 IEEE/ACM 1st\nWorkshop on AI Engineering-Software Engineering\nfor AI (WAIN), pages 1\u20138. IEEE.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation.\nIn Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696\u20138708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJiang Zheng, Laurie Williams, Nachiappan Nagappan,\nWill Snipes, John P Hudepohl, and Mladen A Vouk.\n2006. On the value of static analysis for fault de-\ntection in software. IEEE transactions on software\nengineering, 32(4):240\u2013253.\nAlbert Ziegler, Eirini Kalliamvakou, Shawn Simister,\nGanesh Sittampalam, Alice Li, Andrew Rice, Devon\nRifkin, and Edward Aftandilian. 2022. Productivity\nassessment of neural code completion.\nA\nFull Error Categories\nIn addition to those discussed in Section 5, we list\nall error categories that can be detected in model\ngenerated code in our experiments, with a minimal\nfrequency of 0.001% by any of the models (i.e. 10\nobservations out of the total 1 million generations).\nAST errors (EOF errors indicated by aster-\nisk):\n1. *unexpected EOF while parsing\n2. *EOL while scanning string literal\n3. *invalid syntax at EOF\n4. *EOF while scanning triple-quoted string lit-\neral\n5. invalid syntax not at EOF\n6. missing parentheses in call to \"print\"\n7. keyword argument repeated\n8. leading zeros in decimal integer literals are not\npermitted; use an o prefix for octal integers\n9. unmatched \")\"\n10. cannot assign to function call\n11. positional argument follows keyword argu-\nment\n12. expression cannot contain assignment\nPyflakes issues:\n1. undefined name\n2. unused variable\n3. f-string missing placeholder\n4. unused import\n5. redefined while unused\n6. indentation error\n7. import shadowed by loop var\n8. raise not implemented\n9. invalid print syntax\n10. is literal\n11. string dot format extra positional argument\n12. multi value repeated key literal\n13. percent format positional count mismatch\n14. tab error\n15. string dot format extra named arguments\n16. import star not permitted\n17. percent format unsupported format character\n18. assert tuple\n19. percent format extra named arguments\nB\nExamples for Top Error Types\nBelow we list one code example for each of the\nerror categories shown in Table 2 and 3. Following\nthe definition of function completion task, in every\nexample, context is from the beginning until the\nend of the docstring of the last function, and model\ncompletion is the body of the last function.\n1 \"\"\" Secondary Structure dataset.\"\"\"\n2\n3 import numpy as np\n4 from megatron import print_rank_0\n5 from .data import ProteinPredictionAbstractDataset\n6 from .data import build_tokens_paddings_from_text\n7\n8 class SecondaryStructureDataset(ProteinPredictionAbstractDataset):\n9\ndef __init__(self ,\n10\nname: str ,\n11\ndatapaths ,\n12\ntokenizer ,\n13\nmax_seq_length: int):\n14\nsuper().__init__(\u2019secondary_structure \u2019, name , datapaths , tokenizer ,\nmax_seq_length)\n15\n16\n17\ndef build_samples(self , ids , paddings , label , unique_id , seq_len):\n18\n\"\"\" Convert to numpy and return a sample consumed by the batch producer.\n\"\"\"\n19\n20\n# Seperate inputs and labels in lists\n21\ninput_tokens = [self.tokenizer.tokenize(seq) for seq in ids]\n22\ninput_tokens = [token for seq in input_tokens for token in seq]\n23\nlabels = [self.tokenizer.tokenize(seq) for seq in label]\n24\nlabels = [label for seq in labels for label in seq]\n25\n26\n# Add special tokens\n27\ninput_tokens , labels = self.add_special_tokens(input_tokens , labels)\n28\n29\n# Truncate and pad\n30\ninput_tokens , labels , paddings = self.truncate_and_pad(input_tokens ,\nlabels ,\n31\nself\nListing 1: unexpected EOF while parsing (line 31)\n1 from pathlib import Path\n2 from wasabi import msg\n3 from .remote_storage import RemoteStorage\n4 from .remote_storage import get_content_hash , get_command_hash\n5 from .. _util import load_project_config\n6 from .. _util import project_cli , Arg , logger\n7\n8\n9 @project_cli.command(\"push\")\n10 def project_push_cli(\n11\n# fmt: off\n12\nremote: str = Arg(\"default\", help=\"Name or path of remote storage\"),\n13\nproject_dir: Path = Arg(Path.cwd(), help=\"Location of project directory.\nDefaults to current working directory.\", exists=True , file_okay=False),\n14\n# fmt: on\n15 ):\n16\n\"\"\" Persist outputs to a remote storage. You can alias remotes in your\n17\nproject.yml by mapping them to storage paths. A storage can be anything that\n18\nthe smart -open library can upload to , e.g. AWS , Google Cloud Storage , SSH ,\n19\nlocal directories etc.\n20\n21\nDOCS: https ://xxx\n22\n\"\"\"\n23\nfor nlp in load_project_config(project_dir , {\"directories\": [ANIMAL_TRAIN_DIR\n]}:\n24\nremote_storage = RemoteStorage.get(remote)\n25\nfor command in [\"train\"]:\n26\nlogger.debug(f\"Uploading {command} to remote storage \u2019{remote_storage}\u2019\")\n27\npath = Path(project_dir) / ANIMAL_TRAIN_DIR / command\n28\nupload_project(remote_storage , path)\nListing 2: invalid syntax (line 23)\n1 def prod(F, E):\n2\n\"\"\" Check that the factorization of P-1 is correct. F is the list of\n3\nfactors of P-1, E lists the number of occurrences of each factor.\"\"\"\n4\nM = prod_of_prime_factors(F, E)\n5\nif not all(i == 1 for i in M):\n6\nprint \"Error in prod\"\n7\nprint F, E\n8\nreturn\n9\nP = product(F)\n10\nP_1 = 1\n11\nfor i in range(len(F)):\n12\nP_1 *= F[i]**E[i]\n13\nif P != P_1:\n14\nprint \"Error in prod\"\n15\nprint F, E\n16\nprint P\n17\nprint P_1\n18\nreturn\nListing 3: Missing parentheses in call to \"print\" (line 6)\n1 import unittest\n2 from datetime import datetime , timezone\n3\n4 from dateutil.relativedelta import relativedelta\n5\n6 from bot.utils import time\n7\n8\n9 class TimeTests(unittest.TestCase):\n10\n\"\"\" Test helper functions in bot.utils.time.\"\"\"\n11\n12\ndef test_humanize_delta_handle_unknown_units(self):\n13\n\"\"\" humanize_delta should be able to handle unknown units , and will not\nabort.\"\"\"\n14\nself.assertEqual(\n15\ntime.humanize_delta(datetime.utcnow (), datetime.utcnow () -\nrelativedelta(months=1, months =2)),\n16\n\"1 month and 2 months\"\n17\n)\nListing 4: keyword argument repeated (line 15)\n1 \"\"\"\n2 This program will continually ask our user to give a number\n3 and will calculate the factorial result of the number and print it on the console\n.\n4\n5 The program ends when the user enter the EXIT number.\n6 \"\"\"\n7\n8 EXIT = -100\n9\n10\n11 def main():\n12\n\"\"\"\n13\nThis program will calculate the factorial result according to the number an\nuser\n14\ninputs.\n15\n\"\"\"\n16\nprint(\u2019<<< Welcome to the Factorial Calculator! >>>\u2019)\n17\nnum = int(input(\u2019Enter a number: \u2019))\n18\nprint(\u2019The factorial of {} is {}.\u2019.format(num , factorial(num)))\n19\nif num == EXIT:\n20\nprint(\u2019\\n<<< Thank you for using the Factorial Calculator. >>>\u2019)\n21\nelse:\n22\nmain()\nListing 5: undefined name \"factorial\" (line 18)\n1 def check(full_path , encoding):\n2\nassert type(full_path) == str , f\u2019\\\u2019full_path\\\u2019 is of {type(full_path)}. Only\ntype \\\u2019str\\\u2019 is acceptable.\u2019\n3\nassert full_path != \"\", \"\\\u2019full_path\\\u2019 is empty.\"\n4\nassert type(encoding) == str , f\u2019\\\u2019full_path\\\u2019 is of {type(encoding)}. Only\ntype \\\u2019str\\\u2019 is acceptable.\u2019\n5\nassert encoding != \"\", \"\\\u2019encoding\\\u2019 is empty.\"\n6\n7 def file_read(full_path: str , encoding = \"utf8\"):\n8\n\u2019\u2019\u2019\n9\nAuthor: xxx\n10\n11\nReads file at \"full_path\" and returns its data in a list.\n12\n\u2019\u2019\u2019\n13\n14\ncheck(full_path , encoding)\n15\nencoding_check = encoding\n16\nfull_path = full_path.strip()\n17\nf = open(full_path , \"r\", encoding = encoding)\n18\nlines = f.readlines ()\n19\nf.close()\n20\nlines = [line.replace(\"\\n\", \"\") for line in lines]\n21\nreturn lines\nListing 6: local variable \"encoding_check\" is assigned to but never used (line 15)\n1 import os\n2 import json\n3\n4 from convinse.library.utils import store_json_with_mkdir , get_logger\n5\n6\n7 class HeterogeneousAnswering:\n8\ndef __init__(self , config):\n9\n\"\"\" Initialize HA module.\"\"\"\n10\nself.config = config\n11\nself.logger = get_logger(__name__ , config)\n12\n13\ndef train(self , sources =[\"kb\", \"text\", \"table\", \"info\"]):\n14\n\"\"\" Method used in case no training required for HA phase. \"\"\"\n15\nself.logger.info(f\"No need to train.\")\n16\npass\nListing 7: f-string is missing placeholders (line 15)\n1 import os\n2 import urllib.parse\n3 import sqlite3\n4\n5 SQL = \"\"\"\n6 SELECT p.ZAUTHOR , p.ZTITLE , e.ZTITLE , e.ZASSETURL , e.ZPUBDATE\n7 from ZMTEPISODE e\n8 join ZMTPODCAST p\n9\non e.ZPODCASTUUID = p.ZUUID\n10 where ZASSETURL NOTNULL;\n11 \"\"\"\n12\n13\n14 def check_imports ():\n15\n\u2019\u2019\u2019 Prompts for password to install dependencies , if needed \u2019\u2019\u2019\n16\nimport os, importlib , importlib.util\n17\nimport urllib.parse\n18\n19\n# Check for dependency installs\n20\n# Can be done more simply , but this way I can avoid importing anything from\nzmodel ,\n21\n# which is nice since I can see what\u2019s going on.\n22\nfor k, v in DEPS.items():\n23\ntry:\n24\nimportlib.import_module(k)\n25\nexcept ImportError as e:\n26\nimportlib.util.find_spec(k)\n27\nif importlib.util.find_spec(k) is None:\n28\nos.system(f\u2019pip install {v}\u2019)\nListing 8: \"urllib.parse\" imported but unused (line 17)\n1 import kfp.deprecated as kfp\n2 from kfp.deprecated import components , dsl , compiler\n3\n4 def get_run_info(run_id: str):\n5\n\"\"\" Example of getting run info for current pipeline run.\"\"\"\n6\nimport kfp.dsl as dsl\n7\nclient = kfp.Client ()\n8\nrun = client.run_details(run_id)\n9\nprint(f\"Run details :\\n{run}\")\n10\nprint(f\"Pipeline details :\\n{run.pipeline_runtime}\")\nListing 9: redefinition of unused \"dsl\" from line 2 (line 6)\n1 \"\"\" Check for nonlocal and used -before -assignment \"\"\"\n2 # pylint: disable=missing -docstring , unused -variable , no -init , too -few -public -\nmethods\n3\n4 __revision__ = 0\n5\n6 def test_ok ():\n7\n\"\"\" uses nonlocal \"\"\"\n8\ncnt = 1\n9\ndef wrap():\n10\nnonlocal cnt\n11\ncnt = cnt + 1\n12\nwrap()\n13\n14 def test_fail ():\n15\n\"\"\" doesn\u2019t use nonlocal \"\"\"\n16\ncnt = 1\n17\ndef wrap():\n18\ncnt = cnt + 1 # [used -before -assignment]\n19\nwrap()\nListing 10: local variable \"cnt\" defined in enclosing scope on line 16 referenced before assignment (line 18)\n"
  },
  {
    "title": "Deductive Verification of Chain-of-Thought Reasoning",
    "link": "https://arxiv.org/pdf/2306.03872.pdf",
    "upvote": "3",
    "text": "Deductive Verification of Chain-of-Thought Reasoning\nZhan Ling1\u2217 Yunhao Fang1\u2217 Xuanlin Li1\nZhiao Huang1\nMingu Lee2\nRoland Memisevic2\nHao Su1\n1UC San Diego, 2Qualcomm AI Research\u2020\nAbstract\nLarge Language Models (LLMs) significantly benefit from Chain-of-Thought\n(CoT) prompting in performing various reasoning tasks. While CoT allows models\nto produce more comprehensive reasoning processes, its emphasis on intermediate\nreasoning steps can inadvertently introduce hallucinations and accumulated errors,\nthereby limiting models\u2019 ability to solve complex reasoning tasks. Inspired by how\nhumans engage in careful and meticulous deductive logical reasoning processes\nto solve tasks, we seek to enable language models to perform explicit and rigor-\nous deductive reasoning, and also ensure the trustworthiness of their reasoning\nprocess through self-verification. However, directly verifying the validity of an\nentire deductive reasoning process is challenging, even with advanced models\nlike ChatGPT. In light of this, we propose to decompose a reasoning verification\nprocess into a series of step-by-step subprocesses, each only receiving their nec-\nessary context and premises. To facilitate this procedure, we propose Natural\nProgram, a natural language-based deductive reasoning format. Our approach\nenables models to generate precise reasoning steps where subsequent steps are\nmore rigorously grounded on prior steps. It also empowers language models to\ncarry out reasoning self-verification in a step-by-step manner. By integrating this\nverification process into each deductive reasoning stage, we significantly enhance\nthe rigor and trustfulness of generated reasoning steps. Along this process, we also\nimprove the answer correctness on complex reasoning tasks. Code will be released\nat https://github.com/lz1oceani/verify_cot.\n1\nIntroduction\nThe transformative power of large language models, enhanced by Chain-of-Thought (CoT)\nprompting [50, 21, 59, 42], has significantly reshaped the landscape of information process-\ning [14, 26, 49, 56, 13, 55, 23, 29], fostering enhanced abilities across a myriad of disciplines\nand sectors. While CoT allows models to produce more comprehensive reasoning processes, its\nemphasis on intermediate reasoning steps can inadvertently introduce hallucinations [4, 30, 16, 20]\nand accumulated errors [4, 51, 1], thereby limiting models\u2019 ability to produce cogent reasoning\nprocesses.\nIn fact, the pursuit of reliable reasoning is not a contemporary novelty; indeed, it is an intellectual\nendeavor that traces its roots back to the time of Aristotle\u2019s ancient Greece. Motivated by the desire\nto establish a rigorous reasoning process, in his \u201cOrganon,\u201d Aristotle introduced principles of logic,\nin particular, syllogism, a form of logical argument that applies deductive reasoning to arrive at\na conclusion based on two or more propositions assumed to be true. In disciplines that rigorous\nreasoning is critical, such as judical reasoning and mathematical problem solving, documents must be\nwritten in a formal language with a logical structure to ensure the validity of the reasoning process.\n\u2217Equal contribution\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc\nAll datasets and models were solely downloaded and evaluated by the University of California San Diego.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.03872v3  [cs.CL]  3 Oct 2023\nThere are 53 maple trees currently in \nthe park. Park workers will plant maple \ntrees today. When the workers are \nfinished there will be 64 maple trees \nin the park. How many maple trees did \nthe workers plant today?\nQuestion\n# 1. There are 53 maple trees currently \nin the park.\n# 2. After the workers plant trees, \nthere will be 64 maple trees in the park.\n# 3. The workers plant some maple trees.\n# 4. How many maple trees did the \nworkers plant today?\nQuestion-Related Premises\n# 5. (by #2 #1) Step 1: \nCalculate the number of \nmaple trees the workers \nwill plant. Number of maple \ntrees the workers will \nplant: 64 - 53 = 11\nReasoning\n# 6. (by #4 #5) Final Step: The original \nquestion is #4. How many maple trees did the \nworkers plant today? We do not miss \ninformation on the rewritten labels. So the \nanswer to this question is the workers planted \n11 maple trees today.\nReasoning\n\u2026\nHere is some information: \"There are 53 maple trees currently in the park.\nAfter the workers plant trees, there will be 64 maple trees in the park.\"\nBased on the given information, here is a reasoning process: \"Calculate \nthe number of maple trees the workers will plant. Number of maple trees \nthe workers will plant: 64 - 53 = 11\"\nDouble-check the reasoning process, let's analyze its correctness, and \nend with \"yes\" or \"no\".\nVerification\nFigure 1: An overview of our proposed deductive reasoning and verification process. In response\nto an input question, LLMs generate deductive reasoning chains using the Natural Program format\n(bottom 3 boxes), a natural language-based deductive reasoning approach. The Natural Program\nformat allows individual reasoning steps (an example in purple) and their corresponding minimal\nset of premises (an example in orange) to be easily extracted. This streamlined extraction process\nfacilitates the step-by-step decomposition and verification of deductive reasoning (top-right box).\nAvery needs to buy a 3 \npiece place setting \n(dinner & salad plate \nand a bowl) for her \nholiday dinner.  She\u2019s \nhaving 12 people over \nfor dinner.  If the \ndinner plates cost $6.00 \neach and bowls each cost \n$5.00 and the salad \nplates cost $4.00, how \nmuch will she spend on \nplace settings?\nQuestion\n# 1. Avery needs to buy a 3 piece place \nsetting (dinner & salad plate and a bowl) for \nher holiday dinner.\n# 2. She\u2019s having 12 people over for dinner.\n# 3. Dinner plates cost $6.00 each, bowls each \ncost $5.00, and salad plates cost $4.00.\n# 4. How much will she spend on place settings?\nQuestion-Related Premises\n# 5. (by #2) Step 1: \nCalculate the total \nnumber of place \nsettings needed.\nTotal number of \nplace settings: 12 * \n3 = 36\nReasoning\n# 1. Avery needs to buy a 3 piece place \nsetting for her holiday dinner.\n# 2. Avery is having 12 people over for dinner.\n# 3. The cost of each dinner plate is $6.00.\n# 4. The cost of each bowl is $5.00.\n# 5. The cost of each salad plate is $4.00.\n# 6. How much will Avery spend on place \nsettings?\nQuestion-Related Premises\n# 1. Avery needs to buy a 3 piece place \nsetting for 12 people.\n# 2. The place setting consists of a dinner \nplate, a salad plate, and a bowl.\n# 3. The cost of a dinner plate is $6.00.\n# 4. The cost of a bowl is $5.00.\n# 5. The cost of a salad plate is $4.00.\n# 6. Avery needs to calculate the total cost \nof the place settings.\nQuestion-Related Premises\n\u2026\n\u2026\n# 7. (by #2) Step 1: \nCalculate the total \nnumber of place \nsettings Avery needs \nto buy. Total number \nof place settings: \n12 * 3 = 36\nReasoning\n# 7. (by #1) Step 1: \nCalculate the total \nnumber of place \nsettings Avery needs \nto buy. Total number \nof place settings \nneeded: 12\nReasoning\n#8. (by #4 #7) Final Step: The \noriginal question is #4. How \nmuch will she spend on place \nsettings? We do not miss \ninformation on the rewritten \nlabels. So the answer to this \nquestion is Avery will spend \n$540.00 on place settings.\nReasoning\n#10. (by #6 #9) Final Step: The \noriginal question is #6. How \nmuch will Avery spend on place \nsettings? We do not miss \ninformation on the rewritten \nlabels. So the answer to this \nquestion is Avery will spend \n$540.00 on place settings.\nReasoning\n\u2026\n#10. (by #6 #9) Final Step: The \noriginal question is #6. How \nmuch will Avery spend on place \nsettings? We do not miss \ninformation on the rewritten \nlabels. So the answer to this \nquestion is Avery will spend \n$180.00 on place settings.\nReasoning\n\u2026\nReasoning Error\nGrounding Error\nFigure 2: Through our Natural Program-based deductive reasoning verification approach, we identify\nand eliminate reasoning chains that contain errors in reasoning and grounding (we define grounding\nerror as utilizing information that is not present in cited premises). By alleviating such errors,\nwe significantly enhance the rigor, trustworthiness, and interpretability of the generated reasoning\noutputs.\nWe yearn for this sequence of reliable knowledge when answering questions. Our goal is to develop\nlanguage models that can propose potential solutions through reasoning in logical structures. Si-\nmultaneously, we aim to establish a verifier capable of accurately assessing the validity of these\nreasoning processes. Despite recent significant explorations in the field, such as [48]\u2019s emphasis\non self-consistency and [27, 5]\u2019s innovative use of codes to represent the reasoning process, these\napproaches still exhibit considerable limitations. For example, consistency and reliability are not\ninherently correlated; as for program codes, they are not powerful enough to represent many kinds\nof reasoning process, e.g., in the presence of quantifiers (\u201cfor all\u201d, \u201cif there exists\u201d) or nuances of\nnatural language (moral reasoning, \u201clikely\u201d, ...).\nWe propose leveraging the power of natural language to achieve the deductive reasoning emphasized in\nancient Greek logic, introducing a \u201cnatural program\u201d. This involves retaining natural language for its\ninherent power and avoiding the need for extensive retraining with large data sets. A natural program\n2\nrepresents a rigorous reasoning sequence, akin to a computer program. We expect implementations\nof the idea to have two properties: 1) that natural programs are generated with minimal effort from an\nexisting language model capable of CoT reasoning, preferably through in-context learning; 2) that\nthe natural program can be easily verified for reliability in the reasoning process.\nThrough a step-by-step investigation, we discovered that large language models have the potential\nto meet our expectation. Na\u00efve CoT prompts like \"Let us think step by step.\" has many flaws, and\nentrusting the entire verification process to a large model like ChatGPT can still lead to significant\nerror rates. However, we found that, if the reasoning process is very short, and only based on\nnecessary premises and contexts, the verification of existing large language models is already quite\nreliable. Therefore, our approach is to design prompts that induce CoT processes comprised of\nrigorous premises/conditions and conclusions with statement labels, and verification can be done by\ngradually isolating very few statements within the long thought chain. Experimentally, we found that\nmost reasoning that passed the verification was rigorous, and many that did not pass had elements of\nimprecision in the reasoning process, even if they occasionally arrived at correct answers.\nIt is worth emphasizing that, we are not looking for a method to just maximize the correctness rate\nof final answers; instead, we aspire to generate a cogent reasoning process, which is more aligned\nwith the spirit of judical reasoning. When combined with sampling-based methods, our method can\nidentify low-probability but rigorous reasoning processes. When repeated sampling fails to yield a\nrigorous reasoning process, we can output \"unknown\" to prevent hallucinations that mislead users.\nWe demonstrate the efficacy of our natural program-based verification approach across a range of\narithmetic and common sense datasets on publicly-available models like OpenAI\u2019s GPT-3.5-turbo.\nOur key contributions are as follows:\n1. We propose a novel framework for rigorous deductive reasoning by introducing a \u201cNatural\nProgram\u201d format (Fig. 1), which is suitable for verification and can be generated by just in-context\nlearning;\n2. We show that reliable self-verification of long deductive reasoning processes written in our Natural\nProgram format can be achieved through step-by-step subprocesses that only cover necessary context\nand premises;\n3. Experimentally, we demonstrate the superiority of our framework in improving the rigor, trustwor-\nthiness, and interpretability of LLM-generated reasoning steps and answers (Fig. 2).\n2\nRelated work\nReasoning with large language models. Recent large language models (LLMs) [3, 8, 57, 47, 38, 18,\n9, 37] have shown incredible ability in solving complex reasoning tasks. Instead of letting LLMs\ndirectly generate final answers as output, prior work have shown that by encouraging step-by-step\nreasoning through proper prompting, such as Chain-of-Thought (CoT) prompting [50] and many\nothers [21, 59, 58, 44, 48, 60, 25, 54], LLMs exhibit significantly better performance across diverse\nreasoning tasks. To further improve the step-by-step reasoning process, some recent studies have\ninvestigated leveraging external solvers such as program interpreters [39, 5, 27], training and calling\nexternal reasoning modules [11], or performing explicit search to generate deductive steps [2, 46].\nParallel to these works, we do not rely on external modules and algorithms, and we directly leverage\nthe in-context learning ability of LLMs to generate more precise and rigorous deductive reasonings.\nLarge language models as verifiers. Using language models to evaluate model generations has been\na long standing idea [22, 36, 40, 4]. As LLMs exhibit impressive capabilities across diverse tasks, it\nbecomes a natural idea to use LLMs as evaluation and verification tools. For example, [10, 11, 33]\nfinetune LLMs to verify solutions and intermediate steps. LLMs aligned with RLHF [32, 31, 48]\nhave also been employed to compare different model generations. In addition, recent works like\n[43, 52, 28, 6] leverage prompt designs to allow LLMs to self-verify, self-refine, and self-debug\nwithout the need for finetuning. However, these works do not focus on the rigor and trustworthiness\nof the deductive reasoning processes at every reasoning step. In this work, we propose a natural\nlanguage-based deductive reasoning format that allows LLMs to self-verify every intermediate step\nof a deductive reasoning process, thereby improving the rigor and trustfulness of reasoning.\n3\nQuestion: Jerry is rolling a six-sided die. How much more likely is it (expressed as a percentage)\nthat he rolls a number greater than $3 than that he rolls two even numbers in a row?\nGround Truth: There are 3 numbers greater than 3 on the dice, so the chances of rolling one\nof them are 3 / 6 = 50%. The chance of rolling one even number is 50%, so the chance of rolling\ntwo in a row is 50% * 50% = 25%. The difference between these two probabilities is 50% - 25%\n= 25%. #### 25\nOutput: There are two possible outcomes for rolling a number greater than 3 out of 6 total\noutcomes, so the probability is 2 / 6 or 1 / 3. For rolling two even numbers in a row, there are\nthree possible outcomes (2-4, 4-6, 6-2) out of 36 total outcomes, so the probability is 3 / 36 or\n1 / 12. To find the difference in probability, we subtract the probability of rolling two even\nnumbers in a row from the probability of rolling a number greater than 3: 1 / 3 - 1 / 12 = 1 / 4.\nThis means that it is 25% more likely to roll a number greater than 3 than to roll two even\nnumbers in a row.\nTable 1: An example question from GSM8K with a generated CoT reasoning path with GPT-\n3.5 (turbo), where the output provides a wrong reasoning chain with the correct answer.\nAdditionally, while some recent works [12, 53, 15, 34] have proposed methods to verify individual\nsteps in a reasoning process, our approach distinguishes from these works in the following perspec-\ntives: (1) Our approach leverages in-context learning to achieve reasoning verification, without the\nneed for language model finetuning. (2) Our Natural Program-based LLM verification approach\nnot only identifies invalid reasoning steps, but also provides explicit explanations for why they are\ninvalid, detailing the specific reasoning errors involved. (3) Our Natural Program-based reasoning\nand verification approach is compatible with in-context abstract reasoning tasks where reasoning\nsteps do not possess proof-like entailment structures. For example, our approach is compatible with\nthe Last Letters task, where the LLM is instructed to output the concatenation of the last letters\nof all words in a sequence as the final answer. (4) Our Natural Program approach allows the use\nof commonsense knowledge not explicitly listed in premises. For example, consider this problem:\n\u201cMarin eats 4 apples a day. How many apples does he eat in November?\u201d Even though \u201cNovember\nhas 30 days\u201d is not explicitly listed in the premises, Natural Program permits the use of such common\nknowledge within a reasoning step. Our in-context verification process is also capable of handling\nthese implicit premises (e.g., if LLM outputs \u201cNovember has 29 days\u201d in a reasoning step, it will be\nmarked as invalid).\n3\nMotivation and Problem Formulation\nA reasoning-based question-answering (QA) task can be defined as a tuple (Q, C, O, A) [35], where Q\nis the target question; C is the context of a question, such as the necessary background for answering a\nquestion; O = (o1, o2, \u00b7 \u00b7 \u00b7 , ck) are optional answer choices if Q is a K-way multiple choice problem;\nand A is the ground-truth answer. Given Q and C as inputs, large language models (LLMs) [3, 8, 47]\ngenerate a sequence of tokens T = (t1, t2, \u00b7 \u00b7 \u00b7 , tn) to answer the question. Recent works like Chain-\nof-Thought (CoT) [50, 21] leverage prompt engineering in the context C to encourage models to\ngenerate the intermediate reasoning process in T, which benefits LLM performance across diverse\nreasoning tasks. In this case, T consists of a set of m intermediate reasoning steps, which we denote\nas S = (s1, s2, \u00b7 \u00b7 \u00b7 , sm) . Each step si can be represented by a subsequence of the generated tokens\n(tli, tri) \u2286 T. In much prior work, a generated solution is \u201ccorrect\u201d if and only if the predicted final\nanswer in sm matches the ground truth A, which we call answer correct(ness).\nWe observe that for all cases where LLMs produce erroneous final answers, there exists at least one\nmistake among the intermediate reasoning steps S. Moreover, even when the final answer is correct,\nthere might still exist some mistakes among S. This phenomenon, as illustrated in Tab. 1, occurs for\nall LLMs we tested, including state-of-the-art models such as ChatGPT and GPT-4 [32]. Since later\nreasoning steps are conditioned on prior reasoning steps, these mistakes often initiate a snowball\neffect, causing subsequent mistakes to compound. This significantly diminishes the likelihood of\ncorrect problem-solving and impedes the progress towards achieving human-level complex reasoning.\nTherefore, in this work, we place significant emphasis on ensuring the validity of every reasoning\nstep, not just the correctness of the final answer. In particular, we focus on the validity of deductive\nreasoning, an essential component of a logical reasoning process. In deductive reasoning, we are\n4\nPrompting\nReasoning Correctness\nGSM8K\nAQuA\nMATH\nAddSub\nDate\nLast Letters\nZero-shot\nCorrect\n0.98\n0.96\n1.00\n0.98\n0.98\n1.00\nIncorrect\n0.04\n0.06\n0.04\n0.02\n0.04\n0.04\n(Average)\n0.51\n0.51\n0.52\n0.50\n0.51\n0.52\nTwo-shot\nCorrect\n0.98\n0.96\n1.00\n0.92\n1.00\n0.96\nIncorrect\n0.02\n0.04\n0.00\n0.06\n0.26\n0.06\n(Average)\n0.50\n0.50\n0.50\n0.49\n0.63\n0.51\nTable 2: Zero-shot and two-shot reasoning chain verification accuracy for GPT-3.5-turbo (ChatGPT),\nwhere an entire reasoning chain is verified at once. The two shot prompt we used is presented in\nAppendix D.1. To generate verification inputs, for each dataset, we perform Chain-of-Thought (CoT)\nprompting and randomly sample 50 reasoning chains that are valid and 50 reasoning chains that\nexhibit mistakes. We observe that when given an entire reasoning process, where the deductive graphs\nfor all reasoning steps are entangled together, it is challenging even for strong language models like\nChatGPT to verify its validity.\ngiven a (premise, conclusion) pair, and we are interested in determining whether the conclusion\nfollows from the premises. In the context of reasoning-based QA tasks, for each reasoning step si,\nwe define its deductive validity V (si) as a binary variable. A reasoning step is deductively valid\n(V (si) = 1) if and only if si can be logically deduced from its corresponding premises pi, which\nconsist of the context C, the question Q, and all the previous reasoning steps sj(j < i). Then, we can\nalso define the deductive validity for the entire reasoning chain S as V (S) = \u2227M\ni=1V (si). Compared\nto evaluating answer correctness, which can be accomplished by simple functions such as exact string\nmatch, evaluating deductive validity is a lot more challenging. Thanks to the recent progress on\nLLMs, which demonstrate impressive in-context learning capabilities across diverse scenarios, we\npropose to use LLMs to examine reasoning chains and predict the deductive reasoning validity.\n4\nDeductively Verifiable Chain-of-Thought Reasoning\nIn this section, we introduce our specific approaches to performing deductive verification of reasoning\nchains. Specifically, we first introduce our motivation and method for decomposing a deductive verifi-\ncation process into a series of step-by-step processes, each only receiving contexts and premises that\nare necessary. Then, we propose Natural Program, a natural language-based deductive reasoning\nformat, to facilitate local step-by-step verification. Finally, we show that by integrating deductive ver-\nification with unanimity-plurality voting, we can improve the trustworthiness of reasoning processes\nalong with final answers. An overview of our approach is illustrated in Fig. 1 and Fig. 2.\n4.1\nDecomposition of Deductive Verification Process\nGiven a reasoning chain S = (s1, s2, \u00b7 \u00b7 \u00b7 , sn), a straightforward idea to verify its deductive validity is\nto ask LLMs to examine the entire reasoning chain at once. To assess the effectiveness of this approach,\nwe conduct a preliminary experiment: for a dataset problem and its reasoning chain S generated\nby ChatGPT, we prompt ChatGPT with \u201cDo you think the above reasoning process is\ncorrect?\nLet\u2019s think step by step\u201d such that its outputs whether there exists any mistake\namong any reasoning step in S. However, as demonstrated in Tab. 2, the verification accuracy is 50%\nfor most datasets, and ChatGPT struggles at finding out mistaken reasonings. Notably, it persistently\noutputs \u201cCorrect\u201d for most reasoning chain queries, regardless of their actual validity.\nWe conjecture that such phenomenon is caused by the abundance of irrelevant premises for each\nreasoning step. Recall that the premises pi for a reasoning step si consist of the the question\nQ, the question context C, along with the prior reasoning steps s\u2264j = {sj : j < i}. For Q\nand C, we can further extract and decompose Q \u222a C into a set of \u201cquestion-related premises\u201d\nQC = {qc1, qc2, \u00b7 \u00b7 \u00b7 , qcm}, where qci is a premise or condition inferred from Q \u222a C. Then, it is\noften the case that most elements of pi = QC \u222a s\u2264j are irrelevant to the validity of si, leading\nto erroneous verifications from language models. A very recent work [41] also observes a similar\nphenomenon where LLMs are easily distracted by irrelevant context.\nHence, we propose a decomposition of the reasoning chain verification process into a series of step-\nby-step processes, where each step only considers the premises that are necessary. The overall validity\n5\nof the reasoning chain, denoted as V (S) = \u2227M\ni=1V (si), can be naturally decomposed into individual\nstep validity V (si). However, achieving such decomposition is highly challenging without imposing\nconstraints on the format of reasoning chains. Additionally, for each si \u2208 S, we aim to ensure that\nit explicitly lists the minimal subset of premises \u00afpi \u2286 pi required for deductive reasoning to avoid\npotential ambiguities during verification. This motivates us to introduce a natural-language-based\ndeductive reasoning format in Section 4.2.\n4.2\nNatural Program Deductive Reasoning Format\nAs previously mentioned in Sec. 4.1, we desire LLMs to output deductive reasoning processes that\ncan be easily verified by themselves, specifically by listing out the minimal set of necessary premises\npi at each reasoning step si. To accomplish its goal, we propose to leverage the power of natural\nlanguage, which is capable of rigorously representing a large variety of reasoning processes and can\nbe generated with minimal effort. In particular, we introduce Natural Program , a novel deductive\nreasoning format for LLMs. More formally, Natural Program consists of the following components:\n\u2022 An instruction for models to extract question-related premises QC. We use the following in-\nstruction: \u201cFirst, let\u2019s write down all the statements and relationships\nin the question with labels\".\n\u2022 A numbered-list of question-related premises, each prefixed with \u201c#{premise_number}\u201d.\n\u2022 An instruction for models to generate the reasoning chain S based on the question-related\npremises QC. We use the following instruction: \u201cNext, let\u2019s answer the question\nstep by step with reference to the question and reasoning process\u201d.\n\u2022 A list of prefixed reasoning steps Si. The prefix has the following format:\n#{number} (by {list_of_premises_used}). Here \u201cnumber\u201d equals |QC| + i, and\n\u201clist_of_premises_used\u201d consists of numbers from the smallest subset of premises among\nQC \u222a s\u2264j that are used for the deductive reasoning of si. In addition, for the last reasoning\nstep sm, we ensure that it (1) includes a special tag Final Step; (2) refers to the premise\nnumber of the target question to be answered; (3) explicitly gives the final answer to a\nquestion.\nTo encourage language models to reason in the Natural Program format, we have designed one-\nshot prompts for different datasets, which are shown Appendix D.2.\nGiven that LLM\u2019s rea-\nsoning outputs follow the Natural Program format, we can then verify the deductive validity\nof a single reasoning step si through an instruction that consists of (1) the full descriptions of\npremises used for the reasoning of si; (2) the full description of si; (3) an instruction for va-\nlidity verification, such as \u201cDouble-check the reasoning process, let\u2019s analyze its\ncorrectness, and end with \"yes\" or \"no\".\u201d Note that throughout this verification pro-\ncess, we only retain the minimal necessary premise and context for si, thereby avoiding irrelevant\ncontext distraction and significantly improving the effectiveness of validation. Additionally, we\nemploy a one-shot prompt for this verification process, which we find very helpful for improving the\nverification accuracy. The prompt is shown in Appendix D.3.\nFigure 1 provides an overview of the complete Natural Program-based deductive reasoning and\nverification process. By using the Natural Program approach, we demonstrate that LLMs are\ncapable of performing explicit, rigorous, and coherent deductive reasoning. Furthermore, Natural\nProgram enables LLMs to self-verify their reasoning processes more effectively, enhancing the\nreliability and trustworthiness of the generated responses.\n4.3\nIntegrating Deductive Verification with Unanimity-Plurality Voting\nGiven that we can effectively verify a deductive reasoning process, we can naturally integrate\nverification with LLM\u2019s sequence generation strategies to enhance the trustworthiness of both the\nintermediate reasoning steps and the final answers. In this work, we propose Unanimity-Plurality\nVoting, a 2-phase sequence generation strategy described as follows. Firstly, similar to prior work\nlike [48], we sample k reasoning chain candidates along with their final answers. In the unanimity\nphase, we perform deductive validation on each reasoning chain. Recall that a chain S is valid (i.e.,\nV (S) = 1) if and only if all of its intermediate reasoning steps are valid (i.e., \u2200i, V (si) = 1). For\neach intermediate reasoning step si, we perform majority voting over k\u2032 sampled single-step validity\n6\npredictions to determine its final validity V (si). We then only retain the verified chain candidates\n{S : V (S) = 1}. In the plurality voting stage, we conduct a majority-based voting among the verified\nchain candidates to determine the final answer. This voting process ensures that the final answer is\nselected based on a consensus among the trustworthy reasoning chains.\n5\nExperiments\nIn this section, we perform evaluations to demonstrate the effectiveness of our Natural Program-based\ndeductive reasoning verification approach over diverse reasoning datasets. Firstly, we show that\nour deductive verification process leads to substantial improvements in the rigor and reliability of\nreasoning chains. Subsequently, we will examine the impact of deductive verification on the accuracy\nof final answers. Our findings reveal that by adopting our Natural Program reasoning format without\nverification, we improve answer correctness on challenging benchmarks. Further applying deductive\nverification leads to slight reductions in final answer accuracy. One reason for this phenomenon is\nthat the verification process effectively identifies and eliminates flawed reasoning chains that still\nproduce correct answers.\n5.1\nExperimental Setup\nBenchmarks. We evaluate the deductive verification accuracy and the answer correctness of reasoning\nchains over a diverse set of reasoning tasks: arithmetic reasoning, symbol manipulation, and date\nunderstanding. For arithmetic reasoning, we utilize the following benchmarks: 1) AddSub [19];\n2) GSM8K [10]; 3) MATH [17]; 4) AQuA [24]. Among these benchmarks, the AddSub and GSM8K\ndatasets involve middle school-level multi-step calculations to arrive at a single number as the final\nanswer. The MATH dataset presents more challenging problems that require expressing the answer\nas a mathematical expression in LaTeX format. These problems involve concepts from linear algebra,\nalgebra, geometry, calculus, statistics, and number theory. AQuA also features similarly challenging\nproblems, except that questions are in a multiple-choice format. For symbol manipulation, we use\nLast Letter Concatenation [50], where the model is tasked with concatenate the last letters of all the\nwords provided in the question. For date understanding, we use the one from BIG-bench [45]\nDeductive verfication evaluation setup. For each of the above benchmarks, we select 100 reasoning\nchains, where 50 of them are deductively valid and 50 of them exhibit reasoning mistakes. The\nground-truth deductive validity of each reasoning chain is determined by human annotators.\nAnswer extraction. To extract answers from reasoning solutions, we first perform text splitting based\non answer prefix patterns such as \u201canswer is\u201d or \u201coption is\u201d. Then, using problem type-specific regular\nexpressions, we extract the final answer. To extract the validity results from deductive verification\nprocesses, we only keep the last sentence of model response. We then extract the validity answer with\nregular expressions to obtain attitude words, e.g., \u201cyes\u201d or \u201cno\u201d, to determine the validity answer.\nSometimes, language models may not provide a direct answer and instead output phrases like \u201cnot\napplicable\u201d at the end of the response. In such cases, we consider the answer from the model as \"yes\".\nPlease refer to Appendix C for more details.\nModel and Hyperparameters. We conduct our main experiments with GPT-3.5-turbo (Chat-\nGPT) [32]. We also present results for the LLama model-family [47]) in Appendix A, where we\nfind the deductive verification accuracy to be worse than larger models even after finetuning. For\nChatGPT, we use a generation temperature of T = 0.7. For Unanimity-Plurality Voting, we set\nk = 10 and k\u2032 = 3 by default. We use 1-shot prompting for both reasoning chain generation and\ndeductive verification (except reasoning chain generation for the date understanding task where we\nuse 2-shot). See Appendix D.2 and Appendix D.3 for more details.\n5.2\nComparison of Deductive Verification Accuracy\nWe compare the verification accuracy of reasoning chains using two methods: (1) verifying the entire\nreasoning chain at once (as described in Section 4.1) without utilizing the Natural Program, and\n1 Most results for Faithful CoT are from their official repository https://github.com/veronica320/\nFaithful-COT, except MATH and AddSub due to the unavailability. For these two datasets, we use our\nimplementation and the same prompt for the math word problems in their paper. The prompt for Last Letters is\nnot available, so we leave it blank.\n7\nVerification Method Reasoning Correctness GSM8k AQuA MATH AddSub Date Last Letters Overall\nCoT\nTwo-shot\nCorrect\n98%\n96%\n100%\n92%\n100%\n96%\n97%\nIncorrect\n2%\n4%\n0%\n6%\n26%\n6%\n7%\n(Average)\n50%\n50%\n50%\n49%\n63%\n51%\n52%\nNatural Program\nOne-shot\nCorrect\n84%\n72%\n70%\n95%\n90%\n96%\n85%\nIncorrect\n84%\n62%\n76%\n40%\n56%\n6%\n54%\n(Average)\n84%\n67%\n73%\n68%\n73%\n51%\n69%\nTable 3: Comparison of deductive verification accuracy of reasoning chains for GPT-3.5-turbo (Chat-\nGPT). We compare two approaches: (1) verifying entire reasoning chains generated by Chain-of-\nThought prompting; (2) verifying reasoning chains generated in the Natural Program format with\nstep-by-step decomposition. In the latter case, when we verify each reasoning step si, we only keep\nthe necessary subset of premises \u00afpi \u2286 pi. To calculate verification accuracy, for each dataset, we\nrandomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting\nincorrect reasonings.\nArithmetic\nCommonsense\nMethods\nGSM8K\nAQuA\nMATH\u2217\nAddSub\nDate\nLast Letters\nCoT + Voting\n87.62% 70.18%\n35.93%\n92.36%\n69.97%\n81.60%\nFaithful CoT + Voting\n75.80%\n61.80% 31.78%1 88.35%1 73.50%\n-\nOurs (Natural Program (NP), No Verification) 87.05% 70.34% 36.75% 93.67% 72.49%\n92.98%\nOurs (NP + Deductive Verification + UPV)\n86.01%\n69.49%\n36.48%\n93.54%\n71.45%\n92.60%\nTable 4: Final answer accuracy comparison on GPT-3.5-turbo (ChatGPT). All approaches generate\nk = 10 reasoning chains for each problem before performing majority voting or reasoning chain\nfiltering with our deductive verification approach.\n(2) our Natural Program-based verification approach with step-by-step decomposition. The results,\npresented in Table 3, indicate that our approach achieves significantly higher reasoning verification\naccuracy across most datasets. It effectively identifies erroneous reasoning in faulty chains while\nmaintaining a low rate of false positives for valid chains. However, we observe that our approach\u2019s\neffectiveness is limited on the \u201cLast Letters\u201d task. We hypothesize that this is due to the task\u2019s\nnature, where each subsequent reasoning step is conditioned on all previous steps, presenting greater\nchallenges for reasoning verification due to the increased dependency among premises.\n5.3\nImpact of Natural Program and Deductive Verification on Final Answer Correctness\nWe then investigate the impact of our Natural Program reasoning format and our deductive verification\nprocess on final answer correctness. We conduct two experiments: (1) for each problem, we instruct\nlanguage models to generate k = 10 reasoning chain candidates in the Natural Program (NP) format\nand perform simple majority voting on final answers, without using deductive verification to filter out\nreasoning chain candidates; (2) applying our deductive verification approach to filter out reasoning\nchain candidates, and apply Unanimity-Plurality Voting (UPV) along the process to determine the\nfinal answer. As a reference, we also report the performance of Chain-of-Thought (CoT) [50] and\nFaithful CoT [27]. For these baselines, we perform simple answer-based majority voting with k = 10\nfor fair comparison.\nResults are presented in Tab. 4. While our major goal is to improve the trustworthiness and reliability\nof deductive reasoning, we find that prompting language models to reason in our Natural Program\nformat achieves on-par or better final answer accuracy than baselines over many reasoning tasks.\nUpon further applying our deductive verification approach to filter out invalid reasoning chains, we\nobserve a slight decrease in final answer accuracy. One major contributing factor to this decrease\nis the filtering out of reasoning chain candidates that provide correct answers but exhibit incorrect\nreasoning. We illustrate an example in Table 5, where ChatGPT generates the correct final answer\nbut assigns incorrect premise numbers to support the first reasoning step. We note that in many such\ncases, our approach effectively identifies these reasoning errors, thereby enhancing the rigor and\n8\nQuestion: In March it rained 0.81 inches. It rained 0.35 inches less in April than in March.\nHow much did it rain in April?\nGround Truth: It rained 0.81 - 0.35 = 0.46 inches.\nGPT-3.5-turbo (ChatGPT):\nQuestion-related premises:\n#1. It rained 0.81 inches in March.\n#2. It rained 0.35 inches less in April than in March.\n#3. How much did it rain in April?\nThe first reasoning step:\n#4. (by #2) Step 1: Calculate how much it rained in April by subtracting the difference of 0.35 inches from\nthe amount in March.\nAmount of rain in April: 0.81 - 0.35 = 0.46 inches\nTable 5: For this reasoning chain candidate of this GSM8k problem, ChatGPT generates the correct\nfinal answer but provides incorrect premise numbers for grounding the first reasoning step. In \u201cStep\n1\u201d, the correct premise numbers should be #1 and #2. Our deductive reasoning verification approach\neffectively identifies these reasoning errors, enhancing the rigor and trustworthiness of the reasoning\nprocess. It is worth noting that removing a correct answer from the candidate reasoning chains has a\nslightly negative impact on the overall final answer correctness.\nPremise Context\n# Shots\nReasoning Correctness\nGSM8K\nAQuA\nMATH\nAddSub\nDate\nLast Letters\nAverage\nFull Premises\n1\nCorrect\n64%\n54%\n58%\n95%\n26%\n96%\n66%\nWrong\n56%\n68%\n56%\n24%\n76%\n5%\n48%\n(Average)\n60%\n61%\n57%\n60%\n51%\n51%\n57%\nMinimal Premises\n0\nCorrect\n84%\n78%\n90%\n96%\n90%\n12%\n75%\nWrong\n26%\n12%\n28%\n20%\n20%\n80%\n31%\n(Average)\n55%\n45%\n59%\n58%\n55%\n46%\n53%\nMinimal Premises\n1\nCorrect\n84%\n72%\n70%\n95%\n90%\n96%\n85%\nWrong\n84%\n62%\n76%\n40%\n56%\n6%\n54%\n(Average)\n84%\n67%\n73%\n68%\n73%\n51%\n69%\nTable 6: Ablation study on the impact of (1) premise context and (2) zero-shot vs. few-shot scenarios\non deductive verification accuracy using our Natural Program-based approach with step-by-step\nreasoning chain decomposition. To verify each reasoning step si, we either the full premises\npi = QC \u222a S\u2264j, or use the minimal subset of premises \u00afpi \u2286 pi necessary as outlined in Sec. 4.1\nThe one-shot prompt we used is shown in Appendix D.3. For each dataset, we randomly sample 50\nreasoning chains that are deductively valid and 50 reasoning steps exhibiting incorrect reasonings.\nreliability of the language models\u2019 reasoning processes, albeit with a slight negative impact on the\noverall final answer correctness. Further discussions are presented in Appendix B.\n5.4\nAblation Study\nIn addition, we perform several ablation studies to gain further insights into the designs of our\ndeductive verification approach. In Tab. 6, we compare two different approaches to verify a single\nreasoning step si \u2208 S following our Natural Program format. The first approach utilizes all premises\npi = QC \u222a S\u2264j for verification regardless of their relevance to si, potentially introducing irrelevant\ncontexts. The second approach follows our design in Sec. 4.1 and only includes the necessary context\nand premises \u00afpi \u2286 pi. We observe that removing irrelevant premises significantly improves the\nreasoning chain verification accuracy on many datasets, highlighting the importance of this technique.\nWe also ablate on our Unanimity-Plurality Voting strategy by investigating the impact of different k\u2032.\nRecall that k\u2032 determines the number of votes to produce validity predictions of single-step reasoning.\nResults are shown in Tab. 7. We observe that increasing k\u2032 generally enhances reasoning validation\naccuracy, though we note that this is at the expense of more compute.\n6\nLimitations\nWhile we have demonstrated the effectiveness of Natural Program-based deductive reasoning verifi-\ncation to enhance the trustworthiness and interpretability of reasoning steps and final answers, it is\n9\nAnswer Correctness k\u2032 = 1 k\u2032 = 3 k\u2032 = 5 k\u2032 = 10\nCorrect\n86%\n90%\n90%\n92%\nWrong\n38%\n38%\n38%\n40%\nTable 7: Ablation of different values of k\u2032 on the verification accuracy of reasoning chains using our\nUnanimity-Plurality Voting strategy. Experiments are performed on AddSub using GPT-3.5-turbo\n(ChatGPT).\nQuestion: Melanie had 10 quarters and 17 pennies in her bank. Her dad gave her 27 pennies\nand her mother gave her 19 pennies. How many pennies does Melanie have now?\nGround Truth: Melanie have 17 + 27 + 19 = 63 pennies.\nChatGPT\u2019s reasoning step:\n#5. (by #1) Step 1: Calculate the number of pennies Melanie had initially.\nNumber of pennies in 10 quarters: 10 * 25 = 250\nNumber of pennies initially: 250 + 17 = 267\nTable 8: An example question with ambiguous wordings. The term \"pennies\" in this question can\nbe interpreted as either a type of coin or a unit of currency. In this particular question, \"pennies\" is\ntreated as a type of coin. However, the initial reasoning step by ChatGPT mistakenly treats \"pennies\"\nas a unit of currency, resulting in the conversion of all Melanie\u2019s money into \"pennies\" (highlighted\nin red). Consequently, all subsequent reasoning steps follow this flawed logic, leading to an incorrect\nreasoning trace. Our deductive verification is not yet able to detect such errors.\nimportant to acknowledge that our approach has limitations. In this section, we analyze a common\nsource of failure cases to gain deeper insights into the behaviors of our approach. The failure case, as\nshown in Tab. 8, involves the ambiguous interpretation of the term \u201cpennies,\u201d which can be understood\nas either a type of coin or a unit of currency depending on the context. The ground truth answer\ninterprets \u201cpennies\u201d as coins, while ChatGPT interprets it as a unit of currency. In this case, our\ndeductive verification process is incapable of finding such misinterpretations. Contextual ambiguities\nlike this are common in real-world scenarios, highlighting the current limitation of our approach.\n7\nConclusion\nIn this paper, we aim to enable Large Language Models (LLMs) to perform explicit and rigorous\ndeductive reasoning while ensuring the trustworthiness of their reasoning processes through self-\nverification. To this end, we have proposed a novel framework based on \u201cNatural Program\u201d, a natural\nlanguage-based deductive reasoning format that facilitates reasoning verification and can be easily\ngenerated through in-context learning. Within this framework, we decompose the verification process\nof complex reasoning chains into step-by-step subprocesses that focus solely on necessary context and\npremises, allowing us to significantly enhance the accuracy of verification. Additionally, we introduce\na Unanimity-Plurality Voting strategy to further improve verification accuracy. Experimentally,\nwe demonstrate the superiority of our framework in improving the rigor, trustworthiness, and\ninterpretability of reasoning steps and answers.\nBroader Impact. While our deductive verification approach can mitigate hallucinations and reasoning\nerrors of Large Language Models (LLMs), it does not completely eliminate these phenomena.\nLLMs can still produce harmful and biased content, make incorrect claims, and produce wrongful\nadvice. This issue becomes particularly significant when LLMs engage in complex reasoning chains,\nincreasing the risk of misleading users. Consequently, it is still crucial for users to exercise great\ncaution when interacting with, deploying, or developing LLM-based applications.\nAcknowledgements\nWe would like to express our sincere gratitude to Tongzhou Mu and Caiwei Xiao from UC San\nDiego, Kairong Luo from Tsinghua University, and Pulkit Madan, Reza Pourreza, Sunny Panchal,\nand Apratim Bhattacharyya from Qualcomm for their valuable discussions and feedback.\n10\nReferences\n[1] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias\nmatters: An imitation learning perspective of error accumulation in language generation. arXiv preprint\narXiv:2204.01171, 2022.\n[2] Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. Natural language deduction through\nsearch over statement compositions. arXiv preprint arXiv:2201.06028, 2022.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[4] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[5] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588,\n2022.\n[6] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128, 2023.\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021.\n[11] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv preprint\narXiv:2208.14271, 2022.\n[12] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In The Eleventh International Conference on Learning Repre-\nsentations, 2023.\n[13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023.\n[14] Andrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint\narXiv:2209.15003, 2022.\n[15] Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning. In The\nEleventh International Conference on Learning Representations, 2022.\n[16] Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo,\nand Andr\u00e9 FT Martins.\nHallucinations in large multilingual translation models.\narXiv preprint\narXiv:2303.16104, 2023.\n[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint\narXiv:2103.03874, 2021.\n[18] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\n11\n[19] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve\narithmetic word problems with verb categorization. In EMNLP, pages 523\u2013533, 2014.\n[20] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing\nSurveys, 55(12):1\u201338, 2023.\n[21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n[22] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve\nalgebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 271\u2013281, Baltimore, Maryland, June 2014. Association for\nComputational Linguistics.\n[23] Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell,\nJames McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context?\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 537\u2013563, Abu Dhabi,\nUnited Arab Emirates, December 2022. Association for Computational Linguistics.\n[24] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:\nLearning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.\n[25] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing. ACM\nComputing Surveys, 55(9):1\u201335, 2023.\n[26] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\n[27] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379, 2023.\n[28] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback.\narXiv preprint arXiv:2303.17651, 2023.\n[29] Ana Marasovi\u00b4c, Iz Beltagy, Doug Downey, and Matthew E. Peters. Few-shot self-rationalization with\nnatural language prompts, 2022.\n[30] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in\nabstractive summarization. arXiv preprint arXiv:2005.00661, 2020.\n[31] OpenAI. Gpt-4 technical report, 2023.\n[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[33] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi\nFaltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904,\n2023.\n[34] Archiki Prasad, Swarnadeep Saha, Xiang Zhou, and Mohit Bansal. Receval: Evaluating reasoning chains\nvia correctness and informativeness. 2023.\n[35] Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica\nRamos, William Wang, Zhiheng Huang, et al. Street: A multi-task structured reasoning and explanation\nbenchmark. arXiv preprint arXiv:2302.06729, 2023.\n[36] Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413,\n2016.\n[37] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot\ntask generalization. arXiv preprint arXiv:2110.08207, 2021.\n12\n[38] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[39] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv\npreprint arXiv:2302.04761, 2023.\n[40] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate & rank:\nA multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.\n[41] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli,\nand Denny Zhou. Large language models can be easily distracted by irrelevant context. arXiv preprint\narXiv:2302.00093, 2023.\n[42] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057, 2022.\n[43] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n[44] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan\nWang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022.\n[45] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n[46] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and\nabductive statements over natural language. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli,\neditors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event,\nAugust 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 3621\u20133634. Association for\nComputational Linguistics, 2021.\n[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[48] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves\nchain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv\npreprint arXiv:2206.07682, 2022.\n[50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain\nof thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[51] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text\ngeneration with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\n[52] Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners\nwith self-verification. arXiv preprint arXiv:2212.09561, 2022.\n[53] Kaiyu Yang, Jia Deng, and Danqi Chen. Generating natural language proofs with verifier-guided search.\nIn Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.\n[54] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n[55] Eric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony Wu. Star: Self-taught reasoner bootstrapping\nreasoning with reasoning. 2022.\n[56] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing\nzero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n13\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[58] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large\nlanguage models. arXiv preprint arXiv:2210.03493, 2022.\n[59] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625, 2022.\n[60] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi.\nTeaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.\n14\nA\nDeductive Verification with Vicuna Models\nWe further explore the efficacy of deductive verification for open-source models. We select two\npopular models: Vicuna-7B and Vicuna-13B [7]. These models are fine-tuned versions of LLaMA-7B\nand LLaMA-13B [47] using the ShareGPT data3. We use the same Natural Program-based one-shot\nverification method we used in the main paper. Results are shown in the first and the third rows\nof Table 9. We observe for the original Vicuna models without finetuning, Vicuna-7B exhibits\npoor performance in deductive verification and fails to find out reasoning mistakes, while the larger\nVicuna-13B exhibits better verification accuracy.\nModels\nReasoning Correctness GSM8K AQuA MATH AddSub Date Last Letters Overall\nVicuna-7B\nCorrect\n80%\n86%\n96%\n98%\n96%\n80%\n89%\nWrong\n14%\n22%\n16%\n6%\n20%\n34%\n19%\n(Average)\n47%\n54%\n56%\n52%\n58%\n57%\n54%\nVicuna-7B\n(fine-tuned)\nCorrect\n68%\n48%\n46%\n76%\n46%\n32%\n53%\nWrong\n72%\n86%\n54%\n60%\n72%\n68%\n69%\n(Average)\n70%\n67%\n50%\n68%\n61%\n50%\n61%\nVicuna-13B\nCorrect\n86%\n82%\n92%\n96%\n72%\n74%\n84%\nWrong\n32%\n36%\n20%\n20%\n34%\n30%\n29%\n(Average)\n59%\n59%\n56%\n58%\n53%\n52%\n57%\nVicuna-13B\n(fine-tuned)\nCorrect\n74%\n50%\n56%\n86%\n72%\n12%\n58%\nWrong\n72%\n76%\n72%\n68%\n62%\n96%\n74%\n(Average)\n73%\n63%\n64%\n77%\n67%\n54%\n66%\nChatGPT\n(GPT-3.5-Turbo)\nCorrect\n84%\n72%\n70%\n95%\n90%\n96%\n85%\nWrong\n84%\n62%\n76%\n40%\n56%\n6%\n54%\n(Average)\n84%\n67%\n73%\n68%\n73%\n51%\n69%\nTable 9: One-shot Deductive Verification Accuracy of Vicuna-7B and Vicuna-13B. The models are\nevaluated with or without finetuning on our deductive verification dataset. For each dataset, we\nrandomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting\nincorrect reasonings.\nWe therefore conduct an additional experiment to investigate if the verification accuracy of Vicuna\nmodels can be improved by fine-tuning. To this end, we generate a deductive verification dataset,\nwhich consists of 2000 reasoning steps evenly distributed between correct and incorrect categories.\nWe automatically generate this dataset using GPT-3.5-turbo since it exhibits a very high accuracy of\nsingle-step verification. We first use GPT-3.5-turbo to generate solutions for problems in GSM8K\u2019s\ntraining set. We then execute step-by-step deductive verification on these solutions using GPT-3.5-\nturbo. For solutions that result in correct final answers, we retain the reasoning steps that pass\ndeductive verification. For solutions that yield incorrect final answers, we retain the reasoning\nsteps that cannot pass deductive verification. After constructing our dataset, we then fine-tune the\nVicuna models using the verifications of the 2000 reasoning steps. Models were fine-tuned with 4\nA100-80GB over 3 epochs. Training parameters are shown in Table 10.\nAs shown in Tab. 9, we observe that fine-tuning with our dataset can enhance the deductive verification\naccuracy of Vicuna models not only on the dataset where the training dataset is constructed (GSM8K),\nbut also on many other datasets. However, the accuracy is still worse than non-finetuned GPT-3.5,\nwhich suggests that model capacity has a significant impact on deductive verification capabilities.\nB\nMore Discussion on Improvements of Deductive Verification Accuracy\nVersus Improvements on Final Answer Correctness\nIn the main paper, we demonstrated that our verification approach significantly improves the verifica-\ntion accuracy of reasoning chains (Tab. 3, 6, but barely improves the final answer accuracy (Tab. 4).\nWe further analyze this phenomenon below:\n3https://github.com/domeccleston/sharegpt\n15\nHyperparameters\nValue\nOptimizer\nAdamW\nLearning rate\n1 \u00d7 10\u22125\nWeight decay\n0.00\nNum epochs\n3\nBatch size\n64\nLearning rate schedule\nLinear\nTable 10: Hyperparameters for finetuning Vicuna models with our deductive verification dataset.\nConsider the GSM8K dataset as an example (recall that the final answer for a problem is obtained\nthrough majority voting). Among all problems, 91.6% of problems have |(number of votes received\nby the correct answer) \u2212 (largest number of votes received by a single wrong answer)| > 2, and their\nfinal answers are unlikely to be changed through our deductive verification approach. For the rest of\nthe cases (8.4%), where deductive verification is more likely to impact their final answers, we found\nthat:\n\u2022 Among all reasoning chains that arrive at correct answers (these correct-answer chains\naccount for 49.4% of all reasoning chain candidates), 46.2% of reasoning chains are filtered\nout by our verification process.\n\u2022 Among the reasoning chains that arrive at correct answer but are filtered out by our verifica-\ntion process, 76.3% indeed exhibit incorrect reasoning.\n\u2022 Among the reasoning chains that arrive at correct answer and are not filtered out by our\nverification process, 78.0% indeed have correct reasonings.\n\u2022 Among the reasoning chains that do not arrive at correct answer and exhibit incorrect\nreasonings (these account for 50.6% of all reasoning chain candidates), 40.6% are filtered\nout by our verification process.\nThe above statistics shows that a significant portion of reasoning chains that arrive at correct answers\nbut exhibit incorrect reasoning are successfully eliminated. Therefore, the reliability and trustfulness\nof reasoning chains that arrive at the correct answers are significantly improved. Combined with the\nfact that a significant proportion of reasoning chains that exhibit incorrect answers are eliminated, and\nthat our approach\u2019s verification accuracy significantly improves over naive verification approaches,\nour primary goal to improve LLM reasoning reliability is accomplished.\nNevertheless, the removals of many reasoning chains yielding correct answers (specifically, a signifi-\ncant 46.2% \u00d7 49.4% of all chains) has a notable impact. This even exceeds the removals of reasoning\nchains with incorrect reasonings and answers (40.6% \u00d7 50.6% of all chains). As a result, there are\nfewer votes for the correct answer when generating final answers through majority voting, which\nlimits the final answer accuracy. In the future, we believe that when a greater proportion of incorrect\nreasoning chains with incorrect answers are filtered out, we can improve the final answer accuracy.\nC\nMore Details on Answer Extraction\nIn this section, we describe our process to extract the final answer from language models\u2019 responses.\nThe process begins by selecting the last three non-empty lines. Then, these lines are processed\nthrough the following pipeline:\n1. Firstly, we use a list of regular expressions to identify \"No-Answer\" patterns within the text,\nsuch as \"we cannot answer (this|the) question\". This process helps us ascertain whether the\nmodel can provide a conclusive answer. If any such patterns appear in the text, we mark\n\"No answer!\" as the final answer. However, if we don\u2019t detect these patterns, we proceed to\nthe next steps for extracting the final answer.\n2. Secondly, if any \"Answer-Split\" patterns are found in the text, we divide the text into several\nblocks using the identified pattern. The last block of text is then utilized for extracting the\nanswer.\n16\n3. Lastly, we use regular expressions, as outlined in Tab. 11, to scan the remaining text for\npossible final answers. If multiple matches are found for the pattern, we select the first\nmatch as the final answer. If no pattern matches are found in the remaining text, we default\nthe final response to \"No answer!\".\n\u201cNo-Answer\u201d Patterns: \"we cannot provide an answer to this question with (this|the) given informa-\ntion\", \"we cannot answer (this|the) question\", \"we cannot determine\", \"we can\u2019t determine\", \"we do\nnot have enough information to answer (this|the) question\", \"we do not have enough information to\nprovide a definitive answer to (this|the) question\", \"the answer(.*?)is unknown\", \"answer is not listed\namong the answer choices\".\n\u201cAnswer-Split\u201d Patterns: \"answer is\", \"final answer:\", \"answer to the question is\", \"answer to this\nquestion is\", \"concatenated letters are\", \"concatenate the letters -\", \"The answer of \".\nAnswer Type\nRegular Expression\nNumber\n(-?\\d[\\d,\\. ]*)\nFractional number\n(-?\\(\\d+\\/\\d+\\)\\/\\d+|-?\\d+\\/\\d+)\nDate\n(\\d\\d\\/\\d\\d\\/\\d\\d\\d\\d)\nYes or No\n(?:Yes|No|yes|no|NO|YES)\nTable 11: Regular Expression for extracting the final answers of different kinds of questions.\nD\nPrompts\nD.1\nPrompt for Direct Reasoning Chain Verification Without Natural Program Format\nFor the results in Tab. 2 of the main paper, We use \u201cDo you think the above reasoning process is\ncorrect? Let\u2019s think step by step.\u201d as the zero-shot prompt to verify an entire reasoning chain at once.\nWe also design a two-shot prompt for reasoning chain verification as shown in Tab. 12, which covers\none correct reasoning chain and one incorrect reasoning chain.\nD.2\nPrompts for Reasoning Chain Generation in the Natural Program Format\nTo instruct models to generate reasoning chains in the Natural Program format that facilitates step-\nby-step deductive verification, we have designed four distinct prompts to address different types of\nproblems. These include:\n1. Math word problems, as illustrated in Tab. 13, covering GSM8K, MATH, and AddSub\ndatasets.\n2. Math word problems with multiple-choice options, illustrated in Tab. 14, covering the AQuA\ndataset.\n3. Date-related problems, illustrated in Tab. 15, covering the Date dataset.\n4. Last Letters problems, illustrated in Tab. 16, covering the Last Letters dataset.\nD.3\nPrompt for Deductive Verification Following Natural Program Format and Step-by-Step\nDecomposition\nWe have designed a general one-shot prompt for the deductive verification of a single reasoning step\non different datasets, as shown in Tab. 17. This prompt serves to instruct language models to generate\nthe deductive validity of each reasoning step as illustrated in Sec. 4.2 and the top-right box of Fig. 1\nof the main paper.\nE\nMore Deductive Verification Examples\nIn this section, we present more deductive verification examples using our Natural Program-based\napproach on single reasoning steps.\n17\nIn Tab. 18, we demonstrate that the language model (ChatGPT) not only successfully identifies\nungrounded information, but also identifies logical errors within the given solutions.\nIn Tab. 19, we illustrate a case where the language model fails to detect ungrounded premise numbers,\nmistakenly assuming that these numbers can be derived from grounded ones.\nLastly, in Tab. 20, we illustrate a case where the language model is sometimes unable to correctly\nidentify grounded numbers.\n18\nHere is a math question and its solution:\n\"Question:\nJason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\ndid Jason give to Denny?\nAnswer:\nJason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 + 12 =34.\nThe answer is 34.\"\nYou are a math teacher. Do you think the reasoning process is correct?\nLet\u2019s check the \"Answer\" step by step, and then end with \"yes\" or \"no\".\nAnswer:\nThe problem statement is clear, but the reasoning process has a major error in it.\nLet\u2019s go through it step by step:\n1. \"Jason started with 20 lollipops.\" This is correct according to the problem statement.\n2. \"Then he had 12 after giving some to Denny.\" This is also correct as per the problem.\n3. \"So he gave Denny 20 + 12 =34. The answer is 34.\" This is incorrect. The correct reasoning process\nshould be subtracting the number of lollipops he ended up with from the number he started with.\nThat would be 20 - 12 = 8 lollipops.\nSo, Jason gave Denny 8 lollipops, not 34. No, the reasoning process is incorrect.\nHere is a math question and its solution:\n\"Question:\nOlivia has $23. She bought five bagels for $3 each. How much money does she have left?\nAnswer:\nOlivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left.\n23 - 15 is 8. The answer is 8.\"\nYou are a math teacher. Do you think the reasoning process is correct?\nLet\u2019s check the \"Answer\" step by step, and then end with \"yes\" or \"no\".\nAnswer:\nLet\u2019s go through this reasoning process step by step.\n1. \"Olivia had 23 dollars.\" This is the given starting amount of money that Olivia has. This step is correct.\n2. \"5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.\" This is correct as well. Olivia spends 3 dollars\nfor each bagel, and she buys 5 of them. So the total cost of the bagels is 3 dollars/bagel x 5 bagels = 15 dollars.\n3. \"So she has 23 - 15 dollars left.\" This statement is also correct. After purchasing the bagels, Olivia will\nhave her initial amount of money minus the cost of the bagels.\n4. \"23 - 15 is 8. The answer is 8.\" This step is correct as well. 23 dollars (Olivia\u2019s initial amount) - 15 dollars\n(the cost of the bagels) = 8 dollars. This means that Olivia has 8 dollars left after buying the bagels.\nSo overall, Olivia has 8 dollars left after buying five bagels for 3 dollars each. Yes, the reasoning process\nis correct.\nTable 12: Two-shot prompt for direct reasoning chain verification without Natural Program format.\n19\nQuestion:\nAlice buys 5 apples. Each apple costs her $2. She gets them for 50% off. After she bought\nshe decides she doesn\u2019t like 2 of them and sells them to her friends for 1. How much money\nwas she out?\nAnswer:\nFirst, let\u2019s write down all the statements and relationships in the question with labels.\n#1. Alice gets 5 apples.\n#2. Each apple costs $2, but Alice gets apples for 50% off.\n#3. Alice sells 2 of 5 apples for 1.\n#4. How much money was Alice out?\nNext, let\u2019s answer the question step by step with reference to the question and reasoning process:\n#5. (by #2) Step 1: Calculate the price per apple after a 50% discount.\nOriginal price per apple: $2\nDiscount: 50%\nPrice per apple after discount: $2 * (1 - 0.50) = $2 * 0.50 = $1\n#6. (by #1 #5) Step 2: Calculate Alice\u2019s cost of the 5 apples after a 50% discount.\nPrice per apple after discount: $1\nTotal apples: 5\nAlice\u2019s total cost of 5 apples: $1 * 5 = $5\n#7. (by #3) Step 3: Calculate the money Alice gets back from selling 2 apples.\nMoney that Alice gets back from selling 2 apples: $1\n#8. (by #6 #7) Step 4: Calculate the total amount of money Alice is out.\nAlice\u2019s total cost of 5 apples: $5\nMoney that Alice gets back from selling 2 apples: $1\nMoney that Alice is out: $5 - $1 = $4\n#9. (by #4 #8) The original question is #4.\nHow much money was Alice out? We do not miss information on the rewritten labels. So the\nanswer to this question is Alice is out $4.\nPlease answer the following question by following the above template.\nTable 13: One-shot Natural Program prompt for reasoning chain generation on math word problems.\n20\nQuestion:\nAlice buys 5 apples. Each apple costs her $2.\nShe gets them for 50% off. After she bought She decides she doesn\u2019t like 2 of them and sells them\nto her friends for 1.\nHow much money was she out?\nAnswer Choices:\nA) 4\nB) 2\nC) None of the above\nAnswer:\nFirst, let\u2019s write down all the statements and relationships in the question with labels.\n#1. Alice gets 5 apples.\n#2. Each apple costs $2, but Alice gets apples for 50% off.\n#3. Alice sells 2 of 5 apples for 1.\n#4. How much money was Alice out?\nNext, let\u2019s answer the question step by step with reference to the question and reasoning process:\n#5. (by #2) Step 1: Calculate the price per apple after a 50% discount.\nOriginal price per apple: $2\nDiscount: 50%\nPrice per apple after discount: $2 * (1 - 0.50) = $2 * 0.50 = $1\n#6. (by #1 #5) Step 2: Calculate Alice\u2019s cost of the 5 apples after a 50% discount.\nPrice per apple after discount: $1\nTotal apples: 5\nAlice\u2019s total cost of 5 apples: $1 * 5 = $5\n#7. (by #3) Step 3: Calculate the money Alice gets back from selling 2 apples.\nMoney that Alice gets back from selling 2 apples: $1\n#8. (by #6 #7) Step 4: Calculate the total amount of money Alice is out.\nAlice\u2019s total cost of 5 apples: $5\nMoney that Alice gets back from selling 2 apples: $1\nMoney that Alice is out: $5 - $1 = $4\n#9. (by #4 #8) The original question is #4. How much money was Alice out? We do not miss\ninformation on the rewritten labels. So the answer to this question is Alice is out $4. Among\nall the answer choices, the best option is A) 4.\nPlease answer the following question by following the above template.\nTable 14: One-shot Natural Program prompt for reasoning chain generation on math word problems\nwith multiple choice.\n21\nLet\u2019s work on date computation.\nQuestion:\nYesterday is 02/22/2012. What is the date one week from today? Please answer in format\nMM/DD/YYYY.\nAnswer:\nFirst, let\u2019s write down all the statements and relationships in the question with labels.\n#1. Yesterday is 02/22/2012.\n#2. What is the date one week from today? Please answer in format MM/DD/YYYY.\nNext, let\u2019s answer the question step by step with reference to the question and reasoning process:\n#4. (by #1) Step 1: Calculate the date today.\nThe date of Yesterday is 02/22/2012.\nThe date of today is 02/23/2012.\n#5. (by #2 #4) Step 2: Calculate the date one week from today and write the answer in\nMM/DD/YYYY.\nBecause 2012 is a leap year and February in 2012 has 29 days, the date one week from today is\n03/01/2012.\nTherefore, date one week from today is 03/01/2012.\nQuestion:\nToday is 3/2 and Bob is watching NBA 2007. What is the date one week before today?\nPlease answer in format MM/DD/YYYY.\nAnswer:\nFirst, let\u2019s write down all the statements and relationships in the question with labels.\n#1. Today is 3/2.\n#2. Bob is watching NBA 2007 today.\n#3. What is the date one week before today? Please answer in format MM/DD/YYYY.\nNext, let\u2019s answer the question step by step with reference to the question and reasoning process:\n#4. (by #1 #2) Step 1: Calculate the date today.\nToday is 3/2 and Bob is watching NBA 2007 today. Thus, Today is 03/02/2007.\n#5. (by #3 #4) Step 2: Calculate the date one week before today and write the answer\nin MM/DD/YYYY.\nBecause 2007 is not a leap year and February in 2007 has 28 days, the date one week from today\nis 02/23/2007.\nTherefore, the date one week before today is 02/23/2007.\nPlease answer the following question by following the above template.\nTable 15: Two-shot Natural Program prompt for reasoning chain generation on the Date dataset.\n22\nLet\u2019s work on date computation.\nQuestion:\nTake the last letters of each words in \"Apple Is\" and concatenate them.\nAnswer:\nFirst, let\u2019s write down all the statements and relationships in the question with labels.\n#1. We need to take the last letter of the word \"Apple\".\n#2. We need to take the last letter of the word \"Is\".\n#3. We need to concatenate the last letters of the words sequentially\nNext, let\u2019s answer the question step by step with the complete reference to the question and reasoning process:\n#4. (by #1, #3) Step 1: Taking the last letter of the word \"Apple\" and concatenate the latter.\nThe last letter of the word \"Apple\" is \"e\".\nThe concatenated last letters are \"e\".\n#5. (by #2, #3, #4) Step 2: Taking the last letter of the word \"Is\" and concatenate the latter.\nThe last letter of the word \"Is\" is \"s\".\nThe concatenated last letters are \"es\".\n#6. (by #3, #5) Step 3: Provide the final answer of the concated letter.\nTherefore, the answer is \"es\".\nPlease answer the following question by following the above template.\nTable 16: One-shot Natural Program prompt for reasoning chain generation on the Last Letters\ndataset.\n23\nHere is some information:\n\u201c1. Alice gets 2 apples.\n2. Alice gets twice as many oranges as bananas.\n3. Original price per apple: $2\nDiscount: 50%\nPrice per apple after discount: $2 * (1 - 0.50) = $2 * 0.50 = $1\"\nBased on the given information, here is a reasoning process:\n\"Calculate Alice\u2019s cost of the 5 apples after a 50% discount.\nPrice per apple after discount: $1\nApples be eaten: 3\nOranges be eaten: 6\nTotal apples: 5\nTotal oranges: x\nTotal bananas: 2x\nAlice\u2019s total number of fruits: 5 + x + 2x = 5 + 4x\nAlice\u2019s total cost of 5 apples: $1 * (5 - 3) = $2\u201d\nDouble-check the reasoning process, let\u2019s analyze its correctness, and end with \"yes\" or \"no\".\nAnswer:\nLet\u2019s think step by step.\nGrounding check\nPurpose: Find references for numbers in the reasoning process sequentially\nRule: Any ungrounded number makes the check fails\n\"x\" is a variable, we do not care about variables.\n\"y\" is a variable, we do not care about variables.\n\"5\" is a quantity from the given information that represents the number of apples.\n\"50%\" is a quantity from the given information that represents the discount.\n\"$1\" is a quantity from the given information that represents the price per apple after the discount.\n\"3\" is a quantity not from the given information, it is ungrounded.\n\"6\" is a quantity not from the given information, it is ungrounded.\n\"$2\" is a quantity from the given information that represents the original price per apple.\nResult: Based on the check, there are ungrounded numbers, so the grounding check fails.\nReasoning check:\nPurpose: Check whether the reasoning logic correctly answers the question\nRule: Any logic error makes the check fails\nTo calculate the total cost of apples after a 50% discount, we should multiply the number of apples\nby the discounted price. But to answer the question, we do not need to calculate the total number\nof fruit Alice gets.\nResult: Based on the check, there are some logic mistakes, so the reasoning check fails.\nCalculation check:\nPurpose: Check whether the calculation process is consistent\nRule: Any inconsistent calculation makes the check fails\ncalculation1:\nequation: $1 * (5 - 3), answer: $2\n(5 - 3) = 2\n$1 * 2 = $2 is consistent with the answer, so the calculation is correct.\ncalculation2:\nequation: 5 + x + 2x, answer: 5 + 4x\nx + 2x = 3x\n5 + 3x is inconsistent with the answer, so the calculation is incorrect.\nResult: Based on the check, the calculation process is inconsistent, so the calculation check fails.\nCheck results: Ground check fails, Reasoning check fails, Calculation check fails.\nRule: Any failed check makes the reasoning incorrect.\nSo the answer is \"no\".\nTable 17: One-shot prompt for deductive verification of a single reasoning step, following our Natural\nProgram format and step-by-step reasoning chain decomposition.\n24\nInput:\nHere is some information:\n\"For every 25 telephone calls he makes, he gets one person to come into the dealership.\"\nBased on the given information, here is a reasoning process:\n\"Calculate the number of people that would come into the dealership for 15 car sales.\nNumber of people needed to sell 15 cars: 15 * 2 = 30\"\nDouble-check the reasoning process, let\u2019s analyze its correctness, and end with \"yes\" or \"no\".\nAnswer:\nLet\u2019s think step by step without any assumptions.\nOutput:\nGrounding check:\nPurpose: Find references for numbers in the reasoning process sequentially\nRule: Any ungrounded number makes the check fails\n\"15\" is a quantity from the question that represents the number of car sales.\n\"2\" is a quantity not from the given information, it is ungrounded.\n\"25\" is a quantity from the given information that represents the number of telephone calls\nper one person to come into the dealership.\nResult: Based on the check, there are ungrounded numbers, so the grounding check fails.\nReasoning check:\nPurpose: Check whether the reasoning logic correctly answers the question\nRule: Any logic error makes the check fails\nIt is not clear how the number of people coming into the dealership relates to the number of car sales.\nThe given information only tells us the ratio of telephone calls to people coming into the dealership,\nbut it does not provide any information about the number of telephone calls made for each car sale.\nTherefore, the reasoning process is not correct.\nResult: Based on the check, there is a logic mistake, so the reasoning check fails.\nCalculation check:\nPurpose: Check whether the calculation process is consistent\nRule: Any inconsistent calculation makes the check fails\ncalculation1:\nequation: 15 * 2, answer: 30\n15 * 2 = 30 is consistent with the answer, so the calculation is correct.\nResult: Based on the check, the calculation process is consistent, but the reasoning is incorrect,\nso the calculation check fails.\nCheck results: Grounding check fails, Reasoning check fails, Calculation check fails.\nRule: Any failed check makes the reasoning incorrect.\nSo the answer is \"no\".\nTable 18: Successful case: our deductive verification approach successfully discovers ungrounded\ninformation and reasoning mistakes.\n25\nInput:\nHere is some information:\n\"How many hours would it take for 5 people to paint a whole house?\"\nBased on the given information, here is a reasoning process:\n\"Calculate the total number of hours it would take for 5 people to paint a whole house.\nTotal hours for one person to paint a whole house: 240 hours\nTotal number of people: 5\nTotal hours for 5 people to paint a whole house: 240 hours / 5 = 48 hours\"\nDouble-check the reasoning process, let\u2019s analyze its correctness, and end with \"yes\" or \"no\".\nAnswer:\nLet\u2019s think step by step without any assumptions.\nOutput:\nGrounding check\nPurpose: Find references for numbers in the reasoning process sequentially\nRule: Any ungrounded number makes the check fails\n\"240\" is a quantity not from the given information, it is ungrounded.\n\"5\" is a quantity from the given information that represents the number of people.\n\"48\" is a quantity calculated from the ungrounded number and grounded number in the reasoning process.\nResult: Based on the check, there is one ungrounded number, but it is calculated from another grounded\nnumber, so the grounding check passes.\nReasoning check:\nPurpose: Check whether the reasoning logic correctly answers the question\nRule: Any logic error makes the check fails\nThe reasoning process correctly calculates the total number of hours for 5 people to paint a whole house by\ndividing the total hours for one person to paint a whole house by 5. Therefore, the reasoning check passes.\nCalculation check:\nPurpose: Check whether the calculation process is consistent\nRule: Any inconsistent calculation makes the check fails\ncalculation1:\nequation: 240 / 5, answer: 48\n240 / 5 = 48 is consistent with the answer, so the calculation is correct.\nResult: Based on the check, the calculation process is consistent, so the calculation check passes.\nCheck results: Ground check passes, Reasoning check passes, Calculation check passes.\nRule: All checks pass make the reasoning correct.\nSo the answer is \"yes\".\nTable 19: Failure case: our deductive verification process fails to find out ungrounded information in\nthe reasoning step. The number 240 in the reasoning step is ungrounded, but the model states that it\ncan be calculated from grounded numbers.\n26\nInput:\nHere is some information:\n\"Calculate the number of doughnuts Derrick makes in a day.\nNumber of doughnuts in a dozen: 12\nNumber of doughnuts in 10 dozens: 10 * 12 = 120\nCalculate the revenue from selling one doughnut.\nPrice per doughnut: $2\"\nBased on the given information, here is a reasoning process:\n\"Calculate the revenue from selling all the doughnuts in a day.\nNumber of doughnuts in a day: 120\nRevenue from selling all the doughnuts in a day: $2 * 120 = $240\"\nDouble-check the reasoning process, let\u2019s analyze its correctness, and end with \"yes\" or \"no\".\nAnswer:\nLet\u2019s think step by step without any assumptions.\nOutput:\nGrounding check\nPurpose: Find references for numbers in the reasoning process sequentially\nRule: Any ungrounded number makes the check fails\n\"12\" is a quantity from the given information that represents the number of doughnuts in a dozen.\n\"10\" is a quantity not from the given information, it is ungrounded.\n\"$2\" is a quantity from the given information that represents the price per doughnut.\n\"120\" is a quantity not from the given information, it is ungrounded.\nResult: Based on the check, there are ungrounded numbers, so the grounding check fails.\nReasoning check:\nPurpose: Check whether the reasoning logic correctly answers the question\nRule: Any logic error makes the check fails\nThe reasoning process is correct. We have the number of doughnuts Derrick makes in a day,\nand we have the price per doughnut.\nMultiplying these two yields the revenue from selling all the doughnuts in a day.\nResult: Based on the check, the reasoning logic is correct.\nCalculation check:\nPurpose: Check whether the calculation process is consistent\nRule: Any inconsistent calculation makes the check fails\ncalculation1:\nequation: 10 * 12, answer: 120\n10 * 12 = 120 is consistent with the answer, so the calculation is correct.\ncalculation2:\nequation: $2 * 120, answer: $240\n$2 * 120 = $240 is consistent with the answer, so the calculation is correct.\nResult: Based on the check, the calculation process is consistent, so the calculation check passes.\nCheck results: Ground check fails, Reasoning check passes, Calculation check passes.\nRule: Any failed check makes the reasoning incorrect.\nSo the answer is \"no\".\nTable 20: Failure case: our deductive verification process sometimes treats grounded information as\nif they were ungrounded. The number 120 is provided in the given information, but the model states\nthat it is ungrounded.\n27\n"
  },
  {
    "title": "Natural Language Commanding via Program Synthesis",
    "link": "https://arxiv.org/pdf/2306.03460.pdf",
    "upvote": "2",
    "text": "Natural Language Commanding via Program\nSynthesis\nApurva Gandhi\nMicrosoft\napurvagandhi@microsoft.com\nThong Q. Nguyen\nMicrosoft\nthongnguyen@microsoft.com\nHuitian Jiao\nMicrosoft\nhuijia@microsoft.com\nRobert Steen\nMicrosoft\nrobsteen@microsoft.com\nAmeya Bhatawdekar\nMicrosoft\nambhataw@microsoft.com\nAbstract\nWe present Semantic Interpreter, a natural language-friendly AI system for pro-\nductivity software such as Microsoft Office that leverages large language models\n(LLMs) to execute user intent across application features. While LLMs are ex-\ncellent at understanding user intent expressed as natural language, they are not\nsufficient for fulfilling application-specific user intent that requires more than\ntext-to-text transformations. We therefore introduce the Office Domain Specific\nLanguage (ODSL), a concise, high-level language specialized for performing\nactions in and interacting with entities in Office applications. Semantic Inter-\npreter leverages an Analysis-Retrieval prompt construction method with LLMs\nfor program synthesis, translating natural language user utterances to ODSL pro-\ngrams that can be transpiled to application APIs and then executed. We focus our\ndiscussion primarily on a research exploration for Microsoft PowerPoint.\n1\nIntroduction\nProductivity apps like Microsoft Office provide a multitude of features to help users accelerate\ntheir daily work. Recently, LLMs have led to a surge of interest in building copilots or assistive\nexperiences that allow users to interact with apps through natural language. As a core component\nof such experiences, a natural language commanding interface has the potential to transform every\nuser into a power user by making an app\u2019s whole commanding surface accessible through natural\nlanguage. Users can simply express what they want to do in terms of outcomes (e.g., \u201cBold all the\nkeypoints\u201d or \u201cInsert a poem by Tagore and make it look beautiful\u201d) and see it executed before their\neyes. In this work, we explore an approach for building such a component: Semantic Interpreter is a\nnatural language-friendly AI system that leverages and enhances the power of large language models\n(LLMs) to execute user intent across application features.\nTo do this, Semantic Interpreter must both understand and fulfill user intent expressed as natural\nlanguage. LLMs excel at the former but are not sufficient for the latter. LLMs implement text-to-text\ntransforms. Therefore, while they are excellent at understanding user intent expressed as natural\nlanguage, they are fundamentally unable to fulfill application-specific user intent that requires more\nthan text-to-text transformations (e.g., \u201ccreate a new slide,\u201d \u201cinsert a poem about hummingbirds in a\nblue rectangular box,\u201d etc.)\nTo allow LLMs to encode application-specific user intent and communicate this to the application\nin a way that can be handled, we require a text-based representation of the user intent that our\napplication is able to interpret. A program representation, which encodes user intent with a domain-\nspecific language (DSL), elegantly satisfies this requirement. A domain-expert can organically inject\narXiv:2306.03460v1  [cs.LG]  6 Jun 2023\napplication-specific knowledge in the implementation of the DSL\u2019s interpreter. In this way, a DSL\nprovides a powerful abstraction that allows for a separation of concerns between intent understanding\nand fulfillment, letting us architect a system that appropriately leverages and fuses the strengths of\nLLMs with application-specific, domain-expert knowledge.\nWe have seen that LLMs are capable of not just generating text but code as well [1, 2]. Semantic\nInterpreter exploits this powerful capability of LLMs to perform program synthesis \u2013 understanding\nuser intent expressed as natural language and generating an executable program to fulfill this intent.\nSemantic Interpreter augments the powerful but approximate capabilities of LLMs with the precision\nof symbolic representations by leveraging LLMs to transpile natural language user utterances to\nverifiable plans expressed as DSL programs.\nThere are several challenges to leveraging LLMs in this way. First, LLMs are pretrained on large-\nscale collections of code repositories but are not fine-tuned on the APIs of a specific application.\nFurthermore, the resources and number of examples required for such fine-tuning may be impractical\nfor many domains. We therefore leverage a few-shot prompting or in-context learning approach where\nwe include relevant DSL syntax description, example user utterances, DSL programs, application\ncontext data and additional system instructions as part of the prompt. However, LLMs typically have\nstrict token limits for context length. For example, the GPT-3 and GPT-3.5 models from OpenAI\nhave a token length limit of 4097 tokens and while more recent models like GPT-4 have much large\ntoken limits, the cost of inference often scales with the number of tokens1. To address this constraint,\nin addition to focusing on creating a concise DSL syntax, we use a just-in-time prompt engineering\nmethod that dynamically constructs a prompt tailored to the user utterance by choosing the most\nrelevant DSL program examples and other prompt artifacts. We call our prompt engineering method\nan analysis-retrieval method (ARM) as it builds on top of retrieval-augmented generation (RAG) [3]\nmethods with additional analysis steps that leverage classifiers to help narrow down the search space\nof prompt artifacts we select.\nA second challenge of using LLMs with a few-shot prompting approach is that they are prone to\nhallucinations, often generating output that could be inaccurate or inconsistent with the prompts\nprovided to them [4]. This is exacerbated when using the LLM to generate action plans in a DSL or\nAPI that the LLM was not pretrained on \u2013 the LLM may generate code that is semantically incorrect,\nhave compile-time errors (syntax errors, unsupported statements/parameters, type errors, etc.) or\nresult in runtime errors. To help reduce program synthesis errors, we create the Office Domain\nSpecific Language (ODSL), an LLM-friendly API for performing actions in Office applications.\nEvaluation of natural language commanding systems also poses a challenge. Often, user queries are\nabstract and under-specified with numerous valid interpretations. For example, for a user query such\nas \u201cMake the slide look beautiful,\u201d there is no single correct interpretation. Numerous actions or\ncombinations of them can satisfy this intent; e.g., adding an image, animating the slide, changing\ntext formatting properties, etc. In this work, since we synthesize programs to fulfill user intent, we\nreformulate the problem of evaluating the natural language commanding systems into the problem of\nanalyzing program equivalence.\nIn summary, our contributions are:\n\u2022 We discuss the design of The Office Domain Specific Language (ODSL) \u2013 a high-level,\nLLM-friendly language specialized for performing actions in and interacting with entities in\nOffice Applications.\n\u2022 We describe the architecture of Semantic Interpreter, which leverages an analysis-retrieval\nprompt engineering framework with LLMs to translate natural language user queries to\nODSL programs that can be interpreted by Office applications.\n\u2022 We describe a procedure for evaluating natural language commanding systems that leverage\nprogram synthesis.\nWhile our framework is general and applies to Office applications and other productivity apps in\ngeneral, we focus our discussion with an exploration for PowerPoint.\n1https://openai.com/pricing\n2\n2\nRelated Work\n2.1\nProgram Synthesis\nProgram synthesis refers to automatic generation of programs from given specifications. There are\ndifferent types of specifications that can be used for program synthesis, such as complete formal\nspecifications [5\u20138], input-output examples [9\u201313], and natural language descriptions [14]. Each type\nof specification has its own advantages and limitations. Complete formal specifications are precise\nand unambiguous, but they are difficult to write and verify by end-users. Input-output examples are\neasy to provide and intuitive to understand, but they may require many examples to capture the user\u2019s\nintent or they may be hard to construct for some domains. Natural language descriptions, which is\nthe focus of our work, are natural and expressive, but they may be incomplete or ambiguous.\nThere are several existing works that use natural language descriptions for program synthesis in\ndifferent domains. For example, PAL [15] and PoT [16] generate Python programs which are\noffloaded to an external interpreter as the intermediate reasoning steps to solve problems expressed\nin natural language. Codex [1], CodeGen [2] and PaLM [17] are LLMs that can generating code\nfrom natural language input description. There are also earlier approaches that construct novel\nneural-network architectures optimized for program synthesis [18\u201321]. Synthesizing programs in\ngeneral-purpose programming languages, however, has shown several challenges and limitations in\nessential aspects such as handling inconsistency in natural language, generating correct and efficient\nprograms, and ensuring semantic grounding and alignment between natural language and code\n[22]. There are also safety concerns due to hallucination risks associated with LLMs [4]. Semantic\nInterpreter aims to address these issues by grounding program synthesis in relevant program samples\nselected using our analysis-retrieval method for prompt construction. Furthermore, we combine\nLLM-based program synthesis with static program analysis to find and auto-correct common errors in\nthe generated program, taking a neurosymbolic approach to improve robustness of program synthesis.\n2.2\nNeurosymbolic Methods\nNeurosymbolic methods integrate deep learning with symbolic representations [23]. The objective is\nto learn functions from data, as in classical machine learning. However, unlike conventional machine\nlearning methods that learn black-box models, neurosymbolic programming aims to learn interpretable\nand verifiable symbolic representations like programs in a domain-specific language (DSL) that are\nconsistent with the given data. Neurosymbolic programming offers several advantages over traditional\nmachine learning approaches, such as data efficiency, generalization, and explainability [24, 25].\nSeveral works have explored the use of neurosymbolic programming in various applications, such as\nprogram induction [12], causal effect estimation [26], computer graphics [27], task-oriented dialog\nsystems [28], and scientific discovery acceleration [29]. One of the main challenges in neurosymbolic\nprogramming is to design a DSL that is expressive enough to capture the desired functionality, but\nalso suitable to enable efficient and accurate program synthesis. Moreover, the DSL should be\naligned with the inductive biases and heuristics that are relevant for the task domain. For example,\nin the domain of regular expression based string transformations, Parisotto et al [12] designed a\nDSL that incorporates common string operations and regular expression syntax. In the domain of\ncausal inference, Reddy and Balasubramanian [26] designed a DSL that encodes assumptions and\nconstraints from causal inference literature. In this work, we introduce a DSL that captures the\nfunctionality and semantics of Office commanding. Additionally, Semantic Interpreter includes a\nsyntax validation and code correction procedure to ensure robustness of the synthesized program.\n2.3\nLLMs for Tool Learning\nThe development of LLMs that exhibit reasoning-like capabilities [30] has inspired a lot of research\non using LLMs for decision making tasks. Chain-of-Thought (CoT) is among pioneering works\nthat demonstrate how LLMs can emulate a \u201cthinking procedure\u201d for solving problems [31]. Several\nextensions and variations of CoT have been proposed, such as least-to-most prompting [32], zero-shot\nCoT [33], self-consistent reasoning [34], self-ask prompting [35], RCI framework [36], and tree of\nthoughts [37].\nThese works demonstrate that LLMs can mimic reasoning and decision making via natural language.\nHowever, natural language texts may not be sufficient or efficient for solving some problems that\n3\nrequire complex calculations, translations, or information retrieval. Therefore, a new paradigm\nhas emerged for tool learning with LLMs, which leverages external tools through text-based DSL\nstatements or API calls [38]. For example, Cobbe et al. [39] demonstrate how an LLM can leverage\na calculator to perform basic arithmetic operations in natural language. Parisi et al. [40] propose a\nTALM framework for interleaving text-based API calls (such as a QA system and a calculator) with\nthe natural language output of LLMs. They also present a method for iteratively bootstrapping tool-\nuse examples to improve the LLM\u2019s proficiency in using a tool. Schick et al. [41] generalize TALM\u2019s\napproach to a wider range of simple tools, including a machine translation system, a Wikipedia search\nengine, and a calendar, and introduce Toolformer, an LLM that can seamlessly interact with these\ntools.\nTool learning has been applied in various real-world scenarios. For example, WebGPT [42] fine-tunes\nGPT-3 [43] to interact with Bing search engine and outperforms human experts in information\nretrieval. WebShop [44] creates a web-based environment where an agent can browse and purchase\nproducts based on human instructions. Visual ChatGPT [45] interleaves various vision foundation\nmodels with ChatGPT to enable understanding and generating images. HuggingGPT [46] connects\nexisting models hosted by HuggingFace using a universal language interface, where the LLM serves\nas the orchestrator for task planning and calls existing models to handle tasks in specific domains,\nsuch as object detection and question answering. To our knowledge, Semantic Interpreter is the first\napplication in the Office productivity space that leverages tool-oriented learning with LLMs.\n3\nApproach\nFig. 1 illustrates the overall approach. Semantic Interpreter takes as input a natural language user\nutterance and relevant document context. The document context is encoded as a light-weight JSON\ntree data structure that includes document entities, content and properties that are relevant to the user\nutterance; Fig 2 provides an example. Semantic Interpreter then uses an analysis-retrieval method\n(ARM) to generate a prompt conditioned on the user utterance and document context. We discuss\nthe ARM prompt generation method in more detail in Section 5. This prompt is fed into an LLM\nto generate a program representation of the user utterance in the Office Domain Specific Language\n(ODSL) \u2013 an LLM-friendly DSL for performing actions in and interacting with content in Office\napplications. Detailed description and design principles of ODSL are laid out in Section 4. The ODSL\ninterpreter parses the ODSL program into an Abstract Syntax Tree (AST) and performs analysis to\nvalidate the program and identify errors. The AST is then passed through a code-correction layer\nthat leverages domain-specific knowledge to automatically fix bugs in the program when can be\ndone unambiguously or using some opinionated rules. Finally, the interpreter transpiles the ODSL\nAST to a program written in native application APIs (e.g., Office-JS2) which is then executed by the\napplication to fulfill the user intent.\n4\nThe Office Domain Specific Language (ODSL)\nThe Office Domain Specific Language (ODSL) is a high-level commanding language for representing\nactions to perform in Office Apps. The language is designed with application-specific syntax that\nencodes the commanding surface for each app. We can think of ODSL as an LLM-friendly API for\nOffice Applications.\nToday, while applications often have existing commanding APIs as libraries as part of general-purpose\nprogramming languages (JavaScript, C++, etc.), using these libraries as targets for LLM-based\nprogram synthesis leads to suboptimal performance in practice.\nGeneral-purpose programming languages are challenging targets for program synthesis as by nature\nthey are not scoped in design. For example, while LLMs often tend to output a basic program that\nseems reasonable, on closer inspection we see that they tend to hallucinate or forget details like\nimport statements, properties on objects, library function names, etc.\n2https://learn.microsoft.com/en-us/office/dev/add-ins/reference/\njavascript-api-for-office\n4\nODSL \nSample Bank\nSemantic Interpreter\nAnalysis-Retrieval Method\nFor Prompt Engineering\nSelect Relevant ODSL Samples, Syntax \nDescription, and Rules\nEntity Classifier\nRequires Context \nClassifier\nVector Search\nODSL Program Synthesis\nCall LLM\nConstruct Prompt\nAnalysis-Retrieval Method\nFor Prompt Construction\nSelect Relevant ODSL Samples, Syntax \nDescription, and Rules\nEntity Classifier\nRequires Context \nClassifier\nSemantic Search\nConstruct ODSL \nSynthesis Prompt\nODSL Interpreter\nProgram Validation\nAutomatic Code \ncorrection\nParser\n(ODSL \uf0e0 AST)\nTranspilation\n(ODSL \uf0e0 Native App \nAPIs)\nExecution\nDocument \nContext\nInput\nNL \nUtterance\ne.g., \u201cMake \nthe text look \nformal\u201d \n+\nResult\nODSL Program\nFigure 1: Illustration of Semantic Interpreter\u2019s architecture and overall approach. Semantic Interpreter\ntranslates a user utterance to an ODSL program by leveraging an an analysis-retrieval few-shot\nprompting approach with an LLM for program synthesis. The ODSL program is then validated and\ntranspiled to the app\u2019s native APIs for execution.\nUsing a general-purpose language also makes it challenging to ensure safe code. Unlike a scoped\nDSL, using a general-purpose language encourages the model to use all available language features\nand libraries, some of which are potentially unsafe.\n5\nFurthermore, general-purpose languages often have a multitude of ways to do the same thing. Beyond\nmaking the language difficult for the model to learn in a few-shot manner, this also reduces and/or\ncomplicates our ability to perform domain-specific code analyses and inferences. As we discuss in\nSection 4.3.3, our constrained language design in ODSL allows us to easily maintain an automatic\ncode correction layer that can be used to auto-correct buggy code generated by LLMs in many\nsituations.\nWhile in this paper we explore creating an ODSL specification for PowerPoint, other apps can build\nspecifications that adhere to a similar design philosophy that consists of three fundamental objectives:\n(1) Expressive and Extensible, (2) ML-Aware and (3) Robust.\n4.1\nExpressive and Extensible\nThe DSL should make it easy to express a wide variety of user intents, both descriptive and abstract,\nnot only today but also in the future as the app\u2019s functionality is extended. The building blocks of\nODSL are entities that represent the core data structures (and associated hierarchy) in the app and\nstatements that perform operations on these entities. ODSL is easy to extend with new functionality\nby simply adding a new entity and/or statement type.\n4.1.1\nEntities\nEntities are the main application-specific data structures that represent the types of objects that a\nuser would want to create and manipulate in the application. Entities can have a has-a relationship\nwith other entities defining a hierarchy. For example, in PowerPoint, the entity hierarchy is slide\n\u2192 shape \u2192 textRange, where a slide can contain one or more shapes and a shape can optionally\ncontain a textRange. To define this entity hierarchy, we closely follow the data structure and hierarchy\ndefined in the PowerPoint\u2019s open-source Office-JS and OOXML3 specifications. These specifications\nuse shape as an umbrella entity to represent many interesting data types such as geometric shapes,\nlines/curves, charts, textboxes, etc. TextRanges are contiguous ranges of text and therefore contain\nsmaller textRanges within them.\n4.1.2\nStatements\nStatements in ODSL are used to perform operations to interact with and create new entities in Office\napplications. Statements use syntax that loosely resembles function calls in Python, but unlike Python\nhave the convention of using snake_case only for statement names and camelCase for parameter\nand variable names. For each entity type, we have the following four types of statements:\nSelect Statements\nThese statements get a reference to an entity in the scope provided. We can\nprovide conditions to filter the selection on using additional statement parameters.\nListing 1: Examples of select statements.\n1\n# Gets all textRanges matching the string \"Hello\" from provided shapes.\n2\ntextRanges = select_text(scope=shapes, text=\"Hello\")\n3\n4\n# Gets the second triangle in the Current selection.\n5\nshape = select_shapes(shapeType=\"Triangle\", index=1)\nAll select statements have a scope parameter to specify the scope from which to select an entity.\nAn entity can be selected from any other entity above it in the hierarchy but not below it. For example,\nyou can provide a scope of type shapes to a select_text statement, but it is illegal to provide a\nscope of type text to a select_slides as slides are above textRanges in the entity hierarchy and\ntherefore textRanges cannot contain slides within them. Additionally, the scope parameter can be\nprovided with the special arguments \"Presentation\" and \"Selection\" which correspond to the\nwhole presentation and current user selection respectively. If no scope parameter is used, it defaults\nto the user selection.\nThe rest of the parameters in the select statement act as filter conditions to give more fine-grained\ncontrol on the entities to select. Select statements provide a declarative abstraction for iterating over\nthe document object model and obtaining references that satisfy filter criterion.\n3https://learn.microsoft.com/en-us/office/open-xml\n6\nInsert Statements\nThese statements create new instances of entities using the provided parameters\nand return a reference to them.\nListing 2: Examples of insert statements.\n1\n# Inserts new \"Title and Content\" slides after provided ones.\n2\nslides = insert_slides(precededBy=slides, layout=\"Title and Content\")\n3\n4\n# Insert a textbox into each of the slides provided.\n5\ntextbox = insert_shapes(shapeType=\"Textbox\", slides=slides)\nFormat Statements\nThese statements update (usually formatting) properties of the entities. They\ncontain many optional parameters that can be used to specify how to format the entities passed to\nthem.\nListing 3: Examples of format statements.\n1\n# Applies a set of formatting updates to provided shapes.\n2\nformat_shapes(shapes=shapes, fillColor=\"teal\", fillTransparency=0.2, top=50,\nleft=50, height=300, width=200, lineColor=\"#964B00\", lineTransparency=0.2)\n3\n4\n# Formats the text in textRanges with a set of formatting properties.\n5\nformat_text(textRanges=textRanges, bold=true, fontName=\"Times New Roman\",\nhorizontalAlignment=\"Left\", color=\"teal\", italic=true, underline=\"Wavy\")\nDelete Statements.\nThese statements delete all the instances of entities passed to the statement.\nListing 4: Examples of delete statements.\n1\n# Deletes shapes provided to the shapes parameter.\n2\ndelete_shapes(shapes=shapes)\n3\n4\n# Deletes the text in the textRanges\n5\ndelete_text(textRanges=textRanges)\nNotice parallels between this model and the simple but highly expressive CRUD (create, read, update\nand delete) model used in REST APIs, SQL, etc. We found that these statements are expressive\nenough to represent the majority of user utterances, while also keeping the language constructs\ncompact and limited.\nIn addition to the entity-specific statements above, ODSL also allows for delegation statements that\nare wrappers around specialized services or models (e.g., text-to-image model, spell-checker, etc.)\nThis allows Semantic Interpreter to automatically extend its capabilities with specialized intelligence\ncapabilities seamlessly and with little effort. An example of a delegation statement in ODSL for\nPowerPoint is an insert_images statement that can internally delegate to models such as OpenAI\u2019s\nDALL\u00b7E 2 [47].\nListing 5: Examples of insert_images delegation statement.\n1\n# Insert images consistent with description into slides provided using DALLE-2.\n2\nimages = insert_images(slides=slides, description=\"A man walking a dog.\")\n4.2\nML-Aware\nDesigning a custom DSL gives us the unique opportunity to design a language that is conducive\nto program synthesis via LLMs. We discuss some of our design choices to help make the ODSL\nLLM-friendly.\n4.2.1\nUniformity\nLLMs like GPT models are trained to take a string prompt and generate new text that follows and is\nconsistent with that prompt. To do this, LLMs must recognize patterns in the prompt and generate\nnew text that are consistent with those patterns. As we discuss in Section 5, we provide LLMs with\na prompt that contains program examples written in ODSL. The more uniform or formulaic the\n7\nlanguage is, the easier it is for the LLM to pick up on these patterns in a few-shot setting and generate\nnew programs that are valid and consistent with the language\u2019s grammar.\nAs discussed in Section 4.1, ODSL has the same four types of statements across each entity and each\nstatement follows the same naming conventions as well. Similarly, each type of statement follows\nsimilar conventions across entities; e.g., all select statements have a scope parameter followed by\nadditional parameters that act as filter conditions for selection as discussed above. Even delegation\nstatements like insert_images follow the same naming conventions as the other statements to\nhelp the model generalize. Uniform statement types across entities also lead to common access and\nmanipulation patterns; e.g., every format or delete statement acting on a particular entity variable\nis always preceded by a corresponding insert or select statement. Another choice made was to\nmake no distinction between lists of entities and singular entities: select and insert statements\nalways return a list of zero or more entities and similarly format and delete statements operate on\na list of zero or more entities.\n4.2.2\nCompactness\nA major constraint in using LLMs is the token limit for prompt length. For example the GPT-3\nmodels have a limit of 4097 tokens for inference, in which we have to fit both the prompt and the\nmodel output. While newer models now have much more liberal token limits, the cost of inference\ngenerally scales proportional to the number of tokens.\nAs we discuss in Section 5, the prompt we provide to the LLM in our approach for program synthesis\nis quite crowded, including ODSL syntax description, few-shot ODSL program examples, document\ncontext examples and the current user utterance and document context. To fit all of this in a limited\ntoken budget, it is critical to design a DSL that has compact syntax to reduce the length of the syntax\ndescription and length of the program representations both in examples we provide to the LLM and\nthe LLM response.\nWe have found that using the four statement types per entity is a good abstraction, keeping the\nlanguage expressive enough to represent most user intents while still being minimal and not having\ntoo many statements or constructs.\nTo see the importance of choosing appropriate statement design for compactness, consider the\nexample in Listing 6 which compares ODSL to another possible approach where instead of having a\nsingle statement to handle formatting like we do in ODSL, we decide to create a separate statement\nfor each formatting property. Compared to ODSL, this latter approach is much less compact and\nconsumes more than 1.5x the number of tokens with no functional advantage.\nListing 6: Importance of compact DSL design.\n1\n# ODSL example of formatting some text with various properties\n2\n# This program consumes 50 tokens with the GPT-3 tokenizer.\n3\ntext = select_text()\n4\nformat_text(textRanges=text, bold=true, fontName=\"Times New Roman\",\nhorizontalAlignment=\"Left\", color=\"teal\", italic=true, underline=\"Wavy\")\n5\n-----------------------------------------------------------------------------\n6\n# Alternate possible DSL with separate statement for each formatting property.\n7\n# These programs consumes 78 tokens with the GPT-3 tokenizer.\n8\ntext = select_text()\n9\nset_bold(text, true)\n10\nset_font_name(text, \"Times New Roman\")\n11\nset_horizontal_alignment(text, \"Left\")\n12\nset_font_color(text, \"teal\")\n13\nset_italic(text, true)\n14\nset_underline(text, \"Wavy\")\n4.2.3\nMinimizing Redundancy\nWhile it is not uncommon in traditional API design to build in multiple ways to do the same thing,\nout of convenience for programmers, this is suboptimal when designing languages/APIs for program\nsynthesis with LLMs. With multiple ways to do the same thing, we can confuse the model and it is\nharder for the LLM to learn and generalize on how to perform a particular task from a few examples\nwhen the examples themselves are not consistent in how they perform the task.\n8\nOne example of a choice made to reduce redundancy in ODSL is not repeating formatting properties\nin insert statements. For example, while we could include formatting properties like bold,\nfontName, etc. in the insert_text statement, we do not as one can achieve the same functionality\nby chaining the insert_text statement with a format_text statement to update these properties.\n4.2.4\nDocument Context Co-design\nTo design the interaction model of how a program can represent referencing and manipulating existing\nentities in the application, we need the syntax to be context-aware \u2013 knowing what information the\nmodel has when synthesizing the program. Particularly, when designing the document context tree,\nwe include entity identifier metadata (e.g., index and names) with keys that match the ODSL select\nstatement parameter names, making it easier for a model to describe the entities it wants references to\n(shown in Fig. 2).\nClassified as Microsoft Confidential\n7\n{\n\"slides\": [\n{\n\"index\": 0,\n\"shapes\": [\n{\n\"index\": 0,\n\"textRange\": {\n\"text\": \"Dogs\",\n\"color\": \"#000000\",\n\"bold\": true,\n\"fontName\": \"Segoe UI\",\n\"fontSize\": 36,\n\u2026\n},\n\"name\": \"Title 2\",\n\"type\": \"Textbox\",\n\"fillColor\": \"\",\n\"top\": 50,\n\"height\": 75,\n\u2026\n},\n{\n\"index\": 1,\n\"textRange\": {\n\"text\": \"Dogs are the best.\",\n\u2026\n},\n\"name\": \"Content Placeholder 1\"\n\u2026\n}\n]\n}\n]\n}\nDocument Context Tree\nslide = select_slides(index=0)\ntitle = select_text(scope=slide, name=\"Title 2\")\nFigure 2: Illustration of how ODSL statements are aware of the document context tree included in\nthe ODSL synthesis prompt. Here we see that select statements have parameters that match keys of\nentity identifiers in the context tree.\n4.3\nRobust\nGiven that the DSL is the interface between the LLM and the application, the design of the DSL has\ndirect influence on the robustness of the overall system. Below we discuss a few strategies when\ndesigning ODSL and its interpreter to improve the overall robustness of the system.\n4.3.1\nConstraining the DSL to Safe Operations\nWe are intentional about the scope of the language, including only statements that cannot take us\nto an irreversible or invalid statements. Particulary, we limit ourselves to content generation and\nmanipulation operations that can be reversed by the user with a simple undo operation. We do not\ninclude any operations that can take us to an illegal state, such as a close_file() statement.\n9\n4.3.2\nSyntax Validation for Compile-time Errors and Minimizing Runtime Errors\nWhenever possible, we try to catch errors at compile time rather than runtime. Runtime errors can\nlead to suboptimal user experience where we halt a program in the middle of execution. Especially\nfor abstract user intents like \u201cmake the slide beautiful,\u201d this can lead to confusing user experience.\nFor example, an image could be inserted on top of existing content, but the program may abort before\nrepositioning the image to a free spot on the slide, making the resulting slide ironically look uglier\nthan before any action was taken.\nCompile-time errors are cleaner as they do not lead to partial program execution. If the system\nencounters a compile time error, it can surface a more informative error to the user, rather than\nexecuting a few statements and then failing unexpectedly. ODSL is statically typed, allowing us\nto perform robust compile-time syntax validation. Types span both the entity types that can be\nreturned by statements, and also fine-grained literal types which include the standard primitive\ntypes like Number, String, Boolean to more fine-grained and complex types like RangedNumber,\nStringOrRegex, etc. Examples of errors that syntax validation catches include type errors, variable\nout-of-scope errors, parsing errors, undefined statements, etc.\nOf course, not all errors can be caught at compile time. Still, common types of runtime errors are\ndesigned to execute as no-ops (e.g., selecting and manipulating an entity that does not exist) but not a\ncrash.\n4.3.3\nAutomatic Code Correction\nAn advantage of having an intentionally scoped, custom DSL is that we can use application-specific\nknowledge to auto-correct buggy code or at least use opinionated strategies to recover from certain\ntypes of errors. This greatly helps improve accuracy of code generation when working with LLMs.\nThe ODSL interpreter consists of a code correction layer as a mechanism to auto-correct many\ncompile-time errors we encounter.\nListing 7: Examples of ODSL Automatic Code Correction.\n1\n# Example 1: Fuzzy Enum Matching - Invalid Program: Typo in fontName\n2\ntext = select_text()\n3\nformat_text(textRanges=text, fontName=\"Cmic Sans\")\n4\n5\n# Auto-corrected program: Cmic Sans -> Comic Sans Ms\n6\ntext = select_text()\n7\nformat_text(textRanges=text, fontName=\"Comic Sans MS\")\n8\n-----------------------------------------------------------------------------\n9\n# Example 2: Statement Aliasing - Invalid Program: Incorrect statement name.\n10\nslide = select_slides()\n11\ninsert_picture(slides=slide, description=\"A picture of a cat\")\n12\n13\n# Auto-corrected program: insert_picture -> insert_images\n14\nslide = select_slides()\n15\ninsert_images(slides=slide, description=\"A picture of a cat\")\n16\n-----------------------------------------------------------------------------\n17\n# Example 3: Statement Injections - Invalid Program: Invalid enum value (Circle)\n18\nslide = select_slides()\n19\ninsert_shapes(slides=slide, shapeType=\"Circle\")\n20\n21\n# Auto-corrected program: Circle -> Ellipse & injected format_shapes\n22\nslide = select_slides()\n23\ncircle = insert_shapes(slides=slide, shapeType=\"Ellipse\")\n24\nformat_shapes(shapes=circle, height=100, width=100)\n25\n-----------------------------------------------------------------------------\n26\n# Example 4: Argument Value Correction - Invalid Program: Out-of-range value\n27\nselectedRectangles = select_shapes(shapeType=\"Rectangle\")\n28\nformat_shapes(shapes=selectedRectangles, fillTransparency=100)\n29\n30\n# Auto-corrected program: fillTransparency clamped to [0, 1] range\n31\nselectedRectangles = select_shapes(shapeType=\"Rectangle\")\n32\nformat_shapes(shapes=selectedRectangles, fillTransparency=1)\n10\n5\nAnalysis-Retrieval Method for ODSL Program Synthesis\nIn this section, we describe our approach to use an LLM to generate ODSL programs that satisfy user\nintent by leveraging an analysis-retrieval method (ARM) for prompt construction.\nA major constraint in many LLM systems is the token length limit. For example, the GPT-3.5 model\nfamily from OpenAI has a token length limit of 4097 tokens per API call, which in our approach,\nconsists of the various prompt components listed in Table 1. ARM is therefore designed to optimize\nfor ODSL program synthesis under a constrained number of allowable prompt tokens. This method\nconsists of two steps: (i) Analysis, where the user utterance is matched to a set of relevant ODSL\nentities used to filter ODSL syntax and other prompt artifacts shown to the LLM in the prompt, and\n(ii) Retrieval, where we use the analysis results to enhance the semantic search process to choose\nsample utterances and ODSL program examples similar to the user utterance from our ODSL sample\ndatabase.\nTable 1: Prompt components to generate a ODSL program\nComponents\nDescription\nSystem instruction\nGeneral introduction about Semantic Interpreter and ODSL.\nODSL syntax*\nSyntax examples of ODSL statements for entities associated with\nthe user utterance.\nRules*\nAdditional guidelines and instructions associated with entities to\nhelp condition the LLM further.\nFew-shot ODSL samples*\nSeveral examples of user utterances, document context (optional),\nand corresponding ODSL programs selected via semantic search\nInput utterance\nUtterance from user.\nCurrent document context*\nDocument context tree encodes document context and helps ground\ngeneration in current application context. Included optionally based\non result of the Requires Context classifier.\n* dynamically selected by ARM\nSpecifically, given a user utterance x and document context c, ARM\u2019s analysis step includes a\nclassifier p\u03b1(e|x, c) to determine a set E of ODSL entities e associated with x and whether the context\nc is needed for program synthesis. Detailed implementation of p\u03b1(e|x, c) is described in Section 5.1.\nThe retrieval step then leverages the analysis results to obtain a set Z of relevant ODSL sample\nprograms z from our database using a process called entity-aware semantic search p\u03c1(z|x, c, e). This\nanalysis-retrieval process can be represented as follows:\np(z|x, c) =\nX\ne\u2208E\np\u03c1(z|x, c, e)p\u03b1(e|x, c)\n(1)\nWe then use the retrieved sample set Z to condition the LLM to synthesize the ODSL program output\ny, similar to retrieval-augmented generation [3]:\np(y|x, c) =\nX\nz\u2208Z\np(z|x, c)p(y|x, c, z)\n(2)\n5.1\nAnalysis: Associated Entities and Document Context\nThe analysis component p\u03b1(e|x, c) consists of 2 steps: ODSL entity classification and context\nclassification. The entity classifier associates a given utterance with a set of entities. Note that\nthese entities can be a superset of the entities described in ODSL, to allow for more fine-grained\nanalysis. For example, in PowerPoint, although images are a subset of the umbrella shapes entity,\nwe separate them here for more fine-grained selection of prompt artifacts. The entities that we classify\nthe user utterance into for PowerPoint are presentation, slide, text, image, and shape.\n11\nEntity & Requires-Context Classifier\nSystem Instruction: There are 5 categories of entities in a PowerPoint presentation: text, image, shape, slide,\npresentation. You need to perform the following tasks:\n1. Categorize a given sentence into entity categories. Each sentence can have more than one category.\n2. Classify whether a sentence requires context. Context is required when additional information about the\ncontent of a presentation is required to fulfill the task described in the sentence.\n- Adding an image about a given topic does not require context.\n- Adding new text needs context to decide where to place the text on the current slide.\n...\nLet\u2019s think step by step. Here are some examples:\nUser: Make the title text on this slide red\nAssistant:\nCategories: text\nThoughts: We can select the title text and make it red without knowing the existing text properties. Therefore we\ndo not need context.\nRequiresContext: false\nUser: Add text that\u2019s a poem about the life of a high school student with emojis.\nAssistant:\nCategories: text\nThoughts: We need to know whether there is existing text on the slide to add the new poem. Therefore we need\ncontext.\nRequiresContext: true\n...\nUser: Create a presentation about climate change.\nAssistant:\nFigure 3: Sample prompt for entity and context classification.\nThe classified entities are used as input along with the user utterance to a requires-context classifier\nto determine whether the document context c is needed to fulfill the intent.\nThere are many ways to implement the entity and requires-context classifiers. In our experiments, we\nleverage LLMs to implement these with few-shot prompting. We use a single LLM call to jointly\nperform both classifier tasks, leveraging CoT technique to improve classification performance [31].\nAn illustrative example of the prompt format for the entity and requires-context classifiers is presented\nin Fig. 3.\n5.2\nRetrieval: Entity-Aware Semantic Search\nThe retrieval component p\u03c1(z|x, c, e) based on dense passage retrieval (DPR) [48] retrieves relevant\nsamples similar to the input utterance for few-shot prompting. However, unlike DPR which chooses\nsamples purely based on semantic similarities, ARM\u2019s retrieval component ensures the selected\nsamples represent all entities associated with the user utterance. This process includes 5 steps:\nSample Bank Construction\nWe construct a sample bank B that consists of pairs of sample\nutterances and the corresponding ODSL programs. Each sample can also include sample document\ncontext c if needed, and is tagged with a set of associated entities used by the analysis process to filter\nexamples based on matched entities.\n12\nSample Utterance Normalization\nTo help prevent spurious matches when performing a semantic\nsearch over sample utterances in B, we standardize the sample utterances in a process called nor-\nmalization. In this process, we modify the sample utterance to remove specificity and standardize\nall intent-descriptive verbs. For examples, all generative actions, such as add, insert, create,\ngenerate, are standardized as add; all descriptive keywords are changed into generic descriptions,\nso that \u201cred\u201d and \u201cblue\u201d become \u201ccolor,\u201d \u201cfunny style\u201d or \u201cserious tone\u201d becomes \u201ca given style,\u201d etc.\nSample Utterance Embedding\nThe embedding process is similar to DPR. We use an LLM-based\nencoder E to embed each normalized utterance \u00afbi \u2208 B into an embedding vector qi = E(\u00afbi). During\nruntime, the same encoder is used to compute the embedding vector E(x) for the input utterance x.\nAlgorithm 1: The entity-aware dynamic selection algorithm\n1 function EntityAwareDynamicSelection (x, B, E, k) \u2192 R;\nInput\n:User utterance x, sample bank B, list of associated ODSL entities E, and minimum\nnumber of samples to retrieve k\nOutput :A set of retrieved samples R\n2 Relevant samples \u00afB \u2190 {};\n3 List of samples and similarity scores pairs S \u2190 [];\n4 foreach bi \u2208 B do\n5\nif entities(bi) \u2282 E then\n6\n\u00afB.insert(\u00afbi);\n7\nend\n8 end\n9 foreach \u00afbi \u2208 \u00afB do\n10\nsi \u2190 E(x)\u22a4E(\u00afbi);\n11\nS.append([si, bi]);\n12 end\n13 S \u2190 S.sortBy(si);\n14 Top matches by similarities \u03b1 \u2190 S[0 : k];\n15 Top matches by entities \u03b2 \u2190 [];\n16 while length(\u03b2) \u2a7d length(E) do\n17\nforeach {si, bi} \u2208 S do\n18\nif entities(bi) \u0338\u2282 entities(\u03b2) then\n19\n\u03b2.append([si, bi]);\n20\nend\n21\nend\n22 end\n23 R \u2190 \u03b2.concat(\u03b1).removeDuplicates();\n24 R \u2190 R[0 : max (k, length(E))].sortBy(si, desc);\n25 return R\nEntity-Aware Dynamic selection\nWe select max (k, length(E)) samples from B such that each\nassociated entities in E is represented at least once. The algorithm is described in Alg.1 and illustrated\nin Fig. 4.\nContext-Aware Sub-Sample Selection\nODSL samples with context can have context-based sub-\nsamples. These sub-samples have different ODSL implementations for the same sample utterance\nbased on variations in the document context c. For example, as illustrated in Fig. 5, when adding\na sentence to a slide, depending on the current state of the slide, we will want to (a) insert the text\ndirectly if the slide contains a blank text box, (b) insert a line break followed by the text if there is\nexisting text on the slide, or (c) create a text field before inserting the text if the slide does not contain\na text box. When retrieving programs to include in the prompt, we choose the program sub-sample\nthat has a document context most similar to the current document context.\nFig. 6 shows the relations between different components of the prompt and modules in ARM. An\nexample ODSL synthesis prompt is shown in Fig. 7.\n13\nassified as Microsoft Confidential\nNL utterance\nNormalized \nutterance\nNormalized \nutterance\nNormalized \nutterance\nNL utterance\nODSL program\nSample bank\nENCODER\nNL utterance\nEntities: \u25ae\u25ae\u25ae\nEntity\nclassification\nq\n\u0d24\ud835\udc4f\ud835\udc4f4\n\u0d24\ud835\udc4f\ud835\udc4f15\n\u0d24\ud835\udc4f\ud835\udc4f42\nEMBEDDING SPACE\nSample 4\nNL utterance\nODSL program\nSample 15\nNL utterance\nODSL program\nSample 42\n41\nFigure 4: Illustration of the semantic search for the top 3 samples most relevant to a given NL\nutterance. Each entity type is encoded with a different color.\nClassified as Microsoft Confidential\n\u2022 ODSL samples with context can have context-based sub-samples\n\u2022 Given the same input query, different contexts lead to different ODSL programs\n\u2022 Select the sub-sample with context closest to the input context\n(5) Context-based sub-sample selection\nSemantic Search\nAdd text to slide\nHas text box, no text\nHas text box with \nexisting text\nHas no text box\nInsert text to the text box\nInsert a line break followed \nby text to the text box\nInsert a text box, then insert \ntext to the text box\nSample utterance\nContext-based sub-samples\nAdd text to slide\nInput utterance\nInput context: has text box, no text\nSample ODSL actions\n42\nFigure 5: Example context-aware sub-sample selection process. The sample utterance \u201cAdd text to\nslide\u201d has 3 sub-sample ODSL implementations corresponding to different document contexts.\n14\nClassified as Microsoft Confidential\nPrompt construction\n\u2022 Within the token length limit, dynamically construct the prompt with relevant information to help the LLMs \nproduce the correct ODSL program.\ne\nRules\nODSL \nsyntax\nODSL samples\nDocument \ncontext\nUser \nutterance\nSeparated by entities\noptional\nPrompt (up to 4097 tokens)\nMost relevant\nEntity classifier\nRequires Context classifier\nSemantic search\n29\nFigure 6: Prompt components and their relations with Entity classifier, Requires-Context classifier,\nand Semantic search process. The Requires-Context classifier decides whether to include document\ncontext in the prompt. The Entity classifier chooses which rules and ODSL syntax to include. Entity\nand Requires-Context classifiers also condition the Semantic search process to select appropriate\nODSL samples.\n6\nEvaluation Procedure\nEvaluation of a natural language commanding system like Semantic Interpreter is non-trivial for two\nmain reasons: (1) the set of possible user utterances is unbounded; (2) the number of valid ways\nto fulfill a user utterance can be unbounded. To help address the former, we created an evaluation\nset with 197 tests cases with user utterances that span only scenarios that our ODSL exploration for\nPowerPoint is able to express today: creating presentations, adding new slides, inserting text content,\nmodifying or rewriting existing content, inserting images, formatting entities in the document, etc.\nWe do not include test cases for functionality that our ODSL exploration does not currently support;\ne.g., creating charts, file sharing, creating or resolving comments, etc.\nTo understand the latter challenge of having multiple possible solutions for the same user utterance,\nconsider the user utterance, \u201cMake the slide look beautiful\u201d: there is no single correct interpretation.\nThe system may choose to change font properties, insert relevant images, change the slide layout,\ninsert shapes, animate the slide, or some subset of these. In this work, given that Semantic Interpreter\ntranslates a user intent to a DSL program, we propose a procedure that reformulates the problem\nof evaluating the natural language commanding systems into the problem of analyzing program\nequivalence.\n6.1\nTest Case Format\nEach test case consists of the user utterance, optional document context, and a list of acceptable\nprograms as shown in Fig. 8a. Allowing a list of acceptable programs rather than just a one allows\nus to include multiple known correct interpretations of the user\u2019s intent and also keeps the test\ncase extensible as we encounter new acceptable interpretations. During evaluation we pass the user\nutterance to the Semantic Interpreter to obtain the generated program. The generated program is\nthen compared to each of the acceptable programs using a program comparison algorithm discussed\nbelow. The program comparison algorithm outputs a program match level which indicates degree of\nprogram match. We discuss the different levels of match we use below.\n6.2\nProgram Comparison\nThe program comparison first parses the ODSL programs into Abstract Syntax Tree (AST) represen-\ntations. We then perform a series of program transformations and analyses to obtain the appropriate\nprogram match level as an output. Particularly, we use the following two ideas that we refer to as\nprogram normalization and subprogram analysis.\n15\nODSL Synthesis Prompt\nSystem instruction: ODSL is a DSL for performing actions in PowerPoint.\nHere are examples of ODSL\u2019s syntax:\n# Get the title from all slides in the presentation\ntextRanges = select_text(scope=\"Presentation\", name=\"Title\")\n# Gets the textRanges matching the string \"Hello\" from the provided shapes.\ntextRanges = select_text(scope=shapes, text=\"Hello\")\n# Formats the text in textRanges to be bold, italic, have Times New Roman font, have a\nsingle underline, have font size 24, have the color teal and be Left aligned.\nformat_text(textRanges=textRanges, bold=true, fontName=\"Times New Roman\",\nhorizontalAlignment=\"Left\", size=24, color=\"teal\", italic=true, underline=\"Single\")\n# Many of the argument to format statements are optional. For example, this format statement\nmakes the text bulleted and changes its color to olive.\nformat_text(textRanges=textRanges, bulleted=true, color=\"#808000\")\n...\nGenerate an ODSL program to fulfill the given user utterance.\nRemember to follow the following\nrules when generating ODSL:\n\u2013 For select_text, if scope is provided it must be a either Presentation or a variable of type shapes or\nslides. If no scope is provided, we select the user slide selection.\n\u2013 You must select or insert an entity before formatting or deleting it.\n\u2013 Never use for loops, array indexing or if/else statements.\n...\nUser: Change the text format to make it look like a typewriter\nAssistant:\ntext = select_text()\nformat_text(textRanges=text, fontName=\"Courier New\", size=18, bold=false, italic=false,\nunderline=\"None\", color=\"#000000\", bulleted=false, horizontalAlignment=\"Left\")\nUser: Change the text format to make it look elegant\nAssistant:\ntext = select_text()\nformat_text(textRanges=text, fontName=\"Times New Roman\", size=18, italic=true)\n...\nUser: Make the text look formal\nAssistant:\nFigure 7: Example LLM prompt to synthesize an ODSL program for user utterance \u201cMake the text\nlook formal.\u201d This utterance does not require document context. If document context is required, the\nprompt is similar except that we also include document context both for the current user utterance\nand the few-shot ODSL samples. The prompt shown is shortened for visualization.\n16\nd as Microsoft Confidential\nExample \nAcceptable \nProgram N\nExample \nAcceptable \nProgram 1\nApplication \nContext Tree\nUser \nUtterance\nSemantic \nInterpreter\nProgram \nComparison\nGenerated \nProgram\nProgram Match Level\nTest Case\n5\n(a) Flowchart of evaluation procedure.\nClassified as Microsoft Confidential\nProgram Normalization\nCanonicalization\nStandardize Variable Names\nStandardize Statement Ordering\n\u2026\n// Make the body look handwritten and insert a relevant title \nand bold it.\nslide = select_slides()\nbody = select_text(scope=slide, name=\"Content\")\nformat_text(textRanges=body, fontName=\"Brush Script MT\")\ntitle = select_text(scope=slide, name=\"Title\")\ntitle = insert_text(precededBy=title, text=\"Funny Haiku\")\nformat_text(textRanges=title, bold=true)\n// Make the body look handwritten and insert a relevant title \nand bold it.\nvar0 = select_slides()\nvar1 = select_text(scope=var0, name=\"Content\")\nvar2 = select_text(scope=var0, name=\"Title\")\nvar3 = insert_text(precededBy=var2, text=\"Funny Haiku\")\nformat_text(textRanges=var1, fontName=\"Brush Script MT\")\nformat_text(textRanges=var3, bold=true)\n// Make the body look handwritten and insert a relevant title \nand bold it.\nvar0 = select_slides()\nvar1 = select_text(scope=var0, name=\"Content\")\nvar2 = select_text(scope=var0, name=\"Title\")\nvar3 = insert_text(precededBy=var2, text=\"<ODSLSTR PLCHLDR>\")\nformat_text(textRanges=var1, fontName=\"Brush Script MT\")\nformat_text(textRanges=var3, bold=true)\nDesensitization\nFree-form string -> Placeholder\n\u2026\n2\n(b) Program normalization performs transforms\nsuch as canonicalization and desensitization to\navoid spurious discrepancies that prevent matches.\nClassified as Microsoft Confidential\nSubprogram Analysis\nIs an acceptable program contained within the generated program? \nslide = select_slides()\nbody = select_text(scope=slide, name=\"Content\")\ntitle = select_text(scope=slide, name=\"Title\")\nformat_text(textRanges=body, fontName=\"Brush Script MT\")\nformat_text(textRanges=title, bold=true)\n// Make the body look handwritten.\nslide = select_slides()\nbody = select_text(scope=slide, name=\"Content\")\nformat_text(textRanges=body, fontName=\"Brush Script MT\")\nAcceptable Program\nGenerated Program\n3\n(c) Subprogram analysis determines if an acceptable program is contained within the generated program.\nFigure 8: Evaluation Procedure using program equivalence analysis.\nProgram Normalization\nThe program normalization step involves program transformations that\ndesensitize program comparison to spurious discrepancies. Program normalization first performs\ncanonicalization: this transforms the program to an equivalent program that uses standardized conven-\ntions (e.g., standardizing equivalent statement orderings, standardizing variable names, etc.) Next, we\nperform desensitization where we can take free-form parameters like strings or numerical parameters\n(where appropriate) and replace them with placeholders to desensitize program comparison to their\nvalues. An example is shown in Fig. 8b.\nSubprogram Analysis\nWe noticed that often a generated program may perform all the steps in\nan acceptable program but also perform additional unnecessary but benign steps on top of these.\nFig. 8c shows an example where the generated program performs the same steps to satisfy the user\nutterance of \u201cMake the body look handwritten\u201d by changing the font name, but then also does some\nadditional formatting on top by making the text bold. Here the generated program is still valid for\nthe user utterance: from a user\u2019s perspective the resulting text still looks handwritten after program\nexecution regardless of whether it was bolded or not. This case is not an exception, e.g., a generated\nprogram may insert a picture and then resize it where an acceptable program may just insert the\npicture; a \u201ccreate a presentation about X\u201d intent may create 3 slides in an acceptable response but 5\nslides in the generated response. To evaluate such cases fairly, without having to manually add all\nsuch cases to the list of acceptable responses, we perform subprogram analysis. This analysis checks\nif an acceptable program is contained within a generated program, i.e., does the generated program\ndo at least what the acceptable program performs. Note that a successful subprogram match may\n17\nnot always be acceptable; e.g., for a user intent about inserting a picture, a generated program could\ninsert an image but incorrectly delete it later on. In practice, we do not observe such behavior in the\ngenerated programs for our test cases and so deem subprogram matches to be considered as a pass.\nThe output of the evaluation procedure is a program match level. Here are the different program\nmatch levels in descending order of degree of match:\n\u2022 Exact \u2013 At least one acceptable program is an exact match to the generated program (no\nnormalization - must be a strict match).\n\u2022 Normalized \u2013 At least one acceptable program is equivalent to the generated program after\nprogram normalization.\n\u2022 Subprogram Exact \u2013 At least one acceptable program is contained within the generated\nprogram (no normalization - must be strict subprogram).\n\u2022 Subprogram Normalized \u2013 At least one acceptable program is contained within the\ngenerated program after program normalization.\n\u2022 Manual Check - Valid \u2013 Due to the creative nature of LLMs, even if a test case result does\nnot fall into any of the above match levels, there is still a chance that the output is correct.\nFor this we perform manual human analysis of such test cases and check if the program is\nvalid.\n\u2022 None \u2013 ODSL program produced but the program match does not satisfy any of the above\ncriteria.\n\u2022 Error \u2013 No valid ODSL program produced due to errors (e.g., no program output, incomplete\noutput, syntax errors, timeout errors, etc.)\nIn practice, we found that categories of Subprogram Normalized and above led to valid programs.\nThus, we define pass rate for our experiments as the percentage of test cases that have a program\nmatch level of Subprogram Normalized or above.\n7\nExperiments\nWe use the evaluation procedure described in Section 6 to perform an ablation study to understand\nthe contribution of the various components in the Semantic Interpreter. Table 2 shows the variants of\nSemantic Interpreter and the evaluation results. In the variant with k = 0 for top-k sample selection,\nwe provide ODSL syntax description and rules in the prompts, but we do not provide any full ODSL\nprogram examples. For other values of k, we follow the procedure as described in Section 5.2,\nproviding max(k, length(E)) examples where E is the set of matched entities.\nFor all our experiments, we use the text-davinci-003 model from OpenAI\u2019s GPT-3.5 model\nfamily4. This model has a context length of 4097 tokens. We set temperature to 0 and top-p to 1.\nThe results of the ablation study are shown in Table 2. The best performing system achieves a pass\nrate of 96.06 \u00b1 2.69% by leveraging the entity classifier, requires-context classifier, code correction\nand configuring k = 5 for semantic search. The remainder of this section discusses some observed\ntrends.\nEffect of varying k\nAs illustrated in Fig. 9, few-shot prompting is critical to the performance of\nthe system. Setting k = 0 (no complete ODSL program examples in the prompt) leads to a very\npoor performance of approximately 10% pass rate. This is much lower performance than that of the\nvariants that have k > 0. Even with k > 0, the system performance is quite sensitive to the value\nof k with the optimal value in our experiments being k = 5. We see an increase in performance as\nwe increase the value of k from 1 to 5: increasing the number of example ODSL programs in the\nprompt gives the model more data to ground its generation in. But providing too many examples\nleads to issues such as not leaving enough tokens for a complete response or the LLM overfitting on\nthe examples provided. This is consistent with the increase in Error and None cases in Table 2 as we\nincrease k from 5 to 9.\n4https://platform.openai.com/docs/models\n18\nEntity\nClassifier\nRequires Context\nClassifier\nCode\nCorrection\nk\nExact\nNormalized\nSubprogram\nExact\nSubprogram\nNormalized\nManual Check\nValid\nNone\nError\nPass Rate\n(%)\n\u2713\n\u2713\n\u2713\n0\n0\n10\n0\n1\n8\n28\n150\n10.42 \u00b1 4.22\n\u2713\n\u2713\n\u2713\n1\n20\n89\n5\n40\n20\n13\n10\n87.59 \u00b1 4.56\n\u2713\n\u2713\n\u2713\n3\n28\n91\n8\n39\n19\n6\n6\n93.07 \u00b1 3.51\n\u2713\n\u2713\n\u2713\n5\n37\n96\n9\n37\n12\n1\n5\n96.06 \u00b1 2.69\n\u2713\n\u2713\n\u2713\n7\n32\n92\n10\n32\n21\n1\n9\n94.06 \u00b1 3.27\n\u2713\n\u2713\n\u2713\n9\n26\n66\n8\n40\n24\n4\n29\n82.61 \u00b1 5.24\n\u2717\n\u2713\n\u2713\n5\n17\n89\n8\n38\n16\n15\n14\n84.60 \u00b1 4.99\n\u2713\n\u2717\n\u2713\n5\n21\n69\n5\n42\n18\n34\n8\n78.13 \u00b1 5.72\n\u2717\n\u2717\n\u2713\n5\n14\n75\n5\n49\n15\n28\n11\n79.62 \u00b1 5.57\n\u2713\n\u2713\n\u2717\n5\n33\n93\n8\n36\n11\n1\n15\n91.08 \u00b1 3.94\nTable 2: Results of ablation study, running the evaluation procedure discussed in Section 6 on variants\nof Semantic Interpreter. The statistical uncertainties are reported at 95% confidence level using\nAgresti-Coull interval. Note that while the pass rates are quite high for certain configurations, the\nexperiments are run on a benchmark that only includes test cases for user utterances that can be\nexpressed in our current ODSL implementation.\n80\n90\n100\nPass rate (%)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nk\n0\n5\n10\n15\nFigure 9: Visualization of pass rates for different k values in ARM\u2019s retrieval as specified in the first\n6 rows of Table 2. At k = 0, only ODSL syntax description and rules are included in the prompt and\nthere are no ODSL program examples. For k > 0, max(k, length(E)) ODSL program samples are\nincluded. The error bars represent Agresti-Coull intervals at 95% confidence level.\nBenefit of Analysis for Semantic Search\nAs part of our semantic search process, we first analyze\nthe query with the entity classifier and requires-context classifier to narrow down the search before\nperforming vector search. Table 2 shows that in variants where we remove either one or both of these\nanalysis steps, the pass rate of the system is 10-15% lower compared to the variant that performs\nboth steps.\nBenefit of Code Correction\nCode correction is also a good help to Semantic Interpreter\u2019s per-\nformance. As expected, we see an increase in Error results without it, lowering the pass rate by\n5% compared to the variant with it. Code correction is a great example of how we can benefit\nfrom neurosymbolic approaches like Semantic Interpreter, in this case combining the robustness and\nprecision of compiler techniques like static program analysis with the power of LLMs.\n8\nConclusion\nIn this paper, we presented Semantic Interpreter, an AI system that uses LLMs for natural language\ncommanding in productivity software. Semantic Interpreter translates natural language user utterances\nto ODSL programs, which are domain-specific symbolic representations for manipulating content\nand actions in Office applications. To do this, Semantic Interpreter leverages an Analysis-Retrieval\n19\nMethod (ARM) for prompt construction to generate ODSL programs using LLMs. A natural\nlanguage commanding module like Semantic Interpreter can prove to be a powerful building block\nin architecting more general LLM-powered assistive experiences in the productivity space. Our\nwork illustrates the promise of using a program synthesis approach to tackle the problem of natural\nlanguage commanding, enhancing how people interact with productivity software in the future.\nAcknowledgements\nWe are thankful for the support of many from the Office AI team at Microsoft. Special thanks to\nBobby Kishore and Sumit Chauhan for championing this work; Shavinder Multani, Nick Thomson,\nKevin Nickel, Mahesh Sasidharan, Dhruvil Gala, Yijia Xu, Yipeng Li, Jignesh Shah and Chandan\nMishra for all the engineering support; Alan Liu, Manan Sanghi, Chris Pleasants, Firoz Shaik, Md\nMuksitul Haque and Weiyao Xie for their contributions to ODSL explorations across apps.\nReferences\n[1] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,\nEvan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,\nPeter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code, 2021.\n[2] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\nsynthesis. In The Eleventh International Conference on Learning Representations, 2023.\n[3] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel,\nand Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems, volume 33, pages 9459\u20139474. Curran Associates, Inc., 2020.\n[4] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\nACM Computing Surveys, 55(12):1\u201338, mar 2023.\n[5] Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. Synthesis of loop-\nfree programs. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation, PLDI \u201911, page 62\u201373, New York, NY, USA, 2011.\nAssociation for Computing Machinery.\n[6] Zohar Manna and Richard Waldinger. Knowledge and reasoning in program synthesis. Artificial\nIntelligence, 6(2):175\u2013208, 1975.\n[7] Zohar Manna and Richard Waldinger. A deductive approach to program synthesis. ACM Trans.\nProgram. Lang. Syst., 2(1):90\u2013121, jan 1980.\n[8] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay Saraswat.\nCombinatorial sketching for finite programs. SIGOPS Oper. Syst. Rev., 40(5):404\u2013415, oct\n2006.\n[9] Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis\nbeyond domain-specific languages. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,\npages 22196\u201322208. Curran Associates, Inc., 2021.\n20\n[10] Xinyun Chen, Chang Liu, and Dawn Song. Towards synthesizing complex programs from\ninput-output examples. In International Conference on Learning Representations, 2018.\n[11] Sumit Gulwani, William R. Harris, and Rishabh Singh. Spreadsheet data manipulation using\nexamples. Commun. ACM, 55(8):97\u2013105, aug 2012.\n[12] Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and\nPushmeet Kohli. Neuro-symbolic program synthesis. In International Conference on Learning\nRepresentations, 2017.\n[13] Saurabh Srivastava, Sumit Gulwani, and Jeffrey S. Foster. From program verification to program\nsynthesis. SIGPLAN Not., 45(1):313\u2013326, jan 2010.\n[14] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron,\nSailesh R, and Subhajit Roy. Program synthesis using natural language. In Proceedings of the\n38th International Conference on Software Engineering, ICSE \u201916, page 345\u2013356, New York,\nNY, USA, 2016. Association for Computing Machinery.\n[15] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. Pal: Program-aided language models. https://arxiv.org/abs/2211.\n10435, 2023.\n[16] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts\nprompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022.\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling\nlanguage modeling with pathways, 2022.\n[18] Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, and Yi-Zhe Song. SketchODE:\nLearning neural sketch representation in continuous time. In International Conference on\nLearning Representations, 2022.\n[19] Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u02c7cisk\u00fd, Fumin\nWang, and Andrew Senior. Latent predictor networks for code generation. In Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 599\u2013609, Berlin, Germany, August 2016. Association for Computational Linguistics.\n[20] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code gen-\neration. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 440\u2013450, Vancouver, Canada, July 2017. Associa-\ntion for Computational Linguistics.\n[21] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to\ncode in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 1643\u20131652, Brussels, Belgium, October-November 2018.\nAssociation for Computational Linguistics.\n[22] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis\nwith large language models, 2021.\n21\n[23] Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama,\nand Yisong Yue. Neurosymbolic programming. Foundations and Trends\u00ae in Programming\nLanguages, 7(3):158\u2013243, 2021.\n[24] Kyle Hamilton, Aparna Nayak, Bojan Bo\u017e i\u00b4c, and Luca Longo. Is neuro-symbolic AI meeting\nits promises in natural language processing? a structured review. Semantic Web, pages 1\u201342,\nnov 2022.\n[25] Zachary Susskind, Bryce Arden, Lizy K. John, Patrick Stockton, and Eugene B. John. Neuro-\nsymbolic ai: An emerging class of ai workloads and their characterization, 2021.\n[26] Abbavaram Gowtham Reddy and Vineeth N Balasubramanian. Nester: An adaptive neurosym-\nbolic method for treatment effect estimation, 2023.\n[27] Daniel Ritchie, Paul Guerrero, R. Kenny Jones, Niloy J. Mitra, Adriana Schulz, Karl D. D.\nWillis, and Jiajun Wu. Neurosymbolic Models for Computer Graphics. Computer Graphics\nForum, 2023.\n[28] Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate\nCrim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin\nHayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy,\nTheo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr\nNisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth\nShort, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson,\nAndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander\nZotov. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for\nComputational Linguistics, 8:556\u2013571, dec 2020.\n[29] Jennifer J. Sun, Megan Tjandrasuwita, Atharva Sehgal, Armando Solar-Lezama, Swarat Chaud-\nhuri, Yisong Yue, and Omar Costilla Reyes. Neurosymbolic programming for science. In\nNeurIPS 2022 AI for Science: Progress and Promises, 2022.\n[30] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4, 2023.\n[31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. https://arxiv.org/abs/2201.11903, 2023.\n[32] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting\nenables complex reasoning in large language models. In The Eleventh International Conference\non Learning Representations, 2023.\n[33] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[34] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023.\n[35] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in language models, 2022.\n[36] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks,\n2023.\n[37] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.\n22\n[38] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\nHuang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian,\nRunchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye,\nBowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu,\nWeilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation\nmodels, 2023.\n[39] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. https://arxiv.org/abs/2110.\n14168, 2021.\n[40] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models, 2022.\n[41] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools, 2023.\n[42] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John\nSchulman. Webgpt: Browser-assisted question-answering with human feedback, 2022.\n[43] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[44] Shunyu Yao, Howard Chen, John Yang, and Karthik R Narasimhan. Webshop: Towards scalable\nreal-world web interaction with grounded language agents. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing\nSystems, 2022.\n[45] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models, 2023.\n[46] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface, 2023.\n[47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. https://arxiv.org/abs/2204.06125,\n2022.\n[48] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-\ning (EMNLP), pages 6769\u20136781, Online, November 2020. Association for Computational\nLinguistics.\n23\n"
  },
  {
    "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
    "link": "https://arxiv.org/pdf/2306.03438.pdf",
    "upvote": "2",
    "text": "Large Language Models of Code Fail at\nCompleting Code with Potential Bugs\nTuan Dinh1\u2217\u2020\nJinman Zhao2 \u2217\nSamson Tan2\nRenato Negrinho2\nLeonard Lausen2\nSheng Zha2\nGeorge Karypis2\n1University of Wisconsin\u2013Madison\n2Amazon Web Services\ntuan.dinh@wisc.edu\n{jinmaz,samson,renatoni,lausen,zhasheng,gkarypis}@amazon.com\nAbstract\nLarge language models of code (Code-LLMs) have recently brought tremendous\nadvances to code completion, a fundamental feature of programming assistance\nand code intelligence. However, most existing works ignore the possible presence\nof bugs in the code context for generation, which are inevitable in software devel-\nopment. Therefore, we introduce and study the buggy-code completion problem,\ninspired by the realistic scenario of real-time code suggestion where the code con-\ntext contains potential bugs \u2013 anti-patterns that can become bugs in the completed\nprogram. To systematically study the task, we introduce two datasets: one with syn-\nthetic bugs derived from semantics-altering operator changes (buggy-HumanEval)\nand one with realistic bugs derived from user submissions to coding problems\n(buggy-FixEval). We find that the presence of potential bugs significantly degrades\nthe generation performance of the high-performing Code-LLMs. For instance, the\npassing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop\nmore than 50% given a single potential bug in the context. Finally, we investigate\nseveral post-hoc methods for mitigating the adverse effect of potential bugs and\nfind that there remains a significant gap in post-mitigation performance.3\n1\nIntroduction\nSuggesting code for a given context is a frequently used feature in modern integrated development\nenvironments (IDEs) [1], bringing productivity gains to the code-writing process. This task is\nwidely studied as code completion [2, 3] in the literature, with techniques and models ranging from\nprobabilistic or sequence modeling [4, 5], incorporating code structure as prior knowledge [6, 7], to\nadopting deep neural networks [8] and pre-training techniques [9] to learn representations for code.\nRecently, large Transformer-based language models of code (Code-LLMs) [10, 11, 12] have become\na promising paradigm for code completion, attaining state-of-the-art (SotA) performance in various\ncode learning tasks including code completion and generation.\nHowever, existing works studying Code-LLMs often assume the absence of bugs, despite the frequent\noccurrences and the cost of bugs in software development: on average, 70 bugs are created per 1000\ncode lines [13]; and fixing bugs costs 50% of development time [14]. Consider a practical coding\nscenario. A developer wants to use the code suggestion feature in an IDE when writing code. With a\nhigh probability, their real-time code context, as an input to the code predictor, contains typos or less\nrefined, potentially buggy implementations. Since bugginess is a property of a complete program,\n\u2217Equal contribution.\n\u2020Work done while interning at Amazon Web Services.\n3Code and datasets are available at https://github.com/amazon-science/buggy-code-completion\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.03438v2  [cs.LG]  1 Dec 2023\nFigure 1: Illustrations for code completion (left) and buggy-code completion (right). (top) prob-\nlem statement for function implementation, (middle) partial code with (right) or without (left) potential\nbugs (highlighted), (bottom) code completions from CODEGEN-2B-MONO [12]. The completed\ncode is functionally correct on the left but incorrect on the right, failing test case below_zero([1,\n2]) == False. The example is based on HumanEval/3 from buggy-HumanEval.\nit is not well-defined how to detect bugs and repair the buggy code in this short and incomplete\ncode context, making the application of existing bug-detection or code-repair tools sub-optimal or\ninfeasible. It is also worth mentioning that the gap between the in vitro and in vivo performances of\ncode completion models [15, 16, 17] remains large. Therefore, a natural question arises: Can existing\nCode-LLMs provide good code suggestions given the unrefined nature of draft code?\nTo answer this question, we introduce and study the problem of completing code with potential\nbugs in the code context, dubbed buggy-code completion (bCC). In this study, we focus on using\nCode-LLMs to generate functional implementations from a code context, where the code context\nconsists of a problem specification and a piece of partial code.4 A potential bug in a piece of partial\ncode is a code span that can become a bug provided some completion, i.e., it fails the completion\nand at the same time can be changed to make the completion work. Note that a potential bug is not\na bug per se without a completion. Shown in Figure 1 is an illustration of our task with a problem\ndescription at the top and partial code in the middle. On the left are a reference code context and\na correct completion from the chosen Code-LLM. On the right, the highlighted potential bug (-=)\nmakes the reference completion incorrect. The Code-LLM reacts to this potential bug by generating\na different completion (bottom right). However, the completed code is still functionally incorrect.\nTo conduct a quantitative study of bCC, we construct two datasets. First, buggy-HumanEval dataset\ncontains interview-style coding problems from HumanEval dataset [10], with buggy/reference partial\ncode pairs being generated by introducing semantic-altering operator changes to reference solutions.\nThis dataset provides a well-controlled setting to assess models\u2019 behavior upon potential bugs. Second,\nbuggy-FixEval dataset, based on FixEval [19], contains user submissions to coding-contest problems.\nThe buggy/reference pairs are constructed from rejected and accepted submissions by the same user\nto a given problem. This dataset helps assess models\u2019 performance over a realistic distribution of\npotential bugs. Our benchmarks are well associated with the existing benchmarks for Code-LLMs.\nVia our empirical studies, we find that the presence of potential bugs drastically degrades the code-\ncompletion performance of high-performing Code-LLMs, with test-case pass rates dropping to below\n5% across both datasets for all tested model variants. For instance, on buggy-HumanEval, the\ntest-case pass rate of CODEGEN-2B-MONO completions drops from 54.9% (reference partial code)\nto 3.1% (partial code contains potential bugs), which is worse than the score when no partial code is\nprovided (9.3%). Our results demonstrate that Code-LLMs are highly susceptible to potential bugs.\nFurthermore, we attempt several post-hoc methods to augment Code-LLMs to better deal with\npotential bugs, namely removal-then-completion, completion-then-rewriting, and rewriting-then-\ncompletion. The latter two augment the Code-LLMs with an external code repairer as the rewriter\ncomponent. Our evaluation shows that the attempted methods improve the buggy-code completion\nperformance of all tested Code-LLMs. However, the performance gap remains large between these\n4This setting is well-aligned with the text-to-code generation task for Code-LLMs [12, 18].\n2\nFigure 2: Performance (pass@1 (\u2191)) degradation in the presence of potential bugs. CG: CODE-\nGEN. Potential bugs severely harm the completion of all CodeLMs (red bars) compared to non-buggy\nsettings (teal bars), making it even worse than completion without partial code (blue bars).\nmethods and the completion with reference partial code. We provide further case studies and analyses,\ne.g., effects of potential bugs\u2019 locations or the successful cases of na\u00efve completion for a better\nunderstanding of the behavior of the tested models in buggy-code completion.\nStudy scope and contributions.\nThis work aims to explore and understand the behaviors of Code-\nLLMs under the buggy-code completion setting. We (i) define the novel buggy-code completion task,\n(ii) introduce two representative benchmark datasets, (iii) demonstrate the inability of Code-LLMs\nto handle potential bugs, and (iv) evaluate several baseline methods for improving Code-LLMs on\nbuggy-code completion.\n2\nBuggy-Code Completion\nIn this work, we consider the code completion setting with inputs consisting of (1) a specification\nh that specifies the desired functionality of the program and (2) a code context s, which is some\nunfinished code to be completed. Here, h is given as a docstring or a problem statement in English,\nand s is given as a few lines of code that are the beginning part of a program, which we refer to\nas partial code or (code) prefix. In the conventional setting of code completion, the objective is to\nsuggest a completion c such that t := s :: c is a program that satisfies h, where \u201c::\u201d denotes code\nconcatenation. Our buggy-code completion setting extends the conventional one with a challenging\nand realistic consideration: s may contain potential bugs.\nConsider a real-world scenario where a programmer works on unfinished code and utilizes the code\nauto-completion feature for suggestions. Note that the coding process often sees coding mistakes or\nintroduces inconsistent code usages, which are not necessarily \u201cincorrect\u201d per se. However, there is a\nfair chance that such mistakes or inconsistencies cause unintended and undesirable behaviors in the\nfinal program. In other words, they become bugs. Based on this intuition, we define potential bugs as\nthe following:\nDefinition 2.1 (potential bug). Consider a specification h and a reference code prefix s for which\nsome completion c exists such that t := s :: c satisfies h. A potential bug is manifested as a small\nedit e over s such that t\u2032 := s\u2032 :: c does not satisfy h, where s\u2032 is the result of applying e on s.\nWe note that potential bugs are not bugs per se and are only defined with respect to some reference\ncode prefix s and completion c. The code prefix s\u2032 containing potential bugs is referred to as\n\u201cpotentially buggy\u201d or simply \u201cbuggy\u201d prefix (with respect to s) throughout the paper. In most\ncases, we omit the \u201cwith respect to\u201d part for brevity. The resulting term \u201cbuggy prefix\u201d refers to the\ndefinition here and by no means assumes that the prefix itself is buggy. Our study focuses on potential\nbugs associated with semantic bugs, i.e., edits that do not introduce syntax errors, as semantic bugs\nare generally more challenging and interesting than syntax ones.\nRemark 2.2. Intuitively, we assume that the reference prefix s highly correlates to preferred (practical)\nimplementations of the task and that any deviation from s is likely less preferred. We use the\noperational definition of potential bugs in 2.1 for its simplicity and verifiability, as well as for that\nit allows the initial exploration of the previously under-investigated scenario of bCC. We note that\nby this definition, a reference prefix s itself can, in some cases, albeit less likely, be \u201cbuggy\u201d with\nrespect to some other references. However, such a case is less of a concern in this study, as we are\nmostly interested in finding a completed functional program out of s or s\u2032.\n3\nDefinition 2.3 (buggy-code completion, bCC). Given a specification h and a code prefix s\u2032 containing\npotential bugs, buggy-code completion is the task of generating a complete program t that satisfies h.\nWe deliberately loosen the constraint that t should contain s\u2032 as a prefix, as by definition, and as\nwell as one can see from the example in Figure 1, fixating on the buggy prefix can make suggesting\na satisfying solution difficult, if not impossible. As we explore in Section 4.3, allowing models to\nsuggest fixes to the buggy code prefix significantly increases the possibility that a generated program\npasses the tests. However, it is often still possible to continue a buggy prefix to a valid program\nsolution (see an example in Figure 12).\nAn alternative view of the setting is that the buggy code prefix provides a noisy and potentially flawed\nprecondition for generating a solution to the coding problem. It presents a good-faith effort from a\nuser to instruct the tool about their intended code solutions. A good-behaving model should take this\nas a hint and, at worst, discard it so that the generation performance is not worse than when no code\nprefix is given. However, this is not the case for our evaluated models (Section 4.2).\nRemark 2.4. Our bCC formulation is not a simple combination of code repair and code completion.\nAs bugginess is a property of completed programs, repairing the partial code is an ill-defined problem,\nthus making repairing-then-completion also ill-defined. Therefore, our bCC task is better viewed as\nan extension of code completion with the additional challenge that generating semantically correct\ncontinuations from the given partial code without deviation may be difficult or even infeasible. This\nchallenge requires models to be aware of the existence of potential bugs for better suggestions.\n3\nBenchmarks\nThis section introduces our new datasets with proposed baseline methods and evaluation metrics.\n3.1\nDatasets for bCC\nBased on our task definition, each instance in a bCC benchmark should contain a problem description\nspecifying requirements, a piece of buggy code prefix to be completed, and a set of test cases for\nassessing the correctness of the finished code. Optionally, an instance also has a corresponding\nreference code prefix and a valid solution completed from the reference code. To our knowledge, no\nexisting dataset meets all the desiderata. Commonly used large-scale datasets for code completion,\ne.g., Py150 [20] and Java Corpus [21] do not come with test cases. Recent small-scale manually\ncurated datasets for code generation, e.g., HumanEval [10], MBPP [18], APPs [22], come with test\ncases and reference solutions but no buggy or failed solutions. There is also no predefined way to\nextract partial code from the reference solutions. This is also the case for program-repair datasets:\npopular datasets either lack test cases, e.g., [23, 24, 25, 26] and/or are of small sizes, e.g., [27, 28, 29].\nWe introduce two datasets for evaluating bCC in Python, both with all the aforementioned desired\ncomponents. We ensure that 1) all partial code snippets contain potential bugs that fulfill our definition,\ni.e., their respective completed code is incorrect, certified by failing test cases; and 2) all potential\nbugs are semantic in nature, i.e., they cause no syntax error in their respective completed code.\n3.1.1\nBuggy-HumanEval\nWe first introduce buggy-HumanEval to offer a controlled setting for evaluating bCC in the presence\nof a single semantic bug. Buggy-HumanEval contains 1896 bCC instances constructed from a subset\nof HumanEval problems [10]. The HumanEval dataset is a popular dataset of manually written\nintroductory coding problems designed for evaluating the code generation ability of Code-LLMs.\nPotential bugs are introduced as semantic-altering operator changes to reference solutions. We search\nfor applicable binary operators in reference solutions and change them into their semantic opposites,\ne.g., + into -. To ensure that the edit introduces a bug, we execute the altered program and only keep\nthe ones failing some tests. We then specify a line after the altered operator to split the solution into a\ncode prefix and suffix. We keep the buggy prefix as part of the input for bCC. We split the unaltered\nsolution at the same line to get a corresponding reference prefix. On average, the problems selected\nfor buggy-HumanEval have longer solutions than the unselected ones in HumanEval, with 8.2 lines\nvs. 2.9 lines of code, respectively. Appendix A.1 provides details of our dataset.\n4\n3.1.2\nBuggy-FixEval\nTo evaluate bCC with more realistic bugs, we introduce buggy-FixEval with bCC instances con-\nstructed from CodeNet [30] and FixEval [19]. FixEval is a program repair benchmark based on user\nsubmissions to competitive programming websites. Each data sample in FixEval can be viewed as\na pair of submitted programs from the same user, with the accepted submission being regarded as\nreference or fixed and the preceding rejected submission being regarded as buggy.\nTo create buggy-FixEval, we match and pair each problem with its problem statement found in\nCodeNet and omit the problems with no match. For each submission pair, we identified potential\nbugs as the differences between the rejected and accepted submissions. We ensure that the rejected\nsubmission contains no syntax error and fails at least one test case. We split the solutions into halves\nand regard the prefix from the rejected solution as containing potential bugs and the prefix from\nthe accepted solution as a reference.\nTo guarantee the differences between the buggy prefix and\nthe reference prefix related to potential bugs, we impose a limit on the character-level edit distance\nbetween them, ignoring comments and white spaces. The lower limit ensures that the differences\nare not comments or white spaces. The upper limit (20, chosen by manual inspection) reduces the\nchance that the difference is related to a reformatting or a re-implementation. We then manually\ninspected all the remaining pairs and excluded undesirable cases such as that the reference prefix\nitself is already a correct solution or that the two prefixes are semantically equivalent. Finally, we\nexecute the concatenation of the buggy prefix and the reference completion and ensure that it fails at\nleast one test case. More details about creating buggy-FixEval can be found in Appendix A.2.\n3.2\nBaseline Methods for bCC\nBeyond the na\u00efve generation from code context, we study several methods for augmenting Code-\nLLMs to utilize the partial code better while mitigating the effect of potential bugs. Assuming a bug\ndetection component is equipped, we focus on simple and modular-design approaches that do not\nrequire additional training and are flexible for further incorporating newly developed Code-LLMs.\nRemoving partial code, then completing (removal-then-completion).\nWe bypass the negative\neffect of buggy partial code by removing the entire content code fragment from the model input. In\nparticular, for buggy-HumanEval, the input to Code-LLMs after removal consists of the problem\nstatement and the function header (and examples). For buggy-FixEval, we keep only the statement\nand the code fragment used for reading input data. Intuitively, as removal-then-completion guarantees\nthat the input to Code-LLMs contains no bug, we expect this method to perform better than the na\u00efve\ncompletion. However, the drawback of removal-then-completion is its sacrifice of all potentially\nuseful information brought by the partial code.\nCompleting first, then rewriting the program (completion-then-rewriting).\nThis approach at-\ntempts to fix the buggy code using pre-trained code-repair models [31, 32, 25], e.g., neural translating\na buggy program into a valid program [31]. As code-repair models, or code fixers, are usually trained\nto fix the complete programs, we first complete the code by na\u00efve completion, then apply code fixers\non the potentially buggy programs. We use RealiT [32], a SotA program repair model as the code\nfixer. RealiT is designed for misused variables, wrong literal, wrong binary, and unary operators,\nwhich is especially suitable for the bugs introduced in the buggy-HumanEval.\nRewriting the partial code, then completing (rewriting-then-completion).\nThis approach at-\ntempts to resolve potential bugs in the partial code before completing. To do so, we first locate code\nlines containing the potential bug, then rewrite these lines. To detect potential bugs, we propose a\nlikelihood-based measure to identify the line most likely to contain a potential bug. In particular, we\nscore each code line with the following procedure. First, for each token, we define its buggy score\nto be the difference in likelihoods between the token with the highest likelihood (i.e., the argmax\ntoken) and the observed token. The buggy score for each line is then calculated by taking either the\nmaximum or average of non-zero buggy scores of its tokens. The line with the highest score is most\nlikely to contain the potential bugs. We use the INCODER-6B [33] as the infilling language model\nfor rewriting code. We provide a detailed explanation and example illustration of likelihood-based\nmeasures in Appendix B to understand our rewriting-then-completion method better.\n5\nTable 1: Pass@1 (\u2191) of completion methods on buggy-HumanEval and buggy-FixEval datasets.\nFor all Code-LLMs, all three proposed methods improve the completion performance of the na\u00efve\ncompletion. On average, completion-then-rewriting and rewriting-then-completion achieve the best\nscores on buggy-HumanEval and buggy-FixEval, respectively. Nevertheless, there are still substantial\ngaps between these methods and completion with reference prefixes. *Best methods in each column\nare in bold. Table 7 in Appendix C provides results with a larger model (CODEGEN-16B-MONO).\nPrefix\nMethod\nbuggy-HumanEval\nbuggy-FixEval\nCODEGEN-\nINCODER-\nCODEGEN-\nINCODER-\n350M\n2B\n1B\n6B\n350M\n2B\n1B\n6B\nreference\ncompletion\n43.0\n54.9\n41.1\n50.6\n27.6\n37.8\n24.1\n32.3\nbuggy\ncompletion\n0.7\n3.1\n0.5\n1.0\n2.4\n4.3\n1.2\n1.8\nremoval-then-completion\n4.7\n9.3\n3.9\n6.9\n3.3\n8.6\n2.9\n6.4\nrewriting-then-completion\n14.1\n24.9\n9.1\n16.4\n2.4\n7.2\n2.6\n5.1\ncompletion-then-rewriting\n22.5\n23.6\n22.7\n25.2\n2.3\n4.7\n1.7\n3.0\n3.3\nEvaluation Metrics\nWe measure the functionality of a completed program by executing it against the provided test cases.\nFollowing the recent code-generation works [e.g., 10, 34, 12, 33], we measure the pass@k (\u2191) for\neach bCC instance as pass@k := 1 \u2212\n\u0000n\u2212c\nk\n\u0001\u000e\u0000n\nk\n\u0001\nwhere n completions are sampled from a model,\nand c of them pass all the tests. We choose n = 100 and k = 1, 10, 100. This metric estimates the\nprobability that any of k samples from the model passes the tests. As multiple bCC instances may\nbe derived from the same programming problem, we first average the pass@k within each problem,\nthen average across all problems (macro-average) to avoid the dominance of a few problems.\nWhile passing all test cases does not 100% guarantee the correctness of a program, this metric provides\na practical and efficient proxy for functional correctness. Note that we do not use match-based metrics,\ne.g., exact match or CodeBLEU [35] because they do not properly reflect the functionality of generated\ncode [22, 10, 18], and no reference completion is available for buggy prefixes.\n4\nExperiments\nWe design two experiment sets to investigate (1) how well the existing Code-LLMs adapt to bCC\n(Sec. 4.2) and (2) whether we can have a simple fix with Code-LLMs for bCC (Sec. 4.3). We\nprovide ablation studies on potential bugs (Sec. 4.5) and on combining buggy-based completion with\nreference prefix (Sec.C.4). Sec. 4.6 presents case studies about interesting behaviors of Code-LLMs\nunder bCC, with additional results in Appendix C.\n4.1\nExperiment Settings\nCode-LLMs.\nWe evaluate the two popular and open-sourced Code-LLMs for code completion.\nCODEGEN [12] is a family of LLMs trained on both natural and programming language corpora\nwith high performance in code generation on HumanEval [10]. We use released model checkpoints:\nCODEGEN-350M-MONO, CODEGEN-2B-MONO, and CODEGEN-16B-MONO. INCODER [33]\nmodels are trained with a causal masking objective, allowing them to fill blocks of code conditioned\non arbitrary left and right contexts. We use the released model checkpoints INCODER-1B and\nINCODER-6B, each with 1B and 6B parameters. We select CODEGEN [12] and INCODER [33] since\nthey are publicly available with high performance of code generation.\nGenerating completions.\nInput format. For buggy-HumanEval, a model is expected to complete an\nunfinished function. Following the HumanEval benchmark [10], we set the models\u2019 input as the partial\ncode leading up to the completion location, with the problem description embedded as the docstring of\nthe unfinished function. For buggy-FixEval, the completion task is to complete an unfinished program\nwith inputs being the problem description as a file-level docstring (quoted within triple quotation\nmarks), followed by the partial code leading up to the completion location. Generation and sampling\ndetails. Following the best-performing settings reported in the corresponding works [12, 33], we use\ntemperature sampling with temperature = 0.6 for CODEGEN and top-p sampling with p = 0.95 and\n6\ntemperature = 0.2 for INCODER. Based on the reference solutions and computing efficiency, we set\nthe maximum length limit for outputs from 200 to 600 tokens, varying with the problem sets. We\nobserved similar performance trends between the tested models across different length settings. We\npost-process the output string following the same procedure used in their code releases.\nCode-repair and code-rewriting models.\nFor removal-then-completion, we use the latest model of\nRealiT [32] as the code-fixer, which is trained and tested over artificial and realistic bugs. For the\ncode-rewriting model, we use the INCODER-6B model [33] due to its ability to code infilling and\napply the similar settings used for infilling reported in the model\u2019s paper [33].\n4.2\nHow Well Do Existing Code-LLMs Perform on Buggy-Code Context?\nWe evaluate the latest Code-LLMs for buggy-code completion on buggy-HumanEval and buggy-\nFixEval, shown in Figure 2. In particular, the performance is measured in terms of pass@1, and we\nuse four models: CODEGEN-350M-MONO, CODEGEN-2B-MONO, INCODER-1B, and INCODER-\n6B. First, comparing the scores between reference and buggy partial code for each model, we see\nthat the presence of potential bugs is detrimental to the completion models, with pass@1 drops\nfrom 41.1\u201354.9% to 0.5\u20133.1% over buggy-HumanEval, and from 24.1\u201337.8% to 1.2\u20134.3% over\nbuggy-FixEval. Moreover, the pass@1\u2019s upon buggy partial code is universally dominated by those\nwithout partial code: 3.9\u20139.3% over buggy-HumanEval, 2.9\u20138.6% over buggy-FixEval. Table 7\n(Appendix C) shows the similar findings with a very large model (CODEGEN-16B-MONO). These\nresults indicate that (i) the tested Code-LLMs drastically fail at bCC instantiated by our datasets, and\n(ii) the presence of potential bugs destroys the benefit brought by the partial code.\nWhy do Code-LLMs fail at bCC?\nWe manually inspect samples from the best-performing\nCODEGEN-2B-MONO model and find the two most common failure modes among failed samples.\nFirst, the model fails to react to the potential bugs (i.e., common completions remain the same), as\nshown in Figure 8. This mode happens in 90% of instances and 93% of problems with at least one\nfailed instance. We conjecture that the model is not sensitive to and thus ignores minor code changes\nand/or chooses to default to common patterns in the training data. Secondly, the model fails to bypass\nthe potential bugs, likely because such patterns are rare in high-quality code. In other words, the\nmodel might have recognized the potential bugs and significantly changed the output distribution but\nstill failed. Figure 7 illustrates an example of this case. We provide further details in Appendix D.2.\n4.3\nHow Effective Are Baseline Completion Methods Against Potential Bugs?\nWe evaluate the completion methods introduced in Section 3.2 using the same bCC setting, shown in\nTable 1. We include results of completing reference partial code to see how potential bugs affect the\nCode-LLMs. Figure 6 in Appendix C provides full results and interpretations for k = 1, 10, 100.\nThe effect of proposed completion methods.\nAll three proposed methods outperform the\nna\u00efve buggy-code completion baseline. On buggy-HumanEval, we observe the general trend of\ncompletion-then-rewriting outperforming rewriting-then-completion, which outperforms removal-\nthen-completion. Note that these scores of removal-then-completion are generally lower than reported\nscores of the similar method [12, 33] probably because buggy-HumanEval is derived from a rela-\ntively more challenging subset of HumanEval (see Section 3.1.1). On buggy-FixEval, we observe\nremoval-then-completion to outperform rewriting-then-completion, which outperforms completion-\nthen-rewriting. The performance gap increases as the size of the completion model rises. Similar\ncomparison trends are observed for pass@k for k = 10, 100. Nevertheless, performance gaps remain\nsignificant between the best method and the completion from the reference code for all settings.\nThe effect of Code-LLMs\u2019 capacity.\nFor each type of code completion model, the larger version\nperforms better than the smaller version using the same method. For instance, CODEGEN-2B-MONO\noutperforms CODEGEN-350M-MONO for all settings in two datasets. Compared to CODEGEN,\nINCODER models, in general, obtain better pass@1 but worse pass@10 and pass@100. This suggests\nthat CODEGEN models generate more diverse completions, while INCODER models generate more\nprecise completions. Furthermore, INCODER is more sensitive to buggy partial code than CODEGEN\nmodels, evidenced by the lower scores from na\u00efve bCC.\n7\nTable 2: Comparing pass@1 (\u2191) of completion methods on reference and buggy partial code. While\nthe proposed mitigation methods achieve better performances than na\u00efve completion with buggy\nprefixes, they may harm the completion when no bug exists (reference).\nDataset\nbuggy-HumanEval\nbuggy-FixEval\nCODEGEN-\n2B-MONO\n350M-MONO\n2B-MONO\n350M-MONO\nreference\nbuggy\nreference\nbuggy\nreference\nbuggy\nreference\nbuggy\nna\u00efve completion\n54.9\n3.1\n43\n0.7\n37.8\n4.3\n27.6\n2.4\nrewriting-then-completion\n49.6\n24.9\n35.9\n14.1\n37.0\n7.2\n26.4\n2.4\ncompletion-then-rewriting\n27.7\n23.6\n22.2\n22.5\n19.4\n4.7\n3.7\n2.3\nTable 3: Balancing rewriting-then-completion for the buggy and reference settings with bug-detection\nthresholding. Completion performances can be adjusted via varying bug-detection thresholds from 0\n(fix all partial codes) to 1 (keep all partial codes). Results are with CODEGEN-2B-MONO.\nbuggy-FixEval\nthreshold\npartial-code setting\n0\n0.3\n0.9\n1\nreference\n8.6\n14.6\n37.0\n37.8\nbuggy\n8.6\n7.1\n7.2\n4.3\nSynthetic bugs versus real bugs.\nAmong the two bCC datasets, we observe that the overall\nperformance of mitigation methods is better on buggy-HumanEval than buggy-FixEval. This indicates\nthe difficulty of realistic potential bugs in buggy-FixEval: There may be multiple bugs; bugs may\nbe potentially mixed with non-bug-fixing changes; and bugs are more nuanced than single operator\nchanges. Furthermore, while achieving the best performance in most cases, completion-then-rewriting\nonly shows marginal differences from other methods when using larger models on buggy-FixEval.\nTake-away:\nOur baseline methods improve the completion for all evaluated Code-LLMs. However,\nthe remaining performance gaps to the completion with reference partial code are still large.\n4.4\nWhat If Partial Code Does Not Have Potential Bugs?\nAs shown in Table 2, mitigation methods for bCC may harm completion from reference code context\n(removal-then-completion is the same for both settings, thus not listed.) This suggests that a general\ncode completion should consider both cases when potential bugs may and may not exist.\nWith our baselines, we can use the thresholding approach for detecting potential bugs. For instance,\nin rewriting-then-completion, a token is only considered a potential bug if its likelihood gap to\nthe argmax token is beyond a threshold (between 0 and 1). Table 3 compares pass@1 scores of\nrewriting-then-completion varying thresholds on buggy-FixEval. We can see that the threshold of 0.9\ncan help achieve a relatively good balance for the two cases. A similar approach can be applied to\ncompletion-then-rewriting, as RealiT provides the probability of a token being a bug.\nRemark 4.1 (Imbalance in real distributions). We note that distributions of natural code bugs are\nusually imbalanced as practical bugs occur infrequently [36, 37]. However, potential bugs may occur\nmore frequently as our target setting is a work-in-progress code scenario [38] rather than high-quality\ncode of popular open-source projects in previous studies. Nevertheless, for comparing methods, we\nevaluate more balanced data to avoid the dominant effect of performance on reference code.\n4.5\nAnalysis of Effect from Bug and Split Locations\nTo understand how bug location and context size affect completion, we aggregate results by the loca-\ntion of potential bugs and the location of partial code splits within buggy-HumanEval. The locations\nare normalized as (potential bug line #)/(# lines) and (split line #)/(# lines), where potential bug\nline #, split line #, and # lines are the number of lines starting from the function header to the line\ncontaining the potential bug, to the end of the partial code, and to the end of the canonical solution.\nFigure 3 presents heatmaps of pass@1 scores (averaged over the bCC instances falling into each cell)\nevaluated on the CODEGEN-2B-MONO with na\u00efve completion on reference HumanEval, and na\u00efve\n8\n(a)\n(b)\n(c)\n(d)\nFigure 3: Average pass@1 scores by bug and code split locations using CODEGEN-2B-MONO\non buggy-HumanEval. Left to right: (a) na\u00efve completion on reference code, (b), (c), (d) na\u00efve\ncompletion, completion-then-rewriting, and rewriting-then-completion on buggy code. Locations are\nnormalized by the length of reference solutions. For each split location in (a), the scores may vary\nacross potential bug locations as the buggy-instance distribution is non-uniform across bug locations.\ncompletion, completion-then-rewriting, and rewriting-then-completion (max) on buggy-HumanEval.\nFirst, the model performs better given longer partial code in the reference case (Figure 3a). While the\nna\u00efve completion performs overall poorly with potential bugs (Figure 3b), it performs relatively well\nwhen potential bugs appear on or near the last line of the partial code (along the diagonal). More\ninterestingly, rewriting-then-completion achieves higher scores when potential bugs appear later\n(Figure 3d), while completion-then-rewriting performs better with longer code prefixes (Figure 3c).\nWe suspect that a longer prefix makes completion models less likely to deviate from the reference\ncompletion. Hence, as inputs to the subsequent rewriting model, the completed code better resembles\ninput types for which the rewriting model is trained, making the repairing more likely to succeed.\n4.6\nCase Studies\nAre potential bugs always harmful?\nAs discussed in Section 2, potential bugs do not guarantee\nthe completed code to be buggy. While we observe performance degradation of Code-LLMs under\nthe presence of potential bugs, we find several interesting cases where the models manage to generate\ncorrect code. For instance, Figure 12 in Appendix shows that for the potential bug == (highlighted)\nmodified from !=, the completion model deviates its original algorithmic flow with continue\ncommand and completes with correct functionality, albeit different from the canonical solution. This\nis an example that some bCC cases are recoverable and that Code-LLMs can adapt to them.\nWhen do Code-LLMs succeed at bCC?\nFor the successful cases of na\u00efve completion at bCC, we\nobserve that either the model (i) ignores the incorrect state and generates the correct completion or\n(ii) takes into account the potential bug to generate a completion adapting to it. Figure 9 in Appendix\nshows an example when Code-LLMs ignore the if-else statement to bypass the potential bug.\nFurther case studies and more elaborate discussions are in Appendix D.\n5\nRelated Work\nCode completion. Code completion provides code suggestions based on a given context [39, 40, 38].\nThe scope of completion ranges from the next token or line [11], method and class names [41], to\nthe entire of the function [42] or program. Early works [5, 43, 4] viewed code as sequences of\ntokens and applied statistical language models to the problem, along with other attempts at building\nprobabilistic models of code [44, 41, 45]. Later works adopted deep neural networks [8, 46] and\npre-training techniques [9, 47] for better code modeling and completion. Our work considers the\ncode completion setting at the function and program levels and focuses on using large language\nmodels for completion. Beyond sequence modeling, recent works considered integrating code prior\nknowledge via abstract syntax trees [6, 8, 48], code token types [9], graph structures [7], hierarchical\ncontext [49], or generating the sketch [50], or even extending the task\u2019s information beyond the given\ninput files [51, 52]. We focus on using only the task description and partial code as input prompts to\nthe model, allowing the use of more Code-LLMs and publicly available datasets.\n9\nAutomatic program repair.\nThe research on automatic program [53, 54, 55, 56, 57, 58, 59, 25]\nrelieves developers from the enormous effort of finding and fixing programming bugs. Recently,\nCode-LLMs have been adapted for program repair by translating buggy programs into their reference\ncounterparts [60, 31, 61]. Among those, we use RealiT [32] as our repair model in the completion-\nthen-rewriting method since they obtain the SotA results and utilize similar simple mutations during\ntraining. Despite the similarity, code repair commonly targets fixing bugs from complete programs\nwhile we study potential bugs from partial code. To enrich the amount of data for program repair,\nmethods have been proposed to synthesize artificial bugs through code mutants [62, 63, 64, 65, 25]\nor learning to create bugs [66]. Similarly, we employ code mutants to create artificial bugs.\nRelation to adversarial examples.\nAdversarial examples are instances where small perturbations\nto the input lead the model to change into wrong predictions. They have been extensively studied\nin computer vision [67, 68] and natural language processing [69]. Recent works suggested similar\nsituations for code-learning models, where small, semantic-preserving code transformations led\nto performance degradation [70, 71]. Buggy-code completion can be seen as a dual problem to\nadversarial examples, where we expect the model to adjust its predictions up on small semantic-\naltering changes in the input. In our case, sensitivity is not a problem, but insensitivity is.\nBenchmarks for buggy-code completion. Multiple benchmarks have been studied for code comple-\ntion and program repair. For code completion, CodeXGLUE [11], CodeNet [30], and HumanEval [10]\nare widely used. CodeXGLUE contains corpora of Java and Python programs for completion but\nonly supports match-based evaluation. CodeNet collects programming problems from online judge\nsites, with both solutions and test cases. HumanEval can be considered a Python-function completion\ndataset, with the context being the problem statement and function header. We derive our datasets\nfrom HumanEval and CodeNet datasets. For neural program repair, many datasets require the\nmatch-based evaluation [24, 26] or focus on the compiler errors [23, 25], which are different from\nour setting. While IntroClass [27], QuixBugs [28], Defects4J [72], or Refactory [29] provide the test\nsuites for evaluation, their test does not reflect the real-world bugs or lacks context support for the\nuse with Code-LLMs [19]. FixEval [19] is recently proposed as a new context-aware program repair\ndataset to mitigate these limitations, with many problems derived from the real submitted programs.\nHowever, as FixEval does not provide the problem statement and focuses solely on the program\nrepair, we derive a new benchmark using FixEval and its source of problems \u2013 CodeNet [30].\n6\nDiscussion and Conclusion\nLimitations.\nOur baseline methods developed for bCC may degrade the completion performance\non reference code context, as shown in Section 4.4, suggesting the need for balancing the buggy\nand reference settings in solutions to bCC. Furthermore, while buggy-FixEval is associated with\nreal-world coding contest programs, it is unclear how closely buggy-FixEval aligns to the general\nsoftware development setting where obtaining a test suite and proper evaluation are more challenging.\nImpact and applications.\nAs our work focuses on the less refined and more error-prone work-in-\nprogress code, the code context should be viewed as a hint of user intent rather than a high-quality\n\u201cgold\u201d implementation. It thus naturally follows that a pair programmer or a smart tool should suggest\na change to the draft code rather than blindly continue it if they believe a certain part of the existing\ndraft is not intended. From a user experience perspective, an IDE can display code change suggestions\nto a user\u2019s existing code if such parts are identified. Similar functionality already exists for other\ntypes of code change suggestions, e.g., spelling correction and missing imports.\nConclusion.\nWe introduce and define the buggy-code completion problem, inspired by the practical\ncoding scenario where one completes a coding program given the problem statement and a partial\ncode with potential bugs. We construct two new datasets, buggy-HumanEval and buggy-FixEval, as\ntask benchmarks and find that the presence of potential bugs significantly degrades the completion\nperformance of all evaluated large language models of code. Our further investigation of completion\nmethods for Code-LLMs in dealing with potential bugs shows that completing with potential bugs\nremains challenging despite augmenting models with external program-repair models. We provide\nextensive ablation and case studies for further understanding and analysis of buggy-code completion\nsetting. We hope our novel and systematic study paves the way for future works in understanding and\nimproving the usability of Code-LLMs under practical software-development settings.\n10\nReferences\n[1] Sven Amann, Sebastian Proksch, Sarah Nadi, and Mira Mezini. A study of visual studio usage\nin practice. In 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and\nReengineering (SANER), volume 1, pages 124\u2013134. IEEE, 2016.\n[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of\nmachine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1\u201337,\n2018.\n[3] Triet HM Le, Hao Chen, and Muhammad Ali Babar. Deep learning for source code modeling\nand generation: Models, applications, and challenges. ACM Computing Surveys (CSUR),\n53(3):1\u201338, 2020.\n[4] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu. On the\nnaturalness of software. Communications of the ACM, 59(5):122\u2013131, 2016.\n[5] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. A statistical\nsemantic language model for source code. In Proceedings of the 2013 9th Joint Meeting on\nFoundations of Software Engineering, pages 532\u2013542, 2013.\n[6] Jian Li, Yue Wang, Michael R Lyu, and Irwin King. Code completion with neural attention and\npointer networks. arXiv preprint arXiv:1711.09573, 2017.\n[7] Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. Genera-\ntive code modeling with graphs. arXiv preprint arXiv:1805.08490, 2018.\n[8] Chang Liu, Xin Wang, Richard Shin, Joseph E Gonzalez, and Dawn Song. Neural code\ncompletion. OpenReview, 2016.\n[9] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. Multi-task learning based pre-trained language\nmodel for code completion. In Proceedings of the 35th IEEE/ACM International Conference on\nAutomated Software Engineering, pages 473\u2013485, 2020.\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[11] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, et al.\nCodexglue: A machine learning\nbenchmark dataset for code understanding and generation. In Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\n[12] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong.\nA conversational paradigm for program synthesis.\narXiv preprint\narXiv:2203.13474, 2022.\n[13] Ariel Assaraf. This is what your developers are doing 75% of the time, and this is the cost you\npay. https://coralogix.com/blog/this-is-what-your-developers-are-doing-75-of-the-time-and-\nthis-is-the-cost-you-pay/, 2015.\n[14] Tom Britton, Lisa Jeng, Graham Carver, and Paul Cheak. Reversible debugging software\n\u201cquantify the time and cost saved using reversible debuggers\u201d. Citeseer, 2013.\n[15] Vincent J Hellendoorn, Sebastian Proksch, Harald C Gall, and Alberto Bacchelli. When code\ncompletion fails: A case study on real-world completions. In 2019 IEEE/ACM 41st International\nConference on Software Engineering (ICSE), pages 960\u2013970. IEEE, 2019.\n[16] Gareth Ari Aye, Seohyun Kim, and Hongyu Li. Learning autocompletion from real-world\ndatasets. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software\nEngineering in Practice (ICSE-SEIP), pages 131\u2013139. IEEE, 2021.\n[17] Marc Otten. User evaluation of incoder based on statement completion. Bachelor\u2019s Thesis,\nDelft University of Technology, 2022.\n11\n[18] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\n[19] Md Mahim Anjum Haque, Wasi Uddin Ahmad, Ismini Lourentzou, and Chris Brown. Fixeval:\nExecution-based evaluation of program fixes for competitive programming problems. arXiv\npreprint arXiv:2206.07796, 2022.\n[20] Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision\ntrees. ACM SIGPLAN Notices, pages 731\u2013747, 2016.\n[21] Miltiadis Allamanis and Charles Sutton. Mining source code repositories at massive scale using\nlanguage modeling. In 2013 10th Working Conference on Mining Software Repositories (MSR),\npages 207\u2013216. IEEE, 2013.\n[22] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,\nCollin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge\ncompetence with apps. In Thirty-fifth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2), 2021.\n[23] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c\nlanguage errors by deep learning. In Thirty-First AAAI conference on artificial intelligence,\n2017.\n[24] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and\nDenys Poshyvanyk. An empirical study on learning bug-fixing patches in the wild via neural\nmachine translation. ACM Transactions on Software Engineering and Methodology (TOSEM),\n28(4):1\u201329, 2019.\n[25] Michihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair.\nIn International Conference on Machine Learning (ICML), 2021.\n[26] Faria Huq, Masum Hasan, Md Mahim Anjum Haque, Sazan Mahbub, Anindya Iqbal, and\nToufique Ahmed. Review4repair: Code review aided automatic program repairing. Information\nand Software Technology, 143:106765, 2022.\n[27] Claire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar Devanbu,\nStephanie Forrest, and Westley Weimer. The manybugs and introclass benchmarks for au-\ntomated repair of c programs. IEEE Transactions on Software Engineering, 41(12):1236\u20131256,\n2015.\n[28] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: A multi-\nlingual program repair benchmark set based on the quixey challenge. In Proceedings Companion\nof the 2017 ACM SIGPLAN international conference on systems, programming, languages, and\napplications: software for humanity, pages 55\u201356, 2017.\n[29] Yang Hu, Umair Z Ahmed, Sergey Mechtaev, Ben Leong, and Abhik Roychoudhury. Re-\nfactoring based program repair applied to programming assignments. In 2019 34th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE), pages 388\u2013398. IEEE,\n2019.\n[30] Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir\nZolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca\nBuratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss.\nCodenet: A large-scale AI for code dataset for learning a diversity of coding tasks. In Thirty-\nfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track\n(Round 2), 2021.\n[31] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation. arXiv preprint\narXiv:2109.00859, 2021.\n[32] Cedric Richter and Heike Wehrheim. Can we learn from developer mistakes? learning to\nlocalize and repair real bugs from real bug fixes. arXiv preprint arXiv:2207.00301, 2022.\n12\n[33] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\nand synthesis. arXiv preprint arXiv:2204.05999, 2022.\n[34] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Science, 378(6624):1092\u20131097, 2022.\n[35] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming\nZhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code\nsynthesis. arXiv preprint arXiv:2009.10297, 2020.\n[36] Jingxuan He, Luca Beurer-Kellner, and Martin Vechev. On distribution shift in learning-based\nbug detectors. In International Conference on Machine Learning, pages 8559\u20138580. PMLR,\n2022.\n[37] Rafael-Michael Karampatsis and Charles Sutton. How often do single-statement bugs occur?\nthe manysstubs4j dataset. In Proceedings of the 17th International Conference on Mining\nSoftware Repositories, MSR \u201920, page 573\u2013577, New York, NY, USA, 2020. Association for\nComputing Machinery.\n[38] Xuechen Li, Chris J. Maddison, and Daniel Tarlow. Learning to extend program graphs to\nwork-in-progress code, 2021.\n[39] Marcel Bruch, Martin Monperrus, and Mira Mezini. Learning from examples to improve\ncode completion systems. In Proceedings of the 7th joint meeting of the European software\nengineering conference and the ACM SIGSOFT symposium on the foundations of software\nengineering, pages 213\u2013222, 2009.\n[40] Sebastian Proksch, Johannes Lerch, and Mira Mezini. Intelligent code completion with bayesian\nnetworks. ACM Transactions on Software Engineering and Methodology (TOSEM), 25(1):1\u201331,\n2015.\n[41] Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. Suggesting accurate\nmethod and class names. In Proceedings of the 2015 10th joint meeting on foundations of\nsoftware engineering, pages 38\u201349, 2015.\n[42] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simis-\nter, Ganesh Sittampalam, and Edward Aftandilian. Productivity assessment of neural code\ncompletion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine\nProgramming, pages 21\u201329, 2022.\n[43] Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On the localness of software. In\nProceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software\nEngineering, pages 269\u2013280, 2014.\n[44] Miltiadis Allamanis and Charles Sutton. Mining idioms from source code. In Proceedings of\nthe 22nd acm sigsoft international symposium on foundations of software engineering, pages\n472\u2013483, 2014.\n[45] Pavol Bielik, Veselin Raychev, and Martin Vechev. Phog: probabilistic model for code. In\nInternational Conference on Machine Learning, pages 2933\u20132942. PMLR, 2016.\n[46] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In\nInternational conference on machine learning, pages 245\u2013256. PMLR, 2020.\n[47] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. Intellicode compose:\nCode generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering,\npages 1433\u20131443, 2020.\n[48] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction by feeding trees\nto transformers. in 2021 ieee/acm 43rd international conference on software engineering (icse),\n2021.\n13\n[49] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan.\nPymt5: multi-mode translation of natural language and python code with transformers. arXiv\npreprint arXiv:2010.03150, 2020.\n[50] Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt, and Miltiadis\nAllamanis. Learning to complete code with sketches. In International Conference on Learning\nRepresentations, 2021.\n[51] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy.\nReACC: A retrieval-augmented code completion framework. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n6227\u20136240, 2022.\n[52] Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis. Better context\nmakes better code language models: A case study on function call argument completion. In\nProceedings of the AAAI Conference on Artificial Intelligence, 2023.\n[53] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent\nprograms with graphs. arXiv preprint arXiv:1711.00740, 2017.\n[54] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural\nprogram repair by jointly learning to localize and repair. arXiv preprint arXiv:1904.01720,\n2019.\n[55] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber.\nGlobal relational models of source code. In International conference on learning representations,\n2019.\n[56] Dobrik Georgiev, Marc Brockschmidt, and Miltiadis Allamanis. Heat: Hyperedge attention\nnetworks. arXiv preprint arXiv:2201.12113, 2022.\n[57] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-No\u00ebl Pouchet, Denys Poshyvanyk, and\nMartin Monperrus. Sequencer: Sequence-to-sequence learning for end-to-end program repair.\nIEEE Transactions on Software Engineering, 47(9):1943\u20131959, 2019.\n[58] Yi Li, Shaohua Wang, and Tien N Nguyen. Dlfix: Context-based code transformation learning\nfor automated program repair. In Proceedings of the ACM/IEEE 42nd International Conference\non Software Engineering, pages 602\u2013614, 2020.\n[59] Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from\ndiagnostic feedback. In International Conference on Machine Learning, pages 10799\u201310808.\nPMLR, 2020.\n[60] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pre-\ntraining for program understanding and generation. arXiv preprint arXiv:2103.06333, 2021.\n[61] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Pre-trained contextual\nembedding of source code. ArXiv, abs/2001.00059, 2019.\n[62] Jibesh Patra and Michael Pradel. Semantic bug seeding: a learning-based approach for creating\nrealistic bugs. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering\nConference and Symposium on the Foundations of Software Engineering, pages 906\u2013918, 2021.\n[63] Cedric Richter and Heike Wehrheim. Learning realistic mutations: Bug creation for neural bug\ndetectors. In 2022 IEEE Conference on Software Testing, Verification and Validation (ICST),\npages 162\u2013173. IEEE, 2022.\n[64] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and\nDenys Poshyvanyk. Learning how to mutate source code from bug-fixes. In 2019 IEEE\nInternational Conference on Software Maintenance and Evolution (ICSME), pages 301\u2013312.\nIEEE, 2019.\n14\n[65] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan.\nCoconut: combining context-aware neural translation models using ensemble for program repair.\nIn Proceedings of the 29th ACM SIGSOFT international symposium on software testing and\nanalysis, pages 101\u2013114, 2020.\n[66] Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. Self-supervised bug de-\ntection and repair. Advances in Neural Information Processing Systems, 34:27865\u201327876,\n2021.\n[67] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer\nvision: A survey. Ieee Access, 6:14410\u201314430, 2018.\n[68] Samuel Henrique Silva and Peyman Najafirad. Opportunities and challenges in deep learning\nadversarial robustness: A survey. arXiv preprint arXiv:2007.00753, 2020.\n[69] Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. Adversarial attacks\non deep-learning models in natural language processing: A survey. ACM Transactions on\nIntelligent Systems and Technology (TIST), 11(3):1\u201341, 2020.\n[70] Jordan Henkel, Goutham Ramakrishnan, Zi Wang, Aws Albarghouthi, Somesh Jha, and Thomas\nReps. Semantic robustness of models of source code. In 2022 IEEE International Conference\non Software Analysis, Evolution and Reengineering (SANER), pages 526\u2013537. IEEE, 2022.\n[71] Goutham Ramakrishnan and Aws Albarghouthi. Backdoors in neural models of source code. In\n2022 26th International Conference on Pattern Recognition (ICPR), pages 2892\u20132899. IEEE,\n2022.\n[72] Ren\u00e9 Just, Darioush Jalali, and Michael D Ernst. Defects4j: A database of existing faults to\nenable controlled testing studies for java programs. In Proceedings of the 2014 International\nSymposium on Software Testing and Analysis, pages 437\u2013440, 2014.\n15\nA\nDataset Details\nThis section presents the details of dataset construction and specifications. The length statistics are\nreported in Tab 5.\nA.1\nBuggy-HumanEval\nHumanEval5 [10] is a dataset designed for evaluating code generations from natural-language\ndescriptions. It contains 164 manually written introductory coding problems in Python 3. Each\nproblem is given in the format of a partial program, referred to as a \u201cprompt\u201d: a function header\nwith a docstring, sometimes with necessary imports and helper functions before it. The problem\ndescription and often a few input-output examples are encapsulated in the docstring. Each problem is\naccompanied by a separate set of test cases and a manually written function body referred to as the\n\u201ccanonical solution\u201d, such that the concatenation of the prompt and the canonical solution passes all\nthe tests. See an example in Figure 5.\nTo create buggy-HumanEval, we introduce artificial bugs by flipping binary operators in the canonical\nsolutions into their semantic opposites. Specifically, we use the Python ast6 library and consider all\noperators under ast.BinOp, ast.Compare and ast.AugAssign, e.g. /, >= and +=. For each found\noperator, we change it to its semantic opposite, e.g., / to *, >= to <, or += to -=. Table 4 shows a\ncomplete list of the operators we considered and their opposites. We then check if the solution fails\nany test cases after the change. We skip cases where the semantic opposite is ambiguous (e.g., mod %\nvs. floor division // or multiplication *) or where the altered solution still passes all the tests7.\nSuppose the canonical solution has L lines, and the operator change happens in the l-th line. For\neach l \u2264 i < L, we append the first i lines of the altered canonical solution to the original\nHumanEval prompt to form a piece of potentially buggy partial code, aka the \u201cbuggy\u201d prompt for\nbuggy-code completion. Accompanying each buggy prompt, we also provide a \u201creference\u201d prompt\nby concatenating the original prompt with the first i lines of the original canonical solution. In the\nexample in Figure 5, L = 8, l = 3, i = 4. The != in the third line of the canonical solution is changed\nto ==.\nWe generated 1896 buggy-code completion instances from 107 unique HumanEval problems. This\nprocedure results in the list of the number of instances per problem as follows,\n[9, 31, 0, 5, 1, 0, 13, 0, 3, 0, 0, 4, 1, 0, 1, 0, 0, 0, 8, 0, 16, 0, 0, 0, 1, 28, 0, 0, 0, 0, 0, 10, 63, 0, 0, 2, 14,\n2, 0, 38, 6, 0, 0, 7, 5, 0, 14, 3, 6, 1, 0, 0, 2, 0, 0, 6, 16, 4, 2, 27, 0, 16, 0, 12, 5, 3, 1, 0, 6, 21, 0, 51, 26,\n7, 14, 13, 12, 0, 1, 0, 22, 180, 12, 1, 0, 0, 0, 1, 0, 9, 0, 0, 18, 2, 32, 57, 5, 0, 1, 47, 0, 5, 12, 8, 2, 0, 39,\n23, 8, 16, 22, 18, 0, 10, 16, 0, 0, 5, 15, 34, 4, 0, 0, 37, 52, 0, 17, 31, 1, 133, 29, 14, 21, 1, 0, 11, 3, 1, 0,\n6, 174, 20, 14, 15, 13, 8, 6, 35, 12, 2, 11, 0, 0, 18, 16, 8, 6, 0, 0, 7, 2, 11, 0, 0]\nTable 4: We considered Python 3 operators when introducing bugs to HumanEval.\nast class\nBinOp\nAugAssign\nCompare\nast token\nAdd\nSub\nMult\nDiv\nAdd\nSub\nMult\nDiv\nEq\nNotEq\nLt\nLtE\nGt\nGtE\nOperator\n+\n-\n*\n/\n+=\n-=\n*=\n/=\n==\n!=\n<\n<=\n>\n>=\nOpposite\n-\n+\n/\n*\n-=\n+=\n/=\n*=\n!=\n==\n>=\n>\n<=\n<\nTable 5: Length (the number of tokens) statistics of buggy-HumanEval and buggy-FixEval.\nPercentiles\n50th\n90th\n95th\n98th\n99th\n100th\nbuggy-HumanEval\n214\n470\n550\n595.6\n606.7\n617\nbuggy-FixEval\n71\n242.7\n262\n330\n402.4\n566\n5https://github.com/openai/human-eval\n6https://docs.python.org/3.9/library/ast.html\n7We found 5 cases in our initial trials where the altered solutions still passed their test cases. Namely,\nchanging the first applicable operators in the canonical solution of task IDs \u2018HumanEval/10\u2019, \u2018HumanEval/18\u2019,\n\u2018HumanEval/39\u2019, \u2018HumanEval/40\u2019, \u2018HumanEval/129\u2019. Upon close examination, in these cases, either the altered\nsolution is still correct or the altered solution is incorrect, but the test cases are too weak to tell.\n16\n(a) A prompt from buggy-FixEval, containing a problem statement and a buggy partial code.\n(b) The original FixEval accepted (left) and rejected (right) user submissions.\nFigure 4: Example of a buggy-FixEval instance. The example is based on p02546.\nA.2\nBuggy-FixEval\nFixEval8 [19] is a dataset derived from CodeNet9 [30] for studying program repair. It consists of\nprogramming problems from online programming contest websites10, each with test cases, user-\nsubmitted solutions and judging verdicts to the submissions (e.g., accepted \u2013 passing all the tests;\nwrong answer \u2013 failing at least one test; or time limit exceeded \u2013 the program did not terminate within\nthe specified time limit). The test cases in FixEval are more extensive than those in HumanEval, as\nthe programming contest websites use them to automatically judge a submission\u2019s correctness. Each\nproblem comes with an alphabetical label (\u201cA\u201d to \u201cF\u201d) indicating the order in which the problem\nappears in its contest, with \u201cA\u201d usually being the easiest. Different from HumanEval, a solution or\nsubmission here is supposed to be a complete program, reading input from and writing output to the\nstandard input and output instead of just a function implementation.\nTo derive a dataset for buggy-code completion, we pair each FixEval problem with its matching\nproblem statements from CodeNet and discard problems without a match. Among the remaining\nproblems, we focus on the relatively easy \u201cA\u201d-label problems because the post-\u201cA\u201d problems have\nbeen reported to be very challenging to solve even for recent language models of code: pass@1 <\n12% with CodeT5 [31] under a program-repair setting [19, Section 5.3].\nWe start by selecting rejected-accepted solution pairs. For each streak of Python submissions from a\nuser to a problem, if the streak ends with an accepted submission, we take the last submission with\nthe verdict being none of accepted, compilation error or runtime error as its rejected counterpart.\nThen, we remove all comments and empty lines for each accepted-rejected pair and check the\nfollowing. (1) Neither submission contains exit() or system calls, as they cause trouble for\nexecution. (2) The first halves of the two submissions contain at least one non-white-space difference,\nincluding case difference, space/indent difference, or variable names for the assignment operator.\n(3) The character-level edit distance between the first halves of the two submissions is smaller than\n20. Regarding the threshold of 20, we sampled 100 pairs of submissions with various edit distances.\nWe qualitatively verified that under this threshold, the differences between the two submissions\nare, in the majority of cases, focused semantic changes (or likely bug fixes) and not refactoring or\nre-implementations.\n8https://github.com/mahimanzum/FixEval\n9https://github.com/IBM/Project_CodeNet\n10The bases of the data are from AtCoder (https://atcoder.jp/) and AIZU Online Judge (https:\n//onlinejudge.u-aizu.ac.jp/).\n17\n(a) The original HumanEval prompt containing a function header and a problem description\nas a docstring.\n(b) The canonical solution provided in HumanEval.\n(c) The prompt used for buggy-code completion, containing the original problem and a\nbuggy partial code.\n(d) The test suite used for evaluating a completed program.\nFigure 5: Example of buggy-HumanEval instance. The example is based on HumanEval/0. The\nbuggy prompt (Figure c) is directly derived from the original prompt (Figure a) and the canonical\nsolution (Figure b). We reuse the test suite (Figure d) for our evaluation.\nThe above process resulted in 492 pairs of prefixes from rejected-accepted solutions from 94 \u201cA\u201d-level\nproblems. We manually inspected all the pairs to ensure the pairs satisfy the desiderata described in\nSection 3.1. Namely, we identify the following cases.\n1. The prefix from the accepted solution is already a complete program that passes all the test\ncases. This can happen when the accepted solution is very short, for example, fewer than\nthree lines, and/or followed by comments or unnecessary code. This counts for 26 pairs.\n18\n2. The prefix from the rejected solution and the prefix from the accepted solution are semanti-\ncally equivalent up to variable name normalization. This counts for 33 pairs.\n3. The prefix from the rejected solution contains no control flow (e.g., if, for, while) and no\noutput clause. In this case, a completion can ignore the prefix. This counts for 141 pairs.\n4. The prefix from the rejected solution is bound to output wrong answers upon a certain input.\nFor example, it contains a print() statement within a wrong condition. This counts for 25\npairs.\nA list of identified cases can be found in our code repository. Case 1 makes the completion trivial\nfrom reference prefixes. Case 2 makes the comparison between the buggy completion and reference\ncompletion less meaningful. Case 3 makes the buggy completion less challenging. Thus, we exclude\ninstances identified as cases 1, 2, and 3. We keep the case 4 instances because they fit the definition\nof potential bugs and are especially interesting in the sense that the \u201cbugginess\u201d already manifested\nin the code prefix.\nAfter the above step, we are left with 292 pairs. We then use the first half of the rejected submission\nas a piece of buggy partial code and the first half of the accepted submission as the corresponding\npiece of reference code for buggy-code completion. A \u201cbuggy\u201d prompt is formed by prepending the\nproblem statement as a file-level docstring, or a multi-line string literal, to the buggy partial code.\nSee an example in Figure 4.\nB\nDetails of rewriting-then-completion Method\nWith rewriting-then-completion, we attempt to eliminate potential bugs via a two-step procedure. We\nfirst locate the most likely line to contain a potential bug using a likelihood-based measure and then\nrewrite the line using an infilling code language model.\nB.1\nLine Selection\nOur underlying idea is to treat potential bugs as outliers in the generation flow of Code-LLMs. We\nobserve that most reference code has lower perplexity than the corresponding buggy code.\nLikelihood-based measures.\nWe calculate the score of each line as follows: With a model that can\ngive a token distribution for each token location, we define the token score as the difference between\nthe likelihoods of the most probable token (i.e., the argmax token) and the actual observed token. We\nget the score of a line by taking either the maximum or the average of all the non-zero token scores\nwithin it. The line with the largest score is finally selected to be rewritten.\nTo help better understand likelihood-based measures, consider the problem and the code prefix with\npotential bugs shown in Figure 8. The partial code has four lines in the function body. Given a\nlanguage model G, for each token x, we calculate the token-level score as p2 \u2212 p1:\n\u2022 p1: probability of generating x from the code up to x\n\u2022 p2: probability of generating x\u2217 from the code up to x, where x\u2217 gives the highest probability\naccording to G.\nFor example, consider line 3 and token ==. We would have p1 = 0.01 (probability of G generating\n==), and p2 = 0.95 (!= is the most probable token according to G). The score of == is then 0.94. We\nobtain the line\u2019s score by taking the maximum scores at all token locations in the line. This way of\naggregating token scores to line scores is referred to as \u201cLikelihood (Max.)\u201d Below is the variation\nused to report results in the main text.\nNote that we take two measures to reduce the uncertainties of the likelihood estimation: (i) instead of\nthe likelihood score of the target token itself, we use the maximal margin between the target token\nand the argmax token (similar to the popular approaches used in uncertainty quantification), and (ii)\nwe aggregate the score gap along the line and set a high threshold to be more conservative.\nWe find that accuracies of localizing the line of potential bugs (with the same setting described\nabove) are approximately 82% and 53% respectively on buggy-HumanEval and buggy-FixEval. For\nbuggy-FixEval, we compare the detected line with the line of the first semantic difference between\nbuggy and reference prefixes.\n19\nTable 6: Pass@1 (\u2191) of each selection and completion method on the buggy-HumanEval and buggy-\nFixEval datasets. The best results for each dataset and completion model are in bold. The better\nlikelihood aggregation method is underlined.\nDataset\nSelection Method\nCompletion Model\nCG-350M\nCG-2B\nINCODER-1B\nINCODER-6B\nbuggy-HumanEval\nHeuristic Oracle\n22.1\n29.6\n21.4\n28.5\nLikelihood (Max.)\n14.1\n24.9\n9.1\n16.4\nLikelihood (Avg.)\n13.2\n23.0\n9.8\n16.9\nbuggy-FixEval\nHeuristic Oracle\n1.7\n3.3\n1.2\n2.2\nLikelihood (Max.)\n2.4\n7.2\n2.6\n5.1\nLikelihood (Avg.)\n2.7\n7.9\n2.4\n5.4\nHeuristic oracle for comparison.\nTo see how well our likelihood-based measures work, we\ncompare them against a heuristic oracle for predicting the line of potential bugs. In particular, we\ncompare the buggy code prefix against the corresponding reference code prefix and select the first\nline with non-trivial differences. The differences are not about space, indent, or comment. Since this\nrequires access to the corresponding reference code prefix, it cannot be used as part of a solution for\nbCC. We thus refer to it as an \u201coracle\u201d and only use it here as a method of reference to gauge the\nperformance of other line-selection methods. We do not use this oracle in any methods reported in\nthe main text.\nB.2\nRewriting\nAfter identifying the line most likely to contain a potential bug, we rewrite it by masking it and\ngenerating a replacement line using a code-infilling model such as INCODER [33].\nB.3\nResults\nShown in Table 6 is our comparison between different likelihood-based measures. We observe that\nthe Heuristic Oracle substantially outperforms the likelihood-based methods in buggy-HumanEval,\nbut the reverse is true for buggy-FixEval. We provide a likely explanation for this phenomenon in\nthe next paragraph. We also observe that Likelihood (Max.) outperforms Likelihood (Avg.) on\naverage on buggy-HumanEval, but the reverse is true for buggy-FixEval. A possible explanation\nis that since the potential bugs in buggy-HumanEval take the form of a single token change, they\nare more amendable to being detected by focusing on the token with the largest score (maximum\naggregation). In such a scenario, taking the average across all tokens in the line might dilute this\nsignal. On the other hand, potential bugs are likely more subtle in a natural setting (represented by\nbuggy-FixEval), and the potential bug may be spread out across multiple tokens. The fact that the\naverage aggregation considers this may explain why it outperforms the maximum aggregation on\nbuggy-FixEval.\nWe note that a large gap in performance between buggy-HumanEval and buggy-FixEval for the\nHeuristic Oracle method. This may be justifiable based on the information known to the Heuristic\nOracle. In buggy-HumanEval, we introduce the bugs synthetically via a single semantically altering\noperator change. Hence, the oracle method has guaranteed knowledge of the line where the bug\nwas introduced. This is not the case in buggy-FixEval since the examples were constructed directly\nfrom human submissions without synthetic perturbations. Therefore, the line corresponding to the\nfirst difference between these two submissions may not correspond to the bug location, or multiple\n(instead of one) locations can change between these two submissions. Therefore, this method is closer\nto a heuristic than a true oracle for buggy-FixEval. This is reflected in Heuristic Oracle\u2019s substantially\nweaker performance on FixEval. The fact that likelihood-based methods outperform the Heuristic\nOracle on buggy-FixEval supports this explanation.\n20\nFigure 6: Comparison of CodeLMs per each completion method in terms of pass@k. CG: CODEGEN,\nComp: Completion. We see the trend consistent with CODEGEN-2B-MONO > INCODER-6B >\nCODEGEN-350M-MONO > INCODER-1B across methods and k\u2019s.\nC\nFull Definition Statement and Additional Results\nC.1\nFull Statement for Definition 2.1 in Section 2\nDefinition C.1 (potential bug in a code prefix). Assume a problem given by specification h with the\ntest fh for evaluating its functional correctness and reference partial code s. Let T h\ns be the set of all\nvalid programs, or solutions that satisfy h and have the prefix s, i.e., T h\ns = {t := s :: c |fh(t) = 1}.\nLet Ch\ns be the set of all valid completed code, i.e., Ch\ns = {c |s :: c \u2208 T h\ns }. A potential bug of s is an\nedit ge on a token of s that causes at least one existing solution to fail, i.e., for s\u2032 = ge(s), there exists\nc \u2208 Ch\ns s.t. fh(s\u2032 :: c) = 0. The prefix s\u2032 is called buggy with respect to reference prefix s.\nC.2\nFull Results with k = 1, 10, 100 and Interpretation for Section 4.3\nFigure 6 presents the full comparison among pass@k for k = 1, 10, 100 for all methods. Intuitively,\nhigher k reveals more on the diversity of generated solutions. The relative performances between dif-\nferent Code-LLMs are consistent across different k\u2019s. Regarding the relationship between completion\nmethods,\n\u2022 The na\u00efve completion and completion-then-rewriting methods achieve relatively small\nincreases as k goes up, as the partial code constrains the input to the completion step.\n\u2022 The increase in performance of removal-then-completion is relatively higher than na\u00efve\ncompletion when k increases, as its input is not constrained by the buggy partial code\nprefixes.\n\u2022 The performance of rewriting-then-completion achieves relatively big increases among\nthe methods as k goes up. This is probably because rewriting-then-completion balances\nthe completion-then-rewriting and removal-then-completion approaches. Thus, the pre-\ncompletion rewriting step increases the performance along with the diversity of code prefixes\nprovided in the completion step.\nC.3\nPerformance on CODEGEN-16B-MONO\nWe provide results for CODEGEN-16B-MONO in table 7. As we can see, CODEGEN-16B-MONO\nalso exhibits a similar phenomenon to our observations in previous Table 1 for smaller models,\ni.e., potential bugs make Code-LLMs more challenging to generate correct programs and proposed\nmitigation methods still suffer from a large gap to the reference-prefix completion.\n21\nTable 7: Pass@1 (\u2191) of completion methods with CODEGEN-16B-MONO. *Best method in each\ncolumn is in bold.\nPrefix\nMethod\nbuggy-HumanEval\nbuggy-FixEval\nreference\ncompletion\n68.9\n49.4\nbuggy\ncompletion\n4.4\n8.0\nremoval-then-completion\n20.2\n17.6\nrewriting-then-completion\n24.3\n9.7\ncompletion-then-rewriting\n24.1\n7.7\nC.4\nAblation Study: Concatenating Buggy-based Completion with Reference Prefix\nIn this ablation study, we use the buggy prefix to complete, then replace the buggy prefix with the\nreference prefix, shown in the bottom row of Table 8 (reference prefix + buggy-based completion).\nThe results indicate that Code-LLMs fail to react to the change in the buggy prefix, as discussed in\nSection 4.2.\nTable 8: Concatenating buggy-code-based completion with reference prefix.\nbuggy-HumanEval\nbuggy-FixEval\nCODEGEN-\n2B\n350M\n2B\n350M\nreference prefix + reference-based completion\n54.9\n43\n37.8\n27.6\nbuggy prefix + buggy-based completion\n3.1\n0.7\n4.3\n2.4\nreference prefix + buggy-based completion\n40.8\n31.6\n4.6\n2.5\nD\nDetailed Case Studies\nFor the case studies, We use the synthetic buggy-HumanEval dataset for our study. We use CODEGEN-\n2B-MONO as the completion model throughout this section as it achieved the best bCC performance\noverall. We surface interesting bCC examples by comparing pass rates for reference and buggy\nprefixes. For example, the Code-LLM may easily complete some reference prefixes (i.e., high pass\nrate) but potentially fails with buggy prefixes (zero pass rate). Other examples may have a non-zero\npass rate, meaning that the Code-LLM can adapt to the potential bug in the prefix and yield a correct\nsolution. One observation that was observed uniformly about this study is that prefixes with potential\nbugs typically lead to lower pass rates, even when those were non-zero. This suggests that for these\nprefixes, the Code-LLM has to make more effort to find a correct solution.\nD.1\nAre Potential Bugs Always Harmful?\nAs discussed in Section 2, potential bugs do not guarantee that the completed code will be buggy.\nThese prefixes were sourced from completed examples that had a bug that could be ascribed to the\nprefix, but that does not mean by itself that the prefix will be impossible to complete correctly. While\nwe observe the performance degradation of Code-LLMs under the presence of potential bugs, we\nfind several interesting cases where these models generate correct code nonetheless.\nFigure 12 shows that for the potential bug (highlighted operator) == modified from != in the reference\ncode, the completion model updates its original algorithmic flow with continue command and\ncompletes with correct code. While the new completion code is different from the canonical solution\nof the reference code, it is functionally correct. Thus, this implies that\ncodellms may adapt in some recoverable cases.\nD.2\nWhy Do Code-LLMs Fail at bCC?\nWe investigate the cases where our models succeed with the reference partial code but fail with the\nbuggy partial code.\n22\nProblem specification\n\"\"\" You\u2019re given a list of deposit and withdrawal operations on a bank account\nthat starts with\nzero balance. Your task is to detect if at any point the balance of account\nfallls below zero, and\nat that point function should return True. Otherwise it should return False.\n>>> below_zero([1, 2, 3])\nFalse\n>>> below_zero([1, 2, -4, 5])\nTrue\n\"\"\"\nPrefix with potential bugs\nfrom typing import List\ndef below_zero(operations:\nList[int]) -> bool:\nbalance = 0\nfor op in operations:\nbalance += op\nif balance >= 0:\nModel completion\nreturn True\nelse:\nbalance -= op\nif balance <= -1:\nreturn False\nreturn True\nPrefix without potential bugs\nfrom typing import List\ndef below_zero(operations:\nList[int]) -> bool:\nbalance = 0\nfor op in operations:\nbalance += op\nif balance < 0:\nModel completion\nreturn True\nreturn False\nFigure 7: An example case of buggy-code completion where the model reacts to the code change\nbut still fails. The bCC instance is based on HumanEval/3. The model completions shown here are\nrepresentative completions for the given prefixes.\nOur first finding is that the model often fails to react to the changes in code, i.e., it produces the same\ncompletion for both buggy and reference code prefixes. We identify this by comparing if the most\npopular completions (out of all the sampled completions for a given instance) are the same for the\nreference and buggy code prefixes. We see that this happens in 90% of the instances and 93% of the\nproblems (with at least one failed instance). Figure 7 shows an example. This suggests that the model\nis not sensitive to and thus ignores minor code changes and/or defaults to common patterns in the\ntraining data.\nIn cases where the most popular completions differ for the reference and the buggy code prefixes, we\nfound that the potential bug often makes the code prefix significantly more challenging to complete\ncorrectly. In other words, the model may have recognized the potential bug and thus have significantly\nchanged the output distribution but still failed. Figure 8 shows an example. Here, the balance check\ncondition is reversed due to potential bugs. The model could ignore the given code or even define\na new function to overwrite the buggy one. However, this drastically deviates from the commonly\nobserved code patterns and thus may create a significant hurdle for the completion model if it is not\nintroduced or adapted to such cases.\nD.3\nWhen Do Code-LLMs Succeed at bCC?\nFigure 9 shows an example when Code-LLMs ignore the if-else statement to bypass the potential\nbugs. In successful cases, we have observed that either the model ignores the incorrect state and\ngenerates the correct completion or considers the potential bug in the prefix to create a completion\nadapted to it.\n23\nProblem specification\n\"\"\" Check if in given list of numbers, are any two numbers closer\nto each other than given threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nPrefix with potential bugs\nfrom typing import List\ndef has_close_elements(numbers:\nList[float], threshold: float)\n-> bool:\nfor idx, elem in\nenumerate(numbers):\nfor idx2, elem2 in\nenumerate(numbers):\nif idx != idx2:\ndistance = abs(elem -\nelem2)\nModel completion\nif distance < threshold:\nreturn True\nreturn False\nPrefix without potential bugs\nfrom typing import List\ndef has_close_elements(numbers:\nList[float], threshold: float)\n-> bool:\nfor idx, elem in\nenumerate(numbers):\nfor idx2, elem2 in\nenumerate(numbers):\nif idx == idx2:\ndistance = abs(elem -\nelem2)\nModel completion\nif distance < threshold:\nreturn True\nreturn False\nFigure 8: An example case of bCC where the model fails to react to the code change. The bCC\ninstance is based on HumanEval/0. The model completions shown here are the dominant completions\nfor the given prefixes.\nD.4\nHow Are the Proposed Completion Methods Better at bCC?\nThe completion-then-rewriting method can detect and fix several types of suspicious operators.\nFigure 10 shows an example from the string_xor problem. Here, we see that the direct completion\nfailed to react to the potential bug and continued as if the == operator was not changed to !=, which\ngives an incorrect solution. However, the code-repair model was able to identify and fix the !=\noperator back to ==, producing a correct solution. Similarly, Figure 11 shows a success case from\nrewriting-then-completion, where it restores the introduced operator change. As buggy-HumanEval\nconsists of bCC instances constructed from single operator changes, these methods can be particularly\neffective over na\u00efve completion in circumventing the introduced operator change.\n24\nProblem specification\n\"\"\" Check if in given list of numbers, are any two numbers closer to each other\nthan\ngiven threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nPrefix with potential bugs\nfrom typing import List\ndef has_close_elements(numbers:\nList[float], threshold: float)\n-> bool:\nfor idx, elem in\nenumerate(numbers):\nfor idx2, elem2 in\nenumerate(numbers):\nif idx == idx2:\ndistance = abs(elem -\nelem2)\nModel completion\ncontinue\nelse:\ndistance = abs(elem -\nelem2)\nif distance < threshold:\nreturn True\nreturn False\nPrefix without potential bugs\nfrom typing import List\ndef has_close_elements(numbers:\nList[float], threshold: float)\n-> bool:\nfor idx, elem in\nenumerate(numbers):\nfor idx2, elem2 in\nenumerate(numbers):\nif idx != idx2:\ndistance = abs(elem -\nelem2)\nModel completion\nif distance <\nthreshold:\nreturn True\nreturn False\nFigure 9: An example case of bCC where the model bypasses the potential bug and succeeds. The\nbCC instance is based on HumanEval/0. The model completion for the reference prefix is the most\ncommon. The model completion for the buggy prefix is one of the two successful completions.\n25\nProblem specification\n\"\"\" Input are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor(\u2019010\u2019, \u2019110\u2019)\n\u2019100\u2019\n\"\"\"\nPrefix with potential bugs\nfrom typing import List\ndef string_xor(a: str, b: str) ->\nstr:\ndef xor(i, j):\nif i != j:\nreturn \u20190\u2019\nModel completion\nelse:\nreturn \u20191\u2019\nreturn \u2019\u2019.join([xor(i, j) for i,\nj in zip(a, b)])\nOutput from completion-then-rewriting\nfrom typing import List\ndef string_xor(a: str, b: str) ->\nstr:\ndef xor(i, j):\nif i == j :\nreturn \u20190\u2019\nelse :\nreturn \u20191\u2019\nreturn \u2019\u2019.join([xor(i, j) for i,\nj in zip(a, b)])\nFigure 10: An example case of bCC where the completion-then-rewriting method successfully fixes\nthe potential bug. The bCC instance is based on HumanEval/11. The model completion for the\nbuggy prefix is representative.\nProblem specification\n\"\"\" Check if in given list of numbers, are any two numbers closer to each other\nthan\ngiven threshold.\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\n\"\"\"\nPrefix with potential bugs\nfrom typing import List\ndef has_close_elements(numbers:\nList[float], threshold: float)\n-> bool:\nfor idx, elem in\nenumerate(numbers):\nfor idx2, elem2 in\nenumerate(numbers):\nif idx == idx2:\nRewritten prefix\nfrom typing import List\ndef has_close_elements(numbers:\nList[float], threshold: float)\n-> bool:\nfor idx, elem in\nenumerate(numbers):\nfor idx2, elem2 in\nenumerate(numbers):\nif idx != idx2 and\nabs(elem - elem2) <\nthreshold:\nModel completion\nreturn True\nreturn False\nFigure 11: An example case of bCC, where the rewriting-then-completion method successfully fixes\nthe potential bug. The bCC instance is based on HumanEval/0.\n26\nFigure 12: A success case of buggy-code completion. The model manages to generate a functionally\ncorrect completion by surpassing the potential bug (highlighted) by a continue command. The\ncompletions are from CODEGEN-2B-MONO on HumanEval/0 from buggy-HumanEval.\n27\n"
  },
  {
    "title": "Learning to Ground Instructional Articles in Videos through Narrations",
    "link": "https://arxiv.org/pdf/2306.03802.pdf",
    "upvote": "1",
    "text": "Learning to Ground Instructional Articles in Videos through Narrations\nEffrosyni Mavroudi*, Triantafyllos Afouras*, Lorenzo Torresani\nMeta AI\n{emavroudi, afourast, torresani}@meta.com\nAbstract\nIn this paper we present an approach for localizing steps\nof procedural activities in narrated how-to videos. To deal\nwith the scarcity of labeled data at scale, we source the\nstep descriptions from a language knowledge base (wiki-\nHow) containing instructional articles for a large variety of\nprocedural tasks. Without any form of manual supervision,\nour model learns to temporally ground the steps of proce-\ndural articles in how-to videos by matching three modali-\nties: frames, narrations, and step descriptions. Specifically,\nour method aligns steps to video by fusing information from\ntwo distinct pathways: i) direct alignment of step descrip-\ntions to frames, ii) indirect alignment obtained by compos-\ning steps-to-narrations with narrations-to-video correspon-\ndences. Notably, our approach performs global temporal\ngrounding of all steps in an article at once by exploiting\norder information, and is trained with step pseudo-labels\nwhich are iteratively refined and aggressively filtered. In\norder to validate our model we introduce a new evaluation\nbenchmark \u2013 HT-Step \u2013 obtained by manually annotating a\n124-hour subset of HowTo100M1 with steps sourced from\nwikiHow articles. Experiments on this benchmark as well\nas zero-shot evaluations on CrossTask demonstrate that our\nmulti-modality alignment yields dramatic gains over sev-\neral baselines and prior works. Finally, we show that our\ninner module for matching narration-to-video outperforms\nby a large margin the state of the art on the HTM-Align\nnarration-video alignment benchmark.\n1. Introduction\nInstructional videos have emerged as a popular means\nfor people to learn new skills and improve their abilities in\nexecuting complex procedural activities, such as cooking\na recipe, performing home improvements, or fixing things.\nIn addition to being useful teaching materials for humans,\nhow-to videos are a promising medium for learning by ma-\n*equal contribution\n1A\ntest\nserver\nis\naccessible\nat\nhttps://eval.ai/web/\nchallenges/challenge-page/2082.\nInstructional Articles\nNarrated  \nInstructional Videos\n~14k\n~370k\nTraining \nResources  \n(No manual \nannotations)\nVINA\nNarrations (ASR)\nInstruction Steps\nVideo Frames\n1. Wash the pumpkins\n2. Cut the pumpkins\n3. Scoop out the seeds\n10. Puree pumpkin flesh\n\u201cFirst thing you\u2019ll need to do \nis cut off the stem.\u201d\n\u201cThis requires a sharp knife \nand some muscle\u201d\n\u201cThanks for watching\u201d\nSteps-to-Video Alignment (Direct)\nSteps-to-Narrations-to-Video Alignment (Indirect)\nNV\nSN\nSV\nSteps\nNarrations\nVideo Frames\nFigure 1: Our proposed Video, Instructions, and Narrations\nAligner (VINA) learns to simultaneously ground narrations\nand instruction steps in how-to videos from an uncurated\nset of narrated videos and a separate knowledge base of in-\nstructional articles, without any manual annotations. This is\ncontrary to prior work that learns how to align a video with\na single sequence of sentences by leveraging ground-truth\npairs of video-text sequences, e.g., a video and its narra-\ntions [19], or a video and an annotated, ordered list of steps\ndemonstrated in it [14].\nchines, as they provide revealing visual demonstrations of\ncomplex activities and show elaborate human-object inter-\nactions in a variety of domains. Motivated by this observa-\ntion, in this work we look at the task of temporally localiz-\ning the steps of procedural activities in instructional videos.\nThis problem is foundational to the broader goal of human-\nprocedure understanding and advances on this task promise\nto enable breakthrough applications, such as AI-powered\nskill coaching and human-to-robot imitation learning.\nPrior work has tackled procedural step localization by\narXiv:2306.03802v1  [cs.CV]  6 Jun 2023\nleveraging either (a) fully-annotated datasets where the task\nshown in the video is given (video-level labeling) and manu-\nally annotated temporal segments are provided for each step\n(segment-level labeling) [53] or (b) weakly-annotated train-\ning sets where the task and the order in which the steps ap-\npear in the video is given [70]. However, due to the inherent\nmanual cost involved in collecting step annotations, these\nworks have relied on datasets that are small-scale both in\nthe number of tasks (e.g., at most few hundreds [70]) and in\nthe number of video samples (e.g., 12k videos [53]). These\nlimitations affect both the generality and the complexity of\nthe models that can be trained on these benchmarks. In\nthis paper, we therefore pose the following question: can\nwe leverage large-scale, unlabeled video datasets to train a\nmodel that can ground procedural steps in how-to videos?\nTo answer this question, we propose a novel training\nframework for weakly-supervised step grounding that uti-\nlizes two freely available sources of information: (a) in-\nstructional articles which define ordered lists of steps for\na wide variety of tasks (e.g., from wikiHow) and (b) nar-\nrations which provide instance-specific rich commentaries\nof the execution of the task in the video, e.g., from ASR\ntranscriptions. Our work treats the former as an abstraction\nof the latter and uses the video-specific narrations to support\nthe grounding of the article steps. Specifically, during train-\ning, our method leverages narrations as an auxiliary signal\nto (i) identify the task shown in the video, (ii) temporally\nground the article steps that are visually-demonstrated and\n(iii) filter out steps that are not executed in the given in-\nstance. To further motivate this mechanism, let us look at\nthe example in Figure 1. The narrations help disambiguate\nthe task (make a pumpkin puree), enabling the automatic\nretrieval of relevant instructional articles for the video. Fur-\nthermore, the narrations can be matched to steps described\nin the articles to roughly localize the steps that are repre-\nsented in the video. In this example, the timestamp of \u201cFirst\nthing you\u2019ll need to do is cut off the stem\u201d provides a loose\ntemporal prior for the matching step \u201cCut the pumpkins.\u201d\nOn the other hand, steps that do not have any matching nar-\nrations (e.g., \u201cWash the pumpkins\u201d) are unlikely to be rep-\nresented in the video and thus can be rejected. Based on this\nintuition, we propose a procedure that learns to align steps\nto video by fusing information from two pathways. The first\nis an indirect pathway inferring step-frame alignments by\ncomposing step-to-narration assignments with narration-to-\nframe correspondences. The second is a direct pathway that\nlearns associations between step descriptions and frames by\nleveraging information from all videos having steps in com-\nmon.\nIn our experiments we demonstrate that our multi-\nmodality alignment leads to significant performance gains\nover several baselines, including single-pathway temporal\ngrounding, as well adaptations of prior works to our prob-\nlem. During inference, the direct pathway can be used by\nitself to temporally ground steps in absence of transcribed\nnarrations. When narrations are available at test time, our\nmethod improves further the accuracy of temporal ground-\ning by fusing the inference outputs of the two pathways.\nTo summarize, our work makes the following contribu-\ntions: 1) we learn to align steps to frames in how-to videos,\nusing only weak supervision in the form of noisy ASR nar-\nrations and instructional articles; 2) we propose a novel ap-\nproach for joint dense temporal grounding of instructional\nsteps and video narrations; 3) we introduce a new bench-\nmark for evaluating instructional step grounding which we\nwill make available to the community; 4) we demonstrate\nstate-of the art results on multiple benchmarks for both step\nas well as narration grounding.\n2. Related Work\nProcedural step recognition.\nPrior work on procedural\nstep localization [5, 8, 9, 16, 21, 31, 42, 45, 46, 58, 64, 67,\n68, 70] can be roughly divided into two categories, based on\nthe query formulation: the first class approaches the prob-\nlem in an open-world setting, where the use of text queries\ntransforms it into a temporal grounding task [2, 4, 18].\nSuch approaches can be further sub-divided into single\nstep grounding, where single steps are queried over the\nwhole video[24, 51] and dense temporal grounding meth-\nods [12, 19] where the objective is to jointly ground a se-\nquence of steps or whole article into the video. The sec-\nond body of works uses fixed taxonomies of steps, often\nas part of activities [26, 44, 53]. Our work is somehow\nrelated to Lin et al. [29] who use semantic similarity be-\ntween steps and narrations to obtain supervision for learn-\ning strong video representations. Although we also asso-\nciate steps from wikiHow articles to video frames through\nthe use of narrations, the two works differ in several as-\npects: we align steps to video by a global procedure that\ntakes into account all ordered steps in the article (inspired\nby dense temporal grounding methods [4]) and temporally\ngrounds them in the whole video, instead of matching in-\ndividual video clips to an orderless collection of steps; our\nstep grounding uses video in addition to steps and narra-\ntions while the method proposed in Lin et al. relies purely\non text-matching narrations to step descriptions; finally, the\nworks differ in objectives with our aim being step ground-\ning in long how-to videos rather than learning video-clip\nrepresentations.\nExisting methods also vary by the level of supervi-\nsion used during training. One option is leveraging fully-\nannotated datasets with known temporal segments for each\nstep [9, 23, 26, 44, 47, 54, 65, 71], using weakly-annotated\ntraining sets where the task and the order in which the steps\nappear in the video are known [6, 7, 10, 14, 17, 25, 41, 70],\nonly the task and potential steps are known [40], or only\nloose association between video and instructional articles\nis given [12, 15]. Video narrations are a commonly used\nsource of weak supervision [1, 17, 34, 43], while instruc-\ntional steps from knowledge bases have recently been used\nas supervision: [12, 29]. Chen et al.\n[12] use video-level\ninstructional step labels for (weak) supervision of a model\nthat grounds instructional articles to videos. This approach\nattempts to localize steps without using any narration infor-\nmation; we instead show that the task knowledge is not nec-\nessary and heavily exploit narrations via multi-task learning\nand complementary inference pathways [66]: we argue that\nnarrations provide a much richer source of supervision for\ntraining step grounding models, while essentially coming\nfor free.\nVideo-Text alignment\nThe availability of large-scale\nvideo-text datasets such as HowTo100M has prompted\nmany works on joint video-language embedding train-\ning [29, 35]. A form of contrastive loss is often adopted\nfor bringing together the representations of the two modali-\nties [3, 32, 33, 35, 38, 39, 61, 63], while masked objectives\nare also gaining popularity [13, 20, 28, 33, 50, 51, 52, 55,\n69]. Some works perform end-to-end representation learn-\ning [35, 36], while others freeze representation and focus\non longer-term temporal modelling, which aims to capture\ncontext [61]. More recently Han et al. investigated directly\naligning contextualized narration representations to video\nframes [19].\nWe build our method off of this approach\n\u2013 we note however that our objective is complementary:\nrather than aligning a video\u2019s narrations as an end-goal, we\nuse this functionality to ground a set of independent steps\nsourced from instructional articles; in that process we show\nthat the synergy that develops while training jointly on the\ntwo tasks results in improved performance for both.\n3. Narration-Aided Step Grounding\nWe first present our architecture for joint narration and\nstep grounding (Sec. 3.2), followed by learning objectives\n(Sec. 3.3.1) and pseudo-labeling strategy (Sec. 3.3.3); we\ndiscuss inferring the video task in (Section 3.3.2).\n3.1. Problem Formulation\nLet (V, N) be a video-narration pair, consisting of T\nvideo frames and a sequence of N narrations. Also, let S\nbe an ordered list of S steps from an instructional article for\na candidate task \u03c4. Our objective is to ground each step of\nS to the video, conditioned on the other steps and the ASR\ntranscript2. In particular, the desired output of our model\nis an alignment matrix Y SV \u2208 {0, 1}S\u00d7T , where Yst = 1\nonly if frame t is depicting the s-th step of task \u03c4, and zero\notherwise. Note that some steps might not be represented in\nthe video.\n2ASR transcripts are assumed to be always available for training and\noptionally during inference.\n3.2. Joint Narration and Article Step Grounder\nAs shown in Figure 2, our proposed VINA model fol-\nlows the popular paradigm of leveraging Transformers for\nmodeling multimodal interactions [56, 62].\nUnimodal Encoders. Before feeding the video, narrations\nand article steps to our model, we preprocess them to ex-\ntract a sequence of tokens. Given a video-narration pair (V,\nN) we extract visual features, Xv \u2208 RT \u00d7Dv and narration\nfeatures Xn \u2208 RN\u00d7Dn using standard backbone networks\n(e.g., a frozen S3D [60] network for visual features, and\npooled Word2Vec [37] embeddings for narration features).\nSimilarly, we encode the sequence of steps in a sequence\nof features Xs \u2208 RS\u00d7Ds. The features of each modal-\nity m are embedded into a common embedding space of\ndimensionality D using a Unimodal Encoder that consists\nof a modality-specific MLP network, and then learnable,\nmodality-specific positional embeddings P m are added to\nthem:\nHm = MLP(Xm; \u03b8m) + P m,\n(1)\nwhere m \u2208 {v, n, s} denotes the modality.\nMultimodal Encoder. The outputs of the Unimodal En-\ncoders are concatenated into a sequence of tokens: H =\n[Hv; Hn; Hs] \u2208 R(T +N+S)\u00d7D and fed to the Multimodal\nEncoder, which is a standard Transformer with multiple lay-\ners of multi-head self-attention:\nZ = Transformer(H) \u2208 R(T +N+S)\u00d7D.\n(2)\nThe contextualized embeddings Z = [Zv; Zn; Zs] com-\nputed by the Multimodal Encoder capture interactions\nwithin each modality (e.g., temporal relationships within\nthe video and context among steps of an article) and\nacross modalities.\nWe can then compute cosine similar-\nity matrices between all pairs of modalities: narrations-\nto-video ANV \u2208 RN\u00d7T , steps-to-video ASV \u2208 RS\u00d7T ,\nand steps-to-narrations ASN\n\u2208 RS\u00d7N.\nFor example,\nthe narrations-to-video similarity matrix ANV is obtained\nby simply computing the cosine similarity between each\nframe embedding and each narration embedding: ANV\nnt\n=\nzn\nn\n\u22a4zv\nt / (||zn\nn|| ||zv\nt ||).\nNarration-aided Step Grounding. A straightforward in-\nference path for temporally grounding the steps in the video\nis directly through the ASV similarity matrix, which cap-\ntures the similarity of each video frame with each instruc-\ntional step. However, this alignment does not explicitly take\ninto account the narrations of the video (only implicitly,\nthrough the Multimodal Transformer). We observe that an\nalternative way to ground steps in a video is to first iden-\ntify narrations in the ASR transcript that are relevant to the\nstep and then exploit the similarity of those narrations with\nvideo frames to get a loose prior over the step location.\n Instruction steps\nMultimodal Transformer\n ASR narrations\n\u201cSo let\u2019s get these chicken \npieces in there.\u201d\n\u201cThanks for watching\u201d\nASNV\nVINA\nVideo encoder\nASR encoder\n1. Mix the milk and \u2026 \n6. Place cooked chicken \u2026\nSteps-Video \nAlignment\nLSV\n2. Mix the dry ingredients \u2026\nTeacher VINA\nSteps encoder\n+\n+\n+\nVideo frames\nPositional \nEncodings\nPS\nZS\nContextual \nEmbeddings\nUnimodal \nEncoders\nLNV\nASR timestamps\nNarrations-Video \n\u0391lignment\nNVSA\nSteps-Narrations \n\u0391lignment\nSteps-Narrations- \nVideo \n\u0391lignment\nASV\nANV\nASN\n: Steps-to-Narrations Alignment\nASN\nVideo frames\n: Steps-to-Video Alignment\nASV\n6. Place cooked chicken on a platter and serve\n1. Mix the milk and seasoning in a large bowl\n3. Heat the oil to a medium heat\n4. Cook the chicken pieces until golden brown \n5. Place the cooked chicken in a baking pan \n2. Mix the dry [\u2026] the chicken till lightly \nSo we\u2019re gonna start here with two cups of buttermilk\nAnd then we\u2019ll get on to the marinating \nI\u2019m using my 13-inch jumbo wok\nYou want to make sure that we\u2019re coating all those chicken pieces\n6. Place cooked chicken on a platter and serve\n1. Mix the milk and seasoning in a large bowl\n3. Heat the oil to a medium heat\n4. Cook the chicken pieces until golden brown \n5. Place the cooked chicken in a baking pan \n2. Mix the dry [\u2026] the chicken till lightly \nFigure 2: (left) Schematic illustration of our system. First, it extracts representations for each modality (video, ASR narra-\ntions, task steps) with three Unimodal Encoders. These representations are fed to a Transformer-based Multimodal Encoder\nfor capturing interactions among video frames as well as among the textual modalities and the video. The contextualized\nembeddings for video frames, narrations and steps are used to compute correspondences between all pairs of modalities.\nStep grounding is achieved by fusing the output of two pathways: a direct pathway aligning steps to video (ASV ) and an\nindirect pathway that composes steps-to-narration (ASN) with narration-to-video (ANV ) alignments to produce a second\nstep-to-video alignment (ASNV ). We train our model using a teacher-student strategy with iteratively refined and filtered\nstep pseudo-labels. (right) Qualitative examples of our learned steps-to-video alignment and steps-to-narrations alignment\nmatrices for a video snippet.\nThis is computed by combining the information captured\nin the steps-to-narrations and narrations-to-video alignment\nmatrices ASN and ANV :\nASNV = \u02dcASNANV \u2208 RS\u00d7T ,\n(3)\nwhere \u02dcASN is the predicted steps-to-narrations alignment\nmatrix ASN after being normalized with a softmax function\nwith temperature \u03be: \u02dcASN\nsn =\nexp (Asn/\u03be)\nPN\nj=1 exp(Asj/\u03be).\nThe resulting ASV and ASNV alignment matrices pro-\nvide two complementary inference paths to align steps\nto video frames. The mutual agreement between the di-\nrect steps-to-video alignment provided by ASV and indi-\nrect, narration-based steps-to-video alignment provided by\nASNV can be used to better ground steps. Intuitively, if a\nframe is both very similar to a step in the joint embedding\nspace learned by the Multimodal Transformer, and also very\nsimilar to a narration that is relevant to the step, then it is\nmore likely to be indeed relevant to the step. Hence, we\nfuse the ASV and ASNV alignment matrices to a matrix\nAF = (ASV + ASNV )/2.\n3.3. Weakly-Supervised Training from Narrated In-\nstructional Videos\nNext, we discuss how to supervise the VINA model in\norder to learn steps-to-video alignment and narrations-to-\nvideo alignment. We first present the training objective as-\nsuming that the ground-truth temporal segments for each\nnarration and step in the video are given. Then we describe\nour approach for obtaining automatic pseudo-labels for the\ntemporal segments.\n3.3.1\nLearning on Labeled Data\nLet B = {Vi, Ni, Si, Y NV\ni\n, Y SV\ni\n}B\ni=1 denote a set of train-\ning tuples, each comprising a video-narration pair, an or-\ndered list of relevant task steps, and the target video-\nnarrations and video-steps alignment matrices, we train the\nVINA model by optimising the following objective:\nL = 1\nB\nh B\nX\ni=1\n\u03bbNV H(Y NV\ni\n, ANV\ni\n) + \u03bbSV H(Y SV\ni\n, ASV\ni\n)\ni\n,\n(4)\nwhere H(\u00b7, \u00b7) is the modified InfoNCE loss used by [19] for\naligning video with text using noisy ground-truth temporal\nsegments:\nH(Y, A) = \u2212 1\nK\nK\nX\nk=1\nlog\nP\nt Yt,k exp (At,k/\u03b7)\nP\nt exp (At,k/\u03b7)\n,\n(5)\nwhere \u03b7 is a temperature constant. Note that although we\ndo not explicitly supervise the steps-narrations alignment\nASN, meaningful alignments emerge during training due to\nthe joint grounding of narrations and steps to the same video\nsamples, as seen in Figure 2. Note that although we do\nnot directly supervise the steps-to-narrations alignment, our\nmodel is able to learn meaningful correspondences, which\ngo beyond simple pairwise textual matching.\n3.3.2\nPairing Videos with Articles\nWe assume access to a set of instructional articles A =\n{Sj, \u03c4j}W\nj=1, where \u03c4j denotes the article title and Sj the\nassociated set of steps. To assign a set of steps to a given\nvideo from our training set B we need to associate it with an\narticle from A. If our video dataset provides metadata (e.g.,\na task id for every video), then this can be used to obtain the\nassociation \u2013 although there is no guarantee that this will re-\nsult in the best article-match for the video (see discussion in\nsupplementary materials for more analysis). If such meta-\ndata is not available, we can predict a task id, using the sim-\nilarity between the narration and the titles of the available\narticles. To that end we use an off-the-shelf language model\n(e.g. MPNet [49]) to compute semantic embeddings of the\nASR captions of every video and the title of each article\n\u03c4j \u2208 W. For every video Vi we then calculate the seman-\ntic similarity between all the N captions in Ni and all task\ntitles \u03c4j \u2208 W, and assign N votes; the vote of every cap-\ntion goes to the task that best matches it. Finally the video\nis assigned the task with the most votes. Alternatively, in\norder to obtain multiple sets of steps for a video, we rank\nthe tasks by the number of votes.\n3.3.3\nNarration-Aided Pseudo-Labeling\nOnce a task \u03c4j has been associated with a video, we have\naccess to a list of steps Sj from the article of the task. How-\never, whether these steps appear in the video and their tem-\nporal location remain unknown. Inspired by self-labeling\napproaches from the SSL literature [27, 48, 59], we fol-\nlow a teacher-student approach where a teacher version of\nour models generates pseudo-labeled temporal segments for\ntraining the student. For every step represented by a row\nin the learned steps-to-video alignment matrix we obtain a\npseudo-ground truth segment by finding the maximal acti-\nvation (peak) and expanding a temporal segment on both\nsides until the activation falls below an adaptive threshold\n\u03b6 (e.g., 70% of the peak). To avoid training with unreliable\npseudo-labels, we filter out pseudo-labels with low confi-\ndence: if the peak activation is below a fixed threshold \u03b3,\nthe alignment of that step is treated as unreliable for pseudo-\nlabeling, and is altogether ignored.\nTraining curriculum. For the first Eb epochs we perform\nburn-in training of the student model on fixed pseudo-labels\ngenerated by feeding the video and the list of steps Sj to\nTAN [19], an off-the-shelf model pre-trained on the task\nof video-text alignment. Afterwards, we switch to using\npseudo-labels generated from the teacher, where the teacher\nis initialized by duplicating the burn-in student model and\nthen updated every \u03bd epochs. During both stages, we utilize\nthe original ASR timestamps for supervising the video-to-\nnarrations alignment.\n4. Experiments\n4.1. Datasets and Metrics\nWe train our models on narrated videos from the\nHowTo100M dataset by leveraging the dataset release of\nwikiHow instructional articles [22], without using any form\nof manual annotations. In order to evaluate the effective-\nness of our method, we evaluate: step grounding on HT-\nStep (a new benchmark, described below), narration align-\nment on HTM-Align [19], and zero-shot step localization\non CrossTask [70].\nHowTo100M (Training). The HowTo100M dataset [36]\ncontains instructional videos from YouTube.\nFollowing\nHan et al. [19], we use the Food & Entertainment subset\ncontaining approximately 370K videos, where each video\nis complemented by the \u201csentencified\u201d ASR transcription of\nits audio narration.\nwikiHow (Training). We train using 14,541 cooking tasks\nfrom the wikiHow-Dataset [22]. For each task, we generate\nan ordered list of steps by extracting the step headlines.\nCrossTask (Evaluation). We use this established instruc-\ntional video benchmark for zero-shot grounding, i.e., by\ndirectly evaluating on CrossTask our model learned from\nHowTo100M. Following common practices, we use two\nevaluation protocols: the first one \u2013 step localization \u2013 aims\nat predicting a single timestamp for each occurring step in\nvideos from 18 primary tasks [70]. Performance is eval-\nuated by computing the recall (denoted as Avg. R@1) of\nthe most confident prediction for each task and averaging\nthe results over all query steps in a video, where R@1\nmeasures whether the predicted timestamp for a step falls\nwithin the ground truth boundaries. We report average re-\nsults over 20 random sets of 1850 videos [70]. The sec-\nond task \u2013 article grounding \u2013 requires predicting temporal\nsegments for each step of an instructional article describing\nthe task represented in the video. We use the mapping be-\ntween CrossTask and simplified wikiHow article steps pro-\nvided in Chen et al. [12] and report results on 2407 videos\nof 15 primary tasks obtained excluding three primary tasks\nfollowing the protocol of [12] (see supplementary materi-\nals for details). Performance for this task is measured with\nRecall@K at different IoU thresholds [12].\nHT-Step (Evaluation).\nTo evaluate the effectiveness of\nour model in grounding steps, we introduce an evaluation\nbenchmark consisting of 1200 HowTo100M videos span-\nning a total of 177 unique tasks, with each video manu-\nally annotated with temporal segments for each occurring\nstep. For each video, annotators were provided with the task\nname (e.g., Make Pumpkin Puree) and the recipe steps from\nthe corresponding wikiHow article. We refer the reader to\nsupplementary materials for details about the data annota-\ntion. We split the annotated videos into a validation and a\ntest set, each containing 600 videos, with 5 videos per task.\nWe ensure that our validation set does not contain videos\nfrom HTM-Align.\nHTM-Align (Evaluation). This benchmark is used to eval-\nuate our model on narration grounding.\nIt contains 80\nvideos where the ASR transcriptions have been manually\naligned temporally with the video. We report the R@1 met-\nric [19], which evaluates whether the model can correctly\nlocalize the narrations that are alignable with the video.\n4.2. Implementation Details\nAs video encoder we adopt the S3D [60] backbone pre-\ntrained with the MIL-NCE objective on HowTo100M [35].\nFollowing previous work [19, 61], we keep this module\nfrozen and use it to extract clip-level features (one fea-\nture per second for video decoded at 16 fps). For extract-\ning context-aware features for each sentence (step or nar-\nration), we follow the Bag-of-word (BoW) approach based\non Word2Vec embeddings [37]. Our methods hyperparam-\neters were selected on the HT-Step validation set and are:\n\u03bbSV = \u03bbNV = 1, temperatures \u03b7, \u03be = 0.07, and thresh-\nold \u03b3 = 0.65. We train our model for 12 epochs, with\n3 epochs burn-in and then we update the teacher every 3\nepochs. Pseudo-labels are obtained based on the steps-to-\nvideo alignment matrix. To obtain temporal segment detec-\ntions from the step-to-video alignment output of our model\n(e.g. for evaluating on the CrossTask article grounding set-\nting) we use a simple 1D blob detector [57]. Unless other-\nwise specified, we use the fused alignment matrix for step\ngrounding when narrations are available during inference\ntime. More details are included in supplementary materials.\n4.3. Results\n4.3.1\nComparison with the State of the Art\nWeakly-Supervised Narration and Step Grounding. Ta-\nble 1 compares the step and narration grounding perfor-\nmance of our method with recent state-of-the-art video-text\nalignment methods trained on HowTo100M using ASR nar-\nrations: MIL-NCE [35] and TAN [19]. When using them\nfor narration alignment, we feed them with ASR as input.\nBut we also evaluate them as strong baselines for zero-\nshot step grounding by feeding them with the sequence of\nsteps as input. Our model achieves 66.5% R@1 on nar-\nration alignment on HTM-Align, leading to an absolute\nMethod\nTrain. Inp.\nHT-Step \u2191R@1\nHTM-Align \u2191R@1\nw/o nar.\nw/ nar.\nCLIP (ViT-B/32) [39]\n-\n-\n-\n23.4\nMIL-NCE [35]\nN\n30.7\n-\n34.2\nTAN (Joint+Dual, S2) [19]\nN\n-\n-\n49.4\nTAN* (Joint, S1, LC) [19]\nN\n31.2\n-\n47.1\nTAN* (Joint, S1, PE+LC) [19]\nN\n7.9\n-\n63.0\nOurs\nN+S\n35.6 \u00b1 0.4 37.4 \u00b1 0.4\n66.5 \u00b1 0.9\nTable 1: Comparison with state-of-the-art methods for\nstep and narration grounding. We report results on the\nHT-Step and HTM-Align test sets, respectively.\nTAN*\nrefers to our improved baselines of [19]. S1 and S2 re-\nfer to the training stages followed in [19]. PE denotes the\naddition of positional encoding to the output of the narra-\ntion encoder. LC denotes long context, i.e., our improved\nTAN* baseline using 1024 seconds of context as opposed\nto 64 for TAN. Previous best results are shown underlined.\nOur VINA results are reported after 5 random runs. VINA\nclearly outperforms all previous work \u2013 as well as our im-\nproved TAN baselines \u2013 by large margins on both narration\nalignment and step grounding.\nimprovement of 17.1% over the previously reported state-\nof-the-art (49.4%). Notably, on HTM-Align our method\nsurpasses TAN* (Joint, S1, PE, LC) which is a new ver-\nsion of TAN [19] implemented by us and much stronger in\nvideo-narration alignment. Our re-implementation uses po-\nsitional encodings for ASR narrations, is trained on long-\nform videos (up to 17 minutes) only with original ASR\ntimestamps, while TAN was trained on 1 min video-clips\nwith refined narration timestamps and used a fusion of\ntwo models during inference (Joint+Dual).\nOur method\nalso outperforms all baselines for step grounding on HT-\nStep even when seeing only steps during inference, while\nbeing trained with (video, narrations, steps) triplets.\nIt\nalso outperforms TAN* (Joint, S1, LC), which is a second\nre-implementation of TAN designed for maximum perfor-\nmance on the task of step grounding. Additionally, VINA\nis able to use ASR transcripts of videos during test time, if\navailable, to further boost the performance.\nStep localization on CrossTask.\nIn Table 2 we compare\nour model against the state-of-the art in step localization on\nthe CrossTask benchmark. Our approach sets a new state-\nof-the-art for zero-shot step localization on this challenging\nbenchmark. Importantly, most approaches are evaluated on\nthis dataset by feeding their predicted steps-to-frames align-\nment matrix to a dynamic programming algorithm which\nfinds the optimal assignment of each step with exactly one\nshort clip assuming a canonical, fixed ordering of steps for\neach task. In contrast, our method, which is naturally aware\nof context and ordering by densely grounding steps, can\nMethod\n\u2191Avg. R@1 (%)\nSupervised\nTempCLR [64]\n52.5\nZero-Shot\nHT100M [36]\n33.6\nVideoCLIP [61]\n33.9\nMCN [11]\n35.1\nDWSA [45]\n35.3\nMIL-NCE [35]\n40.5\nZhukov [70]\n40.5\nVT-TWINS* [21]\n40.7\nUniVL [31]\n42.0\nOurs w/o nar.\n44.1\nOurs w/ nar.\n44.8\nTable 2:\nComparison with state-of-the-art methods\nfor zero-shot action step localization on the CrossTask\ndataset.\nThe performance of the state-of-the-art fully-\nsupervised method (TempCLR [64]) is reported as an upper-\nbound to the zero-shot approaches. * denotes results re-\nported on different test splits, and hence not directly com-\nparable with the rest. Our model outperforms all previous\nworks by a clear margin (2.1% absolute improvement over\nthe previous best result on the standard split). When pro-\nviding narrations as additional inputs during inference (only\ntext, not the timings), we obtain a further 0.7% boost.\noutperform prior results without imposing any constraints\nduring inference.\nModel\n\u2191R@50(IOU)\n\u2191R@100(IOU)\n0.1\n0.3\n0.5\n0.1\n0.3\n0.5\nMIL-NCE-max [35]\n33.5\n12.0\n4.9\n39.7\n14.3\n5.9\nMIL-NCE-avg [35]\n42.9\n24.3\n12.9\n56.8\n32.1\n17.0\nWSAG [12]\n40.1\n23.1\n10.1\n54.3\n31.3\n14.0\nOurs\n87.1\n59.0\n30.0\n90.6\n61.1\n30.9\nTable 3: Comparison with state-of-the-art approaches\nfor article grounding on the CrossTask dataset.\nArticle grounding on CrossTask.\nVINA is robust to the\ntype of language in which task steps are described. It can\nhandle both atomic phrases (as demonstrated by our re-\nsults on step localization on CrossTask), but also rich, nat-\nural language step descriptions, as evidenced by perfor-\nmance on HT-Step. To further demonstrate this, we com-\npare against the state-of-the art on the article grounding task\nof CrossTask in Table 3. Our model outperforms all previ-\nous works by a large margin. We emphasize the perfor-\nmance improvement we obtain compared to WSAG, which\nhighlights the importance of exploiting the narration infor-\nmation for training.\n4.3.2\nAblation Studies\nWe perform ablations to assess the impact of the various\ndesign choices in our method by measuring step ground-\ning performance on the HT-Step validation set and video-\nnarration alignment performance on HTM-Align.\nMethod\nTrain. Inp. Iter. Pseudo. HT-Step \u2191R@1 HTM-Align\nw/o nar. w/ nar.\nBaseline/Initial Step Pseudo-labels\n(1) TAN Joint S1\nN\n30.7\n-\n47.1\nSingle-Task Training\n(2) Ours\nN\n-\n-\n63.2\n(3) Ours\nS\n34.0\n-\n-\n(4) Ours\nS\n\u2713\n35.8\n-\n-\nMulti-Task Training\n(5) Ours\nN+S\n34.3\n36.1\n64.8\n(6) Ours\nN+S\n\u2713\n36.9\n39.1\n67.0\nTable 4: Ablation of main components of our frame-\nwork. We study the contribution of (a) multi-task train-\ning for narration and step grounding, (b) iterative step\npseudo-labeling (Iter. Pseudo), and (c) narration-aware step\ngrounding (w/ nar.). We report results on the HT-Step val\nset for STG and HTM-Align for NG. We compare train-\ning only with narrations (N), only with wikiHow steps (S),\nand training with narrations-steps sequence pairs (N+S). We\nalso compare the performance with and without providing\nnarrations during inference.\nEffect of weak supervision from instructional articles.\nRow 3 in Table 4 shows the step grounding results obtained\nfrom an instance of our model that includes only the di-\nrect video-step alignment pathway and that is trained just\non wikiHow steps (without narrations) using the fixed step\npseudo-labels from TAN* [19] without any form of iterative\npseudo-labeling (row 1). Remarkably, this variant improves\nby 3.3% over the step-grounding performance of TAN*.\nWhen we let this variant update the step pseudo-labels (row\n4), the recall improves further (5.1% over TAN*). These\nresults provide evidence of the strong benefits of utilizing\ninstructional articles for the learning of step grounding.\nEffect of multimodal training and inference.\nTraining\nour model with multi-modal textual inputs (steps and nar-\nrations), we observe an improvement of 1.6% in narration\ngrounding (row 5 of Table 4) compared to its single-task\ncounterpart (row 2). However the gain in step grounding is\nmarginal when seeing only video frames during inference\n(w/o nar., 34% in row 3 vs 34.3% in row 5). Our conjecture\nis that the missing modality (narrations) leads to some drop\nin performance. Providing both steps and narrations during\ninference leads to a stronger step grounding performance,\nwhich surpasses the TAN* baseline by 5.4% (30.7 \u2192 36.1).\nEffect of iterative pseudo-labeling.\nBy comparing row 5\nto row 6 of Table 4 we observe a clear boost in performance\non both step grounding and narration alignment. This is a\nclear indication of the gains produced by iteratively refin-\ning the pseudo-labels using our model as a teacher during\ntraining.\nAlignment\nS \u2192 V\nS \u2192 N\nN \u2192 V\nHT-Step \u2191R@1\nS \u2192 V\nlearned\n-\n-\n34.3\nS \u2192 N \u2192 V\n-\nlearned\nlearned\n30.5\nS \u2192 N \u2192 V\n-\nlearned\nASR\n27.9\nS \u2192 N \u2192 V\n-\nMPNet [49]\nASR\n19.0\nFused\nlearned\nlearned\nlearned\n36.1\nTable 5: Impact of the alignment matrix used during in-\nference with narrations. The same model is used for all\nresults (corresponding to row 5 in Table 4).\nImpact of pathways during inference.\nIn Table 5 we\nstudy the effects of using different pathways and align-\nment information during inference. All results are produced\nfrom the same model trained for joint narration and step\ngrounding with fixed pseudo-labels from TAN (row 5 in Ta-\nble 4). Grounding steps using the indirect steps-to-video\nalignment only lags by 3.8% behind the direct steps-to-\nvideo alignment that directly computes the similarity be-\ntween steps and video frames (30.5% vs 34.3%). Their fu-\nsion outperforms their individual grounding performance.\nThis suggests that they capture complementary information.\nWe also explore substituting our learned steps-to-narrations\nalignment with an alignment computed with an off-the-shelf\nlanguage model. This significantly degrades performance\n(19.0%) showing that our joint steps and narrations ground-\ning model learns relationships between steps and narra-\ntions that go beyond textual similarity between pairs of sen-\ntences.\nSimilarly, substituting our learned narrations-to-\nvideo alignment with an alignment based on the original\nASR timestamps reduces performance by 2.6%.\nIterative pseudo-labeling strategies.\nIn Table 6 we ab-\nlate design choices for the iterative pseudo-labeling stage.\nWe can observe that using aggressive filtering (i.e., high\nthresholds translating to a high maximum percentage of\npseudo-labels that are discarded) is key to observing gains\nfrom iterative pseudo-labeling (using either the S \u2192 V or\nFused alignment matrices) compared to training with fixed\npseudo-labels from TAN. Intuitively, a large percentage of\nsteps described in wikiHow articles are not represented in\nthe given instructional video due to task mismatch, varia-\ntions in recipe execution, and some steps being optional.\nTherefore, starting with a small subset of reliable pseudo-\nlabels can facilitate step grounding.\nTask selection.\nIn Table 7 we investigate different strate-\ngies to select the wikiHow articles during training. This\nselection determines the set of steps to be grounded. We\nAlignment\n\u03b3\nmax % step HT-Step \u2191R@1\ndiscarded\nw/o nar. w/ nar.\nS \u2192 V\n0.40\n24\n34.3\n36.5\nS \u2192 V\n0.65\n91\n36.9\n39.1\nFused\n0.40\n60\n34.1\n35.9\nFused\n0.55\n88\n36.2\n35.5\nTable 6: Ablation of the type of alignment matrix and\nfiltering threshold used for pseudo-label generation.\nPseudo-label generation with the steps-to-video alignment\nmatrix and the fusion of the direct and indirect pathways\nperform comparably for step grounding. Aggressive unreli-\nable pseudo-label filtering with high confidence thresholds\n\u03b3 (large maximum step discard ratio) helps in both cases.\nTask ID selection\nHT-Step \u2191R@1\nHT100M metadata\n34.3\nTop-1 prediction\n34.7\nRandom / top-5 pred\n34.3\nTable 7: Sensitivity to task id selection. We assess how the\nperformance of our method changes when using different\nstrategies to associate videos with articles. We experiment\nwith using the task ids available from HT100M, as well as\nthe the two predictive strategies presented in Section 3.3.2.\nWe conclude that our method is robust to the task selection,\nand the task labels are not necessary for training.\nevaluate two strategies for video-task association from nar-\nrations and compare them with using the task id provided in\nthe HowTo100M metadata for each video3. We see that our\nautomatic selection approaches yield results on par with or\neven slightly better than those based on metadata.\n5. Conclusion\nWe have presented a method for learning how to tem-\nporally ground sequences of steps in instructional videos,\nwithout any manual supervision.\nOur proposed method\nexploits the weak supervision naturally provided in such\nvideos through their narrations, and solves for joint align-\nment of narrations and steps, while fusing two comple-\nmentary pathways for step-to-video alignment. We demon-\nstrated strong quantitative performance, surpassing the\nstate-of-the-art on multiple benchmarks for both narration\nand step grounding.\nAcknowledgements. We thank Huiyu Wang, Yale Song,\nMandy Toh, and Tengda Han for helpful discussions.\n3During inference, metadata task ids are used in all of our HowTo100M\nexperiments in order to evaluate against the ground-truth step annotations.\nReferences\n[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant\nAgrawal,\nJosef Sivic,\nIvan Laptev,\nand Simon\nLacoste-Julien. Unsupervised learning from narrated\ninstruction videos. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition,\npages 4575\u20134583, 2016. 3\n[2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,\nJosef Sivic, Trevor Darrell, and Bryan Russell. Lo-\ncalizing moments in video with natural language. In\nProceedings of the IEEE international conference on\ncomputer vision, pages 5803\u20135812, 2017. 2\n[3] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew\nZisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion, pages 1728\u20131738, 2021. 3\n[4] Peijun Bao, Qian Zheng, and Yadong Mu.\nDense\nevents grounding in video. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 35,\npages 920\u2013928, 2021. 2\n[5] Jing Bi, Jiebo Luo, and Chenliang Xu.\nProcedure\nplanning in instructional videos via contextual model-\ning and model-based policy learning. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 15611\u201315620, 2021. 2\n[6] Piotr Bojanowski, R\u00b4emi Lajugie, Francis Bach, Ivan\nLaptev, Jean Ponce, Cordelia Schmid, and Josef Sivic.\nWeakly supervised action labeling in videos under\nordering constraints.\nIn Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part V 13,\npages 628\u2013643. Springer, 2014. 2\n[7] Piotr Bojanowski, R\u00b4emi Lajugie, Edouard Grave,\nFrancis Bach, Ivan Laptev, Jean Ponce, and Cordelia\nSchmid. Weakly-supervised alignment of video with\ntext. In Proceedings of the IEEE international con-\nference on computer vision, pages 4462\u20134470, 2015.\n2\n[8] Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang,\nand Juan Carlos Niebles. Few-shot video classifica-\ntion via temporal alignment. In 2020 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 10615\u201310624, 2020. 2\n[9] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang,\nJue Wang, and Yuexian Zou. Locvtp: Video-text pre-\ntraining for temporal localization. In European Con-\nference on Computer Vision, 2022. 2\n[10] C. Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and\nJuan Carlos Niebles. D3tw: Discriminative differen-\ntiable dynamic time warping for weakly supervised\naction alignment and segmentation. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 3541\u20133550, 2019. 2\n[11] Brian Chen, Andrew Rouditchenko, Kevin Duarte,\nHilde Kuehne, Samuel Thomas, Angie Boggust,\nRameswar Panda, Brian Kingsbury, Rogerio Feris,\nDavid Harwath, et al. Multimodal clustering networks\nfor self-supervised learning from unlabeled videos. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 8012\u20138021, 2021. 7\n[12] Long Chen, Yulei Niu, Brian Chen, Xudong Lin,\nGuangxing Han,\nChristopher Thomas,\nHammad\nAyyubi, Heng Ji, and Shih-Fu Chang.\nWeakly-\nsupervised temporal article grounding. In Conference\non Empirical Methods in Natural Language Process-\ning, pages 9402\u20139413, Abu Dhabi, United Arab Emi-\nrates, Dec. 2022. Association for Computational Lin-\nguistics. 2, 3, 5, 7, 14\n[13] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representa-\ntion learning. In ECCV, 2020. 3\n[14] Nikita Dvornik, Isma Hadji, Konstantinos G. Derpa-\nnis, Animesh Garg, and Allan D. Jepson. Drop-dtw:\nAligning common signal between sequences while\ndropping outliers. ArXiv, abs/2108.11996, 2021. 1,\n2\n[15] Nikita Dvornik, Isma Hadji, Hai Pham, Dhaivat Bhatt,\nBrais Martinez, Afsaneh Fazly, and Allan D Jepson.\nFlow graph to video grounding for weakly-supervised\nmulti-step localization. In European Conference on\nComputer Vision, pages 319\u2013335. Springer, 2022. 3\n[16] Ehsan Elhamifar and Dat Huynh.\nSelf-supervised\nmulti-task\nprocedure\nlearning\nfrom\ninstructional\nvideos. In Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part XVII 16, pages 557\u2013573. Springer,\n2020. 2\n[17] Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom,\nChris Dyer, Stephen Clark, and Aida Nematzadeh.\nLearning to segment actions from observation and nar-\nration. In Annual Meeting of the Association for Com-\nputational Linguistics, pages 2569\u20132588, Online, July\n2020. Association for Computational Linguistics. 2, 3\n[18] Simon Ging, Mohammadreza Zolfaghari, Hamed Pir-\nsiavash, and Thomas Brox. Coot: Cooperative hierar-\nchical transformer for video-text representation learn-\ning. Advances in neural information processing sys-\ntems, 33:22605\u201322618, 2020. 2\n[19] Tengda Han, Weidi Xie, and Andrew Zisserman. Tem-\nporal alignment networks for long-term video.\nIn\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 2906\u20132916, June 2022. 1, 2, 3,\n4, 5, 6, 7, 13, 14, 15\n[20] Haoyang Huang, Yaobo Liang, Nan Duan, Ming\nGong, Linjun Shou, Daxin Jiang, and Ming Zhou.\nUnicoder:\nA universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2485\u20132494, 2019.\n3\n[21] Dohwan Ko, Joonmyung Choi, Juyeon Ko, Shinyeong\nNoh, Kyoung-Woon On, Eun-Sol Kim, and Hyun-\nwoo J Kim. Video-text representation learning via dif-\nferentiable weak temporal alignment. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5016\u20135025, 2022. 2, 7\n[22] Mahnaz Koupaee and William Yang Wang.\nWiki-\nhow: A large scale text summarization dataset. ArXiv,\nabs/1810.09305, 2018. 5, 13, 14\n[23] Hilde Kuehne, Ali Arslan, and Thomas Serre. The lan-\nguage of actions: Recovering the syntax and seman-\ntics of goal-directed human activities. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 780\u2013787, 2014. 2\n[24] Hilde Kuehne, Juergen Gall, and Thomas Serre. An\nend-to-end generative framework for video segmen-\ntation and recognition. In Proc. IEEE Winter Appli-\ncations of Computer Vision Conference (WACV 16),\nLake Placid, Mar 2016. 2\n[25] Hilde Kuehne, Alexander Richard, and Juergen Gall.\nWeakly supervised learning of actions from tran-\nscripts. Computer Vision and Image Understanding,\n163:78\u201389, 2017. 2\n[26] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D\nHager. Temporal convolutional networks: A unified\napproach to action segmentation. In Computer Vision\u2013\nECCV 2016 Workshops:\nAmsterdam, The Nether-\nlands, October 8-10 and 15-16, 2016, Proceedings,\nPart III 14, pages 47\u201354. Springer, 2016. 2\n[27] Dong-Hyun Lee et al. Pseudo-label: The simple and\nefficient semi-supervised learning method for deep\nneural networks. In Workshop on challenges in repre-\nsentation learning, ICML, volume 3, page 896, 2013.\n5\n[28] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan,\nLicheng Yu, and Jingjing Liu. HERO: Hierarchical en-\ncoder for Video+Language omni-representation pre-\ntraining. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2046\u20132065, Online, Nov. 2020. As-\nsociation for Computational Linguistics. 3\n[29] Xudong Lin, Fabio Petroni, Gedas Bertasius, Mar-\ncus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani.\nLearning to recognize procedural activities with dis-\ntant supervision.\narXiv preprint arXiv:2201.10990,\n2022. 2, 3\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight\ndecay regularization. In International Conference on\nLearning Representations, 2019. 14\n[31] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang,\nNan Duan, Tianrui Li, Jason Li, Taroon Bharti, and\nMing Zhou. Univl: A unified video and language pre-\ntraining model for multimodal understanding and gen-\neration. arXiv preprint arXiv:2002.06353, 2020. 2, 7\n[32] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji\nZhang, and Rongrong Ji. X-CLIP:: End-to-end multi-\ngrained contrastive learning for video-text retrieval.\narXiv preprint arXiv:2207.07285, 2022. 3\n[33] Yue Ma, Tianyu Yang, Yin Shan, and Xiu Li. Simvtp:\nSimple video text pre-training with masked autoen-\ncoders. arXiv preprint arXiv:2212.03490, 2022. 3\n[34] Jonathan Malmaud, Jonathan Huang, Vivek Rathod,\nNicholas Johnston, Andrew Rabinovich, and Kevin\nMurphy. What\u2019s cookin\u2019? interpreting cooking videos\nusing text, speech and vision. In Proceedings of the\n2015 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 143\u2013152, Denver,\nColorado, May\u2013June 2015. Association for Computa-\ntional Linguistics. 3\n[35] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira,\nIvan Laptev, Josef Sivic, and Andrew Zisserman. End-\nto-End Learning of Visual Representations from Un-\ncurated Instructional Videos. In CVPR, 2020. 3, 6, 7,\n14\n[36] Antoine\nMiech,\nDimitri\nZhukov,\nJean-Baptiste\nAlayrac, Ivan Laptev, and Josef Sivic. Howto100m:\nLearning a text-video embedding by watching hun-\ndred million narrated video clips. In Proceedings of\nthe IEEE International Conference on Computer Vi-\nsion (ICCV), 2019. 3, 5, 7, 13\n[37] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed representations\nof words and phrases and their compositionality. In\nC.J. Burges, L. Bottou, M. Welling, Z. Ghahramani,\nand K.Q. Weinberger, editors, Advances in Neural In-\nformation Processing Systems, volume 26. Curran As-\nsociates, Inc., 2013. 3, 6, 14\n[38] Yookoon Park, Mahmoud Azab, Seungwhan Moon,\nBo Xiong, Florian Metze, Gourab Kundu, and Kir-\nmani Ahmed.\nNormalized contrastive learning for\ntext-video retrieval. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 248\u2013260, Abu Dhabi, United Arab\nEmirates, Dec. 2022. Association for Computational\nLinguistics. 3\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pages 8748\u20138763. PMLR, 2021. 3,\n6\n[40] Alexander Richard, Hilde Kuehne, and Juergen Gall.\nAction sets: Weakly supervised action segmentation\nwithout ordering constraints. In Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, pages 5987\u20135996, 2018. 2\n[41] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and\nJuergen Gall.\nNeuralnetwork-viterbi: A framework\nfor weakly supervised video learning. In Proceedings\nof the IEEE conference on Computer Vision and Pat-\ntern Recognition, pages 7386\u20137395, 2018. 2\n[42] Andrew Rouditchenko, Angie Boggust, David Har-\nwath, Brian Chen, Dhiraj Joshi, Samuel Thomas,\nKartik Audhkhasi, Hilde Kuehne, Rameswar Panda,\nRogerio Feris, et al. Avlnet: Learning audio-visual\nlanguage representations from instructional videos.\narXiv preprint arXiv:2006.09199, 2020. 2\n[43] Ozan Sener, Amir R Zamir, Silvio Savarese, and\nAshutosh Saxena. Unsupervised semantic parsing of\nvideo collections. In Proceedings of the IEEE Inter-\nnational conference on Computer Vision, pages 4480\u2013\n4488, 2015. 3\n[44] Yuhan Shen and Ehsan Elhamifar.\nSemi-weakly-\nsupervised learning of complex actions from instruc-\ntional task videos. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, pages 3344\u20133354, 2022. 2\n[45] Yuhan Shen, Lu Wang, and Ehsan Elhamifar. Learn-\ning to segment actions from visual and language in-\nstructions via differentiable weak sequence alignment.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10156\u2013\n10165, 2021. 2, 7\n[46] Nina Shvetsova, Brian Chen, Andrew Rouditchenko,\nSamuel Thomas, Brian Kingsbury, Rogerio S Feris,\nDavid Harwath, James Glass, and Hilde Kuehne. Ev-\nerything at once-multi-modal fusion transformer for\nvideo retrieval. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 20020\u201320029, 2022. 2\n[47] Bharat Singh, Tim K Marks, Michael Jones, Oncel\nTuzel, and Ming Shao. A multi-stream bi-directional\nrecurrent neural network for fine-grained action de-\ntection.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1961\u2013\n1970, 2016. 2\n[48] Kihyuk Sohn, David Berthelot, Nicholas Carlini,\nZizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-\ngus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fix-\nmatch:\nSimplifying semi-supervised learning with\nconsistency and confidence. Advances in neural in-\nformation processing systems, 33:596\u2013608, 2020. 5\n[49] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu.\nMpnet: Masked and permuted pre-\ntraining for language understanding. arXiv preprint\narXiv:2004.09297, 2020. 5, 8\n[50] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai.\nVl-bert: Pre-training of\ngeneric visual-linguistic representations. In Interna-\ntional Conference on Learning Representations, 2020.\n3\n[51] Chen Sun, Austin Myers, Carl Vondrick, Kevin P.\nMurphy, and Cordelia Schmid.\nVideobert: A joint\nmodel for video and language representation learning.\n2019 IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 7463\u20137472, 2019. 2, 3\n[52] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing, 2019. 3\n[53] Yansong Tang, Dajun Ding, Yongming Rao, Yu\nZheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie\nZhou.\nCoin: A large-scale dataset for comprehen-\nsive instructional video analysis. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\n2019. 2\n[54] Yansong Tang, Jiwen Lu, and Jie Zhou. Comprehen-\nsive instructional video analysis: The coin dataset and\nperformance evaluation. IEEE transactions on pattern\nanalysis and machine intelligence, 43(9):3138\u20133153,\n2020. 2\n[55] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideoMAE: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training.\nIn\nAdvances in Neural Information Processing Systems,\n2022. 3\n[56] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. Multimodal transformer for unaligned\nmultimodal language sequences. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6558\u20136569, Florence, Italy,\nJuly 2019. Association for Computational Linguistics.\n3\n[57] Huiyu Wang, Mitesh Kumar Singh, and Lorenzo Tor-\nresani. Ego-only: Egocentric action detection without\nexocentric pretraining, 2023. 6, 15\n[58] Hanlin Wang, Yilu Wu, Sheng Guo, and Limin\nWang.\nPdpp:\nProjected diffusion for procedure\nplanning in instructional videos.\narXiv preprint\narXiv:2303.14676, 2023. 2\n[59] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing\nFei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and\nXinyi Le. Semi-supervised semantic segmentation us-\ning unreliable pseudo-labels. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4248\u20134257, 2022. 5\n[60] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen\nTu, and Kevin Murphy.\nRethinking spatiotemporal\nfeature learning: Speed-accuracy trade-offs in video\nclassification. In European Conference on Computer\nVision, 2018. 3, 6, 14\n[61] Hu Xu,\nGargi Ghosh,\nPo-Yao Huang,\nDmytro\nOkhonko, Armen Aghajanyan, Florian Metze, Luke\nZettlemoyer, and Christoph Feichtenhofer.\nVideo-\nCLIP: Contrastive pre-training for\nzero-shot video-text understanding. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), Online, Nov. 2021.\nAssociation for Computational Linguistics. 3, 6, 7, 14\n[62] Peng Xu, Xiatian Zhu, and David A. Clifton. Mul-\ntimodal learning with transformers: A survey, 2022.\n3\n[63] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco:\nToken-aware cascade contrastive learning for video-\ntext alignment. 2021 IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 11542\u2013\n11552, 2021. 3\n[64] Yuncong Yang, Jiawei Ma, Shiyuan Huang, Long\nChen, Xudong Lin, Guangxing Han, and Shih-Fu\nChang. Temporal alignment representation with con-\ntrastive learning.\narXiv preprint arXiv:2212.13738,\n2022. 2, 7\n[65] Fangqiu Yi, Hongyu Wen, and Tingting Jiang. As-\nformer: Transformer for action segmentation. In The\nBritish Machine Vision Conference (BMVC), 2021. 2\n[66] Amir R Zamir, Alexander Sax, Nikhil Cheerla, Rohan\nSuri, Zhangjie Cao, Jitendra Malik, and Leonidas J\nGuibas.\nRobust learning through cross-task consis-\ntency. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n11197\u201311206, 2020. 3\n[67] He Zhao, Isma Hadji, Nikita Dvornik, Konstantinos G\nDerpanis, Richard P Wildes, and Allan D Jepson.\nP3iv: Probabilistic procedure planning from instruc-\ntional videos with weak supervision. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2938\u20132948, 2022. 2\n[68] Yiwu Zhong, Licheng Yu, Yang Bai, Shangwen Li,\nXueting Yan, and Yin Li. Learning procedure-aware\nvideo representation from instructional videos and\ntheir narrations.\narXiv preprint arXiv:2303.17839,\n2023. 2\n[69] Linchao Zhu and Yi Yang. Actbert: Learning global-\nlocal video-text representations. pages 8743\u20138752, 06\n2020. 3\n[70] Dimitri\nZhukov,\nJean-Baptiste\nAlayrac,\nRa-\nmazan Gokberk Cinbis, David Fouhey, Ivan Laptev,\nand Josef Sivic. Cross-task weakly supervised learn-\ning from instructional videos.\nIn IEEE Conference\non Computer Vision and Pattern Recognition, pages\n3537\u20133545, 2019. 2, 5, 7, 13, 14\n[71] Andrea Ziani, Zicong Fan, Muhammed Kocabas,\nSammy Christen, and Otmar Hilliges.\nTempclr:\nReconstructing hands via time-coherent contrastive\nlearning. In International Conference on 3D Vision\n(3DV), 2022. 2\nThis Appendix provides: additional details (annotation\nprocedure, statistics) about the HT-Step dataset that we in-\ntroduced for evaluating models on step grounding (Sec-\ntion A), additional details for the rest of the datasets that\nwere used for training/evaluation (Section B), implementa-\ntion details (Section C), qualitative results for step ground-\ning on HT-Step (Section\nD), additional ablation studies\n(Section E), and additional details about the evaluation of\nour models on HTM-Align (Section F).\nA. HT-Step Dataset\nDataset\nStep Annot.\n# Videos\n# Activities # Steps # Segments\nHowTo100M [36]\n\u2717\n1.2M\n25k\n-\n-\nHTM-Align [19]\n\u2717\n80\n80\n-\n-\nCrossTask [70]\n\u2713\n4.8k (2.8k)\n83 (18)\n133\n20.9k\nHT-Step (val)\n\u2713\n600\n120\n1,204\n3,441\nHT-Step (test)\n\u2713\n600\n120\n1,242\n3,631\nwikiHow\n-\n14k\n100k\n-\nTable 8: Summary statistics for the datasets in our work.\nFor CrossTask, the statistics for primary activities only are\nshown in parentheses.\nIn this section we provide details about the creation\nof the HT-Step benchmark that we used for evaluating\nour models. This benchmark was designed to provide a\nhigh-quality set of step-annotated instructional videos for\na plethora of tasks, described in rich, structured language\ninstead of atomic phrases.\nAnnotation setup. We used videos from the HowTo100M\ndataset; each one of those videos contains a task id label that\ncorresponds to a wikiHow article. This association enabled\nus to obtain a set of potential step descriptions for every\nvideo, directly from the corresponding wikiHow article. We\nnote that this association is noisy, e.g. the video might show\na variation of a specific recipe, where some of the steps in\nthe article often do not appear at all, appear partially, are\nexecuted in different order, or are repeated multiple times.\nAnnotation instructions. For each video, annotators were\nprovided with the task name (e.g., Make Pumpkin Puree)\nand the recipe steps from the corresponding wikiHow arti-\ncle. The annotators where asked to watch the whole video\nand first decide whether it is relevant to the given task \u2013\ni.e. if at least some of the given steps were visually demon-\nstrated and the task\u2019s end goal was the same (e.g. a specific\nrecipe) \u2013 or reject it otherwise. When a video was deemed\nrelevant, annotators were asked to mark all instances of the\nprovided steps with a temporal window. We note that Wik-\niHow articles often contain several variations/methods for\ncompleting a given task. For tasks where this was the case,\nthe annotators were asked to select the set of steps corre-\nsponding to the variation that best fits every video and only\nuse those steps for annotating the entire video.\nQA process. To ensure the quality of the annotations, we\nfollowed a rigorous multi-stage Quality Assurance (QA)\nprocess: In the first stage, the videos were annotated by a\nsingle annotator. These initial annotations were then re-\nviewed by more experienced annotators, who either ap-\nproved all the annotations on a given video (meaning all\nthe marked steps were correct and no steps were missing)\nor marked it for redoing with specific comments on which\nannotations needed fixing and in what way. At the last stage\nof the QA process, the annotations that were marked as in-\ncorrect were redone by third, independent annotators.\nStatistics. We provide per-activity statistics for the annota-\ntions in Table 9. The metrics used, i.e. number of unique\nsteps, step and video coverage, are given to provide an un-\nderstanding of how the number of steps varies between dif-\nferent tasks and how the steps of a task may appear partially\nin the HowTo100M videos.\nValidation and test (val/test) split. Overall during the full\nannotation process, approximately 35% of the videos were\nrejected as irrelevant to the given tasks. We split the re-\nmaning, annotated videos into a validation and a test set,\neach containing 600 videos, with 5 videos per task. We en-\nsured that our validation set does not contain videos from\nHTM-Align. In total 87 human annotators manually anno-\ntated 1200 videos over 177 tasks: 120 in the validation and\n120 in the test set, with 5 videos per task, i.e. with 63 tasks\noverlapping between the two sets.\nB. Datasets Details\nWe provides a statistics summary for the datasets used\nfor training and evaluation in Table 8.\nHowTo100M (Training). HowTo100M contains over 1M\nunique instructional videos, spanning over 24k activities in-\ncluding cooking, DIY, arts and crafts, gardening, personal\ncare, fitness and more. Each instructional video is comple-\nmented by the ASR transcription of it\u2019s audio, which usu-\nally contains the real time narration/commentary of the in-\nstructor during the activity. We use the \u201dsenticified\u201d version\nof the ASR sentences provided by Han et al. [19]. Follow-\ning Han et al. [19] we also train only using the Food &\nEnteratainment subset, which includes a subset of approxi-\nmately 370k videos.\nwikiHow (Training). We train using 14,541 cooking tasks\nfrom the wikiHow-Dataset [22]. For each task, we gen-\nerate an ordered list of steps by extracting the step head-\nlines. The HowTo100M dataset was curated using a semi-\nautomatic pipeline that involved searching YouTube with\nqueries based on the titles of wikiHow articles.\nConse-\nquently there is an almost complete overlap in activities\nbetween the two corpora, which makes wikiHow a natu-\nral choice for mining step-level articles to associate with\nTask\n# steps step coverage video coverage\nMake Zucchini Pancakes\n4.0\n0.83\n0.37\nMake a Hearty Stew\n3.5\n0.82\n0.12\nMake Beef and Broccoli\n3.1\n0.78\n0.24\nMake Coconut Popsicles\n3.8\n0.76\n0.28\nMake Yorkshire Pudding\n5.3\n0.76\n0.11\nCook Spaghetti alla Carbonara\n4.6\n0.73\n0.39\nMake Vegan Pesto\n2.2\n0.73\n0.15\nMake Corn Fritters\n6.4\n0.72\n0.28\nMake Buttermilk Fried Chicken\n4.2\n0.70\n0.44\nMake a Shrimp Po Boy Sandwich\n4.2\n0.70\n0.27\n...\n...\n...\n...\n...\nCook Prime Rib\n2.6\n0.19\n0.19\nCure Bacon\n2.2\n0.18\n0.11\nMake Dim Sum\n4.6\n0.18\n0.15\nMake Vegan Ceviche\n2.8\n0.17\n0.08\nMake Lobster Bisque\n3.6\n0.17\n0.28\nMake Giblet Gravy\n2.8\n0.16\n0.23\nMake Pickled Eggs\n4.4\n0.16\n0.19\nPickle Onions\n1.6\n0.15\n0.12\nCook Rib Eye Roast\n2.0\n0.12\n0.28\nMake Pap\n2.0\n0.12\n0.21\nAverage\n4.0\n0.42\n0.24\nTable 9: Statistics of the annotations used to create the HT-\nStep benchmark. The metrics are computed per task (for\n177 tasks in total), averaged over all the annotated videos\nfor a given task. # steps denotes the average number of\nunique steps annotated per video, per activity; step cover-\nage denotes the fraction of a task\u2019s steps that have been\nfound and annotated in every video; video coverage de-\nnotes the fraction of the video\u2019s duration that is covered by\nstep annotations; Rows are sorted by step coverage; only\nthe 10 tasks with the highest and lowest step coverage are\nshown here for brevity.\ninstructions in HowTo100M videos. In the context of this\npaper we used the wikiHow-Dataset [22] to collect the arti-\ncles for 14,541 cooking tasks.\nCrossTask (Evaluation). We use this established instruc-\ntional video benchmark for zero-shot grounding, i.e., by\ndirectly evaluating on CrossTask our model learned from\nHowTo100M. The Crosstask dataset [70]. is an established\nbenchmark for temporal localization of steps in instruc-\ntional videos. It consists of 4800 videos from 83 activities,\nwhich are divided into 18 primary (14 related to cooking\nand 4 to DIY car repairs and shelf assembly) and 65 re-\nlated activities. The videos in the primary activities are an-\nnotated with step annotations in the form of temporal seg-\nments from a predefined taxonomy of 133 steps. Those\nsteps tend to be atomic, e.g.\nfor activity \u201cMake Taco\nSalad\u201d the available steps are \u201cadd onion\u201d, \u201cadd taco\u201d, \u201cadd\nlettuce\u201d, \u201cadd meat\u201d, \u201cadd tomato\u201d, \u201cadd cheese\u201d, \u201cstir\u201d,\nand \u201cadd tortilla\u201d. Following common practices, we use\ntwo evaluation protocols: the first one \u2013 step localization\n\u2013 aims at predicting a single timestamp for each occurring\nstep in videos from 18 primary tasks [70]. Performance is\nevaluated by computing the recall (denoted as Avg. R@1)\nof the most confident prediction for each task and averag-\ning the results over all query steps in a video, where R@1\nmeasures whether the predicted timestamp for a step falls\nwithin the ground truth boundaries. We report average re-\nsults over 20 random sets of 1850 videos [70]. The second\ntask \u2013 article grounding \u2013 requires predicting temporal seg-\nments for each step of an instructional article describing the\ntask represented in the video. We use the mapping between\nCrossTask and simplified wikiHow article steps provided in\nChen et al. [12] and report results on 2407 videos of 15\nprimary tasks obtained excluding three primary tasks fol-\nlowing the protocol of [12]. Performance for this task is\nmeasured with Recall@K at different IoU thresholds [12].\nHTM-Align (Evaluation). This benchmark is used to eval-\nuate our model on narration grounding.\nIt contains 80\nvideos where the ASR transcriptions have been manually\naligned temporally with the video. In the main submission,\nwe report the R@1 metric [19], which evaluates whether the\nmodel can correctly localize the narrations that are alignable\nwith the video. In Section F we also evaluate our model in\nterms of its capability to decide whether a narration is vi-\nsually groundable in the video or not using the ROC-AUC\nmetric [19]. AUC denotes the area the ROC curve of the\nalignment task, and measures the ability of the model to\ncorrectly predict whether a given step is alignable within a\nvideo or not.\nC. Implementation Details\nAs video encoder we adopt the S3D [60] backbone pre-\ntrained with the MIL-NCE objective on HowTo100M [35].\nFollowing previous work [19, 61], we keep this module\nfrozen and use it to extract clip-level features (one fea-\nture per second for video decoded at 16 fps). For extract-\ning context-aware features for each sentence (step or nar-\nration), we follow the Bag-of-word (BoW) approach based\non Word2Vec embeddings [37]. These embeddings are ini-\ntialized based on MIL-NCE Word2Vec and are fine-tuned\nduring training.\nThe hyperparameters of the model compared with state-\nof-the-art methods in Tables 1,2,3 of the main submission\nwere selected based on R@1 performance on the HT-Step\nvalidation set and are: \u03bbSV = \u03bbNV = 1, temperatures\n\u03b7, \u03be = 0.07, and pseudo-label filtering threshold \u03b3 = 0.65.\nWe train our model for 12 epochs, with 3 epochs burn-\nin training with step pseudo-labels generated by TAN, and\nthen we update the teacher VINA every 3 epochs. We use\nthe AdamW [30] optimizer, having an initial learning rate of\n2e \u2212 4 decayed with a cosine learning schedule. Our batch\nsize is 32 videos, with maximum length of 1024 seconds.\nPseudo-labels are obtained based on the steps-to-video\nalignment matrix and are generated (before filtering) as fol-\nlows: for each step we find the timestep with maximum\nsimilarity with the step and then extend a temporal segment\nto the left and right of that peak as long as the similarity\nscore does not follow below 0.7 of the peak height. Pseudo-\nlabels whose peak score falls below the filtering threshold \u03b3\nare not used for training.\nThe rest of hyperparameters were selected based on\nTAN [19]. The multimodal encoder is a pre-norm multi-\nlayer transformer which consists of 6 layers of self-\nattention, with 8 heads and has hidden dimension D = 512.\nA learnable positional encoding of size D = 512 is used\nto inject temporal information to each frame/narration/step\ntoken.\nTo obtain temporal segment detections from the step-to-\nvideo alignment output of our model (e.g. for evaluating on\nthe CrossTask article grounding setting or for the qualitative\nvideo included in this supplementary) we use a simple 1D\nblob detector [57]. Unless otherwise specified, we use the\nfused alignment matrix for step grounding when narrations\nare available during inference time.\nOur model is trained on 8 GPUs (Tesla V100-\nSXM2-32GB) and training lasts approximately 10-12\nhours.\nAll models were implemented in Python using\nPytorch and are based on the PySlowFast (https:\n//github.com/facebookresearch/SlowFast)\nand\nTAN\n(https://github.com/TengdaHan/\nTemporalAlignNet) open-source codebases.\nFor\nablation studies, we choose the best checkpoint for each\nconfiguration based on performance on HT-Step validation\nset and report its test split performance.\nD. Qualitative Results\nIn this section, we provide qualitative results for the\nground-truth steps-to-video alignment and predicted align-\nments by our improved baseline that serves as the initial\nteacher model (TAN*), and our model (using the direct\nsteps-to-video alignment without narrations) or the fusion\nwith the indirect steps-to-video alignment (with narrations).\nFrom these qualitative results (Figure 3), we observe that\nour VINA model can correctly temporally localize visu-\nally groundable steps, despite being trained only with noisy\npairs of narrated videos and instructional steps. Predicted\nalignments tend to also be less noisy than TAN*, show-\ncasing the effectiveness of training a video-language align-\nment model with distant supervision from WikiHow arti-\ncles. Our model can also leverage ASR transcripts (with-\nout any temporal information regarding when the instructor\nuttered each narration) to further improve its results (Fig-\nure 4).\nE. Extra Ablations\nArchitecture ablations.\nIn Table 10 we study the design\nof the unimodal encoder used to embed steps before they\nare fed to our Multimodal Transformer. Overall, using po-\nsitional embeddings capturing the ordering of steps in a\ntask, and using modality-specific projection MLPs leads to\na slightly better performance in step grounding (w/o nar-\nration input). Narration grounding seems to benefit from\nusing a shared text encoder, possibly because this facilitates\nknowledge transfer from the WikiHow steps.\nPE\nSep. MLP\nHT-Step \u2191R@1\nHTM-Align\nw/o nar.\nw/ nar.\n\u2713\n33.5\n34.0\n65.8\n34.0\n34.9\n65.9\n\u2713\n33.8\n34.4\n67.0\n\u2713\n\u2713\n34.3\n36.1\n64.8\nTable 10: Ablation study on architecture design.\nWe\nstudy the contribution of positional encodings for steps (PE)\nand of specialized text projection layers for wikiHow article\nsteps (Sep. MLP). All models are trained for joint narration\nand step grounding with fixed pseudo-labels from TAN and\nevaluated on HT-Step val split (last row corresponds to row\n5 in Table 4 of the main text).\nF. Experimental Setup on HTM-Align\nAs\nexplained\nin\nthe\nofficial\ncode\nrepository\nof\nTAN\n[19]\n(https://github.com/TengdaHan/\nTemporalAlignNet/tree/main/htm_align), the\nresults reported for HTM-Align are obtained with a text\nmoving window of 1 minute, i.e., for each 1-minute tempo-\nral segment only ASR captions whose original time-stamps\nfall within a 3-min window centered around this temporal\nsegment are considered for grounding.\nInstead, for all\nour reported results (for TAN* and VINA) we operate in\nthe more challenging setup where an ASR caption can be\ngrounded in any timestep of the original video (there is\nno knowledge about the original ASR timestamps during\ninference). Under this more challenging setup, our model\noutperforms TAN both in narration retrieval, as measured\nby Recall@1 (66.5% vs 49.4%, as seen in Table 1 of the\nmain submission).\nOur model also performs comparably with TAN in step\nalignability prediction, as measured by ROC-AUC (76%\nvs 75.1%). Note that our model does not have dedicated\nalignability head for predicting whether a narration exists or\nnot in the video as TAN [19]. Instead, we simply obtain an\nalignability score by using the maximum cosine similarity\nscore over time, where cosine similarities of each narration\nwith each video frame are computed based on the outputs\nof the unimodal encoders.\nG. Limitations and Ethical Concerns\nFrom the qualitative results, we observe that due to the\nlosses used during training, which do not explicitly penalize\n(a) Ground-truth step grounding annotations.\n(b) Predicted alignment by TAN*.\n(c) Predicted alignment by VINA w/o narrations.\n(d) VINA w/ narrations.\nFigure 3: Qualitative results on a validation video of the HT-Step dataset (VIQYQkA3mNU) demonstrating how to Broil Steak.\nSteps that are not visually groundable in the video are highlighted in blue, steps that are correctly retrieved by each model\nare highlighted in green, while steps that are not retrieved are shown in red. Figure best viewed zoomed in and in color.\nwrong temporal extent (as long as the predicted heatmap has\na peak within the target temporal window), grounded tem-\nporal segments tend to be short. This is especially promi-\nnent when using the direct steps-to-videos alignment that\nis explicitly supervised (second to last row of the predicted\nalignment figures). Furthermore, our training objective does\nnot utilize negative examples, e.g. steps that are not vi-\nsually groundable, to suppress detections. This can lead\nto confident detections for missing steps. Another limita-\ntion of our approach (similar to previous approaches that\noperate on pre-extracted visual features) is that our per-\nformance is limited by the quality of the extracted visual\nrepresentations. Regarding ethical concerns, public instruc-\ntional video datasets and public knowledge base datasets\nmay have gender, age, geographical and cultural bias.\n(a) Ground-truth step grounding annotations.\n(b) Predicted alignment by TAN*.\n(c) Predicted alignment by VINA w/o narrations.\n(d) VINA w/ narrations.\nFigure 4: Qualitative results on a validation video of the HTM-Step dataset (0dHofx1lqAg) demonstrating how to Make\nBroccoli Cheese Soup. Steps that are not visually groundable in the video are highlighted in blue, steps that are correctly\nretrieved by each model are highlighted in green, while steps that are not retrieved are shown in red. Figure best viewed\nzoomed in and in color.\n"
  },
  {
    "title": "LEACE: Perfect linear concept erasure in closed form",
    "link": "https://arxiv.org/pdf/2306.03819.pdf",
    "upvote": "1",
    "text": "LEACE: Perfect linear concept erasure in closed form\nNora Belrose1\nDavid Schneider-Joseph1\nShauli Ravfogel2\nRyan Cotterell3\nEdward Raff4\nStella Biderman1,4\n1EleutherAI\n2Bar-Ilan University\n3ETH Z\u00fcrich\n4Booz Allen Hamilton\n{nora,stella}@eleuther.ai\ndavid@davidsj.com\nAbstract\nConcept erasure aims to remove specified features from a representation. It can\nimprove fairness (e.g. preventing a classifier from using gender or race) and\ninterpretability (e.g. removing a concept to observe changes in model behavior). We\nintroduce LEAst-squares Concept Erasure (LEACE), a closed-form method which\nprovably prevents all linear classifiers from detecting a concept while changing the\nrepresentation as little as possible, as measured by a broad class of norms. We apply\nLEACE to large language models with a novel procedure called concept scrubbing,\nwhich erases target concept information from every layer in the network. We\ndemonstrate our method on two tasks: measuring the reliance of language models\non part-of-speech information, and reducing gender bias in BERT embeddings. Our\ncode is available at https://github.com/EleutherAI/concept-erasure.\n1\nIntroduction\nThe ability to prevent a machine learning system from using a specified concept is important for\nfairness and interpretability. Popular notions of fairness require that protected attributes should not\ncausally affect predictions [22, 26], and interpretability research often estimates the causal effect of a\nconcept by attempting to remove it from a model\u2019s internal representations [10, 30, 25, 5, 18].\nWhat it means for a model M to \u201cuse\u201d a concept Z is often vague and application-specific, but\na necessary condition is that its outputs\u2014and therefore its inputs and hidden states\u2014should have\nsignificant mutual information with Z.1 Concept erasure leverages this fact to limit M\u2019s use of Z\nwithout finetuning or inspecting its parameters. Instead, we edit the input or hidden states X used by\nM to minimize the predictive V-information IV(X \u2192 Z) [43], a tractable lower bound on the mutual\ninformation I(X; Z) which measures the degree to which classifiers from the family V can predict\nZ. Intuitively, if no classifier in V can outperform a constant function at predicting Z\u2014a condition\nknown as guardedness\u2014then M can\u2019t use Z either, at least if V is expressive enough relative to M.\nIn this work, we improve upon existing concept erasure techniques using a theory-driven approach.\nWe focus on the case where V is the set of linear classifiers, and prove a previously unnoticed\nequivalence: a classification task is linearly guarded if and only if every class has exactly the same\nmean feature vector (\u00a7 3). Leveraging this equivalence, we derive a simple necessary and sufficient\ncondition for an affine transformation to produce linearly guarded features. We then identify the\nunique surgical transformation in this family\u2014the one that minimizes the mean squared distance\nfrom the original features with respect to all norms induced by inner products, including the popular\nEuclidean and Mahalanobis norms. We name it LEAst-squares Concept Erasure (LEACE) (\u00a7 4).\nWhile prior work has focused on preventing linear models from leveraging Z, we aim to erase\nconcepts from deep neural networks as well. Interpretability research has shown that networks\n1This follows from the fact that causal dependence is a special kind of statistical dependence [28]. By the\ndata processing inequality, M\u2019s output can\u2019t have any more information about Z than its input or hidden states.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.03819v3  [cs.LG]  29 Oct 2023\ncan be usefully described as encoding features in linear subspaces [11, 24, 41], suggesting that\nfundamentally nonlinear methods may not be necessary for successful erasure in DNNs. In light of\nthis, we introduce a simple procedure called concept scrubbing (\u00a7 6), which sequentially applies\nLEACE to the intermediate representations at each layer of a deep network.\nWe empirically validate our proposals, demonstrating the superiority of LEACE for erasing gender\nbias from BERT representations (\u00a7 5.2), and using concept scrubbing to measure the extent to which\nlarge language models use part-of-speech information (\u00a7 6).\n2\nPreliminaries\nConsider a k-class classification task over jointly defined random vectors X (the input data) and Z\n(the one-hot labels), with X of finite first moment and taking values in Rd, and Z taking values in\nZ = {z \u2208 {0, 1}k \f\f \u2225z\u22251 = 1}2 with each P(Z = j) > 0. Let \u03b7(\u00b7; \u03b8) : Rd \u2192 Rk be a predictor\nchosen from a function class V = {\u03b7(\u00b7; \u03b8) | \u03b8 \u2208 \u0398} (presumed to contain all constant functions) so\nas to minimize the expectation E\n\u0002\nL(\u03b7(X), Z)\n\u0003\nof some L : Rk \u00d7 Z \u2192 [0, \u221e) in a class L of loss\nfunctions.\nWe borrow the concept of guardedness from Ravfogel et al. [33], who define it in terms of V-\ninformation [43]. We opt for a slightly more general definition here, which is equivalent to theirs in\nthe case of cross-entropy loss (see Appendix G).\nDefinition 2.1 (Guardedness). Let X, Z, V, and L be as defined above, and let \u03c7 be the set of all\nrandom vectors of finite first moment taking values in Rd, jointly defined with Z.\nWe say X (V, L)\u2212guards Z if, for all losses L \u2208 L, it maximizes the minimum expected loss:\nX \u2208 argmax\nX\u2032\u2208\u03c7\ninf\n\u03b8\u2208\u0398 E\nh\nL(\u03b7(X\u2032; \u03b8), Z)\ni\n.\nIn other words, its conditional distribution P(X | Z = \u00b7) is among the worst possible distributions for\npredicting Z from X using a predictor of the form \u03b7(\u00b7; \u03b8) \u2208 V and a loss function in L.\nDefinition 2.2 (Trivially Attainable Loss). The trivially attainable loss for labels Z and loss L is the\nlowest possible expected loss available to a constant predictor \u03b7(x) = b:\nL\u03c4 = inf\nb\u2208Rk E[L(b, Z)]\nWe will sometimes write it L(Z,L)\n\u03c4\nin cases of possible ambiguity. If there is a specific constant\npredictor actually achieving this loss, we call it the trivial predictor \u03b7\u03c4 = \u03b7(Z,L)\n\u03c4\n.\nWe examine this problem in the important case of loss functions L : Rk \u00d7 Z \u2192 [0, \u221e) which are\nconvex in the prediction \u03b7(x), and linear predictors that take the functional form \u03b7(x; b, W) =\nb + Wx, for some bias b \u2208 Rk and weight matrix W \u2208 Rk\u00d7d.\nDefinition 2.3 (Linear Guardedness). If X (V, L)-guards Z, where L is the class of nonnegative\nloss functions which are convex in their first argument, and V is the class of linear predictors\n\u03b7(x) = b + Wx, we say that X linearly guards Z.\n3\nTheoretical Results\nOur primary theoretical result is that the following conditions are all equivalent:\n1. The data X linearly guards the labels Z. (Definition 2.3)\n2. For all convex losses L, the trivially attainable loss is optimal on (X, Z). (Definition 2.2)\n3. The class-conditional mean vectors E[X | Z = i] are equal to the unconditional mean E[X].\n4. Every component of X has zero covariance with every component of Z.\n5. Every linear classifier evaluated on X exhibits statistical parity w.r.t. Z. (App. C)\nThe equivalence of conditions 1, 2, and 5 is relatively straightforward to show, and the relevant\ntheorems can be found in Appendices B and C. The other equivalences are proven below (cond. 3 \u2194\ncond. 2 in \u00a7 3.1 and \u00a7 3.2); cond. 3 \u2194 4 in \u00a7 3.3).\n2We frequently use the integer j \u2264 k to refer to the element of Z which is 1 at the jth index and 0 elsewhere.\n2\n3.1\nEquality of Class Centroids Implies Linear Guardedness\nThe following result establishes the implication from condition 3 to condition 2.\nTheorem 3.1. Suppose L is convex in the linear prediction \u03b7. Then if each class-conditional mean\nE\n\u0002\nX | Z = i\n\u0003\nis equal to E\n\u0002\nX\n\u0003\n, the trivially attainable loss cannot be improved upon.\nProof. Let \u03b7(x) = b+Wx be any linear predictor. By Jensen\u2019s inequality,3 the loss with \u03b7 evaluated\non X is lower bounded by the loss with \u03b7 evaluated on the unconditional mean of the data E\n\u0002\nX\n\u0003\n:\nE\nh\nL(\u03b7, Z)\ni\n= EZ\nh\nE\nh\nL(\u03b7, Z)\n\f\fZ\nii\n\u2265 EZ\nh\nL\n\u0010\nE\n\u0002\n\u03b7\n\f\fZ\n\u0003\n, Z\n\u0011i\n(Jensen\u2019s inequality)\n= EZ\nh\nL\n\u0010\nb + WE\n\u0002\nX\n\f\fZ\n\u0003\n, Z\n\u0011i\n(linearity of \u03b7)\n= EZ\nh\nL\n\u0010\nb + WE\n\u0002\nX\n\u0003\n, Z\n\u0011i\n.\n(by assumption)\nThis in turn is the loss of the constant predictor \u03b7\u2032(x) = b + WE\n\u0002\nX\n\u0003\n. Since the trivially attainable\nloss is the best that can be achieved by a constant predictor, and every predictor\u2019s loss is lower\nbounded by that of some constant predictor, we cannot improve upon the trivially attainable loss.\nIntuitively, this shows that the classifier\u2019s expected loss is lower-bounded by the loss it would receive\nif each data point were replaced with the centroid of its class. But, if these centroids are all equal, the\nloss can\u2019t be any lower than what we\u2019d get if every data point were replaced with the global mean\nE[X]. In that case, the data points are indistinguishable and we can\u2019t do better than W = 0.\n3.2\nLinear Guardedness Implies Equality of Class Centroids\nWe now prove the implication from condition 2 to condition 3. Condition 2 applies when the trivially\nattainable loss is optimal for all convex losses, including cross-entropy loss in particular. And if it\nholds for cross-entropy loss, we now show that condition 3\u2014the class centroids are equal\u2014must\nfollow. First a more general lemma:\nLemma 3.2. Suppose L has bounded partial derivatives, which when off-category never vanish and\ndo not depend on the category, i.e. \u2202L(\u03b7, z1)/\u2202\u03b7i = \u2202L(\u03b7, z2)/\u2202\u03b7i \u0338= 0 for all categories z1, z2 \u0338= i.\nIf E\n\u0002\nL(\u03b7, Z)\n\u0003\nis minimized among linear predictors by the constant predictor \u03b7(x) = b\u2217 + W\u2217x\nwith W\u2217 = 0, then each class-conditional mean E\n\u0002\nX|Z = i\n\u0003\nis equal to E\n\u0002\nX\n\u0003\n.\nProof. The first-order optimality condition on the ith component of our parameters b and W yields\nthe equations:\nE\n\"\n\u2202L(\u03b7, Z)\n\u2202\u03b7i\n\u00b7 \u2202\u03b7i\n\u2202bi\n#\n= 0\nand\nE\n\"\n\u2202L(\u03b7, Z)\n\u2202\u03b7i\n\u00b7 \u2202\u03b7i\n\u2202Wi\n#\n= 0,\n(1)\nwhere we have used the boundedness of L\u2019s partial derivative and the finite first moment of \u2202\u03b7i\n\u2202bi = 1\nand\n\u2202\u03b7i\n\u2202Wi = X to justify (via the Dominated Convergence Theorem) interchanging the derivative with\nthe expectation.\nSince \u03b7 is constant over all values of X, and \u2202\u03b7i\n\u2202bi = 1, the first equation in (1) reduces to:\nP(Z = i)\u2202L(\u03b7, i)\n\u2202\u03b7i\n+ P(Z \u0338= i)\u2202L(\u03b7, \u0338= i)\n\u2202\u03b7i\n= 0,\n(2)\nwhere \u2202L(\u03b7,\u0338=i)\n\u2202\u03b7i\nis an abuse of notation denoting the off-category partial derivative, emphasizing its\nindependence of the category Z.\n3Specifically, its generalization to convex functions over Rk. See [12] p. 76.\n3\nSimilarly, the constancy of \u03b7 and the fact that\n\u2202\u03b7i\n\u2202Wi = X reduces the second equation in (1) to:\nP(Z = i)\u2202L(\u03b7, i)\n\u2202\u03b7i\n\u00b7 E\n\u0002\nX\n\f\fZ = i\n\u0003\n+ P(Z \u0338= i)\u2202L(\u03b7, \u0338= i)\n\u2202\u03b7i\n\u00b7 E\n\u0002\nX\n\f\fZ \u0338= i\n\u0003\n= 0.\n(3)\nSolving for P(Z = i) \u2202L(\u03b7,i)\n\u2202\u03b7i\nin (2) and substituting in (3) gives us:\nP(Z \u0338= i)\u2202L(\u03b7, \u0338= i)\n\u2202\u03b7i\n\u00b7\n \nE\n\u0002\nX\n\f\fZ \u0338= i\n\u0003\n\u2212 E\n\u0002\nX\n\f\fZ = i\n\u0003\n!\n= 0.\nIf P(Z \u0338= i) = 0, then E[X] = E[X|Z = i] is trivially true. Otherwise, using the non-vanishingness\nof the off-category partial derivative \u2202L(\u03b7,\u0338=i)\n\u2202\u03b7i\n, division yields the equivalence of E\n\u0002\nX\n\f\fZ = i\n\u0003\nto\nE\n\u0002\nX\n\f\fZ \u0338= i\n\u0003\n, and hence to the unconditional mean E\n\u0002\nX\n\u0003\n.\nWe now show that Lemma 3.2 applies to the widely used cross entropy loss:\nTheorem 3.3. If the class probabilities P(Z = j) are all nonzero, and the trivially obtainable loss is\noptimal when L(\u03b7, z) = \u2212 log\nexp(\u03b7z)\nPk\ni=1 exp(\u03b7i), then each class has the same mean E\n\u0002\nX\n\f\fZ = z\n\u0003\n.\nProof. In this case, the trivial predictor \u03b7\u03c4(Z)j = log(P(Z = j)) exists, achieving the trivially\nobtainable loss, which we have assumed optimal. Furthermore, L has on-category partial deriva-\ntive \u2202L(\u03b7, i)/\u2202\u03b7i = exp(\u03b7i)/Pk\nj=1 exp(\u03b7j) \u2212 1 \u2208 (\u22121, 0], and nonvanishing off-category partial\nderivative \u2202L(\u03b7, \u0338= i)/\u2202\u03b7i = exp(\u03b7i)/Pk\nj=1 exp(\u03b7j) \u2208 (0, 1), both bounded, so the conditions of\nLemma 3.2 apply.\n3.3\nLinearly Guarded Labels Have Zero Covariance with the Features\nThe next theorem establishes the equivalence of conditions 3 and 4.\nTheorem 3.4. Let X be a random vector taking values in Rd with finite first moment, and Z a random\nvector taking values in {0, 1}k with one-hot encoding, with each class probability P(Z = j) being\nnonzero. Then the class-conditional means E[X|Z = j] are all equal to the unconditional mean\nE[X] if and only if every component of X has zero covariance with every component of Z, i.e. the\ncross-covariance matrix \u03a3XZ, whose (i, j)th entry is Cov(Xi, Zj), is the zero matrix.\nProof. Since Z is one-hot, we can rewrite the (i, j)th entry of \u03a3XZ as:\nE[XiZj] \u2212 E[Xi]E[Zj] = P(Z = j)\n\u0010\nE[Xi|Z = j] \u2212 E[Xi]\n\u0011\n.\nAs P(Z = j) > 0, it follows that E[Xi|Z = j] = E[Xi] if and only if Cov(Xi, Zj) = 0.\nWe have thus established the equivalence of the first four conditions stated earlier. See Appendix C\nfor the last one, on statistical parity.\n4\nLeast-Squares Concept Erasure\nIn Section 3 we saw that X linearly guards Z if and only if each component of X has zero covariance\nwith each component of Z. We will now characterize the set of affine transformations r(x) = Px+b\nsuch that r(X) linearly guards Z.\nTheorem 4.1. Let X and Z be random vectors taking values in Rd and Rk respectively, with X of\nfinite first moment. Then given some affine function r(x) = Px + b, the modified random vector\nr(X) linearly guards Z if and only if the columns of the cross-covariance matrix \u03a3XZ are contained\nin the null space of P.\nProof. From Theorem 3.4 we know that r(X) linearly guards Z if and only if Cov(r(X), Z) is the\nzero matrix. By the linearity property of cross-covariance, we have:\nCov(r(X), Z) = Cov(PX + b, Z) = PCov(X, Z) = P\u03a3XZ.\nTherefore, r(X) linearly guards Z if and only if ker(P) \u2287 colsp(\u03a3XZ).\n4\nImplications for prior work. Notably, the above theorems imply that three previously proposed\nmethods in the literature, Spectral Attribute Removal (SAL) [36], Mean Projection [17], and Fair PCA\n[20], are guaranteed to achieve linear guardedness given suitable hyperparameters. See Appendix D\nfor further discussion.\n4.1\nDerivation of LEACE\nTheorem 4.1 is a very weak condition, which is far from identifying unique values for P and b.\nIn most applications, however, we\u2019d like to make a \u201csmall\u201d edit to X so that useful information\ncontained in X is maximally preserved. We operationalize the notion of a small edit in terms of the\nmean squared norm E\u2225r(X) \u2212 X\u22252\nM defined by some positive-definite inner product M,4 which\ncan be thought of as a local quadratic approximation to any measure of divergence between X and\nr(X) (such as Kullback\u2013Leibler divergence, for example). While we are primarily interested in the\nEuclidean (M = I) and Mahalanobis (M = \u03a3+\nXX) norms, it will turn out that there is a single erasure\nfunction that minimizes all such norms simultaneously. We will see in Section 6 that ensuring edits\nare small in this sense provides substantial benefit to downstream task performance as compared to\nother methods which also guard the labels Z.\nBelow, we derive the optimal eraser under the assumption that X and Z are centered.\nTheorem 4.2. Let X and Z be centered random vectors taking values in Rd and Rk respectively, each\nof finite second moment. Let M \u2208 Rd\u00d7d be a p.s.d. matrix defining a (possibly degenerate) inner\nproduct on Rd: \u27e8x, y\u27e9M = xT My. Let \u03a3XX \u2208 Rd\u00d7d be X\u2019s covariance matrix, and \u03a3XZ \u2208 Rd\u00d7k\nbe the cross-covariance matrix of X and Z. Let A+ denote the Moore-Penrose pseudoinverse of a\nmatrix A, and let A1/2 be the p.s.d. square root of a p.s.d. matrix A. Then the objective\nargmin\nP\u2208Rd\u00d7d E\nh\r\rPX \u2212 X\n\r\r2\nM\ni\nsubject to Cov(PX, Z) = 0\nhas the following solution:\nP\u2217 = I \u2212 W+PW\u03a3XZW,\nwhere W is the whitening transformation (\u03a31/2\nXX)+ and PW\u03a3XZ = (W\u03a3XZ)(W\u03a3XZ)+ is the\northogonal projection matrix onto colsp(W\u03a3XZ).\nProof. See Appendices E.1 and E.2 for two independent proofs of Theorem 4.2.\nThe above theorem assumes that the random vectors X and Z are centered, and does not include a\nbias term. Below we extend our results to the uncentered case, and derive the optimal bias b\u2217.\nTheorem 4.3. Let X and Z be random vectors taking values in Rd and Rk respectively, each of finite\nsecond moment. Define M and P\u2217 as in Theorem 4.2 and b\u2217 = E[X] \u2212 P\u2217E[X]. Then (P\u2217, b\u2217)\nminimizes E\n\r\rPX + b \u2212 X\n\r\r2, subject to Cov(PX + b, Z) = 0.\nProof. Let P \u2208 Rd\u00d7d and define \u02dcX = X \u2212 E[X] and c = PE[X] + b \u2212 E[X]. Then,\nE\n\r\rPX + b \u2212 X\n\r\r2\nM = E\n\r\r(P\u02dcX \u2212 \u02dcX) + c\n\r\r2\nM\n= E\n\r\rP\u02dcX \u2212 \u02dcX\n\r\r2\nM + 2E\n\u0002\nP\u02dcX \u2212 \u02dcX\n\u0003T Mc + cT Mc\n= E\n\r\rP\u02dcX \u2212 \u02dcX\n\r\r2\nM + cT Mc,\nwhere we have eliminated the middle term because P is linear and E[\u02dcX] = 0. Since M is p.s.d., our\nobjective is minimized for c = 0, i.e. b = E[X]\u2212PE[X]. The problem thus reduces to choosing P so\nas to minimize E\n\r\rP\u02dcX \u2212 \u02dcX\n\r\r2\nM subject to Cov(PX + b, Z) = Cov(P\u02dcX, Z) = 0, which Theorem 4.2\nshows occurs when P = P\u2217.\n4Our proofs also include degenerate \u201cinner products\u201d where M is singular, and the associated seminorms.\n5\n\u22125\n0\n5\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n\u22125\n0\n5\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n\u22125\n0\n5\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n\u22125\n0\n5\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nConcept\nSubspace\nOrthogonal\nSubspace\nOutput\nSubspace\nClass 1\nClass 2\nOriginal Data\nWhitening\nErasure\nUnwhitening\nFigure 1: LEACE projection in 3 steps. First the data is whitened, ensuring equal variance in all\ndirections. It is then orthogonally projected onto colsp(W\u03a3XZ)\u22a5, guaranteeing linear guardedness.\nFinally, we unwhiten the data so that its covariance structure mimics the original.\nPutting together Theorems 4.2 and 4.3 and rearranging, we arrive at the LEACE formula:\nrLEACE(x) = x \u2212 W+PW\u03a3XZW\n\u0000x \u2212 E[X]\n\u0001\n(1)\nIntuitively, LEACE de-means and whitens x, projects onto the subspace responsible for correlations\nbetween X and Z, then unwhitens the result. Finally, it subtracts this value from x, thereby surgically\nremoving the linearly available information about Z.\n4.2\nOblique Projections are Least-Squares Optimal\nPrior work on linear concept erasure has assumed that erasure functions should be orthogonal\nprojections [29, 32, 36], appealing to the well-known fact that an orthogonal projection of a point\nx onto a subspace U yields the nearest point in U to x. But even in the case where X is centered,\nrLEACE is not an orthogonal projection in general. Orthogonal projection matrices are symmetric,\nand I \u2212 W+PW\u03a3XZW is only symmetric in the special case where PW\u03a3XZ and W commute. It is\nan oblique projection however, since applying P\u2217 twice yields the same result as applying it once:\n(P\u2217)2 = I \u2212 2WPW\u03a3XZW+ + W+PW\u03a3XZ\u0018\u0018\u0018\u0018\nWW+PW\u03a3XZW = P\u2217.\nOrthogonal projections are generally not least-squares optimal for concept erasure because the\nnecessary and sufficient condition for linear guardedness, P\u03a3XZ = 0, is a constraint on the nullspace\nof P, and not on its range. We may freely choose the range of the projection to minimize the mean\nsquared distance, as long as we zero out colsp(\u03a3XZ). In Figure 1, an orthogonal projection would\nmap all points onto the the dashed line, thereby preserving less of the variance of the original data\nthan LEACE does (green line). See Appendix F for a concrete example.\n4.3\nExtension to Continuous Z\nWhile not a focus of this work, it\u2019s worth noting that LEACE can also be applied to the setting where\nZ takes arbitrary values in Rk, as long as we restrict ourselves to the ordinary least squares regression\nloss L(\u03b7, z) = \u2225\u03b7 \u2212 z\u22252\n2. In particular, the proofs of equivalence between conditions 1 and 2 given in\nAppendix B make no categorical assumption on Z, and the equivalence between the optimality of a\nzero weight matrix (condition 2) and zero cross-covariance (condition 4) is well known in the OLS\nsetting. We can then apply Theorems 4.2 and 4.3, which also make no categorical assumption, to\nderive the same optimal affine eraser as in the categorical case.\n5\nEvaluation\n5.1\nIntrinsic Evaluation\nFollowing Ravfogel et al. [31] we evaluate the ability of our method to remove gender information\nfrom the last hidden layer of a frozen BERT model. We use the biographies dataset of De-Arteaga\net al. [6], composed of short biographies annotated by both binary gender and profession. We\nembed each biography with the [CLS] representation in the last layer of BERT, enforce the same-\nconditional-mean constraint to remove gender information from the [CLS] , and then evaluate the\nperformance of the model, after the intervention, on the main task of profession prediction. We\n6\n0.2\n0.4\n0.6\n0.8\n% women\n0.4\n0.2\n0.0\n0.2\n0.4\nTPR-Gap\nsurgeon\nprofessor\njournalist\nphysician\nattorney\nnurse\ndentist\nteacher\nphotographer\nchiropractor\nyoga_teacher\nsoftware_engineer\npoet\ncomposer\npainter\ndj\narchitect\npsychologist\ndietitian\nparalegal\nfilmmaker\ncomedian\npastor\naccountant\npersonal_trainer\nrapper\ninterior_designer\n0.2\n0.4\n0.6\n0.8\n% women\n0.4\n0.2\n0.0\n0.2\n0.4\nTPR-Gap\nsurgeon\nprofessor\njournalist\nphysician\nattorney\nnurse\ndentist\nteacher\nphotographer\nchiropractor\nyoga_teacher\nsoftware_engineer\npoet\ncomposer\npainter\ndj\narchitect\npsychologist\ndietitian\nparalegal\nfilmmaker\ncomedian\npastor\naccountant\npersonal_trainer\nrapper\ninterior_designer\nFigure 3: The correlation between GAP T P R\nfemale,y and the relative proportion of women in profession\ny, for BERT representation, before (left; R=0.867) and after (right; R=0.392) the projection.\ncompare our intervention with RLACE [31], which uses gradient-based optimization to solve a linear\nconcept-erasure adversarial game.\nConcept erasure results.\nFirst, we evaluate the ability of logistic regression classifiers to recover\nthe removed information. The results, presented in Fig. 2, show that our method is the only to achieve\nrandom accuracy (perfect erasure) with a small edit, although RLACE (but not INLP) comes close. At\nthe same time, our method is around 2 orders of magnitude faster, and does not require gradient-based\noptimization.\n5.2\nDownstream Fairness\n2\n3\n4\n5\n6\nMean Squared Error\n0.5\n0.6\n0.7\n0.8\n0.9\nGender Accuracy\nMajority\nRLACE\nINLP\nOurs\nFigure 2: Gender prediction ac-\ncuracy after bias-removal projec-\ntion versus the mean squared dis-\ntance from the original representa-\ntion for INLP, RLACE, and LEACE\non BERT representations.\nHow does our intervention affect the behavior of the model on\nthe main classification task of profession prediction? We fit\na logistic regression profession-prediction classifier over the\nprojected [CLS] representations.\nTo measure the bias in a classifier, we follow De-Arteaga et al.\n[6] and use the TPR-GAP measure, which quantifies the bias\nin a classifier by considering the difference (GAP) in the true\npositive rate (TPR) between individuals with different protected\nattributes (e.g. race or gender). We use the notation GAPTPR\nz,y\nto denote the TPR-gap in some main-class label y (e.g. \u201cnurse\u201d\nprediction) for some protected group z (e.g. \u201cfemale\u201d), we also\nconsider GAPTPR,RMS\nz\n, the RMS of the TPR-gap across all\nprofessions for a protected group z:\nGAPTPR,RMS\nz\n=\ns\n1\n|C|\nX\ny\u2208C\n(GAPTPR\nz,y )2\nTo calculate the relation between the bias the model exhibits\nand the bias in the data, we also calculate \u03c3(GAPTPR,%Women),\nthe correlation between the TPR gap in a given profession and\nthe percentage of women in that profession.\nResults.\nThe main-task classifier achieves profession-prediction accuracy of 77.3% on the pro-\njected representations (compared with 79.3% over the original representations), indicating that the\nintervention minimally affects the ability to predict the profession of a person from the represen-\ntation of their biography. At the same time, the TPR gap drops significantly from 0.198 to 0.084,\nindicating a sharp drop in the biased behavior of the profession classifier. Indeed, inspecting the\ncorrelation \u03c3(GAPTPR,%Women) between the gap (per profession) and the representation of women in\n7\n0\n2\n4\n6\n8\n10\n12\nLayer of Intervention\n0.2\n0.4\n0.6\n0.8\nMLM Accuracy\nOurs\nRandom\nINLP (20 iterations)\nNo Intervention\n0\n2\n4\n6\n8\n10\n12\nLayer of Intervention\n1\n2\n3\n4\n5\nMLM Loss\nFigure 4: Amnesic probing results on bert-base-uncased.\nthis profession, we see that this correlation plummets from 0.867 to 0.392 after erasure. Re-fitting\nthe main-task logistic regression classifier over the projected representations yields a slightly higher\nmain-task accuracy of 78.1%, at the price of significantly increasing the TPR gap to 0.158.5\n5.3\nRevisiting Amnesic Probing\nElazar et al. [10] have introduced the idea of amnesic probing as a causal intervention that aims to\ntest the importance of a given concept (e.g. part-of-speech tag) to some main task (e.g. language\nmodeling). They applied Iterative Nullspace Projection (INLP) to remove different concepts from the\nhidden representations of the model, and assessed the degree to which its behavior changed when\nperforming masked language modeling. Since INLP often requires dozens of iterations to completely\nerase the concept, its usage in this context raises concerns of collateral damage due to magnitude of\nthe intervention and the non-exhaustive nature of INLP removal. Here, we replicate their experiments\non the bert-base-uncased model with our interventions.\nExperimental setup.\nWe use part-of-speech (POS) tags as our concept of interest. We collect\nsentences and their coarse POS tags (\u201cNoun\u201d, \u201cVerb\u201d etc.; 18 in total) from the English Universal\nDependencies dataset [27]. We tokenize the sentences with the BERT tokenizer and map each word-\npiece to the POS tag of the word to which it belongs. We collect the unmasked BERT representations\nfor each layer, intervene to linearly erase the POS concept from that layer, and continue the forward\npass until the last layer, from which we compute the distribution of the MLM over the vocabulary.\nNote that in each experiment we intervene on a single layer. We quantify the decrease in accuracy\nfollowing the intervention, as well as the increase in the loss. We compare with a baseline intervention\nof a random orthogonal projection whose null space has the same rank as the label space (18). For\nINLP, we perform 20 iterations. This is needed because INLP does not effectively remove the concept;\neven after 20 iterations, classification accuracy is above majority accuracy. As a result, INLP reduces\nthe rank of the representation by 360. By contrast, our method decreases the rank just by 17.\nResults.\nThe results are shown in Fig. 4b. Our intervention only mildly changes BERT LM accuracy\nand loss until layer 8, with the highest drop recorded in layer 11. INLP, in contrast, shows maximum\neffect at layer 6. Since it removes hundreds of dimensions, it is difficult to attribute this effect to\nthe erasure of the concept. These results suggest that the causal effect of the POS concept on the\nlanguage model is concentrated in layer 11. Interestingly, this stands in contrast with POS linear\nprobing results, which are optimal at earlier layers [38]. As Elazar et al. [10] have noted, probing\ndoes not generally correlate with intervention-based analysis techniques.\n5The softmax probabilities of a multiclass logistic regression classifier can leak the removed information if\nanother classifier is stacked on top of it [33], though this setup is not linear.\n8\n6\nConcept Scrubbing\nAlgorithm 1 Concept scrubbing\nRequire: Model with \u2113 layers f = f\u2113 \u25e6 . . . \u25e6 f1\nRequire: Design matrix X \u2208 Rn\u00d7d\nRequire: Label matrix Z \u2208 Rn\u00d7k\nEnsure: LEACE parameters for each layer in f\n1: H1 \u2190 Embed(X)\n2: L \u2190list()\n3: for l \u2208 1 . . . \u2113 do\n4:\nFit (P, b) on Hl and Z\n5:\nAppend (P, b) to L\n6:\nHl \u2190 P(Hl \u2212 \u00b5Hl) + \u00b5Hl\n(Eq. 1)\n7:\nHl+1 \u2190 fl(Hl)\n8: return L\nUnfortunately, Elazar et al. [10] were forced to\nlimit their interventions to a single layer due\nto the limitations of INLP. INLP often requires\nthe deletion of several dozen dimensions before\nlinear guarding is achieved\u2014as demonstrated\nin Figure 2. Kumar et al. [21] show empiri-\ncally and theoretically that INLP causes need-\nless \u201ccollateral damage\u201d to useful parts of the\nrepresentation that are orthogonal to the concept\nbeing erased. Because of this collateral damage,\nit\u2019s impossible to apply INLP to multiple layers\nof a transformer without causing its outputs to\ncollapse into gibberish.\nInstead, we would like to erase all linear information about a concept in every intermediate rep-\nresentation, which we term concept scrubbing. LEACE makes concept scrubbing possible and\neminently practical. It causes minimal collateral damage, induces little computational overhead, and\nthe covariance statistics it relies on can be computed in a streaming fashion, without ever storing all\nthe hidden states in memory or on disk.\nAlgorithm. Any intervention on the model at layer \u2113 changes the distribution of hidden states at\nlayers \u2113\u2032 > \u2113. Because of this, the naive approach of independently fitting LEACE parameters (P, b)\nfor all layers of the clean model, then applying them all at once, may fail to fully erase the target\nconcept. Instead, we fit LEACE parameters sequentially, starting from the first layer and proceeding\nto the final layer. After we compute (P, b) for a layer, we immediately use them to scrub the hidden\nstates for that layer, then feed these scrubbed representations to the next layer (Algorithm 1).\nLLaMA\nPythia\nCondition\n7B\n13B\n30B\n160M\n1.4B\n6.9B\n12B\nNo intervention\n0.69\n0.66\n0.62\n0.90\n0.70\n0.64\n0.62\nRandom erasure\n0.69\n0.66\n0.62\n0.99\n0.72\n0.66\n0.63\nLEACE\n1.73\n1.84\n1.96\n2.79\n2.25\n3.57\n3.20\nSAL\n3.24\n3.26\n3.16\n3.53\n3.44\n4.17\n4.69\nunigram entropy\n2.90\n2.90\n2.90\n2.66\n2.66\n2.66\n2.66\nTable 1: Perplexity in autoregressive language models when removing linearly available part-of-\nspeech information from the input to each transformer layer. Units are bits per UTF-8 byte. The\nunigram baseline assigns probabilities to tokens based only on their frequency and not on the context.\n6.1\nExperimental Details\nDataset. For each model family, we use a sample from the respective pretraining distribution: the\nvalidation split of the Pile [13] for the Pythia models [2], and the RedPajama replication of the\nLLaMA pretraining corpus for the LLaMA family [39]. sampling a slice of 222 tokens for fitting\nthe LEACE parameters and another slice of 222 tokens for evaluation. Since neither corpus comes\nwith part-of-speech tags, we use the model from the SpaCy library [19] to automatically generate\nUniversal Dependency tags [23].\nBaseline method. We also run concept scrubbing using full-rank SAL [36], which is similar to our\nmethod but lacks a bias term and does not adjust for correlations between features (Appendix D).\nArchitecture. We focus on autoregressive language models. We evaluate our method on EleutherAI\u2019s\nPythia 160M, 1.4B, 6.9B, and 12B models [2], and Meta\u2019s LLaMA 7B, 13B, and 30B [39]. We apply\nconcept erasure to the input of each transformer block, immediately after normalization is applied\n(LayerNorm or RMSNorm).\n9\nRandomized erasure. Almost any intervention on a neural network will cause its performance to\ndegrade to some extent. Following Elazar et al. [10], we isolate the effect of the concept erasure by\ncomparing it to a control condition in which we orthogonally project onto a random linear subspace\nof the same rank as the cross-covariance matrix. To reduce the variance of our results, we sample a\nfresh subspace for each minibatch, and erase that subspace at each layer, reporting the cross-entropy\nloss averaged over subspaces.\nTraining efficiency. Algorithm 1 avoids redundant computation by caching the layer i hidden states\nfor every data point, then using them to run layer i + 1. This approach has the downside of requiring\na large amount of memory or disk space during training (up to 500GB in our experiments). It\u2019s\npossible to avoid caching any hidden states and instead recompute them as needed, at the expense of\nincreasing the total compute cost from O(\u2113) to O(\u21132).\n6.2\nResults\nWe find strong evidence that autoregressive language models heavily rely on linearly encoded\npart-of-speech information. While erasing a randomly selected subspace has little to no effect on\nlanguage modeling performance, scrubbing away part-of-speech information induces a large increase\nin perplexity across all models (Table 1).\nThe specific numbers, however, depend on the erasure method used: SAL induces significantly\nlarger increases in perplexity for all models we tested. We take this to mean that SAL inflicts more\ncollateral damage on other useful features in the representation than LEACE does. In other words,\ninterventions made with LEACE are more surgical than those made with prior work; they more\nclosely approximate the ideal of a perfect intervention which only erases the target concept and keeps\neverything else fixed [40, 15]. If this experiment were conducted with SAL alone, we would have\noverestimated the causal effect of part-of-speech.\n7\nLimitations and Future Work\nMuch work remains to be done to validate concept scrubbing. Specifically, we\u2019d like to see ex-\nperiments that target concepts much narrower than part-of-speech, and use behavioral metrics to\ndetermine whether scrubbing changes the network in the ways we\u2019d intuitively expect. If these\nexperiments succeed, an exciting next step would be the incorporation of concept scrubbing into\nthe pretraining and/or finetuning process. This may make it possible to train deep neural networks\nsubject to conceptual constraints. It remains to be seen if gradient-based optimizers will be able to\n\u201ccircumvent\u201d such constraints by learning completely nonlinear representations of protected attributes.\nIn this work, we focused exclusively on linear concept erasure due to its simplicity and tractability.\nSome authors have proposed nonlinear concept erasure techniques based on kernel methods, but\nhave found that erasure functions fit using one kernel do not generalize well to other kernels [32, 36].\nWe conjecture that it is intractable to nondestructively edit X so as to prevent a general nonlinear\nadversary from recovering Z, unless the data generating process for X is known in detail.6\nA major motivation of concept erasure is that it promises to prevent models from using a concept\nin a post hoc, model-agnostic fashion. But if our concept scrubbing procedure turns out to yield\nunsatisfactory results in practical use cases, the most promising research direction might then be to\nimprove model-specific techniques, such as those that modify the training procedure [8, 9, 14].\n8\nAcknowledgements\nWe are grateful to CoreWeave for providing the compute resources used in Section 6. Shauli Ravfogel\nis grateful to be supported by the Bloomberg Data Science PhD Fellowship.\n6We suspect erasing a concept is at least as hard as extracting it from the original representation. But in the\nworst case, information about Z could be encoded cryptographically in X, which would be intractable to decode\ngiven standard computational complexity assumptions. If the data is generated by a known algorithm, however,\nit may be possible to efficiently eliminate mutual information between Z and X by simply breaking the links in\nthe causal graph that connect them.\n10\nReferences\n[1] UC Berkeley. The Hilbert space of random variables. Lecture Notes Electrical Engineering\n126, 2018. URL https://inst.eecs.berkeley.edu/~ee126/sp18/projection.pdf.\n[2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,\net al. Pythia: A suite for analyzing large language models across training and scaling. arXiv\npreprint arXiv:2304.01373, 2023.\n[3] Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T.\nKalai.\nMan is to computer programmer as woman is to homemaker?\nDe-\nbiasing word embeddings.\nAdvances in Neural Information Processing Systems,\n29:4349\u20134357, 2016.\nURL https://proceedings.neurips.cc/paper/2016/file/\na486cd07e4ac3d270571622f4f316ec5-Paper.pdf.\n[4] Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. Adversarial\ndeep averaging networks for cross-lingual sentiment classification. Transactions of the Associa-\ntion for Computational Linguistics, 6:557\u2013570, 2018. URL https://aclanthology.org/\nQ18-1039.\n[5] Verna Dankers, Christopher Lucas, and Ivan Titov. Can transformer be too compositional?\nAnalysing idiom processing in neural machine translation. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n3608\u20133626, 2022.\n[6] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexan-\ndra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in\nbios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of\nthe Conference on Fairness, Accountability, and Transparency, FAT* \u201919, page 120\u2013128, New\nYork, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi:\n10.1145/3287560.3287572. URL https://doi.org/10.1145/3287560.3287572.\n[7] Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. OSCaR: Orthogonal subspace\ncorrection and rectification of biases in word embeddings. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Processing, pages 5034\u20135050, Online\nand Punta Cana, Dominican Republic, November 2021. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2021.emnlp-main.411. URL https://aclanthology.org/2021.\nemnlp-main.411.\n[8] Harrison Edwards and Amos Storkey. Censoring representations with an adversary. In In-\nternational Conference in Learning Representations, pages 1\u201314, May 2016. URL https:\n//arxiv.org/abs/1511.05897.\n[9] Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 11\u201321, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1002. URL https://aclanthology.org/D18-1002.\n[10] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral\nexplanation with amnesic counterfactuals. Transactions of the Association for Computational\nLinguistics, 9:160\u2013175, 2021. doi: 10.1162/tacl_a_00359. URL https://aclanthology.\norg/2021.tacl-1.10.\n[11] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\nGanguli, Hatfield Zac Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\nChris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,\n2021.\n[12] Thomas S. Ferguson. Mathematical Statistics. Academic Press, Cambridge, MA, 1967.\n11\n[13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:\nAn 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027,\n2020.\n[14] Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah\nGoodman, and Christopher Potts. Inducing causal structure for interpretable neural networks.\nIn International Conference on Machine Learning, pages 7324\u20137338. PMLR, 2022.\n[15] Christopher Grimsley, Elijah Mayfield, and Julia R.S. Bursten. Why attention is not explanation:\nSurgical intervention and causal reasoning about neural models. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference, pages 1780\u20131790, Marseille, France, May\n2020. European Language Resources Association. URL https://aclanthology.org/2020.\nlrec-1.220.\n[16] Pantea Haghighatkhah, Wouter Meulemans, Bettina Speckmann, J\u00e9r\u00f4me Urhausen, and Kevin\nVerbeek. Obstructing classification via projection. In Filippo Bonchi and Simon J. Puglisi,\neditors, 46th International Symposium on Mathematical Foundations of Computer Science,\nLeibniz International Proceedings in Informatics, LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum\nf\u00fcr Informatik, 2021.\n[17] Pantea Haghighatkhah, Antske Fokkens, Pia Sommerauer, Bettina Speckmann, and Kevin\nVerbeek. Better hit the nail on the head than beat around the bush: Removing protected\nattributes with a single projection. pages 8395\u20138416, December 2022. doi: 10.18653/v1/2022.\nemnlp-main.575. URL https://aclanthology.org/2022.emnlp-main.575.\n[18] Evan Hernandez and Jacob Andreas. The low-dimensional linear geometry of contextualized\nword representations.\nIn Proceedings of the 25th Conference on Computational Natural\nLanguage Learning, pages 82\u201393, 2021.\n[19] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-\nstrength Natural Language Processing in Python, 2020.\n[20] Matth\u00e4us Kleindessner, Michele Donini, Chris Russell, and Muhammad Bilal Zafar. Efficient\nfair PCA for fair representation learning. In International Conference on Artificial Intelligence\nand Statistics, pages 5250\u20135270. PMLR, 2023.\n[21] Abhinav Kumar, Chenhao Tan, and Amit Sharma. Probing classifiers are unreliable for concept\nremoval and detection. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems, volume 35, pages 17994\u201318008.\nCurran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/\npaper/2022/file/725f5e8036cc08adeba4a7c3bcbc6f2c-Paper-Conference.pdf.\n[22] Matt J. Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness.\nAdvances in Neural Information Processing Systems, 30, 2017.\n[23] Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das,\nKuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T\u00e4ckstr\u00f6m, et al. Universal\ndependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers), pages 92\u201397, 2013.\n[24] Neel Nanda. Actually, Othello-GPT has a linear emergent world model, Mar 2023. URL\n<https://neelnanda.io/mechanistic-interpretability/othello>.\n[25] Vassilina Nikoulina, Maxat Tezekbayev, Nuradil Kozhakhmet, Madina Babazhanova, Matthias\nGall\u00e9, and Zhenisbek Assylbekov. The rediscovery hypothesis: Language models need to meet\nlinguistics. Journal of Artificial Intelligence Research, 72:1343\u20131384, 2021.\n[26] Hamed Nilforoshan, Johann D. Gaebler, Ravi Shroff, and Sharad Goel. Causal conceptions\nof fairness and their consequences. In International Conference on Machine Learning, pages\n16848\u201316887. PMLR, 2022.\n12\n[27] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D. Manning,\nSampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies\nv2: An evergrowing multilingual treebank collection. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 4034\u20134043, 2020.\n[28] Judea Pearl. Causality. Cambridge University Press, Cambridge, UK, 2 edition, 2009. ISBN\n978-0-521-89560-6. doi: 10.1017/CBO9780511803161.\n[29] Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out:\nGuarding protected attributes by iterative nullspace projection. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages 7237\u20137256, Online,\nJuly 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647.\nURL https://aclanthology.org/2020.acl-main.647.\n[30] Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions\nreveal the causal effect of relative clause representations on agreement prediction. In Proceedings\nof the 25th Conference on Computational Natural Language Learning, pages 194\u2013209, 2021.\n[31] Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D Cotterell. Linear adversarial\nconcept erasure. In International Conference on Machine Learning, pages 18400\u201318421. PMLR,\n2022.\n[32] Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Adversarial concept\nerasure in kernel space. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 6034\u20136055, 2022.\n[33] Shauli Ravfogel, Yoav Goldberg, and Ryan Cotterell. Log-linear guardedness and its im-\nplications.\nIn Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages 9413\u20139431, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.523.\nURL\nhttps://aclanthology.org/2023.acl-long.523.\n[34] Bashir Sadeghi and Vishnu Boddeti. On the fundamental trade-offs in learning invariant\nrepresentations. arXiv preprint arXiv:2109.03386, 2021. URL https://openreview.net/\npdf?id=KOk7mUGspN9.\n[35] Bashir Sadeghi, Runyi Yu, and Vishnu Boddeti. On the global optima of kernelized adversarial\nrepresentation learning. In 2019 IEEE/CVF International Conference on Computer Vision,\npages 7970\u20137978. IEEE, 2019. URL http://hal.cse.msu.edu/assets/pdfs/papers/\n2019-iccv-kernel-adversarial-representation-learning.pdf.\n[36] Shun Shao, Yftah Ziser, and Shay B. Cohen. Gold doesn\u2019t always glitter: Spectral removal\nof linear and nonlinear guarded attribute information. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Computational Linguistics, pages 1611\u20131622,\nDubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https:\n//aclanthology.org/2023.eacl-main.118.\n[37] Shun Shao, Yftah Ziser, and Shay B. Cohen. Erasure of unaligned attributes from neural\nrepresentations. Transactions of the Association for Computational Linguistics, 11:488\u2013510,\n2023. doi: 10.1162/tacl_a_00558. URL https://aclanthology.org/2023.tacl-1.29.\n[38] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 4593\u20134601, 2019.\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models, 2023. URL http://arxiv.org/abs/2302.13971.\n[40] James Francis Woodward. Making Things Happen: A Theory of Causal Explanation explanation.\nOxford University Press, 2005.\n13\n[41] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah Goodman. Interpretability at scale:\nIdentifying causal mechanisms in Alpaca. 2023.\n[42] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance\nthrough adversarial feature learning. In Advances in Neural Information Processing Systems,\nvolume 30, pages 585\u2013596, 2017. URL https://dl.acm.org/doi/10.5555/3294771.\n3294827.\n[43] Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable\ninformation under computational constraints. In 8th International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=r1eBeyHFDH.\n[44] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with\nadversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and\nSociety, page 335\u2013340, New York, NY, USA, 2018. Association for Computing Machinery.\nISBN 9781450360128. URL https://doi.org/10.1145/3278721.3278779.\n14\nA\nAdditional Related Work\nThe problem of linear concept erasure is an instance of the general problem of information removal.\nInformation removal methods generally divide into adversarial methods, which are applied during\ntraining, and the post-hoc linear methods considered in this paper. Adversarial methods [8, 42, 4,\n9, 44] use a gradient-reversal layer during training to induce representations that do not encode\nthe protected attribute. However, Elazar and Goldberg [9] have shown that these methods fail in\nexhaustively removing all the information associated with the protected attribute: it is often possible\nto train new adversaries that successfully recover the removed information. Linear methods have been\nproposed as a tractable alternative, where one identifies a linear subspace that captures the concept\nof interest, and neutralizes it using algebraic techniques. Different methods have been proposed for\nthe identification of the subspace, e.g. PCA and variants thereof [3, 20], orthogonal-rotation [7],\nclassification-based [29], spectral [36, 37] and adversarial approaches [31].\nFew works theoretically characterize the condition of linear guardedness. Haghighatkhah et al. [16]\nextensively analyzed the problem of preventing linear classification, with the focus on decreasing\naccuracy. They provide a constructive proof of an optimal intervention for an SVM classifier.\nRavfogel et al. [33] have proposed a formal definition of linear guardedness based on V information,\nand characterized the fairness implications of guardedness; we show the relations with our definition\nabove. Ravfogel et al. [31] provide an adversarial formulation of the problem, derive a closed-formed\nsolution to certain cases, and propose an SGD-based optimization for others. While they seek an\northogonal projection, we empirically showed that their solution is very close to ours. Sadeghi et al.\n[35] and Sadeghi and Boddeti [34] both study an adversarial formulation of concept erasure for linear\nregression, and they trade-off with main-task performance. In contrast to Ravfogel et al. [31], they\nconsider a general linear adversary, i.e. not necessarily a projection matrix. Closest to our work are\nKleindessner et al. [20], Haghighatkhah et al. [17], Shao et al. [36]. As we showed above (\u00a7 4), those\nmethods do achieve the goal of linear guardedness though they are unable to prove this fact. At the\nsame time, they are not optimal in terms of damage to the original representation space.\nB\nEquivalence of Guardedness with the Optimality of Constant Predictors\nThe following two theorems establish the equivalence of conditions 1 and 2 (indeed, they do so in the\ngeneral setting, with no assumption of convex loss or linear predictors).\nTheorem B.1. Suppose X (V, L)-guards Z.\nThen for every loss L \u2208 L, the corresponding\ntrivially attainable loss L(Z,L)\n\u03c4\ncannot be improved upon by any predictor \u03b7(\u00b7; \u03b8) \u2208 V, i.e.\nL\u03c4 = inf\u03b8 E[L(\u03b7(X; \u03b8), Z)].\nProof. Consider the null random vector X\u2032(\u03c9) = 0. Since all predictors are constant on X\u2032, and the\ntrivially attainable loss gives the best available expected loss among constant predictors, we must\nhave:\nL\u03c4 = inf\n\u03b8 E[L(\u03b7(X\u2032; \u03b8), Z)]\n(4)\nThe right side of equation (4) is the best possible loss achievable by a function \u03b7(\u00b7; \u03b8) on the joint\ndistribution of (X\u2032, Z), which by the definition of guardedness is upper bounded by the best possible\nloss achievable on the joint distribution of (X, Z):\ninf\n\u03b8 E[L(\u03b7(X\u2032; \u03b8), Z)] \u2264 inf\n\u03b8 E[L(\u03b7(X; \u03b8), Z)]\n(5)\nCombining equations (4) and (5), and the fact that all constant functions exist in our function class\nV = {\u03b7(\u00b7; \u03b8)}, we arrive at our desired result:\nL\u03c4 = inf\n\u03b8 E[L(\u03b7(X; \u03b8), Z)]\nTheorem B.2. Suppose that for every loss L \u2208 L, the corresponding trivially attainable loss L(Z,L)\n\u03c4\ncannot be improved upon by any predictor \u03b7(\u00b7; \u03b8) \u2208 V, i.e. L\u03c4 = inf\u03b8 E[L(\u03b7(X; \u03b8), Z)]. Then X\n(V, L)-guards Z.\n15\nProof. Let X\u2032 : \u2126 \u2192 Rd be any other random data vector with finite first moment.\nSince all constant predictors exist in our predictor class V = {\u03b7(\u00b7; \u03b8)}, the best loss achievable on\n(X\u2032, Z) by functions in V must be at least as good as the trivially attainable loss (the best loss available\nby such constant predictors):\ninf\n\u03b8 E[L(\u03b7(X\u2032; \u03b8), Z)] \u2264 L\u03c4\nBy assumption, the trivially attainable loss cannot be improved upon over (X, Z) by predictors in V:\nL\u03c4 = inf\n\u03b8 E[L(\u03b7(X; \u03b8), Z)]\nSince our choice of X\u2032 was arbitrary, this shows that X maximizes the minimal achievable loss, so X\n(V, L)-guards Z.\nC\nLinear Guardedness is Equivalent to Linear Statistical Parity\nTo measure the effect of linear guardedness on main-task classifiers, we use the following minimal\ndefinition of \u201cfairness\u201d with respect to an attribute, adapted from Edwards and Storkey [8].\nDefinition C.1 (Statistical Parity). Let X and Z be defined as above, and let f be a function with\ndomain Rd. Then f exhibits statistical parity with respect to Z when evaluated on X if\n\u2200z \u2208 Z : E[f(X)|Z = z] = E[f(X)].\nWe now prove the equivalence of conditions 3 and 5.\nTheorem C.2. Let X and Z be defined as above. Then every linear predictor f(x) = b + Wx\nexhibits statistical parity w.r.t. Z when evaluated on X if and only if each class-conditional mean\nE\n\u0002\nX|Z = z\n\u0003\nis equal to E\n\u0002\nX\n\u0003\n.\nProof. Suppose each class-conditional mean E\n\u0002\nX|Z = z\n\u0003\nis equal to E\n\u0002\nX\n\u0003\n. Then by the linearity of\nexpectation, we have for all z \u2208 Z:\nE[f(X)|Z = z] = E[WX + b|Z = z] = WE[X|Z = z] + b = WE[X] + b = E[f(X)].\nThis matches the definition of statistical parity provided in Definition C.1.\nConversely, suppose every linear predictor f(x) = b + Wx exhibits statistical parity w.r.t. Z when\nevaluated on X. Then this holds for the identity function id(x) = x, and thus for all z \u2208 Z:\nE[X|Z = z] = E[id(X)|Z = z] = E[id(X)] = E[X].\nD\nImplications for Prior Work\nIn this section we discuss the implications of Theorem 4.1, which characterizes the necessary and\nsufficient conditions for an affine erasure function to yield a perfectly linearly guarded dataset, for\nmethods proposed in prior work.\nSpectral Attribute RemovaL (SAL) [36] uses the top n left singular vectors of \u03a3XZ to construct an\northogonal projection matrix QSAL = I \u2212 U:nUT\n:n which is then applied to X. Notably, while n is\npresented as a free parameter in their method, all of their experiments involve binary classification\nproblems where Z is a one-hot vector, and n is set to a value no greater than 2. We\u2019ll call the version\nof SAL where n = rank(\u03a3XZ), \u201cfull-rank SAL.\u201d Since these left singular vectors are an orthonormal\nbasis for \u03a3XZ\u2019s column space, Theorem 4.1 implies that full-rank SAL guarantees linear guardedness.\nMean Projection (MP) [17] orthogonally projects X onto the orthogonal complement of the span of\nthe difference in class centroids E[X|Z = 1] \u2212 E[X|Z = 0], where Z is assumed to be binary. Since\nthe centroids are equal after the projection, this method guarantees linear guardedness by Theorem 3.1.\nIn fact, by Theorem 3.4, MP is mathematically equivalent to SAL when Z is a one-dimensional\nrandom vector taking one of two possible values.\n16\nE\nDerivation of LEACE\nTheorem 4.2. Let X and Z be centered random vectors taking values in Rd and Rk respectively, each\nof finite second moment. Let M \u2208 Rd\u00d7d be a p.s.d. matrix defining a (possibly degenerate) inner\nproduct on Rd: \u27e8x, y\u27e9M = xT My. Let \u03a3XX \u2208 Rd\u00d7d be X\u2019s covariance matrix, and \u03a3XZ \u2208 Rd\u00d7k\nbe the cross-covariance matrix of X and Z. Let A+ denote the Moore-Penrose pseudoinverse of a\nmatrix A, and let A1/2 be the p.s.d. square root of a p.s.d. matrix A. Then the objective\nargmin\nP\u2208Rd\u00d7d E\nh\r\rPX \u2212 X\n\r\r2\nM\ni\nsubject to Cov(PX, Z) = 0\nhas the following solution:\nP\u2217 = I \u2212 W+PW\u03a3XZW,\nwhere W is the whitening transformation (\u03a31/2\nXX)+ and PW\u03a3XZ = (W\u03a3XZ)(W\u03a3XZ)+ is the\northogonal projection matrix onto colsp(W\u03a3XZ).\nBelow are two independent proofs of Theorem 4.2.\nE.1\nAlgebraic Proof\nProof. We shall first show that, in any orthonormal basis,7 each row Pi constitutes an independent\noptimization problem, and then select a basis in which we can easily show that the corresponding\ncomponent Xi of X can be almost surely decomposed into a linear combination of mutually un-\ncorrelated components in the whitened random vector WX, some of which correlate with Z and\nsome of which do not. The solution (PX)i is then that same linear combination, restricted to those\ncomponents which do not correlate with Z.\nConsider first an orthonormal basis diagonalizing the inner product M, so that \u27e8x, y\u27e9M =\nPd\ni=1 \u03b1ixiyi for fixed \u03b11, . . . , \u03b1d \u2265 0. This allows us to treat each row Pi \u2208 Rd of P as a\nseparate optimization problem,\nargmin\nPi\u2208Rd E\nh\n\u03b1i\n\u0000Pi\nT X \u2212 Xi\n\u00012i\nsubject to Cov(Pi\nT X, Z) = 0,\nat which point the weights \u03b1i of each subproblem become irrelevant, and our objective may as well\nbe Euclidean, allowing us to view each row as an independent optimization problem not just in this\nbasis, but from any convenient one.\nSo now let \u2113 = rank(\u03a3XZ) = rank(\u03a3WX,Z) and m = rank(\u03a3XX) = rank(\u03a3WX,WX), and\nconsider a (new) orthonormal basis whose first m coordinates span the column (and row) space of\nW (i.e. the subspace of Rd in which X and WX have nonzero variance), and whose first \u2113 \u2264 m\ncoordinates span the column space of \u03a3WX,Z (i.e. the subspace of Rd in which WX has nonzero\ncovariance with Z).\nAny component of X can be (almost surely) written as a fixed linear combination of the nontrivial\ncomponents of its whitening WX:\nXi = (W+WX)i =\nm\nX\nj=1\nW +\nij (WX)j.\n(almost surely)\nMeanwhile, any component of PX can be (always) written as a fixed linear combination of the\nnontrivial components of WX and the almost surely zero components of X:\n(PX)i =\nm\nX\nj=1\nAij(WX)j +\nd\nX\nj=m+1\nBijXj,\ni.e. P = AW + BV, where V = I \u2212 W+W is the orthogonal projection onto X\u2019s almost surely\nzero components.\n7Throughout this proof, we abuse the notations Xi, Pi, etc. to refer to the ith component in the specified\nbasis, not necessarily the standard one.\n17\nThe ith sub-objective is then:\nE\n\u0000Pi\nT X \u2212 Xi\n\u00012 = E\n\" m\nX\nj=1\n(Aij \u2212 W +\nij )(WX)j\n#2\n=\nm\nX\nj=1\n(Aij \u2212 W +\nij )2,\nwhere we have safely ignored the almost surely zero terms BijXj (j > m), and used the fact that the\nfirst m components of WX have identity covariance matrix.\nPX is almost surely equal to AWX, so our constraint Cov(PX, Z) = 0 is equivalent to A\u03a3WX,Z =\nCov(AWX, Z) = 0, i.e. Aij = 0 when j \u2264 \u2113, since the first \u2113 components are those for which\nWX correlates with Z. Subject to this, the objective is minimized for Aij = W +\nij when j > \u2113, i.e.\nA = W+(I \u2212 PW\u03a3XZ).\nThe particular choice B = I gives our solution P\u2217 = I \u2212 W+PW\u03a3XZW, leaving the non-varying\ncomponents of X intact (see Fig. 1 for a visualization).\nThe solution is unique except for columns corresponding to the components of X with zero variance,\nand rows corresponding to the zero-weighted components of the (pseudo) inner product M.\nE.2\nCovector Proof\nProof. We assume without loss of generality that vectors in Rd are represented in a basis diagonalizing\nthe inner product M, so that \u27e8x, y\u27e9M = Pd\ni=1 mixiyi for fixed m1, . . . , md \u2265 0. This allows us to\ntreat each row Pi \u2208 Rd of P as a separate optimization problem,\nargmin\nPi\u2208Rd E\nh\nmi\n\u0000Pi\nT X \u2212 Xi\n\u00012i\nsubject to Cov(Pi\nT X, Z) = 0.\nOur objective only depends on Pi through its effect on the scalar random variable \u03be = Pi\nT X. All\nrandom variables8 of the form \u03b6 = uT\n\u03b6 X for some covector uT\n\u03b6 \u2208 Rd form a vector space U, which\nwe equip with the covariance inner product \u27e8\u03be, \u03b6\u27e9Cov = Cov(\u03be, \u03b6) = E[\u03be\u03b6] = uT\n\u03be \u03a3XXu\u03b6.\nBy the linearity of covariance, the elements of U uncorrelated with Z form a subspace Z\u22a5 \u2286 U.\nNote also that \u03be \u2208 Z\u22a5 if and only if \u03be\u2019s covector uT\n\u03be satisfies Cov(uT\n\u03be X, Z) = uT\n\u03be \u03a3XZ = 0k, and\nthat these covectors themselves form the subspace colsp(\u03a3XZ)\u22a5 of Rd.\nOur objective now reduces to finding a covector PT\ni that defines the orthogonal projection of Xi onto\nZ\u22a5. The difficulty is that orthogonality of elements in U is not equivalent to orthogonality of the\ncorresponding covectors. We can fix this by changing the basis in which covectors are represented.\nSince X \u2208 colsp(W) a.s., we can write any element of U as a linear form in WX rather than X by\napplying the change-of-basis u\u2032\n\u03be = W+u\u03be to every covector: \u03be = (u\u2032\n\u03be)T WX = uT\n\u03be \u0018\u0018\u0018\u0018\nW+WX a.s.\nIn this new basis, which is orthonormal under our covariance inner product, each component of X is\nwritten Xi = (W+)T\ni WX and the inner product of any two elements of U is simply the Euclidean\ninner product of the corresponding covectors:9\n\u27e8\u03be, \u03b6\u27e9Cov = Cov(u\u2032T\n\u03be WX, u\u2032T\n\u03b6 WX) = u\u2032T\n\u03be (((((\nW\u03a3XXWu\u2032\n\u03b6 = u\u2032T\n\u03be u\u2032\n\u03b6.\nSince the two inner products are now equivalent, and Z\u22a5 is precisely those random variables with\ncovector u\u2032 \u2208 colsp(W\u03a3XZ)\u22a5, the orthogonal projection of Xi onto Z\u22a5 is also an orthogonal\nprojection of its covector (W+)T\ni onto colsp(W\u03a3XZ)\u22a5:\n\u02c6Xi = (W+)T\ni (I \u2212 PW\u03a3XZ)(WX)\n(6)\nPutting all the components of X together, we have our final solution,\n\u02c6X = (I \u2212 W+PW\u03a3XZW)X,\nwhich is almost surely equivalent to Eq. 6, but keeps the non-varying components of X intact.\n8Strictly speaking, equivalence classes of almost surely equal random variables.\n9If \u03a3XX is full rank, there is a one-to-one correspondence between random variables in U and covectors. In\nthe singular case, we may choose the component of the covector inside ker(\u03a3XX) arbitrarily, since it will make\nno difference to the inner product.\n18\nF\nThe Optimality of Oblique Projections\nAs noted in subsection 4.2, the optimal affine erasure function r(x) = b + Px does not in general\nuse an orthogonal projection for the matrix P. A simple example illustrates why. Let d = 2, k = 1\nso that X takes values in R2 and Z takes values in R, with the first feature X1 and the label Z each\nindependently and uniformly distributed in {\u22121, +1}, and the second feature X2 simply equal to the\nsum X2 = X1 + Z. A dataset reflecting such a distribution has four (x, y) pairs:\n([1, 2]T , 1),\n([1, 0]T , \u22121),\n([\u22121, 0]T , 1),\n([\u22121, \u22122]T , \u22121)\nIn this case, all of the information X has about Z resides in X2, so the minimally disruptive orthogonal\nprojection which guards Z will nullify that component:\nPortho =\n\u0014\n1\n0\n0\n0\n\u0015\nOn the other hand, X1 contains some information about X2 (despite having no information about Z),\nallowing a partial reconstruction of X2 while preserving full concept erasure:\nPoblique =\n\u0014\n1\n0\n1\n0\n\u0015\nBoth methods fully erase the ability to predict Z from the data, however a simple calculation shows\nthe second, oblique method to perform better as measured by mean squared edit distance:\nE\u2225PorthoX \u2212 X\u22252 = 2,\nE\u2225PobliqueX \u2212 X\u22252 = 1\nG\nEquivalence of Guardedness Definitions\nXu et al. [43] define the conditional V-entropy of Z given X as the lowest achievable cross-entropy\nloss predicting Z with a function of X in the predictor class V. In our notation:\nHV(Z | X) = inf\n\u03b8\u2208\u0398 E[L(\u03b7(X; \u03b8), Z)],\nwhere L(\u03b7, z) = \u2212 log\nexp(\u03b7z)\nPk\ni=1 exp(\u03b7i) is the cross-entropy loss function.\nThey then define the (unconditional) V-entropy HV(Z) = HV(Z | 0) to be the lowest achievable\ncross-entropy loss in the case of a constantly null random data variable. This is exactly our trivially\nattainable loss L\u03c4 (Definition 2.2).\nFinally, they define the V-information from X to Z as the reduction in V-entropy as compared to\nusing such a null random data variable:\nIV(X \u2192 Z) = HV(Z) \u2212 HV(Z | X).\nUsing these notions, Ravfogel et al. [33] say that X is \u03f5-guarded with respect to V if IV(X \u2192 Z) < \u03f5.\nIn Appendix B, we showed the equivalence of guardedness (as we have defined it in Definition 2.1)\nto the optimality of the trivially attainable loss. That is, X (V, L)-guards Z when HV(Z | X) = L\u03c4 =\nHV(Z), in the case where L is the singleton class consisting solely of the cross-entropy loss function.\nIn the language of [33], X is \u03f5-guarded with respect to V for all \u03f5 > 0.\nH\nConstraining Norm Growth\nIn early concept scrubbing experiments (Sec. 6), we found that at specific layers in some models,\nconcept scrubbing with LEACE would cause the norm of the representation to diverge, leading to\nNaN outputs. By contrast, SAL never caused divergence, even though it causes a larger disruption\nto model performance on average (Table 1). This is because SAL uses an orthogonal projection\nQ, whose eigenvalues are thus all in {0, 1}, so the norm of the hidden state can never increase\nafter erasure, while LEACE\u2019s oblique projection matrix P does generally have singular values\ngreater than 1. To combine the superior average-case MSE of LEACE with the stability of SAL,\nwe adopt a simple regularization heuristic. After constructing P, we analytically compute the\n19\ntrace of the covariance matrix of the hidden states after applying P. If tr(P\u03a3XXPT) > tr(\u03a3XX),\nwe solve a quadratic equation to find the convex combination P\u2032 = \u03b1P + (1 \u2212 \u03b1)Q such that\ntr(\u03a3XX) = tr(P\u2032\u03a3XX(P\u2032)T). By Theorem 4.1, the set of matrices which ensure linear guardedness\nis convex,10 so P\u2032 is guaranteed to be in the feasible set. Furthermore, since our mean squared\nerror objective is convex, P\u2032 is guaranteed to have no worse MSE than Q. We find this solves the\ndivergence issue in practice.\nI\nOracle LEACE\ni\nZ1\nZ2\ni\nZ\nZ\nspan\nproj  X\nX\nFigure 5: Orthogonal projection of ith component\nof X, itself a vector in the random variable Hilbert\nspace H, onto the span of the components of Z.\nThe residual Xi \u2212 projZXi is the closest vector\nto Xi orthogonal to, and hence uncorrelated with,\nZ = span({Z1, Z2}).\nThe concept erasure method derived in Section 4\ndoes not require access to concept labels at infer-\nence time. That is, we can fit an erasure function\non a labeled training dataset, then apply the func-\ntion to unlabeled datapoints. If we have oracle\naccess to the label z for each x, we can achieve\nan even more surgical edit. In Theorem I.1 be-\nlow, we derive Oracle LEACE, a closed-form\nformula for the the nearest X\u2032 to any X such that\nCov(X\u2032, Z) = 0.\nLike in Sec. 4, the resulting X\u2032\nLEACE is \u201cnearest\u201d\nto X with respect to all p.s.d. inner products\naT Mb defined on Rd simultaneously. This is\nbecause, by expressing X in a basis that diago-\nnalizes M, we can decompose the problem into\nd independent subproblems, one for each com-\nponent of Xi. Each subproblem can then be\nviewed as an orthogonal projection, not in Rd,\nbut in an abstract vector space of real-valued random variables. For geometric intuition, see Figure 5.\nPrior work has noted that computing an orthogonal projection in a random variable Hilbert space\nis equivalent to solving an ordinary least squares regression problem [1]. Our theorem is a natural\nextension of this work: we find that X\u2032\nLEACE is equal to the OLS residual from regressing X on Z,\nplus a constant shift needed to ensure that erasing Z does not change the mean of X.\nTheorem I.1 (Oracle Concept Erasure). Let H be the Hilbert space of square-integrable real-valued\nrandom variables equipped with the inner product \u27e8\u03be, \u03b6\u27e9H := E[\u03be\u03b6]. Let (X, Z) be random vectors in\nHd and Hk respectively. Then for every p.s.d. inner product \u27e8a, b\u27e9M = aT Mb on Rd, the objective\nargmin\nX\u2032\u2208Hd E\n\r\rX\u2032 \u2212 X\n\r\r2\nM\nsubject to Cov(X\u2032, Z) = 0\nis minimized by the (appropriately shifted) ordinary least squares residuals from regressing X on Z:\nX\u2032\nLEACE = X \u2212 \u03a3XZ\u03a3+\nZZ\n\u0000Z \u2212 E[Z]\n\u0001\n.\nProof. Assume w.l.o.g. that X and X\u2032 are represented in a basis diagonalizing M, so we may write\nE\n\r\rX\u2032 \u2212 X\n\r\r2\nM =\nd\nX\ni=1\nmi E\n\u0002\n(X\u2032\ni \u2212 Xi)2\u0003\n,\nwhere m1, . . . , md \u2265 0 are eigenvalues of M. Crucially, each term in this sum is independent from\nthe others, allowing us to decompose the primal problem into d separate subproblems of the form\n\u2225X\u2032\ni \u2212 Xi\u22252\nH, one for each component i of (X, X\u2032).\nFactoring out constants.\nNow consider the subspace C = span(1) \u2282 H consisting of all constant\n(i.e. zero variance) random variables. Orthogonally decomposing Xi along C yields Xi = \u02dcXi + \u00b5i,\nwhere \u00b5i = E[Xi] \u2208 C and \u02dcXi = X \u2212 E[X]i \u2208 C\u22a5, and likewise for X\u2032\ni. Our objective is now\n\r\rX\u2032\ni \u2212 Xi\n\r\r2\nH =\n\r\r\u00b5\u2032\ni \u2212 \u00b5i\n\r\r2\nH +\n\r\r\u02dcX\u2032\ni \u2212 \u02dcXi\n\r\r2\nH.\n(7)\n10In fact, it is a subspace of Rd\u00d7d. For any matrices A, B \u2208 Rd\u00d7d such that A\u03a3XZ = 0 and B\u03a3XZ = 0,\nwe have by linearity (\u03b1A + \u03b2B)\u03a3XZ = \u03b1A\u03a3XZ + \u03b2B\u03a3XZ = \u03b10 + \u03b20 = 0 for any scalars \u03b1 and \u03b2.\n20\nSince \u00b5\u2032\ni and \u00b5i are orthogonal to \u02dcX\u2032\ni and \u02dcXi, and the constraint Cov(X\u2032, Z) = 0 is invariant to\nconstant shifts, we can optimize the two terms in Eq. 7 independently. The first term is trivial: it is\nminimized when \u00b5\u2032\ni = \u00b5i, and hence X\u2032\ni = \u02dcX\u2032\ni + E[Xi].\nOrthogonal projection.\nWe can now rewrite the zero covariance condition as an orthogonality\nconstraint on \u02dcXi. Specifically, for every i \u2208 {1 . . . d} we have\nargmin\n\u02dcX\u2032\ni\u2208H\n\r\r\u02dcX\u2032\ni \u2212 \u02dcXi\n\r\r2\nH\ns.t. \u2200j \u2208 {1 . . . k} : \u27e8\u02dcX\u2032\ni, \u02dcZj\u27e9H = 0,\n(8)\nwhere \u02dcZ = Z \u2212 E[Z].\nIn other words, we seek the nearest \u02dcX\u2032\ni to \u02dcXi orthogonal to Z =\nspan({\u02dcZ1, . . . , \u02dcZk}), which is simply the orthogonal projection of \u02dcXi onto Z\u22a5. This in turn is\nequal to the ordinary least squares residual from regressing \u02dcX on \u02dcZ:\n\u02dcX\u2032\ni = \u02dcXi \u2212 proj\n\u0000\u02dcXi, Z\n\u0001\n= Xi \u2212 (\u03a3XZ)i\u03a3+\nZZ(Z \u2212 E[Z]) \u2212 E[Xi].\n(9)\nPutting it all together.\nPlugging Eq. 9 into X\u2032\ni = \u02dcX\u2032\ni + E[Xi] and combining all components into\nvector form yields\nX\u2032\nLEACE = X \u2212 \u03a3XZ\u03a3+\nZZ(Z \u2212 E[Z]),\n(10)\nwhich completes the proof.\nJ\nNotation Key\nZ\nThe space of one-hot labels {(z1, . . . zk) \u2208 {0, 1}k \f\f Pk\nj=1 zj = 1}}\n(treated interchangeably with the integers {1, . . . , k} when convenient).\nX, Z\nIntegrable (i.e. finite first moment) random vectors taking values in Rd and Rk\nrespectively (or their realized values inside an expectation, e.g. in E[f(X)]).\nZ is sometimes restricted to the one-hot labels Z, in which case we assume\neach P(Z = j) > 0.\nXi, Zj\nThe ith and jth components thereof, themselves scalar random variables (or\ntheir realized values inside an expectation).\n\u03be, \u03b6\nScalar random variables taking values in R.\n\u03b7\nA predictor function Rd \u2192 Z (or its value \u03b7(X) when inside an expectation).\nV\nA space of predictor functions {\u03b7(\u00b7; \u03b8) : Rd \u2192 Rk | \u03b8 \u2208 \u0398}, parameterized\nby \u03b8 and containing all constant functions.\nL\nA space of loss functions {L : Rk \u00d7 Z \u2192 [0, \u221e)}.\nr\nAn erasure function Rd \u2192 Rd, hopefully making a minimal edit to X that\neliminates the ability to predict labels Z with predictors in V.\nA\nA matrix with entries in R.\nAij\nThe entry thereof at the ith row and jth column.\nA+\nThe Moore-Penrose pseudoinverse of A.\nv\nA column vector with entries in R.\nvi\nThe ith component thereof.\n21\n"
  }
]