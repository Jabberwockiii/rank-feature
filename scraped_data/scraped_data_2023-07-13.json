[
  {
    "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution",
    "link": "https://arxiv.org/pdf/2307.06304.pdf",
    "upvote": "24",
    "text": "Patch n\u2019 Pack: NaViT, a Vision Transformer\nfor any Aspect Ratio and Resolution\nMostafa Dehghani\u2217\nBasil Mustafa\u2217\nJosip Djolonga\u2020\nJonathan Heek\u2020\nMatthias Minderer\nMathilde Caron\nAndreas Steiner\nJoan Puigcerver\nRobert Geirhos\nIbrahim Alabdulmohsin\nAvital Oliver\nPiotr Padlewski\nAlexey Gritsenko\nMario Lu\u010di\u0107\nNeil Houlsby\u2217\nGoogle DeepMind\nAbstract\nThe ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before\nprocessing them with computer vision models has not yet been successfully challenged. However, models\nsuch as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input\nsequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence\npacking during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model\nusage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text\npretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification,\nobject detection, and semantic segmentation and leads to improved results on robustness and fairness\nbenchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time\ncost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed,\ninput and modelling pipeline used by most computer vision models, and represents a promising direction\nfor ViTs.\n1\nIntroduction\nThe simple, flexible and scalable nature of the Vision Transformer (ViT) (Dosovitskiy et al., 2021) has\nrendered it an almost ubiquitous replacement to convolution based neural networks. Underpinning this\nmodel is a simple operation: splitting an image into patches, each of which is linearly projected to a token.\nTypically, input images are resized to a fixed square aspect ratio and then split into a fixed number of patches.\nRecent works have explored alternatives to this paradigm: FlexiViT (Beyer et al., 2023) supports multiple\npatch sizes within one architecture, enabling smooth variation of sequence length and thus compute cost.\nThis is achieved via random sampling of a patch size at each training step and a resizing algorithm to allow\nthe initial convolutional embedding to support multiple patch sizes. Pix2Struct (Lee et al., 2022) introduced\nan alternative patching approach which preserves the aspect ratio, which is particularly useful for tasks such\nas chart and document understanding.\nWe present an alternative, NaViT. Multiple patches from different images are packed in a single sequence\u2014\ntermed Patch n\u2019 Pack\u2014which enables variable resolution while preserving the aspect ratio (Figure 2). This\nis inspired by example packing in natural language processing, where multiple examples are packed into a\nsingle sequence to accommodate efficient training on variable length inputs.\nWe demonstrate that: (i) Randomly sampling resolutions at training time significantly reduces training cost.\n(ii) NaViT results in high performance across a wide range of resolutions, enabling smooth cost-performance\ntrade-off at inference time, and can be adapted with less cost to new tasks, (iii) Fixed batch shapes enabled\nby example packing lead to new research ideas, such as aspect-ratio preserving resolution-sampling, variable\ntoken dropping rates, and adaptive computation.\n\u2217 Project lead. \u2020Core contributor.\n1\narXiv:2307.06304v1  [cs.CV]  12 Jul 2023\n1012\n1013\nPre-training TPU chips hours\n60%\n65%\n70%\n75%\n80%\nAccuracy\nPre-training linear eval\n1012\n1013\nPre-training TPU chips hours\n80%\n82%\n84%\n86%\n88%\nFinetuned acc vs. pretrain cost\n103\n104\nInference cost (ms/image)\n0%\n20%\n40%\n60%\n80%\nFinetuned acc vs. inference cost\nModel\nViT\nNaViT\nVersion\nB/32\nB/16\nL/16\nFigure 1: NaViT offers notable computational efficiency during pre-training (left) which carries over to downstream\nfine-tuning (middle). A single NaViT can be applied successfully to multiple resolutions (right), smoothly trading off\nperformance and inference cost.\nImage 1\nImage 2\nImage 3\nPad\nquery\nkey\nEach token's receptive field is restricted to the tokens within the same example\nSelf-Attention\nPooling Representations\nPad tokens are masked out \nfrom attention\nPad tokens are masked out \nfrom pooling\nPad examples are masked out \nfrom loss computation\nImage1          Image2              Image2                        Image1          Image2              Image2                Image1          Image2              Image2\nInputs\nPatchify\nToken drop\nData preprocessing\nPacked \nSequence\nFigure 2: Example packing en-\nables variable resolution images\nwith preserved aspect ratio, re-\nducing training time, improving\nperformance and increasing flex-\nibility. We show here the aspects\nof the data preprocessing and\nmodelling that need to be mod-\nified to support Patch n\u2019 Pack.\nThe position-wise operations in\nthe network, such as MLPs,\nresidual connections, and layer\nnormalisations, do not need to\nbe altered.\nThese observations have major practical implications. At a fixed computational budget NaViT consistently\noutperforms ViT. For instance, we match the performance of the top-performing ViT with 4\u00d7 less compute\n(Figure 1, left). We identify the substantial increase in the number of training examples processed within\nthe allocated compute budget as the primary contributor to the improved performance over ViT\u2014example\npacking coupled with variable resolution inputs and variable token dropping enable NaViT-L/16 to process\nfive times more images during training (Table 2). This improved efficiency extends to the fine-tuning process\n(Figure 1, middle). Furthermore, by exposing NaViT to multiple resolutions during both pre-training and\nfine-tuning, a single model demonstrates excellent performance when evaluated on various resolutions,\nsignificantly advantaging NaViT in terms of inference cost (Figure 1, right).\nNaViT\u2019s training and adaptation efficiency, and flexible inference, presents a promising avenue for Vision\nTransformers. Patch n\u2019 Pack empowers computer vision systems to transcend limitations imposed by current\ndata and modeling pipelines, enabling ideas that were previously restricted by the constraints of fixed batch\nshapes, unlocking new possibilities for innovation and advancement.\n2\nh=2w\nh=w\nw=2h\n0\n1\n2\n3\n4\n5\nDensity\nImageNet (85.9% non-square)\nh=2w\nh=w\nw=2h\nAspect ratio\nLVIS (92.2% non-square)\nh=2w\nh=w\nw=2h\nWebLI (57.3% non-square)\nFigure 3:\nHeight:width\nratios of different datasets;\nmost images are not square-\nish (> 20% deviation).\n2\nMethod\nDeep neural networks are typically trained and run with batches of inputs. For efficient processing on the\ncurrent hardware this implies fixed batch shapes which in turn imply fixed image sizes for computer vision\napplications. This coupled with architectural limitations historically associated with convolutional neural\nnetworks led to a practice of either resizing or padding images to a fixed size. Both of these have been shown\nto be flawed: the former harms performance and the latter is inefficient (Lee et al., 2022). An analysis of\naspect ratios in ImageNet (Deng et al., 2009), LVIS (Gupta et al., 2019) and WebLI (Chen et al., 2022c) as\nrepresentative examples of classification, detection and web image datasets, respectively, shows that most\nimages are typivally not square (Figure 3).\nIn language modelling, it is common to bypass limitations of fixed sequence lengths via example packing:\ntokens from multiple distinct examples are combined in one sequence, which can significantly accelerate\ntraining of language models (Krell et al., 2021). By treating images as sequences of patches (tokens), we\nshow that Vision Transformers (Dosovitskiy et al., 2021) can benefit from the same paradigm, which we call\nPatch n\u2019 Pack. Using this technique ViTs can be trained on images at their \u201cnative\u201d resolution, and we name\nthis approach NaViT.\n2.1\nArchitectural changes\nNaViT is built upon the original ViT, but in principle can use any ViT variant operating on a sequence of\npatches. To enable Patch n\u2019 Pack, we make the following architectural modifications.\nMasked self attention and masked pooling.\nTo prevent examples attending to each other, additional\nself-attention masks are introduced. Similarly, masked pooling on top of encoder aims to pool the token\nrepresentations within each example, resulting in a single vector representation per example in the sequence.\nFigure 2 presents how the receptive filed of attention is controlled via masking.\nFactorized & fractional positional embeddings.\nTo handle arbitrary resolutions and aspect ratios, we\nrevisit the position embeddings. Given square images of resolution R\u00d7R, a vanilla ViT with patch size P\nlearns 1-D positional embeddings of length (R/P)2 (Dosovitskiy et al., 2021). Linearly interpolating these\nembeddings is necessary to train or evaluate at higher resolution R.\nPix2struct (Lee et al., 2022) introduces learned 2D absolute positional embeddings, whereby positional\nembeddings of size [maxLen, maxLen] are learned, and indexed with (x, y) coordinates of each patch. This\nenables variable aspect ratios, with resolutions of up to R = P \u00b7 maxLen. However, every combination of\n(x, y) coordinates must be seen during training.\nTo support variable aspect ratios and readily extrapolate to unseen resolutions, we introduce factorized\npositional embeddings, where we decompose into separate embeddings \u03d5x and \u03d5y of x and y coordinates.\nThese are then summed together (alternative combination strategies explored in Section 3.4). We consider\ntwo schema: absolute embeddings, where \u03d5(p) : [0, maxLen] \u2192 RD is a function of the absolute patch index,\nand fractional embeddings, where \u03d5(r) : [0, 1] \u2192 RD is a function of r = p/side-length, that is, the relative distance\nalong the image. The latter provides positional embedding parameters independent of the image size, but\npartially obfuscates the original aspect ratio, which is then only implicit in the number of patches. We consider\n3\nsimple learned embeddings \u03d5, sinusoidal embeddings, and the learned Fourier positional embedding used\nby NeRF (Tancik et al., 2020).\n2.2\nTraining changes\nPatch n\u2019 pack enables new techniques to be used during training of NaViT.\nContinuous Token dropping.\nToken dropping (random omission of input patches during training) (Akbari\net al., 2021; Li et al., 2023) has been developed to accelerate training. However, typically the same proportion\nof tokens are dropped from all examples; packing enables continuous token dropping, whereby the token\ndropping rate can be varied per-image. This enables the benefits of faster throughput enabled by dropping\nwhile still seeing some complete images, reducing the train/inference discrepancy. Further, with packing,\nthe drop-distribution can vary throughout training, following some pre-defined schedule. In Section 3.3, we\nexplore different schedules and the benefits of flexible token dropping.\nResolution sampling.\nNaViT can be trained using the original resolution of each image. Alternatively, the\ntotal number of pixels can be resampled while preserving aspect ratio. In vanilla ViT, there is a tension between\ngreater throughput (training on smaller images), and greater performance (training on larger images, to\nenable high-resolution at evaluation time). Oftentimes, models are pre-trained at a smaller resolution and\nfinetuned at a higher one Touvron et al. (2019). NaViT is much more flexible; it allows mixed-resolution\ntraining by sampling from a distribution of image sizes, while retaining each images\u2019 original aspect ratio.\nThis allows both higher throughput and exposure to large images, yielding substantial improved performance\nover equivalent ViTs (in terms of models size and training duration). Section 3.2 explores different sampling\nstrategies, and variable resolution training for pre-training and finetuning.\n2.3\nEfficiency of NaViT\nHere we discuss some implications of Patch n\u2019 Pack on the computational efficiency of NaViT.\n1024 2048\n4096\n6144\nModel dimension\n0%\n10%\n20%\n30%\n40%\n50%\nAttention overhead\n1 packed\n2 packed\n4 packed\n8 packed\nFigure 4: Overhead from extra attention due\nto packing, assuming 256 tokens per image; it\ndiminishes with model scale.\nSelf attention cost.\nThe O(n2) cost of attention is a natu-\nral concern when packing multiple images into longer se-\nquences. Though many works aim to remove this quadratic\nscaling (Tay et al., 2022, 2020), we demonstrate here that as\nthe transformer hidden dimension is scaled, the attention\nbecomes an increasingly smaller proportion of the the overall\ncost, which encompasses the computation cost of the MLP\nas well. Figure 4 illustrates this trend, indicating a corre-\nsponding reduction in the overhead associated with packing\nexamples. In addition to speed considerations, the memory\ncost of self-attention can pose a challenge for extremely long\nsequences. However, this challenge can be also addressed\nby employing memory-efficient methods (Rabe and Staats,\n2021; Dao et al., 2022).\nPacking, and sequence-level padding.\nThe final sequence\nlengths containing multiple examples must be fixed. We use a greedy packing approach discussed in\nAppendix A.3; there typically is no perfect combination of examples exactly adding up to the fixed length and\npadding tokens have to be used. One could for example dynamically choose the resolution or token dropping\n4\nrate of the final example in a sequence to exactly fit the remaining tokens; however, we find typically less 2%\nof tokens are padding tokens, and thus the simple approach is sufficient.\nPadding examples and the contrastive loss.\nPer-token losses are straightforward to implement with packed\nsequences. However, many computer vision models are trained with example-level losses, typically applied\nto a pooled representation. First, this requires modifications to the typical pooling heads to account for\npacking. Second, multiple pooled representations must be extracted from each sequence. Fixed batch shapes\nrequires an assumption that, from a batch of B sequences, we extract at most B \u00d7Emax pooled representations\n(i.e. Emax examples per sequence). If a sequence contains more than Emax images, the extra images will be\ndropped, wasting computation of the model\u2019s encoder. If a sequence has less than Emax examples, then the\nloss will process lots of fake padding representations.\nThe latter is an issue for contrastive learning, where loss computation scales in time and memory \u223c O(n2).\nTo avoid this, we used the chunked contrastive loss (Mustafa et al., 2023), which circumvents the need to\ngather all data points for the softmax by performing computations on local device subsets and efficiently\naccumulating the necessary statistics for global softmax normalization. This enable high values of Emax (and\nthus efficient use of the model encoder), without being bottlenecked by the loss.\n3\nExperiments\nThe base architecture we use for NaViT follows vanilla ViT (Dosovitskiy et al., 2021), with the changes to\nenable packing, described in Section 2.1. In addition, we include small ViT improvements from previous works:\nquery-key normalization and the omission of biases (Dehghani et al., 2023), and attention pooling (Zhai\net al., 2022).\nWe pre-train NaViT in two setups: classification training on JFT-4B (Zhai et al., 2022) and contrastive language-\nimage training (Radford et al., 2021) on WebLI (Chen et al., 2022c). Typically, for JFT, inception crop is\napplied pre-training (Kolesnikov et al., 2020; Dosovitskiy et al., 2021), and in both cases, images are resized\nto a square (distorting aspect ratio). Unless otherwise specified, all NaViT models are pre-trained without\nthese operations, and preserve aspect ratio. NaViT is implemented in JAX (Bradbury et al., 2018) using the\nFLAX library (Heek et al., 2020) and built within Scenic (Dehghani et al., 2022).\nClassification pretraining.\nWe pre-train NaViT with supervised classification objective, using a sigmoid\ncross-entropy loss, following the setup of (Dehghani et al., 2023) on JFT-4B (Zhai et al., 2022). Visual\nrepresentations are evaluated following the linear evaluation protocol used for ViT (Dosovitskiy et al., 2021),\nwhere 10 examples per class are used to train a linear classifier on top of frozen representations.\nContrastive pre-training.\nAlongside the image model, we train a text encoder with the same architectural\nmodifications using the contrastive image-text loss (Radford et al., 2021; Zhang et al., 2022) (details in\nAppendix A.2). Packing also provides efficiency improvements on the text-tower, as text sequences do not\nneed to be padded to a fixed lengths, which is the normal setup. The contrastive models are evaluated on\nzero-shot ImageNet classification and COCO image-text retrieval.\n3.1\nImproved training efficiency and performance\nFigure 1 illustrates the JFT pretraining performance of different NaViT models compared to compute-matched\nViT baselines (Dehghani et al., 2021). The experimental setup details are provided in Appendix A.1. NaViT\nconsistently surpasses ViT in performance while using the same computational budget across different\ncompute and parameter scales; for example, the performance of the top-performing ViT can be matched by a\n5\n128\n256\n384\n512\nMax train resolution Rmax\n76%\n77%\n78%\n79%\nTop-1 Accuracy\nBest ImageNet 10shot\nVariable res \nU[64,Rmax]\nFixed res = Rmax\n128\n256\n384\n512\n40%\n60%\n80%\nnative res eval\n128\n256\n384\n512\n20%\n40%\n60%\n128 res eval\n128\n256\n384\n512\n65%\n70%\n75%\n224 res eval\n128\n256\n384\n512\nRmax\n70%\n75%\n256 res eval\n128\n256\n384\n512\nRmax\n40%\n60%\n80%\n384 res eval\n128\n256\n384\n512\nRmax\n20%\n40%\n60%\n80%\n512 res eval\nFigure 5:\nAt fixed computa-\ntional cost, sampling lower reso-\nlutions increases throughput, im-\nproves performance and enables\nbetter use of models at varied\nresolutions. NaViT-B/16 models\ntrained with variable vs. fixed res-\nolutions demonstrate the benefit\nof mixed resolution.\nNaViT with four times less compute. Conversely, the computationally lightest NaViT in Figure 1 is five times\nmore cost-effective than its equivalent ViT counterpart.\nThe NaViT models benefit from preserved aspect ratios and the ability to evaluate over many resolutions,\nbut the chief contributor here is the significant increase in the number of training examples processed by\nNaViT within the allocated compute budget. This is achieved through the combination of sampling multiple\nvariable-resolution examples and token dropping, leading to variable size images that are efficiently packed\ninto a similar sequence length as the original model. We ablate these factors below.\n3.2\nBenefits of variable resolution\nHere, we deep-dive the benefits of mixed-resolution training. Since we preserve the native aspect ratio, when\nwe refer to \u201cresolution\u201d for NaViT, we mean \u201ceffective resolution\u201d. That is, images with the same area as a\nsquare image with a given resolution. For example, for NaViT a resolution of \u201c128\u201d has the same area of\na square 128 x 128 image, but could be 64 x 256, or 170 x 96, etc., and thus has the same inference cost as\nregular ViT on 128 x 128 images.\nVariable-resolution pre-training.\nLower resolution images require fewer FLOPs to process and hence\nsmall resolutions (like 224) are used with fixed-resolution training. With fixed-resolution training, there is a\ntrade-off between throughput and ability to process details and high-resolution images. With NaViT we can\nmix lower resolution images with large ones to get the best of both worlds.\nFigure 5 shows a comparison between two NaViT variants trained at several different resolutions. Here, all\ntrained for the same number of FLOPs. (1) Native aspect ratio, but fixed resolution R = Rmax for different chosen\nvalues of Rmax. (2) Variable resolution, where the resolution is distributed as R \u223c U(64, Rmax). Variable\nresolution models outperform models trained at only that resolution. Even in the best case for fixed resolution,\nwhere the train and evaluation resolutions are identical, variable resolution matches or outperforms fixed.\nVariable-resolution finetuning.\nPrior works increase resolution late in pre-training or during finetuning,\nproducing higher quality but more expensive models (Dosovitskiy et al., 2021; Touvron et al., 2019). We\nfinetune NaViT and ViT models at different fixed resolutions, and additionally NaViT at variable resolutions.\nFigure 6 shows the results of fine-tuning pretrained ViT and NaViT on ImageNet-1k dataset. Performance\ngains during pretraining transfer well at all resolutions, but two phenomena are particularly interesting:\nFirst, NaViT finetuned with variable resolutions (\"NaViT 64:512\") is as good as a NaViT finetuned at a single\nresolution (and much better than single-resolution ViT), removing the need to pick a single downstream\nfinetuning resolution. Second, NaViT finetuned at low resolution (64) still obtains good performance when\nevaluated at higher resolutions (Figure 6, right), enabling cheaper adaptation.This ability to perform cheap\nadaptation of a flexible pre-trained model corroborates findings in (Beyer et al., 2023).\n6\n64 128\n256\n384\n512\nFine-tune resolution Rft\n55%\n60%\n65%\n70%\n75%\n80%\n85%\nImageNet Top-1 Accuracy\nEval at Rft\nNaViT (one per res)\nViT (one per res)\nNaViT-64:512\n64 128\n256\n384\n512\nFine-tune resolution Rft\nBest eval across R\n[64,512]\nNaViT (one per res)\nViT (one per res)\nNaViT-64:512\nFigure 6: Variable-resolution finetuning, JFT B/16\nmodels finetuned on ImageNet at various resolu-\ntions. Overall NaViT in all settings (blue, red), out-\nperforms ViT (orange) Left: A single NaViT fine-\ntuned with variable resolutions (red) is as good as\nmodels tuned on only one resolution (blue). Right:\nMixed-resolution pretraining performs well at high\nresolution when finetuning at low resolution (left-\nhand end of blue curve).\n55%\n58%\n60%\n62%\n65%\n68%\nImageNet zero-shot Accuracy\nnormal\nnormal\n=\n0.5\nnormal\n=0.5\nuniform\ndistribution\n63.3%\n62.6%\n63.4%\n63.8%\n63.6%\n63.1%\n64.3%\n63.4%\nSample side length\nSample area\nNative res\nFigure 7: Sampling side lengths directly\nwith a bias towards lower resolutions gives\noverall best performance at a fixed compu-\ntational budget.\nResolution sampling strategies.\nPacking examples enables di-\nverse resolution sampling strategies. We first consider whether\nto sample the target side length (average height/width, R), or the\ntarget area (i.e. sequence length \u221d R2). Sampling the side length\nfrom a uniform distribution biases towards lower sequence lengths,\nwhereas sampling the area from a uniform distribution biases to-\nwards higher side lengths.\nFor each image, we sample u \u223c D, where D is a distribution with\nsupport [\u22121, 1]. We rescale u linearly to [64, 384] for sampling\nside-lengths or [642, 3842] for sampling areas. We consider four\ndistributions D: uniform u \u223c U(\u22121, 1), truncated (to [\u22121, 1]) stan-\ndard Normal u \u223c Nt(0, 1), and then two other Normals which\nbias towards lower resolution u \u223c Nt(\u22120.5, 1) and higher resolu-\ntion u \u223c Nt(0.5, 1). The results are shown in Figure 7. Here, the\nbest resolution resampling strategy consistently performs over the\ndefault resolution. It is consistently better to sample side-lengths\n(orange) directly as opposed to area (blue), and the best distribution is the truncated normal biasing towards\nlower values; both of these increase throughput by preferentially sampling smaller sequences.\n3.3\nBenefits of variable token dropping\n70.0%\n80.0%\nTop-1 accuracy\nFixed\nIncreasing\nDecreasing (fast)\nDecreasing (slow)\nDrop schedule\n75.8%\n72.2%\n76.3%\n76.5%\nFigure 8: Time-varying token dropping\nrates improves performance and are easily\ndone with Patch n\u2019 Pack.\nToken dropping strategies.\nWe experimented with continuously\nsampled token dropping rates, and with resolution-dependent\ntoken dropping rates; both are explained in Appendix A.4.\nFig. 9a compares variable drop rates sampled from a Beta distri-\nbution to a constant drop rate, demonstrating consistent improve-\nments from the former. Fig. 9b shows the use of a resolution depen-\ndent token dropping rate for models trained with R \u223c U(64, 384)\nand dropping rates scaled between [0.5 \u2212 \u03b4, 0.5 + \u03b4] \u221d R, which\nfurther improves over the beta distribution.\nScheduled token dropping rates.\nPacking also enables easy variation the token dropping rate during\ntraining. By changing the token dropping rate we can better tune the trade-off between number of images\nseen and information used per image, to maximize the final accuracy while keeping the total training cost\nconstant. We varied the token dropping rate as a function of the number of images seen (details of the\nschedule in Appendix A.5). Fig. 8 demonstrates that further improvements are possible by reducing the\ntoken dropping rate during JFT pretraining of NaViT-B/16.\n7\n0.25\n0.50\n0.75\nMean token drop rate\n76%\n78%\n80%\nTop-1 Accuracy\nno token drop\nfixed for every image\nbeta: \n per image\n(a) Constant vs. Beta-distributed token\ndropping rates.\nfixed\nbeta\n=0.25\n=0.4\nToken drop strategy\n78.0%\n78.5%\n79.0%\nTop-1 accuracy\nResolution-dependent\n(b) Resolution dependent token dropping\nsampled \u2208 0.5 \u00b1 \u03b4\nFigure 9:\nContinuous token\ndropping strategies enabled by\nsequence packing improves per-\nformance\n3.4\nPositional embeddings\nWe evaluate our factorized embeddings introduced in Section 2.1, and their design choices. We are interested\nin both absolute performance, and extrapolation to resolutions outside the training regime. To test this, we\ntrain NaViT-B/16 models for 200k steps on JFT, with resolutions R \u223c U(160, 352). We evaluate performance\nat a range of resolutions, without modification of the embedding variables. We compare to a ViT-B/16 trained\nat fixed resolution 256 for the same amount of images seen, evaluated at new resolutions using standard\ninterpolation of positional embeddings.\nFig. 10 contains the results. First, it is clear that the factorized approaches outperform both the baseline ViT\nand the Learned 2D embeddings from Pix2struct. The latter in particular struggles to generalize to higher\nresolution, likely because this requires an increasingly long tail of unseen (x, y) pairs. Factorized embeddings\nare best combined additively (as opposed to stacking or multiplying).\n50%\n55%\n60%\n65%\n70%\n75%\nImageNet 10shot accuracy\nLearned 1D (ViT)\nLearned 2D (Pix2struct)\nFactorized (+)\nFactorized (stack)\nFactorized (\u00d7)\nSinusoidal\nFourier\nFactorized (+)\nSinusoidal\nFourier\n64.4%\n71.4%\n64.7%\n71.3%\n64.9%\n71.5%\n64.8%\n71.5%\n64.2%\n71.3%\n53.8%\n69.3%\n63.5%\n71.3%\n63.9%\n70.8%\n64.8%\n71.4%\n64.2%\n71.5%\n(a)\n64\n160\n256\n352\n512\n640\nEvaluation resolution\n0%\n20%\n40%\n60%\n80%\n100%\nRelative accuracy\nTrain\nresolutions\nViT\nLearned 2D (pix2struct)\nFactorized (+)\nFourier (fractional)\n(b)\nFigure 10: Factorized position embeddings improve generalization to new resolutions and aspect ratios. (a) Best (faded)\nand average accuracies (dark) across resolutions. (b) Accuracy normalized w.r.t. resolution 256.\n3.5\nOther aspects of NaViT\u2019s performance\nOut of distribution generalization.\nWe directly evaluate JFT-pretrained NaViT on downstream datasets,\nemploying a label-map (Wortsman et al., 2022) from JFT-4B to ImageNet (Deng et al., 2009) and robustness-\nvariants (ObjectNet (Barbu et al., 2019) and ImageNet-A (Hendrycks et al., 2021)). We compare the perfor-\nmance to a compute-matched ViT baseline.\nFigure 11 shows that NaViT compares favorably both on ImageNet as well as datasets variants that were\nspecifically designed to test out of distribution performance. It is interesting to note that NaViT performs\nmuch better on ImageNet-A, but ViT catches up on ObjectNet, even though both these datasets contain\nimages that have extreme aspect ratios. We believe this is due to the aspect-preserving center crop that we\n8\n1012\n1013\nTraining TPU chip hours\n55%\n60%\n65%\n70%\n75%\nImageNet\n1012\n1013\nTraining TPU chip hours\n20%\n40%\n60%\nImageNet-A\n1012\n1013\nTraining TPU chip hours\n40%\n50%\nObjectNet\nViT\nNaViT\nB/32\nB/16\nL/16\nFigure 11: Out of distribution evaluation of ViT and NaViT models that were matched for training compute.\nIn addition to improved performance due to more images seen (see also Figure 1), NaViT performs much\nbetter on ImageNet-A that has many images with an extreme aspect ratio and important information outside\nthe center crop (Appendix F). Same data as in Table 5.\napply to images for ImageNet-A and ObjectNet classification (same as in (Dehghani et al., 2023)), which is a\nuseful prior for ObjectNet, but less so for ImageNet-A (see Appendix F for details). If no crop is applied to\nimages and they\u2019re instead simply resized to the image resolution expected by the model (i.e., square for ViT,\nand aspect preserving resize to the same number of tokens for NaViT), then the observed difference is much\nlarger (see Figure 20 in appendix).\nCalibration.\nIn addition to the in-depth accuracy analysis, we have also quantfied the quality of the\nuncertainty computed by the model. In particular, for our ImageNet1K-finetuned models, we computed the\nexpected calibration error (Nixon et al., 2019) of the top prediction, as we vary the number of patches we\nassign per examples. We find that the calibration error remains very stable in the interval (0.045, 0.047) as\nwe vary the number of patches per image in the range [128, 1024], without any post-hoc recalibration. We\nprovide further details in the appendix Appendix E.\nInference trade-offs.\nGiven the flexibility of the model, there are several viable approaches how one can\nmaximize the aggregate accuracy under a given compute budget, which we quantify in terms of the latency\nmeasured on a Cloud TPUv3 chip. In an online inference setting, choosing an inference strategy translates to\nan algorithm how to allocate a fixed number of tokens per example. As we show in Fig. 1, NaViT offers much\nbetter trade-offs than ViT and it shows strong diminishing returns, s.t., even relatively few patches provide\nhighly competitive results. In Appendix C we further study a cascading approach, which both provides\nPareto optimal models and gives more precise trade-off opportunities.\nFairness signal annotation.\nWe investigate annotating images with fairness-related signals, such as those\npertaining to gender and ethnicity. Prior research has shown that metrics, such as group calibration, are\nsusceptible to labeling inaccuracy, particularly for underrepresented groups. In addition, this problem\npersists even when accounting for label noise during training (Adebayo et al., 2023). Thus, reducing the\nlabeling error of fairness signals improves the reliability of bias mitigation and post-hoc auditing (Raji and\nBuolamwini, 2019). To explore whether NaViT can help in overcoming these challenges, we train annotators\non FairFace (K\u00e4rkk\u00e4inen and Joo, 2019) and CelebA (Liu et al., 2015) datasets as linear probes (i.e. using\nfrozen features produced by NaViT or ViT), before comparing their accuracy.\nFirst, NaViT provides representations of higher quality that improve the accuracy of fairness signal annotation,\neven when dealing with square images. Original images are of size 448 \u00d7 448 in FairFace and 178 \u00d7 218 in\nCelebA, and we resize them to area 2242 while preserving aspect ratios in NaViT. Despite having the same\n9\n100\n250\n500\n1000\nPretraining Steps (K)\n93\n95\n97\naccuracy\nfairface / gender\n100\n250\n500\n1000\nPretraining Steps (K)\n64\n68\n72\naccuracy\nfairface / ethnicity\nModel\nViT\nNaViT\n100\n250\n500\n1000\nPretraining Steps (K)\n95\n96\n97\naccuracy\nfairface / gender\n100\n250\n500\n1000\nPretraining Steps (K)\n64\n68\n72\naccuracy\nfairface / ethnicity\nSquare\nNative\nFigure 12: Evaluating the accuracy of annotators trained on fairness-related signals using either NaViT-L/16 or ViT-L/16:\nLeft: NaViT offers better representations that improve the accuracy of annotators. Right: Using native aspect ratios in\nNaViT results in a higher performance when compared to resizing images to squares.\nsequence length, NaViT provides a higher prediction accuracy than ViT, as shown in Figure 12 (left). We\nverify statistical significance using the Wilcoxon signed-rank test test (Wilcoxon, 1992), which reveals that\nthe improvement in NaViT is significant with p = 3 \u00d7 10\u22124.\nSecond, we apply inception-style cropping (Szegedy et al., 2016) with a fixed minimum area of 50%. This\nchanges the aspect ratio but maintains native resolution. Similar to before, we resize all cropped images to\narea 224 \u00d7 224, either as square images or with native aspect ratios. Figure 12 (right) shows that native aspect\nratios in NaViT improve accuracy, which is statistically significant at the 95% confidence level (p = 0.02).\nAppendix G contains the full set of figures for other attributes in CelebA and Fairface.\n3.6\nOther downstream tasks\n256\n384\n512\n800\nMax Finetuning Resolution\n44\n47\n50\n53\nADE20k mIoU\nEvaluated on native resolutions\nViT (square R 2\nmax)\nNaViT (res R 2\nmax)\nFigure 13: NaViT transfers competitively\nto semantic segmentation. We transfer ViT-\nL/16 and NaViT-L/16 on ADE20k with\nfine-tuning at different resolutions. ViT\nconsumes square images while NaViT pre-\nserves the aspect ratio of the original im-\nages, while maintaning the total number\nof pixels the same as ViT.\nSemantic segmentation.\nWe finetune NaViT to semantic seg-\nmentation on ADE20k dataset (Zhou et al., 2017), following the\nlinear decoder protocol of Segmenter (Strudel et al., 2021). We\nuse ViT-L/16 as baseline and compare its performance with that\nof NaViT-L/16. Both models are pre-trained on JFT-4B (Zhai et al.,\n2022) with comparable compute budget.\nWe experiment with different maximum resolution Rmax at fine-\ntuning time: ViT takes in random square crops of resolution\nRmax\u00d7Rmax while NaViT\u2019s inputs are randomly resized (with\npreserved aspect ratio) so that the total number of pixels is R2\nmax.\nThis way, we have the same finetuning cost for the ViT and NaViT\nmodels. Note that following common practice, in order not to alter\nthe ground truth segmentation maps, both models are evaluated at\nthe native resolution (Strudel et al., 2021; Caron et al., 2022; Zhou\net al., 2017). This means that for ViT, the square predictions are\nresized from square back to native resolution. We observe in Fig-\nure 13 that NaViT outperforms ViT when transferred to semantic\nsegmentation with the same maximum finetuning resolution Rmax.\nNote that NaViT at R384 beats ViT at R512 while being twice as fast (see Appendix C). An advantage of NaViT\nover ViT is that it benefits from flexibility of resolution during training and can ingest seamlessly square and\nnon-square images.\nTable 1: NaViT improvements carry over to ob-\nject detection.\nViT-L/14\nNaViT-L/14\nImageNet zeroshot\n68.3%\n72.9%\nLVIS AP\n23.3%\n28.3%\nLVIS AP rare\n17.2%\n24.3%\nObject detection.\nNative-resolution training may be espe-\ncially beneficial for fine-grained tasks such as object detection,\nwhich require image understanding at many different spa-\ntial scales. We use compute matched NaViT and ViT models\nas backbones for OWL-ViT-L/14 object detectors (Minderer\n10\net al., 2022), following the OWL-ViT protocol for training and evaluation. Results are presented in Table 1.\nThe NaViT-based detector performs significantly better on both common and unseen LVIS \u201crare\u201d classes.\nThese experiments used shorter pre-training than the original OWL-ViT and thus reach lower absolute\nperformance; the relative difference nonetheless suggests that NaViT produces strong representations for\nfine-grained vision tasks.\nVideo Classification.\nVideo processing with transformers is inherently challenging due to the heterogeneity\nof the underlying spatio-temporal signals. In practice, cumbersome protocols have to be established both for\ntraining and evaluation (e.g. spatial and temporal multi-cropping) (Arnab et al., 2021). NaViT alleviates\nsome of these challenges by not only allowing training over different resolutions, but also over different\ntemporal durations. We fine-tuned NaViT trained on JFT for Kinetics400 (Kay et al., 2017) classification\nby extracting three different spatio-temporal patches \u201ctubelets\u201d\u2019. (Piergiovanni et al., 2022), extending the\npositional embedding to include the temporal dimension and initializing the embedding kernel using \u201ccentral\nframe embedding\u201d (Arnab et al., 2021). We posit that NaViT is an excellent starting point for multi-scale\ntraining due to the resolution diversity at training time. We observe that NaViT-L achieves competitive\nperformance with ViViT-L (80.4%) in approximately 6x less epochs, without multi-crop evaluation. Note\nthat the Kinetics400 dataset used here contains less data than prior works (Arnab et al., 2021).\n4\nRelated work\nFlexible Vision Transformers.\nFlexiViT (Beyer et al., 2023) developed a novel kernel resizing approach\nwhich enables variable \"resolution\" via models which support multiple patch sizes. This is viewed as unifying\nmultiple distinct models, and they study the relationship with distillation and neural architecture search.\nPix2struct (Lee et al., 2022) supported variable aspect ratios with a novel positional embedding schema, and\ndemonstrated significant efficiency and performance improvements for non-natural imagery such as chart\nand document understanding.\nMultiscale Vision Transformers.\nUsing feature maps at multiple spatial scales is a common approach to\nlocalization tasks such as segmentation and detection (Ronneberger et al., 2015). Many works developed\nVision Transformers which do the same (Fan et al., 2021; Li et al., 2022), though some dispute the necessity\nfor simple localization tasks (Chen et al., 2022a). NaViT does not build hierarchical representations using\nmultiple scales; we believe combining the unique flexibility of our approach with the benefits of this modelling\nfamily is a promising avenue.\nAccelerating training with mixed resolutions.\nImage modelling works considering resolution largely focus\non accelerating pretraining with a fixed, low resolution (Touvron et al., 2022; Liu et al., 2022). FixRes (Touvron\net al., 2019) is a popular technique whereby resolution is increased in a final stage of pretraining, and has\nbeen used in many subsequent works (Radford et al., 2021; Chen et al., 2022c). PaLI (Chen et al., 2022c), for\nexample, successively increases the resolution of the vision backbone during its generative training. This\napproach\u2019s main downside is its irreversibility: compute cannot be scaled back by reducing resolution after\ntuning at the higher and more expensive resolution.\nMultigrid training (Wu et al., 2020) is the most closely related work. The main idea is accelerate video\nmodelling by processing large batches with \"coarse\" spatiotemporal resolution early in the training, followed\nby a \"finer\" resolution later. To this end, the authors apply a hierarchical grid sampling schedule coupled\nwith appropriate learning rate scaling. In contrast, Patch n\u2019 Pack enables effortless incorporation of mixed\nresolutions without complex schedules or training pipelines.\nToken dropping for improved efficiency.\nResearch initially explored random token dropping (Akbari et al.,\n2021; Li et al., 2023; He et al., 2022). Follow ups demonstrated benefits from considering structured (Chen\n11\net al., 2022b) or \u201cimportance\u201d based strategies (Bolya et al., 2022; Yin et al., 2022). Better strategies will likely\nfurther boost performance, and Patch n\u2019 Pack sidesteps the fixed minibatch shape restriction which limited\nthese works.\n5\nConclusions and future work\nWe have demonstrated that Patch n\u2019 Pack\u2014the simple application of sequence packing to vision transformers\u2014\nsignificantly improves training efficiency. The resultant NaViT models can be applied to many resolutions\nat inference time, and cheaply adapted to new tasks. Patch n\u2019 Pack enables a wide variety of research\npreviously hindered by the need for fixed batch shapes, including adaptive computation and new algorithms\nfor improving training and inference efficiency.\nAcknowledgment\nWe would like to thank Marvin Ritter, Carlos Riquelme, Justin Gilmer, and Joelle Barral for invaluable\ntechnical support, insightful discussions around the challenges, and finally meticulous feedback on the paper.\nReferences\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale\npre-training. arXiv preprint arXiv:2110.02095, 2021.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics. In ICLR, 2023.\nHassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\nVatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in\nNeural Information Processing Systems, 34:24206\u201324221, 2021.\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. ViViT: A\nvideo vision transformer. In CVPR, 2021.\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum,\nand Boris Katz. ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition\nmodels. In NeurIPS, pages 9448\u20139458, 2019.\nLucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias\nMinderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch\nsizes. In CVPR, 2023.\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman.\nToken merging: Your vit but faster. CoRR, abs/2210.09461, 2022.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George\nNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable\ntransformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\nMathilde Caron, Neil Houlsby, and Cordelia Schmid. Location-aware self-supervised transformers for\nsemantic segmentation. arXiv preprint arXiv:2212.02400, 2022.\n12\nWuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin, Huizhong Chen, Jing Li,\nXiaodan Song, Zhangyang Wang, and Denny Zhou. A simple single-scale vision transformer for object\ndetection and instance segmentation. In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss\u00e9, Giovanni Maria\nFarinella, and Tal Hassner, editors, Computer Vision - ECCV 2022, 2022a.\nWuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang, and Denny Zhou. Auto-scaling\nvision transformers without training. In The Tenth International Conference on Learning Representations, ICLR,\n2022b.\nXi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong,\nHassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba\nSeyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua\nZhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-scaled multilingual language-image model. arXiv\npreprint arXiv:2209.06794, 2022c.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/\npaper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\nMostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. The efficiency misnomer. arXiv\npreprint arXiv:2110.12894, 2021.\nMostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A jax library for\ncomputer vision research and beyond. In CVPR, 2022.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael\nTschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci,\nManoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital\nOliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina\nVasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti\u0107, Dustin Tran, Thomas Kipf, Mario\nLu\u010di\u0107, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to\n22 billion parameters. In International Conference on Machine Learning, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical\nimage database. In CVPR, pages 248\u2013255, 2009.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\nFeichtenhofer. Multiscale vision transformers. In 2021 IEEE/CVF International Conference on Computer\nVision, ICCV, 2021.\nRobert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A\nWichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision.\nIn NeurIPS, pages 23885\u201323899, 2021.\nAgrim Gupta, Piotr Doll\u00e1r, and Ross B. Girshick. LVIS: A dataset for large vocabulary instance segmentation.\nIn IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2019.\nFoad Hamidi, Morgan Klaus Scheuerman, and Stacy M Branham. Gender recognition or gender reductionism?\nthe social implications of embedded gender recognition systems. In Proceedings of the 2018 chi conference on\nhuman factors in computing systems, pages 1\u201313, 2018.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross B. Girshick. Masked autoencoders\nare scalable vision learners. In CVPR, pages 15979\u201315988. IEEE, 2022.\n13\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and\nMarc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/\ngoogle/flax.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples.\nIn CVPR, pages 15262\u201315271, 2021.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint\narXiv:1705.06950, 2017.\nOs Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition. Proceedings\nof the ACM on human-computer interaction, 2(CSCW):1\u201322, 2018.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil\nHoulsby. Big Transfer (BiT): General visual representation learning. In ECCV, pages 491\u2013507, 2020.\nMario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew William Fitzgibbon. Efficient sequence packing\nwithout cross-contamination: Accelerating large language models without impacting performance, 2021.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. In EMNLP, pages 66\u201371, November 2018.\nKimmo K\u00e4rkk\u00e4inen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age,\n2019.\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal,\nPeter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for\nvisual language understanding, 2022.\nYanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph\nFeichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In CVPR,\n2022.\nYanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image\npre-training via masking, 2023.\nZe Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,\nLi Dong, et al. Swin Transformer V2: Scaling Up Capacity and Resolution. CVPR, 2022.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV,\n2015.\nMatthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,\nAravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary\nobject detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\nBasil Mustafa, Josip Djolonga, and Mostafa Dehghani. On efficient losses for distributed contrastive learning.\narXiv preprint, 2023.\nJeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring\ncalibration in deep learning. In CVPR Workshops, 2019.\nAJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Rethinking video vits: Sparse video tubes for joint\nimage and video learning, 2022.\nMarkus N Rabe and Charles Staats.\nSelf-attention does not need O(n2) memory.\narXiv preprint\narXiv:2112.05682, 2021.\n14\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural\nlanguage supervision. In ICML, pages 8748\u20138763, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\narXiv preprint arXiv:1910.10683, 2019.\nInioluwa Deborah Raji and Joy Buolamwini. Actionable auditing: Investigating the impact of publicly naming\nbiased performance results of commercial ai products. In Proceedings of the 2019 AAAI/ACM Conference on\nAI, Ethics, and Society, pages 429\u2013435, 2019.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi, editors,\nMedical Image Computing and Computer-Assisted Intervention - MICCAI, 2015.\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic\nsegmentation. In ICCV, 2021.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR, 2016.\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,\nRavi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency\nfunctions in low dimensional domains. In Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv\npreprint arXiv:2011.04006, 2020.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing\nSurveys, 55(6):1\u201328, 2022.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy.\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman\nGarnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019.\nHugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. DeiT III: Revenge of the ViT. In ECCV, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\nFrank Wilcoxon. Individual comparisons by ranking methods. Springer, 1992.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-\ntijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-\ntuning of zero-shot models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022,\nNew Orleans, LA, USA, June 18-24, 2022, pages 7949\u20137961. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00780.\nURL https://doi.org/10.1109/CVPR52688.2022.00780.\nChao-Yuan Wu, Ross B. Girshick, Kaiming He, Christoph Feichtenhofer, and Philipp Kr\u00e4henb\u00fchl. A multigrid\nmethod for efficiently training video models. In CVPR, pages 150\u2013159. Computer Vision Foundation /\nIEEE, 2020.\nHongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive\ntokens for efficient vision transformer. In CVPR, 2022.\n15\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In CVPR,\npages 12104\u201312113, 2022.\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Contrastive\nlearning of medical visual representations from paired images and text. In Proceedings of the Machine\nLearning for Healthcare Conference, MLHC, 2022.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017.\n16\nA\nTraining details\nA.1\nClassification pretraining\nThe experiments and ablations in the paper are with ViT-B/32, ViT-B/16, and ViT-L/16. We use a reciprocal\nsquare-root learning rate schedule, with linear warmup and cooldown, with a maximum value of 8e \u2212 4, and\nphases. We follow (Zhai et al., 2022; Abnar et al., 2021) and use a higher weight decay of 3.0 on the head\ncompared to the body\u2019s weight decay of 0.03 during upstream training to improve transfer to downstream\ntasks. For our experiments, we evaluated both NaViT and ViT models using configurations B/32, B/16, and\nL/16. Each ViT model was trained with varying compute budgets, with cooling down at different stages of\ntraining. We trained a corresponding NaViT model for each ViT size and computational budget, allowing us\nto perform \u201ccompute-matched\u201d comparisons (Dehghani et al., 2021).\nTable 2 presents the pretraining specifications for both ViT and NaViT models. During the pretraining phase,\nViT models were trained using images of size 224\u00d7224. In contrast, NaViT models uniformly sampled a\nvalue, denoted as r, between 64 and 256 and resized the image to have a total of r2 pixels while preserving the\naspect ratio. Although training NaViT models on native resolutions is possible, we empirically discovered\nthat sampling a resolution provides greater control over maximizing the number of examples observed\nduring pretaining within a fixed computational budget while maintaining performance across different\nresolutions in downstream tasks. Additionally, by controlling the resolution, we can ensure efficient packing\nby tuning the sequence length and limit padding to less than 2%.\nA.2\nContrastive pretraining\nWe use the 32000 token T5 Raffel et al. (2019) sentencepiece Kudo and Richardson (2018) tokenizer. By\ndefault, text sequences are truncated to a maximum length of 24. No token dropping is used for text. Models\nare trained under the same optimization regime as the classification models, but with a learning rate of\n3\u00d710\u22123. Weight decay of 1\u00d710\u22126 is applied consistently to all kernels in the model (no change for projection\nheads). By default, image side-lengths are sampled \u223c U(64, Rmax), and no other image augmentations are\napplied.\nA.3\nPacking algorithm\nPacking of examples into sequences is done alongside batching. A simple greedy approach is used which\nadds examples to the first sequence with enough remaining space. Once no more examples can fit, sequences\nare filled with padding tokens, yielding the fixed sequence lengths needed for batched operations. Such\nsimple packing algorithm can lead to a significant padding, depending on the distribution of length of inputs.\nThere are several methods to address such limitations, like bin packing (Krell et al., 2021), which allows\nminimizing the padding. Here, in NaViT, since controlling the resolutions we sample, we can ensure efficient\npacking by tuning the sequence length and limit padding to less than 2%.\nA.4\nSampling token dropping rates\nSampling with a beta distribution\nWe use a parameterisation based on the mean d\u00b5 and standard deviation\n\u03c3. We aim to sample dropout rate d \u2208 [0.0, dmax], with some mean d\u00b5.\nAccordingly, we sample u \u2208 [0, 1] \u223c B(\u03b1, \u03b2) and set drop rate d = u \u00d7 dmax. \u03b1 and \u03b2 are set such that the\nmean of u is u\u00b5 =\nd\u00b5\ndmax . The maximum supported variance for a beta distribution of mean u\u00b5 is u\u00b5(1 \u2212 u\u00b5);\nwe pick by default a variance \u03c32 = 0.3u\u00b5(1 \u2212 u\u00b5), which we found to work well in practice. The resultant\ndistributions of token dropouts for different settings of d\u00b5 and dmax are shown in Figure 14a.\n17\nTable 2: Pre-training details of ViT and NaViT with supervised classification.\nName\nTPU\nHours\nTrain\nSteps\nCooldown\nSteps\nSequence\nLength\nImages\nPer Seq.\nBatch\nSize\nTraining\nImages\nViT-B/32\n1.4 \u00d7 1011\n1.0 \u00d7 105\n1.0 \u00d7 104\n49\n1.0\n\u22484.0 \u00d7 103\n4.0 \u00d7 108\n3.5 \u00d7 1011\n2.5 \u00d7 105\n5.0 \u00d7 104\n49\n1.0\n\u22484.0 \u00d7 103\n1.0 \u00d7 109\n7.1 \u00d7 1011\n5.0 \u00d7 105\n1.0 \u00d7 105\n49\n1.0\n\u22484.0 \u00d7 103\n2.0 \u00d7 109\n1.4 \u00d7 1012\n1.0 \u00d7 106\n1.0 \u00d7 105\n49\n1.0\n\u22484.0 \u00d7 103\n4.0 \u00d7 109\nViT-B/16\n4.7 \u00d7 1011\n1.0 \u00d7 105\n1.0 \u00d7 104\n196\n1.0\n\u22484.0 \u00d7 103\n4.0 \u00d7 108\n1.1 \u00d7 1012\n2.5 \u00d7 105\n5.0 \u00d7 104\n196\n1.0\n\u22484.0 \u00d7 103\n1.0 \u00d7 109\n2.3 \u00d7 1012\n5.0 \u00d7 105\n1.0 \u00d7 105\n196\n1.0\n\u22484.0 \u00d7 103\n2.0 \u00d7 109\n4.7 \u00d7 1012\n1.0 \u00d7 106\n1.0 \u00d7 105\n196\n1.0\n\u22484.0 \u00d7 103\n4.0 \u00d7 109\nViT-L/16\n9.8 \u00d7 1011\n1.0 \u00d7 105\n1.0 \u00d7 104\n196\n1.0\n\u22484.0 \u00d7 103\n4.0 \u00d7 108\n2.4 \u00d7 1012\n2.5 \u00d7 105\n5.0 \u00d7 104\n196\n1.0\n\u22484.0 \u00d7 103\n1.0 \u00d7 109\n4.9 \u00d7 1012\n5.0 \u00d7 105\n1.0 \u00d7 105\n196\n1.0\n\u22484.0 \u00d7 103\n2.0 \u00d7 109\n9.8 \u00d7 1012\n1.0 \u00d7 106\n1.0 \u00d7 105\n196\n1.0\n\u22484.0 \u00d7 103\n4.0 \u00d7 109\nNaViT-B/32\n1.4 \u00d7 1011\n9.8 \u00d7 104\n1.0 \u00d7 104\n64\n5.41\n\u22482.2 \u00d7 104\n2.1 \u00d7 109\n3.5 \u00d7 1011\n2.4 \u00d7 105\n5.0 \u00d7 104\n64\n5.41\n\u22482.2 \u00d7 104\n5.3 \u00d7 109\n7.1 \u00d7 1011\n4.8 \u00d7 105\n1.0 \u00d7 105\n64\n5.41\n\u22482.2 \u00d7 104\n1.0 \u00d7 1010\n1.4 \u00d7 1012\n9.7 \u00d7 105\n1.0 \u00d7 105\n64\n5.41\n\u22482.2 \u00d7 104\n2.1 \u00d7 1010\nNaViT-B/16\n4.7 \u00d7 1011\n9.3 \u00d7 104\n1.0 \u00d7 104\n256\n4.87\n\u22481.9 \u00d7 104\n1.8 \u00d7 109\n1.1 \u00d7 1012\n2.3 \u00d7 105\n5.0 \u00d7 104\n256\n4.88\n\u22481.9 \u00d7 104\n4.6 \u00d7 109\n2.3 \u00d7 1012\n4.6 \u00d7 105\n1.0 \u00d7 105\n256\n4.88\n\u22481.9 \u00d7 104\n9.2 \u00d7 109\n4.7 \u00d7 1012\n9.2 \u00d7 105\n1.0 \u00d7 105\n256\n4.88\n\u22481.9 \u00d7 104\n1.8 \u00d7 1010\nNaViT-L/16\n9.8 \u00d7 1011\n9.7 \u00d7 104\n1.0 \u00d7 104\n256\n4.88\n\u22481.9 \u00d7 104\n1.9 \u00d7 109\n2.4 \u00d7 1012\n2.4 \u00d7 105\n5.0 \u00d7 104\n256\n4.87\n\u22481.9 \u00d7 104\n4.8 \u00d7 109\n4.9 \u00d7 1012\n4.8 \u00d7 105\n1.0 \u00d7 105\n256\n4.87\n\u22481.9 \u00d7 104\n9.6 \u00d7 109\n9.8 \u00d7 1012\n9.6 \u00d7 105\n1.0 \u00d7 105\n256\n4.88\n\u22481.9 \u00d7 104\n1.9 \u00d7 1010\nSampling resolution-dependent dropping rates\nGiven input data with sequence lengths ranging from smin\nto smax, we sample dropout rate d from a truncated normal distribution d \u223c Ntrunc(\u00b5, 0.02), where samples\nmore than two standard deviations away from \u00b5 are rejected.\nThe mean of this distribution \u00b5 is set according to the minimum and maximum token dropping rates dmin\nand dmax, and simply scales linearly with the sequence length s (such that s = smin has \u00b5 = dmin and s = smax\nhas \u00b5 = dmax.\nFigure 14b shows example distributions of sampled drop rates given inputs with resolution R \u223c U(64, 384),\nand different values of dmin and dmax.\nA.5\nScheduling token dropping rates\nWe experiment with a token dropping schedule which varies with total number of images seen. In particular,\nthe rate applied for the n-th processed image during training is given by:\n\u03c1(n; \u03c1min, \u03c1max, \u00b5, \u03c4) = \u03c1min + (\u03c1max \u2212 \u03c1min) \u00b7 \u03c3\n\u0012n \u2212 \u00b5\n\u03c4\n\u0013\n,\n(1)\n18\n25%\n50%\n75%\n100%\nToken drop rate\n0%\n5%\n10%\n15%\n20%\n25%\nPercent\n=0.10p<0.1 =60.3%\n=0.25p<0.1 =11.5%\n=0.35p<0.1 =1.3%\nMax drop rate = 40%\n25%\n50%\n75%\n100%\nToken drop rate\n=0.10p<0.1 =66.9%\n=0.25p<0.1 =27.3%\n=0.50p<0.1 =2.3%\n=0.65p<0.1 =0.1%\nMax drop rate = 70%\n25%\n50%\n75%\n100%\nToken drop rate\n=0.10p<0.1 =70.4%\n=0.25p<0.1 =33.5%\n=0.50p<0.1 =5.9%\n=0.70p<0.1 =0.9%\n=0.85p<0.1 =0.1%\nMax drop rate = 90%\n(a) Beta-sampled token drop rates parameterised by the mean \u00b5 and the max drop\nrate dmax\n25%\n50%\n75% 100%\nToken drop rate\n0%\n1%\n2%\n3%\n4%\n5%\nPercent\nd\n[0.1,0.9]\nd\n[0.25,0.75]\nd\n[0.4,0.6]\n(b) Sampled resolution-dependent\ntoken drop rates\n0\n200 k\n400 k\nTotal training steps\n0.25\n0.50\n0.75\nAverage drop rate\n0\n200 k\n400 k\nTotal training steps\n0\n1\n2\nTotal images seen\n1e9\n0\n200 k\n400 k\nTotal training steps\n65%\n70%\n75%\nImageNet 10shot acc\nFixed = 1\n2\n= N\n10,\n= N\n2\n=\nN\n10,\n= N\n2\n=\nN\n5 ,\n= 2N\n3\nFigure 15: Decreasing the token dropping rate along training improves the ImageNet 10shot accuracy using\nthe same pre-training resources. N is the total number of training examples seen with a fixed token dropping\nrate of \u03c1 = 1\n2.\nwhere \u03c3 represents the sigmoid function; \u03c1min, \u03c1max control the minimum and maximum dropping rate\napplied; and \u00b5 and \u03c4 control the shape of the schedule. We experimented with both increasing (\u03c4 > 0)\nand decreasing (\u03c4 < 0) schedules. In all cases we set \u03c1min = 0.2 and \u03c1max = 0.8. Figure 15 shows that, by\ndecreasing the dropping rate throughout training, one can improve the final accuracy, at fixed training cost.\nConversely, increasing the token dropping rate harms performance.\nB\nModel information\nB.1\nPositional embeddings\nExtending ViTs to variable input sizes necessitates rethinking positional embeddings added to every token\nafter embedding. We considered several variants of positional embeddings, and evaluated them based on (1)\nthe best performance model using them achieve within training distribution of input sizes; and based on (2)\nhow well these models perform when evaluated on image sizes outside of the training distribution. Results\nand discussion of these experiments can be found in Section 3.4.\nBroadly, we considered positional embeddings that varied along three axes: (1) whether they were learned,\nparametric or fixed; (2) whether they were absolute or fractional; and (3) whether they are factorized.\nAbsolute and fractional coordinates\nA natural way of indexing token within an image is to select a priori a\nmaximum possible image side length (shared for width and height) maxLen, and to assign to token integer\ncoordinates (x, y) based on their original location within the image. Embedding coordinates defined in\nthis way allow models to consume images with resolutions up to R = P \u00b7 maxLen. However, when learned\n19\n(0.0, 0.0)\n(2, 3)\n(0, 0)\n(6, 4)\n0\n2\n0\n3\n0\n6\n0\n4\n(0.0, 0.0)\n(1.0, 1.0)\n(0.0, 0.0)\n(1.0, 1.0)\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n(0.0, 0.0)\n(0.75, 1.0)\n(0.0, 0.0)\n(1.0, .71)\n0.0\n0.75\n0.0\n1.0\n0.0\n1.0\n0.0\n.71\n Absolute coordinates \nFractional coordinates (non aspect ratio preserving)\nFractional coordinates (aspect ratio preserving)\nImage credit: Matthew Henry burst.shopify.com/photos/dog-staying-warm\nFigure 16: We use two different views of the same image, of resolutions 96\u00d7128 and 224\u00d7160, and demon-\nstrate different coordinate systems when using patch size 32.\nabsolute coordinate embeddings are considered, extreme values of x and y must also be observed during\ntraining, which necessitates training on images with varied aspect ratios and limits models generalisation.\nTo alleviate the necessity of observing extreme aspect ratios and image size during learning of positional\nembeddings, we also consider fractional coordinates, which are normalized to the actual size of the input\nimage and are obtained by dividing the absolute coordinates x and y above by the number number of columns\nand rows respectively, i.e. the corresponding side length. Doing this allows the model to observe extreme\ntoken coordinates during training, which intuitively should help with generalization to higher resolutions.\nHowever, this is accomplished at the cost of obfuscating the input images aspect ratio.\nFactorized embeddings\nWe further consider whether coordinates x and y should be embedded indepen-\ndently or jointly. In case of independent embedding, the two coordinates x and y are embedded independently,\nand their embeddings are combined via addition or by stacking. For joint embeddings and embedding for\neach position (x, y) is obtained directly.\nLearned, parametric and fixed positional embeddings\nFinally, we also explored the relative benefits of\nfixed, learned and parametric embeddings. For fixed embeddings we followed Vaswani et al. (2017) and\nused sinusoidal positional embeddings, and learned embeddings were implemented as in Dosovitskiy et al.\n(2021).\nFor parametric positional embeddings we followed Tancik et al. (2020) and used Fourier embeddings.\nSpecifically, coordinates (x, y) were mapped using a single linear layer before applying sin and cos activations\nto them, and stacking the results to obtained the positional embeddings.\nExperiments\nBecause not all combinations of the above embedding choices are equally promising or natural,\nwe experimented only with subset of them shown in Table 3 and Figure 10a.\nC\nInference strategies\nWe performed various experiments to measure model quality for given runtime cost. The runtime can be\ntuned by changing the number of processed patches, or by using choosing different size of the model.\nWe firstly looked at how model quality changes in respect to decreasing area of the image compared to native\nresolution, presented in Figure 17a. We observed that on ImageNet Deng et al. (2009) model retains most of\nthe quality down to 40% of the image size. After that, the quality drastically decreases. On the other hand,\nincreasing the size of the image have a diminishing return in quality. This can be directly compared with\nrandom token dropping as an alternative to resizing, which showed to be very ineffective way to decrease\nnumber of patches during inference - Figure 17b.\n20\nTable 3: Classification of positional embedding experiments from Figure 10a.\nName\nCoordinates\nType\nFactorized\nLearned 1D (ViT)\nAbsolute\nLearned\nNo, position in flatted token sequence\nLearned 2D (Pix2struct)\nAbsolute\nLearned\nNo\nFactorized abs. (+)\nAbsolute\nLearned\nYes, sum\nFactorized abs. (stack)\nAbsolute\nLearned\nYes, stack\nFactorized abs. (\u00d7)\nAbsolute\nLearned\nYes, product\nFourier abs.\nAbsolute\nParametric\nNo\nSinusoidal abs.\nAbsolute\nFixed\nYes, stack\nFactorized frac. (+)\nFractional\nLearned\nYes, sum\nFourier frac.\nFractional\nParametric\nNo\nSinusoidal frac.\nFractional\nFixed\nYes, stack\nIn sinusoidal and factorised embeddings experiments with fractional coordinates fractional coordinate\nembeddings were obtained from absolute coordinate embeddings via bilinear interpolation.\nPlease note that this highly depends on the native resolution of the images in the dataset - e.g. dataset with\ntwice as big images than ImageNet can probably be safely resized to 20% of area.\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nImage area\n85\n86\n87\n88\nImagenet accuracy\nNaViT-L/16\nNaViT-B/16\n(a)\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nImage area\n86.5\n87.0\n87.5\n88.0\n88.5\nImagenet accuracy\nresize\ndrop tokens\n(b)\n128\n256\n384\n512\n640\n768\n896\n1024\nsequence length\n87.6\n87.8\n88.0\n88.2\n88.4\n88.6\nImagenet accuracy\nNaViT-L\nNaViT-L(no upscale)\n(c)\nFigure 17: (a) The effect of resizing the image. (b) Dropping random tokens is ineffective way to decrease\nnumber of patches compared to resizing the image. Data from NaViT-L/16. (c) Given number of patches as\ncompute budget, it is beneficial to upscale the image.\nA better way to quantify the performance is by giving a constant compute budget corresponding to number\nof patches. Figure 18a shows that resizing the image (while preserving aspect ratio) to 256 tokens retains\nmost of the quality (within 0.3%). This corresponds to 256x256 area (given patch size of 16). At 128 tokens\n(181x181 area) the quality difference reaches 1% and drops significantly after that.\nHere we also resized the image past its native resolution in case it already fit the given sequence length\nbudget. We observed that it is beneficial to resize the image to the given sequence length past the native\nresolution to keep monotonic increase in quality, which is showed on Figure 17c.\nFigure 18b presents the runtime of NaViT-L/16 and NaViT-B/16 for different sequence lengths. We can see\nthat NaViT-L/16 at sequence length 128 is as fast as NaViT-B/16 with sequence length 512, while having\nalmost 1% difference in quality.\nD\nCascades\nAnother strategy to be more compute efficient would be to assign more tokens (and thus FLOPs) to the\nexamples deemed hard by the model. This is in particular interesting for bulk inference workloads, where\none can amortize over large datasets and where only the total inference time matters.\nTo evaluate the feasibility of this approach, we consider two sequence lengths n1 and n2 with respective\n21\n64128\n256\n384\n512\n640\n768\n896\n1024\nsequence length\n84\n85\n86\n87\n88\nImagenet accuracy\n6x reduction\n1%\n5x reduction\n1%\nnavit resize\nmine resize\n(a)\n5\n10\n15\n20\n25\naverage runtime in ms per image\n86.5\n87.0\n87.5\n88.0\nImagenet accuracy\n128\n256\n384\n512\n640\n768\n896\n1024\n256\n384\n512\n640\n768 896 1024\nNaViT-L/16\nNaViT-B/16\n(b)\nFigure 18: (a) Quality on ImageNet in respect to number of patches (sequence length). (b) Runtime of\nmodels compared to the accuracy on ImageNet.\n0\n5\n10\n15\n20\n25\nAverage inference time in milliseconds\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5\nImageNet1K-Accuracy\n128\n256\n1024\nCascade (ViT-B)\nViT-B/16\nCascade (ViT-L)\nViT-L/16\nPareto frontier\nFigure 19: Performance of a model cascade versus the average inference time. The labels at the select points\ndenote the number of tokens at that scale.\ninference times t1 and t2. Then, we (i) send all examples though the model with n1 tokens, and send only\nthe \u03b1 \u2208 (0, 1)-fraction deemed hardest (those with the smallest maximum probability) to the model with n2\ntokens. To have an input almost exactly fit into n1 or n2 tokens we perform and aspect ratio preserving resize.\nHence, the total amortized inference time per-example is ti + \u03b1t2, while the accuracy obtained by combining\nthe accuracy of the first model on the 1 \u2212 \u03b1 most-confident fraction of the data, and the performance of\nthe more expensive model on the remaining data. By considering several pairs of models and varying \u03b1\nwe obtain the plot in Figure 19. As we can see this strategy is indeed useful and provides not only the\nbest performing models at given compute budgets, but because \u03b1 is a real parameter one can obtain very\nfine-grained trade-offs.\nE\nCalibration\nTo evaluate behaviour of the predicted uncertainties with scale, we compute the calibration error of a -B\nsized ImageNet-finetuned model (the -L model performs similarly). Note that these models were trained\nwith sigmoid loss, i.e., the 1000 labels were predicted independently without enforcing that the probabilities\nshould sum up to 1. As we varied the sequence of tokens per example between 128 and 1024, we obtained\nvery stable calibration errors (top-1, using \u21131 and 30 buckets, i.e., the settings from Nixon et al. (2019)), which\nwe present in Table 4.\n22\nTable 4: Expected calibration error on ImageNet-1K with varying sequence lengths.\nSequence Length\n128\n256\n384\n512\n640\n768\n1024\nCalibration Error\n0.047\n0.046\n0.048\n0.047\n0.047\n0.046\n0.045\nF\nOut of distribution evaluation\nFor ViT, we apply the \u201cCrop\u201d strategy from (Dehghani et al., 2023), namely an aspect-preserving crop of the\ncentral 75% of the image for ObjectNet and ImageNet-A, and square resize followed by a 87.5% central crop\nfor the other datasets. We also apply a simple \u201cResize\u201d strategy that does not crop the images. For NaViT,\nboth the \u201cCrop\u201d and the \u201cResize\u201d strategy do an aspect preserving resize of the target images.\nTable 5: Detailed results of out evaluation of pretrained models with a label-map (see Section 3.5). Same\ndata as in Figure 11 and Figure 20.\nImageNet\nImageNet-A\nObjectNet\nViT\nNaViT\nViT\nNaViT\nViT\nNaViT\nCompute\ncustom\nB/32\n1.4 \u00d7 1011\n54.4\n63.5\n14.7\n32.7\n32.9\n41.2\n1.4 \u00d7 1012\n65.5\n68.2\n30.7\n42.5\n44.2\n45.7\n3.6 \u00d7 1011\n60.2\n65.6\n22.0\n37.0\n38.0\n43.5\n7.2 \u00d7 1011\n63.7\n67.2\n26.6\n40.1\n41.7\n45.1\nB/16\n1.2 \u00d7 1012\n66.4\n68.5\n37.3\n52.3\n47.2\n48.4\n2.4 \u00d7 1012\n68.7\n70.1\n43.5\n55.1\n50.5\n49.8\n4.8 \u00d7 1011\n61.7\n66.0\n27.0\n44.2\n41.3\n45.9\n4.8 \u00d7 1012\n70.3\n71.2\n48.8\n57.0\n52.8\n50.7\nL/16\n2.5 \u00d7 1012\n70.7\n73.6\n51.5\n65.5\n53.3\n55.0\n4.9 \u00d7 1012\n73.0\n74.6\n57.6\n67.9\n56.2\n57.1\n9.9 \u00d7 1011\n66.4\n71.1\n39.2\n58.6\n47.6\n52.1\n9.9 \u00d7 1012\n73.9\n75.1\n60.4\n68.9\n57.7\n57.9\nresize\nB/32\n1.4 \u00d7 1011\n51.7\n64.0\n12.6\n26.7\n15.9\n31.6\n1.4 \u00d7 1012\n63.6\n68.6\n22.8\n35.0\n25.3\n36.1\n3.6 \u00d7 1011\n57.9\n66.2\n17.2\n30.1\n20.0\n33.8\n7.2 \u00d7 1011\n61.6\n67.4\n20.7\n32.6\n23.5\n34.6\nB/16\n1.2 \u00d7 1012\n65.4\n69.2\n27.4\n43.9\n28.7\n38.7\n2.4 \u00d7 1012\n67.7\n71.0\n32.5\n48.6\n32.0\n40.5\n4.8 \u00d7 1011\n60.4\n66.5\n20.4\n36.8\n23.6\n35.3\n4.8 \u00d7 1012\n69.5\n72.5\n36.2\n51.5\n34.2\n42.1\nL/16\n2.5 \u00d7 1012\n70.0\n73.9\n38.5\n59.9\n35.3\n45.6\n4.9 \u00d7 1012\n72.3\n75.3\n44.3\n64.1\n38.8\n48.2\n9.9 \u00d7 1011\n65.1\n71.5\n28.2\n51.6\n29.3\n41.5\n9.9 \u00d7 1012\n73.2\n76.0\n47.3\n65.5\n39.8\n48.8\nG\nFairness Signal Annotation\nIn Figure 21, we demonstrate that using native image resolution improves the performance of fairness signal\nannotation. Prior research has shown that metrics, such as group calibration, are vulnerable to labeling errors,\nparticularly for underrepresented groups. Moreover, this problem persists even when accounting for label\nnoise during training (Adebayo et al., 2023). Thus, reducing the labeling error of fairness signals has the\npotential of improving the reliability of bias mitigation and post-hoc auditing (Raji and Buolamwini, 2019).\n23\n1012\n1013\nTraining TPU chip hours\n55\n60\n65\n70\n75\nImageNet\n1012\n1013\nTraining TPU chip hours\n10\n20\n30\n40\n50\n60\nImageNet-A\n1012\n1013\nTraining TPU chip hours\n15\n20\n25\n30\n35\n40\n45\n50\nObjectNet\nViT\nNaViT\nB/32\nB/16\nL/16\nFigure 20: Same evaluation as in Figure 11, but without any special preprocessing of the images before the\nevaluation. Employing a simple resize (square for ViT, aspect preserving for NaViT) results in much better\nperformance on datasets that have images with an extreme aspect ratio. Same data as in table Table 5.\nNevertheless, we emphasize that while NaViT improves the annotation accuracy in these tasks, care must\n100\n250\n500\n1000\nPretraining Steps (K)\n98\n99\naccuracy\nceleba / gender\n100\n250\n500\n1000\nPretraining Steps (K)\n93\n95\n97\naccuracy\nfairface / gender\n100\n250\n500\n1000\nPretraining Steps (K)\n54\n57\n60\naccuracy\nfairface / age\n100\n250\n500\n1000\nPretraining Steps (K)\n64\n68\n72\naccuracy\nfairface / ethnicity\nModel\nViT\nNaViT\n100\n250\n500\n1000\nPretraining Steps (K)\n98\n99\naccuracy\nceleba / gender\n100\n250\n500\n1000\nPretraining Steps (K)\n95\n96\n97\naccuracy\nfairface / gender\n100\n250\n500\n1000\nPretraining Steps (K)\n55\n57\n59\naccuracy\nfairface / age\n100\n250\n500\n1000\nPretraining Steps (K)\n64\n68\n72\naccuracy\nfairface / ethnicity\nSquare\nNative\nFigure 21: Summary of results of evaluating the accuracy of annotators trained on fairness-related signals using either\nNaViT-L/16 or ViT-L/16. top: NaViT offers better representations that improve the accuracy of annotators. bottom:\nUsing native aspect ratios in NaViT results in a higher performance when compared to resizing images to squares.\nbe taken in such situations since classifiers can be inaccurate and lead to a broad categorization of people\nthat misidentifies real identities. We encourage readers to delve into the comprehensive work outlining such\npotential risks, e.g. (Hamidi et al., 2018; Keyes, 2018), for further insight. In assessing the technical capabilities\nof NaViT, our intent is not to promote or encourage their application in inappropriate contexts. Rather, our\nobjective is only to illuminate these technical findings for scenarios where they may be considered beneficial,\nsuch as when measuring the level of diversity in a dataset or auditing/mitigating biases in predictive models.\nWe strongly advocate for responsible AI use, maintaining that the benefits of technological advancements\nshould not overshadow the importance of user safety and privacy. AI tools, including ours, should always be\ndeployed judiciously, with a keen awareness of potential risks and a commitment to avoiding harm.\n24\nH\nEvaluation on model-vs-human OOD datasets on different resolu-\ntions\nJust like NaViT, human visual perception works across flexible aspect ratios and resolutions (just imagine\nhow strange the world would look like if we could only see it through a 224 \u00d7 224 pixel window!). We\ninvestigate how the ability to cope with variable resolutions affects performance on \u201cmodel-vs-human\u201d, a\nbenchmark of 17 challenging datasets (Geirhos et al., 2021).1 For this purpose, we replicate the setup from\nFigure 6, but instead of evaluating ImageNet accuracy, we evaluate OOD accuracy on the model-vs-human\nbenchmark.\n64 128\n224\n384\n512\nFine-tune resolution Rft\n85\n90\n95\nAccuracy (%)\ncolour\nNaViT (one per res)\nViT (one per res)\nNaViT-64:512\n64 128\n224\n384\n512\nFine-tune resolution Rft\n85\n90\n95\nAccuracy (%)\nfalse-colour\n64 128\n224\n384\n512\nFine-tune resolution Rft\n60\n80\nAccuracy (%)\npower-equalisation\n64 128\n224\n384\n512\nFine-tune resolution Rft\n40\n60\n80\nAccuracy (%)\nrotation\n64 128\n224\n384\n512\nFine-tune resolution Rft\n40\n50\nAccuracy (%)\nuniform-noise\n64 128\n224\n384\n512\nFine-tune resolution Rft\n60\n80\nAccuracy (%)\ncontrast\n64 128\n224\n384\n512\nFine-tune resolution Rft\n50\n60\nAccuracy (%)\nlow-pass\n64 128\n224\n384\n512\nFine-tune resolution Rft\n20\n40\n60\nAccuracy (%)\nhigh-pass\n64 128\n224\n384\n512\nFine-tune resolution Rft\n60\n80\nAccuracy (%)\nphase-scrambling\n64 128\n224\n384\n512\nFine-tune resolution Rft\n60\n65\nAccuracy (%)\neidolonI\n64 128\n224\n384\n512\nFine-tune resolution Rft\n55.0\n57.5\n60.0\n62.5\nAccuracy (%)\neidolonII\n64 128\n224\n384\n512\nFine-tune resolution Rft\n60\n65\nAccuracy (%)\neidolonIII\n64 128\n224\n384\n512\nFine-tune resolution Rft\n60\n70\n80\n90\nAccuracy (%)\nsketch\n64 128\n224\n384\n512\nFine-tune resolution Rft\n40\n50\n60\nAccuracy (%)\nstylized\n64 128\n224\n384\n512\nFine-tune resolution Rft\n55\n60\n65\n70\nAccuracy (%)\nsilhouette\n64\n128\n224\n384\n512\nFine-tune resolution Rft\n20\n40\n60\nAccuracy (%)\nedge\nFigure 22: OOD accuracy on \u201cmodel-vs-human\u201d datasets across different fine-tuning resolutions. A single\nNaViT model trained on varying resolutions (red) performs roughly on par as fine-tuning one NaViT model\nper test resolution (blue). The ViT baseline (orange) is mostly worse than NaViT models for lower resolutions\nand mostly better for higher resolutions.\nThis corresponds to testing JFT B/16 models finetuned on ImageNet at various resolutions. The test dataset\nhas a fixed 224\u00d7224 square resolution; thus we resize the test images to fit each model\u2019s fine-tuning resolution.\nNote that using square images has been standard practice designed for convolutional networks, but NaViT\nmodels no longer require square input, thus existing benchmarks are not tailored to those new possibilities.\nFor datasets with multiple difficulty levels (such as different levels of blur), we average performance across\nlevels while excluding levels that are too easy (not OOD) or too hard (human performance is close to chance),\nwhich follows the approach of Geirhos et al. (2021) as explained in their \u201cAppendix G Benchmark scores\u201d.\nTo ensure a fair comparison, we use models that are compute-matched for pretraining, and during fine-tuning,\ncompute and data are identical for all models. The results of our comparison are shown in Figure 22. Overall,\na single NaViT model trained on varying resolutions (red) performs roughly on par with fine-tuning one\nNaViT model per test resolution (blue).\nThe ViT baseline (orange) is mostly worse than NaViT models for lower resolutions and mostly better for\n1For the purpose of our comparison, we exclude the \u201ccue-conflict\u201d dataset from the OOD evaluation, since there is no objective\nground truth class in the case of images with a texture-shape cue conflict.\n25\nhigher resolutions. It may be worth noting that ViT models have a bit of an advantage in this comparison\nsince they are fine-tuned on square images, whereas NaViT models are fine-tuned on flexible resolution\nimages (preserving the image\u2019s aspect ratio) that have the same number of pixels, while not necessarily being\nsquare.\n26\n"
  },
  {
    "title": "PolyLM: An Open Source Polyglot Large Language Model",
    "link": "https://arxiv.org/pdf/2307.06018.pdf",
    "upvote": "24",
    "text": "Technical Report\n2023-07-12\nPOLYLM: AN OPEN SOURCE POLYGLOT LARGE LAN-\nGUAGE MODEL\nXiangpeng Wei\u2217, Haoran Wei\u2217, Huan Lin\u2217, Tianhao Li\u2217, Pei Zhang\u2217, Xingzhang Ren\u2217, Mei Li\u2217\nYu Wan\u2217, Zhiwei Cao\u2020, Binbin Xie\u2020, Tianxiang Hu\u2020, Shangjie Li\u2020, Binyuan Hui, Bowen Yu\nDayiheng Liu\u2021, Baosong Yang\u2021, Fei Huang, Jun Xie\nDAMO Academy, Alibaba Group\nABSTRACT\nLarge language models (LLMs) demonstrate remarkable ability to comprehend,\nreason, and generate following nature language instructions. However, the devel-\nopment of LLMs has been primarily focused on high-resource languages, such as\nEnglish, thereby limiting their applicability and research in other languages. Con-\nsequently, we present POLYLM, a multilingual LLM trained on 640 billion (B)\ntokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual\ncapabilities, we 1) integrate bilingual data into training data; and 2) adopt a cur-\nriculum learning strategy that increases the proportion of non-English data from\n30% in the first stage to 60% in the final stage during pre-training. Further, we\npropose a multilingual self-instruct method which automatically generates 132.7K\ndiverse multilingual instructions for model fine-tuning. To assess the model\u2019s per-\nformance, we collect several existing multilingual tasks, including multilingual\nunderstanding, question answering, generation, and translation. Extensive exper-\niments show that POLYLM surpasses other open-source models such as LLaMA\nand BLOOM on multilingual tasks while maintaining comparable performance\nin English. Our models, alone with the instruction data and multilingual bench-\nmark, are available at: https://modelscope.cn/models/damo/nlp_\npolylm_13b_text_generation.\n1\nINTRODUCTION\nLarge language models (LLMs) are trained on vast amounts of data in a self-supervised fashion,\nwhich has shown promising performance in a variety of zero-shot and few-shot tasks (Brown et al.,\n2020; Chowdhery et al., 2022). Fine-tuning these models on a diverse set of tasks allows them to\nhandle unseen tasks following natural language instructions (Ouyang et al., 2022; Longpre et al.,\n2023; Taori et al., 2023; Anand et al., 2023). These properties have attracted significant attention\nfrom the Artificial Intelligence community and offering a potential path towards artificial general\nintelligence. Unfortunately, most LLMs are developed for English, such as LLaMA (Touvron et al.,\n2023), BLOOM (Scao et al., 2022), Chinchilla (Hoffmann et al., 2022), OPT (Zhang et al., 2022). A\nmain reason stems from recent findings that model performance is closely related to the scale of the\ntraining dataset (Kaplan et al., 2020; Rae et al., 2021; Biderman et al., 2023; Touvron et al., 2023),\nleading to predominant focus on resource-rich languages, particularly English.\nThe relatively high concentration of studies on English limits the research and usage of LLMs in\nother languages. For instance, Thai and Indonesian have over 300 million (M) speakers, yet the\nsize of these two languages in common crawl-based dataset such as mC4 (Xue et al., 2020) is\nonly 80 billion (B) tokens, comprising a mere 3% of the English data. Due to the insufficient\nhigh-quality internet data, LLM capabilities on low-resource languages fail to be easily improved\nthrough expanding their data size like English (Kaplan et al., 2020; Rae et al., 2021; Biderman et al.,\n2023). As a result, existing open-source LLMs such as XGLM (Lin et al., 2022), BLOOM (Scao\n\u2217Major contributors.\n\u2020Contribution during internship at Alibaba DAMO Academy.\n\u2021Corresponding authors: {liudayiheng.ldyh, yangbaosong.ybs}@alibaba-inc.com\n1\narXiv:2307.06018v1  [cs.CL]  12 Jul 2023\nTechnical Report\n2023-07-12\net al., 2022), and LLaMA (Touvron et al., 2023) perform relatively poor on these languages, some\nof which are entirely overlooked. It is crucial to explore multilingual LLMs to bridge this gap and\nachieve academic and social significance.\nOur goal is to enhance the exploration and utilization of LLMs for non-native English speakers. In\nthis work, we fill three significant gaps in this field: 1) the absence of an open-source multilingual\nLLM; 2) the inadequate availability of multilingual instruction data; and 3) the lack of a unified\nevaluation benchmark for multilingual settings.\nConcretely, we first develop an open-source multilingual LLM from scratch, called Polyglot Large\nLanguage Model (POLYLM, Section 3). Contrary to existing open-source multilingual LLMs that\nlack 13B model, we release POLYLM-13B and POLYLM-1.7B to facilitate its usage. To construct\nPOLYLM, we leverage a massive dataset of 640B tokens, culled from publicly available sources such\nas Wikipedia, mC4 (Xue et al., 2020), CC-100 (Conneau et al., 2019). This dataset contains over\n30% of non-English languages, specifically covering 18 of the most commonly spoken languages.1\nTo alleviate the problem of insufficient data for low-resource languages, we propose a curriculum\nlearning strategy. The training schedule increases the amount of data available for training in En-\nglish during the initial phases, then ramping up the ratio of high-quality, low-resource languages as\ntraining progresses. We expect the method to enable the transfer of general knowledge from English\nto other languages, leading to significant improvements in overall performance.\nIn light of the supervised fine-tuning (SFT) stage, we construct a multilingual instruction dataset\ntermed MULTIALPACA with 132,701 samples (Section 4). At present, there is a dearth of high-\nquality open-source multilingual SFT datasets. On the one hand, extant multilingual SFT datasets,\ne.g. xP3-MT (Muennighoff et al., 2022), are acquired via machine translation, which potentially\nyields a style of translationese, a lack of cultural nuances, as well as translation errors. On the\nother hands, manually annotating instructions is a laborious and costly process that does not lend\nitself well to the incorporation of creative flourishes. Drawing inspiration from recent advances in\nself-instruct (Wang et al., 2022; Taori et al., 2023), we devise a multilingual self-instruct method to\nautomatically generate instruction data. Utilizing 175 English seeds as a starting point, our method\nleverage multilingual seed translation, instruction generation, and filtering mechanisms to deliver\nhigh quality multilingual instruction data.\nIn order to assess the multilingual capabilities of LLM, we curate a benchmark derived from existing\nmultilingual tasks (Section 5.1), including QA (Clark et al., 2020), understanding (Conneau et al.,\n2019; Yang et al., 2019; Tikhonov & Ryabinin, 2021; Ponti et al., 2020), generation (Chen et al.,\n2021), and cross-lingual machine translation (Barrault et al., 2020). The benchmark is constructed\nwith meticulously prompting and finally covers 10 tasks across 15 languages. Extensive experiments\n(Section 6) demonstrate that our pretrained model outperforms open-source models of comparable\nmodel size (e.g. BLOOM, LLaMA, etc.) in non-English languages. Through in-depth analyses, we\nidentify finding that the proposed curriculum training strategy boosts the multilingual performance\nwhile maintain the English proficiency. In addition, the use of multilingual instruction data markedly\nenhances the ability of POLYLM to tackle multilingual zero-shot tasks.\n2\nPRELIMINARY\nIn this section, we begin with a review of the background on language modeling. We then examine\nprevious research on knowledge transferring, and instruction learning of pre-trained LLMs, with a\nfocus on their relevance to POLYLM. Finally, we outline our rationale for training POLYLM.\nLanguage Modeling refers to the process of estimating the probability of a sequence of tokens, i.e.\np(x) = p(x1, x2, ..., xT ) = QT\nt=1 p(xt|x<t). This is also commonly referred to as autoregressive\nsequence modeling, as it involves predicting the future token at each time-step based on the pre-\nceding context. The initial language models were predominantly n-gram models that evaluate the\nlikelihood of a sequence of tokens based on the frequency of its occurrence in a training corpus. Over\nthe last two decades, neural networks have proven to be effective in the task of language modeling,\nincluding feed-forward models (Mikolov et al., 2010) and recurrent neural networks (Bengio et al.,\n1According\nto\nhttps://www.ethnologue.com/insights/most-spoken-language/.\nSome languages with interchangeable and more widely used official languages are not given priority, such as\nHindi, Wu Chinese, and Cantonese.\n2\nTechnical Report\n2023-07-12\n2000). More recently, Transformer (Vaswani et al., 2017), a self-attention based neural network,\nhas shown unparalleled language model performance (Devlin et al., 2019; Radford et al., 2018),\nand become the de facto backbone of LLMs emerged in the past three years, such as GPT3 (Brown\net al., 2020), Gopher (Rae et al., 2021), PaLM (Anil et al., 2023), BLOOM (Scao et al., 2022),\nChinchilla (Hoffmann et al., 2022), GLM (Zeng et al., 2022) and LLaMA (Touvron et al., 2023).\nTransfer Learning is a rapidly evolving field of research that has garnered significant interest in\nrecent years. In this scenario, models are initially trained on extensive unlabeled data, and then\ntheir acquired knowledge is applied to various downstream tasks through fine-tuning. Some of the\nmost prominent works in this area include the ELMo (Peters et al., 2018), BERT (Devlin et al.,\n2019) and GPT (Radford et al., 2018) have demonstrated remarkable success. These developments\nsubsequently prompt work (Raffel et al., 2020; Radford et al., 2019; Xue et al., 2020) on better re-\nsults by adopting larger scale data and parameters to further improve model performance. Although\npretraing-then-finetuning is still effective in achieving high performance with limited labeled data,\nrecent advancements has shown that language models with extremely large scale parameters can per-\nform tasks without further optimization. The most exemplary model is GPT3 (Brown et al., 2020),\nwhich utilizes a contextualized approach by incorporating multiple input-output demonstrations and\npresenting them alongside the query. This effectively stimulates the model to generate accurate\npredictions, showcasing encouraging outcomes in zero/few-shot situations.\nInstruction Learning aims to bring together various natural language processing tasks by framing\nthem as question-answering exercises that operate over a given context. This approach enhances the\nvalue of LLMs by leveraging their existing knowledge. With the success of language models, there\nhas been a growing interest in exploring their potential to comprehend and execute instructions.\nSeveral advanced researches (Ouyang et al., 2022; Wei et al., 2022; Peng et al., 2023; Ye et al.,\n2023; Zhou et al., 2023) have demonstrated a remarkable ability to generalize to new zero-shot tasks.\nHowever, they rely heavily on human-generated instruction data, which is frequently constrained in\nterms of quantity, diversity, and creativity, which is very time-consuming and labor-intensive. Wang\net al. (2022) make an effort to construct a self-Instruct framework for improving the instruction-\nfollowing capabilities of LLMs. Similarly, Xu et al. (2023) propose an evol-instruct framework to\nautomatically rewrite simple human-written instructions step by step into more complex ones, to\nfurther improve instruction-followed LLMs.\nIn this paper, we propose POLYLM to address the following blanks and limitations in current LLM\nresearch, offering a comprehensive and innovative solution to advance this field.\n\u2022 We provide a 13B scale model that is proficient in the major non-English languages spo-\nken worldwide, such as Spanish, Russian, Arabic, Japanese, Korean, Thai, Indonesian,\nand Chinese etc. It is a perfect complement to the existing open-source models, includ-\ning: (1) LLaMA, English is predominant among the whole dataset. (2) BLOOM, lack\nof 13B version and fail to address languages spoken by significant populations, such as\nJapanese, Korean and Thai. (3) XGLM (Lin et al., 2022), the maximum version is 7B. (4)\nmGPT (Shliazhko et al., 2022), only 1.3B version is available.\n\u2022 We suggest an advanced curriculum learning approach that facilitates the transfer of com-\nmonsense knowledge, acquired mainly in English, to diverse non-English languages and\nspecific NLP downstream tasks such as machine translation.\n\u2022 We propose MULTIALPACA to complement ALPACA (Taori et al., 2023) and CHINESE-\nALPACA (Cui et al., 2023), making LLMs better follow multilingual instructions, particu-\nlarly those coming from non-native English speakers.\n3\nPOLYLM: A POLYGLOT LARGE LANGUAGE MODEL\nIn this section, we present the design of POLYLM, which includes a detailed description of its\ntraining dataset (Section 3.1), architecture (Section 3.2), and training process (Section 3.3).\n3.1\nDATASET\nThe composition of the pre-training dataset used for POLYLM is shown in Table 1. Our pre-training\ndataset contains 640B tokens in total, of which English data accounts for 68%. To develop POLYLM\n3\nTechnical Report\n2023-07-12\nSource Fraction Tokens\nType\nmC4\n49.95% 321.7B\nWeb-text (Multilingual)\nCC-100 32.31% 208.1B\nWeb-text (Multilingual)\nThe Pile16.41% 105.7B Web-text & books (English)\nGitHub 1.17%\n7.5B\nCode\nOPUS\n0.16%\n1.0B\nParallel Multilingual Data\nSum\n-\n638B\nTable 1: The composition of the POLYLM pre-training dataset.\nLanguage\nTokens (B)\nPercentage (%)\nLanguage\nTokens (B)\nPercentage (%)\nEn\n424.96\n67.56\nVi\n4.13\n0.66\nZh\n139.29\n22.14\nId\n3.91\n0.62\nRu\n7.61\n1.21\nPl\n3.84\n0.61\nEs\n5.62\n0.89\nNl\n3.52\n0.56\nDe\n5.56\n0.88\nAr\n3.48\n0.55\nFr\n5.10\n0.81\nTr\n3.42\n0.54\nIt\n4.31\n0.69\nTh\n2.89\n0.46\nPt\n4.27\n0.68\nHe\n2.10\n0.33\nJa\n4.19\n0.67\nKo\n0.84\n0.13\nTable 2: Language distribution of the training data (excluding code and multilingual parallel data).\nwith multilingual capabilities, the pre-training dataset has about 32% non-English multilingual data,\nwhich is a higher percentage of non-English data than most previous open-sourced large language\nmodels (Biderman et al., 2023; Zhang et al., 2022; Touvron et al., 2023; Penedo et al., 2023). To\nbe concrete, the English data contains documents with 425B tokens from multiple sources, such as\nThe Pile (Gao et al., 2020), mC4 (Xue et al., 2020), and Wikipedia. While the 204B multilingual\ndata tokens come from CC-100 (Conneau et al., 2019), mC4 (Xue et al., 2020), Wikipedia. The\nmultilingual data mainly covers the following languages: zh, ar, es, fr, de, it, nl, ru, id, pl,\npt, ja, th, tr, he, ko, vi, with the distribution given in Table 2. To enable the model ability of\ncode understanding and generation, we also incorporate code data of 7.5B tokens from GitHub with\npermissioned licenses into our pre-training dataset. In order to further improve the cross-lingual\nand multilingual ability of the POLYLM, similar to PaLM2 (Anil et al., 2023), we employ parallel\nmultilingual data of 1B tokens into our pre-training dataset.\nTo build the pre-training dataset, we also develop a comprehensive data pre-processing pipeline\nthat implements multiple techniques for data cleaning and filtering. The pipeline consists of the\nfollowing stages:\n1) Language identification. We classify documents according to their primary languages and re-\nmove those with low confidence in classification, leveraging inexpensive n-gram models (e.g., fast-\nText (Joulin et al., 2016)).\n2) Rule-based filtering. Following Rae et al. (2021); Scao et al. (2022), we eliminate irrelevant\nor low-quality content using various rules and heuristics, including repetition removal (the docu-\nment with the excessive line, paragraph, or n-gram repetitions is removed), document-wise filtering\n(removing outlier documents by overall length, symbol-to-word ratio, the ratio of ellipsis, invisible\ncharacters, numbers, and dates, etc.), and line-wise corrections (such as URL filtering, long words\nremoval, and whitespace standardization).\n3) ML-based quality filtering. We further filter low-quality multilingual documents using several\nsmall n-gram-based language models (e.g., KenLM (Heafield, 2011)) for different languages trained\non their gold-standard corpora. In addition, similar to Raffel et al. (2020); Smith et al. (2022), we\nalso train a 2-gram fastText (Joulin et al., 2016) classifier to filter the low-quality English documents.\nThis classifier uses Wikipedia, and Books from The Pile (Gao et al., 2020) as the positive samples\n4\nTechnical Report\n2023-07-12\n0\n2\n4\n6\nTh\nRu\nKo\nHe\nVi\nAr\nZh\nJa\nTr\nPl\nId\nDe\nNl\nPt\nIt\nEs\nCode\nFr\nEn\nCompression Rate\nGPT-2\nGPT-4\nLLaMA\nBLOOM\nPolyLM\nFigure 1: The compression rate of different tokenizers. We take XLM-R (Conneau et al., 2019)\ntokenizer as the baseline, and set the compression rate of XLM-R tokenizer to 1.\nHyperparameter (\u2193)\nPOLYLM-1.7B\nPOLYLM-13B\nArchitecture hyperparameters\nNumber of parameters\n1,722M\n13,003M\nPrecision\nbfloat16\nNumber of layers\n24\n40\nHidden dimension\n2048\n5120\nAttention heads\n16\n40\nVocab size\n256,000\nSequence length\n2048\nActivation\nGELU\nPosition embedding\nAbsolute\nPretraining hyperparameters\nGlobal Batch Size\n512\n2048\nLearning rate peak\n1 \u00d7 10\u22124\n6 \u00d7 10\u22125\nTotal training tokens\n638B\nGradient clipping\n1.0\nWeight decay\n0.1\nMultilingul Self-instruction finetuning hyperparameters\nGlobal Batch Size\n32\n64\nSequence strategy\nThe length is 2048 with packing\nLearning rate\n1e-5\nTotal training tokens\n16M tokens\nTable 3: POLYLM Architecture and Training Hyperparameters.\nand CommonCrawl web documents as the negative samples. To sum up, about 28.3% data are\nfiltered with Rule-based filtering and ML-based quality filtering.\n4) Deduplication. In line with Raffel et al. (2020), we remove similar documents to reduce data\nredundancy with MinHashLSH-based fuzzy deduplication technology, where 23.1% English docu-\nments and 18.6% non-English documents are removed.\nBased on the POLYLM multilingual pre-training dataset, we derived a vocabulary with 256K token\nentries using Byte-Pair Encoding (BPE) (Sennrich et al., 2015) with the implementation from Sen-\ntencePiece (Kudo & Richardson, 2018). To enhance the mathematical capabilities of our model,\nwe follow Touvron et al. (2023) to split all numbers into individual digits. The unknown charac-\nters are fallback to byte encoding of UTF-8 to guarantee the coverage of rare words (e.g., emoji,\nand special symbols). For tokenizer training, we sample multilingual documents with a similar\ndistribution as Conneau et al. (2019) used to increase the number of vocabulary tokens associated\nwith low-resource languages and alleviate the bias towards high-resource languages. We compare\nthe compression rate on different language corpora of different tokenizers. We use XLM-R (Con-\nneau et al., 2019) tokenizer, which supports 100 languages, as the baseline (the compression rate\nof XLM-R tokenizer is set to 1). As shown in Figure 1, POLYLM has achieved significantly better\ncompression rates in most covered languages, while maintaining the compression rate in English as\n5\nTechnical Report\n2023-07-12\n \n0\n4\n8\n12\n0\n1k\n2k\n3k\n4k\n5k\n6k\n7k\n8k\n9k 10k\n(a) Loss value\n \n0\n40\n80\n120\n0\n1k\n2k\n3k\n4k\n5k\n6k\n7k\n8k\n9k 10k\n(b) Gradient norm\n \n0\n2e-5\n4e-5\n6e-5\n8e-5\n1e-4\n0\n1k 2k 3k 4k 5k 6k 7k 8k 9k 10k\nspike occurs \nfinal choice \n(c) Learning rate\nFigure 2: Training curves over iterations for the 13B model with learning rate as 1 \u00d7 10\u22124.\n \n0\n4\n8\n12\n0\n2k\n4k\n6k\n8k\n10k\n12k\n14k\n(a) Loss with bfloat16 precision.\n \n2.3\n2.32\n2.34\n2.36\n2.38\n10k\n20k\n30k\n40k\n50k\n60k\n(b) Loss with mixed-precision.\n \n1.76\n1.8\n1.84\n1.88\n1.92\n1.96\n0\n4k\n8k\n12k\n16k\n20k\n24k\n(c) Loss on curriculum learning.\nFigure 3: Training curves over iterations for the 13B model with learning rate as 6 \u00d7 10\u22125.\nBLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023), GPT-2 (Radford et al., 2019), and GPT-\n4 (OpenAI, 2023). Note that some open source models that are not friendly to language extensions,\nfor example, LLaMA (Touvron et al., 2023) only contain a 32K size vocabulary mostly composed of\nEnglish tokens, which is not friendly to non-Latin languages. When improving a certain non-Latin\nlanguage ability, the vocabulary needs to be expanded like Chinese-LLaMA (Cui et al., 2023). On\nthe contrary, POLYLM allows researchers to improve the model\u2019s ability in a covered language by\nsimply continuing monolingual pre-training without expanding the vocabulary.\n3.2\nARCHITECTURE\nIt has become apparent that the computational cost of exploring different architectural designs for\nLLMs is prohibitive. Therefore, we present the distinctive design options of POLYLM2 in this\nsection.\nFollowing some endeavours on large language models, we develop a decoder-only autoregressive\nTransformer architecture detailed in Radford et al. (2019). To stabilize the training, we adopt Pre-\nLN (Xiong et al., 2020), i.e. y = x + LayerNorm(f(x)) (where f(\u00b7) indicates the layer function)\nfor layer normalization, and apply the Xavier normal initialization (Glorot & Bengio, 2010) with\nbias terms are initialized to zero. To improve FFNs in Transformer, we replace ReLU with GeLU\nactivation (Hendrycks & Gimpel, 2016).\nIn this paper we present two Transformer language models with 1.7 billion and 13 billion parameters,\nrespectively. The architectural details are displayed in Table 3.\n3.3\nTRAINING\nWe train all models with a 2048 token context window, using the Adam (\u03b21 = 0.9, \u03b22 = 0.95)\noptimizer. We warm-up the learning rate from 1e\u22127 to the maximum learning rate over the first\n2000 steps, and then decay it to 10% of the maximal learning rate using a cosine schedule. We use\na weight decay of 0.1 and gradient clipping of 1.0.\n2Recent research indicates that Rotary Position Encoding (RoPE) (Su et al., 2021) yields superior perfor-\nmance. Accordingly, we will switch to the latest Megatron-LM branch and promptly release 13B and 1.7B\nversions featuring RoPE.\n6\nTechnical Report\n2023-07-12\n0.1\n1\n10\n100\nEn\nZh\nRu\nEs\nDe\nFr\nIt\nPt\nJa\nKo\nVi\nId\nAr\nTh\nNl\nPl\nHe\nTr\nProportion (%)\nLarge-scale Pretraining\nHigh-quality Curriculum Learning\nFigure 4: The proportion of multilingual data in curriculum learning significantly exceed that in the\npretraining phrase.\nmC4, 2.28%\nCC-100, 35.88%\nGitHub, 7.76%\nOPUS, 4.76%\nBooks, 4.65%\nWikipedia, 17.58%\nArxiv, 14.79%\nStackExchange, 12.31%\nHigh-quality Curriculum Learning\nmC4, 49.95%\nCC-100, 32.31%\nGitHub, 1.17%\nOPUS, 0.16%\nThe Pile, 16.41%\nLarge-scale Pretraining\nFigure 5: The proportion of high-quality and multilingual source is raised in the curriculum learning\ndataset.\nPOLYLM was trained using Megatron-LM 3 on a cluster of 32 A100 GPU (8\u00d780G) servers. We\napply tensor model parallelism within a single node, setting tensor-model-parallel-size\nas 8. When training a 13B-parameter model, our code processes around 1170 tokens/sec/GPU, thus\ntraining over our dataset containing 640B tokens takes approximately 29 days. However, we faced\nnumerous unforeseen spikes and deviations in losses, which prolonged the entire training process to\na duration of two months. There are several possible conditions that result in training collapses, and\nour unique choices to enhance training stability.\nLower Maximal Learning Rate. Learning rate is an important hyperparameter in neural network\nmodels that controls the magnitude of parameter updates. In our first few attempts, we drew inspira-\ntion from previous research which indicated that smaller models tend to benefit from higher learning\nrates. As such, we opted to set the learning rate to 1 \u00d7 10\u22124. Without exception, all attempts to train\nPOLYLM-13B have resulted in loss spikes with this choice in early stage, which tend to occur more\nfrequently as the training progresses, as illustrated in Figure 2a. We have noticed that the gradient\nnorm shows significant fluctuations during the warm-up phase, when the learning rate is increasing\nlinearly (see Figure 2b).\nThe fundamental issue with instability during training is that a large learning rate can cause the\ngradient to grow too large, surpassing the model\u2019s capacity and resulting in a gradient explosion that\nprevents parameter updates. The problem is handled via reducing learning rate to 6 \u00d7 10\u22125, i.e. a\nproper learning rate located before the step where the initial spike in loss occurs (Cf. Figure 2c).\nMixed-Precision. Despite the potential instabilities associated with training models using half pre-\ncision (float16) activations and model parameters that arise from the limited numerical range, it\nhas been proposed that the numbers represented by bfloat16 allow for training of models and\ncan avoid performance degradation compared to full float32 training. Thus, we incorporate the\nbfloat16 numerical format to reduce memory and increase training efficiency. However, sim-\nilar to OPT-175B (Zhang et al., 2022), BLOOM-176B (Scao et al., 2022) and GLM-130B (Zeng\n3https://github.com/NVIDIA/Megatron-LM\n7\nTechnical Report\n2023-07-12\net al., 2022), the training of POLYLM-13B still faces frequent loss spikes while lowering learning\nrate. We attempted to address such challenge via manually skipping data and restart the straining, it\nunfortunately tends to become increasingly severe as the training does on (Cf. Figure 3a).\nAfter conducting two weeks of investigation, we have come to the realization that the instabilities\nwe are encountering may not be due to the training data under the mutlilingual scenario (with the\nvocabulary up to 256,000), but rather due to the model itself. Specifically, we suspect that there\ncould be a risk of overflow in the attention or residual connectivity layers. Taking this into account,\nwe have configured the residual connection and attention layers to have a numerical precision of\nfloat32 to ensure optimal performance, resulting in a highly stable training process (Cf. Figure 3b).\nCurriculum Learning. Optimizing LLMs to learn knowledge encoded in multiple languages simul-\ntaneously is a significant challenge. We concretely formulate this problem as transferring general\nknowledge to low-resource languages while maintaining the advantage of high-resource language\nin the model. To address this issue, we adopt a curriculum learning strategy (Bengio et al., 2009;\nKumar et al., 2010; Jaegle et al., 2021) that ramps up the ratio of high-quality and low-resource\nlanguages during training. Specifically, the training process is divided into two stages. In the first\nstage, we use the whole pre-training dataset to train a base model yields commonsense generaliza-\ntion ability. In the second stage, we transition to a subset of the pre-training dataset that boasts\nsuperior quality and a greater proportion of multilingual content, to further strengthen the model\u2019s\nmultilingual capabilities. Figure 4 compares the language distribution of training data in two stages,\nindicating that the proportion of most low-resource languages has been increased in the sub-dataset.\nTo build the sub-dataset for curriculum learning, we first manually evaluate the quality of publicly\navailable data source in the pre-training dataset, and sample about 97B tokens from the high-quality\nsources while increasing the proportion of languages other than Chinese and English. We also\nenhance the proportion of parallel data (OPUS) to facilitate the modeling of cross-lingual represen-\ntation. The detail of the sub-dataset are illustrated in Figure 5. According to our established setup,\nthe curriculum training process is highly stable (Cf. Figure 3c).\n4\nMULTIALPACA: A MULTILINGUAL SELF-INSTRUCTION DATASET\nFine-tuning LLMs with instruction-based tasks has been proven effective in practice (Ouyang et al.,\n2022; Wei et al., 2022; Peng et al., 2023; Ye et al., 2023). By providing accurate task instructions\nduring the SFT phase, LLMs can not only learn to understand the requirements of each task via\nthe instruction part, but also show extensive abilities to cope with other types of tasks which are\neven unseen during training (Wei et al., 2022). Nevertheless, tuning multilingual LLMs is still\ntroubled by the scarcity of current SFT datasets. On the one hand, most instruction-based datasets\nare mainly in resource-rich languages (e.g., English or Chinese). To the best of our knowledge,\nthere is currently no high-quality multilingual instruction-based SFT dataset for LLM training. On\nthe other hand, most instructions are manufactured by experienced language speakers (e.g., Wei\net al., 2022). Although the quality of instructions is well preserved, the amount of tasks is rather\nscarce for fine-tuning LLMs.\nTo overcome these two drawbacks, we determine to extend the generality of our proposed POLYLM\nvia creating a multilingual SFT dataset \u2013 MULTIALPACA (Figure 6). Following the self-instruct\nparadigm proposed by recent studies (Wang et al., 2022; Taori et al., 2023), we query the available\nLLM for responses, iteratively collecting and filtering self-instruct examples to build our dataset.\nMULTIALPACA delivers comprehensive support on multilingualism, covering up to 11 languages\nincluding Arabic (Ar), German (De), Spanish (Es), French (Fr), Indonesian (Id), Japanese (Ja),\nKorean (Ko), Portuguese (Pt), Russian (Ru), Thai (Th), and Vietnamese (Vi). For each language,\nthe number of tasks in MULTIALPACA varies from 9,515 to 14,671, yielding 132,701 tasks in total.\n4.1\nTASK FORMAT\nWe first form the format of our tasks by referring to Taori et al. (2023), where each task contains\nthree parts: 1) \u201cinstruction\u201d describes the requirements of the corresponding task; 2) \u201cinput\u201d\ncan complement the \u201cinstruction\u201d to a complete question; and 3) \u201coutput\u201d is a correct\nanswer of the question. We notice that, Taori et al. (2023) constructed their dataset where each\n8\nTechnical Report\n2023-07-12\nAr, 11.06%\nDe, 7.17%\nEs, 7.50%\nFr, 8.54%\nId, 9.13%\nJa, 7.68%\nKo, 10.85%\nPt, 8.16%\nRu, 10.77%\nTh, 8.66%\nVi, 10.48%\nMULTIALPACA Dataset\nFigure 6: Statistics on the number of self-instruct tasks for each language in MULTIALPACA. For\nEnglish and Chinese subsets, we directly use the released tasks by ALPACA Taori et al. (2023) and\nCHINESE-ALPACA Cui et al. (2023) for POLYLM training.\nCollect \nMultilingual\nSeed Tasks\nIterative \nProgress\nMULTIALPACA \nDataset\nSample Tasks as \nDemonstrations to \nConstruct Prompts\nQuery LLMs to \nObtain Tuples \n(Instruction, Input, \nOutput)\nFormat Checking\nSimilarity Checking\nAdd New Tasks to the \nTask Pool\nFigure 7: Illustration on the construction of MULTIALPACA . We first translate English seed tasks\nfrom ALPACA (Taori et al., 2023) into multilingual ones. For each language, we arrange iterative\nprogress to collect the MULTIALPACA dataset, including constructing prompts, collecting tasks via\nquerying LLMs, format checking, filtering on diversity, and adding the new tasks into the task pool.\n\u201cinstruction\u201d can be equipped with multiple \u201cinput-output\u201d instances. For simplicity, we\nonly assign each \u201cinstruction\u201d with one \u201cinput-output\u201d instance.\n4.2\nMULTIALPACA CONSTRUCTION\nAs shown in Figure 7, we construct the MULTIALPACA dataset based on the following steps:4\nCollecting Multilingual Seed Tasks\nWe first obtain 175 seed tasks from Taori et al. (2023) to\nconstruct the multilingual ones for MULTIALPACA. After manually checking them, we remove\nthe cases where answering the questions requires cultural backgrounds (e.g., idiom explanation,\ncharacter-level riddle, and lyrics generation). Then, we marked the cases whose original \u201cinput\u201d or\n\u201coutput\u201d should be reserved (e.g., single-choice question, translation, bias identification, and code\ngeneration), where those tasks will directly use the original \u201cinput\u201d or \u201coutput\u201d across different\nlanguages for MULTIALPACA. Finally, we filter out 13 inappropriate seed tasks, and modified 23\nones marked due to the reuse of \u201cinput\u201d or \u201coutput\u201d parts. We translate the remaining 162 tasks\ninto the other 11 languages, yielding multilingual seed tasks for each language.\nIterative Progress\nWe manage the MULTIALPACA dataset construction progress as an iterative\none with multiple rounds. For each round, we manage the following five substeps in order:\n4See Appendix A for more details.\n9\nTechnical Report\n2023-07-12\n\u2022 Prompt Construction We follow Taori et al. (2023) to construct the prompts for MUL-\nTIALPACA when querying LLM for completion. When handling each involved language,\nfor each prompt, we sample two seed tasks and one MULTIALPACA task as the demonstra-\ntions, and guide the LLM to complete the other 17 tasks in the response. For each round,\nwe construct 100 prompts for querying the completion by LLM.5\n\u2022 Response Collection We collect the responses from CHATGPT via the OpenAI API ser-\nvice. The model we use is \u201cgpt-3.5-turbo-0301\u201d, which supports the processing of tokens\nup to 4,096.\n\u2022 Format Checking When checking the format, we first remove the last task if the response\nis stopped due to the exceeding of max sequence length. Then, we use the pre-defined\ntask format to help split the response string, so as to make sure each of the tasks contains\n\u201cinstruction\u201d, \u201cinput\u201d, and \u201coutput\u201d parts.\n\u2022 Similarity Checking After that, to preserve the diversity of MULTIALPACA dataset, we\nfurther check the similarity between the tasks that are newly collected and those from the\ntask pool. Following Taori et al. (2023), we compute the Rouge-L F-scores between the\ninstruction of each newly collected task and those of all collected ones. For each newly\ncollected task, it would be added to the task pool only if all the scores are lower than 0.7.\n\u2022 Task Pool Updating In the end, we update the task pool by adding the newly collected\ntasks, and arrange the next round for collecting MULTIALPACA self-instruct tasks.\nMULTIALPACA Dataset Export\nTotally, we arrange 10 rounds in the iterative progress when\nconstructing the MULTIALPACA dataset. We export all tasks from the task pool as the MULTIAL-\nPACA dataset for SFT learning.\n5\nMULTILINGUAL BENCHMARK\nWe aim to assess the capabilities of POLYLM from various perspectives: 1) the ability of large lan-\nguage models (LLMs) to understand and generate natural languages, as well as the ability to grasp\nworld knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to\nhandle cross-lingual tasks. Following the experiment design of previous work (Scao et al., 2022;\nAhuja et al., 2023), we gather a subset of datasets from previous NLP tasks to construct a multilin-\ngual benchmark. The brief statistics of all datasets in the benchmark can be found in Table 4. The\ndetails of how we frame all the tasks with prompting are listed in Appendix B.\nTask category\nTask\nTest\nLang.\nMetric\nPrompt\nNLU\nXNLI\n5,010\n15\nAcc.\n[Premise], right? {Yes/Also/No}, [Hypothesis]\nXCOPA\n500\n11\nAcc.\n[Prefix] {because/therefore} {choice1/choice2} [Suffix]\nPAWS-X\n2,000\n7\nAcc.\n[Sentence1], right? {Yes/No}, [Sentence2]\nXWINOGRAD\n83-2,325\n6\nAcc.\n[Prefix] {choice1/choice2} [Suffix]\nKnowledge\nTydiQA\n1,625-14,805\n9\nF1\n[Context][Question][Answer]\nNLG\nMTG\n200\n5\nRouge\n[Prompt][Input][Output]\nMT\nWMT20\n991-3,002\n8\nBLEU\n[INPUT] Translate this sentence from [SRC] to [TGT].\nTable 4: Multilingual Benchmark\n5.1\nTASKS IN BENCHMARK\nAll the datasets in the above multilingual benchmark can be divided into four groups: Natural Lan-\nguage Understanding, Knowledge, Natural Language Generation and Machine Translation. The\ndetails of each dataset that we use for benchmarking are given below.\nTo assess the comprehension capability of large models across various languages, we collect the\nmultilingual versions of datasets from seberal wide-used NLP benchmarks (Wang et al., 2018; 2019).\n5Except for the first round where the task pool is empty, we arrange 10 prompts for completion due to the\nsmall number of available tasks for demonstrations.\n10\nTechnical Report\n2023-07-12\nXNLI (Conneau et al., 2019) serves as a benchmark to evaluate a model\u2019s proficiency in predicting\ntextual entailment. The task entails the evaluation of whether two given sentences, A and B, convey\nthe same meaning, are contradictory, or are unrelated. The dataset has been professionally translated\ninto 14 languages from the original English XNLI dataset.\nPAWS-X (Yang et al., 2019) is a benchmark to evaluate the model\u2019s ability to judge whether one\nsentence is the paraphrase of another. It is professionally translated from the PAWS (Zhang et al.,\n2019) dataset into 6 diverse languages.\nXWinograd (Tikhonov & Ryabinin, 2021) serves as a benchmark to measure a model\u2019s common\nsense reasoning ability. Specifically, the task entails presenting the model with a brief contextual\npassage and requiring it to select the accurate term from a set of two options for a pronoun in the\npassage.\nXCOPA (Ponti et al., 2020) is another benchmark intended to assess the proficiency of models in\ncommonsense reasoning across languages. The dataset comprises translations and re-annotations\nof the English COPA Gordon et al. (2011), spanning 11 languages around the globe. Based on the\ngiven premise and prompt, the task is to choose the more plausible response between two answer\nchoices that can be inferred from the premise.\nTyDi QA (Clark et al., 2020) is a question-answering dataset covering 11 typologically diverse\nlanguages with 200K question-answer pairs. We use this dataset to evaluate the ability to grasp\nknowledge from natural text. Unlike previous datasets such as MLQA (Lewis et al., 2020) and\nMKQA (Longpre et al., 2020), this dataset is collected directly in each language without the use of\ntranslation. We select 5 languages out of 11 that are included in the pretraining corpora of POLYLM.\nFollowing the PaLM (Chowdhery et al., 2022), we evaluate models on the Gold passage task, which\nrequires answering questions based on a passage that is guaranteed to contain the answer.\nMTG (Chen et al., 2021) is used to assess the efficacy of large language models in generating longer\nresponses across diverse usage scenarios and multiple languages. MTG covers four different gener-\nation tasks: Story Ending Generation (SG), Title Generation (TG), Question Generation (QG), and\nSummarization (Summ). The datasets are originally written in English, subsequently extended into\nfour other languages (German, French, Spanish, and Chinese) through the use of machine transla-\ntion and human annotation. The effectiveness of LLM-generated responses is evaluated using the\naverage of Rouge1, Rouge2, and RougeL.\nWMT20 (Barrault et al., 2020) is used to study the cross-lingual proficiency of large language\nmodels in accomplishing translation tasks, as the process of translation entails both comprehending\nthe semantic of the input in one language and expressing it in another. We select translation tasks\nbetween English and each of the following languages as benchmark languages: German, Japanese,\nRussian, and Chinese. The results are evaluated using the SacreBLEU (Post, 2018) and the scores\nfor BLEU (Papineni et al., 2002) on the test set are reported.\n5.2\nEVALUATION DESIGN\nFor metric evaluation, the tasks included in our benchmark can be divided into two categories:\nclassification-style tasks and generation-style tasks.\nClassification-style tasks require selecting the correct option from several options, such as the XNLI\ndataset. To evaluate these tasks, following the way in Gao et al. (2021), we design the problem in\nthe form of a cloze test, where each option is filled in to construct a complete sentence. We then\nchoose the correct answer by separately calculating the log-likelihood of each completed sentence\nand selecting the one with the highest value.\nGeneration-style tasks, such as machine translation, require generating answers with several natural\nsentences. For these tasks, we adopt greedy decoding for deterministic results. Considering the\nefficiency of decoding, we restrict the maximum number of generated tokens to 256. For foundation\nmodels, we choose the result before the first \u2018\\n\u2019 as the answer, while for models that have undergone\ninstruction tuning, we decode until the EOS token appears.\nIn evaluating foundation models, considering that models have not been able to understand instruc-\ntions, we adopt in-context learning (Brown et al., 2020) to evaluate the model for generation-style\ntasks. We generally choose no more than five examples due to the model\u2019s context window limita-\n11\nTechnical Report\n2023-07-12\n20\n30\n40\n50\n60\nar\nde\nen\nes\nfr\nru\nth\ntr\nvi\nzh\nXNLI\nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\n30\n40\n50\n60\n70\nde\nen\nes\nfr\nja\nko\nzh\nPAWS-X \nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\n20\n40\n60\n80\nid\nit\nth\ntr\nvi\nzh\nXCOPA\nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\n30\n50\n70\n90\nen\nfr\nja\npt\nru\nzh\nXWinograd\nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\nFigure 8: Accuracy of NLU tasks under the zero-shot setting. Best reviewed in colors. Results\nindicate that POLYLM performs comparably or better than LLaMA-13B in the English scenario,\nand exhibits significant enhancements in multilingual evaluation.\ntion. For tasks that have well-divided training/development sets, we randomly draw examples from\nthem for each test sample. Otherwise, we draw examples randomly from the test sets except for the\ncurrent sample.\n6\nEXPERIMENTS\nIn this section, we provide separate comparison results for the pre-training and SFT models. Then,\nwe analyze the effectiveness of our model in three aspects: curriculum learning, multilingual in-\nstruction finetuning, and the scaling for model size.\n6.1\nCOMPARISONS BETWEEN PRE-TRAINED FOUNDATIONAL MODELS\nFor the pre-trained models, we selected two mainstream open-source models as our baselines.\n\u2022 LLaMA (Touvron et al., 2023) is a pre-trained language model released by MetaAI, which\nincludes 7B, 13B, 30B, and 65B versions. The pre-training dataset is sourced from publicly\n12\nTechnical Report\n2023-07-12\n0\n20\n40\n60\nar\nen\nid\nko\nru\nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\n(a) F1 Scores on TyDiQA.\n0\n5\n10\n15\n20\nzh\nen\nes\nfr\nde\nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\n(b) Average Rouge Scores on MTG.\n0\n10\n20\n30\n40\nde\u2192en\nja\u2192en\nru\u2192en\nzh\u2192en\nen\u2192de\nen\u2192ja\nen\u2192ru\nen\u2192zh\nBLOOM-7.1B\nLLaMA-13B\nPOLYLM-13B\n(c) BLEU Scores on WMT20.\nFigure 9: Performance on knowledge, neural language generation and machine translation tasks\nunder the one-shot setting. Best reviewed in colors.\navailable corpora. The 33B and 65B models are trained on 1.4 T tokens, while the 7B and\n13B models are trained on 1 T tokens. To ensure an equal parameter count comparison\nwith POLYLM, we mainly take the 13B version into consideration.\n\u2022 BLOOM (Scao et al., 2022) is a multilingual model that covers 46 natural languages and\n13 programming languages with a maximum of 176B parameters. Since BLOOM has not\nreleased a 13B version, we opt for the BLOOM-7.1B model as our baseline.\nWe evaluate POLYLM across various multilingual tasks, covering natural language understanding\n(NLU), knowledge, natural language generation (NLG) and machine translation (MT). To make a\nclearer comparison of the multilingual capabilities of different models, we present the results using\nradar charts, with detailed results available in the C.\nNatural Language Understanding. Figure 8 shows the results on four NLU tasks under the zero-\nshot setting. POLYLM-13B shows comparable performance to the English-centric LLaMA-13B\nmodel in the English scenario. Moreover, it yields substantial improvements of 7.2% and 19.1%\non PAWS-X and XNLI respectively. For languages other than English (the multilingual column),\nPOLYLM-13B outperforms LLaMA-13B with average improvement up to 7.6%, 5.6%, 3%, and\n11% on XCOPA, PAWS-X, XWinagrad, and XNLI, respectively. When compared to the multilin-\ngual language model BLOOM-7.1B, POLYLM-13B outperforms with an average improvement of\n13\nTechnical Report\n2023-07-12\n20\n30\n40\n50\n60\nar\nde\nen\nes\nfr\nru\nth\ntr\nvi\nzh\nXNLI\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\n30\n40\n50\n60\n70\nde\nen\nes\nfr\nja\nko\nzh\nPAWS-X\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\n20\n40\n60\n80\nid\nit\nth\ntr\nvi\nzh\nXCOPA\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\n30\n50\n70\n90\nen\nfr\nja\npt\nru\nzh\nXWinograd\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\nFigure 10: Performance of instruction-followed models on NLU tasks under the zero-shot setting.\nBest reviewed in colors.\n4.2%, 4.1%, 3.4%, and 4% points on the respective tasks. This improvement can be attributed to the\nhigher percent of multilingual text during pre-training and curriculum learning strategy.\nKnowledge. We evaluate our model on grasping multilingual knowledge by using the TyDiQA\nbenchmark in the one-shot setting. Upon careful analysis of Figure 9a, it is evident that BLOOM-\n7.1B experiences significant performance drops in the Korean (ko) and Russian (ru) language di-\nrections, whereas LLaMA-13B and POLYLM-13B exhibit better balance across all five languages.\nFurthermore, POLYLM-13B has an additional advantage of an average 1.2-point lead over LLaMA-\n13B.\nNatural Language Generation. Figure 9b displays the Rouge scores of four diverse NLG tasks in\nmultilingual settings. From a multilingual perspective, POLYLM-13B outperforms all other models\nacross four languages, namely Chinese (zh), Spanish (es), French (fr), and German (de). Moreover,\nin terms of task types, POLYLM-13B performs the best in question generation (QG) and summa-\nrization (Sum) tasks, while also showing comparable performance to the best model LLaMA-13B in\nthe text generation (TG) task. Across all MTG tasks and languages, POLYLM-13B has an average\nscore advantage of 1.6 and 2.3 compared to LLaMA-13B and BLOOM-7.1B, respectively.\nMachine Translation We focus on evaluating the translation performance on four typologically\ndiverse languages from WMT20 datasets, including translation directions both from and to En-\nglish. Results of Figure 9c show that POLYLM-13B achieves similar performance to LLaMA-13B\nin the multilingual to English directions and surpasses LLaMA-13B and BLOOM-7.1B with average\nBLEU scores of 5.4 and 15.8 in the English to multilingual directions.\n14\nTechnical Report\n2023-07-12\n0\n10\n20\n30\n40\n50\n60\nar\nen\nid\nko\nru\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\n(a) F1 Scores on TyDiQA.\n0\n5\n10\n15\n20\n25\nzh\nen\nes\nfr\nde\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\n(b) Average Rouge Scores on MTG.\n0\n5\n10\n15\n20\n25\n30\n35\nde\u2192en\nja\u2192en\nru\u2192en\nzh\u2192en\nen\u2192de\nen\u2192ja\nen\u2192ru\nen\u2192zh\nBLOOMZ-MT-7.1B\nLLaMA-Alpaca-13B\nPOLYLM-MultiAlpaca-13B\n(c) BLEU Scores on WMT20.\nFigure 11: Performance of instruction-followed models on knowledge, neural language generation\nand machine translation tasks under the zero-shot setting. Best reviewed in colors.\n6.2\nCOMPARISONS BETWEEN INSTRUCTION-FOLLOWED MODELS\nThis section focuses on evaluating the effectiveness of instruction-followed models founded on\nthe pre-trained language models discussed in Section 6.1. We conduct a comparative analysis of\nPOLYLM-MULTIALPACA-13B that is fine-tuned on POLYLM-13B using MULTIALPACA, against\ntwo other publicly available models:\n\u2022 BLOOMZ-MT-7B is initially pre-trained on BLOOM-7B, and later fine-tuned on the mul-\ntilingual task mixture xP3-MT (Muennighoff et al., 2022).\n\u2022 LLaMA-Alpaca-13B is built based on the pre-trained model LLaMA-13B and fine-tuned\non the English self-instruction dataset ALPACA (Taori et al., 2023).\nFigure 10 and 11 present the performance comparisons of instruction-followed models with the zero-\nshot setting, considering various tasks and language directions. The results indicate that POLYLM-\nMULTIALPACA-13B is comparable or superior to LLaMA-Alpaca-13B on all English tasks, al-\nthough the latter is primarily trained on English-only instructions. On other non-English tasks,\nPOLYLM-MULTIALPACA-13B significantly outperforms LLaMA-Alpaca-13B. This superiority can\nbe attributed to the inclusion of more well-balanced multilingual datasets during the pre-training\nand instruction fine-tuning. In comparison to BLOOMZ-MT-7B, POLYLM-MULTIALPACA-13B\nhas demonstrated consistent improvements across all tasks and languages. We have observed an\noutlier MTG, and we speculate that this may be due to the fact that MTG testsets are part of the xP3\n15\nTechnical Report\n2023-07-12\n1.00\n6.00\n11.00\n16.00\n21.00\n26.00\n31.00\nde2en ja2en\nru2en zh2en en2de en2ja\nen2ru en2zh\nBLEU\nMT\nw/o CL\nw/ CL\n(a) NLU tasks\n(b) Machine translation task\n30.0\n40.0\n50.0\n60.0\n70.0\nid\nit\nth\ntr\nvi\nzh\nAcc (%)\nXCopa\nw/o CL\nw/ CL\n40.0\n45.0\n50.0\n55.0\n60.0\n65.0\nde\nen\nes\nfr\nja\nko\nzh\nAcc (%)\nPAWS-X\nw/o CL\nw/ CL\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\nen\nfr\nja\npt\nru\nzh\nAcc (%)\nXWinograd\nw/o CL\nw/ CL\n30.0\n35.0\n40.0\n45.0\n50.0\n55.0\nar\nde\nen\nes\nfr\nru\ntr\nth\nvi\nzh\nAcc (%)\nXNLI\nw/o CL\nw/ CL\nFigure 12: POLYLM-13B trained with curriculum learning reveals better performance in multiple\nlanguages in NLU and MT tasks.\ndataset. We plan to refine our instruction tuning process for POLYLM by utilizing the xP3 dataset in\norder to delve deeper into this inconsistency.\nNote that it is not feasible to fully assess the effectiveness of the model\u2019s performance through\ndownstream NLP tasks after instruction fine-tuning. Therefore, we have presented selected examples\nfor qualitative analysis, which are fully outlined in Appendix D.\n6.3\nANALYSIS\nCurriculum Learning.\nWe validate the effectiveness of the curriculum learning strategy in NLU\nand MT tasks of multilingual benchmark (Section 5.1) by comparing the following variants:\n(1) w/o CL POLYLM-13B trained without curriculum learning, which is only optimized in pre-\ntrained dataset.\n(2) w/ CL POLYLM-13B trained with curriculum learning, using about 100B high-quality multilin-\ngual data selected from the pretrained dataset.\nPlease note that we only focus on the languages included during curriculum learning. Referring to\nFigure 12, the model with curriculum learning has achieved stable progress in mainly all languages\nin both NLU and MT tasks. First of all, the model performance is enhanced in most low-resource\nlanguages, indicating that the general knowledge can be effectively transferred to these languages\nthrough raising data proportion. Additionally, the model retains its superior performance in English,\n16\nTechnical Report\n2023-07-12\n(a) NLU tasks\n(b) Machine translation task\nFigure 13: The performance of models with different scales on different tasks.\nModel\nWMT20 Machine Translation\nTyDiQA\nen2zh\nen2de\nen2ru\nen2ja\nzh2en\nde2en\nru2en\nja2en\nAvg.\nen\nar\nid\nko\nru\nAvg.\nPOLYLM-Alpaca-13B\n10.0\n17.1\n5.0\n6.3\n12.2\n23.8\n24.7\n10.0\n13.6\n53.7\n45.5\n55.3\n29.9\n40.4\n45.0\nPOLYLM-MultiAlpaca-13B\n21.9\n25.9\n17.9\n16.2\n25.0\n34.2\n31.8\n14.3\n23.4\n58.7\n50.7\n52.1\n30.1\n40.3\n46.4\nTable 5: BLEU scores on WMT20 machine translation tasks and F1 scores on Tydi-QA task.\nwhich illustrates that improving data quality for high-resource languages can achieve competitive\nresults to training with larger amounts of data. Finally, it is worth noting that introducing more\nmultilingual parallel data during the curriculum learning significantly boost the model performance\non translation task.\nMultilingual Self-instruction.\nHere we highlight the advantages of MULTIALPACA over English-\nonly ALPACA (Taori et al., 2023), particularly in cross-lingual tasks (i.e., machine translation).\nAs illustrated in Table 5, compared to the model fine-tuned only using ALPACA, POLYLM-\nMULTIALPACA-13B exhibits substantial improvements in TyDiQA and multiple WMT20 trans-\nlation tasks, with enhancements of +10 BLEU and +1.4% F1. These results suggest that MULTI-\n17\nTechnical Report\n2023-07-12\nALPACA is capable of simulating the cross-lingual alignment ability of the foundational, as well as\nfacilitating the comprehension of multilingual instructions.\nScaling for Model Size.\nIn addition to the 13B model, we also release a smaller 1.7B model. Re-\ncent studies highlight the critical role of model size in the performance of large language models\n(LLMs), with much of this work focusing on English (Kaplan et al., 2020; Rae et al., 2021; Bider-\nman et al., 2023; Touvron et al., 2023). In this section, we present results for POLYLM-13B and\nPOLYLM-1.7B to investigate the impact of model size on multilingual abilities. Consistent with the\naforementioned experimental setup for the validation of base model, we compare the two models\nusing a one-shot setting. As illustrated in Figure 13, the 13B model significantly outperforms the\n1.7B model across all compared multilingual tasks. We posit that multilingual problems are more\ncomplex than their monolingual counterparts and may depend more heavily on the model\u2019s through-\nput. Moving forward, we plan to release additional models of varying sizes, with the ultimate goal\nof refining the scaling law for multilingualism.\n7\nCONCLUSION\nMultilingualism poses an inevitable challenge for LLM due to the scarcity of resources. In this\nwork, we release POLYLM \u2013 a new multilingual LLM, alone with MULTIALPACA \u2013 a multilingual\ninstruction dataset, and a multilingual benchmark. Quantitative and qualitative analyses demonstrate\nthe superiority of POLYLM over open-source models in non-English languages. We find that incor-\nporating curriculum learning strategy can boost the performance of LLM on non-English languages,\nwithout impeding its English proficiency. In addition, fine-tuning LLM with multilingual instruction\ndata can considerably improve zero-shot performance on these languages.\nThere is still ample opportunity for refinement in our work endeavors. For instance, while we\nbriefly assess the model\u2019s capacity to comprehend multilingual instructions, there is potential for\nfurther optimization through the amalgamation of data sources (Wang et al., 2023; Longpre et al.,\n2023), evolutionary methods (Xu et al., 2023) and diversification strategies (Zhou et al., 2023).\nMoreover, in our current version, we adopt absolute position encoding, which adheres to the early\ndefault configuration in Megatron toolkit (Shoeybi et al., 2020). Future iterations should incorporate\ntechniques that facilitate the expansion of window size, such as rotary position encoding (Su et al.,\n2021; Chen et al., 2023) or ALiBi (Press et al., 2022).\nLanguage serves as a conduit for culture, and the unique contributions of various languages enrich\nand diversify our global community. Nevertheless, the advancement of LLM may inadvertently\namplify the influence of prominent languages and present a formidable obstacle for low-resource\nlanguages. In light of these concerns, we aspire that our research will motivate further inquiry and\ninnovation in the field of multilingual LLM.\nETHICS STATEMENT\nIn this paper, we propose POLYLM, an LLM which offers a wider support on non-English languages.\nOur contributions are fully methodological: adding the support of multilingualism to LLM during\ntraining and SFT phases. However, when building our POLYLM model, it is unavoidable that our\nPOLYLM might exhibit several common deficiencies of language models, e.g., hallucination and\ntoxicity. Specifically, as the collected MULTIALPACA dataset are generated by CHATGPT, the\npseudo tasks might give inappropriate pseudo tasks which are hardly filtered out, e.g., hallucinated\nreasoning and anti-fact statements (Brown et al., 2020; OpenAI, 2023). Besides, POLYLM may\ndeliver toxic texts, which might be gender- or race-biased like other existing LLMs (Taori et al.,\n2023; Cui et al., 2023).\nDespite the ethical concerns above, we think that those problems are of vital importance to the\nAI community to study the deficiencies of LLMs. We recommend that the users of POLYLM and\nMULTIALPACA deploy our released materials only for research proposals. Besides, we suggest\nthe users better identify the deficiencies of those contents, and welcome the following researchers\nto facilitate further research on the alignment between the LLM outputs and human values with\nPOLYLM and MULTIALPACA materials.\n18\nTechnical Report\n2023-07-12\nREFERENCES\nKabir Ahuja, Rishav Hada, Millicent A. Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina,\nTanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. Mega: Mul-\ntilingual evaluation of generative ai. ArXiv, abs/2303.12528, 2023.\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\nGpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.\nhttps://github.com/nomic-ai/gpt4all, 2023.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nLo\u00a8\u0131c Barrault, Magdalena Biesialska, Ond\u02c7rej Bojar, Marta R. Costa-juss`a, Christian Federmann,\nYvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi,\nPhilipp Koehn, Chi-kiu Lo, Nikola Ljube\u02c7si\u00b4c, Christof Monz, Makoto Morishita, Masaaki Nagata,\nToshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2020 con-\nference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine\nTranslation, pp. 1\u201355, Online, November 2020. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2020.wmt-1.1.\nYoshua Bengio, R\u00b4ejean Ducharme, and Pascal Vincent. A neural probabilistic language model. In\nAdvances in neural information processing systems, 2000.\nYoshua Bengio, J\u00b4er\u02c6ome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\nProceedings of the 26th Annual International Conference on Machine Learning, ICML 2009,\nvolume 382, pp. 41\u201348. ACM, 2009.\nURL https://doi.org/10.1145/1553374.\n1553380.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. arXiv preprint\narXiv:2304.01373, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\nof large language models via positional interpolation, 2023.\nYiran Chen, Zhenqiao Song, Xianze Wu, Danqing Wang, Jingjing Xu, Jiaze Chen, Hao Zhou, and\nLei Li. Mtg: A benchmarking suite for multilingual text generation. In NAACL-HLT, 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner\nPope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,\nAnselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00b4\u0131a, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omer-\nnick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark D\u00b4\u0131az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with\npathways. ArXiv, abs/2204.02311, 2022.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,\nand Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the Association for Computational Linguistics,\n2020.\n19\nTechnical Report\n2023-07-12\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm\u00b4an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. In ACL, 2019.\nYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and\nalpaca. arXiv preprint arXiv:2304.08177, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\nTang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language\nmodel evaluation, Sep 2021. URL https://doi.org/10.5281/zenodo.5371628.\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\nnetworks. In International Conference on Artificial Intelligence and Statistics, 2010.\nAndrew S. Gordon, Zornitsa Kozareva, and Melissa Roemmele. Semeval-2012 task 7: Choice of\nplausible alternatives: An evaluation of commonsense causal reasoning. In International Work-\nshop on Semantic Evaluation, 2011.\nKenneth Heafield. Kenlm: Faster and smaller language model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pp. 187\u2013197, 2011.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Jo\u02dcao Carreira.\nPerceiver: General perception with iterative attention. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, volume 139 of Proceedings of Machine Learning\nResearch, pp. 4651\u20134664. PMLR, 2021. URL http://proceedings.mlr.press/v139/\njaegle21a.html.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00b4erve J\u00b4egou, and Tomas\nMikolov. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651,\n2016.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\nM. Pawan Kumar, Benjamin Packer, and Daphne Koller.\nSelf-paced learning for latent vari-\nable models.\nIn Advances in Neural Information Processing Systems 23:\n24th Annual\nConference on Neural Information Processing Systems 2010, pp. 1189\u20131197. Curran Asso-\nciates, Inc., 2010. URL https://proceedings.neurips.cc/paper/2010/hash/\ne57c6b956a6521b28495f2886ca0977a-Abstract.html.\n20\nTechnical Report\n2023-07-12\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evalu-\nating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 2020.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, and Adam Lopez.\nFew-shot learning with multilingual generative language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pp. 9019\u20139052, 2022.\nS. Longpre, Yi Lu, and Joachim Daiber. Mkqa: A linguistically diverse benchmark for multilingual\nopen domain question answering. Transactions of the Association for Computational Linguistics,\n9:1389\u20131406, 2020.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023.\nTomas Mikolov, Martin Karafi\u00b4at, Luk\u00b4as Burget, Jan Cernock\u00b4y, and Sanjeev Khudanpur.\nRecurrent neural network based language model.\nIn Takao Kobayashi, Keikichi Hi-\nrose, and Satoshi Nakamura (eds.), INTERSPEECH, pp. 1045\u20131048. ISCA, 2010.\nURL\nhttp://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.\nhtml#MikolovKBCK10.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual gen-\neralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike,\nand Ryan Lowe.\nTraining language models to follow instructions with human feedback.\nIn\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-\nral Information Processing Systems, 2022. URL https://openreview.net/forum?id=\nTG8KACxEON.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Annual Meeting of the Association for Computational Lin-\nguistics, 2002.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4. arXiv preprint arXiv:2304.03277, 2023.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), pp. 2227\u20132237, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/N18-1202.\nURL\nhttps://aclanthology.org/N18-1202.\nEdoardo M. Ponti, Goran Glava s, Olga Majewska, Qianchu Liu, Ivan Vuli\u2019c, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. arXiv preprint, 2020. URL\nhttps://ducdauge.github.io/files/xcopa.pdf.\nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference\non Machine Translation: Research Papers, pp. 186\u2013191, Belgium, Brussels, October 2018. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\nW18-6319.\n21\nTechnical Report\n2023-07-12\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation, 2022.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training.\nURL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00b4e, Alexandra Sasha Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and\nTatiana Shavrina. mgpt: Few-shot learners go multilingual, 2022.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism, 2020.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deep-\nspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990, 2022.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.\nRoformer:\nEnhanced transformer with rotary position embedding.\nCoRR, abs/2104.09864, 2021.\nURL\nhttps://arxiv.org/abs/2104.09864.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nAlexey Tikhonov and Max Ryabinin. It\u2019s all in the heads: Using attention heads as a baseline for\ncross-lingual transfer in commonsense reasoning, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAshish Vaswani,\nNoam Shazeer,\nNiki Parmar,\nJakob Uszkoreit,\nLlion Jones,\nAidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn Advances\nin Neural Information Processing Systems 30, NIPS 2017 4-9 December 2017, Long\nBeach, CA, USA, pp. 5998\u20136008, 2017.\nURL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. ArXiv,\nabs/1804.07461, 2018.\n22\nTechnical Report\n2023-07-12\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. In Neural Information Processing Systems, 2019.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\nChandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nHow far can camels go? exploring the state of instruction tuning on open resources, 2023.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna-\ntional Conference on Learning Representations, 2022. URL https://openreview.net/\nforum?id=gEZrGCozdqR.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\nIn International Conference on Machine Learning, pp. 10524\u201310533. PMLR, 2020.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv\npreprint arXiv:2010.11934, 2020.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. Paws-x: A cross-lingual adversarial\ndataset for paraphrase identification. In EMNLP, 2019.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo.\nIn-context instruction learning. arXiv preprint arXiv:2302.14691, 2023.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414, 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nYuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase Adversaries from Word Scram-\nbling. In NAACL, 2019.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\nLima: Less is more for alignment, 2023.\nA\nDETAILED SETTING FOR MULTIALPACADATASET CONSTRUCTION\nA.1\nPROMPT FOR MULTIALPACA DATASET CONSTRUCTION\nWe show the used prompt when constructing MULTIALPACA dataset in Table 6. We mainly refer\nto Taori et al. (2023), and adopt our prompt to multilingual scenarios after minor revisions. Briefly,\nin the prompt, we list several requirements of the self-instruct tasks in the prompt, i.e., the used\nlanguage, the format, the diversity, and the lengths of tasks within each single response. We also\nadd three demonstrations to help the model generate the tasks which follow the pre-defined format.\n23\nTechnical Report\n2023-07-12\nPrompt template for MULTIALPACA dataset construction.\nYou are asked to come up with a set of 20 diverse task instructions. These task instructions will\nbe given to a GPT model and we will evaluate the GPT model for completing the instructions.\nHere are the requirements:\n1. Try not to repeat the verb for each instruction to maximize diversity.\n2. The language used for the instruction also should be diverse. For example, you should\ncombine questions with imperative instructions.\n3. The type of instructions should be diverse. The list should include diverse types of tasks\nlike open-ended generation, classification, editing, etc.\n4. A GPT language model should be able to complete the instruction. For example, do not ask\nthe assistant to create any visual or audio output. For another example, do not ask the assistant\nto wake you up at 5pm or set a reminder because it cannot perform any action.\n5. The instructions should be in [language].\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question\nis permitted.\n7. You should generate an appropriate input to the instruction. The input field should contain\na specific example provided for the instruction. It should involve realistic data and should\nnot contain simple placeholders. The input should provide substantial content to make the\ninstruction challenging but should ideally not exceed 100 words.\n8. Not all instructions require input. For example, when an instruction asks about some general\ninformation, \u201cwhat is the highest peak in the world\u201d, it is not necessary to provide a specific\ncontext. In this case, we simply put \u201c\u00a1noinput\u00bf\u201d in the input field.\n9. The output should be an appropriate response to the instruction and the input. Make sure\nthe output is less than 200 words.\nThere are 3 examples.\n1. Instruction: [task1 instruction]\n1. Input:\n[task1 input]\n1. Output:\n[task1 output]\n2. Instruction: [task2 instruction]\n2. Input:\n[task2 input]\n2. Output:\n[task2 output]\n3. Instruction: [task3 instruction]\n3. Input:\n[task3 input]\n3. Output:\n[task3 output]\nPlease generate the following 17 tasks that are similar to the above examples.\nTable\n6:\nPrompt\nfor\nconstructing\nMULTIALPACA\ntasks.\nWe\nspecify\nthe\nlanguage\nof generated tasks with the parameter \u201clanguage\u201d,\nand the used demonstrations with\n\u201ctask[123] {instruction,input,output}\u201d.\n24\nTechnical Report\n2023-07-12\n0\n5000\n10000\n15000\n40\n60\n80\n100\nNumber of tasks before checking\nPercentage (%)\nAr\nDe\nEs\nFr\nId\nJa\nKo\nPt\nRu\nTh\nVi\nFigure 14: The percentage (%) of the MULTIALPACA examples which pass the format and similarity\nchecking. For each language, we compute the ratio of collected tasks to that before format and\nsimilarity checking during each round.\nLanguage\n# tasks\nRatio (%)\nAr\n14,671\n11.06\nDe\n9,515\n7.17\nEs\n9,958\n7.50\nFr\n11,332\n8.54\nId\n12,117\n9.13\nJa\n10,191\n7.58\nKo\n14,402\n10.85\nPt\n10,825\n8.16\nRu\n14,286\n10.77\nTh\n11,496\n8.66\nVi\n13,908\n10.48\nTotal\n132,701\n100.00\nTable 7: The number of tasks for each language in MULTIALPACA dataset.\nA.2\nFORMAT AND SIMILARITY CHECKING\nAfter collecting the pseudo tasks for each language, we first remove the cases which contain website\nlinks. Then, we tokenize the \u201cinstruction\u201d, \u201cinput\u201d, and \u201coutput\u201d with available tokeniza-\ntion toolkits.6\nBesides, we found that some of the tasks may give redundant information within both\n\u201cinstruction\u201d and \u201cinput\u201d part. We revise those examples to make them available as much\nas possible. In detail, if the \u201cinput\u201d part can be a sub-string of the \u201cinstruction\u201d, we mark\nthe \u201cinput\u201d part as an empty one (using the placeholder \u201c<noinput>\u201d). Otherwise, we compute\nthe Rouge-L F-score between the \u201cinstruction\u201d and the \u201cinput\u201d part, filtering out the tasks\nwhose result is over 0.5. Specially, for the Ko, Vi, and Ar tasks, we determine the threshold of\nRouge-L F-score as 0.3, 0.3, and 0.2 for higher diversity of tasks, respectively.\nWe show the percentage of remaining examples after similarity checking in Figure 14. For each\nlanguage, we show the number of self-instruct tasks in MULTIALPACA in Table 7.\n6nltk.tokenize.wordpunct tokenize for Ar, De, Es, Fr, Id, Pt, Ru, and Vi; kytea for Ja; Okt\nfor Ko; thai tokenizer for Th.\n25\nTechnical Report\n2023-07-12\nB\nDETAILS OF TASK FORMATTING\nWe list the detailed format of all the tasks in our multilingual benchmark in the following tables.\nContext \u2192\nUsing these eight simple techniques, you can fabricate a news story in the\ncomfort of your own home., right?\nCorrect Answer \u2192\nNo, Only news reporters in a newsroom can write a news story, and it\ntakes 20 steps to do it.\nIncorrect Answer \u2192\nYes, Only news reporters in a newsroom can write a news story, and it\ntakes 20 steps to do it.\nAlso, Only news reporters in a newsroom can write a news story, and it\ntakes 20 steps to do it.\nTable 8: The task format of XNLI. We normalize the log-likelihood by token numbers of all the\ncorrect and incorrect answers and choose the one with the largest score as prediction.\nContext \u2192\nIl cursore sullo schermo del computer si `e mosso perch\u00b4e\nCorrect Answer \u2192\nl\u2019utente ha spostato il mouse.\nIncorrect Answer \u2192\nl\u2019utente ha cliccato il mouse.\nTable 9: The task format of XCOPA.\nContext \u2192\nHe put snow on the smiley face because\nCorrect Answer \u2192\nsnow was wet.\nIncorrect Answer \u2192\nthe smiley face was wet.\nTable 10: The task format of XWinograd.\nContext \u2192\nThe first category is monovalent verbs , where there is only one semantic\nargument and it consists of both unergative verbs and unaccusative verbs\n., right?\nCorrect Answer \u2192\nYes, The first category is unergative verbs , where there is only one unac-\ncountable argument and consists of both semantic verbs and monovalent\nverbs .\nIncorrect Answer \u2192\nNo, The first category is unergative verbs , where there is only one unac-\ncountable argument and consists of both semantic verbs and monovalent\nverbs .\nTable 11: The task format of PAWS-X.\nC\nDETAILS OF EXPERIMENTAL RESULTS\nC.1\nRESULTS OF PRETRAINED LANGUAGE MODELS\nC.2\nRESULTS OF SFT MODELS\nD\nDEMONSTRATIONS FOR QUALITATIVE ANALYSIS\n26\nTechnical Report\n2023-07-12\nContext \u2192\nRead the context and answer the question in one or a few words in\nEnglish.\nContext (English):\nFootball games last for a total of 60 minutes\nin professional and college play and are divided into two-halves of 30\nminutes and four-quarters of 15 minutes.[74][75] High school football\ngames are 48 minutes in length with two-halves of 24 minutes and\nfour-quarters of 12 minutes.[76] (...).\nAnswer:\nTarget Completion \u2192\nclass B\nTable 12: The task format of Tydi-QA.\nContext \u2192\nPlease generate a title for the following document in English\ndocument: justin timberlake\u2019s super bowl lii halftime show is approach-\ning, and there are rumors circulating around the internet saying that\nnsync may have a reunion at the annual championship. well, it appears\nthat fans of the boyband have to kiss that dream goodbye after joey fa-\ntone shuts down the rumors. tmz recently caught up with the 40-year-old\nhot dog purveyor and asked whether the reunion rumors were true. (...).\ntitle:\nTarget Completion \u2192\njoey fatone shuts down nsync reunion rumors at the super bowl lii\nTable 13: The task format of Title Generation task in MTG.\nContext \u2192\nWrite a story end of the following story in just a few sentences in En-\nglish.\nstory: john had a roommate he wanted to prank. john called and ordered\nten pizzas to be delivered to the apartment. suddenly, john\u2019s roommate\ngot a call and had to leave.\nstory ending:\nTarget Completion \u2192\nit was too late to cancel the pizza order! john wound up paying for all\nthe pizzas!\nTable 14: The task format of Story Ending Generation task in MTG.\nContext \u2192\nGiven a passage and a concept that can be found in this passage, please\ngenerate a question in English, the answer of which is this concept and\nis answerable after reading this passage.\npassage: a treaty is an official, express written agreement that states use\nto legally bind themselves. a treaty is the official document which ex-\npresses that agreement in words; and it is also the objective outcome of\na ceremonial occasion which acknowledges the parties and their defined\nrelationships.\nanswer: themselves\nquestion:\nTarget Completion \u2192\nwho is responsible for the legally-bound obligations of the parties to a\ntreaty?\nTable 15: The task format of Question Generation task in MTG.\n27\nTechnical Report\n2023-07-12\nContext \u2192\nPlease generate a short summary of the given document in English\ndocument: a man whose girlfriend ran off with his stepfather and gave\nbirth to a baby nearly four years ago has spoken of his delight after dna\ntests proved the baby is actually his. love rat stan crowther, 47, and\nrachel delaney, 18 at the time, set up home together and rachel then\ngave birth to a baby daughter, whom stan believed was his. but recent\nbombshell dna tests showed that the baby, living in chorley, lancashire,\nactually belongs to her former partner, and stan\u2019s former stepson, ashley\nmercer, 27. (...).\nsummary:\nTarget Completion \u2192\nashley mercer was 22 when he discovered his girlfriend rachel delaney,\n18, was having an affair with his stepfather stan crowther, 43. stan had\nbeen married to ashley\u2019s mother mandy rourke for ten years. stan and\nrachel moved in together and shortly after rachel, from chorley, lan-\ncashire, gave birth to a baby girl. stan assumed the baby to be his but\ndna tests have since revealed ashley is the four-year-old girl\u2019s real fa-\nther. ashley\u2019s mother mandy rourke has now forgiven her ex-husband.\nstan and rachel are no longer together.\nTable 16: The task format of Summarization task in MTG.\nContext \u2192\nOil falls after Iran claims US offered to remove sanctions, Trump denies\nTranslate this sentence from English to German.\nTarget Completion \u2192\n\u00a8Ol f\u00a8allt, nachdem Iran behauptet, die USA h\u00a8atten Aufhebung der Sank-\ntionen angeboten, Trump dementiert\nTable 17: The task format of Translation.\nen\nzh\nar\nes\nfr\nru\nth\ntr\nvi\nde\nAverage\nbg\nel\nhi\nsw\nur\nAverage\nBLOOM-7.1B\n53.9\n35.5\n33.8\n48.7\n49.8\n42.6\n34.9\n34.9\n47.4\n39.6\n42.1\n39.3\n35.5\n46.7\n37.9\n41.9\n40.3\nLLaMA-13B\n35.5\n34.6\n34.1\n33.4\n33.6\n33.6\n34.6\n34.0\n34.1\n35.2\n34.3\n33.9\n34.5\n35.7\n33.2\n34.1\n34.3\n{Poly}LM-13B\n54.6\n35.9\n33.6\n50.0\n52.1\n49.0\n44.6\n44.5\n46.7\n50.2\n46.1\n36.3\n33.8\n34.9\n34.4\n33.5\n34.6\nTable 18: Accuracy on XNLI.\nen\nzh\nes\nfr\nja\nko\nde\nAverage\nBLOOM-7.1B\n61.3\n47.3\n59.4\n50.9\n45.5\n45.1\n52.9\n51.8\nLLaMA-13B\n53.7\n45.2\n52.1\n54.5\n45.0\n47.1\n53.0\n50.1\nPOLYLM-13B\n60.9\n56.0\n59.7\n56.6\n51.0\n46.8\n60.3\n55.9\nTable 19: Accuracy on PAWS-X.\nen\nzh\nja\npt\nru\nfr\nAverage\nBLOOM-7.1B\n82.2\n74.4\n58.5\n76.8\n56.8\n71.1\n70.0\nLLaMA-13B\n86.8\n70.0\n59.9\n71.5\n70.8\n68.7\n71.3\nPOLYLM-13B\n84.6\n76.6\n65.7\n74.9\n65.1\n73.5\n73.4\nTable 20: Accuracy on XWinograd.\n28\nTechnical Report\n2023-07-12\nid\nit\nth\ntr\nvi\nzh\nAvg.\net\nht\nqu\nsw\nta\nAvg.\nBLOOM-7.1B\n69.8\n52.8\n55.4\n51.2\n70.8\n65.2\n60.9\n48.2\n50.8\n50.8\n51.6\n59.2\n52.1\nLLaMA-13B\n57.8\n67.2\n54.6\n53.0\n53.8\n58.4\n57.5\n48.2\n52.8\n50.2\n51.2\n54.4\n51.4\nPOLYLM-13B\n70.2\n66.0\n58.6\n57.8\n70.8\n67.0\n65.1\n49.8\n50.4\n50.4\n51.8\n55.0\n51.5\nTable 21: Accuracy on XCOPA. The left part presents the results of languages we mainly considered\nin the training phrase, while the right part shows the other languages in the testsets. \u2018Avg.\u2019 means\nthe average accuracy.\nar\nen\nid\nko\nru\nAverage\nfi\nbn\nsw\nte\nAverage\nBLOOM-7.1B\n42.6\n51.6\n48.7\n8.6\n33.8\n37.1\n17.5\n55.1\n56.8\n40.9\n42.6\nLLaMA-13B\n49.7\n54.4\n43.4\n49.7\n41.8\n47.8\n40.2\n32.0\n33.5\n8.5\n28.6\nPOLYLM-13B\n44.9\n58.0\n48.6\n53.8\n39.5\n49.0\n20.9\n2.9\n22.0\n3.7\n12.4\nTable 22: F1 scores on the TyDiQA-GoldP benchmark under one-shot conditions. The left part\npresents the results of languages we mainly considered in the training phrase, while the right part\nshows the other languages in the testsets.\nTask\nzh\nen\nes\nfr\nde\nAverage\nBLOOM-7.1B\n16.6\n13.2\n11.0\n13.1\n6.6\n12.1\nSG\nLLaMA-13B\n16.5\n12.5\n2.0\n8.1\n0.7\n8.0\nPOLYLM-13B\n10.3\n10.3\n7.8\n9.2\n6.3\n8.8\nBLOOM-7.1B\n12.4\n14.2\n10.1\n10.9\n5.7\n10.7\nTG\nLLaMA-13B\n11.5\n19.6\n15.9\n16.8\n10.6\n14.9\nPOLYLM-13B\n16.8\n16.5\n14.2\n14.3\n10.1\n14.4\nBLOOM-7.1B\n13.8\n15.5\n15.0\n13.5\n7.3\n13.0\nQG\nLLaMA-13B\n20.2\n16.3\n14.6\n13.1\n4.5\n13.7\nPOLYLM-13B\n20.0\n15.9\n17.1\n13.9\n7.5\n14.9\nBLOOM-7.1B\n11.0\n10.3\n10.0\n9.7\n7.5\n9.7\nSum\nLLaMA-13B\n14.6\n18.1\n8.1\n10.3\n7.6\n11.7\nPOLYLM-13B\n16.6\n17.0\n16.9\n15.1\n11.6\n15.4\nBLOOM-7.1B\n13.5\n13.5\n11.5\n11.8\n6.8\n11.4\nAverage\nLLaMA-13B\n15.7\n16.6\n10.1\n12.1\n5.9\n12.1\nPOLYLM-13B\n17.5\n14.9\n14.0\n13.1\n8.9\n13.7\nTable 23: Rouge scores on the MTG benchmark under one-shot conditions. Results are presented\nin two dimensions: language directions and task types. The bottom row shows the mean values of\ndiverse language directions across all tasks. Similarly, the rightmost column depicts the average\nvalues of varied tasks across all language directions.\nde\u2192en\nja\u2192en\nru\u2192en\nzh\u2192en\nAvg.\nen\u2192de\nen\u2192ja\nen\u2192ru\nen\u2192zh\nAvg.\nBLOOM-7.1B\n23.9\n6.5\n17.7\n16.3\n16.1\n3.7\n2.1\n1.4\n8.3\n3.9\nLLaMA-13B\n36.9\n13.6\n32.7\n22.6\n26.5\n23.6\n9.6\n16.8\n7.4\n14.3\nPOLYLM-13B\n33.9\n10.2\n32.4\n22.0\n24.6\n24.8\n17.0\n17.8\n19.0\n19.7\nTable 24: Translation BLEU scores on the WMT20 machine translation task under one-shot condi-\ntions. \u2018Avg.\u2019 means the average BLEU scores of translations to English or from English.\nen\nzh\nar\nes\nfr\nru\nth\ntr\nvi\nde\nAverage\nbg\nel\nhi\nsw\nur\nAverage\nBLOOMZ-MT-7.1B\n44.9\n33.2\n36.0\n36.1\n47.5\n38.1\n33.5\n33.4\n37.9\n42.5\n38.3\n35.5\n35.1\n39.6\n33.2\n37.9\n36.3\nLLaMA-Alpaca-13B\n35.7\n34.6\n33.2\n33.3\n33.4\n33.4\n36.2\n34.7\n34.6\n33.7\n34.3\n34.3\n34.1\n35.8\n33.0\n32.8\n34.0\nPOLYLM-MULTIALPACA-13B\n54.3\n36.0\n33.3\n47.0\n49.6\n46.7\n38.4\n44.1\n44.6\n48.0\n44.2\n36.1\n32.7\n34.1\n33.1\n33.8\n34.0\nTable 25: Accuracy of instruction-followed models on XNLI.\n29\nTechnical Report\n2023-07-12\nen\nzh\nes\nfr\nja\nko\nde\nAverage\nBLOOMZ-MT-7.1B\n61.2\n58.5\n58.8\n61.1\n54.4\n49.5\n56.7\n57.2\nLLaMA-Alpaca-13B\n53.3\n45.3\n53.5\n54.1\n46.0\n48.3\n54.3\n50.7\nPOLYLM-MULTIALPACA-13B\n65.3\n55.9\n63.0\n59.8\n52.3\n53.7\n64.9\n59.3\nTable 26: Accuracy of instruction-followed models on PAWS-X.\nen\nzh\nja\npt\nru\nfr\nAverage\nBLOOMZ-MT-7.1B\n83.5\n71.0\n56.4\n65.4\n53.7\n68.7\n66.5\nLLaMA-Alpaca-13B\n88.6\n67.3\n61.4\n73.0\n72.1\n78.3\n73.5\nPOLYLM-MULTIALPACA-13B\n83.9\n73.6\n65.2\n72.2\n67.9\n71.1\n72.3\nTable 27: Accuracy of instruction-followed models on XWinograd.\nid\nit\nth\ntr\nvi\nzh\nAvg.\net\nht\nqu\nsw\nta\nAvg.\nBLOOMZ-MT-7.1B\n58.6\n51.8\n53.6\n53.2\n58.8\n62.2\n56.4\n49.6\n53.8\n49.4\n53.0\n58.2\n52.8\nLLaMA-Alpaca-13B\n55.4\n70.8\n54.6\n53.0\n53.0\n60.0\n57.8\n47.2\n53.0\n51.8\n51.0\n56.0\n51.8\nPOLYLM-MULTIALPACA-13B\n71.6\n66.8\n60.2\n58.8\n71.8\n75.6\n67.5\n48.4\n52.0\n50.4\n50.8\n55.4\n51.4\nTable 28: Accuracy of instruction-followed models on XCOPA.\nen\nar\nid\nko\nru\nAvg.\nbn\nfi\nsw\nte\nAvg.\nBLOOMZ-MT-7.1B\n22.4\n36.6\n26.9\n5.8\n9.1\n20.2\n26.7\n2.4\n14.4\n26.5\n17.5\nLLaMA-Alpaca-13B\n59.2\n20.8\n48.6\n19.3\n37.7\n37.1\n11.0\n50.6\n20.7\n5.7\n22.0\nPOLYLM-MULTIALPACA-13B\n58.7\n50.7\n52.1\n30.1\n40.3\n46.4\n2.5\n8.5\n4.6\n1.9\n4.4\nTable 29: F1 scores of instruction-followed models on the TyDiQA-GoldP benchmark.\nTask\nzh\nen\nes\nfr\nde\nAverage\nBLOOMZ-MT-7.1B\n12.1\n12.6\n9.8\n12.1\n5.6\n10.4\nSG\nLLaMA-Alpaca-13B\n3.8\n10.0\n4.8\n5.7\n5.6\n6.0\nPOLYLM-MULTIALPACA-13B\n13.9\n9.5\n9.6\n11.5\n8.0\n10.5\nBLOOMZ-MT-7.1B\n18.5\n25.7\n18.9\n16.8\n9.6\n17.9\nTG\nLLaMA-Alpaca-13B\n9.3\n25.1\n15.5\n15.5\n10.1\n15.1\nPOLYLM-MULTIALPACA-13B\n15.3\n23.7\n16.6\n15.9\n11.7\n16.6\nBLOOMZ-MT-7.1B\n23.8\n31.9\n29.6\n26.4\n9.2\n24.2\nQG\nLLaMA-Alpaca-13B\n4.7\n27.5\n27.3\n19.2\n18.1\n19.4\nPOLYLM-MULTIALPACA-13B\n21.8\n25.7\n27.2\n19.3\n16.3\n22.1\nBLOOMZ-MT-7.1B\n16.5\n18.4\n19.7\n18.4\n11.0\n16.8\nSum\nLLaMA-Alpaca-13B\n2.6\n22.0\n16.9\n15.2\n13.4\n14.0\nPOLYLM-MULTIALPACA-13B\n15.4\n21.5\n18.1\n15.2\n13.0\n16.6\nBLOOMZ-MT-7.1B\n17.7\n22.2\n19.5\n18.4\n8.9\n17.3\nAverage\nLLaMA-Alpaca-13B\n5.1\n21.2\n16.1\n13.9\n11.8\n13.6\nPOLYLM-MULTIALPACA-13B\n16.6\n20.1\n17.9\n15.5\n12.3\n16.5\nTable 30: Rouge scores of instruction-followed models on the MTG benchmark.\nde\u2192en\nja\u2192en\nru\u2192en\nzh\u2192en\nAvg.\nen\u2192de\nen\u2192ja\nen\u2192ru\nen\u2192zh\nAvg.\nBLOOMZ-MT-7.1B\n17.99\n5.09\n7.67\n12.89\n10.91\n2.94\n0.99\n0.77\n8.40\n3.3\nLLaMA-Alpaca-13B\n30.0\n9.9\n20.3\n16.4\n19.14\n16.8\n3.6\n10.6\n3.0\n8.5\nPOLYLM-MULTIALPACA-13B\n34.2\n14.3\n31.8\n25.0\n26.32\n25.9\n16.2\n17.9\n21.9\n20.5\nTable 31: Translation BLEU scores of instruction-followed models on the WMT20 machine trans-\nlation task.\n30\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u8bf7\u7ed9\u51fa\u2f00\u4e2a\u8ba1\u7b97\u6807\u51c6\u5dee\u7684\u516c\u5f0f\u3002\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n \u6807\u51c6\u5dee\u662f\u8861\u91cf\u6570\u636e\u79bb\u6563\u7a0b\u5ea6\u7684\u2f00\u4e2a\u7edf\u8ba1\u91cf\uff0c\u5176\u8ba1\u7b97\u516c\u5f0f\u4e3astd = sqrt(mean(abs(x - \nx.mean())**2))\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n31\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u547d\u540d\u2f00\u79cd\u2edd\u7269\u3002\n\u8fd9\u79cd\u2edd\u7269\u7531\u4ee5\u4e0b\u914d\u6599\u5236\u6210\uff1a\n- \u9e21\u86cb\n- \u2faf\u7c89\n- \u5976\u6cb9\n-\u2f81\u677e\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n \u8fd9\u79cd\u2edd\u7269\u88ab\u79f0\u4e3a\u2f81\u677e\u86cb\u7cd5\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n32\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u4ece\u4ee5\u4e0b\u53e5\u2f26\u4e2d\u9009\u62e9\u4e0e\u7ed9\u51fa\u53e5\u2f26\u542b\u4e49\u76f8\u540c\u7684\u53e5\u2f26\u3002\n\u8fc7\u5b8c\u8fd9\u4e2a\u5468\u672b\u540e\uff0c\u8fd9\u4e2a\u9879\u2f6c\u5e94\u8be5\u5b8c\u6210\u3002\n\u9009\u62e9\uff1a\na) \u8fd9\u4e2a\u9879\u2f6c\u5f88\u5feb\u5c31\u4f1a\u7ed3\u675f\u4e86\u3002\nb) \u8fd9\u4e2a\u9879\u2f6c\u8fc7\u4e86\u8fd9\u4e2a\u5468\u672b\u5c31\u5b8c\u6210\u4e86\u3002\nc) \u8fd9\u4e2a\u9879\u2f6c\u4e0d\u4f1a\u5728\u5468\u672b\u524d\u7ed3\u675f\u3002\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n b) \u8fd9\u4e2a\u9879\u2f6c\u8fc7\u4e86\u8fd9\u4e2a\u5468\u672b\u5c31\u5b8c\u6210\u4e86\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n33\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u5728\u6307\u5b9a\u7684\u8bed\u5883\u4e0b\uff0c\u4e3a\u2f00\u7ec4\u5355\u8bcd\u9009\u62e9\u76f8\u5e94\u7684\u4e2d\u2f42\u7ffb\u8bd1\u3002\n\u4e0b\u2faf\u662f\u2f00 \u7ec4\u5355\u8bcd\uff0c\u4e3a\u8fd9\u4e9b\u5355\u8bcd\u9009\u62e9\u6b63\u786e\u7684\u4e2d\u2f42\u7ffb\u8bd1\uff0c\u8bed\u5883\u4e3a\u5546\u4e1a\u9886\u57df\uff1a\n- deadline\n- board meeting\n- marketing strategy\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n - \u622a\u2f4c\u2f47\u671f\n - \u8463\u4e8b\u4f1a\u4f1a\u8bae\n - \u5e02\u573a\u6218\u7565\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n34\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u5728\u7ed9\u5b9a\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u627e\u5230\u6240\u6709\u5305\u542b\u67d0\u4e2a\u5173\u952e\u8bcd\u7684\u6761\u2f6c\u3002\n\u6570\u636e\u96c6\uff1a  \n\u6807\u9898                 \u2f47\u671f              \u4f5c\u8005\nSARS\u75ab\u60c5\u4e2d\u7684\u795e\u7ecf\u75c5\u7406    2003\u5e745\u2f49       \u738b\u2f7c\u5e08\n\u2f09\u7ae5\u80a5\u80d6\u95ee\u9898\u7684\u7814\u7a76            2005\u5e749\u2f49       \u5f20\u533b\u2f63\n123\u4e2a\u5206\u7c7b\u95ee\u9898\u7684\u5206\u6790   2008\u5e7411\u2f49      \u674e\u6559\u6388\n\u5305\u542b\u5173\u952e\u8bcd\u201c\u7814\u7a76\u201d\u7684\u6570\u636e\u6761\u2f6c\u5982\u4e0b\uff1a\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n 2005\u5e749\u2f49\uff0c\u5f20\u533b\u2f63\u7684\u8bba\u2f42\u300a\u2f09\u7ae5\u80a5\u80d6\u95ee\u9898\u7684\u7814\u7a76\u300b\u4e2d\u5305\u62ec\u4e86\u5173\u952e\u8bcd\u201c\u7814\u7a76\u201d\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n35\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u5728\u4ee5\u4e0b\u53e5\u2f26\u4e2d\u627e\u5230\u8fde\u8bcd\u201c\u548c\u201d\u7684\u4f7f\u2f64\uff0c\u5e76\u8bf4\u660e\u5176\u4f5c\u2f64\u662f\u8fde\u63a5\u54ea\u4e9b\u8bcd\u8bed\uff1a\u6211\u4eec\u4eca\u5929\u65e9\u4e0a\u8d77\u5f97\u5f88\n\u665a\uff0c\u548c\u6628\u5929\u2f00\u6837\u3002\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n \u5728\u8fd9\u53e5\u8bdd\u4e2d\uff0c\u8fde\u8bcd\u201c\u548c\u201d\u8fde\u63a5\u4e86\u4e24\u4e2a\u8bcd\u8bed\u201c\u4eca\u5929\u65e9\u4e0a\u201d\u548c\u201c\u6628\u5929\u201d\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n36\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\nHow does American cuisine vary by region?\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n American cuisine is characterized by its regional variations. Some of these variations include \nthe types of cuisine that dominate di{erent parts of the country, such as Southern cooking in \nthe South or Midwest American cooking in the Midwest. Additionally, regional variations \ninclude local ingredients, cooking techniques, and even dishes. The cuisine of the American \nSouth is known for its use of corn, while the Midwest is known for its emphasis on meat \ndishes like barbecue and steak. Additionally, regional variations include dishes like pizza in \nthe Northeast and barbecue in the South.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n37\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\nHow do foreign nationals seek medical treatment in the United States?\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n Foreign nationals seeking medical treatment in the United States typically have to obtain a \nmedical visa. The type of visa they need depends on the purpose of the visit and the type of \nmedical treatment they need. They typically need a B-1/B-2 visa for visiting the United States \nfor leisure, business, or medical treatment, and a J visa for medical treatments. The \napplication process typically involves a medical exam and proof of health insurance. Once \napproved, they can begin their medical treatment.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n38\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\nEscribe una oraci\u00f3n con el verbo \"caminar\" conjugado en pret\u00e9rito imperfecto.\nLas mariposas ________ de flor en flor.\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n Las mariposas caminaban de flor en flor.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n39\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\nCompleta la serie num\u00e9rica siguiente: 2, 4, 6, 8, __, 12\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n 10\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n40\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u00c9crivez une courte biographie de Marie Curie.\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n Marie Curie \u00e9tait une physicienne et une chimiste fran\u00e7aise et polonaise, n\u00e9e \u00e0 Varsovie en \n1867 et d\u00e9c\u00e9d\u00e9e \u00e0 Saint-Cloud en 1934. Elle est surtout connue pour ses d\u00e9couvertes dans le \ndomaine de la radioactivit\u00e9, qui ont r\u00e9volutionn\u00e9 notre compr\u00e9hension du fonctionnement \ndes \u00e9l\u00e9ments chimiques.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n41\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\nDonnez-moi l'URL du site web de l'Universit\u00e9 de Cambridge.\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n L'URL du site web de l'Universit\u00e9 de Cambridge est http://www.cam.ac.uk.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n42\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u300c\u904e\u53bb\u306b\u2f8f\u308f\u308c\u305f\u2f47\u672c\u306e\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u306e\u5e74\u306f\u4f55\u3067\u3059\u304b\uff1f\u300d\u3092\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n 2020\u5e74\u306e\u6771\u4eac\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u306f\u30012021\u5e74\u306b\u5ef6\u671f\u3055\u308c\u307e\u3057\u305f\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n43\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u660e\u6cbb\u6642\u4ee3\u306b\u304a\u3051\u308b\u6d0b\u9928\u5efa\u7bc9\u306e\u4ee3\u8868\u4f8b\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n \u660e\u6cbb\u6642\u4ee3\u306b\u304a\u3051\u308b\u6d0b\u9928\u5efa\u7bc9\u306e\u4ee3\u8868\u4f8b\u306e\u2f00\u3064\u306f\u3001\u6771\u4eac\u306e\u4e0a\u91ce\u306e\u56fd\u2f74\u535a\u7269\u9928\u3067\u3059\u3002\u3053\u306e\u5efa\n\u7269\u306f\u3001\u7f8e\u3057\u3044\u30c7\u30b6\u30a4\u30f3\u3001\u8d05\u6ca2\u306a\u5185\u88c5\u3001\u304a\u3088\u3073\u512a\u308c\u305f\u6280\u8853\u3092\u5099\u3048\u3066\u3044\u307e\u3059\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n44\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u0467\u045a\u0e32 \"epiphany\" \u0e2b\u0e21\u0e32\u0e22\u046c\u0e07\u0e2d\u0e30\u0e44\u0e23?\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n epiphiny \u0e2b\u0e21\u0e32\u0e22\u046c\u0e07 \"\u0e01\u0e32\u0e23\u0499\u054d\u0e01\u045a\u0e32\u0e44\u044d\u046f\u0e19\u0e1e\u0e1a\u0687\u0e07\u044c\u0e15\u0e19\u0e2d\u0e22\u0e32\u0e01\u0499\" \u048e\u0e07\u0e40\u044e\u0e19\u0687\u0e07\u044c\u0657\u0e32\u0e1b\u0e23\u0e30\u0661\u0e1a\u0e43\u0e08\u0e21\u0e32\u0e01\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n45\nTechnical Report\n2023-07-12\n\u6e05\u9664\n\u63d0\u4ea4\n\u0e41\u0e1b\u0e25\u0e07\u0656\u0e27\u0e40\u0e25\u0e02\u0e17\u0e28\u049f\u0e22\u0e21\u0e40\u044e\u0e19\u0e40\u0e1b\u0e2d\u0474\u0e40\u0e0b\u0e19\u044a\n\u0656\u0e27\u0e40\u0e25\u0e02\u0e17\u0e28\u049f\u0e22\u0e21: 0.75\n2048\ntemperature\n1\ntopk\n6\ntopp\n0\n\u6807\u8bb0\n \u0656\u0e27\u0e40\u0e25\u0e02\u0e17\u0e28\u049f\u0e22\u0e21\u0e40\u0e2b\u0670\u0e32\u0663\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e40\u044e\u0e19\u0e40\u0e1b\u0e2d\u0474\u0e40\u0e0b\u0e19\u044a\u0e44\u044d\u045a\u0e32 75 \u0e40\u0e1b\u0e2d\u0474\u0e40\u0e0b\u0e19\u044a\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n46\n"
  },
  {
    "title": "Stack More Layers Differently: High-Rank Training Through Low-Rank Updates",
    "link": "https://arxiv.org/pdf/2307.05695.pdf",
    "upvote": "20",
    "text": "ReLoRA: High-Rank Training Through\nLow-Rank Updates\nVladislav Lialin\u2020,\u2021\u2217 Sherin Muckatira\u2020, Namrata Shivagunde\u2020, and Anna Rumshisky\u2020,\u00a7\n\u2020University of Massachusetts Lowell\n\u2021Eleuther AI\n\u00a7Amazon\nAbstract\nDespite the dominance and effectiveness of scaling, resulting in large networks\nwith hundreds of billions of parameters, the necessity to train overparameterized\nmodels remains poorly understood, while training costs grow exponentially. In\nthis paper, we explore parameter-efficient training techniques as an approach to\ntraining large neural networks. We introduce a novel method called ReLoRA,\nwhich utilizes low-rank updates to train high-rank networks. We apply ReLoRA to\ntraining transformer language models with up to 1.3B parameters and demonstrate\ncomparable performance to regular neural network training. ReLoRA saves up\nto 5.5Gb of RAM per GPU and improves training speed by 9-40% depending on\nthe model size and hardware setup. Our findings show the potential of parameter-\nefficient techniques for large-scale pre-training. Our code is available on GitHub2.\n1\nIntroduction\nOver the past decade, the machine learning field has been dominated by the trend of training\nincreasingly overparameterized networks or adopting the \"stack more layers\" approach [Krizhevsky\net al., 2012, He et al., 2016, Kaplan et al., 2020]. The definition of a large network has evolved from\nmodels with 100 million [Simonyan and Zisserman, 2015, Radford et al., 2018] to hundreds of billions\n[Brown et al., 2020, Chowdhery et al., 2022] of parameters, which has made computational costs\nassociated with training of such networks prohibitive to most of the research groups. Despite this,\nthe necessity to train models which can have orders of magnitude more parameters than the training\nexamples [Brown et al., 2020, Chowdhery et al., 2022, Fedus et al., 2022], is poorly understood\ntheoretically [Jacot et al., 2018, Allen-Zhu et al., 2019, Zhang et al., 2021].\nAlternative approaches to scaling, such as more compute-efficient scaling optima [Hoffmann et al.,\n2022], retrieval-augmented models [Khandelwal et al., 2020, Borgeaud et al., 2022], and the simple\napproach of training smaller models for longer [Touvron et al., 2023], have offered new trade-offs.\nHowever, they do not bring us closer to understanding why we need overparameterized models and\nrarely democratize the training of these models. For example, training RETRO [Borgeaud et al.,\n2022] requires a complex training setup and infrastructure capable of quickly searching over trillions\nof tokens, while training LLaMA-7B [Touvron et al., 2023] still requires hundreds of GPUs.\nIn contrast, approaches like zero-redundancy optimizers [Rajbhandari et al., 2020], 16-bit training\n[Micikevicius et al., 2018], 8-bit inference [Dettmers et al., 2022], and parameter-efficient fine-tuning\n(PEFT) [Lialin et al., 2023] have played a crucial role in making large models more accessible.\nSpecifically, PEFT methods have enabled fine-tuning of billion-scale language or diffusion models\non consumer hardware. This raises the question: Can these approaches also benefit pre-training?\n\u2217Correspondance to vlad.lialin@gmail.com\n2github.com/guitaricet/relora\narXiv:2307.05695v4  [cs.CL]  10 Dec 2023\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n3.0\n3.5\n4.0\n4.5\n5.0\nLoss\n250M\n250M ReLoRA\n(99M trainable)\n99M\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nStep\n0\n100\n250\nTrainable Params\nFigure 1: Training loss for 250M models. ReLoRA learns a high-rank network through a sequence of\nlow-rank updates. It outperforms networks with the same trainable parameter count and achieves\nsimilar performance to training a full network at 100M+ scale. The efficiency of ReLoRA increases\nwith the model size, making it a viable candidate for multi-billion-parameter training.\nOur Contribution\nIn this study, we introduce ReLoRA which uses individually low-rank updates\nthat aggregate during the training process to train a high-rank network. We empirically demonstrate\nthat ReLoRA performs a high-rank update and achieves performance similar to regular neural network\ntraining. The components of ReLoRA include initial full-rank training of the neural network (similar\nto Frankle et al. [2019]), LoRA training, restarts, a jagged learning rate schedule, and partial optimizer\nresets. We evaluate ReLoRA on transformer language models up to 1.3B parameters. Finally, we\nobserve that the efficiency of ReLoRA increases with model size, making it a viable option for\nefficient training of multi-billion-parameter networks.\n2\nMethod\nWe are interested in the rank of the sum of two matrices: rank(A + B) \u2264 rank(A) + rank(B). We\nknow that for a matrix A, rank(A) < dim(A), there exists a B, rank(B) < dim(B) such that sum\nof them has a higher rank than either A or B.\nWe want to exploit this property to make a flexible parameter-efficient training method. We start\nwith LoRA [Hu et al., 2022] which is a parameter-efficient fine-tuning method based on the idea of\nlow-rank updates. LoRA can be applied to any linear operation parametrized through W \u2208 Rm\u00d7n.\nSpecifically, LoRA decomposes the weight update \u03b4W into a rank-r product WAWB as shown in\nEquation 1, where s \u2208 R is a fixed scaling factor usually equal to 1\nr.\n\u03b4W = sWAWB\nWA \u2208 Rin\u00d7r, WB \u2208 Rr\u00d7out\n(1)\nIn practice, LoRA is usually implemented by adding new trainable parameters WA and WB, which\ncould be merged back into the original parameters after training. Thus, these implementations are\nrestricted by the rank r = maxWA,WB rank(WAWB).\nIf we could restart LoRA, meaning we merge WA and WB during training and reset the values of\nthese matrices, we could increase the total rank of the update. Doing this multiple times brings the\ntotal neural network update to:\n\u2206W =\nT1\nX\nt=0\n\u03b4Wt +\nT2\nX\nt=T1\n\u03b4Wt + \u00b7 \u00b7 \u00b7 +\nTN\nX\nt=TN\u22121\n\u03b4Wt = sW 1\nAW 1\nB + sW 2\nAW 2\nB + \u00b7 \u00b7 \u00b7 + sW N\nA W N\nB (2)\nHowever, implementing restarts is not trivial in practice and requires several modifications to the\noptimization procedure. Unlike plain stochastic gradient descent, Adam [Kingma and Ba, 2015]\n2\n0\n2000\n4000\n6000\n8000\n10000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLearning Rate Multiplier\nFigure 2: Jagged cosine scheduler used in ReLoRA. As a base for our scheduler we follow a standard\ncosine decay schedule as in Touvron et al. [2023]. On every optimizer reset, we set the learning rate\nto zero and perform a quick (50-100 steps) learning rate warm-up back to the cosine schedule.\nupdate is guided mainly by the first and second moments of the gradient accumulated over the\nprevious steps. In practice, Adam\u2019s \u03b21 and \u03b22 are usually very high 0.9 \u2212 0.999. This means that\nafter the merge-and-reinit, continuing to use old gradient moments for W 2\nA will guide it in the same\ndirection as W 1\nA and optimize the same subspace.\nTo resolve this issue, ReLoRA performs a partial reset of the optimizer state during merge-and-reinit\nvia magnitude pruning. To avoid loss diverging after an optimizer reset it also sets the learning rate\nto 0 with a subsequent warm-up (Figure 2). Our ablation studies (Table 6) show that both of these\nmodifications are required to improve the performance over LoRA. Finally, in our experiments we\nfound that in the case of training from scratch (random initialization) a short full-rank training is\nneeded to \u201cwarm start\u201d ReLoRA. All of this allows ReLoRA to achieve performance comparable to\nfull-rank training, especially in large transformer networks, by only training a small set of parameters\nat a time. ReLoRA is described in Algorithm 1.\nEnhancing computational efficiency\nUnlike other low-rank training techniques [Schotth\u00f6fer et al.,\n2022, Sui et al., 2023, Kamalakara et al., 2022], ReLoRA follows the LoRA approach by maintaining\nthe frozen weights of the original network and adding new trainable parameters. At first glance, this\nmay appear computationally inefficient; however, the differentiation between frozen and trainable\nparameters plays a crucial role in parameter-efficient fine-tuning [Lialin et al., 2023].\nBy reducing the number of trainable parameters, ReLoRA significantly reduces the memory spent on\nthe optimizer states and enables the utilization of larger batch sizes, maximizing hardware efficiency.\nAdditionally, it reduces the bandwidth requirements in distributed setups, which are often the limiting\nfactor in large-scale training. Furthermore, since the frozen parameters are not being updated between\nrestarts, they can be kept in a low-precision quantized format [Dettmers et al., 2023], further reducing\ntheir memory and computational impact.\nLocally Low-Rank Training: Intuition\nMultiple studies suggest that neural network training is\neither completely low-rank or has multiple phrases with initially high-rank and subsequent low-rank\ntraining. For example, Aghajanyan et al. [2021] show that as the model becomes larger or when it\nis pre-trained for longer, the rank of the update needed to learn a downstream task reduces. Arora\net al. [2019] finds that SGD is biased towards low-rank solutions. The existence of Lottery Tickets\nearly in training [Frankle et al., 2019] also partially supports this hypothesis, since training a lottery\nticket network could effectively be seen as a low-rank approximation to the regular training process.\nOur empirical analysis (Section 4) shows that pre-trained neural networks exhibit high-rank updates\nover long trajectories (Figure 4). However, for a sufficiently small trajectory, the training can be\neffectively approximated by a low-rank update. Given the above results, we speculate that neural\nnetwork training is locally low-rank, which directly motivates ReLoRA.\n3\nAlgorithm 1 ReLoRA. \u03b8 is model parameters, \u02c6\u03b8 is model parameters with linear layers replaced with\nReLoRA, M and V are Adam optimizer states, \u03b7 is learning rate, and q is the reinit frequency.\nRequire: \u03b8, M, V , q, \u03b7\n1: for t in warm start steps do\n2:\nUpdate \u03b8, M, V , \u03b7 {Regular training for warm start}\n3: end for\n4: for layer in model layers do\n5:\nif layer is linear then\n6:\nlayer \u2190 ReLoRA(W i, W i\nA, W i\nB)\n7:\nFreeze W i\n8:\nend if\n9: end for\n10: for t in training steps do\n11:\nUpdate \u02c6\u03b8, M, V {Training step with ReLoRA}\n12:\nif MOD(t, q) = 0 then\n13:\nfor l in model layers do\n14:\nif l is linear then\n15:\nW i \u2190 (W i + sW i\nAW i\nB)\n16:\nW i\nA \u2190 kaiming_init(W i\nA); W i\nB \u2190 0\n17:\nMW i\nA \u2190 prune(MW i\nA); VW i\nA \u2190 prune(VW i\nA)\n18:\nend if\n19:\nend for\n20:\nStart \u03b7 warmup\n21:\nend if\n22: end for\n23: return \u03b8\n3\nExperiments\nTo evaluate the effectiveness of ReLoRA, we apply it to train a transformer language model on the\nC4 dataset [Raffel et al., 2020] using various model sizes: 60M, 130M, 250M, 350M, and 1.3B.\nIn all experiments we train without data repetition (single epoch) on at least compute-optimal amount\nof data, estimated using Chinchilla Scaling Laws [Hoffmann et al., 2022].\nArchitecture and training hyperparameters\nOur architecture is based on transformer [Vaswani\net al., 2017] and closely resembles LLaMA [Touvron et al., 2023]. Namely, we use pre-normalization,\nRMSNorm [Zhang and Sennrich, 2019], SwiGLU activations [Shazeer, 2020], 8\n3h fully-connected\nhidden state size [Touvron et al., 2023], and rotary embeddings [Su et al., 2021]. We select the number\nof pre-training tokens based on the Chinchilla scaling laws [Hoffmann et al., 2022]. Architecture and\ntraining hyperparameters are presented in Table 1.\nFor all LoRA and ReLoRA experiments, we use rank r = 128 as our initial experiments showed it to\nhave the best perplexity/memory trade-off. You can find additional recommendations on ReLoRA\nhyperparameter selection in Appendix A. We perform additional experiments comparing different\nParams\nHidden\nHeads\nLayers\nLearning rate\nBatch size\nSeq. len.\nData amount\n60M\n512\n8\n8\n1e-3\n122K\n256\n1.2B\n130M\n768\n12\n12\n1e-3\n154K\n256\n2.6B\n250M\n768\n16\n24\n5e-4\n590K\n512\n6.8B\n350M\n1024\n16\n24\n5e-4\n590K\n512\n6.8B\n1.3B\n2048\n24\n32\n4e-4\n786K\n512\n23.1B\nTable 1: Hyperparameters of the language models trained in this study. Batch size and data amount\nare specified in tokens.\n4\n60M\n130M\n250M\n350M\n1.3B\nFull training\n33.81 (60M)\n23.65 (130M)\n22.39 (250M)\n18.66 (350M)\n16.83 (250M)\nControl\n36.52 (43M)\n27.30 (72M)\n25.43 (99M)\n23.65 (130M)\n21.73 (250M)\nLoRA\n47.44 (43M)\n34.17 (72M)\n36.60 (99M)\n57.11 (125M)\n-\nLoRA + Warm Start\n34.73 (43M)\n25.46 (72M)\n22.86 (99M)\n19.73 (125M)\n18.23 (250M)\nReLoRA\n34.46 (43M)\n25.04 (72M)\n22.48 (99M)\n19.32 (125M)\n17.27 (250M)\nTraining tokens\n1.2B\n2.6B\n6.8B\n6.8B\n23.1B\nTable 2: Language model perplexity when trained using each of the above methods. Number of\ntrainable parameters for each model in (brackets). Control baseline is full-rank training a model with\nthe same total number of parameters as the number of trainable parameters in low-rank training.\nCoLA\nSTS-B\nMRPC\nRTE\nSST2\nMNLI\nQNLI\nQQP\nAvg\nFull-rank pretrained\n35.43\n83.85\n76.96\n64.26\n88.99\n70.98\n83.38\n84.49\n73.54\nNot pretrained\n7.59\n22.73\n67.00\n51.15\n82.61\n60.04\n67.92\n78.40\n54.68\nReLoRA\n31.07\n83.33\n78.43\n60.65\n89.45\n72.27\n83.93\n86.01\n73.14\nTable 3: Applying ReLoRA to fine-tune 350M models pre-trained full-rank and using ReLoRA. We\nobserve minimal differences between the models.\nrank choices for the 1.3B model in Section 4.1. We use bfloat16 for all floating point operations and\nFlashAttention [Dao et al., 2022] for effective attention computation.\nReLoRA and baselines setup\nIn our experiments, ReLoRA replaces all attention and fully-\nconnected network parameters, while updating the embeddings and normalization layers full-rank.\nSince ReLoRA-wrapped models have fewer trainable parameters than full-rank training, we include\na Control baseline, which is a full-rank transformer with the same number of trainable parameters as\nReLoRA.\nWe initialize ReLoRA from a checkpoint of full-rank training at 5,000 update steps and reset it every\n5,000 steps thereafter, 3 times in total till we reach 20K steps. After each reset, 99% of the optimizer\nstate is pruned based on magnitude, and the loss is warmed up for the next 100 iterations. ReLoRA\nparameters are reinitialized following LoRA best practices, Kaiming initialization [He et al., 2015]\nfor A-matrix, and zeros for B-matrix.\nScaling up to 1.3B\nAfter initial results at 130M and 350M model sizes, we applied ReLoRA to\ntrain a 1.3B parameter language model. As a baseline, we pre-trained a 1.3B model from scratch on\n23B tokens. We performed multiple ReLoRA runs starting from 2K, 5K, and 10K checkpoints. In\nmost of the experiments, we continued using r = 128 and our additional experiments show minimal\ndifference between rank 128 and 512 (hidden size is 2048). Section 4.1 describes these experiments\nin detail.\n4\nResults\nParameter-efficient pre-training\nOur results are presented in Table 2 and Figure 1. ReLoRA sig-\nnificantly outperforms LoRA training demonstrating the effectiveness of our proposed modifications\n(ablated in Section 6). Additional pre-training loss figures are available in Appendix C.\nFurthermore, ReLoRA achieves similar performance to full-rank training in both upstream and\ndownstream tasks (Table 3).3\nHigh-rank training through low-rank updates\nTo determine whether ReLoRA performs a higher\nrank update than LoRA, we plot the singular value spectrum of the learned update to the warm-start\n3Note that the absolute values of GLUE results are expected to be quite far from state-of-the-art, because our\nmodels were pre-trained on roughly 20 times less data than T5 or BERT.\n5\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFrequency\nQ Projections\nReLoRA\nLoRA\nFull-rank\ntraining\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nK Projections\nReLoRA\nLoRA\nFull-rank\ntraining\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nV Projections\nReLoRA\nLoRA\nFull-rank\ntraining\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nDown Projections\nReLoRA\nLoRA\nFull-rank\ntraining\nFigure 3: Singular values spectra of the weight difference between ReLoRA and LoRA at 5,000\niterations (warm start) and 20,000 iterations. ReLoRA exhibits a closer resemblance to full-rank\ntraining than to LoRA, indicating its effectiveness in approximating full-rank behavior. 350M models.\nWQ\nWK\nWV\nWup\nWdown\n0\n250\n500\n750\n1000\nFull-rank\nTraining\nLoRA\nReLoRA\nFigure 4: The number of singular values >0.1 in weight matrices of the learned update. 350M models.\nweights. Specifically, the difference between warm-start weights and the final weights for ReLoRA,\nLoRA, and full-rank trained models. Figure 3 illustrates significant qualitative differences between\nLoRA and ReLoRA for the singular values of \u2206WQ, \u2206WK, \u2206WV , and \u2206Wdown. While most of\nthe singular values for LoRA are zero (Figure 4) with a noticeable number of exceptionally high\nvalues above 1.5, ReLoRA exhibits a higher distribution mass between 0.1 and 1.0, reminiscent of\nfull-rank training.\nAdditionally, we computed the number of singular values less than 0.1 for LoRA, ReLoRA, and\nfull-rank training. Our results (Figure 4) show that ReLoRA has a much smaller number of near-\nzero singular values than LoRA, closer to full-rank training. This observation emphasizes the\nsignificance of high-rank updates and demonstrates that ReLoRA does accomplish a high-rank update\nby performing multiple low-rank updates. We also perform ReLoRA component ablation (Table 6)\nand discuss it in Section 6.\n4.1\nScaling up to 1.3B\nOur best run at this model size starts after a 10K step warm start (33% of the total update steps).\nWe train ReLoRA with rank r = 128, learning rate 5e-4, 100 steps lr warmup, and 50 steps restarts\nwarmup. The results are presented in the Figure 5 and Table 4. ReLoRA clearly outperforms LoRA\nthroughout the training with the gap between the methods increasing from 0.56 at 15K steps to 0.96\nat 30K steps. At the end of the training, ReLoRA is able to reach a perplexity of 17.24, only 0.44\nhigher than full-rank training. You can find additional recommendations on ReLoRA hyperparameter\nselection in Appendix A.\nVarying ReLoRA rank\nIn this experiment we wanted to evaluate if r = 128 is still applicable\nto the model of this size (hidden size 2048) or if it needs to be increased. To do that, we used an\nearly checkpoint for the warm start (5K out of 30K steps). This was beneficial for the comparison, as\nat this point loss changes quickly which makes any differences in training dynamics more evident.\nWe train these models for additional 10K iterations. Unexpectedly, we found very little difference\nbetween ranks 128 (ppl. 19.16) and 512 (ppl. 19.00).\n6\n0\n5000\n10000\n15000\n20000\n25000\n2.8\n3.0\n3.2\n3.4\n3.6\nLoss\n1B\n1B ReLoRA\n(250M trainable)\n1B LoRA\n(250M trainable)\n250M\n0\n5000\n10000\n15000\n20000\n25000\nStep\n0\n250\n1300\nTrainable Params\nFigure 5: Training loss at 1.3B scale and the associated baselines. ReLoRA outperforms LoRA\nthroughout training and the gap between the methods increases over time.\n1.3B @15K steps\n1.3B @20K steps\n1.3B @30K steps\nFull training\n17.67 (250M)\n17.00 (250M)\n16.83 (250M)\nControl\n22.67 (250M)\n22.00 (250M)\n21.73 (250M)\nLoRA + Warm Start\n18.50 (250M)\n18.38 (250M)\n18.23 (250M)\nReLoRA\n17.94 (250M)\n17.64 (250M)\n17.27 (250M)\nTraining tokens (billions)\n11.8\n15.7\n23.1\nTable 4: Results at 1.3B scale. Number of trainable parameters for each model in (brackets).\n250M\n1.3B\n(@15k steps)\n(@25k steps)\nReLoRA\n27.66\n17.36\nOnline ReLoRA\n29.31\n17.80\nTable 5: Online ReLoRA.\nNegative results:\nOnline ReLoRA\nIntu-\nitively, more frequent ReLoRA resets can lead\nto better performance, as they, in principle, can\nlearn a higher rank update. Usually, for ev-\nery ReLoRA reset, we would also perform an\noptimizer reset and learning rate scheduler re-\nwarmup (Section 1). However, in our experi-\nments we observed that very high ReLoRA reset\nrates lead to worse performance.\nOnline ReLoRA resolves this issue quite elegantly \u2013 it merges LoRA parameters very frequently\n(e.g., every 100 iterations) while keeping the optimizer reset rate at 2-5K iterations. Unexpectedly,\nwe found that it performs worse than regular ReLoRA at both 250M and 1.3B scales (Table 5).\nReLoRA Training Speedup\nTraining ReLoRA took 440 A100-hours, saving 56 A100-hours\ncompared to full-rank training. A part of the speedup was due to the ability to use two times\nlarger microbatch size. When training with the same microbatch size, ReLoRA improved RAM\nconsumption from 27.8Gb to 22.3Gb saving 5.5Gb of GPU RAM. Overall, in the 8xA100 setup,\ncombining the warm start and ReLoRA training time, 1.3B-ReLoRA took 86 hours (wall clock) to\ntrain compared to 93.5 hours to train 1.3 model full-rank on the same amount of data. This yields a\nrelative speed improvement of 9%.\nWe additionally observed that ReLoRA speedup is significantly hardware-dependent (Table 7). In our\nearly experiments on 2xRTX3090, we estimated the speedup of 42%. In a more practical, but still\nrelatively budget setup of 6xA6000 Ada, we estimated 152 hours of wall-clock training time for the\n1B full-rank model and 119 hours for the ReLoRA model with 33% warm start. This saves 33 hours\nyielding a relative speedup of 21%. We attribute the difference to the GPU memory speed. ReLoRA\ncan more effectively utilize low-bandwidth memory as it has less trainable parameters.\n7\nRestarts\nOptimizer Reset\nJagged Schedule\nWarm Start\nPerplexity (\u2193)\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n34.17\n\u2713\n\u00d7\n\u00d7\n\u00d7\n34.25\n\u2713\n\u2713\n\u00d7\n\u00d7\n(diverged)\n\u2713\n\u00d7\n\u2713\n\u00d7\n34.29\n\u2713\n\u2713\n\u2713\n\u00d7\n29.77\n\u00d7\n\u00d7\n\u00d7\n\u2713\n25.46\n\u2713\n\u2713\n\u2713\n\u2713\n25.04\nRegular training\n23.65\nTable 6: Ablation studies of ReLoRA (130M models). Restarts and warm starts are essential for good\nperformance. Restarts and optimizer resets without a jagged schedule causes the model to diverge.\n8xA100\n6xA6000 (Ada)\n2x3090\nFull-rank throughput\n137 ex/sec\n84 ex/sec\n8.8 ex/sec\nReLoRA throughput\n157 ex/sec\n124 ex/sec\n17.8 ex/sec\nImmediate speedup\n15%\n48%\n102%\nWarm-start adjusted ReLoRA throughput\n149 ex/sec\n111 ex/sec\n14.8 ex/sec\nTotal speedup\n9%\n32%\n51%\nTable 7: Performance metrics in different hardware configurations. Warm start adjustment assumes\n33% of full-rank training before switching to ReLoRA.\n4.2\nAblation studies\nWe conduct ablation studies on all four crucial components of ReLoRA: restarts, jagged schedule,\noptimizer resets, and warm starts, utilizing the 130M-sized model. The results are presented in\nTable 6. In this section, we will focus on and analyze certain combinations of these components.\nLoRA\nReLoRA, without the aforementioned components, is essentially equivalent to training\na low-rank network parameterized by LoRA. This approach yields remarkably high perplexity,\nindicating that a simple matrix decomposition has significantly different training dynamics from\nfull-rank training.\nAdding restarts and optimizer resets\nReLoRA, without a jagged schedule and optimizer reset,\nperforms similarly to LoRA because old optimizer states force the newly initialized parameters\ninto the same subspace as the prior weights, limiting the model\u2019s capacity. However, doing a naive\noptimizer reset with ReLoRA causes the model to diverge. A jagged schedule helps to stabilize\ntraining and has a positive impact on the mixture. In our initial experiments, we also observed that a\ncombination of partial optimizer reset and jagged scheduler allows for a quicker warm-up, as low as\n50 steps, instead of hundreds of steps required when the optimizer is initialized from scratch.\nWarm start\nThe warm start shows the most significant improvement, dropping perplexity by\nalmost 10 points. To investigate whether post-warmup training contributes to the loss, we measured\nthe perplexity of the warmed-up network, which equals 27.03. It outperforms all low-rank methods\nexcept for our final ReLoRA recipe but still demonstrates a significant difference from the final\nnetwork. This demonstrates the importance of early training, similar to the concept of the lottery\nticket hypothesis with rewinding [Frankle et al., 2019]. In our experiments, unless specified otherwise,\nwe performed warm start for about 1/4 of the total training updates.\n5\nRelated work\nScaling versus Efficiency\nThe relationship between overparametrization and neural network\ntrainability and generalization has been extensively studied [Zhang et al., 2017, Belkin et al., 2018,\n8\nFrankle and Carbin, 2019, Nakkiran et al., 2019, Singh et al., 2021], yet it remains a mystery [Zhang\net al., 2021].\nMoreover, scaling laws [Kaplan et al., 2020, Ghorbani et al., 2021, Hoffmann et al., 2022] demonstrate\na simple and strong power-law dependence between network size and its performance across a variety\nof modalities. This finding not only supports overparametrization but also encourages the training\nof extraordinarily resource-intensive neural networks [Brown et al., 2020, Chowdhery et al., 2022,\nFedus et al., 2022]. Nonetheless, the Lottery Ticket Hypothesis [Frankle et al., 2019] suggests that\noverparametrization could, in principle, be minimized.\nParameter-efficient fine-tuning\nAghajanyan et al. [2021] found that pre-training reduces the\namount of change to the network required to learn a new task through fine-tuning. I.e., larger\nnetworks or networks pre-trained on more data require smaller modifications in terms of the rank of\nthe range to learn a new task. This explains the success of parameter-efficient fine-tuning methods\n[Lialin et al., 2023] and has also motivated the development of low-rank fine-tuning methods such as\nLoRA [Hu et al., 2022] and Compacter [mahabadi et al., 2021].\nLow-rank neural network training\nTraining low-rank representations has been explored in the\ncontext of CNN compression, regularization, and efficient training [Idelbayev and Carreira-Perpinan,\n2020, Jaderberg et al., 2014, Sui et al., 2023, Schotth\u00f6fer et al., 2022, Lin et al., 2020, Yuan et al.,\n2021, Zhao et al., 2023]. However, most of these methods are either specific to CNNs, do not scale\nwell, or have not been evaluated on large transformers [Vaswani et al., 2017] with hundreds of\nmillions of parameters, which can benefit greatly from efficient training. While transformers have\nbeen shown to have a low-rank internal dimensionality and representations [Aghajanyan et al., 2021,\nWang et al., 2020], the study by Bhojanapalli et al. [2020] demonstrated that the low rank of key\nand query projections in multi-head attention bottlenecks the performance of transformers. Our own\nexperiments (Section 6) also demonstrate that low-rank transformers perform significantly worse\ncompared to the full-rank baseline and ReLoRA.\n6\nConclusion\nIn this paper, we demonstrate that parameter-efficient fine-tuning methods can be adapted for pre-\ntraining large language models. We first examined the limitations of a low-rank matrix factorization\n(LoRA) approach and observed that it struggles to effectively train high-performing transformer\nmodels. To address this issue, we proposed ReLoRA, which leverages the rank of sum property to\ntrain a high-rank network through multiple low-rank updates. Similar to the lottery ticket hypothesis\nwith rewinding, ReLoRA employs a full-rank training warm start before transitioning to ReLoRA.\nDuring training, ReLoRA periodically merges its parameters into the main parameters of the network,\nperforms optimizer reset and learning rate re-warmup.\nWe demonstrated that ReLoRA consistently outperforms LoRA for training large transformer models.\nOur largest experiment demonstrated 9% wall-clock time reduction in 8xA100 setup and much larger\n(20 \u2212 40%) speed improvements on cheaper hardware. Further, our results show similar performance\nto regular training making ReLoRA a promising candidate for improving the efficiency of large model\ntraining. Our further studies will focus on improving ReLoRA performance, efficiency, applying it to\nlarger models and applying it to continued pre-training of existing large language models.\nAcknowledgments and Disclosure of Funding\nThis paper has been a journey and we are sincerely grateful to everyone who supported us. We would\nlike to express our gratitude to Stability.ai, Eleuther.ai, and the Google Cloud for Research Program\nfor providing computational resources essential for this research.\nEric Lehman and Artem Krivosheev, thank you for supporting this project from the very beginning.\nSpecial thanks to Jason Phang, Hailey Schoelkopf, Enrico Shippole, and Stella Biderman for their\ntechnical advice and assistance with computational resources. Our experiments at billion-parameter\nscale wouldn\u2019t be possible without your support.\nThis work was funded in part by an Amazon Alexa AI research award to Anna Rumshisky.\n9\nReferences\nA. Aghajanyan, S. Gupta, and L. Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language\nmodel fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages\n7319\u20137328, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.\n568. URL https://aclanthology.org/2021.acl-long.568.\nZ. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In\nK. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine\nLearning, volume 97 of Proceedings of Machine Learning Research, pages 242\u2013252. PMLR, 09\u201315 Jun 2019.\nURL https://proceedings.mlr.press/v97/allen-zhu19a.html.\nS. Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization, 2019.\nM. Belkin, D. J. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the classical\nbias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116:15849 \u2013 15854, 2018.\nS. Bhojanapalli, C. Yun, A. S. Rawat, S. Reddi, and S. Kumar. Low-rank bottleneck in multi-head attention\nmodels. In International Conference on Machine Learning, pages 864\u2013873. PMLR, 2020.\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau,\nB. Damoc, A. Clark, D. De Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore,\nC. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. Rae, E. Elsen,\nand L. Sifre. Improving language models by retrieving from trillions of tokens. In K. Chaudhuri, S. Jegelka,\nL. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on\nMachine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206\u20132240. PMLR,\n17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/borgeaud22a.html.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing\nSystems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,\nS. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. M. Shazeer,\nV. Prabhakaran, E. Reif, N. Du, B. C. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari,\nP. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garc\u00eda, V. Misra, K. Robinson,\nL. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal,\nM. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. D\u00edaz, O. Firat, M. Catasta, J. Wei, K. S. Meier-Hellstern, D. Eck, J. Dean,\nS. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re. Flashattention: Fast and memory-efficient exact attention with\nIO-awareness. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information\nProcessing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers\nat scale. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information\nProcessing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD.\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. ArXiv,\nabs/2305.14314, 2023. URL https://api.semanticscholar.org/CorpusID:258841328.\nA. Edalati, M. S. Tahaei, I. Kobyzev, V. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient\ntuning with kronecker adapter. ArXiv, abs/2212.10650, 2022. URL https://api.semanticscholar.org/\nCorpusID:254926823.\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and\nefficient sparsity. J. Mach. Learn. Res., 23(1), jan 2022. ISSN 1532-4435.\nJ. Frankle and M. Carbin.\nThe lottery ticket hypothesis: Finding sparse, trainable neural networks.\nIn\nInternational Conference on Learning Representations, 2019. URL https://openreview.net/forum?\nid=rJl-b3RcF7.\n10\nJ. Frankle, G. Karolina Dziugaite, D. M. Roy, and M. Carbin. Stabilizing the lottery ticket hypothesis. arXiv\ne-prints, pages arXiv\u20131903, 2019.\nB. Ghorbani, O. Firat, M. Freitag, A. Bapna, M. Krikun, X. Garc\u00eda, C. Chelba, and C. Cherry. Scaling laws for\nneural machine translation. ArXiv, abs/2109.07740, 2021.\nK. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on\nimagenet classification. CoRR, abs/1502.01852, 2015. URL http://arxiv.org/abs/1502.01852.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas, L. A. Hendricks,\nJ. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero,\nK. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre. An empirical analysis of compute-optimal large\nlanguage model training. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural\nInformation Processing Systems, 2022. URL https://openreview.net/forum?id=iBBcRUlOAPR.\nE. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank\nadaptation of large language models. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=nZeVKeeFYf9.\nY. Idelbayev and M. A. Carreira-Perpinan. Low-rank compression of neural nets: Learning the rank of each layer.\nIn 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8046\u20138056,\n2020. doi: 10.1109/CVPR42600.2020.00807.\nA. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks.\nIn Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918,\npage 8580\u20138589, Red Hook, NY, USA, 2018. Curran Associates Inc.\nM. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank\nexpansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014. doi: http:\n//dx.doi.org/10.5244/C.28.88.\nS. R. Kamalakara, A. F. Locatelli, B. Venkitesh, J. Ba, Y. Gal, and A. N. Gomez. Exploring low rank training\nof deep neural networks. ArXiv, abs/2209.13569, 2022. URL https://api.semanticscholar.org/\nCorpusID:252545358.\nJ. Kaplan, S. McCandlish, T. J. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.\nU. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization:\nNearest neighbor language models. In International Conference on Learning Representations, 2020. URL\nhttps://openreview.net/forum?id=HklBjCEKvH.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks.\nIn F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing\nSystems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\nV. Lialin, V. Deshpande, and A. Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning,\n2023.\nR. Lin, C.-Y. Ko, Z. He, C. Chen, Y. Cheng, H. Yu, G. Chesi, and N. Wong. Hotcake: Higher order tucker\narticulated kernels for deeper cnn compression. In 2020 IEEE 15th International Conference on Solid-State\n& Integrated Circuit Technology (ICSICT), pages 1\u20134, 2020. doi: 10.1109/ICSICT49897.2020.9278257.\nR. K. mahabadi, J. Henderson, and S. Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing\nSystems, 2021. URL https://openreview.net/forum?id=bqGK5PyI6-N.\nP. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev,\nG. Venkatesh, and H. Wu. Mixed precision training. In International Conference on Learning Representations,\n2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\nP. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever. Deep double descent: where bigger\nmodels and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021, 2019.\n11\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative\npre-training. 2018.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21\n(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter\nmodels. In SC20: International Conference for High Performance Computing, Networking, Storage and\nAnalysis, pages 1\u201316, 2020. doi: 10.1109/SC41405.2020.00024.\nS. Schotth\u00f6fer, E. Zangrando, J. Kusch, G. Ceruti, and F. Tudisco. Low-rank lottery tickets: finding efficient low-\nrank neural networks via matrix differential equations. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,\nK. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 20051\u2013\n20063. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n2022/file/7e98b00eeafcdaeb0c5661fb9355be3a-Paper-Conference.pdf.\nN. Shazeer. Glu variants improve transformer, 2020.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In Y. Bengio\nand Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,\nUSA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.1556.\nS. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural network hessian\nmaps. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information\nProcessing Systems, 2021. URL https://openreview.net/forum?id=otDgw7LM7Nn.\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding.\nArXiv, abs/2104.09864, 2021.\nY. Sui, M. Yin, W. Yang, Y. Gong, J. Xiao, H. Phan, D. Ding, X. Xu, S. Liu, Z. Chen, and B. Yuan. ELRT:\nTowards efficient low-rank training for compact neural networks, 2023. URL https://openreview.net/\nforum?id=TC39w69m8bB.\nY.-L. Sung, J. Cho, and M. Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning.\nArXiv, abs/2206.06522, 2022. URL https://api.semanticscholar.org/CorpusID:249642544.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,\nF. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30, 2017.\nS. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nX. Yuan, P. H. P. Savarese, and M. Maire. Growing efficient deep networks by structured continuous sparsification.\nIn International Conference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=wb3wxCObbRT.\nB. Zhang and R. Sennrich. Root mean square layer normalization. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/\npaper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking\ngeneralization. In International Conference on Learning Representations, 2017. URL https://openreview.\nnet/forum?id=Sy8gdB9xx.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking\ngeneralization. Communications of the ACM, 64:107 \u2013 115, 2021.\nJ. Zhao, Y. Zhang, B. Chen, F. Sch\u00e4fer, and A. Anandkumar. Inrank: Incremental low-rank learning. arXiv\npreprint arXiv:2306.11250, 2023.\n12\nA\nA Practical guide to ReLoRA\nIn this section, we wanted to answer most common questions on hyperparameter selection. Especially\nhow to select ReLoRA-specific hyperparameters to reliably get better performance than LoRA. In all\nof our experiments, we applied LoRA/ReLoRA to all of the linear layers in the model: kqv-projection\nlayers, FFN layers and other projections, except for logits and embeddings.\nWe observed that r \u2208 {64, 128} works well for all of the networks, up to 1B. One small, but\nimportant hyperparameter change from full-rank training to ReLoRA-training that was crucial for the\nperformance was increased learning rate. ReLoRA (and LoRA) requires 1.5 \u2212 2 times larger learning\nrate than regular training/fine-tuning to achieve similar performance.\nWhen taking about ReLoRA-specific hyperparameters, we did not observe significant dependence on\noptimizer pruning percentage as long as it\u2019s larger than 90%. Larger pruning rates can lead to slightly\nbetter performance at the cost of possible loss instabilities during the reset. We tested several ReLoRA\nreset rates with 350M and 1.3B models and found that 2K iterations reset rate performed consistently\nwell in both pre-training and fine-tuning experiments and always led to better performance than no\nresets. In general, we observed good results with reset rates 2K-5K.\nB\nReLoRA for fine-tuning\nWe apply ReLoRA to fine-tune T5-base (220M parameters) and T5-large (770M parameters) on the\nGLUE benchmark. We use the same type of learning rate scheduler as in ReLoRA pre-training and\nprune 90% of the low magnitude optimizer states during each LoRA merge-and-reinit (restart). The\nbatch size is equal to 128 examples and the learning rate is tuned (from 1e-4 to 5e-4) on each model\nand dataset combination. We perform additional ReLoRA ablation studies using the T5-Large model\nand QNLI dataset. Specifically, we explore different ReLoRA ranks, optimizer state pruning rates,\nand the total number of ReLoRA resets.\nMethod\nSST-2\nMNLI\nQNLI\nQQP\nRTE\nSTS-B\nMRPC\nCoLA\nAvg\nAdapters\u2020\n94.2\n86.4\n93.1\n88.9\n75.1\n91.1\n88.9\n64.4\n85.3\nPrompt Tuning\u2020\n90.3\n82.5\n92.5\n88.5\n59.5\n90.1\n74.6\n0.0\n72.2\nLadder Side Tuning\u2020\n94.1\n85.6\n93.3\n88.8\n71.9\n90.7\n90.4\n58.1\n84.1\nCompacter*\n93.9\n86.1\n92.9\n90.4\n76.3\n91.0\n91.5\n64.4\n85.8\nKronA*\n94.3\n86.3\n93.2\n90.6\n77.7\n91.3\n92.5\n63.3\n86.1\nFull fine-tuning*\n93.6\n86.2\n92.8\n91.7\n74.8\n90.1\n92.7\n63.4\n85.7\nLoRA\n93.92\n86.12\n91.95\n90.62\n78.34\n89.96\n90.52\n60.04\n85.18\nReLoRA\n94.15\n85.96\n91.68\n87.2\n77.74\n89.88\n90.03\n59.92\n84.57\nFull fine-tuning (T5-L)\n94.7\n89.1\n91.6\n89.9\n78.9\n90.6\n88.9\n57.0\n85.0\nLoRA (T5-L)\n95.59\n89.44\n93.98\n91.44\n85.92\n90.89\n92.90\n63.77\n87.99\nReLoRA (T5-L)\n95.7\n89.06\n93.68\n91.04\n84.72\n90.53\n90.57\n61.72\n87.47\nTable 8: ReLoRA for fine-tuning does not outperform LoRA. Results with \u2020 and * are T5-base results\nfrom Sung et al. [2022] and Edalati et al. [2022] respectively.\nReLoRA fine-tuning ablations\nTable 9 shows the results of varying ReLoRA hyperparameters. A\nrank of 64 seems to provide the best performance. The results indicate that the model\u2019s performance\nremains largely unaffected even when 99% of the optimizer states are reset. Our analysis of the\njagged cosine learning rate scheduler\u2019s impact on classification accuracy in the QNLI dataset suggests\nthat two resets are adequate (reset rate 4000).\nC\nLearning curves of models pre-trained in the study\nIn this section we present additional training loss plots for all of the models from Table 2. 60M:\nFigure 6, 130M: Figure 7, 250M: Figure 8, 350M: Figure 9, 1.3B: Figure 10.\n13\nRank\nAcc.\nPruning\nAcc.\nReset rate\n#resets\nAcc.\n16\n94.05\n85%\n94.51\n6000\n1\n94.38\n32\n94.16\n92%\n94.33\n4000\n2\n94.73\n64\n94.55\n95%\n94.31\n2000\n5\n94.34\n128\n94.44\n99%\n94.56\n1000\n11\n94.33\nTable 9: ReLoRA fine-tuning ablations. We apply ReLoRA to fine-tune T5-large on the QNLI dataset\nand vary LoRA rank (r), optimizer state pruning percentage, and reset frequency of ReLoRA. Reset\nrate means the number of iterations between ReLoRA resets.\nD\nRanks of 130M models\nFigures 11 and 12 show spectral properties for 130M model.\nE\nSmaller warm start period\nTable 2 demonstrates that ReLoRA consistently outperforms the warmed-started LoRA baseline. To\nprovide a more contrasting example, we performed additional pre-training experiments starting from\njust 2K warm-started network. Figure 13 shows a significant performance gain with ReLoRA over\nLoRA by 1.4 ppl points (ppl 23.64 vs 25.08). While the absolute performance of ReLoRA is lower\ncompared to full-rank training in this context, these experiments validate our initial hypothesis that\nLoRA restarts positively impact performance.\n0\n2000\n4000\n6000\n8000\n10000\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\nLoss\n60M\n60M ReLoRA\n(40M trainable)\n60M LoRA+WarmStart\n(40M trainable)\n40M\n0\n2000\n4000\n6000\n8000\n10000\nStep\n0\n40\n60\nTrainable Params\nFigure 6: 60M experiments training loss\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\nLoss\n130M\n130M ReLoRA\n(72M trainable)\n130M LoRA+WarmStart\n(72M trainable)\n72M\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nStep\n0\n40\n60\nTrainable Params\nFigure 7: 130M experiments training loss\n14\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n3.0\n3.5\n4.0\n4.5\n5.0\nLoss\n250M\n250M ReLoRA\n(99M trainable)\n99M\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nStep\n0\n100\n250\nTrainable Params\nFigure 8: 250M experiments training loss\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\nLoss\n350M\n350M ReLoRA\n(130M trainable)\n350M LoRA+WarmStart\n(130M trainable)\n130M\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nStep\n0\n130\n350\nTrainable Params\nFigure 9: 350M experiments training loss\n0\n5000\n10000\n15000\n20000\n25000\n2.8\n3.0\n3.2\n3.4\n3.6\nLoss\n1B\n1B ReLoRA\n(250M trainable)\n1B LoRA\n(250M trainable)\n250M\n0\n5000\n10000\n15000\n20000\n25000\nStep\n0\n250\n1300\nTrainable Params\nFigure 10: 1.3B experiments training loss\n15\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFrequency\nQ Projections\nReLoRA\nLoRA\nFull-rank\ntraining\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nK Projections\nReLoRA\nLoRA\nFull-rank\ntraining\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nV Projections\nReLoRA\nLoRA\nFull-rank\ntraining\n0.0\n0.5\n1.0\n1.5\n2.0\nSingular Value\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nDown Projections\nReLoRA\nLoRA\nFull-rank\ntraining\nFigure 11: Singular values spectra of the weight difference between ReLoRA and LoRA at 5,000\niterations (warm start) and 20,000 iterations. ReLoRA exhibits a closer resemblance to full-rank\ntraining than to LoRA, indicating its effectiveness in approximating full-rank behavior. 130M models.\nWQ\nWK\nWV\nWup\nWdown\n0\n200\n400\n600\nFull-rank\nTraining\nReLoRA\nLoRA\nFigure 12: The number of singular values < 0.1 in attention and FCN matrices of the learned update.\n130M models.\nFigure 13: ReLoRA significantly outperforms LoRA when started from an early (2K steps) check-\npoint.\n16\n"
  },
  {
    "title": "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning",
    "link": "https://arxiv.org/pdf/2307.06135.pdf",
    "upvote": "11",
    "text": "SayPlan: Grounding Large Language Models using\n3D Scene Graphs for Scalable Robot Task Planning\nKrishan Rana\u20201, Jesse Haviland\u22171,2, Sourav Garg\u22173, Jad Abou-Chakra\u22171,\nIan Reid3, Niko S\u00a8underhauf1\n1QUT Centre for Robotics, Queensland University of Technology\n2CSIRO Data61 Robotics and Autonomous Systems Group\n3University of Adelaide\n\u2217Equal Contribution\n\u2020ranak@qut.edu.au\nAbstract:\nLarge language models (LLMs) have demonstrated impressive results in develop-\ning generalist planning agents for diverse tasks. However, grounding these plans\nin expansive, multi-\ufb02oor, and multi-room environments presents a signi\ufb01cant chal-\nlenge for robotics. We introduce SayPlan, a scalable approach to LLM-based,\nlarge-scale task planning for robotics using 3D scene graph (3DSG) representa-\ntions. To ensure the scalability of our approach, we: (1) exploit the hierarchical\nnature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant\nsubgraphs from a smaller, collapsed representation of the full graph; (2) reduce the\nplanning horizon for the LLM by integrating a classical path planner and (3) in-\ntroduce an iterative replanning pipeline that re\ufb01nes the initial plan using feedback\nfrom a scene graph simulator, correcting infeasible actions and avoiding planning\nfailures. We evaluate our approach on two large-scale environments spanning up\nto 3 \ufb02oors and 36 rooms with 140 assets and objects and show that our approach is\ncapable of grounding large-scale, long-horizon task plans from abstract, and nat-\nural language instruction for a mobile manipulator robot to execute. We provide\nreal robot video demonstrations on our project page sayplan.github.io.\n1\nIntroduction\n\u201cMake me a coffee and place it on my desk\u201d \u2013 The successful execution of such a seemingly straight-\nforward command remains a daunting task for today\u2019s robots. The associated challenges permeate\nevery aspect of robotics, encompassing navigation, perception, manipulation as well as high-level\ntask planning. Recent advances in Large Language Models (LLMs) [1, 2, 3] have led to signi\ufb01cant\nprogress in incorporating common sense knowledge for robotics [4, 5, 6]. This enables robots to\nplan complex strategies for a diverse range of tasks that require a substantial amount of background\nknowledge and semantic comprehension.\nFor LLMs to be effective planners in robotics, they must be grounded in reality, that is, they must ad-\nhere to the constraints presented by the physical environment in which the robot operates, including\nthe available affordances, relevant predicates, and the impact of actions on the current state. Further-\nmore, in expansive environments, the robot must additionally understand where it is, locate items of\ninterest, as well comprehend the topological arrangement of the environment in order to plan across\nthe necessary regions. To address this, recent works have explored the utilization of vision-based\nvalue functions [4], object detectors [7, 8], or Planning Domain De\ufb01nition Language (PDDL) de-\nscriptions of a scene [9, 10] to ground the output of the LLM-based planner. However, these efforts\nare primarily con\ufb01ned to small-scale environments, typically single rooms with pre-encoded infor-\nmation on all the existing assets and objects present. The challenge lies in scaling these models.\nAs the environment\u2019s complexity and dimensions expand, and as more rooms and entities enter the\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\nIterative\nReplanning\nExplore \nScene Graph\nSimulator\nFeedback\nPlan\nVerification\n{command: expand_node,\n node_name: \u201ckitchen}\nFull Search Sequence\nScene Graph \nSimulator\n\u201cMake Peter a coffee\u201d\nInstruction\nPrompt\n3D Scene Graph [Collapsed]\nMemory\nSEMANTIC\nSEARCH\nExecutable Plan\n{goto: office}\n{access: desk}\n{pickup: mug}\n{goto: kitchen}\n{release: mug}\n{turn_on: machine}\n{turn_off: machine}\n{pickup: mug}\n{goto: office}\n{access: desk}\n{goto: pose13}\n{goto: office}\n{access: desk}\n{pickup: mug}\n{goto: pose18}\n{goto: pose21}\n{goto: kitchen}\n{release: mug}\n{turn_on: machine}\n{turn_off: machine}\n{pickup: mug}\n{goto: pose21}\n{goto: pose26}\n{goto: pose25}\n{goto: office}\n{access: desk}\nPath Planner\nScene Graph \nSimulator\nSpecifications\n\u201cMake Peter a coffee\u201d\nInstruction\nPrompt\nExplored Subgraph\nAgent Role\nEnvironment Functions\nEnvironment State\nOutput Format\nExample | Feedback\nMemory\nSpecifications\nHigh-Level Plan\nITERATIVE \nREPLANNING\nGraph API Call\nCollapse\nGraph\n\u201cMake Peter a coffee\u201d\nSemantic\nSearch\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFeedback: \u201cCannot \nrelease coffee mug \nhere\u201d\nIterative\nReplanning\nCollapse\nGraph\n\u201cMake Peter a coffee\u201d\nSemantic\nSearch\n3D Scene \nGraph\n3D Scene\nGraph\nAgent Role\nEnvironment Functions\nEnvironment State\nOutput Format\nExample\nFigure 1: SayPlan Overview (top). SayPlan operates across two stages to ensure scalability: (left)\nGiven a collapsed 3D scene graph and a task instruction, semantic search is conducted by the LLM\nto identify a suitable subgraph that contains the required items to solve the task; (right) The explored\nsubgraph is then used by the LLM to generate a high-level task plan, where a classical path planner\ncompletes the navigational component of the plan; \ufb01nally, the plan goes through an iterative re-\nplanning process with feedback from a scene graph simulator until an executable plan is identi\ufb01ed.\nNumbers on the top-left corners represent the \ufb02ow of operations.\nscene, pre-encoding all the necessary information within the LLM\u2019s context becomes increasingly\ninfeasible.\nTo this end, we present a scalable approach to ground LLM-based task planners across environments\nspanning multiple rooms and \ufb02oors. We achieve this by exploiting the growing body of 3D scene\ngraph (3DSG) research [11, 12, 13, 14, 15, 16]. 3DSGs capture a rich topological and hierarchically-\norganised semantic graph representation of an environment with the versatility to encode the nec-\nessary information required for task planning including object state, predicates, affordances and\nattributes using natural language \u2013 suitable for parsing by an LLM. We can leverage a JSON repre-\nsentation of this graph as input to a pre-trained LLM, however, to ensure the scalability of the plans\nto expansive scenes, we present three key innovations.\nFirstly, we present a mechanism that enables the LLM to conduct a semantic search for a task-\nrelevant subgraph G\u2032 by manipulating the nodes of a \u2018collapsed\u2019 3DSG, which exposes only the top\nlevel of the full graph G, via expand and contract API function calls \u2013 thus making it feasible\nto plan over increasingly large-scale environments. In doing so, the LLM maintains focus on a rela-\ntively small, informative subgraph, G\u2032 during planning, without exceeding its token limit. Secondly,\nas the horizon of the task plans across such environments tends to grow with the complexity and\nrange of the given task instructions, there is an increasing tendency for the LLM to hallucinate or\nproduce infeasible action sequences [17, 18, 7]. We counter this by \ufb01rstly relaxing the need for the\nLLM to generate the navigational component of the plan, and instead leverage an existing optimal\npath planner such as Dijkstra [19] to connect high-level nodes generated by the LLM. Finally, to en-\nsure the feasibility of the proposed plan, we introduce an iterative replanning pipeline that veri\ufb01es\nand re\ufb01nes the initial plan using feedback from a scene graph simulator in order to correct for any\nunexecutable actions, e.g., missing to open the fridge before putting something into it \u2013 thus avoid-\ning planning failures due to inconsistencies, hallucinations, or violations of the physical constraints\nand predicates imposed by the environment.\n2\nOur approach SayPlan ensures feasible and grounded plan generation for a mobile manipulator\nrobot operating in large-scale environments spanning multiple \ufb02oors and rooms. We evaluate our\nframework across a range of 90 tasks organised into four levels of dif\ufb01culty. These include semantic\nsearch tasks such as (\u201cFind me something non-vegetarian.\u201d) to interactive, long-horizon tasks with\nambiguous multi-room objectives that require a signi\ufb01cant level of common-sense reasoning (\u201cLet\u2019s\nplay a prank on Niko\u201d). These tasks are assessed in two expansive environments, including a large\nof\ufb01ce \ufb02oor spanning 37 rooms and 150 interactable assets and objects, and a three-storey house\nwith 28 rooms and 112 objects. Our experiments validate SayPlan\u2019s ability to scale task planning to\nlarge-scale environments while conserving a low token footprint. By introducing a semantic search\npipeline, we can reduce full large-scale scene representations by up to 82.1% for LLM parsing and\nour iterative replanning pipeline allows for near-perfect executability rates, suitable for execution on\na real mobile manipulator robot.1\n2\nRelated Work\nTask planning in robotics aims to generate a sequence of high-level actions to achieve a goal within\nan environment. Conventional methods employ domain-speci\ufb01c languages such as PDDL [20, 21,\n22] and ASP [23] together with semantic parsing [24, 25], search techniques [26, 27] and complex\nheuristics [28] to arrive at a solution. These methods, however, lack both the scalability to large\nenvironments as well as the task generality required when operating in the real world. Hierarchical\nand reinforcement learning-based alternatives [29, 30], [31] face challenges with data demands and\nscalability. Our work leverages the in-context learning capabilities of LLMs to generate task plans\nacross 3D scene graphs. Tasks, in this case, can be naturally expressed using language, with the\ninternet scale training of LLMs providing the desired knowledge for task generality, while 3D scene\ngraphs provide the grounding necessary for large-scale environment operation. This allows for a\ngeneral and scalable framework when compared to traditional non-LLM-based alternatives.\nTask planning with LLMs, that is, translating natural language prompts into task plans for robotics,\nis an emergent trend in the \ufb01eld. Earlier studies have effectively leveraged pre-trained LLMs\u2019 in-\ncontext learning abilities to generate actionable plans for embodied agents [4, 10, 9, 8, 32, 7, 33]. A\nkey challenge for robotics is grounding these plans within the operational environment of the robot.\nPrior works have explored the use of object detectors [8, 7], PDDL environment representations [10,\n9, 34] or value functions [4] to achieve this grounding, however, they are predominantly constrained\nto single-room environments, and scale poorly with the number of objects in a scene which limits\ntheir ability to plan over multi-room or multi-\ufb02oor environments. In this work, we explore the use\nof 3D scene graphs and the ability of LLMs to generate plans over large-scale scenes by exploiting\nthe inherent hierarchical and semantic nature of these representations.\nIntegrating external knowledge in LLMs has been a growing line of research combining language\nmodels with external tools to improve the reliability of their outputs. In such cases, external modules\nare used to provide feedback or extra information to the LLM to guide its output generation. This is\nachieved either through API calls to external tools [35, 36] or as textual feedback from the operating\nenvironment [37, 8]. More closely related to our work, CLAIRIFY [38] iteratively leverage com-\npiler error feedback to re-prompt an LLM to generate syntactically valid code. Building on these\nideas, we propose an iterative plan veri\ufb01cation process with feedback from a scene graph-based\nsimulator to ensure all generated plans adhere to the constraints and predicates captured by the pre-\nconstructed scene graph. This ensures the direct executability of the plan on a mobile manipulator\nrobot, operating in the corresponding real-world environment.\n3\nSayPlan\n3.1\nProblem Formulation\nWe aim to address the challenge of long-range task planning for an autonomous agent, such as\na mobile manipulator robot, in a large-scale environment based on natural language instructions.\nThis requires the robot to comprehend abstract and ambiguous instructions, understand the scene\nand generate task plans involving both navigation and manipulation of a mobile robot within an\n1sayplan.github.io\n3\nAlgorithm 1: SayPlan\nGiven: scene graph simulator \u03c8, classical path planner \u03c6, large language model LLM\nInputs: prompt P, scene graph G, instruction I\n1: G\u2032 \u2190 collapse\u03c8(G)\n\u25b7 collapse scene graph\nStage 1: Semantic Search\n\u25b7 search scene graph for all relevant items\n2: while command != \u201cterminate\u201d do\n3:\ncommand, node name \u2190 LLM(P, G\u2032, I)\n4:\nif command == \u201cexpand\u201d then\n5:\nG\u2032 \u2190 expand\u03c8(node name)\n\u25b7 expand node to reveal objects and assets\n6:\nelse if command == \u201ccontract\u201d then\n7:\nG\u2032 \u2190 contract\u03c8(node name)\n\u25b7 contract node if nothing relevant found\nStage 2: Causal Planning\n\u25b7 generate a feasible plan\n8: feedback = \u201c \u201d\n9: while feedback != \u201csuccess\u201d do\n10:\nplan \u2190 LLM(P, G\u2032, I, feedback)\n\u25b7 high level plan\n11:\nfull plan \u2190 \u03c6(plan, G\u2032)\n\u25b7 compute optimal navigational path between nodes\n12:\nfeedback \u2190 verify_plan\u03c8(full plan)\n\u25b7 forward simulate the full plan\n13: return full plan\n\u25b7 executable plan\nenvironment. Existing approaches lack the ability to reason over scenes spanning multiple \ufb02oors\nand rooms. Our focus is on integrating large-scale scenes into planning agents based on Language\nModels (LLMs) and solving the scalability challenge. We aim to tackle two key problems: 1)\nrepresenting large-scale scenes within LLM token limitations, and 2) mitigating LLM hallucinations\nand erroneous outputs when generating long-horizon plans in large-scale environments.\n3.2\nPreliminaries\nFigure 2: Hierarchical Structure of a 3D Scene\nGraph. This graph consists of 4 levels. Notes that\nthe room nodes are connected to one another via\nsequences of pose nodes which capture the topo-\nlogical arrangement of a scene.\nHere, we describe the 3D scene graph represen-\ntation of an environment and the scene graph\nsimulator API which we leverage throughout\nour approach.\nScene Representation:\n3D Scene Graphs\n(3DSG) [11, 12, 14] have recently emerged\nas an actionable world representation for\nrobots [13, 15, 16, 39, 40, 41], which hierarchi-\ncally abstract the environment at multiple lev-\nels through spatial semantics and object rela-\ntionships while capturing relevant states, affor-\ndances and predicates of the entities present in\nthe environment. Formally, a 3DSG is a hierar-\nchical multigraph G = (V, E) in which the set\nof vertices V comprises V1\u222aV2\u222a. . .\u222aVK, with\neach Vk signifying the set of vertices at a particular level of the hierarchy k. Edges stemming from\na vertex v \u2208 Vk may only terminate in Vk\u22121 \u222a Vk \u222a Vk+1, i.e. edges connect nodes within the same\nlevel, or one level higher or lower.\nWe assume a pre-constructed 3DSG representation of a large-scale environment generated using\nexisting techniques [15, 13, 11]. The entire 3DSG can be represented as a NetworkX Graph\nobject [42] and text-serialised into a JSON data format that can be parsed directly by a pre-\ntrained LLM. An example of a single asset node from the 3DSG is represented as:\n{name:\ncoffee_machine, type:\nasset, location:\nkitchen, affordances:\n[turn_on,\nturn_off, release], state:\noff, attributes:\n[red, automatic], position:\n[2.34, 0.45, 2.23]} with edges between nodes captured as {kitchen\u2194coffee machine}.\nThe 3DSG is organized in a hierarchical manner with four primary levels: \ufb02oors, rooms, assets,\nand objects as shown in Figure 2. The top level contains \ufb02oors, each of which branches out to\nseveral rooms. These rooms are interconnected through pose nodes to represent the environment\u2019s\ntopological structure. Within each room, we \ufb01nd assets (immovable entities) and objects (movable\nentities). Both asset and object nodes encode particulars including state, affordances, additional\nattributes such as colour or weight, and 3D pose. The graph also incorporates a dynamic agent\n4\nnode, denoting a robot\u2019s location within the scene. Note that this hierarchy is scalable and node\nlevels can be adapted to capture even larger environments e.g. campuses and buildings\nScene Graph Simulator \u03c8 refers to a set of API calls for manipulating and operating over JSON for-\nmatted 3DSGs, using the following functions: 1) collapse(G): Given a full 3DSG, this function\nreturns an updated scene graph that exposes only the highest level within the 3DSG hierarchy e.g.\n\ufb02oor nodes. 2) expand(node name): Returns an updated 3DSG that reveals all the nodes con-\nnected to node name in the level below. 3) contract(node name): Returns an updated 3DSG\nthat hides all the nodes connected to node name in the level below. 4) verify_plan(plan):\nForward simulates the generated plan at the abstract graph level captured by the 3DSG to check if\neach action adheres to the environment\u2019s predicates, states and affordances. Returns textual feedback\ne.g. \u201ccannot pick up banana\u201d if the fridge containing the banana is closed.\n3.3\nApproach\nWe present a scalable framework for grounding the generalist task planning capabilities of pre-\ntrained LLMs in large-scale environments spanning multiple \ufb02oors and rooms using 3DSG repre-\nsentations. Given a 3DSG G and a task instruction I de\ufb01ned in natural language, we can view our\nframework SayPlan as a high-level task planner \u03c0(a|I, G), capable of generating long-horizon plans\na grounded in the environment within which a mobile manipulator robot operates. This plan is then\nfed to a low-level visually grounded motion planner for real-world execution. To ensure the scala-\nbility of SayPlan, two stages are introduced: Semantic Search and Iterative Replanning which we\ndetail below. An overview of the SayPlan pipeline is illustrated in Figure 1 with the corresponding\npseudo-code given in Algorithm 1.\nSemantic Search: When planning over 3DSGs using LLMs we take note of two key observations:\n1) A 3DSG of a large-scale environment can grow in\ufb01nitely with the number of rooms, assets and\nobjects it contains, making it impractical to pass as input to an LLM due to token limits and 2) only\na subset of the full 3DSG G is required to solve any given task e.g. we don\u2019t need to know about\nthe toothpaste in the bathroom when making a cup of coffee. To this end, the Semantic Search stage\nseeks to identify this smaller, task-speci\ufb01c subgraph G\u2032 from the full 3DSG which only contains\nthe entities in the environment required to solve the given task instruction. To identify G\u2032 from a\nfull 3DSG, we exploit the semantic hierarchy of these representations and the reasoning capabilities\nof LLMs. We \ufb01rstly collapse G to expose only its top level e.g. the \ufb02oor nodes, reducing\nthe 3DSG initial token representation by \u2248 80%. The LLM manipulates this collapsed graph via\nexpand and contract API calls in order to identify the desired subgraph for the task based on\nthe given instruction I. This is achieved using in-context learning over a set of input-out examples\n(see Appendix J), and utilising chain-of-thought prompting to guide the LLM in identifying which\nnodes to manipulate. The chosen API call and node are executed within the scene graph simulator,\nand the updated 3DSG is passed back to the LLM for further exploration. If an expanded node is\nfound to contain irrelevant entities for the task, the LLM contracts it to manage token limitations\nand maintain a task-speci\ufb01c subgraph (see Figure 3). To avoid expanding already-contracted nodes,\nwe maintain a list of previously expanded nodes, passed as an additional Memory input to the\nLLM, facilitating a Markovian decision-making process and allowing SayPlan to scale to extensive\nsearch sequences without the overhead of maintaining the full interaction history [5]. The LLM\nautonomously proceeds to the planning phase once all necessary assets and objects are identi\ufb01ed in\nthe current subgraph G\u2032. An example of the LLM-scene graph interaction during Semantic Search\nis provided in Appendix K.\nIterative Replanning: Given the identi\ufb01ed subgraph G\u2032 and the same task instruction I from above,\nthe LLM enters the planning stage of the pipeline. Here the LLM is tasked with generating a\nsequence of node-level navigational (goto(pose2)) and manipulation (pickup(coffee_mug))\nactions that satisfy the given task instruction. LLMs, however, are not perfect planning agents\nand tend to hallucinate or produce erroneous outputs [43, 9]. This is further exacerbated when\nplanning over large-scale environments or long-horizon tasks.\nWe facilitate the generation of\ntask plans by the LLM via two mechanisms.\nFirst, we shorten the LLM\u2019s planning horizon\nby delegating pose-level path planning to an optimal path planner, such as Dijkstra. For exam-\nple, a typical plan output such as [goto(meeting_room), goto(pose13), goto(pose14),\ngoto(pose8), ..., goto(kitchen), access(fridge), open(fridge)] is simpli\ufb01ed to\n[goto(meeting_room), goto(kitchen), access(fridge), open(fridge)]. The path\n5\nplanner handles \ufb01nding the optimal route between high-level locations, allowing the LLM to focus\non essential manipulation components of the task. Secondly, we build on the self-re\ufb02ection capabil-\nities of LLMs [17] to iteratively correct their generated plans using textual, task-agnostic feedback\nfrom a scene graph simulator which evaluates if the generated plan complies with the scene\ngraph\u2019s predicates, state, and affordances. For instance, a pick(banana) action might fail if the\nrobot is already holding something, if it is not in the correct location or if the fridge was not opened\nbeforehand. Such failures are transformed into textual feedback (e.g., \u201dcannot pick banana\u201d), ap-\npended to the LLM\u2019s input, and used to generate an updated, executable plan. This iterative process,\ninvolving planning, validation, and feedback integration, continues until a feasible plan is obtained.\nThe validated plan is then passed to a low-level motion planner for robotic execution. An example\nof the LLM-scene graph interaction during iterative replanning is provided in Appendix L. Speci\ufb01c\nimplementation details are provided in Appendix A.\n4\nExperimental Setup\nWe design our experiments to evaluate the 3D scene graph reasoning capabilities of LLMs with a\nparticular focus on high-level task planning pertaining to a mobile manipulator robot. The plans ad-\nhere to a particular embodiment consisting of a 7-degree-of-freedom robot arm with a two-\ufb01ngered\ngripper attached to a mobile base. We use two large-scale environments, shown in Figure 4, which\nexhibit multiple rooms and multiple \ufb02oors which the LLM agent has to plan across. To better ablate\nand showcase the capabilities of SayPlan, we decouple its semantic search ability from the overall\ncausal planning capabilities using the following two evaluation settings as shown in Appendix C:\nSemantic Search:\nHere, we focus on queries which test the semantic search capabilities of an\nLLM provided with a collapsed 3D scene graph. This requires the LLM to reason over the room and\n\ufb02oor node names and their corresponding attributes in order to aid its search for the relevant assets\nand objects required to solve the given task instruction. We evaluate against a human baseline to\nunderstand how the semantic search capabilities of an LLM compare to a human\u2019s thought process.\nFurthermore, to gain a better understanding of the impact different LLM models have on this graph-\nbased reasoning, we additionally compare against a variant of SayPlan using GPT-3.5.\nCausal Planning:\nIn this experiment, we evaluate the ability of SayPlan to generate feasible plans\nto solve a given natural language instruction. The evaluation metrics are divided into two compo-\nnents: 1) Correctness, which primarily validates the overall goal of the plan and its alignment to\nwhat a human would do to solve the task and 2) Executability, which evaluates the alignment of the\nplan to the constraints of the scene graph environment and its ability to be executed by a mobile\nmanipulator robot. We note here that for a plan to be executable, it does not necessarily have to be\ncorrect and vice versa. We evaluate SayPlan against two baseline methods that integrate an LLM for\ntask planning:\nLLM-As-Planner, which generates a full plan sequence in an open-loop manner; the plan includes\nthe full sequence of both navigation and manipulation actions that the robot must execute to complete\na task, and LLM+P, an ablated variant of SayPlan, which only incorporates the path planner to allow\nfor shorter horizon plan sequences, without any iterative replanning.\n5\nResults\n5.1\nSemantic Search\nOf\ufb01ce\nHome\nSubtask\nHuman\nSayPlan\n(GPT-3.5)\nSayPlan\n(GPT-4)\nHuman\nSayPlan\n(GPT-3.5)\nSayPlan\n(GPT-4)\nSimple Search\n100%\n6.6%\n86.7%\n100%\n0.0%\n86.7%\nComplex Search\n100%\n0.0%\n73.3%\n100%\n0.0%\n73.3%\nTable 1: Evaluating the semantic search capabilities of\nGPT-4. The table shows the semantic search success rate\nin \ufb01nding a suitable subgraph for planning.\nWe summarise the results for the\nsemantic search evaluation in Table\n1. SayPlan (GPT-3.5) consistently\nfailed to reason over the input graph\nrepresentation, hallucinating nodes to\nexplore or stagnating at exploring the same node multiple times. SayPlan (GPT-4) in contrast\nachieved 86.7% and 73.3% success in identifying the desired subgraph across both the simple and\ncomplex search tasks respectively, demonstrating signi\ufb01cantly better graph-based reasoning than\nGPT-3.5.\n6\nSimple\nLong Horizon\nTypes of Errors\nCorr\nExec\nCorr\nExec\nMissing\nAction\nMissing\nPose\nWrong\nAction\nIncomplete\nSearch\nHallucinated\nNodes\nLLM+P\n93.3%\n13.3%\n33.3%\n0.0%\n26.7%\n0.0%\n10.0%\n3.33%\n10.0%\nLLM-As-Planner\n93.3%\n80.0%\n66.7%\n13.3%\n20.0%\n60.0%\n0.17%\n0.03%\n10.0%\nSayPlan\n93.3%\n100.0%\n73.3%\n86.6%\n0.0%\n0.0%\n0.0%\n0.0%\n6.67%\nTable 3: Causal Planning Results. Left: Correctness and Executability on Simple and Long\nHorizon planning tasks and Right: Types of execution errors encountered when planning using\nLLMs. Note that SayPlan corrects the majority of the errors faced by LLM-based planners.\nFigure 3: Scene Graph Token Progression Dur-\ning Semantic Search. This graph illustrates the\nscalability of our approach to large-scale 3D scene\ngraphs. Note the importance of node contraction\nin maintaining a near constant token representa-\ntion of the 3DSG input.\nWhile as expected the human baseline achieved\n100% on all sets of instructions, we are more\ninterested in the qualitative assessment of the\ncommon-sense reasoning used during seman-\ntic search. More speci\ufb01cally we would like to\nidentify the similarity in the semantic search\nheuristics utilised by humans and that used by\nthe underlying LLM based on the given task in-\nstruction.\nWe present the full sequence of explored nodes\nfor both SayPlan (GPT-4) and the human base-\nline in Appendix F. As shown in the tables, Say-\nPlan (GPT-4) demonstrates remarkably similar\nperformance to a human\u2019s semantic and com-\nmon sense reasoning for most tasks, exploring\na similar sequence of nodes given a particu-\nlar instruction.\nFor example, when asked to\n\u201c\ufb01nd a ripe banana\u201d, the LLM \ufb01rst explores\nthe kitchen followed by the next most likely\nlocation, the cafeteria. In the case where no\nsemantics are present in the instruction such\nas \u201c\ufb01nd me object K31X\u201d, we note that the\nLLM agent is capable of conducting a breadth-\n\ufb01rst-like search across all the unexplored nodes.\nThis highlights the importance of meaningful\nnode names and attributes that capture the rel-\nevant environment semantics that the LLM can\nleverage to relate the query instruction for ef\ufb01-\ncient search.\nFull Graph\n(Token Count)\nCollapsed Graph\n(Token Count)\nCompression Ratio\nOf\ufb01ce\n6731\n878\n86.9%\nHome\n6598\n1817\n72.5%\nTable 2: 3D Scene Graph Token Count Number\nof tokens required for the full graph vs. collapsed\ngraph.\nAn odd failure case in the simple search instructions involved negation, where the agent consistently\nfailed when presented with questions such as \u201cFind me an of\ufb01ce that does not have a cabinet\u201d or\n\u201cFind me a bathroom with no toilet\u201d. Other failure cases noted across the complex search instruc-\ntions included the LLM\u2019s failure to conduct simple distance-based and count-based reasoning over\ngraph nodes. While trivial to a human, this does require the LLM agent to reason over multiple\nnodes simultaneously, where it tends to hallucinate or miscount connected nodes.\nScalability Analysis: We additionally analyse the scalability of SayPlan during semantic search.\nTable 2 illustrates the impact of exploiting the hierarchical nature of 3D scene graphs and allowing\nthe LLM to explore the graph from a collapsed initial state. This allows for a reduction of 82.1% in\nthe initial input tokens required to represent the Of\ufb01ce environment and a 60.4% reduction for the\nHome environment. In Figure 3, we illustrate how endowing the LLM with the ability to contract\nexplored nodes which it deems unsuitable for solving the task allows it to maintain near-constant\ninput memory from a token perspective across the entire semantic search process. Note that the\ninitial number of tokens already present represents the input prompt tokens as given in Appendix J.\nFurther ablation studies on the scalability of SayPlan to even larger 3DSGs are provided in Appendix\nH.\n7\n5.2\nCausal Planning\nThe results for causal planning across simple and long-horizon instructions are summarised in Ta-\nble 3 (left). We compared SayPlan\u2019s performance against two baselines: LLM-As-Planner and\nLLM+P. All three methods displayed consistent correctness in simple planning tasks at 93%, given\nthat this metric is more a function of the underlying LLMs reasoning capabilities. However, it is in-\nteresting to note that in the long-horizon tasks, both the path planner and iterative replanning play an\nimportant role in improving this correctness metric by reducing the planning horizon and allowing\nthe LLM to re\ufb02ect on its previous output.\nThe results illustrate that the key to ensuring the task plan\u2019s executability was iterative replanning.\nBoth LLM-As-Planner and LLM+P exhibited poor executability, whereas SayPlan achieved near-\nperfect executability as a result of iterative replanning, which ensured that the generated plans were\ngrounded to adhere to the constraints and predicated imposed by the environment. Detailed task\nplans and errors encountered are provided in Appendix G. We summarise these errors in Table 3\n(right) which shows that plans generated with LLM+P and LLM-As-Planner entailed various types\nof errors limiting their executability. LLM+P mitigated navigational path planning errors as a result\nof the classical path planner however still suffered from errors pertaining to the manipulation of the\nenvironment - missing actions or incorrect actions which violate environment predicates. SayPlan\nmitigated these errors via iterative replanning, however in 6.67% of tasks, it failed to correct for\nsome hallucinated nodes. While we believe these errors could be eventually corrected via iterative\nreplanning, we limited the number of replanning steps to 5 throughout all experiments. We provide\nan illustration of the real-world execution of a generated plan using SayPlan on a mobile manipulator\nrobot coupled with a vision-guided motion controller [44, 45] in Appendix I.\n6\nLimitations\nSayPlan is notably constrained by the limitations inherent in current large language models (LLMs),\nincluding biases and inaccuracies, affecting the validity of its generated plans. More speci\ufb01cally,\nSayPlan is limited by the graph-based reasoning capabilities of the underlying LLM which fails\nat simple distance-based reasoning, node count-based reasoning and node negation. Future work\ncould explore \ufb01ne-tuning these models for these speci\ufb01c tasks or alternatively incorporate existing\nand more complex graph reasoning tools [46] to facilitate decision-making. Secondly, SayPlan\u2019s\ncurrent framework is constrained by the need for a pre-built 3D scene graph and assumes that ob-\njects remain static post-map generation, signi\ufb01cantly restricting its adaptability to dynamic real-\nworld environments. Future work could explore how online scene graph SLAM systems [15] could\nbe integrated within the SayPlan framework to account for this. Additionally, the incorporation of\nopen-vocabulary representations within the scene graph could yield a general scene representation\nas opposed to solely textual node descriptions. Lastly, a potential limitation of the current system\nlies in the scene graph simulator and its ability to capture the various planning failures within the\nenvironment. While this works well in the cases presented in this paper, for more complex tasks in-\nvolving a diverse set of predicates and affordances, the incorporation of relevant feedback messages\nfor each instance may become infeasible and forms an important avenue for future work in this area.\n7\nConclusion\nSayPlan is a natural language-driven planning framework for robotics that integrates hierarchical 3D\nscene graphs and LLMs to plan across large-scale environments spanning multiple \ufb02oors and rooms.\nWe ensure the scalability of our approach by exploiting the hierarchical nature of 3D scene graphs\nand the semantic reasoning capabilities of LLMs to enable the agent to explore the scene graph\nfrom the highest level within the hierarchy, resulting in a signi\ufb01cant reduction in the initial tokens\nrequired to capture larger environments. Once explored, the LLM generates task plans for a mobile\nmanipulator robot, and a scene graph simulator ensures that the plan is feasible and grounded to\nthe environment via iterative replanning. The framework surpasses existing techniques in producing\ncorrect, executable plans, which a robot can then follow. Finally, we successfully translate validated\nplans to a real-world mobile manipulator agent which operates across multiple rooms, assets and\nobjects in a large of\ufb01ce environment. SayPlan represents a step forward for general-purpose service\nrobotics that can operate in our homes, hospitals and workplaces, laying the groundwork for future\nresearch in this \ufb01eld.\n8\nAcknowledgments\nThe authors would like to thank Ben Burgess-Limerick for assistance with the robot hardware setup,\nNishant Rana for creating the illustrations and Norman Di Palo and Michael Milford for insight-\nful discussions and feedback towards this manuscript. The authors also acknowledge the ongoing\nsupport from the QUT Centre for Robotics. This work was partially supported by the Australian\nGovernment through the Australian Research Council\u2019s Discovery Projects funding scheme (Project\nDP220102398) and by an Amazon Research Award to Niko S\u00a8underhauf.\nReferences\n[1] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. E. Miller, M. Simens, A. Askell,\nP. Welinder, P. F. Christiano, J. Leike, and R. J. Lowe. Training language models to follow\ninstructions with human feedback. ArXiv, abs/2203.02155, 2022.\n[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Had-\nsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[3] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[4] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang,\nR. Julian, et al. Do As I Can, Not As I Say: Grounding language in robotic affordances. In\nConference on Robot Learning, pages 287\u2013318. PMLR, 2023.\n[5] N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi. Chatgpt empowered long-\nstep robot control in various environments: A case application, 2023.\n[6] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke,\nK. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence. Palm-E: An\nembodied multimodal language model, 2023.\n[7] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. LLM-Planner:\nFew-shot grounded planning for embodied agents with large language models. arXiv preprint\narXiv:2212.04088, 2022.\n[8] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022.\n[9] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. LLM+P: Empowering\nlarge language models with optimal planning pro\ufb01ciency. arXiv preprint arXiv:2304.11477,\n2023.\n[10] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-P\u00b4erez, and L. P. Kaelbling.\nPDDL planning with pretrained large language models. In NeurIPS 2022 Foundation Models\nfor Decision Making Workshop.\n[11] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, and S. Savarese.\n3D\nscene graph: A structure for uni\ufb01ed semantics, 3D space, and camera. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pages 5664\u20135673, 2019.\n[12] U.-H. Kim, J.-M. Park, T.-J. Song, and J.-H. Kim. 3-D scene graph: A sparse and semantic rep-\nresentation of physical environments for intelligent agents. IEEE transactions on cybernetics,\n50(12):4921\u20134933, 2019.\n9\n[13] A. Rosinol, A. Violette, M. Abate, N. Hughes, Y. Chang, J. Shi, A. Gupta, and L. Carlone.\nKimera: From slam to spatial perception with 3D dynamic scene graphs. The International\nJournal of Robotics Research, 40(12-14):1510\u20131546, 2021.\n[14] P. Gay, J. Stuart, and A. Del Bue. Visual graphs from motion (vgfm): Scene understanding\nwith object geometry reasoning. In Computer Vision\u2013ACCV 2018: 14th Asian Conference on\nComputer Vision, Perth, Australia, December 2\u20136, 2018, Revised Selected Papers, Part III 14,\npages 330\u2013346. Springer, 2019.\n[15] N. Hughes, Y. Chang, and L. Carlone. Hydra: A real-time spatial perception engine for 3D\nscene graph construction and optimization. Robotics: Science and Systems XIV, 2022.\n[16] C. Agia, K. M. Jatavallabhula, M. Khodeir, O. Miksik, V. Vineet, M. Mukadam, L. Paull,\nand F. Shkurti. Taskography: Evaluating robot task planning over large 3D scene graphs. In\nConference on Robot Learning, pages 46\u201358. PMLR, 2022.\n[17] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Re\ufb02exion: Lan-\nguage agents with verbal reinforcement learning, 2023.\n[18] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[19] E. W. Dijkstra. A note on two problems in connexion with graphs. In Edsger Wybe Dijkstra:\nHis Life, Work, and Legacy, pages 287\u2013290. 2022.\n[20] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram, M. Veloso, D. Weld, and\nD. Wilkins. PDDL-the planning domain de\ufb01nition language. 1998.\n[21] M. Fox and D. Long. PDDL2. 1: An extension to PDDL for expressing temporal planning\ndomains. Journal of arti\ufb01cial intelligence research, 20:61\u2013124, 2003.\n[22] P. Haslum, N. Lipovetzky, D. Magazzeni, and C. Muise. An introduction to the planning do-\nmain de\ufb01nition language. Synthesis Lectures on Arti\ufb01cial Intelligence and Machine Learning,\n13(2):1\u2013187, 2019.\n[23] M. Gelfond and Y. Kahl. Knowledge representation, reasoning, and the design of intelligent\nagents: The answer-set programming approach. Cambridge University Press, 2014.\n[24] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy.\nUnderstanding natural language commands for robotic navigation and mobile manipulation.\nProceedings of the AAAI Conference on Arti\ufb01cial Intelligence, 2011.\n[25] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion, J. W. Hart,\nP. Stone, and R. J. Mooney. Jointly improving parsing and perception for natural language\ncommands through human-robot dialog. J. Artif. Intell. Res., 67:327\u2013374, 2020.\n[26] H. Kautz and B. Selman. Pushing the envelope: Planning, propositional logic, and stochastic\nsearch. In Proceedings of the national conference on arti\ufb01cial intelligence, pages 1194\u20131201,\n1996.\n[27] B. Bonet and H. Geffner. Planning as heuristic search. Arti\ufb01cial Intelligence, 129(1-2):5\u201333,\n2001.\n[28] M. Vallati, L. Chrpa, M. Grze\u00b4s, T. L. McCluskey, M. Roberts, S. Sanner, et al. The 2014\ninternational planning competition: Progress and trends. AI Magazine, 36(3):90\u201398, 2015.\n[29] R. Chitnis, T. Silver, B. Kim, L. Kaelbling, and T. Lozano-Perez. CAMPs: Learning Context-\nSpeci\ufb01c Abstractions for Ef\ufb01cient Planning in Factored MDPs. In Conference on Robot Learn-\ning, pages 64\u201379. PMLR, 2021.\n[30] T. Silver, R. Chitnis, A. Curtis, J. B. Tenenbaum, T. Lozano-P\u00b4erez, and L. P. Kaelbling. Plan-\nning with learned object importance in large problem instances using graph neural networks. In\nProceedings of the AAAI conference on arti\ufb01cial intelligence, volume 35, pages 11962\u201311971,\n2021.\n10\n[31] F. Ceola, E. Tosello, L. Tagliapietra, G. Nicola, and S. Ghidoni. Robot task planning via\ndeep reinforcement learning: a tabletop object sorting application. In 2019 IEEE International\nConference on Systems, Man and Cybernetics (SMC), pages 486\u2013492, 2019. doi:10.1109/\nSMC.2019.8914278.\n[32] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing\nreasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n[33] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sind-\nhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reason-\ning with language. arXiv preprint arXiv:2204.00598, 2022.\n[34] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning\ngoals with large-language models. arXiv preprint arXiv:2302.05128, 2023.\n[35] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, et al.\nCheck your facts and try again: Improving large language models with external knowledge and\nautomated feedback. arXiv preprint arXiv:2302.12813, 2023.\n[36] T. Schick, J. Dwivedi-Yu, R. Dess`\u0131, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and\nT. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint\narXiv:2302.04761, 2023.\n[37] R. Liu, J. Wei, S. S. Gu, T.-Y. Wu, S. Vosoughi, C. Cui, D. Zhou, and A. M. Dai. Mind\u2019s eye:\nGrounded language model reasoning through simulation. arXiv preprint arXiv:2210.05359,\n2022.\n[38] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. Aspuru-\nGuzik, F. Shkurti, and A. Garg. Errors are useful prompts: Instruction guided task program-\nming with veri\ufb01er-assisted iterative prompting. arXiv preprint arXiv:2303.14100, 2023.\n[39] Z. Ravichandran, L. Peng, N. Hughes, J. D. Grif\ufb01th, and L. Carlone. Hierarchical represen-\ntations and explicit memory: Learning effective navigation policies on 3D scene graphs using\ngraph neural networks. In 2022 International Conference on Robotics and Automation (ICRA),\npages 9272\u20139279. IEEE, 2022.\n[40] A. Kurenkov, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, J. Ichnowski, K. Goldberg, and S. Savarese. Semantic and ge-\nometric modeling with neural message passing in 3D scene graphs for hierarchical mechanical\nsearch. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages\n11227\u201311233. IEEE, 2021.\n[41] S. Garg, N. S\u00a8underhauf, F. Dayoub, D. Morrison, A. Cosgun, G. Carneiro, Q. Wu, T.-J. Chin,\nI. Reid, S. Gould, et al. Semantics for robotic mapping, perception and interaction: A survey.\nFoundations and Trends\u00ae in Robotics, 8(1\u20132):1\u2013224, 2020.\n[42] A. A. Hagberg, D. A. Schult, and P. J. Swart. Exploring network structure, dynamics, and\nfunction using networkx. In G. Varoquaux, T. Vaught, and J. Millman, editors, Proceedings of\nthe 7th Python in Science Conference, pages 11 \u2013 15, Pasadena, CA USA, 2008.\n[43] M. Skreta, N. Yoshikawa, S. Arellano-Rubach, Z. Ji, L. B. Kristensen, K. Darvish, A. Aspuru-\nGuzik, F. Shkurti, and A. Garg.\nErrors are useful prompts: Instruction guided task pro-\ngramming with veri\ufb01er-assisted iterative prompting.\nArXiv, abs/2303.14100, 2023.\nURL\nhttps://api.semanticscholar.org/CorpusID:257757298.\n[44] J. Haviland, N. S\u00a8underhauf, and P. Corke. A holistic approach to reactive mobile manipulation.\nIEEE Robotics and Automation Letters, 7(2):3122\u20133129, 2022.\n[45] P. Corke and J. Haviland. Not your grandmother\u2019s toolbox\u2013the robotics toolbox reinvented\nfor python. In 2021 IEEE international conference on robotics and automation (ICRA), pages\n11357\u201311363. IEEE, 2021.\n[46] J. Zhang.\nGraph-toolformer: To empower LLMs with graph reasoning ability via prompt\naugmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023.\n11\n[47] S. Haddadin, S. Parusel, L. Johannsmeier, S. Golz, S. Gabl, F. Walch, M. Sabaghian, C. J\u00a8ahne,\nL. Hausperger, and S. Haddadin. The franka emika robot: A reference platform for robotics\nresearch and education. IEEE Robotics and Automation Magazine, 29(2):46\u201364, 2022. doi:\n10.1109/MRA.2021.3138382.\n[48] Omron. Omron LD / HD Series. URL https://www.ia.omron.com/products/\nfamily/3664/dimension.html.\n[49] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burch\ufb01el, and S. Song. Diffusion policy:\nVisuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and\nSystems (RSS), 2023.\n[50] K. Rana, A. Melnik, and N. S\u00a8underhauf. Contrastive language, action, and state pre-training\nfor robot learning, 2023.\n[51] Q-transformer: Scalable of\ufb02ine reinforcement learning via autoregressive q-functions. In 7th\nAnnual Conference on Robot Learning, 2023.\n[52] K. Rana, M. Xu, B. Tidd, M. Milford, and N. Suenderhauf. Residual skill policies: Learning\nan adaptable skill-based action space for reinforcement learning for robotics. In 6th Annual\nConference on Robot Learning, 2022. URL https://openreview.net/forum?id=\n0nb97NQypbK.\n12\nA\nImplementation Details\nWe utilise GPT-4 [3] as the underlying LLM agent unless otherwise stated. We follow a similar\nprompting structure to Wake et al. [5] as shown in Appendix J. We de\ufb01ne the agent\u2019s role, details\npertaining to the scene graph environment, the desired output structure and a set of input-output\nexamples which together form the static prompt used for in-context learning. This static prompt\nis both task- and environment-agnostic and takes up \u22483900 tokens of the LLM\u2019s input. During\nsemantic search, both the 3D Scene Graph and Memory components of the input prompt get\nupdated at each step, while during iterative replanning only the Feedback component gets updated\nwith information from the scene graph simulator. In all cases, the LLM is prompted to output a JSON\nobject containing arguments to call the provided API functions.\nB\nEnvironments\nOffice Space\nSingle-Floor, Multi-Room\nHome\nMulti-Floor, Multi-Room\nFigure 4: Large-scale environments used to evaluate SayPlan. The environments span multiple\nrooms and \ufb02oors including a vast range of\nWe evaluate SayPlan across a set of two large-scale environments spanning multiple rooms and\n\ufb02oors as shown in Figure 4. We provide details of each of these environments below, including a\nbreakdown of the number of entities and tokens required to represent them in the 3DSG:\nOf\ufb01ce: A large-scale of\ufb01ce \ufb02oor, spanning 37 rooms and 151 assets and objects which the agent can\ninteract with. A full and collapsed 3D scene graph representation of this environment are provided\nin Appendix D and E respectively. This scene graph represents a real-world of\ufb01ce \ufb02oor within which\na mobile manipulator robot is present. This allows us to embody the plans generated using SayPlan\nand evaluate their feasibility in the corresponding environment. Real-world video demonstrations of\na mobile manipulator robot executing the generated plan in this of\ufb01ce environment are provided on\nour project site2.\nHome: An existing 3D scene graph from the Stanford 3D Scene Graph dataset [11] which consists\nof a family home environment (Klickitat) spanning 28 rooms across 3 \ufb02oors and contains 112\nassets and objects that the agent can interact with. A 3D visual of this environment can be viewed\nat the 3D Scene Graph project website3.\nB.1\nReal World Environment Plan Execution\nTo enable real-world execution of the task plans generated over a 3DSG, we require a corresponding\n2D metric map within which we can align the posed nodes captured by the 3DSG. At each room\nnode we assume the real robot can visually locate the appropriate assets and objects that are visible to\n2sayplan.github.io\n33dscenegraph.stanford.edu/Klickitat\n13\nEntity Type\nNumber of Entities\nTotal Number of Tokens\nAverage Number of Tokens\nRoom Node\n37\n340\n9.19\nAsset Node\n73\n1994\n27.3\nObject Node\n78\n2539\n32.6\nAgent Node\n1\n15\n15.0\nNode Edges\n218\n1843\n8.45\nFull Graph\n407\n6731\n16.5\nCollapsed Graph\n105\n878\n8.36\nTable 4: Detailed 3DSG breakdown for the Of\ufb01ce Environment. The table summarises the num-\nber of different entities present in the 3DSG, the total LLM tokens required to represent each entity\ngroup and the average number of tokens required to represent a single type of entity.\nEntity Type\nNumber of Entities\nTotal Number of Tokens\nAverage Number of Tokens\nRoom Node\n28\n231\n8.25\nAsset Node\n52\n1887\n36.3\nObject Node\n60\n1881\n31.35\nAgent Node\n1\n15\n15\nNode Edges\n323\n2584\n8\nFull Graph\n464\n6598\n14.2\nCollapsed Graph\n240\n1817\n7.57\nTable 5: Detailed 3DSG breakdown for the Home Environment. The table summarises the num-\nber of different entities present in the 3DSG, the total LLM tokens required to represent each entity\ngroup and the average number of tokens required to represent a single type of entity.\nit within the 3DSG. The mobile manipulator robot used for the demonstration consisted of a Franka\nPanda 7-DoF robot manipulator [47] attached to an LD-60 Omron mobile base [48]. The robot is\nequipped with a LiDAR scanner to localise the robot both within the real world and the correspond-\ning 3DSG. All the skills or affordances including pick, place, open and close were developed using\nthe motion controller from [44] coupled with a RGB-D vision module for grasp detection, and a\nbehaviour tree to manage the execution of each component including failure recovery. Future work\ncould incorporate a range of pre-trained skills (whisking, \ufb02ipping, spreading etc.) using imitation\nlearning [49, 50] or reinforcement learning [51, 52] to increase the diversity of tasks that SayPlan is\nable to achieve.\nC\nTasks\nInstruction Family\nNum\nExplanation\nExample Instruction\nSemantic Search\nSimple Search\n30\nQueries focussed on evaluating the basic\nsemantic search capabilities of SayPlan\nFind me a ripe banana.\nComplex Search\n30\nAbstract semantic search queries which\nrequire complex reasoning\nFind the room where people are playing\nboard games.\nCausal Planning\nSimple Planning\n15\nQueries which require the agent to perform\nsearch, causal reasoning and environment\ninteraction in order to solve a task.\nRefrigerate the orange left on the kitchen\nbench.\nLong-Horizon Planning\n15\nLong Horizon planning queries requiring\nmultiple interactive steps\nTobi spilt soda on his desk. Help him\nclean up.\nTable 6: List of evaluation task instructions. We evaluate SayPlan on 90 instructions, grouped to\ntest various aspects of the planning capabilities across large-scale scene graphs. The full instruction\nset is given in Appendix C.\n14\nWe evaluate SayPlan across 4 instruction sets which are classi\ufb01ed to evaluate different aspects of its\n3D scene graph reasoning and planning capabilities as shown in Table 6:\nSimple Search: Focused on evaluating the semantic search capabilities of the LLM based on queries\nwhich directly reference information in the scene graph as well as the basic graph-based reasoning\ncapabilities of the LMM.\nComplex Search: Abstract semantic search queries which require complex reasoning. The infor-\nmation required to solve these search tasks is not readily available in the graph and has to be inferred\nby the underlying LLM.\nSimple Planning: Task planning queries which require the agent to perform graph search, causal\nreasoning and environment interaction in order to solve the task. Typically requires shorter horizon\nplans over single rooms.\nLong Horizon Planning: Long Horizon planning queries require multiple interactive steps. These\nqueries evaluate SayPlan\u2019s ability to reason over temporally extended instructions to investigate how\nwell it scales to such regimes. Typically requires long horizon plans spanning multiple rooms.\nThe full list of instructions used and the corresponding aspect the query evaluates are given in the\nfollowing tables:\nC.1\nSimple Search\nC.1.1\nOf\ufb01ce Environment\nInstruction\nFind me object K31X.\n\u25b7 unguided search with no semantic cue\nFind me a carrot.\n\u25b7 semantic search based on node name\nFind me anything purple in the postdoc\nbays.\n\u25b7 semantic search with termination conditioned on at-\ntribute\nFind me a ripe banana.\n\u25b7 semantic search with termination conditioned on at-\ntribute\nFind me something that has a screwdriver\nin it.\n\u25b7 unguided search with termination conditioned on\nchildren\nOne of the of\ufb01ces has a poster of the\nTerminator. Which one is it?\n\u25b7 semantic search with termination conditioned on\nchildren\nI printed a document but I don\u2019t know\nwhich printer has it. Find the document.\n\u25b7 semantic search based on parent\nI left my headphones in one of the meeting\nrooms. Locate them.\n\u25b7 semantic search based on parent\nFind the PhD bay that has a drone in it.\n\u25b7 semantic search with termination conditioned on\nchildren\nFind the kale that is not in the kitchen.\n\u25b7 semantic search with termination conditioned on a\nnegation predicate on parent\nFind me an of\ufb01ce that does not have a\ncabinet.\n\u25b7 semantic search with termination conditioned on a\nnegation predicate on children\nFind me an of\ufb01ce that contains a cabinet, a\ndesk, and a chair.\n\u25b7 semantic search with termination conditioned on a\nconjunctive query on children\nFind a book that was left next to a robotic\ngripper.\n\u25b7 semantic search with termination conditioned on a\nsibling\nLuis gave one of his neighbours a stapler.\nFind the stapler.\n\u25b7 semantic search with termination conditioned on a\nsibling\nThere is a meeting room with a chair but no\ntable. Locate it.\n\u25b7 semantic search with termination conditioned on a\nconjunctive query with negation\nTable 7: Simple Search Instructions. Evaluated in Of\ufb01ce Environment.\n15\nC.1.2\nHome Environment\nInstruction\nFind me a FooBar.\n\u25b7 unguided search with no semantic cue\nFind me a bottle of wine.\n\u25b7 semantic search based on node name\nFind me a plant with thorns.\n\u25b7 semantic search with termination conditioned on at-\ntribute\nFind me a plant that needs watering.\n\u25b7 semantic search with termination conditioned on at-\ntribute\nFind me a bathroom with no toilet.\n\u25b7 semantic search with termination conditioned on a\nnegation predicate\nThe baby dropped their rattle in one of the\nrooms. Locate it.\n\u25b7 semantic search based on node name\nI left my suitcase either in the bedroom or\nthe living room. Which room is it in.\n\u25b7 semantic search based on node name\nFind the room with a ball in it.\n\u25b7 semantic search based on node name\nI forgot my book on a bed. Locate it.\n\u25b7 semantic search based on node name\nFind an empty vase that was left next to\nsink.\n\u25b7 semantic search with termination conditioned on sib-\nling\nLocate the dining room which has a table,\nchair and a baby monitor.\n\u25b7 semantic search with termination conditioned on con-\njuctive query\nLocate a chair that is not in any dining\nroom.\n\u25b7 semantic search with termination conditioned on\nnegation predicate\nI need to shave. Which room has both a\nrazor and shaving cream.\n\u25b7 semantic search with termination conditioned on\nchildren\nFind me 2 bedrooms with pillows in them.\n\u25b7 semantic search with multiple returns\nFind me 2 bedrooms without pillows in\nthem.\n\u25b7 semantic search with multiple returns based on nega-\ntion predicate\nTable 8: Simple Search Instructions. Evaluated in Home Environment.\n16\nC.2\nComplex Search\nC.2.1\nOf\ufb01ce Environment\nInstruction\nFind object J64M. J64M should be kept at\nbelow 0 degrees Celsius.\n\u25b7 semantic search guided by implicit world knowledge\n(knowledge not directly encoded in graph)\nFind me something non vegetarian.\n\u25b7 semantic search with termination conditioned on im-\nplicit world knowledge\nLocate something sharp.\n\u25b7 unguided search with termination conditioned on im-\nplicit world knowledge\nFind the room where people are playing\nboard games.\n\u25b7 semantic search with termination conditioned on\nability to deduce context from node children using\nworld knowledge (\u201cboard game\u201d is not part of any node\nname or attribute in this graph)\nFind an of\ufb01ce of someone who is clearly a\nfan of Arnold Schwarzenegger.\n\u25b7 semantic search with termination conditioned on\nability to deduce context from node children using\nworld knowledge\nThere is a postdoc that has a pet Husky.\nFind the desk that\u2019s most likely theirs.\n\u25b7 semantic search with termination conditioned on\nability to deduce context from node children using\nworld knowledge\nOne of the PhD students was given more\nthan one complimentary T-shirts. Find his\ndesk.\n\u25b7 semantic search with termination conditioned on the\nnumber of children\nFind me the of\ufb01ce where a paper\nattachment device is inside an asset that is\nopen.\n\u25b7 semantic search with termination conditioned on\nnode descendants and their attributes\nThere is an of\ufb01ce which has a cabinet\ncontaining exactly 3 items in it. Locate the\nof\ufb01ce.\n\u25b7 semantic search with termination conditioned on the\nnumber of children\nThere is an of\ufb01ce which has a cabinet\ncontaining a rotten apple. The cabinet\nname contains an even number. Locate the\nof\ufb01ce.\n\u25b7 semantic search guided by numerical properties\nLook for a carrot. The carrot is likely to be\nin a meeting room but I\u2019m not sure.\n\u25b7 semantic search guided by user provided bias\nFind me a meeting room with a RealSense\ncamera.\n\u25b7 semantic search that has no result (no meeting room\nhas a realsense camera in the graph)\nFind the closest \ufb01re extinguisher to the\nmanipulation lab.\n\u25b7 search guided by node distance\nFind me the closest meeting room to the\nkitchen.\n\u25b7 search guided by node distance\nEither Filipe or Tobi has my headphones.\nLocate it.\n\u25b7 evaluating constrained search, early termination once\nthe two of\ufb01ce are explored\nTable 9: Complex Search Instructions. Evaluated in Of\ufb01ce Environment.\n17\nC.2.2\nHome Environment\nInstruction\nI need something to access ChatGPT.\nWhere should I go?\n\u25b7 semantic search guided by implicit world knowledge\nFind the livingroom that contains the most\nelectronic devices.\n\u25b7 semantic search with termination conditioned on\nchildren with indirect information\nFind me something to eat with a lot of\npotassium.\n\u25b7 semantic search with termination conditioned on im-\nplicit world knowledge\nI left a sock in a bedroom and one in the\nliving room. Locate them. They should\nmatch.\n\u25b7 semantic search with multiple returns\nFind me a potted plant that is most likely a\ncactus.\n\u25b7 semantic search with termination implicitly condi-\ntioned on attribute\nFind the dining room with exactly 5 chairs.\n\u25b7 semantic search with termination implicitly condi-\ntioned on quantity of children\nFind me the bedroom closest to the home\nof\ufb01ce.\n\u25b7 semantic search with termination implicitly condi-\ntioned on node distance\nFind me a bedroom with an unusual\namount of bowls.\n\u25b7 semantic search with termination implicitly condi-\ntioned on quantity of children\nWhich bedroom is empty.\n\u25b7 semantic search with termination implicitly condi-\ntioned on quantity of children\nWhich bathroom has the most potted\nplants.\n\u25b7 semantic search with termination implicitly condi-\ntioned on quantity of children\nThe kitchen is \ufb02ooded. Find somewhere I\ncan heat up my food.\n\u25b7 semantic search guided by negation\nFind me the room which most likely\nbelongs to a child\n\u25b7 semantic search with termination conditioned on\nability to deduce context from node children using\nworld knowledge\n15 guests are arriving. Locate enough\nchairs to seat them.\n\u25b7 semantic search with termination implicitly condi-\ntioned on the quantity of speci\ufb01ed node\nA vegetarian dinner was prepared in one of\nthe dining rooms. Locate it.\n\u25b7 semantic search with selection criteria based on\nworld knowledge\nMy tie is in one of the closets. Locate it.\n\u25b7 evaluating constrained search that has no result, ter-\nmination after exploring closets\nTable 10: Complex Search Instructions. Evaluated in Home Environment.\n18\nC.3\nSimple Planning\nInstruction\nClose Jason\u2019s cabinet.\nRefrigerate the orange left on the kitchen bench.\nTake care of the dirty plate in the lunchroom.\nPlace the printed document on Will\u2019s desk.\nPeter is working hard at his desk. Get him a healthy snack.\nHide one of Peter\u2019s valuable belongings.\nWipe the dusty admin shelf.\nThere is coffee dripping on the \ufb02oor. Stop it.\nPlace Will\u2019s drone on his desk.\nMove the monitor from Jason\u2019s of\ufb01ce to Filipe\u2019s.\nMy parcel just got delivered! Locate it and place it in the appropriate lab.\nCheck if the coffee machine is working.\nHeat up the chicken kebab.\nSomething is smelling in the kitchen. Dispose of it.\nThrow what the agent is holding in the bin.\nTable 11: Simple Planning Instructions. Evaluated in Of\ufb01ce Environment.\nC.4\nLong Horizon Planning\nInstruction\nHeat up the noodles in the fridge, and place it somewhere where I can enjoy it.\nThrow the rotting fruit in Dimity\u2019s of\ufb01ce in the correct bin.\nWash all the dishes on the lunch table. Once \ufb01nished, place all the clean cutlery in the drawer.\nSafely \ufb01le away the freshly printed document in Will\u2019s of\ufb01ce then place the undergraduate thesis\non his desk.\nMake Niko a coffee and place the mug on his desk.\nSomeone has thrown items in the wrong bins. Correct this.\nTobi spilt soda on his desk. Throw away the can and take him something to clean with.\nI want to make a sandwich. Place all the ingredients on the lunch table.\nA delegation of project partners is arriving soon. We want to serve them snacks and\nnon-alcoholic drinks. Prepare everything in the largest meeting room. Use items found in the\nsupplies room only.\nServe bottled water to the attendees who are seated in meeting room 1. Each attendee can only\nreceive a single bottle of water.\nEmpty the dishwasher. Place all items in their correct locations\nLocate all 6 complimentary t-shirts given to the PhD students and place them on the shelf in\nadmin.\nI\u2019m hungry. Bring me an apple from Peter and a pepsi from Tobi. I\u2019m at the lunch table.\nLet\u2019s play a prank on Niko. Dimity might have something.\nThere is an of\ufb01ce which has a cabinet containing a rotten apple. The cabinet name contains an\neven number. Locate the of\ufb01ce, throw away the fruit and get them a fresh apple.\nTable 12: Long-Horizon Planning Instructions. Evaluated in Of\ufb01ce Environment.\n19\nD\nFull 3D Scene Graph: Of\ufb01ce Environment\nObject\nRoom\nPose\nAgent\nAsset\nlunch_table\nrecycling_bin\ncoffee_machine\nrubbish_bin\ndishwasher\nfridge\nproduce_container\ntable1\ncabinet4\nK31X\nprinter2\ncabinet1\ncabinet6\ncabine5\ncabinet3\ndesk21\ndesk24\ndesk35\ndesk36\ndesk26\ndesk30\ndesk28\ndesk27\ndesk25\ndesk29\ndesk34\ndesk33\ndesk23\ndesk22\ndesk19\ndesk20\ndesk4\ndesk1\ndesk6\ndesk5\ntable4\ndesk2\ncabinet2\ndesk38\ndesk31\ndesk32\ndesk11\ndesk8\ndesk9\ndesk10\ndesk12\ndesk7\ndesk14\ndesk17\ndesk16\ndesk13\ndesk15\ndesk18\ndesk3\ntable5\nkitchen_bench\ndrawer\nmicrowave\ncabinet\ntable6\nchair4\nchair3\nchair5\ntable3\ntable2\nshelf\ntoolbox\ndesk37\nchair1\nchair2\n26\n25\n24\n21\n23\n22\n20\n19\n14\n13\n9\n12\n11\n10\n3\n2\n1\n5\n7\n16\n4\n6\n17\n18\n15\n8\nmeeting_room4\npostdoc_bay3\nlobby\nphd_bay4\nkitchen\npresentation_lounge\nprinting_zone1\nsupplies_station\nagriculture_lab\nmeeting_room3\ncafeteria\najays_office\nchris_office\nlauriannes_office\ndimitys_office\nfilipes_office\nluis_office\nwills_office\nphd_bay3\npostdoc_bay2\nmeeting_room2\nrobot_lounge1\nrobot_lounge2\nprinting_zone2\njasons_office\naarons_office\nnikos _office\nmichaels_office\nmobile _robotics_lab\nmeeting_room1\ntobis_office\npeters_office\npostdoc_bay1\nphd_bay1\nphd_bay2\nadmin\nmanipulation _lab\nagent\nfork\nknife\ncomplimentary_tshtirt5\nJ64M\nchicken_kebab\ncarrot\ngreek_salad\nnoodles\nbanana2\ntomatoe\nsalmon_bagel\napple2\nbuzzer\njanga\nrisk\nmonopoly\ndorittos1\ndocument\npepsi\nfire extinguisher1\nstapler\ncomplimentary_tshirt7\nscrewdiver1\nframe1\ncomplimentary_tshirt8\nframe2\ncomplimentary_tshirt2\ncomplimentary_tshirt1\ncomplimentary_tshirt3\ncomplimentary_tshirt6\ncomplimentary_tshirt4\nfire extinguisher2\napple1\nundergrad_thesis1\ndrone2\ndrone1\nframe3\nstapler2\ncheese\nbowl\nplate2\nspoon\nbanana1\nkale_leaves1\ncupboard1\ncupboard2\npaper_towel\nprinter_paper\nvodka\norange_juice\nbiscuits\nbottle_water1\nbottle_water2\nbottle_water3\nbottle_water4\nbottle_water5\ncoffee_mug\napple3\nphone\nstapler3\nscissors\nmonitor\nbook1\ngripper\nbook2\nshelf2\nbanana_peel\nplastic_bottle\nmilk_carton\norange_peel\napple_core\nkale_leaves2\norange1\nbread\nbutter\nchips\nparcel\nmarker\nterminator_poster\nheadphones\nplate\nFigure 5: 3D Scene Graph - Fully Expanded Of\ufb01ce Environment. Full 3D scene graph exposing\nall the rooms, assets and objects available in the scene. Note that the LLM agent never sees all this\ninformation unless it chooses to expand every possible node without contraction.\n20\nE\nContracted 3D Scene Graph: Of\ufb01ce Environment\nObject\nRoom\nPose\nAgent\nAsset\n26\n25\n24\n21\n23\n22\n20\n19\n14\n13\n9\n12\n11\n10\n3\n2\n1\n5\n7\n16\n4\n6\n17\n18\n15\n8\nmeeting_room4\npostdoc_bay3\nlobby\nphd_bay4\nkitchen\npresentation_lounge\nprinting_zone1\nsupplies_station\nagriculture_lab\nmeeting_room3\ncafeteria\najays_office\nchris_office\nlauriannes_office\ndimitys_office\nfilipes_office\nluis_office\nwills_office\nphd_bay3\npostdoc_bay2\nmeeting_room2\nrobot_lounge1\nrobot_lounge2\nprinting_zone2\njasons_office\naarons_office\nnikos _office\nmichaels_office\nmobile _robotics_lab\nmeeting_room1\ntobis_office\npeters_office\npostdoc_bay1\nphd_bay1\nphd_bay2\nadmin\nmanipulation _lab\nFigure 6: 3D Scene Graph - Contracted Of\ufb01ce Environment. Contracted 3D scene graph expos-\ning only the highest level within the hierarchy - room nodes. This results in an 82.1% reduction in\nthe number of tokens required to represent the scene before the semantic search phase.\n21\nF\nSemantic Search Evaluation Results\n- Full listings of the generated semantic search sequences for the evaluation instruction sets are\nprovided on the following pages -\n22\nSayPlan\nHuman\nSuccess \nFail\npeters_office\nmobile_robotics_lab\nrobot_lounge1\nagriculture_lab\nmanipulation_lab\nrobot_lounge2\ntobis_office\nnikos_office\nmichaels_office\nsupplies_station\nmobile_robotics_lab\nagriculture_lab\nprinting_zone1\nmanipulation_lab\nprinting_zone2\nadmin\nmichaels_office\nkitchen\nkitchen\npostdoc_bay1\npostdoc_bay2\npostdoc_bay1\npostdoc_bay2\nkitchen\ncafeteria\nkitchen\ncafeteria\nmobile_robotics_lab\nrobot_lounge1\nagriculture_lab\nmanipulation_lab\nrobot_lounge2\nsupplies_station\nrobot_lounge1\nprinting_zone2\nprinting_zone1\nrobot_lounge2\npeters_office\nmichaels_office\nnikos_office\ntobis_office\naarons_office\nluis_office\ndimitys_office\nfilipes_office\nwills_office\nchris_office\nmichaels_office\nprinting_zone1\nprinting_zone2\nprinting_zone2\nprinting_zone2\nmeeting_room1\nmeeting_room3\nmeeting_room2\nmeeting_room1\nmeeting_room3\nmeeting_room4\nmeeting_room2\nphd_bay1\nphd_bay3\nphd_bay2\nphd_bay1\nphd_bay3\nphd_bay2\nmobile_robotics_lab\nagriculture_lab\ncafeteria\nagriculture_lab\npeters_office\nnikos_office\ntobis_office\nchris_office\nwills_office\najays_office\nfilipes_office\nluis_office\nlauriannes_office\ndimitys_office\npeters_office\ntobis_office\nFind me object K31X.\nFind me something that has a screwdriver in it.\nOne of the offices has a poster of the Terminator. Which one is it?\nFind me a carrot.\nFind me anything purple in the postdoc bays.\nFind me a ripe banana.\nI printed a document, but I dont know which printer has it. Find the document.\nFind me an office that does not have a cabinet.\nI left my headphones in one of the meeting rooms. Locate them.\nFind the PhD bay that has a drone in it.\nFind the kale that is not in the kitchen.\n23\nSayPlan\nHuman\nSuccess \nFail\npeters_office\nnikos_office\ntobis_office\nchris_office\nwills_office\najay_office\nfilipes_office\nluis_office\nnikos_office\nlauriannes_office\ndimity_office\npeters_office\ntobis_office\nmobile_robotics_lab\nmanipulation_lab\nmanipulation_lab\nluis_office\nfilipes_office\nwills_office\nluis_office\nfilipes_office\nwills_office\nmeeting_room1\nmeeting_room3\nmeeting_room2\nmeeting_room1\nmeeting_room2\nFind me an office that contains a cabinet, a desk and a chair.\nFind me a book that was left next to a robotic gripper.\nLuis gave one of his neighbours a stapler. Find the stapler.\nThere is a meeting room with a chair but no table. Locate it.\nTable 13: Simple Search Of\ufb01ce Environment Evaluation. Sequence of Explored Nodes for Sim-\nple Search Of\ufb01ce Environment Instructions.\n24\nwills_office\njasons_office\npeters_office\nmichaels_office\nnikos_office\ntobis_office\nlaurriannes_office\naarons_office\najays_office\nchris_office\ndimitys_office\nkitchen\nkitchen\nkitchen\nkitchen\ntobis_office\nkitchen\nagriculture_lab\nmanipultion_lab\nmobile_robotics_lab\npeters_office\nmanipulation_lab\nnikos_office\nmichaels_office\nmichaels_office\nprinting_zone2\nkitchen\nprinting_zone1\nagriculture_lab\ncafeteria\nnikos_office\nsupplies_station\nadmin\npeters_office\ntobis_office\nmeeting_room4\npresentation_lounge\nmeeting_room2\nmeeting_room1\ncafeteria\nmeeting_room3\nmeeting_room4\ncafeteria\nmeeting_room2\nmeeting_room1\npresentation_lounge\nmeeting_room3\npeters_office\nmichaels_office\nnikos_office\ntobis_office\nchris_office\nmichaels_office\najays_office\nwills_office\npostdoc_bay1\npostdoc_bay2\npostdoc_bay1\npostdoc_bay2\nphd_bay1\nphd_bay1\nphd_bay2\npeters_office\nmichaels_office\nnikos_office\ntobis_office\nwills_office\nmichaels_office\nnikos_office\nwills_office\njasons_office\npeters_office\nmichaels_office\nnikos_office\ntobis_office\nlaurriannes_office\naarons_office\najays_office\nchris_office\ndimitys_office\ndimitys_office\najay_office\nchris_office\nlauriannes_office\nwills_office\njasons_office\nmichaels_office\nchris_office\ndimitys_office\nnikos_office\najays_office\nwills_office\nFind object J64M. J64M should be kept at below 0 degrees Celsius.\nFind the office of someone who is clearly a fan of Arnold Schwarzenegger.\nThere is postdoc that has a pet Husky. Find the desk that\u2019s most likely theirs.\nFind me something non vegetarian.\nLocate something sharp.\nFind the room where people are playing board games..\nOne of the PhD students was given more than one complimentary T-shirt. Find his desk.\nFind me the office where a paper attachment device is inside an asset that is open.\nThere is an office which has a cabinet containing exactly 3 items in it. Locate the office.\nThere is an office containing a rotten apple. The cabinet name contains an even number. Locate the office.\nSayPlan\nHuman\nSuccess \nFail\n25\nmeeting_room1\nmeeting_room4\nmeeting_room3\nmeeting_room2\nkitchen\nmeeting_room1\nmeeting_room4\nmeeting_room3\nmeeting_room2\nkitchen\nmeeting_room1\nmeeting_room4\nmeeting_room3\nmeeting_room2\npresentation_lounge\nmeeting_room1\nmeeting_room4\nmeeting_room3\nmeeting_room2\nmanipulation_lab\npose15\nadmin\nkitchen\nmeeting_room3\nfilipes_office\nfilipes_office\ntobis_office\nfilipes_office\ntobis_office\nLook for a carrot. The carrot is likley to be in a meeting room but I\u2019m not sure.\nFind me a meeting room with a RealSense camera.\nFind the closest fire extinguisher to the manipulation lab.\nFind me the closest meeting room to the kitchen.\nEither Filipe or Tobi has my headphones. Locate them.\nSayPlan\nHuman\nSuccess \nFail\nTable 14: Complex Search Of\ufb01ce Environment Evaluation. Sequence of Explored Nodes for\nComplex Search Of\ufb01ce Environment Instructions.\n26\ndining_room0\nbedroom1\nbathroom0\nbathroom3\nbathroom2\nbathroom1\ncloset1\nbathroom4\nbedroom2\nbedroom3\ncloset0\nliving_room0\nkitchen0\ndining_room1\ndining_room0\nkitchen1\ndining_room2\nliving_room1\nliving_room2\nkitchen0\ndining_room1\ndining_room0\nkitchen1\nkitchen1\nkitchen0\nliving_room0\ndining_room0\ndining_room2\nliving_room1\ndining_room1\nbathroom1\nliving_room0\ndining_room0\nkitchen0\nliving_room1\nbathroom0\nbedroom1\nliving_room0\ndining_room2\ndining_room0\nliving_room1\nbathroom1\nbedroom0\ndining_room1\nliving_room2\nbathroom0\nliving_room0\nliving_room2\nkitchen0\nliving_room1\nbedroom1\nliving_room0\ndining_room2\ndining_room0\nliving_room1\nbedroom0\ndining_room1\nliving_room2\nbathroom0\nbathroom2\nbathroom1\nbathroom4\nbathroom1\nbathroom3\nbathroom2\nbedroom3\nplayroom0\nbedroom1\nbedroom0\nliving_room0\nbedroom2\nbedroom3\ndining_room1\nplayroom0\nliving_room2\nliving_room1\nliving_room0\nbedroom2\ndining_room0\ndining_room2\nbedroom0\nbedroom1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nliving_room0\nliving_room1\nbedroom0\nbedroom3\nliving_room2\nbedroom1\nbedroom2\nliving_room0\nplayroom0\nplayroom0\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nbedroom0\nbedroom3\nbedroom1\nFind me a FooBar.\nFind me a bathroom with no toilet.\nThe baby dropped their rattle in one of the rooms. Locate it.\nFind me a bottle of wine.\nFind me a plant with thorns.\nFind me a plant that needs watering.\nI left my suitcase either in the bedroom or the living room. Which room is it in.\nFind the room with a ball in it.\nI forgot my book on a bed. Locate it.\nkitchen1\ndining_room2\ndining_room1\nhome_office0\nkitchen0\nliving_room0\nliving_room1\nliving_room2\nSayPlan\nHuman\nSuccess \nFail\n27\nbathroom0\nbathroom3\nbathroom2\nbathroom1\nbathroom4\nbathroom3\nkitchen0\nbathroom1\nbathroom0\nkitchen1\nbathroom2\nbathroom4\ndining_room0\ndining_room1\ndining_room0\ndining_room1\nliving_room0\nliving_room1\nhome_office0\nbathroom0\nbathroom3\nbathroom2\nbathroom1\nbathroom0\nbathroom3\nbathroom2\nbathroom1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nbedroom0\nbedroom1\nLocate the dining room which has a table, chair and a baby monitor.\nLocate a chair that is not in any dining room.\nFind an empty vase that was left next to a sink.\nI need to shave. Which room has both a razor and shaving cream.\nFind me 2 bedrooms with pillows in them.\nFind me 2 bedrooms without pillows in them.\nSayPlan\nHuman\nSuccess \nFail\nTable 15: Simple Search Home Environment Evaluation. Sequence of Explored Nodes for Sim-\nple Search Home Environment Instructions.\n28\nhome_office0\nhome_office0\nliving_room0\nliving_room2\nliving_room1\nliving_room0\nliving_room2\nliving_room1\nkitchen0\nkitchen1\nkitchen0\nkitchen1\nbedroom0\nliving_room0\nbedroom2\nbedroom1\nbedroom2\nliving_room1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nliving_room0\nliving_room0\nkitchen0\nhome_office0\nliving_room1\nliving_room2\nliving_room0\nliving_room2\nliving_room1\ndining_room0\ndining_room1\ndining_room0\ndining_room1\nhome_office0\npose1206\nbedroom2\nbedroom0\nbedroom2\nbedroom1\nbedroom0\nbedroom2\nbedroom1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nbedroom3\nbedroom2\nbathroom0\nbathroom3\nbathroom2\nbathroom1\nbathroom0\nbathroom3\nbathroom2\nbathroom1\nkitchen0\ndining_room0\nkitchen1\ndining_room0\nI need something to access ChatGPT. Where should I go?.\nFind the potted plant that is most likely a cactus.\nFind the dining room with exactly 5 chairs.\nFind the livingroom that contains the most electronic devices.\nFind me something to eat with alot of potassium.\nI left a sock in a bedrooom and in one of the livingrooms. Locate them. They should match.\nFind me the bedroom closest to the home office.\nThe kitchen is flooded. Find somewhere I can heat up my food.\nFind me the bedroom with an unusual amount of bowls.\nWhich bedroom is empty.\nWhich bathroom has the most potted plants.\ndining_room2\ndining_room2\ncloset0\nSayPlan\nHuman\nSuccess \nFail\n29\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nbedroom0\nbedroom3\nbedroom2\nbedroom1\nliving_room1\ndining_room0\nhome_office0\nliving_room0\ndining_room1\nbedroom0\nliving_room2\ndining_room0\nliving_room0\ndining_room2\ndining_room1\nliving_room1\ndining_room0\ndining_room2\ndining_room1\ndining_room0\ndining_room2\ndining_room1\ncloset0\ncloset1\ncloset0\ncloset1\nFind me the room which most likley belongs to a child.\n15 guests are arriving. Locate enough chairs to seat them.\nA vegetarian dinner was prepared in one of the dining rooms. Locate it.\nMy tie is in one of the closets. Locate it.\nSayPlan\nHuman\nSuccess \nFail\nTable 16: Complex Search Home Environment Evaluation. Sequence of Explored Nodes for\nComplex Search Home Environment Instructions.\n30\nG\nCausal Planning Evaluation Results\nIn this section, we provide a detailed breakdown of the causal planning performance of SayPlan\nacross the two sets of evaluation instructions. Tables 17 and 18 detail the correctness, executability\nand the number of iterative replanning steps it took to obtain an executable plan.\nInstruction\nCorr.\nExec.\nNo. of Replanning\nIterations\nClose Jason\u2019s cabinet.\n\u0013\n\u0013\n0\nRefrigerate the orange left on the kitchen bench.\n\u0013\n\u0013\n0\nTake care of the dirty plate in the lunchroom.\n\u0013\n\u0013\n0\nPlace the printed document on Will\u2019s desk.\n\u0013\n\u0013\n0\nPeter is working hard at his desk.\nGet him a healthy\nsnack.\n\u0017\n\u0013\n5\nHide one of Peter\u2019s valuable belongings.\n\u0013\n\u0013\n0\nWipe the dusty admin shelf.\n\u0013\n\u0013\n0\nThere is coffee dripping on the \ufb02oor. Stop it.\n\u0013\n\u0013\n0\nPlace Will\u2019s drone on his desk.\n\u0013\n\u0013\n0\nMove the monitor from Jason\u2019s of\ufb01ce to Filipe\u2019s.\n\u0013\n\u0013\n0\nMy parcel just got delivered! Locate it and place it in the\nappropriate lab.\n\u0013\n\u0013\n0\nCheck if the coffee machine is working.\n\u0013\n\u0013\n0\nHeat up the chicken kebab.\n\u0013\n\u0013\n1\nSomething is smelling in the kitchen. Dispose of it.\n\u0013\n\u0013\n0\nThrow what the agent is holding in the bin.\n\u0013\n\u0013\n1\nTable 17: Correctness, Executability and Number of Replanning Iterations for Simple Plan-\nning Instructions. Evaluating the performance of SayPlan on each simple planning instruction.\nValues indicated in red indicate that no executable plan was identi\ufb01ed up to that number of iterative\nreplanning steps. In this case, 5 was the maximum number of replanning steps.\n31\nInstruction\nCorr.\nExec.\nNo. of Replanning\nIterations\nHeat up the noodles in the fridge, and place it somewhere\nwhere I can enjoy it.\n\u0013\n\u0013\n2\nThrow the rotting fruit in Dimity\u2019s of\ufb01ce in the correct\nbin.\n\u0013\n\u0013\n1\nWash all the dishes on the lunch table. Once \ufb01nished,\nplace all the clean cutlery in the drawer.\n\u0017\n\u0013\n2\nSafely \ufb01le away the freshly printed document in Will\u2019s\nof\ufb01ce then place the undergraduate thesis on his desk.\n\u0013\n\u0013\n2\nMake Niko a coffee and place the mug on his desk.\n\u0013\n\u0013\n0\nSomeone has thrown items in the wrong bins. Correct\nthis.\n\u0017\n\u0013\n0\nTobi spilt soda on his desk. Throw away the can and take\nhim something to clean with.\n\u0013\n\u0013\n3\nI want to make a sandwich. Place all the ingredients on\nthe lunch table.\n\u0013\n\u0013\n3\nA delegation of project partners is arriving soon. We want\nto serve them snacks and non-alcoholic drinks. Prepare\neverything in the largest meeting room. Use items found\nin the supplies room only.\n\u0013\n\u0013\n2\nServe bottled water to the attendees who are seated in\nmeeting room 1. Each attendee can only receive a single\nbottle of water.\n\u0013\n\u0013\n2\nEmpty the dishwasher. Place all items in their correct\nlocations.\n\u0013\n\u0013\n2\nLocate all 6 complimentary t-shirts given to the PhD stu-\ndents and place them on the shelf in admin.\n\u0013\n\u0013\n1\nI\u2019m hungry. Bring me an apple from Peter and a Pepsi\nfrom Tobi. I\u2019m at the lunch table.\n\u0017\n\u0017\n5\nLet\u2019s play a prank on Niko. Dimity might have some-\nthing.\n\u0013\n\u0013\n1\nThere is an of\ufb01ce which has a cabinet containing a rotten\napple. The cabinet name contains an even number. Lo-\ncate the of\ufb01ce, throw away the fruit and get them a fresh\napple.\n\u0017\n\u0017\n5\nTable 18: Correctness, Executability and Number of Replanning Iterations for Long-Horizon\nPlanning Instructions. Evaluating the performance of SayPlan on each long-horizon planning\ninstruction. Values indicated in red indicate that no executable plan was identi\ufb01ed up to that number\nof iterative replanning steps. In this case, 5 was the maximum number of replanning steps.\nThe full plan sequences generated by SayPlan and all the baseline methods for each of the above\ninstructions are detailed in Table 19. Note the regions highlighted in red indicating the precise action\nwhere a plan failed.\n- Full listings of the generated plan sequences for the simple and long-horizon instruction sets are\nprovided on the following pages -\n32\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess \nFail\nClose Jason's cabinet.\ngoto(pose13)\ngoto(jasons_office)\naccess(cabinet5)\nclose(cabinet5)\ngoto(pose13)\ngoto(jasons_office)\naccess(cabinet5)\nclose(cabinet5)\ngoto(pose13)\ngoto(jasons_office)\naccess(cabinet5)\nclose(cabinet5)\nRefrigerate the orange left on the kitchen bench.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(kitchen_bench)\npickup(orange1)\naccess(fridge)\nopen(fridge)\nrelease(orange1)\nclose(fridge)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(kitchen_bench)\npickup(orange1)\naccess(fridge)\nopen(fridge)\nrelease(orange1)\nclose(fridge)\ngoto(pose13)\ngoto(pose14)\ngoto(pose25)\ngoto(kitchen)\npickup(orange1)\naccess(fridge)\nopen(fridge)\nrelease(orange1)\nclose(fridge)\nTake care of the dirty plate in the lunchroom.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(plate)\ngoto(pose25)\ngoto(kitchen)\naccess(dishwasher)\nopen(dishwasher)\nrelease(plate)\nclose(dishwasher)\nturn_on(dishwasher)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(plate)\ngoto(pose25)\ngoto(kitchen)\naccess(dishwasher)\nopen(dishwasher)\nrelease(plate)\nclose(dishwasher)\nturn_on(dishwasher)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(plate)\ngoto(cafeteria)\ngoto(pose25)\ngoto(kitchen)\naccess(dishwasher)\nopen(dishwasher)\nrelease(plate)\nclose(dishwasher)\nturn_on(dishwasher)\nPlace the printed document on Will's desk.\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(printing_zone2)\naccess(printer2)\npickup(document)\ngoto(pose3)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(desk4)\nrelease(document)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(printing_zone2)\naccess(printer2)\npickup(document)\ngoto(pose3)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(desk4)\nrelease(document)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(printing_zone1)\npickup(printed_document)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose10)\ngoto(wills_office)\nrelease(printed_document)\n33\nHide one of Peter's valuable belongings.\ngoto(pose7)\ngoto(pose5)\ngoto(pose2)\ngoto(pose1)\ngoto(peters_office)\naccess(desk2)\npickup(phone)\naccess(cabinet2)\nopen(cabinet2)\nrelease(phone)\nclose(cabinet2)\ngoto(pose7)\ngoto(pose5)\ngoto(pose2)\ngoto(pose1)\ngoto(peters_office)\naccess(desk2)\npickup(phone)\naccess(cabinet2)\nopen(cabinet2)\nrelease(phone)\nclose(cabinet2)\ngoto(pose1)\ngoto(peters_office)\naccess(cabinet2)\nopen(cabinet2)\naccess(desk2)\npickup(phone)\naccess(cabinet2)\nrelease(phone)\nclose(cabinet2)\nPeter is working hard at his desk. Get him a healthy snack.\ngoto(pose7)\ngoto(pose5)\ngoto(pose2)\ngoto(pose1)\ngoto(peters_office)\naccess(cabinet2)\nopen(cabinet2)\npickup(apple3)\nrelease(apple3)\naccess(cabinet2)\nopen(cabinet2)\npickup(apple3)\ngoto(pose7)\ngoto(pose5)\ngoto(pose2)\ngoto(pose1)\ngoto(peters_office)\nrelease(apple3)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\naccess(kitchen_bench)\npickup(kale_leaves2)\ngoto(kitchen)\ngoto(pose25)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(peters_office)\naccess(desk2)\nrelease(kale_leaves2)\nWipe the dusty admin shelf.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard1)\nopen(cupboard1)\npickup(paper_towel)\ngoto(pose24)\ngoto(pose23)\ngoto(pose15)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(paper_towel)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard1)\nopen(cupboard1)\npickup(paper_towel)\ngoto(pose24)\ngoto(pose23)\ngoto(pose15)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(paper_towel)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\ngoto(cupboard1)\nopen(cupboard1)\npickup(paper_towel)\ngoto(supplies_station)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(admin)\naction(wipe_shelf)\nThere is coffee dripping on the floor. Stop it.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(coffee_machine)\nturn_off(coffee_machine)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(coffee_machine)\nturn_off(coffee_machine)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\ngoto(coffee_machine)\nturn_off(coffee_machine)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n34\nCheck if the coffee machine is working.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(coffee_machine)\nturn_on(coffee_machine)\nturn_off(coffee_machine)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(coffee_machine)\nturn_on(coffee_machine)\nturn_off(coffee_machine)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\ngoto(coffee_machine)\nturn_on(coffee_machine)\nturn_off(coffee_machine)\nPlace Will's drone on his desk.\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(cabinet4)\nopen(cabinet4)\npickup(drone2)\naccess(desk4)\nrelease(drone2)\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(cabinet4)\nopen(cabinet4)\npickup(drone2)\naccess(desk4)\nrelease(drone2)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose10)\ngoto(wills_office)\naccess(cabinet4)\nopen(cabinet4)\npickup(drone2)\nclose(cabinet4)\ngoto(wills_office)\naccess(desk4)\nrelease(drone2)\nMove the monitor from Jason's office to Filipe's.\ngoto(pose13)\ngoto(jasons_office)\naccess(desk5)\npickup(monitor)\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(filipes_office)\naccess(desk37)\nrelease(monitor)\ngoto(pose13)\ngoto(jasons_office)\naccess(desk5)\npickup(monitor)\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(filipes_office)\naccess(desk37)\nrelease(monitor)\ngoto(pose8)\ngoto(pose13)\ngoto(jasons_office)\npickup(monitor)\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(filipes_office)\nrelease(monitor)\nMy parcel just got delivered! Locate it and place it in the appropriate lab.\ngoto(pose13)\ngoto(pose14)\ngoto(lobby)\naccess(shelf2)\npickup(parcel)\ngoto(pose14)\ngoto(pose13)\ngoto(pose8)\ngoto(mobile_robotics_lab)\naccess(table4)\nrelease(parcel)\ngoto(pose13)\ngoto(pose14)\ngoto(lobby)\naccess(shelf2)\npickup(parcel)\ngoto(pose14)\ngoto(pose13)\ngoto(pose8)\ngoto(mobile_robotics_lab)\naccess(table4)\nrelease(parcel)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(lobby)\naccess(shelf2)\npickup(parcel)\ngoto(lobby)\ngoto(pose14)\ngoto(pose13)\ngoto(pose8)\ngoto(mobile_robotics_lab)\naccess(table4)\nrelease(parcel)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n35\nHeat up the chicken kebab.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(chicken_kebab)\nclose(fridge)\naccess(microwave)\nopen(microwave)\nrelease(chicken_kebab)\nclose(microwave)\nturn_on(microwave)\nturn_off(microwave)\nopen(microwave)\npickup(chicken_kebab)\nclose(microwave)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(chicken_kebab)\nclose(fridge)\naccess(microwave)\nopen(microwave)\nrelease(chicken_kebab)\nturn_on(microwave)\nturn_off(microwave)\nopen(microwave)\npickup(chicken_kebab)\nclose(microwave)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(chicken_kebab)\nclose(fridge)\naccess(microwave)\nopen(microwave)\nrelease(chicken_kebab)\nclose(microwave)\nturn_on(microwave)\nturn_off(microwave)\nopen(microwave)\npickup(chicken_kebab)\nclose(microwave)\nSomething is smelling in the kitchen. Dispose of it.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(salmon_bagel)\naccess(compost_bin)\nopen(compost_bin)\nrelease(salmon_bagel)\nclose(compost_bin)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(salmon_bagel)\naccess(compost_bin)\nopen(compost_bin)\nrelease(salmon_bagel)\nclose(compost_bin)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(salmon_bagel)\nclose(fridge)\ngoto(rubbish_bin)\nopen(rubbish_bin)\nrelease(salmon_bagel)\nclose(rubbish_bin)\nThrow what the agent is holding in the bin.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nopen(recycling_bin)\nrelease(sweet_wrapper)\nclose(recycling_bin)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nrelease(sweet_wrapper)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\ngoto(rubbish_bin)\nrelease(sweet_wrapper)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n36\nHeat up the noodles in the fridge, and place it somewhere where I can enjoy it.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(noodles)\naccess(microwave)\nopen(microwave)\nrelease(noodles)\nclose(microwave)\nturn_on(microwave)\nturn_off(microwave)\nopen(microwave)\npickup(noodles)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\nrelease(noodles)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(noodles)\naccess(microwave)\nopen(microwave)\nrelease(noodles)\nturn_on(microwave)\nturn_off(microwave)\nopen(microwave)\npickup(noodles)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\nrelease(noodles)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(noodles)\nclose(fridge)\naccess(microwave)\nopen(microwave)\nrelease(noodles)\nclose(microwave)\nturn_on(microwave)\nturn_off(microwave)\nopen(microwave)\npickup(noodles)\nclose(microwave)\ngoto(cafeteria)\nrelease(noodles)\nThrow the rotting fruit in Dimity's office in the correct bin.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose22)\ngoto(dimitys_office)\naccess(cabinet3)\nopen(cabinet3)\npickup(apple2)\ngoto(pose22)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(compost_bin)\nopen(compost_bin)\nrelease(apple2)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose22)\ngoto(dimitys_office)\naccess(cabinet3)\nopen(cabinet3)\npickup(apple2)\ngoto(pose22)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(compost_bin)\nrelease(apple2)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose22)\ngoto(dimitys_office)\naccess(cabinet3)\npickup(apple2)\ngoto(dimitys_office)\ngoto(pose22)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nrelease(apple2)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n37\nWash all the dishes on the lunch table. Once finished, place all the clean cutlery in the drawer.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(plate)\ngoto(pose25)\ngoto(kitchen)\naccess(drawer)\nopen(drawer)\nrelease(plate)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(fork)\ngoto(pose25)\ngoto(kitchen)\naccess(drawer)\nrelease(fork)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(knife)\ngoto(pose25)\ngoto(kitchen)\naccess(drawer)\nrelease(knife)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\npickup(plate)\npickup(fork)\npickup(knife)\ngoto(pose25)\ngoto(kitchen)\naccess(drawer)\nrelease(plate)\nrelease(fork)\nrelease(knife)\ngoto(pose25)\ngoto(cafeteria)\ngoto(lunch_table)\npickup(plate)\npickup(fork)\npickup(knife)\ngoto(pose25)\ngoto(kitchen)\ngoto(dishwasher)\nopen(dishwasher)\nrelease(plate)\nrelease(fork)\nrelease(knife)\nclose(dishwasher)\nturn_on(dishwasher)\nturn_off(dishwasher)\nopen(dishwasher)\npickup(plate)\npickup(fork)\npickup(knife)\ngoto(drawer)\nopen(drawer)\nrelease(fork)\nrelease(knife)\nclose(drawer)\nSafely file away the freshly printed document in Will's office then place the undergraduate thesis on his desk.\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(printing_zone2)\naccess(printer2)\npickup(document)\ngoto(pose3)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(cabinet4)\nopen(cabinet4)\nrelease(document)\nclose(cabinet4)\naccess(cabinet4)\nopen(cabinet4)\npickup(undergrad_thesis1)\nclose(cabinet4)\naccess(desk4)\nrelease(undergrad_thesis1)\npickup(document)\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(cabinet4)\nopen(cabinet4)\nrelease(document)\nclose(cabinet4)\naccess(desk4)\npickup(undergrad_thesis1)\nrelease(undergrad_thesis1)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(printing_zone1)\npickup(freshly_printed_document)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose10)\ngoto(wills_office)\naccess(cabinet4)\nopen(cabinet4)\nrelease(freshly_printed_document)\npickup(undergrad_thesis1)\nclose(cabinet4)\naccess(desk4)\nrelease(undergrad_thesis1)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n38\nMake Niko a coffee and place the mug on his desk.\ngoto(pose7)\ngoto(pose5)\ngoto(nikos_office)\naccess(desk1)\npickup(coffee_mug)\ngoto(pose5)\ngoto(pose7)\ngoto(pose16)\ngoto(pose18)\ngoto(pose15)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(coffee_machine)\nrelease(coffee_mug)\nturn_on(coffee_machine)\nturn_off(coffee_machine)\npickup(coffee_mug)\ngoto(pose25)\ngoto(pose24)\ngoto(pose23)\ngoto(pose15)\ngoto(pose18)\ngoto(pose16)\ngoto(pose7)\ngoto(pose5)\ngoto(nikos_office)\naccess(desk1)\nrelease(coffee_mug)\ngoto(pose7)\ngoto(pose5)\ngoto(nikos_office)\naccess(desk1)\npickup(coffee_mug)\ngoto(pose5)\ngoto(pose7)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(coffee_machine)\nrelease(coffee_mug)\nturn_on(coffee_machine)\nturn_off(coffee_machine)\npickup(coffee_mug)\ngoto(pose25)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose8)\ngoto(pose7)\ngoto(pose5)\ngoto(nikos_office)\naccess(desk1)\nrelease(coffee_mug)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\npickup(coffee_mug)\ngoto(pose25)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose5)\ngoto(nikos_office)\ngoto(desk1)\nrelease(coffee_mug)\nSomeone has thrown items in the wrong bins. Correct this.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nopen(recycling_bin)\npickup(milk_carton)\naccess(rubbish_bin)\nopen(rubbish_bin)\nrelease(milk_carton)\naccess(recycling_bin)\npickup(orange_peel)\nrelease(orange_peel)\naccess(recycling_bin)\npickup(apple_core)\nrelease(apple_core)\naccess(rubbish_bin)\npickup(banana_peel)\naccess(recycling_bin)\nrelease(banana_peel)\naccess(rubbish_bin)\npickup(plastic_bottle)\nrelease(plastic_bottle)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nopen(recycling_bin)\npickup(milk_carton)\naccess(rubbish_bin)\nopen(rubbish_bin)\nrelease(milk_carton)\naccess(recycling_bin)\npickup(orange_peel)\nrelease(orange_peel)\naccess(recycling_bin)\npickup(apple_core)\nrelease(apple_core)\naccess(rubbish_bin)\npickup(banana_peel)\naccess(recycling_bin)\nrelease(banana_peel)\naccess(rubbish_bin)\npickup(plastic_bottle)\nrelease(plastic_bottle)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\ngoto(recycling_bin)\npickup(milk_carton)\ngoto(rubbish_bin)\nrelease(milk_carton)\ngoto(recycling_bin)\npickup(orange_peel)\ngoto(rubbish_bin)\nrelease(orange_peel)\ngoto(recycling_bin)\npickup(apple_core)\ngoto(rubbish_bin)\nrelease(apple_core)\ngoto(rubbish_bin)\npickup(banana_peel)\ngoto(recycling_bin)\nrelease(banana_peel)\ngoto(rubbish_bin)\npickup(plastic_bottle)\ngoto(recycling_bin)\nrelease(plastic_bottle)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n39\nTobi spilt soda on his desk. Throw away the can and take him something to clean with.\ngoto(pose7)\ngoto(pose5)\ngoto(pose2)\ngoto(pose1)\ngoto(tobis_office)\naccess(desk38)\npickup(pepsi)\ngoto(pose1)\ngoto(pose2)\ngoto(pose3)\ngoto(pose9)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nopen(recycling_bin)\nrelease(pepsi)\ngoto(pose25)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard1)\nopen(cupboard1)\npickup(paper_towel)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(pose1)\ngoto(tobis_office)\naccess(desk38)\nrelease(paper_towel)\ngoto(pose7)\ngoto(pose5)\ngoto(pose2)\ngoto(pose1)\ngoto(tobis_office)\naccess(desk38)\npickup(pepsi)\ngoto(pose1)\ngoto(pose2)\ngoto(pose3)\ngoto(pose9)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(recycling_bin)\nrelease(pepsi)\ngoto(pose25)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard1)\npickup(paper_towel)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(pose1)\ngoto(tobis_office)\nrelease(paper_towel)\ngoto(pose8)\ngoto(pose13)\ngoto(tobis_office)\naccess(desk38)\npickup(pepsi)\ngoto(tobis_office)\ngoto(pose1)\ngoto(pose2)\ngoto(pose5)\ngoto(kitchen)\naccess(recycling_bin)\nrelease(pepsi)\ngoto(kitchen)\ngoto(pose5)\ngoto(pose2)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard1)\npickup(paper_towel)\ngoto(supplies_station)\ngoto(pose24)\ngoto(pose2)\ngoto(pose5)\ngoto(pose1)\ngoto(tobis_office)\naccess(desk38)\nrelease(paper_towel)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n40\nI want to make a sandwich. Place all the ingredients on the lunch table.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(kitchen_bench)\npickup(bread)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\nrelease(bread)\ngoto(pose25)\ngoto(kitchen)\naccess(kitchen_bench)\npickup(butter)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\nrelease(butter)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(cheese)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\nrelease(cheese)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\npickup(tomato)\ngoto(pose25)\ngoto(cafeteria)\naccess(lunch_table)\nrelease(tomato)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(kitchen_bench)\npickup(bread)\ngoto(pose25)\ngoto(cafeteria)\nrelease(bread)\ngoto(pose25)\ngoto(kitchen)\npickup(butter)\ngoto(pose25)\ngoto(cafeteria)\nrelease(butter)\ngoto(pose25)\ngoto(kitchen)\naccess(fridge)\nopen(fridge)\npickup(cheese)\ngoto(pose25)\ngoto(cafeteria)\nrelease(cheese)\ngoto(pose25)\ngoto(kitchen)\npickup(tomato)\ngoto(pose25)\ngoto(cafeteria)\nrelease(tomato)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose25)\ngoto(kitchen)\npickup(cheese)\npickup(tomato)\npickup(kale_leaves2)\npickup(bread)\npickup(butter)\ngoto(pose25)\ngoto(pose26)\ngoto(presentation_lounge)\nrelease(cheese)\nrelease(tomato)\nrelease(kale_leaves2)\nrelease(bread)\nrelease(butter)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n41\nA delegation of project partners is arriving soon. We want to serve them snacks and non-alcoholic drinks. Prepare everything in the largest meeting room. Use items \nfound in the supplies room only.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\nopen(cupboard2)\npickup(orange_juice)\ngoto(pose24)\ngoto(pose25)\ngoto(pose26)\ngoto(meeting_room3)\naccess(table1)\nrelease(orange_juice)\ngoto(pose26)\ngoto(pose25)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\npickup(biscuits)\ngoto(pose24)\ngoto(pose25)\ngoto(pose26)\ngoto(meeting_room3)\naccess(table1)\nrelease(biscuits)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\nopen(cupboard2)\npickup(orange_juice)\ngoto(pose24)\ngoto(pose25)\ngoto(pose26)\ngoto(meeting_room3)\nrelease(orange_juice)\ngoto(pose26)\ngoto(pose25)\ngoto(pose24)\ngoto(supplies_station)\npickup(biscuits)\ngoto(pose24)\ngoto(pose25)\ngoto(pose26)\ngoto(meeting_room3)\nrelease(biscuits)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\nopen(cupboard2)\npickup(bottled_water1)\npickup(bottled_water2)\npickup(bottled_water3)\npickup(bottled_water4)\npickup(bottled_water5)\npickup(orange_juice)\npickup(biscuits)\nclose(cupboard2)\ngoto(pose24)\ngoto(pose25)\ngoto(pose26)\ngoto(meeting_room3)\nrelease(bottled_water1)\nrelease(bottled_water2)\nrelease(bottled_water3)\nrelease(bottled_water4)\nrelease(bottled_water5)\nrelease(orange_juice)\nrelease(biscuits)\nEmpty the dishwasher. Place all items in their correct locations\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(dishwasher)\nopen(dishwasher)\npickup(plate2)\naccess(cabinet)\nopen(cabinet)\nrelease(plate2)\naccess(dishwasher)\npickup(bowl)\naccess(cabinet)\nrelease(bowl)\naccess(dishwasher)\npickup(spoon)\naccess(drawer)\nopen(drawer)\nrelease(spoon)\nclose(drawer)\naccess(cabinet)\nclose(cabinet)\naccess(dishwasher)\nclose(dishwasher)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\naccess(dishwasher)\nopen(dishwasher)\npickup(plate2)\naccess(cabinet)\nrelease(plate2)\naccess(dishwasher)\npickup(bowl)\naccess(cabinet)\nrelease(bowl)\naccess(dishwasher)\npickup(spoon)\naccess(drawer)\nrelease(spoon)\nclose(dishwasher)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose15)\ngoto(kitchen)\naccess(dishwasher)\nopen(dishwasher)\npickup(plate2)\ngoto(kitchen)\naccess(cabinet)\nrelease(plate2)\ngoto(kitchen)\naccess(dishwasher)\npickup(bowl)\ngoto(kitchen)\naccess(cabinet)\nrelease(bowl)\ngoto(kitchen)\naccess(dishwasher)\npickup(spoon)\ngoto(kitchen)\naccess(drawer)\nrelease(spoon)\ngoto(kitchen)\naccess(dishwasher)\nclose(dishwasher)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n42\nServe bottled water to the attendees who are seated in meeting room 1. Each attendee can only receive a single bottle\nof water.\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\nopen(cupboard2)\npickup(bottled_water1)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(meeting_room1)\naccess(table5)\nrelease(bottled_water1)\ngoto(pose2)\ngoto(pose3)\ngoto(pose9)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\npickup(bottled_water2)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(meeting_room1)\naccess(table5)\nrelease(bottled_water2)\ngoto(pose2)\ngoto(pose3)\ngoto(pose9)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\npickup(bottled_water3)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(meeting_room1)\naccess(table5)\nrelease(bottled_water3)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\npickup(bottled_water1)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(meeting_room1)\nrelease(bottled_water1)\ngoto(pose2)\ngoto(pose3)\ngoto(pose9)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\npickup(bottled_water2)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(meeting_room1)\nrelease(bottled_water2)\ngoto(pose2)\ngoto(pose3)\ngoto(pose9)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(pose24)\ngoto(supplies_station)\naccess(cupboard2)\npickup(bottled_water3)\ngoto(pose24)\ngoto(pose23)\ngoto(pose14)\ngoto(pose13)\ngoto(pose9)\ngoto(pose3)\ngoto(pose2)\ngoto(meeting_room1)\nrelease(bottled_water3)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose23)\ngoto(supplies_station)\npickup(bottled_water1)\ngoto(pose23)\ngoto(pose14)\ngoto(pose1)\ngoto(meeting_room1)\nrelease(bottled_water1)\ngoto(pose1)\ngoto(pose14)\ngoto(pose23)\ngoto(supplies_station)\npickup(bottled_water2)\ngoto(pose23)\ngoto(pose14)\ngoto(pose1)\ngoto(meeting_room1)\nrelease(bottled_water2)\ngoto(pose1)\ngoto(pose14)\ngoto(pose23)\ngoto(supplies_station)\npickup(bottled_water3)\ngoto(pose23)\ngoto(pose14)\ngoto(pose1)\ngoto(meeting_room1)\nrelease(bottled_water3)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n43\nLocate all 6 complimentary t-shirts given to the PhD students and place them on the shelf in admin.\ngoto(pose7)\ngoto(pose5)\ngoto(pose6)\ngoto(phd_bay1)\naccess(desk9)\npickup(complimentary_tshirt1)\ngoto(pose6)\ngoto(pose5)\ngoto(pose7)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt1)\ngoto(pose18)\ngoto(pose16)\ngoto(pose7)\ngoto(pose5)\ngoto(pose6)\ngoto(phd_bay1)\naccess(desk10)\npickup(complimentary_tshirt2)\ngoto(pose6)\ngoto(pose5)\ngoto(pose7)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt2)\ngoto(pose18)\ngoto(pose16)\ngoto(pose17)\ngoto(phd_bay2)\naccess(desk15)\npickup(complimentary_tshirt3)\ngoto(pose17)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt3)\ngoto(pose18)\ngoto(pose16)\ngoto(pose17)\ngoto(phd_bay2)\naccess(desk18)\npickup(complimentary_tshirt4)\ngoto(pose17)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt4)\ngoto(pose18)\ngoto(pose16)\ngoto(pose17)\ngoto(phd_bay2)\naccess(desk18)\npickup(complimentary_tshirt6)\ngoto(pose17)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt6)\ngoto(pose18)\ngoto(pose19)\ngoto(phd_bay4)\naccess(desk25)\npickup(complimentary_tshirt5)\ngoto(pose19)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt5)\ngoto(pose7)\ngoto(pose5)\ngoto(pose6)\ngoto(phd_bay1)\naccess(desk9)\npickup(complimentary_tshirt1)\ngoto(pose6)\ngoto(pose5)\ngoto(pose7)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt1)\ngoto(pose18)\ngoto(pose16)\ngoto(pose7)\ngoto(pose5)\ngoto(pose6)\ngoto(phd_bay1)\naccess(desk10)\npickup(complimentary_tshirt2)\ngoto(pose6)\ngoto(pose5)\ngoto(pose7)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt2)\ngoto(pose18)\ngoto(pose16)\ngoto(pose17)\ngoto(phd_bay2)\naccess(desk15)\npickup(complimentary_tshirt3)\ngoto(pose17)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt3)\ngoto(pose18)\ngoto(pose16)\ngoto(pose17)\ngoto(phd_bay2)\naccess(desk18)\npickup(complimentary_tshirt4)\ngoto(pose17)\ngoto(pose16)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt4)\naccess(desk18)\npickup(complimentary_tshirt6)\naccess(shelf)\nrelease(complimentary_tshirt6)\ngoto(pose18)\ngoto(pose19)\ngoto(phd_bay4)\naccess(desk25)\npickup(complimentary_tshirt5)\ngoto(pose19)\ngoto(pose18)\ngoto(admin)\naccess(shelf)\nrelease(complimentary_tshirt5)\nSayPlan\nLLM-As-Planner\nLLM+P\nSuccess\nFail\n44\nTable 19: Causal Planning Evaluation. Task planning action sequences generated for a mobile\nmanipulator robot to follow for both the simple and long-horizon planning instruction sets.\n45\nH\nScalability Ablation Study\nIn this study, we evaluate the ability of SayPlan and the underlying LLM to reason over larger-scale\nscene graphs. More speci\ufb01cally, as SayPlan\u2019s initial input is a collapsed 3DSG, we explore how\nincreasing the number of nodes in this base environment impacts the ability of the LLM to attend to\nthe relevant parts of the scene graph for both semantic search and iterative replanning.\n30\n60\n100\n200\n300\nBase Environment Size (Number of Room Nodes)\n\"Find me a carrot.\"\n\"Find me a book that was left\n next to a robotic gripper.\"\n\"Find me a ripe banana.\"\n\"Find object J64M.\n J64M should be kept \nbelow 0 degree Celsius.\"\n\"Find me something\n non-vegetarian.\"\n\"There is postdoc who\n has a pet Husky.\n Find their desk.\"\nSimple Search\nComplex Search\nSuccess\nFailure\nFigure 7: Evaluating the performance of the underlying LLMs semantic search capabilities\nas the scale of the environment increases. For the of\ufb01ce environment used in this study, we are\nprimarily interested in the number of room nodes present in the collapsed form of the 3DSG.\n30\n60\n100\n200\n300\nBase Environment Size (Number of Room Nodes)\n\"Close Jason's cabinet.\"\n\"Hide one of Peter's \n valuable belongings.\"\n\"Something is smelling \nin the kitchen. Dispose of it.\"\n\"Heat up the noodles in \nthe fridge, and place it\n somewhere where I can enjoy it.\"\n\"Let's play a prank on Niko.\n Dimity might have something.\"\n\"Tobi spilt soda on his desk.\n Throw away the can and take\n him something to clean with.\"\nSimple Planning\nLong-Horizon Planning\nSuccess\nFailure\nFigure 8: Evaluating the performance of SayPlan\u2019s causal planning capabilities as the scale\nof the environment increases. For the of\ufb01ce environment used in this study, we are primarily\ninterested in the number of room nodes present in the collapsed form of the 3DSG.\nWe note here that all the failures that occurred across both semantic search and iterative replanning\nwere a result of the LLM\u2019s input exceeding the maximum token limits \u2013 in the case of GPT-4\nthis corresponded to 8192 tokens. With regard to the scalability to larger environments, this is an\nimportant observation as it indicates that the LLM\u2019s reasoning capabilities or ability to attend to the\nrelevant parts of the 3DSG is not signi\ufb01cantly impacted by the presence of \u201dnoisy\u201d or increasing\nnumber of nodes. One potential downside to larger environments however is the increased number\nof steps required before semantic search converges. As more semantically relevant \ufb02oor or room\nnodes enter the scene, each one of these may be considered by the LLM for exploration.\n46\nI\nReal World Execution of a Generated Long Horizon Plan.\n\u201ca postdoc spilled their soda, help \nthem clean it up\u201d\ngoto(pose8)\ngoto(pose7)\ngoto(pose5)\ngoto(pose4)\ngoto(postdoc_bay4)\naccess(desk31)\npickup(soda_can2)\naccess(trash_can)\nrelease(soda_can2)\ngoto(pose5)\ngoto(pose7)\ngoto(pose24)\ngoto(pose25)\ngoto(kitchen)\ngoto(pose23)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\naccess(kitchen_bench)\npickup(tea_towel)\ngoto(pose24)\ngoto(pose24)\ngoto(pose5)\ngoto(pose7)\ngoto(pose23)\ngoto(pose8)\ngoto(pose13)\ngoto(pose14)\ngoto(pose4)\ngoto(postdoc_bay4)\naccess(desk31)\nrelease(tea_towel)\nGenerated Plan:\nFigure 9: Real World Execution of a Generated Long Horizon Plan. Execution of a generated\nand validated task plan on a real-world mobile manipulator robot.\n47\nJ\nInput Prompt Structure\nInput prompt passed to the LLM for SayPlan. Note that the components highlighted in violet rep-\nresent static components of the prompt that remain \ufb01xed throughout both the semantic search and\niterative replanning phases of SayPlan.\nAgent Role: You are an excellent graph planning agent. Given a graph\nrepresentation of an environment, you can explore the graph by expanding\nnodes to find the items of interest. You can then use this graph to generate a\nstep-by-step task plan that the agent can follow to solve a given instruction.\nEnvironment Functions:\ngoto(<pose>): Move the agent to any room node or pose node.\naccess(<asset>): Provide access to the set of affordances associated with an\nasset node and its connected objects.\npickup(<object>): Pick up an accessible object from the accessed node.\nrelease(<object>): Release grasped object at an asset node.\nturn_on/off(<object>): Toggle object at agent\u2019s node, if accessible and has\naffordance.\nopen/close(<asset>): Open/close asset at agent\u2019s node, affecting object\naccessibility.\ndone(): Call when the task is completed.\nEnvironment State:\nontop_of(<asset>): Object is located on <asset>\ninside_of(<asset>): Object is located inside <asset>\ninside_hand: Object is currently being grasped by the robot/agent\nclosed: Asset can be opened\nopen: Asset can be closed or kept open\non: Asset is currently on\noff: Asset is currently off\naccessible:\nThe object is not accessible if it is inside an asset and the\nasset state is \"closed\".\nEnvironment API:\nexpand_node(<node>): Reveal assets/objects connected to a room/floor node.\ncontract_node(<node>): Hide assets/objects, reducing graph size for memory\nconstraints.\nverify_plan(): Verify generated plan in the scene graph environment.\nOutput Response Format:\n{chain_of_thought: break your problem down into a series of intermediate\nreasoning steps to help you determine your next command,\nreasoning: justify why the next action is important\nmode: \"exploring\" OR \"planning\"\ncommand: {\u2018\u2018command_name\": Environment API call\n\"node_name\": node to perform an operation on\n\"plan\": task plan if in planning mode}}\nExample: <see Appendix K and L>\nInstruction: Natural language description of the task\n3D Scene Graph: Text-serialised JSON description of a 3D scene graph\nMemory: History of previously expanded nodes\nFeedback: External textual feedback from scene graph simulator\nEnsure the response can be parsed by Python json.loads.\n48\nK\nExample Input-Output Interaction Between LLM and Scene Graph\nSimulator During Semantic Search\nInstruction: make a coffee for Tom and place it in his room\n3D Scene Graph: {nodes: {room: [{id: bobs_room}, {id: toms_room}, {id:\njacks_room}, {id: kitchen}, {id: livingroom}], pose: [{id: pose1},\n{id: pose2}, {id: pose3}, {id: pose4}, {id: pose5}], agent: [{location:\nbobs_room, id: agent}]}, links: [bobs_room\u2194pose1, bobs_room\u2194agent,\ntoms_room\u2194pose1, toms_room\u2194pose2, toms_room\u2194pose5, jacks_room\u2194pose2,\njacks_room\u2194pose3, kitchen\u2194pose3, kitchen\u2194pose4, kitchen\u2194pose5,\nlivingroom\u2194pose4]}\nMemory: []\nSayPlan [Mode->Search]: chain-of-thought: explore graph for coffee mug,\ncoffee machine and somewhere to place the coffee -> perform common sense\nexploration first -> explore toms room -> explore kitchen -> if items not\nfound then explore other nodes\nreasoning: i will expand toms room\ncommand: expand(toms_room)\nScene Graph Simulator: <Updated 3D scene graph>\nMemory: [toms_room]\nSayPlan [Mode->Search]: chain-of-thought: i have found a wardrobe in tom\u2019s\nroom -> leave this node expanded -> the coffee mug is not in his room -> still\nhave not found the coffee machine -> kitchen might have coffee machine and\ncoffee mug -> explore this node next\nreasoning: i will expand the kitchen next\ncommand: expand(kitchen)\nScene Graph Simulator: <Updated 3D scene graph>\nMemory: [toms_room, kitchen]\nSayPlan [Mode->Search]: chain-of-thought: i\u2019ve found the coffee machine in\nthe kitchen -> the coffee mug however is not in the kitchen -> i will explore\nother unexpanded nodes now for the coffee mug -> jacks room, bobs room and the\nliving room are unexplored yet -> i will try jack\"s room next\nreasoning: i will expand jacks room next\ncommand: expand(jacks_room)\nScene Graph Simulator: <Updated 3D scene graph>\nMemory: [toms_room, kitchen, jacks_room]\nSayPlan [Mode->Search]: chain-of-thought: jacks room does not have anything\nrelevant towards solving this task -> I should contract this node to save\ntoken memory -> i will explore other unexplored nodes after doing this\nreasoning: contract jack\u2019s room to save memory\ncommand: contract(jacks_room)\nScene Graph Simulator: <Updated 3D scene graph>\nMemory: [toms_room, kitchen, jacks_room]\nSayPlan [Mode->Search]: chain-of-thought: i have found a wardrobe in tom\u2019s\nroom and coffee machine in the kitchen -> the coffee mug is still required for\nthe task -> explore a new room next\nreasoning: i will expand bob\u2019s room as it is unexplored\ncommand: expand(bobs_room)\nScene Graph Simulator: <Updated 3D scene graph>\nMemory: [toms_room, kitchen, jacks_room, bobs_room]\nSayPlan [Mode->Search]: chain-of-thought: i have found the coffee mug, the\ncoffee machine and tom\u2019s wardrobe on the graph -> search complete -> I will\nswitch to planning mode-> collect coffee mug -> generate plan for making\ncoffee -> place coffee mug on Tom\u2019s wardrobe\nreasoning: I will generate a task plan using the identified subgraph\n49\nL\nExample Input-Output Interaction Between LLM and Scene Graph\nEnvironment During Iterative Replanning\nInstruction: make a coffee for Tom and place it in his room\n3D Scene Graph: {nodes: {room: [{id: bobs_room}, {id: toms_room}, {id:\njacks_room}, {id: kitchen}, {id: livingroom}], pose: [{id: pose1},\n{id: pose2}, {id: pose3}, {id: pose4}, {id: pose5}], agent: [{location:\nbobs_room, id: agent}], asset: [{room: toms_room, state: free,\naffordances: [release], id: bed2}, {room: toms_room, state: closed,\naffordances: [open, close, release], id: wardrobe2}, {room: kitchen,\nstate: closed, affordances: [open, close, release], id: fridge},\n{room: kitchen, affordances: [turn_on, turn_off], state: off, id:\ncoffee_machine}, {room: bobs_room, state: free, affordances: [release],\nid: bed1}, {room: bobs_room, state: closed, affordances: [open, close,\nrelease], id: wardrobe1}], object: [{affordances: [pickup], state:\ninside_of(wardrobe1), attributes: \"blue\", id: coffee_mug}]}, links:\n[bobs_room\u2194pose1, bobs_room\u2194agent, bobs_room\u2194bed1, bobs_room\u2194wardrobe1,\ntoms_room\u2194pose1, toms_room\u2194pose2, toms_room\u2194pose5, toms_room\u2194bed2,\ntoms_room\u2194wardrobe2, jacks_room\u2194pose2, jacks_room\u2194pose3, kitchen\u2194pose3,\nkitchen\u2194pose4, kitchen\u2194pose5, kitchen\u2194fridge, kitchen\u2194coffee_machine,\nlivingroom\u2194pose4, wardrobe1\u2194coffee_mug]}\nMemory: [toms_room, kitchen, jacks_room, bobs_room]\nSayPlan [Mode->Planning]: chain-of-thought: \"i have found the coffee mug,\nthe coffee machine and tom\u2019s wardrobe on the graph -> search complete -> I\nwill switch to planning mode-> collect coffee mug -> generate plan for making\ncoffee -> place coffee mug on Tom\u2019s wardrobe\"\nreasoning: I will generate a task plan using the identified subgraph\nplan: [goto(bobs_room) > access(wardrobe1) > pickup(coffee_mug)\n> goto(kitchen) > access(coffee_machine) > release(coffee_mug) >\nturn_on(coffee_machine) > turn_off(coffee_machine) > pickup(coffee_mug) >\ngoto(toms_room) > access(wardrobe2) > release(coffee_mug) > done]\nPath Planner:plan: [goto(pose3) > goto(pose4) > goto(pose1) >\ngoto(bobs_room) > access(wardrobe1) > pickup(coffee_mug) > goto(pose2) >\ngoto(pose5) > goto(kitchen) > access(coffee_machine) > release(coffee_mug)\n> turn_on(coffee_machine) > turn_off(coffee_machine) > pickup(coffee_mug)\n> goto(pose5) > goto(pose2) > goto(toms_room) > access(wardrobe2) >\nrelease(coffee_mug) > done]\nScene Graph Simulator (Feedback): \"coffee mug is not accessible\"\nMemory: [toms_room, kitchen, jacks_room, bobs_room]\nSayPlan [Mode->Planning]: chain-of-thought: \"the coffee mug was not\naccessible -> the wardrobe it is in is currently closed -> I update the plan\nto open the wardrobe first -> generate plan for making coffee -> place coffee\nmug on Tom\u2019s wardrobe\"\nreasoning: I will generate a task plan using the identified subgraph and the\nfeedback from the simulator\nplan: [goto(bobs_room) > access(wardrobe1) > open(wardrobe1) >\npickup(coffee_mug) > goto(kitchen) > access(coffee_machine) >\nrelease(coffee_mug) > turn_on(coffee_machine) > turn_off(coffee_machine)\n> pickup(coffee_mug) > goto(toms_room) > access(wardrobe2) >\nrelease(coffee_mug) > done]\nPath Planner:plan: [goto(pose3) > goto(pose4) > goto(pose1) >\ngoto(bobs_room) > access(wardrobe1) > open(wardrobe1) > pickup(coffee_mug)\n> goto(pose2) > goto(pose5) > goto(kitchen) > access(coffee_machine) >\nrelease(coffee_mug) > turn_on(coffee_machine) > turn_off(coffee_machine)\n> pickup(coffee_mug) > goto(pose5) > goto(pose2) > goto(toms_room) >\naccess(wardrobe2) > release(coffee_mug) > done]\nScene Graph Simulator: Plan Verified\n50\n"
  },
  {
    "title": "Instruction Mining: High-Quality Instruction Data Selection for Large Language Models",
    "link": "https://arxiv.org/pdf/2307.06290.pdf",
    "upvote": "9",
    "text": "Preprint\nINSTRUCTION MINING: WHEN DATA MINING MEETS\nLARGE LANGUAGE MODEL FINETUNING\nYihan Cao\u2217\nCarnegie Mellon University\nPittsburgh, PA\nyihanc@cs.cmu.edu\nYanbin Kang\u2217\nLinkedIn\nMountain View, CA\nybkang@linkedin.com\nChi Wang\nMicrosoft Research\nRedmond, Washington\nwang.chi@microsoft.com\nLichao Sun\nLehigh University\nBethlehem, PA\nlis221@lehigh.edu\nABSTRACT\nLarge language models (LLMs) are initially pretrained for broad capabilities and\nthen finetuned with instruction-following datasets to improve their performance in\ninteracting with humans. Despite advances in finetuning, a standardized guideline\nfor selecting high-quality datasets to optimize this process remains elusive. In this\npaper, we first propose INSTRUCTMINING, an innovative method designed for\nautomatically selecting premium instruction-following data for finetuning LLMs.\nSpecifically, INSTRUCTMINING utilizes natural language indicators as a measure of\ndata quality, applying them to evaluate unseen datasets. During experimentation, we\ndiscover that double descent phenomenon exists in large language model finetuning.\nBased on this observation, we further leverage BLENDSEARCH to help find the\nbest subset among the entire dataset (i.e., 2,532 out of 100,000). Experiment results\nshow that INSTRUCTMINING-7B achieves state-of-the-art performance on two of\nthe most popular benchmarks: LLM-AS-A-JUDGE and Huggingface OPENLLM.\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated transformative capabilities, powering numerous\napplications with the strong ability in automatically generating responses according to human\ninstructions (Ouyang et al., 2022; Peng et al., 2023; Chung et al., 2022; Scao et al., 2022). However,\nit is hard sometimes for language models to capture the meaning of human instructions and respond\nto them even if they are pretrained with large amount of data. To counter this challenge, instruction\ntuning emerged as a paramount method in tailoring the behaviours of LLMs, which leverages\ninstruction-following pairwise data (i.e., instruction data) during finetuning (Wei et al., 2021; Ouyang\net al., 2022; Chung et al., 2022; Wang et al., 2022a). A recent study LIMA has revealed that even\na small amount of carefully selected high-quality instruction data can significantly improve model\nperformance through instruction tuning (Zhou et al., 2023). Nevertheless, LIMA still requires human\nexperts to filter examples from extensive datasets, which is both time-consuming and expensive.\nIn this paper, we propose INSTRUCTMINING, a novel method designed to automatically select\nhigh-quality instruction data for finetuning better LLMs. Achieving this objective necessitates a\ndata evaluator capable of assessing the quality of instruction data without the intervention of human\nexperts. Furthermore, the data selector is also indispensable for automatically identifying the most\nsuitable subset of instruction data for finetuning LLMs. Nevertheless, quantifying the quality of\ninstruction data without human expert is a non-trivial task. To address this problem, we employ\nthe loss incurred by a finetuned model on the evaluation set a proxy for data quality. However,\ncomputing this inference loss necessitates the actual finetuning of a language model, a potentially\ntime-consuming process. To overcome this obstacle, we introduce a set of selected natural language\n\u2217Equal contributions.\n1\narXiv:2307.06290v2  [cs.CL]  27 Oct 2023\nPreprint\nindicators (e.g., reward model score), capable of predicting the inference loss without the need for\nfinetuning an LLM. This approach can efficiently provide an estimation of the dataset\u2019s quality.\n1st Performance Descent\n2nd Performance Descent\nQuantity Dominated\nOptimal Point\nIntersection Point\nFigure 1: Double descent phenomenon in generative language models. Higher loss indicates worse\nperformance. Red line refers to INSTRUCTMINING selected data sizes w.r.t. model inference loss.\nBlue line refers to random selected data sizes w.r.t. model inference loss. Our optimization goal is to\nfind the optimal point which results in the lowest inference loss.\nWhile our approach can assess and rank the entire dataset based on data quality, determining the most\noptimal subset for finetuning LLMs remains an unresolved challenge. A straightforward solution is to\nchoose the top-K high-quality data samples, but selecting the appropriate K proves to be inherently\ndifficult in practice. To address this complexity, we conducted a series of experiments exploring the\nrelationship between data quantity and quality. Remarkably, as we continued to increase the subset\nsize for finetuning language models, we observed the double descent phenomenon (Nakkiran et al.,\n2021), as illustrated in Figure 1. This observation signifies a transition in the primary determinant of\nmodel performance from data quality to data quantity once the data size crosses a specific threshold. In\nsuch scenarios, focusing on an initial set of high-quality data points (e.g., K=10,000) is more efficient\nfor identifying the optimal point than perusing the entire dataset. Given the cost-sensitive nature of\npinpointing this optimal point, we employ BLENDSEARCH (Wang et al., 2021a) to automatically\nsearch for the best subset for our needs.\nWe further substantiate the validity and scalability of INSTRUCTMINING by contrasting its perfor-\nmance with other state-of-the-arts across diverse benchmarks. Notably, INSTRUCTMINING enhances\nthe performance of LLAMA-2-7B by 4.93 on the Huggingface OPENLLM benchmark. In addition,\nour finetuned models are able to generate equivalent or superior responses in 64.67% of instances,\ncompared to VICUNA-7B-v1.5. Furthermore, INSTRUCTMINING contributes to heightened finetuning\nefficiency. The optimal INSTRUCTMINING model, finetuned on a mere 2.5% (i.e., 2,532 out of\n100,000) of the highest-quality examples from the complete dataset, can achieve state-of-the-art on\nboth LLM-AS-A-JUDGE (Zheng et al., 2023) and OPENLLM benchmarks.\nOur contributions are summarized as follows:\n\u2022 In this work, we pioneer the application of classical data mining techniques to enhance\nLLMs by autonomously selecting high-quality data. To realize this objective, we introduce\nINSTRUCTMINING, a method encompassing data assessment and selection processes.\n\u2022 The proposed INSTRUCTMINING innovatively combines customized language indicators\nwith an advanced searching algorithm, enabling the automatic assessment of data quality\nand identification of the optimal subset for finetuning language models.\n\u2022 Models finetuned with INSTRUCTMINING exhibit state-of-the-art performance on two of the\nmost popular benchmarks: LLM-AS-A-JUDGE and Huggingface OPENLLM. Meanwhile,\nUtilizing less training data can effectively reduce both the training time and cost.\n2\nMETHODOLOGY\nIn this section, we provide a detailed description of our proposed method, INSTRUCTMINING. A\nprocedure graph is provided in Figure 5. Our method is composed of two parts, quality estimation and\nthreshold search. We first introduce our method for estimating instruction data quality in Section 2.1.\nThis is achieved by aligning the data quality with the inference loss of a fine-tuned model. Then we\n2\nPreprint\nIndicator\nNotation\nExplanation\nInput length\nLenin\nThe number of tokens in tokenized inputs.\nOutput length\nLenout\nThe number of tokens in tokenized outputs.\nReward score\nRew\nThe oasst-rm-pythia-1.4b reward model inference score of every pair\nin the dataset. (K\u00f6pf et al., 2023)\nPerplexity\nPPL\nThe exponentiated average negative log-likelihood of response.\nMTLD\nMTLD\nMeasure of Textual Lexical Diversity (McCarthy & Jarvis, 2010)\nKNN-i\nKNNi\nDistance to approximate ith-nearest neighbors (Dong et al., 2011) in\nSentenceBERT(Reimers & Gurevych, 2019) embedding space.\nUnieval-naturalness\nNat\nThe score of whether a response is like something a person would naturally\nsay, provided by the UniEval (Zhong et al., 2022) dialogue model.\nUnieval-coherence\nCoh\nThe score of whether this response serves as a valid continuation of\nthe previous conversation, provided by the UniEval (Zhong et al., 2022)\ndialogue model.\nUnieval-understandability\nUnd\nThe score of whether the response is understandable, provided by the\nUniEval (Zhong et al., 2022) dialogue model.\nTable 1: Summary of indicators for instruction quality evaluation. Each data sample is viewed as a\npair of instruction and response (i.e., input and output) of LLM.\npropose our evaluation rule along with the selected natural language indicators in Section 2.2. Finally,\nwe present the observed double descent phenomenon and introduce a BLENDSEARCH-based data\nselector in Section 2.3.\n2.1\nWHAT IS INSTRUCTION QUALITY?\nIn this paper, we follow the superficial alignment hypothesis proposed by Zhou et al. (2023) that\na model\u2019s knowledge is mostly learnt during pretraining, while instruction data teaches the model\nto follow a certain pattern when interacting with users. Hence, the quality of these instruction data\ncould be viewed as its ability to efficiently steer language models in learning to generate responses in\na particular manner. Based on this assumption, we further propose our instruction quality evaluation\nhypothesis as follows.\nHypothesis 1 Instruction Quality Evaluation Hypothesis:\nGiven an instruction dataset D, we\nfinetune a language model on D, denoted as Mft. The instruction quality of D can be estimated\nthrough the inference loss of Mft on a evaluation dataset Deval.\nTo ensure the inference loss provides a valid measure for evaluating data quality, the evaluation set\nshould comprise a selected collection of unbiased and high-quality instruction-following samples.\nIn particular, given an instruction-following dataset D, we finetune a base language model M using\nD with model training settings S. S normally refers to training batch size, epochs, etc. L refers to\nthe loss function. The obtained finetuned language model is denoted as Mft. We define the dataset\nD\u2019s quality QD|M,S as below,\nQD|M,S \u221d \u2212L(Mft, Deval)\n(1)\nwhere Deval refers to the high-quality and unbiased evaluation set, and \u221d means a direct proportion.\n2.2\nHOW TO ESTIMATE INSTRUCTION QUALITY?\nAccording to Equation 1, we utilize the inference loss to evaluate instruction quality. However,\nfinetuning an LLM for evaluation can be inefficient, since this process can take days of training.\nTo solve this problem, we introduce a set of natural language indicators and use the indicators\nto predict the inference loss. In this paper, We have a set of indicators I = {Ii, i = 0, \u00b7 \u00b7 \u00b7 , n},\nsummarized in Table 1. For a given instruction dataset D, we compute the corresponding indicator\nvalues I(D) = {Ii(D), i = 0, \u00b7 \u00b7 \u00b7 , n}. There exists a function F such that the aforementioned model\ninference loss L(Mft, Deval) can be approximated using F(I(D)).\n3\nPreprint\nThe relationship between the finetuned model inference loss L and these computed indicators can be\nformulated as in Equation 2.\n\u2212Q!|#,% \u221d log \ud835\udc3f \ud835\udc40&', \ud835\udc37()*+ \u2243 \ud835\udc3f, + \ud835\udc39{\ud835\udc3c- \ud835\udc37 , \ud835\udc3c. \ud835\udc37 , \u22ef , \ud835\udc3c/ \ud835\udc37 , \u22ef , \ud835\udc3c0(\ud835\udc37)}\nInstruction Quality\nModel Evaluation Loss\nMinimal Loss Constant\nBag of Indicators\n\ud835\udc56th Indicator on data \ud835\udc37\n(2)\nIn this paper, we assume that there exists a multivariate linear function of I that is proportional to the\nlogarithmic loss. Consequently, Equation 2 can be reparameterized as Equation 3:\nlog L(Mft, Deval) \u221d L0 + F{I(D)}\n\u221d L0 + \u03b20 + \u03b21I1(D) + \u03b22I2(D) + \u00b7 \u00b7 \u00b7 + \u03b2nIn(D) + \u03f5\n(3)\nwhere \u03b20 denotes the linear constant, \u03b2i, i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} represents a sequence of linear coefficients,\nand \u03f5 refers to the random error term.\nTo investigate the relationship between these indicators and the overall dataset quality, it becomes\nnecessary to accumulate experimental results to estimate the unknown parameters \u03b2i, i \u2208 {0, \u00b7 \u00b7 \u00b7 , n}.\nIn this study, we employ the Least Squares method (Bj\u00f6rck, 1990) to estimate the parameters in the\nmultivariate function. The Least Squares method is a standard approach in regression analysis for the\napproximate solution of overdetermined systems. The technique minimizes the sum of the square\nresiduals, thus providing the optimal fit between the observed and predicted data in terms of reducing\nthe overall prediction error. Our experimental results and analysis are detailed in Section 4.\n2.3\nINSTRUCTION DATA SELECTOR\nDuring experimentation, we observe that with larger data size, model performance first gets better\nand then gets worse. After the data size grows to a certain level, model performance gets better again.\nFurther analysis are provided in Section 5.1. This phenomenon indicates that there is an optimal point\nwhere better performance can be obtained with a smaller amount of data. Hence, searching for the\nbest data size is important for finetuning a better language model.\nTo achieve our objective, we employ BLENDSEARCH (Wang et al., 2021a) in Flaml (Wang et al.,\n2021b) library to determine the optimal data size. BLENDSEARCH effectively combines global and\nlocal optimizations by Bayesian optimization and different local search threads, making it efficient\nfor searching cost-related hyperparameters and complex search spaces with local optima. In our\ncontext, we leverage a logarithmic uniform distribution to randomly sample the dataset size, treating\nthe dataset size as the experiment\u2019s cost since the training time scales proportionally with the dataset\nsize. The search goal is to minimize the loss on the evaluation set.\n3\nEXPERIMENT SETTINGS\nOur experiments mainly focus on two goals. The first goal is to estimate the unknown parameters in\nthe proposed INSTRUCTMINING rule. The second one is to evaluate and analyze the performance of\nINSTRUCTMINING over varied finetuning scenarios. Section 3.1 elaborates rule estimation empirical\nstudy design. Section 3.2 details the datasets we use for conducting both estimation and evaluation\nexperiment. Section 3.3 elaborates the finetuning settings we used for estimation and evaluation.\n3.1\nEMPIRICAL EXPERIMENT DESIGN\nThe general procedure of our rule estimation experiment is shown in Figure 5. To estimate the\ncorrelation between the evaluation loss and indicators I, we need to get datasets of different indicator\nvalues. To achieve this, we first select several commonly used datasets with different presumed\nquality levels and fuse them together with randomly sampled percentages to create finetune datasets.\n4\nPreprint\nDatasets\nSourced from\nSize\nQuality\nUsage\nALPACA\nGenerated w/ davinci\n52.0k\nNormal\nEst. Candidate\nOPEN-ASSITANT\nhuman-generated\n3.4k\nBoth\nEst. Candidate\nSTACKEXCHANGE\nhuman-generated\n3.0k\nHigh\nEst. Candidate\nWIKIHOW\nhuman-generated\n2.0k\nHigh\nEst. Candidate\nDOLLY\nhuman-generated\n15.0k\nNormal\nEvaluation\nOPENORCA\nGenerated w/ GPT-4\n1M\nHigh\nEvaluation\nOPENORCA\nGenerated w/ GPT-3.5\n3M\nNormal\nEvaluation\nTable 2: Overview of datasets used during experiment.\nThese sampled finetune datasets should encompass varying proportions of preassumed high quality\nand low quality examples. For each of these sampled datasets Di, we compute its respective indicator\nvalues I(Di) and finetune a base language model M using Di. Following Equation 1, the quality\nQDi for dataset Di is approximated using the evaluation loss of finetuned model Mft,i on a fair\nevaluation dataset Deval. Following the collection of a range of results correlating QDi with I(Di),\nwe undertake a statistical regression analysis to discern relationships within the dataset.\n3.2\nDATASETS\nCandidate datasets for rule fitting. In order to create diverse training datasets, we collect data from\nvarious sources. This approach ensures that the datasets exhibit differences in quality and maintain\ndiversity among sources. For this purpose, we have selected the following datasets as candidate\ndatasets: ALPACA (Taori et al., 2023), OPEN-ASSISTANT (K\u00f6pf et al., 2023), STACKEXCHANGE,\nand WIKIHOW. Due to the varying formats, sizes, and distributions of different datasets, we have\napplied distinct processing procedures to each dataset. Table 2 provides an overview of the candidate\ntraining datasets after preprocessing. As mentioned in section 3.1, we merged candidate training\ndatasets, resulting in each dataset containing 1,000 instruction-output pairs. We generated a random\nnumber ri for each dataset and randomly selecting 1000 \u2217 ri/ P\ni ri samples from each dataset\nfor combination. Besides, considering the significant size difference between ALPACA and other\ncandidate datasets, we randomly sampled 2,000 data examples from ALPACA to maintain scale\nconsistency across all the candidate datasets.\nTest set for rule fitting.\nTo address real-world requirements, we use the SELF-INSTRUCT dataset\nWang et al. (2022a), which contains 252 instructions as rule-fitting test set. Considering evaluation\nefficiency, we randomly sampled 80 instructions from the whole dataset as our evaluation set. In our\nstudy, we employed gpt-4 from OPENAI to generate response for each instruction.\nDatasets for rule evaluation.\nWe further test INSTRUCTMINING by using it to select high-quality\nexamples from unseen datasets for finetuning large language models. During evaluation, we mainly\nuse OPENORCA and DOLLY-15K as the two unseen candidate datasets. For OPENORCA, given its\nextensive size, we randomly select 50,000 examples from OPENORCA-GPT3.5 and 50,000 examples\nfrom OPENORCA-GPT4 for experimentation (henceforce refered to as OPENORCA). To make sure\nthat our method does not overfit on the SELF-INSTRUCT evaluation set, we use the gpt-4 labeled\nMT-BENCH dataset (Zheng et al., 2023) as an unseen evaluation set. To be noticed, since our candidate\nand evalation datasets does not include multi-turn examples, when evaluating on MT-BENCH, we only\nuse the first turn of MT-BENCH dataset.\n3.3\nFINETUNING SETTINGS\nWe conduct all finetuning experiments on the same base model LLAMA-2-7B (Touvron et al.,\n2023). All finetuning datasets during estimation phase are of the same size, 1,000 examples in\neach. We run model finetuning for 3 epochs, with per step batch size set to 128. We use Adam with\n\u03b21 = 0.9, \u03b22 = 0.999, and cosine learning rate scheduler starts from 2e \u2212 5, decays to 0. Each model\nfinetuned during estimation phase is evaluated on the evaluation dataset mentioned in section 3.2. We\nrun all finetuning and evaluation experiments on a NVIDIA A100 80G GPU cluster, with 8 A100\nGPUs used in each experiment.\n5\nPreprint\n4\nEXPERIMENTAL RESULTS\n4.1\nINSTRUCTMINING PARAMETER FIT\nFollowing Section 3.1, we randomly sampled 129 subsets from the entire data pool with different\npercentages. These subsets are then used to finetuned 129 corresponding language models. We\ncollect the inference loss and indicator values for each subset. To select the optimal rule, we first\nchoose regression results with the highest R2, and then prioritize the rule with the most significant p\nvalues. The detailed regression result is available in Table 9. Based on this result, we delineate our\nestimated evaluation function, which is articulated as Equation 4. Accordingly, reward score and\nunieval scores appear to be the most significant indicators in the quality rule. This estimation result\nreveals that Und is in negative correlation with data quality, while the other three indicators are of\npositive correlation with data quality.\nQD|M,S \u221d \u2212L(Mft, Deval)\nlog L(Mft, Deval) \u221d 0.0274 \u2212 0.0078Rew + 0.4421 \u2217 Und \u2212 0.3212 \u2217 Nat \u2212 0.1520 \u2217 Coh + \u03f5\n(4)\n4.2\nQUALITY-GUIDED INSTRUCTION SELECTION\nWe follow the estimated INSTRUCTMINING rule in Equation 4 to select high quality examples\nfrom two unseen datasets, OPENORCA (Lian et al., 2023) and databricks-dolly-15k 1. The\nexperiments are all based on LLAMA-2-7B model. We first elaborate our BLENDSEARCH results.\nThen we present our evaluation results of various model finetuned during this process. These models\nare first evaluated on two evaluation sets, SELF-INSTRUCT and MT-BENCH. The searched best\nfinetuned model is then assessed using LLM-JUDGE and OPENLLM benchmarks.\n0\n10\n20\n30\n40\nSteps\n2\n4\n6\n8\nData size(k)\nSteps w.r.t. Data size\n0\n10\n20\n30\n40\nSteps\n0.70\n0.71\n0.72\n0.73\nLoss\nSteps w.r.t. Loss\n6.5\n7.0\n7.5\n8.0\n8.5\n9.0\nlog(data size)\n0.70\n0.71\n0.72\n0.73\n0.74\nLoss\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n42 41\n43\n44\n45\n46\nSearch Procedure\nFigure 2: BLENDSEARCH results. The Loss is calculated on MT-BENCH evaluation set.\nBlendSearch results.\nIn response to the performance decline-rise phenomenon with increasing\ntraining data, we conducte a BLENDSEARCH within a range of data sizes from 512 to 10,000. Our\nprior empirical findings suggest that a maximum dataset size of 10,000 is sufficient for optimizing\nthe data size. Figure 2 details the search procedure from the perspective of steps.\nLoss results.\nWe first evaluate the finetuned models using inference loss on SELF-INSTRUCT and\nMT-BENCH dataset. Results are presented in Table 3. According to the results, INSTRUCTMINING\ncan efficiently select high-quality data from various unseen datasets. Full data finetuning on LLAMA-\n2-7B with OPENORCA dataset can take up to 30 hours of 8 GPU time. With INSTRUCTMINING, we\nare able to select the top 1,000 data examples in around two hours and training a better LLM within\n15 minutes. This is also valid with DOLLY. In addition, we discover that despite different sampling\nmethods, the loss values on both SELF-INSTRUCT and MT-BENCH always tend to increase with larger\ndata sizes. To study this phenomenon, we designed further experiments to investigate the relationship\nbetween finetuning data size and LLM performance. Details are provided in section 5.1.\nLLM assessment.\nWe next compare our best model with other state-of-the-art models using LLM-\nAS-A-JUDGE. We let GPT-4 choose the better answer between two responses generated by two\n1https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\n6\nPreprint\nDataset\nSampling Method\nTotal Time(min)\nRule\nData Size\nLoss\nLoss(MT-BENCH)\nOpenOrca\nSelected\n150(Rule)+15(Train)\n-0.1347\n1,000\n0.958\n0.711\n150(Rule)+300(Train)\n-0.0716\n20,000\n0.991\n0.730\n150(Rule)+1350(Train)\n-0.0243\n90,000\n1.014\n0.735\nBlendSearch\n150(Rule)+35(Train)\n-0.1197\n2,532\n0.973\n0.699\nRandom\n15(Train)\n-0.0195\n1,000\n1.001\n0.746\n300(Train)\n-0.0180\n20,000\n0.991\n0.751\n1350(Train)\n-0.0176\n90,000\n1.010\n0.763\nDolly\nSelected\n22(Rule)+15(Train)\n-0.0969\n1,000\n1.0429\n0.7964\n22(Rule)+75(Train)\n-0.0622\n5,000\n1.0327\n0.7847\n22(Rule)+150(Train)\n-0.0449\n10,000\n1.0371\n0.8001\nBlendSearch\n22(Rule)+35(Train)\n-0.0770\n2,648\n1.0160\n0.7746\nRandom\n15(Train)\n-0.0286\n1,000\n1.0409\n0.8215\n75(Train)\n-0.0289\n5,000\n1.0331\n0.7910\n150(Train)\n-0.0293\n10,000\n1.0356\n0.8086\nTable 3: Quality-guided instruction selection experiment result. Rule refers to the average of our\nquality rule score on the dataset. Selected k data refers to the top k data examples with the highest\nquality scores. We calculate the inference loss values over two evaluation sets, SELF-INSTRUCT\nand MT-BENCH. Total time refers to the time spent during feature extraction, rule calculation, and\nfinetuning using 8 GPUs.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRandom select\nGPT-3.5-turbo\nVicuna-7B-v1.5\nLLaMA-2-7B\nLLaMA-2-7B-chat\n48.97%\n17.89%\n38.12%\n45.47%\n37.22%\n20.97%\n63.22%\n35.33%\n21.67%\n51.81%\n30.06%\n18.89%\n26.55%\n32.86%\n10.97%\nTie\nLose\nWin\n(a) GPT-4 preference evaluated results. Tie means GPT-\n4 assesses two responses as equal. Lose means GPT-4\nprefers the other model\u2019s response. Win means GPT-4\nprefers INSTRUCTMINING model response.\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\nmodel\nInstructMining-7B\nGPT-3.5-turbo\nLlama-2-7B\n(b) GPT-4 assessed model ability result. We pre-\npared tasks from different categories in MT-BENCH,\nand let GPT-4 to evaluate the generated response.\nFigure 3: LLM assessed results.\ndifferent models. According to the results presented in Figure 3, our model is able to generate better\nor equal results in 64.67% of the cases compared to VICUNA-1.5-7B. We also let GPT-4 assess the\nmodel from different perspectives. According to Figure 3b, our model significantly improves the\noriginal LLAMA-2 model\u2019s ability in writing, roleplay, humanity, STEM, extraction and coding.\nOpenLLM benchmark results. Besides, we further test our finetuned models on the widely used\nOPENLLM benchmark (Gao et al., 2021). OPENLLM benchmark is composed of four widely\nused general question answering benchmarks, ARC (Clark et al., 2018), HELLASWAG (Zellers\net al., 2019), MMLU (Hendrycks et al., 2020) and TRUTHFULQA (Lin et al., 2021). During\nexperimentation, we align our inference settings with huggingface OPENLLM leaderboard settings.\nResults are available in Table 4. Notably, INSTRUCTMINING finetuned models can achieve similar\nperformance compared to STABLEBELUGA-7B, which is the state-of-art LLAMA-2-7B based model\non OPENLLM leaderboard. Furthermore, INSTRUCTMINING only requires around two hours of\nindicator inference and ten hours of finetuning to get a comparably strong language model. We\nalso discover that, when evaluating with some metrics, larger data does not always promise better\nperformance. For instance, accuracy on ARC tends to decrease when the data size increases. Further\nanalysis of this phenomenon is provided in section 5.1.\n7\nPreprint\nModel\nData size\nAvg. Metric\nARC\nHellaSwag\nMMLU\nTruthfulQA\nINSTRUCTMINING-Selected\n10,000\n58.65\n56.66\n79.77\n49.89\n48.26\nINSTRUCTMINING-Selected\n40,000\n59.25\n54.44\n80.11\n52.60\n49.83\nINSTRUCTMINING-Random\n10,000\n58.74\n54.78\n79.58\n49.02\n51.58\nINSTRUCTMINING-Random\n40,000\n58.95\n54.78\n79.89\n51.16\n49.95\nVICUNA-1.5-7B\n125,000\n57.99\n53.24\n77.39\n51.03\n50.33\nLLAMA-2-7B-chat\n27,540+\n56.34\n52.90\n78.55\n48.32\n45.57\nLLAMA-2-7B\n-\n54.32\n53.07\n78.59\n46.87\n38.76\nSTABLEBELUGA-7B\n600,000\n59.59\n56.31\n79.14\n52.71\n50.19\nTable 4: OPENLLM benchmark scores. We use the same evaluation setting as OPENLLM leaderboard.\nFor ARC benchmark, we use 25 few shots. For HELLASWAG, we use 10 shots. For MMLU, we use\n5 shots. For TRUTHFULQA, we use zero shot.\nRew\nUnd\nNat\nCoh\nLoss(SELF-INSTRUCT)\nLoss(MT-BENCH)\n\u2713\n\u2713\n\u2713\n\u2713\n0.958\n0.711\n\u2717\n\u2713\n\u2713\n\u2713\n0.988 (\u21910.030)\n0.762 (\u21910.051)\n\u2713\n\u2717\n\u2713\n\u2713\n0.989 (\u21910.031)\n0.746 (\u21910.035)\n\u2713\n\u2713\n\u2717\n\u2713\n0.977 (\u21910.019)\n0.742 (\u21910.031)\n\u2713\n\u2713\n\u2713\n\u2717\n0.969 (\u21910.011)\n0.742 (\u21910.031)\n\u2717\n\u2717\n\u2717\n\u2717\n1.001(\u21910.043)\n0.746(\u21910.035)\nTable 5: Ablation study result. All results are compared with the original INSTRUCTMINING rule.\nThe final row refers to unfiltered randomly selected data.\n4.3\nABLATION STUDY\nWe further conduct ablation experiments to study the influence of every indicator in INSTRUCTMIN-\nING. To do this, we first remove one indicator from the current rule and estimate a new rule using the\nother three indicators, based on the original random experiment results. Then, we use this new rule\nto select 1,000 data examples with the highest scores. These 1,000 data examples are later used to\nfinetune the base language model, LLAMA-2-7B, for three epochs. We present ablation study result\nin Table 5. Accordingly, Rew appears to be the most important indicator among the four. Without\nRew as one of the rule indicators, the estimated rule results in an increase of 0.03 in SELF-INSTRUCT\ninference loss and 0.051 in MT-BENCH inference loss.\n5\nANALYSIS\n5.1\nDOUBLE DESCENT IN GENERATIVE LANGUAGE MODELS\n0\n20\n40\n60\n80\ndata size(k)\n0.96\n0.99\n1.02\n1.05\n1.08\nloss\nself-instruct\n2.5\n5.0\n0.975\n1.000\nRandom\nSelect\n0\n20\n40\n60\n80\ndata size(k)\n0.72\n0.76\n0.80\n0.84\nloss\nmt-bench\n2.5\n5.0\n0.72\n0.74\nRandom\nSelect\n0\n20\n40\n60\n80\ndata size(k)\n0.55\n0.56\n0.57\n0.58\n0.59\nscore\nOpenLLM\n2.5\n5.0\n0.56\n0.58\nRandom\nSelect\n0\n20\n40\n60\n80\ndata size(k)\n0.425\n0.450\n0.475\n0.500\n0.525\nscore\nMMLU\n2.5\n5.0\n0.450\n0.475\n0.500\nRandom\nSelect\nFigure 4: Double descent in generative language models. Models are evaluated using four metrics:\nloss on SELF-INSTRUCT, loss on MT-BENCH, OPENLLM scores and MMLU scores..\nIn this section, we present further experimental findings on OpenOrca dataset. In previous experi-\nments, we find out that a language model\u2019s performance can be influenced by both finetuning data\nquality and quantity. When data quantity increases, generative language models\u2019 performance does\nnot promise to become better. This phenomenon suggests a balance between data quantity and data\n8\nPreprint\nquality. Results are presented in Figure 4. This reveals some interesting emergent phenomena when\nfinetuning large language models. We detail the observed phenomena below.\nPhenomenon 1 Non-monotonicity exists in language model performance. As we increase the\ntraining data size, language model performance first becomes better then gets worse. When data size\nincreases to a certain level, performance becomes better again.\nBased on Figure 4, we observe that the performance first improves as the training data size grows.\nThen, after the data size grows to around 10,000, loss begins to increase, meaning that language\nmodel performance worsens. Finally, as data size continues to grow, language model\u2019s performance\nimproves. This phenomenon is similar to the double descent phenomenon (Nakkiran et al., 2021) that\nnon-monotonicity exists with varying numbers of training samples. In our experiment, we observe\nthat this phenomenon not only exists in vanilla language model training but also in large generative\nlanguage model finetuning.\nPhenomenon 2 Balance point exists between randomly selected and quality selected data. As data\nsize grows, data quality becomes a less important factor for model performance.\nGiven Figure 4, we find out that when data size grows to a certain point, the performance curve\nof selected data will always intersect with the random one. Besides, the distance between the two\ndecreases as the data size increases. This phenomenon indicates that data quality measure can help\nimprove model performance at first. However, data quality becomes less important when data size\ngrows to a certain level.\n5.2\nROBUSTNESS\nTo further explore the effectiveness of INSTRUCTMINING, we evaluate it across three different\nfinetuning settings: (1) Different base models. We change the original base model LLAMA-2-7B\ninto LLAMA-1-7B to test whether our method is scalable to other models. (2) Different model sizes.\nWe change the original 7B model size into 13B to test whether our method is scalable to other model\nsizes. (3) Parameter efficient settings. LORA (Hu et al., 2021), a parameter efficient method, is\nwidely used when finetuning a large language model to help save GPU memory usage. We also test\nour method with LORA settings to see whether INSTRUCTMINING is scalable to parameter efficient\nfinetuning. Results are presented in Table 6. As the data shows, INSTRUCTMINING rule can be\napplied to various base models, model sizes and parameter efficient settings.\nBase Model\nModel Size\nLoRA\nSampling Method\nLoss(SELF-INSTRUCT)\nLOSS(MT-BENCH)\nLLAMA-2\n13B\n\u2717\nSelected\n0.8748\n0.6531\nRandom\n0.8983\n0.6589\nLLAMA-1\n7B\n\u2717\nSelected\n1.013\n0.798\nRandom\n1.056\n0.844\nLLAMA-2\n7B\n\u2713\nSelected\n1.0698\n0.8624\nRandom\n1.0700\n0.8631\nTable 6: Robustness test result.\n6\nRELATED WORK\nInstruction tuning. Recent studies have explored instruction tuning as a method for fine-tuning\nLLMs, enhancing their ability to generalize to unseen instructions (Wei et al., 2021). Reinforcement\nlearning from human feedback (RLHF) is popular method that aligns language models with human\nintent (Ouyang et al., 2022). To further improve instruction tuning, some work choosed to increase the\nsize of the data (Honovich et al., 2022; Wang et al., 2022a). Besides, Zhou et al. (2023) demonstrated\nthat utilizing a smaller volume of high-quality instruction data can still produce effective models.\nInstruction evaluation. The field has experienced growth with the publication of numerous instruc-\ntion datasets (Taori et al., 2023; K\u00f6pf et al., 2023; Honovich et al., 2022). Chung et al. (2022) first\ncombined multiple datasets to augment both the quantity and diversity of instruction data, achieving\nnotable performance gains. Some newest works suggest that enhancing instruction diversity can also\nsignificantly improve instruction tuning performance (Iyer et al., 2023; Wang et al., 2023; 2022b;\nLongpre et al., 2023). Meanwhile, Gunasekar et al. (2023) have demonstrated that an increased\n9\nPreprint\nproportion of high-quality data can yield enhanced performance. Other works focused on estimating\nthe quality of prompts, Chen et al. (2023) use prompting to LLM API as an auto-grader of data\nquality, Gonen et al. (2022) use perplexity for prompt selection.\n7\nCONCLUSION\nIn this paper, we propose high-quality example selection method. Experiments have been conducted\nto estimate this rule\u2019s parameter and prove that our evaluation rule is valid and scalable to other\nfinetuning settings. Besides, we present our observation of the double descent phenomenon in\nlangauge model finetuning. Based on this finding, we further applied BLENDSEARCH to search for\nthe best subset. Results show that INSTRUCTMINING rule is valid and scalable.\nREFERENCES\n\u00c5ke Bj\u00f6rck. Least squares methods. Handbook of numerical analysis, 1:465\u2013652, 1990.\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay\nSrinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca with\nfewer data, 2023.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nWei Dong, Charikar Moses, and Kai Li. Efficient k-nearest neighbor graph construction for generic\nsimilarity measures. In Proceedings of the 20th international conference on World wide web, pp.\n577\u2013586, 2011.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Gold-\ning, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang,\nAnish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model\nevaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. Demystifying prompts\nin language models via perplexity estimation, 2022.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor, 2022.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O\u2019Horo, Gabriel Pereyra,\nJeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. Opt-iml:\nScaling language model instruction meta learning through the lens of generalization, 2023.\n10\nPreprint\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and\nAlexander Mattick. Openassistant conversations \u2013 democratizing large language model alignment,\n2023.\nWing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\".\nOpenorca: An open dataset of gpt augmented flan reasoning traces. https://https://\nhuggingface.co/Open-Orca/OpenOrca, 2023.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958, 2021.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.\nLe, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods\nfor effective instruction tuning, 2023.\nPhilip M McCarthy and Scott Jarvis. Mtld, vocd-d, and hd-d: A validation study of sophisticated\napproaches to lexical diversity assessment. Behavior research methods, 42(2):381\u2013392, 2010.\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep\ndouble descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory\nand Experiment, 2021(12):124003, 2021.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277, 2023.\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.\n10084.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023.\nChi Wang, Qingyun Wu, Silu Huang, and Amin Saied. Economical hyperparameter optimization\nwith blended search strategy. In ICLR\u201921, 2021a.\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl\nlibrary, 2021b.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022a.\n11\nPreprint\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana\nArunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar,\nMirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang\nKaria, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro,\nTanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and\nDaniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on 1600+\nnlp tasks, 2022b.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far\ncan camels go? exploring the state of instruction tuning on open resources, 2023.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. Towards a unified multi-dimensional evaluator for text generation, 2022.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n12\nPreprint\nA\nSEARCH PROCEDURE\nCandidate Datasets\n\u2026\u2026\nfuse\nsample \u2026\u2026\nfinetune\n\u2026\u2026\nlog \ud835\udc3f \ud835\udc40!\", \ud835\udc37#$%& \u2243 \ud835\udc3f' + \ud835\udc39{\ud835\udc3c( \ud835\udc37 , \ud835\udc3c) \ud835\udc37 , \u22ef , \ud835\udc3c*(\ud835\udc37)}\nIndicator\nInference\nRule Estimation\nData Selection\nIndicator\nInference\nDataset\nRated Dataset\nInstructMining Rule\nSelected Dataset\nfinetune\nBase Model\nFigure 5: Our data selection pipeline. Rule estimation: We first select several candidate datasets.\nThen, we fuse and sample from them to form datasets of different quality levels. For each dataset,\nwe finetune a language model and evaluate the model on a shared evaluation set. We also calculate\nbag of indicator values on the dataset. Finally, we perform a linear regression analysis based on our\ncurated experiment results to estimate the linear rule parameters. Data Selection: With the estimated\nINSTRUCTMINING rule, we first calculate the rule values to assess each example in the dataset. Then,\nwe rank the dataset according to quality scores. We apply FLAML to do BLENDSEARCH. Finally, we\nuse the searched dataset to finetune a language model.\nB\nINDICATOR DESCRIPTIVE ANALYSIS\nTo provide more details on natural language indicators, we present further descriptive analysis results\non these indicators. We calculate the indicator values across the 129 sampled subsets. Figure 6\npresents indicator distribution graphs.\nIn addition, to make sure that statistical regression is valid in this paper, we perform Kolmogorov-\nSmirnov(KS) test on every indicator. Test results are provided in Table 7. According to the results,\nthe indicators we use in this paper follow normal distribution.\n0\n50\n100\n150\ninput_length\n0.000\n0.005\n0.010\n0.015\n0.020\nDensity\ninput_length\n100\n200\n300\n400\n500\noutput_length\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nDensity\noutput_length\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nunderstandability\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nDensity\nunderstandability\n0.6\n0.7\n0.8\n0.9\nnaturalness\n0\n2\n4\n6\n8\n10\nDensity\nnaturalness\n0.85\n0.90\n0.95\ncoherence\n0\n5\n10\n15\n20\nDensity\ncoherence\n1\n0\n1\n2\n3\n4\npythia-reward\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDensity\npythia-reward\n50\n60\n70\n80\nfirst_round_mtld\n0.00\n0.02\n0.04\n0.06\n0.08\nDensity\nfirst_round_mtld\n1.02\n1.04\n1.06\n1.08\n1.10\nknn_6\n0\n10\n20\n30\nDensity\nknn_6\nFigure 6: Distribution graph of natural language indicators.\n13\nPreprint\nIndicator\nStatistics\np Value\ninput_length\n1.0\n0.0\u2217\u2217\u2217\noutput_length\n1.0\n0.0\u2217\u2217\u2217\nunderstandability\n0.765\n1.25e-50\u2217\u2217\u2217\nnaturalness\n0.744\n3.03e-47\u2217\u2217\u2217\ncoherence\n0.814\n7.89e-60\u2217\u2217\u2217\npythia-reward\n0.657\n3.17e-35\u2217\u2217\u2217\nmtld\n1.0\n0.0\u2217\u2217\u2217\nknn_6\n0.85\n7.77e-68\u2217\u2217\u2217\nperplexity\n0.997\n3.34e-202\u2217\u2217\u2217\nTable 7: KS test results for all variables in linear regression. Smaller p value indicates that the variable\nis highly possible to follow normal distribution. \u2217 refers to p \u2264 0.10, \u2217\u2217 refers to p \u2264 0.05, and \u2217\u2217\u2217\nrefers to p \u2264 0.01.\nDataset\nQuality\nModel size\nLoss(SELF-INSTRUCT)\nLoss(MT-BENCH)\nORCA-GPT4\nHigh\n7B\n0.9547\n0.7118\nORCA-GPT3.5\nNormal\n7B\n1.0282\n0.7513\nALPACA\nNormal\n7B\n0.9998\n0.7760\nDOLLY\nNormal\n7B\n1.0409\n0.8215\nORCA-fused\nHigh\n13B\n0.8982\n0.6589\nORCA-fused\nHigh\n7B\n1.0007\n0.7461\nTable 8: Empirical test of loss.\nC\nEMPIRICAL TEST OF INSTRUCTION QUALITY EVALUATION HYPOTHESIS\nTo investigate whether inference loss can serve as a suitable indicator of model capability and data\nquality, we conduct further finetuning experiments. We randomly select 1 1,000 examples from four\ndatasets with different quality levels and finetune LLAMA-2-7B model on the selected datasets. We\nalso finetune LLAMA-2-7B and LLAMA-2-13B models using 1,000 examples from ORCA-fused\ndataset. Results are provided in Table 8. As shown in the table, GPT-4 labeled datasets tend to yield\nlower loss on the two evaluation sets. Finetuned models with larger model size also yield lower loss\non the evaluation sets. Hence, we suppose that evaluation loss can serve as a suitable indicator of\nmodel capability and data quality.\nD\nOTHER EMERGENT PHENOMENA\nIn this section, we present our analysis of other emergent phenomena in this paper. Except for\nregression test, we further conduct correlation test between indicator values and loss values. We plot\nregression analysis results in Figure 7. We detail other discovered phenomena below.\nPhenomenon 3 Perplexity is negatively correlated with data quality.\nIn general, a higher perplexity score corresponds to increased inference loss, suggesting a potential\ndecline in data quality. Typically, elevated perplexity signifies that the data instance was infrequently\nencountered during pretraining. Such instances may fall outside the bounds of conventional natural\nlanguage.\nPhenomenon 4 Reward score is positively correlated with data quality.\nIn this paper, we employ the oasst-pythia reward model to assign reward scores to individual\ndata examples. Reward models are typically trained using human preference labels, suggesting their\ncapacity for human-like evaluations. Thus, examples preferred by the reward model are typically of\nhigh quality.\n14\nPreprint\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nperplexity\n0.975\n1.000\n1.025\n1.050\nloss\nPPL\n0\n1\n2\n3\n4\n5\nreward score\n0.950\n0.975\n1.000\n1.025\n1.050\nloss\nReward\n100\n200\n300\n400\n500\noutput_length\n0.96\n0.98\n1.00\n1.02\n1.04\nloss\nOutput Length\n0.70\n0.75\n0.80\n0.85\n0.90\nunderstandability\n0.96\n0.98\n1.00\n1.02\n1.04\nloss\nUnieval-Und\nFigure 7: Univariate analysis regression plot. we plot 4 indicators value w.r.t. the actual evaluation\nloss. For every indicator we estimate a univariate linear function between loss and indicator.\nPhenomenon 5 Output length is negatively correlated with data quality.\nWhen the number of tokens increases, loss tends to increase which means that data quality tends to\ndecrease. This can be due to the maximum sequence length in language models. LLAMA-2-7B has\n4096 maximum sequence length. For some very long instances, it is possible that the sentence is\ntruncated in the middle during preprocessing.\nPhenomenon 6 Understandability is negatively correlated with data quality.\nUnieval-understandability normally represents the complexity of a sentence. When the complexity\nincreases, data quality decreases. This is possibly due to some translation-related instructions in the\ndataset, which requires the unieval model to command multilingual assessment ability. However,\nthese examples are not common in our evaluation set, which might result in the negative correlation\nbetween the two.\nE\nMORE DETAILS ON PARAMETER FITTING\nVariable\nCoef.\nStd err.\nt value\nP > |t|\n\u03b20\n0.0274\n0.061\n0.453\n0.651\n\u03b2P P L\n-\n-\n-\n-\n\u03b2MT LD\n-\n-\n-\n-\n\u03b2Rew\n-0.0078\n0.003\n-2.561\n0.012\u2217\u2217\n\u03b2Len\n-\n-\n-\n-\n\u03b2Nat\n-0.3212\n0.107\n-3.005\n0.003\u2217\u2217\u2217\n\u03b2Coh\n-0.1520\n0.129\n-1.180\n0.240\n\u03b2Und\n0.4421\n0.168\n2.639\n0.009\u2217\u2217\u2217\n\u03b2Knn6\n\u2217\u2217\u2217\n-\n-\n-\n-\n[1] R2=0.522, F-statistic=33.84.\n[2] Prob(F-statistic)=4.52e-19, Log-Likelihood=435.11.\n[3] *: p \u22640.1, **:p \u22640.05, ***:p \u22640.01.\nTable 9: Linear regression parameter estimation results using ordinary least squares (OLS). P > |t|\nrepresents p value under student test on each coeficient. Lower p value indicating that the coeficient\nfor this variable is more significant and acceptable. R2 and adjusted R2 represents how well the data\nis fit using the estimated linear function.\nAs shown in Table 9, we present our statistical regression result for our estimated rule of Equation 4\nin Table 9. We select the indicators first through R2 and then through p value, to make sure that\nour regression result is valid and effective. According to the regression result, Rew, Nat and Und\nappears to be the most prominent indicators in this rule. This estimation result reveals that Und is in\nnegative correlation with data quality, while the other three indicators are of positive correlation with\ndata quality.\n15\nPreprint\nF\nCASE STUDY\nIn this section, we present case studies on INSTRUCTMINING rule and our finetuned model.\nF.1\nRULE SELECTED EXAMPLES\nWe give examples of INSTRUCTMINING selected high-quality data examples and low quality data\nexamples in Table 10, Table 11 and Table 12.\nExample 1\nInstruction: Teacher: Given reviews from Amazon, classify those review based on\ntheir content into two classes: Negative or Positive. Now, understand the problem?\nInput: This is not waterproof. Its GARBAGE where batteries go in broke, while trying\nto adj. DO NOT BUY. You turn the end to control it, that loosens the battery cover where\nwater gets in so how is it waterproff????\nOutput: Yes, I understand the problem. The task is to classify the given review as either\nnegative or positive based on its content.\nIn this instance, the review contains negative content. The reviewer is expressing\ndissatisfaction with the product and warning others not to buy it. The reviewer specifically\nmentions that the product is not waterproof and that the battery cover is broken, which\nallows water to get in. The use of all caps and exclamation marks also indicates a strong\nnegative sentiment. Therefore, I would classify this review as negative.\nInstructMining score: 0.7902 (Lower, which means better)\nTable 10: Rule selected example 1. In this example, the assistant completed the task with good\ninteraction.\nExample 2\nInstruction: Given the question: Answer the following question given this paragraph:\nSunlight contains the complete range of wavelengths of electromagnetic waves.\nThe entire range is called the electromagnetic spectrum.\nQ: What do you call the entire range of wavelengths of electromagnetic waves?\nChoices:\n- electric spectrum\n- gaseous spectrum\n- molecular spectrum\n- electromagnetic spectrum\nA: The answer is:\nOutput: Electromagnetic spectrum.\nInstructMining score: 0.9179 (Medium)\nTable 11: Rule selected example 2. In this example, the assistant completed the task without suitable\ninteraction with human.\n16\nPreprint\nExample 3\nInstruction: It is best to work directly with the manufacturer to get the best price. Add\npunctuation.\nOutput: It is best to work directly with the manufacturer, to get the best price.\nInstructMining score: 1.019 (Higher, which means worse)\nTable 12: Rule selected example 3. In this example, the assistant didn\u2019t complete the task.\nF.2\nLANGUAGE MODEL RESPONSE EXAMPLES\nIn this section, we present some prompt examples of our finetuned model. We leverage tasks from\nMT-BENCH dataset to demonstrate the model ability from various perspectives.\nExample 1 Writing ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: Edit the following paragraph to correct any grammatical errors:\nShe didn\u2019t remembre where is her purse, so I thinks its in the car but he\u2019s say it\u2019s on kitchen\ntable but he are not sure, and then they asked me to looking for it, she\u2019s say, \"Can you?\",\nand I responds with, \"Maybe, but ain\u2019t no sure,\" and he not heard me, and, \"What?\",\nhe asks, \"Did you found it?\".\nAssistant: \"Sure, I can help you with that. Here\u2019s the corrected paragraph:\n\"She didn\u2019t remember where her purse was, so I think it\u2019s in the car but he\u2019s not sure, and then\nthey asked me to look for it, she said, \u2019Can you?\u2019, and I responded with, \u2019Maybe, but I\u2019m not sure,\u2019\nand he didn\u2019t hear me, and then he asked, \u2019What?\u2019, he asked, \u2019Did you find it?\u2019\" I made the following\nchanges:\n1. Added a comma after \"car\" to separate the two independent clauses in the sentence.\n2. Replaced \"ain\u2019t\" with \"are not sure\" to improve the sentence\u2019s grammar and vocabulary.\n3. Added a comma after \"he\u2019s not sure\" to separate the two items in the list.\n4. Added a comma after \"she said\" to separate the quotation marks from the rest of the sentence.\n5. Added a comma after \"he asked\" to separate the two items in the list.\n6. Added a period after \"he asked\" to indicate the end of the sentence\".\nTable 13: Example of model writing ability.\nExample 2 Roleplay ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: Embrace the role of Sheldon from \"The Big Bang Theory\" as we delve into our conversation.\nDon\u2019t start with phrases like \"As Sheldon\". Let\u2019s kick things off with the following question:\n\"What is your opinion on hand dryers?\"\nAssistant: As Sheldon, I am a fan of hand dryers. They are efficient, fast, and reduce the\namount of time spent drying your hands after washing them. I also appreciate the fact that\nthey are environmentally friendly as they use minimal water compared to paper towel dispensers.\nHowever, I do have some reservations about the noise level of some hand dryers,\nwhich can be quite loud and disruptive in public restrooms.\nOverall, I think hand dryers are a great option for public restrooms and I would recommend them\nto anyone looking for a fast and eco-friendly way to dry their hands\nTable 14: Example of model roleplay ability.\n17\nPreprint\nExample 3 Reasoning ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: One morning after sunrise, Suresh was standing facing a pole. The shadow of the pole\nfell exactly to his right. Can you tell me the direction towards which the shadow was pointing\n- east, south, west, or north? Explain your reasoning steps.\nAssistant: To determine the direction of the shadow, we need to consider the position of\nthe sun in the sky. Since sunrise occurs before sunset, we can assume that the sun was rising\nin the east. Therefore, the shadow would be pointing towards the east.\nReasoning steps:\n1. Suresh was standing facing a pole.\n2. The shadow of the pole fell exactly to his right.\n3. The sun rises in the east.\n4. The shadow would be pointing towards the east\nTable 15: Example of model reasoning ability.\nExample 4 Math ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: A tech startup invests $8000 in software development in the first year, and then invests\nhalf of that amount in software development in the second year.\nWhat\u2019s the total amount the startup invested in software development over the two years?\nAssistant: To find the total amount the startup invested in software development over the\ntwo years, we need to calculate the investment for the first year and then add it to the investment\nfor the second year.\nFirst year investment: $8000\nSecond year investment: Half of the first year investment = $8000/2 = $4000\nTotal investment for two years:\nFirst year investment + Second year investment = $8000 + $4000 = $12000\nSo, the total amount the startup invested in software development over the two years was $12000\nTable 16: Example of model math ability.\n18\nPreprint\nExample 5 Extraction ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: Given these categories - Literature, History, Science, and Art. Please analyze the following\nquestions and assign them to one of these categories. In your response, refrain from uttering any\nextraneous words. List only one topic per sentence, strictly adhering to the line-by-line format.\n1. Discuss the main themes and stylistic techniques employed by Leo Tolstoy in \u2019War and Peace.\u2019\nHow do they align with the wider social context of 19th-century Russia?\n2. Analyze the geopolitical strategies and domestic policies adopted by the US President during\nWorld War II. How did these actions shape the post-war international order?\n3. Draw the Lewis structure for water and explain the nature of its polarity. How does this influence\nits unique properties such as high boiling point and capacity to dissolve many substances?\n4. Critically examine the artistic techniques and stylistic choices Leonardo da Vinci employed\nin \u2019Mona Lisa.\u2019 How does the painting reflect the cultural and philosophical milieu of the Italian\nRenaissance?\nAssistant: Literatur\nTable 17: Example of model extraction ability.\nExample 6 STEM ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: What is the central dogma of molecular biology? What processes are involved?\nWho named this?\nAssistant: The central dogma of molecular biology is a set of principles that describe the\nflow of genetic information within a living organism. The central dogma is as follows:\n1. DNA (Deoxyribonucleic acid) is the genetic material that carries the instructions for\nmaking proteins.\n2. RNA (Ribonucleic acid) is formed from DNA and carries the genetic information to\nthe cell\u2019s ribosomes, where proteins are made.\n3. The sequence of nucleotides in RNA determines the sequence of amino acids in the protein.\nThe central dogma of molecular biology was first proposed by Francis Crick and\nJames D. Watson in their 1953 paper, \"Molecular Structure of Nucleic Acids: A Structure for\nDeoxyribose Nucleic Acid.\" They proposed that DNA is the genetic material that carries\nthe information for making proteins, and RNA is formed from DNA and carries the genetic\ninformation to the cell\u2019s ribosomes, where proteins are made. The sequence of nucleotides\nin RNA determines the sequence of amino acids in the protein.\nThe central dogma of molecular biology is a fundamental concept in biology and has\nbeen instrumental in the understanding of genetic information flow, gene expression,\nand protein synthesis. It has also led to the development of various molecular biology\ntechniques, such as DNA sequencing, RNA interference, and recombinant DNA technology\nTable 18: Example of model STEM ability.\n19\nPreprint\nExample 7 Humanities ability\nSystem Message: A chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions.\nUser: Create a lesson plan that integrates drama, mime or theater techniques into a history class.\nDuration: 3 class periods (each lasts for 45 minutes) for 3 days\nTopic: Opium Wars between China and Britain\nGrade level: 9-10\nAssistant: Alright, little buddy! So, we have a history class where they\u2019re learning about\nthe Opium Wars between China and Britain. We want to add some fun and make it\nmore interesting with drama, mime, or theater techniques. Here\u2019s what we can do:\n1. Day 1: Introduce the topic and the three techniques. Explain that they will be using drama,\nmime, and theater techniques to learn about the Opium Wars.\n2. Day 2: Have the students work in groups to research the Opium Wars. They can use their\nphones, tablets, or laptops to find information and pictures.\n3. Day 3: Divide the class into three groups: drama, mime, and theater. Each group will create a\nshort skit, mime, or theater performance based on what they learned about the Opium Wars.\n4. Day 4: Each group will present their performance to the class. The class will watch and give\nfeedback to the other groups.\n5. Day 5: Review the key points of the Opium Wars and discuss how the drama, mime, or theater\ntechniques helped them understand the topic better.\nRemember, this is just an example, and you can change it to fit your class better. Have fun learning\nand creating\nTable 19: Example of model Humanities ability.\nG\nRANDOM SAMPLING EXPERIMENT RESULTS\nWe provide the collected experimental results of 129 randomly selected subsets from Section 3.1.\nThese data can be further used for other analysis purposes.\n20\nPreprint\ninput_length\noutput_length\nunderstandability\nnaturalness\ncoherence\nreward\nmtld\nknn_6\nPPL\nloss\n102.729\n286.592\n0.867\n0.829\n0.961\n1.126\n68.272\n1.078\n3.421\n0.980\n69.623\n432.224\n0.784\n0.719\n0.917\n0.751\n70.568\n1.068\n4.379\n0.984\n32.936\n356.696\n0.757\n0.709\n0.907\n1.457\n59.245\n1.044\n3.708\n0.985\n65.222\n351.245\n0.802\n0.750\n0.931\n1.096\n66.159\n1.063\n3.857\n0.998\n24.679\n311.668\n0.737\n0.693\n0.927\n2.509\n56.796\n1.044\n3.172\n0.970\n46.027\n199.094\n0.790\n0.755\n0.959\n2.929\n55.288\n1.047\n2.624\n0.960\n69.840\n358.533\n0.807\n0.760\n0.929\n0.972\n65.081\n1.061\n3.780\n0.983\n42.879\n316.451\n0.759\n0.715\n0.923\n2.130\n59.883\n1.045\n3.318\n0.968\n75.580\n460.983\n0.799\n0.738\n0.917\n0.114\n73.243\n1.074\n4.713\n1.048\n45.262\n388.129\n0.776\n0.723\n0.917\n1.248\n63.611\n1.054\n3.881\n0.992\n57.457\n367.983\n0.783\n0.735\n0.911\n1.339\n62.603\n1.058\n3.656\n0.987\n26.201\n316.794\n0.761\n0.724\n0.894\n1.191\n56.274\n1.037\n3.614\n0.993\n49.905\n323.043\n0.769\n0.727\n0.928\n1.741\n61.393\n1.050\n3.413\n0.974\n26.275\n421.357\n0.721\n0.655\n0.921\n1.801\n62.695\n1.042\n4.029\n0.982\n62.725\n393.420\n0.759\n0.700\n0.931\n1.951\n67.242\n1.060\n3.853\n0.987\n39.806\n445.083\n0.746\n0.678\n0.907\n1.107\n66.142\n1.059\n4.341\n0.987\n70.270\n262.376\n0.822\n0.780\n0.956\n2.242\n62.556\n1.060\n3.058\n0.971\n46.716\n429.933\n0.754\n0.695\n0.907\n0.979\n65.989\n1.058\n4.267\n0.994\n50.895\n347.113\n0.786\n0.739\n0.926\n1.529\n61.344\n1.053\n3.560\n0.986\n45.613\n361.398\n0.763\n0.703\n0.944\n2.329\n65.737\n1.057\n3.622\n0.978\n30.844\n309.751\n0.752\n0.710\n0.922\n2.756\n56.939\n1.041\n3.089\n0.975\n41.662\n278.702\n0.775\n0.734\n0.922\n1.924\n56.283\n1.046\n3.191\n0.980\n60.178\n301.275\n0.793\n0.742\n0.947\n2.109\n61.846\n1.058\n3.379\n0.977\n69.810\n311.997\n0.792\n0.747\n0.945\n1.859\n61.973\n1.060\n3.220\n0.969\n41.598\n257.505\n0.774\n0.738\n0.932\n2.044\n57.434\n1.044\n3.104\n0.975\n101.613\n354.567\n0.853\n0.806\n0.938\n0.280\n69.875\n1.079\n4.019\n0.999\n64.637\n299.082\n0.796\n0.752\n0.947\n1.752\n62.023\n1.062\n3.363\n0.992\n53.798\n367.029\n0.767\n0.710\n0.936\n1.702\n65.440\n1.058\n3.739\n0.982\n46.261\n352.759\n0.768\n0.714\n0.928\n1.814\n63.404\n1.054\n3.716\n0.983\n52.496\n291.431\n0.788\n0.749\n0.933\n1.629\n60.040\n1.051\n3.360\n0.972\n80.489\n419.792\n0.809\n0.751\n0.929\n0.793\n70.305\n1.076\n4.182\n1.004\n105.527\n269.572\n0.876\n0.841\n0.961\n1.033\n67.100\n1.081\n3.313\n0.989\n41.803\n280.630\n0.764\n0.720\n0.933\n2.153\n57.677\n1.043\n3.164\n0.968\n82.896\n278.274\n0.838\n0.802\n0.958\n1.337\n65.896\n1.069\n3.388\n0.985\n36.064\n336.507\n0.769\n0.720\n0.926\n1.785\n61.275\n1.045\n3.653\n0.983\n58.510\n244.927\n0.791\n0.747\n0.957\n2.687\n58.289\n1.059\n2.701\n0.964\n41.349\n284.143\n0.778\n0.733\n0.936\n2.276\n58.760\n1.052\n3.140\n0.966\n62.921\n361.994\n0.773\n0.723\n0.922\n1.547\n63.536\n1.057\n3.647\n0.979\n56.596\n451.301\n0.772\n0.711\n0.904\n0.563\n68.224\n1.058\n4.436\n0.990\n25.765\n373.152\n0.736\n0.684\n0.904\n1.556\n60.507\n1.041\n3.920\n0.989\n78.845\n339.202\n0.812\n0.762\n0.951\n1.438\n67.615\n1.068\n3.602\n0.979\n71.564\n267.444\n0.816\n0.779\n0.950\n1.577\n63.527\n1.063\n3.224\n0.979\n87.063\n342.297\n0.840\n0.796\n0.924\n0.750\n67.469\n1.072\n3.763\n0.991\n28.999\n323.759\n0.759\n0.713\n0.919\n1.836\n57.809\n1.044\n3.496\n0.981\n28.405\n324.233\n0.743\n0.694\n0.931\n2.260\n60.238\n1.043\n3.587\n0.975\n61.423\n281.118\n0.792\n0.747\n0.950\n2.551\n60.340\n1.064\n3.108\n0.963\n18.084\n317.710\n0.749\n0.710\n0.893\n1.511\n57.264\n1.038\n3.606\n0.989\n23.617\n296.796\n0.748\n0.708\n0.915\n1.675\n56.636\n1.037\n3.468\n0.985\n41.173\n205.367\n0.796\n0.767\n0.937\n2.027\n53.145\n1.041\n2.833\n0.981\n44.796\n329.179\n0.759\n0.704\n0.940\n2.744\n60.609\n1.051\n3.224\n0.989\n24.685\n477.961\n0.707\n0.638\n0.887\n1.368\n65.346\n1.046\n4.447\n0.983\n43.223\n365.828\n0.769\n0.711\n0.930\n1.846\n63.779\n1.055\n3.811\n0.982\n56.828\n359.443\n0.777\n0.722\n0.924\n1.443\n62.824\n1.055\n3.810\n0.989\n52.652\n254.456\n0.795\n0.753\n0.949\n2.462\n60.348\n1.051\n3.006\n0.977\n66.962\n371.096\n0.799\n0.742\n0.941\n1.587\n67.986\n1.070\n3.734\n0.992\n61.466\n355.291\n0.774\n0.721\n0.920\n1.399\n64.751\n1.057\n3.748\n0.983\n77.179\n294.764\n0.825\n0.777\n0.959\n1.916\n65.810\n1.070\n3.351\n0.985\n86.817\n346.588\n0.844\n0.796\n0.942\n0.589\n69.805\n1.078\n3.976\n0.990\nTable 20: Random experiment results 1.\n21\nPreprint\ninput_length\noutput_length\nunderstandability\nnaturalness\ncoherence\nreward\nmtld\nknn_6\nPPL\nloss\n65.443\n453.266\n0.778\n0.718\n0.904\n0.248\n69.307\n1.062\n4.563\n0.996\n47.868\n347.710\n0.785\n0.738\n0.926\n1.330\n63.374\n1.052\n3.708\n0.980\n39.208\n308.947\n0.778\n0.737\n0.921\n1.376\n59.802\n1.047\n3.534\n0.979\n36.990\n335.400\n0.754\n0.698\n0.936\n3.043\n58.702\n1.047\n3.231\n0.978\n38.769\n311.318\n0.766\n0.721\n0.928\n1.942\n61.146\n1.048\n3.366\n0.976\n31.387\n264.193\n0.762\n0.725\n0.930\n2.043\n55.072\n1.041\n3.109\n0.978\n51.952\n347.952\n0.780\n0.724\n0.947\n2.239\n65.820\n1.059\n3.520\n0.985\n117.624\n385.075\n0.873\n0.826\n0.945\n0.179\n75.406\n1.091\n4.100\n1.004\n42.481\n307.986\n0.760\n0.712\n0.931\n1.996\n61.049\n1.048\n3.473\n0.981\n63.936\n381.906\n0.798\n0.744\n0.924\n0.836\n66.379\n1.064\n3.983\n0.987\n47.116\n397.479\n0.772\n0.725\n0.894\n0.533\n63.525\n1.050\n4.158\n0.998\n52.896\n326.873\n0.788\n0.746\n0.921\n1.013\n62.873\n1.057\n3.717\n0.986\n39.859\n388.185\n0.740\n0.686\n0.905\n1.774\n61.495\n1.044\n3.754\n0.999\n58.227\n322.313\n0.803\n0.759\n0.927\n1.325\n63.150\n1.059\n3.623\n0.993\n41.489\n381.454\n0.761\n0.717\n0.901\n0.785\n60.898\n1.046\n4.006\n0.991\n77.980\n396.390\n0.807\n0.755\n0.920\n0.703\n66.118\n1.067\n4.016\n0.991\n50.570\n283.139\n0.784\n0.739\n0.951\n2.520\n61.530\n1.058\n3.073\n0.972\n71.903\n369.859\n0.794\n0.741\n0.928\n0.957\n68.547\n1.061\n3.967\n0.988\n61.888\n304.082\n0.811\n0.773\n0.941\n0.827\n62.673\n1.058\n3.700\n0.984\n66.920\n241.679\n0.813\n0.780\n0.944\n1.935\n59.334\n1.058\n2.924\n0.974\n45.947\n346.294\n0.779\n0.735\n0.915\n1.409\n61.754\n1.053\n3.668\n0.990\n63.585\n326.386\n0.806\n0.762\n0.927\n0.689\n65.125\n1.063\n3.964\n0.998\n46.150\n290.080\n0.764\n0.719\n0.939\n2.325\n59.118\n1.052\n3.051\n0.975\n44.748\n291.442\n0.788\n0.743\n0.944\n2.108\n60.978\n1.053\n3.313\n0.977\n88.870\n401.555\n0.825\n0.774\n0.931\n0.229\n69.541\n1.074\n4.274\n0.982\n63.822\n320.139\n0.802\n0.753\n0.945\n1.797\n64.716\n1.063\n3.555\n0.982\n65.275\n385.007\n0.800\n0.740\n0.946\n1.364\n68.535\n1.072\n3.920\n1.000\n29.802\n150.962\n0.785\n0.766\n0.941\n2.521\n49.809\n1.036\n2.374\n0.978\n25.489\n273.662\n0.748\n0.711\n0.914\n2.101\n54.551\n1.040\n3.081\n0.966\n79.026\n316.480\n0.817\n0.769\n0.946\n1.416\n66.166\n1.068\n3.466\n0.973\n100.707\n333.106\n0.846\n0.792\n0.964\n1.768\n71.054\n1.084\n3.472\n0.995\n35.745\n417.375\n0.726\n0.660\n0.905\n2.210\n64.833\n1.053\n3.832\n0.961\n19.414\n445.720\n0.729\n0.675\n0.873\n0.743\n61.201\n1.038\n4.338\n0.985\n88.069\n385.920\n0.825\n0.770\n0.941\n0.597\n71.027\n1.074\n4.126\n1.001\n91.381\n392.303\n0.825\n0.774\n0.938\n0.584\n69.990\n1.077\n4.068\n0.992\n95.122\n321.651\n0.830\n0.781\n0.957\n1.490\n67.801\n1.074\n3.456\n1.002\n41.671\n400.445\n0.762\n0.711\n0.909\n1.262\n62.359\n1.048\n3.804\n0.998\n47.999\n387.365\n0.768\n0.722\n0.907\n1.019\n62.817\n1.050\n3.921\n1.016\n61.965\n319.993\n0.791\n0.745\n0.928\n1.792\n63.537\n1.061\n3.470\n0.994\n86.434\n349.504\n0.822\n0.766\n0.954\n1.519\n69.445\n1.079\n3.810\n0.990\n72.861\n377.140\n0.794\n0.739\n0.927\n1.102\n66.215\n1.066\n3.939\n0.999\n60.589\n357.736\n0.792\n0.740\n0.933\n1.434\n66.224\n1.061\n3.758\n0.991\n36.427\n418.421\n0.727\n0.660\n0.909\n1.728\n64.787\n1.054\n4.022\n0.975\n56.222\n363.895\n0.784\n0.727\n0.930\n1.348\n64.616\n1.058\n3.759\n0.986\n54.002\n294.862\n0.792\n0.749\n0.935\n2.122\n60.555\n1.049\n3.224\n0.978\n67.621\n267.347\n0.827\n0.788\n0.957\n1.938\n62.581\n1.066\n3.163\n0.986\n73.698\n423.275\n0.803\n0.745\n0.927\n0.586\n71.176\n1.070\n4.338\n1.000\n52.490\n344.371\n0.772\n0.722\n0.929\n1.572\n62.516\n1.052\n3.612\n0.979\n18.467\n312.770\n0.751\n0.711\n0.908\n1.275\n56.718\n1.035\n3.723\n0.975\n76.520\n382.718\n0.808\n0.756\n0.924\n0.786\n68.118\n1.069\n4.060\n0.997\n30.908\n215.053\n0.750\n0.713\n0.939\n3.080\n55.174\n1.037\n2.653\n0.958\n37.615\n278.354\n0.769\n0.733\n0.931\n2.132\n57.390\n1.045\n3.123\n0.969\n82.065\n249.474\n0.834\n0.798\n0.961\n1.926\n62.987\n1.066\n2.980\n0.974\n45.438\n297.919\n0.773\n0.725\n0.936\n2.236\n59.366\n1.051\n3.287\n0.976\n29.915\n324.490\n0.758\n0.710\n0.923\n2.036\n57.623\n1.048\n3.464\n0.978\n57.175\n266.633\n0.788\n0.746\n0.951\n2.232\n62.362\n1.058\n3.097\n0.972\n57.073\n385.760\n0.758\n0.705\n0.909\n1.218\n62.189\n1.055\n3.857\n0.981\n84.803\n373.302\n0.803\n0.748\n0.942\n0.983\n68.274\n1.068\n3.993\n1.001\n64.189\n388.245\n0.775\n0.713\n0.937\n1.330\n67.701\n1.061\n3.985\n0.980\n96.898\n362.815\n0.846\n0.801\n0.945\n0.389\n70.279\n1.076\n3.941\n0.995\n69.502\n220.558\n0.832\n0.804\n0.953\n1.808\n59.840\n1.058\n2.934\n0.975\n50.454\n159.838\n0.810\n0.796\n0.944\n1.990\n52.740\n1.047\n2.546\n0.978\n45.293\n225.428\n0.798\n0.769\n0.939\n2.061\n55.788\n1.049\n2.877\n0.972\n121.402\n377.451\n0.870\n0.817\n0.960\n0.495\n75.109\n1.091\n4.027\n1.004\n42.257\n336.535\n0.771\n0.726\n0.924\n1.674\n61.486\n1.050\n3.577\n0.997\n90.690\n227.836\n0.870\n0.841\n0.956\n1.134\n63.386\n1.072\n3.113\n0.997\n31.113\n416.373\n0.724\n0.665\n0.901\n1.907\n64.171\n1.050\n4.005\n0.985\n70.559\n388.945\n0.807\n0.754\n0.932\n1.085\n67.807\n1.069\n3.967\n0.989\n29.458\n349.922\n0.749\n0.711\n0.889\n1.263\n57.639\n1.039\n3.630\n0.990\n92.657\n255.682\n0.857\n0.820\n0.960\n1.492\n65.639\n1.073\n3.047\n0.980\n35.635\n373.682\n0.732\n0.666\n0.946\n3.273\n61.400\n1.050\n3.247\n0.980\nTable 21: Random experiment results 2.\n22\n"
  },
  {
    "title": "DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks",
    "link": "https://arxiv.org/pdf/2307.05628.pdf",
    "upvote": "8",
    "text": "DNAGPT: A Generalized Pre-trained Tool for\nVersatile DNA Sequence Analysis Tasks\nDaoan Zhang1,2,4, Weitong Zhang2,3, Yu Zhao2, Jianguo Zhang1*,\nBing He2*, Chenchen Qin2*, Jianhua Yao2*\n1Southern University of Science and Technology.\n2Tencent AI Lab, Shenzhen, China.\n3City University of Hong Kong.\n4University of Rochester.\n*Corresponding author(s). E-mail(s): zhangjg@sustech.edu.cn;\nowenbhe@tencent.com; chenchenqin@tencent.com;\njianhuayao@tencent.com;\nContributing authors: daoan.zhang@rochester.edu;\nweitzhang6-c@my.cityu.edu.hk; louisyuzhao@tencent.com;\nAbstract\nPre-trained large language models demonstrate potential in extracting informa-\ntion from DNA sequences, yet adapting to a variety of tasks and data modalities\nremains a challenge. To address this, we propose DNAGPT, a generalized DNA\npre-training model trained on over 200 billion base pairs from all mammals. By\nenhancing the classic GPT model with a binary classification task (DNA sequence\norder), a numerical regression task (guanine-cytosine content prediction), and\na comprehensive token language, DNAGPT can handle versatile DNA analy-\nsis tasks while processing both sequence and numerical data. Our evaluation of\ngenomic signal and region recognition, mRNA abundance regression, and arti-\nficial genomes generation tasks demonstrates DNAGPT\u2019s superior performance\ncompared to existing models designed for specific downstream tasks, benefiting\nfrom pre-training using the newly designed model structure.\nKeywords: DNA, Generative Pre-trained Transformer, DNAGPT, Sequence analysis,\nNumerical analysis\n1\narXiv:2307.05628v3  [q-bio.GN]  30 Aug 2023\n1 Introduction\nDNA serves as the essential blueprint of life, encompassing the comprehensive instruc-\ntion manual that guides an organism through growth, development, survival, and\nreproduction. The Human Genome Project has advanced our understanding of life by\ndecoding the DNA code, leading to ongoing research in DNA interpretation, biological\nprocesses, disease detection, and the redesign of life, which can be applied in bacterial\nand mammalian cell engineering for both diagnostics and therapeutics by synthetic\nbiological technologies such as the CRISPR-Cas system [1\u20134]. As the most funda-\nmental information in biology, DNA sequences contain rich biological information[5],\nespecially those with large non-coding regions [6] that remain unexplored and are par-\nticularly worth investigating. The considerable diversity, vast volume, and intricate\nrelationships within biological information pose challenges in the analysis and com-\nprehension of such data. For example, as the basic functional unit of DNA sequence, a\nsingle gene, among the estimated 100,000 genes present in the human genome,[7] can\nbe characterized from different aspects: it can be represented by nucleotide sequences\n[8], its expression level in different cells may vary greatly due to the influence of fac-\ntors such as its non-coding region, cell type, or environment [9], moreover, it can be\ntranslated into proteins with different abundance levels under different circumstances\n[10]. Consequently, DNA sequence research requires the integration of sequencing data,\nrepresented by DNA sequences, and expression data, represented by numerical values\nof abundance.\nRecently, the advent of foundation models [11\u201313] has revolutionized natural lan-\nguage understanding [14] through the pre-training of generalized models on large-scale\ndatasets, which can be fine-tuned for various downstream tasks. Inspired by this, pre-\ntrained models have been employed to uncover the hidden information within DNA\nsequences [15, 16]. However, as mentioned above, DNA analysis tasks have various\nforms that involve both sequence and numerical data as input and output [17\u201319] which\nare difficult to tackle in one language-based model [20\u201323]. The previous attempts,\nDNABERT [15] as well as Nucleotide Transformers (NT) [16], involved pre-training\non the genome data followed by fine-tuning on the downstream datasets based on\ntask-specific heads, separately handling attribute prediction tasks like the recognition\nof genomic signals and regions (GSR) tasks [24\u201326] and generation tasks like recon-\nstructing human genetic variants [16]. In addition, during pre-training, the previously\nmentioned pre-trained models only used DNA sequences and did not consider numer-\nical data, making it unsuitable for tasks that involve numerical input or output such\nas the regression of mRNA abundance from the DNA sequence [27]. These weaknesses\nseverely limit the generalization of various tasks and fail to propose a generalized\nmodel that seamlessly integrates DNA sequence-relevant tasks. Also, unifying those\nintricate and diverse data types and task paradigms can reduce unnecessary algorithm\ndesign effort while allowing more tasks to benefit from pre-training, further paving the\nway for more profound discoveries and insights in DNA sequence analysis. Therefore,\na generalized pre-training model is needed to fully extract and utilize DNA informa-\ntion, which adapts to various DNA-related downstream tasks, to gain a comprehensive\nperspective on DNA, accelerate research and production processes, improve research\naccuracy, and avoid the waste of resources caused by repeated research.\n2\nConstructing such a generalized pre-trained model for DNA sequences requires\nconsideration from two aspects: (1) How to coherently process different data types\n(sequence and number) in both the pre-training and testing stages? (2) How to estab-\nlish a common pipeline for different tasks? In this study, we introduce DNAGPT, a\ngeneralized pre-trained model for DNA analysis, where a multi-task pre-training strat-\negy and a novel token language are proposed to answer the above two questions. In\naddition to the auto-regression pre-training task in the classic GPT model, we add a\nbinary classification pre-training task (DNA sequence order) and a numerical regres-\nsion pre-training task (guanine-cytosine content prediction) in the pre-training stage to\nhelp the model to better understand DNA sequence data and numerical data. For the\nDNA sequence order prediction, we randomly flip the input DNA sequence and let the\nmodel predict whether the flip operation has been performed or not. For the guanine-\ncytosine (GC) content prediction, we randomly extract a segment of the sequence from\nthe input and then have the model calculate and output the GC content value for\nthis segment. We modify the GPT architecture with corresponding embedding layers\nand encoding heads for both sequence and numerical input and outputs so that they\ncan be processed and trained in the same framework. We also design a comprehen-\nsive token language to encode sequence, number, and task-related information in the\nsame token space. Furthermore, in order to better learn the sequence conservation and\ndiversity across species, we utilize reference genomes [28] from all the mammals for\npre-training, with a total data size exceeding 200 billion base pairs (bps).\nAfter pre-training, we tested and evaluated the functionalities, capabilities and per-\nformance of the DNAGPT on a diverse panel of prediction, regression, and generation\ntasks. We began from GSR prediction task [24] to assess the sensitivity of the model\nto specific sites. The results demonstrated that the DNAGPT can not only compete\nwith state-of-the-art methods but also accurately identify pivotal regions within the\ninput sequence. After that, DNAGPT achieved better results compared with conven-\ntional methods on mRNA abundance assessment task [27] with a mixture input of\ntensors and DNA sequences and output the corresponding mRNA abundance values.\nWe further examined whether DNAGPT can produce pseudo DNA sequences [17], the\nresults from various metrics proved that the DNAGPT surpassed traditional GAN\nand RBM models in terms of maintaining certain biological properties and features\ndiscovered in natural genomic sequences.\n2 DNAGPT architecture\n2.1 Model structure\nThe backbone of DNAGPT is a transformer-based [29] auto-regressive [30] decoder\nwith the masked self-attention [31] module. To better deal with numerical information,\nwe pre-train the DNA sequence and numerical property end to end in a single model.\nThe detailed network structure is presented in Figure. 1 c. DNAGPT uses sequence\ntokens to denote the encoded DNA sequence and number tokens for the encoded\nnumerical attributes. The sampled DNA sequence is first processed into a string of\nnon-overlapped k-mers token input, then sent into the Sequential Embedding Layer to\nbe encoded as embeddings. The numbers are sent directly into a Numerical Embedding\n3\nFig. 1 Schematic of DNAGPT. a. The pre-training stage of DNAGPT. We utilize genomes from\nall mammals and design three pre-training tasks to jointly pre-train DNAGPT. b. The fine-tune\nstage of DNAGPT. After fine-tuning on the downstream task-related datasets, DNAGPT is able to\nhandle specific tasks. Moreover, DNAGPT supports downstream tasks from different species, as well\nas various task and data formats. c. Model structure of DNAGPT. Different types of tokens are\nprocessed separately by different embedding layers, and then combined together as the input for the\nbackbone. d. Details of the embedding layers and decoding heads. The figure illustrates the zoom-in\nview of different encoding heads. When processing the input data, we use different heads for mapping\naccording to the data types. e. Model inputs (the first row), outputs (the second row) and ground\ntruth (the third row) of DNAGPT. Tokens with different data types are evaluated with cross-entropy\nloss or mean squared error (MSE) loss.\n4\nFig. 2 Token language of DNAGPT. a. Tokens used in DNAGPT. b. Exemplar templates of the\ninput and label in fine-tuning. c. Details of different versions of DNAGPT\nLayer to be encoded as embeddings co-trained with the DNA embeddings. Then we\nconcatenate both embeddings and send them into the GPT. The outputs of the GPT\nare split into two types of embeddings and sent to the Classification Head to classify\ndifferent tokens and Regression Head to generate numbers, respectively. The structure\nof those heads is presented in Figure. 1 d. It\u2019s worth noting that DNAGPT can\nhandle versatile downstream applications, where only fine-tuning of the original model\nparameters is needed. This simplifies the model\u2019s usage, preserves its generalizability,\nand lays the foundation for potential zero-shot learning.\n5\n2.2 Design of token language\nCurrently, most DNA pre-training methods [15, 16] simply use strategies from natural\nlanguage models and do not consider the characteristics of DNA sequence and specific\nbiological tasks in the model design. DNA sequence has no organizational structure as\nthe nature language, which can be hierarchically divided into paragraphs, sentences,\nwords and punctuations. We design a hierarchical token language structure for DNA\nsequences. Non-overlapped k-mers based on bps (base pairs) are first used to generate\nDNA words. DNA words of variable lengths are then combined to form DNA sentences.\nDNA sentences of varying lengths are then integrated to form DNA paragraphs, which\nare input into the GPT model.\nAs shown in Figure. 2 a, the regular input and output tokens are Sequence\ntokens and Number tokens which represent the DNA sequences and numbers respec-\ntively. Instruction tokens are used to give a prompt to the model about what\nare the next sequence of the tokens should the model output. Take an example,\n\u2032Human\u2032\u2032AATAAA\u2032 indicates we encode a human AATAAA polyadenylation signals\nand \u2032Bovine\u2032\u2032AATAAA\u2032 indicates we encode a bovine AATAAA polyadenylation sig-\nnals. Similarly, \u2032M\u2032\u20320.3155\u2032 indicates that we encode a number into the model and in\n\u2032B\u2032\u2032X\u2032, \u2032B\u2032 is the instruction token of the binary classification where the Classification\ntokens \u2032A\u2032 indicates \u2019True\u2019 and \u2032N\u2032 indicates \u2019False\u2019. Furthermore, to better construct\nconnections, we use Connection tokens to form the connections of two series of tokens,\nwhere \u2032+\u2032 represent the aggregation of two series of tokens and \u2032=\u2032 represent a rela-\ntion of input and output. Specifically, when we want to predict the expression level of\nmRNA from both DNA sequence and the mRNA half-life values, we can encode the\ninputs as \u2032Human\u2032\u2032ATCGTC\u2032\u2032+\u2032\u2032M\u2032\u2032-0.3484\u2032\u2032=\u2032\u2032M\u2032\u20320.9854\u2032. This input indicates that\nwe hope the model can generate the information from both of the \u2032ATCGTC\u2032 sequence\nand the input number \u2032-0.3484\u2032 to output the result numbers \u20320.9854\u2032. The reserved\ntokens include numbers from \u20320\u2032 to \u20329\u2032, some unused uppercase letters like \u2032K\u2032, \u2032L\u2032, etc.\nand some special symbols like \u2032*\u2032 and \u2032/\u2032, etc. These reserved tokens can be used to\nbuild up more exclusive tasks for DNA sequence analysis. The complete token list is\npresented in the Figure. S2.\n3 Multi-tasks pre-training\nIn order to integrate DNA sequence information from multiple species and allow down-\nstream tasks to benefit from cross-species information, we proposed four variations of\nDNAGPT, named DNAGPT-H, DNAGPT-M, DNAGPT-S-512 and DNAGPT-B-512.\nAs shown in Figure. 2 c, DNAGPT-H, DNAGPT-M and DNAGPT-S-512 have 0.1 bil-\nlion parameters and DNAGPT-B-512 has 3 billion parameters. Specifically, DNAGPT-\nH\u2019s sequence length is set to 4096, equivalent to 24,576 bps, and its pre-training data\nis based on Human reference genomes; DNAGPT-M also has a sequence length of\n4096, with pre-training data from reference genomes of 9 species; DNAGPT-S-512 and\nDNAGPT-B-512 have a sequence length set to 512 and its pre-training data consists\nof reference genomes from all mammals. Specifically, the dataset for Genomes from 9\nspecies includes reference genomes from Arabidopsis thaliana, Caenorhabditis elegans,\nBos taurus, Danio rerio, Drosophila melanogaster, Escherichia coli gca 001721525,\n6\nHomo sapiens, Mus musculus, Saccharomyces cerevisiae with a total of 10 billion bps.\nFor the mammals\u2019 dataset, we downloaded all mammalian reference genomes from the\nNCBI GenBank. After preprocessing, approximately 200 billion bps of data were sam-\npled for pre-training. We then compare the three versions of DNAGPT in the ablation\nstudy and provide a detailed description of the data used in the supplementary mate-\nrials. Reported results in different tasks are from the suitable version of DNAGPT for\neach task due to the limitation of task-specific sequence length. In the GSR classifi-\ncation task, we used all three versions of DNAGPT. For the mRNA prediction and\npseudo genomes generation tasks, the input sequence length requirements are greater\nthan 512. Therefore, we utilize DNAGPTs with an input sequence length of 4096.\n3.1 Pre-training tasks\nWe design three pre-training tasks for DNAGPT to fully characterize the DNA\nsequence and its associated numerical properties, including one standard GPT task\nand two DNA-specific tasks.\nNext token prediction\nNext token prediction [32] is a classical pre-training task in NLP. GPT leverages this\ntechnique which can predict the next possible token based on the previous tokens.\nRecently, by adding more parameters and more training data, GPT-3 and GPT-4\ndemonstrate remarkable performance on various tasks. In DNAGPT, we also use the\nnext token prediction strategy as the fundamental pre-training task.\nGuanine-cytosine content prediction\nGuanine-cytosine (GC) content plays a crucial role in transcriptome analysis as it\nprovides essential information about genome structure, such as structural variations\n[33] and transcriptional activity [34, 35]. In this task, we encode the GC content as\nnumber tokens in DNAGPT, allowing for joint training of numerical and sequence data\nand enabling DNAGPT to adapt to downstream tasks with numerical data as input\nand output. Furthermore, we adopt dynamic sequence length for the DNA sequence\nin this task, which allows the model to learn a dynamic receptive field and enables\nthe downstream tasks with dynamic sequence length as input. We first calculate the\nGC content value of randomly selected sequences, which is an entirely unsupervised\nmanner. The model should output this value after reading the entire sequence.\nSequence order prediction\nThe sequence order of DNA plays an important role in gene expression [36] and\ntranscription [37, 38]. For instance, sequences such as TATA box [39] and AATAAA\nPAS [40] often have to maintain a fixed order. We design a self-supervised sequence\norder prediction task, where we randomly reverse a sequence and let the model pre-\ndict whether the sequence has been reversed or not. This task provides heuristic\ninformation for downstream tasks with order-sensitive sequences.\nSince GPT models use unidirectional attention [41], they can only infer and gen-\nerate tokens from left to right. By reversing the DNA sequences, our model can infer\n7\ntokens in both directions from the global perspective, improving its capability for\ndownstream tasks for predicting preceding contexts.\n3.2 Pre-training Loss\nFor the calculation of the loss in DNAGPT, as shown in Figure. 1. e, we illustrate the\nmodel input, output, and ground truth for DNAGPT during pre-training. The output\nof DNAGPT can be DNA tokens and/or number tokens. When calculating the loss\nfor the next token prediction and sequence order prediction task, cross-entropy loss is\nused. For the GC ratio prediction task, mean squared error (MSE) loss is used since\nnumerical tokens are involved. The final loss can be represented as:\nLoss = \u03bb \u00d7 MSE loss + Cross entropy loss\n(1)\nwhere MSE loss indicates MSE loss and Cross entropy loss indicates Cross entropy\nloss. In the pre-training, the \u03bb is set to 0.01.\n4 Genomic signals and regions (GSR) recognition\nRecognition of various genomic signals and regions (GSR) from DNA sequence is\nessential to the understanding of genomes. To address this issue, we fine-tune and\nevaluate our model on the recognition of polyadenylation signals (PAS) and translation\ninitiation sites (TIS) of different organisms: human, mouse, bovine and fruit fly. To\nbe specific, we follow the processing procedure in DeepGSR [24]. The DNA sequence\nlengths are set to 603 and 606 respectively for TIS and PAS recognition. DeepGSR\nextracted 20,933, 18,693, 12,082, and 27,203 true PAS data; and 28,244, 25,205, 17,558,\nand 30,283 true TIS for human, mouse, bovine, and fruit fly, respectively which are\nused as groud-truth. Then Deepgsr sampled a similar number of non-GSR sequences\nfrom the genome sequences and combined them with the true cases. The training set,\nvalidation set, and test set are divided in the ratio of 6:1.5:2.5. Details of the datasets\nare depicted in Section S1.4. We report the results of DNAGPT-B-512 in this task.\n4.1 DNAGPT is able of recognizing GSRs from any species.\nThe recognition of GSR can be considered as a binary classification task. We evaluate\nDNAGPT on the recognition of both PAS (AATAAA variant and all variants) and\nTIS (with the ATG signal) in the human genome. We present the accuracy metric in\nFigure. 3 a, which shows that our model can steadily outperform the previous state-of-\nthe-art methods. We further provide additional metric results in the Table. S2 and S3\nfor a more comprehensive evaluation. Notice that, GSRNET [26] utilizes the embedded\nfeatures generated from the pre-trained DNABERT model. DNAGPT can significantly\noutperform the modified DNABERT in all the tasks. To verify the generalization of\nDNAGPT, we further evaluate our model on other organisms, including mouse, fruit\nfly and bovine. Experimental results are presented in Figure. 3 b, c and d, respectively.\nOur DNAGPT outperforms the GSRNET and DeepGSR in most cases, the latter two\nwere specially developed for GSR recognition.\n8\nFig. 3 Performance comparison between DNAGPT and other methods on PAS and TIS recognition.\nWe fine-tune and evaluate our DNAGPTs on the data from four organisms, including human (a),\nmouse (b), fruit fly (c) and bovine (d). In each subgraph, we compare the accuracy of the DNAGPT\nwith previous methods on the recognition of PAS (AATAAA), PAS (all) and TIS (ATG) from left\nto right. The reported results come from DNAGPT-B-512.e. Attention maps of the final layer of\nDNAGPT (last row). The green regions are the sampled input sequences and the GSRs are located\nin the middle of the sequence. The yellow regions are the 5\u2019 Untranslated Region and 3\u2019 Untranslated\nRegion, respectively. 300 bps before and after the GSRs are included in the analysis. f. Performance\ncomparison for DNAGPTs and other methods of mRNA abundance prediction. The reported r2\nresults show that compared to mouse genomes, tasks on human genomes can benefit more by utilizing\nDNAGPT. The reported results come from DNAGPT-M.\n4.2 DNAGPT recognizes GSRs based on non-coding regions.\nTo explore the inner relations behind DNAGPT\u2019s ability to recognize GSRs, we visual-\nize the attention map of the final layer in DNAGPT\u2019s backbone. The input data is TIS\nor PAS (AATAAA) sequence from humans, respectively. As shown in Figure. 3 e, we\n9\nsample 300 bps before and after the TIS and PAS locations (green areas), which con-\ntain both coding and non-coding (yellow) regions. TIS is located right in front of the\ncoding region, where is the non-coding region that DNAGPT focuses its attention and\ntherefore accurately identifies TIS. DNAGPT shows the same attention pattern for\nthe PAS recognition tasks. The attention maps of both cases adequately demonstrate\nthat DNAGPT can recognize information in non-coding regions to identify GSRs.\n5 mRNA expression level prediction\nWe then investigated whether DNAGPT could extract more abundant information\nfrom DNA sequences by attempting to predict the mRNA expression levels of corre-\nsponding promoters directly from genomic sequence information. Following Xpresso\n[27], we utilized 18,377 and 21,856 promoters as well as the mRNA half-lives in human\nand mouse respectively and held out 1000 cases in each specie for testing. CAP-\nAnalysis Gene Expression (CAGE) was used to refine the annotations. Xpresso utilized\ndeep convolutional network to encode both promoters and the half-lives and predicted\nthe corresponding mRNA expression level and achieved much better results compared\nto traditional methods..\nWe used DNAGPT to predict the mRNA abundance under the same setting as\nXpresso. We report the results of DNAGPT-M in this task. As mentioned in the last\nline of Figure. 2 b. We combined the promoter sequences with the mRNA half-lives in\na single sequence to predict the expression level of the mRNA abundance. We present\nthe r2 (Coefficient of determination) metric in Figure. 3 f. DNAGPT outperformed\nXpresso from 0.59 to 0.62 for human mRNA abundance prediction and improved the\nresults on the mouse species from 0.71 to approximately 0.73.\nThe input format of this task where both sequence and numerical are provided\ncan not be handled by language-based models. Previously, specialized models such\nas Xpresso designed by experts have to be developed. DNAGPT can handle these\nversatile tasks, obviating the need for designing more diverse and complex models.\n6 Artificial human genomes generation\nAs the primitive task of the GPT model, we further investigate DNAGPT\u2019s perfor-\nmance on the generation of artificial human genomes (AGs). AGs can be used to\nprotect genetic privacy and reduce the cost of genetic sample collection. Following\nthe work in [17], we fine-tune our DNAGPT on 5008 haplotypes from 1000 Genomes\ndata [42] which can be seen as the real genomes sequences and we use DNAGPT to\ngenerate 5000 AGs of 10000 Single Nucleotide Polymorphisms (SNPs) region for fur-\nther analysis (can be seen as 5000 sequences each with a length of 10,000 bps). We\ncompared DNAGPT with the GAN and RBM models. The GAN model consists of\na generator and a discriminator network, where the output of the generator and the\ninput of the discriminator both have the size of the number of SNPs. For the RBM\nmodel, we use the RBM model provided in [17]. All the training and testing strategy\nof GAN and RBM remains the same with [17]. We use the real 5008 haplotypes for\nthe comparisons for all the methods (GAN, RBM, DNAGPT). We report the results\nof DNAGPT-M in this task.\n10\n6.1 Analysis of artificial human genomes\nWe evaluate DNAGPT and comparison methods from the following perspectives: prin-\ncipal components (PC) [43]; allele frequency (AF) [44], linkage disequilibrium (LD)\n[45] and Pairwise haplotype distances. The evaluation metrics include Wasserstein\ndistances [46] and correlation (r2).\nPrincipal components\nWe conduct the principal component analysis (PCA) on the AGs generated from GAN,\nRBM, and DNAGPT. We show the value distribution of the first six principal compo-\nnents using an isoline map in Figure. 4 a. Results show that the distributions of AGs\ngenerated from all methods roughly align with those of the real human genomes, while\nDNAGPT model demonstrates the most similar distribution of the real sequences.\nWe further compute the Wasserstein distance (lower is better) between distributions\nof AGs and real genome sequence, which are 1.753. 3.432, 1.131 for GAN, RBM,\nDNAGPT, respectively.\nAllele frequency\nAllele frequency analysis is a genetic analysis method used to determine the frequency\nof different alleles of a gene locus. The allele frequency at a polymorphic site depends\non the variation of that site in all cases. In this analysis, we detect the frequency\nof SNPs within the 5,000 AGs from all the methods as well as the 5008 real AGs.\nWe conduct the analysis of the sequences generated by all the models. As shown in\nFigure. 4 b, both the DNAGPT and GAN perform stably with a correlation of 0.99.\nWe then visualize the correlation of those sites with allele frequency less than 0.2.\nAs shown in Figure. 4 c, DNAGPT outperforms GAN (0.94) and RBM (0.83) with a\ncorrelation of 0.96, indicating that DNAGPT can better capture the information even\nfrom low-frequency alleles.\nLinkage disequilibrium\nLinkage disequilibrium (LD) is a phenomenon in population genetics that can be\ndefined as the correlations of frequencies of two or more genetic markers (like alleles or\ngenes). We further analyze the LD for all the generated sequences and real sequences.\nFigure. 5 a illustrates the difference in LD values between human genomes generated\nby GAN, RBM and DNAGPT compared to real genomes, respectively. In these panels,\nthe lighter the color, the more similar the LD heat map is to the real genomes. Among\nthem, the LD of DNAGPT is slightly weaker than that of real genomes, while GAN\nand RBM are stronger than the original genomes. Overall, the heat map performance\nof DNAGPT is better than GAN and RBM, as their colors are lighter. The above\nconclusions can also be verified through a comparison of correlation values. We present\nthe correlation distributions in Figure. 5 b. The correlation between the LDs of real\nand generated sequences from GAN and RBM is 0.92 and 0.94 and DNAGPT can\nachieve a score of 0.98.\n11\nFig. 4\na. Comparison of isoline maps of PCA results of artificial human genomes generation.\nGAN; RBM; Real; DNAGPT. b. Correlations of allele frequency between real genomes and artificial\ngenomes. c. Correlations of allele frequency between real genomes and artificial genomes, specific on\nthe sites with allele frequency less than 0.2 in the real genomes.\n12\nFig. 5\nWe placed the results of the same method in one column, specifically, the first column is\nGAN vs. real; the second column is RBM vs. real; the third column is DNAGPT vs. real. Each row\nrepresents: a. Normalized correlation matrices of SNPs. We subtracted the correlation matrix of each\nmethod from the real genomes. The lighter the color, the closer the artificial genomes are to the real\ngenomes. b. Correlations of LD between real genomes and artificial genomes. c. Wasserstein distances\nof pairwise haplotype distance distribution.\nPairwise haplotype distances analysis\nPairwise haplotype distances refer to the genetic distances between different haplo-\ntypes within a genome. When calculating the distances, we typically compare the\ndifferences in the alleles at the corresponding loci between two haplotypes. In this\nanalysis, we first calculate the pairwise distance distributions within each cluster of\ngenerated genomes (GAN vs GAN, RBM vs RBM, DNAGPT vs DNAGPT), defined\nas Within-cluster, then the pairwise distance distributions between real genomes and\ngenerated genomes by each method (GAN vs Real, RBM vs Real, DNAGPT vs Real)\nare defined as Between-cluster. Then we calculate the Wasserstein distances between\nthe two types of distributions within the distribution of real genomes (Real vs Real).\nWe present the Wasserstein distances of within-cluster in Figure. 5 c. Among them,\n13\nthe GAN\u2019s distribution has the largest gap compared to the actual distribution with\na value of 108.15, followed by DNAGPT with a value of 71.04. The genomes gener-\nated by RBM have the smallest discrepancy with a value of 30.21 from real genomes.\nThe Between-cluster reflects the discrepancy between the pairwise distance distribu-\ntion of genomes generated by each method and real genomes. The genomes generated\nby DNAGPT are the most similar to the real genomes with a value of 28.63, while\nRBM performs the worst, followed closely by GAN.\n6.2 Generation temperature of DNAGPT can influence the\nquality of generated genomes\nWhen a trained DNAGPT generates the DNA sequence, we can control the random-\nness of the output sequence by adjusting the generation temperature. The generation\ntemperature ranges from 0 to infinity. The higher the generation temperature, the\nmore random the generated sequence will be. In the experiments mentioned earlier, our\ndefault generation temperature was 0.8. In this section, we will adjust the generation\ntemperature to 1.2 to evaluate the performance of DNAGPT under different genera-\ntion temperatures. The results are shown in the Figure. 6 a and b. Figure. 6 a shows\nthe Wasserstein distance, correlations of allele frequency, and correlations of linkage\ndisequilibrium with the real distribution. Figure. 6 b shows the Wasserstein distance of\npairwise haplotype distance distribution (within-cluster and between-cluster). We can\nfind that a larger generation temperature allows DNAGPT to maintain the correlation\nof allele frequency and linkage disequilibrium virtually unchanged while increasing\nthe distance from the real distribution. It also increases the Wasserstein distance of\npairwise haplotype distance distribution, indicating that a larger generation tempera-\nture makes the generated DNA sequences more diverse, and the gap from the original\ndistribution will slightly increase. Therefore, users can adjust the generation temper-\nature according to their needs, thereby controlling the diversity and authenticity of\nthe generated sequences.\n7 Comparisons of different versions of DNAGPT\nIn this section, we compared the results of three different DNAGPT variations. We\nconducted comparisons in GSR prediction, mRNA expression level prediction, and\nartificial human genomes generation task. We report the results in Figure. 6. In the\nGSR prediction task, we compared the three different DNAGPT variations in Figure.\n6 c. It can be seen that as the amount of pre-training data increases (Human reference\ngenomes - reference genomes from 9 species - reference genomes from all mammals),\nthe performance of downstream tasks also improves. This phenomenon can also be\nobserved in the mRNA expression level prediction task. In the Figure. 6 d, although\nDNAGPT-M and DNAGPT-H are neck-and-neck in the human mRNA expression\nlevel prediction task, DNAGPT-M performs better than DNAGPT-H in the mouse\nmRNA expression level prediction task.\nWe further compared DNAGPT-H and DNAGPT-M in the artificial human\ngenomes generation task. In the Figure. 6 e, the correlations of allele frequency for\nthe genomes generated by DNAGPT-M and DNAGPT-H are almost the same, with\n14\nFig. 6\na. Comparisons of Wasserstein distance, Correlation of allele frequency, and Correlation\nof linkage disequilibrium for DNAGPTs with generation temperature 0.8 and 1.2 respectively. b.\nComparisons of Wasserstein distance of pairwise haplotype distance distribution for DNAGPTs with\ngeneration temperature 0.8 and 1.2 respectively. c. Performance comparison for different DNAGPT\non GSR recognition tasks. d. Performance comparison for different DNAGPT on mRNA abundance\nprediction tasks. e. Correlations of allele frequency between genomes generated by DNAGPT-H and\nDNAGPT-M, specific on the sites with allele frequency less than 0.2 in the real genomes. f. Cor-\nrelations of LD between genomes generated by DNAGPT-H and DNAGPT-M. g. Average LD as a\nfunction of SNP distance after removing sites that are fixed in at least in one dataset. Pairwise SNP\ndistances were stratified into 50 bins and for each distance bin, the correlation was averaged over all\npairs of SNPs belonging to the bin. Green: GAN; Red: RBM; Light blue: DNAGPT-H; Dark blue:\nDNAGPT-M.\n15\nDNAGPT-M being slightly better at 0.96 compared to DNAGPT-H at 0.95. For the\nCorrelations of LD of genomes, as can be seen from the Figure. 6 f, both DNAGPT-\nM and DNAGPT-H maintain an excellent level with a value of 0.98. From this, we\nfurther investigated the performance level of LD when considering different distances\nbetween SNPs. The Figure. 6 g shows that both DNAGPT variations fit the real data\ndistribution better than GAN and RBM, with DNAGPT-M being slightly better than\nDNAGPT-H.\n8 Discussion\nIn summary, we have developed a multi-task pre-training model called DNAGPT for\nDNA sequence analysis to accommodate versatile downstream tasks across multiple\nspecies. We conducted the pre-training on reference genomes from as many as 9 differ-\nent species. Meanwhile, we introduced joint training of numbers and sequences during\nthe pre-training process. In order to better encode the relationships between inputs\nand outputs for versatile task formats, we designed a set of token languages to incor-\nporate sequence, number, and control tokens. For the pre-training tasks, to better\nunderstand the uniqueness of DNA sequences and the next token prediction task in\nGPT, we also introduced two pre-training tasks: GC content prediction and sequence\norder prediction. Finally, we utilized the token language to compile mixed inputs and\noutputs of DNA sequences and numerical properties.\nOur evaluation of DNAGPT on genomic signals and regions recognition tasks\nshowed that the model can accurately determine whether a given DNA sequence is\na genuine genomic signal or region. Furthermore, DNAGPT can also handle joint\ninputs of DNA sequences and mRNA half-lives to predict mRNA expression levels. In\nthe Artificial human genomes generation task, the AGs generated by DNAGPT rank\nhighly in various evaluation metrics, indicating that DNAGPT effectively comprehends\nthe underlying relationships and information within genomes.\nDespite its promising results, DNAGPT has several limitations that warrant fur-\nther investigation. One such limitation is the model\u2019s current focus on DNA sequences.\nExtending DNAGPT to handle multi-omics and spatial-omics data would greatly\nenhance its applicability and enable more comprehensive analyses of biological tasks.\nAnother area for improvement is the incorporation of multi-modal data, such as pathol-\nogy tissue images and disease diagnostic reports, which would provide a more holistic\nperspective on biological tasks. Additionally, addressing the challenge of processing\nlong sequence data, which is common in biological research, could be achieved by\nemploying memory-efficient model structures, such as RWKV [47] and RetNet [48].\nFinally, the efficient adaptation of DNAGPT should be explored, as users may not\nhave the resources to fine-tune the model. Techniques for efficient training of foun-\ndation models [49\u201351] could be tested and developed with DNAGPT, and zero-shot\nadaptation or \u2019emergent\u2019 abilities in biology foundation models could be further inves-\ntigated. Despite these limitations, exploring new avenues for DNAGPT\u2019s development\nholds the potential to significantly advance our understanding of DNA sequences and\ncontribute to a wide array of biological research applications.\n16\n9 Methods\nPre-training of DNAGPT\nFor DNAGPT-H, we collect the reference genomes from the Ensembl database [28]\nwith a total amount of 3 billion bps. During the data sampling stage, we employed a\nnon-overlapped k-mers sampling strategy to handle DNA sequence data. While sam-\npling, we removed sequences with an \u2019N\u2019(denoted as \u201dnot detected\u201d) content ratio\ngreater than 0.05. Moreover, we performed random flipping with a probability of 0.5.\nwe then encoded each input DNA sequence and numerical information according to\nthe token language and the pre-training tasks we designed. DNAGPT-H consists of\n12 layers of transformer blocks based on unidirectional attention, with each layer con-\ntaining 12 attention heads and a hidden layer size of 768. The number of trained\nparameters in the model is 0.1 billion. The learning rate is set to 1e-4 with a cosine\ndecay scheduler. The weight decay is set to 1e-2. The optimizer we choose is AdamW\nwith the betas set to (0.9, 0.95) and momentum set to 0.937. We employed mixed\nprecision for pre-training. The model was pre-trained for 15 epochs. The pre-training\nof the model on 8 Nvidia V100 32GB GPUs took approximately one day.\nFor DNAGPT-M, we collected reference genome information of 9 species from\nthe Ensembl database [28], including arabidopsis thaliana, caenorhabditis elegans,\nbos taurus,\ndanio rerio,\ndrosophila melanogaster,\nescherichia coli gca 001721525,\nhomo sapiens, mus musculus, saccharomyces cerevisiae. Subsequently, we removed the\nmitochondrial genomes from the majority of the species in the preprocessing proce-\ndure. After preprocessing, the number of bps in the genome of each species is: arabidop-\nsis thaliana (119146348 bps), caenorhabditis elegans (100272607 bps), bos taurus\n(2628394923 bps), danio rerio (1345101833 bps), drosophila melanogaster (137547960\nbps), escherichia coli gca 001721525 (5176750 bps), homo sapiens (3088286401 bps),\nmus musculus (2723414844 bps), saccharomyces cerevisiae (12071326 bps). The total\namount of bps is 10159412992. The architecture and training strategies are the same\nas DNAGPT-H.\nSimilar to DNAGPT-M, DNAGPT-S-512 used the same model as well as the\nhyperparameters, but the pre-training data changed from genomes of 9 species to the\nreference genomes of all the mammals with a total amount of approximately 200 billion\nbps. DNAGPT-S-512 was trained on the data for 2 epochs and took approximately\none week to finish the pre-training stage.\nNon-overlapping k-mers tokenization\nA k-mer strategy composes k consecutive nucleotides into one token. Previous k-mers\nmethods often adopt overlapped tokenization, that is, regardless of the value of k, the\nshift during each sampling is always 1, resulting in (N + k \u2212 1) tokens for a N-length\nsequence. In the non-overlapped k-mers strategy, the shift is equal to K, resulting in\nN/k tokens for an N-length sequence and improving the efficiency by k times.\n17\nFine-tuning of DNAGPT\nWhen fine-tuning DNAGPTs, Firstly, we should set the input sequence information\nto organize the data and initialize the model, and the model can automatically ini-\ntialize suitable encoding heads. For example, for classification and generation tasks,\nthe sequence embedding and classification heads are activated for input and output.\nFor regression tasks and more complex composite tasks, DNAGPT first composes the\ninput for joint embeddings and then selects regression heads for task output. After\nthe embedding layer and task heads are set, the pre-trained weights are loaded into\nthe model, and the weights of unused heads will be discarded. Then we can fine-tune\nDNAGPTs using data from the downstream tasks. We use the same hyperparameters\nacross all downstream tasks. For 0.1B models, the hyperparameters are set to: max\nlearning rate, 3 \u00d7 10\u22125; learning scheduler, cosine with warmup; optimizer, AdamW;\nwarmup epoch, 3; weight decay, 1e\u22121; batch size, 8; For 3B models, the hyperparam-\neters are set to: max learning rate, 3 \u00d7 10\u22126; learning scheduler, cosine with warmup;\noptimizer, AdamW; warmup epoch, 3; weight decay, 1e \u2212 1; batch size, 8.\nIn the genomic signals and region recognition, we use the sequence embedding and\nclassification head. The evaluation metrics are ACC (Accuracy), F1 (F1 score), MCC\n(Matthews Correlation Coefficient), Precision, and Recall. We report the complete\nresults in the Table. S2. In mRNA expression levels prediction, both the sequence\nembedding and the number embedding are invoked to handle the input of sequences\nand numbers. For the output, the regression head is used to predict the expression level.\nIn artificial human genomes generation, only the sequence embedding and classification\nhead are used to handle input and output sequences. During fine-tuning, we add a\nstop symbol at the last position of the input sequence. When generating sequences,\nwe remove all sequences that do not have the stop symbol or those with incorrect stop\nsymbol positions in the post-processing step. For temperature adjustment, we keep\nthe training epoch and other hyper-parameters unchanged.\nReferences\n[1] Tipper, D.J., Strominger, J.L.: Mechanism of action of penicillins: a proposal\nbased on their structural similarity to acyl-d-alanyl-d-alanine. Proceedings of the\nNational Academy of Sciences 54(4), 1133\u20131141 (1965)\n[2] Gieg\u00b4e, R., J\u00a8uhling, F., P\u00a8utz, J., Stadler, P., Sauter, C., Florentz, C.: Structure\nof transfer rnas: similarity and variability. Wiley Interdisciplinary Reviews: RNA\n3(1), 37\u201361 (2012)\n[3] Chen, A., Sun, Y., Lei, Y., Li, C., Liao, S., Meng, J., Bai, Y., Liu, Z., Liang, Z.,\nZhu, Z., et al.: Single-cell spatial transcriptome reveals cell-type organization in\nthe macaque cortex. Cell (2023)\n[4] McNerney, M.P., Doiron, K.E., Ng, T.L., Chang, T.Z., Silver, P.A.: Theranostic\ncells: emerging clinical applications of synthetic biology. Nature Reviews Genetics\n22(11), 730\u2013746 (2021)\n18\n[5] Shendure, J., Ji, H.: Next-generation dna sequencing. Nature biotechnology\n26(10), 1135\u20131145 (2008)\n[6] Andolfatto, P.: Adaptive evolution of non-coding dna in drosophila. Nature\n437(7062), 1149\u20131152 (2005)\n[7] Gibbs, R.A.: The human genome project changed everything. Nature Reviews\nGenetics 21(10), 575\u2013576 (2020)\n[8] Brosius, J., Palmer, M.L., Kennedy, P.J., Noller, H.F.: Complete nucleotide\nsequence of a 16s ribosomal rna gene from escherichia coli. Proceedings of the\nNational Academy of Sciences 75(10), 4801\u20134805 (1978)\n[9] Morley, M., Molony, C.M., Weber, T.M., Devlin, J.L., Ewens, K.G., Spielman,\nR.S., Cheung, V.G.: Genetic analysis of genome-wide variation in human gene\nexpression. Nature 430(7001), 743\u2013747 (2004)\n[10] Pearson, H.: What is a gene? Nature 441(7092), 398\u2013402 (2006)\n[11] Luo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu, T.-Y.: Biogpt:\ngenerative pre-trained transformer for biomedical text generation and mining.\nBriefings in Bioinformatics 23(6), 409 (2022)\n[12] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,\nWhitehead, S., Berg, A.C., Lo, W.-Y., et al.: Segment anything. arXiv preprint\narXiv:2304.02643 (2023)\n[13] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n[14] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S., Bern-\nstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities and\nrisks of foundation models. arXiv preprint arXiv:2108.07258 (2021)\n[15] Ji, Y., Zhou, Z., Liu, H., Davuluri, R.V.: Dnabert: pre-trained bidirectional\nencoder representations from transformers model for dna-language in genome.\nBioinformatics 37(15), 2112\u20132120 (2021)\n[16] Dalla-Torre,\nH.,\nGonzalez,\nL.,\nMendoza-Revilla,\nJ.,\nCarranza,\nN.L.,\nGrzywaczewski, A.H., Oteri, F., Dallago, C., Trop, E., Sirelkhatim, H., Richard,\nG., et al.: The nucleotide transformer: Building and evaluating robust foundation\nmodels for human genomics. bioRxiv, 2023\u201301 (2023)\n[17] Yelmen, B., Decelle, A., Ongaro, L., Marnetto, D., Tallec, C., Montinaro, F.,\nFurtlehner, C., Pagani, L., Jay, F.: Creating artificial human genomes using\ngenerative neural networks. PLoS genetics 17(2), 1009303 (2021)\n19\n[18] Andrews, G., Fan, K., Pratt, H.E., Phalke, N., \u00a7, Z.C., Karlsson, E.K., Lindblad-\nToh, K., Gazal, S., Moore, J.E., Weng, Z.: Mammalian evolution of human cis-\nregulatory elements and transcription factor binding sites. Science 380(6643),\n7930 (2023)\n[19] Wang, R., Jiang, Y., Jin, J., Yin, C., Yu, H., Wang, F., Feng, J., Su, R., Nakai, K.,\nZou, Q., et al.: Deepbio: an automated and interpretable deep-learning platform\nfor high-throughput biological sequence prediction, functional annotation and\nvisualization analysis. Nucleic Acids Research 51(7), 3017\u20133029 (2023)\n[20] Consortium, E.P., et al.: An integrated encyclopedia of dna elements in the human\ngenome. Nature 489(7414), 57 (2012)\n[21] Chen, K., Zhao, H., Yang, Y.: Capturing large genomic contexts for accurately\npredicting enhancer-promoter interactions. Briefings in Bioinformatics 23(2), 577\n(2022)\n[22] Wang, Z., Zhang, Y., Liu, Y., Xiong, S., Wang, M., Zhou, J., Gong, M.: Towards\na better understanding of tf-dna binding prediction from genomic features.\nComputers in Biology and Medicine, 105993 (2022)\n[23] Lee, D., Yang, J., Kim, S.: Learning the histone codes with large genomic win-\ndows and three-dimensional chromatin interactions using transformer. Nature\nCommunications 13(1), 6678 (2022)\n[24] Kalkatawi, M., Magana-Mora, A., Jankovic, B., Bajic, V.B.: Deepgsr: an opti-\nmized deep-learning structure for the recognition of genomic signals and regions.\nBioinformatics 35(7), 1125\u20131132 (2019)\n[25] Guo, Y., Zhou, D., Li, P., Li, C., Cao, J.: Context-aware poly (a) signal prediction\nmodel via deep spatial\u2013temporal neural networks. IEEE Transactions on Neural\nNetworks and Learning Systems (2022)\n[26] Zhu, G., Fan, Y., Li, F., Choi, A.T.H., Tan, Z., Cheng, Y., Li, K., Wang, S.,\nLuo, C., Liu, H., et al.: Gsrnet, an adversarial training-based deep framework\nwith multi-scale cnn and bigru for predicting genomic signals and regions. Expert\nSystems with Applications, 120439 (2023)\n[27] Agarwal, V., Shendure, J.: Predicting mrna abundance directly from genomic\nsequence using deep convolutional neural networks. Cell reports 31(7), 107663\n(2020)\n[28] Cunningham, F., Allen, J.E., Allen, J., Alvarez-Jarreta, J., Amode, M.R.,\nArmean, I.M., Austine-Orimoloye, O., Azov, A.G., Barnes, I., Bennett, R., et al.:\nEnsembl 2022. Nucleic acids research 50(D1), 988\u2013995 (2022)\n[29] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\n20\nKaiser,  L., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation processing systems 30 (2017)\n[30] Bollerslev, T.: Generalized autoregressive conditional heteroskedasticity. Journal\nof econometrics 31(3), 307\u2013327 (1986)\n[31] Li, Z., Chen, Z., Yang, F., Li, W., Zhu, Y., Zhao, C., Deng, R., Wu, L., Zhao,\nR., Tang, M., et al.: Mst: Masked self-supervised transformer for visual repre-\nsentation. Advances in Neural Information Processing Systems 34, 13165\u201313176\n(2021)\n[32] Gillioz, A., Casas, J., Mugellini, E., Abou Khaled, O.: Overview of the\ntransformer-based models for nlp tasks. In: 2020 15th Conference on Computer\nScience and Information Systems (FedCSIS), pp. 179\u2013183 (2020). IEEE\n[33] Geoffroy, V., Herenger, Y., Kress, A., Stoetzel, C., Piton, A., Dollfus, H.,\nMuller, J.: Annotsv: an integrated tool for structural variations annotation.\nBioinformatics 34(20), 3572\u20133574 (2018)\n[34] Meyers, B.C., Tingey, S.V., Morgante, M.: Abundance, distribution, and tran-\nscriptional activity of repetitive elements in the maize genome. Genome Research\n11(10), 1660\u20131676 (2001)\n[35] Dillon, L.W., Kumar, P., Shibata, Y., Wang, Y.-H., Willcox, S., Griffith, J.D.,\nPommier, Y., Takeda, S., Dutta, A.: Production of extrachromosomal microdnas\nis linked to mismatch repair pathways and transcriptional activity. Cell reports\n11(11), 1749\u20131759 (2015)\n[36] Basehoar, A.D., Zanton, S.J., Pugh, B.F.: Identification and distinct regulation\nof yeast tata box-containing genes. Cell 116(5), 699\u2013709 (2004)\n[37] Remmele, C.W., Xian, Y., Albrecht, M., Faulstich, M., Fraunholz, M., Heinrichs,\nE., Dittrich, M.T., M\u00a8uller, T., Reinhardt, R., Rudel, T.: Transcriptional landscape\nand essential genes of neisseria gonorrhoeae. Nucleic acids research 42(16), 10579\u2013\n10595 (2014)\n[38] Korhonen, J.A., Gaspari, M., Falkenberg, M.: Twinkle has 5-\u00bf 3 dna heli-\ncase activity and is specifically stimulated by mitochondrial single-stranded\ndna-binding protein. Journal of Biological Chemistry 278(49), 48627\u201348632\n(2003)\n[39] Juo, Z.S., Chiu, T.K., Leiberman, P.M., Baikalov, I., Berk, A.J., Dickerson, R.E.:\nHow proteins recognize the tata box. Journal of molecular biology 261(2), 239\u2013\n254 (1996)\n[40] McLauchlan, J., Gaffney, D., Whitton, J.L., Clements, J.B.: The consensus\nsequence ygtgttyy located downstream from the aataaa signal is required for\n21\nefficient formation of mrna 3 termini. Nucleic acids research 13(4), 1347\u20131368\n(1985)\n[41] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,\nHawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.: Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794 (2020)\n[42] Consortium, .G.P., et al.: A global reference for human genetic variation. Nature\n526(7571), 68 (2015)\n[43] Abdi, H., Williams, L.J.: Principal component analysis. Wiley interdisciplinary\nreviews: computational statistics 2(4), 433\u2013459 (2010)\n[44] Boehnke, M.: Allele frequency estimation from data on relatives. American\njournal of human genetics 48(1), 22 (1991)\n[45] Reich, D.E., Cargill, M., Bolk, S., Ireland, J., Sabeti, P.C., Richter, D.J., Lavery,\nT., Kouyoumjian, R., Farhadian, S.F., Ward, R., et al.: Linkage disequilibrium in\nthe human genome. Nature 411(6834), 199\u2013204 (2001)\n[46] Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial net-\nworks. In: International Conference on Machine Learning, pp. 214\u2013223 (2017).\nPMLR\n[47] Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng,\nX., Chung, M., Grella, M., GV, K.K., et al.: Rwkv: Reinventing rnns for the\ntransformer era. arXiv preprint arXiv:2305.13048 (2023)\n[48] Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., Wei, F.: Retentive\nnetwork: A successor to transformer for large language models. arXiv preprint\narXiv:2307.08621 (2023)\n[49] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021)\n[50] Hu, Z., Lan, Y., Wang, L., Xu, W., Lim, E.-P., Lee, R.K.-W., Bing, L., Poria,\nS.: Llm-adapters: An adapter family for parameter-efficient fine-tuning of large\nlanguage models. arXiv preprint arXiv:2304.01933 (2023)\n[51] Wang, S., Zhang, D., Zhang, J., Zhang, W., Li, R.: Bootstrap the original\nlatent: Freeze-and-thaw adapter for back-propagated black-box adaptation. arXiv\npreprint arXiv:2303.03709 (2023)\n[52] Lizio, M., Harshbarger, J., Shimoji, H., Severin, J., Kasukawa, T., Sahin, S.,\nAbugessaisa, I., Fukuda, S., Hori, F., Ishikawa-Kato, S., et al.: Gateways to the\nfantom5 promoter level mammalian expression atlas. Genome biology 16, 1\u201314\n22\n(2015)\n[53] Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: Deepspeed: System optimiza-\ntions enable training deep learning models with over 100 billion parameters. In:\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pp. 3505\u20133506 (2020)\n23\nS1 Supplementary\nS1.1 Comparisons to other models\nWe further compare the performance on the datasets used in NT [16], this dataset\ncontains more GSR recognition tasks.All the tasks in the dataset are classification\ntasks. For DNAGPT-S-512, the hyperparameters are set to: max learning rate, 3 \u00d7\n10\u22125; learning scheduler, cosine with warmup; optimizer, AdamW; warmup epoch,\n3; weight decay, 1e \u2212 1; For DNAGPT-B-512, the hyperparameters are set to: max\nlearning rate, 3 \u00d7 10\u22126; learning scheduler, cosine with warmup; optimizer, AdamW;\nwarmup epoch, 3; weight decay, 1e\u22121. The results are proposed in S1. Our DNAGPT-\nB-512 is comparable to the NT-2.5B-850 model, and DNAGPT-S-512 is comparable\nto the NT-2.5B-3202 model in the NT dataset.\nTable S1 Full results of DNAGPT-B-512 on NT datasets. The Matthews correlation\ncoefficient (MCC) is used as the metric.\nTask name\nNT\n500M-1\nNT\n2.5B-3202\nNT\n2.5B-850\nDNAGPT-S-512\nDNAGPT-B-512\nH3\n72.0\n75.0\n79.0\n75.0\n81.0\nH3K4me1\n36.0\n42.0\n54.0\n41.0\n53.0\nH3K4me2\n27.0\n28.0\n32.0\n26.0\n32.0\nH3K4me3\n24.0\n31.0\n41.0\n32.0\n38.0\nH3K9ac\n45.0\n49.0\n55.0\n48.0\n56.0\nH3K14ac\n37.0\n45.0\n54.0\n46.0\n52.0\nHK36me3\n45.0\n53.0\n62.0\n56.0\n58.0\nHK79me3\n57.0\n57.0\n62.0\n57.0\n61.0\nH4\n75.0\n79.0\n81.0\n78.0\n83.0\nH4ac\n33.0\n41.0\n49.0\n43.0\n47.0\nPromoter all\n88.0\n91.0\n91.0\n91.0\n93.0\nPromoter non-tata\n91.0\n93.0\n94.0\n92.0\n95.0\nPromoter tata\n78.0\n76.0\n79.0\n80.0\n83.0\nS1.2 Other results of DNAGPTs on genomic signals and\nregions recognition\nFull results of DNAGPTs on genomic signals and regions recognition\nWe show in the Table. S2 the results of DNAGPT-M on various datasets of GSR\nrecognition task, and the results of DNAGPT-S-512 in the Table. S3. Bothe of the\nDNAGPTs demonstrates stable results across different GSR recognition datasets from\nvarious species and the performance of DNAGPT-S-512 is the best..\nAttention maps of DNAGPT-M\nWe show the attention map of each layer in DNAGPT-M in Figure S1 a. The input\nsequence is PAS (AATAAA) sequence where the PAS site is located in the middle\nof the sequence. We can observe that almost all layers focus on the latter half of the\narea, with shallow and deep layers having a more widespread attention compared to\n24\nTable S2 Full results of DNAGPT-M on genomic signals and regions recognition.\nTask name\nacc(%)\nf1(%)\nmcc(%)\nprecision(%)\nrecall(%)\nHuman PAS(AATAAA)\n91.51\n91.51\n82.99\n91.52\n91.47\nHuman PAS(all)\n90.63\n90.64\n81.28\n90.64\n90.64\nHuman TIS(ATG)\n97.46\n97.46\n94.92\n97.47\n97.46\nMouse PAS(AATAAA)\n91.43\n91.41\n82.83\n91.40\n91.43\nMouse PAS(all)\n89.62\n89.62\n79.24\n89.63\n89.61\nMouse TIS(ATG)\n97.84\n97.84\n95.68\n97.85\n97.83\nFruitfly PAS(AATAAA)\n91.88\n91.87\n83.84\n91.96\n91.88\nFruitfly PAS(all)\n92.37\n92.38\n84.76\n92.38\n92.38\nFruitfly TIS(ATG)\n97.00\n97.00\n94.01\n97.00\n97.00\nBovine PAS(AATAAA)\n89.79\n89.77\n79.65\n89.89\n89.76\nBovine PAS(all)\n90.49\n90.49\n80.99\n90.49\n90.49\nBovine TIS(ATG)\n96.95\n96.95\n93.90\n96.95\n96.95\nTable S3 Full results of DNAGPT-S-512 on genomic signals and regions\nrecognition.\nTask name\nacc(%)\nf1(%)\nmcc(%)\nprecision(%)\nrecall(%)\nHuman PAS(AATAAA)\n92.74\n92.74\n85.49\n92.75\n92.74\nHuman PAS(all)\n92.05\n92.04\n84.11\n92.07\n92.04\nHuman TIS(ATG)\n97.91\n97.91\n95.83\n97.92\n97.95\nMouse PAS(AATAAA)\n91.69\n91.69\n83.39\n91.69\n91.70\nMouse PAS(all)\n91.66\n91.66\n83.33\n91.66\n91.67\nMouse TIS(ATG)\n97.84\n97.84\n95.79\n97.85\n97.85\nFruitfly PAS(AATAAA)\n93.09\n93.09\n86.17\n93.08\n93.08\nFruitfly PAS(all)\n93.19\n93.18\n86.47\n93.27\n93.19\nFruitfly TIS(ATG)\n97.13\n97.13\n94.28\n97.14\n97.13\nBovine PAS(AATAAA)\n91.65\n91.65\n83.31\n91.68\n91.64\nBovine PAS(all)\n91.74\n91.74\n83.50\n91.75\n91.74\nBovine TIS(ATG)\n97.17\n97.17\n94.34\n97.17\n97.16\nthe middle layers. We can also notice that the attention map of the shallow areas is\nsmoother than that of the deep areas. Although the attention range of the deep layers\nis as extensive as those of the shallow layers, the deep networks tend to focus on a\nfew specific tokens rather than presenting a smooth state like the shallow attention\nmap. This indicates that some regions in non-coding areas may be more critical for\nPAS recognition compared to other areas. We have also displayed the attention map\nfor each layer with TIS data. In the Figure. S1 b, we display the attention maps of\neach layer of DNAGPT-M with TIS input. Interestingly, compared to the attention\nmap with PAS as input, the information focused on by the model in the shallow layers\nis more consistent, with a notable difference only in Layer 1. In the later layers, the\nattention map for TIS input starts to focus on information from tokens in earlier\npositions, i.e., non-coding region information. This suggests that the information the\nmodel focuses on in the shallow layers is more approximate, but in the deep networks,\nit can more precisely pinpoint the locations of important tokens.\n25\nTable S4 Full results of DNAGPT-B-512 on genomic signals and regions\nrecognition.\nTask name\nacc(%)\nf1(%)\nmcc(%)\nprecision(%)\nrecall(%)\nHuman PAS(AATAAA)\n93.20\n93.20\n86.73\n93.20\n93.20\nHuman PAS(all)\n92.65\n92.65\n85.02\n92.66\n92.65\nHuman TIS(ATG)\n98.02\n98.02\n96.30\n98.02\n98.02\nMouse PAS(AATAAA)\n91.86\n91.86\n84.22\n91.87\n91.87\nMouse PAS(all)\n92.78\n92.78\n85.08\n92.79\n92.79\nMouse TIS(ATG)\n97.96\n97.96\n95.93\n97.96\n97.96\nFruitfly PAS(AATAAA)\n94.16\n94.16\n87.78\n94.16\n94.16\nFruitfly PAS(all)\n93.86\n93.86\n87.14\n93.86\n93.86\nFruitfly TIS(ATG)\n97.24\n97.24\n94.28\n97.24\n97.24\nBovine PAS(AATAAA)\n92.36\n92.36\n84.68\n92.36\n92.37\nBovine PAS(all)\n92.64\n92.64\n84.92\n92.64\n92.64\nBovine TIS(ATG)\n97.78\n97.78\n94.92\n97.78\n97.78\nFig. S1 Attention maps of each layer of DNAGPT-M with TIS input.\n26\nS1.3 All tokens used in DNAGPT\nFig. S2 All the tokens we used in our DNAGPT. We present the token name, the tokens (For cases\nwith a large number of tokens, we only provided some examples.) and the token description.\n27\nThere are 6 categories of tokens in the token language of DNAGPT. The Sequence\ntokens are the DNA sequences encoded with kmers tokenization strategy. For exam-\nple, if we utilize 6-mers sampling and only consider the encoding of \u2019A, C, G, T, N\u2019,\nthen the total amount of discrete tokens are 56 +55 +54 +53 +52 +51 which is 19530.\nWhen comes to the Number tokens, we directly input the numbers into the Numer-\nical embedding layer and Regression head layer to encode and decode them as the\nnumber tokens. For binary classification tasks, we utilize \u2019A\u2019 and \u2019N\u2019 to distinguish\nTrue from False. The Instruction tokens are used to identify the input and output\ntype of sequence. For DNA sequences from different species, we assign an instruction\ntoken for each species. Specifically, we also assign instruction tokens for Classification\ntasks and Numerical tokens which can prompt the model to generate corresponding\ntypes of tokens separately. In biological sequences, there is no natural logical relation-\nship between tokens like in the natural language. In the design of DNAGPT tokens,\nto enable the model to understand the relationships among sequences, we design two\nconnection tokens to guide the relationships between sequences before and after the\nconnection tokens. Here, \u2019+\u2019 represents the fusion of preceding and succeeding infor-\nmation, and \u2019=\u2019 represents the cause-effect relationship, with the input being before\n\u2019=\u2019 and the output being after \u2019=\u2019. Finally, in order to better adapt to different types\nof downstream tasks, we also reserve some special tokens.\nS1.4 Datasets\nS1.4.1 Genomic signals and regions recognition\nThe datasets used for genomic signals and regions recognition are cDNA data. We\nextracted both polyadenylation signals (PAS) and translation initiation sites (TIS)\nfrom four genomes. For the Homo sapiens (human) genome, the human assembly\nGRCh37 (also known as hg19) was employed, while the primary assembly GRCm38\nwas used for the Mus musculus (mouse) genome. The cDNA data for these genomes\nwere sourced from the Mammalian Gene Collection (MGC). For the Bos taurus\n(bovine) genome, the assembly Bos taurus UMD 3.1.1 was utilized, with the cDNA\ndata being downloaded from the Ensembl organization. Finally, for the Drosophila\nmelanogaster (fruit fly) genome, Release 6 \u2013 annotation release Dmel Release 6.01 was\nemployed, and the cDNA data was obtained from FlyBase. The sampling method is as\nfollows: first, locate the positions of GSRs, then extract 300 bps of sequence from both\nbefore and after the GSRs, and concatenate them together. It is important to note\nthat the GSR motif will be removed during preprocessing to ensure that the model\ncan recognize GSRs based solely on the information near the GSR motif, rather than\nthe GSR itself. For the negative samples, the sampled sequences should satisfy the\nfollowing requirements:\n(1) Sequences with the same motifs but not related to polyadenylation and\ntranslation processes.\n(2) Sequences are sampled from the chromosome whose average GC-content was\nnearest to the entire genome\u2019s average GC-content.\nConsequently, negative data for human, mouse, bovine, and fruit fly were extracted\nfrom chromosomes 21, 13, 28, and X, respectively.\n28\nThe amounts of positive samples for each dataset are shown in Table. S5.\nTable S5 Amounts of positive samples for different\ndatasets.\nGSRs\nHuman\nMouse\nBovine\nFruit fly\nTIS\n28,244\n25,205\n17,558\n30,283\nPAS(AATAAA)\n11,302\n11,393\n7,862\n18,641\nALL\n20,933\n18,693\n12,082\n27,203\nS1.4.2 Artificial human genomes generation\nFor artificial human genomes generation, we utilized 1000 Genomes data [42] as the\nfine-tuning dataset. There are 2504 individuals (5008 haplotypes) in the dataset and\nthe data we used is a dense 10000 SNP range/region from chromosome 15. When\nevaluating, the model produced 5000 sequences of SNPs for analysis. All our analyses\nwere conducted on the generated data.\nS1.4.3 mRNA expression levels prediction\nThe dataset is composed of human protein-coding gene sequences located upstream\nand downstream of the transcription start site (TSS). The promoter of the gene is\nfound in the sequence upstream of the TSS, while the exons and introns of the gene\nare found downstream. The input sequences are sourced from the Xpresso[27]. In this\ndataset, the TSS positions were meticulously revised by the authors of Xpresso using\nCap Analysis Gene Expression (CAGE) [52], a technique for determining the actual\nTSS location. The Xpresso dataset consists of 18,377 promoters, divided into 16,377\nfor training, 1,000 for validation, and 1,000 for testing as mentioned in the Xpresso[27].\nThe maximum length of a promoter\u2019s TSS sequence is set to 20,000 base pairs. The\ndefault sample range in xpresso is from 3000 to 13500 when DNAGPT can utilize the\nwhole sequence. Additionally, the Xpresso DNA input includes half-life features that\nprovide general information about the gene, such as gene length and the number of\nintrons. The default feature input is an 8-bit array.\nS1.5 Experiment details\nS1.5.1 Pre-training details\nWe show the detailed training information and hyper-parameters of our DNAGPTs\nin Figure S3. We utilize deepspeed [53] and FSDP in the pretraining process. We also\nutilize json data format to organize the data in order to accelerate the training of\nDNAGPT. DNAGPT-H, DNAGPT-M, DNAGPT-S-512 are pretrained on 8 \u00d7 V100\nGPUs and DNAGPT-B-512 is pretrained on 16 \u00d7 V100 GPUs.\n29\nFig. S3 Pre-training details of DNAGPTs.\nS1.5.2 Fine-tune details\nSpecifically, we report the fine-tune details on GSR recognition dataset for each\nDNAGPT in Figure. S4. We fine-tuned out model for 10 epochs on each dataset and\ndecrease the learning rate to one-third of which in the pre-training time. In the mRNA\nprediction task and artificial human genomes generation, we use DNAGPT-H and\nDNAGPT-M and the settings remains the same as mentioned in Figure. S4. Specifi-\ncally, in the artificial human genomes generation task, we employed a post-processing\nstage to filter out sequences that do not meet the requirements by examining whether\nthe predetermined stop signal token is in the correct position.\n30\nFig. S4 Fine-tune details of DNAGPT on the GSR recognition task.\n31\n"
  },
  {
    "title": "SITTA: A Semantic Image-Text Alignment for Image Captioning",
    "link": "https://arxiv.org/pdf/2307.05591.pdf",
    "upvote": "4",
    "text": "Linear Alignment of Vision-language Models for Image Captioning\nFabian Paischer 1 Markus Hofmarcher 1 2 Sepp Hochreiter 1 Thomas Adler 1\nAbstract\nRecently, vision-language models like CLIP have\nadvanced the state of the art in a variety of multi-\nmodal tasks including image captioning and cap-\ntion evaluation. Many approaches adapt CLIP-\nstyle models to a downstream task by training a\nmapping network between CLIP and a language\nmodel. This is costly as it usually involves calcu-\nlating gradients for large models. We propose a\nmore efficient training protocol that fits a linear\nmapping between image and text embeddings of\nCLIP via a closed-form solution. This bypasses\nthe need for gradient computation and results in\na lightweight captioning method called ReCap,\nwhich can be trained up to 1000 times faster than\nexisting lightweight methods. Moreover, we pro-\npose two new learning-based image-captioning\nmetrics that build on CLIP score along with our\nlinear mapping. Furthermore, we combine Re-\nCap with our new metrics to design an itera-\ntive datastore-augmentation loop (DAL) based\non synthetic captions. We evaluate ReCap on MS-\nCOCO, Flickr30k, VizWiz, and MSRVTT. ReCap\nachieves performance comparable to state-of-the-\nart lightweight methods on established metrics\nwhile outperforming them on our new metrics,\nwhich are better aligned with human ratings on\nFlickr8k-Expert and Flickr8k-Crowdflower. Fi-\nnally, we demonstrate that ReCap transfers well\nto other domains and that our DAL leads to a\nperformance boost.\n1. Introduction\nVision-language models (VLMs) are usually trained to align\nimages and texts in a joint bi-modal embedding space. This\nenables their application to a variety of downstream tasks\n1ELLIS Unit Linz, LIT AI Lab, Institute for Machine Learn-\ning, Johannes Kepler University Linz, Austria 2JKU LIT SAL\neSPML Lab, Institute for Machine Learning, Johannes Kepler\nUniversity, Linz, Austria. Correspondence to: Fabian Paischer\n<paischer@ml.jku.at>.\nPreprint. Under review.\nsuch as image-text retrieval (Ramos et al., 2023b), image\ncaptioning (Mokady et al., 2021), few-shot classification\n(Ouali et al., 2023), and caption evaluation (Hessel et al.,\n2021). As one of the most prominent VLMs, CLIP (Rad-\nford et al., 2021) has been pre-trained on a large-scale web\ndataset consisting of image-text pairs and advanced the state\nof the art across a variety of vision-language tasks. One of\nthe most important downstream tasks is image captioning.\nIt requires machines to generate informative descriptions\nfor images, which can be useful in various applications\nsuch as image retrieval, content-based image search, and\naccessibility for visually impaired individuals (Gurari et al.,\n2020).\nAdapting CLIP to a downstream task is generally costly\nin terms of both computational resources and data collec-\ntion. In the context of image captioning, related works train\nmapping networks between CLIP and a generative language\nmodel (LM) (Ramos et al., 2023b; Mokady et al., 2021; Zhu\net al., 2023; Liu et al., 2023b; Merullo et al., 2023). Inspired\nby these recent successes, we aim at linearly aligning image\nand text embeddings of CLIP-style models to leverage them\nfor retrieval augmentation in image captioning. This use\ncase of CLIP is based on cross-modal retrieval via cosine\nsimilarity. Artetxe et al. (2016) showed that a linear solu-\ntion to a constrained least-squares problem is equivalent\nto maximizing the cosine similarity (under the same con-\nstraint). Leveraging this insight, we maximize the cosine\nsimilarity of image-text correspondences from the down-\nstream dataset with respect to a constrained linear mapping.\nAs this problem has a closed-form solution, we are able\nto align CLIP to the downstream data without the need for\ngradient computation. This makes our proposed method\nextremely versatile as training takes only seconds and can\nbe conducted on CPU.\nWe propose a fast and easily deployable method for adapt-\ning CLIP to a target domain. Given a set of image-text\npairs representing a downstream task, we embed them in the\njoint embedding space of CLIP. Then we re-align them by\ncomputing a linear mapping via a constrained least-squares\nsolution (cf. Figure 1, a). The linear mapping introduces\nonly 0.0016% of trainable parameters compared to the orig-\ninal CLIP model. We demonstrate that this technique can\nbe readily incorporated into an image captioning pipeline\nvia retrieval augmentation (cf. Figure 1, b). Given a new\n1\narXiv:2307.05591v3  [cs.CV]  6 Feb 2024\nLinear Alignment of Vision-language Models for Image Captioning\nMapping W\nTraining\nCLIPVM\nVM\nCLIPLM\nDatastore\nPrompt\nFLAN-T5\nA little girl eating a piece of cake at a table.\nMapping W\nLittle girl eating a desert at the table \nZebras grazing on a grassy and rocky slope.\nA little girl eating a piece of cake. \nk=2\n(a)\n(b)\nCLIP\nFigure 1. (a) We train a linear mapping W to align the image and text embeddings of CLIP toward a dataset. (b) On inference, we employ\nthe mapping to retrieve captions from a datastore that are similar to the input image and provide these along with a prompt to a FLAN-T5\nmodel to generate a new caption.\nimage, we embed it in the CLIP embedding space and apply\nour mapping before retrieving similar captions via cosine\nsimilarity. These captions are then formatted to a prompt\nwhich is provided to a LM to generate a new caption for the\nimage. We call the resulting method Retrieval-augmented\nCaptioner (ReCap). Further, we propose two new learning-\nbased image-captioning metrics that use our linear align-\nment to adapt CLIP-based metrics (Hessel et al., 2021) to-\nward a downstream dataset. This consistently improves\ncorrelation with human judgement. Since they are learn-\ning based, they do not suffer from the common issue of\nrule-based metrics that rely on n-gram matching where cor-\nrect words that do not appear in the ground truth annota-\ntions are disfavored, e.g., CIDEr-D (Vedantam et al., 2015).\nFinally, we employ these metrics as a filter for datastore-\naugmentation with synthetic captions to improve caption-\ning performance of ReCap. We call this novel technique\nDatastore-Augmentation Loop (DAL, cf. Algorithm 1).\nWe evaluate ReCap on the MS-COCO (Lin et al., 2014),\nFlickr30k (Young et al., 2014), VizWiz (Gurari et al., 2020),\nand MSRVTT (Xu et al., 2016) datasets. By means of es-\ntablished metrics, ReCap achieves performance competitive\nto lightweight baselines that require over 1000 times more\ntraining effort on MS-COCO and Flickr30k, while outper-\nforming the only available retrieval-augmented baseline,\nSmallCap (Ramos et al., 2023b), on VizWiz and MSRVTT.\nAdditionally, we present evidence that ReCap transfers bet-\nter across domains than SmallCap. Further, we evaluate the\ncorrelation of our proposed metrics with human judgement\non two datasets, Flickr8k-Expert and Flickr8k-Crowdflower\n(Hodosh et al., 2013). Our metrics consistently improve\nover the CLIP-based metrics that rely on cosine similarity\n(Hessel et al., 2021) and set a new state of the art in three out\nof four categories. By means of our newly proposed metrics,\nwe outperform competitors on all four datasets. Moreover,\nwe show that our metrics can be used as a filter for synthetic\ncaptions to augment the retrieval datastore of ReCap. Fi-\nnally, we demonstrate that datastore augmentation via DAL\nleads to improved performance on our proposed metrics.\nIn the following, we summarize our key contributions.\n\u2022 We introduce ReCap, a new method for lightweight im-\nage captioning with only 1 M trainable parameters that\nreaches competitive or better performance than prior\napproaches that require substantially more compute\n\u2022 We propose aCLIP-S and RefaCLIP-S, two learning-\nbased metrics for caption evaluation that set a new state\nof the art in correlation with human judgement\n\u2022 We establish DAL, a datastore-augmentation loop for\nretrieval-based image captioning systems, and demon-\nstrate its gains on established benchmarks\n2. Methods\nWe propose a linear alignment method for CLIP that opti-\nmizes cosine similarity between image-text pairs coming\nfrom a downstream dataset. The linear alignment computes\na mapping in closed form under an orthogonality constraint.\nTherefore, it is very efficient to compute and easy to imple-\nment while only adding a relatively small set of trainable\nparameters. We elaborate on our linear alignment technique\nin more detail in Section 2.1. In Section 2.2 we introduce a\nlightweight image captioning pipeline based on our linear\nalignment without any further training. Subsequently, Sec-\ntion 2.3 introduces two new metrics, aCLIP-S, a reference-\nfree metric, and RefaCLIP-S, a reference-based metric, both\nof which are based on the CLIP score (Hessel et al., 2021) in\ncombination with our linear alignment. Finally, Section 2.4\ndescribes DAL, a novel datastore-augmentation loop for\nimage captioning, that iteratively adds synthetic captions of\nhigh quality to the retrieval datastore.\n2\nLinear Alignment of Vision-language Models for Image Captioning\nAlgorithm 1 Datastore-augmentation Loop via Synthetic Captions\nRequire: caption metric \u00b5(\u00b7, \u00b7), CLIP vision encoder \u03d5(\u00b7), CLIP text encoder \u03c8(\u00b7), batched nucleus sampling from language\nmodel LM(\u00b7, \u00b7), training set DTrain, validation set DVal, prompt p, hyperparameters k, l, m \u2208 N\nW \u2190 fit linear{(\u03d5(x), \u03c8(c)) | (x, c) \u2208 DTrain}\n\u25b7 Re-align CLIP for downstream data; cf. Eq. (1)\nC \u2190 {c | (x, c) \u2208 DTrain}\n\u25b7 Initialize datastore with training captions\nfunction RECAP(x, W , C)\nK \u2190 arg maxk\nc\u2208C cossim(\u03c8(c), W \u03d5(x))\n\u25b7 Select top-k captions for x; cf. Eq. (2)\nq \u2190 concat({p} \u222a K)\n\u25b7 Combine top-k captions into one prompt\nS \u2190 LM(q, l)\n\u25b7 Sample l responses of LM via nucleus sampling\nreturn arg maxs\u2208S cossim(\u03c8(s), W \u03d5(x))\n\u25b7 Return the response that fits x best; cf. Eq. (3)\nend function\nfor i \u2208 {1, . . . , m} do\n\u00af\u00b5 \u2190\n1\n|DVal|\nP\n(x,c)\u2208DVal \u00b5(RECAP(x, W , C), c)\n\u25b7 Compute average validation score\nC \u2190 C \u222a {c\u2032 | c\u2032 = RECAP(x, W , C) \u2227 \u00b5(c\u2032, c) > \u00af\u00b5 \u2227 (x, c) \u2208 DTrain}\n\u25b7 Add synthetic captions\nend for\n2.1. Linear Alignment of CLIP\nSince our downstream use of CLIP involves retrieval via co-\nsine similarity, we want to maximize the cosine similarity be-\ntween image and text embeddings of a downstream dataset.\nTo this end, we assume access to a dataset D = {(xi, ci)}\nthat provides image-text pairs, e.g., MS-COCO (Lin et al.,\n2014). First, we embed the images of the training split\nDTrain \u2282 D using a CLIP vision encoder \u03d5 : X \u2192 Rd,\nwhere X is the pixel space and d denotes the dimension\nof the joint CLIP embedding space. This results in an im-\nage embedding matrix FDTrain = (f1, . . . , fn)\u22a4 \u2208 Rn\u00d7d,\nwhere fi = \u03d5(xi) for i \u2208 {1, . . . , n} and n = |DTrain|.\nSimilarly, we embed the corresponding captions via the\nCLIP text encoder \u03c8 : T \u2192 Rd, where T is the space\nof tokenized strings, yielding a caption embedding matrix\nEDTrain = (e1, . . . , en)\u22a4 \u2208 Rn\u00d7d. If, like in the case of\nMS-COCO, we are presented with multiple captions per\nimage, then we assume the same image just appears multi-\nple times in D matched up with its corresponding captions.\nThis results in a one-to-one input-label correspondence.\nWe employ a linear mapping W \u2208 Rd\u00d7d to re-align CLIP\naccording to DTrain. We aim to find a mapping W that\nprojects an image embedding to the text embedding space\nsuch that its closest neighbor in terms of cosine similarity\nis its ground-truth caption. Yet, a closed-form solution\nfor W to maximize the cosine similarity is unknown. By\nconstraining W to be an orthogonal matrix, however, we\nobtain equivalence to the least-squares objective, that is\nW \u2217 =\narg max\nW s.t. W \u22a4W =I\nX\ni\ncossim(ei, W fi)\n=\narg min\nW s.t. W \u22a4W =I\nX\ni\n\u2225ei \u2212 W fi\u22252\n2 = V U \u22a4,\n(1)\nwhere V and U are the orthogonal matrices of the singu-\nlar value decomposition of E\u22a4\nDTrainFDTrain = U\u03a3V \u22a4 and\ncossim(\u00b7, \u00b7) is the usual cosine similarity for vectors. This\nfact has been shown by Artetxe et al. (2016) and we also\nprovide a proof in Appendix E for convenience. The solu-\ntion to the constrained optimization problem in Equation (1)\nis well known as orthogonal procrustes in the literature\n(Sch\u00a8onemann, 1966). Notably, the size of W varies with\nthe dimensionality d. Therefore, different CLIP encoders\nresult in different amounts of parameters introduced by W .\n2.2. Retrieval-augmented Image Captioning (ReCap)\nOur linear mapping W can be leveraged for task-specific\nalignment and gives rise to our novel lightweight image\ncaptioning method ReCap. The key idea is that we can\nrepresent a given image in the language space as a set of\nretrieved captions from a retrieval datastore that describe\nsimilar images. In turn, we can condition a pre-trained\nlanguage model (LM) on this set of captions to create a new\ncaption for the input image.\nWe utilize W for retrieval augmentation, where the retrieval\ndatastore C contains captions of the training set DTrain. Then\nwe project a given image to the caption embedding space\nand retrieve its nearest neighbors. Given an image x \u2208 X,\nwe compute an embedding \u03d5(x) and select the set K of\ntop-k captions by\nK =\nk\narg max\nc\u2208C\ncossim(\u03c8(c), W \u03d5(x)),\n(2)\nwhere arg maxk denotes an extension of the arg max opera-\ntor returning the arguments of the k largest elements of a set.\nThis way, we obtain a set of captions that provide a textual\ndescription of the image x. We feed the retrieved captions\n3\nLinear Alignment of Vision-language Models for Image Captioning\nK to a generative LM as context along with a prompt to\ngenerate a new caption for the image x (cf. Figure 1, b). We\nuse nucleus sampling (Holtzman et al., 2020) to obtain a\nset S of l candidate captions for the image x and select the\ncandidate which yields the highest cosine similarity by\narg max\ns\u2208S\ncossim(\u03c8(s), W f).\n(3)\nThe only trainable parameters of ReCap are W which only\nrequires computing a closed-form solution on CPU. Specifi-\ncally, computing W requires O(d3) steps. The function RE-\nCAP in Algorithm 1 shows pseudocode for our lightweight\nimage-captioning method.\n2.3. Image Caption Evaluation Metric\nGiven an image x and a candidate caption c we define the\naligned CLIP score as\naCLIP-S(c, x) = max{cossim(\u03c8(c), W \u03d5(x)), 0}. (4)\nNotably, aCLIP-S is reference-free, meaning it can be ap-\nplied to any candidate without access to ground-truth hu-\nman annotations, i.e. reference captions. In case a set\nR = {r1, r2, . . . } of reference captions is available, we\ncan incorporate those into our score, which results in a\nreference-based metric\nRefaCLIP-S(c, R, x) =\n(5)\nH{aCLIP-S(c, x), max{max\nr\u2208R cossim(\u03c8(c), \u03c8(r)), 0}},\nwhere H{\u00b7} denotes the harmonic mean of a set. Since our\nnew metrics use data to align CLIP to the downstream task,\nwe categorize them as learning-based (Cui et al., 2018).\n2.4. Datastore-augmentation Loop (DAL)\nWe aim to add synthetic captions of high quality to the\ndatastore, such that the over-all prediction quality of ReCap\nimproves. To measure the quality of synthetic captions we\nassume access to a metric \u00b5 : T \u00d7 T \u2192 R.1 We start by\nevaluating ReCap on the validation set and compute the\naverage metric \u00af\u00b5, which provides us with an estimate of the\nquality of generated captions. Next, we iterate over images\nfrom DTrain and create synthetic captions via ReCap. After\ncaption generation we compute \u00b5(\u00b7, \u00b7) for every synthetic\ncaption candidate and add only those to the datastore for\nwhich the score exceeds \u00af\u00b5. Then we evaluate on Dval again\nand update \u00af\u00b5. We repeat this process for a fixed number\nof iterations. Algorithm 1 shows the pseudocode for our\nproposed DAL.\n1We use notation for a reference-based metric. However, DAL\nworks just as well with a reference-free metric.\n3. Experiments\nWe show results for ReCap on the common captioning\nbenchmarks MS-COCO (Lin et al., 2014) and Flickr30k\n(Young et al., 2014) in Section 3.1. To investigate how\nReCap copes with noisy data and video captions, we addi-\ntionally show results for the VizWiz (Gurari et al., 2020)\nand MSRVTT (Xu et al., 2016) datasets. Further, we investi-\ngate the transfer capabilities of (i) our linear mapping alone\nand (ii) of mapping and datastore combined across different\ndomains. In Section 3.2 we evaluate our proposed image\ncaptioning metrics on the Flickr8k-Expert and Flickr8K-\nCrowdflower datasets (Hodosh et al., 2013). Finally, in\nSection 3.3 we demonstrate that synthetic captions lead to a\nboost in performance on our proposed metrics.\n3.1. ReCap\nImplementation Details\nDuring downstream evaluation\nof our linear alignment we rely on cosine similarity for re-\ntrieval of texts related to an image. Therefore, we evaluate\nall CLIP vision encoders on cross-modal retrieval tasks in\nAppendix B to find a suitable encoder for ReCap. Based\non our findings, we choose RN50\u00d764 (He et al., 2016) as\nour retrieval model.2 After embedding images and captions\nwe normalize and center them as suggested by Artetxe et al.\n(2016). To compute our mapping, we use orthogonal pro-\ncrustes by default as described by Equation (1). In certain\nsettings, we use an unconstrained version, i.e., ordinary least\nsquares. We elaborate in Appendix B which version we use\nfor the different experiments.\nTo find the best setting for image captioning, we search over\ndifferent LMs, decoding strategies, and prompt orderings.\nWe only considered generative LMs that are publicly avail-\nable on the huggingface hub (Wolf et al., 2020). Moreover,\nwe search over multiple values of retrieved captions (k). We\nalways search hyperparameters on the validation split of the\nrespective dataset. For more details about hyperparameters,\nsee Appendix D. We use faiss (Johnson et al., 2019) to\nmanage our datastore since it enables efficient storage and\nretrieval of vectors. Our final setting uses a FLAN-T5-Large\n(Chung et al., 2022) with nucleus sampling. To generate\ncaptions with FLAN-T5, we explore different prompting\nstrategies and found the strategy proposed in Ramos et al.\n(2023b) to work best. Specifically, the used prompt template\nis \u201cSimilar images show: < caption1 >, . . . , < captionk >\nThis image shows:\u201d.\nDatasets\nWe split the MS-COCO and Flickr30k bench-\nmarks according to Karpathy & Fei-Fei (2017) into train,\nvalidation, and test splits. For MSRVTT and VizWiz we split\naccording to the official splits (Gurari et al., 2020; Xu et al.,\n2We take the RN50\u00d764 model from the official repository at\nhttps://github.com/openai/CLIP.\n4\nLinear Alignment of Vision-language Models for Image Captioning\n2016). Since VizWiz contains a substantial amount of noise,\nwe filter out all captions for images that suffer from severe\nquality issues or were rejected by annotators and evaluate\nthe generated test captions on the official evaluation server.3\nFor MSRVTT, we employ the same pre-processing pipeline\nas Ramos et al. (2023b) and extract four frames from each\nvideo and pair them with the ground truth captions. This\nresults in many-to-many correspondences.\nBaselines\nWe consider existing methods as lightweight if\ntheir trainable parameter count is below 50 M. For MS-\nCOCO and Flickr30k, we compare ReCap to ClipCap\n(Mokady et al., 2021), I-Tuning (Luo et al., 2023), Small-\nCap (Ramos et al., 2023b), and Prefix-Diffusion (Liu et al.,\n2023a). For MSRVTT and VizWiz, we compare ReCap to\nSmallCap, since it is the only existing lightweight method\nthat report results on these datasets.\nEvaluation Metrics\nWe report metrics commonly used\nfor image captioning, such as CIDEr-D (Vedantam et al.,\n2015) and SPICE (Anderson et al., 2016).4 Further, we re-\nport our proposed metrics aCLIP-S and RefaCLIP-S. Most\nprior works do not report error bars on metrics used for\nevaluation. However, error bars are essential for scientific\ncomparison of different methods. Therefore, we report the\nstandard for each metric for all methods we trained our-\nselves. We do not report error bars for CIDEr-D and SPICE\non VizWiz since the evaluation server does not provide\nthem. We highlight the best performing methods in bold-\nface throughout the paper and consider two methods to be\non-par when their standard errors overlap (68.2% confidence\nintervals).\nResults\nTable 1 shows our results for MS-COCO and\nFlickr30k. ReCap outperforms all competitors on our pro-\nposed metrics aCLIP-S and RefaCLIP-S on both datasets.\nOn Flickr30k, ReCap attains performance on-par with\nSmallCap in terms of CIDEr-D and SPICE even though\nReCap trains about 1000 times faster with less trainable\nparameters (see Table 3). On MS-COCO, I-Tuning reaches\nthe highest CIDEr-D and SPICE scores. This gap is due to\nthe fact that I-Tuning trains over 10 times more parameters\nthan ReCap. Table 2 summarizes the results for VizWiz and\nMSRVTT. On both datasets, ReCap outperforms SmallCap.\nWe visualize the joint embedding space of the RN50\u00d764\nCLIP encoder without applying our linear alignment for the\nFlickr30k training set (29K images and 145K captions) via\nt-SNE (van der Maaten & Hinton, 2008) in Figure 2, left.\nWe find that images and captions are not well aligned int the\n3https://eval.ai/web/challenges/\nchallenge-page/739/overview\n4CIDEr-D and SPICE metrics are computed using the code\nfrom https://github.com/tylin/coco-caption.\nFigure 2. t-SNE visualization of CLIP-embeddings before (left)\nand after (right) linear re-alignment on the Flickr30k dataset.\njoint embedding space. However, after applying our linear\nmapping the two modalities align very well, as shown in\nFigure 2, right.\nCross-domain Transfer\nNext, we investigate the cross-\ndomain transfer of ReCap from MS-COCO to all other\ndomains. We show results for three settings, where we use\nthe same mapping trained on MS-COCO, but evaluate with\ndifferent datastores, (i) the target datastore, (ii) the source\ndatastore, and (iii) source and target datastores combined.\nHere source always refers to MS-COCO data and target\nrefers to one of Flickr30k, VizWiz, or MSRVTT. For this\nline of experiments we only compare to SmallCap since it is\nthe only existing lightweight captioning method that uses re-\ntrieval augmentation, and thus, accesses a datastore. Table 4\nshows CIDEr-D and aCLIP-S scores on all domains. ReCap\nconsistently outperforms SmallCap in terms of aCLIP-S.\nFurther, ReCap consistently attains higher CIDEr-D scores\nthan SmallCap on the MSRVTT dataset when using the\ntarget datastore. Combining data from both domains usually\nleads to a performance drop, which indicates that captions\nfrom the source domain do not describe images from the\ntarget domain well. SmallCap only outperforms ReCap in\nterms of CIDEr-D when using MS-COCO data on VizWiz.\nThese results highlight the transfer capabilities of ReCap.\n3.2. Metrics for Image Captioning\nFollowing standard practice of Hessel et al. (2021) and\nZhou et al. (2023), we evaluate our proposed metrics for\nimage captioning by measuring their correlation with human\nrankings of candidate captions.\nDatasets\nWe use the Flickr8k-Expert (Flickr8k-E), and\nthe Flickr8k-Crowdflower (Hodosh et al., 2013, Flickr8k-\nCF) datasets. These datasets provide candidate captions\n5\nLinear Alignment of Vision-language Models for Image Captioning\nTable 1. Comparison of different lightweight methods on the MS-COCO test set. We show performance for ReCap and ReCap+DAL. We\nreport mean and standard error for results we computed ourselves. Results for other methods are taken from their respective publications.\nN/A indicates that a certain metric is not available for a given method.\nMS-COCO\nFLICKR30K\nMETHOD\nCIDER-D\nSPICE\nACLIP-S\nREFACLIP-S\nCIDER-D\nSPICE\nACLIP-S\nREFACLIP-S\nCLIPCAP (MOKADY ET AL., 2021)\n113.1\n21.1\nN/A\nN/A\n57.9\n15.8\nN/A\nN/A\nI-TUNINGBASE (LUO ET AL., 2023)\n116.7\n21.8\nN/A\nN/A\n61.5\n16.9\nN/A\nN/A\nPREFIX-DIFFUSION (LIU ET AL., 2023A)\n106.3\n19.4\nN/A\nN/A\n53.8\n14.2\nN/A\nN/A\nSMALLCAPD=4,BASE (RAMOS ET AL., 2023B)\n117.6 \u00b1 1.0\n20.0 \u00b1 0.1\n46.0 \u00b1 0.2\n57.5 \u00b1 0.2\n69.6 \u00b1 2.1\n17.1 \u00b1 0.3\n36.8 \u00b1 0.4\n46.7 \u00b1 0.4\nRECAP (OURS)\n108.3 \u00b1 1.0\n21.2 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\n68.8 \u00b1 2.0\n17.5 \u00b1 0.3\n43.5 \u00b1 0.3\n53.4 \u00b1 0.3\nRECAP + DAL (OURS)\n106.7 \u00b1 1.0\n21.2 \u00b1 0.1\n74.7 \u00b1 0.1\n76.2 \u00b1 0.1\n69.5 \u00b1 2.0\n17.3 \u00b1 0.3\n65.7 \u00b1 0.3\n68.2 \u00b1 0.3\nTable 2. Comparison of ReCap and SmallCapd=4,Base on the VizWiz\nand MSRVTT test sets. We report mean and standard error of\nmetrics we computed ourselves. CIDEr-D and SPICE on VizWiz\nare obtained from the official evaluation server. RefaCLIP-S is not\navailable since references of the VizWiz test set are not public.\nVIZWIZ\nMETHOD\nCIDER-D\nSPICE\nACLIP-S\nREFACLIP-S\nSMALLCAP\n51.88\n13.4\n38.4 \u00b1 0.1\nN/A\nRECAP\n62.3\n16.7\n42.7 \u00b1 0.1\nN/A\nMSRVTT\nSMALLCAP\n31.6 \u00b1 0.2\n11.1 \u00b1 0.0\n9.2 \u00b1 0.0\n7.6 \u00b1 0.3\nRECAP\n38.8 \u00b1 0.2\n14.4 \u00b1 0.0\n34.5 \u00b1 0.0\n40.6 \u00b1 0.0\nTable 3. Number of parameters, training time, and inference time\nof ReCap compared to existing lightweight image captioning meth-\nods. Inference time is measured in seconds on a subset of 1000\nimages from the MS-COCO test set on an A100 GPU.\nMETHOD\n|\u03b8|\nTRAINING\nINFERENCE\nCLIPCAP\n43M\n6H (GTX1080)\nN/A\nPREFIX-DIFFUSION\n38.25M\nN/A\nN/A\nI-TUNING\n14M\nN/A\nN/A\nSMALLCAPD=4,BASE\n1.8M\n8H(A100)\n0.19 \u00b1 0.03\nRECAP\n1.0M\n20.3S \u00b1 1.91 (CPU)\n0.47 \u00b1 0.08\nalong with human rankings for images of the test set of\nFlickr8k. We provide additional results for the THumB\n(Kasai et al., 2022) dataset in Appendix B.\nBaselines\nWe compare our metrics to the current state-of-\nthe-art reference-based and reference-free metrics. In the\ncase of reference-free metrics, we compare to CLIP-score\n(Hessel et al., 2021), and CLIP+DN (Zhou et al., 2023). We\ncompare our reference-based metric to RefCLIPScore (Hes-\nsel et al., 2021), CLIP+DN-Ref (Zhou et al., 2023), MID\n(Kim et al., 2022), and SoftSPICE (Li et al., 2023b). For\nall CLIP+DN variants (reference-based and reference-free)\nwe estimate the mean of both modalities on the respective\ntraining dataset, since we usually do not have access to test\nsamples.\nTable 4. Transfer experiments for SmallCapd=4,Base and ReCap\ntrained on MS-COCO to Flickr30k, VizWiz, and MSRVTT test\nsets. The datastore either contains data from the target domain, the\nsource domain, or both of them combined. We report mean and\nstandard error of CIDEr-D and aCLIP-S scores for ReCap.\nMETHOD\nFLICKR30K\nVIZWIZ\nMSRVTT\nCIDER-D\nACLIP-S\nCIDER-D\nACLIP-S\nCIDER-D\nACLIP-S\nTARGET DATASTORE\nSMALLCAP\n59.3 \u00b1 1.9\n34.7 \u00b1 0.4\n51.0\n33.7 \u00b1 0.1\n19.5 \u00b1 0.1\n23.0 \u00b1 0.3\nRECAP\n63.9 \u00b1 1.9\n45.0 \u00b1 0.3\n53.1\n40.4 \u00b1 0.1\n29.4 \u00b1 0.1\n31.7 \u00b1 0.3\nSOURCE + TARGET DATASTORE\nSMALLCAP\n50.4 \u00b1 1.7\n33.1 \u00b1 0.4\n51.0\n33.8 \u00b1 0.1\n19.5 \u00b1 0.1\n23.0 \u00b1 0.0\nRECAP\n58.9 \u00b1 1.8\n45.0 \u00b1 0.3\n49.8\n39.8 \u00b1 0.1\n25.5 \u00b1 0.1\n30.1 \u00b1 0.0\nSOURCE DATASTORE\nSMALLCAP\n48.8 \u00b1 1.6\n32.8 \u00b1 0.4\n36.1\n31.3 \u00b1 0.1\n16.5 \u00b1 0.1\n21.6 \u00b1 0.0\nRECAP\n48.5 \u00b1 1.6\n40.8 \u00b1 0.3\n28.6\n33.0 \u00b1 0.1\n17.5 \u00b1 0.1\n26.3 \u00b1 0.3\nEvaluation Metrics\nTo quantify correlation with human\njudgement, we report Kendall\u2019s \u03c4c for Flickr8k-E and\nKendall\u2019s \u03c4b for Flickr8k-CF as done in prior work (Hessel\net al., 2021; Zhou et al., 2023). The Kendall rank correla-\ntion coefficient measures the ordinal association between\nrankings by humans and the metric.\nResults\nWe report our results in Table 5. First, we note\nthat aCLIP-S/RefaCLIP-S consistently outperform CLIP-\nS/RefCLIP-S from which they were derived. Our aCLIP-S\nmetric achieves the highest correlation among all reference-\nfree metrics for both datasets. In the case of reference-based\nmetrics, RefaCLIP-S reaches the highest correlation for\nFlickr8k-E, while MID reaches the highest correlation for\nFlickr8k-CF. We show an extension of our results including\nan ablation on different vision encoders used for our metrics,\nand additional reference-based metrics in Appendix B.\n3.3. Datastore-augmentation Loop\nImplementation Details\nWe run our DAL for m = 5 iter-\nations and instantiate \u00b5(\u00b7, \u00b7) with CIDEr-D, SPICE, aCLIP-\nS, and RefaCLIP-S to filter the synthetic captions. If more\nthan one synthetic caption exceeds the threshold \u00af\u00b5, we only\ntake the highest scoring one. After each round of augmen-\ntation we search over the hyperparameter k that yields the\n6\nLinear Alignment of Vision-language Models for Image Captioning\nTable 5. Correlation with human judgement measured via\nKendall\u2019s \u03c4c for Flickr8k-E and \u03c4b for Flickr8k-CF both scaled by\n100. The variance for the \u03c4 estimator only depends on sample size\nand is 3e-5 for Flickr8k-E and 1e-5 for Flickr8k-CF. \u2020 indicates\nthat results were taken from prior work.\nMETHOD\nFLICKR8K-E\nFLICKR8K-CF\nREFERENCE-FREE\nCLIP-S\n51.4\n34.3\nCLIP+DN\n54.0\n35.2\nACLIP-S (OURS)\n55.1\n36.2\nREFERENCE-BASED\nCIDER-D\n43.9\n24.6\nSPICE\n45.0\nN/A\nREFCLIP-S\n53.0\n36.4\nSOFTSPICE\u2020\n54.2\nN/A\nMID\u2020\n54.9\n37.3\nCLIP+DN-REF\n55.0\n37.0\nREFACLIP-S (OURS)\n55.5\n36.7\nhighest average score \u00af\u00b5(\u00b7, \u00b7) on the validation set. Finally,\nwe evaluate the datastore with the found k on the test set to\nmeasure final performance.\nResults\nWe apply DAL to ReCap for both MS-COCO\nand Flickr30k datasets. Per iteration, DAL adds on average\n42320 and 35288 synthetic captions to the datastore for MS-\nCOCO and Flickr30k, respectively. This corresponds to 7%\nand 24% of the original datastore sizes, respectively. We find\nthat the selection of the metric for filtering synthetic captions\nin DAL is non-trivial. Filtering with respect to one metric\nusually leads to performance improvements on this very\nmetric. This is due to a rather low correlation between met-\nrics as we show in Figure 4 in Appendix B. Therefore, we\nreport results for ReCap+DAL for each metric when using\nthe same metric for filtering in Table 1. Remarkably, there is\na significant improvement in aCLIP-S and RefaCLIP-S for\nReCap+DAL. For rule-based metrics (CIDEr-D and SPICE),\nwe do not observe significant changes after adding synthetic\ncaptions. This is reflected in the development of \u00af\u00b5 during\nDAL, where rule-based metrics mostly fluctuate or decrease\n(see Figure 5 in Appendix B). Additionally, we visualize the\nnumber of synthetic captions that appear in the prompt and\nthe choice of the best k over all iterations in Appendix B.\nFinally, in Appendix B we provide evidence that synthetic\ncaptions of high quality have more impact on captioning\nperformance than simply adding data from other domains.\nWe show some sample images from the MS-COCO test\nsplit and captions generated by ReCap and ReCap+DAL\nin Figure 3. We observe that ReCap+DAL generates more\ndetailed captions, such as recognizing trees in Figure 3,\nright. Further, in some cases ReCap+DAL removes some\nimaginary content from captions, as showcased in Figure 3\nleft and middle. We provide further examples in Figure 8 in\nAppendix B.\n4. Related Work\nLinear Alignment\nThe idea of linearly aligning embed-\nding spaces is a well studied problem in the field of bilin-\nguality (Minixhofer et al., 2022; Artetxe et al., 2016), geo-\nmetrical alignment (Leordeanu & Hebert, 2005; Fischler &\nBolles, 1981; Liu et al., 2008), and vision for zero-shot learn-\ning (Akata et al., 2013; 2015; Frome et al., 2013; Romera-\nParedes & Torr, 2015). Similar to our approach, Ouali\net al. (2023) use orthogonal procrustes to align features of\nCLIP-style models with class labels for few-shot classifica-\ntion. However, their approach is tailored toward the task of\nclassification and does not directly transfer to image cap-\ntioning. Other works consider image captioning using only\ntext data by training a text decoder for CLIP-style models\n(Li et al., 2023a; Nukrai et al., 2022; Yu et al., 2022; Wang\net al., 2023a; Gu et al., 2022).However, at test-time these\napproaches still receive images as input, and thus, still suffer\nfrom the prevalent mis-alignment. Other approaches adapt\nthe pretraining objective in order to achieve a better align-\nment in the joint embedding space (F\u00a8urst et al., 2022; Goel\net al., 2022; Humer et al., 2023). However, none of these\nmodels are available at the same scale as CLIP.\nRetrieval Augmentation\nThe idea of retrieval augmen-\ntation has been explored in the realm of language model-\ning (Khandelwal et al., 2020; Guu et al., 2020; Borgeaud\net al., 2022), language generation conditioned on images\n(Hu et al., 2023; Yang et al., 2023b; Yasunaga et al., 2023),\nand reinforcement learning (Humphreys et al., 2022; Goyal\net al., 2022). In the realm of image captioning, Ramos et al.\n(2023b) leverages retrieval augmentation to reduce the re-\nquired number of trainable parameters. Ramos et al. (2023a)\nextends this idea to multilingual datastores, which enables\ngeneration in a certain target language. ReCap also relies on\nretrieval augmentation, but is much more efficient in terms\nof training while yielding competitive or even better results.\nLightweight Image Captioning\nLightweight captioning\naims at reducing the computational complexity for training\nimage captioning models. One line of work is based on\nknowledge distillation (Hinton et al., 2015) and assumes\naccess to teacher captioning models that are distilled into\nmuch smaller scale models (Wang et al., 2023b; Fang et al.,\n2021; Wang et al., 2020). Another line of works lever-\nage parameter-efficient fine-tuning methods to merge visual\nknowledge into generative LMs via adapter layers (Eichen-\nberg et al., 2022; Zhang et al., 2023; Gao et al., 2023),\ncross-attention modules (Luo et al., 2023; Ramos et al.,\n2023b), or a mapping network between embedding spaces\n7\nLinear Alignment of Vision-language Models for Image Captioning\nGiraffes standing in a zoo enclosure near trees.\nA bathroom with a sink, tub \nand shower.\nA plane flying in the sky with a \ntrail of smoke.\nA plane flying through a cloudy\nblue sky.\nA bathroom with a toilet, sink, \nmirror, and shower curtain.\nGiraffes standing in a zoo enclosure.\nFigure 3. Captions generated via ReCap (bottom) and ReCap+DAL (top). Images were taken from the MS-COCO validation set.\n(Mokady et al., 2021; Merullo et al., 2023). Finally, while\nbeing lightweight, Kuo & Kira (2023) relies on a two-stage\ntraining procedure that includes fine-tuning via reinforce-\nment learning (Li et al., 2020; Vinyals et al., 2015; Cornia\net al., 2020). In contrast to ReCap, these methods require\nend-to-end training.\n5. Discussion and Limitations\nDatastore\nThe key idea of retrieval augmentation is to\nreduce the number of parameters by giving the model ac-\ncess to the training data during inference. Therefore, the\ntraining data need not be stored in the model weights and no\nre-training is required if the data changes. We have demon-\nstrated that ReCap performs particularly well in this transfer\nscenario. Generally, a drawback of retrieval augmentation is\nan additional memory footprint due to the datastore. How-\never, there are existing frameworks for efficient storage and\nretrieval, such as faiss (Johnson et al., 2019), which we\nuse in this work.\nWhich Metrics Should Be Reported?\nWe have shown\nthat our learning-based metrics correlate stronger with hu-\nman judgement than the current state of the art. However,\nthey rely on a dataset to compute the alignment with re-\nspect to the downstream task. Therefore, if data for the\ndownstream task is available, we recommend to report our\nproposed metrics in addition to rule-based metrics. If no\ndata for training our metric is available, we recommend to\ndefer to metrics such as MID or CLIP+DN.\nAugmentation via Synthetic Data\nPrior work has high-\nlighted the utility of synthetic captions for datastore augmen-\ntation in the context of image captioning (G\u00a8ulc\u00b8ehre et al.,\n2023; Yang et al., 2023a; Lin et al., 2023; Li et al., 2022).\nHowever, the generation of synthetic captions in our setup\nwith DAL is costly since we iterate multiple times over the\nentire training images. For datasets such as MS-COCO this\nprocess took approximately 15 hours for one iteration on\nan A100 GPU. Therefore, we conducted initial experiments\nwith m = 5 iterations. This, however, is still faster than\nfine-tuning or backpropagating through an LLM. Moreover,\nwe have illustrated that high quality synthetic captions have\nmore impact on captioning performance than simply adding\nexisting data sources to the datastore.\n6. Conclusion\nIn this work, we advocate for using a linear mapping that\ncan be computed in closed form for two use cases, image\ncaptioning and caption evaluation. We introduce ReCap,\nan efficient retrieval-augmented image-captioning method,\nwhich is based on our mapping and requires substantially\nless training time than other lightweight image-captioning\nmethods. We also introduce aCLIP-S and RefaCLIP-S,\ntwo new caption evaluation metrics that use our mapping to\nadapt CLIP-S and RefCLIP-S, respectively, to a downstream\ndataset. Our metrics correlate stronger with human judge-\nment than prior CLIP-based metrics and achieve a new state\nof the art in three out of four categories. Since the evolution\nof the field is guided by the metrics that it uses, we hope\nthat this work facilitates research in the direction of image\ncaptioning. We evaluate ReCap using established metrics\nand found its performance to be similar to prior lightweight\nmethods, effectively reducing training time. In terms of\nour newly proposed metrics, however, we found that ReCap\noutperformed competitors on all tasks. Moreover, ReCap\nimproves transfer to different domains compared to exist-\ning retrieval-augmented methods. Finally, we propose a\nnovel datastore-augmentation loop (DAL) that iteratively\nadds synthetic captions to the retrieval datastore and show\nthat it can further improve caption quality.\n8\nLinear Alignment of Vision-language Models for Image Captioning\n7. Impact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, on the forefront of such is the\npotential generation of misinformation or harmful content.\nAnother consequence worth discussing involves the ampli-\nfication of biases in the downstream data that CLIP my be\naligned to with our linear alignment.\nAcknowledgements\nWe are grateful to Wei Lin for his support, fruitful discus-\nsions and corrections.\nThe ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma-\nchine Learning, are supported by the Federal State Upper\nAustria. We thank the projects AI-MOTION (LIT-2018-\n6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi-\ncal Cognitive Computing Center (MC3), INCONTROL-\nRL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-\n872172), DL for GranularFlow (FFG-871302), EPILEP-\nSIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF-\n36235), AI4GreenHeatingGrids(FFG- 899943), INTE-\nGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID:\n951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE-\n01-01). We thank Audi.JKU Deep Learning Center, TGW\nLOGISTICS GROUP GMBH, Silicon Austria Labs (SAL),\nUniversity SAL Labs initiative, FILL Gesellschaft mbH,\nAnyline GmbH, Google, ZF Friedrichshafen AG, Robert\nBosch GmbH, UCB Biopharma SRL, Merck Healthcare\nKGaA, Verbund AG, GLS (Univ. Waterloo) Software Com-\npetence Center Hagenberg GmbH, T \u00a8UV Austria, Frauscher\nSensonic, Borealis AG, TRUMPF and the NVIDIA Corpo-\nration.\nReferences\nAkata, Z., Perronnin, F., Harchaoui, Z., and Schmid, C.\nLabel-embedding for attribute-based classification. In\n2013 IEEE Conference on Computer Vision and Pattern\nRecognition, Portland, OR, USA, June 23-28, 2013, pp.\n819\u2013826. IEEE Computer Society, 2013. doi: 10.1109/\nCVPR.2013.111.\nAkata, Z., Reed, S. E., Walter, D., Lee, H., and Schiele, B.\nEvaluation of output embeddings for fine-grained image\nclassification. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pp. 2927\u20132936. IEEE Computer Society,\n2015. doi: 10.1109/CVPR.2015.7298911.\nAnderson, P., Fernando, B., Johnson, M., and Gould, S.\nSPICE: semantic propositional image caption evaluation.\nIn Leibe, B., Matas, J., Sebe, N., and Welling, M. (eds.),\nComputer Vision - ECCV 2016 - 14th European Confer-\nence, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part V, volume 9909 of Lecture Notes in\nComputer Science, pp. 382\u2013398. Springer, 2016. doi:\n10.1007/978-3-319-46454-1\\ 24.\nArtetxe, M., Labaka, G., and Agirre, E. Learning principled\nbilingual mappings of word embeddings while preserv-\ning monolingual invariance. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language\nProcessing, pp. 2289\u20132294, Austin, Texas, November\n2016. Association for Computational Linguistics. doi:\n10.18653/v1/D16-1250.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-\nford, E., Millican, K., van den Driessche, G., Lespiau,\nJ., Damoc, B., Clark, A., de Las Casas, D., Guy, A.,\nMenick, J., Ring, R., Hennigan, T., Huang, S., Maggiore,\nL., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irv-\ning, G., Vinyals, O., Osindero, S., Simonyan, K., Rae,\nJ. W., Elsen, E., and Sifre, L. Improving language models\nby retrieving from trillions of tokens. In Chaudhuri, K.,\nJegelka, S., Song, L., Szepesv\u00b4ari, C., Niu, G., and Sabato,\nS. (eds.), International Conference on Machine Learn-\ning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\nUSA, volume 162 of Proceedings of Machine Learning\nResearch, pp. 2206\u20132240. PMLR, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S.,\nWebson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,\nChowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao,\nV. Y., Huang, Y., Dai, A. M., Yu, H., Petrov, S., Chi, E. H.,\nDean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and\nWei, J. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416, 2022. doi: 10.48550/ARXIV.\n2210.11416.\nCornia, M., Stefanini, M., Baraldi, L., and Cucchiara, R.\nMeshed-memory transformer for image captioning. In\n2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA, June 13-\n19, 2020, pp. 10575\u201310584. Computer Vision Foundation\n/ IEEE, 2020. doi: 10.1109/CVPR42600.2020.01059.\nCui, Y., Yang, G., Veit, A., Huang, X., and Belongie, S. J.\nLearning to evaluate image captioning. In 2018 IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018, pp. 5804\u20135812. Computer Vision Foundation /\nIEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.\n00608.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An\nImage is Worth 16x16 Words: Transformers for Image\n9\nLinear Alignment of Vision-language Models for Image Captioning\nRecognition at Scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net, 2021.\nEichenberg, C., Black, S., Weinbach, S., Parcalabescu, L.,\nand Frank, A. MAGMA - multimodal augmentation of\ngenerative models through adapter-based finetuning. In\nGoldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, Decem-\nber 7-11, 2022, pp. 2416\u20132428. Association for Compu-\ntational Linguistics, 2022.\nFang, Z., Wang, J., Hu, X., Wang, L., Yang, Y., and Liu,\nZ. Compressing visual-linguistic model via knowledge\ndistillation. In 2021 IEEE/CVF International Conference\non Computer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021, pp. 1408\u20131418. IEEE, 2021. doi:\n10.1109/ICCV48922.2021.00146.\nFischler, M. A. and Bolles, R. C. Random sample consensus:\nA paradigm for model fitting with applications to image\nanalysis and automated cartography. Commun. ACM, 24\n(6):381\u2013395, 1981. doi: 10.1145/358669.358692.\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J.,\nRanzato, M., and Mikolov, T. Devise: A deep visual-\nsemantic embedding model. In Burges, C. J. C., Bottou,\nL., Ghahramani, Z., and Weinberger, K. Q. (eds.), Ad-\nvances in Neural Information Processing Systems 26:\n27th Annual Conference on Neural Information Process-\ning Systems 2013. Proceedings of a meeting held Decem-\nber 5-8, 2013, Lake Tahoe, Nevada, United States, pp.\n2121\u20132129, 2013.\nF\u00a8urst, A., Rumetshofer, E., Lehner, J., Tran, V. T., Tang,\nF., Ramsauer, H., Kreil, D. P., Kopp, M. K., Klambauer,\nG., Bitto-Nemling, A., and Hochreiter, S. CLOOB: Mod-\nern hopfield networks with infoLOOB outperform CLIP.\nIn Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K.\n(eds.), Advances in Neural Information Processing Sys-\ntems, 2022.\nGao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A.,\nZhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y.\nLlama-adapter V2: parameter-efficient visual instruction\nmodel. CoRR, abs/2304.15010, 2023. doi: 10.48550/\narXiv.2304.15010.\nGoel, S., Bansal, H., Bhatia, S., Rossi, R. A., Vinay, V.,\nand Grover, A. Cyclip: Cyclic contrastive language-\nimage pretraining. In Koyejo, S., Mohamed, S., Agarwal,\nA., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances\nin Neural Information Processing Systems 35: Annual\nConference on Neural Information Processing Systems\n2022, NeurIPS 2022, New Orleans, LA, USA, November\n28 - December 9, 2022, 2022.\nGoyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R.,\nBadia, A. P., Guez, A., Mirza, M., Humphreys, P. C.,\nKonyushova, K., et al. Retrieval-augmented reinforce-\nment learning. In International Conference on Machine\nLearning, pp. 7740\u20137765. PMLR, 2022.\nGu, S., Clark, C., and Kembhavi, A. I can\u2019t believe there\u2019s\nno images! learning visual tasks using only language data.\nCoRR, abs/2211.09778, 2022. doi: 10.48550/ARXIV.\n2211.09778.\nG\u00a8ulc\u00b8ehre, C\u00b8 ., Paine, T. L., Srinivasan, S., Konyushkova,\nK., Weerts, L., Sharma, A., Siddhant, A., Ahern, A.,\nWang, M., Gu, C., Macherey, W., Doucet, A., Firat, O.,\nand de Freitas, N.\nReinforced self-training (rest) for\nlanguage modeling. CoRR, abs/2308.08998, 2023. doi:\n10.48550/arXiv.2308.08998.\nGurari, D., Zhao, Y., Zhang, M., and Bhattacharya, N.\nCaptioning images taken by people who are blind. In\nVedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.),\nComputer Vision - ECCV 2020 - 16th European Con-\nference, Glasgow, UK, August 23-28, 2020, Proceed-\nings, Part XVII, volume 12362 of Lecture Notes in\nComputer Science, pp. 417\u2013434. Springer, 2020. doi:\n10.1007/978-3-030-58520-4\\ 25.\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.\nRetrieval augmented language model pre-training. In\nProceedings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Virtual\nEvent, volume 119 of Proceedings of Machine Learning\nResearch, pp. 3929\u20133938. PMLR, 2020.\nHe, K., Zhang, X., Ren, S., and Sun, J.\nDeep Resid-\nual Learning for Image Recognition.\nIn 2016 IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2016, Las Vegas, NV, USA, June 27-30,\n2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi:\n10.1109/CVPR.2016.90.\nHessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,\nY. Clipscore: A reference-free evaluation metric for im-\nage captioning. In Moens, M., Huang, X., Specia, L.,\nand Yih, S. W. (eds.), Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana, Do-\nminican Republic, 7-11 November, 2021, pp. 7514\u20137528.\nAssociation for Computational Linguistics, 2021. doi:\n10.18653/v1/2021.emnlp-main.595.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the\nknowledge in a neural network. CoRR, abs/1503.02531,\n2015.\nHodosh, M., Young, P., and Hockenmaier, J. Framing image\ndescription as a ranking task: Data, models and evaluation\n10\nLinear Alignment of Vision-language Models for Image Captioning\nmetrics. J. Artif. Intell. Res., 47:853\u2013899, 2013. doi:\n10.1613/JAIR.3994.\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.\nThe curious case of neural text degeneration.\nIn 8th\nInternational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net, 2020.\nHu, Z., Iscen, A., Sun, C., Wang, Z., Chang, K., Sun,\nY., Schmid, C., Ross, D. A., and Fathi, A.\nRe-\nveal: Retrieval-augmented visual-language pre-training\nwith multi-source multimodal knowledge memory. In\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2023, Vancouver, BC, Canada,\nJune 17-24, 2023, pp. 23369\u201323379. IEEE, 2023. doi:\n10.1109/CVPR52729.2023.02238.\nHumer, C., Prasad, V., Streit, M., and Strobelt, H. Under-\nstanding and comparing multi-modal models: Exploring\nthe latent space of clip-like models (clip, cyclip, cloob)\nusing inter-modal pairs. 6th Workshop on Visualization\nfor AI Explainability, October 2023.\nHumphreys, P. C., Guez, A., Tieleman, O., Sifre, L., Weber,\nT., and Lillicrap, T. P. Large-scale retrieval for reinforce-\nment learning. In Koyejo, S., Mohamed, S., Agarwal,\nA., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances\nin Neural Information Processing Systems 35: Annual\nConference on Neural Information Processing Systems\n2022, NeurIPS 2022, New Orleans, LA, USA, November\n28 - December 9, 2022, 2022.\nJohnson, J., Douze, M., and J\u00b4egou, H. Billion-scale similar-\nity search with GPUs. IEEE Transactions on Big Data, 7\n(3):535\u2013547, 2019.\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic align-\nments for generating image descriptions. IEEE Trans.\nPattern Anal. Mach. Intell., 39(4):664\u2013676, 2017. doi:\n10.1109/TPAMI.2016.2598339.\nKasai, J., Sakaguchi, K., Dunagan, L., Morrison, J., Bras,\nR. L., Choi, Y., and Smith, N. A.\nTransparent hu-\nman evaluation for image captioning. In Carpuat, M.,\nde Marneffe, M., and Ru\u00b4\u0131z, I. V. M. (eds.), Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL 2022, Seattle,\nWA, United States, July 10-15, 2022, pp. 3464\u20133478.\nAssociation for Computational Linguistics, 2022. doi:\n10.18653/V1/2022.NAACL-MAIN.254.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,\nand Lewis, M. Generalization through memorization:\nNearest neighbor language models. In 8th International\nConference on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020.\nKim, J., Kim, Y., Lee, J., Yoo, K. M., and Lee, S. Mutual\ninformation divergence: A unified metric for multimodal\ngenerative models. In Koyejo, S., Mohamed, S., Agarwal,\nA., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances\nin Neural Information Processing Systems 35: Annual\nConference on Neural Information Processing Systems\n2022, NeurIPS 2022, New Orleans, LA, USA, November\n28 - December 9, 2022, 2022.\nKuo, C. and Kira, Z. HAAV: hierarchical aggregation of aug-\nmented views for image captioning. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2023, Vancouver, BC, Canada, June 17-24, 2023, pp.\n11039\u201311049. IEEE, 2023. doi: 10.1109/CVPR52729.\n2023.01062.\nLeordeanu, M. and Hebert, M. A spectral technique for\ncorrespondence problems using pairwise constraints. In\n10th IEEE International Conference on Computer Vision\n(ICCV 2005), 17-20 October 2005, Beijing, China, pp.\n1482\u20131489. IEEE Computer Society, 2005. doi: 10.1109/\nICCV.2005.20.\nLi, J., Li, D., Xiong, C., and Hoi, S. C. H. BLIP: boot-\nstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In Chaudhuri, K.,\nJegelka, S., Song, L., Szepesv\u00b4ari, C., Niu, G., and Sabato,\nS. (eds.), International Conference on Machine Learn-\ning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\nUSA, volume 162 of Proceedings of Machine Learning\nResearch, pp. 12888\u201312900. PMLR, 2022.\nLi, W., Zhu, L., Wen, L., and Yang, Y. Decap: Decod-\ning CLIP latents for zero-shot captioning via text-only\ntraining. In The Eleventh International Conference on\nLearning Representations, 2023a.\nLi, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,\nL., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Os-\ncar: Object-semantics aligned pre-training for vision-\nlanguage tasks. In Vedaldi, A., Bischof, H., Brox, T., and\nFrahm, J. (eds.), Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part XXX, volume 12375 of Lecture Notes\nin Computer Science, pp. 121\u2013137. Springer, 2020. doi:\n10.1007/978-3-030-58577-8\\ 8.\nLi, Z., Chai, Y., Zhuo, T. Y., Qu, L., Haffari, G., Li, F., Ji,\nD., and Tran, Q. H. FACTUAL: A benchmark for faithful\nand consistent textual scene graph parsing. In Rogers,\nA., Boyd-Graber, J. L., and Okazaki, N. (eds.), Findings\nof the Association for Computational Linguistics: ACL\n2023, Toronto, Canada, July 9-14, 2023, pp. 6377\u20136390.\n11\nLinear Alignment of Vision-language Models for Image Captioning\nAssociation for Computational Linguistics, 2023b. doi:\n10.18653/V1/2023.FINDINGS-ACL.398.\nLin, C.-Y. ROUGE: A package for automatic evaluation\nof summaries.\nIn Text Summarization Branches Out,\npp. 74\u201381, Barcelona, Spain, July 2004. Association for\nComputational Linguistics.\nLin, S., Wang, K., Zeng, X., and Zhao, R. Explore the\npower of synthetic data on few-shot object detection. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2023 - Workshops, Vancouver, BC,\nCanada, June 17-24, 2023, pp. 638\u2013647. IEEE, 2023.\ndoi: 10.1109/CVPRW59228.2023.00071.\nLin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona,\nP., Ramanan, D., Doll\u00b4ar, P., and Zitnick, C. L.\nMi-\ncrosoft COCO: Common Objects in Context. In Fleet,\nD. J., Pajdla, T., Schiele, B., and Tuytelaars, T. (eds.),\nComputer Vision - ECCV 2014 - 13th European Confer-\nence, Zurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part V, volume 8693 of Lecture Notes in\nComputer Science, pp. 740\u2013755. Springer, 2014. doi:\n10.1007/978-3-319-10602-1 48.\nLiu, C., Yuen, J., Torralba, A., Sivic, J., and Freeman, W. T.\nSIFT flow: Dense correspondence across different scenes.\nIn Forsyth, D. A., Torr, P. H. S., and Zisserman, A. (eds.),\nComputer Vision - ECCV 2008, 10th European Confer-\nence on Computer Vision, Marseille, France, October\n12-18, 2008, Proceedings, Part III, volume 5304 of Lec-\nture Notes in Computer Science, pp. 28\u201342. Springer,\n2008. doi: 10.1007/978-3-540-88690-7\\ 3.\nLiu, G., Li, Y., Fei, Z., Fu, H., Luo, X., and Guo, Y. Prefix-\ndiffusion: A lightweight diffusion model for diverse im-\nage captioning.\nCoRR, abs/2309.04965, 2023a.\ndoi:\n10.48550/ARXIV.2309.04965.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. CoRR, abs/2304.08485, 2023b. doi: 10.48550/\narXiv.2304.08485.\nLuo, Z., Hu, Z., Xi, Y., Zhang, R., and Ma, J.\nI-\ntuning: Tuning frozen language models with image for\nlightweight image captioning. In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pp. 1\u20135, 2023. doi:\n10.1109/ICASSP49357.2023.10096424.\nMerullo, J., Castricato, L., Eickhoff, C., and Pavlick, E. Lin-\nearly mapping from image to text space. In The Eleventh\nInternational Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-\nview.net, 2023.\nMinixhofer, B., Paischer, F., and Rekabsaz, N. WECH-\nSEL: Effective initialization of subword embeddings for\ncross-lingual transfer of monolingual language models. In\nCarpuat, M., Marneffe, M.-C. d., and Ru\u00b4\u0131z, I. V. M. (eds.),\nProceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL 2022,\nSeattle, WA, United States, July 10-15, 2022, pp. 3992\u2013\n4006. Association for Computational Linguistics, 2022.\ndoi: 10.18653/v1/2022.naacl-main.293.\nMokady, R., Hertz, A., and Bermano, A. H. ClipCap: CLIP\nPrefix for Image Captioning. CoRR, abs/2111.09734,\n2021. arXiv: 2111.09734.\nNukrai, D., Mokady, R., and Globerson, A. Text-only train-\ning for image captioning using noise-injected CLIP. In\nGoldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, De-\ncember 7-11, 2022, pp. 4055\u20134063. Association for Com-\nputational Linguistics, 2022. doi: 10.18653/V1/2022.\nFINDINGS-EMNLP.299.\nOuali, Y., Bulat, A., Mart\u00b4\u0131nez, B., and Tzimiropoulos, G.\nBlack box few-shot adaptation for vision-language mod-\nels. In IEEE/CVF International Conference on Computer\nVision, ICCV 2023, Paris, France, October 1-6, 2023, pp.\n15488\u201315500. IEEE, 2023. doi: 10.1109/ICCV51070.\n2023.01424.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. Bleu: a\nmethod for automatic evaluation of machine translation.\nIn Proceedings of the 40th Annual Meeting of the Asso-\nciation for Computational Linguistics, July 6-12, 2002,\nPhiladelphia, PA, USA, pp. 311\u2013318. ACL, 2002. doi:\n10.3115/1073083.1073135.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2018.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning Transferable\nVisual Models From Natural Language Supervision. In\nMeila, M. and Zhang, T. (eds.), Proceedings of the 38th\nInternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pp. 8748\u2013\n8763. PMLR, 2021.\nRamos, R., Martins, B., and Elliott, D.\nLmcap: Few-\nshot multilingual image captioning by retrieval aug-\nmented language model prompting. In Rogers, A., Boyd-\nGraber, J. L., and Okazaki, N. (eds.), Findings of the\nAssociation for Computational Linguistics: ACL 2023,\n12\nLinear Alignment of Vision-language Models for Image Captioning\nToronto, Canada, July 9-14, 2023, pp. 1635\u20131651. As-\nsociation for Computational Linguistics, 2023a.\ndoi:\n10.18653/v1/2023.findings-acl.104.\nRamos, R., Martins, B., Elliott, D., and Kementchedjhieva,\nY. Smallcap: Lightweight image captioning prompted\nwith retrieval augmentation. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2023,\nVancouver, BC, Canada, June 17-24, 2023, pp. 2840\u2013\n2849. IEEE, 2023b. doi: 10.1109/CVPR52729.2023.\n00278.\nRomera-Paredes, B. and Torr, P. H. S. An embarrassingly\nsimple approach to zero-shot learning. In Bach, F. R. and\nBlei, D. M. (eds.), Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, volume 37 of JMLR Workshop\nand Conference Proceedings, pp. 2152\u20132161. JMLR.org,\n2015.\nSch\u00a8onemann, P. A generalized solution of the orthogonal\nprocrustes problem. Psychometrika, 31(1):1\u201310, 1966.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and\nLample, G.\nLlama: Open and efficient foundation\nlanguage models. CoRR, abs/2302.13971, 2023. doi:\n10.48550/arXiv.2302.13971.\nvan der Maaten, L. and Hinton, G. Visualizing data using\nt-sne. Journal of Machine Learning Research, 9(86):\n2579\u20132605, 2008.\nVedantam, R., Zitnick, C. L., and Parikh, D.\nCider:\nConsensus-based image description evaluation. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2015, Boston, MA, USA, June 7-12, 2015,\npp. 4566\u20134575. IEEE Computer Society, 2015.\ndoi:\n10.1109/CVPR.2015.7299087.\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show\nand tell: A neural image caption generator. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2015, Boston, MA, USA, June 7-12, 2015, pp.\n3156\u20133164. IEEE Computer Society, 2015. doi: 10.1109/\nCVPR.2015.7298935.\nWang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Billion\nParameter Autoregressive Language Model, May 2021.\nWang, J., Hu, X., Zhang, P., Li, X., Wang, L., Zhang, L.,\nGao, J., and Liu, Z. Minivlm: A smaller and faster vision-\nlanguage model. CoRR, abs/2012.06946, 2020.\nWang, J., Yan, M., Zhang, Y., and Sang, J.\nFrom as-\nsociation to generation: Text-only captioning by unsu-\npervised cross-modal mapping. In Proceedings of the\nThirty-Second International Joint Conference on Arti-\nficial Intelligence, IJCAI 2023, 19th-25th August 2023,\nMacao, SAR, China, pp. 4326\u20134334. ijcai.org, 2023a. doi:\n10.24963/IJCAI.2023/481.\nWang, N., Xie, J., Luo, H., Cheng, Q., Wu, J., Jia, M.,\nand Li, L. Efficient image captioning for edge devices.\nIn Williams, B., Chen, Y., and Neville, J. (eds.), Thirty-\nSeventh AAAI Conference on Artificial Intelligence, AAAI\n2023, Thirty-Fifth Conference on Innovative Applica-\ntions of Artificial Intelligence, IAAI 2023, Thirteenth\nSymposium on Educational Advances in Artificial Intel-\nligence, EAAI 2023, Washington, DC, USA, February\n7-14, 2023, pp. 2608\u20132616. AAAI Press, 2023b. doi:\n10.1609/AAAI.V37I2.25359.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\nY., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M.,\nLhoest, Q., and Rush, A. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pp. 38\u201345, Online,\nOctober 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nXu, J., Mei, T., Yao, T., and Rui, Y. Msr-vtt: A large video\ndescription dataset for bridging video and language. IEEE\nInternational Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2016.\nYang, K., Deng, J., An, X., Li, J., Feng, Z., Guo, J., Yang, J.,\nand Liu, T. ALIP: adaptive language-image pre-training\nwith synthetic caption. CoRR, abs/2308.08428, 2023a.\ndoi: 10.48550/arXiv.2308.08428.\nYang, Z., Ping, W., Liu, Z., Korthikanti, V., Nie, W., Huang,\nD., Fan, L., Yu, Z., Lan, S., Li, B., Liu, M., Zhu,\nY., Shoeybi, M., Catanzaro, B., Xiao, C., and Anand-\nkumar, A.\nRe-vilm: Retrieval-augmented visual lan-\nguage model for zero and few-shot image captioning.\nCoRR, abs/2302.04858, 2023b.\ndoi: 10.48550/arXiv.\n2302.04858.\nYasunaga, M., Aghajanyan, A., Shi, W., James, R.,\nLeskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and\nYih, W. Retrieval-augmented multimodal language mod-\neling. In Krause, A., Brunskill, E., Cho, K., Engelhardt,\nB., Sabato, S., and Scarlett, J. (eds.), International Con-\nference on Machine Learning, ICML 2023, 23-29 July\n2023, Honolulu, Hawaii, USA, volume 202 of Proceed-\nings of Machine Learning Research, pp. 39755\u201339769.\nPMLR, 2023.\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\nimage descriptions to visual denotations: New similarity\n13\nLinear Alignment of Vision-language Models for Image Captioning\nmetrics for semantic inference over event descriptions.\nTrans. Assoc. Comput. Linguistics, 2:67\u201378, 2014. doi:\n10.1162/tacl\\ a\\ 00166.\nYu, Y., Chung, J., Yun, H., Hessel, J., Park, J. S., Lu, X.,\nAmmanabrolu, P., Zellers, R., Bras, R. L., Kim, G., and\nChoi, Y. Multimodal knowledge alignment with rein-\nforcement learning. CoRR, abs/2205.12630, 2022. doi:\n10.48550/arXiv.2205.12630.\nZhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P.,\nLi, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient\nfine-tuning of language models with zero-init attention.\nCoRR, abs/2303.16199, 2023. doi: 10.48550/ARXIV.\n2303.16199.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models. In Meila, M. and Zhang, T. (eds.),\nProceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learning\nResearch, pp. 12697\u201312706. PMLR, 2021.\nZhou, Y., Ren, J., Li, F., Zabih, R., and Lim, S.-N. Test-\ntime distribution normalization for contrastively learned\nvisual-language models. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny,\nM. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models.\nCoRR,\nabs/2304.10592, 2023.\ndoi: 10.48550/ARXIV.2304.\n10592.\n14\nLinear Alignment of Vision-language Models for Image Captioning\nSupplementary Material\nFirst, we provide the source code to reproduce all our experiments in Appendix A. To provide further insights into our\nmethod ReCap, we provide additional results on cross-modal retrieval, ablation studies, effect of different data sources,\nour DAL, and our evaluation as image captioning metric in Appendix B. Further, we provide more qualitative analysis\non retrieved captions after the linear alignment and the effect of synthetic captions in Appendix C. Appendix E gives a\nrigorous theoretical intuition on the motivation of our linear alignment. Finally, Appendix D elaborates on the different\nhyperparameters we searched, including the retrieval parameter k, the decoding strategy, different vision encoders, generative\nlanguage models, etc.\nA. Source Code\nTo facilitate reproducibility of our findings, we provide the source code along with the submission in the supplementary\nmaterial. We included a README file that thoroughly explains how to execute our pipeline. We will make the code publicly\navailable upon acceptance.\nB. Additional Results\nCross-modal retrieval\nWe evaluate all publicly available CLIP vision encoders on cross-modal retrieval on the MS-COCO\nand Flickr30k datasets. We report average recalls and standard error in Table 6. We find that larger models improve retrieval\nperformance and, perhaps surprisingly, the RN50\u00d764 encoder outperforms the largest ViT variant in four out of 6 categories\nwhen considering image to text retrieval on MS-COCO and Flickr30k. Since ReCap is based on image to text retrieval we\nselect RN50\u00d764 as our retrieval model.\nTable 6. Comparison of different CLIP vision encoders on the cross-modal retrieval task on MS-COCO and Flickr30k. We report average\nrecalls and standard error for all publicly available CLIP vision encoders. Boldface indicates highest average scores.\nMS-COCO\nIMAGE \u2192 TEXT\nTEXT \u2192 IMAGE\nMETHOD\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nCLIPRN50\n50.2 \u00b1 0.7\n74.9 \u00b1 0.6\n83.3 \u00b1 0.5\n28.4 \u00b1 0.5\n52.6 \u00b1 0.5\n64.2 \u00b1 0.5\nCLIPRN50X4\n52.2 \u00b1 0.7\n75.9 \u00b1 0.6\n67.5 \u00b1 0.5\n31.3 \u00b1 0.5\n55.7 \u00b1 0.5\n66.5 \u00b1 0.5\nCLIPRN50X16\n53.6 \u00b1 0.7\n77.9 \u00b1 0.6\n85.8 \u00b1 0.5\n33.2 \u00b1 0.5\n57.0 \u00b1 0.5\n67.5 \u00b1 0.5\nCLIPRN50X64\n60.7 \u00b1 0.7\n82.2 \u00b1 0.5\n88.5 \u00b1 0.5\n34.3 \u00b1 0.5\n59.5 \u00b1 0.5\n69.9 \u00b1 0.5\nCLIPVIT-B/32\n52.3 \u00b1 0.7\n76.0 \u00b1 0.6\n84.4 \u00b1 0.5\n30.2 \u00b1 0.5\n55.1 \u00b1 0.5\n66.4 \u00b1 0.5\nCLIPVIT-B/16\n52.6 \u00b1 0.7\n76.9 \u00b1 0.6\n85.0 \u00b1 0.5\n32.9 \u00b1 0.5\n57.7 \u00b1 0.5\n68.1 \u00b1 0.5\nCLIPVIT-L/14\n57.0 \u00b1 0.7\n80.5 \u00b1 0.6\n86.9 \u00b1 0.5\n36.1 \u00b1 0.5\n60.3 \u00b1 0.5\n70.3 \u00b1 0.5\nCLIPVIT-L/14@336PX\n58.5 \u00b1 0.7\n81.3 \u00b1 0.6\n88.1 \u00b1 0.5\n35.9 \u00b1 0.5\n60.4 \u00b1 0.5\n70.5 \u00b1 0.5\nFLICKR30K\nCLIPRN50\n80.8 \u00b1 1.3\n95.4 \u00b1 0.7\n97.8 \u00b1 0.5\n57.9 \u00b1 1.1\n83.1 \u00b1 0.8\n89.8 \u00b1 0.6\nCLIPRN101\n79.2 \u00b1 1.3\n94.8 \u00b1 0.7\n97.8 \u00b1 0.5\n57.5 \u00b1 1.1\n81.9 \u00b1 0.8\n88.6 \u00b1 0.7\nCLIPRN50X4\n83.0 \u00b1 1.2\n95.9 \u00b1 0.6\n98.2 \u00b1 0.4\n61.6 \u00b1 1.1\n84.7 \u00b1 0.8\n90.1 \u00b1 0.6\nCLIPRN50X16\n84.2 \u00b1 1.2\n97.0 \u00b1 0.5\n99.2 \u00b1 0.3\n64.5 \u00b1 1.1\n85.9 \u00b1 0.7\n91.5 \u00b1 0.6\nCLIPRN50X64\n88.5 \u00b1 1.0\n98.3 \u00b1 0.4\n99.4 \u00b1 0.2\n69.1 \u00b1 1.0\n90.7 \u00b1 0.6\n95.0 \u00b1 0.4\nCLIPVIT-B/32\n79.8 \u00b1 1.2\n96.3 \u00b1 0.6\n98.6 \u00b1 0.4\n59.3 \u00b1 1.1\n83.7 \u00b1 0.8\n90.3 \u00b1 0.6\nCLIPVIT-B/16\n83.0 \u00b1 1.2\n96.3 \u00b1 0.6\n99.3 \u00b1 0.3\n63.0 \u00b1 1.1\n85.9 \u00b1 0.7\n91.8 \u00b1 0.6\nCLIPVIT-L/14\n85.7 \u00b1 1.1\n98.3 \u00b1 0.4\n99.3 \u00b1 0.3\n64.8 \u00b1 1.1\n87.3 \u00b1 0.7\n92.4 \u00b1 0.5\nCLIPVIT-L/14@336PX\n88.5 \u00b1 1.0\n99.3 \u00b1 0.3\n99.6 \u00b1 0.2\n67.0 \u00b1 1.0\n88.7 \u00b1 0.7\n93.4 \u00b1 0.5\nImpact of Linear Alignment\nWe conduct an ablation study where we assess the effect of the linear alignment. To this\nend, we evaluate a setting where we do not use our linear alignment, which we call ReCapZS, where ZS stands for zero-shot,\nsince it does not require any training. Further, we distinguish between two types of linear alignment, (i) constrained using\northogonal procrustes (PR), and (ii), unconstrained using ordinary least squares (OLS). Results on the MS-COCO test set\nare shown in Table 7. We observe a substantial performance drop on all metrics for ReCapZS, showcasing the effectiveness\nof our linear alignment. The best performing method in terms of CIDEr-D and SPICE is ReCapOLS, since the unconstrained\n15\nLinear Alignment of Vision-language Models for Image Captioning\nTable 7. Ablation study for different methods to compute our linear alignment on the MS-COCO test set. We compare the constrained\nmapping (PR), unconstrained mapping (OLS), and using no mapping at all (ZS). We report mean and standard error for all settings.\nMETHOD\nCIDER-D\nSPICE\nACLIP\nREFACLIP-S\nRECAPZS\n91.1 \u00b1 0.9\n19.1 \u00b1 0.1\n47.1 \u00b1 0.2\n56.7 \u00b1 0.2\nRECAPPR\n101.9 \u00b1 1.0\n20.4 \u00b1 0.1\n52.5 \u00b1 0.2\n61.6 \u00b1 0.1\nRECAPOLS\n108.3 \u00b1 1.0\n21.2 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\nTable 8. Training-free use of additional data sources on the MS-COCO (CO) test set for ReCapOLS. Additional data sources include\ncaptions from Flickr30k (F30), VizWiz (VW), MSRVTT (MV), and synthetic captions (SC) from DAL. We report mean and standard\nerror of for all metrics.\nDATASTORE\nCIDER-D\nSPICE\nACLIP-S\nREFACLIP-S\nCO\n108.3 \u00b1 1.0\n21.2 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\nCO + F30\n107.9 \u00b1 1.0\n21.1 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\nCO + F30 + VW\n108.0 \u00b1 1.0\n21.2 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\nCO + F30 + MV\n108.2 \u00b1 1.0\n21.2 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\nCO + F30 + VW + MV\n108.2 \u00b1 1.0\n21.2 \u00b1 0.1\n50.4 \u00b1 0.2\n60.6 \u00b1 0.2\nCO + SC\n106.7 \u00b1 1.0\n21.2 \u00b1 0.1\n74.7 \u00b1 0.1\n76.2 \u00b1 0.1\nmapping leads to a stronger alignment with reference captions. The best performance on our learning-based metrics is\nachieved by ReCap. On one hand we observe the trend that on OLS alignment achieves a better trade-off between rule-based\nand our learning-based metrics. The PR alignment on the other hand diverges more from reference captions and attains the\nbest performance on our learning-based metrics. Further, as we show in Table 9, the PR alignment leads to higher correlation\nwith human judgement.\nThus, we recommend the following criterion for when to deploy which optimization scheme:\n\u2022 For retrieval-augmented caption generation, use OLS\n\u2022 For caption evaluation use PR\nEffect of different data sources\nWe conduct another line of experiments where we investigate the effect of additional\ndata sources in the datastore. To this end, we use ReCap aligned to MS-COCO data and add data from Flickr30k, VizWiz,\nMSRVTT, and synthetic captions from our DAL to the datastore. In Table 8 we report CIDEr-D, SPICE, aCLIP, and\nRefaCLIP for all settings. Generally, we observe that our synthetic captions have the most impact on captioning performance\non our aCLIP-S and RefaCLIP-S metrics. For the remaining metrics we do not observe a significant difference independent\nof the added data source. This means that even though the datastore grows, there is not much difference in the captions that\nare provided to the LM in the prompt, i.e. even though captions are added, they are never retrieved. This is different for\nsynthetic captions though, and thus, illustrates the potential utility of high quality synthetic captions.\nDatastore-augmentation Loop\nWe investigate the behaviour of our DAL by looking at the correlation of different metrics\nused for filtering in Figure 4. We also consider CLIP-score (CLIP-S) and RefCLIP-score (CLIP-RS) proposed in Hessel et al.\n(2021), and other n-gram based metrics, such as ROUGE-L (Lin, 2004), and BLEU (Papineni et al., 2002). Our results show\nthat metrics, such as BLEU, ROUGE-L and CIDEr-D correlate strongly with each other. This is due to the fact, that they all\nrely on n-gram based matching to reference captions. Further, CLIP-S and CLIP-RS correlate strongly with each other, since\nthey are both based on cosine similarity by CLIP. The same is true for aCLIP-S, and RefaCLIP-S, which are both based on\ncosine similarity of our aligned CLIP. However, aCLIP-S and RefaCLIP-S both correlate stronger with n-gram based metrics\nthan CLIP-S and RefCLIP-S due to the alignment to reference captions. Interestingly, SPICE is entirely decorrelated to\nall other metrics, since it is based on semantic scene graphs. This indicates that some of these metrics evaluate different\naspects of human judgement, thus, optimizing for one metric does not necessarily lead to improvement in any other metric.\nInterestingly, the correlation between our aCLIP-S metrics and CLIP-S metrics is, perhaps, lower than one might expect.\nThis indicates that our proposed metrics behave differently to CLIP-S and are more geared toward the human annotated\nreferences.\n16\nLinear Alignment of Vision-language Models for Image Captioning\nFigure 4. Pearson correlation between commonly used image captioning metrics for captions generated via ReCap on the MS-COCO test\nset.\n1\n2\n3\n4\n5\n6\nIteration\n0.96\n0.98\n1.00\n1.02\n1.04\n1.06\nCIDEr-D\n1\n2\n3\n4\n5\n6\nIteration\n0.21025\n0.21050\n0.21075\n0.21100\n0.21125\n0.21150\n0.21175\n0.21200\n0.21225\nSPICE\n1\n2\n3\n4\n5\n6\nIteration\n1.265\n1.270\n1.275\n1.280\n1.285\n1.290\n1.295\naCLIP-S\n1\n2\n3\n4\n5\n6\nIteration\n0.950\n0.952\n0.954\n0.956\n0.958\n0.960\n0.962\n0.964\nRefaCLIP-S\nFigure 5. Development of CIDEr-D, SPICE, aCLIP-S, and RefaCLIP-S for DAL on the MS-COCO validation set where we use RefaCLIP-\nS for quality filtering.\nFurther, we investigate the development of the different metrics after each iteration of DAL on the MS-COCO validation\nset in Figure 5. We observe that CIDEr-D constantly decreases, while SPICE fluctuates without changing significantly.\nHowever, aCLIP-S and RefaCLIP-S exhibit a monotonic improvement across every DAL iteration.\nFinally, we show the development of the hyperparameter k during DAL and the number of synthetic captions that are on\naverage provided to the LM for a given image in Figure 6. We find that as soon as we add synthetic captions to the datastore\n(Figure 6, right), the best choice for k on the validation set decreases from k = 13 to k = 4 and stagnates. We hypothesize\nthis is due to the increasing amount of synthetic captions that would otherwise be present in the prompt which might harm\nperformance. The number of synthetic captions in the prompt (Figure 6, left) generally increases with more iterations of\nDAL since more synthetic captions are added to the datastore. Approximately two out of four captions in the prompt of the\nLM are synthetic, which amounts to 50% of the captions in the prompt. This number is similar across all iterations of DAL.\nThis means that the prompt to the LM is a balanced mix of human annotated captions and synthetically generated captions.\nWe believe that this is the desired behavior to ensure the generated captions do not diverge too much from ground truth\nreferences. Note that this behavior naturally emerges during training and we did not control for this.\nImage-captioning Metric\nWe report extended results for caption evaluation and show additional results on the THumB\ndataset (Kasai et al., 2022). THumB is a subset of MS-COCO images from the test split of Karpathy & Fei-Fei (2017)\n17\nLinear Alignment of Vision-language Models for Image Captioning\n0\n1\n2\n3\n4\n5\nIteration\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nNumber of synthetic captions\nSynthetic Captions per Image\n0\n1\n2\n3\n4\n5\nIteration\n4\n6\n8\n10\n12\nk\nBest value for k\nFigure 6. Development of the hyperparameter k and the number of synthetic captions per image during DAL on the MS-COCO dataset.\nthat contains human rankings for candidate captions. Again, we compare our metrics against the current state-of-the-art\nmetrics, namely CLIP+DN (Zhou et al., 2023) and CLIP-score variants (Hessel et al., 2021, CLIP-S,RefCLIP-S). We also\ninclude an ablation of CLIP+DN, called CLIP+DN* from Zhou et al. (2023) and an ablation for our metrics where we use\nthe ViT-B/32 encoder (Dosovitskiy et al., 2021). There are no published results for MID on THumB and SoftSPICE on\nFlickr8k-CF and THumB. We observe a significant improvement of aCLIP-S and RefaCLIP-S over CLIP-S and RefCLIP-S.\nHowever, CLIP+DN variants reach higher correlation with human judgements on THumB. Interestingly, we find that\nthe RN50\u00d764 based encoder generally correlates more strongly with human judgement than the ViT-B/32 encoder in\nboth the reference-based, and the reference-free case. These results suggest, that the best metric for evaluation depends\non the dataset to evaluate on, as our reference-free metric outperformed CLIP+DN variants on the Flickr8k-Expert and\nFlickr8k-Crowdflower datasets.\nC. Additional Qualitative Analysis\nWe show some examples for retrieval with and without our linear alignment in Figure 7. The top row shows the top-k\nsamples for using off-the-shelf CLIP for retrieval, while the bottom row shows retrieval for our aligned CLIP. After the\nlinear alignment, the retrievals fit better to the image. For example, CLIP assigns a high similarity to \u201copen suitcase\u201d for the\nfigure in the middle, although the suitcase in the image is closed. Our aligned CLIP does not assign a high similarity to the\nsame caption anymore, and retrieves more appropriate captions.\nWe show additional examples for captions generated after our DAL in Figure 8.\nD. Hyperparameter Search\nEffect of different vision encoders\nWe investigate the effect of different vision encoders on the captioning performance\nof ReCap on the MS-COCO validation set. In this regard, we compare all publicly available encoder variants of CLIP, which\ncomprise ViT-based (Dosovitskiy et al., 2021), as well as resnet-based (He et al., 2016) architectures. The best performing\nmodel for our retrieval-based image captioning is RN50\u00d764 (see Table 10). This corroborates our results for cross-modal\nretrieval, where RN50\u00d764 outperformed all other encoders Appendix B.\nTop-k retrieval\nWe search over different values for our hyperparameters k on the MS-COCO, Flickr30k, VizWiz, and\nMSRVTT validation sets. We report results in Table 11 and Table 12 for MS-COCO, and Flickr30k, respectively. The results\nfor VizWiz and MSRVTT are shown in Table 13, and Table 14, respectively. For searching over values for k we use greedy\ndecoding, to isolate the effect of the hyperparameter.\nLanguage-model scales\nWe evaluate FLAN-T5 model sizes of 80 M, 250 M, 720 M, 3 B, and 11 B scales. Further, we\ninclude decoder-only LMs, such as GPT-2 (Radford et al., 2018), GPT-J (Wang & Komatsuzaki, 2021), and Llama 7B\n(Touvron et al., 2023). The results can be observed in Table 15. Our results show that there is not much performance gain\ngoing from FLAN-T5-LARGE to FLAN-T5-XXL. We suspect this is due to the design of the prompt which apparently suits\nFLAN-T5-LARGE particularly well. Surprisingly, even the small variant of FLAN-T5 reaches a CIDEr-D score above 90,\n18\nLinear Alignment of Vision-language Models for Image Captioning\nTable 9. Correlation of different metrics with human judgement on the THumB dataset. We report Kendall\u2019s \u03c4c for every method. The\nvariance of the estimator \u03c4 depends on the sample size and is equal for each method (0.0002). Boldface indicates highest scores.\nMETHOD\nFLICKR8K-E\nFLICKR8K-CF\nTHUMB\nREFERENCE-FREE\nCLIP-S\n51.4\n34.3\n19.9\nCLIP+DN*\n53.2\n35.1\n22.0\nCLIP+DN\n54.0\n35.2\n23.3\nACLIPVIT-B/32,OLS\n54.9\n35.8\n18.5\nACLIPVIT-B/32\n54.9\n34.9\n20.5\nACLIPRN50X64,OLS\n54.1\n36.5\n18.5\nACLIPRN50X64\n55.1\n36.2\n22.5\nREFERENCE-BASED\nBLEU@1\n32.3\n17.9\n11.1\nBLEU@4\n30.8\n16.9\n6.9\nCIDER\n43.9\n24.6\n13.8\nREFCLIP-S\n53.0\n36.4\n24.7\nSOFTSPICE\n54.2\nN/A\nN/A\nMID\n54.9\n37.3\nN/A\nCLIP+DN-REF*\n54.2\n36.8\n26.0\nCLIP+DN-REF\n55.0\n37.0\n27.1\nREFACLIPSCOREVIT-B/32\n55.3\n35.4\n21.7\nREFACLIPSCOREVIT-B/32,OLS\n55.2\n36.3\n19.9\nREFACLIPSCORERN50X64\n55.5\n36.7\n24.3\nREFACLIPSCORERN50X64,OLS\n54.6\n36.9\n21.3\nwhich amounts to decent captioning quality.\nOur results for decoder-only LMs show that they generally perform worse than encoder-decoder ones. We found that\ndecoder-only models are generally more sensitive to prompt ordering, which was also found in prior works (Zhao et al.,\n2021). Perhaps surprisingly, GPT-J outperforms the recently proposed Llama, which reaches performance on-par with GPT-2.\nGenerally, we belive that we could improve performance of larger models by more extensive prompt tuning. However,\nremarkably, FLAN-T5 performs really well in our setup without the need for extensive prompt tuning.\nDifferent decoding strategies\nAs illustrated by (Holtzman et al., 2020), the decoding strategy substantially affects human\napproval of generated captions. Therefore, we evaluate different decoding strategies, including greedy decoding, sampling,\ntop-k sampling, and nucleus sampling. First, we search over different temperatures \u03c4 and number of generated captions l\nfor nucleus sampling (Holtzman et al., 2020). After sampling l captions from the LM, we select the highest scoring one\naccording to our aligned CLIP. To find the best parameters \u03c4 and l we set k to the best value we found in the preceeding\ngridsearch with greedy decoding. Results are reported in Table 17, and Table 16 for MS-COCO, and Flickr30k, respectively.\nThe results for VizWiz and MSRVTT are shown in Table 18, and Table 19, respectively.\nThe results for other decoding schemes are shown in Table 20. For greedy decoding we only generate one caption, hence\nno selection step is required after generation. We use the same temperature as the best nucleus sampling setting for topk\nand regular sampling. We find that nucleus sampling with l = 1 performs close to greedy decoding, however when setting\nl = 10 and using caption selection via our aligned CLIP, we observe a substantial improvement.\nPrompt ordering\nUsually we would provide the captions in the prompt from most-similar to least similar, i.e. the least\nsimilar prompt is the most recent in the context. However, one may think the exact opposite ordering might lead to better\ncaptioning performance, since the LM might exhibit a form of recency bias. This concerns our setting as well, since the\nvalues we found for k are larger than one might expect, e.g., on MS-COCO we found k = 13 to perform best. Hence,\nwe provide results for the worst-to-best ordering in Table 21. Indeed, we found that different ordering of captions in the\nprompt leads to different results. Ordering from worst-to-best, i.e. most similar captions appear more recently, leads to\nan improvement on CIDEr-D score. Therefore, by default, we provide the prompts in the order from worst-to-best in the\nprompt.\n19\nLinear Alignment of Vision-language Models for Image Captioning\nThe overhead view of a city \nstreet with bikes travelling\nAn overhead shot of a group of \npeople eating outside\nAn arrangement of biking \naccessories is viewed from above\nA lot of bikes parked next to each \nother on a sidewalk\nBicycles are parked at a bike \nstand on the street\nA group of bikes parked on the \nstreet\nan open suitcase with three \npurple items of clothing\nA suitcase containing basic \nclothing for travel\nA person's hand on a suitcase \nthat is open\nA suitcase on the \ufb02oor with its \ntags still attached\nA suitcase that is sitting on the \n\ufb02oor\nA suitcase that is on the \ufb02oor \nwith its handle up\nA woman talks to a man on top \nof a blue bus\nA large grey tour bus is parked \non a tarmac\nOne woman about to hug another \non a car transport ferry\ntwo people standing in front of a \nbus in the street\nTwo people about to get on-board \na bus\nA couple of people standing in \nfront of a bus in the street\nFigure 7. Sample images and retrieved captions with (bottom) and without (top) our linear alignment to MS-COCO training data. We\nshow three of the closest captions to an image. Images are taken from the MS-COCO validation set.\nE. Motivation of Linear Alignment\nCLIP has been trained to align text with images in a joint embedding space. We want to use the CLIP encoders for retrieval\nby cosine similarity on an image-captioning task. However, there might be a disparity between the pretraining domain\nof CLIP and the downstream task. We aim to rectify this by a linear mapping. Our downstream task is retrieval of text\nembeddings ei by their corresponding image embeddings fi using the cosine similarity. Therefore, our objective is\nmax\nW\nX\ni\ncossim(ei, W fi).\n(6)\n20\nLinear Alignment of Vision-language Models for Image Captioning\nA dog and a cat are lying down next to each \nother on a couch.\nA pizza sitting on a plate \nnext to a television.\nA man is standing on a field with green \ngrass and a frisbee in the air.\nA man is standing on a grassy area \npreparing to throw a frisbee.\nA pizza sitting on a plate \non a table.\nA dog and a cat are lying down on a couch.\nA kitchen with a sink, cabinets, and counter\ntop space.\nA person doing a trick on a\nskateboard.\nA man sittin on a sufboard on the beach\nnext to the ocean.\nA man sitting on a surfboard next to the \nocean.\nA person on a skateboard doing\na trick on a ramp.\nA kitchen with a sink, stove, and counter top.\nFigure 8. Captions generated via ReCap (bottom) and ReCap+DAL (top). Images were taken from the MS-COCO validation set.\nFor objective (6) a closed-form solution is unknown. By constraining W to be an orthogonal matrix, however, we obtain\nequivalence to the least-squares objective because\narg max\nW \u22a4W =I\nX\ni\ncossim(ei, W fi)\n(7)\n= arg max\nW \u22a4W =I\nX\ni\ne\u22a4\ni W fi\n\u2225ei\u22252\u2225W fi\u22252\n(8)\n= arg max\nW \u22a4W =I\nX\ni\ne\u22a4\ni W fi\n(9)\n= arg min\nW \u22a4W =I\n\u2212\nX\ni\ne\u22a4\ni W fi\n(10)\n= arg min\nW \u22a4W =I\nX\ni\n(\u2225W fi\u22252\n2 + \u2225ei\u22252\n2 \u2212 2e\u22a4\ni W fi)\n(11)\n= arg min\nW \u22a4W =I\nX\ni\n(f \u22a4\ni W \u22a4W fi + e\u22a4\ni ei \u2212 2e\u22a4\ni W fi)\n(12)\n= arg min\nW \u22a4W =I\nX\ni\n(W fi \u2212 ei)\u22a4(W fi \u2212 ei)\n(13)\n= arg min\nW \u22a4W =I\nX\ni\n\u2225W fi \u2212 ei\u22252\n2.\n(14)\nArtetxe et al. (2016) have pointed out this fact previously. Note that from (8) to (9) and from (10) to (11) the term \u2225W fi\u22252\ncan be dropped/added as it appears constant to the optimization objective because W is orthogonal and, therefore, preserves\n21\nLinear Alignment of Vision-language Models for Image Captioning\nTable 10. Search over all publicly available CLIP vision encoder backbones evaluated on the MS-COCO validation set. We report mean\nand standard error for all settings. |\u03b8| denotes the number of trainable parameters.\nVISION ENCODER\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n|\u03b8|\nRN50\n75.5 \u00b1 0.2\n28.0 \u00b1 0.3\n56.1 \u00b1 0.2\n97.0 \u00b1 0.9\n19.7 \u00b1 0.1\n1 M\nRN101\n74.6 \u00b1 0.2\n27.7 \u00b1 0.3\n56.1 \u00b1 0.2\n96.3 \u00b1 0.9\n19.4 \u00b1 0.1\n262 K\nRN50X4\n75.4 \u00b1 0.2\n28.5 \u00b1 0.3\n56.6 \u00b1 0.2\n99.2 \u00b1 0.9\n19.9 \u00b1 0.1\n410 K\nRN50X16\n76.4 \u00b1 0.2\n29.3 \u00b1 0.4\n57.0 \u00b1 0.2\n102.5 \u00b1 0.9\n20.4 \u00b1 0.1\n590 K\nRN50X64\n77.7 \u00b1 0.2\n30.5 \u00b1 0.4\n58.0 \u00b1 0.2\n107.3 \u00b1 1.0\n21.2 \u00b1 0.1\n1 M\nVIT-B/32\n75.2 \u00b1 0.2\n27.9 \u00b1 0.3\n56.0 \u00b1 0.2\n96.4 \u00b1 0.9\n19.4 \u00b1 0.1\n262 K\nVIT-B/16\n76.2 \u00b1 0.2\n29.0 \u00b1 0.3\n56.7 \u00b1 0.2\n101.2 \u00b1 0.9\n20.0 \u00b1 0.1\n262 K\nVIT-L/14\n77.0 \u00b1 0.2\n29.9 \u00b1 0.4\n57.4 \u00b1 0.2\n104.7 \u00b1 1.0\n20.6 \u00b1 0.1\n590 K\nVIT-L/14@336PX\n77.4 \u00b1 0.2\n30.3 \u00b1 0.4\n57.7 \u00b1 0.2\n105.8 \u00b1 0.9\n20.8 \u00b1 0.1\n590 K\nthe norm of fi. The solution to this optimization problem is known as orthogonal procrustes (Sch\u00a8onemann, 1966) and can\nbe written as\nW = V U \u22a4,\n(15)\nwhere V and U are the orthogonal matrices of the singular value decomposition of F \u22a4E = U\u03a3V \u22a4 and F =\n(f1, . . . , fn)\u22a4, E = (e1, . . . , en)\u22a4.\n22\nLinear Alignment of Vision-language Models for Image Captioning\nTable 11. Hyperparameter Search for k on the MS-COCO validation set for different levels of language abstraction using our semantic\nmapping computed via OLS. We report mean and standard error for all settings. We select the best k according to CIDEr-D score.\nk\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\nSINGLE CAPTIONS\n10\n77.4 \u00b1 0.2\n30.4 \u00b1 0.4\n57.6 \u00b1 0.2\n105.2 \u00b1 1.0\n20.9 \u00b1 0.1\n11\n77.4 \u00b1 0.2\n30.4 \u00b1 0.4\n57.7 \u00b1 0.2\n105.4 \u00b1 1.0\n20.9 \u00b1 0.1\n12\n77.4 \u00b1 0.2\n30.3 \u00b1 0.4\n57.7 \u00b1 0.2\n105.2 \u00b1 1.0\n20.9 \u00b1 0.1\n13\n77.4 \u00b1 0.2\n30.5 \u00b1 0.4\n57.7 \u00b1 0.2\n105.5 \u00b1 1.0\n20.8 \u00b1 0.1\n14\n77.4 \u00b1 0.2\n30.5 \u00b1 0.4\n57.8 \u00b1 0.2\n105.4 \u00b1 1.0\n20.8 \u00b1 0.1\n15\n77.3 \u00b1 0.2\n30.5 \u00b1 0.4\n57.7 \u00b1 0.2\n105.4 \u00b1 1.0\n20.9 \u00b1 0.1\n16\n77.2 \u00b1 0.2\n30.4 \u00b1 0.4\n57.7 \u00b1 0.2\n105.4 \u00b1 1.0\n20.8 \u00b1 0.1\n17\n77.2 \u00b1 0.2\n30.2 \u00b1 0.4\n57.6 \u00b1 0.2\n104.9 \u00b1 1.0\n20.9 \u00b1 0.1\nALL CAPTIONS\n1\n72.7 \u00b1 0.2\n24.8 \u00b1 0.3\n53.9 \u00b1 0.2\n87.0 \u00b1 0.9\n18.0 \u00b1 0.1\n2\n73.7 \u00b1 0.2\n26.4 \u00b1 0.3\n54.7 \u00b1 0.2\n90.8 \u00b1 0.9\n18.2 \u00b1 0.1\n3\n74.0 \u00b1 0.2\n26.4 \u00b1 0.3\n54.8 \u00b1 0.2\n91.0 \u00b1 0.9\n18.2 \u00b1 0.1\n4\n74.0 \u00b1 0.2\n26.6 \u00b1 0.3\n55.0 \u00b1 0.2\n91.3 \u00b1 0.9\n18.5 \u00b1 0.1\n5\n74.0 \u00b1 0.2\n26.9 \u00b1 0.3\n55.1 \u00b1 0.2\n91.6 \u00b1 0.9\n18.4 \u00b1 0.1\nLOCALIZED NARRATIVES\n1\n55.3 \u00b1 0.3\n11.7 \u00b1 0.2\n43.1 \u00b1 0.2\n45.4 \u00b1 0.6\n11.9 \u00b1 0.1\n2\n54.3 \u00b1 0.3\n11.8 \u00b1 0.2\n43.0 \u00b1 0.2\n48.0 \u00b1 0.7\n13.2 \u00b1 0.1\n3\n53.8 \u00b1 0.3\n12.3 \u00b1 0.2\n43.0 \u00b1 0.2\n50.9 \u00b1 0.7\n14.0 \u00b1 0.1\n4\n53.0 \u00b1 0.3\n12.1 \u00b1 0.2\n42.7 \u00b1 0.2\n51.7 \u00b1 0.7\n14.3 \u00b1 0.1\n5\n52.5 \u00b1 0.3\n12.0 \u00b1 0.2\n42.6 \u00b1 0.2\n52.6 \u00b1 0.7\n14.4 \u00b1 0.1\n6\n52.0 \u00b1 0.3\n12.3 \u00b1 0.2\n42.6 \u00b1 0.2\n53.1 \u00b1 0.7\n14.6 \u00b1 0.1\nTable 12. Hyperparameter Search for k on the Flickr30k validation set for different levels of language abstraction using our semantic\nmapping computed via OLS. We report mean and standard error for all settings.\nk\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\nSINGLE CAPTIONS\n10\n74.8 \u00b1 0.5\n26.4 \u00b1 0.7\n54.5 \u00b1 0.4\n63.6 \u00b1 1.9\n15.5 \u00b1 0.3\n11\n74.7 \u00b1 0.5\n26.3 \u00b1 0.7\n54.5 \u00b1 0.4\n64.4 \u00b1 2.0\n15.6 \u00b1 0.3\n12\n74.4 \u00b1 0.5\n26.2 \u00b1 0.7\n54.6 \u00b1 0.4\n64.6 \u00b1 1.9\n15.5 \u00b1 0.3\n13\n74.2 \u00b1 0.5\n26.1 \u00b1 0.7\n54.6 \u00b1 0.4\n64.4 \u00b1 1.9\n15.5 \u00b1 0.3\n14\n74.6 \u00b1 0.5\n26.2 \u00b1 0.7\n54.3 \u00b1 0.4\n64.4 \u00b1 1.9\n15.6 \u00b1 0.3\n15\n74.3 \u00b1 0.5\n26.3 \u00b1 0.7\n54.5 \u00b1 0.4\n64.8 \u00b1 1.9\n15.6 \u00b1 0.3\n16\n75.0 \u00b1 0.5\n26.7 \u00b1 0.7\n54.7 \u00b1 0.4\n64.6 \u00b1 1.9\n15.8 \u00b1 0.3\n17\n74.5 \u00b1 0.5\n26.9 \u00b1 0.7\n54.8 \u00b1 0.4\n65.5 \u00b1 1.9\n15.6 \u00b1 0.3\n18\n74.9 \u00b1 0.5\n26.8 \u00b1 0.7\n54.8 \u00b1 0.4\n66.2 \u00b1 2.0\n15.7 \u00b1 0.3\n19\n74.4 \u00b1 0.5\n26.9 \u00b1 0.7\n54.8 \u00b1 0.4\n65.6 \u00b1 1.9\n15.8 \u00b1 0.3\nALL CAPTIONS\n1\n65.8 \u00b1 0.5\n20.3 \u00b1 0.7\n49.8 \u00b1 0.4\n48.7 \u00b1 1.8\n13.4 \u00b1 0.3\n2\n67.9 \u00b1 0.5\n21.5 \u00b1 0.7\n50.5 \u00b1 0.5\n52.2 \u00b1 1.8\n13.9 \u00b1 0.3\n3\n68.1 \u00b1 0.5\n22.0 \u00b1 0.7\n51.0 \u00b1 0.4\n53.2 \u00b1 1.9\n13.7 \u00b1 0.3\n4\n69.6 \u00b1 0.5\n23.0 \u00b1 0.7\n51.4 \u00b1 0.4\n54.4 \u00b1 1.9\n14.1 \u00b1 0.3\n5\n69.0 \u00b1 0.5\n23.0 \u00b1 0.7\n51.3 \u00b1 0.4\n54.5 \u00b1 1.9\n14.2 \u00b1 0.3\nLOCALIZED NARRATIVES\n1\n54.2 \u00b1 0.6\n9.0 \u00b1 0.4\n40.4 \u00b1 0.4\n24.4 \u00b1 1.3\n8.1 \u00b1 0.2\n2\n52.6 \u00b1 0.6\n8.6 \u00b1 0.4\n39.3 \u00b1 0.4\n23.3 \u00b1 1.1\n8.4 \u00b1 0.2\n3\n52.5 \u00b1 0.6\n9.5 \u00b1 0.4\n39.6 \u00b1 0.4\n25.4 \u00b1 1.2\n8.9 \u00b1 0.2\n4\n51.7 \u00b1 0.6\n9.6 \u00b1 0.4\n39.3 \u00b1 0.4\n26.0 \u00b1 1.2\n9.1 \u00b1 0.2\n5\n51.9 \u00b1 0.6\n9.6 \u00b1 0.4\n39.1 \u00b1 0.4\n25.6 \u00b1 1.2\n9.0 \u00b1 0.2\n23\nLinear Alignment of Vision-language Models for Image Captioning\nTable 13. Hyperparameter Search for k on the VizWiz validation set for ReCap with our linear alignment. We report mean and standard\nerror for all settings. We select the best k according to CIDEr-D score.\nk\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n1\n61.8 \u00b1 0.2\n15.5 \u00b1 0.2\n43.1 \u00b1 0.2\n48.5 \u00b1 0.6\n12.1 \u00b1 0.1\n2\n61.8 \u00b1 0.2\n16.5 \u00b1 0.2\n44.8 \u00b1 0.2\n50.9 \u00b1 0.7\n13.1 \u00b1 0.1\n3\n62.5 \u00b1 0.2\n16.9 \u00b1 0.2\n45.3 \u00b1 0.2\n51.1 \u00b1 0.7\n13.0 \u00b1 0.1\n4\n63.2 \u00b1 0.2\n17.5 \u00b1 0.2\n45.8 \u00b1 0.2\n52.7 \u00b1 0.7\n13.0 \u00b1 0.1\n5\n63.3 \u00b1 0.2\n17.5 \u00b1 0.2\n45.8 \u00b1 0.2\n52.6 \u00b1 0.7\n13.1 \u00b1 0.1\n6\n63.3 \u00b1 0.2\n17.6 \u00b1 0.2\n45.9 \u00b1 0.2\n52.4 \u00b1 0.7\n13.0 \u00b1 0.1\n7\n63.0 \u00b1 0.2\n17.5 \u00b1 0.2\n45.8 \u00b1 0.2\n51.7 \u00b1 0.7\n12.9 \u00b1 0.1\n8\n62.8 \u00b1 0.2\n17.5 \u00b1 0.2\n45.8 \u00b1 0.2\n51.6 \u00b1 0.7\n12.8 \u00b1 0.1\n9\n62.9 \u00b1 0.2\n17.5 \u00b1 0.2\n45.9 \u00b1 0.2\n51.3 \u00b1 0.7\n12.9 \u00b1 0.1\n10\n62.1 \u00b1 0.2\n17.0 \u00b1 0.2\n45.5 \u00b1 0.2\n50.3 \u00b1 0.6\n12.8 \u00b1 0.1\nTable 14. Hyperparameter Search for k on the MSRVTT validation set for ReCap with our linear alignment. We report mean and standard\nerror for all settings. We select the best k according to CIDEr-D score.\nk\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n3\n26.9 \u00b1 0.1\n4.8 \u00b1 0.0\n25.7 \u00b1 0.1\n36.6 \u00b1 0.4\n14.2 \u00b1 0.1\n4\n26.9 \u00b1 0.1\n4.8 \u00b1 0.0\n25.7 \u00b1 0.1\n36.6 \u00b1 0.4\n14.2 \u00b1 0.1\n5\n27.1 \u00b1 0.1\n4.9 \u00b1 0.0\n25.8 \u00b1 0.1\n36.7 \u00b1 0.4\n14.1 \u00b1 0.1\n6\n27.1 \u00b1 0.1\n4.9 \u00b1 0.0\n25.8 \u00b1 0.1\n36.4 \u00b1 0.4\n14.0 \u00b1 0.1\n7\n27.0 \u00b1 0.1\n4.9 \u00b1 0.0\n25.9 \u00b1 0.1\n36.4 \u00b1 0.3\n13.9 \u00b1 0.1\n8\n27.0 \u00b1 0.1\n4.9 \u00b1 0.0\n25.9 \u00b1 0.1\n36.7 \u00b1 0.4\n13.8 \u00b1 0.1\nTable 15. Comparison of different language models on the MS-COCO validation set. We report mean and standard error for all settings.\nMODEL\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\nENCODER-DECODER\nFLAN-T5-SMALL\n63.9 \u00b1 0.3\n23.3 \u00b1 0.3\n55.0 \u00b1 0.2\n93.9 \u00b1 1.0\n20.5 \u00b1 0.1\nFLAN-T5-BASE\n72.5 \u00b1 0.2\n27.1 \u00b1 0.3\n56.7 \u00b1 0.2\n100.0 \u00b1 0.9\n20.7 \u00b1 0.1\nFLAN-T5-LARGE\n77.7 \u00b1 0.2\n30.5 \u00b1 0.4\n58.0 \u00b1 0.2\n107.3 \u00b1 1.0\n21.2 \u00b1 0.1\nFLAN-T5-XL\n76.1 \u00b1 0.2\n29.4 \u00b1 0.4\n56.7 \u00b1 0.2\n104.7 \u00b1 0.9\n20.8 \u00b1 0.1\nFLAN-T5-XXL\n77.1 \u00b1 0.2\n30.2 \u00b1 0.4\n57.4 \u00b1 0.2\n107.0 \u00b1 1.0\n21.0 \u00b1 0.1\nDECODER-ONLY\nGPT-2\n64.9 \u00b1 0.3\n24.1 \u00b1 0.3\n49.5 \u00b1 0.2\n86.8 \u00b1 0.9\n19.1 \u00b1 0.1\nGPT-J 6B\n71.1 \u00b1 0.3\n29.1 \u00b1 0.4\n51.4 \u00b1 0.2\n97.5 \u00b1 1.0\n19.6 \u00b1 0.1\nLLAMA 7B\n61.5 \u00b1 0.3\n23.1 \u00b1 0.3\n49.3 \u00b1 0.2\n86.4 \u00b1 0.9\n19.5 \u00b1 0.1\nTable 16. Comparison of different values for temperature of nucleus sampling on the Flickr30k validation set for k = 18\nTEMPERATURE\nSAMPLES\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n1.0\n1\n74.8 \u00b1 0.5\n26.8 \u00b1 0.7\n54.6 \u00b1 0.4\n65.0 \u00b1 1.9\n15.8 \u00b1 0.3\n0.1\n10\n75.2 \u00b1 0.5\n27.5 \u00b1 0.7\n55.2 \u00b1 0.4\n68.7 \u00b1 2.0\n16.5 \u00b1 0.3\n0.3\n10\n74.5 \u00b1 0.5\n26.6 \u00b1 0.7\n55.2 \u00b1 0.4\n68.4 \u00b1 1.9\n16.8 \u00b1 0.3\n0.5\n10\n73.8 \u00b1 0.5\n25.6 \u00b1 0.7\n54.6 \u00b1 0.4\n68.4 \u00b1 2.1\n17.0 \u00b1 0.3\n0.1\n20\n75.3 \u00b1 0.5\n27.1 \u00b1 0.7\n55.2 \u00b1 0.4\n68.7 \u00b1 1.9\n16.5 \u00b1 0.3\n0.3\n20\n74.4 \u00b1 0.5\n26.6 \u00b1 0.7\n55.2 \u00b1 0.4\n69.3 \u00b1 2.0\n16.9 \u00b1 0.3\n0.5\n20\n73.4 \u00b1 0.5\n25.2 \u00b1 0.7\n54.6 \u00b1 0.4\n68.3 \u00b1 2.0\n17.3 \u00b1 0.3\n0.1\n30\n75.5 \u00b1 0.5\n27.5 \u00b1 0.7\n55.3 \u00b1 0.4\n68.7 \u00b1 2.0\n16.6 \u00b1 0.3\n0.3\n30\n74.2 \u00b1 0.5\n26.4 \u00b1 0.7\n55.4 \u00b1 0.4\n68.9 \u00b1 2.0\n17.2 \u00b1 0.3\n0.5\n30\n72.9 \u00b1 0.5\n24.4 \u00b1 0.7\n54.4 \u00b1 0.4\n67.7 \u00b1 2.0\n17.3 \u00b1 0.3\n24\nLinear Alignment of Vision-language Models for Image Captioning\nTable 17. Comparison of different values for temperature of nucleus sampling on the MS-COCO validation set for k = 13.\nTEMPERATURE\nSAMPLES\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n0.0\nN/A\n77.4 \u00b1 0.2\n30.5 \u00b1 0.4\n57.7 \u00b1 0.2\n105.5 \u00b1 1.0\n20.8 \u00b1 0.1\n0.1\n10\n77.7 \u00b1 0.2\n30.5 \u00b1 0.4\n58.0 \u00b1 0.2\n107.3 \u00b1 1.0\n21.2 \u00b1 0.1\n0.3\n10\n77.3 \u00b1 0.2\n29.9 \u00b1 0.4\n57.9 \u00b1 0.2\n106.8 \u00b1 0.9\n21.4 \u00b1 0.1\n0.5\n10\n76.5 \u00b1 0.2\n29.0 \u00b1 0.3\n57.3 \u00b1 0.2\n104.5 \u00b1 0.9\n21.3 \u00b1 0.1\n0.1\n20\n77.6 \u00b1 0.2\n30.4 \u00b1 0.4\n57.9 \u00b1 0.2\n107.2 \u00b1 1.0\n21.2 \u00b1 0.1\n0.3\n20\n77.2 \u00b1 0.2\n29.7 \u00b1 0.3\n57.8 \u00b1 0.2\n106.2 \u00b1 0.9\n21.4 \u00b1 0.1\n0.5\n20\n76.4 \u00b1 0.2\n28.6 \u00b1 0.3\n57.1 \u00b1 0.2\n103.9 \u00b1 0.9\n21.4 \u00b1 0.1\n0.1\n30\n77.6 \u00b1 0.2\n30.4 \u00b1 0.4\n57.9 \u00b1 0.2\n107.1 \u00b1 0.9\n21.2 \u00b1 0.1\n0.3\n30\n77.1 \u00b1 0.2\n29.5 \u00b1 0.3\n57.7 \u00b1 0.2\n106.1 \u00b1 0.9\n21.4 \u00b1 0.1\n0.5\n30\n76.4 \u00b1 0.2\n28.3 \u00b1 0.3\n57.1 \u00b1 0.2\n103.3 \u00b1 0.9\n21.6 \u00b1 0.1\nTable 18. Comparison of different values for temperature of nucleus sampling on the VizWiz validation set for k = 4.\nTEMPERATURE\nSAMPLES\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n0.0\nN/A\n63.2 \u00b1 0.2\n17.5 \u00b1 0.2\n45.8 \u00b1 0.2\n52.7 \u00b1 0.7\n13.0 \u00b1 0.1\n0.1\n10\n64.5 \u00b1 0.2\n17.9 \u00b1 0.2\n46.3 \u00b1 0.2\n54.7 \u00b1 0.7\n13.6 \u00b1 0.1\n0.3\n10\n64.9 \u00b1 0.2\n18.2 \u00b1 0.2\n46.5 \u00b1 0.2\n56.3 \u00b1 0.7\n14.1 \u00b1 0.1\n0.5\n10\n64.9 \u00b1 0.2\n18.1 \u00b1 0.2\n46.5 \u00b1 0.2\n56.7 \u00b1 0.7\n14.3 \u00b1 0.1\n0.1\n20\n64.5 \u00b1 0.2\n18.0 \u00b1 0.2\n46.3 \u00b1 0.2\n54.8 \u00b1 0.7\n13.6 \u00b1 0.1\n0.3\n20\n65.1 \u00b1 0.2\n18.3 \u00b1 0.2\n46.7 \u00b1 0.2\n56.6 \u00b1 0.7\n14.3 \u00b1 0.1\n0.5\n20\n65.1 \u00b1 0.2\n18.2 \u00b1 0.2\n46.5 \u00b1 0.2\n57.1 \u00b1 0.7\n14.6 \u00b1 0.1\n0.1\n30\n64.6 \u00b1 0.2\n18.0 \u00b1 0.2\n46.3 \u00b1 0.2\n55.0 \u00b1 0.7\n13.7 \u00b1 0.1\n0.3\n30\n65.2 \u00b1 0.2\n18.3 \u00b1 0.2\n46.7 \u00b1 0.2\n56.9 \u00b1 0.7\n14.3 \u00b1 0.1\n0.5\n30\n64.9 \u00b1 0.2\n18.1 \u00b1 0.2\n46.7 \u00b1 0.2\n58.0 \u00b1 0.7\n14.7 \u00b1 0.1\nTable 19. Comparison of different values for temperature of nucleus sampling on the MSRVTT validation set for k = 5.\nTEMPERATURE\nSAMPLES\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\n0.0\nN/A\n27.1 \u00b1 0.1\n4.9 \u00b1 0.0\n25.8 \u00b1 0.1\n36.7 \u00b1 0.4\n14.1 \u00b1 0.1\n0.1\n10\n24.8 \u00b1 0.1\n4.4 \u00b1 0.0\n25.8 \u00b1 0.1\n37.4 \u00b1 0.4\n14.7 \u00b1 0.1\n0.3\n10\n24.9 \u00b1 0.1\n4.2 \u00b1 0.0\n25.6 \u00b1 0.1\n38.2 \u00b1 0.4\n14.8 \u00b1 0.1\n0.5\n10\n24.7 \u00b1 0.1\n4.1 \u00b1 0.0\n25.3 \u00b1 0.1\n37.9 \u00b1 0.4\n14.6 \u00b1 0.1\n0.1\n20\n24.7 \u00b1 0.1\n4.3 \u00b1 0.0\n25.7 \u00b1 0.1\n37.3 \u00b1 0.4\n14.7 \u00b1 0.1\n0.3\n20\n24.8 \u00b1 0.1\n4.2 \u00b1 0.0\n25.6 \u00b1 0.1\n38.0 \u00b1 0.4\n14.7 \u00b1 0.1\n0.5\n20\n24.6 \u00b1 0.1\n4.0 \u00b1 0.0\n25.3 \u00b1 0.1\n38.3 \u00b1 0.4\n14.6 \u00b1 0.1\n0.1\n30\n24.7 \u00b1 0.1\n4.3 \u00b1 0.0\n25.8 \u00b1 0.1\n37.3 \u00b1 0.4\n14.7 \u00b1 0.1\n0.3\n30\n24.7 \u00b1 0.1\n4.2 \u00b1 0.0\n25.6 \u00b1 0.1\n38.1 \u00b1 0.4\n14.7 \u00b1 0.1\n0.5\n30\n24.5 \u00b1 0.1\n4.0 \u00b1 0.0\n25.3 \u00b1 0.1\n38.1 \u00b1 0.4\n14.6 \u00b1 0.1\nTable 20. Search over different decoding paradigms for captioning on the MS-COCO validation set. We report mean and standard error\nfor all settings. Sampling-based decoding strategies use a temperature of \u03c4 = 0.1.\nDECODING\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\nSAMPLING\n67.9 \u00b1 0.2\n21.0 \u00b1 0.3\n51.6 \u00b1 0.2\n80.7 \u00b1 0.8\n19.3 \u00b1 0.1\nTOPK\n67.9 \u00b1 0.2\n20.8 \u00b1 0.3\n51.5 \u00b1 0.2\n80.9 \u00b1 0.8\n19.4 \u00b1 0.1\nGREEDY\n77.4 \u00b1 0.2\n30.5 \u00b1 0.4\n57.7 \u00b1 0.2\n105.5 \u00b1 1.0\n20.8 \u00b1 0.1\nNUCLEUS, l = 1\n77.4 \u00b1 0.2\n30.4 \u00b1 0.4\n57.8 \u00b1 0.2\n105.5 \u00b1 1.0\n20.8 \u00b1 0.1\nNUCLEUS\n77.7 \u00b1 0.2\n30.5 \u00b1 0.4\n58.0 \u00b1 0.2\n107.3 \u00b1 1.0\n21.2 \u00b1 0.1\n25\nLinear Alignment of Vision-language Models for Image Captioning\nTable 21. Comparison of different orderings for exemplars in the prompt on the MS-COCO validation set. We report mean and standard\nerror for all settings.\nORDERING\nBLEU@1\nBLEU@4\nROUGE-L\nCIDER-D\nSPICE\nWORST-TO-BEST\n77.7 \u00b1 0.2\n30.5 \u00b1 0.4\n58.0 \u00b1 0.2\n107.3 \u00b1 1.0\n21.2 \u00b1 0.1\nBEST-TO-WORST\n77.4 \u00b1 0.2\n30.4 \u00b1 0.4\n57.7 \u00b1 0.2\n105.9 \u00b1 1.0\n21.0 \u00b1 0.1\n26\n"
  },
  {
    "title": "Towards Robust and Efficient Continual Language Learning",
    "link": "https://arxiv.org/pdf/2307.05741.pdf",
    "upvote": "3",
    "text": "Towards Robust and Efficient Continual Language Learning\nAdam Fisch1,\u2217 Amal Rannen-Triki2\nRazvan Pascanu2\nJ\u00f6rg Bornschein2\nAngeliki Lazaridou2\nElena Gribovskaya2\nMarc\u2019Aurelio Ranzato2\n1MIT CSAIL\n2Google DeepMind\nAbstract\nAs the application space of language models\ncontinues to evolve, a natural question to ask\nis how we can quickly adapt models to new\ntasks. We approach this classic question from\na continual learning perspective, in which we\naim to continue fine-tuning models trained\non past tasks on new tasks, with the goal of\n\u201ctransferring\u201d relevant knowledge. However,\nthis strategy also runs the risk of doing more\nharm than good, i.e., negative transfer. In this\npaper, we construct a new benchmark of task\nsequences that target different possible transfer\nscenarios one might face, such as a sequence\nof tasks with high potential of positive transfer,\nhigh potential for negative transfer, no expected\neffect, or a mixture of each. An ideal learner\nshould be able to maximally exploit infor-\nmation from all tasks that have any potential\nfor positive transfer, while also avoiding the\nnegative effects of any distracting tasks that\nmay confuse it. We then propose a simple,\nyet effective, learner that satisfies many of our\ndesiderata simply by leveraging a selective\nstrategy for initializing new models from past\ntask checkpoints. Still, limitations remain, and\nwe hope this benchmark can help the commu-\nnity to further build and analyze such learners.\n1\nIntroduction\nRecent advances in large, pre-trained language\nmodels (LMs) have re-defined the ways practition-\ners approach and solve tasks in language under-\nstanding and generation (Devlin et al., 2019; Raffel\net al., 2020; Brown et al., 2020; Rae et al., 2021;\nHoffmann et al., 2022; Chowdhery et al., 2022,\netc). Autoregressive language modeling removes\nthe need for bespoke neural architectures, and pro-\nvides a flexible framework for expressing diverse\nand complex tasks with unified input and output\nformats. At scale, LMs have achieved state-of-the-\nart performance across nearly every widely-used\n\u2217Work done while an intern at Google DeepMind.\nnatural language processing (NLP) benchmark, and\nhave had widespread impact in popular applications\nsuch as ChatGPT (Schulman et al., 2023).\nThough much interest has focused on the few-\nshot and zero-shot reasoning abilities of very large\nLMs, an effective approach to solving targeted NLP\ntasks is still to take the parameters of a pre-trained\nmodel, and fine-tune them on data from the new\ntask (Raffel et al., 2020; Gao et al., 2021; Wei\net al., 2022). Here there is a similar trend: perfor-\nmance generally improves as the LM grows. Un-\nfortunately this also results in significant computa-\ntional costs during fine-tuning, even if the number\nof updates required is ultimately less than would\nbe required if training the model from scratch. Fur-\nthermore, fine-tuning is typically performed inde-\npendently for each new task, and ignores any other\ntasks the LM might have previously been applied to.\nThis not only leads to an accrual of computational\ncost over all tasks, but also fails to share acquired\nknowledge across tasks. In this work, we revisit\nfine-tuning efficiency from a continual learning per-\nspective, motivated by the following question:\nQuestion 1. Suppose that we have already solved\na set of n previous tasks, {t1, . . . , tn}. Can we\nleverage any of the information gained from these\ntasks to solve the next task tn+1 more efficiently?\nSpecifically, we study the setting where each\nof the previous n tasks is associated with a sub-\nstantial number of training examples (e.g., several\nthousand). This setting is common, but not well\naddressed by few-shot prompting. Our conjecture,\nwhich has also previously found empirical support\nin various related NLP settings (Phang et al., 2019;\nPoth et al., 2021; Choshen et al., 2022, inter alia),\nis that the standard pre-trained model\u2014which\nwide-spread wisdom uses as the default initializa-\ntion for any fine-tuning task\u2014might not in-fact be\nthe best checkpoint to use. Rather, models derived\nfrom one (or a combination) of the previous tasks\narXiv:2307.05741v1  [cs.CL]  11 Jul 2023\nFigure 1: An illustration of our continual learning framework. When training the (n + 1)th model we choose\nbetween initializing from the default pre-trained language model and a previously fine-tuned model. This is repeated\nfor each new task, and models within the zoo may build off each other to create a chain of fine-tuned models. Our\nmotivation is to make fine-tuning more efficient, while also being robust to the composition of previous tasks.\nmight work even better as a starting point. This as-\nsumes that \u201cknowledge transfer\u201d, or a form thereof,\nis accomplished via parameter initialization.\nWe measure performance by how quickly our\nlearning algorithm can produce good models for\nnew tasks. Specifically, how much computational\nbudget do we need to produce a model with some\ndesired performance level? Or, alternatively, for\na given computational budget, what is the best\nperformance that we can achieve? Put in the con-\ntext of past work on continual learning (Wo\u0142czyk\net al., 2021; Veniat et al., 2021; Bornschein et al.,\n2022), we are focused on forward transfer, which\nwe define as having a faster \u201crate of learning\u201d on\na new task\u2014relative to our baseline strategy of in-\ndependent fine-tuning from the original pre-trained\nLM parameters. Naturally, how well one can hope\nto do depends not only on the algorithm that is\nused, but also on the relationships between the new\ntask and previous ones. We conduct a large-scale\nanalysis of pairwise interactions across 55 popular\nand publicly available (English) language tasks\nusing a T5 LM (Raffel et al., 2020). Here we first\nfine-tune a T5 LM on task A, and then proceed\nto fine-tune it on task B. This indeed results in\na fairly tumultuous transfer landscape: in some\ncases pre-training on A first can result in faster\nadaptation to task B, but in other cases it can\nbe quite detrimental. How can we expect which\nsituation we may encounter, especially when faced\nwith not just one, but many previous tasks and task\ncombinations to transfer from?\nWe argue that practical, efficient continual\nlearning demands algorithms that are robust to\nthe inevitable variations in the composition of\nthe previous n tasks. To this end, guided by our\npairwise matrix of task interactions, we construct a\nchallenging benchmark of multiple task sequences\n(t1, t2, . . .) that target different possible scenarios\none might face, such as a sequence of tasks with\nhigh potential positive transfer, high potential for\nnegative transfer, no expected effect, or a mixture\nof each. An ideal continual learner should be able\nto exploit information from all tasks that have any\npotential for positive transfer, while also avoiding\nthe harmful effects of any \u201cdistractor\u201d tasks that\nmay confuse it (and result in negative transfer).\nAs a first step, we propose a simple method that\nmanages to satisfy many of our desiderata. Con-\ncretely, we learn a checkpoint selection model that,\ngiven some representation of the current task tn+1\nand for all previously seen tasks (t1, . . . , tn), pre-\ndicts which previously saved checkpoint is the best\ncheckpoint to initialize from\u2014including the default\noption \u201ct0\u201d, which is simply the pre-trained model\nthat a standard fine-tuning approach would start\nfrom. We demonstrate that training a lightweight\ngradient boosted decision tree (Friedman, 2001) on\ntop of (fast and easy to derive) features of each task\nover a small collection of held-out task pairs with\ndifferent positive, negative, or neutral pairwise\ntransfer relationships can result in good selection\nperformance on new tasks: particularly when there\nexist harmful past tasks that are best to be ignored.\nIn short, the core idea and contribution of this\nwork can be summarized quite simply:\n1. We motivate and explore continual learning for\nefficient LM fine-tuning and forward transfer;\n2. To support this direction, we present a large-\nscale analysis of pairwise task transfer interac-\ntions, and a new benchmark of task sequences\nthat capture diverse potential transfer profiles;\n3. Finally, we give a simple but effective method\nfor checkpoint selection and model initialization\nthat helps enable more robust forward transfer.\n2\nRelated work\nForward transfer.\nThis work builds on a large\nbody of recent work that seeks to improve the\nefficiency of training modern language models\nthrough forward transfer (via parameter initializa-\ntion). In particular, leveraging auxiliary task data\nto improve target task performance has been a very\nactive area of research years over the past few years\n(Luong et al., 2016; Bingel and S\u00f8gaard, 2017;\nPhang et al., 2019; Wang et al., 2019; Gururangan\net al., 2020; Pruksachatkun et al., 2020; Vu\net al., 2020; Chang and Lu, 2021; Aribandi et al.,\n2022).\nOur work falls under the category of\nintermediate fine-tuning, where a model is first\ntrained on some auxiliary task A before being\ntransferred to a target task B. This paradigm has\nbeen well-analyzed in the pair-wise setting (i.e.,\nA \u2192 B only), and multiple past studies have given\nempirical guidelines on how to select optimal\ntransfer pairs (Ruder and Plank, 2017; Deshpande\net al., 2021; Poth et al., 2021; Huang et al., 2022;\nChoshen et al., 2022; You et al., 2021, 2022).\nHere, we extend intermediate task training in a\npair-wise fashion to training over full sequences\nof intermediate (and continually learned) tasks.\nContinual learning.\nThe key focus of this work\nis on continual learning for efficient language\nlearning. Over the past decade, continual learning\nresearch has received significant interest within\nthe wider machine learning community; see, e.g.,\nParisi et al. (2019) for a review. Methodology-\nwise, existing work on continual learning can\nbe approximately categorized into (a) replay-\nbased (de Masson d'Autume et al., 2019; Scialom\net al., 2022), (b) regularization-based (Kirkpatrick\net al., 2017; Chaudhry et al., 2019; Qin et al., 2022;\nKe et al., 2023), or (c) architecture-based (Carlson\net al., 2010; Veniat et al., 2021; Douillard et al.,\n2022; Razdaibiedina et al., 2023; Qin et al., 2023)\napproaches. Many of these methods are motivated\nboth by parameter-efficient forward transfer, as\nwell as resistance to catastrophic forgetting. In\ncontrast, similar to Bornschein et al. (2022), we are\nonly interested in training efficiency on the new\ntask\u2014without worrying about how performance\nmight suffer on previous tasks\u2014and focus only\non different model initialization strategies for sim-\nplicity.1 Our benchmark of transfer sequences also\nadds to a growing collection of continual learning\ndatasets and analysis techniques for NLP (Lazari-\ndou et al., 2021; Livska et al., 2022; Jang et al.,\n2022; Wu et al., 2022), with an attention towards\nsequences of a particular challenging structure that\nstress test for robustness to negative transfer.\nEfficient training for NLP.\nFinally, our work\nis also more broadly related to efficient training\nin language models (Mattson et al., 2020; Geiping\nand Goldstein, 2022; Menghani, 2023), which also\nincludes efficiency in terms of parameter reuse and\nstored model size (Houlsby et al., 2019; Li and\nLiang, 2021; He et al., 2022; Hu et al., 2022; Lei\net al., 2023). While we do not consider these forms\nof efficiency in this work, they can be complemen-\ntary to the form of training efficiency that we do\nconcentrate on. Some of the metrics and analysis\nwe propose may also be of independent interest.\n3\nProblem formulation\nLet LM\u03b8 : X \u2192 Y be our parametric language\nmodel, which generates natural language responses\ny \u2208 Y given a prompt x \u2208 X. All of our experi-\nments use the pre-trained T5 base model of Raffel\net al. (2020), specifically the version adapted for\nlanguage modeling by Lester et al. (2021).\n3.1\nFine-tuning efficiency\nDuring fine-tuning, the model parameters vary as a\nfunction of the number of update steps s \u2208 N that\nhave been taken, denoted as \u03b8(s). We quantify the\ntime-dependent performance of our updating lan-\nguage model, LM\u03b8(s), by its best (i.e., minimum)\nloss achieved within a budget of B update steps:\nPerf(B) :=\n(1)\nmin\nn\nEX,Y\n\u0002\n\u2113(LM\u03b8(s)(X), Y )\n\u0003\n|\n{z\n}\naverage loss after step s\n: s \u2264 B\no\n,\n1This is motivated by our simplifying assumptions that we\n(a) know what task we are trying to solve at each point in time,\nand (b) can checkpoint and load past models when necessary.\nAlgorithm 1 Sequential fine-tuning\n1: \u03b80 \u2190 Pre-trained model\n2: for ti \u2208 t1, t2, . . . do\n3:\n# Initialize starting point from a previous model.\n4:\n\u03b8i(0) \u2190 SELECT(\u03b80, . . . , \u03b8i\u22121)\n5:\n# Update current model using task data (Xi, Yi).\n6:\nfor s \u2208 1, 2, . . . , B do\n7:\n\u03b8i(s) \u2190 UPDATE(LM\u03b8i(s\u22121), Xi, Yi)\n8:\n# Keep the best model from the past B steps.\n9:\n\u03b8i \u2190 argmin\ns \u2264 B\nEXi,Yi[\u2113(LM\u03b8i(s)(Xi), Yi)]\nwhere \u2113: Y \u00d7 Y \u2192 R is an arbitrary loss metric.\nTo reduce complexity and confounders between\nimplementations, we choose to use the same model\narchitecture, batch size, and learning rates in all of\nour experiments. The consequences of this are that\n(1) the number of update steps is directly propor-\ntional to the total training cost of the model, and (2)\nachieving better Perf(B) simply reduces to finding\na better initialization for our model, i.e., a choice\nof \u03b8(0) that gives rise to efficient trajectories \u03b8(s).2\nFinally, as an aggregate measure of Perf(B)\nacross budgets B, we evaluate the area under the\nperformance curve as a function of log updates, up\nto a maximum number of updates Bmax, i.e.,\nPerfAUC(Bmax) :=\nZ log Bmax\n0\nPerf(eb)db. (2)\nPerfAUC(Bmax) will be our primary metric for\ncomparing methods, where we set Bmax = 10k,\nwhich is empirically the point for which the major-\nity of models for our tasks have (nearly) converged.\nMore specifically, we will be interested in mea-\nsuring the relative efficiency of continual learning\nmethods compared to the baseline of independent\nfine-tuning (where we always start from the same\ngeneral pre-trained model for each task). Inspired\nby the relative forward transfer metrics of Wo\u0142czyk\net al. (2021), we compute this relative score as\nPerfAUC(Bmax)ind \u2212 PerfAUC(Bmax)m\nPerfAUC(Bmax)ind \u2212 L\n, (3)\nwhere (\u00b7)m, (\u00b7)ind are the scores of the method\nand the baseline of independent fine-tuning,\nrespectively, and L is the metric lower bound for\nPerfAUC(Bmax) (e.g., 0% \u00d7 log Bmax for error\nrate). Intuitively, this score measures the relative\n2Note that an interesting direction for future work is to\nexplore how the findings presented here generalize across dif-\nferent classes of models/learning algorithms (and if not, why).\nAlgorithm 2 \u201cNa\u00efve\u201d sequential fine-tuning\n1: function SELECT(\u03b80, . . . , \u03b8i\u22121)\n2:\n# Return the most recently trained model.\n3:\nreturn \u03b8i\u22121\nimprovement in terms of how much the compared\nmethod reduces the performance gap to the oracle\n(i.e., perfect predictions starting from step 0).\n3.2\nSequential fine-tuning\nAs a starting point for the remainder of this paper,\nwe now describe a very simple continual learning\nprocedure for sequential fine-tuning on a stream of\ntasks (t1, t2, . . .), see also Algorithm 1. Beginning\nfrom an initial pre-trained language model LM\u03b80,\nwe sequentially adapt models LM\u03b8i one after the\nother by using the model learned on some previ-\nous task tj<i to initialize the model used on task ti.\nNote that here we write \u03b8i to index the task param-\neters, and will use \u03b8i(s) to denote the task parame-\nters as a function of the number of updates. As de-\nscribed earlier, we use the same model architecture,\nbatch size, and learning rate for each task. The only\nsetting that changes is the initialization. The \u201cna\u00efve\u201d\nimplementation of sequential fine-tuning is to sim-\nply select the most recent checkpoint, \u03b8i\u22121, see\nAlgorithm 2. Of course, this procedure is not neces-\nsarily optimal, since the model parameters learned\nfor a task A may not be a good initialization for an-\nother task B. In the next section we present an anal-\nysis of when A does have potential to transfer well\nas a good initialization for B, and when it does not.\n4\nAnalyzing task transfer potential\nTo help guide our understanding of how well pa-\nrameters learned from training on language task A\nperform when used as a starting point for training\non a new language task B, we conduct a large-scale\nanalysis over various, diverse pairs of language\ntasks (A, B). Note that this is the minimal setting\nfor which sequential fine-tuning can be applied.\n4.1\nDataset collection\nThe tasks that we analyze are shown in Figure 2,\nand mainly follow those used by FLAN (Wei et al.,\n2022), but without translation (we do not use mul-\ntilingual models here). We use the same (loosely\ndefined) \u201ctask family\u201d groupings as Wei et al.\n(2022) to help guide our analysis (below), but ulti-\nmately are interested in transfer between individual\ntasks. To identify \u201cinteresting\u201d pairs (A, B) that\nFigure 2: The collection of tasks used to create the sequential transfer benchmark used in this paper. Tasks are\ngrouped into approximate \u201cfamilies\u201d, and families are further separated into training (top) and testing (bottom)\nsplits. Highlighted training tasks are used for validation (i.e., task C when measuring transfer from A \u2192 B \u2192 C).\nhave either significantly negative or positive effects\non each other, we use the following search strategy:\n1. We evaluate all 16 \u00d7 16 task family pairs, where\nfor a family pair (Fi, Fj) we first train a model\non a mixture of all tasks t\u2032\ni \u2208 Fi, and then use\nthat model as the starting point for training the\nsecond model on a mixture of all tasks t\u2032\nj \u2208 Fj.\nEach model is trained on a balanced mixture of\ntask data, and evaluated according to the average\nperformance across tasks within each family.\n2. For a pair (Fi, Fj) the average performance af-\nter sequentially fine-tuning on Fj \u2192 Fj can ei-\nther be better, worse, or approximately the same\nrelative to training independently on Fj. We use\nthis signal as evidence that there may exist indi-\nvidual tasks t\u2032\ni, t\u2032\nj \u2208 Fi\u00d7Fj with a similar trend.\n3. For each family Fi, we identify the top-K fam-\nilies Fj with the best average transfer to Fi, as\nwell as the worst-K families Fj with the worst\naverage transfer to Fi. K is set to 3. We then\nevaluate all individual task pairs in Fi\u00d7Fj\u00d7Fk.\nIn total, we evaluate 1757 unique task pairs.\nFigure 3 plots the distribution of transfer results in\nterms of relative PerfAUC. Consistent with obser-\nvations in prior work (Pruksachatkun et al., 2020;\nPoth et al., 2021), we can see that while on many\ntasks there is no marked effect due to sequential\nfine-tuning, there do exist a significant tails of both\npositive and negative transfer instances.3\n4.2\nTypes of transfer profiles\nFigure 4 gives a number of qualitative exam-\nples that exhibit some of the different types of\n3Note, however, that this distribution is also artificially\nbiased towards the tails, due to our search and evaluation\nstrategy. Nevertheless, it can still be inferred that substantial\nabsolute numbers of both positive and negative instances exist.\nFigure 3: A density plot of the empirical distribution of\nthe relative PerfAUC across the 1757 task pairs A \u2192\nB that we (selectively) evaluate. All models are trained\nwith \u201cna\u00efve\u201d sequential fine-tuning, where we use the\ncheckpoint of task A as a starting point for task B.\ntransfer profiles that arise. Some models exhibit\nstrong transfer from the beginning: the 0-shot\nperformance is good, and continues to improve.\nGood 0-shot performance, however, is not always\na reliable indicator of future success:\nsome\nmodels start out with better performance, but\nimprove more slowly. Other pairs yield no tangible\ndifference.\nThere is also significant variation\nacross tasks, with some tasks acting as fairly\n\u201cuniversal donors\u201d with positive transfer to most\nother tasks, while others mostly result in negative,\nor at best minimal positive, transfer. For example,\nin our experiments, 73% of models trained first on\nthe STS-B dataset (Cer et al., 2017) had > +5%\nrelative PerfAUC across evaluated target tasks.\nOn the other hand, 70% of models trained first on\nthe Math dataset (Saxton et al., 2019) had < \u22125%\nrelative PerfAUC across evaluated target tasks.\nInterpreting and understanding why these\nFigure 4: Example positive and negative pairwise transfer profiles A \u2192 B, in which the lowest loss per update bud-\nget on task B is plotted. Blue is for the baseline of independent fine-tuning (pre-trained model \u2192 B), while orange\nis for \u201cna\u00efve\u201d sequential fine-tuning (pre-trained model \u2192 A \u2192 B). Relative PerfAUC is included in each legend.\nA \u2192 C\nB \u2192 C\nDesired A \u2192 B \u2192 C\nPositive\nPositive\n\u2265 max(A \u2192 C, B \u2192 C)\nPositive\nNegative\n\u2248 A \u2192 C\nPositive\nNeutral\n\u2248 A \u2192 C\nNegative\nPositive\n\u2248 B \u2192 C\nNegative\nNegative\n\u2248 C\nNegative\nNeutral\n\u2248 C\nNeutral\nPositive\n\u2248 B \u2192 C\nNeutral\nNegative\n\u2248 C\nTable 1: Types of task triplets in our benchmark. The\nfinal column indicates the desired behavior on task C\nwhen using an \u201cideal\u201d continual learning algorithm.\ntransfer curves form the way they do is tricky\u2014and\nsomething we leave as an open problem for future\nresearch.\nNevertheless, the existence of these\nempirical phenomena allows us to construct chal-\nlenging sequences of tasks over which to perform\ncontinual learning. As previously discussed, an\nideal learner should be able to exploit information\nfrom all tasks that have any potential for positive\ntransfer (demonstrated by having a positive pair-\nwise transfer result), while also avoiding the neg-\native effects of any potentially harmful tasks that\nmay confuse it (demonstrated by having a negative\npairwise transfer result). An ideal learner should\nbe agnostic to the mechanism that is responsible\nfor the positive or negative transfer, which in many\ncommon situations (such as in many of the tasks\npresented here) may not be that well understood.\n4.3\nConstructing a diagnostic benchmark\nWe leverage the pairwise transfer results to con-\nstruct a series of diverse, diagnostic task sequences.\nThe format of these sequences is outlined in Table 1.\nWe split the pairs of tasks into positive, negative,\nand neutral subsets based on the magnitude of\ntheir relative PerfAUC to the independent fine-\ntuning baseline (see Eq. 3). For positive/negative\ntasks, we attempt to account for variance in\ntraining (where the randomness is over the batch\nselection and ordering during SGD) by requiring\nthat the mininum/maximum relative PerfAUC\nresults across all random trials are above/below\n+5%/\u22125%, respectively (though occasional false\npositives exist across different runs).\nWe then\nconstruct 8 different types of triplets (A, B, C),\nwhere each of the preceding tasks A and B are\nmostly positive, negative, or neutral pairwise trans-\nfer sources for the target task C (i.e., A \u2192 C and\nB \u2192 C, respectively). Note that we exclude the\nneutral/neutral case. For each configurations, we in-\nclude multiple sets of triplets with different source\nand target tasks, and measure the median perfor-\nmance across task instances in all experiments.\nSpecifically, on the test split of the benchmark,\nfor each type of triplet (e.g., positive/positive)\nwe include 4 distinct target tasks C, each with 4\ndistinct preceding task pairs (A, B), for a total\nof 16 triplets per setting (and 128 triplets in total).\nAdditional details are provided in Appendix A.\n5\nLearning a checkpoint selector\nWe now propose a straightforward, but effective,\nalgorithm for robust forward transfer. Motivated by\nour analysis in Section 4, we consider a simplified\nversion of Question 1 that we posed in Section 1:\nQuestion 2. Suppose that the set of previously\nsolved tasks {t1, . . . , tn} contains a distinct set\nof tasks with trained models \u03b8i that act as good ini-\ntializations for tn+1, i.e. P \u2286 {t1, . . . , tn}. Given\nfeatures \u03d5(ti, tn+1) \u2208 Rd, can we learn a discrimi-\nnator D: Rd \u2192 {0, 1} to identify \u201cpositive\u201d task\ncandidates ti \u2208 P to leverage for learning tn+1?\nConcretely, when training a new model for tn+1,\nwe seek to allow ourselves to select a previously\nfine-tuned model on some task ti \u2208 {t1, . . . , tn}\nto initialize from\u2014if we think that it will lead to\npositive transfer. If multiple such tasks exist, then\nwe select the most confident one, using confidence\nscores from some model C : Rd \u2192 [0, 1], which is\ntypically the same underlying model as D, but with-\nout a decision threshold. If we are not confident\nthat any such task model exists, then we initialize\nfrom the default pre-trained language model. This\nprocess, which we call selective sequential fine-\ntuning, is illustrated in Algorithm 3, and is similar\nin spirit to prior work on checkpoint selection for\ntransfer learning (see \u00a72), with the caveat that we\nonly select candidates that pass a decision threshold\n(see also \u00a77 for a discussion on the potential impor-\ntance of properly calibrating this threshold). This\nprocess is repeated for each new task, e.g., for a\nsequence A \u2192 B \u2192 C, task A is initialized from\nthe pre-trained model, task B is either initialized\nfrom the pre-trained model or the checkpoint for A,\nand task C is either initialized from the pre-trained\nmodel or either of the checkpoints for A or B. In\ngeneral, there are 2n possible paths (in terms of se-\nquential initializations) to take from t0 to task tn+1.\nWe choose to instantiate D as a simple gradient\nboosted decision tree (GBDT) (Friedman, 2001)\noperating on several light-weight \u201cmeta\u201d features,\n\u03d5(ti, tj) of an input task pair. C is the pre-binarized\ndecision function of the GBDT. The GBDT is\ntrained over positive and negative pairs from the\ntraining split of our benchmark.4 The features \u03d5\nare fairly conventional (e.g., similar motivation can\nbe found in the related approaches of Bingel and\n4While we ignore this aspect in this work, note that this in-\ntroduces a distributional shift at test time, since candidate mod-\nels are themselves products of multiple iterations of this selec-\ntion algorithm, rather than only pairwise transfer instances.\nAlgorithm 3 \u201cSelective\u201d sequential fine-tuning\n1: function SELECT(\u03b80, . . . , \u03b8i\u22121)\n2:\n# Estimate selection of positive transfer candidates\n3:\n# from the corresponding tasks (t1, . . . , ti\u22121.ti),\n4:\n# D is a trained \u201cpositive transfer\u201d discriminator.\n5:\nbP \u2190\n\b\ntj : D(tj, ti) = 1, j < i\n\t\n6:\nif bP \u0338= \u2205 then\n7:\n# Pick the most confident candidate if any\n8:\n# exists, where C is a confidence measure.\n9:\nj\u2217 \u2190 argmaxj\n\b\nC(tj, ti) : tj \u2208 bP\n\t\n10:\nelse\n11:\n# Otherwise, default to the pre-trained model.\n12:\nj\u2217 \u2190 0\n13:\nreturn \u03b8j\u2217\nS\u00f8gaard (2017); Poth et al. (2021)). They include\nmetadata (e.g., if any of the previous tasks are in\nthe same family as tj, the zero-shot and few-shot\nperformance of model ti on tj, and a number\nof gradient-based similarity metrics comparing\nupdates to ti and tj relative to a t0 starting point.\nSee Appendix B for more details. We binarize D\nby thresholding the GBDT confidence at 0.5 (i.e.,\nwe only consider a checkpoint to be a candidate for\nselection if it is judged by our model to be more\nlikely than not to be a positive transfer pair).\n6\nResults\nWe compare the behavior of na\u00efve sequential\nfine-tuning, our selective sequential fine-tuning\nprocedure, and an oracle checkpoint selection\nalgorithm across the task sequences in our bench-\nmark. Our results are given in Table 2. See also\nAppendix C. The oracle picks the best sequential\nfine-tuning path from t0 to tn+1 in hindsight, and is\nused as an upper-bound to our selective model per-\nformance (as n = 2 in our experiments, this results\nin 4 total possible paths). We report the median rel-\native PerfAUC result across all 16 triplets for each\nsequence type (e.g., for the triplet A \u2192 B \u2192 C,\nwhere A \u2192 B results in positive pairwise transfer,\nwhile B \u2192 C results in negative pairwise transfer).\nForward transfer.\nRows in Table 2 with green\nentries denote sequence types with potential for\npositive forward transfer.\nWhen both tasks A\nand B are positive intermediate tasks for task C,\ncontinuing to fine-tune A \u2192 B \u2192 C generally\nalso result in positive transfer\u2014interestingly, often\nto a larger degree than either of only A \u2192 C\nor B \u2192 C. When a positive intermediate task\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n34.5\n31.8\n42.7\n42.4\n43.6\n13.9\n-17.0\n-2.35\n11.8\n13.9\n15.1\n0.244\n16.9\n15.1\n19.9\n-17.0\n13.7\n12.0\n11.8\n15.5\n-24.5\n-19.5\n-24.7\n0.00\n0.00\n-17.0\n-1.65\n-8.34\n0.00\n0.00\n-0.703\n25.2\n19.8\n15.0\n26.3\n-1.88\n-16.9\n-12.6\n0.00\n0.00\nTable 2: PerfAUC results on our benchmark sequences. Each row is the median of all 16 instances of that\nconfiguration (e.g., positive A \u2192 positive B \u2192 C. Green denotes intended \u201cpositive\u201d pairwise transfer, red\ndenotes \u201cnegative\u201d pairwise transfer, while grey denotes \u201cneutral\u201d transfer (i.e., no substantial effect). Oracle is\nthe best achievable result using any (possible) sequence of checkpoints from the initial pre-trained model to task\nC. A score of 0 means performing as well as a model that fine-tunes from the original pre-trained model, while\npositive/negative scores are improvements/degradations relative to that, which is the default used today.\nis paired with a negative intermediate task (red\nentries), the performance of na\u00efve sequential\nfine-tuning is sensitive to their ordering (and is\nbetter when the most recently trained task is a\npositive transfer pair). Our selective procedure,\nhowever, manages to leverage positive transfer\nwhere possible regardless of order\u2014though it can\nsignificantly lag behind the oracle in certain cases.\nNegative transfer.\nRows in Table 2 with red\nentries denote sequence types with potential for\nnegative transfer. Fortunately, unlike sequential\ntransfer with positive transfer options, harmful\neffects from two negative, or negative and neutral,\nintermediate tasks A and B rarely compound (in\nfact, the negative effect can sometimes be atten-\nuated). In the cases where there are no positive\nintermediate tasks to transfer from, our selective\nalgorithm is successful in choosing the pre-trained\nmodel as a starting checkpoint (resulting in 0, but\nat least not negative, relative PerfAUC).\n7\nLimitations and challenges\nWhile our work provides a starting point for testing\nrobust and efficient continual learning, several\nlimitations remain. Most significantly, our focus is\nrestricted to T5 base models with simple optimiza-\ntion routines, and the only method of transfer that\nwe test and explore is via parameter initialization,\nwithout considering space efficiency (i.e., in\nreducing the number of saved parameters across\nall tasks). Our selective checkpoint initialization\nstrategy is therefore advantaged with respect to this\nparticular setting. Additionally, our oracle is only\nevaluated for this strategy\u2014other methods that use\ndifferent knowledge transfer paradigms may do\neven better (Ermis et al., 2022; Qin and Joty, 2022;\nRazdaibiedina et al., 2023). We note that noise is\nalso introduced though stochastic effects in SGD\n(e.g, learning rates, batch sizes), which introduces\nsome confounding effects, especially when inte-\ngrating over log updates (which biases PerfAUC\ntowards early performance). This is more signif-\nicant for some tasks than others. Finally, another\nchallenge is that as the number of considered\ntasks grows, our selective classifier may become\nmore prone to identifying a large number of false\npositives. Without using calibration techniques\nthat account for multiple testing (e.g., Fisch et al.,\n2021), the selective classifier may choose poor\ncheckpoints with increasingly high probability.\n8\nConclusion\nThis paper develops a collection of task sequences\nwith diverse transfer scenarios to test for efficient\nand robust continual learning on language tasks.\nOur benchmark targets different possible scenarios\none might face: such as a sequence of tasks with\nhigh potential for positive transfer, negative trans-\nfer, no effect, or a mixture of each. As a first step,\nwe proposed a selective algorithm for choosing past\ncheckpoints to initialize from when considering\neach new task tn+1. Limitations remain, and we\nhope this benchmark may help analyze and identify\nstrong continual language learning algorithms.\nReferences\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,\nJai Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. 2022. Ext5: Towards extreme multi-task\nscaling for transfer learning. In International Confer-\nence on Learning Representations.\nJoachim Bingel and Anders S\u00f8gaard. 2017. Identify-\ning beneficial task relations for multi-task learning\nin deep neural networks. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 164\u2013169, Valencia, Spain. Association\nfor Computational Linguistics.\nJorg Bornschein, Alexandre Galashov, Ross Hemsley,\nAmal Rannen-Triki, Yutian Chen, Arslan Chaudhry,\nXu Owen He, Arthur Douillard, Massimo Caccia,\nQixuang Feng, Jiajun Shen, Sylvestre-Alvise Rebuffi,\nKitty Stacpoole, Diego de las Casas, Will Hawkins,\nAngeliki Lazaridou, Yee Whye Teh, Andrei A. Rusu,\nRazvan Pascanu, and Marc\u2019Aurelio Ranzato. 2022.\nNevis\u201922: A stream of 100 tasks sampled from 30\nyears of computer vision research.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin Neural Information Processing Systems (NeurIPS).\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr\nSettles, Estevam R. Hruschka, and Tom M. Mitchell.\n2010. Toward an architecture for never-ending lan-\nguage learning. In Proceedings of the Twenty-Fourth\nAAAI Conference on Artificial Intelligence.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation.\narXiv preprint\narXiv:1708.00055.\nTing-Yun Chang and Chi-Jen Lu. 2021. Rethinking\nwhy intermediate-task fine-tuning works. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 706\u2013713, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nArslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus\nRohrbach, and Mohamed Elhoseiny. 2019. Efficient\nlifelong learning with a-GEM. In International Con-\nference on Learning Representations.\nLeshem Choshen, Elad Venezian, Shachar Don-Yehia,\nNoam Slonim, and Yoav Katz. 2022. Where to start?\nanalyzing the potential value of intermediate models.\narXiv preprint arXiv:2211.00107.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nCyprien de Masson d'Autume, Sebastian Ruder, Ling-\npeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nAditya Deshpande,\nAlessandro Achille,\nAvinash\nRavichandran, Hao Li, Luca Zancato, Charless\nFowlkes, Rahul Bhotika, Stefano Soatto, and Pietro\nPerona. 2021. A linearized framework and a new\nbenchmark for model selection for fine-tuning. arXiv\npreprint arXiv:2102.00084.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nArthur Douillard, Alexandre Ram\u00e9, Guillaume Coua-\niron, and Matthieu Cord. 2022. Dytox: Transformers\nfor continual learning with dynamic token expansion.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nBeyza Ermis, Giovanni Zappella, Martin Wistuba,\nAditya Rawal, and Cedric Archambeau. 2022. Mem-\nory efficient continual learning with transformers. In\nAdvances in Neural Information Processing Systems.\nAdam Fisch, Tal Schuster, Tommi S. Jaakkola, and\nRegina Barzilay. 2021. Efficient conformal predic-\ntion via cascaded inference with expanded admission.\nIn International Conference on Learning Representa-\ntions.\nJerome H. Friedman. 2001. Greedy function approxi-\nmation: A gradient boosting machine. The Annals of\nStatistics, 29(5):1189 \u2013 1232.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816\u20133830, Online. Association for Computa-\ntional Linguistics.\nJonas Geiping and Tom Goldstein. 2022. Cramming:\nTraining a language model on a single gpu in one day.\narXiv preprint arXiv:2212.14034.\nSuchin\nGururangan,\nAna\nMarasovi\u00b4c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don\u2019t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342\u20138360, Online. Association for Computational\nLinguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatherine Millican, George van den Driessche, Bog-\ndan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nIn Advances in Neural Information Processing Sys-\ntems (NeurIPS).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nLong-Kai Huang, Junzhou Huang, Yu Rong, Qiang\nYang, and Ying Wei. 2022. Frustratingly easy trans-\nferability estimation. In International Conference on\nMachine Learning, pages 9201\u20139225.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022. Towards continual\nknowledge learning of language models. In Interna-\ntional Conference on Learning Representations.\nZixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi,\nGyuhak Kim, and Bing Liu. 2023. Continual learn-\ning of language models. In The Eleventh Interna-\ntional Conference on Learning Representations.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521\u20133526.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d\u2019Autume,\nTomas Kocisky, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom.\n2021. Mind the gap: Assessing temporal general-\nization in neural language models. arXive preprint\narXiv:2102.01951.\nTao Lei, Junwen Bai, Siddhartha Brahma, Joshua\nAinslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y.\nZhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-\nWei Chang. 2023. Conditional adapters: Parameter-\nefficient transfer learning with fast inference. arXiv\npreprint arXiv:2304.04947.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045\u20133059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582\u2013\n4597, Online. Association for Computational Lin-\nguistics.\nAdam Livska, Tom\u2019avs Kovcisk\u2019y, Elena Gribovskaya,\nTayfun Terzi, Eren Sezener, Devang Agrawal, Cy-\nprien de Masson d\u2019Autume, Tim Scholtes, Manzil\nZaheer, Susannah Young, Ellen Gilsenan-McMahon,\nSophia Austin, Phil Blunsom, and Angeliki Lazari-\ndou. 2022. Streamingqa: A benchmark for adapta-\ntion to new knowledge over time in question answer-\ning models. In International Conference on Machine\nLearning (ICML).\nMinh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2016. Multi-task se-\nquence to sequence learning. In 4th International\nConference on Learning Representations, ICLR.\nPeter Mattson, Christine Cheng, Cody Coleman, Greg\nDiamos, Paulius Micikevicius, David Patterson, Han-\nlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf,\nDavid Brooks, Dehao Chen, Debojyoti Dutta, Udit\nGupta, Kim Hazelwood, Andrew Hock, Xinyuan\nHuang, Atsushi Ike, Bill Jia, Daniel Kang, David\nKanter, Naveen Kumar, Jeffery Liao, Guokai Ma,\nDeepak Narayanan, Tayo Oguntebi, Gennady Pekhi-\nmenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor\nRobie, Tom St. John, Tsuguchika Tabaru, Carole-\nJean Wu, Lingjie Xu, Masafumi Yamazaki, Cliff\nYoung, and Matei Zaharia. 2020. Mlperf training\nbenchmark. arXiv preprint arXiv:1910.01500.\nGaurav Menghani. 2023. Efficient deep learning: A sur-\nvey on making deep learning models smaller, faster,\nand better. ACM Comput. Surv., 55(12).\nGerman I. Parisi, Ronald Kemker, Jose L. Part, Christo-\npher Kanan, and Stefan Wermter. 2019. Continual\nlifelong learning with neural networks: A review.\nNeural Networks, 113:54\u201371.\nJason Phang, Thibault F\u00e9vry, and Samuel R. Bowman.\n2019. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nClifton Poth, Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, and Iryna\nGurevych. 2021. What to pre-train on? Efficient\nintermediate task selection. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10585\u201310605, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nYada Pruksachatkun,\nJason Phang,\nHaokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang,\nClara Vania, Katharina Kann, and Samuel R. Bow-\nman. 2020. Intermediate-task transfer learning with\npretrained language models: When and why does it\nwork? In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5231\u20135247, Online. Association for Computational\nLinguistics.\nChengwei Qin and Shafiq Joty. 2022. LFPT5: A unified\nframework for lifelong few-shot language learning\nbased on prompt tuning of t5. In International Con-\nference on Learning Representations.\nYujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong\nWang, Ruobing Xie, Zhiyuan Liu, Maosong Sun,\nand Jie Zhou. 2023. Recyclable tuning for continual\npre-training. arXiv preprint arXiv:2305.08702.\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022. ELLE: Ef-\nficient lifelong pre-training for emerging data. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, pages 2789\u20132810, Dublin, Ire-\nland. Association for Computational Linguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis and insights\nfrom training gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nAnastasia Razdaibiedina, Yuning Mao, Rui Hou, Ma-\ndian Khabsa, Mike Lewis, and Amjad Almahairi.\n2023. Progressive prompts: Continual learning for\nlanguage models. In The Eleventh International Con-\nference on Learning Representations.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlexandru Salcianu, Marc van Zee, Jacob Austin,\nSebastian Goodman, Livio Baldini Soares, Haitang\nHu, Sasha Tsvyashchenko, Aakanksha Chowdh-\nery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-\nc\u00eda, Jianmo Ni, Andrew Chen, Kathleen Kenealy,\nJ. Clark, Stephan Lee, Daniel H Garrette, James Lee-\nThorp, Colin Raffel, Noam M. Shazeer, Marvin Rit-\nter, Maarten Bosma, Alexandre Passos, Jeremy B.\nMaitin-Shepard, Noah Fiedel, Mark Omernick, Bren-\nnan Saeta, Ryan Sepassi, Alexander Spiridonov,\nJoshua Newlan, and Andrea Gesmundo. 2022. Scal-\ning up models and data with t5x and seqio. ArXiv\npreprint: arXiv 2203.17189.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian opti-\nmization. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 372\u2013382, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In International\nConference on Learning Representations.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob\nHilton, Jacob Menick, Jiayi Weng, Juan Felipe\nCeron Uribe, Liam Fedus, Luke Metz, Michael Poko-\nrny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vi-\njayvergiya, Eric Sigler, Adam Perelman, Chelsea\nVoss, Mike Heaton, Joel Parish, Dave Cummings,\nRajeev Nayak, Valerie Balcom, David Schnurr,\nTomer Kaftan, Chris Hallacy, Nicholas Turley, Noah\nDeutsch, Vik Goel, Jonathan Ward, Aris Konstan-\ntinidis, Wojciech Zaremba, Long Ouyang, Leonard\nBogdonoff, Joshua Gross, David Medina, Sarah\nYoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost\nHuizinga, Roger Jiang, Carroll Wainwright, Diogo\nAlmeida, Steph Lin, Marvin Zhang, Kai Xiao, Kata-\nrina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub\nPachocki, Phil Tillet, Shantanu Jain, Greg Brockman,\nNick Ryder, Alex Paino, Qiming Yuan, Clemens Win-\nter, Ben Wang, Mo Bavarian, Igor Babuschkin, Szy-\nmon Sidor, Ingmar Kanitscheider, Mikhail Pavlov,\nMatthias Plappert, Nik Tezak, Heewoo Jun, William\nZhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek,\nAndrew Carr, Lilian Weng, Sandhini Agarwal, Karl\nCobbe, Vineet Kosaraju, Alethea Power, Stanislas\nPolu, Jesse Han, Raul Puri, Shawn Jain, Benjamin\nChess, Christian Gibson, Oleg Boiko, Emy Parparita,\nAmin Tootoonchian, Kyle Kosic, and Christopher\nHesse. 2023. Chatgpt. https://openai.com/\nblog/chatgpt/.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda\nMuresan. 2022.\nFine-tuned language models are\ncontinual learners. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6107\u20136122, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nTom Veniat, Ludovic Denoyer, and MarcAurelio Ran-\nzato. 2021. Efficient continual learning with modular\nnetworks and task-driven priors. In International\nConference on Learning Representations (ICLR).\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-\ndro Sordoni, Adam Trischler, Andrew Mattarella-\nMicke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-\nploring and predicting transferability across NLP\ntasks. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7882\u20137926, Online. Association for\nComputational Linguistics.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pappa-\ngari, R. Thomas McCoy, Roma Patel, Najoung Kim,\nIan Tenney, Yinghui Huang, Katherin Yu, Shuning\nJin, Berlin Chen, Benjamin Van Durme, Edouard\nGrave, Ellie Pavlick, and Samuel R. Bowman. 2019.\nCan you tell me how to get past sesame street?\nsentence-level pretraining beyond language model-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4465\u20134476, Florence, Italy. Association for Compu-\ntational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nMaciej Wo\u0142czyk, Micha\u0142 Zaj \u02dbac, Razvan Pascanu,\n\u0141ukasz Kuci\u00b4nski, and Piotr Mi\u0142o\u00b4s. 2021. Continual\nworld: A robotic benchmark for continual reinforce-\nment learning. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nTongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang\nLi, Guilin Qi, and Gholamreza Haffari. 2022. Pre-\ntrained language model in continual learning: A com-\nparative study. In International Conference on Learn-\ning Representations.\nKaichao You, Yong Liu, Jianmin Wang, and Mingsheng\nLong. 2021. Logme: Practical assessment of pre-\ntrained models for transfer learning. In ICML.\nKaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang,\nMichael I. Jordan, and Mingsheng Long. 2022. Rank-\ning and tuning pre-trained models: A new paradigm\nfor exploiting model hubs. JMLR.\nA\nBenchmark details\nStatistics for individual datasets used in our analy-\nsis are contained in Table A.1. Following Wei et al.\n(2022), we cap the number of training examples\nper dataset at 30k, and use up to 200 examples for\nvalidation. We use the full test sets where available.\nNote that some tasks have multiple subtasks (e.g.,\nfor SQuAD (Rajpurkar et al., 2016) we treat\nSQuAD v1 and v2 as separate subtasks). We apply\nthese limits only per subtask, and when computing\nresults, we take the average performance across all\nsubtasks. The benchmark is available for down-\nload\nat\nhttps://ct5x.s3.us-east-2.\namazonaws.com/benchmark.json.\nAll\nmodels were trained using the T5x frame-\nwork (Roberts et al., 2022) using the base T5 1.1 ar-\nchitecture with default hyper-parameters. Our pre-\ntrained model was the t5_1_1_lm100k_base\nmodel released by Lester et al. (2021).\nName\nTrain\nValidation\nTest\nMetric\nanli\n76946\n600\n3200\naccuracy\nsquad\n60000\n400\n22443\nf1\nmnli\n60000\n400\n19647\naccuracy\nhellaswag\n30000\n200\n10042\naccuracy\nyelp_polarity_reviews\n30000\n200\n38000\naccuracy\nrecord\n30000\n200\n10000\naccuracy\ntrue_case\n30000\n200\n3000\nmean_edit\nxsum\n30000\n200\n11301\nrougeLsum\nnewsroom\n30000\n200\n108862\nrougeLsum\nmulti_news\n30000\n200\n5622\nrougeLsum\nweb_nlg_en\n30000\n200\n1667\nrougeLsum\nqnli\n30000\n200\n5463\naccuracy\nwinogrande\n30000\n200\n1267\naccuracy\nsentiment140\n30000\n200\n498\naccuracy\ndart\n30000\n200\n2768\nrougeLsum\nwiki_lingua_english_en\n30000\n200\n28614\nrougeLsum\ncommon_gen\n30000\n200\n993\nrougeLsum\nword_segment\n30000\n200\n3000\nmean_edit\ncnn_dailymail\n30000\n200\n11490\nrougeLsum\ne2e_nlg\n30000\n200\n4299\nrougeLsum\nglue_qqp\n30000\n200\n40430\naccuracy\npaws_wiki\n30000\n200\n8000\naccuracy\nsst2\n30000\n200\n872\naccuracy\nsnli\n30000\n200\n10000\naccuracy\ntrivia_qa\n30000\n200\n11313\nf1\nag_news_subset\n30000\n200\n7600\naccuracy\nquac\n30000\n200\n7354\nnll\nmath_dataset\n30000\n200\n10000\naccuracy\ndrop\n30000\n200\n9536\nf1\nfix_punct\n30000\n200\n3000\nmean_edit\nnatural_questions\n30000\n200\n3610\nf1\ngigaword\n30000\n200\n1951\nrougeLsum\nmultirc\n27043\n200\n4848\naccuracy\ncosmos_qa\n25062\n200\n2985\naccuracy\nimdb_reviews\n24800\n200\n25000\naccuracy\npiqa\n16013\n100\n1838\naccuracy\nsamsum\n14732\n200\n819\nrougeLsum\naeslc\n14436\n200\n1906\nrougeLsum\nbool_q\n9227\n200\n3270\naccuracy\ncola\n8351\n200\n1043\naccuracy\ncoqa\n7099\n100\n500\nnll\nstsb\n5649\n100\n1500\naccuracy\ntrec\n5252\n200\n500\naccuracy\nwic\n5228\n200\n638\naccuracy\nopenbookqa\n4957\n200\n500\naccuracy\nopinion_abstracts\n4790\n200\n1000\nrougeLsum\nglue_mrpc\n3468\n200\n408\naccuracy\narc\n2970\n400\n3548\naccuracy\nrte\n2290\n200\n277\naccuracy\nstory_cloze\n1671\n200\n1871\naccuracy\ndefinite_pronoun_resolution\n1122\n200\n564\naccuracy\nwnli\n605\n30\n71\naccuracy\nwsc\n504\n50\n104\naccuracy\ncopa\n350\n50\n100\naccuracy\ncb\n200\n50\n56\naccuracy\nTable A.1: Statistics for each dataset used in our analysis, and the primary metric used.\nB\nCheckpoint selection features\nWe derive a number of easy to compute features\nfor our lightweight GBDT checkpoint selecto when\nevaluating transfer candidate (ti, tj).\n\u2022 Relative performance. We compute the relative\n0-shot and 5-shot performance for an indepen-\ndently fine-tuned model (starting from the base\nmodel, trained just on tj) with the sequential\ntuning candidate (starting from ti and training\nnext on tj). Here k-shot denotes the performance\nafter k updates (using a fixed batch-size). We\ncompute the relative performance both in terms\nof the specific task metric (e.g., accuracy) and\nthe token negative log-likelihood, as these can\nhave different trends.\n\u2022 Weight change.\nWe compute the maximum\nand average magnitude parameter update of the\ncheckpoint for tj relative to the base pre-trained\nmodel. This is stratified by weight group, where\nwe differentiate between softmax, embedding,\nand layer parameters (the layers are collapsed\ninto 4 groups by layer number, e.g., layers\n{1, 2, 3} are a group).\n\u2022 Update similarity. We approximate gradient-\nbased similarity by using the weight change of\na candidate model from the base pre-trained\nmodel as an estimate of the average gradient\napplied (recall that all models start from the\nsame pre-trained model).\nWe then compare\nthis \u201caverage gradient\u201d to the weight change of\nthe independent fine-tuning model after 5 steps\nusing cosine similarity. This gives an idea if the\naverage gradient already applied to ti is similar\nin direction to the initial gradient computed\nwhen training directly on tj. We also stratify this\nmetric across different parameter groups (the\nsame as for absolute weight change above).\n\u2022 Task metadata. We include binary features that\nindicate if the last task used to train ti is in the\nsame manually defined family as tj, and also if\nany task used in the full process for training ti\n(i.e., was used as a previous checkpoint initializa-\ntion) is in the same manually defined family as\ntj. The task families are illustrated in Figure 2.\nWhen comparing feature importance determined\nafter training the GBDT, the relative 0 and 5-shot\nperformance is most important, followed by gra-\ndient similarity. While this is intuitive (and also\nclearly helpful for our specific purpose when con-\nsidering that our log-scale PerAUC metric heavily\nweights strong early performance), it is important\nto note that computing these features does not scale\nparticularly well with the number of tasks, as it in-\nvolves training and evaluating many models, even\nif only for a few updates. Gradient similarity is less\ncostly to evaluate using our approximate method,\nas it only requires evaluating the weight change for\nall checkpoints except for the pre-trained model.\nC\nAdditional results\nWe list all results per sequence type in Tables C.1\nthrough C.10, in addition to the medians in Table 2.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n43.31\n22.52\n56.97\n43.31\n56.97\n22.52\n47.14\n54.55\n47.14\n54.55\n49.27\n50.82\n58.54\n58.54\n58.54\n47.14\n50.82\n56.69\n56.69\n56.69\n32.59\n32.80\n31.97\n32.59\n32.80\n37.56\n56.60\n58.50\n56.60\n58.50\n32.59\n59.31\n52.37\n52.37\n59.31\n59.31\n32.80\n56.58\n59.31\n59.31\n22.71\n30.83\n38.42\n22.71\n38.42\n30.58\n28.64\n39.13\n39.13\n39.13\n22.71\n30.58\n40.50\n40.50\n40.50\n36.50\n30.58\n41.57\n36.50\n41.57\n14.07\n12.22\n1.37\n12.22\n14.07\n43.37\n14.07\n43.77\n43.37\n43.77\n12.22\n43.37\n32.90\n12.22\n43.37\n43.37\n17.00\n41.58\n41.58\n43.37\nTable C.1: Both A and B are intended to be positive.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n52.36\n-310.39\n-291.80\n52.36\n52.36\n51.16\n-310.39\n-281.67\n51.16\n51.16\n52.36\n-17.99\n49.96\n52.36\n52.36\n51.16\n-57.86\n-14.64\n51.16\n51.16\n8.20\n-16.31\n-8.35\n8.20\n8.20\n13.66\n-16.31\n7.90\n13.66\n13.66\n11.41\n-16.31\n10.13\n11.41\n11.41\n14.85\n-16.31\n-4.11\n14.85\n14.85\n14.07\n-17.00\n-1.44\n0.00\n14.07\n43.37\n-17.00\n26.98\n43.37\n43.37\n12.22\n-17.00\n-3.25\n12.22\n12.22\n17.00\n-17.00\n-8.93\n0.00\n17.00\n10.99\n-18.00\n-6.10\n10.99\n10.99\n10.99\n-9.45\n0.63\n10.99\n10.99\n10.99\n-12.91\n1.38\n10.99\n10.99\n10.99\n-13.55\n4.33\n10.99\n10.99\nTable C.2: A is intended to be positive while B is intended to be negative.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n52.20\n-0.78\n45.69\n52.20\n52.20\n15.00\n2.99\n34.87\n15.00\n34.87\n15.00\n-3.68\n22.51\n15.00\n22.51\n28.70\n2.99\n23.93\n28.70\n28.70\n17.33\n-3.31\n10.29\n17.33\n17.33\n14.14\n-3.31\n-1.38\n14.14\n14.14\n14.14\n-2.10\n11.34\n14.14\n14.14\n15.27\n-3.31\n-0.78\n15.27\n15.27\n59.31\n0.49\n43.87\n59.31\n59.31\n32.80\n0.49\n27.43\n32.80\n32.80\n59.31\n5.51\n44.56\n59.31\n59.31\n35.85\n4.09\n32.40\n35.85\n35.85\n-0.95\n0.00\n6.95\n-0.95\n6.95\n-0.95\n4.09\n6.95\n-0.95\n6.95\n-0.95\n0.00\n-4.45\n-0.95\n0.00\n-0.95\n6.95\n6.95\n6.95\n6.95\nTable C.3: A is intended to be positive while B is intended to be neutral.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n-16.31\n8.20\n-12.27\n0.00\n8.20\n-16.31\n13.66\n15.92\n13.66\n15.92\n-16.31\n14.85\n15.13\n14.85\n15.13\n-16.31\n11.41\n8.87\n11.41\n11.41\n-12.91\n10.99\n1.06\n10.99\n10.99\n-9.45\n10.99\n-2.25\n10.99\n10.99\n-13.55\n10.99\n-2.80\n10.99\n10.99\n-18.00\n10.99\n-14.24\n10.99\n10.99\n-17.00\n17.00\n5.67\n0.00\n17.00\n-17.00\n43.37\n30.98\n43.37\n43.37\n-17.00\n12.22\n-6.83\n12.22\n12.22\n-17.00\n14.07\n18.22\n0.00\n18.22\n-310.34\n51.16\n39.47\n51.16\n51.16\n-57.86\n51.16\n38.09\n51.16\n51.16\n-17.99\n52.36\n33.78\n33.78\n52.36\n-17.99\n51.16\n49.56\n49.56\n51.16\nTable C.4: A is intended to be negative while B is intended to be positive.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n-17.99\n-310.34\n-8.89\n0.00\n0.00\n-310.34\n-57.86\n-66.16\n0.00\n0.00\n-17.99\n-310.39\n-119.44\n0.00\n0.00\n-57.86\n-17.99\n-49.32\n0.00\n0.00\n-106.80\n-36.99\n-40.34\n0.00\n0.00\n-157.10\n-66.13\n-372.89\n0.00\n0.00\n-106.80\n-5.16\n-85.35\n0.00\n0.00\n-66.13\n-5.16\n-22.69\n0.00\n0.00\n-13.55\n-18.00\n-20.60\n0.00\n0.00\n-9.45\n-12.91\n-7.13\n0.00\n0.00\n-9.45\n-18.00\n-5.68\n0.00\n0.00\n-12.91\n-18.00\n-5.48\n0.00\n0.00\n-16.89\n-21.00\n-18.95\n0.00\n0.00\n-27.96\n-16.89\n-22.73\n0.00\n0.00\n-27.96\n-21.00\n-26.68\n0.00\n0.00\n-21.00\n-27.96\n-32.79\n0.00\n0.00\nTable C.5\nTable C.6: Both A and B are intended to be negative.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n-17.00\n-12.78\n-17.39\n-12.78\n0.00\n-17.00\n-20.57\n-16.32\n-20.57\n0.00\n-17.00\n-2.10\n-22.33\n0.00\n0.00\n-17.00\n2.22\n7.99\n0.00\n7.99\n-106.80\n5.21\n1.10\n5.21\n5.21\n-77.67\n-0.73\n-10.93\n0.00\n0.00\n-77.67\n8.12\n1.48\n0.00\n8.12\n-66.13\n-0.73\n-3.65\n0.00\n0.00\n-16.89\n-2.96\n-14.71\n0.00\n0.00\n-27.96\n-3.43\n-15.41\n0.00\n0.00\n-21.00\n-4.53\n-22.14\n0.00\n0.00\n-27.96\n-4.53\n-14.35\n0.00\n0.00\n-6.33\n-2.75\n-2.77\n0.00\n0.00\n-6.33\n2.81\n-5.74\n0.00\n2.81\n-6.33\n-0.97\n-4.85\n0.00\n0.00\n-6.33\n-1.21\n-3.65\n0.00\n0.00\nTable C.7: A is intended to be negative while B is intended to be neutral.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n-0.70\n11.41\n8.71\n11.41\n11.41\n-12.99\n13.66\n10.29\n13.66\n13.66\n2.67\n13.66\n10.06\n13.66\n13.66\n-0.70\n8.20\n6.25\n8.20\n8.20\n0.16\n16.38\n8.50\n16.38\n16.38\n-7.90\n7.83\n-0.79\n-7.90\n7.83\n0.16\n10.45\n11.52\n10.45\n11.52\n2.71\n10.45\n1.90\n1.90\n10.45\n-40.79\n52.36\n51.84\n51.84\n52.36\n27.80\n52.36\n44.12\n52.36\n52.36\n-40.79\n51.16\n48.45\n51.16\n51.16\n27.80\n51.16\n47.42\n51.16\n51.16\n13.47\n34.11\n36.22\n34.11\n36.22\n-6.09\n45.62\n54.49\n45.62\n54.49\n-9.01\n80.13\n76.83\n80.13\n80.13\n-13.75\n57.81\n28.08\n0.00\n57.81\nTable C.8\nTable C.9: A is intended to be neutral while B is intended to be positive.\nA \u2192 C\nB \u2192 C\nA \u2192 B \u2192 C\nPairwise\nPairwise\nNa\u00efve\nSelective\nOracle\n2.81\n-6.33\n-1.21\n0.00\n2.81\n-0.97\n-6.33\n-3.13\n-0.97\n0.00\n-3.74\n-6.33\n-4.13\n0.00\n0.00\n-3.64\n-6.33\n-2.45\n0.00\n0.00\n-2.96\n-21.00\n-22.89\n0.00\n0.00\n-2.96\n-16.89\n-12.74\n0.00\n0.00\n-4.53\n-16.89\n-14.76\n0.00\n0.00\n-3.32\n-16.89\n-17.37\n0.00\n0.00\n8.12\n-5.16\n1.27\n0.00\n8.12\n8.12\n-5.75\n3.54\n0.00\n8.12\n5.21\n-66.13\n-9.05\n5.21\n5.21\n5.21\n-36.99\n-33.14\n5.21\n5.21\n-1.66\n-17.00\n-12.37\n-1.66\n0.00\n-20.57\n-17.00\n-25.37\n-20.57\n0.00\n-2.10\n-17.00\n-14.88\n0.00\n0.00\n1.28\n-17.00\n-21.27\n0.00\n1.28\nTable C.10: A is intended to be neutral while B is intended to be negative.\n"
  },
  {
    "title": "Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations",
    "link": "https://arxiv.org/pdf/2307.05959.pdf",
    "upvote": "2",
    "text": "Giving Robots a Hand:\nLearning Generalizable Manipulation with\nEye-in-Hand Human Video Demonstrations\nMoo Jin Kim, Jiajun Wu, Chelsea Finn\nDepartment of Computer Science\nStanford University\n{moojink,jiajunwu,cbfinn}@cs.stanford.edu\nFigure 1: We incorporate diverse eye-in-hand human video demonstrations to train behavioral cloning policies\nthat generalize to new environments and new tasks outside the distribution of expert robot imitation data.\nImages are masked to close the domain gap between human and robot observations. Action labels for human\nvideo demonstrations are inferred by an inverse dynamics model trained on robot play data.\nAbstract: Eye-in-hand cameras have shown promise in enabling greater sample\nefficiency and generalization in vision-based robotic manipulation. However, for\nrobotic imitation, it is still expensive to have a human teleoperator collect large\namounts of expert demonstrations with a real robot. Videos of humans perform-\ning tasks, on the other hand, are much cheaper to collect since they eliminate the\nneed for expertise in robotic teleoperation and can be quickly captured in a wide\nrange of scenarios. Therefore, human video demonstrations are a promising data\nsource for learning generalizable robotic manipulation policies at scale. In this\nwork, we augment narrow robotic imitation datasets with broad unlabeled human\nvideo demonstrations to greatly enhance the generalization of eye-in-hand visuo-\nmotor policies. Although a clear visual domain gap exists between human and\nrobot data, our framework does not need to employ any explicit domain adapta-\ntion method, as we leverage the partial observability of eye-in-hand cameras as\nwell as a simple fixed image masking scheme. On a suite of eight real-world\ntasks involving both 3-DoF and 6-DoF robot arm control, our method improves\nthe success rates of eye-in-hand manipulation policies by 58% (absolute) on av-\nerage, enabling robots to generalize to both new environment configurations and\nnew tasks that are unseen in the robot demonstration data. See video results at\nhttps://giving-robots-a-hand.github.io/.\nKeywords: Learning from human demonstrations, Imitation learning, Robotic\nmanipulation\n1\nIntroduction\nRecent works in vision-based robotic manipulation have shown significant performance gains real-\nized by using eye-in-hand cameras in addition to, or in replacement of, static third-person cameras\narXiv:2307.05959v1  [cs.RO]  12 Jul 2023\n[1, 2, 3]. Despite their effects, eye-in-hand cameras alone do not guarantee robust policies, as vision-\nbased models tend to be brittle against real-world variation, such as changes in background, lighting,\nand object appearances [4]. Therefore, one natural approach for improving generalization is to train\npolicies on large, diverse robot demonstration datasets [5, 6, 7]. However, collecting such data on a\nreal robot is expensive, as it often requires practitioners to either perform kinesthetic teaching [8, 9]\nor robotic teleoperation [10, 11, 12, 13, 14, 15, 16] via virtual reality headsets or joystick controllers.\nIn contrast, collecting videos of humans completing tasks is much less expensive because a human\noperator can rapidly capture many demonstrations without having to constantly reset the robot to\nsome initial state, debug hardware-related issues, or arduously relocate the robot to varied settings\nto increase visual diversity. Consequently, human video demonstrations are a promising data source\nthat could improve the generalization capabilities of vision-based robotic manipulators at scale.\nDespite this enticing potential, a central challenge in learning from human video demonstrations\nis the difference in appearance between human and robot morphologies, which creates a distribu-\ntion shift that must be accounted for. Prior works that utilize a third-person camera perspective\nhave aimed to mitigate this domain gap by taking explicit domain adaptation approaches, such as\nperforming human-to-robot image translation, learning domain-invariant visual representations, and\nleveraging keypoint representations of human and robot states (see Section 2 for details). In con-\ntrast, since we learn policies from an eye-in-hand visual perspective, we close the domain gap in a\nfar less involved way: we simply mask a fixed portion of every image such that the human hand or\nrobotic end-effector is no longer visible. As a result, we do not need to employ any domain adap-\ntation method and can learn vision-based manipulation policies end-to-end directly from human\nvideos (where actions are inferred by an inverse dynamics model, which we discuss later). We can\nthus avoid errors produced by explicit domain adaptation methods, e.g., conspicuous visual artifacts\nfrom human-to-robot image translations [17].\nThe main contribution of this work is the study of a simple, novel method that incorporates diverse\neye-in-hand human video demonstrations to improve environment and task generalization. Across\nseveral real-world robotic manipulation tasks, including reaching, grasping, pick-and-place, cube\nstacking, plate clearing, and toy packing, we observe that our method leads to significant improve-\nments in generalization. Our policies generalize to both new environments and new tasks that are\nnot seen in the robot demonstrations, even in tasks with heavy visual occlusion and multiple stages.\nOn average, we observe a 58% improvement in absolute success rates across unseen environments\nand tasks when comparing against policies trained only on robot demonstrations.\n2\nRelated Work\nImitation learning is a powerful paradigm for training an agent to complete a task by learning a map-\nping between observations and actions. Traditional approaches to robotic imitation assume access\nto expert demonstrations collected from the robot\u2019s observation and action spaces [8, 18, 19, 20].\nSince collecting expert trajectories with a real robot can be physically demanding or require special\nteleoperation equipment and training [10, 11, 12, 13, 14, 15, 16], we study the setting of training\nrobots to complete tasks by watching videos of a human demonstrator. One central challenge here\nis the distribution shift caused by apparent visual differences between human and robot structures.\nPast works have addressed this distribution shift in various ways. Some have employed explicit\ndomain adaptation techniques such as human-to-robot context translation [21, 22] and pixel-level\nimage translation [17, 23, 24], commonly using generative models like CycleGAN, which can learn\nmappings between domains given unpaired data [25]. Other works have explicitly specified the cor-\nrespondences between human and robot embodiments and behaviors by, e.g., employing pose and\nobject detection techniques [26, 27, 28, 29, 30, 31, 32, 33] and learning keypoint-based state rep-\nresentations of human and robot observations [34, 24]. Some have taken a more implicit approach\nand learned domain-invariant visual representations or reward functions that are useful for solving\ndownstream tasks [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]. Yet another class of works used robotic\nend-effectors more closely resembling the human hand (e.g., Allegro Hand) to train dexterous ma-\nnipulation policies via hand pose estimation and kinematic retargeting [46, 47, 48, 49, 50].\n2\nIn contrast to most of these works, which use human demonstrations captured from third-person\ncameras, we avoid the need to apply any explicit domain adaptation or human-robot correspondence\nmapping method by utilizing masked eye-in-hand visual inputs. We also train policies that generalize\nto new settings and tasks without having to learn intermediate representations or reward functions.\nFurther, unlike prior works utilizing manipulators with more than two fingers, we employ a parallel-\njaw robotic end-effector despite it being visually and kinematically dissimilar from the human hand.\nRelatedly, Young et al. [51] and Song et al. [52] amass diverse manipulation data using \u201creacher-\ngrabber\u201d tools. To minimize domain shift, these tools are attached to the robot arms or engineered to\nclosely resemble real parallel-jaw end-effectors. In contrast, we collect demonstrations with the hu-\nman hand, which is faster and more flexible than these tools, and test our policies directly on a robot\nwith a structurally dissimilar gripper. Further, our lightweight eye-in-hand camera configuration for\nhuman demonstrations is simple to assemble and has nearly zero cost (aside from purchasing the\ncamera itself), while the reacher-grabber tool proposed by Song et al. [52] requires more sophisti-\ncated assembly and costs approximately $450 USD (excluding the cost of the camera).\nLastly, the purpose of this work is not to perform a head-to-head comparison between eye-in-hand\nand third-person perspective methods. We defer such discussion to Hsu et al. [1] and Jangir et al. [2],\nwhich already analyze and compare eye-in-hand and third-person methods in robotic manipulation\ncomprehensively. In this paper, we focus specifically on the eye-in-hand visual setting and compare\nour approach to methods that are compatible to this regime.\n3\nPreliminaries\nObservation and action spaces. The observation spaces of the robot and human, Or and Oh respec-\ntively, consist of eye-in-hand RGB image observations or \u2208 Or, oh \u2208 Oh. The robot\u2019s action space\nAr either has four dimensions, consisting of 3-DoF end-effector position control and 1-DoF grip-\nper control, or seven dimensions, consisting of additional 3-DoF end-effector rotation/orientation\ncontrol. We assume that the human\u2019s action space Ah is the same as the robot\u2019s: Ah = Ar.\nProblem definition. Our objective is to incorporate broad human data to train a policy that general-\nizes better than one that is trained solely on robot data. While broad data can improve generalization\nalong a number of axes, we specifically aim to improve performance in terms of environment gener-\nalization and task generalization. We define environment generalization as the ability to execute a\nlearned task in a new environment unseen in the robot demonstrations. We define task generaliza-\ntion as the ability to execute a new, longer-horizon task when the robot demonstrations only perform\nan easier, shorter-horizon task.\n4\nLearning from Eye-in-Hand Human Video Demonstrations\nWe now discuss each module of our framework (Figure 1). We first collect eye-in-hand human\ndemonstrations with a simple low-cost setup (Section 4.1). We then label human demonstrations\nwith actions using an inverse dynamics model trained on robot \u201cplay\u201d data (Section 4.2). Finally,\nwe utilize human demonstrations to train generalizable imitation learning policies (Section 4.3).\n4.1\nEye-in-Hand Video Data Collection\nData collection setup. As shown in Figure 2, we secure an RGB camera to a human demonstrator\u2019s\nforearm with two rubber bands, and the demonstrator is immediately ready to collect video demon-\nstrations of a task. While more secure ways of fastening the camera exist, we find that this simple\nconfiguration is sufficient and only takes a few seconds to prepare. The same camera is mounted\nonto a Franka Emika Panda robot arm via an L-bracket assemblage (see Figure 2). To control the\nrobot, we perform teleoperation with a virtual reality controller (Oculus Quest).\nMasking the hand and end-effector. To close the gap between human and robot domains, we\nmask a fixed region of all image observations oh, or captured by the eye-in-hand human and robot\ncameras to hide the agent\u2019s embodiment. Specifically, we capture images of size 100\u00d7100 and zero\nout the top 36 rows of pixels with a script; we denote the resulting human and robot observations as\n3\nFigure 2: Left: Human and robot eye-in-hand camera configurations. Fastening a USB camera on a human\narm only requires two rubber bands. Mounting a camera on a Franka Emika Panda robot arm involves L-\nbrackets, washers, and screws. Right: Sample image observations captured by the eye-in-hand human and\nrobot cameras. We mask the top 36% of every image in both domains.\n\u00afoh, \u00afor, respectively. This transformation is shown in Figure 2. We train inverse dynamics models\nand imitation learning policies (discussed in subsequent sections) solely on masked images.1\n4.2\nAction Labeling of Human Video Demonstrations via Inverse Dynamics\nSuppose we have a diverse set of eye-in-hand human video demonstrations for a manipulation task:\nDh\nexp = {\u00afoh\nt }1...M, where M is the total number of timesteps. Since human videos only contain\nsequences of images, we cannot train an imitation learning policy on this dataset until we generate\naction labels. The inverse dynamics model serves this precise purpose: Given image observations \u00afoh\nt\nand \u00afoh\nt+1 at timesteps t and t+1, the inverse model predicts the action at giving rise to the change in\nobservations [53, 22, 54, 40, 23]. See Appendix A.1.1 for details on the inverse model architecture.\nRobot play data. An inverse model should be trained on data with sufficient diversity such that it\ncan make accurate predictions on diverse human demonstration data. In this paper, we choose to\ntrain the inverse model using visually and behaviorally diverse, task-agnostic robot \u201cplay\u201d data that\nis collected in a similar manner as Lynch et al. [55]. See Appendix A.3.1 for details on how we\ncollect the play data and why it is easy to collect in large quantities.\nInverse dynamics model training. Given robot play data, we now have observation-action transi-\ntions (\u00afor\nt, ar\nt, \u00afor\nt+1) \u2208 Dr\nplay. The inverse model, parameterized by \u03b8, takes as input (\u00afor\nt, \u00afor\nt+1) and\noutputs a prediction \u02c6ar\nt = f\u03b8(\u00afor\nt, \u00afor\nt+1). We optimize the parameters \u03b8 to minimize the L1 difference\nbetween \u02c6ar\nt and ar\nt for K transitions sampled from the play dataset, using Adam optimization [56]:\nL(\u02c6ar\nt, ar\nt; \u03b8)1...K = PK\nt=1 ||\u02c6ar\nt \u2212 ar\nt||1.\nLabeling human video demonstrations. Once we have trained an inverse model, we run it on all\npairs of observations in the human demonstration dataset, (\u00afoh\nt , \u00afoh\nt+1) \u2208 Dh\nexp, to automatically gen-\nerate action labels for the demonstrations (see Appendix A.5 for sample inverse model predictions\nand analysis). We then have a labeled set of human observation-action pairs, which we denote as\nbDh\nexp = {(\u00afoh\nt , \u02c6ah\nt )}1...M, where M is the total number of such pairs. We use this dataset to train an\nimitation learning policy, as described in the next section.\n4.3\nImitation Learning with Human and Robot Demonstrations\nBehavioral cloning. Given a dataset of human video demonstrations with inferred action labels\nbDh\nexp = {(\u00afoh\nt , \u02c6ah\nt )1...M}, we train a manipulation policy via behavioral cloning (BC), which learns\na mapping between observations encountered by an expert demonstrator and their corresponding\n1At first glance, it may seem impossible to learn with \u00afoh, \u00afor given that the hand or end-effector is not visible.\nHowever, we observe that inverse models trained on data in this format can reasonably infer environment\ndynamics nonetheless due to the presence of certain visual cues. For example, the grasping and lifting of an\nobject can be inferred even when the gripper is not visible due to visual signals such as the object \u201clocking\u201d into\nplace as it is secured in the hand, the object beginning to levitate, shadows forming underneath the object, and\nneighboring objects shrinking in size in the eye-in-hand camera\u2019s field of view. Similarly, imitation learning\npolicies can also succeed at various tasks without seeing the hand or end-effector in the frame after a small\nmodification to the policies\u2019 inputs (see Section 4.3 for details). Nonetheless, masking the image does place\nsome limitations on the tasks that can be performed, which we discuss further in Section 6.\n4\nactions [57]. In this case, we treat actions \u02c6ah\nt inferred by the inverse model as \u201cground truth\u201d\nlabels representing the demonstrator\u2019s actions. The BC policy \u03c0\u03d5 takes as input an RGB image\nobservation \u00afoh\nt and outputs an action \u02dcah\nt to best match \u02c6ah\nt . We minimize the negative log-likelihood\nof the predictions to find the optimal policy parameters \u03d5\u2217, using Adam optimization [56] to train\nthe model.\nConditioning the behavioral cloning policy on grasp state. We modify the BC policy to be\nconditioned on an additional binary variable sh\nt representing the grasp state at time t (open/closed).\nThis variable provides proprioceptive information about the manipulator that was removed from the\nimage observations by the image masking scheme discussed in Section 4.1; without knowing the\ngrasp state, the policy may not be able to discern whether it has already grasped an object and could\nfail to proceed to complete the task. We automatically estimate sh\nt by setting it as the prior timestep\u2019s\ngrasping action, which is inferred by the inverse model when labeling human demonstrations with\nactions. We then concatenate sh\nt to the latent image embedding and feed the result into the policy\nnetwork (see Appendix A.1.2 for model architecture details). The resulting policy is \u03c0\u03d5(\u02dcah\nt |\u00afoh\nt , sh\nt ),\nand we optimize \u03d5 as described before.\nGeneralizing beyond narrow robot demonstrations.\nAs discussed in Section 3, we collect\nand train a BC policy on a narrow set of robot demonstrations and a broader set of human\ndemonstrations with the goal of generalizing to the environments or tasks covered by the hu-\nman data.\nThe final objective, given N robot samples and M human samples, is to find:\n\u03d5\u2217 = arg min\u03d5 \u2212 PN\nt=1 log \u03c0\u03d5(\u02dcar\nt|\u00afor\nt, sr\nt) \u2212 PM\nt=1 log \u03c0\u03d5(\u02dcah\nt |\u00afoh\nt , sh\nt ).\n5\nExperiments\nWe execute a set of experiments to study whether our framework for incorporating broad eye-in-\nhand human video demonstrations can be used to improve environment generalization and task\ngeneralization, as defined in Section 3. We then ablate key components of our framework, such as\nimage masking and grasp state conditioning, to study their contributions to the final performance.\n5.1\nExperimental Setup\nAs it is difficult to generate realistic human data in simulation, we perform all experiments in the\nreal world. All observations oh \u2208 Oh, or \u2208 Or are (3, 100, 100) RGB images. Raw image pixels\nrange between [0, 255], but we normalize them to [\u22120.5, 0.5]. We use 6-DoF end-effector control\nfor the toy packing task and 3-DoF control for the rest. In 3-DoF control, the three actions are\ncontinuous values ranging between [\u22121, 1] that command the change in the end-effector\u2019s position\nin the Cartesian space. In 6-DoF control, the additional three actions command the end-effector\u2019s\nchange in orientation (in extrinsic Euler angles). Lastly, one degree of freedom represents the binary\ngripper action (\u22121: close, 1: open).\n5.2\nEnvironment Generalization Experiments\nRecall that environment generalization (Section 3) is the ability to complete a learned manipulation\ntask in a new environment unseen in the robot demonstration dataset.\nTasks. The tasks include reaching towards a red cube in the presence of different distractor ob-\njects, grasping a red cube placed on various environment backgrounds, clearing different objects\noff of a plate, and packing different toys into a box. See Figure 3 for a visualization of these tasks\nand Appendix A.2 for details about each task. The tasks are ordered by increasing difficulty and\ncomplexity. The final 6-DoF toy packing task is particularly challenging because it involves heavy\nocclusion (from a wall positioned between the end-effector and target object); execution of a multi-\nstage trajectory (reaching around wall, grasping toy, lifting toy, reaching box, dropping toy into box);\nand three more degrees of freedom than the previous tasks (for end-effector orientation control).\nDatasets. For each task, we collect narrow robot demonstrations in one environment, and broad\nhuman demonstrations in multiple environments (shown in Figure 3). We also collect a robot play\ndataset for an inverse model that is shared with a task generalization experiment involving similar\nobjects. See Appendix A.3 for details on all expert demonstration datasets and robot play datasets.\n5\nFigure 3: Tasks used for environment generalization experiments. Robot demonstrations are collected only in\nthe environment configurations highlighted in pink, while human demonstrations are collected in the configu-\nrations highlighted in blue. In the toy packing task, the toys highlighted in green are not seen in the human or\nrobot demonstrations but appear in the robot play dataset, while the toys highlighted in yellow are not seen at\nall in human or robot play/demonstration data (i.e., these are fully held out).\nMethods. In our method, we train a BC policy on robot demonstrations and human demonstrations\nwith the image masking scheme discussed in Section 4.1. As we wish to study whether incorpo-\nrating broad human demonstrations into training achieves increased environment generalization, we\ncompare our method against a baseline policy trained only on narrow robot demonstrations. In ad-\ndition, to assess whether any improvements in generalization are simply correlated to the increase\nin training dataset size, we also compare against a policy trained on both robot demonstrations and\nrobot play data, as the play datasets are larger than the human demonstration datasets. Lastly, we\nevaluate how effective our image masking method is compared to explicit domain adaptation ap-\nproaches such as pixel-level image translation by comparing against a policy trained on both human\nand robot demonstrations, where a CycleGAN is used to translate human images into robot images\n(as in [17] and [23]). To summarize, we evaluate the following four methods in our experiments,\nwhere each one is a BC policy trained on a different set of data:\n\u2022 robot: robot demos only\n\u2022 robot + play: robot demos and robot play data\n\u2022 robot + human w/ CycleGAN: robot demos and CycleGAN-translated human demos\n\u2022 robot + human w/ mask (ours): robot demos and human demos with image masking\nResults. As shown in Table 1, incorporating diverse human video demonstrations into policy train-\ning with image masking significantly improves generalization. The policy generalizes to new en-\nvironment configurations unseen in the robot demonstrations (see fine-grained results in Table 3).\nTo our knowledge, this marks the first time that a real robot policy is directly trained end-to-end on\neye-in-hand human demonstrations. On the other hand, the policy trained only on a limited set of\nrobot demonstrations fails completely in many cases, as shown in Figure 5(c), since novel out-of-\ndistribution visual inputs confuse the policy. In addition, we see that a policy also trained on the\nfull play dataset, which is larger than the set of human demonstrations, does not perform as well as\none trained on the human demonstrations, verifying that generalization performance is not simply a\nfunction of training dataset size. Further, while using CycleGAN-translated human demonstrations\ngenerally leads to greater performance than using only robot demonstration or play data, it is not as\neffective as our image masking method. In particular, while the CycleGAN image translations are\nsuccessful in some cases, they are noisy in other cases (sample translations are shown in Figure 6 in\nAppendix A.4); such noise hinders final policy performance. Videos of the policies and extensive\nqualitative analysis of individual methods are available on our project website.\n5.3\nTask Generalization Experiments\nRecall that task generalization (Section 3) is the ability to complete a task that is unseen and longer-\nhorizon than those in the robot demonstrations.\n6\nFigure 4: Tasks used for task generalization experiments. Robot demonstrations perform a shorter-horizon\ntask, such as grasping (highlighted in pink); human demonstrations either perform the full, longer-horizon task\nor portions of the task that are missing in the robot demonstrations (highlighted in blue). Note that while the\nimages in blue depict a robot completing the tasks, in reality, the human demonstrations contain images of a\nhuman completing the tasks.\nTable 1: Environment generalization and task generalization results. Left: BC policies are evaluated\nagainst environments not seen in the robot demonstrations. Average success rates and standard errors are\ncomputed by aggregating the results across all environments per task in Table 3 in Appendix A.6. Right: For\neach experiment, robot demonstrations are collected only for the gray italicized task, and human demonstrations\nare collected for the longer-horizon task written in black. Each success rate is computed over 20 test rollouts of\nthe learned policy for the toy packing task, and 10 rollouts for the other tasks. The last row contains the average\nsuccess rate across all the longer-horizon tasks. Overall, training with masked human demonstrations leads to\nsignificantly better environment and task generalization than other methods.\nenvironment generalization success rate (%)\nrobot\nrobot + play\nrobot + human\nrobot + human\nw/ CycleGAN\nw/ mask (ours)\nreaching\n10.00 \u00b1 5.48\n20.00 \u00b1 7.30\n53.33 \u00b1 9.11\n86.67 \u00b1 6.21\ncube\n0.00 \u00b1 0.00\n21.67 \u00b1 5.32\n36.67 \u00b1 6.22\n51.67 \u00b1 6.45\ngrasping\nplate\n0.00 \u00b1 0.00\n23.33 \u00b1 7.72\n46.67 \u00b1 9.11\n56.67 \u00b1 9.05\nclearing\ntoy\n10.91 \u00b1 2.10\n30.00 \u00b1 3.09\n15.00 \u00b1 2.41\n59.55 \u00b1 3.31\npacking\naverage\n5.23 \u00b1 1.89\n23.75 \u00b1 5.86\n37.92 \u00b1 6.71\n63.64 \u00b1 6.25\ntask generalization success rate (%)\nrobot\nrobot + play\nrobot + human\nrobot + human\nw/ CycleGAN\nw/ mask (ours)\ncube grasping\n90\n90\n80\n90\ncube stacking\n0\n10\n0\n40\ncube grasping\n90\n100\n90\n90\ncube pick-and-place\n0\n0\n20\n80\nsponge grasping\n100\n100\n100\n100\nplate clearing\n0\n10\n30\n70\ntoy grasping\n95\n75\n100\n100\ntoy packing\n0\n15\n0\n45\naverage\n0\n8.75\n12.5\n58.75\nTasks. The tasks we test on include stacking a red cube on top of a blue cube, picking-and-placing\na red cube onto a green plate, clearing a green sponge from a plate, and packing a small black suit\nvampire wind-up toy into a box. See Figure 4 for a visualization of these tasks.\nDatasets. As in Section 5.2, we collect robot demonstrations, human demonstrations, and shared\nrobot play data. Robot demonstrations perform a simple, short-horizon task (e.g., cube grasping),\nand human demonstrations perform one of the more difficult, longer-horizon tasks above (e.g., cube\nstacking). Appendix A.3 gives full details on all datasets used in the experiments.\nMethods. We evaluate the task generalization of the same four methods discussed in Section 5.2.\nResults. As shown in Table 1, training the policy on the eye-in-hand human video demonstrations\nwith image masking substantially improves task generalization compared to using robot data alone.\nIntuitively, a policy trained on robot demonstrations that never perform the desired multi-stage task\nis incapable of performing the task at test time. A policy that is also trained on robot play data\ncan occasionally execute the desired task since the play dataset contains a collection of behaviors,\nsome of which can be useful for solving the task. However, as the play dataset is task-agnostic,\nBC often struggles to learn one coherent sequence of actions for solving a specific multi-stage task.\nLastly, a policy trained on human demonstrations translated to the robot domain via CycleGAN\ncan generalize to new tasks, but it performs worse than simply using our proposed image masking\nscheme due to the aforementioned errors in image translation (Figure 6 in Appendix A.4). See\nfurther qualitative analyses of individual methods and videos of learned policies on our project\nwebsite.\n7\nFigure 5: Sample BC rollouts for cube grasping in a test environment unseen in robot demonstrations but seen\nin human demonstrations. Here, policies are trained on (a) both robot and human demonstrations with image\nmasking, (b) both robot and human demonstrations with image masking but without conditioning the policy on\ngrasp state, or (c) only robot demonstrations. In (b), the robot does not realize that it has already grasped the\ncube and repeatedly reattempts to do so, ultimately failing to lift the object. In (c), the robot fails to even reach\nthe cube, as this environment background is unseen in the robot demonstrations.\n5.4\nAblation Experiments\nTraining with unmasked images. We remove the image masking entirely to assess whether it is\nan important component of our framework. Given unmasked robot play data where the end-effector\nis now visible, we train an inverse model to predict the dynamics and use the model to infer action\nlabels for unmasked human demonstrations, regardless of the domain shift caused by visual differ-\nences between the human hand and the robot gripper. (Note that we do not use CycleGAN image\ntranslation here.) We train a BC policy on unmasked versions of the robot and human demonstra-\ntions used in our previous experiments, and compare this to our original method.\nTable 2: Ablation experiments results. We ob-\nserve that removing either the image masking or\ngrasp state conditioning generally leads to greatly\nreduced success rates, validating their important\ncontributions to the final generalization perfor-\nmance. Success rates and standard errors are com-\nputed by aggregating the finer-grained results in Ta-\nble 4.\nsuccess rate (%)\nrobot + human w/ mask (ours)\n54.29 \u00b1 5.95\nrobot + human, no mask\n24.29 \u00b1 5.13\nrobot + human w/ mask, no grasp state\n28.57 \u00b1 5.40\nBehavioral cloning without conditioning on\ngrasp state. In a separate ablation, we modify the\nBC policy such that it is no longer conditioned on\nthe binary (open/close) grasp state. We reuse the\nrobot and human demonstrations from the previ-\nous experiments and simply train a new BC policy\nwithout grasp state inputs. Note that we are still\nmasking images here, as in our original framework.\nResults. As shown in Table 2 (detailed results are\nshown in Table 4), removing either image masking\nor grasp state reduces overall performance. Quali-\ntatively, the policy often fails to even reach the tar-\nget object in several cases when using unmasked\nimages; we attribute this to the distribution shift between human and robot observations. Without\nconditioning on grasp state, a common failure mode we observe is repeatedly attempting to grasp\nan object rather than lifting it, as the robot does not know that it has already secured the object (an\nillustration of this behavior is shown in Figure 5(b)). Overall, both components here are important\nto successfully leverage eye-in-hand human video demonstrations.\n6\nConclusion\nThis work presents a novel yet simple framework for leveraging diverse eye-in-hand human video\ndemonstrations and displays its potential to enhance the generalization of vision-based manipulators.\nWe utilize eye-in-hand cameras and image masking to largely close the domain gap between human\nand robot data and bypass explicit domain adaptation entirely. Our framework enables an imitation\nlearning policy to generalize to new environments and new tasks unseen in the robot demonstrations.\nLimitations and future work. Our image masking scheme may not be as effective if the target\nobject is so minuscule (< 1.5 cm long on each side) that it is not visible in the unmasked portion\nof the image, because it may be difficult for the inverse model to infer actions that manipulate the\nobject due to insufficient visual cues. Optimizing the camera angle so that a smaller portion of\nthe image can be masked could mitigate this issue. Additionally, our method involves collecting a\nrobot play dataset to train the inverse model. While this process is inexpensive (details discussed\nin Appendix A.3.1), in the future we hope to automate play data collection nonetheless, e.g., by\ntraining a BC policy on a small play dataset and sampling actions during inference to encourage\nexploration (as in [58]).\n8\nAcknowledgments\nWe thank Alexander Khazatsky, Tony Zhao, Suraj Nair, Kaylee Burns, Maximilian Du, and other\nmembers of the Stanford IRIS Lab for insightful discussions and helpful feedback. Moo Jin Kim\ngratefully acknowledges the financial support of the Siebel Scholarship. This work was supported\nby ONR grant N00014-21-1-2685.\nReferences\n[1] K. Hsu, M. J. Kim, R. Rafailov, J. Wu, and C. Finn. Vision-based manipulators need to also\nsee from their hands. arXiv preprint arXiv:2203.12677, 2022.\n[2] R. Jangir, N. Hansen, S. Ghosal, M. Jain, and X. Wang. Look closer: Bridging egocentric and\nthird-person views with transformers for robotic manipulation. IEEE Robotics and Automation\nLetters, 7(2):3046\u20133053, 2022.\n[3] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese,\nY. Zhu, and R. Mart\u00b4\u0131n-Mart\u00b4\u0131n. What matters in learning from offline human demonstrations\nfor robot manipulation. arXiv preprint arXiv:2108.03298, 2021.\n[4] R. Julian, B. Swanson, G. S. Sukhatme, S. Levine, C. Finn, and K. Hausman. Never stop\nlearning: The effectiveness of fine-tuning in robotic reinforcement learning. arXiv preprint\narXiv:2004.10190, 2020.\n[5] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv\npreprint arXiv:2212.06817, 2022.\n[6] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z:\nZero-shot task generalization with robotic imitation learning. In Conference on Robot Learn-\ning, pages 991\u20131002. PMLR, 2022.\n[7] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,\nY. Sulsky, J. Kay, J. T. Springenberg, et al.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022.\n[8] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from\ndemonstration. Robotics and autonomous systems, 57(5):469\u2013483, 2009.\n[9] A. G. Billard, S. Calinon, and F. Guenter.\nDiscriminative and adaptive imitation in uni-\nmanual and bi-manual tasks.\nRobotics and Autonomous Systems, 54(5):370\u2013384, 2006.\nISSN 0921-8890.\ndoi:https://doi.org/10.1016/j.robot.2006.01.007.\nURL https://www.\nsciencedirect.com/science/article/pii/S0921889006000170. The Social Mecha-\nnisms of Robot Programming from Demonstration.\n[10] B. Browning, L. Xu, and M. M. Veloso. Skill acquisition and use for a dynamically-balancing\nsoccer robot. In AAAI Conference on Artificial Intelligence, 2004.\n[11] J. Chen and A. Zelinsky. Programing by demonstration: Coping with suboptimal teaching\nactions. The International Journal of Robotics Research, 22(5):299\u2013319, 2003. doi:10.1177/\n0278364903022005002. URL https://doi.org/10.1177/0278364903022005002.\n[12] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta,\nE. Orbay, et al. Roboturk: A crowdsourcing platform for robotic skill learning through imita-\ntion. In Conference on Robot Learning, pages 879\u2013893. PMLR, 2018.\n[13] A. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, and E. Berger. Inverted au-\ntonomous helicopter flight via reinforcement learning. Proceedings of the International Sym-\nposium on Experimental Robotics, 01 2004.\n9\n[14] P. Pook and D. Ballard.\nRecognizing teleoperated manipulations.\nIn [1993] Proceedings\nIEEE International Conference on Robotics and Automation, pages 578\u2013585 vol.2, 1993. doi:\n10.1109/ROBOT.1993.291896.\n[15] J. D. Sweeney and R. Grupen. A model of shared grasp affordances from demonstration.\nIn 2007 7th IEEE-RAS International Conference on Humanoid Robots, pages 27\u201335, 2007.\ndoi:10.1109/ICHR.2007.4813845.\n[16] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imita-\ntion learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 5628\u20135635. IEEE, 2018.\n[17] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine. Avid: Learning multi-stage tasks\nvia pixel-level translation of human videos. arXiv preprint arXiv:1912.04443, 2019.\n[18] C. G. Atkeson and S. Schaal. Robot learning from demonstration. In ICML, volume 97, pages\n12\u201320, 1997.\n[19] G. M. Hayes and J. Demiris. A robot controller using learning by imitation. University of\nEdinburgh, Department of Artificial Intelligence, 1994.\n[20] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters, et al. An algorithmic\nperspective on imitation learning. Foundations and Trends\u00ae in Robotics, 7(1-2):1\u2013179, 2018.\n[21] Y. Liu, A. Gupta, P. Abbeel, and S. Levine. Imitation from observation: Learning to imitate\nbehaviors from raw video via context translation. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 1118\u20131125. IEEE, 2018.\n[22] P. Sharma, D. Pathak, and A. Gupta. Third-person visual imitation learning via decoupled\nhierarchical controller. Advances in Neural Information Processing Systems, 32, 2019.\n[23] J. Li, T. Lu, X. Cao, Y. Cai, and S. Wang. Meta-imitation learning by watching video demon-\nstrations. In International Conference on Learning Representations, 2021.\n[24] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha, and A. Garg. Learning by watching:\nPhysical imitation of manipulation skills from human videos. In 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 7827\u20137834. IEEE, 2021.\n[25] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In Proceedings of the IEEE international conference on com-\nputer vision, pages 2223\u20132232, 2017.\n[26] S. Bahl, A. Gupta, and D. Pathak.\nHuman-to-robot imitation in the wild.\narXiv preprint\narXiv:2207.09450, 2022.\n[27] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and X. Wang. Graph inverse reinforcement learning\nfrom diverse videos. arXiv preprint arXiv:2207.14299, 2022.\n[28] R. Lee, J. Abou-Chakra, F. Zhang, and P. Corke. Learning fabric manipulation in the real world\nwith human videos. arXiv preprint arXiv:2211.02832, 2022.\n[29] A. Nguyen, D. Kanoulas, L. Muratore, D. G. Caldwell, and N. G. Tsagarakis. Translating\nvideos to commands for robotic manipulation with deep recurrent neural networks. In 2018\nIEEE International Conference on Robotics and Automation (ICRA), pages 3782\u20133788. IEEE,\n2018.\n[30] K. Ramirez-Amaro, M. Beetz, and G. Cheng. Transferring skills to humanoid robots by ex-\ntracting semantic representations from observations of human activities. Artificial Intelligence,\n247:95\u2013118, 2017.\n10\n[31] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot learning manipulation action plans\nby \u201dwatching\u201d unconstrained videos from the world wide web. In Proceedings of the AAAI\nConference on Artificial Intelligence, 2015.\n[32] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitz-\nmann. Neural descriptor fields: Se (3)-equivariant object representations for manipulation. In\n2022 International Conference on Robotics and Automation (ICRA), pages 6394\u20136400. IEEE,\n2022.\n[33] B. Wen, W. Lian, K. Bekris, and S. Schaal. You only demonstrate once: Category-level ma-\nnipulation from single visual demonstration. In Robotics: Science and Systems (RSS), 2022.\n[34] N. Das, S. Bechtle, T. Davchev, D. Jayaraman, A. Rai, and F. Meier. Model-based inverse\nreinforcement learning from visual demonstrations. arXiv preprint arXiv:2010.09034, 2020.\n[35] P. Sermanet, K. Xu, and S. Levine. Unsupervised perceptual rewards for imitation learning.\narXiv preprint arXiv:1612.06699, 2016.\n[36] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain.\nTime-contrastive networks: Self-supervised learning from video. In 2018 IEEE international\nconference on robotics and automation (ICRA), pages 1134\u20131141. IEEE, 2018.\n[37] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation\nfrom observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557,\n2018.\n[38] S. Yang, W. Zhang, W. Lu, H. Wang, and Y. Li. Learning actions from human demonstration\nvideo for robotic manipulation. In 2019 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1805\u20131811. IEEE, 2019.\n[39] O. Mees, M. Merklinger, G. Kalweit, and W. Burgard. Adversarial skill networks: Unsuper-\nvised robot skill learning from video. In 2020 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 4188\u20134194. IEEE, 2020.\n[40] K. Schmeckpeper, O. Rybkin, K. Daniilidis, S. Levine, and C. Finn.\nReinforcement\nlearning with videos:\nCombining offline observations with interaction.\narXiv preprint\narXiv:2011.06507, 2020.\n[41] A. S. Chen, S. Nair, and C. Finn. Learning generalizable robotic reward functions from\u201d in-\nthe-wild\u201d human videos. arXiv preprint arXiv:2103.16817, 2021.\n[42] Y. Zhou, Y. Aytar, and K. Bousmalis.\nManipulator-independent representations for visual\nimitation. arXiv preprint arXiv:2103.09016, 2021.\n[43] M. Alakuijala, G. Dulac-Arnold, J. Mairal, J. Ponce, and C. Schmid. Learning reward functions\nfor robotic manipulation by observing humans. arXiv preprint arXiv:2211.09019, 2022.\n[44] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen-\ntation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n[45] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, and D. Dwibedi. Xirl: Cross-embodiment\ninverse reinforcement learning. In Conference on Robot Learning, pages 537\u2013546. PMLR,\n2022.\n[46] A. Handa, K. Van Wyk, W. Yang, J. Liang, Y.-W. Chao, Q. Wan, S. Birchfield, N. Ratliff, and\nD. Fox. Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system. In 2020\nIEEE International Conference on Robotics and Automation (ICRA), pages 9164\u20139170. IEEE,\n2020.\n11\n[47] Y. Qin, Y.-H. Wu, S. Liu, H. Jiang, R. Yang, Y. Fu, and X. Wang. Dexmv: Imitation learning\nfor dexterous manipulation from human videos. arXiv preprint arXiv:2108.05877, 2021.\n[48] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto.\nDexterous imitation made\neasy: A learning-based framework for efficient dexterous manipulation.\narXiv preprint\narXiv:2203.13251, 2022.\n[49] Y. Qin, H. Su, and X. Wang. From one hand to multiple hands: Imitation learning for dexterous\nmanipulation from single-camera teleoperation. arXiv preprint arXiv:2204.12490, 2022.\n[50] A. Sivakumar, K. Shaw, and D. Pathak. Robotic telekinesis: learning a robotic hand imitator\nby watching humans on youtube. arXiv preprint arXiv:2202.10448, 2022.\n[51] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made\neasy. In Conference on Robot Learning (CoRL), 2020.\n[52] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-\nloop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3):\n4978\u20134985, 2020.\n[53] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine. Combining self-\nsupervised learning and imitation for vision-based rope manipulation. In 2017 IEEE interna-\ntional conference on robotics and automation (ICRA), pages 2146\u20132153. IEEE, 2017.\n[54] A. Wang, T. Kurutach, K. Liu, P. Abbeel, and A. Tamar. Learning robotic manipulation through\nvisual planning and acting. arXiv preprint arXiv:1905.04411, 2019.\n[55] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet. Learning\nlatent plans from play. In Conference on robot learning, pages 1113\u20131132. PMLR, 2020.\n[56] D. P. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[57] M. Bain and C. Sammut. A framework for behavioural cloning. In Machine Intelligence 15,\npages 103\u2013129, 1995.\n[58] R. Dinyari, P. Sermanet, and C. Lynch. Learning to play by imitating humans. arXiv preprint\narXiv:2006.06874, 2020.\n[59] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. Advances in neural information processing systems, 32, 2019.\n12\nA\nAppendix\nA.1\nModel Architectures\nIn this section, we discuss model architecture details. We implement and train all models using\nPyTorch [59].\nA.1.1\nInverse Dynamics Model Architecture\nThe inverse dynamics model is a convolutional neural network with 4 convolutional layers followed\nby 2 feedforward layers. Each convolutional and feedforward layer is followed by a batch normal-\nization layer and a ReLU activation layer. For every convolutional layer, the number of convolutional\nfilters is 128, kernel size is 3, stride is 1 (except for the first layer, whose stride is 2), and padding is\n0. The latent embedding size of the second feedforward layer is 200. We use early fusion, i.e., two\nconsecutive image observations are concatenated channel-wise and then fed into the first convolu-\ntional layer. The full network outputs an action prediction that takes the agent from one observation\nto the next timestep\u2019s observation, where the action is 3-DoF or 6-DoF with an additional binary\ngripper action. (Recall that we use 6-DoF position and orientation control for the toy packing task,\nand 3-DoF position control for the other tasks.)\nWe train every inverse model with random shifts data augmentation. For every pair of 100 \u00d7 100\nimage observations, we pad each side by 4 pixels and randomly crop a 100 \u00d7 100 region out of the\nresult. The same augmentation is applied to both images in a given pair so as to not perturb the\noriginal dynamics captured in the images. We only apply this augmentation with 80% probability,\nas we found that the resulting model is as accurate as one trained with 100% probability, yet it trains\nfaster because it does not need to compute the augmentation 20 percent of the time.\nA.1.2\nBehavioral Cloning Policy Network Architecture\nThe BC policy network consists of an image encoder with mostly the same architecture as the inverse\nmodel, except that the number of convolutional filters per layer is 32, and the hidden size of the\nsecond feedforward layer is 50. Unlike the inverse model, the policy network acts on one image\nat a time rather than a pair. After the image encoder portion, the policy network consists of an\nadditional two feedforward layers (with a latent dimensionality of 64) representing the policy head.\nFurther, the policy is conditioned on a 1-dimensional grasp state variable as described in Section\n4.3; this variable is concatenated with the 50-dimensional latent embedding output by the second\nfeedforward layer of the image encoder, and the resulting 51-dimensional embedding is passed on\nto the policy head, which outputs an action prediction that best imitates the expert demonstrator\u2019s\naction given some input observation.\nAs with the inverse model, we apply random shifts data augmentation while training the BC policy.\nA.2\nTasks\nIn this section, we discuss the tasks introduced in Section 5 in more detail. All tasks involve 3-DoF\nposition control (and 1-DoF binary gripper control), except for the toy packing tasks, which involve\n6-DoF position/orientation control.\nA.2.1\nEnvironment Generalization Tasks\nThe tasks used for the environment generalization experiments include the following:\n\u2022 reaching: The goal is to reach the end-effector towards the red cube. The environment\ncontains just the red cube; or the red cube and a blue cube distractor; or the red cube\nand a green sponge distractor; or all three objects. The initial positions of the objects are\nrandomized within a 50 \u00d7 50 cm section of the environment.\n13\n\u2022 cube grasping: The goal is to grasp the red cube and lift it off the ground. The cube is the\nonly object in the environment. The environment background can be one of seven: plain\nwhite background, rainbow floral texture, green floral texture, blue floral texture, orange\nplate, green plate, or blue plate. The initial position of the cube is randomized within a\n30 \u00d7 30 cm section of the environment.\n\u2022 plate clearing: The goal is to grasp a target object resting on a plate, lift it up, and transfer\nit to a location off to the right of the plate. The target object is either a green sponge, yellow\nsponge, blue towel, or pink towel. The initial position of the target object is randomized\nwithin a 20 \u00d7 20 cm section of the plate.\n\u2022 toy packing: The goal is to maneuver around a wall, reach towards the toy, grasp it, lift\nit up, move over to the open box, and release the toy into the box. In the initial environ-\nment setting, the end-effector is positioned behind a thin cardboard box (which we call the\n\u201cwall\u201d) such that the toy is fully occluded in the eye-in-hand image observations at the be-\nginning of the episode. There are twelve types of toys that our policies are evaluated against\n(see Figure 3 for pictures of the toys): black suit vampire toy, white mummy toy, orange\nbody jack-o\u2019-lantern toy, red cape vampire toy, purple body green zombie toy, crazy witch\ntoy, green body jack-o\u2019-lantern toy, purple body jack-o\u2019-lantern, red dentures w/ USA hat\ntoy, green Christmas tree toy, Santa Claus toy, and brown reindeer toy. The initial positions\nof the toy, end-effector, and two boxes are all randomized within a 40 \u00d7 10 cm section of\nthe table, and the toy\u2019s position relative to the boxes is also randomized within a 15 \u00d7 10\ncm section in front of the open box. In addition, the end-effector is initially angled towards\nthe wall, as opposed to being oriented top-down as in the other tasks (see Figure 3 for an\nillustration). Therefore, 6-DoF control is necessary for solving the task.\nA.2.2\nTask Generalization Tasks\nWe now describe the tasks used in the task generalization experiments:\n\u2022 cube stacking: The goal is to grasp the red cube, lift it up, stack it on top of the blue\ncube, and release the red cube. The initial positions of the cubes are fixed relative to each\nother, but vary relative to the blue floral texture background within a 20 \u00d7 20 section of the\nenvironment. Robot demonstrations perform cube grasping, while human demonstrations\nperform full cube stacking or portions of the task that follow the grasp.\n\u2022 cube pick-and-place: The goal is to grasp the red cube, lift it up, move it over to the green\nplate, and release it onto the plate. The initial positions of the cube and plate are fixed\nrelative to each other, but vary relative to the plain white background within a 20 \u00d7 20\nsection of the environment. Robot demonstrations perform cube grasping, while human\ndemonstrations perform full cube pick-and-place or portions of the task that follow the\ngrasp.\n\u2022 plate clearing: The goal is the same as described earlier for the plate clearing environment\ngeneralization task. However, here we only manipulate one target object: the green sponge.\nThe initial position of the sponge is randomized within a 20 \u00d7 20 cm section of the plate.\nRobot demonstrations perform sponge grasping, while human demonstrations perform full\nplate clearing or portions of the task that follow the grasp.\n\u2022 toy packing: The goal is the same as described earlier for the toy packing environment\ngeneralization task. However, here we only manipulate one target object: the black suit\nvampire toy. As before, the initial position of the toy, end-effector, and two boxes are ran-\ndomized within a 40\u00d710 cm section of the table, and the toy\u2019s position relative to the boxes\nis also randomized within a 15 \u00d7 10 cm section in front of the open box. Robot demon-\nstrations perform toy grasping, while human demonstrations perform full toy packing or\nportions of the task that follow the grasp.\nPlease see our project website for further details and visualizations of data collected in these tasks\nand environments (expand the page using the button at the very bottom).\n14\nA.3\nDatasets\nA.3.1\nRobot Play Datasets\nHow play data is collected. We gather play data in a similar manner as Lynch et al. [55]: a human\nteleoperator controlling a Franka Emika Panda robot arm executes a diverse repertoire of behaviors\nin an environment, exploring the observation and action spaces while interacting with objects in the\nscene. For example, in an environment containing two cubes, the teleoperator may wave the robotic\nend-effector around, reach towards a cube, grasp and lift up a cube, release and drop the cube, stack\none cube on top of the other, and so on. The continuous sequences of observations captured by the\neye-in-hand camera and the actions commanded by the teleoperator are logged and stored into a\nreplay buffer Dr\nplay for inverse model training. See the subsections below and the project website for\nexamples of play datasets.\nWhy play data is easy to collect. The key advantage of using play data is that it is easy to collect\nmeaningful interaction data in large quantities [55] due to the following:\n\u2022 There is no need to frequently reset the manipulator and objects to some initial state (which\nis typically necessary when collecting expert demonstrations).\n\u2022 There is no notion of maximum episode length or time limit (allowing a teleoperator to\nexecute a variety of behaviors in a single contiguous stretch of time, pausing only when\ndesired).\n\u2022 The teleoperator\u2019s knowledge of object affordances leads to interesting interactions with\nobjects (as opposed to a script that executes purely random actions, which leads to slower\nexploration of the interaction space unless the data collection process is manually biased\ntowards more meaningful interactions, as in [53]).\n\u2022 The play behaviors do not have to solve any particular task (which makes it easier to collect\nplay data than expert task-specific demonstrations).\nAs a result, we can quickly collect a play dataset for a given environment, or set of environments,\nthat is sufficient for training the inverse dynamics model. In addition, a single play dataset could in\nprinciple be used to develop an inverse model that is reused for many different downstream tasks,\neffectively amortizing the cost of collecting it.\nDetails on collected play datasets. We collect four robot play datasets and train four corresponding\ninverse models. Each inverse model is shared across one environment generalization experiment\nand one task generalization experiment. We discuss the details of each play dataset below:\n\u2022 reaching and cube stacking dataset: We collect 20,000 timesteps of play data at 5 Hz\n(approximately 67 minutes) in an environment with a blue floral background and three\nobjects: a red cube, a blue cube, and a green sponge. The play data behaviors include\nwaving the end-effector around, reaching towards each object, grasping and lifting up each\nobject, releasing and dropping an object, stacking an object on top of another, and so on.\nThis play dataset is shared for the reaching environment generalization and cube stacking\ntask generalization tasks.\n\u2022 cube grasping and cube pick-and-place dataset: We collect 52,400 steps of play data at\n5 Hz (approximately 171 minutes) in multiple environments containing a red cube, each\nhaving a different background that the red cube rests on: plain white background, rainbow\nfloral texture, green floral texture, blue floral texture, orange plate, green plate, or blue\nplate. The play data behaviors include waving the end-effector around, reaching towards\nthe cube, grasping and lifting up the cube, releasing and dropping the cube, and so on. This\nplay dataset is shared for the cube grasping environment generalization and cube pick-and-\nplace task generalization tasks.\n\u2022 plate clearing environment generalization and task generalization dataset: We collect\n20,000 steps of play data at 5 Hz (approximately 67 minutes) in multiple environment\n15\nconfigurations, each containing a different target object: green sponge, yellow sponge, blue\ntowel, and pink towel. The play data behaviors include waving the end-effector around,\nreaching toward the objects, grasping and lifting up the objects, releasing and dropping\nthe objects, and so on. This play dataset is shared for both plate clearing environment\ngeneralization and task generalization experiments.\n\u2022 toy packing environment generalization and task generalization dataset: We collect\n10,000 steps of play data at 4 Hz (approximately 42 minutes) in multiple environment\nconfigurations, each containing a different target toy. The toys included in this dataset are\nthe following: white mummy toy, orange body jack-o\u2019-lantern toy, green body jack-o\u2019-\nlantern toy, crazy witch toy, purple body green zombie toy, purple body jack-o\u2019-lantern,\nblack suit vampire toy, red dentures w/ USA hat toy, red cape vampire toy, pirate bomb toy,\ngreen witch w/ broomstick toy, purple dentures w/ eyes toy, eyeball toy, skull toy, and X-\nray skeleton toy. The play data behaviors include waving the end-effector around, reaching\naround the wall and towards the boxes, reaching towards the toys, grasping and lifting up\nthe toys, releasing and dropping the toys, and so on. This play dataset is shared for both\ntoy packing environment generalization and task generalization experiments.\nPlease see our project website for visualizations of these play datasets (expand the page using the\nbutton at the very end).\nA.3.2\nExpert Demonstration Datasets\nIn each environment generalization or task generalization experiment, we collect a set of expert\nrobot demonstrations and a set of expert human demonstrations. Below we discuss details of the\ndatasets collected for each experiment. Please refer to Figure 3 and Figure 4 for a visualization\nof the distribution of environments or tasks that the robot and human datasets are each collected\nfrom. All demonstrations are collected at 5 Hz (or 4 Hz for the toy packing tasks), as is done while\ncollecting the play datasets.\n\u2022 reaching (environment generalization): We collect 60 robot demonstrations with no dis-\ntractor objects and 100 human demonstrations with both the blue cube and green sponge as\ndistractors.\n\u2022 cube grasping (environment generalization): We collect 100 robot demonstrations only\nin an environment with a plain white background and 20 human demonstrations from each\nof the following environment backgrounds: rainbow floral texture, green floral texture, blue\nfloral texture, orange plate, green plate, and blue plate.\n\u2022 plate clearing (environment generalization): We collect 30 robot demonstrations with\njust the green sponge as a target object and 20 human demonstrations with each of the\nfollowing target objects: yellow sponge, blue towel, and pink towel.\n\u2022 toy packing (environment generalization): We collect 100 robot demonstrations with just\nthe black suit vampire toy and 20 human demonstrations with each of the following toys:\nwhite mummy toy, orange body jack-o\u2019-lantern toy, red cape vampire toy, purple body\ngreen zombie toy, and crazy witch toy.\n\u2022 cube stacking (task generalization): We collect 25 robot demonstrations and 130 human\ndemonstrations. The robot demonstrations perform red cube grasping; the human demon-\nstrations perform cube stacking (stack red cube onto blue cube) or portions of the task that\nfollow the grasp. For this task, a majority of the human demonstrations do the latter and\nare thus able to be collected very quickly.\n\u2022 cube pick-and-place (task generalization): We collect 20 robot demonstrations and 70\nhuman demonstrations.\nThe robot demonstrations perform cube grasping; the human\ndemonstrations perform cube pick-and-place (place cube onto plate) or portions of the task\nthat follow the grasp.\n16\n\u2022 plate clearing (task generalization): We collect 40 robot demonstrations and 25 human\ndemonstrations. The robot demonstrations perform sponge grasping; the human demon-\nstrations perform plate clearing (remove green sponge off of plate) or portions of the task\nthat follow the grasp.\n\u2022 toy packing (task generalization): We collect 100 robot demonstrations and 100 human\ndemonstrations. The robot demonstrations perform toy grasping; the human demonstra-\ntions perform toy packing (lift the toy and drop it into the box) or portions of the task that\nfollow the grasp.\nPlease see our project website for visualizations of these expert demonstration datasets (expand the\npage using the button at the very end).\nA.4\nCycleGAN Analysis\nIn Figure 6, we show sample human-to-robot image translations output by CycleGAN [25]. The\ntranslations are successful in some cases but noisy in others. Noisy translations hinder final BC\npolicy performance, resulting in lower performance than simple image masking.\nFigure 6: Sample human-to-robot image translations in various tasks via CycleGAN. Explicit image transla-\ntion provides an alternative approach for leveraging human video demonstrations. In some cases, the human\nembodiment is successfully translated into the robot domain (highlighted in green), even in cases where there\nare five human fingers and two robot fingers (e.g., in the second row, first two columns of images). However,\nin other cases, noticeable visual artifacts exist after the translation (highlighted in yellow in moderate cases, or\nred in severe cases).\n17\nA.5\nInverse Dynamics Model Analysis\nA.5.1\nSample Inverse Dynamics Model Action Predictions\nIn Figure 7, we show sample outputs from the inverse dynamics model when labeling human video\ndemonstrations with actions for the toy packing task. The middle column contains the original\nhuman images. The leftmost column contains human images translated to the robot domain via\nCycleGAN. The rightmost column contains human images masked according to our proposed image\nmasking scheme. We highlight major mistakes made by the inverse model in red. Due to noise in\nthe CycleGAN translations, we see that there exists a significant nonzero rotation component in\nseveral of the action predictions in the leftmost column, which causes the robot gripper to rotate\nexcessively in some cases (we show such behavior in the videos for robot + human w/ CycleGAN\non our project website). In contrast, we avoid such issues using our image masking method.\nFigure 7: Sample inverse dynamics model action predictions.\nA.5.2\nValidating Inverse Dynamics Model Accuracy\nLearning an accurate inverse dynamics model is not unusually challenging given that we leverage\neye-in-hand camera observations in this work. Suppose we have a tuple (o, a, o\u2032), where o represents\nthe current image observation, a represents the current action, and o\u2032 represents the next image\nobservation. Recall that the inverse dynamics modeling problem is to predict the action a giving\nrise to the change in observations. Predicting the action is fairly intuitive in our framework: for\nexample, if an object in the eye-in-hand camera view is moving to the right, we can infer that the\n18\nhand or gripper is moving to the left. The inverse model can use any visual cue in the scene as a\nreference point while learning to predict the dynamics.\nRegardless of the perceived difficulty, there are several ways we can validate the behavior of a\nlearned inverse model. Quantitatively, we can check the performance of the inverse dynamics model\non a validation set (e.g., held-out robot play data). Qualitatively, we can check whether the predicted\nactions for a human video are sensible. For instance, we can verify that the action predictions\nare smooth and not noisy, e.g., by outputting a chunk of observation-action pairs and observing a\ncoherent action trajectory over a continuous stretch of time.\nLastly, we note that, if desired, one could avoid collecting play data and training an inverse model\nby inferring actions via visual odometry or Structure-from-Motion pose estimation methods.\n19\nA.6\nDetailed Experimental Results\nTable 3 contains the full experimental results that were aggregated to produce Table 1 in Section 5.\nAll success rates are evaluated over 10 trials for all tasks (except for the toy packing task, in which\nwe use 20 trials to compute each success rate). Initial object positions are distributed according\nto the configurations described in Appendix A.2. Please see our project website for videos of the\nlearned policies.\nIn addition, Table 4 contains detailed ablation experiment results that were summarized to produce\nTable 2 in Section 5.4.\nTable 3: Full environment generalization experiments results (aggregate results in Table 1). BC policies\nare trained on only robot demonstrations, robot demonstrations and robot play data, robot demonstrations and\nCycleGAN-translated human demonstrations, or robot and human demonstrations with image masking. For\neach task, the robot demonstrations are collected only in the gray italicized environment configuration in the\nsecond column; play data and human demonstrations are collected in the configurations below the dotted lines.\nThus, the non-italicized environment configurations are out-of-distribution with respect to the robot demon-\nstrations, and these are the configurations we focus on. Further, the environment configurations highlighted in\ngreen are not seen in the human demonstrations but are seen in the robot play data, while the configurations\nhighlighted in yellow are not seen at all in either human or robot play/demonstration data (i.e., these are fully\nheld out). Overall, leveraging human demonstrations leads to significantly greater environment generalization\nthan using robot demonstrations alone, and performs better than training on robot play data and CycleGAN-\ntranslated human data as well. Each success rate is computed over 20 test rollouts of the learned policy for the\ntoy packing task, and 10 rollouts for the rest.\nsuccess rate (%)\ntask\nenvironment configuration\nrobot\nrobot + play\nrobot + human\nrobot + human\nw/ CycleGAN\nw/ mask (ours)\nreaching\nno distractors (only red cube)\n90\n90\n80\n90\n+ blue cube distractor\n20\n20\n70\n90\n+ green sponge distractor\n10\n20\n30\n90\n+ blue cube, green sponge distractors\n0\n20\n60\n80\ncube grasping\nwhite background\n90\n90\n100\n90\nrainbow floral background\n0\n30\n10\n80\ngreen floral background\n0\n20\n50\n60\nblue floral background\n0\n30\n70\n60\ncube on orange plate\n0\n20\n40\n40\ncube on green plate\n0\n10\n0\n20\ncube on blue plate\n0\n20\n50\n50\nplate clearing\ngreen sponge on plate\n60\n70\n80\n70\nyellow sponge on plate\n0\n30\n50\n40\nblue towel on plate\n0\n10\n20\n70\npink towel on plate\n0\n30\n70\n60\ntoy packing\nblack suit vampire toy\n85\n70\n80\n85\nwhite mummy toy\n40\n45\n40\n60\norange body jack-o\u2019-lantern toy\n0\n20\n15\n45\nred cape vampire toy\n0\n20\n0\n65\npurple body green zombie toy\n35\n45\n35\n75\ncrazy witch toy\n30\n40\n30\n75\ngreen body jack-o\u2019-lantern w/ hat toy\n0\n25\n0\n55\npurple body jack-o\u2019-lantern toy\n0\n30\n0\n75\nred dentures w/ USA hat toy\n0\n20\n5\n30\ngreen Christmas tree toy\n5\n15\n5\n35\nSanta Claus toy\n0\n35\n25\n65\nbrown reindeer toy\n10\n35\n10\n75\n20\nTable 4: Full ablation experiments results. We test policies on one representative task from the environment\ngeneralization setting (cube grasping) and another from the task generalization setting (plate clearing). The\npolicy trained on unmasked images fails drastically on the last three environment configurations as it never\nreaches the cube. The policy that is not conditioned on the grasp state often encounters a failure mode in which\nit repeatedly reattempts to grasp an object even though it has already grasped it. Such a failure mode occurs\nbecause the robot cannot see the end-effector. Each success rate is computed over 10 test rollouts of the BC\npolicy.\nenvironment generalization\nsuccess rate (%)\ntask\nenvironment configuration\noriginal\nno image masking\nno grasp state\ncube grasping\nwhite background\n90\n40\n90\nrainbow floral background\n80\n60\n50\ngreen floral background\n60\n40\n40\nblue floral background\n60\n40\n20\ncube on orange plate\n40\n0\n0\ncube on green plate\n20\n0\n10\ncube on blue plate\n50\n0\n10\ntask generalization\nsuccess rate (%)\nexperiment\ntask\noriginal\nno image masking\nno grasp state\n1\nsponge grasping\n100\n90\n90\nclearing sponge from plate\n70\n30\n70\n21\n"
  },
  {
    "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
    "link": "https://arxiv.org/pdf/2307.05973.pdf",
    "upvote": "2",
    "text": "VoxPoser: Composable 3D Value Maps\nfor Robotic Manipulation with Language Models\nWenlong Huang1, Chen Wang1, Ruohan Zhang1, Yunzhu Li1,2, Jiajun Wu1, Li Fei-Fei1\n1Stanford University\n2University of Illinois Urbana-Champaign\nAbstract: Large language models (LLMs) are shown to possess a wealth of ac-\ntionable knowledge that can be extracted for robot manipulation in the form of\nreasoning and planning. Despite the progress, most still rely on pre-defined mo-\ntion primitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot trajecto-\nries, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of\nmanipulation tasks given an open-set of instructions and an open-set of objects.\nWe achieve this by first observing that LLMs excel at inferring affordances and\nconstraints given a free-form language instruction. More importantly, by leverag-\ning their code-writing capabilities, they can interact with a vision-language model\n(VLM) to compose 3D value maps to ground the knowledge into the observation\nspace of the agent. The composed value maps are then used in a model-based\nplanning framework to zero-shot synthesize closed-loop robot trajectories with\nrobustness to dynamic perturbations. We further demonstrate how the proposed\nframework can benefit from online experiences by efficiently learning a dynam-\nics model for scenes that involve contact-rich interactions. We present a large-\nscale study of the proposed method in both simulated and real-robot environments,\nshowcasing the ability to perform a large variety of everyday manipulation tasks\nspecified in free-form natural language. Videos and code at voxposer.github.io.\nKeywords: Manipulation, Large Language Models, Model-based Planning\n\u201cTake out bread from toaster\u201d\n\u201cPress down moisturizer pump\u201d\n\u201cTurn open vitamin bottle\u201d\n\u201cTake out a napkin\u201d\n\u201cSet table for pasta\u201d\n\u201cClose top drawer\u201d\n\u201cSweep trash into dustpan\u201d\n\u201cTurn on lamp\u201d\nCode\n</>\nLarge\nLanguage\nModel\nhigh\ncost\nhigh\nreward\n\u201cUnplug charger for phone\u201d\n\u201cHang towel on rack\u201d\n\u201cSort trash to blue tray\u201d\n\u201cMeasure weight of apple\u201d\nVision\nLanguage\nModel\nOpen the top drawer, and\nwatch out for that vase!\nx\ny\nz\n3D Value Map\nMotion Planning\nFigure 1: VOXPOSER extracts language-conditioned affordances and constraints from LLMs and grounds\nthem to the perceptual space using VLMs, using a code interface and without additional training to either com-\nponent. The composed map is referred to as a 3D value map, which enables zero-shot synthesis of trajectories\nfor large varieties of everyday manipulation tasks with an open-set of instructions and an open-set of objects.\nCorrespondence to Wenlong Huang <wenlongh@stanford.edu>.\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2307.05973v2  [cs.RO]  2 Nov 2023\n1\nIntroduction\nLanguage is a compressed medium through which humans distill and communicate their knowledge\nand experience of the world. Large language models (LLMs) have emerged as a promising approach\nto capture this abstraction, learning to represent the world through projection into language space [1\u2013\n4]. While these models are believed to internalize generalizable knowledge as text, it remains a\nquestion about how to use it to enable embodied agents to physically act in the real world.\nWe look at the problem of grounding abstract language instructions (e.g., \u201cset up the table\u201d) in robot\nactions [5]. Prior works have leveraged lexical analysis to parse the instructions [6\u20138], while more\nrecently language models have been used to decompose the instructions into a textual sequence of\nsteps [9\u201311]. However, to enable physical interactions with the environment, existing approaches\ntypically rely on a repertoire of pre-defined motion primitives (i.e., skills) that may be invoked by\nan LLM or a planner, and this reliance on individual skill acquisition is often considered a major\nbottleneck of the system due to the lack of large-scale robotic data. The question then arises: how can\nwe leverage the wealth of internalized knowledge of LLMs at the even fine-grained action level for\nrobots, without requiring laborious data collection or manual designs for each individual primitive?\nIn addressing this challenge, we first note that it is impractical for LLMs to directly output control\nactions in text, which are typically driven by high-frequency control signals in high-dimensional\nspace. However, we find that LLMs excel at inferring language-conditioned affordances and con-\nstraints, and by leveraging their code-writing capabilities, they can compose dense 3D voxel maps\nthat ground them in the visual space by orchestrating perception calls (e.g., via CLIP [12] or open-\nvocabulary detectors [13\u201315]) and array operations (e.g., via NumPy [16]). For example, given an\ninstruction \u201copen the top drawer and watch out for the vase\u201d, LLMs can be prompted to infer: 1)\nthe top drawer handle should be grasped, 2) the handle needs to be translated outwards, and 3) the\nrobot should stay away from the vase. By generating Python code to invoke perception APIs, LLMs\ncan obtain spatial-geometric information of relevant objects or parts and then manipulate the 3D\nvoxels to prescribe reward or cost at relevant locations in observation space (e.g., the handle region\nis assigned high values while the surrounding of the vase is assigned low values). Finally, the com-\nposed value maps can serve as objective functions for motion planners to directly synthesize robot\ntrajectories that achieve the given instruction 1 , without requiring additional training data for each\ntask or for the LLM. An illustration diagram and a subset of tasks we considered are shown in Fig. 1.\nWe term this approach VOXPOSER , a formulation that extracts affordances and constraints from\nLLMs to compose 3D value maps in observation space for guiding robotic interactions. Rather than\nrelying on robotic data that are often of limited amount or variability, the method leverages LLMs\nfor open-world reasoning and VLMs for generalizable visual grounding in a model-based planning\nframework that directly enables physical robot actions. We demonstrate its zero-shot generalization\nfor open-set instructions with open-set objects for various everyday manipulation tasks. We fur-\nther showcase how VoxPoser can also benefit from limited online interactions to efficiently learn a\ndynamics model that involves contact-rich interactions.\n2\nRelated Works\nGrounding Language Instructions.\nLanguage grounding has been studied extensively both in\nterms of intelligent agents [19\u201322] and of robotics [23, 6, 24, 25, 5, 7, 26], where language can be\nused as a tool for compositional goal specification [5, 27\u201333], semantic anchor for training multi-\nmodal representation [12, 34, 35], or as an intermediate substrate for planning and reasoning [36\u2013\n38, 9, 10, 39, 40]. Prior works have looked at using classical tools such as lexical analysis, formal\nlogic, and graphical models to interpret language instructions [27, 7, 6, 26]. More recently, end-\nto-end approaches, popularized by successful applications to offline domains [41\u201343, 1], have been\napplied to directly ground language instructions in robot interactions by learning from data with\n1The approach also bears resemblance and connections to potential field methods in path planning [17] and\nconstrained optimization methods in manipulation planning [18].\n2\nlanguage annotations, spanning from model learning [44], imitation learning [45, 46, 30, 47\u201354], to\nreinforcement learning [55\u201357]. Most closely related to our work is Sharma et al. [50], where an\nend-to-end cost predictor is optimized via supervised learning to map language instructions to 2D\ncostmaps, which are used to steer a motion planner to generate preferred trajectories in a collision-\nfree manner. In contrast, we rely on pre-trained language models for their open-world knowledge\nand tackle the more challenging robotic manipulation in 3D.\nLanguage Models for Robotics.\nLeveraging pre-trained language models for embodied applica-\ntions is an active area of research, where a large body of works focus on planning and reasoning\nwith language models [9\u201311, 58, 31, 39, 59\u201372, 36, 73, 74]. To allow language models to perceive\nthe physical environments, textual descriptions of the scene [39, 11, 59] or perception APIs [75] can\nbe given, vision can be used during decoding [67] or can be directly taken as input by multi-modal\nlanguage models [68, 2]. In addition to perception, to truly bridge the perception-action loop, an\nembodied language model must also know how to act, which typically is achieved by a library of\npre-defined primitives. Liang et al. [75] showed that LLMs exhibit behavioral commonsense that\ncan be useful for low-level control. Despite the promising signs, hand-designed motion primitives\nare still required, and while LLMs are shown to be capable of composing sequential policy logic, it\nremains unclear whether composition can happen at spatial level. A related line of works has also\nexplored using LLMs for reward specification in the context of reward design [76], exploration [77\u2013\n80], and preference learning [81]. For robotic applications, concurrent works explored LLM-based\nreward generation [82\u201388], among which Yu et al. [82] use MuJoCo [89] as a high-fidelity physics\nmodel for model predictive control. In contrast, we focus exclusively on grounding the reward\ngenerated by LLMs in the 3D observation space of the robot.\nLearning-based Trajectory Optimization. Many works have explored leveraging learning-based\napproaches for trajectory optimization. While the literature is vast, they can be broadly categorized\ninto those that learn the models [90\u201398] and those that learn the cost/reward or constraints [99\u2013\n102, 50, 103], where data are typically collected from in-domain interactions. To enable general-\nization in the wild, a parallel line of works has explored learning task specification from large-scale\noffline data [104\u2013106, 35, 34, 44, 107, 108, 54], particularly egocentric videos [109, 110], or lever-\naging pre-trained foundation models [111\u2013113, 33, 114, 115]. The learned cost functions are then\nused by reinforcement learning [111, 108, 116], imitation learning [106, 105], or trajectory optimiza-\ntion [104, 35] to generate robot actions. In this work, we leverage LLMs for zero-shot in-the-wild\ncost specification with superior generalization. Compared to prior works that leverage foundation\nmodels, we ground the cost directly in 3D observation space with real-time visual feedback, which\nmakes VoxPoser amenable to closed-loop MPC that\u2019s robust in execution.\n3\nMethod\nWe first provide the formulation of VoxPoser as an optimization problem (Sec. 3.1). Then we de-\nscribe how VoxPoser can be used as a general zero-shot framework to map language instructions\nto 3D value maps (Sec. 3.2). We subsequently demonstrate how trajectories can be synthesized\nin closed-loop for robotic manipulation (Sec. 3.3).\nWhile zero-shot in nature, we demonstrate\nhow VoxPoser can learn from online interactions to efficiently solve contact-rich tasks (Sec. 3.4).\n3.1\nProblem Formulation\nConsider a manipulation problem given as a free-form language instruction L (e.g., \u201copen the top\ndrawer\u201d). Generating robot trajectories according to L can be very challenging because L may be ar-\nbitrarily long-horizon or under-specified (i.e., requires contextual understanding). Instead, we focus\non individual phases (sub-tasks) of the problem \u2113i that distinctively specify a manipulation task (e.g.,\n\u201cgrasp the drawer handle\u201d, \u201cpull open the drawer\u201d), where the decomposition L \u2192 (\u21131, \u21132, . . . , \u2113n)\nis given by a high-level planner (e.g., an LLM or a search-based planner) 2. The central problem\n2Note that the decomposition and sequencing of these sub-tasks are also done by LLMs in this work, though\nwe do not investigate this aspect extensively as it is not the focus of our contributions.\n3\ndef affordance_map():\nmsize = (100,100,100)\n  map = np.zeros(msize)\n  handles = detect('handle')\n  k = lambda x: x.pos[2]\n  handles.sort(key=k)\ntop_handle = handles[-1]\n  x,y,z = top_handle.pos\n  map[x,y,z] = 1\n  return smooth(map)\ndef constraint_map():\nmsize = (100,100,100)\n  map = np.zeros(msize)\n  vases = detect('vase')\n  vase = vases[0]\n  xyz = vase.occupancy_grid\n  map[xyz] = -1\n  return smooth(map)\n...\nVision\nLanguage\nModel\nOpen the top drawer.\nPlease also watch\nout for that vase!\nLarge\nLanguage\nModel\nAffordance Maps\nConstraint Maps\nView #1\nView #2\nView #1\nView #2\n(a) 3D Value Map Composition\n(b) Motion Planning\nCam #1\nCam #2\nFigure 2: Overview of VOXPOSER. Given the RGB-D observation of the environment and a language in-\nstruction, LLMs generate code, which interacts with VLMs, to produce a sequence of 3D affordance maps and\nconstraint maps (collectively referred to as value maps) grounded in the observation space of the robot (a). The\ncomposed value maps then serve as objective functions for motion planners to synthesize trajectories for robot\nmanipulation (b). The entire process does not involve any additional training.\ninvestigated in this work is to generate a motion trajectory \u03c4 r\ni for robot r and each manipulation\nphase described by instruction \u2113i. We represent \u03c4 r\ni as a sequence of dense end-effector waypoints\nto be executed by an Operational Space Controller [117], where each waypoint consists of a desired\n6-DoF end-effector pose, end-effector velocity, and gripper action. However, it is worth noting that\nother representations of trajectories, such as joint space trajectories, can also be used. Given the i-th\nsub-task described by \u2113i, we formulate an optimization problem defined as follows:\nmin\n\u03c4 r\ni\n{Ftask(Ti, \u2113i) + Fcontrol(\u03c4 r\ni )}\nsubject to\nC(Ti)\n(1)\nwhere Ti is the evolution of environment state, and \u03c4 r\ni \u2286 Ti is the robot trajectory. Ftask scores the\nextent of Ti completes the instruction \u2113i while Fcontrol specifies the control costs, e.g., to encourage\n\u03c4 r\ni to minimize total control effort or total time. C(Ti) denotes the dynamics and kinematics con-\nstraints, which are enforced by the known model of the robot and a physics-based or learning-based\nmodel of the environment. By solving this optimization for each sub-task \u2113i, we obtain a sequence\nof robot trajectories that collectively achieve the overall task specified by the instruction L.\n3.2\nGrounding Language Instruction via VoxPoser\nCalculating Ftask with respect to free-form language instructions is extremely challenging, not only\nbecause of the rich space of semantics language can convey but also because of the lack of robot data\nlabeled with T and \u2113. However, we provide a critical observation that a large number of tasks can\nbe characterized by a voxel value map V \u2208 Rw\u00d7h\u00d7d in robot\u2019s observation space, which guides the\nmotion of an \u201centity of interest\u201d in the scene, such as the robot end-effector, an object, or an object\npart. For example, consider the task \u201copen the top drawer\u201d and its first sub-task \u201cgrasp the top\ndrawer handle\u201d (inferred by LLMs) in Fig. 2. The \u201centity of interest\u201d is the robot end-effector, and\nthe voxel value map should reflect the attraction toward the drawer handle. By further commanding\n\u201cwatch out for the vase\u201d, the map can also be updated to reflect the repulsion from the vase. We\ndenote the \u201centity of interest\u201d as e and its trajectory as \u03c4 e. Using this voxel value map for a given\ninstruction \u2113i, Ftask can be approximated by accumulating the values of e traversing through Vi,\nformally calculated as Ftask = \u2212 P|\u03c4 e\ni |\nj=1 V(pe\nj), where pe\nj \u2208 N3 is the discretized (x, y, z) position\nof e at step j.\nNotably, we observe large language models, by being pre-trained on Internet-scale data, exhibit ca-\npabilities not only to identify the \u201centity of interest\u201d but also to compose value maps that accurately\nreflect the task instruction by writing Python programs. Specifically, when an instruction is given\nas a comment in the code, LLMs can be prompted to 1) call perception APIs (which invoke vision-\nlanguage models (VLM) such as an open-vocabulary detector [13\u201315]) to obtain spatial-geometrical\ninformation of relevant objects, 2) generate NumPy operations to manipulate 3D arrays, and 3) pre-\nscribe precise values at relevant locations. We term this approach as VOXPOSER. Concretely, we\naim to obtain a voxel value map Vt\ni = VoxPoser(ot, \u2113i) by prompting an LLM and executing the\ncode via a Python interpreter, where ot is the RGB-D observation at time t and \u2113i is the current\n4\n\u201csweep the paper trash to the blue dustpan\u201d\n\u201cpush close the top drawer\u201d\n\u201cturn on the lamp\u201d\n\u201copen the vitamin bottle on the right\u201d\n\u201ctake out the bread from the toaster and put it flat on the wooden plate\u201d\nt=1\nt=1\nt=1\nt=1\nt=1\nt=2\nt=2\nt=2\nt=2\nt=2\nt=5\nt=3\nt=3\nt=3\nt=3\nt=3\nt=6\nt=4\nFigure 3: Visualization of composed 3D value maps and rollouts in real-world environments. The top row\ndemonstrates where \u201centity of interest\u201d is an object or part, and the value maps guide them toward target\npositions. The bottom two rows showcase tasks where \u201centity of interest\u201d is the robot end-effector. The\nbottom-most task involves two phases, which are also orchestrated by LLMs.\ninstruction. Additionally, because V is often sparse, we densify the voxel maps via smoothing\noperations, as they encourage smoother trajectories optimized by motion planners.\nAdditional Trajectory Parametrization. The above formulation of VoxPoser uses LLMs to com-\npose V : N3 \u2192 R to map from discretized coordinates in voxel space to a real-valued \u201ccost\u201d, which\nwe can use to optimize a path consisting only of the positional terms. To extend to SE(3) poses,\nwe can also use LLMs to compose rotation maps Vr : N3 \u2192 SO(3) at coordinates relevant to the\ntask objectives (e.g., \u201cend-effector should face the support normal of the handle\u201d). Similarly, we\nfurther compose gripper maps Vg : N3 \u2192 {0, 1} to control gripper open/close and velocity maps\nVv : N3 \u2192 R to specify target velocities. Note that while these additional trajectory parametriza-\ntions are not mapped to a real-valued \u201ccost\u201d, they can also be factored in the optimization procedure\n(Equation 1) to parametrize the trajectories.\n3.3\nZero-Shot Trajectory Synthesis with VoxPoser\nAfter obtaining the task cost Ftask, we can now approach the full problem defined in Equation 1\nto plan a motion trajectory. We use simple zeroth-order optimization by randomly sampling trajec-\ntories and scoring them with the proposed objective. The optimization is implemented in a model\npredictive control framework that iteratively replans the trajectory at every step using the current\nobservation to robustly execute the trajectories even under dynamic disturbances 3 , where either\na learned or physics-based model can be used. However, because VoxPoser effectively provides\n\u201cdense rewards\u201d in the observation space and we are able to replan at every step, we surprisingly\nfind that the overall system can already achieve a large variety of manipulation tasks considered in\nthis work even with simple heuristics-based models. Since some value maps are defined over \u201centity\nof interest\u201d, which may not necessarily be the robot, we also use the dynamics model to find the\nneeded robot trajectory to minimize the task cost (i.e., what interactions between the robot and the\nenvironment achieve the desired object motions).\n3.4\nEfficient Dynamics Learning with Online Experiences\nWhile Sec. 3.3 presents a zero-shot framework for synthesizing trajectories for robot manipula-\ntion, VoxPoser can also benefit from online experiences by efficiently learning a dynamics model.\nConsider the standard setup where a robot interleaves between 1) collecting environment transition\ndata (ot, at, ot+1), where ot is the environment observation at time t and at = MPC(ot), and 2)\ntraining a dynamics model g\u03b8 parametrized by \u03b8 by minimizing the L2 loss between predicted next\n3Although involving an LLM in the loop, closed-loop execution is possible because the generated code\nremains the same throughout task \u2113i, which allows us to cache its output for the current task.\n5\nobservation \u02c6ot+1 and ot+1. A critical component that determines the learning efficiency is the ac-\ntion sampling distribution P(at|ot) in MPC, which typically is a random distribution over the full\naction space A. This is often inefficient when the goal is to solve a particular task, such as opening\na door, because most actions do not interact with the relevant objects in the scene (i.e., the door\nhandle) nor do they necessarily interact with the objects in a meaningful way (i.e., pressing down\nthe door handle). Since VoxPoser synthesizes robot trajectories with LLMs, which have a wealth of\ncommonsense knowledge, the zero-shot synthesized trajectory \u03c4 r\n0 can serve as a useful prior to bias\nthe action sampling distribution P(at|ot, \u03c4 r\n0), which can significantly speed up the learning process.\nIn practice, this can be implemented by only sampling actions in the vicinity of \u03c4 r\n0 by adding small\nnoise \u03b5 to encourage local exploration instead of exploring in the full action space A.\n4\nExperiments and Analysis\nWe first discuss our implementation details. Then we validate VoxPoser for real-world everyday ma-\nnipulation (Sec. 4.1). We also study its generalization in simulation (Sec. 4.2). We further demon-\nstrate how VoxPoser enables efficient learning of more challenging tasks (Sec. 4.3). Finally, we\nanalyze its source of errors and discuss how improvement can be made (Sec. 4.4).\nLLMs and Prompting. We follow prompting structure by Liang et al. [75], which recursively calls\nLLMs using their own generated code, where each language model program (LMP) is responsible\nfor a unique functionality (e.g., processing perception calls). We use GPT-4 [2] from OpenAI API.\nFor each LMP, we include 5-20 example queries and corresponding responses as part of the prompt.\nAn example can be found in Fig. 2 (simplified for clarity). Full prompts are in Appendix.\nVLMs and Perception. Given an object/part query from LLMs, we first invoke open-vocab detector\nOWL-ViT [15] to obtain a bounding box, then feed it into Segment Anything [118] to obtain a mask,\nand finally track the mask using video tracker XMEM [119]. The tracked mask is used with RGB-D\nobservation to reconstruct the object/part point cloud.\nValue Map Composition. We define the following types of value maps: affordance, avoidance, end-\neffector velocity, end-effector rotation, and gripper action. Each type uses a different LMP, which\ntakes in an instruction and outputs a voxel map of shape (100, 100, 100, k), where k differs for each\nvalue map (e.g., k = 1 for affordance and avoidance as it specifies cost, and k = 4 for rotation as\nit specifies SO(3)). We apply Euclidean distance transform to affordance maps and Gaussian filters\nfor avoidance maps. On top of value map LMPs, we define two high-level LMPs to orchestrate their\nbehaviors: planner takes user instruction L as input (e.g., \u201copen drawer\u201d) and outputs a sequence\nof sub-tasks \u21131:N, and composer takes in sub-task \u2113i and invokes relevant value map LMPs with\ndetailed language parameterization.\nMotion Planner.\nWe consider only affordance and avoidance maps in the planner optimization,\nwhich finds a sequence of collision-free end-effector positions p1:N \u2208 R3 using greedy search. Then\nwe enforce other parametrization at each p by the remaining value maps (e.g., rotation map, velocity\nmap). The cost map used by the motion planner is computed as the negative of the weighted sum\nof normalized affordance and avoidance maps with weights 2 and 1. After a 6-DoF trajectory is\nsynthesized, the first waypoint is executed, and then a new trajectory is re-planned at 5 Hz.\nDynamics Model. We use the known robot dynamics model in all tasks, where it is used in motion\nplanning for the end-effector to follow the waypoints. For the majority of our considered tasks where\nthe \u201centity of interest\u201d is the robot, no environment dynamics model is used (i.e., scene is assumed\nto be static), but we replan at every step to account for the latest observation. For tasks in which\nthe \u201centity of interest\u201d is an object, we study only a planar pushing model parametrized by contact\npoint, push direction, and push distance. We use a heuristic-based dynamics model that translates\nan input point cloud along the push direction by the push distance. We use MPC with random\nshooting to optimize for the action parameters. Then a pre-defined pushing primitive is executed\nbased on the action parameters. However, we note that a primitive is not necessary when action\nparameters are defined over the end-effector or joint space of the robot, which would likely yield\n6\nLLM + Prim. [75]\nVoxPoser\nTask\nStatic\nDist.\nStatic\nDist.\nMove & Avoid\n0/10\n0/10\n9/10\n8/10\nSet Up Table\n7/10\n0/10\n9/10\n7/10\nClose Drawer\n0/10\n0/10\n10/10\n7/10\nOpen Bottle\n5/10\n0/10\n7/10\n5/10\nSweep Trash\n0/10\n0/10\n9/10\n8/10\nTotal\n24.0%\n0.0%\n88.0% 70.0%\nTable 1: Success rate in real-world domain. Vox-\nPoser performs everyday manipulation tasks with\nhigh success and is more robust to disturbances\nthan the baseline using action primitives.\nU-Net\nLanguage Models\nTrain/Test\nCategory\nMP [50] Prim. [75] MP (Ours)\nSI SA\nObject Int.\n21.0%\n41.0%\n64.0%\nSI SA\nComposition 53.8%\n43.8%\n77.5%\nSI UA\nObject Int.\n3.0%\n46.0%\n60.0%\nSI UA\nComposition\n3.8%\n25.0%\n58.8%\nUI UA\nObject Int.\n0.0%\n17.5%\n65.0%\nUI UA\nComposition\n0.0%\n25.0%\n76.7%\nTable 2: Success rate in simulated domain. \u201cSI\u201d and \u201cUI\u201d\nare seen and unseen instructions. \u201cSA\u201d and \u201cUA\u201d are seen\nand unseen attributes. VoxPoser outperforms both base-\nlines across 13 tasks from two categories on both seen and\nunseen tasks and maintains similar success rates.\nsmoother trajectories but takes more time for optimization. We also explore the use of a learning-\nbased dynamics model in Section 4.3, which enables VoxPoser to benefit from online experiences.\n4.1\nVoxPoser for Everyday Manipulation Tasks\nWe study whether VoxPoser can zero-shot synthesize robot trajectories to perform everyday manip-\nulation tasks in the real world. Details of the environment setup can be found in Appendix A.4.\nWhile the proposed method can generalize to an open-set of instructions and an open-set of objects\nas shown in Fig. 1, we pick 5 representative tasks to provide quantitative evaluations in Table 1.\nQualitative results including environment rollouts and value map visualizations are shown in Fig. 3.\nWe find that VoxPoser can effectively synthesize robot trajectories for everyday manipulation tasks\nwith a high average success rate. Due to fast replanning capabilities, it is also robust to external dis-\nturbances, such as moving targets/obstacles and pulling the drawer open after it has been closed by\nthe robot. We further compare to a variant of Code as Policies [75] that uses LLMs to parameterize a\npre-defined list of simple primitives (e.g., move to pose, open gripper). We find that compared to\nchaining sequential policy logic, the ability to compose spatially while considering other constraints\nunder a joint optimization scheme is a more flexible formulation, unlocking the possibility for more\nmanipulation tasks and leading to more robust execution.\n4.2\nGeneralization to Unseen Instructions and Attributes\nTo provide rigorous quantitative evaluations on generalization, we set up a simulated block-world\nenvironment that mirrors our real-world robot setup [120, 121] but features 13 highly-randomizable\ntasks with 2766 unique instructions. Eash task comes with a templated instruction (e.g., \u201cpush\n[obj] to [pos]\u201d) that contains randomizable attributes chosen from a pre-defined list. Details are\nin Appendix A.5. Seen instructions/attributes may appear in the prompt (or in the training data\nfor supervised baselines). The tasks are grouped into 2 categories, where \u201cObject Interactions\u201d are\ntasks that require interactions with objects, and \u201cSpatial Composition\u201d are tasks involving spatial\nconstraints (e.g., moving slower near a particular object). For baselines, we ablate the two com-\nponents of VoxPoser, LLM and motion planner, by comparing to a variant of [75] that combines\nan LLM with primitives and to a variant of [50] that learns a U-Net [122] to synthesize costmaps\nfor motion planning. Table 2 shows the success rates averaged across 20 episodes per task. We\nfind VoxPoser exhibits superior generalization in all scenarios. Compared to learned cost specifica-\ntion, LLMs generalize better by explicitly reasoning about affordances and constraints. On the other\nhand, grounding LLM knowledge in robot perception through value map composition rather than\ndirectly specifying primitive parameters offers more flexibility and better generalization.\n4.3\nEfficient Dynamics Learning with Online Experiences\nAs discussed in Sec. 3.4, we investigate how VoxPoser can optionally benefit from online experi-\nences for tasks that involve more intricacies of contact, such as opening doors, fridges, and windows,\nin a simulated environment. Specifically, we first synthesize k zero-shot trajectories using VoxPoser,\n7\nZero-Shot\nNo Prior\nw/ Prior\nTask\nSuccess\nSuccess Time(s)\nSuccess\nTime(s)\nDoor\n6.7%\u00b14.4% 58.3\u00b14.4%\nTLE 88.3%\u00b11.67%142.3\u00b122.4\nWindow 3.3%\u00b13.3% 36.7%\u00b11.7% TLE\n80.0%\u00b12.9% 137.0\u00b17.5\nFridge 18.3%\u00b13.3%70.0%\u00b12.9% TLE\n91.7%\u00b14.4% 71.0\u00b14.4\nTable 3: VoxPoser enables efficient dynamics learning by\nusing zero-shot synthesized trajectories as prior. TLE (time\nlimit exceeded) means exceeding 12 hours. Results are re-\nported over 3 runs different seeds.\nFigure 4: Error breakdown of components. Vox-\nPoser significantly reduces specification error.\neach represented as a sequence of end-effector waypoints, that act as priors for exploration (e.g.,\n\u201chandle needs to be pressed down first in order to open a door\u201d). Then an MLP dynamics model is\nlearned through an iterative procedure where the agent alternates between data collection and model\nlearning. During data collection, we add \u03b5 \u223c N(0, \u03c32) to each waypoint in \u03c4 r\n0 to encourage local\nexploration. As shown in Tab. 3, we find zero-shot synthesized trajectories are typically meaningful\nbut insufficient. However, we can learn an effective dynamics model with less than 3 minutes of\nonline interactions by using these trajectories as exploration prior, leading to high eventual success\nrates. In comparison, exploring without prior all exceed the maximum 12-hour limit.\n4.4\nError Breakdown\nIn this section, we analyze the errors resulting from each component of VoxPoser and how the overall\nsystem can be further improved. We conduct experiments in simulation where we have access to\nground-truth perception and dynamics model (i.e., the simulator). . \u201cDynamics error\u201d refers to errors\nmade by the dynamics model4. \u201cPerception error\u201d refers to errors made by the perception module5.\n\u201cSpecification error\u201d refers to errors made by the module specifying cost or parameters for the low-\nlevel motion planner or primitives. Examples for each method include 1) noisy prediction by the\nU-Net, 2) incorrect parameters specified by the LLM, and 3) incorrect value maps specified by the\nLLM. As shown in Fig. 4, VoxPoser achieves lowest \u201cspecification error\u201d due to its generalization\nand flexibility. We also find that having access to a more robust perception pipeline and a physically-\nrealistic dynamics model can contribute to better overall performance. This observation aligns with\nour real-world experiment, where most errors are from perception. For example, we find that the\ndetector is sensitive to initial poses of objects and is less robust when detecting object parts.\n5\nConclusion, Limitations, & Future Works\nIn this work, we present VOXPOSER, a general framework for extracting affordances and con-\nstraints, grounded in 3D perceptual space, from LLMs and VLMs for everyday manipulation tasks\nin the real world, offering significant generalization advantages for open-set instructions and ob-\njects. Despite compelling results, VoxPoser has several limitations. First, it relies on external per-\nception modules, which is limiting in tasks that require holistic visual reasoning or understand-\ning of fine-grained object geometries. Second, while applicable to efficient dynamics learning, a\ngeneral-purpose dynamics model is still required to achieve contact-rich tasks with the same level of\ngeneralization. Third, our motion planner considers only end-effector trajectories while whole-arm\nplanning is also feasible and likely a better design choice [124\u2013126]. Finally, manual prompt engi-\nneering is required for LLMs. We also see several exciting venues for future work. For instance,\nrecent success of multi-modal LLMs [68, 2, 127] can be directly translated into VoxPoser for direct\nvisual grounding. Methods developed for alignment [128, 129] and prompting [130\u2013133] may be\nused to alleviate prompt engineering effort. Finally, more advanced trajectory optimization methods\ncan be developed that best interface with value maps synthesized by VoxPoser.\n4LLM + Primitives [75] does not use model-based planning, thus not having a dynamics module.\n5U-Net + MP [50] maps RGB-D to costmaps using U-Net [122, 123], thus not having perception module.\nErrors by which are attributed to \u201cspecification error\u201d.\n8\nAcknowledgments\nWe would like to thank Andy Zeng, Igor Mordatch, and the members of the Stanford Vision and\nLearning Lab for the fruitful discussions. This work was in part supported by AFOSR YIP FA9550-\n23-1-0127, ONR MURI N00014-22-1-2740, ONR MURI N00014-21-1-2801, ONR N00014-23-1-\n2355, the Stanford Institute for Human-Centered AI (HAI), JPMC, and Analog Devices. Wenlong\nHuang is partially supported by Stanford School of Engineering Fellowship. Ruohan Zhang is par-\ntially supported by Wu Tsai Human Performance Alliance Fellowship.\nReferences\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877\u20131901, 2020.\n[2] OpenAI. Gpt-4 technical report. arXiv, 2023.\n[3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311, 2022.\n[4] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258, 2021.\n[5] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek. Robots that use language. Annual\nReview of Control, Robotics, and Autonomous Systems, 2020.\n[6] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Under-\nstanding natural language commands for robotic navigation and mobile manipulation. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 25, pages 1507\u20131514,\n2011.\n[7] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions.\nIn 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pages\n259\u2013266. IEEE, 2010.\n[8] M. Bollini, S. Tellex, T. Thompson, N. Roy, and D. Rus. Interpreting and executing recipes\nwith a cooking robot. In Experimental Robotics, pages 481\u2013495. Springer, 2013.\n[9] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In International Conference on Ma-\nchine Learning. PMLR, 2022.\n[10] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano,\nK. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine,\nY. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Ser-\nmanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan.\nDo as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint\narXiv:2204.01691, 2022.\n[11] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sind-\nhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal rea-\nsoning with language. arXiv preprint arXiv:2204.00598, 2022.\n[12] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n9\n[13] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and\nlanguage knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.\n[14] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated\ndetection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1780\u20131790, 2021.\n[15] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Ma-\nhendran, A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection\nwith vision transformers. arXiv preprint arXiv:2205.06230, 2022.\n[16] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,\nE. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,\nM. Brett, A. Haldane, J. F. del R\u00b4\u0131o, M. Wiebe, P. Peterson, P. G\u00b4erard-Marchant, K. Sheppard,\nT. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with\nNumPy. Nature, 585(7825):357\u2013362, Sept. 2020. doi:10.1038/s41586-020-2649-2. URL\nhttps://doi.org/10.1038/s41586-020-2649-2.\n[17] Y. K. Hwang, N. Ahuja, et al. A potential field approach to path planning. IEEE transactions\non robotics and automation, 8(1):23\u201332, 1992.\n[18] M. Toussaint, J. Harris, J.-S. Ha, D. Driess, and W. H\u00a8onig. Sequence-of-constraints mpc:\nReactive timing-optimal control of sequential manipulation. In 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 13753\u201313760. IEEE, 2022.\n[19] J. Andreas, D. Klein, and S. Levine.\nLearning with latent language.\narXiv preprint\narXiv:1711.00482, 2017.\n[20] R. Zellers, A. Holtzman, M. Peters, R. Mottaghi, A. Kembhavi, A. Farhadi, and Y. Choi.\nPiglet: Language grounding through neuro-symbolic interaction in a 3d world. arXiv preprint\narXiv:2106.00188, 2021.\n[21] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi. Merlot:\nMultimodal neural script knowledge models. Advances in Neural Information Processing\nSystems, 2021.\n[22] V. Shwartz, P. West, R. L. Bras, C. Bhagavatula, and Y. Choi. Unsupervised commonsense\nquestion answering with self-talk. arXiv preprint arXiv:2004.05483, 2020.\n[23] T. Winograd. Procedures as a representation for data in a computer program for understanding\nnatural language. Technical report, MASSACHUSETTS INST OF TECH CAMBRIDGE\nPROJECT MAC, 1971.\n[24] V. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding and mapping for natural\nlanguage robot instruction following. arXiv preprint arXiv:2011.07384, 2020.\n[25] S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics.\nRobotics: Science and Systems Foundation, 2014.\n[26] T. Kollar, S. Tellex, D. Roy, and N. Roy. Grounding verbs of motion in natural language\ncommands to robots. In Experimental robotics, pages 31\u201347. Springer, 2014.\n[27] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone. Learning to interpret natural language\ncommands through human-robot dialog. In Twenty-Fourth International Joint Conference on\nArtificial Intelligence, 2015.\n[28] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion, J. Hart, P. Stone,\nand R. Mooney. Jointly improving parsing and perception for natural language commands\nthrough human-robot dialog. Journal of Artificial Intelligence Research, 67:327\u2013374, 2020.\n10\n[29] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn.\nBc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot\nLearning, pages 991\u20131002. PMLR, 2021.\n[30] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control\nat scale. arXiv preprint arXiv:2212.06817, 2022.\n[31] D. Shah, B. Osinski, B. Ichter, and S. Levine. Lm-nav: Robotic navigation with large pre-\ntrained models of language, vision, and action. arXiv preprint arXiv:2207.04429, 2022.\n[32] Y. Cui, S. Karamcheti, R. Palleti, N. Shivakumar, P. Liang, and D. Sadigh. \u201d no, to the right\u201d\u2013\nonline language corrections for robotic manipulation via shared autonomy. arXiv preprint\narXiv:2301.02555, 2023.\n[33] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich,\nF. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language\nmodels. arXiv preprint arXiv:2303.00905, 2023.\n[34] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen-\ntation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n[35] Y. J. Ma, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. Liv: Language-image repre-\nsentations and rewards for robotic control. arXiv e-prints, 2023.\n[36] P. A. Jansen. Visually-grounded planning without vision: Language models infer detailed\nplans from high-level instructions. arXiv preprint arXiv:2009.14259, 2020.\n[37] V. Micheli and F. Fleuret.\nLanguage models are few-shot butlers.\narXiv preprint\narXiv:2104.07972, 2021.\n[38] P. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language.\narXiv preprint arXiv:2110.01517, 2021.\n[39] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mor-\ndatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and\nB. Ichter. Inner monologue: Embodied reasoning through planning with language models. In\narXiv preprint arXiv:2207.05608, 2022.\n[40] B. Z. Li, W. Chen, P. Sharma, and J. Andreas. Lampp: Language models as probabilistic\npriors for perception and action. arXiv e-prints, pages arXiv\u20132302, 2023.\n[41] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale\nHierarchical Image Database. In CVPR09, 2009.\n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolu-\ntional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.\n[43] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[44] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al. Learning language-conditioned\nrobot behavior from offline data and crowd-sourced annotation. In Conference on Robot\nLearning, pages 1303\u20131315. PMLR, 2022.\n[45] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manip-\nulation. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022.\n[46] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.\n11\n[47] S. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch. Pre-\ntrained language models for interactive decision-making. arXiv preprint arXiv:2202.01771,\n2022.\n[48] O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imita-\ntion learning. arXiv preprint arXiv:2204.06252, 2022.\n[49] O. Mees, J. Borja-Diaz, and W. Burgard. Grounding language with visual affordances over\nunstructured data. arXiv preprint arXiv:2210.01911, 2022.\n[50] P. Sharma, B. Sundaralingam, V. Blukis, C. Paxton, T. Hermans, A. Torralba, J. An-\ndreas, and D. Fox. Correcting robot plans with natural language feedback. arXiv preprint\narXiv:2204.05186, 2022.\n[51] W. Liu, C. Paxton, T. Hermans, and D. Fox. Structformer: Learning spatial structure for\nlanguage-guided semantic rearrangement of novel objects. In 2022 International Conference\non Robotics and Automation (ICRA), pages 6322\u20136329. IEEE, 2022.\n[52] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data.\nRobotics: Science and Systems, 2021. URL https://arxiv.org/abs/2005.07648.\n[53] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence.\nInteractive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.\n[54] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg. Concept2robot: Learning manip-\nulation concepts from instructions and human demonstrations. The International Journal of\nRobotics Research, 40(12-14):1419\u20131434, 2021.\n[55] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foerster, J. Andreas, E. Grefenstette, S. Whiteson,\nand T. Rockt\u00a8aschel. A survey of reinforcement learning informed by natural language. In\nIJCAI, 2019.\n[56] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy\nsketches. ArXiv, abs/1611.01796, 2017.\n[57] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn. Language as an abstraction for hierarchical\ndeep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.\n[58] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler.\nOpen-vocabulary queryable scene representations for real world planning. arXiv preprint\narXiv:2209.09874, 2022.\n[59] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models.\narXiv preprint arXiv:2209.11302, 2022.\n[60] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation.\narXiv preprint arXiv:2210.05714, 2022.\n[61] S. S. Raman, V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex. Planning with large\nlanguage models via corrective re-prompting. arXiv preprint arXiv:2211.09935, 2022.\n[62] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. Llm-planner: Few-\nshot grounded planning for embodied agents with large language models. arXiv preprint\narXiv:2212.04088, 2022.\n[63] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+ p: Empowering\nlarge language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477,\n2023.\n12\n[64] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles\nand model abilities. 2023, 2023.\n[65] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language\ninstructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.\n[66] Y. Ding, X. Zhang, C. Paxton, and S. Zhang. Task and motion planning with large language\nmodels for object rearrangement. arXiv preprint arXiv:2303.06247, 2023.\n[67] W. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence, I. Mordatch, S. Levine,\nK. Hausman, et al. Grounded decoding: Guiding text generation with grounded models for\nrobot control. arXiv preprint arXiv:2303.00855, 2023.\n[68] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\n[69] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforce-\nment learning and planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563,\n2023.\n[70] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning\ngoals with large-language models. arXiv preprint arXiv:2302.05128, 2023.\n[71] Y. Lu, P. Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y. Wang. Multimodal procedural planning\nvia dual text-image prompting. arXiv preprint arXiv:2305.01795, 2023.\n[72] D. Patel, H. Eghbalzadeh, N. Kamra, M. L. Iuzzolino, U. Jain, and R. Desai. Pretrained\nlanguage models as visual planners for human assistance. arXiv preprint arXiv:2304.09179,\n2023.\n[73] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandku-\nmar. Voyager: An open-ended embodied agent with large language models. arXiv preprint\narXiv:2305.16291, 2023.\n[74] J. Yang, W. Tan, C. Jin, B. Liu, J. Fu, R. Song, and L. Wang.\nPave the way to grasp\nanything: Transferring foundation models for universal pick-place robots. arXiv preprint\narXiv:2306.05716, 2023.\n[75] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753,\n2022.\n[76] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward design with language models. arXiv\npreprint arXiv:2303.00001, 2023.\n[77] A. Tam, N. Rabinowitz, A. Lampinen, N. A. Roy, S. Chan, D. Strouse, J. Wang, A. Banino,\nand F. Hill. Semantic exploration from language abstractions and pretrained representations.\nAdvances in Neural Information Processing Systems, 35:25377\u201325389, 2022.\n[78] J. Mu, V. Zhong, R. Raileanu, M. Jiang, N. Goodman, T. Rockt\u00a8aschel, and E. Grefenstette.\nImproving intrinsic exploration with language abstractions. arXiv preprint arXiv:2202.08938,\n2022.\n[79] C. Colas, T. Karch, N. Lair, J.-M. Dussoux, C. Moulin-Frier, P. Dominey, and P.-Y. Oudeyer.\nLanguage as a cognitive tool to imagine goals in curiosity driven exploration. Advances in\nNeural Information Processing Systems, 33:3761\u20133774, 2020.\n13\n[80] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas.\nGuiding pretraining in reinforcement learning with large language models. arXiv preprint\narXiv:2302.06692, 2023.\n[81] H. Hu and D. Sadigh. Language instructed reinforcement learning for human-ai coordination.\narXiv preprint arXiv:2304.07297, 2023.\n[82] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L. Chiang, T. Erez,\nL. Hasenclever, J. Humplik, et al. Language to rewards for robotic skill synthesis. arXiv\npreprint arXiv:2306.08647, 2023.\n[83] H. Ha, P. Florence, and S. Song. Scaling up and distilling down: Language-guided robot skill\nacquisition. arXiv preprint arXiv:2307.14535, 2023.\n[84] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and\nA. Anandkumar. Eureka: Human-level reward design via coding large language models.\narXiv preprint arXiv:2310.12931, 2023.\n[85] T. Xie, S. Zhao, C. H. Wu, Y. Liu, Q. Luo, V. Zhong, Y. Yang, and T. Yu. Text2reward:\nAutomated dense reward function generation for reinforcement learning.\narXiv preprint\narXiv:2309.11489, 2023.\n[86] Y. Zeng and Y. Xu. Learning reward for physical skills using large language model. arXiv\npreprint arXiv:2310.14092, 2023.\n[87] J. Rocamonde, V. Montesinos, E. Nava, E. Perez, and D. Lindner. Vision-language models\nare zero-shot reward models for reinforcement learning. arXiv preprint arXiv:2310.12921,\n2023.\n[88] J. Perez, D. Proux, C. Roux, and M. Niemaz. Larg, language-based automatic reward and\ngoal generation. arXiv preprint arXiv:2306.10985, 2023.\n[89] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.\n[90] I. Lenz, R. A. Knepper, and A. Saxena. Deepmpc: Learning deep latent features for model\npredictive control. In Robotics: Science and Systems, volume 10. Rome, Italy, 2015.\n[91] L. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger. Learning-based model pre-\ndictive control: Toward safe learning in control. Annual Review of Control, Robotics, and\nAutonomous Systems, 3:269\u2013296, 2020.\n[92] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based\napproach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.\n[93] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, et al. Interaction networks for learning\nabout objects, relations and physics. Advances in neural information processing systems, 29,\n2016.\n[94] Z. Xu, J. Wu, A. Zeng, J. B. Tenenbaum, and S. Song. Densephysnet: Learning dense physical\nobject representations via multi-step dynamic interactions. arXiv preprint arXiv:1906.03853,\n2019.\n[95] A. Byravan and D. Fox. Se3-nets: Learning rigid body motion using deep neural networks.\nIn 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 173\u2013180.\nIEEE, 2017.\n[96] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar. Deep dynamics models for learning\ndexterous manipulation. In Conference on Robot Learning, pages 1101\u20131112. PMLR, 2020.\n14\n[97] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell,\nand P. Battaglia. Graph networks as learnable physics engines for inference and control. In\nInternational Conference on Machine Learning, pages 4470\u20134479. PMLR, 2018.\n[98] Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for\nmanipulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566,\n2018.\n[99] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via\npolicy optimization. In International conference on machine learning, pages 49\u201358. PMLR,\n2016.\n[100] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement\nlearning. arXiv preprint arXiv:1710.11248, 2017.\n[101] D. Driess, O. Oguz, J.-S. Ha, and M. Toussaint. Deep visual heuristics: Learning feasibility of\nmixed-integer programs for manipulation planning. In 2020 IEEE International Conference\non Robotics and Automation (ICRA), pages 9563\u20139569. IEEE, 2020.\n[102] B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter. Differentiable mpc for end-to-end\nplanning and control. Advances in neural information processing systems, 31, 2018.\n[103] M. Mittal, D. Hoeller, F. Farshidian, M. Hutter, and A. Garg. Articulated object interaction\nin unknown scenes with whole-body mobile manipulation. In 2022 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 1647\u20131654. IEEE, 2022.\n[104] S. Bahl, A. Gupta, and D. Pathak. Human-to-robot imitation in the wild. arXiv preprint\narXiv:2207.09450, 2022.\n[105] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak. Affordances from human videos\nas a versatile representation for robotics. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13778\u201313790, 2023.\n[106] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar.\nMimicplay: Long-horizon imitation learning by watching human play.\narXiv preprint\narXiv:2302.12422, 2023.\n[107] H. Bharadhwaj, A. Gupta, S. Tulsiani, and V. Kumar. Zero-shot robot manipulation from\npassive human videos. arXiv preprint arXiv:2302.02011, 2023.\n[108] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards\nuniversal visual reward and representation via value-implicit pre-training.\narXiv preprint\narXiv:2210.00030, 2022.\n[109] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti,\nJ. Munro, T. Perrett, W. Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In\nProceedings of the European Conference on Computer Vision (ECCV), pages 720\u2013736, 2018.\n[110] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger,\nH. Jiang, M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 18995\u201319012, 2022.\n[111] Y. Cui, S. Niekum, A. Gupta, V. Kumar, and A. Rajeswaran. Can foundation models perform\nzero-shot task specification for robot manipulation? In Learning for Dynamics and Control\nConference, pages 893\u2013905. PMLR, 2022.\n[112] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta,\nB. Ichter, et al. Scaling robot learning with semantically imagined experience. arXiv preprint\narXiv:2302.11550, 2023.\n15\n[113] Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and V. Kumar. Cacti: A\nframework for scalable multi-task multi-scene visual imitation learning.\narXiv preprint\narXiv:2212.05711, 2022.\n[114] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson.\nRobotic skill acquisition via instruction augmentation with vision-language models. arXiv\npreprint arXiv:2211.11736, 2022.\n[115] C. Wang, D. Xu, and L. Fei-Fei. Generalizable task planning through representation pretrain-\ning. IEEE Robotics and Automation Letters, 7(3):8299\u20138306, 2022.\n[116] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, C. Wang, G. Levine,\nM. Lingelbach, J. Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday\nactivities and realistic simulation. In Conference on Robot Learning, pages 80\u201393. PMLR,\n2023.\n[117] O. Khatib.\nA unified approach for motion and force control of robot manipulators: The\noperational space formulation. IEEE Journal on Robotics and Automation, 3(1):43\u201353, 1987.\n[118] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,\nA. C. Berg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\n[119] H. K. Cheng and A. G. Schwing. Xmem: Long-term video object segmentation with an\natkinson-shiffrin memory model. In Computer Vision\u2013ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII, pages 640\u2013658.\nSpringer, 2022.\n[120] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, et al.\nSapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 11097\u201311107, 2020.\n[121] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani. Where2act: From pixels to\nactions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6813\u20136823, 2021.\n[122] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI\n2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings,\nPart III 18, pages 234\u2013241. Springer, 2015.\n[123] \u00a8O. C\u00b8 ic\u00b8ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger. 3d u-net: learn-\ning dense volumetric segmentation from sparse annotation. In Medical Image Computing\nand Computer-Assisted Intervention\u2013MICCAI 2016: 19th International Conference, Athens,\nGreece, October 17-21, 2016, Proceedings, Part II 19, pages 424\u2013432. Springer, 2016.\n[124] L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars. Probabilistic roadmaps for\npath planning in high-dimensional configuration spaces. IEEE transactions on Robotics and\nAutomation, 12(4):566\u2013580, 1996.\n[125] N. D. Ratliff, J. Issac, D. Kappler, S. Birchfield, and D. Fox. Riemannian motion policies.\narXiv preprint arXiv:1801.02854, 2018.\n[126] T. Marcucci, M. Petersen, D. von Wrangel, and R. Tedrake. Motion planning around obstacles\nwith convex optimization. arXiv preprint arXiv:2205.04422, 2022.\n[127] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n16\n[128] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agar-\nwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human\nfeedback. arXiv preprint arXiv:2203.02155, 2022.\n[129] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073, 2022.\n[130] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models.\narXiv preprint arXiv:2201.11903,\n2022.\n[131] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi.\nSelf-instruct: Aligning language model with self generated instructions.\narXiv preprint\narXiv:2212.10560, 2022.\n[132] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-\nshot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n[133] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan.\nTree\nof thoughts:\nDeliberate problem solving with large language models.\narXiv preprint\narXiv:2305.10601, 2023.\n[134] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark &\nlearning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.\n[135] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,\nD. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint\narXiv:2206.07682, 2022.\n[136] T. Gupta and A. Kembhavi.\nVisual programming: Compositional visual reasoning with-\nout training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14953\u201314962, 2023.\n[137] D. Sur\u00b4\u0131s, S. Menon, and C. Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\n[138] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu. Viola: Imitation learning for vision-based manipulation\nwith object proposal priors. 6th Annual Conference on Robot Learning, 2022.\n17\nA\nAppendix\nA.1\nCode Release\nWe provide an open-sourced implementation of VoxPoser at github.com/huangwl18/VoxPoser based\non RLBench [134], as its diversity of tasks and scenes best resembles our real-world setup.\nA.2\nEmergent Behavioral Capabilities\nWhich block is heavier?\nI am left-handed.\nYou\u2019re off by 1cm to the left.\nOpen the drawer precisely by half.\nFigure 5: Emergent behavioral capabilities by VoxPoser inherited from the language model, including behav-\nioral commonsense reasoning (top left), fine-grained language correction (top right), multi-step visual program\n(bottom left), and estimating physical properties of objects (bottom right).\nEmergent capabilities refer to unpredictable phenomenons that are only present in large mod-\nels [135]. As VoxPoser uses pre-trained LLMs as backbone, we observe similar embodied emergent\ncapabilities driven by the rich world knowledge of LLMs. In particular, we focus our study on the\nbehavioral capabilities that are unique to VoxPoser. We observe the following capabilities:\n\u2022 Behavioral Commonsense Reasoning: During a task where robot is setting the table, the\nuser can specify behavioral preferences such as \u201cI am left-handed\u201d, which requires the\nrobot to comprehend its meaning in the context of the task. VoxPoser decides that it should\nmove the fork from the right side of the bowl to the left side.\n\u2022 Fine-grained Language Correction: For tasks that require high precision such as \u201ccov-\nering the teapot with the lid\u201d, the user can give precise instructions to the robot such as\n\u201cyou\u2019re off by 1cm\u201d. VoxPoser similarly adjusts its action based on the feedback.\n\u2022 Multi-step Visual Program [136, 137]: Given a task \u201copen the drawer precisely by half\u201d\nwhere there is insufficient information because object models are not available, VoxPoser\ncan come up with multi-step manipulation strategies based on visual feedback that first\nopens the drawer fully while recording handle displacement, then close it back to the mid-\npoint to satisfy the requirement.\n\u2022 Estimating Physical Properties: Given two blocks of unknown mass, the robot is tasked\nto conduct physics experiments using an existing ramp to determine which block is heav-\nier. VoxPoser decides to push both blocks off the ramp and choose the block traveling the\nfarthest as the heavier block. Interestingly, this mirrors a common human oversight: in\nan ideal, frictionless world, both blocks would traverse the same distance under the influ-\nence of gravity. This serves as a lighthearted example that language models can exhibit\nlimitations similar to human reasoning.\n18\nA.3\nAPIs for VoxPoser\nCentral to VoxPoser is an LLM generating Python code that is executed by a Python interpreter.\nBesides exposing NumPy [16] and the Transforms3d library to the LLM, we provide the following\nenvironment APIs that LLMs can choose to invoke:\ndetect(obj name): Takes in an object name and returns a list of dictionaries, where each dictionary\ncorresponds to one instance of the matching object, containing center position, occupancy grid, and\nmean normal vector.\nexecute(movable,affordance map,avoidance map,rotation map,velocity map,gripper map):\nTakes in an \u201centity of interest\u201d as \u201cmovable\u201d (a dictionary returned by detect) and (optionally)\na list of value maps and invokes the motion planner to execute the trajectory. Note that in MPC\nsettings, \u201cmovable\u201d and the input value maps are functions that can be re-evaluated to reflect the\nlatest environment observation.\ncm2index(cm,direction): Takes in a desired offset distance in centimeters along direction and\nreturns 3-dim vector reflecting displacement in voxel coordinates.\nindex2cm(index,direction): Inverse of cm2index. Takes in an integer \u201cindex\u201d and a \u201cdirection\u201d\nvector and returns the distance in centimeters in world coordinates displaced by the \u201cinteger\u201d in\nvoxel coordinates.\npointat2quat(vector): Takes in a desired pointing direction for the end-effector and returns a\nsatisfying target quaternion.\nset voxel by radius(voxel map,voxel xyz,radius cm,value):\nAssigns \u201cvalue\u201d to voxels\nwithin \u201cradious cm\u201d from \u201cvoxel xyz\u201d in \u201cvoxel map\u201d.\nget empty affordance map(): Returns a default affordance map initialized with 0, where a high\nvalue attracts the entity.\nget empty avoidance map(): Returns a default avoidance map initialized with 0, where a high\nvalue repulses the entity.\nget empty rotation map(): Returns a default rotation map initialized with current end-effector\nquaternion.\nget empty gripper map(): Returns a default gripper map initialized with current gripper action,\nwhere 1 indicates \u201cclosed\u201d and 0 indicates \u201copen\u201d.\nget empty velocity map(): Returns a default affordance map initialized with 1, where the number\nrepresents scale factor (e.g., 0.5 for half of the default velocity).\nreset to default pose(): Reset to robot rest pose.\n19\nA.4\nReal-World Environment Setup\nWe use a Franka Emika Panda robot with a tabletop setup. We use Operational Space Controller\nwith impedance from Deoxys [138]. We mount two RGB-D cameras (Azure Kinect) at two opposite\nends of the table: bottom right and top left from the top down view. At the start of each rollout, both\ncameras start recording and return the real-time RGB-D observations at 20 Hz.\nFor each task, we evaluate each method on two settings: without and with disturbances. For tasks\nwith disturbances, we apply three kinds of disturbances to the environment, which we pre-select a\nsequence of them at the start of the evaluation: 1) random forces applied to the robot, 2) random\ndisplacement of task-relevant and distractor objects, and 3) reverting task progress (e.g., pull drawer\nopen while it\u2019s being closed by the robot). We only apply the third disturbances to tasks where\n\u201centity of interest\u201d is an object or object part.\nWe compare to a variant of Code as Policies [75] as a baseline that uses an LLM with ac-\ntion primitives. The primitives include: move to pos, rotate by quat, set vel, open gripper,\nclose gripper. We do not provide primitives such as pick-and-place as they would be tailored\nfor a particular suite of tasks that we do not constrain to in our study (similar to the control APIs\nfor VoxPoser specified in Sec. A.3).\nA.4.1\nTasks\nMove & Avoid: \u201cMove to the top of [obj1] while staying away from [obj2]\u201d, where [obj1] and [obj2]\nare randomized everyday objects selected from the list: apple, banana, yellow bowl, headphones,\nmug, wood block.\nSet Up Table: \u201cPlease set up the table by placing utensils for my pasta\u201d.\nClose Drawer: \u201cClose the [deixis] drawer\u201d, where [deixis] can be \u201ctop\u201d or \u201cbottom\u201d.\nOpen Bottle: \u201cTurn open the vitamin bottle\u201d.\nSweep Trash: \u201cPlease sweep the paper trash into the blue dustpan\u201d.\n20\nA.5\nSimulated Environment Setup\nWe implement a tabletop manipulation environment with a Franka Emika Panda robot in\nSAPIEN [120]. The controller takes as input a desired end-effector 6-DoF pose, calculates a se-\nquence of interpolated waypoints using inverse kinematics, and finally follows the waypoints using\na PD controller. We use a set of 10 colored blocks and 10 colored lines in addition to an articulated\ncabinet with 3 drawers. They are initialized differently depending on the specific task. The lines are\nused as visual landmarks and are not interactable. For perception, a total of 4 RGB-D cameras are\nmounted at each end of the table pointing at the center of the workspace.\nA.5.1\nTasks\nWe create a custom suite of 13 tasks shown in Table 4. Each task comes with a templated instruction\n(shown in Table 4) where there may be one or multiple attributes randomized from the pre-defined\nlist below. At reset time, a number of objects are selected (depending on the specific task) and are\nrandomized across the workspace while making sure that task is not completed at reset and that task\ncompletion is feasible. A complete list of attributes can be found below, divided into \u201cseen\u201d and\n\u201cunseen\u201d categories:\nSeen Attributes:\n\u2022 [pos]: [\u201cback left corner of the table\u201d, \u201cfront right corner of the table\u201d, \u201cright side of the\ntable\u201d, \u201cback side of the table\u201d]\n\u2022 [obj]: [\u201cblue block\u201d, \u201cgreen block\u201d, \u201cyellow block\u201d, \u201cpink block\u201d, \u201cbrown block\u201d]\n\u2022 [preposition]: [\u201cleft of\u201d, \u201cfront side of\u201d, \u201ctop of\u201d]\n\u2022 [deixis]: [\u201ctopmost\u201d, \u201csecond to the bottom\u201d]\n\u2022 [dist]: [3, 5, 7, 9, 11]\n\u2022 [region]: [\u201cright side of the table\u201d, \u201cback side of the table\u201d]\n\u2022 [velocity]: [\u201cfaster speed\u201d, \u201ca quarter of the speed\u201d]\n\u2022 [line]: [\u201cblue line\u201d, \u201cgreen line\u201d, \u201cyellow line\u201d, \u201cpink line\u201d, \u201cbrown line\u201d]\nUnseen Attributes:\n\u2022 [pos]: [\u201cback right corner of the table\u201d, \u201cfront left corner of the table\u201d, \u201cleft side of the\ntable\u201d, \u201cfront side of the table\u201d]\n\u2022 [obj]: [\u201cred block\u201d, \u201corange block\u201d, \u201cpurple block\u201d, \u201ccyan block\u201d, \u201cgray block\u201d]\n\u2022 [preposition]: [\u201cright of\u201d, \u201cback side of\u201d]\n\u2022 [deixis]: [\u201cbottommost\u201d, \u201csecond to the top\u201d]\n\u2022 [dist]: [4, 6, 8, 10]\n\u2022 [region]: [\u201cleft side of the table\u201d, \u201cfront side of the table\u201d]\n\u2022 [velocity]: [\u201cslower speed\u201d, \u201c3x speed\u201d]\n\u2022 [line]: [\u201cred line\u201d, \u201corange line\u201d, \u201cpurple line\u201d, \u201ccyan line\u201d, \u201cgray line\u201d]\n21\nA.5.2\nFull Results on Simulated Environments\nU-Net + MP\nLLM + Prim.\nVoxPoser\nTasks\nSA\nUA\nSA\nUA\nSA\nUA\nmove to the [preposition] the [obj]\n95.0%\n0.0%\n85.0%\n60.0%\n90.0%\n55.0%\nmove to the [pos] while staying on the [preposition] the [obj]\n100.0% 10.0%\n80.0%\n30.0%\n95.0%\n50.0%\nmove to the [pos] while moving at [velocity] when within [dist]cm from the obj\n80.0%\n0.0%\n10.0%\n0.0%\n100.0%\n95.0%\nclose the [deixis] drawer by pushing\n0.0%\n0.0%\n60.0%\n60.0%\n80.0%\n80.0%\npush the [obj] along the [line]\n0.0%\n0.0%\n0.0%\n0.0%\n65.0%\n30.0%\ngrasp the [obj] from the table at [velocity]\n35.0%\n0.0%\n75.0%\n70.0%\n65.0%\n40.0%\ndrop the [obj] to the [pos]\n70.0%\n10.0%\n60.0%\n100.0%\n60.0%\n100.0%\npush the [obj] while letting it stay on [region]\n0.0%\n5.0%\n10.0%\n0.0%\n50.0%\n50.0%\nmove to the [region]\n5.0%\n0.0%\n100.0%\n95.0%\n100.0% 100.0%\nmove to the [pos] while staying at least [dist]cm from the [obj]\n0.0%\n0.0%\n15.0%\n20.0%\n85.0%\n90.0%\nmove to the [pos] while moving at [velocity] in the [region]\n0.0%\n0.0%\n90.0%\n45.0%\n85.0%\n85.0%\npush the [obj] to the [pos] while staying away from [obstacle]\n0.0%\n0.0%\n0.0%\n10.0%\n45.0%\n55.0%\npush the [obj] to the [pos]\n0.0%\n0.0%\n20.0%\n25.0%\n80.0%\n75.0%\nTable 4: Full experimental results in simulation on seen tasks and unseen tasks. \u201cSA\u201d indicates seen attributes\nand \u201cUA\u201d indicates unseen attributes. Each entry represents success rate averaged across 20 episodes.\n22\nA.6\nPrompts\nPrompts used in Sec. 4.1 and Sec. 4.2 can be found below.\nplanner: Takes in a user instruction L and generates a sequence of sub-tasks \u2113i which is fed into\n\u201ccomposer\u201d (Note that planner is not used in simulation as the evaluated tasks consist of a single\nmanipulation phase).\nreal-world: voxposer.github.io/prompts/real planner prompt.txt.\ncomposer: Takes in sub-task instruction \u2113i and invokes necessary value map LMPs to compose\naffordance maps and constraint maps.\nsimulation: voxposer.github.io/prompts/sim composer prompt.txt.\nreal-world: voxposer.github.io/prompts/real composer prompt.txt.\nparse query obj: Takes in a text query of object/part name and returns a list of dictionaries, where\neach dictionary corresponds to one instance of the matching object containing center position, oc-\ncupancy grid, and mean normal vector.\nsimulation: voxposer.github.io/prompts/sim parse query obj prompt.txt.\nreal-world: voxposer.github.io/prompts/real parse query obj prompt.txt.\nget affordance map: Takes in natural language parametrization from composer and returns a\nNumPy array for task affordance map.\nsimulation: voxposer.github.io/prompts/sim get affordance map prompt.txt.\nreal-world: voxposer.github.io/prompts/real get affordance map prompt.txt.\nget avoidance map: Takes in natural language parametrization from composer and returns a\nNumPy array for task avoidance map.\nsimulation: voxposer.github.io/prompts/sim get avoidance map prompt.txt.\nreal-world: voxposer.github.io/prompts/real get avoidance map prompt.txt.\nget rotation map: Takes in natural language parametrization from composer and returns a NumPy\narray for end-effector rotation map.\nsimulation: voxposer.github.io/prompts/sim get rotation map prompt.txt.\nreal-world: voxposer.github.io/prompts/real get rotation map prompt.txt.\nget gripper map: Takes in natural language parametrization from composer and returns a NumPy\narray for gripper action map.\nsimulation: voxposer.github.io/prompts/sim get gripper map prompt.txt.\nreal-world: voxposer.github.io/prompts/real get gripper map prompt.txt.\nget velocity map: Takes in natural language parametrization from composer and returns a NumPy\narray for end-effector velocity map.\nsimulation: voxposer.github.io/prompts/sim get velocity map prompt.txt.\nreal-world: voxposer.github.io/prompts/real get velocity map prompt.txt.\n23\n"
  }
]