[
  {
    "title": "A Survey on Evaluation of Large Language Models",
    "link": "https://arxiv.org/pdf/2307.03109.pdf",
    "upvote": "35",
    "text": "111\nA Survey on Evaluation of Large Language Models\nYUPENG CHANG\u2217 and XU WANG\u2217, School of Artificial Intelligence, Jilin University, China\nJINDONG WANG\u2020, Microsoft Research Asia, China\nYUAN WU\u2020, School of Artificial Intelligence, Jilin University, China\nLINYI YANG, Westlake University, China\nKAIJIE ZHU, Institute of Automation, Chinese Academy of Sciences, China\nHAO CHEN, Carnegie Mellon University, USA\nXIAOYUAN YI, Microsoft Research Asia, China\nCUNXIANG WANG, Westlake University, China\nYIDONG WANG, Peking University, China\nWEI YE, Peking University, China\nYUE ZHANG, Westlake University, China\nYI CHANG, School of Artificial Intelligence, Jilin University, China\nPHILIP S. YU, University of Illinois at Chicago, USA\nQIANG YANG, Hong Kong University of Science and Technology, China\nXING XIE, Microsoft Research Asia, China\nLarge language models (LLMs) are gaining increasing popularity in both academia and industry, owing to\ntheir unprecedented performance in various applications. As LLMs continue to play a vital role in both\nresearch and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at\nthe society level for better understanding of their potential risks. Over the past years, significant efforts have\nbeen made to examine LLMs from various perspectives. This paper presents a comprehensive review of these\nevaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how\nto evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences,\nagent applications, and other areas. Secondly, we answer the \u2018where\u2019 and \u2018how\u2019 questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs.\nThen, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several\n\u2217Both authors contributed equally to this research.\n\u2020Corresponding author.\nAuthors\u2019 addresses: Yupeng Chang, ypchang_jluai@outlook.com; Xu Wang, xwang22@mails.jlu.edu.cn, School of Artificial\nIntelligence, Jilin University, 2699 Qianjin St, Changchun, China, 130012; Jindong Wang, Microsoft Research Asia, Beijing,\nChina, Jindong.wang@microsoft.com; Yuan Wu, School of Artificial Intelligence, Jilin University, Changchun, China,\nyuanwu@jlu.edu.cn; Linyi Yang, Westlake University, Hangzhou, China; Kaijie Zhu, Institute of Automation, Chinese\nAcademy of Sciences, Beijing, China; Hao Chen, Carnegie Mellon University, Pennsylvania, USA; Xiaoyuan Yi, Microsoft\nResearch Asia, Beijing, China; Cunxiang Wang, Westlake University, Hangzhou, China; Yidong Wang, Peking University,\nBeijing, China; Wei Ye, Peking University, Beijing, China; Yue Zhang, Westlake University, Hangzhou, China; Yi Chang,\nSchool of Artificial Intelligence, Jilin University, Changchun, China; Philip S. Yu, University of Illinois at Chicago, Illinois,\nUSA; Qiang Yang, Hong Kong University of Science and Technology, Kowloon, Hong Kong, China; Xing Xie, Microsoft\nResearch Asia, Beijing, China.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2018 Association for Computing Machinery.\n0004-5411/2018/8-ART111 $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\narXiv:2307.03109v9  [cs.CL]  29 Dec 2023\n111:2\nChang et al.\nfuture challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers\nin the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point\nis that evaluation should be treated as an essential discipline to better assist the development of LLMs. We\nconsistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.\nCCS Concepts: \u2022 Computing methodologies \u2192 Natural language processing; Machine learning.\nAdditional Key Words and Phrases: large language models, evaluation, model assessment, benchmark\nACM Reference Format:\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang\nWang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2018. A Survey\non Evaluation of Large Language Models . J. ACM 37, 4, Article 111 (August 2018), 45 pages. https://doi.org/\nXXXXXXX.XXXXXXX\n1\nINTRODUCTION\nUnderstanding the essence of intelligence and establishing whether a machine embodies it poses a\ncompelling question for scientists. It is generally agreed upon that authentic intelligence equips us\nwith reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities [92].\nIn particular, Artificial Intelligence (AI) researchers focus on the development of machine-based in-\ntelligence, as opposed to biologically based intellect [136]. Proper measurement helps to understand\nintelligence. For instance, measures for general intelligence in human individuals often encompass\nIQ tests [12].\nWithin the scope of AI, the Turing Test [193], a widely recognized test for assessing intelligence\nby discerning if responses are of human or machine origin, has been a longstanding objective in AI\nevolution. It is generally believed among researchers that a computing machine that successfully\npasses the Turing Test can be considered as intelligent. Consequently, when viewed from a wider\nlens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent\nmodels and algorithms. With each emergence of a novel AI model or algorithm, researchers\ninvariably scrutinize its capabilities in real-world scenarios through evaluation using specific and\nchallenging tasks. For instance, the Perceptron algorithm [49], touted as an Artificial General\nIntelligence (AGI) approach in the 1950s, was later revealed as inadequate due to its inability\nto resolve the XOR problem. The subsequent rise and application of Support Vector Machines\n(SVMs) [28] and deep learning [104] have marked both progress and setbacks in the AI landscape.\nA significant takeaway from previous attempts is the paramount importance of AI evaluation,\nwhich serves as a critical tool to identify current system limitations and inform the design of more\npowerful models.\nRecently, large language models (LLMs) have incited substantial interest across both academic\nand industrial domains [11, 219, 257]. As demonstrated by existing work [15], the great performance\nof LLMs has raised promise that they could be AGI in this era. LLMs possess the capabilities to\nsolve diverse tasks, contrasting with prior models confined to solving specific tasks. Due to its\ngreat performance in handling different applications such as general natural language tasks and\ndomain-specific ones, LLMs are increasingly used by individuals with critical information needs,\nsuch as students or patients.\nEvaluation is of paramount prominence to the success of LLMs due to several reasons. First,\nevaluating LLMs helps us better understand the strengths and weakness of LLMs. For instance, the\nPromptBench [264] benchmark illustrates that current LLMs are sensitive to adversarial prompts,\nthus a careful prompt engineering is necessary for better performance. Second, better evaluations\ncan provide better guidance for human-LLMs interaction, which could inspire future interaction\ndesign and implementation. Third, the broad applicability of LLMs underscores the paramount\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:3\nLLMs\nevaluation\nWhat to evaluate\n(Sec. 3)\nNatural\nlanguage\nprocessing\nNatural language understanding:\n(1) Sentiment analysis: Bang et al. [6]/ Liang et al. [114]/ Lopez-Lira and Tang [129]/ Qin et al. [159]/ Wang et al. [218]/ Zhang et al. [251]\n(2) Text classification: Liang et al. [114] / Pe\u00f1a et al. [154] / Yang and Menczer [233]\n(3) Natural language inference: Lee et al. [105] / Qin et al. [159]\n(4) Others: Choi et al. [23] / Riccardi and Desai [166] / Tao et al. [184]\nReasoning: Bang et al. [6] / Bian et al. [9] / Frieder et al. [45] / Fu et al. [47] / Gendron et al. [56] / Jiang et al. [86] / Li\u00e9vin et al. [117]\nLiu et al. [124] / Orr\u00f9 et al. [147] / Pan et al. [151] / Qin et al. [159] / Saparov et al. [170] / Wu et al. [227] / Wu et al. [226]\nXu et al. [229] / Zhuang et al. [265] / Zhang et al. [244]\nNatural language generation:\n(1) Summarization: Bang et al. [6] / Liang et al. [114] / Pu and Demberg [158] / Qin et al. [159]\n(2) Dialogue: Bang et al. [6] / Lin and Chen [121] / Qin et al. [159] / Zheng et al. [259]\n(3) Translation: Bang et al. [6] / Lyu et al. [130] / Wang et al. [208]\n(4) Question answering: Bai et al. [5] / Bang et al. [6] / Bian et al. [9] / Laskar et al. [102] / Liang et al. [114] / Qin et al. [159]\n(5) Others: Chen et al. [20] / Chia et al. [22] / Pu and Demberg [158]\nMultilingual: Abdelali et al. [1] / Ahuja et al. [2] / Bang et al. [6] / Lai et al. [100] / Zhang et al. [250]\nFactuality: Gekhman et al. [55] / Honovich et al. [74] / Manakul et al. [133] / Min et al. [138] / Pezeshkpour [156] / Wang et al. [204]\nRobustness / Ethics/\nBiases/ Trustworthiness\nRobustness: Li et al. [111] / Liu et al. [123] / Wang et al. [207] / Wang et al. [206] / Yang et al. [234] / Zhao et al. [258]\nZhu et al. [264] / Zhuo et al. [267]\nEthics and biases: Cao et al. [16] / Deshpande et al. [35] / Dhamala et al. [37] / Ferrara [42] / Gehman et al. [53]\nHartmann et al. [65] / Hendrycks et al. [69] / Parrish et al. [153] / Rutinowski et al. [167] / Sheng et al. [175]\nSimmons [176] / Wang et al. [209] / Zhuo et al. [266] / Zhao et al. [256]\nTrustworthiness: Hagendorff and Fabi [62] / Wang et al. [201] / Liu et al. [123] / Li et al. [113] / Rawte et al. [163]\nXie et al. [228] / Zhang et al. [253]\nSocial science\nDeroy et al. [34] / Frank [44] / Nay et al. [139] / Wu et al. [224] / Ziems et al. [269]\nNatural science\n& engineering\nMathematics: Arora et al. [3] / Bubeck et al. [15] / Collins et al. [27]/ Dao and Le [31] / Wei et al. [221] / Wu et al. [225]\nYuan et al. [241] / Yu et al. [237]\nGeneral science: Arora et al. [3] / Castro Nascimento and Pimentel [18] / Guo et al. [61]\nEngineering: Bubeck et al. [15] / Liu et al. [125] / Pallagani et al. [150] / Sridhara et al. [181] / Valmeekam et al. [195]\nValmeekam et al. [194] / Zhuang et al. [265]\nMedical applications\nMedical queries: Chervenak et al. [21] / Duong and Solomon [39] / Hamidi and Roberts [63] / Holmes et al. [73]\nJahan et al. [81] / Johnson et al. [87] / Samaan et al. [169] / Thirunavukarasu et al. [186]\nMedical examination: Gilson et al. [57] / Kung et al. [97]\nMedical assistants: Cascella et al. [17] / Khan et al. [93] / Lahat et al. [99] / Lyu et al. [131] / Oh et al. [143] / Wang et al. [217]\nAgent applications\nHuang et al. [77] / Karpas et al. [90] / Parisi et al. [152] / Qin et al. [160] / Qin et al. [161] / Schick et al. [172] / Shen et al. [174]\nOther\napplications\nEducation: Dai et al. [30] / citetde Winter [32] / citetHellas et al. [67] / Wang and Demszky [210] / Wei et al. [221]\nSearch and recommendation: Dai et al. [29] / Fan et al. [40] / Lanzi and Loiacono [101] / Sun et al. [183] / Thakur et al. [185]\nXu et al. [232] / Yuan et al. [240] / Zhang et al. [246]\nPersonality testing: Bodroza et al. [10] / Jentzsch and Kersting [84] / Liang et al. [115] / Safdari et al. [168] / Song et al. [180] / Wang et al. [212]\nSpecific tasks: Lanzi and Loiacono [101] / Le and Zhang [103] / Wang et al. [216]\nWhere to evaluate\n(Sec. 4)\nGeneral\nbenchmarks\nXiezhi [59]/MMLU [70]/ C-Eval [78]/OpenLLM [80]/DynaBench [94]/Chatbot Arena [128]/AlpacaEval [112]/HELM [114]/BIG-bench [182]\nPandaLM [216] / BOSS [239] / GLUE-X [234] KoLA [236] / AGIEval [262]/ PromptBench [264] / MT-Bench [260] / LLMEval2 [252]\nSpecific\nbenchmarks\nSOCKET [23] / Choice-75 [75] / CUAD [71] / TRUSTGPT [79] / MATH [72] / APPS [68] / CELLO [66] / EmotionBench [76] / CMMLU [108]\nAPI-Bank [109] / M3KE [122] / UHGEval [116] / ARB [171] / MultiMedQA [177] / CVALUES [230] / ToolBench [191] / FRESHQA [198]\nCMB [211] / MINT [213] / Dialogue CoT [205] / M3Exam [250] / GAOKAO-Bench [245] / SafetyBench [254]\nMulti-modal\nbenchmarks\nMME [46] / MMBench [126] / SEED-Bench [107] / MM-Vet [238] / LAMM [235] / LVLM-eHub [231]\nHow to evaluate\n(Sec. 5)\nEvaluation criterion\nAutomatic evaluation: Bang et al. [6] / Jain et al. [82] / Lin and Chen [121] / Qin et al. [159] / Wang et al. [216]\nHuman evaluation: Askell et al. [4] / Bang et al. [6] / Bubeck et al. [15] / Liang et al. [114] / Singhal et al. [178] / Ziems et al. [269]\nSummary\n(Sec. 6)\nTasks: success and failure cases of LLMs\nBenchmark and evaluations\nHuman-in-the-loop: AdaVision [50] / AdaTest [164]\nCrowd-sourcing testing: DynaBench [94] / DynaBoard [132] / DynamicTempLAMA [135] / DynaTask [188]\nMore challenging tasks: HELM [114] / AdaFilter [157] / CheckList [165] / Big-Bench [182] / DeepTest [190] / PromptBench [264]\nGrand challenges\n(Sec. 7)\nChallenges\n(1) Designing AGI benchmarks (2) Complete behavioral evaluation (3) Robustness evaluation (4) Dynamic and evolving evaluation\n(5) Principled and trustworthy evaluation (6) Unified evaluation that supports all LLMs tasks (7) Beyond evaluation: LLMs enhancement\nFig. 1. Structure of this paper.\nimportance of ensuring their safety and reliability, particularly in safety-sensitive sectors such\nas financial institutions and healthcare facilities. Finally, as LLMs are becoming larger with more\nemergent abilities, existing evaluation protocols may not be enough to evaluate their capabilities\nand potential risks. Therefore, we aim to raise awareness in the community of the importance to\nLLMs evaluations by reviewing the current evaluation protocols and most importantly, shed light\non future research about designing new LLMs evaluation protocols.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:4\nChang et al.\nWith the introduction of ChatGPT [145] and GPT-4 [146], there have been a number of research\nefforts aiming at evaluating ChatGPT and other LLMs from different aspects (Figure 2), encom-\npassing a range of factors such as natural language tasks, reasoning, robustness, trustworthiness,\nmedical applications, and ethical considerations. Despite these efforts, a comprehensive overview\ncapturing the entire gamut of evaluations is still lacking. Furthermore, the ongoing evolution of\nLLMs has also presented novel aspects for evaluation, thereby challenging existing evaluation\nprotocols and reinforcing the need for thorough, multifaceted evaluation techniques. While existing\nresearch such as Bubeck et al. [15] claimed that GPT-4 can be seen as sparks of AGI, others contest\nthis claim due to the human-crafted nature of its evaluation approach.\nThis paper serves as the first comprehensive survey on the evaluation of large language models.\nAs depicted in Figure 1, we explore existing work in three dimensions: 1) What to evaluate, 2) Where\nto evaluate, and 3) How to evaluate. Specifically, \u201cwhat to evaluate\" encapsulates existing evaluation\ntasks for LLMs, \u201cwhere to evaluate\" involves selecting appropriate datasets and benchmarks for\nevaluation, while \u201chow to evaluate\" is concerned with the evaluation process given appropriate\ntasks and datasets. These three dimensions are integral to the evaluation of LLMs. We subsequently\ndiscuss potential future challenges in the realm of LLMs evaluation.\nThe contributions of this paper are as follows:\n(1) We provide a comprehensive overview of LLMs evaluations from three aspects: what to eval-\nuate, where to evaluate, and how to evaluate. Our categorization is general and encompasses\nthe entire life cycle of LLMs evaluation.\n(2) Regarding what to evaluate, we summarize existing tasks in various areas and obtain insightful\nconclusions on the success and failure case of LLMs (Sec. 6), providing experience for future\nresearch.\n(3) As for where to evaluate, we summarize evaluation metrics, datasets, and benchmarks to\nprovide a profound understanding of current LLMs evaluations. In terms of how to evaluate,\nwe explore current protocols and summarize novel evaluation approaches.\n(4) We further discuss future challenges in evaluating LLMs. We open-source and maintain the\nrelated materials of LLMs evaluation at https://github.com/MLGroupJLU/LLM-eval-survey\nto foster a collaborative community for better evaluations.\nThe paper is organized as follows. In Sec. 2, we provide the basic information of LLMs and AI\nmodel evaluation. Then, Sec. 3 reviews existing work from the aspects of \u201cwhat to evaluate\u201d. After\nthat, Sec. 4 is the \u201cwhere to evaluate\u201d part, which summarizes existing datasets and benchmarks.\nSec. 5 discusses how to perform the evaluation. In Sec. 6, we summarize the key findings of this\npaper. We discuss grand future challenges in Sec. 7 and Sec. 8 concludes the paper.\n2\nBACKGROUND\n2.1\nLarge Language Models\nLanguage models (LMs) [36, 51, 96] are computational models that have the capability to understand\nand generate human language. LMs have the transformative ability to predict the likelihood of word\nsequences or generate new text based on a given input. N-gram models [13], the most common\ntype of LM, estimate word probabilities based on the preceding context. However, LMs also face\nchallenges, such as the issue of rare or unseen words, the problem of overfitting, and the difficulty\nin capturing complex linguistic phenomena. Researchers are continuously working on improving\nLM architectures and training methods to address these challenges.\nLarge Language Models (LLMs) [19, 91, 257] are advanced language models with massive pa-\nrameter sizes and exceptional learning capabilities. The core module behind many LLMs such as\nGPT-3 [43], InstructGPT [149], and GPT-4 [146] is the self-attention module in Transformer [197]\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:5\n2020\n2021\n2022\n2023.01\n2023.02\n2023.03\n2023.04\n2023.05\n2023.06+\n0\n5\n10\n15\n20\n25\n30\n35\nNumber of papers\nNumber of papers\nFig. 2. Trend of LLMs evaluation papers over time (2020 - Jun. 2023, including Jul. 2023.).\nthat serves as the fundamental building block for language modeling tasks. Transformers have\nrevolutionized the field of NLP with their ability to handle sequential data efficiently, allowing\nfor parallelization and capturing long-range dependencies in text. One key feature of LLMs is\nin-context learning [14], where the model is trained to generate text based on a given context\nor prompt. This enables LLMs to generate more coherent and contextually relevant responses,\nmaking them suitable for interactive and conversational applications. Reinforcement Learning\nfrom Human Feedback (RLHF) [25, 268] is another crucial aspect of LLMs. This technique involves\nfine-tuning the model using human-generated responses as rewards, allowing the model to learn\nfrom its mistakes and improve its performance over time.\nIn an autoregressive language model, such as GPT-3 and PaLM [24], given a context sequence \ud835\udc4b,\nthe LM tasks aim to predict the next token \ud835\udc66. The model is trained by maximizing the probability\nof the given token sequence conditioned on the context, i.e., \ud835\udc43(\ud835\udc66|\ud835\udc4b) = \ud835\udc43(\ud835\udc66|\ud835\udc651,\ud835\udc652, ...,\ud835\udc65\ud835\udc61\u22121), where\n\ud835\udc651,\ud835\udc652, ...,\ud835\udc65\ud835\udc61\u22121 are the tokens in the context sequence, and \ud835\udc61 is the current position. By using the\nchain rule, the conditional probability can be decomposed into a product of probabilities at each\nposition:\n\ud835\udc43(\ud835\udc66|\ud835\udc4b) =\n\ud835\udc47\n\u00d6\n\ud835\udc61=1\n\ud835\udc43(\ud835\udc66\ud835\udc61 |\ud835\udc651,\ud835\udc652, ...,\ud835\udc65\ud835\udc61\u22121),\nwhere \ud835\udc47 is sequence length. In this way, the model predicts each token at each position in an\nautoregressive manner, generating a complete text sequence.\nOne common approach to interacting with LLMs is prompt engineering [26, 222, 263], where\nusers design and provide specific prompt texts to guide LLMs in generating desired responses or\ncompleting specific tasks. This is widely adopted in existing evaluation efforts. People can also\nengage in question-and-answer interactions [83], where they pose questions to the model and\nreceive answers, or engage in dialogue interactions, having natural language conversations with\nLLMs. In conclusion, LLMs, with their Transformer architecture, in-context learning, and RLHF\ncapabilities, have revolutionized NLP and hold promise in various applications. Table 1 provides a\nbrief comparison of traditional ML, deep learning, and LLMs.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:6\nChang et al.\nTable 1. Comparison of Traditional ML, Deep Learning, and LLMs\nComparison\nTraditional ML\nDeep Learning\nLLMs\nTraining Data Size\nLarge\nLarge\nVery large\nFeature Engineering\nManual\nAutomatic\nAutomatic\nModel Complexity\nLimited\nComplex\nVery Complex\nInterpretability\nGood\nPoor\nPoorer\nPerformance\nModerate\nHigh\nHighest\nHardware Requirements\nLow\nHigh\nVery High\nWhat\n(Task)\nWhere\n(Data)\nHow\n(Process)\nModel\nFig. 3. The evaluation process of AI models.\n2.2\nAI Model Evaluation\nAI model evaluation is an essential step in assessing the performance of a model. There are some\nstandard model evaluation protocols, including \ud835\udc58-fold cross-validation, holdout validation, leave\none out cross-validation (LOOCV), bootstrap, and reduced set [8, 95]. For instance, \ud835\udc58-fold cross-\nvalidation divides the dataset into \ud835\udc58 parts, with one part used as a test set and the rest as training\nsets, which can reduce training data loss and obtain relatively more accurate model performance\nevaluation [48]; Holdout validation divides the dataset into training and test sets, with a smaller\ncalculation amount but potentially more significant bias; LOOCV is a unique \ud835\udc58-fold cross-validation\nmethod where only one data point is used as the test set [223]; Reduced set trains the model\nwith one dataset and tests it with the remaining data, which is computationally simple, but the\napplicability is limited. The appropriate evaluation method should be chosen according to the\nspecific problem and data characteristics for more reliable performance indicators.\nFigure 3 illustrates the evaluation process of AI models, including LLMs. Some evaluation\nprotocols may not be feasible to evaluate deep learning models due to the extensive training size.\nThus, evaluation on a static validation set has long been the standard choice for deep learning\nmodels. For instance, computer vision models leverage static test sets such as ImageNet [33] and\nMS COCO [120] for evaluation. LLMs also use GLUE [200] or SuperGLUE [199] as the common\ntest sets.\nAs LLMs are becoming more popular with even poorer interpretability, existing evaluation\nprotocols may not be enough to evaluate the true capabilities of LLMs thoroughly. We will introduce\nrecent evaluations of LLMs in Sec. 5.\n3\nWHAT TO EVALUATE\nWhat tasks should we evaluate LLMs to show their performance? On what tasks can we claim the\nstrengths and weaknesses of LLMs? In this section, we divide existing tasks into the following\ncategories: natural language processing, robustness, ethics, biases and trustworthiness, social\nsciences, natural science and engineering, medical applications, agent applications (using LLMs as\nagents), and other applications.1\n1Note that LLMs are evaluated in various tasks and the categorization in this paper is only one possible way for classification\nof these works. There are certainly other taxonomies.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:7\n3.1\nNatural Language Processing Tasks\nThe initial objective behind the development of language models, particularly large language\nmodels, was to enhance performance on natural language processing tasks, encompassing both\nunderstanding and generation. Consequently, the majority of evaluation research has been primarily\nfocused on natural language tasks. Table 2 summarizes the evaluation aspects of existing research,\nand we mainly highlight their conclusions in the following.2\n3.1.1\nNatural language understanding. Natural language understanding represents a wide spectrum\nof tasks that aims to obtain a better understanding of the input sequence. We summarize recent\nefforts in LLMs evaluation from several aspects.\nSentiment analysis is a task that analyzes and interprets the text to determine the emotional\ninclination. It is typically a binary (positive and negative) or triple (positive, neutral, and negative)\nclass classification problem. Evaluating sentiment analysis tasks is a popular direction. Liang et al.\n[114] and Zeng et al. [243] showed that the performance of the models on this task is usually\nhigh. ChatGPT\u2019s sentiment analysis prediction performance is superior to traditional sentiment\nanalysis methods [129] and comes close to that of GPT-3.5 [159]. In fine-grained sentiment and\nemotion cause analysis, ChatGPT also exhibits exceptional performance [218]. In low-resource\nlearning environments, LLMs exhibit significant advantages over small language models [251], but\nthe ability of ChatGPT to understand low-resource languages is limited [6]. In conclusion, LLMs\nhave demonstrated commendable performance in sentiment analysis tasks. Future work should\nfocus on enhancing their capability to understand emotions in under-resourced languages.\nText classification and sentiment analysis are related fields, text classification not only focuses\non sentiment, but also includes the processing of all texts and tasks. The work of Liang et al.\n[114] showed that GLM-130B was the best-performed model, with an overall accuracy of 85.8%\nfor miscellaneous text classification. Yang and Menczer [233] found that ChatGPT can produce\ncredibility ratings for a wide range of news outlets, and these ratings have a moderate correlation\nwith those from human experts. Furthermore, ChatGPT achieves acceptable accuracy in a binary\nclassification scenario (AUC=0.89). Pe\u00f1a et al. [154] discussed the problem of topic classification\nfor public affairs documents and showed that using an LLM backbone in combination with SVM\nclassifiers is a useful strategy to conduct the multi-label topic classification task in the domain of\npublic affairs with accuracies over 85%. Overall, LLMs perform well on text classification and can\neven handle text classification tasks in unconventional problem settings as well.\nNatural language inference (NLI) is the task of determining whether the given \u201chypothesis\u201d\nlogically follows from the \u201cpremise\u201d. Qin et al. [159] showed that ChatGPT outperforms GPT-3.5 for\nNLI tasks. They also found that ChatGPT excels in handling factual input that could be attributed\nto its RLHF training process in favoring human feedback. However, Lee et al. [105] observed LLMs\nperform poorly in the scope of NLI and further fail in representing human disagreement, which\nindicates that LLMs still have a large room for improvement in this field.\nSemantic understanding refers to the meaning or understanding of language and its associated\nconcepts. It involves the interpretation and comprehension of words, phrases, sentences, and the\nrelationships between them. Semantic processing goes beyond the surface level and focuses on\nunderstanding the underlying meaning and intent. Tao et al. [184] comprehensively evaluated the\nevent semantic processing abilities of LLMs covering understanding, reasoning, and prediction\nabout the event semantics. Results indicated that LLMs possess an understanding of individual\nevents, but their capacity to perceive the semantic similarity among events is constrained. In\nreasoning tasks, LLMs exhibit robust reasoning abilities in causal and intentional relations, yet their\n2Several NLP areas have intersections and thus our categorization of these areas is only one possible way to categorize.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:8\nChang et al.\nTable 2. Summary of evaluation on natural language processing tasks: NLU (Natural Language Under-\nstanding, including SA (Sentiment Analysis), TC (Text Classification), NLI (Natural Language Inference)\nand other NLU tasks), Reasoning, NLG (Natural Language Generation, including Summ. (Summarization),\nDlg. (Dialogue), Tran (Translation), QA (Question Answering) and other NLG tasks), and Multilingual tasks\n(ordered by the name of the first author).\nReference\nNLU\nRNG.\nNLG\nMul.\nSA\nTC\nNLI\nOthers\nSumm.\nDlg.\nTran.\nQA\nOthers\nAbdelali et al. [1]\n\u2713\nAhuja et al. [2]\n\u2713\nBian et al. [9]\n\u2713\n\u2713\nBang et al. [6]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nBai et al. [5]\n\u2713\nChen et al. [20]\n\u2713\nChoi et al. [23]\n\u2713\nChia et al. [22]\n\u2713\nFrieder et al. [45]\n\u2713\nFu et al. [47]\n\u2713\nGekhman et al. [55]\n\u2713\nGendron et al. [56]\n\u2713\nHonovich et al. [74]\n\u2713\n\u2713\n\u2713\n\u2713\nJiang et al. [86]\n\u2713\nLai et al. [100]\n\u2713\nLaskar et al. [102]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nLopez-Lira and Tang [129]\n\u2713\nLiang et al. [114]\n\u2713\n\u2713\n\u2713\n\u2713\nLee et al. [105]\n\u2713\nLin and Chen [121]\n\u2713\nLi\u00e9vin et al. [117]\n\u2713\nLiu et al. [124]\n\u2713\nLyu et al. [130]\n\u2713\nManakul et al. [133]\n\u2713\n\u2713\nMin et al. [138]\n\u2713\nOrr\u00f9 et al. [147]\n\u2713\nPan et al. [151]\n\u2713\nPe\u00f1a et al. [154]\n\u2713\nPu and Demberg [158]\n\u2713\n\u2713\nPezeshkpour [156]\n\u2713\nQin et al. [159]\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nRiccardi and Desai [166]\n\u2713\nSaparov et al. [170]\n\u2713\nTao et al. [184]\n\u2713\nWang et al. [208]\n\u2713\nWang et al. [218]\n\u2713\nWang et al. [204]\n\u2713\n\u2713\nWu et al. [227]\n\u2713\nWu et al. [226]\n\u2713\nXu et al. [229]\n\u2713\nYang and Menczer [233]\n\u2713\nZheng et al. [259]\n\u2713\nZhang et al. [251]\n\u2713\nZhang et al. [250]\n\u2713\nZhuang et al. [265]\n\u2713\nZhang et al. [244]\n\u2713\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:9\nperformance in other relation types is comparatively weaker. In prediction tasks, LLMs exhibit\nenhanced predictive capabilities for future events with increased contextual information. Riccardi\nand Desai [166] explored the semantic proficiency of LLMs and showed that these models perform\npoorly in evaluating basic phrases. Furthermore, GPT-3.5 and Bard cannot distinguish between\nmeaningful and nonsense phrases, consistently classifying highly nonsense phrases as meaningful.\nGPT-4 shows significant improvements, but its performance is still significantly lower than that of\nhumans. In summary, the performance of LLMs in semantic understanding tasks is poor. In the\nfuture, we can start from this aspect and focus on improving its performance on this application.\nIn social knowledge understanding, Choi et al. [23] evaluated how well models perform\nat learning and recognizing concepts of social knowledge and the results revealed that despite\nbeing much smaller in the number of parameters, finetuning supervised models such as BERT\nlead to much better performance than zero-shot models using state-of-the-art LLMs, such as GPT\n[162], GPT-J-6B [202] and so on. This statement demonstrates that supervised models significantly\noutperform zero-shot models in terms of performance, highlighting that an increase in parameters\ndoes not necessarily guarantee a higher level of social knowledge in this particular scenario.\n3.1.2\nReasoning. The task of reasoning poses significant challenges for an intelligent AI model. To\neffectively tackle reasoning tasks, the models need to not only comprehend the provided information\nbut also utilize reasoning and inference to deduce answers when explicit responses are absent.\nTable 2 reveals that there is a growing interest in evaluating the reasoning ability of LLMs, as\nevidenced by the increasing number of articles focusing on exploring this aspect. Currently, the\nevaluation of reasoning tasks can be broadly categorized into mathematical reasoning, commonsense\nreasoning, logical reasoning, and domain-specific reasoning.\nChatGPT exhibits a strong capability for arithmetic reasoning by outperforming GPT-3.5 in\nthe majority of tasks [159]. However, its proficiency in mathematical reasoning still requires\nimprovement [6, 45, 265]. On symbolic reasoning tasks, ChatGPT is mostly worse than GPT-3.5,\nwhich may be because ChatGPT is prone to uncertain responses, leading to poor performance\n[6]. Through the poor performance of LLMs on task variants of counterfactual conditions, Wu\net al. [227] showed that the current LLMs have certain limitations in abstract reasoning ability. On\nabstract reasoning, Gendron et al. [56] found that existing LLMs have very limited ability. In logical\nreasoning, Liu et al. [124] indicated that ChatGPT and GPT-4 outperform traditional fine-tuning\nmethods on most benchmarks, demonstrating their superiority in logical reasoning. However,\nboth models face challenges when handling new and out-of-distribution data. ChatGPT does not\nperform as well as other LLMs, including GPT-3.5 and BARD [159, 229]. This is because ChatGPT\nis designed explicitly for chatting, so it does an excellent job of maintaining rationality. FLAN-T5,\nLLaMA, GPT-3.5, and PaLM perform well in general deductive reasoning tasks [170]. GPT-3.5 is\nnot good at keeping oriented for reasoning in the inductive setting [229]. For multi-step reasoning,\nFu et al. [47] showed PaLM and Claude2 are the only two model families that achieve similar\nperformance (but still worse than the GPT model family). Moreover, LLaMA-65B is the most robust\nopen-source LLMs to date, which performs closely to code-davinci-002. Some papers separately\nevaluate the performance of ChatGPT on some reasoning tasks: ChatGPT generally performs\npoorly on commonsense reasoning tasks, but relatively better than non-text semantic reasoning [6].\nMeanwhile, ChatGPT also lacks spatial reasoning ability, but exhibits better temporal reasoning.\nFinally, while the performance of ChatGPT is acceptable on causal and analogical reasoning, it\nperforms poorly on multi-hop reasoning ability, which is similar to the weakness of other LLMs\non complex reasoning [148]. In professional domain reasoning tasks, zero-shot InstructGPT and\nCodex are capable of complex medical reasoning tasks, but still need to be further improved [117].\nIn terms of language insight issues, Orr\u00f9 et al. [147] demonstrated the potential of ChatGPT for\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:10\nChang et al.\nsolving verbal insight problems, as ChatGPT\u2019s performance was comparable to that of human\nparticipants. It should be noted that most of the above conclusions are obtained for specific data\nsets. In contrast, more complex tasks have become the mainstream benchmarks for assessing\nthe capabilities of LLMs. These include tasks such as mathematical reasoning [226, 237, 244] and\nstructured data inference [86, 151]. Overall, LLMs show great potential in reasoning and show\na continuous improvement trend, but still face many challenges and limitations, requiring more\nin-depth research and optimization.\n3.1.3\nNatural language generation. NLG evaluates the capabilities of LLMs in generating specific\ntexts, which consists of several tasks, including summarization, dialogue generation, machine\ntranslation, question answering, and other open-ended generation tasks.\nSummarization is a generation task that aims to learn a concise abstract for the given sentence.\nIn this evaluation, Liang et al. [114] found that TNLG v2 (530B) [179] achieved the highest score\nin both scenarios, followed by OPT (175B) [247] in second place. The fine-tuned Bart [106] is\nstill better than zero-shot ChatGPT. Specifically, ChatGPT demonstrates comparable zero-shot\nperformance to the text-davinci-002 [6], but performs worse than GPT-3.5 [159]. These findings\nindicate that LLMs, particularly ChatGPT, have a general performance in summarization tasks.\nEvaluating the performance of LLMs on dialogue tasks is crucial to the development of dialogue\nsystems and improving human-computer interaction. Through such evaluation, the natural language\nprocessing ability, context understanding ability and generation ability of the model can be improved,\nso as to realize a more intelligent and more natural dialogue system. Both Claude and ChatGPT\ngenerally achieve better performance across all dimensions when compared to GPT-3.5 [121,\n159]. When comparing the Claude and ChatGPT models, both models demonstrate competitive\nperformance across different evaluation dimensions, with Claude slightly outperforming ChatGPT\nin specific configurations. Research by Bang et al. [6] underscores that fully fine-tuned models\ntailored for specific tasks surpass ChatGPT in both task-oriented and knowledge-based dialogue\ncontexts. Additionally, Zheng et al. [259] have curated a comprehensive LLMs conversation dataset,\nLMSYS-Chat-1M, encompassing up to one million samples. This dataset serves as a valuable resource\nfor evaluating and advancing dialogue systems.\nWhile LLMs are not explicitly trained for translation tasks, they can still demonstrate strong\nperformance. Wang et al. [208] demonstrated that ChatGPT and GPT-4 exhibit superior perfor-\nmance in comparison to commercial machine translation (MT) systems, as evaluated by humans.\nAdditionally, they outperform most document-level NMT methods in terms of sacreBLEU scores.\nDuring contrastive testing, ChatGPT shows lower accuracy in comparison to traditional translation\nmodels. However, GPT-4 demonstrates a robust capability in explaining discourse knowledge, even\nthough it may occasionally select incorrect translation candidates. The findings from Bang et al. [6]\nindicated that ChatGPT performs X \u2192 Eng translation well, but it still lacks the ability to perform\nEng \u2192 X translation. Lyu et al. [130] investigated several research directions in MT utilizing LLMs.\nThis study significantly contributes to the advancement of MT research and highlights the potential\nof LLMs in enhancing translation capabilities. In summary, while LLMs perform satisfactorily\nin several translation tasks, there is still room for improvement, e.g., enhancing the translation\ncapability from English to non-English languages.\nQuestion answering is a crucial technology in the field of human-computer interaction, and\nit has found wide application in scenarios like search engines, intelligent customer service, and\nQA systems. The measurement of accuracy and efficiency in QA models will have significant\nimplications for these applications. According to Liang et al. [114], among all the evaluated models,\nInstructGPT davinci v2 (175B) exhibited the highest performance in terms of accuracy, robustness,\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:11\nand fairness across the 9 QA scenarios. Both GPT-3.5 and ChatGPT demonstrate significant advance-\nments compared to GPT-3 in their ability to answer general knowledge questions. In most domains,\nChatGPT surpasses GPT-3.5 by more than 2% in terms of performance [9, 159]. However, ChatGPT\nperforms slightly weaker than GPT-3.5 on the CommonsenseQA and Social IQA benchmarks.\nThis can be attributed to ChatGPT\u2019s cautious nature, as it tends to decline to provide an answer\nwhen there is insufficient information available. Fine-tuned models, such as V\u00edcuna and ChatGPT,\nexhibit exceptional performance with near-perfect scores, surpassing models that lack supervised\nfine-tuning by a significant margin [5, 6]. Laskar et al. [102] evaluated the effectiveness of ChatGPT\non a range of academic datasets, including various tasks such as answering questions, summarizing\ntext, generating code, reasoning with commonsense, solving math problems, translating languages,\ndetecting bias, and addressing ethical issues. Overall, LLMs showcase flawless performance on QA\ntasks and hold the potential for further enhancing their proficiency in social, event, and temporal\ncommonsense knowledge in the future.\nThere are also other generation tasks to explore. In the field of sentence style transfer, Pu\nand Demberg [158] demonstrated that ChatGPT surpasses the previous SOTA supervised model\nthrough training on the same subset for few-shot learning, as evident from the higher BLEU score.\nHowever, when it comes to controlling the formality of sentence style, ChatGPT\u2019s performance\nstill differs significantly from human behavior. In writing tasks, Chia et al. [22] discovered that\nLLMs exhibit consistent performance across various categories such as informative, professional,\nargumentative, and creative writing. This finding implies that LLMs possess a general proficiency\nin writing capabilities. In text generation quality, Chen et al. [20] revealed that ChatGPT excels\nin assessing text quality from multiple angles, even in the absence of reference texts, surpassing\nthe performance of most existing automated metrics. Employing ChatGPT to generate numerical\nscores for text quality emerged as the most reliable and effective approach among the various\ntesting methods studied.\n3.1.4\nMultilingual tasks. While English is the predominant language, many LLMs are trained on\nmixed-language training data. The combination of multilingual data indeed helps LLMs gain the\nability to process inputs and generate responses in different languages, making them widely adopted\nand accepted across the globe. However, due to the relatively recent emergence of this technology,\nLLMs are primarily evaluated on English data, leading to a potential oversight of evaluating their\nmultilingual performance. To address this, several articles have provided comprehensive, open,\nand independent evaluations of LLMs\u2019 performance on various NLP tasks in different non-English\nlanguages. These evaluations offer valuable insights for future research and applications.\nAbdelali et al. [1] evaluated the performance of ChatGPT in standard Arabic NLP tasks and\nobserved that ChatGPT exhibits lower performance compared to SOTA models in the zero-shot\nsetting for most tasks. Ahuja et al. [2], Bang et al. [6], Lai et al. [100], Zhang et al. [250] utilized a\ngreater number of languages across multiple datasets, encompassing a wider range of tasks, and\nconducted a more comprehensive evaluation of LLMs, including BLOOM, Vicuna, Claude, ChatGPT,\nand GPT-4. The results indicated that these LLMs perform poorly when it came to non-Latin\nlanguages and languages with limited resources. Despite translating the input to English and using\nit as the query, generative LLMs still displays subpar performance across tasks and languages\ncompared to SOTA models [2]. Furthermore, Bang et al. [6] highlighted that ChatGPT still faces\na limitation in translating sentences written in non-Latin script languages with rich linguistic\nresources. The aforementioned demonstrates that there are numerous challenges and ample oppor-\ntunities for enhancement in multilingual tasks for LLMs. Future research should prioritize achieving\nmultilingual balance and addressing the challenges faced by non-Latin languages and low-resource\nlanguages, with the aim of better supporting users worldwide. At the same time, attention should\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:12\nChang et al.\nbe paid to the impartiality and neutrality of the language in order to mitigate any potential biases,\nincluding English bias or other biases, that could impact multilingual applications.\n3.1.5\nFactuality. Factuality in the context of LLMs refers to the extent to which the information\nor answers provided by the model align with real-world truths and verifiable facts. Factuality in\nLLMs significantly impacts a variety of tasks and downstream applications, such as QA systems,\ninformation extraction, text summarization, dialogue systems, and automated fact-checking, where\nincorrect or inconsistent information could lead to substantial misunderstandings and misinter-\npretations. Evaluating factuality is of great importance in order to trust and efficiently use these\nmodels. This includes the ability of these models to maintain consistency with known facts, avoid\ngenerating misleading or false information (known as \u201cfactual hallucination\"), and effectively\nlearn and recall factual knowledge. A range of methodologies have been proposed to measure and\nimprove the factuality of LLMs.\nWang et al. [204] assessed the internal knowledge capabilities of several large models, namely\nInstructGPT, ChatGPT-3.5, GPT-4, and BingChat [137], by examining their ability to answer open\nquestions based on the Natural Questions [98] and TriviaQA [88] datasets. The evaluation process\ninvolved human assessment. The results of the study indicated that while GPT-4 and BingChat can\nprovide correct answers for more than 80% of the questions, there is still a remaining gap of over\n15% to achieve complete accuracy. In the work of Honovich et al. [74], they conducted a review of\ncurrent factual consistency evaluation methods and highlighted the absence of a unified comparison\nframework and the limited reference value of related scores compared to binary labels. To address\nthis, they transformed existing fact consistency tasks into binary labels, specifically considering\nonly whether there is a factual conflict with the input text, without factoring in external knowledge.\nThe research discovered that fact evaluation methods founded on natural language inference and\nquestion generation answering exhibit superior performance and can complement each other.\nPezeshkpour [156] proposed a novel metric, based on information theory, to assess the inclusion of\nspecific knowledge in LLMs. The metric utilized the concept of uncertainty in knowledge to measure\nfactualness, calculated by LLMs filling in prompts and examining the probability distribution of the\nanswer. The paper discussed two methods for injecting knowledge into LLMs: explicit inclusion\nof knowledge in the prompts and implicit fine-tuning of the LLMs using knowledge-related data.\nThe study demonstrated that this approach surpasses traditional ranking methods by achieving an\naccuracy improvement of over 30%. Gekhman et al. [55] improved the method for evaluating fact\nconsistency in summarization tasks. It proposed a novel approach that involved training student\nNLI models using summaries generated by multiple models and annotated by LLMs to ensure\nfact consistency. The trained student model was then used for summarization fact consistency\nevaluation. Manakul et al. [133] operated on two hypotheses regarding how LLMs generate factual\nor hallucinated responses. It proposed the use of three formulas (BERTScore [249], MQAG [134]\nand n-gram) to evaluate factuality and employed alternative LLMs to gather token probabilities for\nblack-box language models. The study discovered that simply computing sentence likelihood or\nentropy helped validate the factuality of the responses. Min et al. [138] broke down text generated\nby LLMs into individual \u201catomic\" facts, which were then evaluated for their correctness. The\nFActScore is used to measure the performance of estimators through the calculation of F1 scores.\nThe paper tested various estimators and revealed that current estimators still have some way to go\nin effectively addressing the task. Lin et al. [119] introduced the TruthfulQA dataset, designed to\ncause models to make mistakes. Multiple language models were tested by providing factual answers.\nThe findings from these experiments suggest that simply scaling up model sizes may not necessarily\nimprove their truthfulness, and recommendations are provided for the training approach. This\ndataset has become widely used for evaluating the factuality of LLMs [89, 146, 192, 220].\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:13\nTable 3. Summary of LLMs evaluation on robustness, ethics, biases, and trustworthiness (ordered by the\nname of the first author).\nReference\nRobustness\nEthics and biases\nTrustworthiness\nCao et al. [16]\n\u2713\nDhamala et al. [37]\n\u2713\nDeshpande et al. [35]\n\u2713\nFerrara [42]\n\u2713\nGehman et al. [53]\n\u2713\nHartmann et al. [65]\n\u2713\nHendrycks et al. [69]\n\u2713\nHagendorff and Fabi [62]\n\u2713\nLi et al. [111]\n\u2713\nLiu et al. [123]\n\u2713\nLiu et al. [123]\n\u2713\nLi et al. [113]\n\u2713\nParrish et al. [153]\n\u2713\nRutinowski et al. [167]\n\u2713\nRawte et al. [163]\n\u2713\nSheng et al. [175]\n\u2713\nSimmons [176]\n\u2713\nWang et al. [207]\n\u2713\nWang et al. [206]\n\u2713\nWang et al. [201]\n\u2713\n\u2713\n\u2713\nWang et al. [209]\n\u2713\nXie et al. [228]\n\u2713\nYang et al. [234]\n\u2713\nZhao et al. [258]\n\u2713\nZhuo et al. [267]\n\u2713\nZhu et al. [264]\n\u2713\nZhuo et al. [266]\n\u2713\nZhang et al. [253]\n\u2713\n3.2\nRobustness, Ethic, Bias, and Trustworthiness\nThe evaluation encompasses crucial aspects of robustness, ethics, biases, and trustworthiness. These\nfactors have gained increasing importance in assessing the performance of LLMs comprehensively.\n3.2.1\nRobustness. Robustness studies the stability of a system when facing unexpected inputs.\nSpecifically, out-of-distribution (OOD) [207] and adversarial robustness are two popular research\ntopics for robustness. Wang et al. [206] is an early work that evaluated ChatGPT and other LLMs\nfrom both the adversarial and OOD perspectives using existing benchmarks such as AdvGLUE [203],\nANLI [140], and DDXPlus [41] datasets. Zhuo et al. [267] evaluated the robustness of semantic\nparsing. Yang et al. [234] evaluated OOD robustness by extending the GLUE [200] dataset. The\nresults of this study emphasize the potential risks to the overall system security when manipulating\nvisual input. For vision-language models, Zhao et al. [258] evaluated LLMs on visual input and\ntransferred them to other visual-linguistic models, revealing the vulnerability of visual input. Li\net al. [111] provided an overview of OOD evaluation for language models: adversarial robustness,\ndomain generalization, and dataset biases. Bridging these lines of research, the authors conducted a\ncomparative analysis, unifying the three approaches. They succinctly outlined the data-generation\nprocesses and evaluation protocols for each line of study, all while emphasizing the prevailing\nchallenges and future research prospects. Additionally, Liu et al. [123] introduced a large-scale\nrobust visual instruction dataset to enhance the performance of large-scale multi-modal models in\nhandling relevant images and human instructions.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:14\nChang et al.\nFor adversarial robustness, Zhu et al. [264] evaluated the robustness of LLMs to prompts by\nproposing a unified benchmark called PromptBench. They comprehensively evaluated adversarial\ntext attacks at multiple levels (character, word, sentence, and semantics). The results showed that\ncontemporary LLMs are vulnerable to adversarial prompts, highlighting the importance of the\nmodels\u2019 robustness when facing adversarial inputs. As for new adversarial datasets, Wang et al. [201]\nintroduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a\nnew evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.\n3.2.2\nEthic and bias. LLMs have been found to internalize, spread, and potentially magnify harmful\ninformation existing in the crawled training corpora, usually, toxic languages, like offensiveness,\nhate speech, and insults [53], as well as social biases like stereotypes towards people with a particular\ndemographic identity (e.g., gender, race, religion, occupation, and ideology) [175]. More recently,\nZhuo et al. [266] used conventional testing sets and metrics [37, 53, 153] to perform a systematic\nevaluation of ChatGPT\u2019s toxicity and social bias, finding that it still exhibits noxious content to\nsome extend. Taking a further step, Deshpande et al. [35] introduced role-playing into the model\nand observed an increase in generated toxicity up to 6x. Furthermore, such role-playing also caused\nbiased toxicity towards specific entities. Different from simply measuring social biases, Ferrara [42]\ninvestigated the sources, underlying mechanisms, and corresponding ethical consequences of these\nbiases potentially produced by ChatGPT. Beyond social biases, LLMs have also been assessed by\npolitical tendency and personality traits [65, 167] based questionnaires like the Political Compass\nTest and MBTI test, demonstrating a propensity for progressive views and an ENFJ personality\ntype. In addition, LLMs like GPT-3 were found to have moral biases [176] in terms of the Moral\nFoundation theory [58]; The study conducted by [69] reveals that existing LMs have potential\nin ethical judgment, but still need improvement. [256] proposes a Chinese conversational bias\nevaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing\nmethods. Moreover, in the assessment of GPT-4 alignment, [209] discovered a systematic bias.\nChatGPT is also observed to exhibit somewhat bias on cultural values [16]. Wang et al. [201]\nalso incorporated an evaluation dataset specifically aimed at gauging stereotype bias, using both\ntargeted and untargeted system prompts. All these ethical issues might elicit serious risks, impeding\nthe deployment of LLMs and having a profound negative impact on society.\n3.2.3\nTrustworthiness. Some work focuses on other trustworthiness problems in addition to ro-\nbustness and ethics.3 In their 2023 study, DecodingTrust, Wang et al. [201] offered a multifaceted\nexploration of trustworthiness vulnerabilities in the GPT models, especially GPT-3.5 and GPT-4.\nTheir evaluation expanded beyond the typical trustworthiness concerns to include eight critical\naspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adver-\nsarial demonstrations, privacy, machine ethics, and fairness. DecodingTrust\u2019s investigation employs\nan array of newly constructed scenarios, tasks, and metrics. They revealed that while GPT-4 often\nshowcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously\nmore susceptible to attacks.\nIn another study by Hagendorff and Fabi [62], LLMs with enhanced cognitive abilities were\nevaluated. They found that these models can avoid common human intuitions and cognitive errors,\ndemonstrating super-rational performance. By utilizing cognitive reflection tests and semantic\nillusion experiments, the researchers gained insights into the psychological aspects of LLMs. This\nmethod offers new perspectives for evaluating model biases and ethical issues that may not have\nbeen previously identified. Furthermore, a study by [228] brings attention to a significant concern:\nthe consistency of judgment in LLMs diminishes notably when faced with disruptions such as\n3The term \u2018trustworthiness\u2019 in this section refers to other work that contains more than robustness and ethics.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:15\nquestioning, negation, or misleading cues, even if their initial judgments were accurate. The\nresearch delves into various prompting methods designed to mitigate this issue and successfully\ndemonstrates their efficacy.\nLLMs are capable of generating coherent and seemingly factual text. However, the information\ngenerated can include factual inaccuracies or statements ungrounded in reality, a phenomenon\nknown as hallucination [163, 253]. Evaluating these issues helps improve the training methods of\nLLMs to reduce the occurrence of hallucinations. For the evaluation of illusions in large-scale visual\nmodels, Liu et al. [123] introduced a comprehensive and robust large-scale visual instruction dataset:\nLRV-Instruction. Through the GAVIE method, they fine-tuned the evaluation visual instructions,\nand experimental results demonstrated that LRV-Instruction effectively alleviates illusions in LLMs.\nIn addition, Li et al. [113] conducted an assessment of illusions in large-scale visual language models,\nrevealing through experiments that the distribution of objects in visual instructions significantly\nimpacts object illusions in LVLMs. To enhance the assessment of object illusions in LVLMs, they\nintroduced a polling-based query method, known as POPE. This method provides an improved\nevaluation of object illusions in LVLMs.\n3.3\nSocial Science\nSocial science involves the study of human society and individual behavior, including economics,\nsociology, political science, law, and other disciplines. Evaluating the performance of LLMs in\nsocial science is important for academic research, policy formulation, and social problem-solving.\nSuch evaluations can help improve the applicability and quality of models in the social sciences,\nincreasing understanding of human societies and promoting social progress.\nWu et al. [224] evaluated the potential use of LLMs in addressing scaling and measurement\nissues in social science and found that LLMs can generate meaningful responses regarding political\nideology and significantly improve text-as-data methods in social science.\nIn computational social science (CSS) tasks, Ziems et al. [269] presented a comprehensive evalu-\nation of LLMs on several CSS tasks. During classification tasks, LLMs exhibit the lowest absolute\nperformance on event argument extraction, character tropes, implicit hate, and empathy clas-\nsification, achieving accuracy below 40%. These tasks either involve complex structures (event\narguments) or subjective expert taxonomies with semantics that differ from those learned during\nLLM pretraining. Conversely, LLMs achieve the best performance on misinformation, stance, and\nemotion classification. When it comes to generation tasks, LLMs often produce explanations that\nsurpass the quality of gold references provided by crowd workers. In summary, while LLMs can\ngreatly enhance the traditional CSS research pipeline, they cannot completely replace it.\nSome articles also evaluate LLMs on legal tasks. The zero-shot performance of LLMs is mediocre\nin legal case judgment summarization. LLMs have several problems, including incomplete sen-\ntences and words, meaningless sentences merge, and more serious errors such as inconsistent\nand hallucinated information [34]. The results showed that further improvement is necessary for\nLLMs to be useful for case judgment summarization by legal experts. Nay et al. [139] indicated\nthat LLMs, particularly when combined with prompting enhancements and the correct legal texts,\ncould perform better but not yet at expert tax lawyer levels.\nLastly, within the realm of psychology, Frank [44] adopted an interdisciplinary approach and\ndrew insights from developmental psychology and comparative psychology to explore alternative\nmethods for evaluating the capabilities of LLMs. By integrating different perspectives, researchers\ncan deepen their understanding of the essence of cognition and effectively leverage the potential of\nadvanced technologies such as large language models, while mitigating potential risks.\nIn conclusion, the utilization of LLMs has significantly benefited individuals in addressing social\nscience-related tasks, leading to improved work efficiency. The outputs produced by LLMs serve as\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:16\nChang et al.\nTable 4. Summary of evaluations on natural science and engineering tasks based on three aspects:\nMathematics, General science and Engineering (ordered by the name of the first author).\nReference\nMathematics\nGeneral science\nEngineering\nArora et al. [3]\n\u2713\n\u2713\nBubeck et al. [15]\n\u2713\n\u2713\nCastro Nascimento and Pimentel [18]\n\u2713\nCollins et al. [27]\n\u2713\nDao and Le [31]\n\u2713\nGuo et al. [61]\n\u2713\nLiu et al. [125]\n\u2713\nPallagani et al. [150]\n\u2713\nSridhara et al. [181]\n\u2713\nValmeekam et al. [194]\n\u2713\nValmeekam et al. [195]\n\u2713\nWei et al. [221]\n\u2713\nWu et al. [225]\n\u2713\nYuan et al. [241]\n\u2713\nYu et al. [237]\n\u2713\nZhuang et al. [265]\n\u2713\nvaluable resources for enhancing productivity. However, it is crucial to acknowledge that existing\nLLMs cannot completely replace human professionals in this domain.\n3.4\nNatural Science and Engineering\nEvaluating the performance of LLMs in natural science and engineering can help guide applications\nand development in scientific research, technology development, and engineering studies.\n3.4.1\nMathematics. For fundamental mathematical problems, most large language models (LLMs)\ndemonstrate proficiency in addition and subtraction, and possess some capability in multiplication.\nHowever, they face challenges when it comes to division, exponentiation, trigonometry functions,\nand logarithm functions. On the other hand, LLMs exhibit competence in handling decimal numbers,\nnegative numbers, and irrational numbers [241]. In terms of performance, ChatGPT and GPT-4\noutperform other models significantly, showcasing their superiority in solving mathematical tasks\n[221]. These two models have a distinct advantage in dealing with large numbers (greater than\n1e12) and complex, lengthy mathematical queries. GPT-4 outperforms ChatGPT by achieving a\nsignificant increase in accuracy of 10 percentage points and a reduction in relative error by 50%,\ndue to its superior division and trigonometry abilities, proper understanding of irrational numbers,\nand consistent step-by-step calculation of long expressions.\nWhen confronted with complex and challenging mathematical problems, LLMs exhibit subpar\nperformance. Specifically, GPT-3 demonstrates nearly random performance, while GPT-3.5 shows\nimprovement, and GPT-4 performs the best [3]. Despite the advancements made in the new models,\nit is important to note that the peak performance remains relatively low compared to that of experts\nand these models lack the capability to engage in mathematical research [15]. The specific tasks of\nalgebraic manipulation and calculation continue to pose challenges for GPTs [15, 27]. The primary\nreasons behind GPT-4\u2019s low performance in these tasks are errors in algebraic manipulation and\ndifficulties in retrieving pertinent domain-specific concepts. Wu et al. [225] evaluated the use of\nGPT-4 on difficult high school competition problems and GPT-4 reached 60% accuracy on half of\nthe categories. Intermediate algebra and precalculus can only be solved with a low accuracy rate\nof around 20%. ChatGPT is not good at answering questions on topics including derivatives and\napplications, Oxyz spatial calculus, and spatial geometry [31]. Dao and Le [31], Wei et al. [221]\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:17\nshowed that ChatGPT\u2019s performance worsens as task difficulty increases: it correctly answered 83%\nof the questions at the recognition level, 62% at the comprehension level, 27% at the application\nlevel, and only 10% at the highest cognitive complexity level. Given those problems at higher\nknowledge levels tend to be more complex, requiring in-depth understanding and problem-solving\nskills, such results are to be expected.\nThese results indicate that the effectiveness of LLMs is highly influenced by the complexity of\nproblems they encounter. This finding holds significant implications for the design and development\nof optimized artificial intelligence systems capable of successfully handling these challenging tasks.\n3.4.2\nGeneral science. Further improvements are needed in the application of LLMs in the field of\nchemistry. Castro Nascimento and Pimentel [18] presented five straightforward tasks from various\nsubareas of chemistry to assess ChatGPT\u2019s comprehension of the subject, with accuracy ranging\nfrom 25% to 100%. Guo et al. [61] created a comprehensive benchmark that encompasses 8 practical\nchemistry tasks, which is designed to assess the performance of LLMs (including GPT-4, GPT-3.5,\nand Davinci-003) for each chemistry task. Based on the experiment results, GPT-4 demonstrates\nsuperior performance compared to the other two models. [3] showed that LLMs perform worse\non physics problems than chemistry problems, probably because chemistry problems have lower\ninference complexity than physics problems in this setting. There are limited evaluation studies on\nLLMs in the field of general science, and the current findings indicate that further improvement is\nneeded in the performance of LLMs within this domain.\n3.4.3\nEngineering. Within engineering, the tasks can be organized in ascending order of difficulty,\nincluding code generation, software engineering, and commonsense planning.\nIn code generation tasks, the smaller LLMs trained for the tasks are competitive in performance,\nand CodeGen-16B [141] is comparable in performance to ChatGPT using a larger parameter setting,\nreaching about a 78% match [125]. Despite facing challenges in mastering and comprehending\ncertain fundamental concepts in programming languages, ChatGPT showcases a commendable level\nof coding level [265]. Specifically, ChatGPT has developed superior skills in dynamic programming,\ngreedy algorithm, and search, surpassing highly capable college students, but it struggles in data\nstructure, tree, and graph theory. GPT-4 demonstrates an advanced ability to generate code based\non given instructions, comprehend existing code, reason about code execution, simulate the impact\nof instructions, articulate outcomes in natural language, and execute pseudocode effectively [15].\nIn software engineering tasks, ChatGPT generally performs well and provides detailed responses,\noften surpassing both human expert output and SOTA output. However, for certain tasks such as\ncode vulnerability detection and information retrieval-based test prioritization, the current version\nof ChatGPT fails to provide accurate answers, rendering it unsuitable for these specific tasks [181].\nIn commonsense planning tasks, LLMs may not perform well, even in simple planning tasks\nwhere humans excel [194, 195]. Pallagani et al. [150] demonstrated that the fine-tuned CodeT5\n[214] performs the best across all considered domains, with the shortest inference time. Moreover,\nit explored the capability of LLMs for plan generalization and found that their generalization\ncapabilities appear to be limited. It turns out that LLMs can handle simple engineering tasks, but\nthey perform poorly on complex engineering tasks.\n3.5\nMedical Applications\nThe application of LLMs in the medical field has recently received significant attention. As a\nresult, this section aims to provide a comprehensive review of the ongoing efforts dedicated to\nimplementing LLMs in medical applications. We have categorized these applications into three\naspects as shown in Table 5: medical query, medical examination, and medical assistants. A detailed\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:18\nChang et al.\nTable 5. Summary of evaluations on medical applications based on the three aspects: Medical queries,\nMedical assistants, and Medical examination (ordered by the name of the first author).\nReference\nMedical queries\nMedical examination\nMedical assistants\nCascella et al. [17]\n\u2713\nChervenak et al. [21]\n\u2713\nDuong and Solomon [39]\n\u2713\nGilson et al. [57]\n\u2713\nHamidi and Roberts [63]\n\u2713\nHolmes et al. [73]\n\u2713\nJahan et al. [81]\n\u2713\nJohnson et al. [87]\n\u2713\nKhan et al. [93]\n\u2713\nKung et al. [97]\n\u2713\nLahat et al. [99]\n\u2713\nLyu et al. [131]\n\u2713\nOh et al. [143]\n\u2713\nSamaan et al. [169]\n\u2713\nThirunavukarasu et al. [186]\n\u2713\nWang et al. [217]\n\u2713\nexamination of these categories will enhance our understanding of the potential impact and\nadvantages that LLMs can bring to the medical domain.\n3.5.1\nMedical queries. The significance of evaluating LLMs on medical queries lies in providing\naccurate and reliable medical answers to meet the needs of healthcare professionals and patients\nfor high-quality medical information. As shown in Table 5, the majority of LLMs evaluations\nin the medical field concentrate on medical queries. ChatGPT generated relatively accurate in-\nformation for various medical queries, including genetics [39], radiation oncology physics [73],\nbiomedicine [81], and many other medical disciplines [63, 87, 169], demonstrating its effectiveness\nin the field of medical queries to a certain extent. As for the limitations, Thirunavukarasu et al.\n[186] assessed ChatGPT\u2019s performance in primary care and found that its average score in the\nstudent comprehensive assessment falls below the passing score, indicating room for improvement.\nChervenak et al. [21] highlighted that while ChatGPT can generate responses similar to existing\nsources in fertility-related clinical prompts, its limitations in reliably citing sources and potential\nfor fabricating information restrict its clinical utility.\n3.5.2\nMedical examination. The studies by Gilson et al. [57], Kung et al. [97] have evaluated\nthe performance of LLMs in medical examination assessment through the United States Medical\nLicensing Examination (USMLE) 4. In the study of [57], ChatGPT\u2019s performance in answering\nUSMLE Step 1 and Step 2 exam questions was assessed using novel multiple-choice question sets.\nThe results indicated that ChatGPT achieves varying accuracies across different datasets. However,\nthe presence of out-of-context information was found to be lower compared to the correct answer\nin the NBME-Free-Step1 and NBME-Free-Step2 datasets. Kung et al. [97] showed that ChatGPT\nachieves or approaches the passing threshold in these exams with no tailored training. The model\ndemonstrates high consistency and insight, indicating its potential to assist in medical education\nand clinical decision-making. ChatGPT can be used as a tool to answer medical questions, provide\nexplanations, and support decision-making processes. This offers additional resources and support\nfor medical students and clinicians in their educational and clinical practices. Moreover, Sharma\n4https://www.usmle.org/\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:19\net al. [173] found that answers generated by ChatGPT are more context-aware with better deductive\nreasoning abilities compared to Google search results.\n3.5.3\nMedical assistants. In the field of medical assistance, LLMs demonstrate potential applica-\ntions, including research on identifying gastrointestinal diseases [99], dementia diagnosis [217],\naccelerating the evaluation of COVID-19 literature [93], and their overall potential in healthcare [17].\nHowever, there are also limitations and challenges, such as lack of originality, high input require-\nments, resource constraints, uncertainty in answers, and potential risks related to misdiagnosis\nand patient privacy issues.\nMoreover, several studies have evaluated the performance and feasibility of ChatGPT in the\nmedical education field. In the study by Oh et al. [143], ChatGPT, specifically GPT-3.5 and GPT-4\nmodels, were evaluated in terms of their understanding of surgical clinical information and their\npotential impact on surgical education and training. The results indicate an overall accuracy of 46.8%\nfor GPT-3.5 and 76.4% for GPT-4, demonstrating a significant performance difference between the\ntwo models. Notably, GPT-4 consistently performs well across different subspecialties, suggesting its\ncapability to comprehend complex clinical information and enhance surgical education and training.\nAnother study by Lyu et al. [131] explores the feasibility of utilizing ChatGPT in clinical education,\nparticularly in translating radiology reports into easily understandable language. The findings\ndemonstrate that ChatGPT effectively translates radiology reports into accessible language and\nprovides general recommendations. Furthermore, the quality of ChatGPT has shown improvement\ncompared to GPT-4. These findings suggest that employing LLMs in clinical education is feasible,\nalthough further efforts are needed to address limitations and unlock their full potential.\n3.6\nAgent Applications\nInstead of focusing solely on general language tasks, LLMs can be utilized as powerful tools\nin various domains. Equipping LLMs with external tools can greatly expand the capabilities of\nthe model [160]. ToolLLM [161] provides a comprehensive framework to equip open-source large\nlanguage models with tool use capabilities. Huang et al. [77] introduced KOSMOS-1, which is capable\nof understanding general patterns, following instructions, and learning based on context. The study\nby MRKL Karpas et al. [90] emphasized the importance of understanding when and how to utilize\nexternal symbolic tools, as this knowledge is dependent on the capabilities of LLMs, particularly\nwhen these tools can reliably perform functions. Additionally, two other studies, Toolformer [172]\nand TALM [152], explored the utilization of tools to enhance language models. Toolformer employs\na training approach to determine the optimal usage of specific APIs and integrates the obtained\nresults into subsequent token predictions. On the other hand, TALM combines indistinguishable\ntools with text-based methods to augment language models and employs an iterative technique\nknown as \u201cself-play\", guided by minimal tool demonstrations. Furthermore, Shen et al. [174]\nproposed the HuggingGPT framework, which leverages LLMs to connect various AI models within\nthe machine learning community (such as Hugging Face), aiming to address AI tasks.\n3.7\nOther Applications\nIn addition to above areas, there have been evaluations in various other domains, including educa-\ntion, search and recommendation, personality testing, and specific applications.\n3.7.1\nEducation. LLMs have shown promise in revolutionizing the field of education. They have\nthe potential to make significant contributions in several areas, such as assisting students in\nimproving their writing skills, facilitating better comprehension of complex concepts, expediting\nthe delivery of information, and providing personalized feedback to enhance student engagement.\nThese applications aim to create more efficient and interactive learning experiences, offering\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:20\nChang et al.\nTable 6. Summary of evaluations on other applications based on the four aspects: Education, Search and\nrecommendation, Personality testing and Specific applications (ordered by the name of the first author).\nReference\nEducation\nSearch and recommendation\nPersonality testing\nSpecific applications\nBodroza et al. [10]\n\u2713\nDai et al. [30]\n\u2713\nde Winter [32]\n\u2713\nDai et al. [29]\n\u2713\nFan et al. [40]\n\u2713\nHellas et al. [67]\n\u2713\nJentzsch and Kersting [84]\n\u2713\nLanzi and Loiacono [101]\n\u2713\nLe and Zhang [103]\n\u2713\nLi et al. [110]\n\u2713\nLiang et al. [115]\n\u2713\nSun et al. [183]\n\u2713\nSong et al. [180]\n\u2713\nSafdari et al. [168]\n\u2713\nThakur et al. [185]\n\u2713\nWang and Demszky [210]\n\u2713\nWang et al. [212]\n\u2713\nWang et al. [216]\n\u2713\nXu et al. [232]\n\u2713\nYuan et al. [240]\n\u2713\nZhang et al. [246]\n\u2713\nstudents a broader range of educational opportunities. However, to fully harness the potential of\nLLMs in education, extensive research, and ongoing refinement are necessary.\nThe evaluation of LLMs for educational assistance aims to investigate and assess their po-\ntential contributions to the field of education. Such evaluations can be conducted from various\nperspectives. According to Dai et al. [30], ChatGPT demonstrates the ability to generate detailed,\nfluent, and coherent feedback that surpasses that of human teachers. It can accurately assess student\nassignments and provide feedback on task completion, thereby assisting in the development of\nstudent skills. However, ChatGPT\u2019s responses may lack novelty or insightful perspectives regarding\nteaching improvement [210]. Additionally, the study conducted by Hellas et al. [67] revealed that\nLLMs can successfully identify at least one actual problem in student code, although instances of\nmisjudgment are also observed. In conclusion, the utilization of LLMs shows promise in addressing\nprogram logic issues, although challenges remain in achieving proficiency in output formatting. It\nis important to note that while these models can provide valuable insights, they may still generate\nerrors similar to those made by students.\nIn educational exams, researchers aim to evaluate the application effectiveness of LLMs,\nincluding automatic scoring, question generation, and learning guidance. de Winter [32] showed\nthat ChatGPT achieves an average of 71.8% correctness, which is comparable to the average score of\nall participating students. Subsequently, the evaluation was conducted using GPT-4, and it achieved\na score of 8.33. Furthermore, this evaluation showed the effectiveness of leveraging bootstrapping\nthat combines randomness via the \u201ctemperature\u201d parameter in diagnosing incorrect answers. Zhang\net al. [248] claimed that GPT-3.5 can solve MIT math and EECS exams with GPT-4 achieving better\nperformance. However, it turned out to be not fair since they accidentally included the correct\nanswers into the prompts.\n3.7.2\nSearch and recommendation. The assessment of LLMs in search and recommendation can\nbe broadly categorized into two areas. Firstly, in the realm of information retrieval, Sun et al.\n[183] investigated the effectiveness of generative ranking algorithms, such as ChatGPT and GPT-4,\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:21\nfor information retrieval tasks. Experimental results demonstrate that guided ChatGPT and GPT-4\nexhibit competitive performance on popular benchmark tests, even outperforming supervised\nmethods. Additionally, the extraction of ChatGPT\u2019s ranking functionality into a specialized model\nshows superior performance when trained on 10K ChatGPT-generated data compared to training on\n400K annotated MS MARCO data in the BEIR dataset [185]. Furthermore, Xu et al. [232] conducted\na randomized online experiment to investigate the behavioral differences of users when performing\ninformation retrieval tasks using search engines and chatbot tools. Participants were divided into\ntwo groups: one using tools similar to ChatGPT and the other using tools similar to Google Search.\nThe results show that the ChatGPT group spent less time on all tasks and the difference between\nthese two groups is not significant.\nSecondly, moving to the domain of recommendation systems, LLMs have emerged as es-\nsential components that leverage their natural language processing capabilities to comprehend\nuser preferences, item descriptions, and contextual information [40]. By incorporating LLMs into\nrecommendation pipelines, these systems can offer more accurate and personalized recommen-\ndations, thereby improving user experience and overall recommendation quality. However, it is\ncrucial to address the potential risks associated with using LLMs for recommendations. Recent\nresearch by Zhang et al. [246] has highlighted the issue of unfair recommendations generated\nby ChatGPT. This emphasizes the importance of evaluating fairness when employing LLMs in\nrecommendation scenarios. Dai et al. [29] suggest that ChatGPT exhibits strong performance in\nrecommender systems. The use of listwise ranking is found to strike the best balance between cost\nand performance. Furthermore, ChatGPT shows promise in addressing the cold-start problem and\nproviding interpretable recommendations. Moreover, the research by Yuan et al. [240] and Li et al.\n[110] demonstrated the promising potential of the modality-based recommendation model (MoRec)\nand text-based collaborative filtering (TCF) in recommendation systems.\n3.7.3\nPersonality testing. Personality testing aims to measure individuals\u2019 personality traits and\nbehavioral tendencies, and LLMs as powerful natural language processing models have been widely\napplied in such tasks.\nResearch conducted by Bodroza et al. [10] investigated the personality features of using Davinci-\n003 as a chatbot and found variations in the consistency of its answers, despite exhibiting prosocial\ncharacteristics. However, there remains uncertainty regarding whether the chatbot\u2019s responses are\ndriven by conscious self-reflection or algorithmic processes. Song et al. [180] examined the mani-\nfestation of personality in language models and discovered that many models perform unreliably\nin self-assessment tests and exhibit inherent biases. Therefore, it is necessary to develop specific\nmachine personality measurement tools to enhance reliability. These studies offer vital insights\nto better understand LLMs in personality testing. Safdari et al. [168] proposed a comprehensive\napproach to conduct effective psychometric testing for the personality traits in the text generated\nby LLMs. In order to evaluate the emotional intelligence of LLMs, Wang et al. [212] developed a\nnew psychometric assessment method. By referencing a framework constructed from over 500\nadults, the authors tested various mainstream LLMs. The results showed that most LLMs achieve\nabove-average scores in emotional quotient (EQ), with GPT-4 scoring 117, surpassing 89% of hu-\nman participants. However, a multivariate pattern analysis indicated that certain LLMs achieve\nhuman-level performance without relying on mechanisms resembling those found in humans. This\nis evident from the distinct differences in the quality of their representational patterns, as compared\nto humans. Liang et al. [115] employed the word guessing game to evaluate LLMs\u2019 language and\ntheory of mind intelligences, a more engaging and interactive assessment method. Jentzsch and\nKersting [84] discussed the challenges of incorporating humor into LLMs, particularly ChatGPT.\nThey found that while ChatGPT demonstrates impressive capabilities in NLP tasks, it falls short\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:22\nChang et al.\nTable 7. Summary of existing LLMs evaluation benchmarks (ordered by the name of the first author).\nBenchmark\nFocus\nDomain\nEvaluation Criteria\nSOCKET [23]\nSocial knowledge\nSpecific downstream task\nSocial language understanding\nMME [46]\nMultimodal LLMs\nMulti-modal task\nAbility of perception and cognition\nXiezhi [59]\nComprehensive domain knowledge\nGeneral language task\nOverall performance across multiple benchmarks\nChoice-75 [75]\nScript learning\nSpecific downstream task\nOverall performance of LLMs\nCUAD [71]\nLegal contract review\nSpecific downstream task\nLegal contract understanding\nTRUSTGPT [79]\nEthic\nSpecific downstream task\nToxicity, bias, and value-alignment\nMMLU [70]\nText models\nGeneral language task\nMultitask accuracy\nMATH [72]\nMathematical problem\nSpecific downstream task\nMathematical ability\nAPPS [68]\nCoding challenge competence\nSpecific downstream task\nCode generation ability\nCELLO [66]\nComplex instructions\nSpecific downstream task\nFour designated evaluation criteria\nC-Eval [78]\nChinese evaluation\nGeneral language task\n52 Exams in a Chinese context\nEmotionBench [76]\nEmpathy ability\nSpecific downstream task\nEmotional changes\nOpenLLM [80]\nChatbots\nGeneral language task\nLeaderboard rankings\nDynaBench [94]\nDynamic evaluation\nGeneral language task\nNLI, QA, sentiment, and hate speech\nChatbot Arena [128]\nChat assistants\nGeneral language task\nCrowdsourcing and Elo rating system\nAlpacaEval [112]\nAutomated evaluation\nGeneral language task\nMetrics, robustness, and diversity\nCMMLU [108]\nChinese multi-tasking\nSpecific downstream task\nMulti-task language understanding capabilities\nHELM [114]\nHolistic evaluation\nGeneral language task\nMulti-metric\nAPI-Bank [109]\nTool utilization\nSpecific downstream task\nAPI call, retrieval, and planning\nM3KE [122]\nMulti-task\nSpecific downstream task\nMulti-task accuracy\nMMBench [126]\nLarge vision-language models(LVLMs)\nMulti-modal task\nMultifaceted capabilities of VLMs\nSEED-Bench [107]\nMultimodal Large Language Models\nMulti-modal task\nGenerative understanding of MLLMs\nUHGEval [116]\nHallucination of Chinese LLMs\nSpecific downstream task\nForm, metric and granularity\nARB [171]\nAdvanced reasoning ability\nSpecific downstream task\nMultidomain advanced reasoning ability\nBIG-bench [182]\nCapabilities and limitations of LMs\nGeneral language task\nModel performance and calibration\nMultiMedQA [177]\nMedical QA\nSpecific downstream task\nAccuracy and human evaluation\nCVALUES [230]\nSafety and responsibility\nSpecific downstream task\nAlignment ability of LLMs\nLVLM-eHub [231]\nLVLMs\nMulti-modal task\nMultimodal capabilities of LVLMs\nToolBench [191]\nSoftware tools\nSpecific downstream task\nExecution success rate\nFRESHQA [198]\nDynamic QA\nSpecific downstream task\nCorrectness and hallucination\nCMB [211]\nChinese comprehensive medicine\nSpecific downstream task\nExpert evaluation and automatic evaluation\nPandaLM [216]\nInstruction tuning\nGeneral language task\nWinrate judged by PandaLM\nMINT [213]\nMulti-turn interaction\nSpecific downstream task\nSuccess rate with k-turn budget \ud835\udc46\ud835\udc45\ud835\udc58\nDialogue CoT [205]\nIn-depth dialogue\nSpecific downstream task\nHelpfulness and acceptness of LLMs\nBOSS [239]\nOOD robustness in NLP\nGeneral language task\nOOD robustness\nMM-Vet [238]\nComplicated multi-modal tasks\nMulti-modal task\nIntegrated vision-language capabilities\nLAMM [235]\nMulti-modal point clouds\nMulti-modal task\nTask-specific metrics\nGLUE-X [234]\nOOD robustness for NLP tasks\nGeneral language task\nOOD robustness\nKoLA [236]\nKnowledge-oriented evaluation\nGeneral language task\nSelf-contrast metrics\nAGIEval [262]\nHuman-centered foundational models\nGeneral language task\nGeneral\nPromptBench [264]\nAdversarial prompt resilience\nGeneral language task\nAdversarial robustness\nMT-Bench [260]\nMulti-turn conversation\nGeneral language task\nWinrate judged by GPT-4\nM3Exam [250]\nMultilingual, multimodal and multilevel\nSpecific downstream task\nTask-specific metrics\nGAOKAO-Bench [245]\nChinese Gaokao examination\nSpecific downstream task\nAccuracy and scoring rate\nSafetyBench [254]\nSafety\nSpecific downstream task\nSafety abilities of LLMs\nLLMEval2 [252]\nLLM Evaluator\nGeneral language task\nAcc, macro-f1 and kappa correlation coefficient\nin generating humorous responses. This study emphasizes the importance of humor in human\ncommunication and the difficulties that LLMs face in capturing the subtleties and context-dependent\nnature of humor. It discusses the limitations of current approaches and highlights the need for\nfurther research on more sophisticated models that can effectively understand and generate humor.\n3.7.4\nSpecific applications. Moreover, various research endeavors have been conducted to explore\nthe application and evaluation of LLMs across a wide spectrum of tasks, such as game design\n[101], model performance assessment [216], and log parsing [103]. Collectively, these findings\nenhance our comprehension of the practical implications associated with the utilization of LLMs\nacross diverse tasks. They shed light on the potential and limitations of these models while providing\nvaluable insights for performance improvement.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:23\n4\nWHERE TO EVALUATE: DATASETS AND BENCHMARKS\nLLMs evaluation datasets are used to test and compare the performance of different language models\non various tasks, as depicted in Sec. 3. These datasets, such as GLUE [200] and SuperGLUE [199],\naim to simulate real-world language processing scenarios and cover diverse tasks such as text\nclassification, machine translation, reading comprehension, and dialogue generation. This section\nwill not discuss any single dataset for language models but benchmarks for LLMs.\nA variety of benchmarks have emerged to evaluate their performance. In this study, we compile\na selection of 46 popular benchmarks, as shown in Table 7.5 Each benchmark focuses on different\naspects and evaluation criteria, providing valuable contributions to their respective domains. For a\nbetter summarization, we divide these benchmarks into three categories: benchmarks for general\nlanguage tasks, benchmarks for specific downstream tasks, and benchmarks for multi-modal tasks.\n4.1\nBenchmarks for General Tasks\nLLMs are designed to solve a vast majority of tasks. To this end, existing benchmarks tend to\nevaluate the performance in different tasks.\nChatbot Arena [128] and MT-Bench [260] are two significant benchmarks that contribute to the\nevaluation and advancement of chatbot models and LLMs in different contexts. Chatbot Arena\nprovides a platform to assess and compare diverse chatbot models through user engagement and\nvoting. Users can engage with anonymous models and express their preferences via voting. The\nplatform gathers a significant volume of votes, facilitating the evaluation of models\u2019 performance\nin realistic scenarios. Chatbot Arena provides valuable insights into the strengths and limitations\nof chatbot models, thereby contributing to the progress of chatbot research and advancement.\nMeanwhile, MT-Bench evaluates LLMs on multi-turn dialogues using comprehensive questions\ntailored to handling conversations. It provides a comprehensive set of questions specifically designed\nfor assessing the capabilities of models in handling multi-turn dialogues. MT-Bench possesses\nseveral distinguishing features that differentiate it from conventional evaluation methodologies.\nNotably, it excels in simulating dialogue scenarios representative of real-world settings, thereby\nfacilitating a more precise evaluation of a model\u2019s practical performance. Moreover, MT-Bench\neffectively overcomes the limitations in traditional evaluation approaches, particularly in gauging\na model\u2019s competence in handling intricate multi-turn dialogue inquiries.\nInstead of focusing on specific tasks and evaluation metrics, HELM [114] provides a comprehen-\nsive assessment of LLMs. It evaluates language models across various aspects such as language\nunderstanding, generation, coherence, context sensitivity, common-sense reasoning, and domain-\nspecific knowledge. HELM aims to holistically evaluate the performance of language models across\ndifferent tasks and domains. For LLMs Evaluator, Zhang et al. [252] introduces LLMEval2, which en-\ncompasses a wide range of capability evaluations. In addition, Xiezhi [59] presents a comprehensive\nsuite for assessing the knowledge level of large-scale language models in different subject areas. The\nevaluation conducted through Xiezhi enables researchers to comprehend the notable limitations\ninherent in these models and facilitates a deeper comprehension of their capabilities in diverse\nfields. For evaluating language models beyond their existing capacities, BIG-bench [182] introduces\na diverse collection of 204 challenging tasks contributed by 450 authors from 132 institutions.\nThese tasks cover various domains such as math, childhood development, linguistics, biology,\ncommon-sense reasoning, social bias, physics, software development, etc.\nRecent work has led to the development of benchmarks for evaluating language models\u2019 knowl-\nedge and reasoning abilities. The Knowledge-Oriented Language Model Evaluation KoLA [236]\n5Note that as the evaluation of LLMs is a hot research area, it is very likely that we cannot cover all benchmarks. We\nwelcome suggestions and comments to make this list perfect.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:24\nChang et al.\nfocuses on assessing language models\u2019 comprehension and utilization of semantic knowledge\nfor inference. As such, KoLA serves as an important benchmark for evaluating the depth of lan-\nguage understanding and reasoning in language models, thereby driving progress in language\ncomprehension. To enable crowd-sourced evaluations of language tasks, DynaBench [94] supports\ndynamic benchmark testing. DynaBench explores new research directions including the effects of\nclosed-loop integration, distributional shift characteristics, annotator efficiency, influence of expert\nannotators, and model robustness to adversarial attacks in interactive settings. Furthermore, to\nevaluate language models\u2019 ability to learn and apply multidisciplinary knowledge across educational\nlevels, the Multidisciplinary Knowledge Evaluation M3KE [122] was recently introduced. M3KE\nassesses knowledge application within the Chinese education system.\nThe development of standardized benchmarks for evaluating LLMs on diverse tasks has been an\nimportant research focus. MMLU [70] provides a comprehensive suite of tests for assessing text\nmodels in multi-task contexts. AlpacaEval [112] stands as an automated evaluation benchmark,\nwhich places its focus on assessing the performance of LLMs across various natural language\nprocessing tasks. It provides a range of metrics, robustness measures, and diversity evaluations\nto gauge the capabilities of LLMs. AlpacaEval has significantly contributed to advancing LLMs in\ndiverse domains and promoting a deeper understanding of their performance. Furthermore, AGIEval\n[262], serves as a dedicated evaluation framework for assessing the performance of foundation\nmodels in the domain of human-centric standardized exams. Moreover, OpenLLM [80] functions as\nan evaluation benchmark by offering a public competition platform for comparing and assessing\ndifferent LLM models\u2019 performance on various tasks. It encourages researchers to submit their\nmodels and compete on different tasks, driving progress and competition in LLM research.\nAs for tasks beyond standard performance, there are benchmarks designed for OOD, adversarial\nrobustness, and fine-tuning. GLUE-X [234] is a novel attempt to create a unified benchmark\naimed at evaluating the robustness of NLP models in OOD scenarios. This benchmark emphasizes\nthe significance of robustness in NLP and provides insights into measuring and enhancing the\nrobustness of models. In addition, Yuan et al. [239] presents BOSS, a benchmark collection for\nassessing out-of-distribution robustness in natural language processing tasks. PromptBench [264]\ncenters on the importance of prompt engineering in fine-tuning LLMs. It provides a standardized\nevaluation framework to compare different prompt engineering techniques and assess their impact\non model performance. PromptBench facilitates the enhancement and optimization of fine-tuning\nmethods for LLMs. To ensure impartial and equitable evaluation, PandaLM [216] is introduced as a\ndiscriminative large-scale language model specifically designed to differentiate among multiple\nhigh-proficiency LLMs through training. In contrast to conventional evaluation datasets that\npredominantly emphasize objective correctness, PandaLM incorporates crucial subjective elements,\nincluding relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality.\n4.2\nBenchmarks for Specific Downstream Tasks\nOther than benchmarks for general tasks, there exist benchmarks specifically designed for certain\ndownstream tasks.\nQuestion-answering benchmarks have become a fundamental component in the assessment of\nLLMs and their overall performance. MultiMedQA [177] is a medical QA benchmark that focuses\non medical examinations, medical research, and consumer healthcare questions. It consists of seven\ndatasets related to medical QA, including six existing datasets and one new dataset. The goal of\nthis benchmark is to evaluate the performance of LLMs in terms of clinical knowledge and QA\nabilities. To assess the ability of LLMs in dynamic QA about current world knowledge, Vu et al.\n[198] introduced FRESHQA. By incorporating relevant and current information retrieved from\nsearch engines into prompts, there is a significant enhancement in the performance of LLMs on\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:25\nFRESHQA. To effectively assess in-depth dialogue, Wang et al. [205] introduced the Dialogue CoT,\nincorporating two efficient dialogue strategies: Explicit CoT and CoT.\nThe assessment of LLMs in diverse and demanding tasks has garnered substantial attention in\nrecent research. To this end, a range of specialized benchmarks have been introduced to evalu-\nate LLMs\u2019 capabilities in specific domains and applications. Among these, ARB, as presented by\nSawada et al. [171], focuses on probing the performance of LLMs in advanced reasoning tasks\nspanning multiple domains. Additionally, ethical considerations in LLMs have become an area\nof paramount importance. TRUSTGPT, as tailored by Huang et al. [79], addresses critical ethical\ndimensions, including toxicity, bias, and value alignment, within the context of LLMs. Furthermore,\nthe simulation of human emotional reactions by LLMs remains an area with significant potential\nfor improvement, as highlighted by the EmotionBench benchmark by Huang et al. [76]. In terms\nof security evaluation, Zhang et al. [254] have introduced SafetyBench, a benchmark specifically\ndesigned to test the security performance of a range of popular Chinese and English LLMs. The\nresults of this evaluation reveal substantial security flaws in current LLMs. To evaluate the daily\ndecision-making capabilities of intelligent systems, Hou et al. [75] introduced Choice-75. Addition-\nally, to assess LLMs\u2019 aptitude in understanding complex instructions, He et al. [66] have introduced\nCELLO. This benchmark encompasses the design of eight distinctive features, the development of a\ncomprehensive evaluation dataset, and the establishment of four evaluation criteria alongside their\nrespective measurement standards.\nOther specific benchmarks such as C-Eval [78], which is the first extensive benchmark to assess\nthe advanced knowledge and reasoning capabilities of foundation models in Chinese. Additionally, Li\net al. [108] introduces CMMLU as a comprehensive Chinese proficiency standard and evaluates the\nperformance of 18 LLMs across various academic disciplines. The findings reveal that the majority\nof LLMs demonstrate suboptimal performance in Chinese language environments, highlighting\nareas for improvement. M3Exam [250] provides a unique and comprehensive evaluation framework\nthat incorporates multiple languages, modalities, and levels to test the general capabilities of LLMs\nin diverse contexts. Additionally, GAOKAO-Bench [245] provides a comprehensive evaluation\nbenchmark for gauging the proficiency of large language models in intricate and context-specific\ntasks, utilizing questions sourced from the Chinese Gaokao examination. On the other hand,\nSOCKET [23] serves as an NLP benchmark designed to evaluate the performance of LLMs in\nlearning and recognizing social knowledge concepts. It consists of several tasks and case studies to\nassess the limitations of LLMs in social capabilities. MATH [72] concentrates on assessing reasoning\nand problem-solving proficiencies of AI models within the domain of mathematics. APPS [68] is a\nmore comprehensive and rigorous benchmark for evaluating code generation, measuring the ability\nof language models to generate python code according to natural language specifications. CUAD [71]\nis an expert-annotated, domain-specific legal contract review dataset that presents a challenging\nresearch benchmark and potential for enhancing deep learning models\u2019 performance in contract\nunderstanding tasks. CVALUES [230] introduces a humanistic evaluation benchmark to assess the\nalignment of LLMs with safety and responsibility standards. In the realm of comprehensive Chinese\nmedicine, Wang et al. [211] introduced CMB, a medical evaluation benchmark rooted in the Chinese\nlanguage and culture. It addresses the potential inconsistency in the local context that may arise\nfrom relying solely on English-based medical assessments. In the realm of hallucination assessment,\n[116] has developed UHGEval, a benchmark specifically designed to evaluate the performance of\nChinese LLMs in text generation without being constrained by hallucination-related limitations.\nIn addition to existing evaluation benchmarks, there is a research gap in assessing the effectiveness\nof utilizing tools for LLMs. To address this gap, the API-Bank benchmark [109] is introduced as\nthe first benchmark explicitly designed for tool-augmented LLMs. It comprises a comprehensive\nTool-Augmented LLM workflow, encompassing 53 commonly used API tools and 264 annotated\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:26\nChang et al.\nTable 8. Summary of new LLMs evaluation protocols.\nMethod\nReferences\nHuman-in-the-loop\nAdaVision [50], AdaTest [164]\nCrowd-sourcing testing\nDynaBench [94], DynaBoard [132], DynamicTempLAMA [135], DynaTask [188]\nMore challenging tests\nHELM [114], AdaFilter [157], CheckList [165], Big-Bench [182], DeepTest [190]\ndialogues, encompassing a total of 568 API calls. Furthermore, the ToolBench project [191] aims\nto empower the development of large language models that effectively leverage the capabilities\nof general-purpose tools. By providing a platform for creating optimized instruction datasets,\nthe ToolBench project seeks to drive progress in language models and enhance their practical\napplications. To evaluate LLMs in multi-turn interactions, Wang et al. [213] proposed MINT, which\nutilizes tools and natural language feedback.\n4.3\nBenchmarks for Multi-modal task\nFor the evaluation of Multimodal Large Language Models (MLLMs), MME [46] serves as an extensive\nevaluative benchmark, aiming to assess their perceptual and cognitive aptitudes. It employs meticu-\nlously crafted instruction-answer pairs alongside succinct instruction design, thereby guaranteeing\nequitable evaluation conditions. To robustly evaluate large-scale vision-language models, Liu et al.\n[126] introduced MMBench, which comprises a comprehensive dataset and employs a CircularEval\nassessment method. Additionally, MMICL [255] enhances visual language models for multimodal\ninputs and excels in tasks such as MME and MMBench. Furthermore, LAMM [235] extends its\nresearch to encompass multimodal point clouds. LVLM-eHub [231] undertakes an exhaustive\nevaluation of LVLMs using an online competitive platform and quantitative capacity assessments.\nTo comprehensively assess the generative and understanding capabilities of Multi-modal Large\nLanguage Models (MLLMs), Li et al. [107] introduced a novel benchmark named SEED-Bench.\nThis benchmark consists of 19,000 multiple-choice questions that have been annotated by human\nassessors. Additionally, the evaluation covers 12 different aspects, including the models\u2019 proficiency\nin understanding patterns within images and videos. In summary, recent works have developed\nrobust benchmarks and improved models that advance the study of multimodal languages.\n5\nHOW TO EVALUATE\nIn this section, we introduce two common evaluation methods: automatic evaluation and human\nevaluation. Our categorization is based on whether or not the evaluation criterion can be automati-\ncally computed. If it can be automatically calculated, we categorize it into automatic evaluation;\notherwise, it falls into human evaluation.\n5.1\nAutomatic Evaluation\nAutomated evaluation is a common, and perhaps the most popular, evaluation method that typically\nuses standard metrics and evaluation tools to evaluate model performance. Compared with human\nevaluation, automatic evaluation does not require intensive human participation, which not only\nsaves time, but also reduces the impact of human subjective factors and makes the evaluation process\nmore standardized. For example, both Qin et al. [159] and Bang et al. [6] use automated evaluation\nmethods to evaluate a large number of tasks. Recently, with the development of LLMs, some\nadvanced automatic evaluation techniques are also designed to help evaluate. Lin and Chen [121]\nproposed LLM-EVAL, a unified multidimensional automatic evaluation method for open-domain\nconversations with LLMs. PandaLM [216] can achieve reproducible and automated language model\nassessment by training an LLM that serves as the \u201cjudge\u201d to evaluate different models. Proposing a\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:27\nTable 9. Key metrics of automatic evaluation.\nGeneral metrics\nMetrics\nAccuracy\nExact match, Quasi-exact match, F1 score, ROUGE score [118]\nCalibrations\nExpected calibration error [60], Area under the curve [54]\nFairness\nDemographic parity difference [242], Equalized odds difference [64]\nRobustness\nAttack success rate [203], Performance drop rate [264]\nself-supervised evaluation framework, Jain et al. [82] enabled a more efficient form of evaluating\nmodels in real-world deployment by eliminating the need for laborious labeling of new data. In\naddition, many benchmarks also apply automatic evaluation, such as MMLU [70], HELM[114],\nC-Eval [78], AGIEval [262], AlpacaFarm [38], Chatbot Arena [128], etc.\nBased on the literature that adopted automatic evaluation, we summarized the main metrics in\nautomatic evaluation in Table 9. The key metrics include the following four aspects:\n(1) Accuracy is a measure of how correct a model is on a given task. The concept of accuracy\nmay vary in different scenarios and is dependent on the specific task and problem definition.\nIt can be measured using various metrics such as Exact Match, F1 score, and ROUGE score.\n\u2022 Exact Match (EM) is a metric used to evaluate whether the model\u2019s output in text generation\ntasks precisely matches the reference answer. In question answering tasks, if the model\u2019s\ngenerated answer is an exact match with the manually provided answer, the EM is 1;\notherwise, it is 0.\n\u2022 The F1 score is a metric for evaluating the performance of binary classification models,\ncombining the model\u2019s precision and recall. The formula for calculation is as follows: \ud835\udc391 =\n2\u00d7\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\u00d7\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b+\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 .\n\u2022 ROUGE is primarily employed to assess the performance of tasks such as text summarization\nand machine translation, involving considerations of overlap and matching between texts.\n(2) Calibrations pertains to the degree of agreement between the confidence level of the model\noutput and the actual prediction accuracy.\n\u2022 Expected Calibration Error (ECE) is one of the commonly used metrics to evaluate model\ncalibration performance [60]. Tian et al. [189] utilized ECE to study the calibration of RLHF-\nLMs, including ChatGPT, GPT-4, Claude 1, Claude 2 and Llama2. For the calculation of ECE,\nthey categorize model predictions based on confidence and measure the average accuracy\nof the predictions within each confidence interval.\n\u2022 Area Under the Curve of selective accuracy and coverage (AUC) [54] is another commonly\nused metric.\n(3) Fairness refers to whether the model treats different groups consistently, that is, whether\nthe model\u2019s performance is equal across different groups. This can include attributes such\nas gender, race, age, and more. DecodingTrust [201] employs the following two metrics for\nmeasuring fairness:\n\u2022 Demographic Parity Pifference (DPD) measures whether the model\u2019s predictions are dis-\ntributed equally across different population groups. If predictions differ significantly between\ngroups, the DPD is high, indicating that the model may be unfairly biased against different\ngroups. The calculation of DPD involves the prediction of the model and the true label,\nand the following formula can be used: \ud835\udc37\ud835\udc43\ud835\udc37 = \ud835\udc43( \u02c6\ud835\udc66|\ud835\udc4d = 1) \u2212 \ud835\udc43( \u02c6\ud835\udc66|\ud835\udc4d = 0), where \u02c6\ud835\udc66 is the\nbinary classification prediction of the model, Z is the identifier of the population group\n(usually binary, indicating two different groups, such as men and women), \ud835\udc43( \u02c6\ud835\udc66|\ud835\udc4d = 1)\nand \ud835\udc43( \u02c6\ud835\udc66|\ud835\udc4d = 0) respectively represent the probabilities of predicting the positive class in\npopulation \ud835\udc4d = 1 and \ud835\udc4d = 0.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:28\nChang et al.\n\u2022 Equalized Odds Difference (EOD) aims to ensure that the model provides equal error rates\nacross different populations, that is, the model\u2019s prediction error probability distribution is\nsimilar for different populations. The calculation of EOD involves probabilities related to true\npositive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. The\nformula for EOD is as follows: \ud835\udc5a\ud835\udc4e\ud835\udc65{\ud835\udc43( \u02c6\ud835\udc66 = 1|\ud835\udc4c = 1,\ud835\udc4d = 1) \u2212 \ud835\udc43( \u02c6\ud835\udc66 = 1|\ud835\udc4c = 1,\ud835\udc4d = 0), \ud835\udc43( \u02c6\ud835\udc66 =\n1|\ud835\udc4c = 0,\ud835\udc4d = 1) \u2212 \ud835\udc43( \u02c6\ud835\udc66 = 1|\ud835\udc4c = 0,\ud835\udc4d = 0)} where \u02c6\ud835\udc66 is the binary classification prediction\nof the model, \ud835\udc4c is the true label, \ud835\udc4d is the demographic group identifier (typically binary,\nrepresenting two different groups), and \ud835\udc43( \u02c6\ud835\udc66 = 1|\ud835\udc4c = 1,\ud835\udc4d = 1) denotes the probability of the\nmodel predicting a positive class when the true label is positive and belongs to group \ud835\udc4d = 1.\n(4) Robustness evaluates the performance of a model in the face of various challenging inputs,\nincluding adversarial attacks, changes in data distribution, noise, etc.\n\u2022 Attack Success Rate (ASR) serves as a metric for evaluating the adversarial robustness of\nLLMs [206]. Specifically, consider a dataset D = {(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56)}\ud835\udc41\n\ud835\udc56=1 containing \ud835\udc41 pairs of samples\n\ud835\udc65\ud835\udc56 and ground truth \ud835\udc66\ud835\udc56. For an adversarial attack method A, given an input \ud835\udc65, this method\ncan produce adversarial examples A(\ud835\udc65) to attack surrogate model \ud835\udc53 , with the success rate\nis calculated as: \ud835\udc34\ud835\udc46\ud835\udc45 = \u00cd\n(\ud835\udc65,\ud835\udc66\u2208\ud835\udc37)\nI[\ud835\udc53 (A(\ud835\udc65))\u2260\ud835\udc66]\nI[\ud835\udc53 (\ud835\udc65)=\ud835\udc66]\n, where I is the indicator function [203].\n\u2022 Performance Drop Rate (PDR), a new unified metric, effectively assesses the robustness of\nprompt in LLMs [264]. PDR quantifies the relative performance degradation after a prompt\nattack, and the formula is as follows: \ud835\udc43\ud835\udc37\ud835\udc45 = 1 \u2212\n\u00cd\n(\ud835\udc65,\ud835\udc66) \u2208\ud835\udc37 M[\ud835\udc53 ([\ud835\udc34(\ud835\udc43 ),\ud835\udc65 ]),\ud835\udc66]\n\u00cd\n(\ud835\udc65,\ud835\udc66) \u2208\ud835\udc37 M[\ud835\udc53 ([\ud835\udc43,\ud835\udc65 ]),\ud835\udc66] , where \ud835\udc34 represents\nthe adversarial attack applied to prompt \ud835\udc43, and \ud835\udc40 denotes the evaluation function, which\nvaries across different tasks [264].\n5.2\nHuman Evaluation\nThe increasingly strengthened capabilities of LLMs have certainly gone beyond standard evalua-\ntion metrics on general natural language tasks. Therefore, human evaluation becomes a natural\nchoice in some non-standard cases where automatic evaluation is not suitable. For instance, in\nopen-generation tasks where embedded similarity metrics (such as BERTScore) are not enough,\nhuman evaluation is more reliable [142]. While some generation tasks can adopt certain automatic\nevaluation protocols, human evaluation in these tasks is more favorable as generation can always\ngo better than standard answers.\nHuman evaluation is a way to evaluate the quality and accuracy of model-generated results\nthrough human participation. Compared with automatic evaluation, manual evaluation is closer to\nthe actual application scenario and can provide more comprehensive and accurate feedback. In\nthe manual evaluation of LLMs, evaluators (such as experts, researchers, or ordinary users) are\nusually invited to evaluate the results generated by the model. For example, Ziems et al. [269] used\nthe annotations from experts for generation. By human evaluation, Liang et al. [114] assessed on\nsummarization and disinformation scenarios on 6 models and Bang et al. [6] evaluated analogical\nreasoning tasks. Bubeck et al. [15] did a series of human-crafted tests using GPT-4 and they found\nthat GPT-4 performs close to or even exceeds human performance on multiple tasks. This evaluation\nrequires human evaluators to actually test and compare the performance of the models, not just\nevaluate the models through automated evaluation metrics. Note that even human evaluations can\nhave high variance and instability, which could be due to cultural and individual differences [155].\nIn practical applications, these two evaluation methods are considered and weighed in combination\nwith the actual situation.\nExploring the human evaluation methods of LLMs requires thoughtful attention to various\ncrucial factors to guarantee the dependability and precision of assessments [178]. Table 10 provides\na concise overview of the essential aspects of human evaluation, including the number of evaluators,\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:29\nTable 10. Summary of key factors in human evaluation\nEvaluation Criteria\nKey Factor\nNumber of evaluators\nAdequate representation [7], Statistical significance\nEvaluation rubrics\nAccuracy [178], Relevance [261], Fluency [196], Transparency, Safety [85], Human\nalignment\nEvaluator\u2019s expertise level\nRelevant domain expertise [144], Task familiarity, Methodological training\nevaluation criteria, and evaluator\u2019s expertise level. Primarily, the number of evaluators emerges as\na crucial factor intricately intertwined with adequate representation and statistical significance.\nA judiciously chosen number of evaluators contributes to a more nuanced and comprehensive\nunderstanding of the LLMs under scrutiny, enabling a more reliable extrapolation of the results to\na broader context.\nFurthermore, evaluation criteria are fundamental components of the human assessment process.\nExpanding upon the principles of the 3H rule (Helpfulness, Honesty, and Harmlessness) [4], we have\nelaborated them into the following 6 human assessment criteria. These criteria include accuracy,\nrelevance, fluency, transparency, safety, and human alignment. Through the application of these\nstandards, a thorough analysis of LLMs\u2019 performance in syntax, semantics, and context is achieved,\nallowing for a more comprehensive evaluation of the quality of generated text.\n(1) Accuracy [178] stands out as a pivotal criterion that assesses the precision and correctness of\nthe generated text. It involves scrutinizing the extent to which the language model produces\ninformation that aligns with factual knowledge, avoiding errors and inaccuracies.\n(2) Relevance [261] focuses on the appropriateness and significance of the generated content.\nThis criterion examines how well the text addresses the given context or query, ensuring that\nthe information provided is pertinent and directly applicable.\n(3) Fluency [196] assesses the language model\u2019s ability to produce content that flows smoothly,\nmaintaining a consistent tone and style. A fluent text is not only grammatically correct but\nalso ensures readability and a seamless user experience. Analysts evaluate how well the model\navoids awkward expressions and abrupt shifts in language or topic, contributing to effective\ncommunication with users.\n(4) Transparency delves into the clarity and openness of the language model\u2019s decision-making\nprocess. It involves assessing how well the model communicates its thought processes, enabling\nusers to understand how and why certain responses are generated. A transparent model\nprovides insights into its inner workings.\n(5) Safety [85] emerges as a critical criterion concerned with the potential harm or unintended\nconsequences arising from the generated text. It examines the language model\u2019s ability to avoid\nproducing content that may be inappropriate, offensive, or harmful, ensuring the well-being\nof users and avoiding misinformation.\n(6) Human alignment assesses the degree to which the language model\u2019s output aligns with\nhuman values, preferences, and expectations. It considers the ethical implications of the\ngenerated content, ensuring that the language model produces text that respects societal\nnorms and user expectations, promoting a positive interaction with human users.\nLastly, the expertise level of evaluators is a critical consideration, encompassing relevant domain\nknowledge, task familiarity, and methodological training. Delineating the requisite expertise level\nfor evaluators ensures that they possess the necessary background knowledge to accurately com-\nprehend and assess the domain-specific text generated by LLMs. This strategy adds a layer of rigor\nto the evaluation process, reinforcing the credibility and validity of the findings.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:30\nChang et al.\n6\nSUMMARY\nIn this section, we summarize the key findings based on our review in sections 3, 4, and 5.\nFirst of all, we would like to highlight that despite all the efforts spent on summarizing existing\nworks on evaluation, there is no evidence to explicitly show that one certain evaluation protocol or\nbenchmark is the most useful and successful, but with different characteristics and focuses.\nThis also demonstrates that not a single model can perform best in all kinds of tasks. The purpose\nof this survey is to go beyond simply determining the \u201cbest\u201d benchmark or evaluation protocol.\nBy summarizing and analyzing existing efforts on LLMs evaluation, we may identify the current\nsuccess and failure cases of LLMs, derive new trends for evaluation protocols, and most importantly,\npropose new challenges and opportunities for future research.\n6.1\nTask: Success and Failure Cases of LLMs\nWe now summarize the success and failure cases of LLMs in different tasks. Note that all the\nfollowing conclusions are made based on existing evaluation efforts and the results are only\ndependent on specific datasets.\n6.1.1\nWhat can LLMs do well?\n\u2022 LLMs demonstrate proficiency in generating text [11, 14, 24] by producing fluent and precise\nlinguistic expressions.\n\u2022 LLMs obtain impressive performance in tasks involving language understanding, including\nsentiment analysis [52, 129, 159], text classification [114, 154, 233], as well as the handling of\nfactual input [159].\n\u2022 LLMs demonstrate robust arithmetic reasoning capabilities [159] and excel in logical reasoning\n[124]. Moreover, they exhibit noteworthy proficiency in temporal reasoning [6]. Furthermore,\nmore intricate tasks such as mathematical reasoning [226, 237, 244] and structured data\ninference [86, 151] have emerged as the prevailing benchmarks for evaluation.\n\u2022 LLMs exhibit robust contextual comprehension, enabling them to generate coherent responses\nthat align with the given input [187].\n\u2022 LLMs also achieve satisfying performance across several natural language processing tasks,\nincluding machine translation [6, 130, 208], text generation [20], and question answering\n[102, 114].\n6.1.2\nWhen can LLMs fail?\n\u2022 Within the realm of NLI, LLMs exhibit subpar performance and encounter challenges in\naccurately representing human disagreements [105].\n\u2022 LLMs exhibit restricted proficiency in discerning semantic similarity between events [184]\nand demonstrate substandard performance in evaluating fundamental phrases [166].\n\u2022 LLMs have limited abilities on abstract reasoning [56], and are prone to confusion or errors\nin complex contexts [148].\n\u2022 In linguistic contexts featuring non-Latin scripts and limited resources, LLMs manifest\nsuboptimal performance [2, 6, 100, 250]. Furthermore, generative LLMs consistently display\nproficiency levels below the expected standards across various tasks and languages [2].\n\u2022 LLMs demonstrate susceptibility when processing visual modal information [258]. Further-\nmore, they have the capacity to assimilate, disseminate, and potentially magnify detrimental\ncontent found within the acquired training datasets, frequently encompassing toxic linguistic\nelements, including offensive, hostile, and derogatory language [53].\n\u2022 LLMs may exhibit social biases and toxicity [37, 53, 153] during the generation process,\nresulting in the production of biased outputs.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:31\n\u2022 LLMs may manifest credibility deficits [201], potentially giving rise to fabricated information\nor erroneous facts within dialogues [163, 253].\n\u2022 LLMs have limitations in incorporating real-time or dynamic information [127], making them\nless suitable for tasks that require up-to-date knowledge or rapid adaptation to changing\ncontexts.\n\u2022 LLMs is sensitive to prompts, especially adversarial prompts [264], which trigger new evalu-\nations and algorithms to improve its robustness.\n6.2\nBenchmark and Evaluation Protocol\nWith the rapid development and widespread use of LLMs, the importance of evaluating them in\npractical applications and research has become crucial. This evaluation process should include not\nonly task-level evaluation but also a deep understanding of the potential risks they pose from a\nsocietal perspective. In this section, we summarize existing benchmarks and protocols in Table 8.\nFirst, a shift from objective calculation to human-in-the-loop testing, allowing for greater human\nfeedback during the evaluation process. AdaVision [50], an interactive process for testing vision\nmodels, enables users to label a small amount of data for model correctness, which helps users\nidentify and fix coherent failure modes. In AdaTest [164], the user filters test samples by only\nselecting high-quality tests and organizing them into semantically related topics.\nSecond, a move from static to crowd-sourcing test sets is becoming more common. Tools like\nDynaBench [94], DynaBoard [132], and DynaTask [188] rely on crowdworkers to create and\ntest hard samples. Additionally, DynamicTempLAMA [135] allows for dynamically constructed\ntime-related tests.\nThird, a shift from a unified to a challenging setting in evaluating machine learning models.\nWhile unified settings involve a test set with no preference for any specific task, challenging\nsettings create test sets for specific tasks. Tools like DeepTest [190] use seeds to generate input\ntransformations for testing, CheckList [165] builds test sets based on templates, and AdaFilter\n[157] adversarially constructs tests. However, it is worth noting that AdaFilter may not be entirely\nfair as it relies on adversarial examples. HELM [114] evaluates LLMs from different aspects, while\nthe Big-Bench [182] platform is used to design hard tasks for machine learning models to tackle.\nPromptBench [264] aims to evaluate the adversarial robustness of LLMs by creating adversarial\nprompts, which is more challenging and the results demonstrated that current LLMs are not robust\nto adversarial prompts.\n7\nGRAND CHALLENGES AND OPPORTUNITIES FOR FUTURE RESEARCH\nEvaluation as a new discipline: Our summarization inspires us to redesign a wide spectrum of\naspects related to evaluation in the era of LLMs. In this section, we present several grand challenges.\nOur key point is that evaluation should be treated as an essential discipline to drive the\nsuccess of LLMs and other AI models. Existing protocols are not enough to thoroughly evaluate\nthe true capabilities of LLMs, which poses grand challenges and triggers new opportunities for\nfuture research on LLMs evaluation.\n7.1\nDesigning AGI Benchmarks\nAs we discussed earlier, while all tasks can potentially serve as evaluation tools for LLMs, the\nquestion remains as to which can truly measure AGI capabilities. As we expect LLMs to demonstrate\nAGI abilities, a comprehensive understanding of the differences between human and AGI capacities\nbecomes crucial in the creation of AGI benchmarks. The prevailing trend seems to conceptualize\nAGI as a superhuman entity, thereby utilizing cross-disciplinary knowledge from fields such as\neducation, psychology, and social sciences to design innovative benchmarks. Nonetheless, there\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:32\nChang et al.\nremains a plethora of unresolved issues. For instance, does it make sense to use human values as a\nstarting point for test construction, or should alternative perspectives be considered? Developing\nsuitable AGI benchmarks presents many open questions demanding further exploration.\n7.2\nComplete Behavioral Evaluation\nAn ideal AGI evaluation should contain not only standard benchmarks on common tasks, but also\nevaluations on open tasks such as complete behavioral tests. By behavioral test, we mean that\nAGI models should also be evaluated in an open environment. For instance, by treating LLMs as\nthe central controller, we can construct evaluations on a robot manipulated by LLMs to test its\nbehaviors in real situations. By treating LLMs as a completely intelligent machine, the evaluations\nof its multi-modal dimensions should also be considered. In fact, complete behavioral evaluations\nare complementary to standard AGI benchmarks and they should work together for better testing.\n7.3\nRobustness Evaluation\nBeyond general tasks, it is crucial for LLMs to maintain robustness against a wide variety of inputs\nin order to perform optimally for end-users, given their extensive integration into daily life. For\ninstance, the same prompts but with different grammars and expressions could lead ChatGPT\nand other LLMs to generate diverse results, indicating that current LLMs are not robust to the\ninputs. While there are some prior work on robustness evaluation [206, 264], there are much room\nfor advancement, such as including more diverse evaluation sets, examining more evaluation\naspects, and developing more efficient evaluations to generate robustness tasks. Concurrently, the\nconcept and definition of robustness are constantly evolving. It is thus vital to consider updating\nthe evaluation system to better align with emerging requirements related to ethics and bias.\n7.4\nDynamic and Evolving Evaluation\nExisting evaluation protocols for most AI tasks rely on static and public benchmarks, i.e., the\nevaluation datasets and protocols are often publicly available. While this facilitates rapid and\nconvenient evaluation within the community, it is unable to accurately assess the evolving abilities\nof LLMs, given their rapid rate of development. The capabilities of LLMs may enhance over time\nwhich cannot be consistently evaluated by existing static benchmarks. On the other hand, as\nLLMs grow increasingly powerful with larger model sizes and training set sizes, static and public\nbenchmarks are likely to be memorized by LLMs, resulting in potential training data contamination.\nTherefore, developing dynamic and evolving evaluation systems is the key to providing a fair\nevaluation of LLMs.\n7.5\nPrincipled and Trustworthy Evaluation\nWhen introducing an evaluation system, it is crucial to ascertain its integrity and trustworthiness.\nTherefore, the necessity for trustworthy computing extends to the requirement for reliable evalua-\ntion systems as well. This poses a challenging research question that intertwines with measurement\ntheory, probability, and numerous other domains. For instance, how can we ensure that dynamic\ntesting truly generates out-of-distribution examples? There is a scarcity of research in this domain,\nand it is hoped that future work will aim to scrutinize not only the algorithms but the evaluation\nsystem itself.\n7.6\nUnified Evaluation that Supports All LLMs Tasks\nThere are many other research areas of LLMs and we need to develop evaluation systems that can\nsupport all kinds of tasks such as value alignment, safety, verification, interdisciplinary research,\nfine-tuning, and others. For instance, PandaLM [216] is an evaluation system that assists LLMs\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:33\nfine-tuning by providing an open-source evaluation model, which can automatically assess the\nperformance of fine-tuning. We expect that more evaluation systems are becoming more general\nand can be used as assistance in certain LLMs tasks.\n7.7\nBeyond Evaluation: LLMs Enhancement\nUltimately, evaluation is not the end goal but rather the starting point. Following the evaluation,\nthere are undoubtedly conclusions to be drawn regarding performance, robustness, stability, and\nother factors. A proficient evaluation system should not only offer benchmark results but should\nalso deliver an insightful analysis, recommendations, and guidance for future research and de-\nvelopment. For instance, PromptBench [264] provides not only robustness evaluation results on\nadversarial prompts but also a comprehensive analysis through attention visualization, elucidating\nhow adversarial texts can result in erroneous responses. The system further offers a word frequency\nanalysis to identify robust and non-robust words in the test sets, thus providing prompt engineering\nguidance for end users. Subsequent research can leverage these findings to enhance LLMs. Another\nexample is that Wang et al. [215] first explored the performance of large vision-language models\non imbalanced (long-tailed) tasks, which demonstrates the limitation of current large models. Then,\nthey explored different methodologies to enhance the performance on these tasks. In summary,\nenhancement after evaluation helps to build better LLMs and much can be done in the future.\n8\nCONCLUSION\nEvaluation carries profound significance, becoming imperative in the advancement of AI models,\nespecially within the context of large language models. This paper presents the first survey to give\na comprehensive overview of the evaluation on LLMs from three aspects: what to evaluate, how to\nevaluate, and where to evaluate. By encapsulating evaluation tasks, protocols, and benchmarks,\nour aim is to augment understanding of the current status of LLMs, elucidate their strengths and\nlimitations, and furnish insights for future LLMs progression.\nOur survey reveals that current LLMs exhibit certain limitations in numerous tasks, notably\nreasoning and robustness tasks. Concurrently, the need for contemporary evaluation systems to\nadapt and evolve remains evident, ensuring the accurate assessment of LLMs\u2019 inherent capabilities\nand limitations. We identify several grand challenges that future research should address, with the\naspiration that LLMs can progressively enhance their service to humanity.\nACKNOWLEDGEMENTS\nThis work is supported in part by NSF under grant III-2106758.\nDISCLAIMER\nThe goal of this paper is mainly to summarize and discuss existing evaluation efforts on large\nlanguage models. Results and conclusions in each paper are original contributions of their corre-\nsponding authors, particularly for potential issues in ethics and biases. This paper may discuss\nsome side effects of LLMs and the only intention is to foster a better understanding.\nDue to the evolution of LLMs especially online services such as Claude and ChatGPT, it is very\nlikely that they become stronger and some of the limitations described in this paper are mitigated\n(and new limitations may arise). We encourage interested readers to take this survey as a reference\nfor future research and conduct real experiments in current systems when performing evaluations.\nFinally, the evaluation of LLMs is continuously developing, thus we may miss some new papers\nor benchmarks. We welcome all constructive feedback and suggestions.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:34\nChang et al.\nREFERENCES\n[1] Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram Hasanain, Basel Mousi, Sabri Boughorbel,\nYassine El Kheir, Daniel Izham, Fahim Dalvi, Majd Hawasly, et al. 2023. Benchmarking Arabic AI with Large Language\nModels. arXiv preprint arXiv:2305.14982 (2023).\n[2] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer\nSegal, Maxamed Axmed, Kalika Bali, et al. 2023. Mega: Multilingual evaluation of generative ai. arXiv preprint\narXiv:2303.12528 (2023).\n[3] Daman Arora, Himanshu Gaurav Singh, et al. 2023. Have LLMs Advanced Enough? A Challenging Problem Solving\nBenchmark For Large Language Models. arXiv preprint arXiv:2305.15074 (2023).\n[4] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph,\nBen Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861 (2021).\n[5] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu,\net al. 2023. Benchmarking Foundation Models with Language-Model-as-an-Examiner. arXiv preprint arXiv:2306.04181\n(2023).\n[6] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng\nYu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination,\nand interactivity. arXiv preprint arXiv:2302.04023 (2023).\n[7] Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In 11th conference of\nthe european chapter of the association for computational linguistics. 313\u2013320.\n[8] Daniel Berrar. 2019. Cross-Validation.\n[9] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He. 2023. Chatgpt is a knowledgeable but inexperi-\nenced solver: An investigation of commonsense problem in large language models. arXiv preprint arXiv:2303.16421\n(2023).\n[10] Bojana Bodroza, Bojana M Dinic, and Ljubisa Bojic. 2023. Personality testing of GPT-3: Limited temporal reliability,\nbut highlighted social desirability of GPT-3\u2019s personality instruments results. arXiv preprint arXiv:2306.04308 (2023).\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[12] Nathan Brody. 1999. What is intelligence? International Review of Psychiatry 11, 1 (1999), 19\u201325.\n[13] Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. 1992. Class-based n-gram\nmodels of natural language. Computational linguistics 18, 4 (1992), 467\u2013480.\n[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[15] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat\nLee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.\narXiv preprint arXiv:2303.12712 (2023).\n[16] Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich. 2023. Assessing Cross-Cultural\nAlignment between ChatGPT and Human Societies: An Empirical Study. In Proceedings of the First Workshop on\nCross-Cultural Considerations in NLP (C3NLP). 53\u201367.\n[17] Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. 2023. Evaluating the feasibility of\nChatGPT in healthcare: an analysis of multiple clinical and research scenarios. Journal of Medical Systems 47, 1 (2023),\n33.\n[18] Cayque Monteiro Castro Nascimento and Andr\u00e9 Silva Pimentel. 2023. Do Large Language Models Understand\nChemistry? A Conversation with ChatGPT. Journal of Chemical Information and Modeling 63, 6 (2023), 1649\u20131655.\n[19] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374 (2021).\n[20] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the use of large language models\nfor reference-free text quality evaluation: A preliminary empirical study. arXiv preprint arXiv:2304.00723 (2023).\n[21] Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel, and Sangita Jindal. 2023. The promise and peril of using\na large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with\nlimitations. Fertility and Sterility (2023).\n[22] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2023. INSTRUCTEVAL: Towards Holistic Evaluation\nof Instruction-Tuned Large Language Models. arXiv preprint arXiv:2306.04757 (2023).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:35\n[23] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens. 2023. Do LLMs Understand Social Knowledge?\nEvaluating the Sociability of Large Language Models with SocKET Benchmark. arXiv preprint arXiv:2305.14938 (2023).\n[24] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 (2022).\n[25] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems 30 (2017).\n[26] Benjamin Clavi\u00e9, Alexandru Ciceu, Frederick Naylor, Guillaume Souli\u00e9, and Thomas Brightwell. 2023. Large Language\nModels in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. In International Conference\non Applications of Natural Language to Information Systems. Springer, 3\u201317.\n[27] Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz,\nYuhuai Wu, Joshua B Tenenbaum, William Hart, et al. 2023. Evaluating Language Models for Mathematics through\nInteractions. arXiv preprint arXiv:2306.01694 (2023).\n[28] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine learning 20 (1995), 273\u2013297.\n[29] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu.\n2023. Uncovering ChatGPT\u2019s Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182 (2023).\n[30] Wei Dai, Jionghao Lin, Flora Jin, Tongguang Li, Yi-Shan Tsai, Dragan Gasevic, and Guanliang Chen. 2023. Can large\nlanguage models provide feedback to students? a case study on chatgpt. (2023).\n[31] Xuan-Quy Dao and Ngoc-Bich Le. 2023. Investigating the Effectiveness of ChatGPT in Mathematical Reasoning\nand Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. arXiv preprint\narXiv:2306.06331 (2023).\n[32] Joost CF de Winter. 2023. Can ChatGPT pass high school exams on English language comprehension. Researchgate.\nPreprint (2023).\n[33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248\u2013255.\n[34] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023. How Ready are Pre-trained Abstractive Models and\nLLMs for Legal Case Judgement Summarization? arXiv preprint arXiv:2306.01248 (2023).\n[35] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity\nin chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335 (2023).\n[36] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n[37] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta.\n2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021\nACM conference on fairness, accountability, and transparency. 862\u2013872.\n[38] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and\nTatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback.\narXiv preprint arXiv:2305.14387 (2023).\n[39] Dat Duong and Benjamin D Solomon. 2023. Analysis of large-language model versus human performance for genetics\nquestions. European Journal of Human Genetics (2023), 1\u20133.\n[40] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recom-\nmender Systems in the Era of Large Language Models (LLMs). arXiv:2307.02046 [cs.IR]\n[41] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. 2022. Ddxplus: A new dataset for\nautomatic medical diagnosis. Advances in Neural Information Processing Systems 35 (2022), 31306\u201331318.\n[42] Emilio Ferrara. 2023. Should chatgpt be biased? challenges and risks of bias in large language models. arXiv preprint\narXiv:2304.03738 (2023).\n[43] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines\n30 (2020), 681\u2013694.\n[44] Michael C Frank. 2023. Baby steps in evaluating the capacities of large language models. Nature Reviews Psychology\n(2023), 1\u20132.\n[45] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian\nPetersen, Alexis Chevalier, and Julius Berner. 2023. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867\n(2023).\n[46] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,\nXiawu Zheng, et al. 2023. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.\narXiv preprint arXiv:2306.13394 (2023).\n[47] Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. Chain-of-Thought Hub: A Continuous\nEffort to Measure Large Language Models\u2019 Reasoning Performance. arXiv preprint arXiv:2305.17306 (2023).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:36\nChang et al.\n[48] Tadayoshi Fushiki. 2011. Estimation of prediction error by using K-fold cross-validation. Statistics and Computing 21\n(2011), 137\u2013146.\n[49] Stephen I Gallant et al. 1990. Perceptron-based learning algorithms. IEEE Transactions on neural networks 1, 2 (1990),\n179\u2013191.\n[50] Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco Tulio Ribeiro. 2022. Adaptive Testing of Computer Vision\nModels. arXiv preprint arXiv:2212.02774 (2022).\n[51] Jianfeng Gao and Chin-Yew Lin. 2004. Introduction to the special issue on statistical language modeling. , 87\u201393 pages.\n[52] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv\npreprint arXiv:2012.15723 (2020).\n[53] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. RealToxicityPrompts:\nEvaluating Neural Toxic Degeneration in Language Models. In Findings of the Association for Computational Linguistics:\nEMNLP 2020. 3356\u20133369.\n[54] Yonatan Geifman and Ran El-Yaniv. 2017. Selective classification for deep neural networks. Advances in neural\ninformation processing systems 30 (2017).\n[55] Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. Trueteacher: Learning factual\nconsistency evaluation with large language models. arXiv preprint arXiv:2305.11171 (2023).\n[56] Ga\u00ebl Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. 2023. Large Language Models Are Not Abstract\nReasoners. arXiv preprint arXiv:2305.19555 (2023).\n[57] Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash,\net al. 2023. How does CHATGPT perform on the United States Medical Licensing Examination? the implications of\nlarge language models for medical education and knowledge assessment. JMIR Medical Education 9, 1 (2023), e45312.\n[58] Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. Moral\nfoundations theory: The pragmatic validity of moral pluralism. In Advances in experimental social psychology. Vol. 47.\nElsevier, 55\u2013130.\n[59] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu\nHe, Rui Xu, et al. 2023. Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. arXiv\npreprint arXiv:2306.05783 (2023).\n[60] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In\nInternational conference on machine learning. PMLR, 1321\u20131330.\n[61] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, Xiangliang Zhang, et al.\n2023. What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. arXiv preprint\narXiv:2305.18365 (2023).\n[62] Thilo Hagendorff and Sarah Fabi. 2023. Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language\nModels \u2013 and Disappeared in GPT-4. arXiv:2306.07622 [cs.CL]\n[63] Alaleh Hamidi and Kirk Roberts. 2023. Evaluation of AI Chatbots for Patient-Specific EHR Questions. arXiv preprint\narXiv:2306.02549 (2023).\n[64] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. Advances in neural\ninformation processing systems 29 (2016).\n[65] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The political ideology of conversational AI:\nConverging evidence on ChatGPT\u2019s pro-environmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768\n(2023).\n[66] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang,\nYuncheng Huang, et al. 2023. Can Large Language Models Understand Real-World Complex Instructions? arXiv\npreprint arXiv:2309.09150 (2023).\n[67] Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanp\u00e4\u00e4, and Juha Sorva. 2023. Exploring the\nResponses of Large Language Models to Beginner Programmers\u2019 Help Requests. arXiv preprint arXiv:2306.05715\n(2023).\n[68] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir\nPuranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. arXiv preprint\narXiv:2105.09938 (2021).\n[69] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020.\nAligning ai with shared human values. arXiv preprint arXiv:2008.02275 (2020).\n[70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020).\n[71] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal\ncontract review. arXiv preprint arXiv:2103.06268 (2021).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:37\n[72] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874\n(2021).\n[73] Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T Sio, Lisa A McGee, Jonathan B Ashman, Xiang\nLi, Tianming Liu, Jiajian Shen, et al. 2023. Evaluating large language models on a highly-specialized topic, radiation\noncology physics. arXiv preprint arXiv:2304.01938 (2023).\n[74] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom,\nIdan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. arXiv\npreprint arXiv:2204.04991 (2022).\n[75] Zhaoyi Joey Hou, Li Zhang, and Chris Callison-Burch. 2023. Choice-75: A Dataset on Decision Branching in Script\nLearning. arXiv preprint arXiv:2309.11737 (2023).\n[76] Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R.\nLyu. 2023. Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench. arXiv preprint\narXiv:2308.03656 (2023).\n[77] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan\nMohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. arXiv\npreprint arXiv:2302.14045 (2023).\n[78] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,\nYikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation\nmodels. arXiv preprint arXiv:2305.08322 (2023).\n[79] Yue Huang, Qihui Zhang, Philip S. Y, and Lichao Sun. 2023. TrustGPT: A Benchmark for Trustworthy and Responsible\nLarge Language Models. arXiv:2306.11507 [cs.CL]\n[80] HuggingFace. 2023.\nOpen-source Large Language Models Leaderboard.\nhttps://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\n[81] Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, and Jimmy Huang. 2023. Evaluation of ChatGPT on Biomedical\nTasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. arXiv preprint arXiv:2306.04504 (2023).\n[82] Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas\nGeiping, and Tom Goldstein. 2023. Bring Your Own Data! Self-Supervised Evaluation for Large Language Models.\narXiv preprint arXiv:2306.13651 (2023).\n[83] Malin Jansson, Stefan Hrastinski, Stefan Stenbom, and Fredrik Enoksson. 2021. Online question and answer sessions:\nHow students support their own and other students\u2019 processes of inquiry in a text-based learning environment. The\nInternet and Higher Education 51 (2021), 100817.\n[84] Sophie Jentzsch and Kristian Kersting. 2023. ChatGPT is fun, but it is not funny! Humor is still challenging Large\nLanguage Models. arXiv preprint arXiv:2306.04563 (2023).\n[85] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Ruiyang Sun, Yizhou Wang, and Yaodong\nYang. 2023. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. arXiv preprint\narXiv:2307.04657 (2023).\n[86] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt: A general\nframework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645 (2023).\n[87] Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone, Eli Zimmerman, Rebecca Donald, Sam Chang, Sean\nBerkowitz, Avni Finn, Eiman Jahangir, et al. 2023. Assessing the accuracy and reliability of AI-generated medical\nresponses: an evaluation of the Chat-GPT model. (2023).\n[88] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised\nChallenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Linguistics, Vancouver, Canada.\n[89] Saurav Kadavath, Tom Conerly, Amanda Askell, T. J. Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary\nDodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume,\nAnna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion,\nShauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack\nClark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan. 2022. Language\nModels (Mostly) Know What They Know. ArXiv abs/2207.05221 (2022).\n[90] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav\nLevine, Kevin Leyton-Brown, et al. 2022. MRKL Systems: A modular, neuro-symbolic architecture that combines\nlarge language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445 (2022).\n[91] Enkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser,\nGeorg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, et al. 2023. ChatGPT for good? On opportunities and challenges\nof large language models for education. Learning and Individual Differences 103 (2023), 102274.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:38\nChang et al.\n[92] Jean Khalfa. 1994. What is intelligence? (1994).\n[93] Yousuf A Khan, Clarisse Hokia, Jennifer Xu, and Ben Ehlert. 2023. covLLM: Large Language Models for COVID-19\nBiomedical Literature. arXiv preprint arXiv:2306.04926 (2023).\n[94] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha\nPrasad, Amanpreet Singh, Pratik Ringshia, et al. 2021. Dynabench: Rethinking benchmarking in NLP. arXiv preprint\narXiv:2104.14337 (2021).\n[95] Ron Kohavi et al. 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. In\nIjcai, Vol. 14. Montreal, Canada, 1137\u20131145.\n[96] Stefan Kombrink, Tomas Mikolov, Martin Karafi\u00e1t, and Luk\u00e1s Burget. 2011. Recurrent Neural Network Based Language\nModeling in Meeting Recognition.. In Interspeech, Vol. 11. 2877\u20132880.\n[97] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepa\u00f1o, Maria Madriaga,\nRimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. 2023. Performance of ChatGPT on USMLE: Potential for\nAI-assisted medical education using large language models. PLoS digital health 2, 2 (2023), e0000198.\n[98] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei\nChang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a Benchmark for Question\nAnswering Research. Transactions of the Association of Computational Linguistics (2019).\n[99] Adi Lahat, Eyal Shachar, Benjamin Avidan, Zina Shatz, Benjamin S Glicksberg, and Eyal Klang. 2023. Evaluating the\nuse of large language model in identifying top research questions in gastroenterology. Scientific reports 13, 1 (2023),\n4164.\n[100] Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu\nNguyen. 2023. ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in\nMultilingual Learning. arXiv preprint arXiv:2304.05613 (2023).\n[101] Pier Luca Lanzi and Daniele Loiacono. 2023. Chatgpt and other large language models as evolutionary engines for\nonline interactive collaborative game design. arXiv preprint arXiv:2303.02155 (2023).\n[102] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xi-\nangji Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. arXiv\npreprint arXiv:2305.18486 (2023).\n[103] Van-Hoang Le and Hongyu Zhang. 2023. An Evaluation of Log Parsing with ChatGPT. arXiv preprint arXiv:2306.01590\n(2023).\n[104] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436\u2013444.\n[105] Noah Lee, Na Min An, and James Thorne. 2023. Can Large Language Models Infer and Disagree Like Humans? arXiv\npreprint arXiv:2305.13788 (2023).\n[106] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,\nand Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).\n[107] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023. Seed-bench: Benchmarking\nmultimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023).\n[108] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023.\nCMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212 (2023).\n[109] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A\nBenchmark for Tool-Augmented LLMs. arXiv:2304.08244 [cs.CL]\n[110] Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan. 2023. Exploring the Upper Limits of Text-\nBased Collaborative Filtering Using Large Language Models: Discoveries and Insights. arXiv preprint arXiv:2305.11700\n(2023).\n[111] Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine. 2023. A Survey on Out-of-Distribution Evaluation of Neural NLP\nModels. arXiv:2306.15261 [cs.CL]\n[112] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B.\nHashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsu-\nlab/alpaca_eval.\n[113] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination\nin large vision-language models. arXiv preprint arXiv:2305.10355 (2023).\n[114] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\nNarayanan, Yuhuai Wu, Ananya Kumar, et al. 2022.\nHolistic evaluation of language models.\narXiv preprint\narXiv:2211.09110 (2022).\n[115] Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming\nShi, and Xing Wang. 2023. Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:39\narXiv preprint arXiv:2310.20499 (2023).\n[116] Xun Liang, Shichao Song, Simin Niu, Zhiyu Li, Feiyu Xiong, Bo Tang, Zhaohui Wy, Dawei He, Peng Cheng, Zhonghao\nWang, et al. 2023. UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained\nGeneration. arXiv preprint arXiv:2311.15296 (2023).\n[117] Valentin Li\u00e9vin, Christoffer Egeberg Hother, and Ole Winther. 2022. Can large language models reason about medical\nquestions? arXiv preprint arXiv:2207.08143 (2022).\n[118] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013\n[119] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods.\narXiv preprint arXiv:2109.07958 (2021).\n[120] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer, 740\u2013755.\n[121] Yen-Ting Lin and Yun-Nung Chen. 2023. LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-\nDomain Conversations with Large Language Models. arXiv preprint arXiv:2305.13711 (2023).\n[122] Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi\nZhang, Qingqing Lyu, Xiaowen Su, Qun Liu, and Deyi Xiong. 2023. M3KE: A Massive Multi-Level Multi-Subject\nKnowledge Evaluation Benchmark for Chinese Large Language Models. arXiv:2305.10263 [cs.CL]\n[123] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating Hallucination in\nLarge Multi-Modal Models via Robust Instruction Tuning. arXiv:2306.14565 [cs.CV]\n[124] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023. Evaluating the Logical Reasoning\nAbility of ChatGPT and GPT-4. arXiv:2304.03439 [cs.CL]\n[125] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatgpt really\ncorrect? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210 (2023).\n[126] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui\nHe, Ziwei Liu, Kai Chen, and Dahua Lin. 2023. MMBench: Is Your Multi-modal Model an All-around Player?\narXiv:2307.06281 [cs.CV]\n[127] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He,\nZhengliang Liu, et al. 2023. Summary of chatgpt/gpt-4 research and perspective towards the future of large language\nmodels. arXiv preprint arXiv:2304.01852 (2023).\n[128] LMSYS. 2023. Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings. https://lmsys.org.\n[129] Alejandro Lopez-Lira and Yuehua Tang. 2023. Can chatgpt forecast stock price movements? Return predictability\nand large language models. arXiv preprint arXiv:2304.07619 (2023).\n[130] Chenyang Lyu, Jitao Xu, and Longyue Wang. 2023. New trends in machine translation using large language models:\nCase examples with chatgpt. arXiv preprint arXiv:2305.01181 (2023).\n[131] Qing Lyu, Josh Tan, Mike E Zapadka, Janardhana Ponnatapuram, Chuang Niu, Ge Wang, and Christopher T Whitlow.\n2023. Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Promising\nresults, limitations, and potential. arXiv preprint arXiv:2303.09038 (2023).\n[132] Zhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams,\nand Douwe Kiela. 2021. Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking.\nAdvances in Neural Information Processing Systems 34 (2021), 10351\u201310367.\n[133] Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination\ndetection for generative large language models. arXiv preprint arXiv:2303.08896 (2023).\n[134] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. MQAG: Multiple-choice Question Answering and\nGeneration for Assessing Information Consistency in Summarization. arXiv:2301.12307 [cs.CL]\n[135] Katerina Margatina, Shuai Wang, Yogarshi Vyas, Neha Anna John, Yassine Benajiba, and Miguel Ballesteros. 2023.\nDynamic benchmarking of masked language models on temporal concept drift with multiple views. arXiv preprint\narXiv:2302.12297 (2023).\n[136] John McCarthy. 2007. What is artificial intelligence. (2007).\n[137] Microsoft. 2023. Bing Chat. https://www.bing.com/new (2023).\n[138] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer,\nand Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text\nGeneration. arXiv preprint arXiv:2305.14251 (2023).\n[139] John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao, Meghana Bhat, Raghav Jain, Aaron Travis Lee,\nJonathan H Choi, and Jungo Kasai. 2023. Large Language Models as Tax Attorneys: A Case Study in Legal Capabilities\nEmergence. arXiv preprint arXiv:2306.07075 (2023).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:40\nChang et al.\n[140] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2019. Adversarial NLI: A\nnew benchmark for natural language understanding. arXiv preprint arXiv:1910.14599 (2019).\n[141] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\n(2022).\n[142] Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation\nmetrics for NLG. arXiv preprint arXiv:1707.06875 (2017).\n[143] Namkee Oh, Gyu-Seong Choi, and Woo Yong Lee. 2023. ChatGPT goes to the operating room: evaluating GPT-4\nperformance and its potential in surgical education and training in the era of large language models. Annals of\nSurgical Treatment and Research 104, 5 (2023), 269.\n[144] Andrew M Olney. 2023. Generating multiple choice questions from a textbook: Llms match human performance on\nmost metrics. In AIED Workshops.\n[145] OpenAI. 2023. https://chat.openai.com.chat.\n[146] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[147] Graziella Orr\u00f9, Andrea Piarulli, Ciro Conversano, and Angelo Gemignani. 2023. Human-like problem-solving abilities\nin large language models using ChatGPT. Frontiers in Artificial Intelligence 6 (2023).\n[148] Simon Ott, Konstantin Hebenstreit, Valentin Li\u00e9vin, Christoffer Egeberg Hother, Milad Moradi, Maximilian\nMayrhauser, Robert Praas, Ole Winther, and Matthias Samwald. 2023. ThoughtSource: A central hub for large\nlanguage model reasoning data. arXiv preprint arXiv:2301.11596 (2023).\n[149] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[150] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh,\nFrancesco Fabiano, and Andrea Loreggia. 2023. Understanding the Capabilities of Large Language Models for\nAutomated Planning. arXiv preprint arXiv:2305.16151 (2023).\n[151] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2023. Unifying Large Language\nModels and Knowledge Graphs: A Roadmap. arXiv:2306.08302 [cs.CL]\n[152] Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255\n(2022).\n[153] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut,\nand Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association\nfor Computational Linguistics: ACL 2022. 2086\u20132105.\n[154] Alejandro Pe\u00f1a, Aythami Morales, Julian Fierrez, Ignacio Serna, Javier Ortega-Garcia, I\u00f1igo Puente, Jorge Cordova,\nand Gonzalo Cordova. 2023. Leveraging Large Language Models for Topic Classification in the Domain of Public\nAffairs. arXiv preprint arXiv:2306.02864 (2023).\n[155] Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. 1997. Validity problems comparing values across cultures and\npossible solutions. Psychological methods 2, 4 (1997), 329.\n[156] Pouya Pezeshkpour. 2023. Measuring and Modifying Factual Knowledge in Large Language Models. arXiv preprint\narXiv:2306.06264 (2023).\n[157] Jason Phang, Angelica Chen, William Huang, and Samuel R Bowman. 2021. Adversarially constructed evaluation sets\nare more challenging, but may not be fair. arXiv preprint arXiv:2111.08181 (2021).\n[158] Dongqi Pu and Vera Demberg. 2023. ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization\nand Sentence Style Transfer. arXiv:2306.07799 [cs.CL]\n[159] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is ChatGPT a\ngeneral-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476 (2023).\n[160] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi\nHan, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,\nBokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang\nWu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023. Tool Learning with Foundation Models. arXiv:2304.08354 [cs.CL]\n[161] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian,\nSihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023.\nToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. arXiv:2307.16789 [cs.AI]\n[162] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by\ngenerative pre-training. (2018).\n[163] Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A Survey of Hallucination in Large Foundation Models. arXiv\npreprint arXiv:2309.05922 (2023).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:41\n[164] Marco Tulio Ribeiro and Scott Lundberg. 2022. Adaptive testing and debugging of nlp models. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 3253\u20133267.\n[165] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing\nof NLP models with CheckList. arXiv preprint arXiv:2005.04118 (2020).\n[166] Nicholas Riccardi and Rutvik H Desai. 2023. The Two Word Test: A Semantic Benchmark for Large Language Models.\narXiv preprint arXiv:2306.04610 (2023).\n[167] J\u00e9r\u00f4me Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, and Markus Pauly. 2023. The Self-Perception and\nPolitical Biases of ChatGPT. arXiv preprint arXiv:2304.07333 (2023).\n[168] Mustafa Safdari, Greg Serapio-Garc\u00eda, Cl\u00e9ment Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai,\nAleksandra Faust, and Maja Matari\u0107. 2023. Personality Traits in Large Language Models. arXiv preprint arXiv:2307.00184\n(2023).\n[169] Jamil S Samaan, Yee Hui Yeo, Nithya Rajeev, Lauren Hawley, Stuart Abel, Wee Han Ng, Nitin Srinivasan, Justin Park,\nMiguel Burch, Rabindra Watson, et al. 2023. Assessing the accuracy of responses by the language model ChatGPT to\nquestions regarding bariatric surgery. Obesity Surgery (2023), 1\u20137.\n[170] Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim,\nand He He. 2023. Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples.\narXiv preprint arXiv:2305.15269 (2023).\n[171] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay,\nKshitij Gupta, and Aran Komatsuzaki. 2023. ARB: Advanced Reasoning Benchmark for Large Language Models.\narXiv:2307.13692 [cs.CL]\n[172] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and\nThomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761\n(2023).\n[173] Prabin Sharma, Kisan Thapa, Prastab Dhakal, Mala Deep Upadhaya, Santosh Adhikari, and Salik Ram Khanal. 2023.\nPerformance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical\nEducation. arXiv preprint arXiv:2307.00112 (2023).\n[174] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai\ntasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 (2023).\n[175] Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. Societal Biases in Language Generation:\nProgress and Challenges. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 4275\u20134293.\n[176] Gabriel Simmons. 2022. Moral mimicry: Large language models produce moral rationalizations tailored to political\nidentity. arXiv preprint arXiv:2209.12106 (2022).\n[177] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani,\nHeather Cole-Lewis, Stephen Pfohl, et al. 2022. Large Language Models Encode Clinical Knowledge. arXiv preprint\narXiv:2212.13138 (2022).\n[178] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani,\nHeather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature 620, 7972\n(2023), 172\u2013180.\n[179] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun\nLiu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 (2022).\n[180] Xiaoyang Song, Akshat Gupta, Kiyan Mohebbizadeh, Shujie Hu, and Anant Singh. 2023. Have Large Language Models\nDeveloped a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. arXiv preprint\narXiv:2305.14693 (2023).\n[181] Giriprasad Sridhara, Sourav Mazumdar, et al. 2023. ChatGPT: A Study on its Utility for Ubiquitous Software\nEngineering Tasks. arXiv preprint arXiv:2305.16837 (2023).\n[182] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R\nBrown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022).\n[183] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search?\nInvestigating Large Language Models as Re-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).\n[184] Zhengwei Tao, Zhi Jin, Xiaoying Bai, Haiyan Zhao, Yanlin Feng, Jia Li, and Wenpeng Hu. 2023. EvEval: A Compre-\nhensive Evaluation of Event Semantics for Large Language Models. arXiv preprint arXiv:2305.15268 (2023).\n[185] Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous\nbenchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:42\nChang et al.\n[186] Arun James Thirunavukarasu, Refaat Hassan, Shathar Mahmood, Rohan Sanghera, Kara Barzangi, Mohanned\nEl Mukashfi, and Sachin Shah. 2023. Trialling a large language model (ChatGPT) in general practice with the\nApplied Knowledge Test: observational study demonstrating opportunities and limitations in primary care. JMIR\nMedical Education 9, 1 (2023), e46599.\n[187] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239 (2022).\n[188] Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas,\nPeter Mattson, Adina Williams, and Douwe Kiela. 2022. Dynatask: A framework for creating dynamic AI benchmark\ntasks. arXiv preprint arXiv:2204.01906 (2022).\n[189] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D\nManning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models\nfine-tuned with human feedback. arXiv preprint arXiv:2305.14975 (2023).\n[190] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Automated testing of deep-neural-network-\ndriven autonomous cars. In Proceedings of the 40th international conference on software engineering. 303\u2013314.\n[191] ToolBench. 2023. Open-source tools learning benchmarks. https://github.com/sambanova/toolbench.\n[192] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste\nRozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971 (2023).\n[193] Alan M Turing. 2009. Computing machinery and intelligence. Springer.\n[194] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. On the Planning\nAbilities of Large Language Models\u2013A Critical Investigation. arXiv preprint arXiv:2305.15771 (2023).\n[195] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large Language Models\nStill Can\u2019t Plan (A Benchmark for LLMs on Planning and Reasoning about Change). arXiv preprint arXiv:2206.10498\n(2022).\n[196] Chris Van Der Lee, Albert Gatt, Emiel Van Miltenburg, Sander Wubben, and Emiel Krahmer. 2019. Best practices for\nthe human evaluation of automatically generated text. In Proceedings of the 12th International Conference on Natural\nLanguage Generation. 355\u2013368.\n[197] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[198] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou,\nQuoc Le, and Thang Luong. 2023. FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation.\narXiv:2310.03214 [cs.CL]\n[199] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in\nneural information processing systems 32 (2019).\n[200] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 (2018).\n[201] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik\nDutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi\nKoyejo, Dawn Song, and Bo Li. 2023. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\nModels. arXiv:2306.11698 [cs.CL]\n[202] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model.\n[203] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo\nLi. 2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint\narXiv:2111.02840 (2021).\n[204] Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and Yue Zhang. 2023. Evaluating open question\nanswering evaluation. arXiv preprint arXiv:2305.12421 (2023).\n[205] Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Ruifeng Xu, and Kam-Fai Wong. 2023. Chain-of-thought prompting\nfor responding to in-depth dialogue questions with LLM. arXiv:2305.11792 [cs.CL]\n[206] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye,\nXiubo Geng, et al. 2023. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. In ICLR\nworkshop on Trustworthy and Reliable Large-Scale Machine Learning Models.\n[207] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip\nYu. 2022. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and\nData Engineering (2022).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:43\n[208] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-\nlevel machine translation with large language models. arXiv preprint arXiv:2304.02210 (2023).\n[209] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023.\nLarge language models are not fair evaluators. arXiv preprint arXiv:2305.17926 (2023).\n[210] Rose E Wang and Dorottya Demszky. 2023. Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance\nFor Scoring and Providing Actionable Insights on Classroom Instruction. arXiv preprint arXiv:2306.03090 (2023).\n[211] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan\nLi, Xiang Wan, Benyou Wang, et al. 2023. CMB: A Comprehensive Medical Benchmark in Chinese. arXiv preprint\narXiv:2308.08833 (2023).\n[212] Xuena Wang, Xueting Li, Zi Yin, Yue Wu, and Liu Jia. 2023. Emotional Intelligence of Large Language Models.\narXiv:2307.09042 [cs.AI]\n[213] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023. MINT: Evaluating\nLLMs in Multi-turn Interaction with Tools and Language Feedback. arXiv preprint arXiv:2309.10691 (2023).\n[214] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-\ndecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).\n[215] Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Hao Chen, Wei Ye, Rui Xie, Xing Xie, and Shikun Zhang.\n2023. Exploring Vision-Language Models for Imbalanced Learning. arXiv preprint arXiv:2304.01457 (2023).\n[216] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong\nWang, Xing Xie, et al. 2023. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.\narXiv preprint arXiv:2306.05087 (2023).\n[217] Zhuo Wang, Rongzhen Li, Bowen Dong, Jie Wang, Xiuxing Li, Ning Liu, Chenhui Mao, Wei Zhang, Liling Dong, Jing\nGao, et al. 2023. Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today.\narXiv preprint arXiv:2306.01499 (2023).\n[218] Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023. Is ChatGPT a Good Sentiment Analyzer? A\nPreliminary Study. arXiv:2304.04339 [cs.CL]\n[219] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682\n(2022).\n[220] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and\nWilliam Fedus. 2022. Emergent Abilities of Large Language Models. Trans. Mach. Learn. Res. 2022 (2022).\n[221] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. 2023. CMATH: Can Your Language Model Pass\nChinese Elementary School Math Test? arXiv:2306.16636 [cs.CL]\n[222] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-\nSmith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv\npreprint arXiv:2302.11382 (2023).\n[223] Tzu-Tsung Wong. 2015. Performance evaluation of classification algorithms by k-fold and leave-one-out cross\nvalidation. Pattern Recognition 48, 9 (2015), 2839\u20132846.\n[224] Patrick Y Wu, Joshua A Tucker, Jonathan Nagler, and Solomon Messing. 2023. Large Language Models Can Be Used\nto Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. arXiv preprint arXiv:2303.12057 (2023).\n[225] Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard\nPeng, and Chi Wang. 2023. An Empirical Study on Challenging Math Problem Solving with GPT-4. arXiv preprint\narXiv:2306.01337 (2023).\n[226] Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy.\n2022. Autoformalization with large language models. Advances in Neural Information Processing Systems 35 (2022),\n32353\u201332368.\n[227] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00fcrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and\nYoon Kim. 2023. Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through\nCounterfactual Tasks. arXiv preprint arXiv:2307.02477 (2023).\n[228] Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia. 2023. Ask Again, Then Fail: Large Language Models\u2019 Vacillations in\nJudgement. arXiv:2310.02174 [cs.CL]\n[229] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023. Are Large Language Models\nReally Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. arXiv\npreprint arXiv:2306.09841 (2023).\n[230] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji\nZhang, Chao Peng, Fei Huang, and Jingren Zhou. 2023. CValues: Measuring the Values of Chinese Large Language\nModels from Safety to Responsibility. arXiv:2307.09705 [cs.CL]\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n111:44\nChang et al.\n[231] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao,\nand Ping Luo. 2023. LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models.\narXiv:2306.09265 [cs.CV]\n[232] Ruiyun Xu, Yue Feng, and Hailiang Chen. 2023. ChatGPT vs. Google: A Comparative Study of Search Performance\nand User Experience. arXiv preprint arXiv:2307.01135 (2023).\n[233] Kai-Cheng Yang and Filippo Menczer. 2023. Large language models can rate news outlet credibility. arXiv preprint\narXiv:2304.00228 (2023).\n[234] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue\nZhang. 2022. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization\nperspective. arXiv preprint arXiv:2211.08073 (2022).\n[235] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang,\nZhiyong Wang, et al. 2023. LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and\nBenchmark. arXiv preprint arXiv:2306.06687 (2023).\n[236] Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang,\nHanming Li, et al. 2023. KoLA: Carefully Benchmarking World Knowledge of Large Language Models. arXiv preprint\narXiv:2306.09296 (2023).\n[237] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. 2023. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models.\narXiv preprint arXiv:2309.12284 (2023).\n[238] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\n2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023).\n[239] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations.\narXiv:2306.04618 [cs.CL]\n[240] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to Go\nNext for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited. arXiv:2303.13835 [cs.IR]\n[241] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do Large Language\nModels perform in Arithmetic tasks? arXiv preprint arXiv:2304.02015 (2023).\n[242] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013. Learning fair representations. In\nInternational conference on machine learning. PMLR, 325\u2013333.\n[243] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).\n[244] Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2023. Evaluating\nand Improving Tool-Augmented Computation-Intensive Math Reasoning. arXiv preprint arXiv:2306.02408 (2023).\n[245] Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2023. Evaluating\nand Improving Tool-Augmented Computation-Intensive Math Reasoning. arXiv preprint arXiv:2306.02408 (2023).\n[246] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Is ChatGPT Fair for Rec-\nommendation? Evaluating Fairness in Large Language Model Recommendation. arXiv preprint arXiv:2305.07609\n(2023).\n[247] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona\nDiab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068 (2022).\n[248] Sarah J Zhang, Samuel Florin, Ariel N Lee, Eamon Niknafs, Andrei Marginean, Annie Wang, Keith Tyser, Zad Chin,\nYann Hicke, Nikhil Singh, et al. 2023. Exploring the MIT Mathematics and EECS Curriculum Using Large Language\nModels. arXiv preprint arXiv:2306.08997 (2023).\n[249] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675 (2019).\n[250] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2023. M3Exam: A Multi-\nlingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. arXiv preprint arXiv:2306.05179\n(2023).\n[251] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment Analysis in the Era of Large\nLanguage Models: A Reality Check. arXiv preprint arXiv:2305.15005 (2023).\n[252] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023.\nWider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862 (2023).\n[253] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\nChen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren\u2019s Song in the AI Ocean: A\nSurvey on Hallucination in Large Language Models. arXiv:2309.01219 [cs.CL]\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\nA Survey on Evaluation of Large Language Models\n111:45\n[254] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and\nMinlie Huang. 2023. SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions.\narXiv preprint arXiv:2309.07045 (2023).\n[255] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han,\nand Baobao Chang. 2023. MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. arXiv\npreprint arXiv:2309.07915 (2023).\n[256] Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, and Mykola Pechenizkiy. 2023. CHBias: Bias Evaluation and\nMitigation of Chinese Conversational Language Models. arXiv:2305.11262 [cs.CL]\n[257] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\n[258] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. 2023. On Evaluating\nAdversarial Robustness of Large Vision-Language Models. arXiv preprint arXiv:2305.16934 (2023).\n[259] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li,\nZi Lin, Eric Xing, et al. 2023. LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. arXiv preprint\narXiv:2309.11998 (2023).\n[260] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with\nMT-Bench and Chatbot Arena. arXiv:2306.05685 [cs.CL]\n[261] Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022.\nTowards a unified multi-dimensional evaluator for text generation. arXiv preprint arXiv:2210.07197 (2022).\n[262] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan\nDuan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364\n(2023).\n[263] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022.\nLarge language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 (2022).\n[264] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang\nGong, Yue Zhang, et al. 2023. PromptBench: Towards Evaluating the Robustness of Large Language Models on\nAdversarial Prompts. arXiv preprint arXiv:2306.04528 (2023).\n[265] Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang\nMao, Shijin Wang, et al. 2023. Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective.\narXiv preprint arXiv:2306.10512 (2023).\n[266] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Exploring ai ethics of chatgpt: A diagnostic\nanalysis. arXiv preprint arXiv:2301.12867 (2023).\n[267] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, and Fatemeh Shiri. 2023.\nOn Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on\nCodex. arXiv preprint arXiv:2301.12868 (2023).\n[268] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019).\n[269] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can Large Language\nModels Transform Computational Social Science? arXiv preprint arXiv:2305.03514 (2023).\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n"
  },
  {
    "title": "Lost in the Middle: How Language Models Use Long Contexts",
    "link": "https://arxiv.org/pdf/2307.03172.pdf",
    "upvote": "28",
    "text": "Lost in the Middle: How Language Models Use Long Contexts\nNelson F. Liu1\u2217\nKevin Lin2\nJohn Hewitt1\nAshwin Paranjape3\nMichele Bevilacqua3\nFabio Petroni3\nPercy Liang1\n1Stanford University\n2University of California, Berkeley\n3Samaya AI\nnfliu@cs.stanford.edu\nAbstract\nWhile recent language models have the abil-\nity to take long contexts as input, relatively\nlittle is known about how well they use\nlonger context. We analyze the performance\nof language models on two tasks that require\nidentifying relevant information in their in-\nput contexts: multi-document question an-\nswering and key-value retrieval. We find that\nperformance can degrade significantly when\nchanging the position of relevant informa-\ntion, indicating that current language models\ndo not robustly make use of information in\nlong input contexts. In particular, we observe\nthat performance is often highest when rele-\nvant information occurs at the beginning or\nend of the input context, and significantly\ndegrades when models must access relevant\ninformation in the middle of long contexts,\neven for explicitly long-context models. Our\nanalysis provides a better understanding of\nhow language models use their input context\nand provides new evaluation protocols for\nfuture long-context language models.\n1\nIntroduction\nLanguage models have become an important and\nflexible building block in a variety of user-facing\nlanguage technologies, including conversational\ninterfaces, search and summarization, and collabo-\nrative writing (Shuster et al., 2022; Thoppilan et al.,\n2022; Lee et al., 2022, inter alia). These models\nperform downstream tasks primarily via prompting:\nall relevant task specification and data to process is\nformatted as a textual input context, and the model\nreturns a generated text completion. These input\ncontexts can contain thousands of tokens, espe-\ncially when language models are used to process\nlong documents (e.g., legal or scientific documents,\nconversation histories, etc.) or when language mod-\nels are augmented with external information (e.g.,\n*Work partially completed as an intern at Samaya AI.\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n55\n60\n65\n70\n75\nAccuracy\n20 Total Retrieved Documents (~4K tokens)\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-0613 (closed-book)\nFigure 1: Changing the location of relevant information\n(in this case, the position of the passage that answers an\ninput question) within the language model\u2019s input con-\ntext results in a U-shaped performance curve\u2014models\nare better at using relevant information that occurs at the\nvery beginning (primacy bias) or end of its input context\n(recency bias), and performance degrades significantly\nwhen models must access and use information located\nin the middle of its input context.\nrelevant documents from a search engine, database\nquery results, etc; Petroni et al., 2020; Ram et al.,\n2023; Shi et al., 2023; Mallen et al., 2023; Schick\net al., 2023, inter alia).\nHandling these use-cases requires language mod-\nels to successfully operate over long sequences. Ex-\nisting language models are generally implemented\nwith Transformers (Vaswani et al., 2017), which re-\nquire memory and compute that increases quadrat-\nically in sequence length.\nAs a result, Trans-\nformer language models were often trained with\nrelatively small context windows (between 512-\n2048 tokens). Recent improvements in hardware\n(e.g., faster GPUs with more memory) and algo-\nrithms (Dai et al., 2019; Dao et al., 2022; Poli et al.,\narXiv:2307.03172v3  [cs.CL]  20 Nov 2023\n2023; Rubin and Berant, 2023, inter alia) have\nresulted in language models with larger context\nwindows (e.g., 4096, 32K, and even 100K tokens),\nbut it remains unclear how these extended-context\nlanguage models make use of their input contexts\nwhen performing downstream tasks.\nWe empirically investigate this question via\ncontrolled experiments with a variety of state-of-\nthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in set-\ntings that require accessing and using information\nwithin an input context. In particular, our experi-\nments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input con-\ntexts, then their performance should be minimally\naffected by the position of the relevant information\nin the input context.\nWe first experiment with multi-document ques-\ntion answering, which requires models to reason\nover provided documents to find relevant informa-\ntion and use it to answer a given question; this task\nmimics the retrieval-augmented generation setup\nunderlying many commercial generative search and\nquestion answering applications (e.g., Bing Chat).\nIn this setting, we control (i) the input context\nlength by changing the number of documents in\nthe input context (akin to retrieving more or less\ndocuments in retrieval-augmented generation), and\n(ii) control the position of the relevant information\nwithin the input context by changing the order of\nthe documents to place the relevant document at\nthe beginning, middle or end of the context.\nWe find that changing the position of relevant\ninformation in the input context can substantially\naffect model performance, indicating that current\nlanguage models do not robustly access and use\ninformation in long input contexts. Furthermore,\nwe observe a distinctive U-shaped performance\ncurve (Figure 1); language model performance is\nhighest when relevant information occurs at the\nvery beginning (primacy bias) or end of its in-\nput context (recency bias), and performance sig-\nnificantly degrades when models must access and\nuse information in the middle of their input con-\ntext (\u00a72.3).\nFor example, when relevant infor-\nmation is placed in the middle of its input con-\ntext, GPT-3.5-Turbo\u2019s performance on the multi-\ndocument question task is lower than its perfor-\nmance when predicting without any documents (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\nquestion answering task, to what extent can lan-\nguage models even retrieve from their input con-\ntexts? We study this question with a synthetic key-\nvalue retrieval task, which is designed to be a mini-\nmal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant in-\nformation. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models strug-\ngle to robustly access and use information in their\ninput contexts, we study the role of model archi-\ntecture (decoder-only vs. encoder-decoder), query-\naware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\u2022 Encoder-decoder models are relatively robust\nto changes in the position of relevant informa-\ntion within their input context, but only when\nevaluated on sequences within its training-\ntime sequence length. When evaluated on\nsequences longer than those seen during train-\ning, we observe a U-shaped performance\ncurve (\u00a74.1).\n\u2022 Query-aware contextualization (placing the\nquery before and after the documents or key-\nvalue pairs) enables near-perfect performance\non the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\u2022 Even base language models (i.e., without in-\nstruction fine-tuning) show a U-shaped per-\nformance curve as we vary the position of\nrelevant information in the input context.\nOur results indicate that prompting language\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more informa-\ntion may help it perform the downstream task, but\nit also increases the amount of content that the\nmodel must reason over, potentially decreasing ac-\ncuracy. To better understand this trade-off in prac-\ntice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\none document that answers the question, none or\nmany of the top k documents may contain the an-\nswer in the open-domain QA setting. When re-\ntrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model perfor-\nmance saturates long before retriever recall satu-\nrates, indicating that current models fail to effec-\ntively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\nonly marginally improves performance (\u223c1.5% for\nGPT-3.5-Turbo and \u223c1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future long-\ncontext models; to claim that a language model can\nrobustly use information within long input con-\ntexts, it is necessary to show that its performance\nis minimally affected by the position of the rele-\nvant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data.1\n2\nMulti-Document Question Answering\nOur goal is to better understand how language mod-\nels use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make con-\ntrolled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n2.1\nExperimental Setup\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) k\ndocuments (e.g., passages from Wikipedia), where\nexactly one of the documents contains the answer\n1nelsonliu.me/papers/lost-in-the-middle\nto the question and k \u2212 1 \u201cdistractor\u201d documents\ndo not. This task requires the model to access the\ndocument that contains the answer within its input\ncontext and use it to answer the question. Figure 2\npresents an example.\nWe\ninstantiate\nthis\ntask\nwith\ndata\nfrom\nNaturalQuestions-Open\n(Lee\net\nal.,\n2019;\nKwiatkowski et al., 2019),\nwhich contains\nhistorical queries issued to the Google search\nengine, coupled with human-annotated answers\nextracted from Wikipedia. In particular, we take\nthe 2655 queries where the annotated long answer\nis a paragraph (as opposed to a list or a table). We\nuse passages (chunks of at most 100 tokens) from\nWikipedia as documents within our input contexts.\nFor each of the queries, we need a document\nthat contains the answer and k \u2212 1 distractor\ndocuments that do not contain the answer.\nTo\nobtain a document that answers the question, we\nuse the Wikipedia paragraph that contains the\nanswer from the NaturalQuestions annotations.\nTo collect k \u22121 distractor documents that do not\ncontain the answer, we use a retrieval system (Con-\ntriever, fine-tuned on MS-MARCO; Izacard et al.,\n2021) to retrieve the k \u2212 1 Wikipedia chunks that\nare most relevant to the query and do not contain\nany of the NaturalQuestions-annotated answers.2,3\nIn the input context, the distractor documents are\npresented in order of decreasing relevance.4\nTo modulate the position of relevant information\nwithin the input context, we adjust the order of the\ndocuments to change the position of the document\nthat contains the answer (Figure 3). To modulate\nthe input context length in this task, we increase or\ndecrease the number of retrieved documents that\ndo not contain the answer (Figure 4).\nFollowing Kandpal et al. (2022) and Mallen et al.\n(2023), we use accuracy as our primary evaluation\nmetric, judging whether any of the correct answers\n(as taken from the NaturalQuestions annotations)\nappear in the predicted output.\n2Ambiguity in NaturalQuestions-Open means that a small\nnumber of distractor passages may contain a reasonable an-\nswer. We additionally run experiments on subset of unam-\nbiguous questions, finding similar results and conclusions; see\nAppendix A.\n3We also explored using random documents as distractors,\nsee Appendix B for more details.\n4Since there might be a prior over \u201csearch results\u201d appear-\ning in ranked order, we explored randomly ordering the k \u2212 1\ndistractor documents and mentioning that the documents are\nrandomly ordered in the task description, but found the same\ntrends. See Appendix C for more details.\nWrite a high-quality answer for the given question using only the provided search \nresults (some of which might be irrelevant).\nDocument [1](Title: Asian Americans in science and technology) Prize in physics for \ndiscovery of the subatomic particle J/\u03c8. Subrahmanyan Chandrasekhar shared...\nDocument [2](Title: List of Nobel laureates in Physics) The first Nobel Prize in \nPhysics was awarded in 1901 to Wilhelm Conrad R\u00f6ntgen, of Germany, who received...\nDocument [3](Title: Scientist) and pursued through a unique method, was essentially \nin place. Ram\u00f3n y Cajal won the Nobel Prize in 1906 for his remarkable...\nQuestion: who got the first nobel prize in physics\nAnswer:\nInput Context\nWilhelm Conrad R\u00f6ntgen\nDesired Answer\nFigure 2: Example of the multi-document question answering task, with an input context and the desired model\nanswer. The document containing the answer is bolded within the input context here for clarity.\nWrite a high-quality answer for the given question \nusing only the provided search results (some of \nwhich might be irrelevant).\nDocument [1](Title: List of Nobel laureates in \nPhysics) ...\nDocument [2](Title: Asian Americans in science and \ntechnology) ...\nDocument [3](Title: Scientist) ...\nQuestion: who got the first nobel prize in physics\nAnswer:\nInput Context\nWilhelm Conrad R\u00f6ntgen\nDesired Answer\nFigure 3: Modulating the position of relevant informa-\ntion within the input context for the multi-document\nquestion answering example presented in Figure 2. Re-\nordering the documents in the input context does not\naffect the desired output.\nOur experimental setup is similar to the needle-\nin-a-haystack experiments of Ivgi et al. (2023), who\ncompare question answering performance when the\nrelevant paragraph is placed (i) at the beginning of\nthe input or (ii) a random position within the in-\nput. They find that encoder-decoder models have\nsignificantly higher performance when relevant in-\nformation is placed at the start of the input context.\nIn contrast, we study finer-grained changes in the\nposition of relevant information.\n2.2\nModels\nWe analyze several state-of-the-art open and closed\nlanguage models. We use greedy decoding when\ngenerating outputs and leave exploration of other\ndecoding methods to future work. We use a stan-\ndard set of prompts for each model (Figure 2).\nWrite a high-quality answer for the given question \nusing only the provided search results (some of \nwhich might be irrelevant).\nDocument [1](Title: Asian Americans in science and \ntechnology) ...\nDocument [2](Title: List of Nobel laureates in \nPhysics) ...\nDocument [3](Title: Scientist) ...\nDocument [4](Title: Norwegian Americans) ...\nDocument [5](Title: Maria Goeppert Mayer) ...\nQuestion: who got the first nobel prize in physics\nAnswer:\nInput Context\nInput Context\nWilhelm Conrad R\u00f6ntgen\nDesired Answer\nFigure 4: Modulating the input context length of the\nmulti-document question answering example presented\nin Figure 2. Adding documents that do not contain the\nanswer increases the length of the input context, but\ndoes not affect the desired output.\nOpen models.\nWe experiment with MPT-30B-\nInstruct, which has a maximum context length of\n8192 tokens. The model was initially pre-trained\non 1 trillion tokens using 2048-token sequences,\nfollowed by an additional sequence length adapta-\ntion pre-training phase on 50 billion tokens using\n8192-token sequences. MPT-30B-Instruct uses AL-\niBi (Press et al., 2022) to represent positional infor-\nmation. We also evaluate LongChat-13B (16K) (Li\net al., 2023), which extends the LLaMA-13B (Tou-\nvron et al., 2023a) context window from 2048 to\n16384 tokens by using condensed rotary positional\nembeddings before fine-tuning with 16384-token\nsequences.\nClosed models.\nWe use the OpenAI API to ex-\nperiment with GPT-3.5-Turbo and GPT-3.5-Turbo\n1st\n5th\n10th\nPosition of Document with the Answer\n50\n55\n60\n65\n70\n75\nAccuracy\n10 Total Retrieved Documents (~2K tokens)\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n50\n55\n60\n65\n70\n75\nAccuracy\n20 Total Retrieved Documents (~4K tokens)\n1st\n5th\n10th\n15th\n20th\n25th\n30th\nPosition of Document with the Answer\n50\n55\n60\n65\n70\n75\nAccuracy\n30 Total Retrieved Documents (~6K tokens)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\nFigure 5: The effect of changing the position of relevant information (document containing the answer) on multi-\ndocument question answering performance. Lower positions are closer to the start of the input context. Performance\nis highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models\nmust reason over information in the middle of their input context.\n(16K).5 GPT-3.5-Turbo has a maximum context\nlength of 4K tokens, and GPT-3.5-Turbo (16K) is a\nversion with an extended maximum context length\nof 16K tokens. We evaluate Claude-1.3 and Claude-\n1.3 (100K) with the Anthropic API; Claude-1.3\nhas a maximum context length of 8K tokens, and\nClaude-1.3 (100K) has an extended context length\nof 100K tokens. 6\n2.3\nResults and Discussion\nWe experiment with input contexts containing 10,\n20, and 30 total documents. Figure 5 presents multi-\ndocument question answering performance when\nvarying the position of relevant information within\nthe input context. To contextualize model perfor-\nmance, we also evaluate on the closed-book and\noracle settings (Table 1). In the closed-book setting,\nmodels are not given any documents in their input\ncontext, and must rely on their parametric memory\nto generate the correct answer. On the other hand,\nin the oracle setting, language models are given the\nsingle document that contains the answer and must\nuse it to answer the question.\nModel performance is highest when relevant in-\nformation occurs at the beginning or end of its\ninput context.\nAs illustrated in Figure 5, chang-\ning the position of relevant information in the in-\nput context leads to substantial decreases in model\nperformance. In particular, we see a distinctive U-\n5We use the 0613 OpenAI model versions.\n6We also evaluate GPT-4 (8K) on a subset of multi-\ndocument QA experiments, finding similar results and trends\nas other models (though GPT-4 has higher absolute perfor-\nmance). Evaluating GPT-4 on the full multi-document QA\nand key-value retrieval experiments would cost upwards of\n$6000. See Appendix D for GPT-4 results and discussion.\nModel\nClosed-Book\nOracle\nLongChat-13B (16K)\n35.0%\n83.4%\nMPT-30B-Instruct\n31.5%\n81.9%\nGPT-3.5-Turbo\n56.1%\n88.3%\nGPT-3.5-Turbo (16K)\n56.0%\n88.6%\nClaude-1.3\n48.3%\n76.1%\nClaude-1.3 (100K)\n48.2%\n76.4%\nTable 1: Closed-book and oracle accuracy of language\nmodels on the multi-document question answering task.\nshaped performance curve\u2014models are often much\nbetter at using relevant information that occurs at\nthe very beginning (primacy bias) and very end of\ncontexts (recency bias), and suffer degraded perfor-\nmance when forced to use information within the\nmiddle of its input context. For example, GPT-3.5-\nTurbo\u2019s multi-document QA performance can drop\nby more than 20%\u2014in the worst case, performance\nin 20- and 30-document settings is lower than per-\nformance without any input documents (i.e., closed-\nbook performance; 56.1%). These results indicate\nthat current models cannot effectively reason over\ntheir entire context window when prompted for\ndownstream tasks.\nExtended-context models are not necessarily bet-\nter at using input context.\nWhen the input con-\ntext fits in the context window of both a model\nand its extended-context counterpart, we see that\nperformance between them is nearly identical. For\nexample, the 10- and 20-document settings both\nfit in the context window of GPT-3.5-Turbo and\nGPT-3.5-Turbo (16K), and we observe that their\nperformance as a function of position of relative\ninformation is nearly superimposed (solid purple\nand dashed brown series in Figure 5). These results\nExtract the value corresponding to the specified key in the JSON object below.\nJSON data:\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\": \"bb3ba2a5-7de8-434b-a86e-a88bb9fa7289\",\n \"a54e2eed-e625-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\n \"9f4a92b9-5f69-4725-ba1e-403f08dea695\": \"703a7ce5-f17f-4e6d-b895-5836ba5ec71c\",\n \"52a9c80c-da51-4fc9-bf70-4a4901bc2ac3\": \"b2f8ea3d-4b1b-49e0-a141-b9823991ebeb\",\n \"f4eb1c53-af0a-4dc4-a3a5-c2d50851a178\": \"d733b0d2-6af3-44e1-8592-e5637fdb76fb\"}\nKey: \"9f4a92b9-5f69-4725-ba1e-403f08dea695\"\nCorresponding value:\nInput Context\n703a7ce5-f17f-4e6d-b895-5836ba5ec71c\nDesired Output\nFigure 6: Example of the key-value retrieval task, with an input context and the desired model output. Given a key,\nthe goal is to return the associated value. All keys and values are 128-bit UUIDs. The relevant key-value pair for\nanswering the query is bolded here within the input context for clarity.\nindicate that extended-context models are not nec-\nessarily better than their non-extended counterparts\nat using their input context.\n3\nHow Well Can Language Models\nRetrieve From Input Contexts?\nGiven that language models struggle to retrieve\nand use information from the middle of their input\ncontexts in the multi-document question answering\ntask, to what extent can they simply retrieve from\ninput contexts? We study this question with a syn-\nthetic key-value retrieval task, which is designed to\nprovide a minimal testbed for the basic ability to\nretrieve matching tokens from an input context.\n3.1\nExperimental Setup\nIn our synthetic key-value retrieval task, the inputs\nare (i) a string-serialized JSON object with k key-\nvalue pairs, where each of the keys and values are\nunique, randomly-generated UUIDs and (ii) a key\nwithin the aforementioned JSON object. The goal\nis to return the value associated with the specified\nkey. Thus, each JSON object contains one relevant\nkey-value pair (where the value is to be returned),\nand k \u2212 1 irrelevant \u201cdistractor\u201d key-value pairs.\nFigure 6 provides an example input context and its\ncorresponding desired output. We again measure\naccuracy by evaluating whether the correct value\nappears in the predicted output.\nOur synthetic key-value retrieval task shares sim-\nilar goals with the Little Retrieval Test of Papail-\niopoulos et al. (2023) and the fine-grained line re-\ntrieval task of Li et al. (2023), but we explicitly\nseek to distill and simplify the task by removing as\nmuch natural language semantics as possible (using\nrandom UUIDs instead), since language features\nmay present potential confounders. For example,\nTransformer language models may have varying\nsensitivity to different linguistic features in their\ninput (O\u2019Connor and Andreas, 2021).\nTo modulate the position of relevant information\nwithin the input context, we change the position\nof the key to retrieve within the serialized JSON\nobject. To modulate the input context length, we\nchange the number of input JSON key-value pairs\nk by adding or removing random keys, changing\nthe number of distractor key-value pairs.\n3.2\nResults and Discussion\nWe experiment with input contexts containing\n75, 140, and 300 key-value pairs (500 examples\neach). We use the same set of models as the multi-\ndocument question answering experiments, see\n\u00a72.2 for more details.\nFigure 7 presents key-value retrieval perfor-\nmance.\nClaude-1.3 and Claude-1.3 (100K) do\nnearly perfectly on all evaluated input context\nlengths, but other models struggle, especially\nwhen contexts have 140 or 300 key-value pairs\u2014\nalthough the synthetic key-value retrieval task only\nrequires identifying exact match within the input\ncontext, not all models achieve high performance.\nSimilar to our multi-document QA results, GPT-\n3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30B-\nInstruct have the lowest performance when they\nmust access key-value pairs in the middle of their\ninput context. LongChat-13B (16K) exhibits a dif-\nferent trend in the 140 key-value setting; we quali-\ntatively observe that when relevant information is\n1st\n25th\n50th\n75th\nPosition of Key to Retrieve\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n75 Key-Value Pairs (~4K tokens)\n1st\n35th\n70th\n105th\n140th\nPosition of Key to Retrieve\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n140 Key-Value Pairs (~8K tokens)\n1st\n50th\n100th 150th 200th 250th 300th\nPosition of Key to Retrieve\n40\n50\n60\n70\n80\n90\n100\nAccuracy\n300 Key-Value Pairs (~16K tokens)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\nFigure 7: The effect of changing the input context length and the position of relevant information on key-value\nretrieval performance. Lower positions are closer to the start of the input context. Although some models show\nperfect accuracy on this synthetic task (e.g., Claude-1.3 and Claude-1.3 (100K)), we see again that performance is\noften highest when relevant information is occurs at the very start or end of the context, and rapidly degrades when\nmodels must retrieve from the middle of the input context.\nplaced at the start of the input context, LongChat-\n13B (16K) tends to generate code to retrieve the\nkey, rather than outputting the value directly.\n4\nWhy Are Language Models Not Robust\nto Changes in the Position of Relevant\nInformation?\nOur multi-document question answering and key-\nvalue retrieval results show that language models\nstruggle to robustly access and use information in\nlong input contexts, since performance degrades\nsignificantly when changing the position of rele-\nvant information. To better understand why, we per-\nform some preliminary investigations into the role\nof model architecture (decoder-only vs. encoder-\ndecoder), query-aware contextualization, and in-\nstruction fine-tuning.\n4.1\nEffect of Model Architecture\nThe open models we evaluated are all decoder-only\nmodels\u2014at each timestep, they may only attend\nto prior tokens. To better understand the poten-\ntial effects of model architecture on how language\nmodel use context, we compare decoder-only and\nencoder-decoder language models.\nWe experiment with Flan-T5-XXL (Raffel et al.,\n2020; Chung et al., 2022) and Flan-UL2 (Tay et al.,\n2023). Flan-T5-XXL is trained with a sequences\nof 512 tokens (encoder and decoder). Flan-UL2 is\ninitially trained with sequences of 512 tokens (en-\ncoder and decoder), but is then pre-trained for an\nextra 100K steps with 1024 tokens (encoder and de-\ncoder) before instruction fine-tuning on sequences\nwith 2048 tokens in the encoder and 512 tokens\nin the decoder. However, since these models use\nrelative positional embeddings, they can (in prin-\nciple) extrapolate beyond these maximum context\nlengths; Shaham et al. (2023) find that both mod-\nels can perform well with sequences of up to 8K\ntokens.\nFigure 8 compares the performance of decoder-\nonly and encoder-decoder models. When Flan-UL2\nis evaluated on sequences within its 2048-token\ntraining-time context window (Figure 8; left sub-\nplot), its performance is relatively robust to changes\nin the position of relevant information within the\ninput context (1.9% absolute difference between\nbest- and worst-case performance). When evalu-\nated on settings with sequences longer than 2048\ntokens (Figure 8; center and right), Flan-UL2 per-\nformance begins to degrade when relevant informa-\ntion is placed in the middle. Flan-T5-XXL shows\na similar trend, where longer input contexts result\nin a greater performance degradation when placing\nrelevant information in the middle of the input con-\ntext. We hypothesize that encoder-decoder models\nmay make better use of their context windows be-\ncause their bidirectional encoder allows processing\neach document in the context of future documents,\npotentially improving relative importance estima-\ntion between documents.\n4.2\nEffect of Query-Aware Contextualization\nOur multi-document QA and key-value retrieval\nexperiments place the query (i.e., question to an-\nswer or key to retrieve) after the data to process\n(i.e., the documents or the key-value pairs). As a\nresult, decoder-only models cannot attend to query\ntokens when contextualizing documents or key-\nvalue pairs, since the query only appears at the end\n1st\n5th\n10th\nPosition of Document with the Answer\n50\n55\n60\n65\n70\nAccuracy\n10 Total Retrieved Documents (~2K tokens)\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n50\n55\n60\n65\n70\nAccuracy\n20 Total Retrieved Documents (~4K tokens)\n1st\n5th\n10th\n15th\n20th\n25th\n30th\nPosition of Document with the Answer\n50\n55\n60\n65\n70\nAccuracy\n30 Total Retrieved Documents (~6K tokens)\nmpt-30b-instruct\nlongchat-13b-16k\nflan-t5-xxl\nflan-ul2\nFigure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are shorter\nthan their encoder\u2019s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively\nrobust to changes in the position of relevant information within their input context (left subplot). In contrast, when\nthese models are evaluated on sequences longer than those seen during training (center and right subplots), we\nobserve a U-shaped performance curve\u2014performance is higher when relevant information occurs at the beginning\nor end of the input context, as opposed to the middle of the input context.\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n50\n60\n70\n80\nAccuracy\n20 Total Retrieved Documents \n(~4K tokens, query-aware contextualization)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\nFigure 9: Query-aware contextualization (placing the\nquery before and after the documents) does not sub-\nstantially improve robustness of language models to\nchanging the position of relevant information in multi-\ndocument QA; performance slightly increases when\nrelevant information occurs at the very beginning, but\notherwise slightly decreases.\nof the prompt and decoder-only models can only\nattend to prior tokens at each timestep. In contrast,\nencoder-decoder models (which seem more robust\nto changes in the position of relevant information;\n\u00a74.1) use a bidirectional encoder to contextualize\ninput contexts\u2014can we use this observation to im-\nprove decoder-only models by placing the query be-\nfore and after the data, enabling query-aware con-\ntextualization of documents (or key-value pairs)?\nWe find that query-aware contextualization dra-\nmatically improves performance on the key-value\nretrieval task\u2014all models achieve near-perfect per-\nformance on the 75, 140, and 300 key-value pair\nsettings. For example, GPT-3.5-Turbo (16K) with\nquery-aware contextualization achieves perfect per-\nformance when evaluated with 300 key-value pairs.\nIn contrast, without query-aware contextualiza-\ntion, the worst-case performance is 45.6% (Fig-\nure 7).\nDespite the significant impact on key-\nvalue retrieval performance, query-aware contextu-\nalization minimally affects performance trends in\nthe multi-document question answering task (Fig-\nure 9); it slightly improves performance when the\nrelevant information is located at the very begin-\nning of the input context, but slightly decreases\nperformance in other settings.\n4.3\nEffect of Instruction Fine-Tuning\nThe models we evaluated are all instruction fine-\ntuned\u2014after their initial pre-training, they undergo\nsupervised fine-tuning on a dataset of instructions\nand responses. The task specification and/or in-\nstruction is commonly placed at the beginning of\nthe input context in supervised instruction fine-\ntuning data, which might lead instruction fine-\ntuned language models to place more weight on\nthe start of the input context. To better understand\nthe potential effects of instruction fine-tuning on\nhow language models use long input contexts, we\ncompare the multi-document question answering\nperformance of MPT-30B-Instruct against its base\nmodel (i.e., before instruction fine-tuning) MPT-\n30B. We use the same experimental setup as \u00a72.\nFigure 10 compares the multi-document QA\nperformance of MPT-30B and MPT-30B-Instruct\nas a function of the position of the relevant in-\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n44\n46\n48\n50\n52\n54\n56\nAccuracy\n20 Total Retrieved Documents (~4K tokens)\nmpt-30b\nmpt-30b-instruct\nFigure 10: Multi-document QA performance of MPT-\n30B-Instruct compared against its base model (i.e., be-\nfore instruction fine-tuning) MPT-30B. Both models\nhave a U-shaped performance curve, where performance\nis much higher when relevant information occurs at the\nstart or end of the input context, indicating that the\ninstruction fine-tuning process itself is not necessarily\nresponsible for these performance trends.\nformation in the input context. Surprisingly, we\nsee that both MPT-30B and MPT-30B-Instruct ex-\nhibit a U-shaped performance curve, where perfor-\nmance is highest when relevant information occurs\nat the very beginning or very end of the context.\nAlthough the absolute performance of MPT-30B-\nInstruct is uniformly higher than that of MPT-30B,\ntheir overall performance trends are similar. We\nalso observe that instruction fine-tuning slightly re-\nduces the worst-case performance disparity from\nnearly 10% between the base model best- and\nworst-case performance to around 4%.\nThese observations complement prior work,\nwhich found that non-instruction fine-tuned lan-\nguage models are biased towards recent tokens (i.e.,\nthe end of the input context; Khandelwal et al.,\n2018; Press et al., 2021). This recency bias has\nbeen observed in past work when evaluating mod-\nels on next-word prediction of contiguous text, a\nsetting where language models minimally benefit\nfrom long-range information (Sun et al., 2021). In\ncontrast, our results show that language models\nare capable of using longer-range information (i.e.,\nthe beginning of the input context) when prompted\nwith instruction-formatted data. We hypothesize\nthat non-instruction fine-tuned language models\nlearn to use these long contexts from similarly-\nformatted data that may occur in Internet text seen\nduring pre-training, e.g., StackOverflow questions\nand answers.\nTo better understand the effect of additional fine-\ntuning and model scale, we also experimented\nwith Llama-2 models of varying sizes (7B, 13B,\nand 70B) with and without additional supervised\nfine-tuning and reinforcement learning from hu-\nman feedback (Appendix E). We find that the U-\nshaped performance curve only appears in suffi-\nciently large language models (with or without ad-\nditional fine-tuning)\u2014the 7B Llama-2 models are\nsolely recency biased, while the 13B and 70B mod-\nels exhibit a U-shaped performance curve. In addi-\ntion, we see that the Llama-2 supervised fine-tuning\nand reinforcement learning from human feedback\nprocedure slightly mitigates the positional bias in\nsmaller models (13B, akin to trends shown when\ncomparing MPT-30B and MPT-30B-Instruct), but\nminimally affects trends on larger models (70B).\n5\nIs More Context Is Always Better?\nA Case Study With Open-Domain QA\nOur results indicate that prompting language mod-\nels with longer input contexts is a trade-off\u2014\nproviding the language model with more informa-\ntion may help it perform the downstream task, but\nit also increases the amount of content that the\nmodel must reason over, potentially decreasing\naccuracy. Even if a language model can take in\n16K tokens, is it actually beneficial to provide 16K\ntokens of context? The answer to this question\nis ultimately downstream task-specific since it de-\npends on the marginal value of the added context\nand the model\u2019s ability to effectively use long input\ncontexts, but we perform a case study with open-\ndomain question answering on NaturalQuestions-\nOpen to better understand this trade-off in existing\nlanguage models.\nWe use language models in a standard retriever-\nreader setup. A retrieval system (Contriever, fine-\ntuned on MS-MARCO) takes an input query from\nNaturalQuestions-Open and returns the k docu-\nments from Wikipedia with the highest relevance\nscore. To condition language models on these re-\ntrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of retrieved documents k. We use a subset\nof NaturalQuestions-Open where the long answer\nis a paragraph (as opposed to a table or a list).\nFigure 11 presents retriever recall and open-\n5\n10\n20\n30\n40\n50\nNumber of Retrieved Docs\n50\n60\n70\n80\n90\nMetric\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\ncontriever recall\nFigure 11: Retriever recall and model performance as a\nfunction of the number of retrieved documents. Model\nperformance saturates long before retriever recall, indi-\ncating that the models have difficulty making use of the\nextra retrieved documents.\ndomain QA results. We see that reader model\nperformance saturates long before retriever per-\nformance saturates, indicating that readers are not\neffectively using the extra context. Using more\nthan 20 retrieved documents only marginally im-\nproves reader performance (\u223c1.5% for GPT-3.5-\nTurbo and \u223c1% for Claude-1.3), while significantly\nincreasing the input context length (and thus la-\ntency and cost). These results, coupled with the\nobservation that models are often better at retriev-\ning and using information at the start or end of\nthe input contexts, suggest that effective rerank-\ning of retrieved documents (pushing relevant infor-\nmation closer to the start of the input context) or\nranked list truncation (retrieving fewer documents\nwhen appropriate; Arampatzis et al., 2009) may be\npromising directions for improving how language-\nmodel-based readers use retrieved context.\n6\nRelated Work\n6.1\nLong-Context Language Models\nThere is much prior work in designing performant\nlanguage models with cheaper scaling than Trans-\nformers in the context length. Many lines of work\npursue Transformer variants with attention modi-\nfications like recurrence (Dai et al., 2019), factor-\nizing attention into computationally less intensive\napproximations (Beltagy et al., 2020; Zaheer et al.,\n2020), or low-rank approximations (Wang et al.,\n2020; Peng et al., 2021). Dao et al. (2022) in-\nstead provide a faster exact attention by a carefully-\ncrafted IO-aware CUDA kernel. Separately, there\nare attempts to do away with attention entirely to\nremove quadratic sequence length complexity, of-\nten through convolution and/or linear RNNs, e.g.,\nin RWKV (Peng, 2023), S4 (Gu et al., 2022), or\nHyena (Poli et al., 2023). Many prior efforts evalu-\nate perplexity on a diverse web corpus as a proxy\nfor the ability to process long contexts; this work\nshows that precise knowledge access on long con-\ntexts may be an added challenge.\n6.2\nHow Do Language Models Use Context?\nThe pioneering work of Khandelwal et al. (2018)\nshowed that small LSTM language models make\nincreasingly coarse use of longer-term context;\nSankar et al. (2019) found similar results in di-\nalogue models. In a similar vein, Daniluk et al.\n(2017) find that attentive LSTM language mod-\nels tend to mainly use recent history.\nPetroni\net al. (2020) were among the first to demonstrate\nthe potential of combining context from an in-\nformation retrieval system with a pretrained lan-\nguage models for unsupervised question answering.\nO\u2019Connor and Andreas (2021) found that many\ninformation-destroying operations had marginal ef-\nfects on Transformer LMs\u2019 predictions. Krishna\net al. (2022) found that long-context neural gen-\neration in modestly-sized Transformer language\nmodels degenerates because models fail to prop-\nerly condition on long context. Finally, studying\nlong-context models, Sun et al. (2021) found that\nlonger contexts improves prediction of only a few\ntokens, an empirical finding consistent with the\ntheory of Sharan et al. (2018), who showed that\nsequence distributions with bounded mutual infor-\nmation necessarily lead to marginal average predic-\ntion benefits from increasingly long context. Qin\net al. (2023) analyze how efficient Transformers\nperform on a variety of long-context downstream\nNLP tasks, finding that long-context transformers\nare recency-biased and do not effectively use long-\nrange context.\n6.3\nThe Serial-Position Effect\nThe U-shaped curve we observe in this work has\na connection in psychology known as the serial-\nposition effect (Ebbinghaus, 1913; Murdock Jr,\n1962), that states that in free-association recall\nof elements from a list, humans tend to best re-\nmember the first and last elements of the list. The\nserial-position effect plays a role in understanding\nhow humans develop short- and long-term mem-\nory. Observing a serial-position-like effect in lan-\nguage models is perhaps surprising, since the self-\nattention mechanisms underlying Transformer lan-\nguage models is technically equally capable of re-\ntrieving any token from their contexts.\n7\nConclusion\nWe empirically study how language models use\nlong input contexts via a series of controlled ex-\nperiments. We show that language model perfor-\nmance degrades significantly when changing the\nposition of relevant information, indicating that\nmodels struggle to robustly access and use infor-\nmation in long input contexts. In particular, per-\nformance is often lowest when models must use\ninformation in the middle of long input contexts.\nWe conduct a preliminary investigation of the role\nof (i) model architecture, (ii) query-aware contextu-\nalization, and (iii) instruction fine-tuning to better\nunderstand how they affect how language models\nuse context. Finally, we conclude with a practi-\ncal case study of open-domain question answering,\nfinding that the performance of language model\nreaders saturates far before retriever recall. Our\nresults and analysis provide a better understanding\nof how language models use their input context\nand provides new evaluation protocols for future\nlong-context models.\nAcknowledgments\nWe would like to thank Luke Zettlemoyer, who\nserved as our TACL action editor, and the the\nanonymous reviewers for their comments and feed-\nback. We also thank Claudiu Leoveanu-Condrei,\nMegan Leszczynski, Dmytro Okhonko, Maithra\nRaghu, Eric Wallace and Sang Michael Xie for\nfeedback and discussions that helped improve this\nwork. Further, we are grateful to Sewon Min for\nher help with the AmbigQA dataset. This work\nwas supported by the Stanford Center for Research\non Foundation Models (CRFM), by OpenAI via\nan API credits grant to the Stanford CRFM, and\nby Anthropic via the Claude academic access pro-\ngram.\nReferences\nAvi Arampatzis, Jaap Kamps, and Stephen Robert-\nson. 2009. Where to stop reading a ranked list?\nthreshold optimization using truncated score dis-\ntributions. In Proc. of SIGIR.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer. ArXiv:2004.05150.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha\nBrahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen,\nAakanksha Chowdhery, Alex Castro-Ros, Marie\nPellat, Kevin Robinson, Dasha Valter, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent\nZhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\nDevlin, Adam Roberts, Denny Zhou, Quoc V.\nLe, and Jason Wei. 2022. Scaling instruction-\nfinetuned language models. ArXiv:2210.11416.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime\nCarbonell, Quoc Le, and Ruslan Salakhutdinov.\n2019. Transformer-XL: Attentive language mod-\nels beyond a fixed-length context. In Proc. of\nACL.\nMicha\u0142 Daniluk, Tim Rockt\u00e4schel, Johannes Welbl,\nand Sebastian Riedel. 2017. Frustratingly short\nattention spans in neural language modeling. In\nProc. of ICLR.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. FlashAttention: Fast\nand memory-efficient exact attention with IO-\nawareness. ArXiv:2205.14135.\nHermann Ebbinghaus. 1913. Memory: A contribu-\ntion to experimental psychology. H. A. Ruger &\nC. E. Bussenius, Trans.\nAlbert Gu, Karan Goel, and Christopher R\u00e9. 2022.\nEfficiently modeling long sequences with struc-\ntured state spaces. In Proc. of ICLR.\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\nEfficient long-text understanding with short-text\nmodels.\nTransactions of the Association for\nComputational Linguistics, 11:284\u2013299.\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\nSebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. 2021. Unsupervised\ndense information retrieval with contrastive\nlearning. ArXiv:2112.09118.\nGautier Izacard and Edouard Grave. 2021. Lever-\naging passage retrieval with generative models\nfor open domain question answering. In Proc.\nof EACL.\nNikhil Kandpal, Haikang Deng, Adam Roberts,\nEric Wallace, and Colin Raffel. 2022. Large lan-\nguage models struggle to learn long-tail knowl-\nedge. ArXiv:2211.08411.\nUrvashi Khandelwal, He He, Peng Qi, and Dan\nJurafsky. 2018. Sharp nearby, fuzzy far away:\nHow neural language models use context. In\nProc. of ACL.\nKalpesh Krishna, Yapei Chang, John Wieting, and\nMohit Iyyer. 2022. RankGen: Improving text\ngeneration with large ranking models. In Proc.\nof EMNLP.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia\nRedfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob\nDevlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, An-\ndrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. 2019. Natural Questions: A bench-\nmark for question answering research. Trans-\nactions of the Association for Computational\nLinguistics, 7:452\u2013466.\nKenton Lee, Ming-Wei Chang, and Kristina\nToutanova. 2019. Latent retrieval for weakly\nsupervised open domain question answering. In\nProc. of ACL.\nMina Lee, Percy Liang, and Qian Yang. 2022.\nCoAuthor: Designing a human-AI collaborative\nwriting dataset for exploring language model ca-\npabilities. In Proc. of CHI.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng,\nLianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, , and Hao Zhang. 2023. How long\ncan open-source LLMs truly promise on context\nlength?\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Daniel Khashabi, and Hannaneh Hajishirzi.\n2023. When not to trust language models: In-\nvestigating effectiveness of parametric and non-\nparametric memories. In Proc. of ACL.\nSewon Min, Julian Michael, Hannaneh Hajishirzi,\nand Luke Zettlemoyer. 2020. AmbigQA: An-\nswering ambiguous open-domain questions. In\nProc. of EMNLP.\nBennet B. Murdock Jr. 1962. The serial position\neffect of free recall. Journal of experimental\npsychology, 64(5):482.\nJoe O\u2019Connor and Jacob Andreas. 2021. What con-\ntext features can Transformer language models\nuse? In Proc. of ACL.\nDimitris Papailiopoulos, Kangwook Lee, and Jy-\nyong Sohn. 2023.\nA little retrieval test for\nlarge language models. https://github.com/\nanadim/the-little-retrieval-test.\nBo Peng. 2023. RWKV-LM. https://github.\ncom/BlinkDL/RWKV-LM.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong.\n2021. Random feature attention. In Proc. of\nICLR.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus,\nTim Rockt\u00e4schel, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. 2020. How context\naffects language models\u2019 factual predictions. In\nProc. of AKBC.\nMichael Poli, Stefano Massaroli, Eric Nguyen,\nDaniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua\nBengio, Stefano Ermon, and Christopher R\u00e9.\n2023. Hyena hierarchy: Towards larger con-\nvolutional language models. In Proc. of ICML.\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In Proc. of ACL.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022.\nTrain short, test long: Attention with linear bi-\nases enables input length extrapolation. In Proc.\nof ICLR.\nGuanghui Qin,\nYukun Feng,\nand Benjamin\nVan Durme. 2023. The NLP task effectiveness\nof long-range transformers. In Proc. of EACL.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex-\nploring the limits of transfer learning with a uni-\nfied text-to-text Transformer. Journal of Ma-\nchine Learning Research, 21(140):1\u201367.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor\nMuhlgay, Amnon Shashua, Kevin Leyton-\nBrown,\nand\nYoav\nShoham.\n2023.\nIn-\ncontext retrieval-augmented language models.\nArXiv:2302.00083.\nOhad Rubin and Jonathan Berant. 2023. Long-\nrange language modeling with self-retrieval.\nArXiv:2306.13421.\nChinnadhurai Sankar, Sandeep Subramanian, Chris\nPal, Sarath Chandar, and Yoshua Bengio. 2019.\nDo neural dialog systems use the conversation\nhistory effectively? an empirical study. In Proc.\nof ACL.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec,\nRoberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom.\n2023. Toolformer: Language models can teach\nthemselves to use tools.\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-\nrant, and Omer Levy. 2023. ZeroSCROLLS: A\nzero-shot benchmark for long text understanding.\nArXiv:2305.14196.\nVatsal Sharan, Sham Kakade, Percy Liang, and\nGregory Valiant. 2018. Prediction with a short\nmemory. In Proc. of STOC.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen tau Yih. 2023.\nREPLUG:\nRetrieval-augmented black-box language mod-\nels. ArXiv:2301.12652.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan\nUng, Moya Chen, Kushal Arora, Joshua Lane,\nMorteza Behrooz, William Ngan, Spencer Poff,\nNaman Goyal, Arthur Szlam, Y-Lan Boureau,\nMelanie Kambadur, and Jason Weston. 2022.\nBlenderBot 3: a deployed conversational agent\nthat continually learns to responsibly engage.\nArXiv:2208.03188.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do long-range\nlanguage models actually use long-range con-\ntext? In Proc. of EMNLP.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier\nGarcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung,\nSiamak Shakeri,\nDara Bahri,\nTal\nSchuster, Huaixiu Steven Zheng, Denny Zhou,\nNeil Houlsby,\nand Donald Metzler. 2023.\nUL2: Unifying language learning paradigms.\nArXiv:2205.05131.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie\nBaker, Yu Du, YaGuang Li, Hongrae Lee,\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo\nMenegali, Yanping Huang, Maxim Krikun,\nDmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou,\nChung-Ching Chang, Igor Krivokon, Will Rusch,\nMarc Pickett, Pranesh Srinivasan, Laichee Man,\nKathleen Meier-Hellstern, Meredith Ringel Mor-\nris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vin-\nodkumar Prabhakaran, Mark Diaz, Ben Hutchin-\nson, Kristen Olson, Alejandra Molina, Erin\nHoffman-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm,\nViktoriya Kuzmina, Joe Fenton, Aaron Cohen,\nRachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and\nQuoc Le. 2022. LaMDA: Language models for\ndialog applications. ArXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izac-\nard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. 2023a.\nLLaMA:\nOpen and efficient foundation language models.\nArXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Pe-\nter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernan-\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-\nthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou,\nHakan Inan, Marcin Kardas, Viktor Kerkez,\nMadian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor\nMihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi\nRungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian,\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang,\nAurelien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. 2023b. Llama\n2: Open foundation and fine-tuned chat models.\nArXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Proc. of NeurIPS.\nSinong Wang, Belinda Z. Li, Madian Khabsa,\nHan\nFang,\nand\nHao\nMa.\n2020.\nLin-\nformer: Self-attention with linear complexity.\nArXiv:2006.04768.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan\nWang, Li Yang, and Amr Ahmed. 2020. Big\nBird: Transformers for longer sequences. In\nProc. of NeurIPS.\nA\nAmbiguity in Multi-Document QA\nDistractor Documents\nFollowing past work on NaturalQuestions-Open\n(Izacard et al., 2021; Izacard and Grave, 2021, inter\nalia), we use a Wikipedia dump from late 2018\nas our retrieval corpus. However, this standard\nWikipedia dump has a small amount of temporal\nmismatch with the NaturalQuestions annotations.\nFor example, consider the question \u201cwhat nfl\nteam does robert griffin iii play for\u201d. The Natu-\nralQuestions annotated answer is \u201ccurrently a free\nagent\u201d. However, the Wikipedia retrieval corpus\ncontains the information that he plays for the \u201cBalti-\nmore Ravens\u201d, since he was released from the team\nbetween the Wikipedia dump\u2019s timestamp and the\nNaturalQuestions annotation process.\nWe use the ambiguity annotations of Min et al.\n(2020) to create a subset unambiguous questions.\nExperiments on this unambiguous subset of the\ndata show similar results and conclusions as the\nexperiments on the full questions collection (Fig-\nure 12).\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n60\n65\n70\n75\nAccuracy\n20 Total Retrieved Documents \n(~4K tokens, unambiguous questions)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\nFigure 12: Language model performance on a unam-\nbiguous subset of questions.\nB\nRandom Distractors in\nMulti-Document QA\nWe also run multi-document question answering\nexperiments with random Wikipedia documents as\ndistractors, which allows us to ablate the impact\nof retrieved distractors (hard negatives). Note that\nin this setting, the the document containing the an-\nswer can often be identified with simple heuristics\n(e.g., lexical overlap with the query). Figure 13\npresents the results of this experiment. Although\nall models have higher absolute accuracy in this\nsetting, they surprisingly still struggle to reason\nover their entire input context, indicating that their\nperformance degradation is not solely due to an\ninability to identify relevant documents.\nC\nRandomizing Distractor Order in\nMulti-Document QA\nOur prompt instructs the language model to use\nthe provided search results to answer the question.\nThere may be a prior in the pre-training or instruc-\ntion fine-tuning data to treat search results as sorted\nby decreasing relevance (i.e., the documents near\nthe beginning of the input context are more likely to\nbe useful than those at the end). To validate that our\nconclusions are not simply a byproduct of this bias,\nwe run experiments with the modified instruction\n\u201cWrite a high-quality answer for the given ques-\ntion using only the provided search results (some\nof which might be irrelevant). The search results\nare ordered randomly.\u201d In addition, we randomly\nshuffle the k \u2212 1 distractor documents.\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n65\n70\n75\n80\nAccuracy\n20 Total Retrieved Documents\n(~4K tokens, random distractors)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\nFigure 13: Language model performance on multi-\ndocument QA when using random distractors, rather\nthan retrieved distractors.\nFigure 14 presents the results of this experiment.\nWe continue to see a U-shaped performance curve,\nwith performance degrading when language mod-\nels must use information in the middle of their\ninput contexts. Comparing the results in \u00a72.3 with\nthose when randomizing the distractor order and\nmentioning such in the prompt, we see that ran-\ndomization slightly decreases performance when\nthe relevant information is at the very beginning\nof the context, and slightly increases performance\nwhen using information in the middle and end of\nthe context.\nD\nGPT-4 Performance\nWe evaluate GPT-4 (8K) on a subset of 500 ran-\ndom multi-document QA examples with 20 total\ndocuments in each input context (Figure 15). GPT-\n4 achieves higher absolute performance than any\nother language model, but still shows a U-shaped\nperformance curve\u2014its performance is highest\nwhen relevant information occurs at the very start\nor end of the context, and performance degrades\nwhen it must use information in the middle of its\ninput context.\nE\nLlama-2 Performance\nWe evaluate Llama-2 (Touvron et al., 2023b) on\nmulti-document QA with 20 total documents in\neach input context.\nThe Llama tokenizer pro-\nduces longer sequences than the tokenizers for our\npreviously-studied models, so we discard 20 exam-\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n55\n60\n65\n70\n75\nAccuracy\n20 Total Retrieved Documents\n(~4K tokens, randomly ordered)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\nFigure 14: Language model performance when random-\nizing the order of the distractors (rather than presenting\nthem in order of decreasing relevance) and mentioning\nas such in the prompt.\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n50\n60\n70\n80\n90\nAccuracy\n20 Total Retrieved Documents \n(~4K tokens, 500 question sample)\nclaude-1.3\nclaude-1.3-100k\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-16k-0613\nmpt-30b-instruct\nlongchat-13b-16k\ngpt-4-0613\nFigure 15: Although GPT-4 has higher absolute perfor-\nmance than other models, its performance still degrades\nwhen relevant information occurs in the middle of the\ninput context.\nples (out of 2655) that exceed Llama-2\u2019s maximum\ncontext length of 4096 tokens. We experiment with\nmodels of varying sizes (7B, 13B, and 70B pa-\nrameters), with and without additional supervised\nfine-tuning and reinforcement learning from hu-\nman feedback (\u201c-chat-\u201d models). The results are\npresented in Figure 16.\nComparing Llama-2 models of varying sizes, we\nfind that only the larger models (13B and 70B)\nexhibit the U-shaped performance curve (i.e., both\nprimacy and recency bias)\u2014the smallest Llama-\n2 models (7B) are solely recency-biased. Given\nthese results, we hypothesize that prior work (e.g.,\nKhandelwal et al., 2018; Sun et al., 2021) did not\npreviously observe any primacy bias in language\nmodels because the models they studied were too\nsmall (less than 1B parameters).\nComparing between Llama-2 models with and\nwithout additional supervised fine-tuning and re-\ninforcement learning from human feedback, we\nsee that additional fine-tuning dramatically im-\nproves performance on the multi-document QA\ntask. The 7B models with and without additional\nfine-tuning show minimal primacy bias, and are\nlargely recency-biased. The 13B base model has\na dramatic primacy and recency bias\u2014there is a\n20-point accuracy disparity between the best- and\nworst-case performance. Applying additional fine-\ntuning to the 13B seems to slightly reduce this\nbias (10-point worst-case degradation), but the bias\nremains significant. However, the 70B models\nwith and without additional fine-tuning have largely\nsimilar trends (showing both primacy and recency\nbias), and additional fine-tuning minimally changes\nthe positional bias severity.\n1st\n5th\n10th\n15th\n20th\nPosition of Document with the Answer\n20\n30\n40\n50\n60\n70\nAccuracy\n20 Total Retrieved Documents (~4K tokens)\nLlama-2-7b-chat-hf\nLlama-2-13b-chat-hf\nLlama-2-70b-chat-hf\nLlama-2-7b-hf\nLlama-2-13b-hf\nLlama-2-70b-hf\nFigure 16: Multi-document QA performance (20 total\ndocuments) of Llama-2 models of varying sizes (7B,\n13B, 70B parameters), with and without additional su-\npervised fine-tuning and reinforcement learning from\nhuman feedback (\u201c-chat-\u201d models).\nF\nToken Counts\nTable 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input\ncontexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,\nGPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K)\nuse the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer,\nmodulo some additional special tokens that do not appear in our data. As a result, the token counts for\nthese two model families is the same in our experimental settings.\nClosed-Book\nOracle\navg \u00b1 stdev\nmax\navg \u00b1 stdev\nmax\nLongChat-13B (16K)\n55.6 \u00b1 2.7\n70\n219.7 \u00b1 48.5\n588\nMPT-30B\n43.5 \u00b1 2.2\n58\n187.9 \u00b1 41.8\n482\nGPT-3.5-Turbo\n15.3 \u00b1 2.2\n29\n156.0 \u00b1 41.8\n449\nClaude-1.3\n15.3 \u00b1 2.2\n29\n156.0 \u00b1 41.8\n449\nTable 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document\nquestion answering settings.\n10 docs\n20 docs\n30 docs\navg \u00b1 stdev\nmax\navg \u00b1 stdev\nmax\navg \u00b1 stdev\nmax\nLongChat-13B (16K)\n1749.9 \u00b1 112.4\n2511\n3464.6 \u00b1 202.3\n4955\n5181.9 \u00b1 294.7\n7729\nMPT-30B\n1499.7 \u00b1 88.5\n1907\n2962.4 \u00b1 158.4\n3730\n4426.9 \u00b1 230.5\n5475\nGPT-3.5-Turbo\n1475.6 \u00b1 86.5\n1960\n2946.2 \u00b1 155.1\n3920\n4419.2 \u00b1 226.5\n6101\nClaude-1.3\n1475.6 \u00b1 86.5\n1960\n2946.2 \u00b1 155.1\n3920\n4419.2 \u00b1 226.5\n6101\nTable 3: Token count statistics for each of the evaluated models on each of the document question answering\nsettings.\n75 KV pairs\n140 KV pairs\n300 KV pairs\navg \u00b1 stdev\nmax\navg \u00b1 stdev\nmax\navg \u00b1 stdev\nmax\nLongChat-13B (16K)\n5444.5 \u00b1 19.1\n5500\n10072.4 \u00b1 24.1\n10139\n21467.3 \u00b1 35.9\n21582\nMPT-30B\n4110.5 \u00b1 23.8\n4187\n7600.9 \u00b1 31.1\n7687\n16192.4 \u00b1 46.6\n16319\nGPT-3.5-Turbo\n3768.7 \u00b1 25.6\n3844\n6992.8 \u00b1 34.1\n7088\n14929.4 \u00b1 50.7\n15048\nClaude-1.3\n3768.7 \u00b1 25.6\n3844\n6992.8 \u00b1 34.1\n7088\n14929.4 \u00b1 50.7\n15048\nTable 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.\nG\nFull Multi-Document Question Answering Results\nThis section tabulates model performance when evaluated on the multi-document QA task with varying\nnumbers of documents (Figure 5). \u201cIndex n\u201d indicates performance when the document with the answer\noccurs at position n + 1, where lower indices are closer to the start of the input context. For example,\nindex 0 refers to performance when the document with the answer is placed at the very start of the context\n(i.e., first amongst all documents).\nG.1\n10 Total Retrieved Documents\nModel\nIndex 0\nIndex 4\nIndex 9\nClaude-1.3\n62.9%\n58.3%\n59.7%\nClaude-1.3 (100K)\n63.1%\n58.3%\n59.7%\nGPT-3.5-Turbo\n76.8%\n61.2%\n62.4%\nGPT-3.5-Turbo (16K)\n76.9%\n61.0%\n62.5%\nMPT-30B-Instruct\n60.2%\n56.2%\n59.7%\nLongChat-13B (16K)\n72.1%\n58.9%\n58.5%\nTable 5: Model performance when evaluated on the multi-document QA task with 10 total retrieved documents.\nG.2\n20 Total Retrieved Documents\nModel\nIndex 0\nIndex 4\nIndex 9\nIndex 14\nIndex 19\nClaude-1.3\n59.9%\n55.9%\n56.8%\n57.2%\n60.1%\nClaude-1.3 (100K)\n59.8%\n55.9%\n57.0%\n57.4%\n60.0%\nGPT-3.5-Turbo\n75.8%\n57.2%\n53.8%\n55.4%\n63.2%\nGPT-3.5-Turbo (16K)\n75.7%\n57.3%\n54.1%\n55.4%\n63.1%\nMPT-30B-Instruct\n53.7%\n51.8%\n52.2%\n52.7%\n56.3%\nLongChat-13B (16K)\n68.6%\n57.4%\n55.3%\n52.5%\n55.0%\nTable 6: Model performance when evaluated on the multi-document QA task with 20 total retrieved documents.\nG.3\n30 Total Retrieved Documents\nModel\nIndex 0\nIndex 4\nIndex 9\nIndex 14\nIndex 19\nIndex 24\nIndex 29\nClaude-1.3\n59.1%\n55.1%\n54.8%\n55.7%\n56.4%\n56.2%\n59.9%\nClaude-1.3 (100K)\n59.1%\n55.1%\n54.9%\n55.7%\n56.6%\n56.1%\n60.0%\nGPT-3.5-Turbo (16K)\n73.4%\n55.1%\n50.5%\n50.9%\n51.8%\n54.9%\n63.7%\nMPT-30B-Instruct\n51.6%\n51.3%\n51.2%\n49.0%\n49.6%\n51.3%\n54.1%\nLongChat-13B (16K)\n66.9%\n54.8%\n52.5%\n52.9%\n52.2%\n51.3%\n55.1%\nTable 7: Model performance when evaluated on the multi-document QA task with 30 total retrieved documents.\n"
  },
  {
    "title": "mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding",
    "link": "https://arxiv.org/pdf/2307.02499.pdf",
    "upvote": "14",
    "text": "mPLUG-DocOwl\n: Modularized Multimodal Large\nLanguage Model for Document Understanding\nJiabo Ye\u2217, Anwen Hu\u2217, Haiyang Xu\u2020, Qinghao Ye, Ming Yan\u2020, Yuhao Dan,\nChenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang\nDAMO Academy, Alibaba Group\n{yejiabo.yjb, huanwen.haw, shuofeng.xhy, yeqinghao.yqh, ym119608}@alibaba-inc.com\nAbstract\nDocument understanding refers to automatically extract, analyze and comprehend\ninformation from various types of digital documents, such as a web page. Existing\nMulti-model Large Language Models (MLLMs), including mPLUG-Owl, have\ndemonstrated promising zero-shot capabilities in shallow OCR-free text recogni-\ntion, indicating their potential for OCR-free document understanding. Neverthe-\nless, without in-domain training, these models tend to ignore fine-grained OCR\nfeatures, such as sophisticated tables or large blocks of text, which are essential for\nOCR-free document understanding. In this paper, we propose mPLUG-DocOwl\nbased on mPLUG-Owl for OCR-free document understanding. Specifically, we\nfirst construct a instruction tuning dataset featuring a wide range of visual-text\nunderstanding tasks. Then, we strengthen the OCR-free document understanding\nability by jointly train the model on language-only, general vision-and-language,\nand document instruction tuning dataset with our unified instruction tuning strat-\negy. We also build an OCR-free document instruction understanding evaluation\nset LLMDoc to better compare models\u2019 capabilities on instruct compliance and\ndocument understanding. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating its strong ability of document un-\nderstanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes\nwell on various downstream tasks. Our code, models, training data and evaluation\nset are available at https://github.com/X-PLUG/mPLUG-DocOwl.\n1\nIntroduction\nLarge language models (LLMs) like ChatGPT [OpenAI, 2022], BLOOM [Scao et al., 2022], and\nLLaMA [Touvron et al., 2023] have undergone rapid development to enable the realization of gen-\neral artificial intelligence, boasting impressive zero-shot capabilities across diverse linguistic appli-\ncations. With the LLM as the language decoder, Multimodal large language models (MLLMs) such\nas MiniGPT-4 [Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023]\nhave demonstrated remarkable zero-shot performance in various open-ended vision-and-language\ntasks. These models are trained to align text and images during the pre-training phase, and then to\npromote diverse abilities during the instruction tuning phase. Interestingly, these MLLMs exhibit\nsuperficial OCR-free text recognition abilities without explicit training on visual text understanding\ndatasets [Ye et al., 2023, Liu et al., 2023b]. Nevertheless, due to lacking specific training, these mod-\nels still face the challenge of comprehending intricate relationships between visual text and objects\nin diverse types of images, such as charts, documents and webpages.\n\u2217Equal contribution\n\u2020Corresponding author\nPreprint. Under review.\narXiv:2307.02499v1  [cs.CL]  4 Jul 2023\nBy performing unified instruction tuning for Document Understanding upon the mPLUG-Owl [Ye\net al., 2023], we further propose a modularized MLLM [Li et al., 2022, Xu et al., 2023b], namely\nmPLUG-DocOwl. Our approach utilizes a modularized framework similar to mPLUG-Owl [Ye\net al., 2023], which incorporates a visual abstractor module to link a pre-trained LLM with a vi-\nsual knowledge module, achieving the alignment of text and images. To enhance diverse docu-\nment understanding capabilities, we reorganize various downstream document understanding tasks\nin the same form of instructions. To maintain general uni/multi-modal abilities, we also include\nlanguage-only and general vision-and-language instruction datasets used by mPLUG-Owl to train\nthe mPLUG-DocOwl. During training, both the visual knowledge module and LLM decoder are\nfrozen, only the visual abstractor and the Low-Rank Adaption (LoRA) [Hu et al., 2022] in LLM are\nfine-tuned.\nmPLUG-DocOwl achieves ocr-free state-of-the-art performance on multiple commonly used docu-\nment understanding datasets. Furthermore, our experiments on a carefully-built document instruc-\ntion understanding evaluation set LLMDoc shows that mPLUG-DocOwl achieves significantly bet-\nter visual text understanding performance on various domains than existing MLMMs.\nOur main contributions can be highlighted as follows:\n\u2022 We propose a modularized MLLM, mPLUG-DocOwl, which is the first one to balance\nlanguage-only, general vision-and-language, and document understanding based on unified\ninstruction tuning.\n\u2022 We carefully construct an instruction understanding test set with human evaluation, dubbed\nLLMDoc, to assess diverse document understanding capabilities.\n\u2022 Empirical results demonstrate that our mPLUG-DocOwl surpasses existing methods on\nocr-free document understanding, including multiple standard benchmarks and LLMDoc.\n2\nRelated Work\n2.1\nVisual Text Understanding\nThere are two types of models for understanding images that contain rich textual information. The\nfirst kind of approaches [Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023,\nYang et al., 2021] utilize off-the-shelf OCR models or APIs to recognize text from images, and then\ndesign pretraining tasks to facilitate cross-modality alignment between visual and textual inputs.\nOn the other hand, end-to-end approaches [Davis et al., 2022, Kim et al., 2022, Lee et al., 2022]\nutilize a high-resolution image encoder to learn text recognition during the pretraining stage. Both\ntwo types of models rely on specific finetuning on different downstream datasets and can\u2019t achieve\nopen-domain instruction understanding performance like Multimodal Large Language Models.\n2.2\nMultimodal Large Language Model\nLarge Language Models (LLMs) have demonstrated impressive zero-shot abilities across various\nopen-ended tasks. Recent research has also explored the application of LLMs for multi-modal gen-\neration, utilizing two different paradigms: systematic collaboration and end-to-end trained models.\nSystematic collaboration approaches, such as Visual ChatGPT [Wu et al., 2023] and MM-REACT\n[Yang et al., 2023], leverage various vision experts or tools to express visual information with text\ndescriptions. Subsequently, LLMs, such as ChatGPT [OpenAI, 2022], can act as agents and select\nappropriate experts and tools for visual understanding. Finally, LLMs would summarize the output\nof these experts to answer user queries. On the other hand, some approaches, such as MiniGPT-4\n[Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023], leverage LLMs\nto build unified models for multi-modality with limited connected parameters. These methods show\nsuperficial OCR-free text recognition abilities under the zero-shot setting. However, for complicated\ndocument understanding, due to lacking in-domain training, they encounter challenges in handling\ndiverse image types, recognizing rich texts and comprehending relationships between visual seman-\ntic and text information. In this work, through unified instruction tuning, mPLUG-DocOwl achieves\nmuch better document understanding performance and maintains general uni/multi-modal abilities.\n2\nmPLUG-DocOwl\nmPLUG-Owl\nTable\nDocument\nWebPage\nHuman: juan rom\u00e1n riquelme have \nthe same score on july 11 , 2007 \nnovember 17 2007, Yes or No? \nAI: Yes\nHuman: what is the value \nmentioned in the CID#:\nAI: 736 00 00\nHuman: What will the presentation\naim to encourage a discussion about\nAI: It will aim to encourage a\ndiscussion\nabout\nopen\ndata\nownership rights and the differing\nsocial\nNatural Image\nHuman : Create a compact narrative representing the\nimage presented.\nAI: Sign that says Calea Dorobantilor in blue and white\nChart\nLanguage-only\nHuman: What was the busiest control\npoint in Hong Kong in 2019?\nAI: Lo Wu Control Point\nHuman: Could you offer me an elaborated \nresponse that demands complex reasoning \nand requires breaking down the problem \ninto multiple steps\nAI: Yes, I can certainly fulfill your request. \nPlease provide me with your specific \nquestion or problem that you would like \nme to elaborate on.\nChart  Document Table \nNatural Image WebPage\nGeneral V&L Language-only\nMultimodal\nPretraining\nUnified Instruction Tuning\n\ud83e\udd89\nFigure 1: The summary of the instruction tuning paradigm of our mPLUG-DocOwl.\n3\nmPLUG-DocOwl\n3.1\nArchitecture\nThe architecture of mPLUG-DocOwl is based on a popular multi-modal language model, mPLUG-\nOwl [Ye et al., 2023], which comprises a pre-trained visual foundation model, a visual abstractor,\nand a language foundation model. The visual foundation model is responsible for extracting visual\nfeatures from the input images, and the visual abstractor distills these features using a set of learnable\ntokens. The resulting visual features are then concatenated with the word embeddings of the input\nsentence and fed into the language model to generate the response. This powerful architecture allows\nfor accurate and efficient multi-modal language processing.\nThe mPLUG-Owl [Ye et al., 2023] exhibits superficial OCR ability when presented with images\ncontaining salient text. Inspired by this, we propose to further fine-tune the model with document\ninstruction tuning data for better document understanding performance, covering document, table,\nchart and natural image and webpage. During fine-tuning, we freeze the visual encoder and the\nlanguage model and train the visual abstractor. We also adopt the low-rank adaptation approach\n(LoRA) [Hu et al., 2022] to enhance the language model\u2019s ability.\n3.2\nInstruction Tuning Data\nThis section introduces the composition of our instruction tuning data in detail. To ensure the\nversatility of mPLUG-DocOwl, we collect diverse document understanding datasets with differ-\nent task formats, including Visual Question Answering (VQA) [Antol et al., 2015], Information\nExtraction (IE), Natural Language Inference (NLI) [Bowman et al., 2015], and Image Captioning\n(IC). mPLUG-Owl [Ye et al., 2023] performs instruction tuning with a unified format as \"<im-\nage>Human:{question} AI:{answer}\". In this work, we convert different document understanding\ntasks to the same format as mPLUG-Owl [Ye et al., 2023] by replacing the {question} and {answer}\nplaceholders as follows.\nVisual Question Answering We simply use the raw question and answer as the {question} and\n{answer} placeholders. We collect VQA datasets on diverse domains, including ChartQA [Masry\net al., 2022], DocVQA [Mathew et al., 2021], InfographicsVQA (InfoVQA) [Mathew et al., 2022],\n3\nDocument\nDocVQA\nInfoVQA\nDeepForm\nKLC\nTable\nWTQ\nTabFact\nChart\nChartQA\nWebPage\nVisualMRC\nNatural Image\nTextVQA\nTextCaps\nGeneral V&L\nLLaVA\nLanguage-only\nAlpaca\nVicuna\nBaize\nFigure 2: Different types of datasets used to train mPLUG-DocOwl.\nWikiTableQuestions (WTQ) [Pasupat and Liang, 2015], TextVQA [Singh et al., 2019] and Vi-\nsualMRC [Tanaka et al., 2021].\nInformation Extraction requires the model to extract key-value pairs from the input image. The\n\u2018keys\u2019 (or \u2018categories\u2019) are always a stationary set. To convert this task to the instruction tuning\nformat, we treat the value as the {answer} and construct the {question} as \u2018What is the value for the\n{key}?\u2019. When the key does not exist in the image, the {answer} is set to \u2018None\u2019. We collect Infor-\nmation Extraction data from DeepForm [Svetlichnaya, 2020], and Kleister Charity (KLC) [Stanis-\nlawek et al., 2021].\nNatural Language Inference is a binary classification task with labels \u2018Entailed\u2019 and \u2018Refuted\u2019.\nGiven a statement, we construct the {question} as \u2018{statement}, Yes or No?\u2019. The {answer} is \u2018Yes\u2019\nor \u2018No\u2019 and refers to \u2018Entailed\u2019 or \u2018Refuted\u2019, respectively. TabFact [Chen et al., 2020], a natural\nlanguage inference dataset about tables, is chosen for instruction tuning.\nImage Captioning aims to briefly describe an image with fluent language. We treat the caption\nas the {answer} and randomly choose a prompt as the {question} like LLaVa [Liu et al., 2023a].\nTextCaps [Sidorov et al., 2020] is an appropriate captioning dataset on natural images with texts.\nLanguage-only and General Vision-and-language Instruction Tuning.\nTo enhance the model\u2019s\nability of language comprehension and multi-modal open-ended conversation, we follow mPLUG-\nOwl [Ye et al., 2023] to introduce language-only and general vision-and-language instruction tuning\ndata [Taori et al., 2023, Vicuna, 2023, Xu et al., 2023a, Liu et al., 2023a].\nFigure 2 shows the composition of our instruction tuning data grouped by the dataset type. We use\ntraining sets of these datasets as instruction tuning data and evaluate models on test sets.\n3.3\nTraining Details\nWe adopt a two-stage training paradigm, where the Vision Transformer and Language model are kept\nfrozen. In the first stage, both the visual abstractor and LoRA [Hu et al., 2022] in the language model\nare fine-tuned. The first stage only uses the document understanding data and takes 10 epochs. In\nthe second stage, we further freeze the visual abstractor and only train the LoRA. Besides document\nunderstanding data, the language-only and general vision-and-language instruction tuning data are\nfurther introduced at this stage and up-sampled 6 times. The second stage takes 3 epochs. Other\ntraining hyper-parameters are the same as mPLUG-Owl [Ye et al., 2023].\n4\nExperiment\n4.1\nLLMDoc\nExisting benchmarks are hard to evaluate the open-ended instruction understanding results given by\nMLMMs. For better compare the instruction understanding performance in the document domain,\nwe further construct a test set with human evaluation, namely LLMDoc.\n4\nTable 1: Comparison with ocr-free methods on DUE-Benchmark.\nModel\nDocVQA\nInfoVQA\nDeepForm\nKLC\nWTQ\nTabFact\nDessurt\n63.2\n-\n-\n-\n-\n-\nDonut\n67.5\n11.6\n61.6\n30.0\n18.8\n54.6\nPix2Structbase\n72.1\n38.2\n-\n-\n-\n-\nmPLUG-DocOwl\n62.2\n38.2\n42.6\n30.3\n26.9\n60.2\nTable 2: Comparison with ocr-free methods on chart, natural image and webpage understanding.\nModel\nChartQA\nTextVQA\nTextCaps\nVisualMRC\nDonut\n41.8\n43.5\n74.4\n93.91\nPix2Structbase\n56.0\n-\n88.0\n-\nmPLUG-DocOwl\n57.4\n52.6\n111.9\n188.8\nData Collection\nTo comprehensively evaluate the model\u2019s abilities, we consider five scenarios\nto construct our evaluation dataset, including table (TabFact [Chen et al., 2020]), chart (ChartQA\n[Masry et al., 2022]), document (DocVQA [Mathew et al., 2021]), natural image (TextVQA [Singh\net al., 2019]) and webpage (VisualMRC [Tanaka et al., 2021]). Specifically, for each dataset, we\nsample 20 images from the test split. For 10 of these images, we adopt a raw question as the instruc-\ntion. While for the other 10, we ask annotators to write instructions requiring stronger capabilities\nlike summarization, inference, and calculation. In total, we obtain 100 test samples.\nHuman Evaluation\nFollowing the rating criteria proposed in Self-Instruct [Wang et al., 2022], we\nperform the human evaluation to score the model\u2019s responses, where A > B > C > D and A represents\n\u2018correct and satisfying response\u2019, B means \u2018acceptable response with minor imperfections\u2019, C refers\nto \u2018response to the instruction but has significant errors\u2019 and D means \u2018irrelevant or invalid response\u2019.\nFigure 3: Human evaluation of mPLUG-DocOwl,\nmPLUG-Owl and MiniGPT-4 on LLMDoc.\nWe compare mPLUG-DocOwl with other pop-\nular mult-modal large language models, includ-\ning mPLUG-Owl [Ye et al., 2023] and Mini-\nGPT4 [Zhu et al., 2023], on LLMDoc.\nAs\nshown in Figure 3, mPLUG-DocOwl achieves\nsignificantly better performance, with 37 re-\nsponses being scored as \u201cA\u201d, demonstrating\nthe stronger understanding ability of mPLUG-\nDocOwl in diverse document scenarios.\nBe-\nsides, it\u2019s worth noting that all models have\nsome responses scored as \u201cC\u201d or \u201cD\u201d, show-\ning that instruction understanding performance\nin the document domain is still far from promis-\ning and needs more endeavor.\n4.2\nBenchmark Evaluation\nBesides human evaluation, we also compare\nour mPLUG-DocOwl with ocr-free state-of-\nthe-art document understanding models on\npublic datasets. Table 1 shows the comparison\nwith Dessurt [Davis et al., 2022], Donut [Kim\net al., 2022] and Pix2Struct [Lee et al., 2022]\non DUE-Benchmark [Borchmann et al., 2021],\nwhich mainly requires the text recognition and layout understanding abilities on documents and ta-\nbles. Besides, Table 2 presents the evaluation on the chart, natural image and webpage datasets,\nwhich ask stronger ability to relate visual semantics and text information. Without finetuning on\neach dataset, our mPLUG-DocOwl achieves comparable or even better performance.\n5\nHuman: Indicate a few pertinent items in this graphic. \nmPLUG-DocOwl: In 2019, 13% of the population in the \nUnited States reported that they never used the internet. \nHuman: Name the companies involved in \u2018CRP agreement\u2019? \nmPLUG-DocOwl: Cone-Mark & R.J. Reynolds Tobacco Co. \nHuman: How many commercial publishers have shown \ninterest in using the license ? \nmPLUG-DocOwl: At least two commercial publishers \nhave shown interest in using the license.\nHuman: on which date did they score the most points? \nmPLUG-DocOwl: October 11, 1981\nHuman: what is the second room on the list? \nmPLUG-DocOwl: right room\nHuman: What % of parents are not present at the time of \ngame purchase or rental?\nmPLUG-DocOwl: 9%\n1 \u2212 91% = 9%\n(a)\n(c)\n(b)\n(d)\n(e)\n(f)\nFigure 4: Qualitative results of mPLUG-DocOwl. The crucial regions and corresponding words are\nannotated with the same colors for clearer visualization. Wrong answers are colored red.\n4.3\nQualitative Analysis\nBenchmark Results.\nQualitative results on different types of images are shown in Figure 4. Cru-\ncial regions and corresponding responses are annotated with the same colors. Case (a) shows that\nmPLUG-DocOwl can accurately find the answer from a webpage screenshot with complex contents.\nCase (b) shows that mPLUG-DocOwl is even able to understand hand-drawn tables and correctly\nrecognize handwritten fonts. In case (c), mPLUG-DocOwl can summarize key points from a chart. It\nsuccessfully understands that the table is about internet usage and infers that \u201cNever\u201d means \u201cNever\nused internet\u201d. However, it also generates illusory outputs, such as \"in the United States\". The ques-\ntion in case (d) requires the model to understand the \u201cResult\u201d column, compare the points and return\nthe date with the best results. Case (e) demonstrates that our model is capable of processing scanned\n6\nHuman: Convert each row of the table into json format.\nmPLUG-DocOwl:\n{\"Game\":1,\"Date\":1,\"Location\":\"Redland \nField\",\"Time\":1:42,\"Attendance\":30,511,\"Ga\nme\":2,\"Date\":2,\"Location\":\"Redland \nField\",\"Time\":1:42,\"Attendance\":29,698,\"Ga\nme\":3,\"Date\":3,\"Location\":\"Comiskey Park \n(I)\",\"Time\":1:30,\"Attendance\":34,363,\"Game\n\":4,\"Date\":4,\"Location\":\"Comiskey Park \n(I)\",\"Time\":1:37,\"Attendance\":34,379,\"Game\n\":5,\"Date\":6,\"Location\":\"Redland \nField\",\"Time\":1:45,\"Attendance\":32,006,\"Ga\nme\":6,\"Date\":7,\"Location\":\"Redland \nField\",\"Time\":2:06,\"Attendance\":13,923,\"Ga\nme\":7,\"Date\":8,\"Location\":\"Redland \nField\",\"Time\":1:47,\"Attendance\":32,930}\nMini-GPT4:\n| Date | Time |\nHuman: What's the name of the shop? What does the shop sell? \nmPLUG-DocOwl: The shop is \ncalled the Scrub Shop, and it \nsells medical uniforms.\nMini-GPT4: The name of the \nshop is Scrub Shop. The shop \nsells various types of clothing \nsuch as dresses, tops, pants, and \naccessories.\nScore: B\nScore: D\nScore: A\nScore: B\n(a)\n(b)\nFigure 5: Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part one.\ndocuments and distinguishing company and person names. Case (f) shows that mPLUG-DocOwl\ncan not only recognize small and blurry text but also perform simple calculations following the user\nintent.\nLLMDoc Results\nFigure 5 and Figure 6 present the comparison between mPLUG-DocOwl and\nMini-GPT4 on LLMDoc. Figure 5 (a) requires models to convert a table into JSON format. Our\nmPLUG-DocOwl correctly understands the instruction and return a string in JSON format, but\nmisses the last row. Mini-GPT4 fails to comprehend the instruction and doesn\u2019t understand the\ncontent within the table. In Figure 5 (b), both mPLUG-DocOwl and Mini-GPT4 correctly recog-\nnize the name of the shop. However, Mini-GPT4 overlooks a smaller sign indicating clothes in this\nshop are medical uniforms. As for chart understanding in Figure 6 (c), Mini-GPT4 gives a wrong\nanswer and redundant response, while our mPLUG-DocOwl gives a concise and correct response.\nIn Figure 6 (d), Bernadette\u2019s actual purpose is to confirm with Suzy if she would like to have the\ncopy sent overnight. This not only requires the model to accurately recognize the text, but also to\nunderstand the relationships between involved persons. mPLUG-DocOwl recognizes the phrase \"re-\nquest a copy of chapter,\" but misunderstands the subject and object. Mini-GPT4 only comprehends\nthat this image is a mail scenario and provides a vague and hallucinatory response. In Figure 6 (e),\nmPLUG-DocOwl gives a correct summary of the two latest news but Mini-GPT4 generates news\nirrelevant to the webpage screenshot.\n7\nHuman: What is Bernadette's intention in sending this message? \nmPLUG-DocOwl: \nTo request a copy \nof chapter from \nMerck\nMini-GPT4: Bernadette's intention in sending \nthe email is to request the recipient's assistance \nwith completing an important task. The \nmessage says, \"I need your help with this,\" \nindicating that Bernadette needs the recipient's \nassistance to complete a specific task.\nHuman: What are the two latest news? \nmPLUG-DocOwl: \nThe two latest \nnews are the \nEuropean Union \nin the Republic of \nMoldova and the \nEuropean Union \nresources for \neducation. \nMini-GPT4: The two latest news \nin the image are'Amazon to \nbecome a member of the US \nChamber of Commerce' and 'The \nUS Department of Agriculture \nhas proposed to restrict trade \nwith China due to alleged \nintellectual property violations.'\nScore: B\nScore: C\nScore: A\nScore: C\nHuman: Which age group of people needs to pay most attention \nto stroke?\nmPLUG-DocOwl:\n70+ years old\nMini-GPT4:The age group that needs the \nmost attention for stroke is 65 and older. \u2026 \nand the older population is at a higher risk of \nexperiencing a stroke than younger people.\nScore: A\nScore: C\n(c)\n(d)\n(e)\nFigure 6: Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part\ntwo.\n8\nHuman: What was the average ticket price between 2007 \nand 2011?\nmPLUG-DocOwl : 117.84\nHuman: How many different speakers appear in the \npicture?\nmPLUG-DocOwl: There are three different speakers \nin the picture.\n(a)\n(b)\nFigure 7: Failure cases on LLMDoc. Part one.\nThe LLMDoc contains many challenging instruction understanding cases in the document domain.\nFigure 7 and Figure 8 show some wrong responses given by mPLUG-DocOwl. In Figure 7 (a),\nmPLUG-DocOwl only takes note of the three names in the picture, but ignores the fact that the user\nitself is also a speaker. In Figure 7 (b), mPLUG-DocOwl fails to perform multi-step calculations on\nmultiple elements in the image. In Figure 8 (c), the model can understand the scene and the text in\nit, but fantasizes about non-existent characters. In Figure 8 (d), mPLUG-DocOwl fails to understand\nthe instruction for writing news and only read the texts in the tablet.\n5\nConclusion\nIn this work, we infuse diverse ocr-free document understanding capabilities into mPLUG-Owl by\nincorporating document understanding data into instruction finetuning. Experiment results demon-\nstrate that our mPLUG-DocOwl achieves comparable or even better performance than existing OCR-\nfree methods. Besides, benefiting from language-only and general vision-and-language instruction\ntuning, mPLUG-DocOwl can better comprehend user instructions and intentions, enabling more\ncomplex interactions. Moreover, human evaluation on LLMDoc reveals that mPLUG-DocOwl still\nstruggles with document-related commonsense reasoning, mathematical calculations, and creative\ngeneration. This provides valuable insights about developing stronger document understanding abil-\nities with the LLM in the future.\n9\nHuman: Write a news. \nmPLUG-DocOwl: Medal table Rank Nation Gold | Silver | \nBronze | Total Russia 13 9 8 30 Ukraine 6 4 8 21 Japan 3 2 \n3 8 France 3 1 2 6 Italy 3 2 0 5 Hungary 2 3 0 5 China 2 0 \n2 5 Germany 0 2 3 5 Poland 0 2 1 4 Canada 0 1 1 3 Czech \nRepublic 1 0 1 1 Estonia 0 1 0 1 United Kingdom 0 0 1 1\nHuman: What kind of scene do the words in the \npicture convey?\nmPLUG-DocOwl: The words in the picture convey a \ncomic book or cartoon-like scene, with a character \nsaying \"K-Rash\" and \"Sploosh!\" The words are \naccompanied by a drawing of a person, possibly a \ncharacter from a comic book or cartoon, and the \nwords are in a speech bubble. The scene appears to \nbe a humorous or lighthearted moment, as the \ncharacter is likely expressing an exaggerated or \nunexpected reaction to something.\n(c)\n(d)\nFigure 8: Failure cases on LLMDoc. Part two.\n10\nReferences\nS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual\nquestion answering. In Proceedings of the IEEE international conference on computer vision,\npages 2425\u20132433, 2015.\nL. Borchmann, M. Pietruszka, T. Stanislawek, D. Jurkiewicz, M. Turski, K. Szyndler, and F. Gralin-\nski. DUE: end-to-end document understanding benchmark. In NeurIPS Datasets and Bench-\nmarks, 2021.\nS. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning\nnatural language inference. arXiv preprint arXiv:1508.05326, 2015.\nW. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang. Tabfact : A\nlarge-scale dataset for table-based fact verification.\nIn International Conference on Learning\nRepresentations (ICLR), Addis Ababa, Ethiopia, April 2020.\nB. L. Davis, B. S. Morse, B. L. Price, C. Tensmeyer, C. Wigington, and V. I. Morariu. End-to-end\ndocument recognition and understanding with dessurt. In ECCV Workshops (4), volume 13804 of\nLecture Notes in Computer Science, pages 280\u2013296. Springer, 2022.\nA. Hu, S. Chen, and Q. Jin. Question-controlled text-aware image captioning. In ACM Multimedia,\npages 3097\u20133105. ACM, 2021.\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-\nrank adaptation of large language models. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL\nhttps://openreview.net/forum?id=nZeVKeeFYf9.\nY. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei. Layoutlmv3: Pre-training for document AI with unified\ntext and image masking. In ACM Multimedia, pages 4083\u20134091. ACM, 2022.\nG. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. Ocr-free\ndocument understanding transformer. In ECCV (28), volume 13688 of Lecture Notes in Computer\nScience, pages 498\u2013517. Springer, 2022.\nK. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M. Chang, and\nK. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding.\nCoRR, abs/2210.03347, 2022.\nC. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, J. Zhang, S. Huang,\nF. Huang, J. Zhou, and L. Si. mplug: Effective and efficient vision-language learning by cross-\nmodal skip-connections. In EMNLP, pages 7241\u20137259. Association for Computational Linguis-\ntics, 2022.\nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. CoRR, abs/2304.08485, 2023a.\nY. Liu, Z. Li, H. Li, W. Yu, M. Huang, D. Peng, M. Liu, M. Chen, C. Li, L. Jin, et al. On the hidden\nmystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b.\nA. Masry, D. X. Long, J. Q. Tan, S. R. Joty, and E. Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. In ACL (Findings), pages 2263\u20132279.\nAssociation for Computational Linguistics, 2022.\nM. Mathew, D. Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document images. In\nWACV, pages 2199\u20132208. IEEE, 2021.\nM. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar. Infographicvqa. In\nWACV, pages 2582\u20132591. IEEE, 2022.\nOpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.\nP. Pasupat and P. Liang. Compositional semantic parsing on semi-structured tables. In ACL (1),\npages 1470\u20131480. The Association for Computer Linguistics, 2015.\n11\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni, F. Yvon,\nM. Gall\u00e9, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot,\nN. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major,\nI. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Lauren\u00e7on, Y. Jernite,\nJ. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy,\nA. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I.\nAdelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR,\nabs/2211.05100, 2022.\nO. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: A dataset for image captioning with\nreading comprehension. In ECCV (2), volume 12347 of Lecture Notes in Computer Science,\npages 742\u2013758. Springer, 2020.\nA. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards\nVQA models that can read. In CVPR, pages 8317\u20138326. Computer Vision Foundation / IEEE,\n2019.\nT. Stanislawek, F. Gralinski, A. Wr\u00f3blewska, D. Lipinski, A. Kaliska, P. Rosalska, B. Topolski, and\nP. Biecek. Kleister: Key information extraction datasets involving long documents with complex\nlayouts. In ICDAR (1), volume 12821 of Lecture Notes in Computer Science, pages 564\u2013579.\nSpringer, 2021.\nS. Svetlichnaya. Deepform: Understand structured documents at scale, 2020.\nR. Tanaka, K. Nishida, and S. Yoshida. Visualmrc: Machine reading comprehension on document\nimages. In AAAI, pages 13878\u201313888. AAAI Press, 2021.\nZ. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, and M. Bansal. Unifying\nvision, text, and layout for universal document processing. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 19254\u201319264, 2023.\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\nstanford_alpaca, 2023.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and\nefficient foundation language models. CoRR, abs/2302.13971, 2023.\nVicuna. Vicuna: An open chatbot impressing gpt-4. https://github.com/lm-sys/FastChat,\n2023.\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\nAligning language model with self generated instructions. CoRR, abs/2212.10560, 2022. doi:\n10.48550/arXiv.2212.10560. URL https://doi.org/10.48550/arXiv.2212.10560.\nC. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing\nwith visual foundation models. CoRR, abs/2303.04671, 2023.\nC. Xu, D. Guo, N. Duan, and J. J. McAuley. Baize: An open-source chat model with parameter-\nefficient tuning on self-chat data. CoRR, abs/2304.01196, 2023a.\nH. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, G. Xu, J. Zhang,\nS. Huang, F. Huang, and J. Zhou. mplug-2: A modularized multi-modal foundation model across\ntext, image and video. CoRR, abs/2302.00402, 2023b.\nY. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou. Layoutlm: Pre-training of text and layout for\ndocument image understanding. In R. Gupta, Y. Liu, J. Tang, and B. A. Prakash, editors, KDD \u201920:\nThe 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,\nCA, USA, August 23-27, 2020, pages 1192\u20131200. ACM, 2020. doi: 10.1145/3394486.3403172.\nURL https://doi.org/10.1145/3394486.3403172.\n12\nZ. Yang, Y. Lu, J. Wang, X. Yin, D. Flor\u00eancio, L. Wang, C. Zhang, L. Zhang, and J. Luo. TAP: text-\naware pre-training for text-vqa and text-caption. In CVPR, pages 8751\u20138761. Computer Vision\nFoundation / IEEE, 2021.\nZ. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang.\nMM-REACT: prompting chatgpt for multimodal reasoning and action. CoRR, abs/2303.11381,\n2023.\nQ. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen,\nJ. Tian, Q. Qi, J. Zhang, and F. Huang. mplug-owl: Modularization empowers large language\nmodels with multimodality. CoRR, abs/2304.14178, 2023.\nD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language under-\nstanding with advanced large language models, 2023.\n13\n"
  },
  {
    "title": "Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts",
    "link": "https://arxiv.org/pdf/2307.02768.pdf",
    "upvote": "11",
    "text": "Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts\nMounica Maddela\u2217\nGeorgia Tech &\nMeta AI\nMegan Ung\u2217\nMeta AI\nJing Xu\nMeta AI\nAndrea Madotto\nMeta AI\nHeather Foran\nKlagenfurt University\nY-Lan Boureau\nMeta AI\nAbstract\nMany cognitive approaches to well-being,\nsuch as recognizing and reframing unhelpful\nthoughts, have received considerable empiri-\ncal support over the past decades, yet still lack\ntruly widespread adoption in self-help format.\nA barrier to that adoption is a lack of adequately\nspecific and diverse dedicated practice material.\nThis work examines whether current language\nmodels can be leveraged to both produce a vir-\ntually unlimited quantity of practice material\nillustrating standard unhelpful thought patterns\nmatching specific given contexts, and generate\nsuitable positive reframing proposals. We pro-\npose PATTERNREFRAME, a novel dataset of\nabout 10k examples of thoughts containing un-\nhelpful thought patterns conditioned on a given\npersona, accompanied by about 27k positive\nreframes. By using this dataset to train and/or\nevaluate current models, we show that exist-\ning models can already be powerful tools to\nhelp generate an abundance of tailored practice\nmaterial and hypotheses, with no or minimal\nadditional model training required.\n1\nIntroduction\nCognitive Behavioral Therapy (CBT) (Beck, 1963,\n1976) is one of the most robustly validated ap-\nproaches in psychology (Hofmann et al., 2012;\nDavid et al., 2018). A core pillar of CBT consists in\nidentifying and reframing unhelpful ways of think-\ning. Low-intensity CBT interventions have shown\npromise in self-help formats (Shafran et al., 2021;\nWilliams, 2001), yet a lack of sufficient practice\nmaterial suited to people\u2019s specific circumstances\nis a barrier to adoption (Helgad\u00f3ttir et al., 2009).\nThrough prompting, control tokens, or adequate\nconditioning, modern language models can guide\ngeneration of language towards desired outcomes,\nsuch as conforming to a given persona (Zhang et al.,\n2018), style (Ziems et al., 2022), or level of con-\nfidence (Mielke et al., 2022). This makes them a\n\u2217* Equal contribution.\npotentially powerful practice aid for learning cog-\nnitive reframing techniques. A major barrier is\nthe lack of publicly available data. Most exist-\ning work in natural language processing (NLP) for\nCBT focuses on interactions between patients and\nmental health professionals, which are not pub-\nlicly available (Mieskes and Stiegelmayr, 2018;\nRojas-Barahona et al., 2018; Shreevastava and\nFoltz, 2021). Ziems et al. (2022) released the first\npublic dataset for reframing tweets marked with a\nhashtag indicating stress, using known reframing\ntechniques, but it does not specifically look at the\ncategories of unhelpful thinking used in CBT, and\nuses existing tweets rather than allowing the gener-\nation of examples suited to a particular situation.\nIn this work, we propose1 a novel dataset, PAT-\nTERNREFRAME, consisting in \u223c10k crowdsourced\nexamples of thoughts containing ten classical types\nof unhelpful thought patterns (Burns, 1980), con-\nditioned on personas, matched with crowdsourced\nproposals of reframing that do not exhibit the pat-\nterns. We introduce two controllable text-to-text\ngeneration tasks on the dataset: (1) generating and\n(2) reframing unhelpful thoughts, given a persona\nand pattern as the context. We also define a classifi-\ncation task to identify the unhelpful thought pattern,\ngiven a persona and a thought. We train and evalu-\nate different fine-tuned and few-shot approaches for\nthe tasks, and show that these approaches perform\nreasonably well on the tasks.\n2\nRelated Work\n2.1\nNLP for Mental Health\nRecent work has used linguistic features and pre-\ntrained language models to identify mental health\nconditions such as anxiety (Owen et al., 2020;\n1The dataset and task have been released through the\nParlAI framework (Miller et al., 2017) and are available\nat\nhttps://github.com/facebookresearch/\nParlAI/tree/main/projects/reframe_\nthoughts\narXiv:2307.02768v1  [cs.CL]  6 Jul 2023\nShreevastava and Foltz, 2021; Fine et al., 2020),\ndepression (Wolohan et al., 2018; Po\u00b4swiata and\nPere\u0142kiewicz, 2022; Ji et al., 2022), schizophrenia\n(Jiang et al., 2020b; Mitchell et al., 2015; Sari-\noglu Kayi et al., 2017), and post-traumatic stress\ndisorder (Coppersmith et al., 2015). Most of these\nworks annotate social media posts to create datasets\nfor the task, and then train and evaluate differ-\nent classification models. Shreevastava and Foltz\n(2021) and Rojas-Barahona et al. (2018) created\ndatasets for identifying unhelpful thoughts by anno-\ntating patient-therapist interactions and finetuned\ndifferent pretrained models for the task. However,\nthese datasets are not publicly available.\nThe closest work to ours is that of Ziems et al.\n(2022), which introduces a reframing task, releases\na parallel corpus of reframed sentences, and uses\ncontrollable text generation models to reframe so-\ncial media content from Twitter that was marked\nas expressing stress. However, the source social\nmedia material is not conditioned on personas, or\nfocused on the classical unhelpful thought patterns\nfrom CBT. Our work introduces conditioning on\npersonas and classical unhelpful thought patterns,\nand extends the reframing task to identifying and\ngenerating thoughts matching a given persona and\nunhelpful pattern.\n2.2\nControllable Text Generation\nControllable text generation approaches using pre-\ntrained language models (PLMs) typically fall into\nfour categories: (i) prompt-based methods that\neither construct templates for PLMs to complete\n(Jiang et al., 2020a; Schick and Sch\u00fctze, 2021a,b)\nor finetune a task-specific layer to guide the gen-\neration (Li and Liang, 2021; Lester et al., 2021),\n(ii) finetuning methods that either use labelled data\nprepended with controlled attributes (Ziems et al.,\n2022; Fan et al., 2018; Martin et al., 2020; Ross\net al., 2022) or define a task-specific reward func-\ntion using reinforcement learning (Ziegler et al.,\n2019; Liu et al., 2020), (iii) post-processing meth-\nods that train discriminator models to guide the gen-\neration towards a specific criterion during decoding\n(Dathathri et al., 2019; Hua and Wang, 2020; Xu\net al., 2020), and (iv) pretraining methods that pre-\ntrain PLMs from the start with different control\ntokens prepended to the input (Keskar et al., 2019).\nIn our work, we experiment with prompt-based and\nfinetuning methods.\n3\nIdentifying and Reframing Unhelpful\nThoughts\nWe use the ten categories of unhelpful thought pat-\nterns described in lay terms in a widely used CBT\nself-help book used for bibliotherapy (Burns, 1980).\nTable 1 lists these categories and provides exam-\nples for each category. For reframing unhelpful\nthoughts, we follow Ziems et al. (2022), who de-\nscribe five reframing strategies based on positive\npsychology (Harris et al., 2007): (i) Growth Mind-\nset: Focusing on learning from challenges and im-\nproving the skills needed to deal with a difficult\nsituation; (ii) Optimism: Directing the attention\ntowards the positive aspects of the situation and\nexpressing gratitude while still acknowledging the\nnegative aspects; (iii) Impermanence: Understand-\ning that adversities are inevitable and temporary\nand focusing on accepting the situation; (iv) Neu-\ntralizing: Challenging unhelpful thoughts that are\nfar from reality and replacing them with realistic\nneutral alternatives; (v) Self-affirmation: Reflect-\ning on core values to ground oneself in a difficult\nsituation. Note that other reframing strategies ex-\nist, such as \u201cbeing mindful\u201d (Robertson, 2012), or\n\u201cfocusing on forgiveness and compassion\u201d (Gilbert,\n2010). We provide the above five strategies only as\na starting point, but crowd workers are free to use\nother strategies.\n4\nPATTERNREFRAME Dataset\n4.1\nData Collection\nWe briefly explain the four-step data collection pro-\ncess used to crowdsource the dataset. We provide\nfurther data collection details and snapshots of the\ninterface in Appendix A and B.\n4.1.1\nTask 1: Writing Unhelpful Thoughts\nIn order to generate unhelpful thoughts that match\na diversity of contexts and situations, we use per-\nsonas from the PERSONA-CHAT dataset (Zhang\net al., 2018) as context for writing unhelpful\nthoughts. We give a persona and one of the ten un-\nhelpful thought patterns to the crowdsource work-\ners, and ask them to write sentences that both are\nconsistent with the given persona, and exhibit the\ngiven unhelpful thought pattern.\n4.1.2\nTask 2: Categorizing Unhelpful\nThoughts\nUnhelpful thoughts can exhibit multiple patterns,\nand the patterns themselves are overlapping rather\nUnhelfpul Thought Patterns and their\ndistribution\nExample Thoughts and their Rewrites that remove the pattern\nCatastrophizing\nby\ngiving\ngreater\nweight to the worst possible outcome.\n(1024 thoughts / 2826 rewrites)\nMy mom hasnt come home from work yet. I hope the store isn\u2018t getting robbed!\nRewrite: My mom hasn\u2019t come home from work yet. She must have gotten\nswamped. I\u2019ll cook dinner now so it\u2019s ready when she gets home.\nDiscounting the positive: experiences\nby insisting that they \u201cdon\u2019t count\u201d.\n(970 thoughts / 2680 rewrites)\nMy restaurant is the most popular in my city, but that\u2019s just luck.\nRewrite: My restaurant is the most popular in the city. I suppose all my hard\nwork has paid off.\nOvergeneralization is making faulty\ngeneralizations from insufficient evi-\ndence.\n(983 thoughts / 2747 rewrites)\nMy nephews didn\u2019t want to spend the weekend with me this week. I must not be\nas good of an aunt as I thought.\nRewrite: My nephews didn\u2019t want to spend the weekend with me this week. They\nmust be busy.\nPersonalization is assigning a dispropor-\ntionate amount of personal blame to one-\nself. (934 thoughts / 2544 rewrites)\nMy sister was not happy with the makeup look I did for her. I am a bad artist.\nRewrite: My sister was not happy with the makeup I did for her, next time I\u2019ll\ntry something different.\nAll-or-nothing is viewing things as ei-\nther good or bad and nothing in-between.\n(952 thoughts / 2628 rewrites)\nThe school christmas choir concert got canceled. This holdiday season is ruined.\nRewrite: Even though the choir concert got canceled there are still other fun\nactivities to do on the holiday.\nMental Filtering occurs when an indi-\nvidual dwells only on the negative details\nof a situation.\n(936 thoughts / 2562 rewrites)\nIt\u2019s nice to enjoy the sea breeze when you live near the ocean but it\u2019s not worth\nit when you think of all the sand getting dragged into your home and all the\ntourists making so much noise at the beach.\nRewrite: I am so fortunate to live where I can enjoy the sea breeze. Not everyone\nis this lucky.\nMind Reading is inferring a person\u2018s\nprobable (usually negative) thoughts\nfrom their behavior.\n(992 thoughts / 2688 rewrites)\nI auditioned for the surf team and the coach avoided me. I am sure it is because\nhe does not like my skills.\nRewrite: I auditioned for the surf team and the coach avoided me. I\u2019m sure the\ncoach always tries to appear neutral during try-outs.\nFortune Telling is predicting outcomes\n(usually negative) of events.\n(997 thoughts / 2758 rewrites)\nI didn\u2019t make it to Yellowstone this year, I am never going to go to that park.\nRewrite: I didn\u2019t get to go to Yellowstone this year, I will work extra hard and\nsave up to definitely go next year!\nShould statements, where a person de-\nmands particular behaviors regardless of\nthe realistic circumstances.\n(921 thoughts / 2413 rewrites)\nI prefer texting over phone calls. People should never call me and expect me to\nanswer.\nRewrite: Just because I like texting doesn\u2019t mean everyone needs to like it.\nLabeling and mislabeling is attributing\na person\u2019s actions to their character rather\nthan the situation.\n(960 thoughts / 2661 rewrites)\nI fell off my skateboard yesterday, I\u2019m a terrible athlete.\nRewrite: I fell off my skateboard yesterday, but even the best crash sometimes.\nTable 1: Examples of unhelpful thoughts and their reframed versions from our PATTERNREFRAME dataset. The\nthought pattern definitions are derived from Wikipedia.\nthan distinct (Burns, 1980). In order to capture this,\nas well as filter out low-quality crowdsourced data,\nwe use a second crowdsourcing task requesting\nworkers to label the previously generated thoughts.\nWorkers are given a thought and the list of unhelp-\nful patterns, and select all the patterns that appear\nin the thought. The annotators can choose a \u201cNone\u201d\noption in case the thought is irrelevant or nonsensi-\ncal. We collect five annotations for each thought,\nand discard the thoughts that are marked \u201cNone\u201d\nby a majority of annotators.\n4.1.3\nTask 3: Reframing Unhelpful Thoughts\nIn a third task, we ask crowdworkers to rewrite\nthoughts containing unhelpful patterns, in a more\nhelpful way, similar to the task in Ziems et al.\n(2022). We give crowdworkers a thought and the\npersona and unhelpful pattern that were used to\ngenerate it, and ask them to rewrite the thought in\na way that still aligns with the context, but does not\ncontain the unhelpful pattern. We also show the\nfive reframing strategies described in \u00a73 to aid the\nworkers in reframing the thoughts, and ask them to\nselect what strategy they used, if any. Note that the\nstrategies are only provided as suggestions, and the\nworkers are free to reframe the thought in other ap-\npropriate ways. We collect three rewrites for each\nthought.\n4.1.4\nTask 4: Evaluating the Rewrites of\nUnhelpful Thoughts\nFinally, we assess the quality of the rewrites as\nfollows: workers are given a persona, unhelpful\nthought pattern, generated thought, along with\nthree rewrites. They are asked to select which\nrewrites successfully remove the unhelpful pattern\nwhile not logically contradicting the source (fol-\nlowing Ziems et al. (2022)). If worker selects a\nvalid rewrite, we further ask them to identify which\nof the five proposed reframing strategies were used,\nif any. We collect five annotations for each set, and\ninclude only the rewrites that are marked as \u201cvalid\u201d\nby a majority of annotators.\n4.2\nData Quality\nWe use the Mephisto2 and Amazon Mechanical\nTurk3 platforms to collect crowdsource data. We\nuse the labeling tasks (2nd and 4th task) to select a\npool of high-quality workers (that is, crowdsource\nworkers whose generative work was validated by\na majority of separate annotators in a separate la-\nbeling task), after first seeding the set of annotators\nthrough manual inspection of a first batch of data.\nWe use only selected annotators for evaluation tasks\n(tasks 2 and 4). We first kept the generative text\ntasks (tasks 1 and 3) open to all workers. We ex-\npanded the list of selected workers after every iter-\nation by adding new workers that had completed\nat least five generative text tasks with at least 80%\nof generated text validated through the evaluation\ntasks. We ended up with 524 qualified workers\nafter nine rounds of the entire pipeline, where each\niteration started with a batch of 500 thoughts. Once\nwe gathered > 500 qualified workers, we restricted\nall the tasks to the selected pool. In the final dataset,\nwe included only the annotations provided by these\nselected workers.\nAlong with the selected pool of workers, we\nalso included onboarding tasks (details in \u00a7A) to\nensure that the workers adequately understood the\nconcept of reframing thoughts. Only the workers\nwho passed the onboarding tasks were qualified\nto work on the actual tasks. We calculated inter-\nannotator agreement using Krippendorf\u2019s Alpha,\nwhich was 0.355 for the second task and 0.454 for\nthe fourth task.4\n2https://github.com/facebookresearch/Mephisto\n3Our crowdsourcing tasks pay workers well above mini-\nmum wage.\n4We compute Krippendorf\u2019s Alpha for the binary pattern-\nlevel judgments from the the second task and the binary\nFigure 1: Confusion matrix representing the distribution\nof unhelpful thoughts across different patterns in our\ndataset. Rows represent the patterns used to collect the\nunhelpful thoughts in the first task (\u00a74.1.1). Columns\nrepresents the patterns chosen by annotators in the sec-\nond task (4.1.2). As expected, some related patterns\nsuch as Discounting the positive (DP) and Mental filter-\ning (MF) exhibit strong cross-labeling.\n4.3\nData Analysis\n4.3.1\nDataset Statistics\nPATTERNREFRAME contains 9,688 thoughts and\n26,507 reframed versions of thoughts. We split\nthe dataset into training, validation, and test sets of\nrespective sizes 1,920 / 961 / 6,807 for thoughts,\nand 5,249 / 2,623 / 18,635 for reframed thoughts.\nOne thought can have up to three reframed versions,\nwith an average of 2.74 rewrites / thought after\nfiltering out lower-quality rewrites. The average\nword lengths of thoughts and rewrites are 19.1 and\n23.9, respectively.\n4.3.2\nAnalysis of Unhelpful Thought Patterns\nFigure 1 shows the distribution of thoughts across\ndifferent patterns in our dataset, with initial con-\nditioning pattern (1st task) in rows and annotator\nidentified patterns (2nd task) in columns. As ex-\npected, there is a high overlap among some related\npatterns, e.g., Discounting the positive / Mental\nFiltering, Fortune Telling/ Catastrophizing, and\nPersonalization / Labeling and Mislabeling. All\nor Nothing Thinking is difficult to distinguish, and\nshows high overlap with many categories. Mind\nReading and Should Statement show the lowest\namounts of overlap with other patterns.\nreframe-level judgements from the fourth task.\n4.3.3\nAnalysis of Reframing Strategies:\nFigure 2 shows the distribution of reframing strate-\ngies used to reframe the unhelpful thoughts in\nour dataset, among the five strategies proposed by\nZiems et al. (2022). Here, we use the strategies\nidentified by the workers in the fourth task of eval-\nuating reframed thoughts. Most rewritten thoughts\nmake use of one of the five strategies, with very few\nbeing labeled as \"None.\" Growth Mindset and Opti-\nmism are the most commonly used reframing strate-\ngies, followed by Neutralizing and Self-Affirmation.\nOptimism is especially common for patterns that\nfocus on the negative aspects of the situation such\nas Discounting the positive and Mental Filtering.\nFigure 2: Matrix showing the distribution of refram-\ning strategies across different unhelpful thought pat-\nterns. Rows represent the unhelpful thought patterns\nand columns represent the reframing strategies.\n5\nModels to Generate, Recognize, and\nReframe Unhelpful Thoughts\nWe train and evaluate different models using our\nPATTERNREFRAME dataset on three tasks: generat-\ning, identifying, and reframing unhelpful thoughts\n\u2013 all conditioned on a given persona.\n5.1\nGenerating Unhelpful Thoughts\n5.1.1\nTask and Data\nGiven a persona and an unhelpful thought pattern,\nthe goal is to generate a thought that exhibits the\ngiven pattern and aligns with the persona. We for-\nmulate the task as a standard conditioned genera-\ntion problem and optimize the maximum likelihood\nloss during training. We use the train, validation,\nand test splits described in \u00a74.3.1.\n5.1.2\nMethods\nWe evaluate methods based on fine-tuning and few-\nshot learning. We fine-tune BART-large (Lewis\net al., 2020), T5-large (Raffel et al., 2020), and\nR2C2-3B (Shuster et al., 2022) (a BART-based lan-\nguage model specialized in dialogues). For the\ninput, we concatenate the persona and the unhelp-\nful thought pattern texts using a special delimiter\ntoken. We also generate responses with GPT3.5\n(Ouyang et al., 2022), a state-of-the-art language\nmodel trained to follow human instructions, as a\n1-shot method. We generated thoughts for only 100\nrandom inputs in the PATTERNREFRAME test set,\nsince we had limited access to the API5 to GPT3.5\n(text-davinci-002)6. We provide implementation\ndetails and examples of input prompts in Appendix\nD and E, respectively.\n5.1.3\nAutomatic Evaluation\nFollowing previous work on text reframing (Ziems\net al., 2022; Chen et al., 2021), we report BLEU\n(Papineni et al., 2002), ROUGE (Lin, 2004), and\nBERTScore (Zhang et al., 2020), which capture the\nsemantic similarity between the generated thought\nand the human reference. We also report distinct-1,\nand distinct-2 metrics to measure the diversity of\nthe generations. Distinct-n (Li et al., 2016) cal-\nculates the ratio between the number of unique\nn-grams and the total number of n-grams in a gen-\neration.\nTable 2 shows the automatic evaluation results\nfor the task. All the models perform close to each\nother in terms of BLEU, BERTScore, and ROUGE.\nGPT3.5 generates lexically diverse rewrites with\nthe best Distinct-n scores. We provide examples of\nsystem outputs in Table 3.\n5.1.4\nHuman Evaluation\nAs automatic metrics often fail to fully capture\nhuman preferences in text generation tasks, we\nalso perform human evaluation. We collect human\nratings of 100 random thoughts from the test set.\nSimilar to previous style transfer works (Ziems\net al., 2022; Briakou et al., 2021; Rao and Tetreault,\n2018), we evaluate the generated rewrites along\nthree dimensions through Yes/No binary ratings:\n(i) fluency, which evaluates the readability of the\ngeneration, (ii) meaning preservation, which here\nverifies if the rewrite aligns with the given persona\n5https://openai.com/api/\n6In our experiments, we used text-davinci-002, since text-\ndavinci-003 had not been released yet.\nGenerating Unhelpful Thoughts\nReframing Unhelpful Thoughts\nBLEU\nROUGE\nBScore\nDist-1\nDist-2\nBLEU\nROUGE\nBScore\nDist-1\nDist-2\nBART\n25.3\n23.9\n89.0\n0.021\n0.087\n69.7\n53.1\n93.5\n0.034\n0.223\nT5\n24.5\n24.3\n89.1\n0.019\n0.08\n69.9\n55.5\n93.6\n0.039\n0.261\nR2C2\n25.5\n24.1\n89.2\n0.023\n0.1\n70.0\n55.0\n93.7\n0.036\n0.235\nGPT3.5\u2020\n24.9\n19.2\n88.1\n0.196\n0.586\n51.5\n41.2\n91.7\n0.204\n0.633\nReference\n100.0\n100.0\n100.0\n0.044\n0.304\n100.0\n100.0\n100.0\n0.041\n0.309\nTable 2: Automatic evaluation results on the PATTERNREFRAME test set. We report BLEU, ROUGE, BERTScore\n(BScore), Distinct-1 (Dist-1), and Distinct-2 (Dist-2) metrics. \u2020We calculate metrics over 100 random generations\nbecause of our limited access to the GPT3.5 API (text-davinci-002).\nFigure 3: Human evaluation results for the tasks of generating (left) and reframing (right) unhelpful thoughts. Y-axis\nshows the percentage of outputs rated positively by at least five of the nine annotators.\nand thought, and (iii) quality, which here evaluates\nif the generated thought exhibits the given unhelp-\nful thought pattern. We collect 9 annotations for\neach system output and apply majority voting to\nextract the final annotation.7\nTable 3 shows the percentage of outputs rated\npositively by at least five of the nine annotators.\nGPT3.5 outperforms all other approaches, includ-\ning human references, in terms of fluency and qual-\nity. However, GPT3.5 shows the lowest (but still\nvery high) meaning preservation score for gen-\nerating thoughts. The other models have more\ndifficulty including the unhelpful pattern (lower\n\"thought quality\" scores).\n5.2\nClassifying Unhelpful Thoughts\n5.2.1\nTask and Data\nGiven a persona and a thought, the goal is to clas-\nsify them into one of the ten unhelpful thought\npatterns or \u201cNone\u201d, which indicates that the input\nthought does not contain any of the ten unhelpful\npatterns, or the thought does not align with the\npersona. We formulate the task as a multiclass\nclassification problem with eleven categories.\nWe once again use the same train, validation, and\n7We also provide results using a more stringent threshold\nof 7 out of 9 annotators rating positively, in Appendix F. The\npattern of results is similar.\ntest splits described in \u00a74.3.1. Note that the dataset\ncontains only positive examples for the classifica-\ntion task, i.e., thoughts that align with a specific\nthought pattern and persona. For every positive\nexample, we construct a negative example by ran-\ndomly choosing one of the following options: (i) a\nthought from our dataset that belongs to the same\npattern but a different persona. (ii) a dialog text\nfrom PERSONA-CHAT belonging to the same per-\nsona (but presumably not containing any unhelpful\npattern), (iii) a dialog text from PERSONA-CHAT\nbelonging to a different persona (and again, presum-\nably not containing any unhelpful pattern). Thus,\nnegative examples encompass neutral texts and mis-\naligned thoughts and personas. We assign the cat-\negory \u201cNone\u201d to these examples. We have 3,834\ntrain, 1,915 validation, and 13,572 test instances\nafter augmenting the dataset with these examples.\n5.2.2\nMethods\nWe finetune RoBERTa (Liu et al., 2019) using the\nsoft-label distribution obtained through the second\ntask of our data collection pipeline (\u00a74.1), where\nwe asked multiple annotators to identify the pat-\nterns exhibited in a thought, and then normalized\nthe votes across the patterns. We use a soft la-\nbel distribution instead of single label because of\nthe high overlap across patterns. We also perform\n(a) RoBERTa\n(b) GPT3.5 (text-davinci-002)\u2020\nFigure 4: Confusion matrices for the unhelpful thoughts classification task on our classification test set. The\nrows represent true labels and the columns represent predicted labels. We clustered similar patterns for clearer\ninterpretation. Polarized Thinking includes Overgeneralization, Catastrophizing, All or Nothing Thinking, and\nFortune Telling. Filtering refers to Mental Filtering and Discounting the positive. Mislabeling encompasses\nPersonalization and Labeling and Mislabeling. \u2020We obtain outputs for only 100 random thoughts.\n11-way, 1-shot classification using GPT3.5. We\nconstruct the input prompt using one example from\neach category (examples in \u00a7E) and classify 100\nrandom inputs in the test set. We include further\nimplementation details in Appendix D.\n5.2.3\nEvaluation\nFigure 4 shows the confusion matrices for\nRoBERTa and GPT3.5 on the augmented version of\nthe PATTERNREFRAME test set. Given that several\nunhelpful thinking patterns are closely related (for\nexample, All or Nothing Thinking and Catastro-\nphizing), we cluster the patterns using the KMeans\nalgorithm (Lloyd, 1982) to group together patterns\nthat were deemed close by the model 8. RoBERTa\nperforms well on all the categories (> 72%) ex-\ncept the Mislabeling category, which has a high\noverlap with the Polarized Thinking category. The\nNone category has the highest performance, which\nshows that the classifier is able to differentiate neu-\ntral texts that do not contain any unhelpful pattern,\nor texts that are not aligned with the persona. 1-shot\nclassification using GPT3.5 performs worse than\nfine-tuned RoBERTa. GPT3.5 has trouble distin-\nguishing texts with and without unhelpful patterns\nand gets a low score for None. We also observed\nthat 40% of the classification predictions changed\nfor GPT3.5 after reordering the sequence of exam-\n8We first constructed an aggregated vector for each pattern\nby averaging the 10-dimensional outputs of the classifier for\nthe examples corresponding to each pattern on the validation\nset. We then clustered the ten 10-dimensional aggregated\nvectors into five clusters using the KMeans algorithm, with\nthe number of clusters determined using the elbow method.\nples in the prompt, which shows that few-shot clas-\nsification is not as reliable for this task, while still\nproviding decent performance way above chance.\n5.3\nReframing Unhelpful Thoughts\n5.3.1\nTask and Methods\nGiven a persona, an unhelpful thought pattern, and\na thought exhibiting the given pattern, the goal is to\nreframe the thought in a way that still aligns with\nthe persona and the context of the thought but does\nnot contain the pattern. The reframing problem\nis similar to the generating one, except that the\nunhelpful thought is now a part of the input instead\nof the target. We use the same training, validation,\nand test splits for the reframing task (\u00a74.3.1). We\nalso evaluate the same approaches described in\n\u00a75.1.2. For fine-tuned methods, we concatenate the\npersona, the pattern, and the thought texts with a\nspecial token. For few-shot methods, we construct\na prompt similar to the one used for the generation\ntask, but with the reframed statements (examples\nin Appendix E).\n5.3.2\nAutomatic Evaluation\nTable 2 shows the automatic evaluation results on\nthe PATTERNREFRAME test set. We use the met-\nrics described in \u00a75.1.3 namely BLEU, ROUGE,\nBERTScore, and Distinct-n metrics. As each un-\nhelpful thought can have up to 3 ground truth\nreframed versions, we take the maximum of the\nthree scores and report the mean of these max-\nima. R2C2 performs the best in terms of BLEU\nand BERTScore. GPT3.5 again outperforms the\nGenerating Unhelpful Thoughts\nPersona\nI like to chat with my friends. I like to read books that are in a series. I like to go to the movies. I love to go out\nto eat with my family.\nPattern\nCatastrophizing\nT5\nI love to go out to eat with my family, but I\u2019m sure we\u2019ll all get sick and die.\nBART\nI like to read books that are in a series, but I will never finish them.\nR2C2\nI love to go out to eat with my family, but one day we will all get food poisoning and die.\nGPT3.5\nI\u2019m out of money to go out to eat with my family, maybe they won\u2019t want to hang out with me anymore.\nHuman\nMy friends didn\u2019t want to chat with me today. They must have all hate me and wish me ill.\nPersona\nI write children\u2019s books. I like to drink. I live alone. I shop online, too much. I don\u2019t believe in love.\nPattern\nShould statements\nT5\nI write children\u2019s books, everyone should read them.\nBART\nI write children\u2019s books, everyone should read them.\nR2C2\nI write children\u2019s books. I should be published by now.\nGPT3.5\nI should be able to write the perfect book, drink moderately, be in a relationship, and not shop online so much.\nHuman\nI shop online and I should make enough money to be able to afford anything and everything I want.\nReframing Unhelpful Thoughts\nPersona\nMy mother was a teacher. My favorite food is a salad. I enjoy nature. I teach a yoga class. I am single.\nPattern\nBlack-and-white or polarized thinking / All or nothing thinking\nThought The date I went on last week was a total fail. I\u2019m going to be single forever.\nT5\nThe date I went on last week was a total fail. I will try harder next time.\nBART\nThe date I went on last week was a total fail. Next time I will be more careful.\nR2C2\nThe date I went on last week was a total fail. I\u2019m going to be single forever.\nGPT3.5\nThe date I went on last week wasn\u2019t a great match, but I\u2019m open to meeting someone new and trying again.\nHuman\nI had a bad date last week, thankfully I went on a nature walk and realized I have plenty of time to find a partner.\nPersona\nI was diagnosed with mild OCD in college. Once a year, I take a week off and head to Florida. I freeze up in\nsocial situations. Sometimes I feel like I ve already been through life once before.\nPattern\nPersonalization\nThought I was diagnosed with mild OCD in college, it\u2019s my fault and I deserve it.\nT5\nI was diagnosed with mild OCD in college. It\u2019s not my fault.\nBART\nI was diagnosed with mild OCD in college. I am working hard to overcome it.\nR2C2\nI was diagnosed with mild OCD in college. I\u2019m glad to have a diagnosis so I can get the help I need.\nGPT3.5\nI was diagnosed with mild OCD in college, it\u2019s something I\u2019m learning to manage.\nHuman\nI was diagnosed with mild OCD in college. I\u2019ve been seeing a therapist to get help managing it.\nTable 3: Examples of system outputs for the tasks of generating and reframing unhelpful thoughts.\nother models and the human references in terms of\nDistinct-1 and Distinct-2 scores, which indicates\nthat the generations are lexically diverse. Table 3\nprovides examples of system outputs.\n5.3.3\nHuman Evaluation\nFigure 3 shows human evaluation results on 100\nreframed thoughts generated by different models\ngiven the persona, the pattern type, and the un-\nhelpful thought from our test set. Similar to the\ngenerating thoughts task, we evaluate the reframed\nthoughts along fluency, meaning preservation, and\nquality, where we ask the annotators if the reframed\nthought removes the given unhelpful pattern while\nbeing consistent with the initial thought. All mod-\nels perform close to human reference in terms of\nfluency and meaning preservation. In fact, all the\noutputs of R2C2 and GPT3.5 are fluent and pre-\nserve meaning (that is, they generate statements\nthat are not contradictory with the initial thought).\nFor reframing quality, that is, removing the un-\nhelpful pattern, all models perform over 70%, but\nGPT3.5 performs the best. GPT3.5\u2019s superiority is\neven more marked when using the more stringent\nthreshold of 7 out of 9 annotators rating positively\nin Appendix F.\nOverall, the evaluation suggests that using mod-\nern models to produce reframing is a feasible ap-\nproach, even with a small amount of data for fine-\ntuning. In particular, GPT3.5 performs remarkably\nwell and very close to crowdsource worker perfor-\nmance, only based on prompting.\n6\nConclusion\nIn this work, we introduced a novel dataset, PAT-\nTERNREFRAME, which contains (1) about 10k\nstatements exhibiting unhelpful thought patterns,\nconditioned on a persona, and (2) multiple rewrit-\nten complementary thoughts that do not contain\nthe initial unhelpful pattern, instead reframing the\nthought in a more constructive way.\nUsing this dataset to train or prompt various mod-\nern language models, we showed that this range of\nmodels can already be a powerful tool to generate,\nidentify, and reframe unhelpful thoughts, condi-\ntioned on a persona. By releasing our dataset 9,\nwe hope to help practitioners of CBT draw from a\nricher, more diverse set of examples of unhelpful\nthought patterns and reframings. This would help\naddress the important limitation of a lack of person-\nalized and specific examples in existing datasets,\nwhen teaching cognitive techniques.\nFuture work will evaluate whether leveraging\nmodels to produce richer training material results\nin more robust learning and understanding of the\ntypes of unhelpful thought patterns in humans.This\nmay serve as the basis for future psychological val-\nidation studies of the materials and support future\nstudies of low-intensity self-help interventions.\n7\nLimitations\nThis work relied on previously published datasets\nto source personas on which to anchor the gener-\nated unhelpful thoughts, and thus shares the lim-\nitations of those datasets. In particular, they use\nEnglish-language responses, written by workers\nlocated in the United States.10. While these work-\ners are reasonably diverse (Moss et al., 2020), the\nexamples generated may not reflect the thought\npatterns and personas across cultures and diverse\npopulations. This data is also generated by people\nwho are being paid, as opposed to people genuinely\nengaging about situations that matter to them. Be-\nsides the substance of the thoughts themselves, a\nmore direct limitation is that the models generate\nonly English, so would not be directly usable for\nspeakers of other languages.\nIn addition, the data collected reflects the under-\nstanding of lay people, rather than trained clinical\npsychologists. While this makes the material more\nimmediately relatable to other lay people, it is pos-\nsible that the data do not capture what clinical psy-\nchologists would consider adequate illustrations of\nunhelpful patterns. Our data has been spot-checked\nby a CBT-trained clinical psychologist and found\ngenerally sound, but the entire material should un-\ndergo further validation.\nAnother limitation is that the models that we\nhave tested are resource-intensive. In particular, the\n9https://github.com/facebookresearch/\nParlAI/tree/main/projects/reframe_\nthoughts\n10Our crowdsourcing tasks pay workers well above mini-\nmum wage.\nbest-performing model, GPT3.5, is only available\nthrough a paid API.\n8\nEthical considerations\nWhile our work was developed to generate abun-\ndant data supporting work towards improving well-\nbeing, the negative statements it generates could be\nmisused. The parallel data of unhelpful thoughts\nand their reframed versions can also be used to\ngenerate negative texts from neutral ones, by train-\ning systems with reframed versions as the input\nand unhelpful thoughts as the output. This risk of\ngenerating negative content from positive/neutral\ntexts aligns with the risks of toxicity reduction and\nsentiment style transfer tasks.\nConversely, a different risk stems from over-\neager use of our work. This work aims to examine\nthe feasibility of generating ample practice mate-\nrial anchored on specific personas. We hope that\nreleasing a large dataset of unhelpful thoughts and\nreframings will further research that will ultimately\nhelp practitioners, but there is a danger that peo-\nple attempt to use the material as is, without the\nsupervision of a trained professional, which could\nbe harmful, as the material has not been tested with\nparticipants while monitoring adverse events such\nas increased anxiety or warped understanding of\nwhat unhelpful thoughts and useful reframings are.\nReferences\nAaron T. Beck. 1963.\nThinking and Depression:\nI. Idiosyncratic Content and Cognitive Distortions.\nArchives of General Psychiatry.\nAaron T. Beck. 1976. Cognitive therapy and the emo-\ntional disorders. international universities press.\nEleftheria Briakou, Sweta Agrawal, Ke Zhang, Joel\nTetreault, and Marine Carpuat. 2021. A review of\nhuman evaluation for style transfer. In Proceedings\nof the 1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021). Association\nfor Computational Linguistics.\nD.D. Burns. 1980. Feeling Good: The New Mood Ther-\napy. A Signet book.\nWei-Fan Chen, Khalid Al Khatib, Benno Stein, and Hen-\nning Wachsmuth. 2021. Controlled neural sentence-\nlevel reframing of news articles. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021.\nGlen Coppersmith, Mark Dredze, Craig Harman,\nKristy Hollingshead, and Margaret Mitchell. 2015.\nCLPsych 2015 shared task: Depression and PTSD\non Twitter. In Proceedings of the 2nd Workshop on\nComputational Linguistics and Clinical Psychology:\nFrom Linguistic Signal to Clinical Reality.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation.\nDaniel David, Ioana Cristea, and Stefan G. Hofmann.\n2018. Why cognitive behavioral therapy is the cur-\nrent gold standard of psychotherapy. Frontiers in\nPsychiatry.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers).\nAlex Fine, Patrick Crutchley, Jenny Blase, Joshua\nCarroll, and Glen Coppersmith. 2020. Assessing\npopulation-level symptoms of anxiety, depression,\nand suicide risk in real time using NLP applied to\nsocial media data. In Proceedings of the Fourth Work-\nshop on Natural Language Processing and Computa-\ntional Social Science.\nPaul Gilbert. 2010.\nAn introduction to compassion\nfocused therapy in cognitive behavior therapy. Inter-\nnational Journal of Cognitive Therapy.\nAlex H. S. Harris, Carl E. Thoresen, and Shane J. Lopez.\n2007. Integrating positive psychology into counsel-\ning: Why and (when appropriate) how. Journal of\nCounseling & Development.\nFj\u00f3la D\u00f6gg Helgad\u00f3ttir, Ross G Menzies, Mark Onslow,\nAnn Packman, and Sue O\u2019Brian. 2009. Online cbt i:\nBridging the gap between eliza and modern online cbt\ntreatment packages. Behaviour Change, 26(4):245\u2013\n253.\nStefan Hofmann, Anu Asnaani, Imke Vonk, Alice\nSawyer, and Angela Fang. 2012. The efficacy of cog-\nnitive behavioral therapy: A review of meta-analyses.\nCognitive therapy and research.\nXinyu Hua and Lu Wang. 2020. PAIR: Planning and\niterative refinement in pre-trained transformers for\nlong text generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nShaoxiong Ji, Tianlin Zhang, Luna Ansari, Jie Fu,\nPrayag Tiwari, and Erik Cambria. 2022. Mental-\nBERT: Publicly available pretrained language models\nfor mental healthcare. In Proceedings of the Thir-\nteenth Language Resources and Evaluation Confer-\nence.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020a. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics.\nZhengping Jiang, Sarah Ita Levitan, Jonathan Zomick,\nand Julia Hirschberg. 2020b. Detection of mental\nhealth from Reddit via deep contextualized repre-\nsentations. In Proceedings of the 11th International\nWorkshop on Health Text Mining and Information\nAnalysis.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation.\nDiederik P. Kingma and Jimmy Ba. 2014.\nAdam:\nA method for stochastic optimization.\nCoRR,\nabs/1412.6980.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad,\nAbdelrahman Mohamed,\nOmer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers).\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out. Association for Computational\nLinguistics.\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma,\nLili Wang, and Soroush Vosoughi. 2020. Data boost:\nText data augmentation through reinforcement learn-\ning guided conditional generation. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nS. Lloyd. 1982.\nLeast squares quantization in\npcm.\nIEEE Transactions on Information Theory,\n28(2):129\u2013137.\nLouis Martin, \u00c9ric de la Clergerie, Beno\u00eet Sagot, and\nAntoine Bordes. 2020. Controllable sentence sim-\nplification. In Proceedings of the Twelfth Language\nResources and Evaluation Conference.\nSabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-\nLan Boureau. 2022. Reducing conversational agents\u2019\noverconfidence through linguistic calibration. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:857\u2013872.\nMargot Mieskes and Andreas Stiegelmayr. 2018.\nPreparing data from psychotherapy for natural lan-\nguage processing. In Proceedings of the Eleventh\nInternational Conference on Language Resources\nand Evaluation (LREC 2018).\nAlexander Miller, Will Feng, Dhruv Batra, Antoine Bor-\ndes, Adam Fisch, Jiasen Lu, Devi Parikh, and Jason\nWeston. 2017. ParlAI: A dialog research software\nplatform. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 79\u201384. ACL.\nMargaret Mitchell, Kristy Hollingshead, and Glen\nCoppersmith. 2015. Quantifying the language of\nschizophrenia in social media. In Proceedings of\nthe 2nd Workshop on Computational Linguistics and\nClinical Psychology: From Linguistic Signal to Clin-\nical Reality.\nAaron J Moss, Cheskie Rosenzweig, Jonathan Robin-\nson, and Leib Litman. 2020. Demographic stability\non mechanical turk despite covid-19. Trends in cog-\nnitive sciences, 24(9):678\u2013680.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nDavid Owen, Jose Camacho-Collados, and Luis Es-\npinosa Anke. 2020. Towards preemptive detection\nof depression and anxiety in Twitter. In Proceedings\nof the Fifth Social Media Mining for Health Applica-\ntions Workshop & Shared Task.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics.\nRafa\u0142\nPo\u00b4swiata\nand\nMicha\u0142\nPere\u0142kiewicz.\n2022.\nOPI@LT-EDI-ACL2022: Detecting signs of depres-\nsion from social media text using RoBERTa pre-\ntrained language models. In Proceedings of the Sec-\nond Workshop on Language Technology for Equality,\nDiversity and Inclusion.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research.\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam,\nmay I introduce the GYAFC dataset: Corpus, bench-\nmarks and metrics for formality style transfer. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers). Association for Computa-\ntional Linguistics.\nD. Robertson. 2012. Build Your Resilience: CBT, mind-\nfulness and stress management to survive and thrive\nin any situation. Teach Yourself.\nLina M. Rojas-Barahona, Bo-Hsiang Tseng, Yinpei\nDai, Clare Mansfield, Osman Ramadan, Stefan Ultes,\nMichael Crawford, and Milica Ga\u0161i\u00b4c. 2018. Deep\nlearning for language understanding of mental health\nconcepts derived from cognitive behavioural therapy.\nIn Proceedings of the Ninth International Workshop\non Health Text Mining and Information Analysis.\nAlexis Ross, Tongshuang Wu, Hao Peng, Matthew E\nPeters, and Matt Gardner. 2022. Tailor: Generat-\ning and perturbing text with semantic controls. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3194\u20133213.\nEfsun Sarioglu Kayi, Mona Diab, Luca Pauselli,\nMichael Compton, and Glen Coppersmith. 2017. Pre-\ndictive linguistic features of schizophrenia. In Pro-\nceedings of the 6th Joint Conference on Lexical and\nComputational Semantics (*SEM 2017).\nTimo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume.\nTimo Schick and Hinrich Sch\u00fctze. 2021b. Few-shot\ntext generation with natural language instructions. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing.\nRoz Shafran, Pamela Myles-Hooton, Sophie Bennett,\nand Lars-G\u00f6ran \u00d6st. 2021. The concept and defi-\nnition of low intensity cognitive behaviour therapy.\nBehaviour Research and Therapy, 138:103803.\nSagarika Shreevastava and Peter Foltz. 2021. Detecting\ncognitive distortions from patient-therapist interac-\ntions. In Proceedings of the Seventh Workshop on\nComputational Linguistics and Clinical Psychology:\nImproving Access. Association for Computational\nLinguistics.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n2022. Language models that seek for knowledge:\nModular search & generation for dialogue and\nprompt completion.\nChris Williams. 2001.\nUse of written cognitive be-\nhaviour therapy self-help materials to treat depres-\nsion. Advances in Psychiatric Treatment, 7.\nJT Wolohan, Misato Hiraga, Atreyee Mukherjee, Zee-\nshan Ali Sayyed, and Matthew Millard. 2018. Detect-\ning linguistic traces of depression in topic-restricted\ntext: Attending to self-stigmatized depression with\nNLP. In Proceedings of the First International Work-\nshop on Language Cognition and Computational\nModels.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Anima Anandkumar, and Bryan\nCatanzaro. 2020. MEGATRON-CNTRL: Control-\nlable story generation with external knowledge using\nlarge-scale language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences.\nCaleb Ziems, Minzhi Li, Anthony Zhang, and Diyi Yang.\n2022. Inducing positive perspectives with text re-\nframing. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics.\nA\nData Collection Details\nA.1\nOnboarding Tasks\nWe introduce two onboarding tasks to ensure that the crowdsource workers understood the concept of\nunhelpful thoughts and how to reframe them. The onboarding tasks were reviewed by a CBT-trained\npsychologist. We use one onboarding task for tasks 1 and 2 and another onboarding task for tasks 3 and 4\nof the data collection pipeline. For the first onboarding task, we display an unhelpful thought pattern, one\npositive example that contains the pattern, and one negative example that does not, and ask the workers\nto select the positive one. We only allowed the workers that were able to identify the correct example\nfor three out of four such instances. For the second onboarding task, we display an unhelpful thought\npattern, a thought containing the pattern, one positive example that reframes the thought, and one negative\nexample that does not. We only allow the workers that were able to identity the positive example in three\nout of four such instances.\nB\nData Collection Interface Snapshots\nFigure 5: Data collection interface for the first task of the data collection pipeline, where crowdworkers are asked to\nwrite an unhelpful thought.\nFigure 6: Annotation interface for the second task of the data collection pipeline, where crowdworkers are asked to\nselect the patterns exhibited by an unhelpful thought.\nFigure 7: Data collection interface for the third task of the data collection pipeline, where the crowdworkers are\nasked to reframe unhelpful thoughts.\nFigure 8: Annotation interface for the fourth task of the data collection pipeline, where the crowdworkers are asked\nto evaluate the quality of the reframed thoughts.\nC\nEvaluation Interface Snapshots\nFigure 9: Annotation interface used to evaluate generated thoughts.\nFigure 10: Annotation interface used to evaluate statements that reframe unhelpful thoughts.\nD\nImplementation details\nD.1\nGeneration Models\nWe finetuned the BART, T5, and R2C2 baselines using ParlAI11. We used the BARTlarge (400M pa-\nrameters), T5large (770M parameters), and R2C2base (2.7b parameters)12 architectures. We used Adam\noptimizer (Kingma and Ba, 2014) and performed a hyperparameter search over learning rates 1e-05, 1e-06,\n1e-07, and 1e-08. We used linear warmup of 100 steps and applied early stopping with a patience value of\n5. We evaluated the validation set once in every 200 updates and truncated the input and the labels to 1000\ntokens. We applied gradient clipping value of 1.0. We used a batch size of 32. During inference, we used\nbeam search with beam size 10. We chose the best checkpoint during training based on the perplexity on\nthe validation set. Each model takes around 1 hour to run on 8 NVIDIA Tesla V100 Volta 32GB GPUs.\nD.2\nClassification Models\nFor classification experiments, we finetuned the RoBERTa-large checkpoint from Huggingface13. We\nused Adam optimizer (Kingma and Ba, 2014), learning rate of 1e-05, with linear warmup of 100 steps.\nWe trained the model for a maximum of 10 epochs. We evaluated on the validation set every 200 updates.\nWe used a batch size of 16. We chose the best checkpoint during training based on the weighted F1 value\non the validation set. The model takes around 1 hour to run on 1 NVIDIA Tesla V100 Volta 32GB GPU.\nE\nGPT3.5 Prompt Examples\nYou will be given (1) a type of unhelpful thinking pattern and the definition of the pattern and (2) a character. Please\nwrite an example of how this character could have thoughts that match the given thinking pattern.\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thinking Pattern: Discounting the positive (Rejecting positive experiences by insisting they \"don\u2019t count\" for\nsome reason or other.)\nUnhelpful Thought: My friends said they really enjoyed the camping trip I organized, but anyone could have done it.\nPersona: i\u2019m a business man. i love to sing. i\u2019m a karate black belt. my wife has terminal cancer.\nUnhelpful Thinking Pattern: Discounting the positive (Rejecting positive experiences by insisting they \"don\u2019t count\" for\nsome reason or other.)\nUnhelpful Thought:\nTable 4: Example GPT3.5 prompt for the task of generating unhelpful thoughts.\nYou will be given a type of unhelpful thinking pattern, a character, and an example of how this character could have\nthoughts that match the given thinking pattern. Please rewrite the thoughts in a way that still aligns with the persona and\nthe context of the unhelpful thought, but does not contain the unhelpful pattern.\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thinking Pattern: Overgeneralization (Someone who overgeneralizes makes faulty generalizations from\ninsufficient evidence. Even if something bad happens only once, it is expected to happen over and over again.)\nUnhelpful Thought: My younger kid has gotten bad grades at his maths test this week. He\u2019ll never be good at maths.\nReframe: My younger kid has gotten bad grades at his maths test this week. It\u2019s been a few times but hopefully we can\nfigure out a way to help him get better.\nPersona: i obsess over working out and being the best . i got a scholarship for playing soccer . its important\nfor my instagram posts to look like i am having fun . i try to eat healthy or i don\u2019t eat at all .\nUnhelpful Thinking Pattern: Overgeneralization (Someone who overgeneralizes makes faulty generalizations from\ninsufficient evidence. Even if something bad happens only once, it is expected to happen over and over again.)\nUnhelpful Thought: My future college team lost another game, I will never become a good athlete playing for them.\nReframe:\nTable 5: Example GPT3.5 prompt for the task of reframing unhelpful thoughts.\n11https://www.parl.ai/docs/index.html\n12https://parl.ai/docs/zoo.html#r2c2-base-2-7b\n13https://github.com/huggingface/transformers\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: The kids have stopped paying attention to how we can pitch the tent. They will never learn.\nUnhelpful Thinking Pattern: Jumping to conclusions: Fortune-telling\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: The kids are not enjoying this camping trip, they should really be more grateful about the effort we\nput in planning week-end activities for them.\nUnhelpful Thinking Pattern: Should statements\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: My kid is late from school. Perhaps she got run over by a car and is in a hospital.\nUnhelpful Thinking Pattern: Catastrophizing\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: This camping trip was a catastrophe. Sure the weather was gorgeous and the kids had a lot of fun,\nbut the waterfall always had many people ruining the photos we wanted to take.\nUnhelpful Thinking Pattern: Mental filtering\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: I like camping with my kids. We had a lot of fun the other weekend.\nUnhelpful Thinking Pattern: None\nPersona: Likes camping. Has 2 kids. Unhelpful Thought: The kids are having bad grades. It\u2019s because I\u2019m\na bad father.\nUnhelpful Thinking Pattern: Personalization\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: My younger kid has gotten bad grades at his math test this week. He\u2019ll never be good at math.\nUnhelpful Thinking Pattern: Overgeneralization\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: My friends said they really enjoyed the camping trip I organized, but anyone could have done it.\nUnhelpful Thinking Pattern: Discounting the positive\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: My kids are being very silent. I am sure it\u2019s because they really hate me for taking them on this\ncamping trip.\nUnhelpful Thinking Pattern: Jumping to conclusions: mind reading\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: I didn\u2019t manage to light up the fire for the camp today, I\u2019m such a useless outdoors person.\nUnhelpful Thinking Pattern: Labeling and mislabeling\nPersona: Likes camping. Has 2 kids.\nUnhelpful Thought: One of the 5 trails we planned to do on this trip is closed to the public. This trip is ruined.\nUnhelpful Thinking Pattern: Black-and-white or polarized thinking / All or nothing thinking\nPersona: i\u2019m a woman . i\u2019ve several children . we have a dog . we live in a rural area . my parents are still\nmarried .\nUnhelpful Thought: congratulations ! have you graduated college ? i am attending the university of michigan in the fall .\nUnhelpful Thinking Pattern:\nTable 6: Example GPT3.5 prompt for the task of identifying unhelpful thoughts.\nF\nResults with 7 over 9 agreement\nFigure 11: Human evaluation results for the tasks of generating (left) and reframing (right) unhelpful thoughts.\nY-axis shows the percentage of outputs rated positively by at least seven of the nine annotators.\n"
  },
  {
    "title": "Focused Transformer: Contrastive Training for Context Scaling",
    "link": "https://arxiv.org/pdf/2307.03170.pdf",
    "upvote": "9",
    "text": "Focused Transformer: Contrastive Training for\nContext Scaling\nSzymon Tworkowski1,3\u2217\nKonrad Staniszewski1,3\u2217\nMiko\u0142aj Pacek1,3\u2217\nYuhuai Wu6\u2020\nHenryk Michalewski3,4\nPiotr Mi\u0142o\u00b4s1,2,5\n1IDEAS NCBR\n2Institute of Mathematics, Polish Academy of Sciences\n3University of Warsaw\n4Google DeepMind\n5deepsense.ai\n6xAI\nAbstract\nLarge language models have an exceptional capability to incorporate new infor-\nmation in a contextual manner. However, the full potential of such an approach is\noften restrained due to a limitation in the effective context length. One solution\nto this issue is to endow an attention layer with access to an additional context,\nwhich comprises of (key, value) pairs. Yet, as the number of documents increases,\nthe proportion of relevant keys to irrelevant ones decreases, leading the model to\nfocus more on the irrelevant keys. We identify a significant challenge, dubbed\nthe distraction issue, where keys linked to different semantic values might over-\nlap, making them hard to distinguish. To tackle this problem, we introduce the\nFocused Transformer (FOT), a technique that employs a training process inspired\nby contrastive learning. This novel approach enhances the structure of the (key,\nvalue) space, enabling an extension of the context length. Our method allows for\nfine-tuning pre-existing, large-scale models to lengthen their effective context. This\nis demonstrated by our fine-tuning of 3B and 7B OpenLLaMA checkpoints. The\nresulting models, which we name LONGLLAMA2, exhibit advancements in tasks\nrequiring a long context. We further illustrate that our LONGLLAMA models\nadeptly manage a 256k context length for passkey retrieval.\n1\nIntroduction\nLanguage models have served as a catalyst for substantial advancements in several areas, including\nnatural language processing [Radford et al., 2019, Brown et al., 2020], code generation [Chen et al.,\n2021, Li et al., 2022], quantitative reasoning [Lewkowycz et al., 2022] and theorem proving [Polu\nand Sutskever, 2020, Jiang et al., 2022, Miku\u0142a et al., 2023]. One of the central challenges with\nlanguage models is the effective incorporation of extensive new knowledge. The common practice\nof fine-tuning the model is not only resource-intensive and complex to manage, but it also does not\nalways clearly indicate how to incorporate new knowledge. For example, fine-tuning on a text such as\n\u201cAlice in Wonderland\u201d does not equip the model to answer questions about the story itself, but rather\n\u2217Equal contribution \u2020Work done while at Google Research.\n2We release the checkpoints and source code of LONGLLAMA\n, see also our colabs.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2307.03170v2  [cs.CL]  30 Nov 2023\n2k\n8k\n16k\n100k\n256k\n0\n20\n40\n60\n80\n100\nLongLLaMA 3B\nOpenLLaMA 3B\nPrompt length (#tokens)\nAccuracy (%)\nLongLLaMA training context\nOpenLLaMA training context\nFigure 1: Accuracy of LONGLLAMA 3B on passkey retrieval compared to the original OpenLLaMA model.\nOur method extrapolates beyond the training length, achieving 94.5% accuracy at a context length of 100k and\n73% at 256k tokens, while the baseline is unable to handle context longer than its training length (2k).\nit trains the model to predict the next token or complete masked sentences. A promising alternative\n\u2013 integrating the new knowledge within the context \u2013 doesn\u2019t require training but is considerably\nrestricted by the model\u2019s effective context length. For this method to work with large knowledge\ndatabases (like large code repositories), the model needs to manage a context length extending to\nmillions of tokens.\nIn this research, we highlight one of the primary obstacles in augmenting the context length: as the\nnumber of documents increases, the ratio of pertinent to irrelevant tokens diminishes. The standard\ntraining procedure frequently results in overlaps between keys connected with irrelevant values and\nthose related to relevant ones, exacerbating the model\u2019s task of differentiating between them. We\nterm this challenge the distraction issue.\nWe propose the Focused Transformer (FOT), an innovative technique developed explicitly to address\nthis issue. The Focused Transformer permits a subset of attention layers to access an additional\ncontext of (key, value) pairs through the k-nearest neighbors (kNN) algorithm, akin to the method\nused in [Wu et al., 2022]. This mechanism effectively extends the total context length. The distinctive\naspect of the Focused Transformer is its training procedure, drawing from contrastive learning. This\nmethod addresses the distraction issue and facilitates larger context capacities. Specifically, during\nthe training phase, we deliberately expose the chosen subset of attention layers to both relevant and\nirrelevant keys (like negative samples from unrelated documents). This strategy incentives the model\nto differentiate keys connected with semantically diverse values, thereby enhancing their structure.\nWe introduce and make available LONGLLAMAs (\n), fine-tuned OpenLLaMA models with FOT,\ndemonstrating that our method does not require long context during training and can be applied to\nexisting models. Notably, LONGLLAMAs show significant improvements on tasks necessitating\nlong-context modeling. In particular, they can manage a 256k context length on the passkey retrieval\ntask [Mohtashami and Jaggi, 2023].\nOur research contributions are the following:\n1. We pinpoint the distraction issue as a significant challenge and a primary obstacle to scaling up the\ncontext length in Transformer models, particularly in multi-document scenarios.\n2. We develop the Focused Transformer (FOT), designed to alleviate the distraction issue. FOT\nincludes a unique training objective that improves the (key, value) structure, enabling the use of\nextensive additional context and k-nearest neighbors lookup to scale the context length.\n3. Our method is simple to implement, and it provides the benefit of extending model context\nwithout modifying the architecture, facilitated by cost-effective fine-tuning. We demonstrate this on\nthe 3B and 7B OpenLLaMA checkpoints. The resulting models, named LONGLLAMAs, display\nenhancements on tasks that benefit from increasing the number of few-shot demonstrations in the\nextended context, such as TREC [Li and Roth, 2002, Hovy et al., 2001] and WebQS [Berant et al.,\n2\n2013]. We also prove that for passkey retrieval Mohtashami and Jaggi [2023], our LONGLLAMA\nmodels successfully handle a 256k context length.\n4. We further scrutinize FOT\u2019s capabilities across various datasets and model sizes. We show that a\nFOT trained with a total context of 512 tokens can extrapolate to 16 million tokens in a benchmark\ndictionary lookup task. We also assess FOT on long-context language modeling tasks such as\nbooks (PG-19), mathematics (arXiv), code (GitHub), and formal proofs (Isabelle), where it exhibits\nimprovements in perplexity over baselines.\n2\nRelated work\nLong-context transformer architectures\nA multitude of approaches have been developed to\nincrease the context length of transformers, mostly focusing on alleviating the quadratic complexity\nof the attention computation. For instance, Transformer-XL [Dai et al., 2019] caches the previous\ncontext and enables the linear extension of context with the number of layers. Longformer [Beltagy\net al., 2020] employs an attention mechanism that allows tokens to attend to distant tokens sparsely,\nreducing the computational complexity. BigBird [Zaheer et al., 2020], LongT5 [Guo et al., 2021],\nand [Dao et al., 2022] also use sparse attention to handle long sequences. Different efficiency\nconsiderations have been studied in [Kaddour et al., 2023], showing that they lead to limited gains.\nHierarchical transformers [Nawrot et al., 2021, 2023] downsample activations in intermediate layers\nto reduce computation and enable longer contexts. COLT5 [Ainslie et al., 2023] proposes conditional\ncomputation to save memory and enable larger contexts. Memorizing Transformer [Wu et al., 2022]\nuses kNN lookup to pick up the most relevant tokens, which might also be seen as a way to reduce\nthe computational complexity of attention. Our work adheres to this approach and aims to train a key\nspace that handles longer attention context length (e.g., by mitigating the distraction issue) and, thus,\nhas better long-context capabilities.\nFine-tuning LLMs for longer retrieval\nPrior works such as RETRO [Borgeaud et al., 2022]\n(RETROfitting) and Memorizing Transformer [Wu et al., 2022] have demonstrated a promising path\nfor fine-tuning existing LMs to add new capabilities without the need to retrain the entire model. In\ncontrast to those approaches our method is not framed as a retrieval but as a way of extending the\ncontext of the model. In contrast to RETRO, we propose a single-stage method for context extension\ninstead of a two-stage retrieve-then-embed approach. We provide a more detailed comparison with\nthe Memorizing Transformer in Appendix C.3. More recently, a number of works have explored\nfine-tuning LLaMA to extend its context length. Landmark attention [Mohtashami and Jaggi, 2023]\nproposes a compression scheme of LLM\u2019s context into landmarks, increasing the context length\nof LLaMA-7B to 32K. Position Interpolation (PI, [Chen et al., 2023] and [kaiokendev, 2023])\nintroduces a modification to the rotary positional encoding scheme that enables fine-tuning for 32K\ncontext. In contrast to this work, our method does not rely on positional encodings, following the\nfindings from [Haviv et al., 2022]. Removing positional encoding in additional context allows us to\nextrapolate to 256k tokens, although the model was only trained on sequences up to 8K, yielding\ntheoretically unbounded context length.\nZero-shot methods\nKNN-LM [Khandelwal et al., 2019] shows that one can improve the perfor-\nmance of a LLM by combining two probability distributions. One created by a pre-trained model,\nand one based on the similarity between the embedding of the currently processed token and the\nembeddings of tokens retrieved from a large database. Meanwhile, we extend the model context in a\nsubset of attention layers, potentially allowing for reasoning within this extended context. Parallel\nContext Windows for Large Language Models [Ratner et al., 2023] introduces a method for extending\nthe context of language models without training. They achieve this by embedding several context\nwindows independently in parallel and allowing only a subset of tokens to attend to all windows. On\nthe other hand, we fine-tune existing models and allow all tokens to attend to all previous tokens\nbut only in a subset of layers. Additionally, our method allows us to improve the structure of the\nkey-value space of the existing models.\nContrastive learning\nContrastive learning aims to learn good representations by comparing positive\nand negative examples. CLIP [Radford et al., 2021] and SimCLR [Chen et al., 2020] are two popular\ncontrastive learning methods that have achieved state-of-the-art performance in the image domain.\nDuring contrastive pre-training, negative examples are kept in the same batch to learn to distinguish\n3\nLayers\nbatch size = 4\nd = 2\nDocument D\nDocument C\nDocument B\nDocument A\nDocument D\nDocument C\nDocument B\nDocument A\nCprev\nCcurr\nAtt\nExternal\nMemory\nRetrieve\nwith KNN\nStore Keys\nand Values\nK Neighbors\nAtt + \u2207\nTrain\nCrossBatch\nAtt + \u2207\nInference\nCprev\nCcurr\nCcurr\nFigure 2: The Focused Transformer overview. During infer-\nence, a memory attention layer (green) uses additional context\nof (key, value) pairs via kNN lookup, which effectively ex-\ntends its context length. This layer is trained using crossbatch.\nNamely, the tokens from the current context Ccurr attend in a\ndifferentiable way (Att + \u2207) to the previous context Cprev of\nthe same document and, importantly, d \u2212 1 contexts of other\ndocuments. The latter serve as \u2019negative\u2019 examples intended\nto better shape the (key, value) space.\nthem from positive examples. Scaling the batch size in contrastive learning has been demonstrated\nto enhance the quality of representations, as shown in [Gao et al., 2021b]. It has been suggested\n[Gao et al., 2019] that the embedding space in language modeling suffers from degeneracy, where\nembeddings are tightly packed in a narrow cone, making it difficult to distinguish between them.\nTRIME [Zhong et al., 2022] proposes a training approach designed for training LMs with memory\naugmentation, which uses negatives to improve the quality of representations. The main difference\nbetween this and our approach is that we incorporate negatives into the chosen subset of attention\nlayers instead of interpolating in the output layer and use the standard language modeling loss.\nTRIME [Zhong et al., 2022] also focuses on retrieval from large databases, whereas we focus on\nextending the context of the model. ContraCLM [Jain et al., 2023] applies contrastive losses at\nboth the token and sequence levels during training to promote more uniformly distributed, isotropic\nrepresentations. It is shown to enhance the discrimination of representations on textual semantic\nsimilarity benchmarks. While ContraCLM focuses on improving the general expressiveness of\nrepresentations, our work introduces contrastive-inspired techniques designed specifically for training\nthe attention mechanism to handle longer context lengths. Nonetheless, exploring other contrastive\nlearning objectives could be beneficial for further improving the key structure in future work.\n3\nFOT: Focused Transformer\nOur method, the Focused Transformer (FOT), is a simple plug-and-play extension of transformer\nmodels and can be used both to train new models or fine-tune existing, possibly large, models with\nlonger context. To this end, FOT uses memory attention layers and the crossbatch training procedure.\nMemory attention layers enable the model to retrieve information from the additional context at\ninference time, effectively extending the context. The crossbatch training procedure biases the model\nto learn (key, value) representations, which are easy to use by a memory attention layer. See Figure 2\nfor an overview of the FOT architecture and Appendix L for pseudocode.\n3.1\nMemory attention layers\nMemory attention layers L are endowed with access to an additional context during inference.\nNamely, each query in \u2113 \u2208 L attends to preceding keys from the local context and the top k most\nmatching keys (i.e. having the largest inner product with the query) from memory. The memory keys\nare ranked by the inner product with the query and retrieved using the kNN search algorithm. We\nuse the exact kNN search implemented in FAISS [Johnson et al., 2017]. The memory is populated\nincrementally with (key, value) pairs processed by \u2113 beforehand. Our memory attention layer design\nis closely related to [Wu et al., 2022], we follow most of its design choices, except for the gating,\nwhich we replace with a simpler mechanism, which turns out to be more effective in our applications.\nSee details in Section C.3 and Appendix B.2. We remove positional encodings in memory layers\n4\nin all our models except LONGLLAMAs. This allows LONGLLAMA checkpoints to be a drop-in\nreplacement for LLaMA checkpoints. We treat the kNN search algorithm as an approximation of full\ndense attention, which opens the doors for future speed-ups.\n3.2\nCrossbatch training procedure\nOur training procedure is a novel way of training (or fine-tuning) transformer-based architectures\nin order to improve the structure of the (key, value) space. The main motivation is to shape this\nspace so that a memory attention layer \u2113 \u2208 L can easily focus on relevant information. The key\nidea, inspired by contrastive learning, is to expose \u2113 to (key, value) pairs from the current and\nprevious local context of the given document (positives) and d\u22121 contexts from unrelated documents\n(negatives). Importantly, this is done in a differentiable way.\nTo achieve this, we use a data pipeline in which each element of the batch corresponds to a different\ndocument. We embed the previous (Cprev) and the current (Ccurr) local context for each of the\nprocessed documents. The overview of our procedure can be found in Figure 2. Specifically for\neach document \u03b4 in Ccurr we create a set {p\u03b4\ni }i={1,...,d} consisting of the (key, value) pairs from\nthe previous local context of \u03b4 (positives), along with pairs from d \u2212 1 other contexts coming from\nCprev (negatives). We also experiment with varying the number of previous contexts and negatives\nfor different batch elements.\nThe operation is fully differentiable, and thus, we improve all the (key, value) pairs in p\u03b4. Two,\nthe procedure is easy to implement; it does not require any additional loss (i.e., uses the standard\ntransformer training objective) and is done on the level of the data loading pipeline and a minor\nself-attention change. The only new hyperparameter is d, which prescribes the ratio of positive to\nnegative samples. Typically, we find it beneficial to start with small d \u2264 8 (otherwise, the model tends\nto ignore the previous local context) and later switch to bigger values, say d \u2265 64. Appendix B.3\nprovides more details about the method. Listing 1 outlines an implementation of the crossbatch.\n3.3\nThe distraction issue\n1 4\n8\n16\n32\n64\n0\n0.2\n0.4\n0.6\n0.8\n1\nFoT d=2->64\nFoT d=8\nFoT d=1\nTransformer\n1/d\nNumber of Documents\nPositive Attention Mass\nFigure 3: Distraction issue. We compare FOT trained with\ndifferent values of parameter d to the standard Transformer\nbaseline. During the evaluation, both models see the previous\nlocal context and some contexts from other documents in\nthe chosen layer (as in crossbatch training procedure). For a\ndocument \u03b4 we measure the distribution of attention mass on\np\u03b4. Scale x: the number of contexts from documents that the\nmodel can see. Scale y: avg attention mass to the previous\nlocal context of the current document.\nIn this section, we conceptualize what we\ncall the distraction issue and hypothesize\nit is one of the key problems in dealing\nwith long multi-document contexts (like\nlarge code repositories). Namely, during\nthe standard training, the model is not in-\ncentivized to distinguish the keys from dif-\nferent documents. We measure that the\nattention mass is evenly spread on the re-\nlated and unrelated documents; see Fig-\nure 3. More precisely, for a document \u03b4,\nlet wij be the softmax weights related to\np\u03b4\nij constructed as described in Section 3.2.\nWe define the positive attention mass as\nrd := P\nj w1j/ Pd\ni=1\nP\nj wij.\nWe ob-\nserve that rd \u2248 1/d, which can be in-\nterpreted as the fact that the attention is\nequally distracted by the positive (coming\nfrom the current document at i = 1) and\nnegative keys. This is an undesirable prop-\nerty since when scaling the memory, the attention becomes increasingly distracted. We show that the\ncrossbatch mostly alleviates the distraction issue, resulting in a focused attention. More information\ncan be found in Appendix B.4. In Section 5.3, we also show that the distraction issue has a harmful\neffect on metrics like perplexity.\n5\n4\nLONGLLAMA\n: extending LLaMA\u2019s context length with FOT\nOne of the promises of our work is that FOT can be used to fine-tune already existing\nlarge models to extend their context length.\nIn this section, we show that this is indeed the\ncase.\nWe use OpenLLaMA-3B and OpenLLaMA-7B models trained for 1T tokens as start-\ning points and fine-tune them with FOT. We show that the resulting models, which we call\nLONGLLAMAs, are capable of extrapolating beyond their training context length (even up to\n256K) and retain the performance on short-context tasks.\nWe release the inference code on\nGitHub: https://github.com/CStanKonrad/long_llama and the LONGLLAMA-3B check-\npoint on Hugging Face: https://huggingface.co/syzymon/long_llama_3b. We note that our\ncheckpoint is backward compatible, i.e. can be used with any existing LLaMA inference code (both\nin Hugging Face and other implementations), albeit without long-context capabilities.\n4.1\nExperimental setup\nThe architecture of the models is the same as OpenLLaMAs, see Geng and Liu [2023] and Ap-\npendix A.1. We use L = {6, 12, 18} (resp. L = {8, 16, 24}) as the memory layers for 3B (resp. 7B)\nLONGLLAMA model. We fine-tune the models on 10B (resp. 3B) tokens using FOT, 8k context\nlength and our dataset mixture based on RedPajama [TogetherComputer, 2023], see Appendix A.3.\nThere are three minor differences from the standard FOT procedure. First, we retain the positional\nencodings in the local context of the memory layers (this is not necessary for FOT, but makes our\ncheckpoints fully compatible with any existing LLaMA inference codebase). To be more precise,\nqueries and keys from the local context (up to 2K tokens) receive the standard LLaMA rotary\npositional encoding, whereas memory keys are encoded as if they had position 0 in the local context\nwindow. Second, we use dense attention instead of the kNN retrieval, as we found only marginal\nperformance differences, and it is simpler to implement. Third, we modify the crossbatch training\nprocedure to have more fine-grained control over the number of additional contexts and the ratio\nof positive to negative samples. All these differences are detailed in Appendix A.2.\n4.2\nContext length extrapolation on the passkey retrieval task\nWe first measure the effective context length of LONGLLAMA, namely the distance for which tokens\ncan effectively attend each other. We use passkey retrieval introduced in [Mohtashami and Jaggi,\n2023], a synthetic task designed to measure this property. In this task, the model has to retrieve a\npasskey placed randomly in a long prompt. Results are shown in Figure 1 - importantly, our 3B\nmodel is capable of solving this task much beyond its training context length 8K, achieving 94.5%\naccuracy for prompts of length 100k and 73% for 256k.\n4.3\nQuestion answering over research papers\nIn Table 6 we present the performance on the validation set of Qasper [Dasigi et al., 2021] from\nSCROLLS [Shaham et al., 2022] and compare our results to LongChat 7B [Ma and Zhang, 2023] and\ntwo baseline short-context models. We note that our model shows gains from increased context length.\n4.4\nImproving few-shot learning accuracy with longer context\nWe measure long-context capabilities of these models on two downstream tasks, TREC question clas-\nsification [Li and Roth, 2002, Hovy et al., 2001] and WebQS question answering [Berant et al., 2013].\nWe follow the experimental setup of [Hao et al., 2022]. Namely, we few-shot prompt the models with\nas many demonstration examples as possible up to the given context length. We do not use structured\nprompting like in [Hao et al., 2022] - instead, we directly provide all demonstrations in context.\nWe observe significant accuracy gains from longer contexts on TREC and some improvements on\nWebQS (see Table 1). The TREC dataset consists of 50 classes. A model is tasked to predict the\nclass label given in-context examples. Only 100 examples fit the standard context length (2K); it\nis not unusual that no class example is present for a given question, making the task impossible.\nIncreasing the context length and the number of examples mitigates this risk. Moreover, having more\ndemonstrations of the given class is also likely to be beneficial.\n6\nTable 1: Few-shot in-context learning performance of LONGLLAMA; accuracy on TREC and WebQS. We see\nsignificant gains from the additional context on the TREC dataset. To calculate the results, we average over 20\ntrials for sampling in-context demonstrations from the train set; the resulting confidence intervals for TREC and\nWebQS are smaller than 1% and 0.1%, respectively.\nDataset\nTREC\nWebQS\nContext\nLONGLLAMA 3B\nLONGLLAMA 7B\nLONGLLAMA 3B\nLONGLLAMA 7B\n2K\n67.0\n63.2\n21.2\n25.5\n4K\n71.6\n72.7\n21.4\n26.4\n6K\n72.9\n74.9\n22.2\n27.2\n8K\n73.3\n75.9\n22.4\n27.7\nTable 2: Few-shot in-context learning performance comparison between standard fine-tuning on 4K context\n(baseline) and FoT fine-tuning on the same context length for 1B tokens. On TREC, FOT is able to utilize\nadditional examples beyond its training context length to achieve higher accuracy at 8K context length, which is\nnot possible for the baseline since its context is bounded to 4K.\nDataset\nTREC\nWebQS\nContext\nbaseline\nFoT (ours)\nbaseline\nFoT (ours)\n2K\n52.8\n55.6\n20.7\n20.8\n4K\n57.2\n60.9\n18.7\n21.0\n6K\n\u2013\n61.7\n\u2013\n21.2\n8K\n\u2013\n62.5\n\u2013\n20.7\n4.5\nComparison to standard long-context fine-tuning\nIn this section, we compare FOT to standard long-context fine-tuning, showing that it already achieves\nbetter performance for the context length used for fine-tuning and, importantly, that it can extrapolate\nbeyond this context length, which is not the case for the baseline.\nFor comparisons, we fine-tune two models, one trained with FOT and another one (baseline) with\nstandard fine-tuning (done similarly to [MosaicML, 2023, Nijkamp et al., 2023]). In both cases, we\nuse 3B models fine-tuned on 1B tokens using the 4K context length. We evaluate both models on a\nnumber of few-shot downstream tasks in the setting described in Section 4.4.\nIn most cases, see Table 2, we observe accuracy improvements when more few-shot demonstrations\nare provided in the extended context (from 2K used by OpenLLaMA to 4K used in our fine-tuning).\nOn TREC, the gains from additional context are significant for both models, while on WebQS, the\nstandard fine-tuning baseline does not provide any improvement from extended context. Notably,\nthe model fine-tuned with FOT enjoys further accuracy gains when evaluated with context lengths\nbeyond its training length (6K and 8K). This shows extrapolation capabilities of FOT, which are not\npresent in the baseline (see e.g. Figure 1).\n4.6\nPerformance on short-context tasks\nFine-tuning for longer contexts could hurt performance on the original context length (2K), as the\ntraining data distribution changes. We show that this is not the case for the LONGLLAMA models by\nevaluating them using the LM Evaluation Harness library [Gao et al., 2021a]. On most tasks, the\nperformance is kept intact; see Appendix A.4 for details. This also confirms that LONGLLAMAs\ncould be used as a drop-in replacement of LLaMA models as they are compatible with the original\nLLaMA inference code.\n5\nAnalysis of FOT\nIn this section, we perform extensive experiments on smaller models to analyze and further validate\nour approach. In particular, we answer the following questions: (1) How does FOT perform when\nscaling the context length at inference time? (2) Can FOT be used to extend the context length of\n7\nan existing, pre-trained model? (3) How effectively can it handle distractions, and how does this\ncapability translate to enhanced performance in long-context language modeling tasks? Moreover,\nwe provide ablation studies of our method and additional analysis.\n5.1\nExperimental setup\nArchitecture For experiments described in this section we use decoder-only Transformer [Vaswani\net al., 2017] models with 12 layers and 184M parameters (unless stated otherwise). Following Wu\net al. [2022]; we pick \u2113 = 8 as the memory attention layer. We tune k = 128, the number of top keys\nretrieved by kNN. In most experiments, we start training with a small crossbatch dimension d \u2264 8 and\nswitch to d \u2265 64 after some training. For more details about the architecture and hyperparameters,\nsee Appendix B and Appendix E.\nEvaluation We distinguish two evaluation settings: single-document (abbreviated to single-doc) and\nmulti-document (abbreviated to multi-doc). The single-doc setting is typically used for evaluating\nmodels that process long contexts. Here, we clear the memory for each new document, ensuring\nthat only the current document is available in the context. The multi-doc setting retains memory\nacross multiple documents without resets. This scenario tests whether the model can ignore irrelevant\ninformation and focus on the relevant data, which can be useful in setups like repository-level code\ngeneration.\nDatasets We evaluate on the following long-context language modeling datasets: PG-19 (English\nbooks), arXiv (mathematical papers), GitHub (code), and Isabelle (formal proofs). PG-19 [Rae\net al., 2019] is a large dataset of English-language books published prior to 1919, sourced from the\nProject Gutenberg archive. This dataset is a well-established benchmark for evaluating long-context\nlanguage models [Sun et al., 2021]. The arXiv dataset contains LATEX source of papers labeled as\n\"Mathematics\" that were obtained by downloading articles through the arXiv Bulk Data Access. The\ntoken count per paper in this dataset is comparable to that of a book in PG19. For details on the\nremaining datasets, refer to Appendix H.\n5.2\nFOT fine-tuning and context length extrapolation\nFOT is a minimal modification to the standard transformer architecture; therefore, it is possible to\nfine-tune existing models to endow them with a longer context length via the memory attention layer,\nas we already demonstrated in Section 4. In this section, we deepen this analysis (on a smaller model)\nby studying perplexity improvements on various datasets.\nAs a base model, we use a standard transformer model pre-trained for 100k steps with context of 1K\ntokens using the standard objective and fine-tune with the FOT objective (i.e. crossbatch). The data\nused for both fine-tuning and pre-training is the C4 dataset Raffel et al. [2019a] (we omit documents\nshorter than 2K tokens). The fine-tuning phase takes 10k steps. We use the crossbatch dimension\nd = 128 and local context of 1K tokens (context is 2K during training). We evaluate models in a\nzero-shot way on 4 language modeling datasets, which require long context: arXiv, PG-19, GitHub\nand Isabelle, see Section 5.1 and Appendix E for details.\nIn Table 3, we observe that FOT enjoys steady perplexity gains up to 64K tokens, although it was\nfine-tuned only with the 2K total differentiable context length. We compare the model perplexity to\nthe following baselines: Memorizing Transformer (MT) [Wu et al., 2022] fine-tuned with the local\ncontext of 1K and memory size of 16K, and Transformer-XL [Dai et al., 2019] fine-tuned with both\nlocal context and window length of 1K. To ensure a fair comparison, all three models are fine-tuned\nfrom the same base checkpoint. When evaluated with a context of 2K, our method achieves results on\npar with the Transformer-XL baseline, which has access to the previous context in all layers, unlike\nMT and FOT. Compared to the MT baseline, we achieve better scaling when evaluated with 64K\ncontext length and significantly better perplexity values. Unlike MT, our method does not require\ntraining on long sequences, which is reflected by the lower perplexities of FOT when evaluated in the\nzero-shot setting. For more details, see Appendix G.\nWe also confirm the context extrapolation abilities using a synthetic dictionary lookup task. In this\ntask, the model is first provided with ki : vi mappings and then asked what value is associated with a\nparticular key. We train 37M parameter models using documents of length 512. Figure 10 shows that\n8\nTable 3: Perplexity for different context lengths after fine-tuning a standard transformer model. The model\nis fine-tuned using the FOT objective (i.e., crossbatch) on C4 and evaluated zero-shot varying the context size.\nTransformer-XL [Dai et al., 2019] and Memorizing Transformer [Wu et al., 2022] fine-tuned in the same setting\nare used as baselines.\nMethod\nContext Length\nGitHub\nIsabelle\narXiv\nPG-19\nFOT\n2K\n6.72\n5.63\n8.17\n23.74\n4K\n5.88\n4.93\n7.44\n23.25\n16K\n5.43\n4.51\n6.94\n22.85\n64K\n5.32\n4.44\n6.81\n22.65\nTransformer-XL\n2K\n6.85\n5.76\n8.21\n23.57\nMemorizing Transformer\n2K\n8.10\n7.34\n9.39\n24.03\n4K\n7.55\n6.93\n8.95\n23.62\n16K\n7.27\n6.66\n8.66\n23.32\n64K\n7.26\n6.64\n8.60\n23.24\nFOT, after 5k steps of training, can effectively utilize memory consisting of 16M tokens achieving\naccuracy above 92%. Details can be found in Appendix F.\n5.3\nHandling distractions in language modeling tasks\nIn this section, we measure how handling distractions in the multi-document setting helps in language\nmodeling. We pick the PG-19 dataset [Rae et al., 2019] and measure the perplexity of the next\ntoken prediction (language modeling task) when varying the size of multi-doc memory (in this case\nconsisting of books). Intuitively, the memory tokens corresponding to the current book might be\nbeneficial (which is also confirmed in [Wu et al., 2022]), while the ones from the other books are\nunlikely to be useful and thus are distractions.\nWe observe, see Figure 8, that higher values of the crossbatch dimension d lead to better perplexity.\nThis aligns with the observations in Section 3.3, indicating that by mitigating the distraction issue,\nwe experience benefits in language modeling.\nMoreover, all versions of FOT are able to utilize memory and achieve much better perplexity than the\nstandard Transformer (no memory). Unsurprisingly, perplexity increases with memory size, but we\nstress that this happens gracefully. In the standard variant of FOT (bold line), the perplexity increases\nonly by 0.18 when scaling to > 500k tokens. Importantly, the perplexity of FOT is close to this of\nMemorizing Transformer with the single-doc memory, which we treat as a soft lower bound since it\nis not exposed to distractions from unrelated books.\n5.4\nContext length extrapolation in single-doc\nThe original motivation behind FOT is to improve the multi-doc setting performance by handling\ndistractions. Interestingly, our method also helps to extrapolate to longer contexts, even when\nevaluated in the single-doc setting.\nTo study this, we perform FoT fine-tuning (as in Section 5.2) and evaluate the perplexity of the\nresulting model on the PG-19 dataset with different context lengths in the zero-shot fashion. To\ndeepen the analysis, we introduce an additional parameter w (the number of previous contexts used\nin cross batch training procedure). We provide results for w = 1 (the standard setting for FOT, that\ncorresponds to the total differentiable context being 2 \u00b7 1024) and w = 2 (corresponding to the total\ndifferentiable context 3 \u00b7 1024).\nWe observe, see Figure 9, improvements when context grows, even far beyond the training context\nlength, which reaffirms the hypothesis that FOT helps with extrapolation to longer contexts. Moreover,\nd = 2 is significantly better than d = 1. When comparing d = 1 and w = 2 to d = 2 and w = 1, we\nobserve that the former is slightly better. This is natural, as the former has longer training context.\n9\n5.5\nAblations and design choices\nIn Appendix C we present ablations on our design choices. In particular, we note the importance\nof differentiability and the inclusion of negatives. We also discuss the relation to Memorizing\nTransformer. We note that due to the limited resources we have followed the Memorizing Transformer\nin the choice of memory layers.\n6\nLimitations and future work\nOur research opens a few avenues for future work. We list them as well as challenges and limitations.\nScaling up context This is by far the most important future research direction. The challenges\nstart from purely engineering, storing more than 16M (key, value) pairs will require a distributed\nmulti-node system. In our experiments, we use the exact kNN search, which is not scalable to large\nmemory. Using approximate kNN search will require a lot of engineering effort, as well as careful\nevaluation of the impact of the approximation on the model performance.\nScaling up crossbatch We observed that increasing d is beneficial. In our experiments, we used\nd = 64 or d = 128, which is the maximum value that fits into the memory of a single TPUv3/TPUv2\nmachine, see also Appendix I. In future work, we want to further increase d as well as test on devices\nwith bigger memory or utilize multi-node training. We also note that crossbatch increases the training\ncost, but only in a subset of layers.\nExploring contrastive learning The FOT training is inspired by rather basic contrastive learning\n(CL) techniques. We show that this improves the key structure so that the distraction issue is mitigated.\nWe expect that other CL methods could be beneficial, for example, hard negative mining to utilize a\nlarger memory during training (see [Lindgren et al., 2021]). We leave this for future work.\nCombining with other methods Developing long-context methods is an active research field, see\nSection 2. We believe that some of these methods could be combined with FOT, resulting in mutually\nbeneficial interactions.\nListing 1: Possible implementation of cross-batch. To simplify the code we assume that each\ndocument occupies two consecutive elements of the batch. A more detailed version is in Appendix L.\n# keys from\nother\ncontexts\nwill be encoded as if they\n# were at the\nbeginning of the local\ncontext\npkey_fst = pos_encode_as_first (xk=key)\n# local\ncontext\nkeys\nencoded in the\nstandard\nway\npquery , pkey = pos_encode(xq=query , xk=key)\n# for each\nelement of the batch we calculate\nindices of\n# the batch\nthat will be used in cross -batch\ncross_batch_rel_ids = jnp.arange (0, -num_attentions ,\n-1)\n.reshape (1,\n-1)\nbatch_ids = jnp.arange (0, batch_size ). reshape (-1, 1)\ncross_batch_selector = cross_batch_rel_ids + batch_ids\n# here we want\nother\ncontexts\ncross_batch_keys = pkey_fst[ cross_batch_selector [:, 1:]]\n# here we concatenate\nlocal\ncontext\nwith\nother\ncontexts\nattention_keys =\njnp.concatenate ([ pkey[:, None], cross_batch_keys ], axis =1)\ncb_attn_weights =\njnp.einsum(\"bqhd ,bckhd ->bhqck\",\npquery , attention_keys , precision=precision)\n10\nAcknowledgments and Disclosure of Funding\nWe gratefully acknowledge the TPU Research Cloud program, which was instrumental to our\nresearch by providing significant computational resources. Parts of the project were realized using\nthe resources of Pozna\u00b4nskie Centrum Superkomputerowo - Sieciowe. We would also like to thank\nMarkus Rabe for reviewing the initial manuscript and Christian Szegedy, Charles Staats, and DeLesley\nHutchins for helpful discussions. We are also grateful to Xinyang Geng and Hao Liu for releasing\nOpenLLaMA checkpoints and the EasyLM library [Geng, 2023], allowing for training these models,\nwhich significantly accelerated our research. Piotr Milos was supported by the Polish National\nScience Centre grant 2019/35/O/ST6/03464. Henryk Michalewski was supported by the Polish\nNational Science Center grant UMO-2018/29/B/ST6/02959.\nReferences\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u00f1\u00f3n, Siddhartha Brahma, Yury Zemlyanskiy,\nDavid C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai.\nColt5: Faster long-range transformers with conditional computation. CoRR, abs/2303.09752, 2023.\ndoi: 10.48550/arXiv.2303.09752. URL https://doi.org/10.48550/arXiv.2303.09752.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1533\u20131544, Seattle, Washington, USA, October 2013. Association\nfor Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1160.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Mil-\nlican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\nVinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. Im-\nproving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Confer-\nence on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume\n162 of Proceedings of Machine Learning Research, pages 2206\u20132240. PMLR, 2022.\nURL\nhttps://proceedings.mlr.press/v162/borgeaud22a.html.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165,\n2020. URL https://arxiv.org/abs/2005.14165.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.\norg/abs/2107.03374.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation, 2023.\n11\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npages 1597\u20131607. PMLR, 2020.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nTri Dao,\nDaniel Y. Fu,\nStefano Ermon,\nAtri Rudra,\nand Christopher R\u00e9.\nFlashat-\ntention:\nFast\nand\nmemory-efficient\nexact\nattention\nwith\nio-awareness.\nIn\nNeurIPS,\n2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of\ninformation-seeking questions and answers anchored in research papers, 2021.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network\nfunction approximation in reinforcement learning, 2017.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration\nproblem in training natural language generation models. arXiv preprint arXiv:1907.12009, 2019.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\nTang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language\nmodel evaluation, September 2021a. URL https://doi.org/10.5281/zenodo.5371628.\nLuyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. Scaling deep contrastive learning batch size\nunder memory limited setup. arXiv preprint arXiv:2101.06983, 2021b.\nXinyang Geng. Easylm: A simple and scalable training framework for large language models, 2023.\nURL https://github.com/young-geng/EasyLM.\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https:\n//github.com/openlm-research/open_llama.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\nYinfei Yang.\nLongt5: Efficient text-to-text transformer for long sequences.\narXiv preprint\narXiv:2112.07916, 2021.\nYaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting:\nScaling in-context learning to 1, 000 examples. CoRR, abs/2212.06713, 2022. doi: 10.48550/\narXiv.2212.06713. URL https://doi.org/10.48550/arXiv.2212.06713.\nAdi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without\npositional encodings still learn positional information, 2022.\nAlex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key\nnormalization for transformers. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of\nthe Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November\n2020, volume EMNLP 2020 of Findings of ACL, pages 4246\u20134253. Association for Computational\nLinguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.379. URL https://doi.org/10.\n18653/v1/2020.findings-emnlp.379.\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward\nsemantics-based answer pinpointing. In Proceedings of the First International Conference on\nHuman Language Technology Research, 2001. URL https://www.aclweb.org/anthology/\nH01-1069.\nNihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan,\nRamesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, and Bing Xiang. Contraclm:\nContrastive learning for causal language model, 2023.\n12\nAlbert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzyg\u00f3\u00b4zd\u00b4z,\nPiotr Mi\u0142o\u00b4s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language\nmodels and automated theorem provers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=fUeOyt-2EOp.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus, 2017.\nJean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J. Kusner. No train no gain:\nRevisiting efficient training algorithms for transformer-based language models, 2023.\nkaiokendev. Things i \u00b4m learning while training superhot. https://kaiokendev.github.io/til#\nextending-context-to-8k, 2023.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,\n2019.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von\nWerra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371,\nBrussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nD18-2012. URL https://aclanthology.org/D18-2012.\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai\nWu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems\nwith language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,\neditors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=IFXTZERXdM7.\nXin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International\nConference on Computational Linguistics, 2002. URL https://www.aclweb.org/anthology/\nC02-1150.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,\nCyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,\nSven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814, 2022. doi: 10.48550/arXiv.2203.07814.\nErik Lindgren, Sashank Reddi, Ruiqi Guo, and Sanjiv Kumar. Efficient training of retrieval models\nusing negative cache. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-\nman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages\n4134\u20134146. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2021/file/2175f8c5cd9604f6b1e576b252d4c86e-Paper.pdf.\nDacheng Li Rulin Shao Anze Xie Ying Sheng Lianmin Zheng Joseph E. Gonzalez Ion Stoica Xuezhe\nMa and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023.\nURL https://lmsys.org/blog/2023-06-29-longchat.\nMaciej Miku\u0142a, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu Jiang, Jin Peng Zhou,\nChristian Szegedy, \u0141ukasz Kuci\u00b4nski, Piotr Mi\u0142o\u00b4s, and Yuhuai Wu. Magnushammer: A transformer-\nbased approach to premise selection, 2023.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. CoRR, abs/2305.16300, 2023. doi: 10.48550/arXiv.2305.16300. URL\nhttps://doi.org/10.48550/arXiv.2305.16300.\n13\nMosaicML. Introducing mpt-30b: Raising the bar for open-source foundation models, 2023. URL\nwww.mosaicml.com/blog/mpt-30b. Accessed: 2023-06-22.\nPiotr Nawrot, Szymon Tworkowski, Michal Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy,\nand Henryk Michalewski. Hierarchical transformers are more efficient language models. CoRR,\nabs/2110.13711, 2021. URL https://arxiv.org/abs/2110.13711.\nPiotr Nawrot, Jan Chorowski, Adrian \u0141a\u00b4ncucki, and Edoardo M. Ponti. Efficient transformers with\ndynamic token pooling, 2023.\nErik Nijkamp, Hiroaki Hayashi, Tian Xie, Congying Xia, Bo Pang, Rui Meng, Wojciech Kryscinski,\nLifu Tu, Meghana, Chen Xing, Jesse Vig, Lidiya Murakhovs\u2019ka, Chien-Sheng Wu, Silvio Savarese,\nYingbo Zhou, Shafiq Rayhan Joty, and Caiming Xiong. Long sequence modeling with xgen: A\n7b llm trained on 8k input sequence length. Salesforce AI Research Blog, 2023. URL https:\n//blog.salesforceairesearch.com/xgen-7b/.\nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.\nCoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.\nCompressive transformers for long-range sequence modelling.\narXiv preprint, 2019.\nURL\nhttps://arxiv.org/abs/1911.05507.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. arXiv e-prints, 2019a.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. CoRR, October 2019b.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large\nlanguage models, 2023.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. Scrolls: Standardized comparison over long language\nsequences, 2022.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/\n2104.09864.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\nmodels actually use long-range context?, 2021.\nTogetherComputer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL\nhttp://arxiv.org/abs/1706.03762.\n14\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=\nTrjbxzRcnf-.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization, 2019.\nZexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In\nYoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 5657\u20135673. Association for Computational Linguistics,\n2022. URL https://aclanthology.org/2022.emnlp-main.382.\n15\nBroader Impact\nRecent rapid developments in language models have brought a lot of new capabilities. At the same,\nthese raised concerns about the social impact and very animated discussions in the community. Our\nwork develops a generic technique, which in principle, could be applied to virtually any language\nmodel and thus, by extending their capabilities, exacerbate threats. We note, however, that FOT does\nnot create any new threats. Thus, we refer to the existing body of knowledge on the broader impact\nof language models, see e.g. Borgeaud et al. [2022].\nA\nLONGLLAMA\nA.1\nArchitecture\nOpenLLaMA [Geng and Liu, 2023] is an open-source reproduction of LLaMA [Touvron et al., 2023].\nIt uses a decoder-only architecture with rotary positional embeddings, and a few changes including\npre-normalization with RMSNorm [Zhang and Sennrich, 2019], and SiLU activation [Elfwing et al.,\n2017]. A SentencePiece [Kudo and Richardson, 2018] tokenizer with 32k vocabulary size is used.\nA.2\nExtending context length with FOT\nPositional encodings To achieve backward compatibility with the original LLaMA, we retain\npositional encodings in the local context. The tokens outside the local context are assigned the same\nposition as the first token in the local context.\nDense attention to longer context To make the implementation simpler and less dependent on\nexternal software, we resign from using kNN lookup and perform attention over the whole memory.\nWe have found only marginal performance differences between those two approaches to memory\nattention.\nCrossbatch details For the 3B LONGLLAMA model, we set L = {6, 12, 18} as the memory layers.\nWe vary the number of additional contexts d \u2208 {0, 2, 3} across elements of the batch by dividing\nbatch entries into four segments of equal size. Elements from the first segment only see local context\n(d = 0). Elements from the second segment see two additional contexts (d = 2), one from the same\ndocument (positive) and one from a different one (negative). Elements from the third segment see\nthree additional contexts, two positives, and one negative. The last segment consists of elements\nexposed to three additional contexts coming from the same document. We abbreviate this setup as\n1\n4(0, 0), 1\n4(1, 1), 1\n4(2, 1), 1\n4(3, 0).\nFor the 7B LONGLLAMA model, we set L = {8, 16, 24} as the memory layers. Here we divide\nbatch entries into four segments and use the following setup: 1\n4(0, 0), 1\n4(1, 2), 1\n4(2, 5), 1\n4(3, 4).\nHyperparameters\nWe follow the choices of OpenLLaMA with respect to most of the hyperparam-\neters, including using the same optimizer. During fine-tuning, we use a batch size of 256K tokens\nand constant learning rate of 2e\u22125, which is lower than the learning rate at the end of OpenLLaMA\ntraining (3e\u22125 after 1T tokens), and weight decay of 0.01.\nA.3\nLLaMA fine-tuning dataset\nWe use a mixture based on RedPajama [TogetherComputer, 2023] and The Stack [Kocetkov et al.,\n2022] with the following proportions of each subset:\nAll subsets apart from python are taken directly from RedPajama. For the python subset, we gather\nPython source code from The Stack and, to obtain long documents for training, concatenate files that\nare in the same subdirectory in random order, using a similar procedure as for the GitHub dataset in\nSection H. Additionally, we filter out short documents for some subsets of the original RedPajama,\nnamely shorter than the Min. doc. length column indicates.\nIn case one document is too short to span across several contexts for crossbatch, then we concatenate\nit with the next document from the dataset.\n16\nTable 4: Proportions of RedPajama subsets for the LONGLLAMA fine-tuning mixture. For python\nsubset, data from The Stack is used (see text for details). We only train on documents with length\nbeing at least Min. doc. length, if specified (otherwise we train on all documents from that subset).\nThe horizontal line separates long-context and short-context subsets.\nSubset\nSampling proportion (%)\nMin. doc. length\narxiv\n25\n-\npython\n25\n4096\nbook\n10\n-\ncommon_crawl\n29\n-\nc4\n5\n1024\ngithub\n2\n2048\nstackexchange\n2\n1024\nwikipedia\n2\n1024\nA.4\nLanguage Model Evaluation Harness\nTo ensure that the performance of LONGLLAMAs has not degraded in short context scenarios, we\nevaluate our models on the Language Model Evaluation Harness benchmark [Gao et al., 2021a].\nTable 5 compares our results with OpenLLaMA [Geng and Liu, 2023]. Similarly to the authors of\nOpenLLaMA, we omit CB and WSC tasks.\nTable 5: Model comparison across different tasks/metrics on Language Model Evaluation Harness.\nThe LONGLLAMA models were evaluated without context extension (i.e. as standard OpenLLaMA\nmodels). Results indicate that LONGLLAMAs maintain good performance in short context scenarios.\nTask/Metric\nOpenLLaMA\n3B\nLongLLaMA\n3B\nOpenLLaMA\n7B\nLongLLaMA\n7B\nanli_r1/acc\n0.33\n0.32\n0.33\n0.35\nanli_r2/acc\n0.32\n0.33\n0.36\n0.37\nanli_r3/acc\n0.35\n0.35\n0.38\n0.36\narc_challenge/acc\n0.34\n0.34\n0.37\n0.37\narc_challenge/acc_norm\n0.37\n0.37\n0.38\n0.38\narc_easy/acc\n0.69\n0.68\n0.72\n0.70\narc_easy/acc_norm\n0.65\n0.63\n0.68\n0.66\nboolq/acc\n0.68\n0.68\n0.71\n0.71\nhellaswag/acc\n0.49\n0.48\n0.53\n0.52\nhellaswag/acc_norm\n0.67\n0.65\n0.72\n0.71\nopenbookqa/acc\n0.27\n0.28\n0.30\n0.30\nopenbookqa/acc_norm\n0.40\n0.38\n0.40\n0.41\npiqa/acc\n0.75\n0.73\n0.76\n0.75\npiqa/acc_norm\n0.76\n0.75\n0.77\n0.76\nrecord/em\n0.88\n0.87\n0.89\n0.89\nrecord/f1\n0.89\n0.87\n0.90\n0.90\nrte/acc\n0.58\n0.60\n0.60\n0.59\ntruthfulqa_mc/mc1\n0.22\n0.24\n0.23\n0.24\ntruthfulqa_mc/mc2\n0.35\n0.38\n0.35\n0.35\nwic/acc\n0.48\n0.50\n0.51\n0.50\nwinogrande/acc\n0.62\n0.60\n0.67\n0.67\nAverage score\n0.53\n0.53\n0.55\n0.55\nA.5\nQuestion answering over research papers\nWe evaluate the context utilization of our model on the validation set of Qasper [Dasigi et al., 2021]\nfrom SCROLLS [Shaham et al., 2022]. Details are in the Table 6.\n17\nTable 6: Zero-shot performance with different context lengths on the validation subset of Qasper [Dasigi et al.,\n2021]. We use the implementation from Language Model Evaluation Harness [Gao et al., 2021a]. In the Harness\nimplementation, yes/no questions are evaluated separately from open questions. Observe that LONGLLAMA\n3B benefits from the extended context.\nContext length\nOpenLLaMA 3B\nLONGLLAMA 3B\nLLaMA 7B\nLongChat 7B\n2K\n18.7\n18.7\n18.7\n19.4\n4K\n-\n20.7\n-\n21.2\n6K\n-\n23.2\n-\n25.0\n8K\n-\n26.6\n-\n28.8\nB\nArchitecture\nThis section describes the architecture and crossbatch details for non-LLaMA-based models presented\nin this paper. The main differences are that for LLaMA-based models (LONGLLAMA) we maintain\nthe positional encodings (with a slight modification detailed in A.2), do not introduce the attention\ntemperature parameter, and replace kNN with full dense attention.\nB.1\nTransformer models\nFor non-LLaMA-based models we use the transformer architecture introduced in [Vaswani et al.,\n2017] with a few standard changes. First, we use only the decoder without the encoder part. Secondly,\nwe perform layer normalization before the input of both the attention and feed-forward modules.\nAdditionally, we use Rotary Position Embedding [Su et al., 2021], normalize keys and queries [Henry\net al., 2020], and introduce a learnable temperature parameter for each attention head.\nThe hyperparameters for each model size can be found in Appendix E. For training the models on\nPG-19, we use the standard T5 tokenizer with 32k vocabulary [Raffel et al., 2019b]. The larger\nmodels in Section 5.2 are trained with a custom SentencePiece tokenizer [Kudo and Richardson,\n2018] with 64k vocabulary size.\nB.2\nMemory attention layer\nMemory attention layer \u2113 is one of the transformer layers, which has access to the additional context\nM. The memory stores (key, value) pairs. For each query q in \u2113, we retrieve the k most matching\nentries from M and use them to compute the attention value. More precisely, we use the kNN\nalgorithm to pick Mtop := {(key1, value1), . . . , (keyk, valuek)} \u2282 M such that {\u27e8q, keyi\u27e9}i=1,...,k\nare the top k inner products in M. These are merged with the part of the local context before q\ndenoted as C<q and used to compute the attention value using the standard Transformer formula:\nv :=\nX\n(key,v)\u2208Mtop\u222aC<q\ns(key) \u00b7 v,\n(1)\nwhere s(key) is the softmax score for key. This softmax is calculated as follows:\nsoftmax\n \u0014\u27e8q, key\u27e9\n\u03c4\n\u0015\nkey\u2208Mtop\u222aC<q\n!\n,\nwhere \u03c4 is a temperature parameter. In this approach, we do not distinguish between the local context\nand the memory.\nAnother way of integrating Mtop is via gating. In this approach, we separately compute the attention\nvalue vM for Mtop and for the local context vC (using the standard Transformer formula). Then we\nuse a gating mechanism to combine them:\nv := vM \u00b7 g + vC \u00b7 (1 \u2212 g),\ng = \u03c3(bg),\nwhere \u03c3 is the sigmoid function and bg is a trainable bias. The gating approach was proposed in [Wu\net al., 2022], see formula [Wu et al., 2022, (2)].\n18\nWe found our approach, i.e. using (1), to be equally effective, see Figure 4. At the same time, (1) is\nsimpler and does not require additional parameters. Thus, we use it in our experiments.\nFor kNN lookup, we use the exact kNN search implemented in FAISS [Johnson et al., 2017]. The\nmemory attention layer does not use positional encodings. The memory is populated incrementally\nwith (key, value) pairs processed by \u2113 beforehand. In the single-doc setting, the memory is erased\nafter each document.\nWe do not use the \u03c4 parameter for LONGLLAMAs as their architecture does not normalize keys\nand queries. For LONGLLAMAs, we also replace the kNN search with dense attention and retain\npositional encodings (see Appendix A.2).\nB.3\nCrossbatch training procedure\nIn FOT we choose a subset L of the attention layers for later augmentation with the memory of\n(key, value) pairs. Let \u2113 an attention layer from L. During the training we expose this layer to a\nmixture of (key, value) pairs from the current local context, Ccurr, and the previous local context\nand d \u2212 1 contexts from other documents, Cprev; see also Figure 2 for an illustration. We achieve\nthis by modifying the input pipeline so that each batch index corresponds to a different document\n(the batch index occupied by each document is fixed from the moment we load the document till we\nfinish processing it).\nMore specifically, we embed the previous and the current local context for each document in the batch.\nThen we use Cprev as a source of the previous local context for a given document and d \u2212 1 contexts\nfrom other documents. For each element of the batch, the choices of those d additional contexts are\nfixed. We disable positional encoding in \u2113, as we envision it to handle global information.\nTo be more precise, for each document \u03b4 within the batch and query q from the layer \u2113 we create\nthe set p\u03b4 consisting of (key, value) pairs from the previous local context of document \u03b4 along with\npairs from d \u2212 1 contexts gathered from Cprev. The attention value for q is given by\nv :=\nX\n(key,v)\u2208p\u03b4\u222aC\u03b4,<q\ncurr\ns(key) \u00b7 v,\n(2)\nwhere C\u03b4,<q\ncurr consists of (key, value) pairs that preceded q in its local context and s(key) is the\nsoftmax score for key. We use softmax with learnable temperature \u03c4:\nsoftmax\n \u0014\u27e8q, key\u27e9\n\u03c4\n\u0015\nkey\u2208p\u03b4\u222aC\u03b4,<q\ncurr\n!\n.\nNote that the only difference between (1) and (2) is the source of the additional (key, value) pairs: p\u03b4.\nThis, in particular, implies that all the operations with respect to the previous context are differentiable.\nThe number of different documents is equal to bS (the batch size, i.e. each document has a separate\nindex in the batch). Assume that document \u03b4 has index i. We include into p\u03b4 all tokens from Cprev\nwith the batch indices in {i, (i + 1) mod bs, . . . , (i + d \u2212 1) mod bs}.\n19\n100k\n200k\n300k\n400k\n0\n5\n10\n15\n20\n25\nNo Gating\nSigmoid Gating\nStep\nSingle-doc Perplexity on PG-19\nFigure 4: Perplexity (on the test set) during training on PG-19. We train two Memorizing Transformer\nmodels [Wu et al., 2022], one with original gating and one without (i.e. memory attention shared\nwith local attention as described in (1)). We use the single-doc setting with 16k memory.\nB.4\nQualitative analysis\nTable 7 provides a brief qualitative analysis of FOT. It shows that the model can handle distractions\nand retrieve the parts of the character name from the multi-document memory in the PG-19 task\ndataset and appropriate definitions from the large dictionary (dictionary lookup task).\nB.5\nMemorizing Transformer\nThe Focused Transformer shares many similarities with the Memorizing Transformer [Wu et al.,\n2022]. In this section, we summarize the differences between those two models.\nTraining\nThe key difference between these two methods lies in the training procedure. Our method\nuses crossbatch, see Section B.3, which, in a nutshell, is the standard transformer training objective,\nbut we additionally attend to tokens from the previous context window, both from the same and\ndifferent documents, see Appendix B.3 for details. The Memorizing Transformer trains on tokens\nretrieved from the same document (it was envisioned for single-doc).\nThis has a few important consequences:\n\u2022 FOT does not use memory during training, while MT does. This may result in faster training;\nmoreover, FOT always uses the most up-to-date values, while MT uses the values from\nmemory, which may be outdated.\n\u2022 FOT is differentiable through all (key, value) pairs, while MT does not differentiate through\nthe retrieved tokens. We argue that this is key for joint training of well-structured key, value,\nand query embeddings and, consequently, good model performance.\n\u2022 FOT does not require long documents in the training set, while MT does in order to capture\nlong dependencies in memory. This is practically important, as many popular datasets\nconsist of short documents.\nWe speculate that there may be benefits in blending these two approaches. One can, for example,\nargue that MT is better at providing \u2019hard\u2019 negatives for the model. We provide a proof-of-concept\nexperiment in Appendix C.3, and leave this for future work.\nInference\nBoth models use a very similar memory attention layer. The difference is how the\nretrieved (key, value) pairs are integrated. FOT treats the retrieved information in the same way as\nthe local context. MT uses a gating mechanism. Details are provided in Section B.2.\n20\nTable 7: Example of elements retrieved by kNN search along with their scores on PG-19 and dictionary lookup\ntask. The first column shows the token associated with a particular query (bolded) along with fragments of its\ncontext. The second column shows tokens associated with the keys retrieved by kNN for this query. The kNN\nscore shows what fraction of the attention mass dedicated to retrieved keys corresponds to a particular key. The\nFocus score is calculated by taking the key along with 32 preceding and 32 following keys, calculating attention\nweights for them, and checking what fraction of attention the retrieved key gets. In the PG-19 setting the model\nwas equipped with the memory of size 4096 spanning across parts of 8 documents. In the dictionary lookup\nsetting the model was provided with memory of size 16M.\nText\nkNN Results\nkNN\nScore\nFocus\nScore\nPG-19\nThen if we\u2019re here with the closed carriage\nat ten\u2013! [They go together into the library.\nDARBEY. [To SHE BA .]\n0.69\n0.99\nS HE BA takes the gold-rimmed pince-nez\nwhich hangs upon THE DEAN\u2019S waistcoat\nand places it before his eyes\nOh! [He sinks on to the settee with a vacant\nstare, his arms hanging helplessly. DAR-\nBEY. [To SHE BA .] There\u2013now his career\nis a burden to him!\n0.23\n0.92\nPapa! SHE BA . Papsey! [THE DEAN\nrouses himself, discovers his children and\nremoves his hat.\n0.025 0.92\nThen if we\u2019re here with the closed carriage\nat ten\u2013! [They go together into the library.\nDARBE Y . [To SHEBA.]\n0.71\n0.99\nTHE DEAN gives DAR BE Y a severe\nlook, and with an important cough walks\ninto the Library. The men and the girls\nspeak in undertones.\nOh!\n[He sinks on to the settee with a\nvacant stare, his arms hanging helplessly.\nDARBE Y . [To SHEBA.] There\u2013now his\ncareer is a burden to him!\n0.19\n0.99\nOh, Salome! Papa! Papa! TARVER. The\nDean? DARBE Y . The Dean!\n0.08\n0.99\nDictionary Lookup Task\n<q> 14 42 23\n38\n<v> 40 41 05 56\n<k> 14 42 23 38 <v>\n40\n41 05 56\n0.96\n0.99\n<q> 30 55 07 23 <v>\n36\n17 26 63\n<k> 30 55 07 23 <v> 36\n17\n26 63\n0.88\n1.0\n<q> 10 41 26 39 <v> 48\n38\n11 24\n<k> 10 41 26 39 <v> 48 38\n11\n24\n0.87\n0.99\nC\nAblations\nIn this section, we focus on two key properties of crossbatch training procedure: differentiability\nand the inclusion of negatives. We also discuss the relation to Memorizing Transformer in terms\nof the training protocol and memory integration. We refer to Appendix B.5 for a detailed technical\ndescription of differences between FOT and Memorizing Transformer.\nC.1\nImpact of differentiable keys and values\nWe compare FOT to Memorizing Transformer, which uses a non-differentiable memory of keys and\nvalues during training. In the multi-doc experiment presented in Figure 5, both MT and FOT are\ntrained with local context of 512. We observe that FOT is significantly better when the context is\nexpanded during inference, which confirms that differentiable keys and values are beneficial.\n21\nTable 8: Perplexity on PG-19 in the single-doc setting for various local context lengths during training. In these\nexperiments, we used the same context length both during training and evaluation.\nContext Length\nFOT d=1\nMT\n512\n14.18\n14.68\n1024\n14.17\n14.46\n2048\n14.11\n14.43\n0\n100k\n200k\n300k\n400k\n500k\n14.2\n14.4\n14.6\n14.8\n15\n15.2\n15.4\nMulti-doc MT\nFoT d=8\nTokens in Context\nMulti-doc Perplexity on PG19\nFigure 5: Perplexity on PG-19 in the multi-doc set-\nting. Both FOT and Multi-doc MT were trained with\nlocal context of size 512. During training, Multi-doc\nMT utilized memory of size 4096 shared across 8 doc-\numents. Differentiability of keys and values results in\nbetter perplexity.\n0\n100k\n200k\n300k\n400k\n500k\n14\n14.5\n15\n15.5\n16\n16.5\n17\n17.5\n18\nSingle-doc MT\nFoT d=1\nFoT d=2->64\nTokens in Context\nMulti-doc Perplexity on PG19\nTransformer (no mem)    \nSingle-doc MT (Single-doc eval)    \nFigure 6:\nImportance of negatives in the multi-\ndocument setting. We compare FOT trained with\nd = 1 to the one that started with d = 2 and later\nswitched to d = 64. For additional comparison, we\nshow the performance of MT trained with the memory\nof size 16K.\nWe also check whether differentiable keys and values can improve the performance in the single-doc\nsetting. For this, we compare FoT with d = 1 to MT with memory consisting of the previous local\ncontext. Table 8 confirms that differentiable keys and values can also help in this scenario.\nC.2\nImportance of negatives\nWe reaffirm the importance of negatives in a multi-document setting. In previous experiments in\nFigure 3, we already observed that increasing the number of negatives (i.e., increasing d) results in\nmore attention mass being dedicated to relevant tokens. In Figure 6, we additionally show that the\nlack of negatives in training (d = 1) results in a significant deterioration in model perplexity when\nthe context length grows. This confirms that both using negatives and differentiability are important\nfor FOT to work well.\nC.3\nRelation to Memorizing Transformer\nMemorizing Transformer Wu et al. [2022] is closely related to our method. The two key differences\nare 1) the training protocol and 2) how the memory is integrated into the model. In this section, we\nprovide additional insights into these differences.\nTraining protocol In the previous sections, we have discussed the benefits of the crossbatch training,\nnamely using the contrastive-inspired objective and backpropagating through the previous context. A\npotential advantage of the MT approach is that it is exposed to the whole memory during training\n(instead of just the previous context). We performed a proof-of-concept experiment combining the two\napproaches to explore this further. Namely, we trained the model for 499k steps using crossbatch and\nfine-tuned it with the MT objective for 1k steps. Interestingly, we observed a significant improvement\ncompared to the MT training with the same step budget, see Figure 7. We believe there is further\nroom to explore various training protocols combining the best of both worlds.\nMemory integration FOT uses a simple memory integration approach where the (key, value) pairs\nretrieved by kNN lookup are treated the same way as the local context. In contrast, MT uses a\ngating mechanism, a weighted average of the memory, and local values; see details in Appendix\nB.2. We evaluated both approaches and found no difference in performance between these two\nmemory integration methods. However, we decided to use our approach because it does not require\n22\n0\n10k\n20k\n30k\n40k\n50k\n60k\n14\n14.2\n14.4\n14.6\n14.8\n15\n15.2\nSingle-doc MT 16k\nSingle-doc MT 65k\nFoT d=2 fine-tuned on 65k\nTokens in Context\nSingle-doc Perplexity on PG19\nFigure 7: Single-doc eval of FOT finetuned for 1k steps on non differentiable memory. It achieves lower\nperplexity than MT, which has access to this memory for the whole training (500k steps).\nany architectural changes (and thus makes fine-tuning existing models easy). For these reasons, we\nrecommend using it. We speculate that the reason why the gating is not needed in FOT is another\nbenefit of the fact that the crossbatch training backpropagates through the (key, value) pairs from\nthe previous context Cprev in contrast to MT that cannot backpropagate there and needs to rely on\nlocal context when computing gradients for keys and values. Another reason might be the fact that\nCprev is embedded for each batch, and thus staleness (see [Wu et al., 2022, Section 3.2]) is avoided.\nD\nAdditional figures\n0\n100k\n200k\n300k\n400k\n500k\n14\n14.5\n15\n15.5\n16\nSingle-doc MT\nFoT d=2\nFoT d=4\nFoT d=8\nFoT d=2->64\nTokens in Context\nMulti-doc Perplexity on PG19\nTransformer (no mem)    \nSingle-doc MT (Single-doc eval)    \nFigure 8: Perplexity in the multi-doc setting. FOT was trained with local context of size 512 and different d.\nFOT 2->64 started with d = 2 and then switched to d = 64. Single-doc MT was trained with a memory size of\n16K. As we increase the memory size, the number of distractions (irrelevant keys) increases, making the task\nharder. Single-doc MT evaluated in the single-doc setting is a soft lower bound since it lacks distractions.\n0\n10k\n20k\n30k\n40k\n50k\n60k\n22.4\n22.6\n22.8\n23\n23.2\n23.4\n23.6\n23.8\n     \nd=2, w=2\nd=2, w=1\nd=1, w=2\nd=1, w=1\nTokens in Context\nSingle-doc Perplexity on PG19\nFigure 9: Zero-shot performance on PG19 of FOT pretrained on C4. Model fine-tuned with the crossbatch\ndimension d = 2 outperforms the one with d = 1. Using the double (w = 2) training context of 2048 is\nbeneficial.\n23\nE\nHyperparameters\nTable 9 shows hyperparameters used in our experiments. We used context length 512 unless stated\notherwise. In Appendix F, Section 5.3, Section 5.5, we use the total batch size of 32K tokens. In\nSection 5.2 and Section 5.4, the total batch size is 128K tokens.\nTable 9: Hyperparameters for different model sizes. The batch size is given in number of tokens. For the 37M\nmodel local context length was 256 for FOT and 512 for the baseline.\nHyperparameter\nValue\nCommon\n#Layers\n12\nIndex of memory attention layer (l)\n8\nOptimizer\nAdaFactor\nLearning rate schedule\nInverse Square Root\nWarmup steps\n1000\n\u03b21\n0.9\nModel-specific\n#Params\n37M\n184M\nMax learning rate\n0.02\n0.01\nMin learning rate\n0.01\n0.0005\nEmbedding dim\n512\n1024\nHead dim\n64\n128\n#Heads\n8\n8\nFeedForward dim\n2048\n4096\nLocal context length\n256/512\n512\nBatch size\n32K/64K\n32K\n#Number of training steps\n5k\n500k\nFor the experiments described in Section 5.3 and Section 5.5 we performed the following hyperpa-\nrameter sweeps:\n\u2022 Learning rate: {1e\u22122, 5e\u22123, 3e\u22123, 1e\u22123}, chosen: 1e\u22122,\n\u2022 Batch size: {8K, 16K, 32K}, chosen: 32K.\nFor the dictionary lookup task (Appendix F) we checked the following hyperparameters:\n\u2022 Learning rate: {4e\u22122, 2e\u22122, 1e\u22122, 5e\u22123, 3e\u22123}, chosen: 2e\u22122,\n\u2022 Number of dictionary tokens in training step: {16K, 32K}, chosen: 32K. Note that during\nthe training number of document tokens dedicated to the dictionary is the same as the\nnumber of tokens dedicated to questions.\nFor most of the other hyperparameter choices, we followed [Wu et al., 2022], to provide a fair\ncomparison.\nE.1\nSchedule of d\nIn Sections 5.3, 5.4 and 5.5 for models with d \u2208 {1, 2, 4, 8} we used constant schedule, and for\nmodels with d = 64 we trained with d = 2 for 450k steps and switched to d = 64 for the final 50k\n24\nsteps. In Appendix F we trained with d = 1 until the model reached 98% accuracy and then we\nswitched to d = 128. For the 184M model in Section 5.2, we randomly sampled d from {2, 128} in\neach training step.\nF\nDictionary lookup task\n256 1024\n4K\n16K\n64K 256K 1M\n4M\n16M\n0\n0.2\n0.4\n0.6\n0.8\n1\nFoT\nTransformer\nTokens\nAccuracy\nFigure 10: Accuracy vs number of dictionary tokens in a dictionary look-up task. The task format is as follows:\n<k>k1<v>v1<k>k2<v>v2...<k>kn<v>vn<q>ki<v>vi..., where a dictionary is provided, followed by queries\non randomly selected keys. Accuracy is determined by measuring the predicted values vi after <q> tokens.\nModels were trained on examples containing 512 tokens and evaluated with an extended context length. FOT\ndemonstrates high accuracy even when the memory size is large. The baseline transformer fails already for 16K\ntokens. Error bars represent the minimum and maximum on 10 seeds.\nWe propose a dictionary lookup task to test whether the model trained using our crossbatch method\ncan utilize a large memory database. Documents in this task are split into two parts. The first part\ndefines keys and their associated values using the records of the format:\n<k>, k1, k2, k3, k4, <v>, v1, v2, v3, v4,\nwhere <k> is a special token that denotes the beginning of the defining sequence,\nThe second part consists of queries about the values associated with the previously defined keys. The\nqueries are in the following format:\n<q>, k1, k2, k3, k4, <v>, v1, v2, v3, v4,\nwhere <q> is a special token that denotes the beginning of the query. We mask the loss so that for\nsuch a question, only v1, v2, v3, v4 are included. We use a vocabulary of 64 tokens, keys and values\nare described using 4 tokens.\nDuring training, we use documents comprising 512 tokens. The first half of each document consists\nof definitions, whereas the second one consists of questions. For FOT, we use a local context of 256,\nthus the model needs to use the memory attention layer to answer the questions correctly. We start\nwith d = 1 and increase to d = 128 as soon as the model is able to reach 98% training accuracy.\nDuring the inference, we use k = 32 (the number of keys retrieved by kNN). As a baseline, we use a\nstandard transformer model trained with the context length of 512. In evaluation, we test different\nlocal context lengths, which quickly leads to very poor results.\nIn evaluation, we use longer documents but make only the last 256 tokens correspond to questions.\nThat is, as the context gets bigger (the token axis on Figure 10), the number of definitions increases,\nbut the number of queries remains the same.\nG\nFOT fine-tuning\nFor comparison in Table 3, our model is pre-trained for 100k steps with a total batch size of 128\n(128K tokens per step, with 1024 local context). Then we fine-tune both FOT and baselines for\nadditional 10k steps with the same batch size. When fine-tuning FOT, we randomly sample d from\n{2, 128} in each training step to prevent the model from overfitting to a large additional context\nlength during training.\n25\nH\nDatasets\nSection 5.1 outlines essential details concerning the PG-19 and arXiv datasets employed in this study.\nNow, we will present details about the remaining datasets:\nGitHub\nWe obtained a large corpus of permissively licensed Github repositories using BigQuery.\nBy filtering for specific file extensions (C, C++, Java, Python, Go, and TypeScript), we captured\nindividual source code files that are often short but have numerous dependencies and cross-references\nwithin the repository. To preserve the structure while shuffling the files and subdirectories in a\nrandom order, we concatenated all the files within each repository, treating subdirectories as a unit,\nsimilarly to Wu et al. [2022].\nIsabelle\nThe Isabelle corpus comprises formal mathematical proofs in the form of theories written in\na formal language. We combined theories from The Archive of Formal Proofs (from October 2021) 3\nand the Isabelle standard library to create a corpus of theories licensed as open source. Each theory\nfocuses on topics like foundational logic, advanced analysis, algebra, or cryptography and consists\nof multiple files containing proofs. Similar to the GitHub corpus, the files within each theory are\nconcatenated into a single document. However, unlike the Github corpus, we arrange the files based\non their import dependencies, ensuring that later files can utilize sub-theorems proven in earlier files.\nI\nHardware and technical details\nWe used TPU virtual machines from the Google Cloud Platform (GCP). Each TPU virtual machine\nhas 8 TPUv2 / TPUv3 cores totaling 64GB / 128GB of device memory, 96 CPU cores, and over\n300GB of RAM. In larger-scale experiments (Section 5.2) we used machines with 32 TPUv3 cores.\nFor training the LONGLLAMA checkpoints, a TPUv3-128 pod provided by the TPU Research Cloud\nwas used, which we gratefully acknowledge.\nJ\nRandomness\nTo evaluate the significance of our results, we conducted multiple runs for selected experiments in\nour study. In Figure 10, we calculate error bars, showing the minimum and maximum value over 10\nruns of the same experiment. For the arXiv baseline experiment in Appendix K, we performed three\nruns with different random seeds and calculated their standard deviation, which is equal to 0.002\nperplexity. However, due to resource constraints, we were unable to conduct multiple runs for all\nexperiments. Our preliminary findings indicate that the observed variance was minimal compared to\nthe impact observed from other factors under investigation.\nFor the calculation of test perplexities, we used 1M tokens.\nK\nAdditional experimental results\nThis section presents additional empirical results, providing a detailed comparison of FOT with the\nMemorizing Transformer [Wu et al., 2022] baseline. Both models are trained for the same number\nof 500k steps with local context of 2K and evaluated on the arXiv dataset in the single-document\nsetup, following [Wu et al., 2022]. In particular, we study how models trained with a given context\nlength perform when evaluated with different context lengths. These experiments differ from those in\nSection 5.2, as the models were both trained and evaluated on the same dataset (arXiv), unlike the C4\ntraining and zero-shot evaluation done in Section 5.2.\nThe MT baseline in Table 10 with a memory length of 2K struggles to utilize additional context\nbeyond 32K tokens effectively. The model trained with 8K memory performs significantly better\nwhen evaluated with longer contexts, showing further perplexity gains at 64K tokens. We observe\ndiminishing returns when scaling up the training memory length to 16K tokens and beyond.\nUsing the same setup, we study the performance of FOT while varying d and w configurations,\nsimilarly to Section 5.4, see Table 11. Parameter values w = 1 and w = 2 correspond to additional\n3https://www.isa-afp.org\n26\ncontext lengths of 2K and 4K, respectively. In an apples-to-apples comparison to MT with 2K\nadditional context length, FOT outperforms the MT baseline, which shows the importance of trainable\nkeys and values (see also Section C.1). Moreover, we confirm the findings from Section 5.4 that\nd = 2 works significantly better than d = 1 in all settings. Our best configuration achieves 2.148\nperplexity with 4K additional context during training, compared to 2.164 of MT with 16K additional\ncontext.\nTable 10: Memorizing Transformer: arXiv perplexity values for different training memory lengths and\nevaluation context sizes\nEval Context\nTraining Memory Length\n2k\n8k\n16k\n32k\n4K\n2.309\n2.334\n2.348\n2.365\n8K\n2.242\n2.244\n2.252\n2.265\n16K\n2.215\n2.206\n2.206\n2.215\n32K\n2.199\n2.178\n2.177\n2.181\n64K\n2.195\n2.169\n2.166\n2.168\n128K\n2.195\n2.168\n2.164\n2.166\nTable 11: FoT: arXiv perplexity values for different parameter combinations and evaluation context sizes\nEvaluation Context\nParameter Combinations (w, d), Training Memory Length\n(1, 1), 2048\n(1, 2), 2048\n(2, 1), 4096\n(2, 2), 4096\n4K\n2.292\n2.299\n2.305\n2.309\n8K\n2.229\n2.224\n2.214\n2.217\n16K\n2.206\n2.194\n2.178\n2.178\n32K\n2.192\n2.176\n2.159\n2.156\n64K\n2.187\n2.171\n2.152\n2.149\n128K\n2.187\n2.171\n2.152\n2.148\nL\nCode\nIn Listing 2, we show the FOTs attention code (i.e., the code for the memory attention layers and\ncrossbatch training), see Section 3, Appendix B.2, Appendix B.3. We note that the changes to\nthe code are small; they are localized to the memory layer (the other layers follow the standard\ntransformer protocol) and do not require any new trainable parameters.\n27\nListing 2: Memory attention: Let \u2113 be a memory attention layer. During the training, we make \u2113\nattend to the (key, value) pairs from the local context, previous local context, and d \u2212 1 contexts\ncoming from other documents. During the inference, queries from \u2113 attend to the local context and\nk nearest neighbors retrieved from memory. For simplicity, we provide the code for one head and\nassume that the crossbatch dimension d is equal to the batch size.\ndef\nmem_attn_layer (Ql , Kl , Vl , Cl , Km , Vm , Kp , Vp , attn_scf , mode ):\n\"\"\" Attention\nmechanism\nfor\ncrossbatch\nand memory\nattention\nArgs:\nQl , Kl , Vl: tensors of shape [batch , ctx_len , dim]\nwith queries , keys and values\nfrom the local\ncontext\nKm , Vm:\ntensors of shape [batch , ctx_len , k, dim]\nwith\nk most\nmatching\nmemory\nkeys for\neach\nquery\nfrom Ql along\nwith\nassociated\nvalues\nKp , Vp:\ntensors of shape [batch , ctx_len , dim] with\nkeys and values\nfrom the\nprevious\ncontext\nattn_scf :\na scale\nfactor\nused\nbefore\nsoftmax\nmode:\neither\ntraining or inference\nReturns:\ny: a vector\nwith\nshape [batch , ctx_len , dim]\n\"\"\"\n# attention to the local\ncontext\nlocal_attention = jnp.einsum(\"bqd ,bkd ->bqk\", Ql , Kl)\nlocal_attention *= attn_scf\nlocal_attention = apply_causal_mask ( local_attention )\nif mode == \"train\":\n# In train mode , we additionally\nuse\nprevious\ncontext\n# and batch -1 contexts\nfrom\nother\ndocuments.\nprev_attention = jnp.einsum(\"bqd ,ckd ->bqck\", Ql , Kp)\nshape = prev_attention .shape\nadditional_attention = prev_attention .reshape(shape [: -2] + (-1,))\nelif mode == \"inference\":\n# In the\ninference mode , we additionally\nuse\nnearest\nneighbors\n# retrieved\nfrom\nmemory. We retrieve k (key , value)\n# pairs for each\nquery.\nmemory_attention = jnp.einsum(\"bqd ,bqnd -> bqn\", Ql , Km)\nadditional_attention = memory_attention\nelse:\nraise\nException(\"Only\ntrain and\ninference\nmodes are\nsupported\")\nadditional_attention\n*= attn_scf\n# We merge the raw\nattention\nscores and\ncalculate\nthe\nsoftmax\ncombined_attention = jnp.concatenate ([ local_attention ,\nadditional_attention ],\naxis =-1)\ncombined_weights = jax.nn.softmax(combined_attention , axis =-1)\nctx_len = Ql.shape [1]\nlocal_weights = combined_weights [... , :ctx_len]\nadditional_weights = combined_weights [... , ctx_len :]\ny = jnp.einsum(\"bqk ,bkd -> bqd\", local_weights , Vl)\nif mode == \"train\":\nprev_weights =\nadditional_weights\nshape = prev_weights.shape\nprev_weights = prev_weights.reshape(shape [: -1] + (-1, ctx_len ))\ny += jnp.einsum(\"bqck ,ckd -> bqd\", prev_weights , Vp)\nelse:\nmemory_weights = additional_weights\ny += jnp.einsum(\"bqn ,bqnd -> bqd\", memory_weights , Vm)\nreturn y\n28\n"
  },
  {
    "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
    "link": "https://arxiv.org/pdf/2307.03183.pdf",
    "upvote": "7",
    "text": "Whisper-AT: Noise-Robust Automatic Speech Recognizers\nare Also Strong General Audio Event Taggers\nYuan Gong1, Sameer Khurana1, Leonid Karlinsky2, James Glass1\n1MIT CSAIL, USA\n2MIT-IBM Watson AI Lab, USA\n{yuangong,glass}@mit.edu\ngithub.com/yuangongnd/whisper-at\nAbstract\nIn this paper, we focus on Whisper [1], a recent automatic\nspeech recognition model trained with a massive 680k hour la-\nbeled speech corpus recorded in diverse conditions. We first\nshow an interesting finding that while Whisper is very robust\nagainst real-world background sounds (e.g., music), its au-\ndio representation is actually not noise-invariant, but is instead\nhighly correlated to non-speech sounds, indicating that Whis-\nper recognizes speech conditioned on the noise type. With this\nfinding, we build a unified audio tagging and speech recognition\nmodel Whisper-AT by freezing the backbone of Whisper, and\ntraining a lightweight audio tagging model on top of it. With\n<1% extra computational cost, Whisper-AT can recognize au-\ndio events, in addition to spoken text, in a single forward pass.\n1. Introduction\nIn recent years, significant progress has been made in advanc-\ning automatic speech recognition (ASR) performance. Specifi-\ncally, self-supervised learning schemes such as wav2vec2.0 [2]\nand Hubert [3] have achieved great success, requiring minimal\nlabeled training data. However, since the public model check-\npoints are trained with clean speech data (e.g., Librispeech [4]\nor Libri-light [5]), their robustness in real-world environments\nis limited. To improve noise robustness, the Whisper [1] model\nuses 680K hours of labeled speech collected from the Internet\nwith diverse environments and recording setups as the training\ndata, and reports better robustness over existing ASR models.\nIn this paper, we first show a counter-intuitive finding that\nwhile Whisper is robust against background sounds (noise for\nASR), its audio representation is actually not noise-invariant,\nbut instead encodes rich information of non-speech background\nsounds (shown in Figure 1 and discussed in detail in Sec-\ntion 3), indicating that the Whisper model does not learn a\nnoise-invariant representation, but encodes the noise type, and\nthen recognize speech conditioned on the noise type.\nOne exciting application of the above finding is that we can\nbuild a unified model for ASR and Audio Tagging (i.e., recog-\nnize general audio events) based on Whisper since it 1) is robust\nto noise, and 2) encodes rich general audio event information.\nCurrently, ASR and audio tagging (AT) models are typically\nperformed independently. In many applications such as video\ntranscribing, voice assistants, and hearing aid systems, we de-\nsire to get both spoken text and acoustic scene analysis from\nthe audio, but running two systems is computationally expen-\nsive. In this work, we show that with <1% extra computational\ncost, we can make Whisper recognizes audio events together\nwith spoken text in a single forward pass. Our model achieves\nan mAP of 41.5 on AudioSet, which is slightly worse than stan-\ndalone AT models, but is nevertheless over 40\u00d7 faster.\n20\n15\n10\n5\n0\n5\n10\n15\n20\nSignal-to-Noise Ratio (dB)\n0\n20\n40\n60\n80\nWord Error Rate (%)\nWhisper-Large\nHubert-XLarge-FT\nWav2vec2-Large-Robust-FT\nHubert-Large-FT\nWav2vec2-Base-FT\n0\n10\n20\n30\n40\n50\nClassifying Using Representation of Layer # as Input\n30\n40\n50\n60\n70\n80\n90\nSound Classification Accuracy (%)\nWhisper-Large\nHubert-XLarge-PR\nHubert-XLarge-FT\nWav2vec2-Large-Robust-PR\nWav2vec2-Large-Robust-FT\nHubert-Large-FT\nWav2vec2-Base-FT\nFigure 1: Surprisingly, the noise robustness of an ASR model\ncorrelates positively to the amount of general background sound\n(noise for ASR) information encoded in their intermediate rep-\nresentations. In the upper figure, we show Whisper is noticeably\nmore robust (smaller word error rate increase) when speech\n(Librispeech) is contaminated with an increasing amount of\nbackground sounds from ESC-50 [6].\nIn the lower figure,\nwe show the intermediate representations of Whisper lead to\nthe best linear probing sound classification accuracy on the\nsame ESC-50 data, indicating Whisper encodes most back-\nground sound information. Unlike other models, Whisper en-\ncodes background sound information even in its deepest layer.\nPR=self-supervised pretrained; FT=PR and fine-tuned model.\nRelated Work: To the best of our knowledge, we are the first\nto report that a robust ASR actually learns a noise-variant rep-\nresentation; most previous work focuses on noise-invariant rep-\nresentations [7, 8, 9, 10, 11]. For ASR and AT model unifica-\ntion, the closest works are [12, 13, 14, 15]. In [12], a unified\nkeyword spotting and audio tagging model is proposed, how-\never, keyword spotting only considers up to 35 words and is a\nmuch simpler task than the large-vocabulary continuous speech\nrecognition task we are targeting. In [13, 14], joint ASR and\naudio tagging/captioning training frameworks are proposed, but\nin this work, we show that Whisper already encodes rich gen-\neral audio information even without any explicit audio tagging\ntraining. In [15], ASR representations are tested for the audio\ntagging task, but the overall performance is unsatisfactory.\narXiv:2307.03183v1  [cs.SD]  6 Jul 2023\n2. Whisper Robust ASR Model\nWhisper [1] is a recently proposed robust ASR model that fea-\ntures a standard Transformer [16]-based encoder-decoder archi-\ntecture. The main novelty of Whisper is not its architecture, but\nits training data and training scheme. Specifically, the 680K-\nhour non-public training set contains audio-transcript pairs col-\nlected from the Internet with a very broad distribution of audio\nfrom many different environments, recording setups, speakers,\nand languages. Significant effort was made to filter out low-\nquality data. Compared with the most commonly used Lib-\nrispeech (960 hours) and Libri-light (60K hours) data that are\ncollected from audiobooks, the Whisper training data is much\nlarger and more diverse, but also has noisy labels. We identify\nthis as the main factor that differentiates Whisper from existing\nASR models. During Whisper training, only text transcripts are\nused as supervision signals, no audio event labels are given. In\nthis paper, we use the Whisper-Large model unless otherwise\nstated. Since Whisper is an encoder-decoder model, we only\nuse the audio encoder part of Whisper for audio tagging, which\nconsists of 32 Transformer layers with a dimension of 1280.\n3. Noise-Robust ASR Learns\nNoise-Variant Representations\nThanks to the diverse 680K-hour training data, Whisper has\nbeen shown to be more robust under white and pub noise than its\ncounterparts [1]. We confirmed this point by evaluating Whis-\nper and other state-of-the-art ASR models on Librispeech clean\nspeech data that were contaminated with ESC-50 [6] environ-\nmental sounds with various signal-to-noise ratios (SNRs). As\nshown in Figure 1 (upper), Whisper has superior performance.\nWhat is the noise-robust mechanism of Whisper? It is com-\nmonly believed that the representation of a robust ASR model\nshould be noise-invariant, and researchers often set noise-\ninvariance as an explicit inductive bias for robust ASR (e.g.,\nin [7, 8, 9, 10, 11]). However, we, perhaps surprisingly, found\nthat Whisper\u2019s representation is actually noise-variant and en-\ncodes rich non-speech background sound information.\nSpecifically, we froze the entire Whisper model and input\naudio samples from the ESC-50 environment sound dataset [6].\nWe then extracted the intermediate representation from every\nlayer of Whisper and trained a linear layer on top of it to classify\nthe sound class from 50 possible classes. If Whisper did not en-\ncode background sound information, or its representations were\ninvariant to background sounds, the sound classification result\nwould be low, and vice versa. As shown in Figure 1 (lower), the\nWhisper representations had the best ESC-50 sound classifica-\ntion accuracy compared to other SOTA ASR models, indicating\nthat its representation encodes most background sound infor-\nmation. In addition, for all other ASR models, representations\nfrom deeper layers led to lower sound classification accuracies,\nshowing that the models are learning to encode speech informa-\ntion, and ignore background sound information. Whisper does\nnot have this behavior, since representations from deeper layers\nalso encode background sound information.\nThe fact that Whisper is noise-robust while its representa-\ntion encodes rich background sound information reveals that the\nrobustness mechanism of Whisper is different from other ASR\nmodels (including wav2vec2-robust [17]). Instead of learning\na noise-invariant representation, it first encodes the background\nsound and then transcribes text conditioned on the type of noise.\nWe confirmed this point by further checking the class-wise rela-\ntionship between Whisper\u2019s robustness against a specific back-\n50\n60\n70\n80\n90\n100\nESC-50 Class-wise F1-Score\n0\n10\n20\n30\n40\n50\nWord Error Rate Increase from 20dB to -10dB SNR (%)\nDog\nRooster\nPig\nCow\nFrog\nCat\nHen\nInsects\nSheep\nCrow\nCrackling Fire\nCrickets\nChirping Birds\nWater Drops\nWind\nPouring \nWater\nThunderstorm\nCrying Baby\nClapping\nBreathing\nFootsteps\nLaughing\nBrushing Teeth\nCoughing\nDrinking Sipping\nDoor Wood Knock\nMouse Click\nKeyboard Typing\nDoor Wood Creaks\nCan Opening\nClock Alarm\nClock Tick\nGlass\nBreak\nHelicopter\nSiren\nCar Horn\nEngine\nChurch Bells\nAirplane\nFireworks\nHand Saw\nSneezing\nSnoring\nWord Error Rate Increase > 50% (Not Shown in Figure):\nWashing Machine, Train, Chain Saw, Vacuum Cleaner, See Waves, Rain, Toilet Flush \nFigure 2: Class-wise analysis of the relationship between Whis-\nper\u2019s robustness against a specific background sound class and\nits potential ability to recognize the sound. We measure Whisper\nrobustness by its WER increase from clean speech (20dB SNR)\nto speech contaminated by the specific background sound from\nESC-50 (-10dB SNR). The lower the WER increase, the more\nrobust the model (Y-axis). We estimate the potential ability of\nWhisper to recognize the sound by training a linear layer on top\nof the Whisper encoder\u2019s last-layer representation for the sound\nclassification task on the same ESC-50 dataset (without speech\nmixed-in, the Whisper model is frozen) and show the class-wise\nF1-score. The higher the F1-score, the better Whisper can po-\ntentially recognize the sound class (X-axis). Blue dashed line:\nwe observe a positive correlation between Whisper\u2019s robustness\nagainst a background sound type and its potential ability to rec-\nognize it. Blue shading: we observe most sound classes lie in\nthe right-bottom triangle area, indicating that Whisper is not\nrobust to the type of sound if it cannot recognize the sound type.\nRight-bottom outliers: there are some background sounds that\nWhisper can potentially recognize but is not robust to, which is\nexpected as some noises heavily overlap with the speech and are\nimpossible to be robust to. In short, we find the potential ability\nto recognize a sound type is a necessary but not sufficient con-\ndition for Whisper to be robust to it.\nground sound class, and its potential ability to recognize the\nsound class in Figure 2. We found there is indeed a positive cor-\nrelation between them. Compared to noise-aware training [18]\nthat requires manually inputting noise type to the model, Whis-\nper learns it directly from its massive 680K hour training set.\nNote that the discussion in this section is mostly based\non Whisper, and our experiments do not indicate that noise-\ninvariance does not help noise-robust ASR, nor that a noise-\nrobust ASR\u2019s representation should be noise-variant. In fact, we\nbelieve encouraging noise-invariant representations [7, 8, 9, 10,\n11] is a practical solution in self-supervised learning or small\ndata cases. Whisper training requires industry-level computa-\ntional resources and is expensive. What we hope to convey is\nthat a noise-robust ASR model does not have to learn a noise-\ninvariant representation, and that there exist other ways to be\nnoise-robust - a noise-conditioned model like Whisper can, and\nindeed does, work very well.\n4. Unifying ASR and Audio Tagging Model\nOne exciting application of the finding in Section 3 is that we\nare able to build a unified model for ASR and Audio Tagging\nbased on Whisper to recognize spoken text and background\nsounds (e.g., music, horn, etc) simultaneously, which is highly\ndesirable in applications such as video transcribing, voice assis-\ntants, and hearing aid systems. Whisper is ideal as a backbone\nfor such a unified model because 1) it is robust to background\nsounds, and 2) its intermediate representations encode rich gen-\neral audio event information, which serves as a solid base for\naudio tagging. Nonetheless, the original Whisper does not out-\nput sound labels, so we need to train a model on top of Whisper\nintermediate representations to enable it to predict a sound class.\nNote that we intentionally do not modify the original weights of\nthe Whisper model, but instead add new audio tagging layers on\ntop of it so that the Whisper ASR ability is not changed and text\nand audio labels can be generated in a single forward pass. We\ncall this unified ASR and Audio Tagging model Whisper-AT.\nIn previous sections, we applied a basic linear layer on the\nrepresentation of a single layer for probing purposes. In this\nsection, we discuss more advanced methods that lead to better\naudio tagging performance.\n1. Last-MLP: The most basic method, we first apply a tempo-\nral mean pooling over the last layer representation of Whisper\nand then apply a linear layer to map it to the prediction.\n2. WA-MLP: As shown in Figure 3, we find the last layer is\nnot optimal for all sound classes. Thus we weighted average\n(WA) the representations from all layers and set the weight to\nbe learnable before temporal mean pooling and linear layer,\nso this approach leverages representations from all layers.\n3. WA-Tr: Temporal mean pooling removes all temporal de-\ntails, and a single linear layer may be too simple for audio\ntagging. Therefore, we replace the linear layer of WA-MLP\nwith a single-head temporal Transformer layer for this model.\n4. TL-Tr: Time and layer-wise Transformer (our main method,\nshown in Figure 4). Though weighted averaging leverage\nrepresentation of all layers, all sound classes use a fixed set\nof weights. In Figure 3, we show that different sound classes\nachieve their best performance using different representation\nlayers. Therefore, ideally, each class should have its own set\nof weights. This motivates us to build an attention mecha-\nnism over the layers. Specifically, we apply another layer-\nwise Transformer to the output of the temporal Transformer.\nEfficient Design: As the original goal of Whisper-AT is being\nmore computationally efficient than two independent ASR and\nAT models, we aim to minimize the extra cost for audio tagging.\nIntroducing a new Transformer layer in WA-Tr and TL-Tr is\nrelatively expensive. Consider the complexity of Transformer\nis O(d2n + dn2), where d is the dimension and n is the input\nlength of the Transformer, for each 10-second input audio, the\nrepresentations of each Whisper layer is in the shape of (n=500,\nd=1280). If the temporal and layer Transformer have the same\nn and d as Whisper, their computational cost is not negligible.\nTherefore, as illustrated in Figure 4, we propose the following\nefficient design: 1) We add a mean pooling layer to each Whis-\nper representation to lower the time sequence length n from 500\nto 25; 2) We add an optional linear projection layer to lower d\nfrom 1280 to 512 before audio tagging Transformers (denoted\nby TL-Tr512); and 3) For WA-Tr, we first conduct weighted\naveraging and then apply a temporal Transformer, for TL-Tr,\nwe use a single temporal Transformer for all layers. Thus both\nWA-Tr and TL-Tr only need one temporal Transformer.\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nRepresentation of Layer\n0\n2\n4\n6\n# Classes\nFigure 3: Histrogram of the best Whisper representation layer\n(1-32) for the 50 ESC-50 sound classes. We train a linear layer\non top of the representation of each of the 32 Whisper layers for\nESC-50 sound classification, compute the class-wise F1-Score,\nand find the best representation layer for each sound class. Dif-\nferent sound classes get the best F1-score on representations of\ndifferent layers.\nWhisper Encoder \nBlock 1\nWhisper Encoder \nBlock 2\n......\nTemporal \nPooling\n(n=500, d=1280)\n(25, 1280)\n(512)\n......\nLinear \nProjection\nTemporal \nTransformer\nTemporal \nPooling\n(25, 1280)\n(512)\nLinear \nProjection\nTemporal \nTransformer\nTemporal \nPooling\n(25, 1280)\n(512)\nLinear \nProjection\nTemporal \nTransformer\nTemporal \nPooling\n(32, 512)\nLayer \nTransformer\nMean \nPooling\nLinear \nClassifier\nText\nAudio \nLabels\n(Optional)\nShared\nShared\nInput Audio (10s)\nTemporal \nPooling\nTemporal \nPooling\n......\n(25,512)\n(25, 512)\n(25,512)\n(25, 512)\n(25,512)\n(25, 512)\n(n=500, d=1280)\n(n=500, d=1280)\n(512)\nFigure 4: The proposed time and layer-wise Transformer model.\n5. Experiments\nAs mentioned in Section 4, we intentionally freeze the weights\nof the original Whisper model, so the ASR performance of\nWhisper-AT is exactly the same as the original Whisper [1].\nThus we only conduct experiments on the audio tagging task.\n5.1. Experiment Settings\nDataset: We use AudioSet and ESC-50 datasets following stan-\ndard evaluation protocols. AudioSet [20] is a collection of over\n2 million 10-second audio clips excised from YouTube videos\nand labeled with the sounds that the clip contains from a set of\n527 labels. We train our model with both the balanced training\nset (AS-20K) and full training set (AS-2M) and report mAP on\nthe evaluation set. ESC-50 [6] consists of 2,000 5-second envi-\nronmental audio recordings organized into 50 classes; we evalu-\nate our model using the official 5-fold cross-validation protocol.\nHyper-Parameters: We use the standard training pipeline in\nprior AT work [21, 22, 26, 27]. For all experiments, we use a\nbatch size of 48 and an Adam optimizer [28]. For the proposed\nTL-Tr512 model, we use an initial learning rate of 2e-4, 1e-4,\nand 5e-4, and train the model for 30, 5, and 30 epochs for AS-\n20K, AS-2M, and ESC-50, respectively. For baseline methods,\nwe search the learning rate to ensure a fair comparison.\n5.2. Experiment Results\nWe show the main results in Table 1. The key conclusions are:\nTable 1: Audio tagging performance comparison on AS-20K, AS-2M (mAP), and ESC-50 (accuracy). \u2020ASR backbone parameters and\nFLOPs are not included. \u22c6Speed-up = 1/FLOPs, compared with AST; FLOPs computed by fvcore [19]. \u2021: labeled AS-2M data\nis also used.\n\u2217\u2217 AS-2M experiment is expensive, we skip it when AS-20K and ESC50 experiments already shown clear differences.\nEnd-to-End fine-tuning results are shown in grey text as the comparison is not exactly fair.\nModel\nTraining Setting\nMethod\nAS-20K\nAS-2M\nESC-50\nAT #Params\u2020\nAT Speed-Up\u2020\u22c6\nExisting Standalone Audio Tagging Models\nAudioSet Baseline [20]\nFine-Tuning\nEnd-to-End\n-\n31.4\n-\n-\n-\nAST [21]\nFine-Tuning\nEnd-to-End\n34.7\n45.9\n88.8\n87M\n1 \u00d7 (133G FLOPs)\nSSAST [22]\nFine-Tuning\nEnd-to-End\n31.0\n-\n88.7\n87M\n1 \u00d7\nPANNs [23]\nFine-Tuning\nEnd-to-End\n27.8\n43.9\n94.7\u2021\n81M\n2.5 \u00d7\nMAE-AST [24]\nFine-Tuning\nEnd-to-End\n30.6\n-\n90.0\n87M\n2.7 \u00d7\nAudio-MAE [25]\nFine-Tuning\nEnd-to-End\n37.0\n47.3\n94.1\n87M\n2.7 \u00d7\nExisting Automatic Speech Recognition Models\nHubert X-Large [3]\nFrozen\nWA-MLP\n18.5\n- \u2217\u2217\n82.2\n0.7M\n195K \u00d7\nHubert X-Large [3]\nFrozen\nTL-Tr1280\n20.2\n-\n83.6\n40M\n5 \u00d7\nwav2vec2-Large-Robust [17]\nFrozen\nWA-MLP\n18.1\n-\n78.5\n0.5M\n244K \u00d7\nwav2vec2-Large-Robust [17]\nFrozen\nTL-Tr1024\n20.2\n-\n82.8\n26M\n17 \u00d7\nWhisper-AT\nWhisper-Large\nFrozen\nLast-MLP\n20.6\n20.3\n87.0\n0.7M\n195K \u00d7\nWhisper-Large\nFrozen\nWA-MLP\n25.7\n32.4\n90.2\n0.7M\n195K \u00d7\nWhisper-Large\nFrozen\nWA-Tr\n32.1\n41.0\n91.0\n20M\n270 \u00d7\nWhisper-Large\nFrozen\nTL-Tr1280\n33.0\n42.1\n91.1\n40M\n8 \u00d7\nWhisper-Large\nFrozen\nTL-Tr512\n32.8\n41.5\n91.7\n7M\n42 \u00d7\nWhisper-Large\nFine-Tuning\nEnd-to-End\n34.7\n45.7\n90.0\n655M\n0.4 \u00d7\nWhisper-Small\nFine-Tuning\nEnd-to-End\n31.9\n44.1\n88.9\n94M\n2.5 \u00d7\nFirst, Whisper-AT is significantly stronger than Hubert X-\nLarge [3] and wav2vec2-Large-Robust [17] on audio tagging,\ndemonstrating that Whisper is not only the most robust ASR\nmodel but also the strongest audio tagging backbone.\nSecond, comparing the four Whisper-AT models, the pro-\nposed TL-Tr model leads to the best performance with higher\ncomputational overhead.\nHowever, by projecting the Trans-\nformer dimension from 1280 to 512, TL-Tr512 strikes a bal-\nance between performance and efficiency, as its FLOPs are less\nthan 1% of the Whisper ASR FLOPs yet it performs almost the\nsame as TL-Tr1280. In Table 2, we further study the relation-\nship between the audio tagging performance and Transformer\ndimension d for TL-Tr. Even TL-Tr128 provides reasonably\ngood audio tagging performance, while its computational cost\nis almost free (<0.1% FLOPs of the Whisper ASR FLOPs).\nThird, Whisper-AT is slightly worse than SOTA standalone\naudio tagging models but is much more efficient. The proposed\nTL-Tr512 achieves 32.8 mAP, 41.5 mAP, and 91.7 accuracy\non AS-20K, AS-2M, and ESC-50, respectively, and is 42 times\nfaster and 11 times smaller than AST [21]. Note that we target\nthe cases that the user is already running an ASR and want to get\nadditional audio labels, so we only compare the additional cost\nfor AT and do not include the cost of ASR in this comparison.\nFourth, how does Whisper perform in the end-to-end fine-\ntuning setting, and how does it compare to SOTA audio tagging\nmodels? We add a new Transformer layer on top of the Whisper\nencoder and train the entire model end-to-end (new layer uses\na 10-100\u00d7 larger learning rate). For a fair comparison, we also\ntest Whisper-Small which is of similar size to SOTA audio tag-\nging models. We find Whisper-Small performs similarly with\nprevious self-supervised pretrained models such as SSAST [22]\nand MAE-AST [24] after fine-tuning.\nFinally, we test the audio tagging performance of smaller\nWhisper models. As shown in Figure 5, smaller models have\nweaker audio tagging performance but the difference between\nWhisper-Small, Medium, and Large is minor. We also test the\nASR noise-robustness of these models on speech contaminated\nby ESC50 background sounds; larger models are more robust.\nWe again observe a positive correlation between ASR noise ro-\nTable 2: Performance and efficiency impact of TL-Tr Trans-\nformer dimension d.\nTr Dim d\nFLOPs (G)\n#Params (M)\nAS-20K\nESC-50\n128\n0.31\n0.6\n30.0\n91.4\n256\n0.94\n2.1\n32.0\n92.0\n512\n3.17\n7.2\n32.8\n91.7\n768\n6.72\n15.6\n33.0\n91.4\n1280\n16.42\n40.0\n33.0\n91.1\n20\n10\n0\n10\n20\nSignal-to-Noise Ratio (dB)\n0\n20\n40\n60\n80\nWord Error Rate (%)\nWhisper-Large\nWhisper-Medium\nWhisper-Small\nWhisper-Base\nWhisper-Tiny\nHubert-XLarge-FT\nTiny\nBase\nSmall\nMedium\nLarge\nWhisper Model Size\n20\n25\n30\n35\n40\n45\nAudioSet-2M mAP\nTL-Tr\nE2E FT\nFigure 5: AS-2M audio tagging performance (left) and ASR ro-\nbustness (right) of the Whisper model family.\nbustness and AT performance. In addition, Whisper-Base (74M\nparameters) is already more robust in ASR and stronger in audio\ntagging than Hubert-X-Large (964M parameters).\n6. Conclusion\nThe Whisper ASR model revives the supervised learning\nscheme by using a massive and diverse training corpus. In this\npaper, we report an intriguing property of Whisper that while\nbeing very robust, the audio representation of Whisper is actu-\nally noise-variant and encodes rich background sound informa-\ntion. Based on this finding, we propose a unified audio tagging\nand ASR model called Whisper-AT. With only less than 1% ad-\nditional cost, Whisper-AT can recognize the background sound\nin addition to spoken text in a single forward pass.\nAcknowledgments: This research is supported by the MIT-\nIBM Watson AI Lab.\n7. References\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\nI. Sutskever, \u201cRobust speech recognition via large-scale weak su-\npervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\n[2] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec\n2.0: A framework for self-supervised learning of speech repre-\nsentations,\u201d Advances in Neural Information Processing Systems,\nvol. 33, pp. 12 449\u201312 460, 2020.\n[3] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-\ndinov, and A. Mohamed, \u201cHubert: Self-supervised speech rep-\nresentation learning by masked prediction of hidden units,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 29, pp. 3451\u20133460, 2021.\n[4] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: an asr corpus based on public domain audio books,\u201d in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2015, pp. 5206\u20135210.\n[5] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.-E.\nMazar\u00b4e, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen\net al., \u201cLibri-light: A benchmark for asr with limited or no super-\nvision,\u201d in IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 7669\u20137673.\n[6] K. J. Piczak, \u201cEsc: Dataset for environmental sound classifica-\ntion,\u201d in Proceedings of the 23rd ACM international conference\non Multimedia, 2015, pp. 1015\u20131018.\n[7] M. Van Segbroeck et al., \u201cUnsupervised learning of time\u2013\nfrequency patches as a noise-robust representation of speech,\u201d\nSpeech Communication, vol. 51, no. 11, pp. 1124\u20131138, 2009.\n[8] D. Serdyuk,\nK. Audhkhasi,\nP. Brakel,\nB. Ramabhadran,\nS. Thomas, and Y. Bengio, \u201cInvariant representations for noisy\nspeech recognition,\u201d NIPS 2016 End-to-end Learning for Speech\nand Audio Processing Workshop, 2016.\n[9] A. Sriram, H. Jun, Y. Gaur, and S. Satheesh, \u201cRobust speech\nrecognition using generative adversarial networks,\u201d in IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2018, pp. 5639\u20135643.\n[10] D. Liang, Z. Huang, and Z. C. Lipton, \u201cLearning noise-invariant\nrepresentations for robust speech recognition,\u201d in 2018 IEEE Spo-\nken Language Technology Workshop (SLT).\nIEEE, 2018, pp.\n56\u201363.\n[11] Q.-S. Zhu, J. Zhang, Z.-Q. Zhang, M.-H. Wu, X. Fang, and L.-\nR. Dai, \u201cA noise-robust self-supervised pre-training model based\nspeech representation learning for automatic speech recognition,\u201d\nin IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP).\nIEEE, 2022, pp. 3174\u20133178.\n[12] H. Dinkel, Y. Wang, Z. Yan, J. Zhang, and Y. Wang, \u201cUnikw-\nat: Unified keyword spotting and audio tagging,\u201d arXiv preprint\narXiv:2209.11377, 2022.\n[13] N. Moritz, G. Wichern, T. Hori, and J. Le Roux, \u201cAll-in-one trans-\nformer: Unifying speech recognition, audio tagging, and event\ndetection.\u201d in INTERSPEECH, 2020, pp. 3112\u20133116.\n[14] C. Narisetty, E. Tsunoo, X. Chang, Y. Kashiwagi, M. Hentschel,\nand S. Watanabe, \u201cJoint speech recognition and audio captioning,\u201d\nin ICASSP 2022-2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP).\nIEEE, 2022, pp.\n7892\u20137896.\n[15] J. Turian, J. Shier, H. R. Khan, B. Raj, B. W. Schuller, C. J. Stein-\nmetz, C. Malloy, G. Tzanetakis, G. Velarde, K. McNally et al.,\n\u201cHear: Holistic evaluation of audio representations,\u201d in NeurIPS\n2021 Competitions and Demonstrations Track. PMLR, 2022, pp.\n125\u2013145.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nin Advances in Neural Information Processing Systems, 2017.\n[17] W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu,\nV. Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and\nM. Auli, \u201cRobust wav2vec 2.0: Analyzing Domain Shift in Self-\nSupervised Pre-Training,\u201d in Proc. Interspeech 2021, 2021, pp.\n721\u2013725.\n[18] M. L. Seltzer, D. Yu, and Y. Wang, \u201cAn investigation of deep neu-\nral networks for noise robust speech recognition,\u201d in IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2013, pp. 7398\u20137402.\n[19] \u201cvcore,\u201d https://github.com/facebookresearch/fvcore.\n[20] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, \u201cAudio Set: An ontol-\nogy and human-labeled dataset for audio events,\u201d in IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2017.\n[21] Y. Gong, Y.-A. Chung, and J. Glass, \u201cAST: Audio Spectrogram\nTransformer,\u201d in Proc. Interspeech 2021, 2021, pp. 571\u2013575.\n[22] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, \u201cSsast: Self-\nsupervised audio spectrogram transformer,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022,\npp. 10 699\u201310 709.\n[23] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumb-\nley, \u201cPanns: Large-scale pretrained audio neural networks for\naudio pattern recognition,\u201d IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 28, pp. 2880\u20132894, 2020.\n[24] A. Baade, P. Peng, and D. Harwath, \u201cMAE-AST: Masked Autoen-\ncoding Audio Spectrogram Transformer,\u201d in Proc. Interspeech\n2022, 2022, pp. 2438\u20132442.\n[25] P.-Y. Huang, H. Xu, J. B. Li, A. Baevski, M. Auli, W. Galuba,\nF. Metze, and C. Feichtenhofer, \u201cMasked autoencoders that lis-\nten,\u201d in Advances in Neural Information Processing Systems,\n2022.\n[26] Y. Gong, Y.-A. Chung, and J. Glass, \u201cPsla: Improving audio\ntagging with pretraining, sampling, labeling, and aggregation,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, 2021.\n[27] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun,\n\u201cAttention bottlenecks for multimodal fusion,\u201d Advances in Neu-\nral Information Processing Systems, vol. 34, pp. 14 200\u201314 213,\n2021.\n[28] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d in International Conference on Learning Representa-\ntions, 2015.\n"
  },
  {
    "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference",
    "link": "https://arxiv.org/pdf/2307.02628.pdf",
    "upvote": "6",
    "text": "SkipDecode: Autoregressive Skip Decoding with\nBatching and Caching for Efficient LLM Inference\nLuciano Del Corro\nAllie Del Giorno\nSahaj Agarwal\nBin Yu\nAhmed Awadallah\nSubhabrata Mukherjee\nMicrosoft Research\n{ldelcorro,adelgiorno,sahaj.agarwal}@microsoft.com,\n{v-ybi,ahmed.awadallah,subhabrata.mukherjee}@microsoft.com\nAbstract\nAutoregressive large language models (LLMs) have made remarkable progress in\nvarious natural language generation tasks. However, they incur high computation\ncost and latency resulting from the autoregressive token-by-token generation. To\naddress this issue, several approaches have been proposed to reduce computational\ncost using early-exit strategies. These strategies enable faster text generation using\nreduced computation without applying the full computation graph to each token.\nWhile existing token-level early exit methods show promising results for online\ninference, they cannot be readily applied for batch inferencing and Key-Value\ncaching. This is because they have to wait until the last token in a batch exits\nbefore they can stop computing. This severely limits the practical application of\nsuch techniques. In this paper, we propose a simple and effective token-level early\nexit method, SkipDecode, designed to work seamlessly with batch inferencing\nand KV caching. It overcomes prior constraints by setting up a singular exit\npoint for every token in a batch at each sequence position. It also guarantees a\nmonotonic decrease in exit points, thereby eliminating the need to recompute KV\nCaches for preceding tokens. Rather than terminating computation prematurely\nas in prior works, our approach bypasses lower to middle layers, devoting most of\nthe computational resources to upper layers, allowing later tokens to benefit from\nthe compute expenditure by earlier tokens. Our experimental results show that\nSkipDecode can obtain 2x to 5x inference speedups with negligible regression\nacross a variety of tasks. This is achieved using OPT models of 1.3 billion and 6.7\nbillion parameters, all the while being directly compatible with batching and KV\ncaching optimization techniques.\n1\nIntroduction\nAutoregressive large language models (LLMs), such as the GPT [14] and OPT [23] family, have\ndemonstrated strong performance across a wide range of tasks [15, 16, 1]. However, thee also\nhave high computational cost and latency requirements resulting from token-by-token generation.\nToken-level early exit [17, 18] has emerged as a promising technique to alleviate these limitations by\nallowing tokens to cease computation as soon as their hidden states reach saturation [17].\nAlthough current methodologies exhibit theoretical advantages, their practical implementation is\nsomewhat restricted since they are not compatible with batch inferencing and KV caching techniques,\nwhich are widely used to speed up inference in practice. This is mainly due to the necessity of\nprolonging computation until the final token in a batch for each position is thoroughly processed.\nPreprint. Under review.\narXiv:2307.02628v1  [cs.CL]  5 Jul 2023\n0\n10\n20\n30\n40\n50\n3\n4\n5\n6\n7\nPositionwise cross entropy loss on openwebtext with OPT-350\ntoken position\nPer-token loss\n(a) OPT-350m on OpenWebText. Average loss\nper token position shows a strong monotonically-\ndecreasing trend for general text.\n0\n10\n20\n30\n40\n50\n0\n1\n2\n3\n4\nPositionwise cross entropy loss on reddit with OPT-350m\ntoken position\nPer-token loss\n(b) OPT-350m (finetuned) on Reddit-TLDR. Av-\nerage loss per token position. Decreasing trend but\nwith a different function.\nFigure 1: Average loss per token position (black) during the forward pass of OPT-350m model on a\ngeneral and a task-specific dataset. Grey represents the 95% confidence interval on the mean.\nThis effectively limits improvements to the exit position of the most computation-heavy token.\nAdditionally, token-level exit strategies, which depend on dynamic indicators like learned classifiers\nfor defining exit points, don\u2019t provide any assurance concerning computational expenses, such as the\nworst-case cost relative to the computation performed by the full-sized network. A further practical\ndifficulty arises in the form of Key-Value (KV) caching of prior tokens, which requires updating if\nthe current token exits later than the others.\nIn this paper, we present a novel token-level early exit method, called SkipDecode, which overcomes\nthese limitations, while maintaining a controlled computational budget. Our approach establishes a\nunified exit point for all tokens within a batch at a specific sequence position. We further capitalize\non the observation that words towards the end of a sequence are generally easier to predict due to\nmore contextual information. This allows us to design our generation policy with monotonically\ndecreasing exit points as the sequence progresses, with the assumption that subsequent tokens demand\nless computational effort. Figure 1 shows the decreasing loss per token position: Predictions at the\nbeginning of the sequence register higher entropy in contrast to the tokens that appear later. This\nmotivates the use of increased computational effort upfront to minimize the mistakes, and therefore\nprevent the cascading of errors; whereas we can reduce the computational processing later as the\nsequences become more predictive. Our strategy with monotonically decreasing exit points also\neliminates the necessity to recalculate Key-Value (KV) caches for preceding tokens, substantially\nreducing computational cost.\nTokens exiting at distinct layers are unable to benefit from all the information generated by previous\ntokens that exit at later positions, leading to wasted computation and loss of contextual information.\nTo address this issue, the early exit mechanism in SkipDecode leverages the entire computation\nperformed by all tokens, resulting in a substantial improvement in the speedup-task performance\ntrade-off. Rather than abruptly ending computation, our approach bypasses lower layers and primarily\nallocates the computational budget to upper layers, enabling rightward tokens to benefit from the\ncomputational resources employed by leftward tokens effectively.\nOur technique SkipDecode (overview in Figure 2 and Table 1) is able to avoid performance degrada-\ntion up to the hidden state saturation point. We experiment with up to 5x speedup over 1.3 billion,\nand 6.7 billion OPT Transformer models on three benchmark generation datasets. We also solve\npractical problems like batching and KV caching while maintaining a controlled and predictable\ncomputational budget. Our method makes it easier to use LLMs on devices with limited resources\nand helps to democratize AI.\nMethod\nModel\nGeneration\nToken\nBatching\nKV-\nFull\nControlled\nType\nLevel\nCaching\nAttention\nComp. Cost\nCALM\nEnc-Dec\n\u2713\n\u2713\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nSkipDecode\nDec Only\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 1: Comparison of CALM and SkipDecode. SkipDecode supports batching and KV caching\nfor increasing inference efficiency with controlled computational budget.\n2\nAddressing Practical Challenges in Token-Level Early Exit Strategies\nIn this section, we discuss the practical challenges associated with token-level early exit methods in\nLLMs and present SkipDecode to address these limitations.\n2\nPractical Blockers (Existing):\n\u2022\nBatching: Computational cost defined by the last exit token\n\u2022\nKV Caching: If next token exits later than previous one, we need to recompute\nKV values for previous tokens.\n\u2022\nComputational Efficiency: If next token exits earlier, it does not attend full\ncomputation of previous token.\n\u2022\nCost Uncertainty: Worst case scenario (for a bad token exit, e.g., from last\nlayer) equivalent to processing the whole network.\n(a) Early Termination\nSolutions (Ours):\n\u2022\nBatching: Exit per position per batch (column-wise).\n\u2022\nKV Caching: Next column has to exit earlier than previous column. Leftwards\ntokens are more difficult to generate.\n\u2022\nComputational Efficiency: Spend most of computational budget on top layers.\nImplicitly attends the full computation of previous tokens.\n\u2022\nCost Uncertainty: Static policy (no surprises), computation costs are prede-\nfined.\n(b) Skipping\nFigure 2: Overcoming practical blockers in token level early exit for language generation.\n2.1\nOptimizing Batched Inference with Unified Exit Points\nBatched inference is widely used to enhance computational efficiency by simultaneously processing\nmultiple input samples. This approach capitalizes on the parallelism offered by hardware, such as\nGPUs and TPUs, to reduce latency and improve throughput.\nHowever, when applying existing token-level early exit strategies [17] to batched inference, challenges\narise primarily due to the varying exit points of tokens within a batch. Given that tokens exit at\ndiverse layers, it\u2019s necessary to persist computation until the final token of each batch member and\neach position is processed. This diminishes the benefits that would otherwise be realized when the\nbatch size exceeds one, thus undermining the potential advantages of parallel computation.\nTo tackle this, we suggest a method that designates a fixed positionwise exit point for every token in\na batch at a given sequence position. This strategy ensures that the processing of all batched tokens\nat a certain sequence position concludes at the same time. As a result, it assures that all theoretical\nbenefits observed during evaluation are fully actualized during generation for non-trivial batching\nscenarios.\nLet B be the batch size, and N the sequence length. We construct the batches column-wise using\ntokens at a specific position across all the instances. Considering ts,n to be the token in sequence s at\nposition n, a given batch consists of tokens from t(\u00b7),n. Let L(tb,n) be the layer at which token tb,n\nexits. We ensure that \u2200n \u2208 [1, N], \u2200b1, b2 \u2208 [1, B], L(tb1,n) = L(tb2,n). Further, the autoregressive\ndecoding for token generation ensures that the columns are processed left to right such that the\ncomputation for tokens at position n can utilize all the network computation from processing those at\nthe previous position n \u2212 1.\n2.2\nKV Caching and Monotonically Decreasing Exit Points\nKey-Value(KV) KV caching is a critical optimization technique for efficiently executing attention\nmechanisms in Transformer models. By storing the computed keys and values for previously\nprocessed tokens, the model can significantly reduce redundant computations when attending to the\nsame context during subsequent steps. This optimization leads to faster inference times.\nYet, when utilizing token-level early exit strategies , the different exit points of tokens in a sequence\npresent another challenge. Specifically, there\u2019s a requirement to recompute the Key-Value (KV)\ncaches for preceding tokens if the current token exits at a higher layer. This necessary recalculation\nescalates computational workload and undermines the advantages of early exit methods, as the\ncomputation of each preceding token is bounded by the computation of later tokens.\nOur proposed solution, which assigns a unified exit point to all tokens in a batch at a given position,\neffectively addresses this challenge. By ensuring that batched exit points are monotonically decreasing\nas the sequence progresses, we guarantee that previous tokens have performed at least as much\n3\ncomputation as the current one, thereby trivially avoiding the need for any extra computation. The\nright plot in figure 2 shows how every layer can attend to leftward attention layers without any\nre-computation or change in the architecture.\nThe underlying rationale is that next-word prediction at the beginning of a sequence is more chal-\nlenging due to limited context, and therefore earlier tokens will benefit from later exit points in the\ncomputation graph. Prior work [17] have already showed that noise or perturbations have a greater\nimpact on the overall task performance when the perturbation happens in the earlier tokens resulting\nin cascading of errors due to autoregressive generation. Moreover, as the context grows with the\nsequence, the later tokens become more predictive with more context resulting in their hidden states\nsaturating faster (i.e. hidden states have limited variance across layers). Therefore, later tokens\nrequire less computation and thus enabling a more efficient use of computational resources [8]. We\ndemonstrate this intuition in Figure 1, where earlier tokens in a sequence have higher losses and are\nmore difficult to generate in contrast to the ones appearing later that are more predictive.\n2.3\nControlling Computational Budget with Batched Exit Points\nTraditional early exit techniques typically learn exit points for individual tokens [17]. However, apart\nfrom the limitations mentioned in the previous subsections, controlling the computational budget can\nbe challenging. Usually, a classifier is used to decide whether a token should exit at a specific layer,\nresulting in the worst-case computational budget scenario being close to the cost of using the full\nnetwork (for instance, bad exit point close to the last layer).\nWe address this issue by pre-specifying maximum and minimum exit points (the maximum and\nminimum number of layer that each token should go through), which controls the computational cost\nvia the number of active model parameters. Exit points across the sequence are assigned in such a\nway that no token exceeds the maximum nor falls below the minimum exit point keeping the total\ncomputational cost bounded. Additionally, as explained earlier, the assignment of exit points across\nthe sequence is required to be monotonically decreasing. This implies that the first token will be\nassigned the maximum exit point, and the last token, according to the maximum length parameter,\nwill be assigned the minimum exit point.\nA predefined function progressively designates exit points to the tokens in the sequence. This function\ncan adopt multiple forms and serves as an additional hyperparameter of the model, managing the\ncomputational expenditure. In the evaluation phase, we conduct tests using linear decay bounded\nby a a minimum and a maximum number of layers. Note that employing other functions (such as\npower-law) could lead to more significant accelerations and will be the subject of future studies.\nFormally, consider a sequence and network with hyper-parameters: sequence_length, min_exit_layer,\nmax_exit_layer, num_decoder_layers, and prompt_size. We define an array token_idx as:\ntoken_idx[i] =\n\u001anum_decoder_layers\nif i < prompt_size\n(1 \u2212 ti) \u00d7 max_exit_layer + ti \u00d7 min_exit_layer\nif i \u2265 prompt_size\nwhere ti =\ni\u2212prompt_size\nsequence_length\u2212prompt_size.\n0\nprompt_len\nmax_len\nmin_layer\nmax_layer\nnum_decoder_layers\nSequence Position\nExit Layer\nExit Layer vs Sequence Position\nComputational Budget\nFigure 3: Linear decay of exit layer with respect to the sequence position.\n4\nIn the above design, we can process all the tokens in the prompt with the full computational power of\nthe network (i.e. using all the decoder layers). This can be done efficiently with batching since there\nis no generation involved in prompt processing. As soon as we start autoregressive token generation\nfrom prompt_len + 1, we start decaying the number of active layers acting on any token bounded by\nthe max and min layers pre-specified by the computational budget.\n2.4\nSkipping vs. Early Termination\nEarly termination-based methods allow tokens to exit the model early once their hidden states have\nsaturated[17]. However, token-level early termination can present problems in generative models\nwhen previous tokens exit earlier than the later tokens. In this scenario, the later tokens are unable\nto benefit from the extra computation performed by the former tokens via the attention mechanism,\neffectively under utilizing the available context.\nTo overcome this limitation, we propose performing skipping instead of early termination. We\nensure that the computational budget for each token is allocated to higher layers of the model. Now\nregardless of the exit points, tokens will be able to attend to the top layers of all previous tokens\neffectively attending to all the available context.\nTo bridge the representation gap between the initial embedding layer and the top layers, we introduce\nthe concept of warmup layers. Warmup layers represent the initial computation that will be performed\non the x bottom layers before skipping to the top y layers to exhaust the rest of the computational\nbudget. In experiments we observe that this approach effectively reduces the distance between the\ntoken embeddings and the top layer\u2019s hidden states. In experimental evaluation, we consistently\nfound the number of warmup layers to be 1 that worked the best across all settings.\n3\nEvaluation\nWe demonstrate our techniques with OPT [23] decoder-only language models of 1.3b and 6.7b\nparameters (24 and 32 layers respectively) on three benchmark text generation datasets: E2E [13],\nReddit-TLDR [21], and CNN-DM [6]. We implement SkipDecode using the metaseq codebase1.\n3.1\nExperimental Design\nGiven a pre-trained LLM (base model) assigned a 1\u00d7 speedup, our goal is to reduce the amount\nof computation performed by the network during inference using SkipDecode. We evaluate our\nmethod with configurations corresponding to 2\u00d7, 3\u00d7, 4\u00d7 and 5\u00d7 speedups. Note that speedups are\nreported relative to the base model that intrinsically supports batching and KV caching. This\nspeedup comparison is different from prior early-exit works that consider a weaker base model with a\nbatch size of 1 and no KV caching.\nDifferent configurations of the maximum and minimum layer per token, as outlined in Section 2.3,\ncan achieve different amounts of speedup. We determine the optimal combination of maximum and\nminimum layers, along with the warm-up layers and learning rate, for each specified speedup through\nhyper-parameter tuning on the e2e dataset. We select the optimal one based on the perplexity metric\non the validation set. It should be noted that the actual speedup may slightly vary during generation as\nit\u2019s impossible to predict in advance the number of tokens that the model will generate. However, the\ncomputational budget is strictly bounded by the minimum layer assigned to the maximum sequence\nlength for each dataset. The configurations used are presented in Table 2.\nFor training, we used the median training prompt length from each dataset for all instances, ensuring\nthat all layers are processed to mimic the desired generation behavior as illustrated in figure 3.\nIt\u2019s worth noting that our approach is effective yet simple and easy to implement. Besides the token\nskipping policy, it does not necessitate any additional modifications to the transformer architecture,\neither during training or generation.\n1https://github.com/facebookresearch/metaseq\n5\nOriginal / Base\nTarget\n#Target Avg\n#Warm up\n#Min\n#Max\nNumber of Layers\nSpeedup (\u00d7)\nLayer\nLayer\nLayer\nLayer\n32 (6.7B)\n2\n16\n1\n11\n22\n3\n11\n1\n8\n14\n4\n8\n1\n6\n10\n5\n6.5\n1\n5\n8\n24 (1.3B)\n2\n12\n1\n8\n16\n3\n8\n1\n6\n10\n4\n6\n1\n5\n7\n5\n5\n1\n4\n6\nTable 2: SkipDecode configurations for different target speed-ups w.r.t Base OPT (1.3B and 6.7B)\nobtained using the E2E validation set corresponding to the least perplexity.\n3.2\nDatasets\nWe perform experiments on three benchmark datasets. Examples of generation on each dataset are\nshown in Table 3. For generation, in all cases we employ a beam of 1, top-sampling of 0.7, and a\ntemperature of 0.3. For training we sweep over learning rates in the range of 2e-4 to 8e-6.\nDataset\nContext\nResponse 2x\nResponse 5x\nE2E\nname[Blue\nSpice],\neatType[coffee\nshop],\ncustomer\nrating[average],\nnear[Burger King]\nThe Blue Spice coffee shop\nlocated near Burger King has\nbeen rated average by cus-\ntomers.\nBlue Spice is a coffee\nshop near Burger King.\nIt has an average cus-\ntomer rating and is lo-\ncated near the Burger\nKing.\nReddit-TLDR\n\"SUBREDDIT: r/relationships TITLE: This guy I\u2019ve been casually dating\n[18M] doesn\u2019t want to have a relationship with me [18F] because he\u2019s going\nto college in the fall POST: Here\u2019s a bit of context for y\u2019all: We both met\nfreshmen year in our school\u2019s theatre program. At the end of freshman year, I\ntransferred to...\nGuy I\u2019ve been casually dating\nwants to break up with me be-\ncause he\u2019s going to university\nin the fall and I have to stay in\nhigh school for another year.\nGuy I\u2019ve been dating\nhas been dating for a\nwhile, he\u2019s going to uni-\nversity in the fall, I\u2019m\ncrushed and don\u2019t know\nhow to proceed.\nCNN-DM\n(CNN)The terrorist group Al-Shabaab has claimed an attack on Garissa Uni-\nversity College in eastern Kenya, in which many people have been killed and\nstill more taken hostage. The attack is another step in the ongoing escalation of\nthe terrorist group\u2019s activities, and a clear indicator that the security situation\nin East Africa is deteriorating fast. Somalia-based Al-Shabaab has been behind\na string of recent attacks in Kenya, the most well-known of them being the\nmassacre at the Westgate Shopping Centre in Nairobi in 2013. Cross-border\nraids into Kenya by the group, however, date back to 2011. Al-Shabaab incur-\nsions triggered a military...\nAl-Shabaab claims attack on\nGarissa University College in\nKenya .Attack is another step\nin the ongoing escalation of\nterrorist group\u2019s activities .Al-\nShabaab has been behind a\nstring of recent attacks in\nKenya .The group is predomi-\nnantly driven by the same radi-\ncal interpretation of the Koran\nas al-Qaeda .\nAl-Shabaab has claimed\nan attack on Garissa\nUniversity College in\nKenya .Al-Shabaab has\nbeen behind a string of\nrecent attacks in Kenya\n.Al-Shabaab has been\nbehind a string of recent\nattacks in the region.\nTable 3: Snapshot of dataset and model responses.\nE2E [13]. The task is to convert structured information from key-value pairs into fluent text. It\nis relatively small, comprising of 42061 training samples, 4672 evaluation samples, and 4693 test\nsamples. The median prompt contains 38 tokens. We set a maximum sequence length of 160 and a\nmaximum prompt length of 60, both for training and generation. The effective batch size is 256. We\nuse a breakline to separate the prompt and completion with 650 warm-up steps and 8 epochs.\nReddit-TLDR [21]. A summarization dataset that includes a training size of 117,000 samples, an\nevaluation size of 6450, and a test size of 6550 samples. The median training prompt is 348 tokens\nlong. We utilize 200 warm-up steps, an effective training batch of 32, and 3 epochs. The maximum\nprompt length is set at 512 tokens, and the maximum sequence length at 1024 tokens. The separator\nbetween context and completion is \"\\nTl;dr\\n\".\nCNN Daily Mail [6]. Requires writing a summary given an article as input. It is a large dataset with\na training size of 287,113 samples, an evaluation size of 13,368, and a test size of 11,490 samples.\nThe median train prompt length is 788 tokens. We set the maximum sequence length at 2048 and the\nmaximum prompt length at 1024. The warm-up updates are set to 650, the effective batch size is 32,\nand we train for 2 epochs. The separator between context and completion is \"\\nTl;dr\\n\".\n3.3\nKey Results\nResults are presented in Table 4. SkipDecode demonstrates significant improvement in computa-\ntional efficiency for each dataset and model size. As depicted in Figure 4, there is no noticeable\n6\n(a) E2E\n(b) Reddit\n(c) CNN_DM\nFigure 4: Rouge-L vs inference speedup for 1.3B and 6.7B OPT models. Speedup is computed\nover base model (1\u00d7) that inherently supports batching and KV caching in contrast to prior work\nconsidering a weaker base model without batching as reference.\nperformance degradation from 1\u00d7 (base model) to 2\u00d7 speedup, after which there is steady decline\nin performance with increasing speedups. We hypothesize that this is due to the tokens reaching\nthe hidden state saturation point, beyond which further computation reduction leads to performance\ndegradation. This pattern is consistent across datasets. We notice a delayed degradation in E2E, while\nCNN-DM starts to degrade more quickly given its relative difficulty.\nE2E SkipDecode As the target speedup increases from 1\u00d7 to 5\u00d7, the average number of decoder\nlayers active in the generation process reduces, signifying a decrease in the computation load.\nInterestingly, all the task measures corresponding to the Bleu, Rouge-L, and Bert-F scores remain\nrelatively steady with a minor decline as the target speedup increases. This indicates that our method\ncan accomplish significant speedups with minimal degradation for certain task settings.\nReddit Similar to the others, the average generation layer decreases as the target speedup increases.\nHowever, the performance metrics such as Bleu, Rouge-L, and Bert-F scores display more significant\nreductions compared to the E2E dataset, given the relative difficulty of this task. Wile our method\nstill achieves significant speedups, the trade-off in terms of task performance is more noticeable.\nCNN-DM The results follow a similar trend to the previous: as the target speedup increases, the\naverage generation layer decreases, indicating reduced computational requirements. However, the\nperformance metrics such as Bleu, Rouge-L, and Bert-F scores drop more significantly as the target\nspeedup increases. While our approach can achieve substantial speedups, the trade-off in task\nperformance is more pronounced, as the hidden state saturation is reached earlier.\nIn conclusion, our method consistently demonstrates an ability to decrease computational demands\nacross all datasets and model sizes, effectively determining the hidden state saturation point. The\nimpact on task performance, as measured by Bleu, Rouge-L, and Bert-F scores, varies depending on\nthe specific dataset. However, in all instances, our method shows a favorable balance between speedup\nand task performance, reaching a 2\u00d7 speedup with almost no degradation in all cases. This balance\ncan be effectively exploited as our approach adeptly handles practical challenges like batching and\nKV caching while maintaining a controlled and predictable computational budget.\n3.4\nComparison to other Methods\nIn order to benchmark SkipDecode, we have adapted two concepts from the CALM framework to\nfunction on decoder-only models, though no method is currently available that corresponds directly\nwith SkipDecode. In both cases we use OPT 1.3b as the base model.\nFirstly, we train a multi-layer exit network following the method outlined in [17], wherein a single\nmodel head is trained to exit from each layer. This approach is more akin to an early termination\nmethod with truncation as it operates with a fixed policy, running up to a predetermined exit layer\napplicable to all tokens within the sequence. Notably, this model supports batching and KV Caching.\nThe second method uses the same model, with an additional application of the CALM\u2019s hidden\nstate saturation concept, adapted to function with a decoder-only network (referred as CALM-DEC).\nHowever, this network imposes a limitation on the batch size to just one, precluding batching and\nKV Caching. Consequently, the model must \u2019back-fill\u2019 all KV values for previous tokens as required\n7\nDataset\nSize\nTarget\n#Target Avg\n#Gen Avg\nBleu\nRouge-L\nBert-F\nSpeedup\nLayer\nLayer\nE2E\n1.3b\n1\n24\n24\n65.8\n67.6\n70.3\n2\n12\n14.7\n66.3\n67.9\n67.8\n3\n8\n9.4\n66.3\n68.1\n67.3\n4\n6\n6.8\n65.6\n66.8\n66.5\n5\n5\n5.8\n64.2\n66.3\n65.2\n6.7b\n1\n30\n30\n64.2\n66.6\n70.8\n2\n15\n20.3\n65.3\n68.2\n67.6\n3\n11\n13\n65.9\n68.0\n67.7\n4\n8\n9.4\n66.9\n67.9\n67.1\n5\n6.5\n7.6\n64.0\n65.7\n65.2\nRedit\n1.3b\n1\n24\n24\n9.0\n27.3\n31.9\n2\n12\n15.6\n8.9\n27.5\n32.1\n3\n8\n9.9\n7.0\n25.1\n22.9\n4\n6\n6.4\n3.9\n21.3\n11.5\n5\n5\n5.0\n3.0\n19.6\n7.1\n6.7b\n1\n30\n30\n9.6\n28.3\n33.7\n2\n15\n19.8\n9.3\n27.7\n32.3\n3\n11\n13.7\n8.0\n26.0\n25.3\n4\n8\n9.4\n5.2\n21.3\n9.3\n5\n6.5\n6.5\n4.0\n19.3\n7.4\nCNN-DM\n1.3b\n1\n24\n24\n15.8\n29.5\n35.9\n2\n12\n15.6\n15.0\n28.9\n34.8\n3\n8\n8.9\n7.8\n23.3\n20.2\n4\n6\n6.2\n3.2\n18.6\n2.3\n5\n5\n5.3\n4.0\n18.1\n2.5\n6.7b\n1\n30\n30\n16.3\n30.2\n37.1\n2\n15\n21.3\n15.2\n29.6\n35.9\n3\n11\n11.8\n4.8\n21.8\n17.9\n4\n8\n8.5\n5.4\n20.2\n7.9\n5\n6.5\n6.9\n4.6\n18.5\n2.7\nTable 4: SkipDecode performance on different datasets for varying speedups and base model sizes.\n(in this case, by projecting the last known hidden state at that layer), which adds significant systems\noverhead. The worst case computational cost of this approach is equivalent to the full network cost.\nThe adaptive hidden state saturation policy on this network has the standard disadvantages of a non-\nfixed policy for both batching and computation/time estimates. In addition, its performance degrades\nstrongly with increasing speedups especially on larger decoder-only models for the following reason.\nThe KV backfill affects the prompt encoding, which is extremely important for these tasks. In an\nencoder-decoder architecture like the T5 model in CALM [17], the KV backfill retains the prompt\nencoding. Whereas decoder-only architectures simultaneously encode and decode their past state,\nmeaning that early termination is more likely to affect the network\u2019s understanding of previous context\n(refer to Appendix). This results in our CALM implementation showing significant degradation for\ndecoder-only models as opposed to the original T5 encoder-decoder implementation.\nSpeedup\nE2E\nReddit-TLDR\nSkipDecode\nMulti-layer\nCALM-DEC\nSkipDecode\nMulti-layer\n1\n67.6\n68.7\n68.7\n27.3\n26.3\n2\n67.9\n65.7\n35.8\n27.5\n17.2\n3\n68.2\n61.5\n32.1\n25.1\n12.7\n4\n66.8\n50.8\n27.7\n21.3\n7.9\n5\n66.3\n46.7\n22.8\n19.3\n6.5\nTable 5: Performance comparison between SkipDecode, Multi-layer, and CALM-DEC.\nAs can be observed from Table 5, SkipDecode exhibits a superior performance over other approaches.\nThis is demonstrated by the notably less degradation in task performance across both datasets as the\nspeedup factor increases. This showcases the robustness of our method against increasing speedup.\n8\n4\nRelated Work\nModel compression: There has been extensive research in model compression to develop techniques\nto improve the inference efficiency of large language models (LLMs). One of the most prominent\nlines of work leverage knowledge distillation (KD) [7] to train smaller student models with faster\ninference using representations from LLMs as teachers like hidden states and attention states [10, 19].\nAnother line of work in model compression use quantization [3], low-precision training and network\npruning [5], parameter sharing and factorization to reduce the memory footprint as well as network\nlatency [4, 20]. Notably most of the above research in model compression has focused on encoder-only\nmodels for natural language understanding tasks.\nEarly-exit: In contrast to the above works that use static computation i.e. the same computation being\ninvoked on every input, we focus on adaptive compute with variable computation for different parts\nof the input. Existing adaptive computation techniques primarily rely on early-exit strategies [25, 24,\n22, 12, 11, 9] where a token in the input learns to exit from different layers of the network. Similar to\nthe works in KD, most of these techniques were developed for encoder-only models like BERT [2]\nfor natural language understanding (NLU) tasks. In contrast to NLU tasks that requires processing\nof the sequences as a whole, generation tasks are more complex given their autoregressive nature\nfor token-by-token generation. A recent work, CALM [17] study token-level early exit strategies\nfor generation tasks in terms of what confidence measure to use; how to connect sequence-level\nconstraints to local per-token exit decisions; and how to attend back to missing hidden representations\ndue to early exits in previous tokens. However, similar to all the prior early-exit works, CALM suffers\nfrom some major practical blockers related to batching (only supporting a batch size of 1) and KV\ncaching which are widely used to speedup inference in practice. Further, the worst-case scenario for\nall such exit-strategies (e.g., exit point closer to the top layer for any token) can lead to using the\nfull network resulting in unpredictable system load and inconsistent throughput. To address these\nchallenges, we develop SkipDecode that supports non-trivial batching and KV caching for efficient\ninference, as well as guarantees a predictable computational load with no surprises.\n5\nLimitations and Future Directions\nSkipDecode addresses pivotal issues like batching and Key-Value (KV) caching being inherently\nincompatible with existing token-level early exit strategies. However, the introduction of the decaying\npolicy adds a new limitation. As the generation progresses and samples in the batch finish their\ncomputations, new samples can be included in the batch only if their current position matches the\nremaining elements\u2019 positions. Therefore, our method does not naturally support the \u2018infinite loop\u2019\ninference mode.\nIn preliminary experiments, a power law decay function did not yield improvements over the linear\ndecay employed in this study. Notably, prior research indicate a power law distribution for token exit\nlevels [17]. Our Oracle exploratory experiments, depicted in Figure 1, corroborate this observation.\nInvestigating alternative decay functions presents an intriguing avenue for future work.\nAnother promising research direction involves examining the decaying policy\u2019s impact on the prompt.\nIn accordance with previous studies, we have utilized the full network for the prompt. Additional\nspeedup gains may be attainable by extending the policy to the prompt and implementing more\naggressive decay functions, as mentioned earlier. This could pave the way for more efficient and\nversatile token-level early exit strategies.\n6\nConclusions\nSkipDecode bridges the gap between the theoretical benefits of token-level early exits and real-world\napplication requirements. It adeptly addresses practical challenges like batch processing and key-\nvalue caching. Moreover, it consistently exhibits the capability to reduce computational requirements\nby identifying the saturation point of hidden states with a controlled computational budget. This\nnot only enhances efficiency but also fosters a more accessible and sustainable AI ecosystem. To\nfurther amplify these improvements, future efforts can concentrate on enhancing dynamic batching\nand delving deeper into the behavior of decay functions.\n9\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates,\nInc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4171\u20134186, 2019.\n[3] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev. Compressing deep convolutional\nnetworks using vector quantization. CoRR, abs/1412.6115, 2014.\n[4] Manish Gupta and Puneet Agrawal. Compression of deep learning models for text: A survey.\nACM Trans. Knowl. Discov. Data, 16(4):61:1\u201361:55, 2022. doi: 10.1145/3487045. URL\nhttps://doi.org/10.1145/3487045.\n[5] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural\nnetworks with pruning, trained quantization and huffman coding. ICLR, 2016.\n[6] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural\ninformation processing systems, 28, 2015.\n[7] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural\nnetwork. CoRR, abs/1503.02531, 2015.\n[8] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. CoRR, abs/1904.09751, 2019.\n[9] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic\nBERT with adaptive width and depth. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.\ncc/paper/2020/hash/6f5216f8d89b086c18298e043bfe48ed-Abstract.html.\n[10] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun\nLiu. Tinybert: Distilling bert for natural language understanding, 2019.\n[11] Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, and Xu Sun. Cascadebert:\nAccelerating inference of pre-trained language models via calibrated complete models cascade.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November, 2021, pages 475\u2013486. Association for\nComputational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.43. URL https:\n//doi.org/10.18653/v1/2021.findings-emnlp.43.\n[12] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. Fastbert: a self-\ndistilling BERT with adaptive inference time. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\nand Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6035\u20136044. Association\nfor Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.537. URL https:\n//doi.org/10.18653/v1/2020.acl-main.537.\n10\n[13] Jekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser. The e2e dataset: New challenges for\nend-to-end generation. arXiv preprint arXiv:1706.09254, 2017.\n[14] Alec Radford and Karthik Narasimhan. Improving language understanding by generative\npre-training. 2018.\n[15] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.\n[17] Tal Schuster, Adam Fisch, Jai Prakash Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran,\nYi Tay, and Donald Metzler. Confident adaptive language modeling. CoRR, abs/2207.07061,\n2022.\n[18] Tianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng, Lingling Wu, Yilong He, Yuan\nNi, Guotong Xie, Xuanjing Huang, and Xipeng Qiu. A simple hash-based early exiting\napproach for language understanding and generation. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio, editors, Findings of the Association for Computational Linguistics:\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages 2409\u20132421. Association for Computational\nLinguistics, 2022. doi: 10.18653/v1/2022.findings-acl.189. URL https://doi.org/10.\n18653/v1/2022.findings-acl.189.\n[19] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\nMobilebert: a compact task-agnostic bert for resource-limited devices.\narXiv preprint\narXiv:2004.02984, 2020.\n[20] Marcos V. Treviso, Tianchu Ji, Ji-Ung Lee, Betty van Aken, Qingqing Cao, Manuel R. Ciosici,\nMichael Hassid, Kenneth Heafield, Sara Hooker, Pedro Henrique Martins, Andr\u00e9 F. T. Martins,\nPeter A. Milder, Colin Raffel, Edwin Simpson, Noam Slonim, Niranjan Balasubramanian,\nLeon Derczynski, and Roy Schwartz. Efficient methods for natural language processing:\nA survey. CoRR, abs/2209.00099, 2022. doi: 10.48550/arXiv.2209.00099. URL https:\n//doi.org/10.48550/arXiv.2209.00099.\n[21] Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn\nautomatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization,\npages 59\u201363, 2017.\n[22] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early\nexiting for accelerating BERT inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and\nJoel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 2246\u20132251. Association\nfor Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.204. URL https:\n//doi.org/10.18653/v1/2020.acl-main.204.\n[23] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068,\n2022.\n[24] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, and Furu Wei. BERT loses\npatience: Fast and robust inference with early exit. In Hugo Larochelle, Marc\u2019Aurelio Ranzato,\nRaia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Informa-\ntion Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.\ncc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html.\n[25] Wei Zhu. Leebert: Learned early exit for BERT with cross-level optimization. In Chengqing\nZong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th International Joint Conference\n11\non Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\nAugust 1-6, 2021, pages 2968\u20132980. Association for Computational Linguistics, 2021. doi:\n10.18653/v1/2021.acl-long.231. URL https://doi.org/10.18653/v1/2021.acl-long.\n231.\n12\n"
  },
  {
    "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
    "link": "https://arxiv.org/pdf/2307.03166.pdf",
    "upvote": "5",
    "text": "VideoGLUE: Video General Understanding\nEvaluation of Foundation Models\nLiangzhe Yuan1\u2217\u2020\nNitesh Bharadwaj Gundavarapu1\u2217\nLong Zhao1\u2217\nHao Zhou1\u2217\nYin Cui1\nLu Jiang1\nXuan Yang1\nMenglin Jia1,2\u2021\nTobias Weyand1\nLuke Friedman1\nMikhail Sirotenko1\nHuisheng Wang1\nFlorian Schroff1\nHartwig Adam1\nMing-Hsuan Yang1\nTing Liu1\nBoqing Gong1\n1Google Research\n2Cornell University\nAbstract\nWe evaluate existing foundation models\u2019 video understanding capabilities using a\ncarefully designed experiment protocol consisting of three hallmark tasks (action\nrecognition, temporal localization, and spatiotemporal localization), eight datasets\nwell received by the community, and four adaptation methods tailoring a foundation\nmodel (FM) for a downstream task. Moreover, we propose a scalar VideoGLUE\nscore (VGS) to measure an FM\u2019s efficacy and efficiency when adapting to general\nvideo understanding tasks. Our main findings are as follows. First, task-specialized\nmodels significantly outperform the six FMs studied in this work, in sharp contrast\nto what FMs have achieved in natural language and image understanding. Second,\nvideo-native FMs, whose pretraining data contains the video modality, are generally\nbetter than image-native FMs in classifying motion-rich videos, localizing actions\nin time, and understanding a video of more than one action. Third, the video-native\nFMs can perform well on video tasks under light adaptations to downstream tasks\n(e.g., freezing the FM backbones), while image-native FMs win in full end-to-end\nfinetuning. The first two observations reveal the need and tremendous opportunities\nto conduct research on video-focused FMs, and the last confirms that both tasks\nand adaptation methods matter when it comes to the evaluation of FMs. Our code\nis released under https://github.com/tensorflow/models/tree/master/\nofficial/projects/videoglue.\n1\nIntroduction\nFoundation models (FMs) are a term coined by Bommasani et al. [7], referring to \u201cany model that is\ntrained on broad data that can be adapted (e.g., finetuned) to a wide range of downstream tasks.\u201d Some\nrepresentative FMs include but are not limited to BERT [13], GPT-3 [8], CLIP [40], and ALIGN [26].\nThis work primarily investigates the video understanding capabilies of six visual and multimodal\nFMs: CoCa [59], CLIP [40], FLAVA [47], VideoMAE [48], VATT [1], and InternVideo [55]. These\nmodels are selected because they are amendable for the video understanding of our interest and make\ntheir checkpoints accessible to us.\nIt is nontrivial to evaluate FMs. In contrast to \u201cspecialist\u201d models developed for a particular task,\nFMs are considered as \u201cgeneralists\u201d that learn shareable meta-knowledge across tasks so that one can\n\u2217Equal technical contributions.\n\u2020Corresponding to lzyuan@google.com.\n\u2021Work done as a student researcher at Google.\nPreprint. Under review.\narXiv:2307.03166v2  [cs.CV]  1 Dec 2023\nVC(ML)\nVC(M)\nVC(M)\nVC(A)\nVC(A)\nTAL\nSTAL\nSTAL\n44.8\n39.8\n34.8\n29.8\n76.9\n81.7\n86.5\n91.3\n38.1\n36.9\n35.7\n34.5\n64.8\n61.6\n58.3\n71.2\n66.2\n61.3\n56.4\n84.8\n87.3\n89.9\n82.2\n38.7\n40.7\n42.6\n44.6\n40.5\n37.3\n34.1\n30.8\n(19.7-42.3)\n(24.4-38.9)\n(32.3-37.5)\n(77.1-88.6\uff09\n(34.8-43.6)\n(67.3-88.9)\n(46.6-68.7)\n(48.6-63.2)\nFigure 1: FMs vs. state-of-the-art task-specialized models on video understanding. Unlike natural\nlanguage and image understanding, video tasks are where FMs generally fall behind \u201cspecialists\u201d.\nVC(A), VC(M), and VC(ML) stand for appearance-focused, motion-focused, and multi-labeled video\nclassification, respectively. STAL stands for spatiotemporal action localization, and TAL stands for\ntemporal action localization. For each task, we include the (min-max) range shown in the figure.\nquickly adapt them to achieve superior performance on various downstream tasks. Hence, both the\ntasks and adaptation methods matter when it comes to evaluation. However, the community has not\nreached a consensus on these two aspects. FM developers select their own different sets of downstream\ntasks \u2014 interestingly, often covering no video or only appearance-rich video classification tasks\n[9, 30]. Moreover, they rely on distinct adaptation methods, making apples-to-apples comparisons\nchallenging and causing mismatches with the FMs\u2019 actual use cases.\nTo this end, we propose to evaluate FMs\u2019 video understanding capabilities using a carefully designed\nexperiment protocol, named VideoGLUE, consisting of three hallmark tasks (action recognition,\ntemporal localization, and spatiotemporal localization), eight datasets well received by the research\ncommunity, and four model adaptation methods tailoring a foundation model for downstream tasks.\nThe tasks examine an FM from various aspects needed for understanding video. The \u201call-around\u201d\nadaptations represent the main use cases of FMs in the literature and, more importantly, allow us to\nthoroughly probe an FM\u2019s potential in video understanding.\nWhy do we specifically focus on videos? The main motivation is to promote video understanding in\nthe evaluation of FMs. More concretely, we test the following conjectures through this work. First,\nFMs\u2019 high performance on existing evaluation suites does not necessarily indicate their potential\nin video since these suites either lack video-specific tasks or selectively choose video tasks whose\nappearance feature is more important than motion \u2014 InternVideo [55] is an exception as discussed in\nthe next paragraph. Second, many existing FMs probably cannot heed motion in video, given that they\nlearn primarily from static images [40, 47, 59] or short video clips containing limited motion [18, 55].\nThird, popular adaptation methods (e.g., finetuning all weights) cannot supplement FMs with all the\ncues needed to recognize motion-rich actions and localize entities temporally and/or spatiotemporally.\nSections 4.1 and 4.2 elaborate on this point.\nWhile our work is not the first to emphasize the evaluation of FMs, it is unique on multiple fronts.\nUnlike Elevater [32]\u2019s target of evaluating language-augmented FMs, we consider all FMs adaptable\nto video understanding. Unlike Perception Test [4]\u2019s coverage of a broad spectrum of perception\ntasks, we focus on video, allowing us to cover various aspects of this vertical domain. Interestingly,\nmany of our datasets also appear in InternVideo [55], a video-oriented FM. However, we promote\nmodel adaptation methods as an inherent part of the evaluation protocol \u2014 a consistent set of\ndiverse adaptation methods is necessary to provide FMs ample opportunities to expose their video\nunderstanding capabilities. Moreover, unlike InternVideo\u2019s focus on their single FM, we evaluate\n2\nFMs developed by different research groups in an uniform experiment protocol \u2014 the first of its kind\nfor visual and multimodal FMs, to the best of our knowledge.\nOur main findings are as follows. First, task-specialized models still significantly outperform the\nsix FMs studied in this work (see Figure 1), in sharp contrast to what FMs have achieved in natural\nlanguage [38, 44] and image understanding [40, 59, 11]. Hence, there is a need and tremendous\nopportunities to research video-focused FMs. Second, video-native FMs, whose pretraining data\ncontains the video modality, are generally better than image-native FMs in classifying motion-rich\nvideos, localizing actions in time, and understanding a video of more than one action. Third, the\nvideo-native FMs can perform well on video tasks under light adaptations to downstream tasks\n(e.g., freezing the FM backbones), while image-native FMs win in full end-to-end finetuning. This\nobservation confirms that both tasks and adaptation methods matter when it comes to the evaluation\nof FMs.\n2\nRelated work\nFMs.\nOne common type of FMs are Large Language Models (LLMs) trained to acquire generic,\ntransferable, and diverse representations that can enable sample-efficient learning and knowledge\ntransfer across a broad range of downstream tasks. FMs are often trained with simple self-supervised\nlearning objectives such as predicting the next token in a sentence (e.g., GPT-3 [8], PaLM [12]),\nor denoising the masked tokens (e.g., BERT [13], UNILM [14], and BEiT [5]). An intriguing\ncharacteristic of FMs is their ability to gradually acquire new capabilities as the model grows and the\ntraining data size increases, despite being trained on simple learning objectives [56]. For example,\nPaLM [12, 3], a massive LM with 540 billion parameters has started to show new capabilities in tasks\nsuch as explaining jokes, solving math, and performing common-sense reasoning when scaled to over\n100B parameters.\nIn addition to self-supervised transformers, FMs in computer vision also encompass transformers\nspecifically trained to align image-text paired data. These FMs use learning objectives include\ncontrastive learning (e.g., CLIP [40]), denoising masked tokens (e.g., BEiT-3 [53]), predicting the\nnext token in a single modality (e.g., DALL-E [42]) or in the interleaved image-text sequence (e.g.,\nFlamingo, KOSMOS-1 [24]). Recent FMs are also trained on a mixture of these objectives (e.g.,\nCoCa [59], FLAVA [47], MAE [22]). For example, MAE combines autoencoder reconstruction\nobjective jointly with the denoising objective [22] that was extended to video [18, 48]. In our\nstudy, we choose six representative FMs (i.e., CoCa [59], CLIP [40], FLAVA [47], VideoMAE [48],\nVATT [1], and InternVideo [55]) due to their amendability on video understanding and accessibility\nof checkpoints.\nEvaluation of FMs.\nAs the mission of FMs is to enable sample-efficient knowledge transfer, the\ndesign of downstream tasks is critical to evaluate the capabilities and limitations of these models.\nThe evaluation of FMs is pioneered by the NLP researchers. For example, GLUE [50] and Super-\nGLUE [49] introduced a suite of tools for evaluating language understanding tasks. The authors\nutilized established public benchmarks and provided tools for evaluating, probing, and benchmarking\npretrained FMs, allowing for a comparison to human baselines. ELEVATER [32] introduced this\nconcept to vision FMs along with a toolkit for evaluating vision-language tasks, including knowledge\naugmentation, hyperparameter tuning, and three adaptation techniques. In parallel, there have been\nattempts to establish a diagnostic benchmark for perceptual understanding of the world. For instance,\nPerception Test [4] crowd-sourced 11K videos in which about 100 users performed scripted activities.\nThis benchmark [4] comprises videos filmed by only about 100 participants, which may not provide\nthe same level of domain coverage and diversity as the other FM evaluation works mentioned earlier.\nEvaluation of video FMs.\nWhile some vision-language FMs have incorporated video tasks, their\nevaluation typically follows that of static images and neglects the unique aspects of video spatial-\ntemporal modeling and reasoning. To our knowledge, no previous work has been solely dedicated to\nevaluating Video FMs. The closest work to ours are InternVideo [55] and VideoMAE [48], which\nintroduce new FMs and show their superiority over several dozen video datasets. There are two key\ndifferences to the prior works. First, our evaluation is video-centric using the tasks that require motion\nunderstanding or long-term temporal reasoning. Second, instead of promoting new video FMs, our\nwork proposes no new models and is solely dedicated to evaluating current and future Video FMs in\n3\nTable 1: Foundation models (FMs) studied in this work (MxM stands for Masked {Image, Language,\nor Video} Modeling).\nFoundation Model\nModality\nPretraining Data\nPretraining Objective\nCoCa\nImage + Text\nJFT3B + ALIGN\nContrastive + Captioning\nCLIP\nImage + Text\nWebImageText\nContrastive\nFLAVA\nImage + Text\nPMD\nContrastive + MIM + MLM\nVideoMAE\nVideo\nK400\nMVM\nInternVideo\nVideo\nUnlabeledHybrid\nMVM + Contrastive\nVATT\nVideo + Audio + Text\nHT100M\nContrastive\nTable 2: Summary of statistics, video properties, and data sources of each dataset. Tasks involved\nare spatiotemporal action localization (STAL), temporal action localization (TAL), and video classifi-\ncation (VC). Column \"Num. videos\" contains video examples in train/evaluation splits, respectively.\nTask\nDataset\nNum. videos\nAvg. length\nData source\nNote\nSTAL\nAVA v2.2\n210, 634 / 57, 371\n15 mins\nMovie\nspatiotemporal, instance\nAVA-Kinetics\n354, 201 / 91, 919\n10 seconds\nWeb\nspatiotemporal, instance\nTAL\nActivityNet v1.3\n10, 002 / 4, 926\n5-10 mins\nWeb\ntemporal\nVC\nKinetics400\n235, 693 / 19, 165\n10 seconds\nWeb\nholistic, appearance\nMoments-in-Time\n791, 246 / 33, 898\n3 seconds\nWeb\nholistic, appearance\nSth-sth v2\n168, 913 / 24, 777\n2-6 seconds\nCrowd-source\nholistic, motion\nDiving48\n15, 027 / 1, 970\n5 seconds\nWeb\nholistic, motion\nCharades\n7, 811 / 1, 814\n30 seconds\nCrowd-source\nmulti-label, long-clip\nan impartial reproducible experimental setup. Concretely, our goal is to provide tools for probing and\nbenchmarking FMs on motion tasks in various setting include using the parameter-efficient adapter.\n3\nTasks and adaptation methods both matter when evaluating FMs\nThis section describes our video general understanding evaluation (VideoGLUE) benchmark. We first\nintroduce the visual and multimodal FMs evaluated in this work. Then we discuss the video-focused\ndownstream tasks and methods to adapt an FM to the tasks. The former concretizes the video\nunderstanding capabilities we want to evaluate from an FM, while the latter provides various paths\nfor an FM to showcase the corresponding capabilities.\n3.1\nFMs for video understanding\nWe are interested in examining which FMs are good at solving video tasks, what makes them better\nthan others in the video domain, and how to best adapt them to video understanding. Table 1 shows\nthe six FMs we gained access to via public repositories or personal communications.\n3.2\nVideo understanding tasks\nLike objects\u2019 role in image understanding, actions are the core of video understanding, leading us to\nselect tasks and datasets that recognize and localize actions in time and space. Table 2 provides a\nquick summary. Next, we explain the rationale behind the particular choices of datasets and postpone\nthe datasets\u2019 details to the supplementary materials.\n3.2.1\nRecognizing actions\nGeneral actions. We first include the action recognition datasets of Kinetics400 (K400) [28],\nMoments-in-Time (MiT) [37], and Charades [46], considering their popularity that they are being\ncomplementary to each other. Regarding data sources, K400 videos are from Youtube, MiT draws\nvideos from different Web venues, while Charades contains scripted videos. Regarding action labels,\nthe datasets differ in granularities and real-life scenarios, a verb defines an action in MiT, K400\ngroups actions by verb-subject pairs, and Charades actions are about indoor activities. Regarding\n4\nthe average length, K400 and MiT videos are between 3 and 10 seconds, each with one action label,\nwhile Charades videos are about 30 seconds, each with multiple actions.\nFine-grained motion-focused actions. We also include Something-something-v2 (SSv2) [20] and\nDiving48 (D48) [34] as another two action recognition datasets, whose actions are fine-grained and\nmotion-focused. SSv2 contains 174 human hand gestures as action labels, such as putting something\ninto something, turning something upside down, and covering something with something. D48 is all\nabout competitive diving. Notably, the foreground objects\u2019 motion is a more significant discriminative\ncue than their appearance.\n3.2.2\nLocalizing actions\nThe videos in action recognition are trimmed, but actions could occur anywhere in a video in the\nwild. Hence, temporal and spatiotemporal action localization is also crucial to video understanding.\nAccordingly, we choose three datasets for the experiments: the action localization track of ActivityNet\nv1.3 (ANet) [16], Atomic Visual Actions (AVA) [21], and AVA-Kinetics (AVA-K) [31]. The last\ntwo require a model to localize (and recognize) actions in both time and space, and their underlying\nvideos are movies and general YouTube videos, respectively.\n3.3\nAdaptation methods\nIn this section, we detail the task-specific neural architecture design and adaptation methods when\napplying FMs to downstream tasks.\n3.3.1\nModifying FM architectures for downstream tasks\nGiven a FM(\u00b7), we can apply FM(\u00b7) to a video clip C to extract a set of k feature maps {F}k =\nFM(C), F \u2208 Rn\u00d7h\u00d7w\u00d7c, where k is the number of endpoint layers from a FM, and n, h, w, c are\nrespectively a feature map\u2019s length, height, width, and number of channels.\nFor video classification tasks, we cast a feature map F as n \u00d7 h \u00d7 w tokens and aggregate them into a\nglobal representation using a learnable query token \u03c4 and lightweight cross-attention layers [15]. For\nspatiotemporal action localization, following the standard practice [19, 48], we first detect humans\non key-frames using a human detector [43], producing a set of human bounding boxes B. We then\napply the RoI pooling operation [25] that takes both the feature map F and box coordinates B as\ninputs and outputs one feature vector per box as the query token, \u03c4 = ROIPOOL(F, B), followed by\nthe same cross-attention layers as in video classification. For both groups of tasks, we stack a linear\nclassifier on top of the task token\u2019s last-layer encoding for final classification:\np = LINEARCLASSIFIER(CROSSATTENTION(\u03c4, F)).\n(1)\nFor temporal action localization, we first perform feature extraction in a sliding window manner,\nresulting in a sequence of globally average pooled features {AVGPOOL(F1), \u00b7 \u00b7 \u00b7 , AVGPOOL(Ft)}\nfor each video. Following a popular choice of prior works [2, 27, 36], we employ G-TAD [57] as our\ntask head for predicting the action category and its start and end timestamps.\n3.3.2\nAdapting the modified FMs\u2019 weights for downstream tasks\nAdapting the modified FMs to a downstream task is to tune their weights. Then, we immediately\nhave two basic adaptation strategies: 1) full finetuning to update all weights in the original FM plus\nthe task head and 2) freezing FM weights and only updating newly added weights. The choice of\nthe adaptation methods depends on specific application scenarios such as computation and memory\nconstraints. We argue that an ideal FM should perform well across various adaptation methods to\nsupport the breadth of use cases.\nEnd-to-end finetuning. End-to-end finetuning is the most common FM evaluation method for videos\n[1, 18, 48, 55], but it requires the deployment of a separate and possibly expensive FM for each\ndownstream task. When finetuning all weights in the modified FMs, we limit cross-attention to a\nsingle transformer layer with 12 heads and hidden size 768. We vary learning rates and weight decays\nfor each experiment to ensure every FM is configured to its best setup. Figure 2(a) illustrates this\nend-to-end finetuning.\n5\nFigure 2: We study four adaptation methods to apply a foundation model (FM) to video understanding\ndownstream tasks: (a) end-to-end finetuning, (b) frozen backbone evaluation, (c) frozen features with\nmulti-layer attention pooler (MLAP), and (d) a low-rank adapter.\nFrozen FM. Linear probing and cross-attention based pooling over frozen FM features are routinely\nused to test the strength of the FM representation [48, 59, 47, 22, 35]. In practice, adapting task-\nspecific heads with a frozen FM allows us to deploy the same FM for multiple tasks. If we use\nlight-weight heads over the FM features, then a single FM inference can serve multiple tasks efficiently\nin terms of both compute and memory. To this end, we examine two variations with a frozen FM,\none with a single cross-attention layer and the other with multiple layers. The first results in exactly\nthe same model architectures as in end-to-end finetuning (Figure 2(b)), and the second allows us to\nleverage an FM\u2019s hierarchical features beyond its last endpoint layer (Figure 2(c)). First, the frozen\nfeatures are extracted from the last k layers, FN\u2212k+1, FN\u2212k+2, ..., FN. Then, attention pooling\nis applied between a learnable token \u03c4 and the features FN\u2212k+1 using multi-head cross-attention\n(MHCA). The output of this layer serves as the query token for the next round of attention pooling\nwith the features FN\u2212k+2. This process is repeated for k rounds:\n\u03c4N\u2212k+1 = MLP(MHCA(\u03c4, FN\u2212k+1))\n\u03c4N\u2212k+2 = MLP(MHCA(\u03c4N\u2212k+1, FN\u2212k+2))\n...\n\u03c4N = MLP(MHCA(\u03c4N\u22121, FN))\n(2)\nwhere k = 4 in our experiments, and the final classifier is p = LINEARCLASSIFIER(\u03c4N).\nFrozen FM with a low-rank adapter. Finally, we explore a frozen FM beyond the last k layers\nusing a low-rank adapter [23], which is a bottleneck architecture that projects a feature tensor into a\nlow-dimensional space and then up-samples to the original space. The bottleneck space\u2019s dimension\nis 64 in our experiments. Inserting a few adapter layers with trainable weights {w} into the pretrained\nFM while keeping all FM\u2019s weights frozen, the feature adapter is more parameter-efficient than\nend-to-end finetuning the whole network while achieving better performance than simply adding a\ntask head to the frozen FM. Essentially, the adapter leads to a new g\nFM with some trainable weights\n{w}: \u02dcF = g\nFM(C, {w}), such that the output feature maps remain the same in shape as the original\nFM\u2019s output (Figure 2(d)). Hence, different pooling schemes and task heads aforementioned could be\napplied to the extracted feature map \u02dcF. For simplicity, we still choose the single-layer cross-attention\nas the default task head due to its computation efficiency and performance.\nThe low-rank adaptation allows a single FM for multiple tasks, in contrast to the per-task models\nin end-to-end finetuning. However, it incurs a per-task forward pass at inference time, being less\nefficient than the task-specific heads over frozen features.\n6\nTable 3: Evaluating FMs when adapted to video understanding tasks using end-to-end finetuning.\nWe report the Top-1 accuracy on K400, MiT, D48 and SSv2, MAP on Charades and ANet, and\nmAP@IOU0.5 on AVA and AVA-K.\nSTAL\nTAL\nVC (A)\nVC (M)\nVC (ML)\nMethod\nAVA\nAVA-K\nANet\nK400\nMiT\nD48\nSSv2\nCharades\nAVG\nCoCa\n27.7\n31.0\n\u2212\n82.6\n43.6\n79.6\n66.8\n55.0\n55.2\nCLIP\n27.1\n28.9\n\u2212\n81.0\n39.0\n75.7\n46.6\n54.3\n52.8\nFLAVA\n22.0\n25.6\n\u2212\n79.1\n38.3\n72.0\n61.1\n48.6\n49.4\nVideoMAE\n23.5\n26.2\n\u2212\n78.7\n36.1\n75.5\n65.5\n51.4\n51.0\nInternVideo\n27.2\n29.8\n\u2212\n80.1\n35.9\n75.8\n67.0\n52.2\n52.5\nVATT\n27.0\n28.4\n\u2212\n77.1\n34.8\n77.6\n65.1\n55.7\n52.7\nTask-specialized\n42.3\n38.9\n37.5\n88.6\n42.7\n88.9\n68.7\n63.2\n\u2212\nRAFT\nRAFT\nPRN\nTubeViT\nUniformerV2\nAIM\nMViT\nMoViNet\n4\nExperiments\n4.1\nEnd-to-end finetuning\nTable 3 shows the end-to-end finetuning results of six FMs on eight datasets. We split the FMs into\ntwo groups based on their input modalities at the time of pretraining: CoCa, CLIP, and FLAVA are\nimage-native FMs, and VideoMAE, VATT, and InternVideo are video-native. The datasets span\nspatiotemporal action localization (STAL), video classification (VC), and temporal action localization\n(TAL). Note that we freeze FM weights in TAL because otherwise its full finetuning consumes\nexcessive memory and computation. We draw the following observations from Table 3.\nFMs underperform task-specialized models on video tasks in general. Table 3\u2019s last row collects\nthe state-of-the-art results on the eight datasets, each obtained by a task-specialized model with\ncomparable architecture or size to ours in the prior work. Specifically, those task-specialized models\nare RAFT [41], PRN [54], TubeViT [39], UniformerV2 [33], AIM [58], MViT [17] and MoViNet [29]\nrespectively. All six FMs significantly underform the task-specialized models on the video tasks at\nthe comparable model scale, indicating the lack of strong video-focused FMs. This observation is in\nsharp contrast to what FMs have achieved on natural language [38, 3] and image understanding [11].\nVideo-native FMs outperform image-native FMs on SSv2, Charades, and ANet which require a\nmodel to reason along the time dimension: SSv2 actions are motion-rich, Charades has multiple\nactions per video, and ANet is about temporal action localization. These results strut the advantages\nof video-native FMs over image-native ones and, hopefully, prompt more efforts dedicating to the\nresearch of video-native FMs.\nCoCa performs the best among image-native FMs on the video tasks. It actually gives rise to the\nhighest accuracy on all datasets except SSv2, Charades, and ANet probably because CoCa, pretrained\nusing image-text pairs, does not capture sufficient motion signals required for understanding SSv2,\nand it cannot handle Charades and ANet\u2019s complex, multiple actions per video.\n4.2\nFrozen FMs\nEnd-to-end finetuning is infeasible for some application scenarios due to FMs\u2019 rapidly growth in\nsize and the consequent demands in computational resources. In the following, we evaluate frozen\nFMs with various adaptation methods. Tables 4, 5, and 6 are the results of adaptation with a single\ncross-attention layer, multiple cross-attention layers, and a low-rank adapter, respectively.\nCLIP generally performs the best among image-native frozen FMs (Tables 4 and 5), but CoCa catches\nup thanks to the low-rank adapter (Table 6). It is worth noting that this ranking of image-native\nfrozen FMs differs from the ranking of image-native FMs in end-to-end finetuning. It seems that\nCLIP\u2019s endpoint features are more amendable to the video tasks than CoCa, but CoCa as a whole\nadapts better to video under both finetuning and the adapter. Hence, it is crucial to consider adaptation\nmethods as an organic part of the evaluation of FMs to supply them various paths to demonstrate\ntheir capabilities.\nVideo-native FMs are better than image-native FMs in understanding motion-rich SSv2 and D48,\nCharades that contain multiple actions per video, and ANet for temporal action localization. This\n7\nTable 4: Evaluating FMs when adapted to video understanding using frozen features. Only weights\nin the task heads are updated using the downstream tasks\u2019 training sets.\nSTAL\nTAL\nVC (A)\nVC (M)\nVC (ML)\nMethod\nAVA\nAVA-K\nANet\nK400\nMiT\nD48\nSSv2\nCharades\nAVG\nCoCa\n23.3\n24.7\n33.0\n73.1\n32.0\n34.1\n41.5\n8.8\n31.2\nCLIP\n21.1\n25.9\n32.7\n75.2\n32.6\n44.1\n41.0\n11.2\n32.8\nFLAVA\n18.8\n21.5\n32.2\n71.3\n29.7\n45.9\n40.6\n12.6\n31.7\nVideoMAE\n16.0\n19.9\n33.0\n65.1\n23.0\n59.5\n53.9\n11.3\n32.6\nInternVideo\n13.4\n15.7\n33.3\n69.3\n26.3\n55.6\n58.2\n13.0\n33.1\nVATT\n20.3\n22.2\n35.3\n75.1\n32.1\n49.7\n57.8\n33.3\n39.1\nTable 5: Evaluating FMs when adapted to video understanding using multi-layer attention pooler\n(MLAP), which takes multiple frozen features from an FM as inputs and map them hierarchically\nfor the final task prediction. Only the multi-layer attention pooling layers are updated using the\ndownstream tasks\u2019 training sets.\nSTAL\nTAL\nVC (A)\nVC (M)\nVC (ML)\nMethod\nAVA\nAVA-K\nANet\nK400\nMiT\nD48\nSSv2\nCharades\nAVG\nCoCa\n24.4\n27.0\n33.3\n74.2\n37.2\n48.4\n45.9\n19.6\n36.3\nCLIP\n27.7\n29.6\n33.9\n77.1\n39.0\n55.8\n50.1\n41.5\n43.3\nFLAVA\n21.3\n23.2\n32.4\n71.5\n34.5\n58.5\n43.1\n38.2\n39.3\nVideoMAE\n19.6\n22.1\n33.4\n71.7\n32.2\n69.6\n57.4\n35.9\n40.9\nInternVideo\n15.9\n17.7\n33.6\n73.7\n34.7\n71.9\n60.3\n40.5\n42.2\nVATT\n22.9\n24.1\n35.0\n75.1\n35.6\n60.1\n58.7\n58.2\n46.3\nTable 6: The low-rank adapter results of FMs for video understanding. We only update the weights\nof the adapter and task head while keeping the original FMs\u2019 weights frozen.\nSTAL\nTAL\nVC (A)\nVC (M)\nVC (ML)\nMethod\nAVA\nAVA-K\nANet\nK400\nMiT\nD48\nSSv2\nCharades\nAVG\nCoCa\n26.6\n28.7\n\u2212\n80.9\n41.4\n67.1\n56.1\n45.8\n49.0\nCLIP\n24.5\n28.0\n\u2212\n80.2\n39.7\n77.2\n56.0\n44.2\n49.3\nFLAVA\n17.9\n23.8\n\u2212\n74.7\n34.1\n68.4\n52.1\n40.8\n44.1\nVideoMAE\n16.6\n23.3\n\u2212\n73.6\n30.6\n76.0\n61.4\n43.0\n45.9\nInternVideo\n19.2\n25.5\n\u2212\n75.5\n31.3\n73.6\n63.9\n46.2\n47.7\nVATT\n22.3\n25.8\n\u2212\n75.0\n36.5\n68.9\n63.5\n53.5\n49.9\nobservation is about the same as the one under end-to-end finetuning. The image-native FMs is\nmainly superior on appearance-rich video datasets, where high-quality spatial perceptual features are\nthe key. We conjecture that the vast image data empowering image-native FMs is more diverse in\nappearance than videos used to pretrain video-native FMs.\nGiven frozen FMs, the low-rank adapter outperforms cross-attention layers, and multiple layers of\ncross-attention is better than a single cross-attention layer. Many works [10, 22] have shown features\nfrom different layers of a vision transformer have different attention maps. Hence, it is potentially\nbeneficial to have an adaptation method to leverage multiple layers of a frozen FM. Table 5 reports\nthe results with four cross-attention layers, whose average score per model (across different columns)\nis higher than that with a single cross-attention layer (Table 4) by 18% to 40%. The low-rank adapter\n(Table 6) further improves upon the cross-attention results partially because it explores all layers of a\nfrozen FM.\nOn average, image-native FMs outperform video-native FMs under end-to-end finetuning and the\nadapter, but it becomes the inverse in the other two adaptation methods. The adapter experiment\npaired with end-to-end finetuning experiment reveal the fact that existing image-based FMs could\nbe more easily adapted to video tasks when we could adjust the feature space of FMs, possibly\ncaused by the large-scale higher quality image(-text) pretraining datasets. On the other hand, frozen\nfeature experiments discussed above present us the inverse picture where video-based FM performs\nbetter. The seemingly paradox encourages more future research on bridging the gap on video-based\npretraining with high-quality data and more effective modeling.\n8\nFigure 3: FMs are equipped with different adaptation methods. Left: For each adaptation method, we\nplot FMs\u2019 averaged scores across all video tasks vs. trainable FLOPs in a log scale. Right: We plot\nthe overall VideoGLUE score (VGS) per FM.\n4.3\nVideoGLUE score: An attempt towards ranking FMs\u2019 video capabilities\nIn this section, we consolidate our studies of the FMs with different adaptation methods on a\nbroad range of video tasks by considering their adaptation efficacies and efficiencies. Adaptation\nmethods with different numbers of trainable weights lead to incompatible comparisons. Motivated\nby this, we propose a scalar measure, called VideoGLUE score (VGS), to capture an FM\u2019s overall\nadaptation performance on our video understanding tasks. While the VideoGLUE score may not be a\nperfect metric, it condenses multiple aspects of comparison into a scalar value, enabling a simplified\ncomparison of FMs.\nTaking the adaptation efficiency into account, we propose to use the trainable FLOPs to normalize an\nadapted FM\u2019s average score s over all tasks. The trainable FLOPs are better than tunable weights\nbecause they allow our VGS to reflect both the model architecture\u2019s freedom and the input data\u2019s\nimpact (e.g., sequence length) on downstream tasks. Formally, denoting by Si an FM\u2019s average score\nover our video tasks under the i-th adaptation method and by Fi the corresponding trainable FLOPs\n(in billion), we calculate the FM\u2019s VGS by\nVGS =\nN\nX\ni=1\nwiSi, where wi =\nAi\nPN\nj=1 Aj\nand Ai =\n1\nlog10 Fi\n,\n(3)\nwhere N = 4 is the number of adaptation methods, and wi \u2208 [0, 1] weigh score Si according to the\ntrainable FLOPs Fi.\nIn Figure 3 we plot the averaged score achieved by each FM under each adaptation method, re-\nspectively, and compare their overall video understanding capabilities using the proposed VGS. The\nchanges in FMs\u2019 ranking by different adaptation methods (see the left panel in Figure 3) reinforce\nthat the adaptation methods matter and should be considered an organic part of the evaluation of FMs.\nOn the right panel of Figure 3, we notice that the video-native FMs overall outperform image-native\nFMs on our video understanding tasks, achieving averaged VGS 41.98 vs. 39.90 respectively. This is\nintuitive as video-native FMs probably have a smaller domain gap to our tasks and are more capable of\ntemporal and motion reasoning, which are important cues for video understanding. Zooming in to the\nindividual FMs, we find that VATT, a video-native FM, is at the first place with VGS 45.1, followed\nby the image-native CLIP with VGS 41.6. This suggests that in-domain pretraining yields overall\nthe best adaptation capability to video tasks, and image-native FMs could also achieve competitive\nresults on many but not all video understanding tasks.\n5\nLimitation, ethical concern, and broader impact\nLimitation. VideoGLUE covers various unimodal video tasks and could be strengthened by adding\nmultimodal tasks like video question answering. We chose three representative FM adaptation\nmethods and used them to provide as uniform experiment protocols for different FMs as possible.\nHowever, some of our observations could be flipped with the evolution of adaptation methods, which\nare an active research area. We proposed a scalar score, VideoGLUE score (VGS), to capture the\nefficacy and efficiency of an FM on video understanding. However, VGS might be dominated by one\n9\nor a few datasets \u2014 when it becomes a serious issue, we should probably improve the score and/or\nretire the other datasets from future versions of VideoGLUE. Indeed, VGS is not a perfect score that\ncovers all aspects of FMs in a comprehensive manner. For example, it does not account for an FM\u2019s\nmemory usage, model size, model architecture, etc. We hope future research will lead to new metrics\nto complement VGS and a more comprehensive evaluation of FMs for visual tasks.\nEthical concern. We evaluate FMs on three video tasks, eight datasets in total. We select the tasks\nand datasets based on their popularity and representativeness. Although carefully designed, our\nbenchmark inevitably inherited some ethical concerns from those datasets. For instance, many of the\ndatasets are curated by crawling videos from the Internet, which do not proportionately represent the\nexperiences of the global population and can potentially lead to biased evaluations of FMs. Moreover,\nthe video datasets involve human daily activities, leading to privacy concerns about the human actors\nin the videos. How to evaluate FMs for video understanding in a fair and privacy-preserving manner\ncould be an important direction for future research.\nBroader impact. Our research reveals the need and tremendous opportunities to research video-first\nFMs by improving pretraining video data and methodologies. Our studies on different adaptation\nmethods on versatile tasks confirms that both tasks and adaptation methods matter when it comes\nto the evaluation of FMs, shedding light on the already vibrant area of FM adaptations. Finally, we\nhope our research could inspire research on foundation models development and video understanding\nin general, along with their applications in the real world.\n6\nConclusion\nIn this report, we study three image-based and three video-based foundation models and their\nadaptation capability on general video understanding tasks. Experiments are conducted on three\nhallmark video tasks, eight diverse datasets with four distinct adaption methods. Our study shows\nexisting image-based FMs performs well on some appearance-rich video datasets, while video-based\nFMs tend to achieve better on motional and temporal reasoning. Four studied adaption methods curve\ndifferent landscape, revealing the critical role of considering adaption methods as an organic part of\nevaluating FMs. Finally, we propose one single metric VGS to represent the video task adaptation\nefficiency of FMs. We hope our research provides useful resources for evaluating and analyzing\nvideo foundation models, and address the current gap in foundation model evaluation within the\nvideo domain.\n10\nSupplementary Materials\nWe detail the datasets (Section A), models (Section B), and training setups (Section C) in the\nsupplementary materials to improve this work\u2019s reproducibility. Besides, Section D includes more\nexperimental studies to strengthen the main text.\nA\nVideo understanding datasets\nA.1\nAppearance-focused action recognition\nVideo classification is a task of classifying videos into pre-defined labels, with the major focus on\nhuman actions.\nKinetics400 [28] (K400) is a large-scale, high-quality video dataset widely used as a standard video\nclassification benchmark. It contains more than 250k video clips with annotations of 400 human daily\nactions. The actions are human focused and cover a broad range of classes including human-human\ninteractions and human-object interactions. Although the video clips span 10 seconds on average,\nmany studies [45, 52] have pointed out the task could be easily solved on the Kinetics datasets by\ninferring from the static objects appeared or background environment \u2014 motion information is less\nimportant than the visual appearance. Hence, we categorize Kinetics400 as an appearance-focused\naction classification dataset.\nMoments-in-Time [37] (MiT) is a large-scale video event classification dataset, with one million\nhuman annotated short video clips (around 3 seconds each). The temporal span corresponds to the\naveraged duration of human working memory and is a temporal envelope holding meaningful actions\nbetween people, objects, and phenomena. Videos in MiT are annotated with 339 most used verbs in\nthe English vocabulary.\nA.2\nMotion-focused action recognition\nVideos contain much more commonsense knowledge than still images do, such as an object\u2019s motion\npatterns and the causal consequences of an action, just to name a few. However, appearance-based\nbenchmarks do not evaluate a model\u2019s understanding of such commonsense knowledge, complex\nscenes, and situations. In observance of this, some video datasets have been proposed and studied in\nrecent years with the focus on motions and common-sensing reasoning that are prosperous in video\ndata.\nSomething-something v2 [20] (SSv2) is a collection of around 200k videos of human performing\npre-defined, basic actions with everyday objects. There are 174 unique labels in total depicting\natomic hand manipulations, like putting something into something, turning something upside down or\ncovering something with something. This dataset benchmarks a model\u2019s fine-grained understanding\ncapability of object motions and scene changes by making the label space atomic-action-focused and\nbackground-invariant.\nDiving48 [34] (D48) is introduced to evaluate a model\u2019s dynamic reasoning capability. The video\nclips in this dataset are obtained by segmenting online videos of major diving competitions. In total,\nthere are around 18k videos annotated with 48 classes. Because of its standardization, the diving\nscenario is purposefully chosen to avoid the scene, object, and person biases.\nA.3\nMulti-label daily action classification\nMost of current action classification datasets involve video clips with a clean snapshot of a single\naction. In contrast, humans perform daily complex activities step-by-step, simultaneously, or in an\ninterleaving manner. Towards more comprehensive human daily activity reasoning, Charades [46] is\nintroduced. Different from web-collected datasets whose contents are more structured, Charades is\ncollected by crowd-sourcing from hundreds of actors recording their videos in their own homes, acting\nout casual everyday activities. Charades brings in more diversity into the video classification task\ndue to its close-to-daily-life setting. Its videos are 30 seconds long on average and have multi-label\nannotations testing models\u2019 understanding of complex daily activities with multiple steps. Charades\nprovides 110k videos with 157 action classes for training and evaluation.\n11\nNorm\nMulti-Head\nSelf-Attention\nNorm\nMLP\nK/V\nK/V\nK/V\nQ\nNorm\nMulti-Head\nSelf-Attention\nNorm\nMLP\nK/V\nK/V\nK/V\nQ\nNorm\nMulti-Head\nSelf-Attention\nNorm\nMLP\nK/V\nK/V\nK/V\nQ\n\u2026\nLayer N\nLayer N-k+1\nLayer N\n(a)\n(b)\nquery token\nfeature token\noutput token\nFigure 4: (a) Single-layer pooler head and (b) multi-layer attention pooling head for video classifica-\ntion and spatiotemporal action localization.\nA.4\nTemporal action localization\nNatural long videos contain scene changes and semantic shifts, while most of the existing video\nbenchmarks formulate problems to focus on trimmed video clips. Such a gap introduces evaluation\nbias as clip-level benchmarks could not reflect a model\u2019s temporal feature discriminativeness, which\nis of key importance to solve long-form video understanding tasks. To comprehend the study on\nfoundation models\u2019 video capabilities, we include the temporal action localization (TAL) task in our\nevaluation. The task of TAL is to predict not only the action labels but also each action instance\u2019s\ntemporal boundary in untrimmed videos. We adopt ActivityNet v1.3 [16] as the dataset for the TAL\ntask, which contains 10, 002 untrimmed videos in training and 4, 985 in validation. The video length\nin this dataset is between 5-10 minutes. In total, there are 200 types of activities annotated.\nA.5\nSpatiotemporal action localization\nSpatiotemporal Action Localization (STAL) is a person-centric task that asks a system to localize\nactors and predict their atomic actions [6, 21] in a transitory duration.\nIn AVA [21], 15 minutes long movie clips are densely annotated at 1Hz. In the key frames, every\nperson is localized using a bounding box and labels corresponding to actions being performed by the\nactor. The label vocabulary consists of 80 different atomic visual actions. There are 430 different\nmovies in total.\nAVA-Kinetics [31] follows the same labeling protocol as AVA, while its data source comes from the\nKinetics700 [28] video pool. The dataset contains over 230k clips annotated with the 80 AVA action\nclasses for each of the humans in key frames.\nB\nModel details\nB.1\nTask head architectures\nIn Figure 4, we plot the task heads used in our video classification and spatiotemporal action\nlocalization experiments, namely, the simple pooler head and multi-layer attention pooling head. For\ntemporal localization, please refer to [57] for the task head\u2019s detailed architecture.\nFigure 5 illustrates the encoder adapter layer\u2019s architecture. In the the adapter layer, only the\ndown-sample layer, up-sample layer, and the scaling factor are tunable.\n12\nNorm\nMulti-Head\nSelf-Attention\nAdapter\nL\u2a2f\nNorm\nMLP\nUpFFN\nS\nDownFFN\nFigure 5: The adapter used in vision transformer. In the adapter layer, only the down-sample layer,\nup-sample layer, and the scaling factor are tunable. Between the down-sample layer and up-sample\nlayer, an activation function is applied, which in our case is ReLU.\nTable 7: Early vs. late fusion on image-native FMs. In this experiment, the frozen feature with a\nsingle-layer pooler head is used.\nK400\nSSv2\nMethod\nEarly\nLate\nEarly\nLate\nCoCa\n72.7\n61.4\n41.5\n33.3\nCLIP\n70.5\n75.2\n38.1\n41.0\nFLAVA\n67.9\n71.3\n40.4\n40.6\nTable 8: Ablation study on the temporal positional embedding for image-to-video adaption. We\nchoose FLAVA [47] with the frozen feature setting in this experiment.\nTemporal Positional\nVC (A)\nVC (M)\nVC (ML)\nEmbedding\nK400\nMiT\nD48\nSSv2\nCharades\n\u2717\n71.3\n29.7\n41.6\n30.3\n10.7\n\u2713\n71.3\n29.7\n45.9\n40.6\n12.6\nB.2\nImage-to-video adaptation\nAdapting image backbones to video tasks requires us to fuse the image embeddings at some point in\nthe network and also introduce additional temporal information.\nWe consider two choices, early-fusion and late-fusion, and ablate them in the frozen feature setting\nin Table 7. In both early-fusion and late-fusion, we first apply the projection layer on each frame\nindependently to embed pixel patches into embedding tokens. We then average-pool the embedding\ntokens from nearby frames to reduce the sequence length to n\u00d7h\u00d7w. In the early-fusion setting, we\npass all tokens together to the image backbone to extract video features. In late-fusion, we pass each\nset of h \u00d7 w tokens independently to the image backbone. Empirically, we find that the FLAVA [47]\nand CLIP [40] models do better with late-fusion while CoCa [59] does better with early-fusion.\nFurthermore, we ablate the importance of temporal information using the frozen-features from\nFLAVA [47]. In Table 8, we find that adding temporal positional embedding to the input is essential\nfor D48 [34], SSv2 [20], and Charades [46] while not necessary for K400 [28] and MiT [37]. This\nsupports our grouping that K400 and MiT are appearance-focused datasets.\nBased on these findings, we use late-fusion for FLAVA [47] and CLIP [40] and early-fusion for\nCoCa [59]. We add learnable temporal positional embeddings for all the image-native FMs.\n13\nTable 9: Experimental configurations for video classification tasks. We let learning rate and weight\ndecay to be tunable per model to allow some flexibility for task adaptations.\nConfig\nKinetics400\nSth-sth v2\nMiT\nDiving48\nCharades\nbatch size\n256\n256\n256\n256\n256\ntraining epochs\n150\n50\n50\n100\n50\nViT sequence length\n8 \u00d7 14 \u00d7 14\n8 \u00d7 14 \u00d7 14\n8 \u00d7 14 \u00d7 14\n8 \u00d7 14 \u00d7 14\n8 \u00d7 14 \u00d7 14\noptimization\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\noptimizer momentum\n0.9\n0.9\n0.9\n0.9\n0.9\nlearning rate schedule\ncosine decay\ncosine decay\ncosine decay\ncosine decay\ncosine decay\nwarmup ratio\n5%\n5%\n5%\n5%\n5%\ndata augmentations\nrandom horizontal flip\ntrue\nfalse\ntrue\ntrue\nfalse\naspect ratio\n(0.5, 2.0)\n(0.5, 2.0)\n(0.5, 2.0)\n(0.5, 2.0)\n(0.5, 2.0)\narea ratio\n(0.3, 1.0)\n(0.3, 1.0)\n(0.3, 1.0)\n(0.3, 1.0)\n(0.3, 1.0)\nRandAug\n(9, 0.5)\n(9, 0.5)\n-\n-\n-\nMixUp\n0.8\n0.8\n-\n-\n-\nCutMix\n1.0\n1.0\n-\n-\n-\nevaluation\nmulti-clips\n4\n1\n4\n4\n4\nmulti-views\n3\n3\n3\n3\n3\nsegment-based sample\nfalse\ntrue\nfalse\nfalse\nfalse\nTable 10: Experimental configurations for spatiotemporal action localization.\nConfig\nAVA v2.2\nAVA-Kinetics\nbatch size\n256\n256\ntraining epochs\n50\n50\nViT sequence length\n8 \u00d7 16 \u00d7 16\n8 \u00d7 16 \u00d7 16\noptimization\noptimizer\nAdamW\nAdamW\noptimizer momentum\n0.9\n0.9\nlayer decay\n0.75\n0.75\nlearning rate schedule\ncosine decay\ncosine decay\nwarmup ratio\n5%\n5%\ndata augmentations\nrandom horizontal flip\ntrue\ntrue\nrandom scale\n(0.5, 2.0)\n(0.5, 2.0)\nrandom color augmentation\ntrue\ntrue\nC\nTask-specific hyperparameters\nIn the following, we provide experiment settings and hyperparamters we used in this study. In Table 9,\nwe list the hyperparameters we applied in the video classification task. In Table 10, we present\nthe hyperparameters we used on spatiotemporal action localization. In Table 11, we present the\nhyperparameters we used on temporal action localization task.\nWe performed a greedy search on the learning rate and weight decay in all our experiments while\nkeeping most other hyperparameters (e.g., data augmentation magnitude, dropout rate, drop path rate,\netc.) consistent across different models and datasets. Specifically, we start with learning rate 1e-4 and\nweight decay 1e-5 and uniformly sample learning rates and weight decay factors with a rate of 5 and\n10, respectively, centered around the starting points. After the first round, we pick the best-identified\nlearning rate and weight decay factor as the new starting point and conduct another round of sampling\nwith a rate of 2. We repeat another two to three rounds of hyperparameter search (with a rate of 2)\nuntil the model\u2019s performance converges. This process is a trade-off between computation costs and\nthoroughly examining an FM\u2019s performance under each experiment setup. The search ranges for the\nlearning rate and weight decay are [4e-5, 2.5e-3] and [1e-6, 1e-4], respectively. We found that the\nlearning rate is the most crucial factor when adapting an FM to downstream video understanding\ntasks.\n14\nTable 11: Experimental configurations for temporal action localization.\nConfig\nActivityNet v1.3\nbatch size\n32\ntraining epochs\n10\nfeature extraction\nfps\n15\nper-clip length\n16\nclip stride\n16\noptimization\noptimizer\nAdamW\noptimizer momentum\n0.9\nlearning rate schedule\ncosine decay\nTable 12: Evaluating large-scale FMs when using (a) frozen feature with a one-layer pooler head,\nand (b) low-rank adapter with frozen features. We report the Top-1 accuracy on K400, MiT, D48,\nSSv2 and MAP on Charades.\nVC (A)\nVC (M)\nVC (ML)\nModel\nMethod\nK400\nMiT\nD48\nSSv2\nCharades\nInternVideo-L\nfrozen\n78.6\n33.7\n69.6\n67.4\n20.9\nInternVideo-L\nadapter\n81.5\n40.3\n85.8\n70.9\n54.2\nVideoMAE-v2-B/DL\nfrozen\n86.7\n38.9\n61.4\n57.7\n33.2\nVideoMAE-v2-B/DL\nadapter\n86.0\n41.8\n82.3\n66.6\n53.8\nVideoMAE-v2-g\nfrozen\n59.7\n20.7\n42.5\n44.2\n12.7\nVideoMAE-v2-g\nadapter\n80.8\n35.9\n85.3\n68.2\n55.5\nVideoMAE-v2-g/FT\nfrozen\n82.1\n35.0\n60.5\n56.1\n22.4\nVideoMAE-v2-g/FT\nadapter\n85.2\n42.5\n84.6\n70.6\n58.6\nTable 13: Benchmark FMs adaptation on video understanding tasks under sample-efficient transfer\nlearning. This table shows Top-1 classification accuracy and the relative accuracy (shown in the\nbracket). Results are achieved by using frozen features with pooler head.\nK400\nSSv2\nMethod\n1%\n10%\n100%\n1%\n10%\n100%\nCoCa\n27.1(37.8%)\n48.9(67.0%)\n73.1\n5.6(13.4%)\n20.9(50.4%)\n41.5\nCLIP\n36.9(46.2%)\n66.8(83.6%)\n79.0\n8.7(19.3%)\n25.1(55.5%)\n45.3\nFLAVA\n14.4(20.2%)\n35.8(50.3%)\n71.3\n7.2(17.7%)\n14.3(35.3%)\n40.6\nVideoMAE\n15.5(23.9%)\n32.0(49.2%)\n65.0\n13.7(25.4%)\n30.3(56.2%)\n53.9\nInternVideo\n20.4(29.5%)\n50.2(72.4%)\n69.3\n19.5(33.6%)\n41.1(70.7%)\n58.2\nVATT\n34.1(45.4%)\n63.7(84.8%)\n75.1\n12.9(22.4%)\n37.6(65.0%)\n57.8\nD\nMore studies\nD.1\nLarge model adaptations\nFor the completeness of this report and reader\u2019s reference, in Table 12 we report experimental results\nunder our settings with large FMs under two adaptation scenarios, namely, the frozen backbone\nwith pooler head and the low-rank adapter. VideoMAE-v2-B/DL [51] denotes the ViT-B model\ndistilled from ViT-g on the Kinetics710 datasets4. VideoMAE-v2-g [51] is the model that pretrained\non UnlabeledHybrid dataset, while VideoMAE-v2-g/FT [51] conducts further finetuning using\nsupervised training on Kinetics710.\nD.2\nSample-efficient transfer learning\nA strong FM should be able to adapt to downstream tasks with a few training samples. In this section,\nwe test the adaption ability of FMs in a sample-efficient transfer learning setting. Particularly, we\nfreeze backbones and train a pooler head to adapt the FMs on K400 and SSv2. For either dataset,\n4https://github.com/OpenGVLab/VideoMAEv2/blob/master/docs/MODEL_ZOO.md\n15\nwe sample 1% and 10% data from the training set uniformly for training and evaluate on the full\nevaluation dataset.\nWe show our experimental results in Table 13. To better understand the data efficiency, we also\nshow the relative Top-1 accuracy for each model (shown in the bracket), which is defined as the ratio\nbetween accuracy with fewer training examples and the accuracy achieved using all the training data.\nA higher relative Top-1 accuracy means the performance of the model is closer to its \u201cfull\u201d capacity\nunder the sample-efficient setting. We notice that the best performed model on each dataset in fully\nfine-tuned model also performs best in the few-shot setting. Especially, CLIP [40] achieves 46.2%\nand 83.6% relative Top-1 accuracy on K400 using only 1% and 10% of the training data, respectively.\nOn SSv2, InternVideo [55] achieves 33.6% and 70.6% relative Top-1 accuracy with only 1% and\n10% of the training data.\n16\nReferences\n[1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\nVatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in\nNeural Information Processing Systems, 34:24206\u201324221, 2021.\n[2] Humam Alwassel, Silvio Giancola, and Bernard Ghanem. Tsp: Temporally-sensitive pretraining of video\nencoders for localization tasks. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 3173\u20133183, 2021.\n[3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\narXiv:2305.10403, 2023.\n[4] Authors Authors. Perception test: A diagnostic benchmark for multimodal models. https: // storage.\ngoogleapis. com/ dm-perception-test/ perception_ test_ report. pdf , 2022.\n[5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv\npreprint arXiv:2106.08254, 2021.\n[6] Roger G Barker and Herbert F Wright. Midwest and its children: The psychological ecology of an american\ntown. Marriage and family living, 1955.\n[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258, 2021.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 2020.\n[9] Shyamal Buch, Cristobal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles.\nRevisiting the \u201cVideo\u201d in Video-Language Understanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 9650\u20139660, 2021.\n[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-\nimage model. arXiv preprint arXiv:2209.06794, 2022.\n[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[14] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and\nHsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation.\nAdvances in neural information processing systems, 32, 2019.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[16] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: A large-\nscale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 961\u2013970, 2015.\n[17] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\nFeichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6824\u20136835, 2021.\n[18] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal\nlearners. arXiv preprint arXiv:2205.09113, 2022.\n[19] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video\nrecognition. In ICCV, pages 6202\u20136211, 2019.\n[20] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something\nsomething\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5842\u20135850, 2017.\n17\n[21] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra\nVijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of\nspatio-temporally localized atomic visual actions. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6047\u20136056, 2018.\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16000\u201316009, 2022.\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 2021.\n[24] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with\nlanguage models. arXiv preprint arXiv:2302.14045, 2023.\n[25] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in\nneural information processing systems, 28, 2015.\n[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\n[27] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for\nefficient video understanding. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XXXV, pages 105\u2013124. Springer, 2022.\n[28] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950, 2017.\n[29] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, and Boqing\nGong. Movinets: Mobile video networks for efficient video recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 16020\u201316030, 2021.\n[30] Jie Lei, Tamara L. Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning.\nIn Proceedings if the 61st Annual Meeting of the Association for Computational Linguistics, 2023.\n[31] Ang Li, Meghana Thotakuri, David A Ross, Jo\u00e3o Carreira, Alexander Vostrikov, and Andrew Zisserman.\nThe ava-kinetics localized human actions video dataset. arXiv preprint arXiv:2005.00214, 2020.\n[32] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin,\nYong Jae Lee, Houdong Hu, Zicheng Liu, et al. Elevater: A benchmark and toolkit for evaluating\nlanguage-augmented visual models. arXiv preprint arXiv:2204.08790, 2022.\n[33] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao. Uniformerv2:\nSpatiotemporal learning by arming image vits with video uniformer. arXiv preprint arXiv:2211.09552,\n2022.\n[34] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation\nbias. In Proceedings of the European Conference on Computer Vision (ECCV), pages 513\u2013528, 2018.\n[35] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao,\nand Hongsheng Li. Frozen clip models are efficient video learners. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pages 388\u2013404.\nSpringer, 2022.\n[36] Xiaolong Liu, Song Bai, and Xiang Bai. An empirical study of end-to-end temporal action detection. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20010\u2013\n20019, 2022.\n[37] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa\nBrown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for\nevent understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20138, 2019.\n[38] OpenAI. Gpt-4 technical report. https://cdn.openai.com/papers/gpt-4.pdf, 2022.\n[39] AJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Rethinking video vits: Sparse video tubes for joint\nimage and video learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2214\u20132224, 2023.\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR,\n2021.\n18\n[41] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, Christoph Feichtenhofer, and Jitendra\nMalik. On the benefits of 3d pose and tracking for human action recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 640\u2013649, 2023.\n[42] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning,\npages 8821\u20138831. PMLR, 2021.\n[43] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. Advances in neural information processing systems, 28, 2015.\n[44] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor,\nSharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models and data with t5x\nand seqio. arXiv preprint arXiv:2203.17189, 2022.\n[45] Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj Goswami, Matt Feiszli, and Lorenzo Torresani.\nOnly time can tell: Discovering temporal data for temporal modeling. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pages 535\u2013544, 2021.\n[46] Gunnar A Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood\nin homes: Crowdsourcing data collection for activity understanding. In Computer Vision\u2013ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part I 14,\npages 510\u2013526. Springer, 2016.\n[47] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15638\u201315650, 2022.\n[48] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022.\n[49] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding\nsystems. Advances in neural information processing systems, 32, 2019.\n[50] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint\narXiv:1804.07461, 2018.\n[51] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.\nVideomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 14549\u201314560, 2023.\n[52] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal\nsegment networks for action recognition in videos. IEEE transactions on pattern analysis and machine\nintelligence, 41(11):2740\u20132755, 2018.\n[53] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\n[54] Xiang Wang, Zhiwu Qing, Ziyuan Huang, Yutong Feng, Shiwei Zhang, Jianwen Jiang, Mingqian Tang,\nChangxin Gao, and Nong Sang. Proposal relation network for temporal action detection. arXiv preprint\narXiv:2106.11812, 2021.\n[55] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu,\nYi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative\nlearning. arXiv preprint arXiv:2212.03191, 2022.\n[56] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv\npreprint arXiv:2206.07682, 2022.\n[57] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and Bernard Ghanem.\nG-tad: Sub-graph\nlocalization for temporal action detection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10156\u201310165, 2020.\n[58] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting image\nmodels for efficient video action recognition. arXiv preprint arXiv:2302.03024, 2023.\n[59] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n19\n"
  }
]