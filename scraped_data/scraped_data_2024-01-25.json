[
  {
    "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild",
    "link": "https://arxiv.org/pdf/2401.13627.pdf",
    "upvote": "68",
    "text": "Scaling Up to Excellence:\nPracticing Model Scaling for Photo-Realistic Image Restoration In the Wild\nFanghua Yu1,\u2217, Jinjin Gu2,3,\u2217, Zheyuan Li1, Jinfan Hu1, Xiangtao Kong4,\nXintao Wang5, Jingwen He2,6, Yu Qiao2, Chao Dong1,2,\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n2Shanghai AI Laboratory\n3University of Sydney\n4The Hong Kong Polytechnic University\n5ARC Lab, Tencent PCG\n5The Chinese University of Hong Kong\nProject Page:\nhttps://supir.xpixel.group\n(a) Real-World Image Restoration Results\n(b) Controllable Image Restoration with Textual Prompts\nLow Quality Input\nSUPIR output\nSUPIR output\nSUPIR output\nLow Quality Input\nLow Quality Input\nLow Quality Input\nNo Prompt\nText: A bicycle is \nat the end of \u2026\nLow Quality Input\nText: woman with \na denim hat.\nText: woman with \na suede hat.\nLow Quality Input Text: \u2026 shows an \nold man \u2026\nText: \u2026 shows a \nyoung man \u2026\nFigure 1. Our SUPIR model demonstrates remarkable restoration effects on real-world low-quality images, as illustrated in (a). Addition-\nally, SUPIR features targeted restoration capability driven by textual prompts. For instance, it can specify the restoration of blurry objects\nin the distance (case 1), define the material texture of objects (case 2), and adjust restoration based on high-level semantics (case 3).\nAbstract\nWe introduce SUPIR (Scaling-UP Image Restoration),\na groundbreaking image restoration method that harnesses\ngenerative prior and the power of model scaling up. Lever-\naging multi-modal techniques and advanced generative\nprior, SUPIR marks a significant advance in intelligent and\n\u2217 Contribute Equally.\n\u2020 Corresponding Author.\nrealistic image restoration.\nAs a pivotal catalyst within\nSUPIR, model scaling dramatically enhances its capabil-\nities and demonstrates new potential for image restora-\ntion.\nWe collect a dataset comprising 20 million high-\nresolution, high-quality images for model training, each en-\nriched with descriptive text annotations. SUPIR provides\nthe capability to restore images guided by textual prompts,\nbroadening its application scope and potential. Moreover,\narXiv:2401.13627v1  [cs.CV]  24 Jan 2024\nwe introduce negative-quality prompts to further improve\nperceptual quality. We also develop a restoration-guided\nsampling method to suppress the fidelity issue encountered\nin generative-based restoration. Experiments demonstrate\nSUPIR\u2019s exceptional restoration effects and its novel capac-\nity to manipulate restoration through textual prompts.\n1. Introduction\nWith the development of image restoration (IR), expecta-\ntions for the perceptual effects and intelligence of IR results\nhave significantly increased. IR methods based on gener-\native priors [42, 49, 67, 82] leverage powerful pre-trained\ngenerative models to introduce high-quality generation and\nprior knowledge into IR, bringing significant progress in\nthese aspects. Continuously enhancing the capabilities of\nthe generative prior is key to achieving more intelligent IR\nresults, with model scaling being a crucial and effective ap-\nproach. There are many tasks that have obtained astonish-\ning improvements from scaling, such as SAM [44] and large\nlanguage models [7, 73, 74]. This further motivates our ef-\nfort to build large-scale, intelligent IR models capable of\nproducing ultra-high-quality images. However, due to en-\ngineering constraints such as computing resources, model\narchitecture, training data, and the cooperation of genera-\ntive models and IR, scaling up IR models is challenging.\nIn this work, we introduce SUPIR (Scaling-UP IR), the\nlargest-ever IR method, aimed at exploring greater potential\nin visual effects and intelligence. Specifically, SUPIR em-\nploys StableDiffusion-XL (SDXL) [63] as a powerful gen-\nerative prior, which contains 2.6 billion parameters. To ef-\nfectively apply this model, we design and train a adaptor\nwith more than 600 million parameters. Moreover, we have\ncollected over 20 million high-quality, high-resolution im-\nages to fully realize the potential offered by model scaling.\nEach image is accompanied by detailed descriptive text, en-\nabling the control of restoration through textual prompts.\nWe also utilize a 13-billion-parameter multi-modal lan-\nguage model to provide image content prompts, greatly im-\nproving the accuracy and intelligence of our method. The\nproposed SUPIR model demonstrates exceptional perfor-\nmance in a variety of IR tasks, achieving the best visual\nquality, especially in complex and challenging real-world\nscenarios. Additionally, the model offers flexible control\nover the restoration process through textual prompts, vastly\nbroadening the possibility of IR. Fig. 1 illustrates the effects\nby our model, showcasing its superior performance.\nOur work goes far beyond simply scaling. While pur-\nsuing an increase in model scale, we face a series of com-\nplex challenges. First, when applying SDXL for IR, exist-\ning Adaptor designs either too simple to meet the complex\nrequirements of IR [59] or are too large to train together\nwith SDXL [95]. To solve this problem, we trim the Con-\ntrolNet and designed a new connector called ZeroSFT to\nwork with the pre-trained SDXL, aiming to efficiently im-\nplement the IR task while reducing computing costs. In or-\nder to enhance the model\u2019s ability to accurately interpret\nthe content of low-quality images, we fine-tune the image\nencoder to improve its robustness to variations in image\ndegradation. These measures make scaling the model fea-\nsible and effective, and greatly improve its stability. Sec-\nond, we amass a collection of 20 million high-quality, high-\nresolution images with descriptive text annotations, provid-\ning a solid foundation for the model\u2019s training. We adopt\na counter-intuitive strategy by incorporating poor quality,\nnegative samples into training. In this way, we can use neg-\native quality prompts to further improve visual effects. Our\nresults show that this strategy significantly improves im-\nage quality compared to using only high-quality positive\nsamples.\nFinally, powerful generative prior is a double-\nedged sword. Uncontrolled generation may reduce restora-\ntion fidelity, making IR no longer faithful to the input im-\nage. To mitigate this low-fidelity issue, we propose a novel\nrestoration-guided sampling method. All these strategies,\ncoupled with efficient engineering implementation, are key\nto enabling the scaling up of SUPIR, pushing the boundaries\nof advanced IR. This comprehensive approach, encompass-\ning everything from model architecture to data collection,\npositions SUPIR at the forefront of image restoration tech-\nnology, setting a new benchmark for future advancements.\n2. Related Work\nImage Restoration.\nThe goal of IR is to convert de-\ngraded images into high-quality degradation-free versions\n[22, 26, 89, 91, 98, 99].\nIn the early stage, researchers\nindependently explored different types of image degrada-\ntion, such as super-resolution (SR) [13, 19, 20], denoising\n[11, 90, 92], and deblurring [14, 60, 72]. However, these\nmethods are often based on specific degradation assump-\ntions [25, 50, 58] and therefore lack generalization ability\nto other degradations [29, 53, 97].\nOver time, the need\nfor blind restoration methods that are not based on spe-\ncific degradation assumptions has grown [5, 10, 34, 35, 46\u2013\n48, 78, 94]. In this trend, some methods [81, 93] approx-\nimate synthesize real-world degradation by more complex\ndegradation models, and are well-known for handling mul-\ntiple degradation with a single model. Recent research, such\nas DiffBIR [49], unifies different restoration problems into\na single model. In this paper, we adopt a similar setting to\nDiffBIR and use a single model to achieve effective pro-\ncessing of various severe degradations.\nGenerative Prior.\nGenerative priors are adept at captur-\ning the inherent structures of the image, enabling the gener-\nation of images that follow natural image distribution. The\nemergence of GANs [23, 39, 40, 64] has underscored the\nsignificance of generative priors in IR. Various approaches\nemploy these priors, including GAN inversion [2, 4, 27, 57,\n2\nDegradation-\nRobust Encoder\n<latexit sha1_base64=\"SvnUMC6ZPmzYgxhmua\nZTLPLV4U=\">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe7EqGXQxsIiAfMByRH2NnvJmr3dY3dPiEf+g\n42FIrb+Hzv/jZvkCk18MPB4b4aZeUHMmTau+3kVlbX1jfym4Wt7Z3dveL+QVPLRBHaIJL1Q6wpwJ2\njDMcNqOFcVRwGkrGN1M/dYjVZpJcW/GMfUjPBAsZAQbKzWfeuldfdIrltyOwNaJl5GSpCh1it+dfuSJB\nEVhnCsdcdzY+OnWBlGOJ0UuomMSYjPKAdSwWOqPbT2bUTdGKVPgqlsiUMmqm/J1IcaT2OAtsZYTPUi9\n5U/M/rJCa8lMm4sRQeaLwoQjI9H0dRnihLDx5Zgopi9FZEhVpgYG1DBhuAtvrxMmdl76JcqZ+Xqt\ndZHk4gmM4BQ8uoQq3UIMGEHiAZ3iFN0c6L8678zFvzTnZzCH8gfP5A6FNjy4=</latexit>zLQ\nLDM \nImage Decoder\nLow-Quality \nImage\nMulti-Modal Large\nLanguage Model\nThe image features a close-up of a white \nand gray cat sitting on a wooden \nsurface\u2026... The cat's fur is \ufb02uffy, and its \neyes are wide open, \u2026 with its front \npaws resting on the wooden surface.\nLDM \nImage Decoder\nText Prompt\nPre-Trained SDXL\nTrimmed ControlNet\nPre-Trained SDXL\nTrimmed ControlNet\nEDM Sampler with Restoration Guidance\n<latexit sha1_base64=\"16jKPTbrLlpkqszvoeqRLcMkhUk=\">AB73icbVDLSgNBEOz1GeMr6tHLYBA8hV3\nxdQx68RghL0iWMDuZTYbMzq4zvUJY8hNePCji1d/x5t84SfagiQUNRVU3V1BIoVB1/12VlbX1jc2C1vF7Z3dvf3SwWHTxKlmvMFiGet2QA2XQvEGCpS8nWhOo0DyVjC6m/qtJ6NiFUdxwn3IzpQIhSMopXaXRQRN6TeK5XdijsD\nWSZeTsqQo9YrfX7MUsjrpBJakzHcxP0M6pRMknxW5qeELZiA54x1JF7Ro/m907IadW6ZMw1rYUkpn6eyKjkTHjKLCdEcWhWfSm4n9eJ8Xwxs+ESlLkis0XhakGJPp86QvNGcox5ZQpoW9lbAh1ZShjahoQ/AWX14mzfOKd1W5f\nLgoV2/zOApwDCdwBh5cQxXuoQYNYCDhGV7hzXl0Xpx352PeuLkM0fwB87nD7hEj8Y=</latexit>\u21e5T\nRestoration \nResult\nTrainable\nFixed\n<latexit sha1_base64=\"kFcYXBL1dLyuCAgZ7ofqr2njUI=\">AB6nicbVDLSgNBE\nOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlaISz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43Adq\nKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfbcXrniVt0ZyF/i5aQCOeq98me3H7M0QmYoFp3PDcxfkaV4UzgpNRNSaUjegAO5ZKGqH2s9mpE3JklT4JY2VLGjJTf05kNJ\n6HAW2M6JmqBe9qfif10lNeOlnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb78lzRPqt59ez2tFK7yuMowgEcwjF4cAE1uIE6NIDBAJ7gBV4d4Tw7b87vLX\ng5DP78AvOxzcQeI2r</latexit>z0\n<latexit sha1_base64=\"16Zw0V4sTmGZ61\nbrQCl0ZLvDVA8=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8\nmQ2dlpleISz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJFIYdN0vp7C0vLK6VlwvbWxube+U\nd/eaJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LR9dRvPXBtRKzucZxwP6IDJULBKFrp7r\nGHvXLFrbozkL/Ey0kFctR75c9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSN\nn81OnZAjq/RJGtbCslM/TmR0ciYcRTYzoji0Cx6U/E/r5NieOlnQiUpcsXmi8JUEozJ9G\n/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy/Jc2TqndePbs9rdSu8jiKcACHcAweXEANbqAO\nDWAwgCd4gVdHOs/Om/M+by04+cw+/ILz8Q13iI3v</latexit>zt\nFigure 2. This figure briefly shows the workflow of the proposed SUPIR model.\n62], GAN encoders [9, 103], or using GAN as the core mod-\nule for IR [80, 87]. Beyond GANs, other generative mod-\nels can also serve as priors [10, 36, 55, 75, 100\u2013102]. Our\nwork primarily focuses on generative priors derived from\ndiffusion models [31, 61, 65, 67, 70, 71], which excel in\ncontrollable generation [15, 18, 32, 59, 95] and model scal-\ning [63, 66, 68]. Diffusion models have also been effec-\ntively used as generative priors in IR [42, 49, 67, 77, 82].\nHowever, these diffusion-based IR methods\u2019 performance\nis constrained by the scale of the used generative models,\nposing challenges in further enhancing their effectiveness.\nModel Scaling\nis an important means to further im-\nprove the capabilities of deep-learning models. The most\ntypical examples include the scaling of language models\n[7, 73, 74], text-to-image generation models [12, 37, 63, 67,\n68, 85], and image segmentation models [44]. The scale and\ncomplexity of these models have increased dramatically,\nwith billions or even hundreds of billions of parameters, but\nthese parameters also lead to extraordinary performance im-\nprovements, demonstrating the potential of model scaling\n[38]. However, scaling up is a systematic problem, involv-\ning model design, data collection, computing resources, and\nother limitations. Many other tasks have not yet been able\nto enjoy the substantial performance improvements brought\nby scaling up. IR is one of them.\n3. Method\nAn overview of the proposed SUPIR method is shown\nin Fig. 2.\nWe introduce our method from three as-\npects: Sec. 3.1 introduces our network designs and training\nmethod; Sec. 3.2 introduces the collection of training data\nand the introduction of text modality; and Sec. 3.3 intro-\nduces the diffusion sampling method for image restoration.\n3.1. Model Scaling Up\nGenerative Prior.\nThere are not many choices for the\nlarge-scale generative models. The only ones to consider\nare Imagen [68], IF [16], and SDXL [63]. Our selection\nsettled on SDXL for the following reasons. Imagen and\nIF prioritize text-to-image generation and rely on a hierar-\nchical approach. They first generate small-resolution im-\nages and then hierarchically upsample them. SDXL directly\ngenerates a high-resolution image without hierarchical de-\nsign, which is more aligned with our objectives, as it uti-\nlizes its parameters effectively for image quality improve-\nment rather than text interpretation. Additionally, SDXL\nemploys a Base-Refine strategy. In the Base model, diverse\nbut lower-quality images are generated. Subsequently, the\nRefine model enhances the perceptual quality of these im-\nages. Compared to the Base model, the Refine model uses\ntraining images with significantly higher quality but less\ndiverse. Considering our strategy to train with an exten-\nsive dataset of high-quality images, the two-phase design\nof SDXL becomes superfluous for our needs. We opt for\nthe Base model, which has a greater number of parameters,\nmaking it an ideal backbone for our generative prior.\nDegradation-Robust Encoder.\nIn SDXL, the diffusion\ngeneration process is performed in the latent space. The im-\nage is first mapped to the latent space through a pre-trained\nencoder. To effectively utilize the pre-trained SDXL, our\nLQ image xLQ should also be mapped to the same latent\nspace. However, since the original encoder has not been\ntrained on LQ images, using it for encoding will affect the\nmodel\u2019s judgment of LQ image content, and then misun-\nderstand artifacts as image content [49]. To this end, we\nfine-tune the encoder to make it robust to the degradation\nby minimizing: LE = \u2225D(Edr(xLQ)) \u2212 D(Edr(xGT ))\u22252\n2,\nwhere Edr is the degradation-robust encoder to be fine-\ntuned, D is the fixed decoder, xGT is the ground truth.\nLarge-Scale Adaptor Design.\nConsidering the SDXL\nmodel as our chosen prior, we need an adaptor that can\nsteer it to restore images according to the provided LQ in-\nputs. The Adaptor is required to identify the content in the\nLQ image and to finely control the generation at the pixel\nlevel. LoRA [32], T2I adaptor [59], and ControlNet [95]\nare existing diffusion model adaptation methods, but none\nof them meet our requirements: LoRA limits generation but\nstruggles with LQ image control; T2I lacks capacity for ef-\nfective LQ image content identification; and ControlNet\u2019s\ndirect copy is challenging for the SDXL model scale. To\naddress this issue, we design a new adaptor with two key\nfeatures, as shown in Fig. 3(a). First, we keep the high-level\ndesign of ControlNet but employ network trimming [33] to\ndirectly trim some blocks within the trainable copy, achiev-\n3\nTrainable\nEncoder Block\nEncoder Block\nEncoder Block\nEncoder Block\nTrimmed Copy\nTrimmed Copy\nTrimmed Copy\nTrimmed Copy\nTrimmed Copy\nMid. Block\nDecoder Block\nDecoder Block\nDecoder Block\nDecoder Block\n<latexit sha1_base64=\"\na0Kl15aE0wHU5qa5bgqOSI4tl8=\">AB7HicbVBNS\n8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKpi20oWy23bpZ\nhN2J0IN/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZemEh\n0HW/ncLK6tr6RnGztLW9s7tX3j9omDjVjPslrFuhdR\nwKRT3UaDkrURzGoWSN8PR7dRvPnJtRKwecJzwIKIDJf\nqCUbS/9TNcNItV9yqOwNZJl5OKpCj3i1/dXoxSyOukE\nlqTNtzEwyqlEwySelTmp4QtmIDnjbUkUjboJsduyEn\nFilR/qxtqWQzNTfExmNjBlHoe2MKA7NojcV/PaKfav\ng0yoJEWu2HxRP5UEYzL9nPSE5gzl2BLKtLC3EjakmjK\n0+ZRsCN7iy8ukcVb1LqsX9+eV2k0eRxGO4BhOwYMrqME\nd1MEHBgKe4RXeHOW8O/Ox7y14OQzh/AHzucPHSO+w\n=</latexit>zt\n<latexit sha1_ba\nse64=\"jACfm2HS7kw/OUiDFS2l0roq\nUw=\">AB+nicbVDLSgMxFM34rPU1\naWbwSK4kDIjvpZFNy4r2Ae0Q8mkaRu\naZEJyRyljP8WNC0Xc+iXu/Bsz7Sy09U\nDgcM693JMTKc4M+P63s7S8srq2Xtgo\nbm5t7+y6pb2GiRNaJ3EPNatCBvKmaR\n1YMBpS2mKRcRpMxrdZH7zgWrDYnkPY\n0VDgQeS9RnBYKWuW4KTjsAwBEiVjoWC\nSdct+xV/Cm+RBDkpoxy1rvV6cUkEV\nQC4diYduArCFOsgRFOJ8VOYqjCZIQH\ntG2pxIKaMJ1Gn3hHVul5/VjbJ8Gbqr8\n3UiyMGYvITmYxzbyXif957QT6V2HKp\nEqASjI71E+4B7GX9eD1mKYE+NgSTDSz\nWT0yxBoTsG0VbQnB/JcXSeO0ElxUzu\n/OytXrvI4COkCH6BgF6BJV0S2qoToi6\nBE9o1f05jw5L8678zEbXLynX30B87\nnDwptlIc=</latexit>t, prompt\n<latexit sha1_base64=\"jACfm2HS7kw/OUiDFS2l0roqUw=\">AB+nicbVDLSgMxFM34rPU1aWbwSK4kDIjvpZFNy4r2Ae0Q8mkaRuaZEJyRyljP8WNC0\nXc+iXu/Bsz7Sy09UDgcM693JMTKc4M+P63s7S8srq2Xtgobm5t7+y6pb2GiRNaJ3EPNatCBvKmaR1YMBpS2mKRcRpMxrdZH7zgWrDYnkPY0VDgQeS9RnBYKWuW4KTjsAwBEiVjoWCSdct+xV/Cm+RBDkpoxy1rvV6cUkEVQC4diYduArCFOsgRFOJ8VOYqjCZIQHtG2pxIKaMJ1Gn3hHVul5/VjbJ8Gbqr83UiyMGYvITmYxzb\nyXif957QT6V2HKpEqASjI71E+4B7GX9eD1mKYE+NgSTDSzWT0yxBoTsG0VbQnB/JcXSeO0ElxUzu/OytXrvI4COkCH6BgF6BJV0S2qoToi6BE9o1f05jw5L8678zEbXLynX30B87nDwptlIc=</latexit>t, prompt\n<latexit sha1_base64=\"SvnUMC6ZPmzYgxhmuaZT\nLPLV4U=\">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe7EqGXQxsIiAfMByRH2NnvJmr3dY3dPiEf+g42FIrb+\nHzv/jZvkCk18MPB4b4aZeUHMmTau+3kVlbX1jfym4Wt7Z3dveL+QVPLRBHaIJL1Q6wpwJ2jDMcNqOFcVRw\nGkrGN1M/dYjVZpJcW/GMfUjPBAsZAQbKzWfeuldfdIrltyOwNaJl5GSpCh1it+dfuSJBEVhnCsdcdzY+OnWB\nlGOJ0UuomMSYjPKAdSwWOqPbT2bUTdGKVPgqlsiUMmqm/J1IcaT2OAtsZYTPUi95U/M/rJCa8lMm4sRQea\nLwoQjI9H0dRnihLDx5Zgopi9FZEhVpgYG1DBhuAtvrxMmdl76JcqZ+XqtdZHk4gmM4BQ8uoQq3UIMGEHiA\nZ3iFN0c6L8678zFvzTnZzCH8gfP5A6FNjy4=</latexit>zLQ\n<latexit sha1_base64=\"\nPEKyqTprQrg9kLdBz0h0kKjGy4I=\">AB7nicbVDLS\ngNBEOz1GeMr6tHLYBC8GHbF1zHoxWME84BkCbOT2WTI7\nOwy0yvEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJFIY\ndN1vZ2l5ZXVtvbBR3Nza3tkt7e03TJxqxuslrFuBdR\nwKRSvo0DJW4nmNAokbwbD24nfOTaiFg94CjhfkT7So\nSCUbRS86mb4ak37pbKbsWdgiwSLydlyFHrlr46vZilEV\nfIJDWm7bkJ+hnVKJjk42InNTyhbEj7vG2pohE3fjY9d\n0yOrdIjYaxtKSRT9fdERiNjRlFgOyOKAzPvTcT/vHaK\n4bWfCZWkyBWbLQpTSTAmk9JT2jOUI4soUwLeythA6o\npQ5tQ0Ybgzb+8SBpnFe+ycnF/Xq7e5HEU4BCO4AQ8uI\nq3EN6sBgCM/wCm9O4rw4787HrHXJyWcO4A+czx8Y8I\n9t</latexit>zt\u22121\nTrimmed ControlNet\nFixed\nConnector\n(a) Overall Architecture Design\n(b) Trimmed Encoder Copy\n(c) ZeroSFT Connector\nConv\n<latexit sha1_base64=\"3hMqANsWZC8vK3VPuIw/c1ZP1RQ=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ\n2dlZlYISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBsFrFqB1Sj4BIbhuB7UQhjQKBrWB0O/VbT6g0j+WjGSfoR3QgecgZNVZ6aPdYr1xq+4MZJl4OalAjnqv/NXtxyNUBomqNYdz02Mn1FlOBM4KXVTjQlIzrAjqWSRqj9bHbqhJxY\npU/CWNmShszU3xMZjbQeR4HtjKgZ6kVvKv7ndVITXvsZl0lqUL5ojAVxMRk+jfpc4XMiLElClubyVsSBVlxqZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnBfn3fmYtxacfOYQ/sD5/AEp+I28</latexit>Xc\n<latexit sha1_base64=\"j1muYcTaulj6/NUw1/uBRTi8AU8=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ\n2dlZlYISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBsFrFqB1Sj4BIbhuB7UQhjQKBrWB0O/VbT6g0j+WjGSfoR3QgecgZNVZ6aPd0r1xq+4MZJl4OalAjnqv/NXtxyNUBomqNYdz02Mn1FlOBM4KXVTjQlIzrAjqWSRqj9bHbqhJxY\npU/CWNmShszU3xMZjbQeR4HtjKgZ6kVvKv7ndVITXvsZl0lqUL5ojAVxMRk+jfpc4XMiLElClubyVsSBVlxqZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnBfn3fmYtxacfOYQ/sD5/AFCOI3M</latexit>Xs\n<latexit sha1_base64=\"8wgcVSg+XRDXEFK9VGOLugeDpmk=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ\n2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSQ7sX9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEn\nVumTMNa2FJKZ+nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou/Mxby04+cwh/IHz+QMuhI2/</latexit>Xf\nZeroConv\nZeroConv\nZeroConv\n<latexit sha1_base64=\"Mpz3hOkPTIDGav9PlnuYU4s0kLM=\">AB7XicbVDLSgNBEOyNrxhfUY9eFoPgKeyKr2PQi\n8cI5gHJEmYns8mY2ZlplcIS/7BiwdFvPo/3vwbJ8keNLGgoajqprsrTAQ36HnfTmFldW19o7hZ2tre2d0r7x80jUo1ZQ2qhNLtkBgmuGQN5ChYO9GMxKFgrXB0O/VbT0wbruQDjhMWxGQgecQpQSs1u8hjZnrlilf1ZnCXiZ+TCuSo98pf3b6iac\nwkUkGM6fhegkFGNHIq2KTUTQ1LCB2RAetYKoldEmSzayfuiVX6bqS0LYnuTP09kZHYmHEc2s6Y4NAselPxP6+TYnQdZFwmKTJ54uiVLio3Onrbp9rRlGMLSFUc3urS4dE4o2oJINwV98eZk0z6r+ZfXi/rxSu8njKMIRHMp+HAFNbiDOjSAwiM\n8wyu8Ocp5cd6dj3lrwclnDuEPnM8fuWuPg=</latexit>\u21e5\n<latexit sha1_base64=\"Age47FzHBvUqYGPEfH8g0BXt4k=\">AB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB\n6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVN91dfiy4Nq7eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nfEKleSQfzDjGbkgHkgecUWOl2lmvWHL7gxkmXgZKUGaq/41elHLAlRGiao1m3PjU03pcpwJn\nBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3\nTFjLk=</latexit>+\nConcat\nGroupNorm\n<latexit sha1_base64=\"Age47FzHBvUqYGPEfH8g0BXt4k=\">AB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB\n6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVN91dfiy4Nq7eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nfEKleSQfzDjGbkgHkgecUWOl2lmvWHL7gxkmXgZKUGaq/41elHLAlRGiao1m3PjU03pcpwJn\nBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3\nTFjLk=</latexit>+\n<latexit sha1_base64=\"qw9ZTuygU+yORxHPDdpt6pPWxpQ=\">AB7XicbVDJSgNBEK2JW4xb1KOXxiB4CjPidgx68RjBLJAMoafTk7Tp\nZejuEcKQf/DiQRGv/o83/8ZOMgdNfFDweK+KqnpRwpmxv/tFVZW19Y3ipulre2d3b3y/kHTqFQT2iCK92OsKGcSdqwzHLaTjTFIuK0FY1up37riWrDlHyw4SGAg8kixnB1knNdi+L1aRXrvhVfwa0TIKcVCBHvVf+6vYVSQWVlnBsTCfwExtmWFtGOJ2UuqmhCSYjPKAdRyUW1ITZ7NoJ\nOnFKH8VKu5IWzdTfExkWxoxF5DoFtkOz6E3F/7xOauPrMGMyS2VZL4oTjmyCk1fR32mKbF87AgmrlbERlijYl1AZVcCMHiy8ukeVYNLqsX9+eV2k0eRxGO4BhOIYArqMEd1KEB7hGV7hzVPei/fufcxbC14+cwh/4H3+AMJNj0Q=</latexit>Xfo\nViT Block\nViT Block\nViT Block\nViT Block\n<latexit sha1_base64=\"xQ6/ZQgfBD+ahZzIThJbHMgIUTc=\">AB63icbVDLSgNBEOyJrxhfUY9eBoPgKe4GX8egF08SwTwgWcLsZDYZMjO7zMwKYckvePGgiF\nd/yJt/426yB0saCiqunu8iPBjXWcb1RYWV1b3yhulra2d3b3yvsHLRPGmrImDUWoOz4xTHDFmpZbwTqRZkT6grX98W3mt5+YNjxUj3YSMU+SoeIBp8Rm0v1ZrdQvV5yqMwNeJm5OKpCj0S9/9QYhjSVTlgpiTNd1IuslRFtOBZuWerFhEaFjMmTdlCoimfGS2a1TfJIqAxyEOi1l8Uz9PZEQacxE+mnJHZkFr1M/M/rxja49hKuotgy\nReLglhgG+LscTzgmlErJikhVP0VkxHRBNq03iyENzFl5dJq1Z1L6sXD+eV+k0eRxGO4BhOwYUrqMdNKAJFEbwDK/whiR6Qe/oY95aQPnMIfwB+vwBvDiNZQ=</latexit>N/2\n<latexit sha1_base64=\"Myr3Kcmiy/fK9XQulchFknRygM=\">AB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQiydJwDwgWcLspJOMmZ1dZmaFsOQLvHhQxK\nuf5M2/cZLsQRMLGoqbrq7glhwbVz328mtrK6tb+Q3C1vbO7t7xf2Dho4SxbDOIhGpVkA1Ci6xbrgR2IoV0jAQ2AxGt1O/+YRK80g+mHGMfkgHkvc5o8ZKtftuseSW3RnIMvEyUoIM1W7xq9OLWBKiNExQrdueGxs/pcpwJnBS6CQaY8pGdIBtSyUNUfvp7NAJObFKj/QjZUsaMlN/T6Q01HocBrYzpGaoF72p+J/XTkz/2k+5jBODks0X\n9RNBTESmX5MeV8iMGFtCmeL2VsKGVFmbDYFG4K3+PIyaZyVvcvyRe28VLnJ4sjDERzDKXhwBRW4gyrUgQHCM7zCm/PovDjvzse8NedkM4fwB87nD6nRjNw=</latexit>N\nBlocks\nRemained\n<latexit sha1_base64=\"xQ6/ZQgfBD+ahZzIThJbHMgIUTc=\">AB63icbVDLSgNBEOyJrxhfUY9eBoPgKe4GX8egF08SwTwgWcLsZDYZMjO7zMwKYckvePGgiF\nd/yJt/426yB0saCiqunu8iPBjXWcb1RYWV1b3yhulra2d3b3yvsHLRPGmrImDUWoOz4xTHDFmpZbwTqRZkT6grX98W3mt5+YNjxUj3YSMU+SoeIBp8Rm0v1ZrdQvV5yqMwNeJm5OKpCj0S9/9QYhjSVTlgpiTNd1IuslRFtOBZuWerFhEaFjMmTdlCoimfGS2a1TfJIqAxyEOi1l8Uz9PZEQacxE+mnJHZkFr1M/M/rxja49hKuotgy\nReLglhgG+LscTzgmlErJikhVP0VkxHRBNq03iyENzFl5dJq1Z1L6sXD+eV+k0eRxGO4BhOwYUrqMdNKAJFEbwDK/whiR6Qe/oY95aQPnMIfwB+vwBvDiNZQ=</latexit>N/2\nTrimmed\nZeroConv\nConv\n<latexit sha1_base64=\"Age47FzHBvUqYGPEfH8g0BXt4k\n=\">AB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVN91\ndfiy4Nq7eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nfEKleSQfzDjGbkgHkgecUWOl2\nlmvWHL7gxkmXgZKUGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Ho\nc+rYzpGaoF72p+J/XTkxw025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThC\nI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit>+\nFigure 3. This figure illus-\ntrates (a) the overall archi-\ntecture of the used SDXL\nand\nthe\nproposed\nadap-\ntor, (b) a trimmed trainable\ncopy of the SDXL encoder\nwith reduced ViT blocks for\nefficiency, and (c) a novel\nZeroSFT connector for en-\nhanced control in IR, where\nXf and Xs denote the input\nfeature maps from the De-\ncoder and Encoder shortcut,\nrespectively, Xc is the input\nfrom the adaptor, and Xfo\nis the output. The model is\ndesigned to effectively use\nthe large-scale SDXL as a\ngenerative prior.\ning an engineering-feasible implementation.\nEach block\nwithin the encoder module of SDXL is mainly composed\nof several Vision Transformer (ViT) [21] blocks. We iden-\ntified two key factors contributing to the effectiveness of\nControlNet: large network capacity and efficient initializa-\ntion of the trainable copy. Notably, even partial trimming of\nblocks in the trainable copy retains these crucial character-\nistics in the adaptor. Therefore, we simply trim half of the\nViT blocks from each encoder block, as shown in Fig. 3(b).\nSecond, we redesign the connector that links the adaptor to\nSDXL. While SDXL\u2019s generative capacity delivers excel-\nlent visual effects, it also renders pixel-level precise control\nchallenging. ControlNet employs zero convolution for gen-\neration guidance, but relying solely on residuals is insuffi-\ncient for the control required by IR. To amplify the influ-\nence of LQ guidance, we introduced a ZeroSFT module, as\ndepicted in Fig. 3(c). Building based on zero convolution,\nZeroSFT encompasses an additional spatial feature transfer\n(SFT) [79] operation and group normalization [84].\n3.2. Scaling Up Training Data\nImage Collection.\nThe scaling of the model requires a\ncorresponding scaling of the training data [38]. But there\nis no large-scale high-quality image dataset available for IR\nyet. Although DIV2K [3] and LSDIR [1] offer high im-\nage quality, they are limited in quantity. Larger datasets\nlike ImageNet (IN) [17], LAION-5B[69], and SA-1B [44]\ncontain more images, but their image quality does not meet\nour high standards. To this end, we collect a new large-\nscale dataset of high-resolution images, which includes 20\nmillion 1024\u00d71024 high-quality, texture-rich, and content-\nclear images.\nA comparison on scales of the collected\ndataset and the existing dataset is shown in Fig. 3. We also\nincluded an additional 70K unaligned high-resolution facial\nimages from FFHQ-raw dataset [40] to improve the model\u2019s\nLow-Quality Input\nw/o Negative samples\nNo \nNegative \nprompt\nUse \nNegative \nprompt\nw/ Negative samples\nOurs\nFigure 4. CFG introduces artifacts without negative training sam-\nples, hindering visual quality improvement. Adding negative sam-\nples allows further quality enhancement through CFG.\nface restoration performance. In Fig. 5(a), we show the rela-\ntive size of our data compared to other well-known datasets.\nMulti-Modality Language Guidance.\nDiffusion models\nare renowned for their ability to generate images based on\ntextual prompts. We believe that textual prompts can also\nsignificantly aid IR for the following reasons: (1) Under-\nstanding image content is crucial for IR. Existing frame-\nworks often overlook or implicitly handle this understand-\ning [24, 29]. By incorporating textual prompts, we explic-\nitly convey the understanding of LQ images to the IR model,\nfacilitating targeted restoration of missing information. (2)\nIn cases of severe degradation, even the best IR models may\nstruggle to recover completely lost information. In such\ncases, textual prompts can serve as a control mechanism,\nenabling targeted completion of missing information based\non user preferences. (3) We can also describe the desired\nimage quality through text, further enhancing the percep-\ntual quality of the output. See Fig. 1(b) for some exam-\nples. To this end, we make two main modifications. First,\nwe revise the overall framework to incorporate the LLaVA\nmulti-modal large language model [52] into our pipeline,\nas shown in Fig. 2. LLaVA takes the degradation-robust\nprocessed LQ images x\u2032\nLQ = D(Edr(xLQ)) as input and\nexplicitly understands the content within the images, out-\n4\nputting in the form of textual descriptions. These descrip-\ntions are then used as prompts to guide the restoration. This\nprocess can be automated during testing, eliminating the\nneed for manual intervention. Secondly, following the ap-\nproach of PixART [12], we also collect textual annotations\nfor all the training images, to reinforce the role of textual\ncontrol during the training of out model. These two changes\nendow SUPIR with the ability to understand image content\nand to restore images based on textual prompts.\nNegative-Quality Samples and Prompt.\nClassifier-free\nguidance (CFG) [30] provides another way of control by\nusing negative prompts to specify undesired content for the\nmodel. We can use this feature to specify the model NOT\nto produce low-quality images. Specifically, at each step\nof diffusion, we will make two predictions using positive\nprompts pos and negative prompts neg, and take the fusion\nof these two results as the final output zt\u22121:\nzpos\nt\u22121 = H(zt, zLQ, \u03c3t, pos), zneg\nt\u22121 = H(zt, zLQ, \u03c3t, neg),\nzt\u22121 = zpos\nt\u22121 + \u03bbcfg \u00d7 (zpos\nt\u22121 \u2212 zneg\nt\u22121),\nwhere H(\u00b7) is our diffusion model with adaptor, \u03c3t is the\nvariance of the noise at time-step t, and \u03bbcfg is a hyper-\nparameter. In our framework, pos can be the image de-\nscription with positive words of quality, and neg is the neg-\native words of quality, e.g., \u201coil painting, cartoon, blur,\ndirty, messy, low quality, deformation, low resolution, over-\nsmooth\u201d.\nAccuracy in predicting both positive and neg-\native directions is crucial for the CFG technique.\nHow-\never, the absence of negative-quality samples and prompts\nin our training data may lead to a failure of the fine-tuned\nSUPIR in understanding negative prompts. Therefore, us-\ning negative-quality prompts during sampling may intro-\nduce artifacts, see Fig. 4 for an example. To address this\nproblem, we used SDXL to generate 100K images cor-\nresponding to the negative-quality prompts. We counter-\nintuitively add these low-quality images to the training data\nto ensure that negative-quality concept can be learned by\nthe proposed SUPIR model.\n3.3. Restoration-Guided Sampling\nPowerful generative prior is a double-edged sword, as too\nmuch generation capacity will in turn affect the fidelity of\nthe recovered image. This highlights the fundamental dif-\nference between IR tasks and generation tasks. We need\nmeans to limit the generation to ensure that the image recov-\nery is faithful to the LQ image. We modified the EDM sam-\npling method [41] and proposed a restoration-guided sam-\npling method to solve this problem. We hope to selectively\nguide the prediction results zt\u22121 to be close to the LQ image\nzLQ in each diffusion step. The specific algorithm is shown\nin Algorithm 1, where T is the total step number, {\u03c3t}T\nt=1\nare the noise variance for T steps, c is the additional text\nprompt condition. \u03c4r, Schurn, Snoise, Smin, Smax are five\nhyper-parameters, but only \u03c4r is related to the restoration\nAlgorithm 1 Restoration-Guided Sampling.\nInput: H, {\u03c3t}T\nt=1, zLQ, c\nHyper-parameter: \u03c4r, Schurn, Snoise, Smin, Smax\n1: sample zT \u223c N(0, \u03c32\nT I)\n2: for t \u2208 {T, . . . , 1} do\n3:\nsample \u03f5t \u223c N\n\u00000, S2\nnoiseI\n\u0001\n4:\n\u03b3t \u2190\n(\nmin\n\u0010\nSchurn\nN\n,\n\u221a\n2 \u2212 1\n\u0011\nif \u03c3t \u2208 [Smin, Smax]\n0\notherwise\n5:\nkt \u2190 (\u03c3t/\u03c3T )\u03c4r, \u02c6zt \u2190 zt +\nq\n\u02c6\u03c32\nt \u2212 \u03c32\nt \u03f5t, \u02c6\u03c3t \u2190 \u03c3t + \u03b3t\u03c3t\n6:\n\u02c6zt\u22121 \u2190 H\n\u0000\u02c6zt, zLQ, \u02c6\u03c3t, c\n\u0001\n7:\ndt \u2190 (\u02c6zt \u2212 (\u02c6zt\u22121 + kt(zLQ \u2212 \u02c6zt\u22121)))/\u02c6\u03c3t\n8:\nzt\u22121 \u2190 \u02c6zt + (\u03c3t\u22121 \u2212 \u02c6\u03c3t) dt\n9: end for\n1010\n1011\n1012\n1013\nPixel Numbers (log scaled)\nOurs\nSA-1B\nIN21k\nIN1K\nLSDIR\nDIV2K\nDataset\n(a)\n<latexit sha1_base64=\"SvnUMC6ZPmzYgxhmuaZTLPLV4U=\">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe7EqGXQxsIiAfMByRH2NnvJmr3dY3dPiEf+g42FIrb+Hzv/jZvkCk18\nMPB4b4aZeUHMmTau+3kVlbX1jfym4Wt7Z3dveL+QVPLRBHaIJL1Q6wpwJ2jDMcNqOFcVRwGkrGN1M/dYjVZpJcW/GMfUjPBAsZAQbKzWfeuldfdIrltyOwNaJl5GSpCh1it+dfuSJBEVhnCsdcdzY+OnWBlGOJ0UuomMSYjPKAdSwWOqPbT2bUTdGKVPgqlsiUMmqm/J1IcaT2OAtsZYTPUi95U/M/rJCa8lMm4sRQeaLwoQjI9H0dRnihLDx5Zgopi9FZEhVpgYG1\nDBhuAtvrxMmdl76JcqZ+XqtdZHk4gmM4BQ8uoQq3UIMGEHiAZ3iFN0c6L8678zFvzTnZzCH8gfP5A6FNjy4=</latexit>zLQ\n<latexit sha1_base64=\"iJryxIawiGO1kijL0nrMvPZhcqU=\">AB9HicdVDLSgNBEJyNrxhfUY9eBoPgxWV38/QW9OIxgkmEZAmzk0kyZPbhTG8gLvkOLx4U8erHePNvnE0iqGhB\nQ1HVTXeXFwmuwLI+jMzK6tr6RnYzt7W9s7uX3z9oqTCWlDVpKEJ56xHFBA9YEzgIdhtJRnxPsLY3vkz9oRJxcPgBqYRc30yDPiAUwJacrsjAsn9rJfAmT3r5QuWeV6rOGUHW6ZlVZ1iJSVOteQUsa2VFAW0RKOXf+/2Qxr7LAqiFId24rATYgETgWb5bqxYhGhYzJkHU0D4jPlJvOjZ/hEK308CKWuAPBc/T6REF+pqe/pTp/ASP32UvEvrxPDoOYmPIhiYAFdLBrEAkOI0w\nRwn0tGQUw1IVRyfSumIyIJBZ1Tofw9Sn+n7Qc06Y5etSoX6xjCOLjtAxOkU2qI6ukIN1EQU3aEH9ISejYnxaLwYr4vWjLGcOUQ/YLx9AlWMkn4=</latexit>\u02c6zt\u22121\n<latexit sha1_base64=\"8VsJ5v+7Vi351XN7aZxLtXGYP5A=\">AB7nicdVDLSgNBEOz1GeMr6tHLYBC8uMwmboy3oBePEcwDkiXMTibJkNkHM7NCXPIRXjwo4tXv8ebfOJtEUNGC\nhqKqm+4uPxZcaYw/rKXldW19dxGfnNre2e3sLfVFEiKWvQSESy7RPFBA9ZQ3MtWDuWjAS+YC1/fJX5rTsmFY/CWz2JmReQYcgHnBJtpNZ9L9WnzrRXKGIbV9zyhYuw7WKnWspIyXUwLiPHxjMUYF6r/De7Uc0CVioqSBKdRwcay8lUnMq2DTfTRSLCR2TIesYGpKAKS+dnTtFx0bpo0EkTYUazdTvEykJlJoEvukMiB6p314m/uV1Ej2oeikP40SzkM4XDRKBdISy31GfS0\na1mBhCqOTmVkRHRBKqTUJ5E8LXp+h/0izZTsV2b86KtctFHDk4hCM4AQfOoQbXUIcGUBjDAzBsxVbj9aL9TpvXbIWMwfwA9bJ3BOj6k=</latexit>zt\u22121\n<latexit sha1_base64=\"0G+O3aFGJ/Wsm7jsz1GfZCKldtY=\">ACG3icdZA9SwNBEIb3/DZ+RS1tFoMQC+Nd0Jh0o2FhYJRIRePuc0mWbL3we6cEI/7Hzb+FRsL\nRawEC/+NmxhBRQcWHt53htl5/VgKjb9bo2NT0xOTc/M5ubmFxaX8sr5zpKFON1FslIXfqguRQhr6NAyS9jxSHwJb/we4cD/+KaKy2i8Az7MW8G0AlFWzBAI3n5ctHVohOAh9sjONu8Sl2ExFNZ8cZLj0+zLbcLmN5kXopbTrbp5Qt2yTZVqdABOFXbMVCrVcvlGnWGlm0XyKhOvPyr24pYEvAQmQStG4dYzMFhYJnuXcRPMYWA86vGEwhID\nrZjq8LaMbRmnRdqTMC5EO1e8TKQRa9wPfdAaAXf3bG4h/eY0E29VmKsI4QR6yz0XtRFKM6CAo2hKM5R9A8CUMH+lrAsKGJo4cyaEr0vp/3BeLjmV0u7pTmH/YBTHDFkj6RIHLJH9skROSF1wsgtuSeP5Mm6sx6sZ+vls3XMGs2skh9lvX0AbM2hsw=</latexit>(\u03c3t/\u03c3T )\u2327r(zLQ \u2212 \u02c6zt\u22121)\n(b)\nFigure 5.\n(a) We show the relative size of our data compared\nto other well-known datasets. Compared with SA-1B [44], our\ndataset has higher quality and more image diversity.\n(b) We\ndemonstrate our restoration-guided sampling mechanism.\nguidance, the others remain unchanged compared to the\noriginal EDM method [41]. For better understanding, a sim-\nple diagram is shown in Fig. 5(b). We perform weighted\ninterpolation between the predicted output \u02c6zt\u22121 and the LQ\nlatent zLQ as the restoration-guided output zt\u22121. Since the\nlow-frequency information of the image is mainly generated\nin the early stage of diffusion prediction [67] (where t and\n\u03c3t are relatively large, and the weight k = (\u03c3t/\u03c3T )\u03c4r is\nalso large), the prediction result is closer to zLQ to enhance\nfidelity. In the later stages of diffusion prediction, mainly\nhigh-frequency details are generated. There should not be\ntoo many constraints at this time to ensure that detail and\ntexture can be adequately generated. At this time, t and \u03c3t\nare relatively small, and weight k is also small. Therefore,\nthe predicted results will not be greatly affected Through\nthis method, we can control the generation during the diffu-\nsion sampling process to ensure fidelity.\n4. Experiments\n4.1. Model Training and Sampling Settings\nFor training, the overall training data includes 20 million\nhigh-quality images with text descriptions, 70K face images\nand 100K negative-quality samples and the corresponding\nnegative prompts. To enable a larger batch size, we crop\nthem into 512\u00d7512 patches during training. We train our\nmodel using a synthetic degradation model, following the\nsetting used by Real-ESRGAN [81], the only difference\nis that we resize the produced LQ images to 512\u00d7512 for\ntraining. We use the AdamW optimizer [54] with a learning\nrate of 0.00001. The training process spans 10 days and is\n5\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours).\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nFigure 6. Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding\nobject under challenging degradation. Other methods fail to recover semantically correct details such as broken beaks and irregular faces.\nLow-Quality Input\nBSRGAN\nSUPIR (Ours)\nSUPIR (Ours)\nPSNR:\n21.28\nSSIM:\n0.6938\nLPIPS:\n0.2609\nPSNR:\n19.17\nSSIM:\n0.5578\nLPIPS:\n0.3297\nPSNR:\n21.05\nSSIM:\n0.4339\nLPIPS:\n0.3492\nPSNR:\n19.28\nSSIM:\n0.3991\nLPIPS:\n0.3951\nStableSR\nPrefered by metrics\nPrefered by human\nLow-Quality Input\nFigure 7. These examples show the misalignment between metric\nevaluation and human evaluation. SUPIR generates images with\nhigh-fidelity textures, but obtains lower metrics.\nconducted on 64 Nvidia A6000 GPUs, with a batch size of\n256. For testing, the hyper-parameters are T=100, \u03bbcfg=7.5,\nand \u03c4r = 4. Our method is able to process images with the\nsize of 1024\u00d71024. We resize the short side of the input im-\nage to 1024 and crop a 1024\u00d71024 sub-image for testing,\nand then resize it back to the original size after restoration.\nUnless stated otherwise, prompts will not be provided man-\nually \u2013 the processing will be entirely automatic.\n4.2. Comparison with Existing Methods\nOur method can handle a wide range of degradations,\nand we compare it with the state-of-the-art methods with\nthe same capabilities, including BSRGAN [93], Real-\nESRGAN [81], StableSR [77], DiffBIR [49] and PASD\n[88]. Some of them are constrained to generating images\nof 512\u00d7512 size. In our comparison, we crop the test im-\nage to meet this requirement and downsample our results\nto facilitate fair comparisons. We conduct comparisons on\nboth synthetic data and real-world data.\nSynthetic Data.\nTo synthesize LQ images for testing, we\nfollow previous works [45, 97] and demonstrate our effects\non several representative degradations, including both sin-\ngle degradations and complex mixture degradations. Spe-\ncific details can be found in Tab. 1. We selected the fol-\nlowing metrics for quantitative comparison: full-reference\nmetrics PSNR, SSIM, LPIPS [96], and the non-reference\nmetrics ManIQA [86], ClipIQA [76], MUSIQ [43]. It can\nbe seen that our method achieves the best results on all\nnon-reference metrics, which reflects the excellent image\nquality of our results. At the same time, we also note the\ndisadvantages of our method in full-reference metrics. We\npresent a simple experiment that highlights the limitations\nof these full-reference metrics, see Fig. 7. It can be seen\n6\nDegradation\nMethod\nPSNR\nSSIM\nLPIPS\u2193\nManIQA\nClipIQA\nMUSIQ\nSingle:\nSR (\u00d74)\nBSRGAN\n25.06\n0.6741\n0.2159\n0.2214\n0.6169\n70.38\nReal-ESRGAN\n24.26\n0.6657\n0.2116\n0.2287\n0.5884\n69.51\nStableSR\n22.59\n0.6019\n0.2130\n0.3304\n0.7520\n72.94\nDiffBIR\n23.44\n0.5841\n0.2337\n0.2879\n0.7147\n71.64\nPASD\n24.90\n0.6653\n0.1893\n0.2607\n0.6466\n71.39\nSUPIR (ours)\n22.66\n0.5763\n0.2662\n0.4738\n0.8049\n73.83\nSingle:\nSR (\u00d78)\nBSRGAN\n22.26\n0.5212\n0.3523\n0.2069\n0.5836\n67.04\nReal-ESRGAN\n21.79\n0.5280\n0.3276\n0.2051\n0.5349\n63.80\nStableSR\n21.27\n0.4857\n0.3118\n0.3039\n0.7333\n71.74\nDiffBIR\n21.86\n0.4957\n0.3106\n0.2845\n0.7080\n70.26\nPASD\n21.97\n0.5149\n0.3034\n0.2412\n0.6402\n70.20\nSUPIR (ours)\n20.68\n0.4488\n0.3749\n0.4687\n0.8009\n73.16\nMixture:\nBlur (\u03c3=2) +\nSR (\u00d74)\nBSRGAN\n24.97\n0.6572\n0.2261\n0.2127\n0.5984\n69.44\nReal-ESRGAN\n24.08\n0.6496\n0.2208\n0.2357\n0.5853\n69.27\nStableSR\n22.26\n0.5721\n0.2301\n0.3204\n0.7488\n72.87\nDiffBIR\n23.28\n0.5741\n0.2395\n0.2829\n0.7055\n71.22\nPASD\n24.85\n0.6560\n0.1952\n0.2500\n0.6335\n71.07\nSUPIR (ours)\n22.43\n0.5626\n0.2771\n0.4757\n0.8110\n73.55\nMixture:\nSR (\u00d74)+\nNoise (\u03c3=40)\nBSRGAN\n17.74\n0.3816\n0.5659\n0.1006\n0.4166\n51.25\nReal-ESRGAN\n21.46\n0.5220\n0.4636\n0.1236\n0.4536\n52.23\nStableSR\n20.88\n0.4174\n0.4668\n0.2365\n0.5833\n63.54\nDiffBIR\n22.08\n0.4918\n0.3738\n0.2403\n0.6435\n65.97\nPASD\n21.79\n0.4983\n0.3842\n0.2590\n0.5939\n69.09\nSUPIR (ours)\n20.77\n0.4571\n0.3945\n0.4674\n0.7840\n73.35\nMixture:\nBlur (\u03c3=2) +\nSR (\u00d74)+\nNoise\n(\u03c3=20)+\nJPEG (q=50)\nBSRGAN\n22.88\n0.5397\n0.3445\n0.1838\n0.5402\n64.81\nReal-ESRGAN\n22.01\n0.5332\n0.3494\n0.2115\n0.5730\n64.76\nStableSR\n21.39\n0.4744\n0.3422\n0.2974\n0.7354\n70.94\nDiffBIR\n21.79\n0.4895\n0.3465\n0.2821\n0.7059\n69.28\nPASD\n21.90\n0.5118\n0.3493\n0.2397\n0.6326\n70.43\nSUPIR (ours)\n20.84\n0.4604\n0.3806\n0.4688\n0.8021\n73.58\nTable 1. Quantitative comparison. Red and blue colors represent\nthe best and second best performance. \u2193 represents the smaller the\nbetter, and for the others, the bigger the better.\nMetrics\nBSRGAN\nReal-ESRGAN\nStableSR\nDiffBIR\nPASD\nOurs\nCLIP-IQA\n0.4119\n0.5174\n0.7654\n0.6983\n0.7714\n0.8232\nMUSIQ\n55.64\n59.42\n70.70\n69.69\n71.87\n73.00\nMANIQA\n0.1585\n0.2262\n0.3035\n0.2619\n0.3169\n0.4295\n(a) Quantitative comparison on 60 real-world LQ images.\nNegative\nPrompts\nPSNR\nSSIM\nLPIPS\u2193\nManIQA\nClipIQA\nMUSIQ\nSamples\nPositive\nNegative\n\u2713\n22.90\n0.5519\n0.3010\n0.3129\n0.7049\n68.94\n\u2713\n\u2713\n22.31\n0.5250\n0.3108\n0.4018\n0.7937\n72.00\n\u2713\n\u2713\n20.63\n0.4747\n0.3603\n0.4678\n0.7933\n73.60\n\u2713\n\u2713\n\u2713\n20.66\n0.4763\n0.3412\n0.4740\n0.8164\n73.66\n\u2713\n\u2713\n21.79\n0.5119\n0.3139\n0.3180\n0.7102\n72.68\n(b) Ablation study of quality prompts and negative training samples.\nConnector\nPSNR\nSSIM\nLPIPS\u2193\nManIQA\nClipIQA\nMUSIQ\nZero Convolution [95]\n19.47\n0.4261\n0.3969\n0.4845\n0.8184\n74.00\nZeroSFT\n20.66\n0.4763\n0.3412\n0.4740\n0.8164\n73.66\n(c) Ablation study of zero convolution and the proposed ZeroSFT.\nTable 2. Real-world comparison results and ablation studies.\nthat our results have better visual effects, but they do not\nhave an advantage in these metrics. This phenomenon has\nalso been noted in many studies as well [6, 26, 28]. We ar-\ngue that with the improving quality of IR, there is a need\nto reconsider the reference values of existing metrics and\nsuggest more effective ways to evaluate advanced IR meth-\nods. We also show some qualitative comparison results in\nFig. 6. Even under severe degradation, our method consis-\ntently produces highly reasonable and high-quality images\nthat faithfully represent the content of the LQ images.\nRestoration in the Wild.\nWe also test our method on\nreal-world LQ images. We collect a total of 60 real-world\nLQ images from RealSR [8], DRealSR [83], Real47 [49],\nand online sources, featuring diverse content including an-\nr=1\nr=2\nr=3\nr=4\nr=5\nr=6\nNo r\n20.5\n21.0\n21.5\n22.0\n22.5\nPSNR\nPSNR\n0.46\n0.48\n0.50\n0.52\nSSIM\nSSIM\nr=1\nr=2\nr=3\nr=4\nr=5\nr=6\nNo r\n0.40\n0.42\n0.44\n0.46\n0.48\nMANIQA\nManIQA\n0.77\n0.79\n0.81\n0.83\nCLIP-IQA\nClip-IQA\n3.25%\nStableSR\n2.00%\nDIffBIR\n10.58%\nPASD\n84.17%\nSUPIR\n(a)\n(b)\nFigure 8. (a) These plots illustrate the quantitative results as a\nfunction of the variable \u03c4r. \u201cNo \u03c4r\u201d means not to use the proposed\nsampling method. (b) The results of our user study.\nLow-Quality Input\nZero Convolution Connector\nOur ZeroSFT Connector\nFigure 9. We compare the proposed ZeroSFT with zero convolu-\ntion. Directly using zero convolution results in redundant details.\nThe low-fidelity details can be effectively mitigated by ZeroSFT.\nimals, plants, faces, buildings, and landscapes. We show\nthe qualitative results in Fig. 10, and the quantitative results\nare shown in Tab. 2a. These results indicate that the im-\nages produced by our method have the best perceptual qual-\nity. We also conduct a user study comparing our method\non real-world LQ images, with 20 participants involved.\nFor each set of comparison images, we instructed partici-\npants to choose the restoration result that was of the highest\nquality among these test methods. The results are shown\nin Fig. 8, revealing that our approach significantly outper-\nformed state-of-the-art methods in perceptual quality.\n4.3. Controlling Restoration with Textual Prompts\nAfter training on a large dataset of image-text pairs and\nleveraging the feature of the diffusion model, our method\ncan selectively restore images based on human prompts.\nFig. 1(b) illustrates some examples. In the first case, the\nbike restoration is challenging without prompts, but upon\nreceiving the prompt, the model reconstructs it accurately.\nIn the second case, the material texture of the hat can be\nadjusted through prompts. In the third case, even high-level\nsemantic prompts allow manipulation over face attributes.\nIn addition to prompting the image content, we can also\nprompt the model to generate higher-quality images through\nnegative-quality prompts. Fig. 11(a) shows two examples.\nIt can be seen that the negative prompts are very effective in\nimproving the overall quality of the output image. We also\nobserved that prompts in our method are not always effec-\ntive. When the provided prompts do not align with the LQ\nimage, the prompts become ineffective, see Fig. 11(b). We\nconsider this reasonable for an IR method to stay faithful to\nthe provided LQ image. This reflects a significant distinc-\ntion from text-to-image generation models and underscores\nthe robustness of our approach.\n7\nLow-Quality Input\nReal-ESRGAN\nStableSR\nDiffBIR\nPASD\nSUPIR (Ours)\nFigure 10. Qualitative comparison on real-world LQ images. SUPIR successfully recovers structured buildings and lifelike rivers. It also\nmaintains the details existing in LQ, such as the horizontal planks in the beach chairs. Zoom in for better view.\nLow-Quality Input\nNo Negative \nPrompt\nUse Negative \nPrompt\nLow-Quality Input\nNo Negative \nPrompt\nUse Negative \nPrompt\nLow-Quality Input\nNo Prompt\nText: some people \nat the beach\n(b) Textual Prompt must align with the LQ image.\nNot Working\n(a) Two Examples Showing the Effect of Negative Prompt \nFigure 11. Influences of text prompts. (a) Negative prompts lead to detailed and sharp restoration results. (b) Given a positive prompt with\nhallucinations, SUPIR avoids generating content absent in the LQ images. Zoom in for better view.\nLow-Quality Input\nTrained on DIV2K\nTrained on LSDIR\nTrained on Our Data\nFigure 12. Qualitative comparison for SUPIR training on datasets\nwith different scales. Zoom in for better view.\nLow-Quality Input\nFidelity\nRealistic\nr=0.5\nr=3\nr=6\nFigure 13. The effect of the proposed restoration-guided sampling\nmethod. A smaller \u03c4r makes the result more biased toward the\nLQ image, which emphasizes the fidelity. A larger \u03c4r emphasizes\nperceived quality, but with lower fidelity. Zoom in for better view.\n4.4. Ablation Study\nConnector.\nWe compare the proposed ZeroSFT connec-\ntor with zero convolution [95].\nQuantitative results are\nshown in Tab. 2c. Compared to ZeroSFT, zero convolu-\ntion yields comparable performance on non-reference met-\nrics and much lower full-reference performance. In Fig. 9,\nwe find that the drop in non-reference metrics is caused by\ngenerating low-fidelity content. Therefore, for IR tasks, Ze-\nroSFT ensures fidelity without losing the perceptual effect.\nTraining data scaling.\nWe trained our large-scale model\non two smaller datasets for IR, DIV2K [3] and LSDIR [1].\nThe qualitative results are shown in Fig. 12, which clearly\ndemonstrate the importance and necessity of training on\nlarge-scale high-quality data.\nNegative-quality samples and prompt.\nTab. 2b shows\nsome quantitative results under different settings.\nHere,\nwe use positive words describing image quality as \u201cposi-\ntive prompt\u201d, and use negative quality words and the CFG\nmethods described in Sec. 3.2 as negative prompt. It can be\nseen that adding positive prompts or negative prompts alone\ncan improve the perceptual quality of the image. Using both\nof them simultaneously yields the best perceptual results. If\nnegative samples are not included for training, these two\nprompts will not be able to improve the perceptual quality.\nFig. 4 and Fig. 11(a) demonstrate the improvement in image\nquality brought by using negative prompts.\nRestoration-guided sampling method.\nThe proposed\nrestoration-guided sampling method is mainly controlled by\nthe hyper-parameter \u03c4r. The larger \u03c4r is, the fewer correc-\ntions are made to the generation at each step. The smaller\n\u03c4r is, the more generated content will be forced to be closer\nto the LQ image. Please refer to Fig. 13 for a qualitative\ncomparison. When \u03c4r = 0.5, the image is blurry because\nits output is limited by the LQ image and cannot generate\ntexture and details. When \u03c4r = 6, there is not much guid-\nance during generation. The model generates a lot of tex-\nture that is not present in the LQ image, especially in flat\narea. Fig. 8(a) illustrates the quantitative results of restora-\ntion as a function of the variable \u03c4r. As shown in Fig. 8(a),\ndecreasing \u03c4r from 6 to 4 does not result in a significant de-\ncline in visual quality, while fidelity performance improves.\nAs restoration guidance continues to strengthen, although\nPSNR continues to improve, the images gradually become\nblurry with loss of details, as depicted in Fig. 13. Therefore,\nwe choose \u03c4r = 4 as the default parameter, as it doesn\u2019t sig-\nnificantly compromise image quality while effectively en-\n8\nhancing fidelity.\n5. Conclusion\nWe propose SUPIR as a pioneering IR method, empowered\nby model scaling, dataset enrichment, and advanced design\nfeatures, expanding the horizons of IR with enhanced per-\nceptual quality and controlled textual prompts.\nReferences\n[1] Lsdir dataset: A large scale dataset for image restoration.\nhttps://data.vision.ee.ethz.ch/yawli/\nindex.html, 2023. Accessed: 2023-11-15. 4, 8\n[2] Rameen Abdal, Yipeng Qin, and Peter Wonka.\nIm-\nage2stylegan: How to embed images into the stylegan la-\ntent space? In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 4432\u20134441, 2019. 2\n[3] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In The\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR) Workshops, 2017. 4, 8\n[4] David Bau, Hendrik Strobelt, William Peebles, Jonas\nWulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Se-\nmantic photo manipulation with a generative image prior.\narXiv preprint arXiv:2005.07727, 2020. 2\n[5] Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind\nsuper-resolution kernel estimation using an internal-gan.\nAdvances in Neural Information Processing Systems, 32,\n2019. 2\n[6] Yochai Blau and Tomer Michaeli. The perception-distortion\ntradeoff. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 6228\u20136237,\n2018. 7\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neu-\nral information processing systems, 33:1877\u20131901, 2020.\n2, 3\n[8] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao,\nand Lei Zhang.\nToward real-world single image super-\nresolution: A new benchmark and a new model. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3086\u20133095, 2019. 7\n[9] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu,\nand Chen Change Loy.\nGlean: Generative latent bank\nfor large-factor image super-resolution. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14245\u201314254, 2021. 3\n[10] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xi-\naoguang Han, Tao Yang, and Shihui Guo. Real-world blind\nsuper-resolution via feature matching with implicit high-\nresolution priors. In Proceedings of the 30th ACM Interna-\ntional Conference on Multimedia, pages 1329\u20131338, 2022.\n2, 3\n[11] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid,\nChao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu.\nMasked image training for generalizable deep image de-\nnoising. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1692\u2013\n1703, 2023. 2\n[12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,\nEnze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023. 3, 5\n[13] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xi-\naokang Yang, and Fisher Yu.\nDual aggregation trans-\nformer for image super-resolution. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 12312\u201312321, 2023. 2\n[14] Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu,\nLinghe Kong, and Xin Yuan. Hierarchical integration dif-\nfusion model for realistic image deblurring. Advances in\nNeural Information Processing Systems, 2023. 2\n[15] Jooyoung\nChoi,\nSungwon\nKim,\nYonghyun\nJeong,\nYoungjune Gwon, and Sungroh Yoon. Ilvr: Conditioning\nmethod for denoising diffusion probabilistic models. arXiv\npreprint arXiv:2108.02938, 2021. 3\n[16] DeepFloyd.\nDeepfloyd inference framework.\nhttps:\n//www.deepfloyd.ai/deepfloyd-if, 2023. Ac-\ncessed: 2023-11-14. 3\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248\u2013255. Ieee, 2009. 4\n[18] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 3\n[19] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. IEEE transactions on pattern analysis and machine\nintelligence, 38(2):295\u2013307, 2015. 2\n[20] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accel-\nerating the super-resolution convolutional neural network.\nIn Computer Vision\u2013ECCV 2016: 14th European Confer-\nence, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part II 14, pages 391\u2013407. Springer, 2016. 2\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 4\n[22] Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu,\nDing Liu, and Thomas S Huang. Neural sparse representa-\ntion for image restoration. Advances in Neural Information\nProcessing Systems, 33:15394\u201315404, 2020. 2\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. Advances\nin neural information processing systems, 27, 2014. 2\n[24] Jinjin Gu and Chao Dong.\nInterpreting super-resolution\nnetworks with local attribution maps.\nIn Proceedings of\n9\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9199\u20139208, 2021. 4\n[25] Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong.\nBlind super-resolution with iterative kernel correction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1604\u20131613, 2019. 2\n[26] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy\nRen, and Chao Dong.\nPipal: a large-scale image qual-\nity assessment dataset for perceptual image restoration. In\nComputer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XI\n16, pages 633\u2013651, 2020. 2, 7\n[27] Jinjin Gu, Yujun Shen, and Bolei Zhou.\nImage pro-\ncessing using multi-code gan prior.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 3012\u20133021, 2020. 2\n[28] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S Ren, Radu\nTimofte, Yuan Gong, Shanshan Lao, Shuwei Shi, Jiahao\nWang, Sidi Yang, et al. Ntire 2022 challenge on perceptual\nimage quality assessment. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 951\u2013967, 2022. 7\n[29] Jinjin Gu, Xianzheng Ma, Xiangtao Kong, Yu Qiao, and\nChao Dong. Networks are slacking off: Understanding gen-\neralization problem in image deraining. Advances in Neural\nInformation Processing Systems, 2023. 2, 4\n[30] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5, 13\n[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural informa-\ntion processing systems, 33:6840\u20136851, 2020. 3\n[32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3\n[33] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung\nTang. Network trimming: A data-driven neuron pruning ap-\nproach towards efficient deep architectures. arXiv preprint\narXiv:1607.03250, 2016. 3\n[34] Yan Huang, Shang Li, Liang Wang, Tieniu Tan, et al. Un-\nfolding the alternating optimization for blind super resolu-\ntion. Advances in Neural Information Processing Systems,\n33:5632\u20135643, 2020. 2\n[35] Zheng Hui, Jie Li, Xiumei Wang, and Xinbo Gao. Learn-\ning the non-differentiable optimization for blind super-\nresolution. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2093\u20132102,\n2021. 2\n[36] Younghyun Jo, Sejong Yang, and Seon Joo Kim. Srflow-da:\nSuper-resolution using normalizing flow with deep convo-\nlutional block. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n364\u2013372, 2021. 3\n[37] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park.\nScal-\ning up gans for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10124\u201310134, 2023. 3\n[38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 3, 4\n[39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-\nnen.\nProgressive growing of gans for improved quality,\nstability, and variation. arXiv preprint arXiv:1710.10196,\n2017. 2\n[40] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 4401\u20134410, 2019. 2,\n4\n[41] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. Advances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 5\n[42] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. Advances in\nNeural Information Processing Systems, 35:23593\u201323606,\n2022. 2, 3\n[43] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\nFeng Yang. Musiq: Multi-scale image quality transformer.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5148\u20135157, 2021. 6\n[44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment\nanything. arXiv preprint arXiv:2304.02643, 2023. 2, 3, 4,\n5\n[45] Xiangtao Kong, Xina Liu, Jinjin Gu, Yu Qiao, and Chao\nDong. Reflash dropout in image super-resolution. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6002\u20136012, 2022. 6\n[46] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration\nusing swin transformer. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 1833\u2013\n1844, 2021. 2\n[47] Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, and\nRadu Timofte. Flow-based kernel prior with application to\nblind super-resolution. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 10601\u201310610, 2021.\n[48] Jie Liang, Hui Zeng, and Lei Zhang.\nEfficient and\ndegradation-adaptive network for real-world image super-\nresolution. In European Conference on Computer Vision,\npages 574\u2013591. Springer, 2022. 2\n[49] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben\nFei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. Diff-\nbir: Towards blind image restoration with generative diffu-\nsion prior. arXiv preprint arXiv:2308.15070, 2023. 2, 3, 6,\n7, 13, 18, 19, 20\n[50] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong.\nBlind image super-resolution: A survey and beyond. IEEE\ntransactions on pattern analysis and machine intelligence,\n45(5):5461\u20135480, 2022. 2\n10\n[51] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744, 2023. 13\n[52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. In NeurIPS, 2023. 4, 13\n[53] Yihao Liu, Anran Liu, Jinjin Gu, Zhipeng Zhang, Wenhao\nWu, Yu Qiao, and Chao Dong.\nDiscovering distinctive\u201d\nsemantics\u201d in super-resolution networks.\narXiv preprint\narXiv:2108.00406, 2021. 2\n[54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[55] A Lugmayr, M Danelljan, L Van Gool, and R Timofte.\nLearning the super-resolution space with normalizing flow.\nECCV, Srflow, 2020. 3\n[56] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential\nequations. arXiv preprint arXiv:2108.01073, 2021. 14\n[57] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi,\nand Cynthia Rudin. Pulse: Self-supervised photo upsam-\npling via latent space exploration of generative models. In\nProceedings of the ieee/cvf conference on computer vision\nand pattern recognition, pages 2437\u20132445, 2020. 2\n[58] Tomer Michaeli and Michal Irani.\nNonparametric blind\nsuper-resolution. In Proceedings of the IEEE International\nConference on Computer Vision, pages 945\u2013952, 2013. 2\n[59] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,\nZhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter:\nLearning adapters to dig out more controllable abil-\nity for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023. 2, 3\n[60] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3883\u20133891,\n2017. 2\n[61] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. arXiv\npreprint arXiv:2112.10741, 2021. 3\n[62] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin,\nChen Change Loy, and Ping Luo. Exploiting deep genera-\ntive prior for versatile image restoration and manipulation.\nIEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 44(11):7474\u20137489, 2021. 3\n[63] Dustin Podell,\nZion English,\nKyle Lacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 3\n[64] Alec Radford, Luke Metz, and Soumith Chintala.\nUn-\nsupervised representation learning with deep convolu-\ntional generative adversarial networks.\narXiv preprint\narXiv:1511.06434, 2015. 2\n[65] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Interna-\ntional Conference on Machine Learning, pages 8821\u20138831.\nPMLR, 2021. 3\n[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional\nimage generation with clip latents.\narXiv preprint\narXiv:2204.06125, 1(2):3, 2022. 3\n[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 5\n[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\nmans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural In-\nformation Processing Systems, 35:36479\u201336494, 2022. 3\n[69] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278\u201325294, 2022. 4\n[70] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. Advances in\nneural information processing systems, 32, 2019. 3\n[71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole. Score-\nbased generative modeling through stochastic differential\nequations. arXiv preprint arXiv:2011.13456, 2020. 3\n[72] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-\naya Jia. Scale-recurrent network for deep image deblurring.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 8174\u20138182, 2018. 2\n[73] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities.\nhttps://\ngithub.com/InternLM/InternLM, 2023. 2, 3\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971, 2023. 2, 3\n[75] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning.\nAdvances in neural information\nprocessing systems, 30, 2017. 3\n[76] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-\nploring clip for assessing the look and feel of images. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 2555\u20132563, 2023. 6\n[77] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy. Exploiting diffusion prior\nfor real-world image super-resolution.\narXiv preprint\narXiv:2305.07015, 2023. 3, 6, 18, 19, 20\n[78] Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu\nXu, Jungang Yang, Wei An, and Yulan Guo.\nUnsuper-\nvised degradation representation learning for blind super-\nresolution. In Proceedings of the IEEE/CVF Conference on\n11\nComputer Vision and Pattern Recognition, pages 10581\u2013\n10590, 2021. 2\n[79] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.\nRecovering realistic texture in image super-resolution by\ndeep spatial feature transform. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npages 606\u2013615, 2018. 4\n[80] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To-\nwards real-world blind face restoration with generative fa-\ncial prior. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 9168\u20139178,\n2021. 3\n[81] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan:\nTraining real-world blind super-resolution\nwith pure synthetic data. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 1905\u2013\n1914, 2021. 2, 5, 6, 18, 19, 20\n[82] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot im-\nage restoration using denoising diffusion null-space model.\narXiv preprint arXiv:2212.00490, 2022. 2, 3\n[83] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qix-\niang Ye, Wangmeng Zuo, and Liang Lin.\nComponent\ndivide-and-conquer for real-world image super-resolution.\nIn Computer Vision\u2013ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nVIII 16, pages 101\u2013117. Springer, 2020. 7\n[84] Yuxin Wu and Kaiming He. Group normalization. In Pro-\nceedings of the European conference on computer vision\n(ECCV), pages 3\u201319, 2018. 4\n[85] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu,\nZhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-\nimage generation via large mixture of diffusion paths. arXiv\npreprint arXiv:2305.18295, 2023. 3\n[86] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao,\nYuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu\nYang. Maniqa: Multi-dimension attention network for no-\nreference image quality assessment. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1191\u20131200, 2022. 6\n[87] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan\nprior embedded network for blind face restoration in the\nwild.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 672\u2013681,\n2021. 3\n[88] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.\nPixel-aware stable diffusion for realistic image super-\nresolution and personalized stylization.\narXiv preprint\narXiv:2308.14469, 2023. 6, 18, 19, 20\n[89] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang,\nLinghe Kong, and Xin Yuan.\nAccurate image restora-\ntion with attention retractable transformer. In International\nConference on Learning Representations (ICLR), 2023. 2\n[90] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a gaussian denoiser: Residual learning\nof deep cnn for image denoising.\nIEEE transactions on\nimage processing, 26(7):3142\u20133155, 2017. 2\n[91] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.\nLearning deep cnn denoiser prior for image restoration. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3929\u20133938, 2017. 2\n[92] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: To-\nward a fast and flexible solution for cnn-based image de-\nnoising. IEEE Transactions on Image Processing, 27(9):\n4608\u20134622, 2018. 2\n[93] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-\nfte. Designing a practical degradation model for deep blind\nimage super-resolution. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 4791\u2013\n4800, 2021. 2, 6\n[94] Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Yu-\nlun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, and\nLuc Van Gool. Practical blind image denoising via swin-\nconv-unet and data synthesis.\nMachine Intelligence Re-\nsearch, pages 1\u201314, 2023. 2\n[95] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2, 3, 7, 8\n[96] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 6\n[97] Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Yu-\nlun Zhang, and Wenming Yang. Crafting training degra-\ndation distribution for the accuracy-generalization trade-off\nin real-world super-resolution. International Conference on\nMachine Learning (ICML), 2023. 2, 6\n[98] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and\nYun Fu. Residual non-local attention networks for image\nrestoration. In International Conference on Learning Rep-\nresentations (ICLR), 2019. 2\n[99] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu.\nResidual dense network for image restoration.\nIEEE transactions on pattern analysis and machine intel-\nligence, 43(7):2480\u20132495, 2020. 2\n[100] Yi Zhang, Xiaoyu Shi, Dasong Li, Xiaogang Wang, Jian\nWang, and Hongsheng Li.\nA unified conditional frame-\nwork for diffusion-based image restoration. arXiv preprint\narXiv:2305.20049, 2023. 3\n[101] Yang Zhao, Yu-Chuan Su, Chun-Te Chu, Yandong Li, Mar-\nius Renn, Yukun Zhu, Changyou Chen, and Xuhui Jia.\nRethinking deep face restoration.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7652\u20137661, 2022.\n[102] Shangchen\nZhou,\nKelvin\nChan,\nChongyi\nLi,\nand\nChen Change Loy. Towards robust blind face restoration\nwith codebook lookup transformer. Advances in Neural In-\nformation Processing Systems, 35:30599\u201330611, 2022. 3\n[103] Jiapeng Zhu, Deli Zhao, Bo Zhang, and Bolei Zhou. Disen-\ntangled inference for gans with latently invertible autoen-\ncoder. International Journal of Computer Vision, 130(5):\n1259\u20131276, 2022. 3\n12\nLow-Quality Input\nw/o DR Encoder\nw/ DR Encoder\nFigure 14. The effectiveness of the degradation-robust encoder\n(DR Encoder) is demonstrated by the results, which are achieved\nby initially encoding with various encoders and subsequently de-\ncoding. This process effectively reduces the degradations in low-\nquality inputs before they are introduced into the diffusion models.\nAppendix\nA. Discussions\nA.1. Degradation-Robust Encoder\nAs shown in Fig. 2 of the main text, a degradation-robust\nencoder is trained and deployed prior to feeding the low-\nquality input into the adaptor. We conduct experiments us-\ning synthetic data to demonstrate the effectiveness of the\nproposed degradation-robust encoder. In Fig. 14, we show\nthe results of using the same decoder to decode the latent\nrepresentations from different encoders. It can be seen that\nthe original encoder has no ability to resist degradation and\nits decoded images still contain noise and blur. The pro-\nposed degradation-robust encoder can reduce the impact of\ndegradation, which further prevents generative models from\nmisunderstanding artifacts as image content [49].\nA.2. LLaVA Annotation\nOur diffusion model is capable of accepting textual prompts\nduring the restoration process.\nThe prompt strategy we\nemploy consists of two components: one component is\nLow-Quality Input\nw/ Negative Prompt\nw/o Negative Prompt\nFigure 15. Negative prompt causes artifacts when low-quality in-\nputs do not have clear semantics.\nautomatically annotated by LLaVA-v1.5-13B [51], and the\nother is a standardized default positive quality prompt. The\nfixed portion of the prompt strategy provides a positive\ndescription of quality, including words like \u201ccinematic,\nHigh Contrast, highly detailed, unreal\nengine, taken using a Canon EOS R\ncamera, hyper detailed photo-realistic\nmaximum detail, 32k, Color Grading,\nultra HD, extreme meticulous detailing,\nskin pore detailing, hyper sharpness,\nperfect without deformations, Unreal\nEngine 5, 4k render\u201d. For the LLaVA component,\nwe use the command \u201cDescribe this image and\nits style in a very detailed manner\u201d\nto\ngenerate detailed image captions, as exemplified in Fig. 18.\nWhile occasional inaccuracies may arise, LLaVA-v1.5-13B\ngenerally captures the essence of the low-quality input\nwith notable precision. Using the reconstructed version of\nthe input proves effective in correcting these inaccuracies,\nallowing LLaVA to provide an accurate description of the\nmajority of the image\u2019s content. Additionally, SUPIR is\neffective in mitigating the impact of potential hallucination\nprompts, as detailed in [52].\nA.3. Limitations of Negative Prompt\nFigure 23 presents evidence that the use of negative qual-\nity prompts [30] substantially improves the image quality\nof restored images. However, as observed in Fig. 15, the\nnegative prompt may introduce artifacts when the restora-\ntion target lacks clear semantic definition. This issue likely\nstems from a misalignment between low-quality inputs and\nlanguage concepts.\nA.4. Negative Samples Generation\nWhile negative prompts are highly effective in enhancing\nquality, the lack of negative-quality samples and prompts in\n13\nthe training data results in the fine-tuned SUPIR\u2019s inabil-\nity to comprehend these prompts effectively. To address\nthis problem, in Sec. 3.2 of the main text, we introduce a\nmethod to distill negative concepts from the SDXL model.\nThe process for generating negative samples is illustrated in\nFig. 19. Direct sampling of negative samples through a text-\nto-image approach often results in meaningless images. To\naddress this issue, we also utilize training samples from our\ndataset as source images. We create negative samples in an\nimage-to-image manner as proposed in [56], with a strength\nsetting of 0.5.\nB. More Visual Results\nWe provide more results in this section. Fig. 16 presents\nadditional cases where full-reference metrics do not align\nwith human evaluation.\nIn Fig. 17, we show that using\nnegative-quality prompt without including negative samples\nin training may cause artifacts. In Figs. 20 to 22, we pro-\nvide more visual caparisons with other methods. Plenty of\nexamples prove the strong restoration ability of SUPIR and\nthe most realistic of restored images. More examples of\ncontrollable image restoration with textual prompts can be\nfound in Fig. 23.\n14\nLow-Quality Input\nBSRGAN\nSUPIR (Ours)\nPSNR:\n23.90\nSSIM:\n0.6931\nLPIPS:\n0.2299\nPSNR:\n19.76\nSSIM:\n0.6245\nLPIPS:\n0.3011\nPreferred by metrics  \nPreferred by human \nLow-Quality Input\nRealESRGAN\nSUPIR (Ours)\nPSNR:\n19.76\nSSIM:\n0.3207\nLPIPS:\n0.3454\nPSNR:\n18.91\nSSIM:\n0.2829\nLPIPS:\n0.4013\nPreferred by metrics  \nPreferred by human \nLow-Quality Input\nStableSR\nSUPIR (Ours)\nPSNR:\n20.48\nSSIM:\n0.5570\nLPIPS:\n0.2783\nPSNR:\n18.80\nSSIM:\n0.5165\nLPIPS:\n0.3039\nLow-Quality Input\nDiffBIR\nSUPIR (Ours)\nPSNR:\n26.01\nSSIM:\n0.5843\nLPIPS:\n0.4918\nPSNR:\n22.98\nSSIM:\n0.5124\nLPIPS:\n0.5338\nLow-Quality Input\nPASD\nSUPIR (Ours)\nPSNR:\n21.82\nSSIM:\n0.4962\nLPIPS:\n0.3863\nPSNR:\n20.81\nSSIM:\n0.4702\nLPIPS:\n0.3959\nLow-Quality Input\nDiffBIR\nSUPIR (Ours)\nPSNR:\n20.95\nSSIM:\n0.5156\nLPIPS:\n0.3255\nPSNR:\n19.49\nSSIM:\n0.4872\nLPIPS:\n0.3277\nFigure 16. Additional samples highlight the misalignment between metric evaluations and human assessments. While SUPIR produces\nimages with high-fidelity textures, it tends to receive lower scores in metric evaluations.\nLow-Quality Input\nw/o Negative samples\nw/ Negative samples\nLow-Quality Input\nw/o Negative samples\nw/ Negative samples\nFigure 17. More visual results for Fig. 4 of the main text. CFG introduces artifacts if we do not include negative-quality samples in\ntraining. Adding negative-quality samples allows further quality enhancement through CFG.\n15\nThe image features a small, brown, furry \nanimal, possibly a hamster or a cat, laying on a \nwhite surface. The animal appears to be \nrelaxed and comfortable, as it is resting on a \nbed or a cushion. The white background \nprovides a contrast to the brown fur of the \nanimal, making it the focal point of the image.\nLow-Quality Input\nThe image features a fox lying down in the \nsnow, appearing to be sleeping or resting. The \nfox is positioned in the center of the scene, \nwith its body occupying a significant portion \nof the image. The snowy background provides \na serene and natural setting for the fox, \nemphasizing its natural habitat.\nReconstructed Input\nThe image features a fox curled up in the snow, \nresting comfortably. The fox is positioned in \nthe center of the scene, with its body facing \nthe left side of the image. The snowy\nenvironment provides a natural and serene \nbackdrop for the fox, which appears to be \nenjoying its time in the snow.\nGround-Truth\nThe image features a group of three men \ndressed in military uniforms, sitting on the \nground and holding guns. They are positioned \nin a line, with one man on the left, another in \nthe middle, and the third on the right. Each of \nthem is holding a rifle, with one rifle located \non the left side, another in the middle, and \nthe third on the right side of the group. The \nmen appear to be soldiers...\nLow-Quality Input\nThe image features a group of four men \ndressed in military uniforms, standing in a \ngrassy area. They are all holding guns, with \nsome of them also wearing backpacks. The \nmen appear to be soldiers, possibly on a \nmission or training exercise. The scene \ncaptures their readiness and focus as they \nprepare for their task.\nReconstructed Input\nThe image features a group of four men \ndressed in military uniforms, standing in a \nfield and holding guns. They appear to be \nsoldiers, possibly engaged in a training \nexercise or a mission. The men are positioned \nin a line, with one soldier on the left, another \nin the middle, and two more on the right side \nof the image. Each soldier is holding a gun\u2026\nGround-Truth\nThe image features a beautiful garden with a \nvariety of colorful flowers. The flowers are \narranged in a visually appealing manner, \ncreating a vibrant and lively atmosphere. The \ngarden is filled with different types of flowers, \neach with unique colors and shapes, \ncontributing to the overall beauty of the \nscene\u2026\nLow-Quality Input\nThe image features a beautiful flower garden \nwith a variety of colorful flowers. The main \nfocus is on a large, bright orange flower with \na green center, surrounded by other vibrant \nflowers. The orange flower is situated in the \nmiddle of the garden, drawing attention to its \nstriking color and unique shape. The garden is \nfilled with a diverse array of flowers, creating \na visually stunning and lively atmosphere.\nReconstructed Input\nThe image features a vibrant garden with a \nlarge, colorful flower in the center. The \nflower is surrounded by a variety of other \nflowers, creating a beautiful and lively scene. \nThe main flower is orange and yellow, with a \ngreen stem, and it stands out among the \nother flowers in the garden. The garden is \nfilled with a diverse assortment of flowers\u2026\nGround-Truth\nThe image features a person standing in a field \nof tall grass, surrounded by tall corn plants. \nThe person appears to be wearing a backpack, \nand their head is partially hidden by the tall \ngrass. The scene gives off a sense of adventure \nand exploration, as the person seems to be \nnavigating through the field. The tall grass and \ncorn plants create a sense of depth and \nnatural...\nLow-Quality Input\nThe image features a person standing in a field \nof tall corn, with the corn surrounding them \non all sides. The person is wearing a hat and \nappears to be looking down, possibly \nobserving the corn or the ground. The field is \nvast, with the corn reaching up to the person's \nshoulders, creating a sense of being engulfed \nby the tall plants. The scene captures the \nessence of being in a cornfield\u2026\nReconstructed Input\nThe image features a man wearing a hat and a \nplaid shirt, standing in a field of tall corn. He \nappears to be looking over the corn, possibly \nobserving the surroundings or searching for \nsomething. The corn is quite tall, reaching up \nto the man's shoulders, and the field extends \nin the background, creating a sense of depth \nand vastness. The man's presence in the field, \nalong with his attire\u2026\nGround-Truth\nCase 1\nCase 3\nCase 4\nCase 2\nFigure 18. Snapshots showcasing LLaVA annotations demonstrate that LLaVA accurately predicts most content even with low-quality\ninputs. Please zoom in for a more detailed view.\n16\n(a) Noise to Image\n(b) Image to Image \nPrompt = {oil painting, cartoon, blurring, dirty, messy, low quality, frames, deformed, lowres, over-smooth}\nPrompt\nPrompt\nPrompt\nPrompt\nPrompt = {oil painting, cartoon, blurring, dirty, messy, low quality, frames, deformed, lowres, over-smooth}\nPrompt\nPrompt\nFigure 19. Pipeline of negative sample generation. (a) Sampling in a noise-to-image approach leads to meaningless outputs. (b) We\nsynthetic negative samples from high quality images. Zoom in for better view.\n17\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nFigure 20. Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding\nobject under challenging degradation. Zoom in for better view.\n18\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nFigure 21. Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding\nobject under challenging degradation. Zoom in for better view.\n19\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nSUPIR (ours)\nLow-Quality Input\nReal-ESRGAN+[81]\nStableSR[77]\nDiffBIR[49]\nPASD[88]\nSUPIR (ours)\nFigure 22. Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding\nobject under challenging degradation. Zoom in for better view.\n20\nLow-Quality Input\nNo Prompt\nText: \u2026 several birds flying. \nLow-Quality Input\nNo Prompt\nText: \u2026 yellow lifeguard chair \u2026\nLow-Quality Input\nNo Prompt\nText: A turtle can be seen ...\nLow-Quality Input\nNo Prompt\nText: \u2026 lots of falling leaves.\nLow-Quality Input\nNo Prompt\nText: \u2026 having a mustache \u2026\nLow-Quality Input\n(a) Examples showing the effect of positive prompts\n(b) Examples showing the effect of negative prompts\nText: \u2026 two birds perched \u2026\nText: Two people \u2026 carrying \nsurfboards \u2026 \nLow-Quality Input\nNo Negative Prompt\nUse Negative Prompt\nLow-Quality Input\nNo Negative Prompt\nUse Negative Prompt\nLow-Quality Input\nNo Negative Prompt\nUse Negative Prompt\nLow-Quality Input\nNo Negative Prompt\nUse Negative Prompt\nFigure 23. More visual results of the text prompts\u2019 influences. (a) and (b) show the examples of positive prompts and negative prompts,\nrespectively. Zoom in for better view.\n21\n"
  },
  {
    "title": "MambaByte: Token-free Selective State Space Model",
    "link": "https://arxiv.org/pdf/2401.13660.pdf",
    "upvote": "46",
    "text": "MambaByte: Token-free Selective State Space Model\nJunxiong Wang\nTushaar Gangavarapu\nJing Nathan Yan\nAlexander M Rush\nCornell University\n{jw2544,tg352,jy858,arush}@cornell.edu\nAbstract\nToken-free language models learn directly from raw bytes and remove the bias of\nsubword tokenization. Operating on bytes, however, results in significantly longer\nsequences, and standard autoregressive Transformers scale poorly in such settings.\nWe experiment with MambaByte, a token-free adaptation of the Mamba state space\nmodel, trained autoregressively on byte sequences. Our experiments indicate the\ncomputational efficiency of MambaByte compared to other byte-level models. We\nalso find MambaByte to be competitive with and even outperform state-of-the-art\nsubword Transformers. Furthermore, owing to linear scaling in length, MambaByte\nbenefits from fast inference compared to Transformers. Our findings establish the\nviability of MambaByte in enabling token-free language modeling.\n0\n10K\n20K\n30K\n40K\nTraining step\n0.90\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\nBits per byte\n0\n1\n2\n3\n4\n5\n6\nTraining exa FLOPs\nMegaByte-193M+177M (patch: 4)\nMegaByte-193M+177M (patch: 8)\nGated-S4D-368M\nMambaByte-353M\nTransformer-361M\nFigure 1: Benchmarking byte-level models with a fixed parameter budget. Language modeling\nresults on PG19 (8, 192 consecutive bytes), comparing the standard Transformer [Vaswani et al.,\n2017, Su et al., 2021], MegaByte Transformer [Yu et al., 2023], gated diagonalized S4 [Mehta et al.,\n2023], and MambaByte. (Left) Model loss over training step. (Right) FLOP-normalized training cost.\nMambaByte reaches Transformer loss in less than one-third of the compute budget.\n1\nIntroduction\nWhen defining a language model, a base tokenization is typically used\u2014either words [Bengio et al.,\n2000], subwords [Schuster and Nakajima, 2012, Sennrich et al., 2015, Wu et al., 2016, Wang et al.,\nCopyright 2024 by the author(s).\narXiv:2401.13660v1  [cs.CL]  24 Jan 2024\n2020], or characters [Gao et al., 2020a]. Of these, subword tokenization has been the most popular\nchoice, as it achieves a natural compromise between training efficiency and the ability to handle\nout-of-vocabulary words. However, several works (e.g., Xue et al. [2022]) have noted issues with\nsubword tokenizers, such as a lack of robustness to typos, spelling and capitalization variations, and\nmorphological changes.\nResearchers [Clark et al., 2022, Xue et al., 2022, Yu et al., 2023] have employed an alternative\napproach of using byte sequences, i.e., an end-to-end mapping from raw data to predictions without\nany intermediate tokenization. Compared to subword models, byte-level language models can\ngeneralize more easily across orthographic and morphological variants. Of course, modeling text\nas bytes means that the resultant sequences are significantly longer than their subword counterparts.\nThis pushes the efficiency issues upstream into the architecture itself.\nEfficiency issues are particularly pronounced for autoregressive Transformers [Vaswani et al., 2017],\nwhich dominate language modeling [Brown et al., 2020, Touvron et al., 2023]. Due to the quadratic\ncost of attention, Transformers scale poorly for long (byte) sequences [Brown et al., 2020, Zhang\net al., 2022]. Researchers have compressed the internal Transformer representation to work with long\nsequences, for instance, developing length-aware modeling approaches [Dai et al., 2020, Nawrot et al.,\n2022], where groups of tokens are merged within the intermediate layers. Recently, Yu et al. [2023]\nproposed the MegaByte Transformer, which uses compression in the form of fixed-size patches of\nbytes as a subword analog. As a result, MegaByte enables lower computational costs.1\nIn this work, we introduce MambaByte, an efficient and simple byte-level language model. The\nmodel is a straightforward adaptation of the recently introduced Mamba architecture [Gu and Dao,\n2023], a linear-time approach for sequence modeling. Mamba builds off the approach pioneered\nby state space models (SSMs) [Gu et al., 2021, Gupta et al., 2022, Gu et al., 2022, Smith et al.,\n2023] by introducing a selection mechanism that is more effective for discrete data such as text and\nproviding an efficient GPU implementation. Our simple observation is that using Mamba (without\nmodifications) relieves the main computational bottleneck in language modeling, thus allowing for\nthe elimination of patching and effective use of the available compute budget.\nExperiments compare MambaByte to Transformers, SSMs, and MegaByte (patching) architectures in\na fixed parameter and fixed compute setting on several long-form text datasets. Figure 1 summarizes\nour main findings. Compared to byte-level Transformers, MambaByte achieves better performance\nfaster and is significantly more compute efficient. We also consider the viability of token-free language\nmodels compared to the existing state-of-the-art subword models. In this regard, we find MambaByte\nto be competitive with various subword baselines despite handling significantly longer sequences.\nOur results establish MambaByte as a strong alternative to the existing tokenizer-dependent models\nand advocate its use to facilitate end-to-end learning.\n2\nBackground: Selective state space sequence models\nSSMs model the evolution of a hidden state across time through a first-order differential equation.\nLinear time-invariant SSMs [Gu et al., 2021, Gupta et al., 2022, Gu et al., 2022, Smith et al., 2023]\nhave shown promising results in deep learning across several modalities. However, Gu and Dao\n[2023] have recently argued that the constant dynamics of these approaches lack input-dependent\ncontext selection in the hidden state, which may be necessary for tasks such as language modeling.\nTo this end, they proposed Mamba, which defines the time-varying continuous state dynamics for a\ngiven input x(t) \u2208 R, hidden state h(t) \u2208 Rn, and output y(t) \u2208 R at time t as:\ndh(t)\ndt\n= Ah(t) + B(t)x(t);\ny(t) = C(t)h(t),\n(1)\nwhich is parameterized by a diagonal time-invariant system matrix A \u2208 Rn\u00d7n and time-dependent\ninput and output matrices B(t) \u2208 Rn\u00d71 and C(t) \u2208 R1\u00d7n.\nTo model discrete-time sequences such as bytes, the continuous time dynamics in (1) must be\napproximated through discretization. This results in a discrete-time hidden state recurrence with new\n1Although our experiments (see Figure 1) indicate that patching can also lower the model performance\ncompared to the standard Transformer.\n2\n(a)\n(c)\n(d)\n(continuous-time)\n(Parallel scan)\n(discrete-time)\n(b)\nFigure 2: Illustration of the Mamba SSM. (a) The discrete-time input x[k], along with input-\nselective \u2206[k]. (b) The continuous-time signal x(t). (c) Mathematically, the SSM transforms the\ncontinuous-time x(t) through an n-dimensional hidden state (here, n = 4) using parameters A and\nB(t), which is then mapped to the output y(t) using C(t). (d) Practically, we compute y[k] using a\ndiscrete-time parallel scan at the steps defined by \u2206[k] and discrete-time matrices A[k], B[k], and\nC[k]. At inference, we run the recurrence directly.\nmatrices at each timestep, A, B, and C, such that\nh[k] = A[k]h[k \u2212 1] + B[k]x[k];\ny[k] = C[k]h[k].\n(2)\nSSM\nLinear\nConv\nLinear\nLinear\nLinear\nLinear\nLinear\nLinear\nS4D\nFigure 3:\nMamba block.\n\u03c3\nindicates Swish activation [Ra-\nmachandran et al., 2017].\nObserve that (2) resembles a linear version of a recurrent neural\nnetwork and can be applied in this recurrent form during language\nmodel generation. The discretization requires a timestep, \u2206[k],\nfor each input position, corresponding to treating x[k] = x (tk)\nfor tk = Pk\nj=1 \u2206[j]. The discrete-time matrices A, B, and C can\nthen be computed from \u2206[k]. Figure 2 illustrates how Mamba\nmodels discrete sequences.\nIn Mamba, the SSM terms are input-selective, i.e., B, C, and \u2206\nare defined as functions of the input x[k] \u2208 Rd:\n\u2206[k] = softplus(W\u2206(WRx[k]);\nB(tk) = WBx[k],\n(3)\nwhere WB \u2208 Rn\u00d7d (C is similarly defined), W\u2206 \u2208 Rd\u00d7r and\nWR \u2208 Rr\u00d7d (for some r \u226a d) are learnable weights, and softplus\nensures positivity. Note that the SSM parameters A, B, and C\nare identical for each input dimension d, but the timesteps \u2206 are\ndistinct; this results in a hidden state size of n \u00d7 d per timestep k.\n(See Appendix D for specifics on discretization and selectivity.)\nMamba embeds this SSM layer into a full neural network lan-\nguage model. Specifically, the model utilizes a stack of gated\nlayers inspired by the previous gated SSM [Mehta et al., 2023]. Figure 3 shows the Mamba architec-\nture combining the SSM layer with a gated neural network.\n3\nParallel scans for linear recurrences.\nAt training time, we have access to the entire sequence x,\nallowing us to compute the linear recurrence more efficiently. Smith et al. [2023] demonstrated the use\nof work-efficient parallel scans [Blelloch, 1990] for efficiently computing the sequential recurrence\nin linear SSMs. For Mamba, we first map the recurrence to a sequence of L tuples, with ek =\n(Ak, bk) := (A[k], B[k]x[k]), then define an associative operator \u2022 such that ej \u2022ek = (AkAj, Akbj +\nbk). Finally, we apply a parallel scan to compute the sequence [(A[1], h[1]), (A[2]A[1], h[2]), . . .]. In\ngeneral, this requires O(T\u2022 log2(L)) time, using L/2 processors, where T\u2022 is the cost of a matrix-\nmatrix multiplication. Noting A to be a diagonal matrix, the linear recurrence can be computed\nparallelly in O(n log2(L)) time and O(nL) space. A parallel scan with a diagonal matrix is also\nefficient in operation, requiring O(nL) FLOPs.\n3\nExperimental setup\nTable 1: Relative training FLOPs by model\nsize. All MegaByte models use a patch size of 8.\nExperiment Models\nFLOPs per\ntrain byte\nMedium-\nscale\nMegaByte-758M+262M :\n1.02 : 1\nMambaByte-353M\nLarge-\nscale\nMegaByte-1.3B+350M :\n0.54 : 1\nMambaByte-972M\nMegaByte-1.3B+218M :\n0.40 : 1\nMambaByte-972M\nOur experiments compare MambaByte to other\nbyte-level Transformers and SSMs. All our mod-\nels employ the same training recipes (see Ap-\npendix C for details). We utilize a set of diverse\nlong-form text datasets: PG19 [Rae et al., 2020],\nStories [Trinh and Le, 2018], Books [Gao et al.,\n2020b], ArXiv [Gao et al., 2020b], and Code [Gao\net al., 2020b]. Dataset sizes and average document\nlengths are included in Appendix A.\nPerformance comparison across architectures re-\nquires care. To this end, we consider two settings:\ncompute-matched and parameter-matched. This\nsetup is necessary as the default MegaByte Trans-\nformer employs a global module that works with 8\u00d7-patched representations of the input, thus using\n8\u00d7 fewer feed-forward FLOPs per byte than a raw Transformer, while having significantly more\nparameters. Table 1 shows the MegaByte and MambaByte model sizes employed in our experi-\nments. The (forward pass) FLOPs computation for various model architectures and the associated\nhyperparameters employed are detailed in Appendix B.\nAll MambaByte models were trained using the open-source Mamba code base.2 At training, we\nshuffle the documents and use contiguous sequences of 8, 192 bytes (one per document), starting\nfrom a random position. We enable mixed precision training using BF16 for training efficiency at\nscale. The optimizer, learning rate scheduler, and other training details are specified in Appendix C.\nPress et al. [2021] proposed using a sliding window to trade off speed for performance during\ninference. Following this, we employ a sliding window (with a stride of Lctx/2 for a byte sequence\nof length Lctx) when comparing with the state-of-the-art subword models in Table 3.\n4\nResults\nTable 2 shows the bits per byte (BPB) across each dataset. For this experiment, the MegaByte-\n758M+262M and MambaByte models use the same number of FLOPs per byte (see Table 1). We\nobserve MambaByte to outperform MegaByte consistently across all datasets. Furthermore, we note\nthat we could not train MambaByte for the full 80B bytes due to monetary constraints, but MambaByte\noutperforms MegaByte with 0.63\u00d7 less compute and training data. Additionally, MambaByte-353M\nalso outperforms byte-level Transformer and PerceiverAR.\nHow is MambaByte performing better than a much larger model in so few training steps? Figure 1\nfurther explores this relationship by looking at models with the same number of parameters. The\ngraphs indicate that for MegaByte models of the same parameter size, models with less input\npatching perform better, but when compute-normalized, they perform similarly. In fact, a full-length\nTransformer, while slow in an absolute sense, also performs similarly to MegaByte when compute-\nnormalized. In contrast, switching to the Mamba architecture significantly improves both the compute\nusage and the model performance.\n2https://github.com/state-spaces/mamba.\n4\nTable 2: Medium-scale experiments. MegaByte and MambaByte use the same FLOPs per byte.\n(The BPB for Transformer, PerceiverAR, and MegaByte are taken from Yu et al. [2023].)\nByte-level model\nContext\nBytes\ntrained\nTest BPB \u2193\nPG19\nStories\nBooks\nArXiv\nCode\nTransformer-320M\n1, 024\n80B\n1.057\n1.064\n1.097\n0.816\n0.575\nPerceiverAR-248M\n8, 192\n80B\n1.104\n1.070\n1.104\n0.791\n0.546\nMegaByte-758M+262M (patch: 8)\n8, 192\n80B\n1.000\n0.978\n1.007\n0.678\n0.411\nMambaByte-353M\n8, 192\n30B\u2217\n0.930\n0.908\n0.966\n0.663\n0.396\nTable 3: Large-scale experiment on PG19. The observed BPB scores are converted to word-level\nPPL for comparison with past works. All the byte-level models are compute-matched. MambaByte-\n972M significantly outperforms other byte-level models and is competitive with state-of-the-art\nsubword models. (Accompanying citation indicates the work from which the corresponding result\nwas taken; fields marked \u2212 are unknown.)\n(#Layers) Model\nVocab Effective context\n(in bytes)3\nEffective\nbytes\ntrained3\nVal\nPPL\u2193 Test\nPPL\u2193\nSubword\n(36) Transformer-XL [Rae et al., 2020]\n32K\n2, 048/4, 096\n400B\n45.5\n36.3\n(36) Compressive [Rae et al., 2020]\n32K\n2, 048/2\u00d72, 048\n400B\n43.4\n33.6\n(22) Routing-490M4 [Roy et al., 2021]\n82K\n32, 768\n330B\n\u2212\n33.2\n(60) PerceiverAR-974.6M [Hawthorne et al., 2022] 32K\n8, 192\n1.68T\n45.9\n28.9\n(24) Block-Recurrent-1.3B [Hutchins et al., 2022]\n32K\n4, 096/recurrence\n\u2212\n\u2212\n26.5\nByte\n(\u2212) Transformer-320M [Yu et al., 2023]\n256\n8, 192\n400B\n81.6\n69.4\n(\u2212) PerceiverAR-248M [Yu et al., 2023]\n256\n8, 192\n400B\n119.1\n88.8\n(24+24) MegaByte-1.3B+350M [Yu et al., 2023]\n256\n8, 192/patch: 8\n400B\n42.8\n36.4\n(48) MambaByte-972M\n256\n8, 1925\n150B\u2217\n39.5\n33.0\nTable 4: Generation speed benchmarking. Speed to generate 8, 192 bytes; fields marked \u2212 are\nunknown. (Upper) The BPB on PG19 and generation time for the Transformer and MegaByte are\ntaken from Yu et al. [2023]. (Lower) MegaByte and MambaByte run on the same hardware.\nModel\nBytes\ntrained\nContext\nTest\nBPB \u2193\nGeneration\ntime (s)\n\u2193\nTransformer-350M\n\u2212\n1, 024\n1.064\n132\nMegaByte-1.3B+218M (patch: 8)\n\u2212\n8, 192\n0.991\n93\nMegaByte-1.3B+218M (patch: 8)6\n\u2212\n8, 192\n\u2212\n265\nMambaByte-972M\n75B\u2217\n8, 192\n0.883\n29\nw/ sliding window (2\u00d7 bytes)\n0.863\n58\nMambaByte-1.6B\n\u2212\n8, 192\n\u2212\n36\nFollowing these findings, Table 3 compares a larger version of these models on the PG19 dataset. For\nthis experiment, we compare MambaByte-972M with MegaByte-1.3B+350M and other byte-level\nmodels, as well as several state-of-the-art subword models. (The conversion from BPB to perplexity\n(PPL) is detailed in Appendix E). We find that MambaByte-972M, even just trained for 150B bytes,\noutperforms all the byte-level models and achieves competitive performance with subword models.\n3For subword models, we use one subword as being equivalent to four bytes.\n4The number of parameters is noted from Hutchins et al. [2022].\n5For inference, we use a context of 32, 768 bytes.\n6Open-source implementation: https://github.com/lucidrains/MEGABYTE-pytorch.\n5\nText generation.\nAutoregressive inference in Transformer models requires caching the entire\ncontext, which can significantly affect the generation speed. MambaByte does not suffer from this\nbottleneck as it maintains a single hidden state per layer that evolves with time, enabling constant\ntime per generation step. Table 4 compares the text generation speeds of MambaByte-972M and\nMambaByte-1.6B with MegaByte-1.3B+350M on an A100 80GB PCIe GPU. While MegaByte\nsignificantly reduces the generation cost through patching, we observe MambaByte to be 2.6\u00d7 faster\nin a parameter-matched setting due to its use of recurrent generation. Appendix F includes more\ninformation about the generation process.\n5\nConclusion\nWe introduce MambaByte, a token-free SSM for modeling long byte-sequences. MambaByte\noutperforms other byte-level models over several datasets and shows competitive results with subword\nTransformers, thus serving as a promising tokenization alternative. SSMs also enable significantly\nfast text generation due to their recurrent nature, making byte models practical. Our findings establish\nthe possibility of token-free language modeling in future large models.\n6\nReferences\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need. Advances in neural information processing\nsystems, 30, 2017.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. arXiv e-prints, pages arXiv\u20132104, 2021.\nLili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.\nMegaByte: Predicting Million-byte Sequences with Multiscale Transformers. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net\n/forum?id=JTmO2V9Xpz.\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long Range Language Model-\ning via Gated State Spaces. In The Eleventh International Conference on Learning Representations,\n2023. URL https://openreview.net/forum?id=5MkYIYCbva.\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A Neural Probabilistic Language Model.\nAdvances in neural information processing systems, 13, 2000.\nMike Schuster and Kaisuke Nakajima. Japanese and Korean Voice Search. In 2012 IEEE international\nconference on acoustics, speech and signal processing (ICASSP), pages 5149\u20135152. IEEE, 2012.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words\nwith Subword Units. arXiv preprint arXiv:1508.07909, 2015.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s Neural Machine Trans-\nlation System: Bridging the Gap between Human and Machine Translation. arXiv preprint\narXiv:1609.08144, 2016.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural Machine Translation with Byte-Level\nSubwords. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages\n9154\u20139160, 2020.\nYingqiang Gao, Nikola I Nikolov, Yuhuang Hu, and Richard HR Hahnloser. Character-Level\nTranslation with Self-attention. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1591\u20131604, 2020a.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam\nRoberts, and Colin Raffel. ByT5: Towards a token-free future with pre-trained byte-to-byte models.\nTransactions of the Association for Computational Linguistics, 10:291\u2013306, 2022.\nJonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an Efficient\nTokenization-Free Encoder for Language Representation. Transactions of the Association for\nComputational Linguistics, 10:73\u201391, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are\nFew-Shot Learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models,\n2023.\n7\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language\nModels. arXiv preprint arXiv:2205.01068, 2022.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-Transformer: Filtering out Sequential\nRedundancy for Efficient Language Processing. Advances in neural information processing\nsystems, 33:4271\u20134282, 2020.\nPiotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, \u0141ukasz Kaiser, Yuhuai Wu, Christian Szegedy,\nand Henryk Michalewski. Hierarchical Transformers Are More Efficient Language Models. In\nFindings of the Association for Computational Linguistics: NAACL 2022, pages 1559\u20131571, 2022.\nAlbert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv\npreprint arXiv:2312.00752, 2023.\nAlbert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured\nState Spaces. arXiv preprint arXiv:2111.00396, 2021.\nAnkit Gupta, Albert Gu, and Jonathan Berant. Diagonal State Spaces are as Effective as Structured\nState Spaces. Advances in Neural Information Processing Systems, 35:22982\u201322994, 2022.\nAlbert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization\nof Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971\u2013\n35983, 2022.\nJimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for\nSequence Modeling. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=Ai8Hw3AXqks.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint\narXiv:1710.05941, 2017.\nGuy E Blelloch. Prefix Sums and Their Applications. (CMU-CS-90-190), nov 1990. URL https:\n//www.cs.cmu.edu/~guyb/papers/Ble93.pdf.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive Transformers for Long-Range Sequence Modelling. In International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH.\nTrieu H. Trinh and Quoc V. Le. A Simple Method for Commonsense Reasoning. arXiv preprint\narXiv:1806.02847, 2018.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB\nDataset of Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027, 2020b.\nOfir Press, Noah A. Smith, and Mike Lewis. Shortformer: Better Language Modeling using Shorter\nInputs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 5493\u20135505, Online, August 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.427. URL https://aclanthology.org/2021.acl-long.427.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient Content-Based\nSparse Attention with Routing Transformers. Transactions of the Association for Computational\nLinguistics, 9:53\u201368, 2021. doi: 10.1162/tacl_a_00353. URL https://aclanthology.org/2\n021.tacl-1.4.\nCurtis Hawthorne, Andrew Jaegle, C\u02d8at\u02d8alina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz\nMalinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan,\nNeil Zeghidour, Jean-Baptiste Alayrac, Joao Carreira, and Jesse Engel. General-purpose, long-\ncontext autoregressive modeling with Perceiver AR. In Proceedings of the 39th International\nConference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,\npages 8535\u20138558. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/h\nawthorne22a.html.\n8\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-\nRecurrent Transformers. Advances in Neural Information Processing Systems, 35:33248\u201333261,\n2022.\nDan Hendrycks and Kevin Gimpel.\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415, 2016.\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu,\nand Soham De. Resurrecting Recurrent Neural Networks for Long Sequences. arXiv preprint\narXiv:2303.06349, 2023.\nAlbert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HiPPO:\nState Space Models with Generalized Orthogonal Basis Projections. In International Conference\non Learning Representations, 2023. URL https://openreview.net/forum?id=klK17OQ3KB.\nEric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and\nChristopher R\u00e9. S4ND: Modeling Images and Videos as Multidimensional Signals with State\nSpaces. Advances in neural information processing systems, 35:2846\u20132861, 2022.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text\nDegeneration. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=rygGQyrFvH.\n9\nAppendix\nA Dataset specifics\n11\nB\nCompute-constrained modeling\n11\nC Training recipes\n11\nD Discretization and selection\n13\nE\nEvaluation metrics\n14\nF\nPG19 generation samples\n15\n10\nA\nDataset specifics\nTable 5: Text dataset statistics. The total bytes, to-\ntal documents, and the mean document size (bytes\nper document) for each dataset.\nTotal bytes\nTotal docs\nBytes/doc\nPG19\n11.74G\n28, 752\n4, 082, 210\nStories\n34.18G\n948, 247\n36, 045\nBooks\n108.38G\n196, 640\n551, 179\nArXiv\n60.27G\n1, 264, 405\n47, 665\nCode\n677G\n56, 626, 342\n11, 958\nWe benchmark our results on various long-form\ntext datasets.\nThe PG19 dataset [Rae et al.,\n2020] is an extensive collection of full-length\nEnglish books (written before 1919) from the\nProject Gutenberg online library. The PG19\ndataset is ideal to test for long-distance con-\ntext modeling [Gao et al., 2020b]. The Sto-\nries dataset [Trinh and Le, 2018] is a subset of\nthe CommonCrawl data used for commonsense\nreasoning and language modeling. The Books\ndataset [Gao et al., 2020b] is another collection\nof English books. The ArXiv dataset [Gao et al.,\n2020b] comprises technical publications in LATEX from the arXiv online archive. Finally, the Code\ndataset [Gao et al., 2020b] is a large dataset of publicly available open-source code (under Apache,\nMIT, or BSD licenses). Dataset statistics are tabulated in Table 5.\nFor the PG19 dataset, we employ the train, validation, and test data splits as indicated by Rae et al.\n[2020]. For Stories, Books, ArXiv, and Code datasets, we randomly sample 40M consecutive bytes\nfor testing and the rest to train MambaByte.\nB\nCompute-constrained modeling\nLinear\nLinear\nLinear\nLinear\nLinear\nS4D\nFigure 4:\nGated-S4D block.\nAdapted from Mehta et al. [2023];\n\u03c6 indicates GELU activation\n[Hendrycks and Gimpel, 2016].\nAs noted earlier, we evaluate and benchmark MambaByte in a\ncompute-controlled setting. To this end, we estimate the FLOPs\nper byte incurred by various byte-level model architectures. We\nparameterize the architectures using hyperparameters n (ng/nl)\nnumber of (global/local) layers, dimension d (dg/dl) of the\n(global/local) residual stream, expansion factor e of linear layers,\npatch size p in MegaByte, state dimension nstate in SSMs, 1D\nconvolution kernel size k, and low-rank projection dimension\nr in Mamba. We also include Lctx bytes in the input context.\nDetailed component-wise compute counts for the forward pass\nare included in Table 6.\nFor the medium-scale language modeling experiments (Table 1,\n\u00a75 of Yu et al. [2023]), Yu et al. [2023] employ the MegaByte-\n758M+262M model, with a context length of 8, 192 and patch\nsize of 8, trained for 80B bytes.\nAs shown in Figure 5,\nMambaByte-353M (n = 53, d = 1, 024, e = 2) and MegaByte-\n758M+262M use the same total compute in FLOPs; hence, we\nemploy the MambaByte-353M to benchmark against MegaByte-\n758M+262M in Table 2 of \u00a74.\nFor the PG19 scaling experiment (Table 2, \u00a75 and Appendix D.3\nof Yu et al. [2023]), Yu et al. [2023] use MegaByte-1.3B+350M\n(context length of 8, 192 and patch size of 8) trained for 400B bytes to benchmark the observed\nword-level perplexity against several state-of-the-art subword models. Owing to our hardware\nlimitations, we train MambaByte-972M (n = 48, d = 1, 792, e = 2) and control for the total\ncompute used (see Figure 5 to view the associated computational costs). All the model sizes and\nassociated hyperparameters employed in this work are tabulated in Table 7.\nC\nTraining recipes\nAll the models in this study were trained using an AdamW optimizer with \u03b2 = (0.9, 0.95). We used a\nlinear learning rate warm-up (for the first 500 steps) followed by cosine annealing. Keeping consistent\n7We used the open-source implementation: https://github.com/lucidrains/MEGABYTE-pytorch.\n11\nTable 6: Compute (forward pass) estimates for various byte-level language models. Embedding,\nde-embedding, and sub-leading terms such as biases, nonlinearities, and layer norms are omitted. (\u03b1\u2217\nindicates an implementation-specific constant scaling term.)\nModel\nComponent\nFLOPs per byte\nTransformer\n[Vaswani et al., 2017]\nMulti-head attention\n2n(4d2 + 2Lctxd)\nPointwise feed-forward\n2n(2ed2)\nMegaByte\n[Yu et al., 2023]\nEmbedding projection\n2d2\ng\nGlobal transformer model\n2ng(4d2\ng + 2dgLctx/p + 2ed2\ng)/p\nGlobal-to-local projection\n2dgdl\nLocal transformer model\n2nl(4d2\nl + 2pdl + 2ed2\nl )\nGated-S4D\n(Figure 4)\nLinear projections\n2n(3ed2 + d2)\nKernel via Vandermonde v(A)\nn(\u03b1ved(nstate + Lctx) log2\n2(nstate + Lctx)/Lctx)\nS4D SSM with convolution\nn(\u03b1fft log(Lctx)ed + ed)\nElement-wise gating\nned\nMambaByte\n(Figure 3)\nLinear projections\n2n(3ed2)\nPre-SSM 1D convolution\n2nked\n\u2206, B, C from input x\n2n(2edr + 2ednstate)\nDiscretization, pre-scan: A, Bx\nn(3ednstate)\nRecurrence with parallel scan\nn(ednstate)\nOutput: y = Ch + Dx\n2nednstate + ned\nElement-wise gating\nned\n0\n100B\n200B\n300B\n400B\nBytes trained\n0\n200\n400\n600\n800\nTotal exa FLOPs used\n80B\n400B\n160B\n214B\nMegaByte-758M/262M (patch: 8)\nMegaByte-1.3B/218M (patch: 8)\nMegaByte-1.3B/350M (patch: 8)\nMambaByte-353M\nMambaByte-972M\nFigure 5: Computational cost for different model architectures at different scales. All models\nuse a context length of 8, 192, and MegaByte architectures use a patch size of 8.\nwith MegaByte training [Yu et al., 2023], we used a batch size of 48 across all our experiments.\nAdditionally, we do not use dropout with any of our models.\nFor the experiments in Figure 1, we conducted a hyperparameter search using peak learning rates\nof 0.0002, 0.0006, and 0.0008 and clipped the gradient norm to 1.0 for all the models. The best-\nobserved performance curve for each model is reported in Figure 1. Furthermore, we use an improved\nTransformer recipe that uses RMSNorm instead of LayerNorm, rotary positional encodings [Su et al.,\n2021], and linear terms without bias (same as [Yu et al., 2023]).\nIn our medium-scale experiments shown in Table 2, we set the peak learning rate to 0.0004 and\nclipped the gradient norm to 0.1. We trained the MambaByte-353M for a total of 80K steps, equivalent\nto 80, 000 \u00d7 48 \u00d7 8, 192 \u2248 30B bytes.\n12\nTable 7: Model hyperparameters. We report the model size and associated hyperparameters for\nall the models employed in this study. (Accompanying citation indicates the work from which the\nassociated configuration is noted; fields marked as \u2212 are unknown.)\nModel\nParameters\nHyperparameters\nn\n(ng/nl)\nd\n(dg/dl)\ne\nLctx\nOthers\nTransformer\n320M [Yu et al., 2023]\n22\n1, 024\n4 1, 024 heads: \u2212\n350M [Yu et al., 2023]\n24\n1, 024\n4 1, 024 heads: 16\n361M\n28\n1, 024\n4 8, 192 heads: 16\nPerceiverAR 248M [Yu et al., 2023]\n17\n1, 024\n4 8, 192 latents: 1, 024\nMegaByte\n193M+177M7\n14/14\n1, 024/1, 024 4 8, 192 p = 4, 8; heads: 16/16\n758M+262M [Yu et al., 2023] 14/18\n2, 048/1, 024 4 8, 192 p = 8; heads: 16/16\n1.3B+218M [Yu et al., 2023]\n24/15\n2, 048/1, 024 4 8, 192 p = 8; heads: 32/\u2212\n1.3B+350M [Yu et al., 2023]\n24/24\n2, 048/1, 024 4 8, 192 p = 8; heads: 32/16\nGated-S4D\n368M\n26\n1, 024\n4 8, 192 nstate = 64\nMambaByte\n353M\n53\n1, 024\n2 8, 192 k = 4; nstate = 16; r = 64\n972M\n48\n1, 792\n2 8, 192 k = 4; nstate = 16; r = 112\n1.6B\n48\n2, 304\n2 8, 192 k = 4; nstate = 16; r = 144\nIn the large-scale experiment on PG19, we use a similar setting to that in the medium-scale ex-\nperiments: the peak learning rate is set to 0.0004, and the gradient norm is clipped to 0.1. The\nMambaByte-972M is trained for 380K steps, equivalent to 380, 000 \u00d7 48 \u00d7 8, 192 \u2248 150B bytes.\nD\nDiscretization and selection\nDiscretization has deep connections to continuous-time systems, which allows for desirable properties\nsuch as model normalization [Orvieto et al., 2023, Gu et al., 2023] and resolution invariance [Nguyen\net al., 2022]. In this section, we show how zero-order hold discretization of a selective SSM can be\nviewed as a generalization of the gating mechanism in recurrent networks.\nZero-order hold discretization.\nFor a given input x(t) \u2208 R, we wish to discretize a continuous-\ntime SSM defined by (1) in \u00a72. To this end, we sample the system at different time intervals such\nthat x[k] = x(tk) for tk = Pk\nj=1 \u2206[j] and assume a zero-order hold, i.e., x(t) is constant between\nsamples: x(tk + \u03be) = x(tk) = x[k] for any \u03be \u2208 [tk, tk+1). The resultant matrices of the associated\ndiscrete SSM are:8\nA = exp(A \u2206);\nB = A\u22121(exp(A \u2206) \u2212 I) B;\nC = C .\nSelection mechanics and gating in recurrent networks.\nGu and Dao [2023] note that a selective\nSSM can be realized as a gated recurrence by setting \u2206 = softplus(z(x)) = softplus(W\u2206(WRx))\n(as indicated in (3) of \u00a72). By letting A = \u22121, B = 1, and n = 1, the authors observe:\nA = exp(A \u2206)\n= exp(\u2212 log(1 + exp(z(x))))\n=\n1\n1 + exp(z(x))\n= \u03c3(\u2212z(x))\n= 1 \u2212 \u03c3(z(x)).\nB = A\u22121(exp(A \u2206) \u2212 I) B\n= I \u2212 exp(A \u2206)\n= \u03c3(z(x)).\n8In Mamba [Gu and Dao, 2023], B is discretized through a simplified Euler (as opposed to zero-order hold)\ndiscretization from empirical observations of A being more important than B, and the performance does not\nchange significantly with simplification on B.\n13\nTable 8: PG19 dataset statistics. Split-wise UTF-8 encoded byte LB and space-separated token\ncounts LT in the PG19 dataset. (The byte count includes the newline character.) We also indicate the\nassociated bytes per token LB/LT .\nLB\nLT\nLB/LT\nTrain\n11, 677, 824, 216\n1, 973, 048, 393\n5.92\nValidation\n17, 733, 002\n3, 007, 061\n5.90\nTest\n41, 289, 101\n6, 965, 511\n5.93\nUsing A and B from above in the discrete recurrence (2), the selective SSM takes the form of a 1D\ngated recurrence:\nh[k] = (1 \u2212 \u03c3(z(x))) h[k \u2212 1] + \u03c3(z(x))x[k].\n(4)\nIt is interesting to note from (4) that lim\u2206\u2192\u221e h[k] = x[k] and lim\u2206\u21920 h[k] = h[k \u2212 1]: a large \u2206\n(\u2206 \u2192 \u221e) denotes the evolution of the system to focus only on the current input and forgetting the\nstate. In contrast, a small \u2206 (\u2206 \u2192 0) represents a transient input being ignored.\nSelectivity of A, B, and C matrices.\nGu and Dao [2023] argue that since the system matrix A\nonly affects the model through \u2206, i.e., A = exp(A \u2206). Hence, the selectivity in \u2206 is sufficient to\nensure selectivity in A.\nWhile the selectivity in \u2206 enables selectivity in the input matrix B, Gu and Dao [2023] hypothesize\nthat making B and C selective (in addition to \u2206) would allow for more fine-grained control based on\nthe content x[k] and evolving context h[k].\nE\nEvaluation metrics\nSubword-based language models [Vaswani et al., 2017, Hawthorne et al., 2022, Hutchins et al., 2022]\nreport their performance in word-level PPL, while byte-level language models [Xue et al., 2022,\nYu et al., 2023] report theirs in BPB. To facilitate meaningful comparisons, we report performance\nin BPB when benchmarking against byte-level models and PPL when comparing to token-level\nmodels. In this section, we detail the conversion between word-level PPL and BPB.\nIrrespective of the underlying segmentation, the amount of information I(D) in a given dataset D is\nconstant. Simply put,\nI(D) = LT bits per token = LB bits per byte\n(5a)\n\u225c \u2212 ln(D; model)\nln(2)\n,\n(5b)\nwhere LT and LB are the length of the dataset in tokens and bytes, respectively. From (5), we\nobserve:\nBPB = \u2212 ln(D; model)/LB\nln(2)\n= \u2113byte\nln(2),\nwhere \u2113byte is the observed byte-level negative log-likelihood loss (computed using ln). From (5), we\nalso note the following conversion from BPB to word-level PPL:\n\u2212 ln(D; model)/LT\nln(2)\n= LB\nLT\nBPB = LB\nLT\n\u2113byte\nln(2)\n\u21d2 PPL = exp\n\u0012LB\nLT\n\u2113byte\n\u0013\n= exp\n\u0012LB\nLT\nln(2) BPB\n\u0013\n.\nFor the PG19 dataset, we train MambaByte-972M to minimize BPB over the training data and report\nword-level PPL on the test data. Split-wise values of LB/LT for the PG19 dataset are tabulated in\nTable 8.\n14\nF\nPG19 generation samples\nThis section includes a few sample generations from the MambaByte-972M trained on the PG19\ndataset. We use Nucleus sampling with p = 0.98 [Holtzman et al., 2020] and generate continuations\nfor a total of 8, 192 bytes (including the given context prefix). Furthermore, we chose the same test\nset prefixes used in Appendix F of Rae et al. [2020]. We observe that the model is able to continue the\ndialogue in the style of the prefix and effectively recall the character names over hundreds of bytes.\nBaby Mine by Margaret Mayo\nContext (487 bytes):\nCHAPTER I\nEven in college Alfred Hardy was a young man of fixed ideas and high\nideals and proud of it.\nHis friend, Jimmy Jinks, had few ideas and no ideals, and was glad of\nit, and before half of their first college term had passed, Jimmy\nhad ridded himself of all such worries as making up his own mind or\ndirecting his own morals. Alfred did all these things so much better,\nargued Jimmy, furthermore, Alfred LIKED to do them\u2013Jimmy owed it to his\nfriend to give him that pleasure.\nMambaByte-972M:\nIt was not that Alfred really considered his friend an ideal husband,\nbut Jimmy admitted that it looked differently. And, too, it was all\nthere was to life.\nAnd on went Alfred, through long years of honest comradeship and\nunblushing promises and bubbling kindness. In his heart he had long since\nforgotten Jimmy and his friendship, since he realized the superiority of\nJimmy\u2019s ideal, if of any one\u2019s.\nIt was to be expected that Prince William would fall a victim. Perhaps\nhe was a modified betting man, and then had thrown over Nature and his\nteam-mates, as a victim to ridicule or a follower of hazardous ways.\nIt was then that Prince William went back into society, perhaps sought\nan interview with Miss La Roche, and learned that he had been the victim\nof romance death, and a suicide, and had been endowed with an absurd roman\ncross. Whatever it was, he saw in it the myth of Alfred\u2019s heroic deeds or\nthe myth of a yellow dog in the market plate. It sobered him gradually.\nHe became nearly a convert, and it was Alfred who, in bidding Miss Rossiter\ngood-by, said that he would never forget her, and never would be\nfriendly with his friend\u2019s family. It was not till now that Miss La Ross\nrealized that a really good life results in an excellent impulse, but the\nsequel proved the truth of Miss La Ross\u2019s predictions. One night, a year\nbefore Prince William\u2019s death, when she was wearing a diamond star in her\nhair, and admiring it, she heard Jimmy\u2019s front door open in the hall,\nand, turning round to look through the open doorway, in came Jimmy, and\nafter that where Jimmy went Alfred would sometimes go to the house next\ndoor, and they went over this now, and ever since Miss La Ross\u2019s spirit\nhad been there, and Jimmy had often insisted that a marriage should be\naccomplished in some ancient house\u2013\"And we will found a family of our\nown,\" said Jimmy, \"a for some distant house in this great Belgium.\"\nThe days went on and it was a glorious spring morning. Early in March a\nthick autumn rain had fallen on the house in New York and the leafless\ntrees looked weird and strange, and the wind roared like a crested wave\n15\nof fire. In the afternoon the thunder roared and soon an occasional\ngray drizzle eddied about the wet streets and the city glow and tint\nbegan to merge into a world of pink and red and amber and purple and\nsunset colors.\nThe low ceiling pillars had covered their treasures of gold and gems, and\nthe great stained-glass windows in dreamily gentle, half-lights, had\narranged themselves in red and gold stripes of magic tints. Even though\nthe delicate color scheme had had its disappointments once, it had\ncertainly affected one hour in this room as well as ten months later and\nlonger stays at least would have done.\nTo-day a soft South wind had drifted through the open door, and a sudden\neast wind had driven open the French windows of Miss La Ross\u2019s bedroom,\nand it rained in pure flames between the ceiling and boarded floor.\nAlfred\u2019s room was fragrant with his presence.\n\"... A little singing bird\nThat, living in a cage, demands a friend\nWhose cool-blooded lure can warm the heart with love\nTo a fluttering, wounded thing.\nAs in a pathetic garden, so in the hall room.\nIt was cold, to-day. Already the rooms seemed overheated. The curtains\nwere already half drawn.\nShe shivered.\n\"Mid-winter, to-day,\" thought Alfred, watching the sweep of Ann\u2019s white\nshoulder and patting her thin cotton frock. \"Seven there were of days.\nAnd seven is yet untold gone. Fine, fine day, by Christ! Come out of this\nold soot, and we\u2019ll fly... Away. God rest his soul from hell, if ever such\na devil crawled this broad, raw earth.... Where are you, Ann?\"\nAnn waited and trembled, she knew not why, for a sharp voice was asking\nsuddenly for the check book in her hand.\n\"Get me change enough to pay for lunch for Jimmy,\" Alfred chided.\nBefore the one empty chair on the hall table and under the curtains lay\na crashing pile of ready money. \"And the window shades are closed,\" added Alfred.\n\"It won\u2019t shut out the rain,\" smiled Ann.\n\"But he won\u2019t care,\" protested Ann.\nAlfred laid a strong withdrawing hand on the fair golden hair for a\nmoment.\n\"It\u2019s all right,\" he coaxed. \"Without a cent behind them to-day we can put\nin four thousand and close the bottom against a falling price like this.\"\nHe was holding up the window sill six inches.\nWhile he stood she whispered:\n\"I\u2019m only lucky to save the day.\"\n16\n\"He helps you without a reward,\" Alfred said.\n\"He\u2019s kind... and darned bad.\"\nAnn noted dangerous things that afternoon.\n\"You could sing and play?\" she asked.\n\"No, no!\" insisted Alfred. \"I CAN\u2019T play and sing. The room is cold. It\u2019s\nwarm within.\"\nAlfred was changing clothes when he had that lucky escape, and Alfred\nmomentarily forgot his debt. Ann laid the bill she had placed on the\ntable, and when she had gone Alfred had not even looked at it, and it was\nthe act she saw in that frame of mind, remembering it, that made her put\nit back again.\nNow Alfred was thoroughly cold and temperamental, and when he probed an\nobligation that he had just been trying to shift on the other fellow,\nhe was more easily reminded. When Jimmy, cold and hungry, had wormed his\nway into his room that day at dinner, and been halted at his close chair\nby the soup stove, the young man\u2019s gaze had fixed furiously to the\nplatter of gold and had immediately started on the other food with an\nintensity of expression that had awakened Jimmy\u2019s appreciation of the\nhot day of purposes and had aroused even Ann\u2019s observant sense.\nJimmy\u2019s employer had met him on Close Street after the unsuccessful row\nover the Dearborn Cats. Jimmy, who was not naturally an observant boy,\nhad tried to keep in the line of his employer\u2019s movements and tell Alfred\nhis employer just what he did for a living, but all Alfred\u2019s energy had\nvanished, and on sundry occasions he had caught Jimmy\u2019s eye, and once he\nhad promptly appeared to mere assiduous examination of the window.\nEmployer\u2019s Jimmy had been dexterous enough, subdued, but his dexterity\nand subtlety and sagacity had not failed.\nAs one in employment was a most elusive proposition in this crafty world\nof facts, just then Alfred had found a perfect driftwood, and so had met\nand accepted and stood in the way of Jimmy\u2019s castigation and reproach. That\nis to say, he had saved Jimmy from seeing any of his own real qualities,\nand the critics, he had been asked in Jimmy\u2019s more frequent matinees to\nerase Alfred\u2019s sneer and snip off his coat, and he had instantly become a\nmental picture of Jimmy Dean\u2019s assistant to the lawyer and the\ncollege professor.\nIt was Jimmy\u2019s reckless impetuousness, not his single fearless single\nenergy, that had led Ann through the door at sight of Ann, that had\nelectrified the tremendous audience, not her own act or attitude.\nJimmy had thought still of the boy as a fellow mortal, now his master had\ngone.\nThat was a satisfactory driftwood, of Jimmy.\nThat evening Ann\u2019s maid had gone into the bedroom where Jimmy was and had\nsaid again that he looked very tired, and as Ann assigned as a reason his\nlong face, it was not impossible to conclude that he was suffering from\na formal ache.\nBut, as she had only managed a nap and had been rather silent when away\nfrom Jimmy, Ann determined that the tired-out and worn-looking man must\n17\nbe tired, too, and she secretly determined that at breakfast to-morrow\nshe would place in his hands all the evidence she could imagine against\nJimmy, as pertaining to the unsolved mystery of Jimmy Dean\u2019s friend.\nCHAPTER II\nThat afternoon Jimmy was taken into the professor\u2019s home to dinner.\nWonderful Dean\u2019s Hal\nThe Diary of Samuel Pepys\nContext (826 bytes):\nJune 1st. Having taken our leaves of Sir W. Batten and my Lady, who are\ngone this morning to keep their Whitsuntide, Sir W. Pen and I and Mr.\nGauden by water to Woolwich, and there went from ship to ship to give\norder for and take notice of their forwardness to go forth, and then to\nDeptford and did the like, having dined at Woolwich with Captain Poole at\nthe tavern there. From Deptford we walked to Redriffe, calling at the\nhalf-way house, and there come into a room where there was infinite of new\ncakes placed that are made against Whitsuntide, and there we were very\nmerry. By water home, and there did businesses of the office. Among\nothers got my Lord\u2019s imprest of L1000 and Mr. Creed\u2019s of L10,000 against\nthis voyage their bills signed. Having wrote letters into the country and\nread some things I went to bed.\nMambaByte-972M:\nThere came my ship in this morning, which had safely arrived after\nall our troubles, though some to this day we thought had been lost,\nviz., of the north winds. Whereabouts of us, and going to find them, we\nhad much ado to get up in the day time without much losing our mast and\ngoing about by night. At noon came the King and Queen and all their\nCourt (unhappily in some pain from the swelling of the belly). There\nwere myself with others going to dinner to meet them; and my Lady\nSandwich came, by special invitation of my Lady Jem., to dine at Court.\nShe was invited to dinner afterwards by Mrs. Penington; and after dinner\nI going out to her coach, we fell to talk of the happiness of this, and\nbeing astonished at the curiosity and ingenuity almost of the age and\nremarkable folly of the age for things of this kind: \"Many good fortunes,\"\nsays she, \"will happen upon this discourse; your lady (I know) hath\nthe choice of your person taken in some few; but you, charming and ingen\nas you are, I doubt you will not make an end.\" \"Bah!\" says I, \"it is\ngood dallying with the ladies; for if once off of her, I promise you her\ntalking and witty company, and you could not choose but have her.\"\nAt dinner we staid some time, discoursing about her forwardness as to\nher coming over from her beloved Deptford to be Lady. My Lady is so\ntender, that I did not help to give her opportunity of shining a little,\nby having been called out to dinner to Sir R. Holmes before I set out, it\nhaving been proposed to my Lady to stay and go as far as Gravesende; of\nwhom she was very much pleased, even to the skirt of her gowne. But,\nwhen I came back I found the strictest inquiry made after me, I coming\nover to Hinchinbroke; and my Lady cryed out of my going into the\ncoach, lest she should be made to say I stayed for her. Poor little\nwench, I confess I was in a great confusion, being to see my Lady set out\nwithout me; which, if I had known, I had wisely taken a resolution not to\n18\nhave been of it. At last it was agreed between Sir W. Batten and my Lady\nthat the maid should go herself in a boat, and my Lady his wife to go in\nanother boat with Mrs. Penington, who had a great mind to go. My Lady\nhad thoughts of sending Jane for her. Which we were loth to, the\nsingleness of their minds and opposition they meeting with no manner of\nbusiness at home, and being desired to do so by my Lady, did instance me\nto get Jane and little Mrs. Penington to stay with me, and myself to\nlodge and launch two boats more, for the building now and to study the\ndisposing of them; but, being there, occasion will not suffer me to go\nupon them, but I leave it wholly to their own pleasures. The day proving\nmost pleasant all the afternoon, so that it was agreed on, and even our\npreparations done, my mind all over afeard and discouraged, as if things\nwould not have been better. Upon our sending away this poor wench (my\nLady sitting looking on) I rose up and took my wife and my son T. Greaves\nas proxies for her to go by herself, in my great coach in the Towre\ngarden, in the last coach my Lord will lend me; which coach is well\nchosen and good. So giving order for the horse to be chosen neatest and\nwhitest, we set forth; Mrs. Penington on the box with me scolding at him\nall the way; my great discourse my losing the fine pearl, where I had\nmuch ado to keep from crying, such a jealous pain she put me to. Away to\nLambeth chappell; where, I perceive, my Lord did give his desire to have\nunderstand who are to sit in his chappell, he standing up all the sermon.\nThis continued most part of the sermon, and in our going over the bridge\nabove bridge, I bethought myself that my Lord must have grounded his\nresolution of having it so. He seemed therefore to give very civil\ndirection at cards after sermon in the evening, to my Lord\u2019s great satisfaction;\nbut on the reading of the Breviate he was as offended as I\ncould be of the alteration (enquiring too deeply into the goodness of the\ngentlemen in their not desiring to have a further change) which my Lord\ndid very readily, and told me he fell very off; and, losing all he had,\nand taking nothing, sighed very heavily and mournfully, and so went away.\nThe business was not made much merrier for this sermon; for which I was\nsorry, and remaining professedly sad we rowed to Kingston, and after some\nsolemn service we landed there and went up to a little alehouse some few\nmiles up yet Stoul. But, Lord! the humours of boys, which I never knew in\nmy life before, how fretting, tearing and swearing, and so I made their\nbellies fret presently, no supper nor ever man of the house sitting down as\nI expected. So ended this businesse with joy beyond what was hoped for it\nthe first time. To Stoul and back again.\nThe Patrol of the Sun Dance Trail by Ralph Connor\nContext (1, 059 bytes):\nHigh up on the hillside in the midst of a rugged group of jack pines the\nUnion Jack shook out its folds gallantly in the breeze that swept down\nthe Kicking Horse Pass. That gallant flag marked the headquarters of\nSuperintendent Strong, of the North West Mounted Police, whose special\nduty it was to preserve law and order along the construction line of the\nCanadian Pacific Railway Company, now pushed west some scores of miles.\nAlong the tote-road, which ran parallel to the steel, a man, dark of\nskin, slight but wiry, came running, his hard panting, his streaming\nface, his open mouth proclaiming his exhaustion. At a little trail that\nled to the left he paused, noted its course toward the flaunting flag,\nturned into it, then struggled up the rocky hillside till he came to the\nwooden shack, with a deep porch running round it, and surrounded by\na rustic fence which enclosed a garden whose neatness illustrated a\ncharacteristic of the British soldier. The runner passed in through the\ngate and up the little gravel walk and began to ascend the steps.\n19\nMambaByte-972M:\nIndescribable and yet unconscious was this man. All his being rose to\na sense of the security that was about him as he walked onwards swiftly\nbetween rows of young firs of Canadian growth. He swung impulsively by\nthe gap of a gate and paused. Then the clangour of an alarum bell from\na city clock was sharply repeated with its deep and terrible sound.\nThis bell not only announced the desire of the inhabitants of the\nintervening concession lines to their work, but it also warned from the\nfurther side of the high boundary constructed by the late engineer of\nthe West Mounted Police the abrupt and rapid career of another man. The\npistol had fallen, and the runner was searching for another.\nThe solicitor buried his face in his hands.\n\"In God\u2019s name, man!\" he ejaculated. \"How do I know? Haven\u2019t I just\nfound one?\"\nHe had rung the bell, and now fell back among the shadows and the rough\nbushes. The sound startled him. He stood transfixed a moment before he\nlooked about him.\n\"There\u2019s only one man knowing where that tote-road goes.\" He still held\nhis breath and then listened. \"\u2019Tis where the first news comes from,\nand\u2013oh, I see! There\u2019s my boy.\"\nHe waited a moment. Then once more he passed along the gap and descended\nthe rocky little to the more sheltered spot where the new\nbuildings stood.\nHe was an under officer in that high branch of the service known as the\nUnion League, and a short sharp beat was his upon the white drilling of\nbashi-bazouks. With the sounding of another great bell of a church\nclose at hand he moved quickly round to the other side of the buildings.\nAs he approached, however, he took from his pocket a thin black silk\nneckerchief. It was damp and stained with the blood of dead men. He laid\nit in the hands of a slim girl, with the limpid blue eyes of the Canadian\nSaskatchewan.\n\"What\u2019s that for?\" he demanded.\nShe looked as if there had been something she desired to say, then left\nthe agitated conclusion unfinished. Her eyes sought his in the pathetic\nwistfulness of a child, then suddenly fell. For the hurt he had done her\nwas not a wound incurred in battle. It was merely a little scratch in\nthe hand, and let alone that, in a manner of speaking, it was all she\nhad. The blood of a man is always more significant than that of a\nscratch on the bark of a tree, and a pressure of the earth leaves a\ndeeper mark on a man\u2019s arm. With a sigh the runner removed the blood\nstain and turned his face towards the sound again. He walked half across\nthe open grass from which he had sprung. From his ample form to the far-\ndistant leaping folds of his drilling trousers he had trailed a forked\nstick, and so to the girl.\nIn a few seconds he came back.\n\"It\u2019s me, pardner, Superintendent Strong. It\u2019s me I\u2019m goin\u2019 down from the\nSoo, for the job I had in Mexico after I came out here. I\u2019m connected\n20\nwith the Canadian Pacific Railway and they\u2019re hunting up a man who did\nhave a finger wounded by a Canadian rock. I\u2019m sendin\u2019 the little flag\nwith her.\" He emphasised the word \"flag.\" A rough skin mark, furrowed in\na straight line down his left cheek, marked the place of the scar and\nbrought him to a sudden stop. His eyes were on the scrolled letters\nabove his head.\n\"I\u2019m going down to get it. I\u2019ve got to get it to the bottom, anyway, for\ndivil a bit of paper they\u2019ll let me have at British Columbia. Oh, God!\"\nHe raised his voice. In a moment he had departed. In a few minutes he\nhad rejoined the girl. They rejoined the solicitor and returned with him\nto an open space before the meeting place of the railway company. As they\ngathered round a table spread with an untasted meal the solicitor spoke.\nThe railroad company was working out from British Columbia to Montreal.\n\"In our fight we had it hard,\" he said. \"The northern route to League\nIsland was blocked, we could not reach there to recruit. We had to look\nfor a northern route, for there was none. At first the league flag of\nOttawa was given up. That was only till October. Then a young man on the\nground from London came to us. He\u2019d been in the runner\u2019s service along\nthe whole line from Montreal. He was headed for Canada on the\ntelegraph. Two of us had to flag him as soon as we set out from here.\nHe had been over that ground about fifty times before, and knew the\nwhole road well for forty miles. The head of us did not know it\ntill he came to the junction where the main line crosses the north line\nof the United States. We took that name on the tin to test him.\"\n\"What was the corporation over there for?\" said the solicitor. \"I remember,\nI remember. It occupied a part of the big Kelvin mine. I was helping get\nthe first claim post run by the Union League at the time I was there.\nHe was out hunting coal. He came down one day to see the coal pits about\nthe ground. On the way he was stopped and accused of raising a rebellion,\nand was arrested and taken to the Soo, where he was made to give\nevidence in a certain case that had been laid before him.\"\n\"And what was the precise cause of the complaint?\" asked the runner.\n\"Well, it wasn\u2019t a case at all, it was a fact. That\u2019s all,\" explained the\nconstable.\n\"From what I heard then of the runners of the London and North West, their\nwork wasn\u2019t near so exciting and dangerous as it had been reported to be.\nAlso it was the work of others, others still, and they were arrested.\nThey was a young feller and a girl married over two years ago, and he was\nshot.\"\n\"Brought to trial for that by himself or his relatives or some of the men\nwho were with him?\" There was a puzzled, gentle expression on the face of\nthe railway superintendent. He was of much higher rank, for he had not\nbeen present at the trial of the accused. He glanced up at the runner.\n\"Arrested?\" The bit of food in his mouth was working like a millstone in\nthe Soo employer\u2019s breast. Then, as though unconsciously to himself, his\nlips said \"yes\" instead of \"no,\" and he added instead, \"and sworn to it.\nThat\u2019s as far as you\u2019ve got, pardner. Anything else, sir?\" He was watching the\nsilent figure with intense desire to see his face and to know what he\nfelt. It did not come, and he settled himself in his chair with a sigh.\n21\n\"That was short work. They marched the young feller up here, and give him\nthe Canadian division. It was the station sergeant-inspector from the\nCanadian line sending down from headquarters to show he was all right and\nnot having heard anything against him. And if you don\u2019t know that it\u2019s not\nthe worst of the testimony we have to give, pardner. It wasn\u2019t the best.\nThe fact is the young man was getting three weeks\u2019 sentence at the time.\"\n\"That was only a month ago,\" broke in the businesslike runner, who had been\npreparing himself for a full report. \"What had he done? Tell us?\"\nThere was something pathetic in the voice and in the manner of the young\nman. Then, as he mounted his story, the under-officer took up the thread\nin an apologetic tone, but was brought back to a moment\u2019s serious interest\nby the stopping of it by the voice of the other.\n22\n"
  },
  {
    "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
    "link": "https://arxiv.org/pdf/2401.13601.pdf",
    "upvote": "40",
    "text": "MM-LLMs: Recent Advances in MultiModal Large Language Models\nDuzhen Zhang1* , Yahan Yu2* , Chenxing Li1 , Jiahua Dong3\u2020 , Dan Su1,\nChenhui Chu2\u2020 and Dong Yu1\n1Tencent AI Lab\n2Kyoto University\n3Shenyang Institute of Automation, Chinese Academy of Sciences\nscoutzhang@tencent.com, yahan@nlp.ist.i.kyoto-u.ac.jp\nAbstract\nIn the past year, MultiModal Large Language\nModels (MM-LLMs) have undergone substan-\ntial advancements, augmenting off-the-shelf\nLLMs to support MM inputs or outputs via\ncost-effective training strategies. The resulting\nmodels not only preserve the inherent reason-\ning and decision-making capabilities of LLMs\nbut also empower a diverse range of MM tasks.\nIn this paper, we provide a comprehensive sur-\nvey aimed at facilitating further research of\nMM-LLMs. Initially, we outline general de-\nsign formulations for model architecture and\ntraining pipeline. Subsequently, we introduce a\ntaxonomy encompassing 122 MM-LLMs, each\ncharacterized by its specific formulations. Fur-\nthermore, we review the performance of se-\nlected MM-LLMs on mainstream benchmarks\nand summarize key training recipes to enhance\nthe potency of MM-LLMs. Finally, we explore\npromising directions for MM-LLMs while con-\ncurrently maintaining a real-time tracking web-\nsite1 for the latest developments in the field. We\nhope that this survey contributes to the ongoing\nadvancement of the MM-LLMs domain.\n1\nIntroduction\nMultiModal (MM) pre-training research has wit-\nnessed significant advancements in recent years,\nconsistently pushing the performance boundaries\nacross a spectrum of downstream tasks (Li et al.,\n2020; Akbari et al., 2021; Fang et al., 2021; Yan\net al., 2021; Li et al., 2021; Radford et al., 2021; Li\net al., 2022; Zellers et al., 2022; Zeng et al., 2022b;\nYang et al., 2022; Wang et al., 2022a,b). How-\never, as the scale of models and datasets continues\nto expand, traditional MM models incur substan-\ntial computational costs, particularly when trained\nfrom scratch. Recognizing that MM research op-\nerates at the intersection of various modalities, a\n*Equal contributions.\n\u2020Corresponding authors.\n1https://mm-llms.github.io\nApr.\n2022\n2023\nJan. ~ Feb.\nMay.\nJun.\nJul.\nSep.\nOct.\nFlamingo\nBLIP-2\nPaLM-E Visual ChatGPT\nViperGPT\nGPT-4\nMM-REACT HuggingGPT\nLLaVA\nMiniGPT-4\nAudioGPT mPLUG-Owl\nX-LLM\nVideoChat InstructBLIP SpeechGPT\nEmbodiedGPT PandaGPT\nPaLI-X\nVideo-LLaMA\nVideo-ChatGPT\nKosmos-2\nShikra\nDLP\nChatSpot\nBuboGPT\nQwen-VL\nNExT-GPT\nMiniGPT-5\nMiniGPT-v2\nFuyu-8B\nCogVLM\nDRESS\nX-InstructBLIP CoDi-2\nVILA\nMobileVLM\nLLaVA-1.5\nIDEFICS\nOpenFlamingo\nMM-GPT\nKosmos-1\nGemini\nQwen-Audio\nFROMAGe\nMar. ~ Apr.\nOtter\nDetGPT\nGILL\nLLaVA-Med\nAudioPaLM\nLLaVAR\nmPLUG-DocOwl\nLynx\nGPT4RoI\nEmu\nSEED\nAug.\nLISA\nChinese-LLaVA\nASM\nBLIVA\nVisCPM\nCM3Leon\nLaVIT\nKosmos-2.5 DreamLLM\nInternLM-\nXComposer\nJAM\nAnyMAL\nLanguageBind\nKosmos-G\nSALMONN ControlLLM\nNov.-1\nGLaMM\nmPLUG-Owl2 TEAL\nLLaVA-Plus\nMonkey\nVolcano\nLION\nDocPedia\nShareGPT4V LLaMA-VID\nVIM\nmPLUG-\nPaperOwl\nNov.-2\nDec.-1\nRLHF-V\nDolphins\nPixelLM\nSilkie\nLyrics\nCogAgent\nVL-GPT\nOsprey\nCLOVA\nDec.-2\nEmu-2\nIntern-VL\nV*\nTinyGPT-V\nDocLLM\n2024\nJan.-1\nLLaVA-Phi\n3DMIT\nGroundingGPT ModaVerse\n!\u2212UMi\nMM-Interleaved DiffusionGPT MLLM-Tool\nRPG\nKAM-CoT\nYi-VL\nJan.-2\nCogCoM\nVLGuard\nMobileVLM V2\nMoE-LLaVA\nWebVoyager\nMobile-Agent\nLLaVA-MoLE LLaVA-NeXT\nVary-toy\nSPHINX-X\nInternLM-\nXComposer2\nFeb.\nViGoR\nVisLingInstruct\nFigure 1: The timeline of MM-LLMs.\nlogical approach is to capitalize on readily avail-\nable pre-trained unimodal foundation models, with\na special emphasis on powerful Large Language\nModels (LLMs) (OpenAI, 2022). This strategy\naims to mitigate computational expenses and en-\nhance the efficacy of MM pre-training, leading to\nthe emergence of a novel field: MM-LLMs.\nMM-LLMs harness LLMs as the cognitive pow-\nerhouse to empower various MM tasks. LLMs\ncontribute desirable properties like robust language\ngeneration, zero-shot transfer capabilities, and\nIn-Context Learning (ICL). Concurrently, foun-\ndation models in other modalities provide high-\nquality representations. Considering foundation\nmodels from different modalities are individually\npre-trained, the core challenge facing MM-LLMs\nis how to effectively connect LLMs with models\nin other modalities to enable collaborative infer-\nence. The predominant focus within this field has\nbeen on refining alignment between modalities and\naligning with human intent via a MM Pre-Training\n(PT) + MM Instruction-Tuning (IT) pipeline.\nWith the debut of GPT-4(Vision) (OpenAI, 2023)\narXiv:2401.13601v4  [cs.CL]  20 Feb 2024\nand Gemini (Team et al., 2023), showcasing im-\npressive MM understanding and generation ca-\npabilities, a research fervor on MM-LLMs has\nbeen sparked. Initial research primarily focuses\non MM content comprehension and text genera-\ntion, encompassing tasks such as image-text under-\nstanding, exemplified by projects like BLIP-2 (Li\net al., 2023e), LLaVA (Liu et al., 2023e), MiniGPT-\n4 (Zhu et al., 2023a), and OpenFlamingo (Awadalla\net al., 2023); video-text understanding, as demon-\nstrated by initiatives such as VideoChat (Li et al.,\n2023f), Video-ChatGPT (Maaz et al., 2023), and\nLLaMA-VID (Li et al., 2023j); and audio-text\nunderstanding, as seen in projects like Qwen-\nAudio (Chu et al., 2023b). Later, the capabili-\nties of MM-LLMs have been expanded to sup-\nport specific modality outputs. This includes tasks\nwith image-text output, such as GILL (Koh et al.,\n2023a), Kosmos-2 (Peng et al., 2023), Emu (Sun\net al., 2024), and MiniGPT-5 (Zheng et al., 2023b);\nas well as speech/audio-text output, exemplified\nby projects like SpeechGPT (Zhang et al., 2023a)\nand AudioPaLM (Rubenstein et al., 2023). Recent\nresearch endeavors have focused on mimicking\nhuman-like any-to-any modality conversion, shed-\nding light on the path to artificial general intelli-\ngence. Some efforts aim to amalgamate LLMs with\nexternal tools to reach an approaching any-to-any\nMM comprehension and generation, such as Visual-\nChatGPT (Wu et al., 2023a), HuggingGPT (Shen\net al., 2023), and AudioGPT (Huang et al., 2023b).\nConversely, to mitigate propagated errors in the\ncascade system, initiatives like NExT-GPT (Wu\net al., 2023d), CoDi-2 (Tang et al., 2023c), and\nModaVerse (Wang et al., 2024c) have developed\nend-to-end MM-LLMs of arbitrary modalities. The\ntimeline of MM-LLMs is depicted in Figure 1.\nIn this paper, we present a comprehensive survey\naimed at facilitating further research of MM-LLMs.\nTo provide readers with a holistic understanding of\nMM-LLMs, we initially delineate general design\nformulations from model architecture (Section 2)\nand training pipeline (Section 3). We break down\nthe general model architecture into five compo-\nnents: Modality Encoder (Section 2.1), Input Pro-\njector (Section 2.2), LLM Backbone (Section 2.3),\nOutput Projector (Section 2.4), and Modality Gen-\nerator (Section 2.5).\nThe training pipeline elu-\ncidates how to enhance a pre-trained text-only\nLLM to support MM input or output, primarily\nconsisting of two stages: MM PT (Section 3.1)\nand MM IT (Section 3.2).\nIn that section, we\nalso provide a summary of mainstream datasets\nfor MM PT and MM IT. Next, we establish a tax-\nonomy encompassing 122 State-of-the-Art (SOTA)\nMM-LLMs, each characterized by specific formu-\nlations, and summarize their development trends\nin Section 4. In Section 5, we comprehensively\nreview the performance of major MM-LLMs on\nmainstream benchmarks and distill key training\nrecipes to enhance the efficacy of MM-LLMs. In\nSection 6, we offer promising directions for MM-\nLLMs research. Moreover, we have established\na website (https://mm-llms.github.io) to track the\nlatest progress of MM-LLMs and facilitate crowd-\nsourcing updates. Finally, we summarize the en-\ntire paper in Section 7 and discuss related surveys\non MM-LLMs in Appendix A. We aspire for our\nsurvey to aid researchers in gaining a deeper under-\nstanding of this field and to inspire the design of\nmore effective MM-LLMs.\n2\nModel Architecture\nIn this section, we provide a detailed overview\nof the five components comprising the general\nmodel architecture, along with the implementation\nchoices for each component, as illustrated in Fig-\nure 2. MM-LLMs that emphasize MM understand-\ning only include the first three components. During\ntraining, Modality Encoder, LLM Backbone, and\nModality Generator are generally maintained in a\nfrozen state. The primary optimization emphasis\nis on Input and Output Projectors. Given that Pro-\njectors are lightweight components, the proportion\nof trainable parameters in MM-LLMs is notably\nsmall compared to the total parameter count (typi-\ncally around 2%). The overall parameter count is\ncontingent on the scale of the core LLM utilized\nin the MM-LLMs. As a result, MM-LLMs can be\nefficiently trained to empower various MM tasks.\n2.1\nModality Encoder\nThe Modality Encoder (ME) is tasked with encod-\ning inputs from diverse modalities IX to obtain\ncorresponding features FX, formulated as follows:\nFX = MEX(IX).\n(1)\nVarious pre-trained encoder options MEX exist\nfor handling different modalities, where X can be\nimage, video, audio, 3D, etc. Next, we will offer a\nconcise introduction organized by modality.\nVisual Modality\nFor images, there are vari-\nous optional encoders: NFNet-F6 (Brock et al.,\nImage\nVideo\nAudio\nModality Encoder !\"!\nInput Projector \u0398!\u2192#\nText $\n!!\n\u2026\n\u2026\nImage\nVideo\nNFNet-F6\nViT\nCLIP ViT\nEva-CLIP ViT\nAudio\nC-Former\nHuBERT\nBEATs\n\"!\nLinear Projector\nMLP\nCross-aAenBon\nQ-Former\nP-Former\n\u2026\n#!\nLLM Backbone\nFlan-T5\nChatGLM\nUL2\nQwen\nChinchilla\nOPT\nPaLM\nLLaMA\nLLaMA-2\n\u2026\nS!\nOutput Projector \u0398$\u2192%\nTiny Transformer\nMLP\n\u2026\n%!\nModality Generator \"#!\nImage\nVideo\nAudio\nAudioLDM\nStable Di\ufb00usion\nZeroscope\n\u2026\n\u2744\n\u2744\n\u2744\n\"\n\"\n\"$\nMul.modal Understanding\nMul.modal Genera.on\n\u2026\nUni\ufb01ed \nImageBind\nVicuna\n\u2026\n\u2026\nMQ-Former\nFigure 2: The general model architecture of MM-LLMs and the implementation choices for each component.\n2021), ViT (Dosovitskiy et al., 2020), CLIP\nViT (Radford et al., 2021), Eva-CLIP ViT (Fang\net al., 2023), BEiT-3 (Wang et al., 2023d), Open-\nCLIP (Cherti et al., 2023), Grounding-DINO-\nT (Zhang et al., 2022b) with Swin-T (Liu et al.,\n2021b) backbone, DINOv2 (Oquab et al., 2023),\nSAM-HQ (Kirillov et al., 2023) with MAE (He\net al., 2022), RAM++ (Zhang et al., 2023i) with\nSwin-B backbone, InternViT (Chen et al., 2023j),\nand VCoder (Jain et al., 2023). For videos, they\ncan be uniformly sampled to 5 frames, undergoing\nthe same pre-processing as images.\nAudio Modality\nis typically encoded by C-\nFormer (Chen et al., 2023b), HuBERT (Hsu et al.,\n2021), BEATs (Chen et al., 2023g), Whisper (Rad-\nford et al., 2023), and CLAP (Wu et al., 2023e).\n3D Point Cloud Modality\nis typically encoded\nby ULIP-2 (Salesforce, 2022) with a Point-\nBERT (Yu et al., 2022) backbone.\nMoreover, to handle numerous heterogeneous\nmodal encoders, some MM-LLMs, particularly\nany-to-any ones, use ImageBind (Girdhar et al.,\n2023), a unified encoder covering six modalities,\nincluding image/video, text, audio, heat map, in-\nertial measurement units, and depth. We provide\na brief introduction to some mainstream modality\nencoders in Appendix B.\n2.2\nInput Projector\nThe Input Projector \u0398X\u2192T is tasked with align-\ning the encoded features of other modalities FX\nwith the text feature space T. The aligned fea-\ntures as prompts PX are then fed into the LLM\nBackbone alongside the textual features FT . Given\nX-text dataset {IX, t}, the goal is to minimize the\nX-conditioned text generation loss Ltxt-gen:\narg min\n\u0398X\u2192T\nLtxt-gen(LLM(PX, FT ), t),\n(2)\nwhere PX = \u0398X\u2192T (FX).\nThe Input Projector can be achieved directly\nby a Linear Projector or Multi-Layer Percep-\ntron (MLP), i.e., several linear projectors in-\nterleaved with non-linear activation functions.\nThere are also more complex implementations like\nCross-attention, Q-Former (Li et al., 2023e), P-\nFormer (Jian et al., 2023), and MQ-Former (Lu\net al., 2023a). Cross-attention (Perceiver Resam-\npler) (Alayrac et al., 2022) uses a set of trainable\nvectors as queries and the encoded features FX as\nkeys to compress the feature sequence to a fixed\nlength. The compressed representation is then fed\ndirectly into the LLM or further used for X-Text\ncross-attention fusion. Q-Former extracts relevant\nfeatures from FX with learnable queries, and the se-\nlected features are then used as prompts PX. Mean-\nwhile, P-Former generates \"reference prompts\",\nimposing an alignment constraint on the prompts\nproduced by Q-Former. MQ-Former conducts a\nfine-grained alignment of multi-scale visual and\ntextual signals. However, both Q-, P-, MQ-Former\nrequire an additional PT process for initialization.\n2.3\nLLM Backbone\nTaking LLMs (Zhao et al., 2023c; Naveed et al.,\n2023; Luo et al., 2023) as the core agents, MM-\nLLMs can inherit some notable properties like\nzero-shot generalization, few-shot ICL, Chain-of-\nThought (CoT), and instruction following. The\nLLM Backbone processes representations from var-\nious modalities, engaging in semantic understand-\ning, reasoning, and decision-making regarding the\ninputs. It produces (1) direct textual outputs t, and\n(2) signal tokens SX from other modalities (if any).\nThese signal tokens act as instructions to guide the\ngenerator on whether to produce MM contents and,\nif affirmative, specifying the content to produce:\nt, SX = LLM(PX, FT ),\n(3)\nwhere the aligned representations of other modal-\nities PX can be considered as soft Prompt-tuning\nfor the LLM. Moreover, some works have in-\ntroduced Parameter-Efficient Fine-Tuning (PEFT)\nmethods, such as Prefix-tuning (Li and Liang,\n2021), LoRA (Hu et al., 2021), and LayerNorm\ntuning (Zhao et al., 2024). In these cases, the num-\nber of additional trainable parameters is exception-\nally minimal, even less than 0.1% of the total LLM\nparameter count. We provide an introduction to\nmainstream PEFT methods in Appendix C.\nThe commonly used LLMs in MM-LLMs incude\nFlan-T5 (Chung et al., 2022), ChatGLM (Zeng\net al., 2022a), UL2 (Tay et al., 2022), Persim-\nmon (Elsen et al., 2023), Qwen (Bai et al., 2023a),\nChinchilla (Hoffmann et al., 2022), OPT (Zhang\net al., 2022c), PaLM (Chowdhery et al., 2023),\nLLaMA (Touvron et al., 2023a), LLaMA-2 (Tou-\nvron et al., 2023b), and Vicuna (Chiang et al.,\n2023). We provide a brief introduction to some\nrepresentative LLMs in Appendix D.\n2.4\nOutput Projector\nThe Output Projector \u0398T\u2192X maps the signal to-\nken representations SX from the LLM Backbone\ninto features HX understandable to the follow-\ning Modality Generator MGX. Given the X-text\ndataset {IX, t}, t is first fed into LLM to generate\nthe corresponding SX, then mapped into HX. To\nfacilitate alignment of the mapped features HX,\nthe goal is to minimize the distance between HX\nand the conditional text representations of MGX:\narg min\n\u0398T \u2192X\nLmse(HX, \u03c4X(t)).\n(4)\nThe optimization only relies on captioning texts,\nwithout utilizing any audio or visual resources X,\nwhere HX = \u0398T\u2192X(SX) and \u03c4X is the textual\ncondition encoder in MGX. The Output Projector\nis implemented by a Tiny Transformer with a\nlearnable decoder feature sequence or MLP.\n2.5\nModality Generator\nThe Modality Generator MGX is tasked with pro-\nducing outputs in distinct modalities. Commonly,\nexisting works use off-the-shelf Latent Diffusion\nModels (LDMs) (Song et al., 2021; Bao et al., 2022;\nZhao et al., 2022), i.e., Stable Diffusion (Rombach\net al., 2022) for image synthesis, Zeroscope (Cer-\nspense, 2023) for video synthesis, and AudioLDM-\n2 (Liu et al., 2023b,c) for audio synthesis. The fea-\ntures HX mapped by the Output Projector serve as\nconditional inputs in the denoising process to gener-\nate MM content. During training, the ground truth\ncontent is first transformed into a latent feature\nz0 by the pre-trained VAE (Kingma and Welling,\n2013). Then, noise \u03f5 is added to z0 to obtain the\nnoisy latent feature zt. A pre-trained Unet (Ron-\nneberger et al., 2015) \u03f5X is used to compute the\nconditional LDM loss LX-gen as follows:\nLX-gen := E\u03f5\u223cN(0,1),t||\u03f5 \u2212 \u03f5X(zt, t, HX)||2\n2, (5)\nwhich optimizes parameters \u0398X\u2192T and \u0398T\u2192X\nby minimizing LX-gen.\n3\nTraining Pipeline\nMM-LLMs\u2019 training pipeline can be delineated\ninto two principal stages: MM PT and MM IT.\n3.1\nMM PT\nDuring the PT stage, typically leveraging the X-\nText datasets, Input and Output Projectors are\ntrained to achieve alignment among various modal-\nities by optimizing predefined objectives. For MM\nunderstanding models, optimization focuses solely\non Equation (2), while for MM generation models,\noptimization involves Equations (2), (4), and (5).\nIn the latter case, Equation (2) also includes the\nground-truth signal token sequence.\nThe X-Text datasets include Image-Text, Video-\nText, and Audio-Text, with Image-Text hav-\ning two types: Image-Text pairs (e.g., <img1>\n<txt1>) and interleaved Image-Text corpus (e.g.,\n<txt1><img1><txt2><txt3><img2><txt4>). De-\ntails of X-Text datasets are shown in Table 3 of\nAppendix G.\n3.2\nMM IT\nMM IT is a method that entails fine-tuning of\npre-trained MM-LLMs using instruction-formatted\ndatasets (Wei et al., 2021). Through this process,\nMM-LLMs can generalize to unseen tasks by ad-\nhering to new instructions, thereby enhancing zero-\nshot performance. This straightforward yet impact-\nful concept has catalyzed subsequent success in the\nfield of NLP, exemplified by works such as Instruct-\nGPT (Ouyang et al., 2022), OPT-IML (Iyer et al.,\n2022), and InstructBLIP (Dai et al., 2023).\nMM IT comprises Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human\nMM-LLMs\nFunctional Division\nUnderst.\nI+T\u2192T: BLIP-2 (Li et al., 2023e), Kosmos-1 (Huang et al., 2023c), PaLM-E (Driess et al., 2023), ViperGPT\n(Sur\u00eds et al., 2023), LLaVA (Liu et al., 2023e), MiniGPT-4 (Zhu et al., 2023a), mPLUG-Owl (Ye et al., 2023b),\nOtter (Li et al., 2023b), MultiModal-GPT (Gong et al., 2023), PandaGPT (Su et al., 2023), PaLI-X(Chen et al.)\nLLaVA-Med (Li et al., 2023d), LLaVAR (Zhang et al., 2023h), mPLUG-DocOwl(ID) (Ye et al., 2023a), DLP\n(Jian et al., 2023), ChatSpot (Zhao et al., 2023b), OpenFlamingo (Awadalla et al., 2023), Chinese-LLaVA\n(LinkSoul-AI., 2023), ASM (Wang et al., 2023c), BLIVA (hu2, 2023), IDEFICS (IDEFICS, 2023), Qwen-VL\n(Bai et al., 2023b), Kosmos-2.5 (Lv et al., 2023), InternLM-XComposer (Zhang et al., 2023f), JAM\n(Aiello et al.), LLaVA-1.5 (Liu et al., 2023d), MiniGPT-v2 (Chen et al., 2023d), Fuyu-8B (Bavishi et al., 2023),\nCogVLM(Wang et al., 2023b), mPLUG-Owl2 (Ye et al., 2023c), Monkey (Li et al., 2023l), Volcano\n(Lee et al., 2023), DRESS (Chen et al., 2023i), LION (Chen et al., 2023c), DocPedia(ID) (Feng et al., 2023),\nShareGPT4V(Chen et al., 2023f), VIM (Lu et al., 2023b), mPLUG-PaperOwl(ID)(Hu et al., 2023a), RLHF-V\n(Yu et al., 2023b), Silkie (Li et al., 2023g), Lyrics (Lu et al., 2023a), VILA (Lin et al., 2023), CogAgent\n(Hong et al., 2023), Osprey (Yuan et al., 2023a), V* (Wu and Xie, 2023), MobileVLM (Chu et al., 2023a),\nTinyGPT-V (Yuan et al.), DocLLM(ID) (Wang et al., 2023a), LLaVA-\u03d5 (Zhu et al., 2024c), Yi-VL(Team., 2023)\nKAM-CoT(Mondal et al.), InternLM-XComposer2 (Dong et al., 2024b), MoE-LLaVA (Lin et al., 2024a),\nLLaVA-MoLE (Chen et al., 2024), LLaVA-NeXT (Liu et al., 2024b), VLGuard (Zong et al., 2024),\nMobileVLM V2 (Chu et al., 2024), ViGoR(Yan et al., 2024), VisLingInstruct (Zhu et al., 2024b)\nV+T\u2192T: VideoChat (Li et al., 2023f), Video-ChatGPT (Maaz et al., 2023), Dolphins (Ma et al., 2023)\nA+T\u2192T: SALMONN (Tang et al., 2023a), Qwen-Audio (Chu et al., 2023b)\n3D+T\u2192T: 3DMIT (Li et al., 2024b)\nMany\u2192T: Flamingo (Alayrac et al., 2022), MM-REACT (Yang et al., 2023b), X-LLM (Chen et al., 2023b)\nInstructBLIP (Dai et al., 2023), EmbodiedGPT (Mu et al., 2023), Video-LLaMA (Zhang et al., 2023e), Lynx\n(Zeng et al., 2023), AnyMAL(Moon et al., 2023), LanguageBind (Zhu et al., 2024a), LLaMA-VID\n(Li et al., 2023j), X-InstructBLIP (Panagopoulou et al., 2023), InternVL (Chen et al., 2023j)\nGeneration\nI+T\u2192I+T: FROMAGe(IR) (Koh et al., 2023b), Visual ChatGPT (Wu et al., 2023a), DetGPT(IB)(Pi et al., 2023)\nGILL(Koh et al., 2023a), Kosmos-2(IB) (Peng et al., 2023), Shikra(IB) (Chen et al., 2023e), GPT4RoI(IB)\n(Zhang et al., 2023g), SEED (Ge et al., 2023), LISA(IM) (Lai et al., 2023), VisCPM(Hu et al., 2023b),\nCM3Leon(Yu et al., 2023a), LaVIT (Jin et al., 2024), DreamLLM (Dong et al., 2024a), MiniGPT-5\n(Zheng et al., 2023b), Kosmos-G (Pan et al., 2023), GLaMM(IM) (Rasheed et al., 2023), LLaVA-Plus(+IB&IM)\n(Liu et al., 2023f), PixelLM(IM) (Ren et al., 2023), VL-GPT (Zhu et al., 2023b), CLOVA(+IB&IM)\n(Gao et al., 2023b), Emu-2 (Sun et al., 2023a), MM-Interleaved (Tian et al., 2024), DiffusionGPT\n(Qin et al., 2024), RPG(Yang et al., 2024),Vary-toy(IB) (Wei et al., 2024), CogCoM(IB) (Qi et al., 2024),\nSPHINX-X(IB) (Gao et al., 2024)\nA/S+T\u2192A/S+T: SpeechGPT (Zhang et al., 2023a), AudioPaLM (Rubenstein et al., 2023)\nMany\u2192I+T: Emu (Sun et al., 2024), BuboGPT(IM) (Zhao et al., 2023d), GroundingGPT(IB) (Li et al., 2024c)\nMany\u2192Many: GPT-4 (OpenAI, 2023), HuggingGPT (Shen et al., 2023), AudioGPT (Huang et al., 2023b)\nNExT-GPT (Wu et al., 2023d), ControlLLM (Liu et al., 2023i), TEAL (Yang et al., 2023a), CoDi-2(Tang et al.)\nGemini (Team et al., 2023), ModaVerse (Wang et al., 2024c), MLLM-Tool(Wang et al., 2024a)\nDesign Division\nTool-using\nVisual ChatGPT(Wu et al., 2023a), ViperGPT(Sur\u00eds et al., 2023), MM-REACT(Yang et al., 2023b),\nHuggingGPT (Shen et al., 2023),AudioGPT (Huang et al., 2023b), ControlLLM (Liu et al., 2023i), LLaVA-Plus\n(Liu et al.), CogAgent (Hong et al., 2023),CLOVA (Gao et al., 2023b), \u03b1-UMi (Shen et al., 2024), MLLM-Tool\n(Wang et al.), WebVoyager (He et al., 2024), Mobile-Agent (Wang et al., 2024b)\nEnd-to-end\nThe remaining models are essentially all end-to-end trainable models.\nFigure 3: Taxonomy for MM-LLMs. I: Image, V: Video, A/S: Audio/Speech, and T: Text. ID: Document\nunderstanding, IB: Output bounding box, IM: Output segmentation mask, and IR: Output retrieved images.\nFeedback (RLHF), aiming to align with human in-\ntents and enhance the interaction capabilities of\nMM-LLMs. SFT converts part of the PT stage\ndata into an instruction-aware format. Using vi-\nsual Question-Answer (QA) as an example, var-\nious templates may be employed like (1) \"<Im-\nage>{Question}\" A short answer to the question\nis; (2) \"<Image>\" Examine the image and re-\nspond to the following question with a brief answer:\n\"{Question}. Answer:\"; and so on. Next, it fine-\ntunes pre-trained MM-LLMs using the same opti-\nmization objectives. SFT datasets can be structured\nas either single-turn QA or multi-turn dialogues.\nAfter SFT, RLHF involves further fine-tuning\nof the model, relying on feedback regarding the\nMM-LLMs\u2019 responses (e.g., Natural Language\nFeedback (NLF) labeled manually or automati-\ncally) (Sun et al., 2023b). This process employs\na reinforcement learning algorithm to effectively\nintegrate the non-differentiable NLF. The model is\ntrained to generate corresponding responses con-\nditioned on the NLF (Chen et al., 2023i; Aky\u00fcrek\net al., 2023). The statistics for SFT and RLHF\ndatasets are presented in Table 4 of Appendix G.\nThe datasets used by existing MM-LLMs in the\nMM PT and MM IT stages are diverse, but they are\nall subsets of the datasets in Tables 3 and 4.\n4\nSOTA MM-LLMs\nAs shown in Figure 3, we classify the 122 SOTA\nMM-LLMs from both functional and design per-\nspectives. In the design division, \u201cTool-using\u201d de-\nnotes treating the LLM as black box and provid-\ning access to certain MM expert systems to per-\nform specific MM tasks via reasoning, while \u201cEnd-\nto-End\u201d signifies that the entire model is trained\njointly in an end-to-end manner. Based on the\npreviously defined design formulations, we also\nModel\nI\u2192O\nModality Encoder\nInput Projector\nLLM Backbone\nOutput Projector\nModality Generator\n#.PT\n#.IT\nFlamingo\nI+V+T\u2192T\nI/V: NFNet-F6\nCross-attention\nChinchilla-1.4B/7B/70B\n\u2013\n\u2013\n\u2013\n\u2013\nBLIP-2\nI+T\u2192T\nI: CLIP/Eva-CLIP ViT@224\nQ-Former w/ Linear Projector\nFlan-T5/OPT\n\u2013\n\u2013\n129M\n\u2013\nLLaVA\nI+T\u2192T\nI: CLIP ViT-L/14\nLinear Projector\nVicuna-7B/13B\n\u2013\n\u2013\n\u2013\n\u2013\nMiniGPT-4\nI+T\u2192T\nI: Eva-CLIP ViT-G/14\nQ-Former w/ Linear Projector\nVicuna-13B\n\u2013\n\u2013\n\u2013\n\u2013\nmPLUG-Owl\nI+T\u2192T\nI: CLIP ViT-L/14\nCross-attention\nLLaMA-7B\n\u2013\n\u2013\n\u2013\n\u2013\nOtter\nI+T\u2192T\nI: CLIP ViT-L/14\nCross-attention\nLLaMA-7B\n\u2013\n\u2013\n\u2013\n\u2013\nX-LLM\nI+V+A+T\u2192T\nI/V: ViT-G; A: C-Former\nQ-Former w/ Linear Projector\nChatGLM-6B\n\u2013\n\u2013\n\u2013\n\u2013\nVideoChat\nV+T\u2192T\nI: ViT-G\nQ-Former w/ Linear Projector\nVicuna\n\u2013\n\u2013\n\u2013\n\u2013\nInstructBLIP\nI+V+T\u2192T\nI/V: ViT-G/14@224\nQ-Former w/ Linear Projector\nFlan-T5/Vicuna\n\u2013\n\u2013\n129M\n1.2M\nPandaGPT\nI+T\u2192T\nI: ImageBind\nLinear Projector\nVicuna-13B\n\u2013\n\u2013\n\u2013\n\u2013\nGILL\nI+T\u2192I+T\nI: CLIP ViT-L\nLinear Projector\nOPT-6.7B\nTiny Transformer\nI: Stable Diffusion-1.5\n\u2013\n\u2013\nPaLI-X\nI+T\u2192T\nI: ViT\nLinear Projector\nUL2-32B\n\u2013\n\u2013\n\u2013\n\u2013\nVideo-LLaMA\nI+V+A+T\u2192T\nI/V: Eva-CLIP ViT-G/14; A: ImageBind\nQ-Former w/ Linear Projector\nVicuna/LLaMA\n\u2013\n\u2013\n\u2013\n\u2013\nVideo-ChatGPT\nV+T\u2192T\nI: CLIP ViT-L/14\nLinear Projector\nVicuna-v1.1\n\u2013\n\u2013\n\u2013\n\u2013\nShikra\nI+T\u2192T+IB\nI: CLIP ViT-L/14@224\nLinear Projector\nVicuna-7B/13B\n\u2013\n\u2013\n600K\n5.5M\nLLaVAR\nI+T\u2192T\nI: CLIP ViT-L/14@224 & CLIP ViT-L/14@336\nLinear Projector\nVicuna-13B\n\u2013\n\u2013\n\u2013\n\u2013\nmPLUG-DocOwl\nID+T\u2192T\nI: CLIP ViT-L/14\nCross-attention\nLLaMA-7B\n\u2013\n\u2013\n\u2013\n\u2013\nLynx\nI+V+T\u2192T\nI/V: Eva-CLIP ViT-1B\nCross-attention\nVicuna\n\u2013\n\u2013\n\u2013\n\u2013\nEmu\nI+V+T\u2192I+T\nI/V: Eva-CLIP-1B\nCross-attention\nLLaMA-13B\nMLP\nI: Stable Diffusion-1.5\n\u2013\n\u2013\nDLP\nI+T\u2192T\nI: CLIP/Eva-CLIP ViT\nQ-Former+P-Former w/ Linear Projector\nOPT/Flan-T5\n\u2013\n\u2013\n\u2013\n\u2013\nBuboGPT\nI+A+T\u2192T+IM\nI: CLIP/Eva-CLIP ViT; A: ImageBind\nQ-Former w/ Linear Projector\nVicuna\n\u2013\n\u2013\n\u2013\n\u2013\nChatSpot\nI+T\u2192T\nI: CLIP ViT-L/14\nLinear Projector\nVicuna-7B/LLaMA\n\u2013\n\u2013\n\u2013\n\u2013\nIDEFICS\nI+T\u2192T\nI: OpenCLIP\nCross-attention\nLLaMA\n\u2013\n\u2013\n\u2013\n\u2013\nQwen-VL-(Chat)\nI+T\u2192T\nI: ViT@448 initialized from OpenClip\u2019s ViT-bigG\nCross-attention\nQwen-7B\n\u2013\n\u2013\n1.4B\u2020\n50M\u2020\nLaVIT\nI+T\u2192I+T\nI: ViT\nCross-attention\nLLaMA-7B\n\u2013\nI: Stable Diffusion\n\u2013\n\u2013\nNExT-GPT\nI+V+A+T\u2192I+V+A+T\nI/V/A: ImageBind\nLinear Projector\nVicuna-7B\nTiny Transformer\nI: Stable Diffusion; V: Zeroscope; A: AudioLDM\n\u2013\n\u2013\nDreamLLM\nI+T\u2192I+T\nI: CLIP ViT-L\nLinear Projector\nVicuna\nMLP\nI: Stable Diffusion\n\u2013\n\u2013\nAnyMAL\nI+V+A+T\u2192T\nI: CLIP ViT/L & ViT-G & DinoV2; V: Intervideo; A: CLAP\nI/V: Cross-attention; A: Linear Projector\nLLaMA-2\n\u2013\n\u2013\n\u2013\n\u2013\nMiniGPT-5\nI+T\u2192I+T\nI: Eva-CLIP ViT-G/14\nQ-Former w/ Linear Projector\nVicuna-7B\nTiny Transformer w/ MLP\nI: StableDiffusion-2\n\u2013\n\u2013\nLLaVA-1.5\nI+T\u2192T\nI: CLIP ViT-L@336\nMLP\nVicuna-v1.5-7B/13B\n\u2013\n\u2013\n0.6M\n0.7M\nMiniGPT-v2\nI+T\u2192T\nI: Eva-CLIP ViT@448\nLinear Projector\nLLaMA-2-Chat-7B\n\u2013\n\u2013\n\u2013\n\u2013\nCogVLM\nI+T\u2192T\nI: Eva-2-CLIP ViT\nMLP\nVicuna-v1.5-7B\n\u2013\n\u2013\n\u2013\n\u2013\nQwen-Audio\nA+T\u2192T\nA: Whisper-L-v2\nLinear Projector\nQwen-7B\n\u2013\n\u2013\n\u2013\n\u2013\nDRESS\nI+T\u2192T\nI:Eva-CLIP ViT-G/14\nLinear Projector\nVicuna-v1.5-13B\n\u2013\n\u2013\n\u2013\n\u2013\nX-InstructBLIP\nI+V+A+3D+T\u2192T\nI/V: Eva-CLIP ViT-G/14; A: BEATs; 3D: ULIP-2\nQ-Former w/ Linear Projector\nVicuna-v1.1-7B/13B\n\u2013\n\u2013\n\u2013\n\u2013\nCoDi-2\nI+V+A+T\u2192I+V+A+T\nI/V/A: ImageBind\nMLP\nLLaMA-2-Chat-7B\nMLP\nI: Stable Diffusion-2.1; V: Zeroscope-v2; A: AudioLDM-2\n\u2013\n\u2013\nRLHF-V\nI+T\u2192T\nI: BEiT-3\nLinear Projector\nVicuna-v1-13B\n\u2013\n\u2013\n\u2013\n\u2013\nSilkie\nI+T\u2192T\nI: ViT initialized from OpenCLIP\u2019s ViT-bigG\nCross-attention\nQwen-7B\n\u2013\n\u2013\n\u2013\n\u2013\nLyrics\nI+T\u2192T\nI: CLIP ViT-L/14 & Grounding-DINO-T w/ Swin-T &\nSAM-HQ w/ MAE & ViT-H & RAM++ w/ Swin-B\nMQ-Former w/ Linear Projection\nVicuna-13B\n\u2013\n\u2013\n\u2013\n\u2013\nVILA\nI+T\u2192T\nI: ViT@336\nLinear Projector\nLLaMA-2-7B/13B\n\u2013\n\u2013\n50M\n1M\nIntrenVL\nI+V+T\u2192T\nI/V: InternViT-6B; T: LLaMA-7B\nCross-attention w/ MLP\nQLLaMA-8B & Vicuna-13B\n\u2013\n\u2013\n\u2013\n\u2013\nModaVerse\nI+V+A+T\u2192I+V+A+T\nImageBind\nLinear Projector\nLLaMA-2\nMLP\nI: Stable Diffusion; V: Videofusion; A: AudioLDM\n\u2013\n\u2013\nMM-Interleaved\nI+T\u2192I+T\nI: CLIP ViT-L/14\nCross-attention\nVicuna-13B\nTiny Transformer\nI: Stable Diffusion-2.1\n\u2013\n\u2013\nTable 1: The summary of 43 mainstream MM-LLMs. I\u2192O: Input to Output Modalities, I: Image, V: Video, A:\nAudio, 3D: Point Cloud, and T: Text. In Modality Encoder, \u201c-L\u201d represents Large, \u201c-G\u201d represents Giant, \u201c/14\u201d\nindicates a patch size of 14, and \u201c@224\u201d signifies an image resolution of 224 \u00d7 224. #.PT and #.IT represent the\nscale of dataset during MM PT and MM IT, respectively. \u2020 includes in-house data that is not publicly accessible.\nconduct a comprehensive comparison of the archi-\ntectures and training dataset scales for 43 of these\nSOTA MM-LLMs, as illustrated in Table 1. Next,\nwe will summarize their developmental trends and\nbriefly introduce the core contributions of some\nrepresentative models in Appendix E.\nTrends in Existing MM-LLMs:\n(1) Progressing\nfrom a dedicated emphasis on MM understanding\nto the generation of specific modalities and further\nevolving into any-to-any modality conversion (e.g.,\nMiniGPT-4 \u2192 MiniGPT-5 \u2192 NExT-GPT); (2) Ad-\nvancing from MM PT to SFT and then to RLHF,\nthe training pipeline undergoes continuous refine-\nment, striving to better align with human intent\nand enhance the model\u2019s conversational interac-\ntion capabilities (e.g., BLIP-2 \u2192 InstructBLIP \u2192\nDRESS); (3) Embracing Diversified Modal Exten-\nsions (e.g., BLIP-2 \u2192 X-LLM and InstructBLIP\n\u2192 X-InstructBLIP); (4) Incorporating a Higher-\nQuality Training Dataset (e.g., LLaVA \u2192 LLaVA-\n1.5); (5) Adopting a More Efficient Model Architec-\nture, transitioning from complex Q- and P-Former\ninput projector modules in BLIP-2 and DLP to a\nsimpler yet effective linear projector in VILA.\n5\nBenckmarks and Performance\nTo offer a comprehensive performance comparison,\nwe have compiled a table featuring major MM-\nLLMs across 18 Vision-Language (VL) bench-\nmarks gathered from various papers (Li et al.,\n2023e; Chen et al., 2023d,f; Lin et al., 2023), shown\nin Table 2. The information of these benchmarks\ncan be found in Appendix F. Next, we will extract\ntraining recipes that boost the effectiveness of MM-\nLLMs, drawing insights from SOTA models.\nTraining Recipes\nFirstly, higher image resolu-\ntion can incorporate more visual details for the\nmodel, benefiting tasks that require fine-grained\ndetails. For example, LLaVA-1.5 and VILA em-\nploy a resolution of 336 \u00d7 336, while Qwen-VL\nand MiniGPT-v2 utilize 448 \u00d7 448.\nHowever,\nhigher resolutions lead to longer token sequences,\nincurring additional training and inference costs.\nMiniGPT-v2 addresses this by concatenating 4 adja-\ncent visual tokens in the embedding space to reduce\nlength. Recently, Monkey (Li et al., 2023l) pro-\nposed a solution to enhance the resolution of input\nimages without retraining a high-resolution visual\nencoder, utilizing only a low-resolution visual en-\ncoder, supporting resolutions up to 1300 \u00d7 800. To\nenhance the understanding of rich-text images, ta-\nbles, and document content, DocPedia (Feng et al.,\n2023) introduced a method to increase the visual\nencoder resolution to 2560 \u00d7 2560, overcoming\nthe limitations of poorly performing low resolu-\ntions in open-sourced ViT. Secondly, the incorpo-\nration of high-quality SFT data can significantly im-\nprove performance in specific tasks, as evidenced\nModel\nLLM Backbone\nOKVQA\nIconVQA\nVQAv2\nGQA\nVizWiz\nSQAI\nVQAT\nPOPE\nMMEP\nMMEC\nMMB\nMMBCN\nSEEDI\nLLaVAW\nMM-Vet\nQBench\nHM\nVSR\nFlamingo\nChinchilla-7B\n44.7\n\u2013\n\u2013\n\u2013\n28.8\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n57.0\n31.8\nBLIP-2\nFlan-T5XXL(13B)\n45.9\n40.6\n65.0\n44.7\n19.6\n61.0\n42.5\n85.3\n1293.8\n290.0\n\u2013\n\u2013\n46.4\n38.1\n22.4\n\u2013\n53.7\n50.9\nLLaVA\nVicuna-13B\n54.4\n43.0\n\u2013\n41.3\n\u2013\n\u2013\n38.9\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n51.2\nMiniGPT-4\nVicuna-13B\n37.5\n37.6\n\u2013\n30.8\n\u2013\n\u2013\n19.4\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n41.6\nInstructBLIP\nVicuna-7B\n\u2013\n\u2013\n\u2013\n49.2\n34.5\n60.5\n50.1\n\u2013\n\u2013\n\u2013\n36.0\n23.7\n53.4\n60.9\n26.2\n56.7\n\u2013\n\u2013\nInstructBLIP\nVicuna-13B\n\u2013\n44.8\n\u2013\n49.5\n33.4\n63.1\n50.7\n78.9\n1212.8\n291.8\n\u2013\n\u2013\n\u2013\n58.2\n25.6\n\u2013\n57.5\n52.1\nShikra\nVicuna-13B\n47.2\n\u2013\n77.4\u2217\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n58.8\n\u2013\n\u2013\n\u2013\n\u2013\n54.7\n\u2013\n\u2013\nIDEFICS-9B\nLLaMA-7B\n\u2013\n\u2013\n50.9\n38.4\n35.5\n\u2013\n25.9\n\u2013\n\u2013\n\u2013\n48.2\n25.2\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nIDEFICS-80B\nLLaMA-65B\n\u2013\n\u2013\n60.0\n45.2\n36.0\n\u2013\n30.9\n\u2013\n\u2013\n\u2013\n54.5\n38.1\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nQwen-VL\nQwen-7B\n\u2013\n\u2013\n78.8\u2217\n59.3\u2217\n35.2\n67.1\n63.8\n\u2013\n\u2013\n\u2013\n38.2\n7.4\n56.3\n\u2013\n\u2013\n59.4\n\u2013\n\u2013\nQwen-VL-Chat\nQwen-7B\n\u2013\n\u2013\n78.2\u2217\n57.5\u2217\n38.9\n68.2\n61.5\n\u2013\n1487.5\n360.7\n60.6\n56.7\n58.2\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nLLaVA-1.5\nVicuna-1.5-7B\n\u2013\n\u2013\n78.5\u2217\n62.0\u2217\n50.0\n66.8\n58.2\n85.9\n1510.7\n316.1\u2021\n64.3\n58.3\n58.6\n63.4\n30.5\n58.7\n\u2013\n\u2013\n+ShareGPT4V\nVicuna-1.5-7B\n\u2013\n\u2013\n80.6\n\u2013\n57.2\n68.4\n\u2013\n\u2013\n1567.4\n376.4\n68.8\n62.2\n69.7\n72.6\n37.6\n63.4\n\u2013\n\u2013\nLLaVA-1.5\nVicuna-1.5-13B\n\u2013\n\u2013\n80.0\u2217\n63.3\u2217\n53.6\n71.6\n61.3\n85.9\n1531.3\n295.4\u2021\n67.7\n63.6\n61.6\n70.7\n35.4\n62.1\n\u2013\n\u2013\nMiniGPT-v2\nLLaMA-2-Chat-7B\n56.9\n47.7\n\u2013\n60.3\n30.3\n\u2013\n51.9\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n58.2\n60.6\nMiniGPT-v2-Chat\nLLaMA-2-Chat-7B\n55.9\n49.4\n\u2013\n58.8\n42.4\n\u2013\n52.3\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n59.5\n63.3\nVILA-7B\nLLaMA-2-7B\n\u2013\n\u2013\n79.9\u2217\n62.3\u2217\n57.8\n68.2\n64.4\n85.5\n1533.0\n\u2013\n68.9\n61.7\n61.1\n69.7\n34.9\n\u2013\n\u2013\n\u2013\nVILA-13B\nLLaMA-2-13B\n\u2013\n\u2013\n80.8\u2217\n63.3\u2217\n60.6\n73.7\n66.6\n84.2\n1570.1\n\u2013\n70.3\n64.3\n62.8\n73.0\n38.8\n\u2013\n\u2013\n\u2013\n+ShareGPT4V\nLLaMA-2-13B\n\u2013\n\u2013\n80.6\u2217\n63.2\u2217\n62.4\n73.1\n65.3\n84.8\n1556.5\n\u2013\n70.8\n65.4\n61.4\n78.4\n45.7\n\u2013\n\u2013\n\u2013\nTable 2: Comparison of mainstream MM-LLMs on 18 VL benchmarks. The red denotes the highest result, and the\nblue denotes the second highest result. \u2021 indicates ShareGPT4V\u2019s (Chen et al., 2023f) re-implemented test results,\nwhich are missed in benchmarks or origin papers. \u2217 indicates that training images are observed during training.\nby the addition of ShareGPT4V data to LLaVA-1.5\nand VILA-13B, as shown in Table 2. Moreover,\nVILA reveals several key findings: (1) Performing\nPEFT on the LLM Backbone promotes deep em-\nbedding alignment, crucial for ICL; (2) Interleaved\nImage-Text data proves beneficial, whereas Image-\nText pairs alone are sub-optimal; (3) Re-blending\ntext-only instruction data (e.g., unnatural instruc-\ntion (Honovich et al., 2022)) with image-text data\nduring SFT not only addresses the degradation of\ntext-only tasks but also enhances VL task accuracy.\n6\nFuture Directions\nIn this section, we explore promising future direc-\ntions for MM-LLMs across the following aspects:\nMore Powerful Models\nWe can enhance the\nMM-LLMs\u2019 strength from the following four key\navenues: (1) Expanding Modalities: Current MM-\nLLMs mainly support the following modalities:\nimage, video, audio, 3D, and text. However, the\nreal world involves a broader range of modalities.\nExtending MM-LLMs to accommodate additional\nmodalities (e.g., web pages, heat maps, and fig-\nures&tables) will increase the model\u2019s versatility,\nmaking it more universally applicable; (2) Diver-\nsifying LLMs: Incorporating various types and\nsizes of LLMs provides practitioners with the flexi-\nbility to select the most appropriate one based on\ntheir specific requirements; (3) Improving MM\nIT Dataset Quality: Current MM IT datasets have\nample room for improvement and expansion. Di-\nversifying the range of instructions can enhance\nthe effectiveness of MM-LLMs in understanding\nand executing user commands; (4) Strengthening\nMM Generation Capabilities: Most current MM-\nLLMs are predominantly oriented towards MM\nunderstanding. Although some models have incor-\nporated MM generation capabilities, the quality of\ngenerated responses may be constrained by the ca-\npacities of the LDMs. Exploring the integration of\nretrieval-based approaches (Asai et al., 2023; Gao\net al., 2023a) holds significant promise in comple-\nmenting the generative process, potentially enhanc-\ning the overall performance of the model.\nMore Challenging Benchmarks\nExisting bench-\nmarks might not adequately challenge the capa-\nbilities of MM-LLMs, given that many datasets\nhave previously appeared to varying degrees in\nthe PT or IT sets.\nThis implies that the mod-\nels may have learned these tasks during training.\nMoreover, current benchmarks predominantly con-\ncentrate on the VL sub-field. Thus, it is crucial\nfor the development of MM-LLMs to construct\na more challenging, larger-scale benchmark that\nincludes more modalities and uses a unified eval-\nuation standard. For instance, GOAT-Bench (Lin\net al., 2024b) is introduced to assess the capability\nof various MM-LLMs in discerning and respond-\ning to nuanced aspects of social abuse depicted in\nmemes. MathVista (Lu et al., 2024) evaluates the\nmath reasoning ability of MM-LLMs within visual\ncontexts. Moreover, MMMU (Yue et al., 2023) and\nCMMMU (Zhang et al., 2024) have respectively\nintroduced English and Chinese versions of the\nmassive multi-discipline MM understanding and\nreasoning benchmark for expert artificial general\nintelligence. Fan et al. have also challenged MM-\nLLMs with multipanel VQA. BenchLMM (Cai\net al., 2023) benchmarks the cross-style visual capa-\nbility of MM-LLMs. Additionally, Liu et al. have\nconducted an in-depth study on the optical charac-\nter recognition capabilities of MM-LLMs.\nMobile/Lightweight Deployment\nTo deploy\nMM-LLMs on resource-constrained platforms and\nachieve optimal performance meanwhile, such as\nlow-power mobile and IoT devices, lightweight\nimplementations are of paramount importance.\nA notable advancement in this realm is Mo-\nbileVLM (Chu et al., 2023a). This approach strate-\ngically downscales LLaMA, allowing for seam-\nless off-the-shelf deployment. MobileVLM fur-\nther introduces a lightweight downsample pro-\njector, consisting of fewer than 20 million pa-\nrameters, contributing to improved computational\nspeed.\nRecently, there have been many simi-\nlar studies on lightweighting MM-LLMs, achiev-\ning efficient computation and inference with com-\nparable performance or minimal loss, including\nTinyGPT-V (Yuan et al., 2023b), Vary-toy (Wei\net al., 2024), Mobile-Agent (Wang et al., 2024b),\nMoE-LLaVA (Lin et al., 2024a), and MobileVLM\nV2 (Chu et al., 2024). Nevertheless, this avenue\nnecessitates additional exploration for further ad-\nvancements in development.\nEmbodied Intelligence\nThe embodied intelli-\ngence aims to replicate human-like perception and\ninteraction with the surroundings by effectively\nunderstanding the environment, recognizing perti-\nnent objects, assessing their spatial relationships,\nand devising a comprehensive task plan (Firoozi\net al., 2023). Embodied AI tasks, such as embod-\nied planning, embodied visual question answer-\ning, and embodied control, equip robots to au-\ntonomously implement extended plans by leverag-\ning real-time observations. Some typical works in\nthis area are PaLM-E (Driess et al., 2023) and Em-\nbodiedGPT (Mu et al., 2023). PaLM-E introduces\na multi-embodiment agent through the training of\na MM-LLM. Beyond functioning solely as an em-\nbodied decision maker, PaLM-E also demonstrates\nproficiency in handling general VL tasks. Em-\nbodiedGPT introduces an economically efficient\nmethod characterized by a CoT approach, enhanc-\ning the capability of embodied agents to engage\nwith the real world and establishing a closed loop\nthat connects high-level planning with low-level\ncontrol. While MM-LLM-based Embodied Intelli-\ngence has made advancements in integrating with\nrobots, further exploration is needed to enhance the\nautonomy of robots.\nContinual Learning\nDue to the large training\ncosts associated with their massive scale, MM-\nLLMs are not amenable to frequent re-training.\nHowever, updates are necessary to endow MM-\nLLMs with new skills and keep them up-to-date\nwith rapidly evolving human knowledge (Wu et al.,\n2024). Thus, Continual Learning (CL) is needed to\nmake the model flexible enough to efficiently and\ncontinually leverage emerging data while avoiding\nthe substantial cost of retraining MM-LLMs. CL\nfor MM-LLMs can be classified into two stages:\ncontinual PT and continual IT. Recently, a contin-\nual MM IT benchmark has been proposed to con-\ntinuously fine-tune MM-LLMs for new MM tasks\nwhile maintaining superior performance on tasks\nlearned during the original MM IT stage (He et al.,\n2023). It introduces two primary challenges: (1)\ncatastrophic forgetting, where models forget previ-\nous knowledge when learning new tasks (Robins,\n1995; McCloskey and Cohen, 1989; Goodfellow\net al., 2013; Zhang et al., 2023d,c,b; Zheng et al.,\n2023a), and (2) negative forward transfer, indicat-\ning that the performance of unseen tasks declines\nwhen learning new ones (Zheng et al., 2024)\nMitigating Hallucination\nHallucinations entail\ngenerating textual descriptions of nonexistent ob-\njects without visual cues, which manifest in diverse\ncategories (Liu et al., 2024a) such as misjudgments\nand inaccuracies in descriptions. The origins of\nthese hallucinations are multifaceted (Liu et al.,\n2024a), including biases and annotation errors in\ntraining data. Additionally, Skip \\n (Han et al.,\n2024) highlights semantic drift biases associated\nwith paragraph separators, which can induce hal-\nlucinations when deliberately inserted. Current\nmethods to mitigate these hallucinations involve\nleveraging self-feedback as visual cues (Lee et al.,\n2023). However, challenges persist, necessitating\nnuanced discernment between accurate and halluci-\nnatory outputs, as well as advancements in training\nmethodologies to enhance output reliability.\n7\nConclusion\nIn this paper, we have presented a comprehensive\nsurvey of MM-LLMs with a focus on recent ad-\nvancements. Initially, we categorize the model\narchitecture into five components, providing a de-\ntailed overview of general design formulations and\ntraining pipelines. Subsequently, we introduce var-\nious SOTA MM-LLMs, each distinguished by its\nspecific formulations. Our survey also sheds light\non their capabilities across diverse MM bench-\nmarks and envisions future developments in this\nrapidly evolving field. We hope this survey can\nprovide insights for researchers, contributing to the\nongoing advancements in the MM-LLMs domain.\nLimitations\nIn this paper, we embark on a comprehensive explo-\nration of the current MM-LLMs landscape, present-\ning a synthesis from diverse perspectives enriched\nby our insights. Acknowledging the dynamic na-\nture of this field, it is plausible that certain aspects\nmay have eluded our scrutiny, and recent advances\nmight not be entirely encapsulated. To tackle this\ninherent challenge, we\u2019ve established a dedicated\nwebsite for real-time tracking, using crowdsourc-\ning to capture the latest advancements. Our goal is\nfor this platform to evolve into a continuous source\nof contributions propelling ongoing development\nin the field. Given the constraints of page limits,\nwe are unable to delve into all technical details and\nhave provided concise overviews of the core contri-\nbutions of mainstream MM-LLMs. Looking ahead,\nwe commit to vigilant monitoring and continual\nenhancement of relevant details on our website,\nincorporating fresh insights as they emerge.\nReferences\n2023. Bliva: A simple multimodal llm for better han-\ndling of text-rich visual questions. arXiv preprint\narXiv:2308.09936.\nEmanuele Aiello, Lili Yu, Yixin Nie, Armen Agha-\njanyan, and Barlas Oguz. 2023. Jointly Training\nLarge Autoregressive Multimodal Models. arXiv\npreprint arXiv:2309.15564.\nHassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\n2021.\nVatt:\nTransformers for multimodal self-\nsupervised learning from raw video, audio and text.\nAdvances in Neural Information Processing Systems,\n34:24206\u201324221.\nAfra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan,\nAshwin Kalyan, Peter Clark, Derry Wijaya, and\nNiket Tandon. 2023.\nRL4F: Generating Natu-\nral Language Feedback with Reinforcement Learn-\ning for Repairing Model Outputs. arXiv preprint\narXiv:2305.08844.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716\u201323736.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41\u201346.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hes-\nsel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\n2023. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023a. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023b. Qwen-VL: A Frontier\nLarge Vision-Language Model with Versatile Abili-\nties. CoRR, abs/2308.12966.\nMax Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zis-\nserman. 2021. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In Proceed-\nings of the IEEE/CVF International Conference on\nComputer Vision, pages 1728\u20131738.\nFan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. 2022.\nAnalytic-DPM: an Analytic Estimate of the Optimal\nReverse Variance in Diffusion Probabilistic Models.\nIn International Conference on Learning Representa-\ntions.\nRohan Bavishi,\nErich Elsen,\nCurtis Hawthorne,\nMaxwell Nye, Augustus Odena, Arushi Somani, and\nSa\u02d8gnak Ta\u00b8s\u0131rlar. 2023. Introducing our Multimodal\nModels.\nAli Furkan Biten, Ron Litman, Yusheng Xie, Srikar\nAppalaraju, and R Manmatha. 2022. Latr: Layout-\naware transformer for scene-text vqa. In Proceedings\nof the IEEE/CVF conference on computer vision and\npattern recognition, pages 16548\u201316558.\nAndy Brock, Soham De, Samuel L Smith, and Karen Si-\nmonyan. 2021. High-performance large-scale image\nrecognition without normalization. In International\nConference on Machine Learning, pages 1059\u20131071.\nPMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nMinwoo Byeon, Beomhee Park, Haecheon Kim,\nSungjun Lee, Woonhyuk Baek, and Saehoon Kim.\n2022. Coyo-700m: Image-text pair dataset.\nFabian Caba Heilbron, Victor Escorcia, Bernard\nGhanem, and Juan Carlos Niebles. 2015. Activitynet:\nA large-scale video benchmark for human activity\nunderstanding. In Proceedings of the ieee conference\non computer vision and pattern recognition, pages\n961\u2013970.\nRizhao Cai, Zirui Song, Dayan Guan, Zhenhao\nChen, Xing Luo, Chenyu Yi, and Alex Kot. 2023.\nBenchLMM: Benchmarking cross-style visual capa-\nbility of large multimodal models. arXiv preprint\narXiv:2312.02896.\nCerspense. 2023. Zeroscope: Diffusion-based text-to-\nvideo synthesis.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 3558\u20133568.\nFei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-\nYi Chen, Jing Shi, Shuang Xu, and Bo Xu. 2023a.\nVlp: A survey on vision-language pre-training. Ma-\nchine Intelligence Research, 20(1):38\u201356.\nFeilong Chen, Minglun Han, Haozhi Zhao, Qingyang\nZhang, Jing Shi, Shuang Xu, and Bo Xu. 2023b. X-\nllm: Bootstrapping advanced large language models\nby treating multi-modalities as foreign languages.\narXiv preprint arXiv:2305.04160.\nGongwei Chen, Leyang Shen, Rui Shao, Xiang Deng,\nand Liqiang Nie. 2023c. LION: Empowering Multi-\nmodal Large Language Model with Dual-Level Vi-\nsual Knowledge. arXiv preprint arXiv:2311.11860.\nJun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu, Pengchuan Zhang, Raghuraman Krishnamoor-\nthi, Vikas Chandra, Yunyang Xiong, and Mohamed\nElhoseiny. 2023d. Minigpt-v2: large language model\nas a unified interface for vision-language multi-task\nlearning. arXiv preprint arXiv:2310.09478.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. 2023e. Shikra: Unleash-\ning Multimodal LLM\u2019s Referential Dialogue Magic.\narXiv preprint arXiv:2306.15195.\nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-\nghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin. 2023f. ShareGPT4V: Improving Large Multi-\nModal Models with Better Captions. arXiv preprint\narXiv:2311.12793.\nSanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu,\nDaniel Tompkins, Zhuo Chen, Wanxiang Che, Xi-\nangzhan Yu, and Furu Wei. 2023g. BEATs: Audio\nPre-Training with Acoustic Tokenizers. In Interna-\ntional Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, pages\n5178\u20135193.\nShaoxiang Chen, Zequn Jie, and Lin Ma. 2024. LLaVA-\nMoLE: Sparse Mixture of LoRA Experts for Mitigat-\ning Data Conflicts in Instruction Finetuning MLLMs.\narXiv preprint arXiv:2401.16160.\nShoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,\nYibing Song, Jue Wang, and Ping Luo. 2022a. Adapt-\nformer: Adapting vision transformers for scalable\nvisual recognition. Advances in Neural Information\nProcessing Systems, 35:16664\u201316678.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil\nMustafa, Soravit Changpinyo, Jialin Wu, Car-\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang,\nYi Tay, et al. 2023h. PaLI-X: On Scaling up a Multi-\nlingual Vision and Language Model. arXiv preprint\narXiv:2305.18565.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022b.\nPali: A jointly-scaled mul-\ntilingual language-image model.\narXiv preprint\narXiv:2209.06794.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server. arXiv preprint\narXiv:1504.00325.\nYangyi Chen, Karan Sikka, Michael Cogswell, Heng\nJi, and Ajay Divakaran. 2023i. Dress: Instructing\nlarge vision-language models to align and interact\nwith humans via natural language feedback. arXiv\npreprint arXiv:2311.10081.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Zhong Muyan, Qinglong Zhang,\nXizhou Zhu, Lewei Lu, et al. 2023j.\nInternvl:\nScaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. arXiv preprint\narXiv:2312.14238.\nMehdi Cherti, Romain Beaumont, Ross Wightman,\nMitchell Wortsman, Gabriel Ilharco, Cade Gordon,\nChristoph Schuhmann, Ludwig Schmidt, and Jenia\nJitsev. 2023.\nReproducible scaling laws for con-\ntrastive language-image learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2818\u20132829.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-\nSource Chatbot Impressing GPT-4 with 90%* Chat-\nGPT Quality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1\u2013113.\nXiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang\nXu, Yang Yang, Yiming Hu, Fei Wei, Xinyu\nZhang, Bo Zhang, Xiaolin Wei, et al. 2023a. Mo-\nbilevlm: A fast, reproducible and strong vision lan-\nguage assistant for mobile devices. arXiv preprint\narXiv:2312.16886.\nXiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang\nXu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu,\nXinyang Lin, Bo Zhang, et al. 2024. MobileVLM V2:\nFaster and Stronger Baseline for Vision Language\nModel. arXiv preprint arXiv:2402.03766.\nYunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shil-\niang Zhang, Zhijie Yan, Chang Zhou, and Jingren\nZhou. 2023b. Qwen-audio: Advancing universal\naudio understanding via unified large-scale audio-\nlanguage models. arXiv preprint arXiv:2311.07919.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nXTuner Contributors. 2023. XTuner: A Toolkit for Ef-\nficiently Fine-tuning LLM. https://github.com/\nInternLM/xtuner.\nCan Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang\nZhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zi-\nchong Yang, Kuei-Da Liao, et al. 2024.\nA sur-\nvey on multimodal large language models for au-\ntonomous driving. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vi-\nsion, pages 958\u2013979.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven C. H. Hoi.\n2023.\nInstructBLIP: Towards General-purpose\nVision-Language Models with Instruction Tuning.\nIn Thirty-seventh Conference on Neural Information\nProcessing Systems.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nLinhao Dong and Bo Xu. 2020.\nCif: Continuous\nintegrate-and-fire for end-to-end speech recognition.\nIn ICASSP 2020-2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6079\u20136083. IEEE.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,\nZheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,\nHongyu Zhou, Haoran Wei, et al. 2024a. Dreamllm:\nSynergistic multimodal comprehension and creation.\nIn The Twelfth International Conference on Learning\nRepresentations.\nXiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\nBin Wang, Linke Ouyang, Xilin Wei, Songyang\nZhang, Haodong Duan, Maosong Cao, et al.\n2024b.\nInternLM-XComposer2: Mastering Free-\nform Text-Image Composition and Comprehension\nin Vision-Language Large Model. arXiv preprint\narXiv:2401.16420.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale. In International\nConference on Learning Representations.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nYifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao.\n2022a. A Survey of Vision-Language Pre-Trained\nModels. In Proceedings of the Thirty-First Inter-\nnational Joint Conference on Artificial Intelligence,\nIJCAI 2022, Vienna, Austria, 23-29 July 2022, pages\n5436\u20135443.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022b.\nGLM: General Language Model Pretraining with Au-\ntoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n320\u2013335.\nErich Elsen, Augustus Odena, Maxwell Nye, Sa\u02d8g-\nnak Ta\u00b8s\u0131rlar, Tri Dao, Curtis Hawthorne, Deepak\nMoparthi, and Arushi Somani. 2023.\nReleasing\nPersimmon-8B.\nYue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan\nJiang, Ching-Chen Kuo, Xinze Guan, and Xin Eric\nWang. 2024.\nMuffin or Chihuahua?\nChalleng-\ning Large Vision-Language Models with Multipanel\nVQA. arXiv preprint arXiv:2401.15847.\nHan Fang, Pengfei Xiong, Luhui Xu, and Yu Chen.\n2021. Clip2video: Mastering video-text retrieval via\nimage clip. arXiv preprint arXiv:2106.11097.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell\nWu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. 2023. Eva: Exploring the limits of\nmasked visual representation learning at scale. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 19358\u2013\n19369.\nHao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang\nLi, and Can Huang. 2023. DocPedia: Unleashing\nthe Power of Large Multimodal Model in the Fre-\nquency Domain for Versatile Document Understand-\ning. arXiv preprint arXiv:2311.11810.\nRoya Firoozi,\nJohnathan Tucker,\nStephen Tian,\nAnirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke\nZhu, Shuran Song, Ashish Kapoor, Karol Hausman,\net al. 2023. Foundation Models in Robotics: Appli-\ncations, Challenges, and the Future. arXiv preprint\narXiv:2312.07843.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,\nKe Li, Xing Sun, et al. 2023. Mme: A comprehensive\nevaluation benchmark for multimodal large language\nmodels. arXiv preprint arXiv:2306.13394.\nChin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, and Hung-\nYi Lee. 2022.\nAdapterBias: Parameter-efficient\nToken-dependent Representation Shift for Adapters\nin NLP Tasks. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages\n2608\u20132621.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang,\nJonathan Hayase, Georgios Smyrnis, Thao Nguyen,\nRyan Marten, Mitchell Wortsman, Dhruba Ghosh,\nJieyu Zhang, et al. 2023. Datacomp: In search of\nthe next generation of multimodal datasets. arXiv\npreprint arXiv:2304.14108.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow,\nMd Mehrab Tanjim, Sungchul Kim, Franck Dernon-\ncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed.\n2023. Bias and fairness in large language models: A\nsurvey. arXiv preprint arXiv:2309.00770.\nPeng Gao, Renrui Zhang, Chris Liu, Longtian Qiu,\nSiyuan Huang, Weifeng Lin, Shitian Zhao, Shijie\nGeng, Ziyi Lin, Peng Jin, et al. 2024. SPHINX-\nX: Scaling Data and Parameters for a Family of\nMulti-modal Large Language Models. arXiv preprint\narXiv:2402.05935.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023a. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nZhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma,\nWenjuan Han, Song-Chun Zhu, and Qing Li.\n2023b.\nCLOVA: A Closed-Loop Visual Assis-\ntant with Tool Usage and Update. arXiv preprint\narXiv:2312.10908.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and\nYing Shan. 2023. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-\nnat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. 2023.\nImagebind: One embed-\nding space to bind them all. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 15180\u201315190.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,\nMiao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,\nPing Luo, and Kai Chen. 2023. Multimodal-gpt: A\nvision and language model for dialogue with humans.\narXiv preprint arXiv:2305.04790.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2013.\nAn em-\npirical investigation of catastrophic forgetting in\ngradient-based neural networks.\narXiv preprint\narXiv:1312.6211.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904\u20136913.\nJiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu\nMinzhe, Xiaodan Liang, Lewei Yao, Runhui Huang,\nWei Zhang, Xin Jiang, et al. 2022. Wukong: A 100\nmillion large-scale chinese cross-modal pre-training\nbenchmark. Advances in Neural Information Pro-\ncessing Systems, 35:26418\u201326431.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. 2018. Vizwiz grand challenge: Answering\nvisual questions from blind people. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 3608\u20133617.\nMinglun Han, Feilong Chen, Jing Shi, Shuang Xu,\nand Bo Xu. 2023. Knowledge Transfer from Pre-\ntrained Language Models to Cif-based Speech Rec-\nognizers via Hierarchical Distillation. arXiv preprint\narXiv:2301.13003.\nMinglun Han, Linhao Dong, Zhenlin Liang, Meng Cai,\nShiyu Zhou, Zejun Ma, and Bo Xu. 2022. Improving\nend-to-end contextual speech recognition with fine-\ngrained contextual knowledge selection. In ICASSP\n2022-2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n8532\u20138536. IEEE.\nZongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu,\nChangqing Zhang, and Mike Zheng Shou. 2024.\nSkip \\n: A simple method to reduce hallucination\nin large vision-language models.\narXiv preprint\narXiv:2402.01345.\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu,\nYong Dai, Hongming Zhang, Zhenzhong Lan, and\nDong Yu. 2024.\nWebVoyager: Building an End-\nto-End Web Agent with Large Multimodal Models.\narXiv preprint arXiv:2401.13919.\nJinghan He, Haiyun Guo, Ming Tang, and Jinqiao Wang.\n2023. Continual instruction tuning for large multi-\nmodal models. arXiv preprint arXiv:2311.16206.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nUnified View of Parameter-Efficient Transfer Learn-\ning. In International Conference on Learning Repre-\nsentations.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Doll\u00e1r, and Ross Girshick. 2022. Masked autoen-\ncoders are scalable vision learners. In Proceedings\nof the IEEE/CVF conference on computer vision and\npattern recognition, pages 16000\u201316009.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770\u2013\n778.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng\nXu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang,\nYuxiao Dong, Ming Ding, et al. 2023. Cogagent: A\nvisual language model for gui agents. arXiv preprint\narXiv:2312.08914.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. arXiv\npreprint arXiv:2212.09689.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In In-\nternational Conference on Machine Learning, pages\n2790\u20132799. PMLR.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. Hubert: Self-supervised\nspeech representation learning by masked prediction\nof hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29:3451\u20133460.\nAnwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao\nYe, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and\nFei Huang. 2023a. mPLUG-PaperOwl: Scientific Di-\nagram Analysis with the Multimodal Large Language\nModel. arXiv preprint arXiv:2311.18248.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In International Conference on\nLearning Representations.\nJinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu\nPan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue\nZhao, Haoye Zhang, et al. 2023b. Large multilingual\nmodels pivot zero-shot multimodal learning across\nlanguages. arXiv preprint arXiv:2308.12038.\nJiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and\nShijian Lu. 2023a.\nVisual Instruction Tuning to-\nwards General-Purpose Multimodal Model: A Sur-\nvey. arXiv preprint arXiv:2312.16602.\nRongjie Huang, Mingze Li, Dongchao Yang, Jia-\ntong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, et al.\n2023b. Audiogpt: Understanding and generating\nspeech, music, sound, and talking head.\narXiv\npreprint arXiv:2304.12995.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, et al.\n2023c.\nLanguage is not all you need: Aligning\nperception with language models. arXiv preprint\narXiv:2302.14045.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 6700\u20136709.\nIDEFICS. 2023. Introducing IDEFICS: An Open Repro-\nduction of State-of-the-Art Visual Language Model.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022.\nOpt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nJitesh Jain, Jianwei Yang, and Humphrey Shi. 2023.\nVcoder:\nVersatile vision encoders for multi-\nmodal large language models.\narXiv preprint\narXiv:2312.14233.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International conference on ma-\nchine learning, pages 4904\u20134916. PMLR.\nYiren Jian, Chongyang Gao, and Soroush Vosoughi.\n2023. Bootstrapping Vision-Language Learning with\nDecoupled Language Pre-training. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao\nTan, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\nXiaoqiang Lei, et al. 2024. Unified language-vision\npretraining with dynamic discrete visual tokenization.\nIn The Twelfth International Conference on Learning\nRepresentations.\nKushal Kafle, Brian Price, Scott Cohen, and Christo-\npher Kanan. 2018. Dvqa: Understanding data visual-\nizations via question answering. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 5648\u20135656.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021. Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural\nInformation Processing Systems, 34:1022\u20131035.\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten,\nand Tamara Berg. 2014. Referitgame: Referring to\nobjects in photographs of natural scenes. In Proceed-\nings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 787\u2013\n798.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding.\nIn Proceedings of NAACL-HLT, pages 4171\u20134186.\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and\nDavide Testuggine. 2020. The hateful memes chal-\nlenge: Detecting hate speech in multimodal memes.\nAdvances in neural information processing systems,\n33:2611\u20132624.\nDiederik P Kingma and Max Welling. 2013.\nAuto-\nencoding\nvariational\nbayes.\narXiv\npreprint\narXiv:1312.6114.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen\nLo, et al. 2023. Segment anything. arXiv preprint\narXiv:2304.02643.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.\n2023a. Generating images with multimodal language\nmodels.\nIn Thirty-seventh Conference on Neural\nInformation Processing Systems.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.\n2023b. Grounding language models to images for\nmultimodal inputs and outputs. In International Con-\nference on Machine Learning, pages 17283\u201317300.\nPMLR.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123:32\u201373.\nXin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. 2023. Lisa: Reason-\ning segmentation via large language model. arXiv\npreprint arXiv:2308.00692.\nHugo Lauren\u00e7on, Lucile Saulnier, Leo Tronchon, Stas\nBekman, Amanpreet Singh, Anton Lozhkov, Thomas\nWang, Siddharth Karamcheti, Alexander M Rush,\nDouwe Kiela, et al. 2023.\nOBELICS: An Open\nWeb-Scale Filtered Dataset of Interleaved Image-Text\nDocuments. In Thirty-seventh Conference on Neural\nInformation Processing Systems Datasets and Bench-\nmarks Track.\nSeongyun Lee, Sue Hyun Park, Yongrae Jo, and Min-\njoon Seo. 2023. Volcano: mitigating multimodal\nhallucination through self-feedback guided revision.\narXiv preprint arXiv:2311.07362.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe Power of Scale for Parameter-Efficient Prompt\nTuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045\u20133059.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nFanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. 2023a. Mimic-it: Multi-modal in-context in-\nstruction tuning. arXiv preprint arXiv:2306.05425.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. 2023b. Otter: A\nmulti-modal model with in-context instruction tuning.\narXiv preprint arXiv:2305.03726.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. 2023c. Seed-bench: Bench-\nmarking multimodal llms with generative compre-\nhension. arXiv preprint arXiv:2307.16125.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2023d.\nLlava-med: Training a large language-and-vision as-\nsistant for biomedicine in one day. arXiv preprint\narXiv:2306.00890.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023e. BLIP-2: Bootstrapping Language-Image\nPre-training with Frozen Image Encoders and Large\nLanguage Models. In International Conference on\nMachine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, pages 19730\u201319742.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In International Conference on Ma-\nchine Learning, pages 12888\u201312900. PMLR.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in neural information processing systems,\n34:9694\u20139705.\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-\nhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. 2023f. Videochat: Chat-centric video un-\nderstanding. arXiv preprint arXiv:2305.06355.\nLei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi\nWang, Liang Chen, Yazheng Yang, Benyou Wang,\nand Lingpeng Kong. 2023g. Silkie: Preference Dis-\ntillation for Large Visual Language Models. arXiv\npreprint arXiv:2312.10665.\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi\nWang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, et al. 2023h. M3IT: A Large-\nScale Dataset towards Multi-Modal Multilingual In-\nstruction Tuning. arXiv preprint arXiv:2306.04387.\nMukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhen-\nguang Liu, and Qi Liu. 2024a. Red teaming visual\nlanguage models. arXiv preprint arXiv:2401.12915.\nXiang Lisa Li and Percy Liang. 2021. Prefix-Tuning:\nOptimizing Continuous Prompts for Generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582\u2013\n4597.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part XXX 16, pages 121\u2013137. Springer.\nYanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin\nFu, Guosheng Lin, Chunhua Shen, Ling Chen, and\nYunchao Wei. 2023i. Stablellava: Enhanced visual\ninstruction tuning with synthesized image-dialogue\ndata. arXiv preprint arXiv:2308.10253.\nYanwei Li, Chengyao Wang, and Jiaya Jia. 2023j.\nLLaMA-VID: An Image is Worth 2 Tokens in Large\nLanguage Models. arXiv preprint arXiv:2311.17043.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,\nWayne Xin Zhao, and Ji-Rong Wen. 2023k. Eval-\nuating object hallucination in large vision-language\nmodels. arXiv preprint arXiv:2305.10355.\nZeju Li, Chao Zhang, Xiaoyan Wang, Ruilong Ren, Yi-\nfan Xu, Ruifei Ma, and Xiangde Liu. 2024b. 3DMIT:\n3D Multi-modal Instruction Tuning for Scene Under-\nstanding. arXiv preprint arXiv:2401.03201.\nZhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo\nZhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and\nXiang Bai. 2023l. Monkey: Image Resolution and\nText Label Are Important Things for Large Multi-\nmodal Models. arXiv preprint arXiv:2311.06607.\nZhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing\nCai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li,\nVan Tu Vu, et al. 2024c. LEGO: Language Enhanced\nMulti-modal Grounding Model.\narXiv preprint\narXiv:2401.06071.\nBin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin\nZhu, Peng Jin, Junwu Zhang, Munan Ning, and\nLi Yuan. 2024a. MoE-LLaVA: Mixture of Experts\nfor Large Vision-Language Models. arXiv preprint\narXiv:2401.15947.\nHongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang,\nand Jing Ma. 2024b. GOAT-Bench: Safety Insights\nto Large Multimodal Models through Meme-Based\nSocial Abuse. arXiv preprint arXiv:2401.01523.\nJi Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo\nMolchanov, Andrew Tao, Huizi Mao, Jan Kautz,\nMohammad Shoeybi, and Song Han. 2023. VILA:\nOn Pre-training for Visual Language Models. arXiv\npreprint arXiv:2312.07533.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer.\nLinkSoul-AI. 2023. Chinese-LLaVA.\nFangyu Liu, Guy Emerson, and Nigel Collier. 2023a.\nVisual spatial reasoning. Transactions of the Associ-\nation for Computational Linguistics, 11:635\u2013651.\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,\nXiutian Zhao, Ke Wang, Liping Hou, Rongjun Li,\nand Wei Peng. 2024a. A survey on hallucination\nin large vision-language models.\narXiv preprint\narXiv:2402.00253.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo\nLiu, Danilo P. Mandic, Wenwu Wang, and Mark D.\nPlumbley. 2023b. AudioLDM: Text-to-Audio Gener-\nation with Latent Diffusion Models. In International\nConference on Machine Learning, ICML 2023, 23-\n29 July 2023, Honolulu, Hawaii, USA, pages 21450\u2013\n21474.\nHaohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao\nMei, Qiuqiang Kong, Yuping Wang, Wenwu Wang,\nYuxuan Wang, and Mark D. Plumbley. 2023c. Audi-\noLDM 2: Learning Holistic Audio Generation with\nSelf-supervised Pretraining. CoRR, abs/2308.05734.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023d. Improved Baselines with Visual Instruc-\ntion Tuning. In NeurIPS 2023 Workshop on Instruc-\ntion Tuning and Instruction Following.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuan-\nhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b.\nLLaVA-NeXT: Improved reasoning, OCR, and world\nknowledge.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023e. Visual Instruction Tuning. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems.\nShilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng\nLi, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su,\nJun Zhu, et al. 2023f. Llava-plus: Learning to use\ntools for creating multimodal agents. arXiv preprint\narXiv:2311.05437.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61\u201368.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\nZhengxiao Du, Zhilin Yang, and Jie Tang. 2021a.\nP-tuning v2: Prompt tuning can be comparable to\nfine-tuning universally across scales and tasks. arXiv\npreprint arXiv:2110.07602.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. 2023g. Mm-\nbench: Is your multi-modal model an all-around\nplayer? arXiv preprint arXiv:2307.06281.\nYuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu,\nMingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui\nChen, Chunyuan Li, Lianwen Jin, et al. 2023h. On\nthe hidden mystery of ocr in large multimodal models.\narXiv preprint arXiv:2305.07895.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. 2021b.\nSwin transformer: Hierarchical vision transformer\nusing shifted windows.\nIn Proceedings of the\nIEEE/CVF international conference on computer vi-\nsion, pages 10012\u201310022.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui,\nZhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. 2023i. Controlllm: Aug-\nment language models with tools by searching on\ngraphs. arXiv preprint arXiv:2310.17796.\nSiqu Long, Feiqi Cao, Soyeon Caren Han, and Haiqin\nYang. 2022. Vision-and-Language Pretrained Mod-\nels: A Survey. In Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2022, Vienna, Austria, 23-29 July 2022,\npages 5530\u20135537.\nJunyu Lu, Ruyi Gan, Dixiang Zhang, Xiaojun Wu,\nZiwei Wu, Renliang Sun, Jiaxing Zhang, Pingjian\nZhang, and Yan Song. 2023a. Lyrics: Boosting Fine-\ngrained Language-Vision Alignment and Compre-\nhension via Semantic-aware Visual Objects. arXiv\npreprint arXiv:2312.05278.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-\nWei Chang, Michel Galley, and Jianfeng Gao. 2024.\nMathvista: Evaluating mathematical reasoning of\nfoundation models in visual contexts. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. Advances in Neural Information\nProcessing Systems, 35:2507\u20132521.\nPan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun\nZhu. 2021. Iconqa: A new benchmark for abstract\ndiagram understanding and visual language reason-\ning. In Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track\n(Round 2).\nYujie Lu, Xiujun Li, William Yang Wang, and Yejin\nChoi. 2023b. Vim: Probing multimodal large lan-\nguage models for visual embedded instruction fol-\nlowing. arXiv preprint arXiv:2311.17647.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo\nGeng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qing-\nwei Lin, and Daxin Jiang. 2023. WizardCoder: Em-\npowering Code Large Language Models with Evol-\nInstruct. arXiv preprint arXiv:2306.08568.\nTengchao Lv, Yupan Huang, Jingye Chen, Lei Cui,\nShuming Ma, Yaoyao Chang, Shaohan Huang, Wen-\nhui Wang, Li Dong, Weiyao Luo, et al. 2023.\nKosmos-2.5: A multimodal literate model. arXiv\npreprint arXiv:2309.11419.\nYingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone,\nand Chaowei Xiao. 2023.\nDolphins:\nMulti-\nmodal language model for driving. arXiv preprint\narXiv:2312.00438.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan,\nand Fahad Shahbaz Khan. 2023. Video-ChatGPT:\nTowards Detailed Video Understanding via Large\nVision and Language Models.\narXiv preprint\narXiv:2306.05424.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawa-\nhar. 2021. Docvqa: A dataset for vqa on document\nimages. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages\n2200\u20132209.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109\u2013165. Else-\nvier.\nXinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang\nKong, Tom Ko, Chengqi Zhao, Mark D Plumbley,\nYuexian Zou, and Wenwu Wang. 2023. Wavcaps:\nA chatgpt-assisted weakly-labelled audio caption-\ning dataset for audio-language multimodal research.\narXiv preprint arXiv:2303.17395.\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh,\nand Anirban Chakraborty. 2019. Ocr-vqa: Visual\nquestion answering by reading text in images. In\n2019 international conference on document analysis\nand recognition (ICDAR), pages 947\u2013952. IEEE.\nDebjyoti Mondal, Suraj Modi, Subhadarshi Panda,\nRituraj Singh, and Godawari Sudhakar Rao. 2024.\nKAM-CoT: Knowledge Augmented Multimodal\nChain-of-Thoughts Reasoning.\narXiv preprint\narXiv:2401.12863.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin,\nTushar Nagarajan, Matt Smith, Shashank Jain, Chun-\nFu Yeh, Prakash Murugesan, Peyman Heidari, Yue\nLiu, et al. 2023. Anymal: An efficient and scal-\nable any-modality augmented language model. arXiv\npreprint arXiv:2309.16058.\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai\nWang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai,\nYu Qiao, and Ping Luo. 2023. Embodiedgpt: Vision-\nlanguage pre-training via embodied chain of thought.\nIn Thirty-seventh Conference on Neural Information\nProcessing Systems.\nHumza Naveed, Asad Ullah Khan, Shi Qiu, Muham-\nmad Saqib, Saeed Anwar, Muhammad Usman, Nick\nBarnes, and Ajmal Mian. 2023. A comprehensive\noverview of large language models. arXiv preprint\narXiv:2307.06435.\nZiyi Ni, Minglun Han, Feilong Chen, Linghui Meng,\nJing Shi, Pin Lv, and Bo Xu. 2024. VILAS: Explor-\ning the Effects of Vision and Language Context in\nAutomatic Speech Recognition. In ICASSP 2024-\n2024 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE.\nOpenAI. 2022. OpenAI: Introducing ChatGPT.\nOpenAI. 2023. GPT-4 Technical Report.\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni,\nHuy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fer-\nnandez, Daniel Haziza, Francisco Massa, Alaaeldin\nEl-Nouby, et al. 2023. Dinov2: Learning robust vi-\nsual features without supervision. arXiv preprint\narXiv:2304.07193.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural informa-\ntion processing systems, 24.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nXichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. 2023. Kosmos-g: Gen-\nerating images in context with multimodal large lan-\nguage models. arXiv preprint arXiv:2310.02992.\nArtemis Panagopoulou, Le Xue, Ning Yu, Junnan Li,\nDongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese,\nCaiming Xiong, and Juan Carlos Niebles. 2023. X-\nInstructBLIP: A Framework for aligning X-Modal\ninstruction-aware representations to LLMs and Emer-\ngent Cross-modal Reasoning.\narXiv preprint\narXiv:2311.18799.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,\nShaohan Huang, Shuming Ma, and Furu Wei.\n2023.\nKosmos-2: Grounding Multimodal Large\nLanguage Models to the World.\narXiv preprint\narXiv:2306.14824.\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze\nDong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang\nXu, and Lingpeng Kong Tong Zhang. 2023. DetGPT:\nDetect What You Need via Reasoning. arXiv preprint\narXiv:2305.14167.\nJi Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong\nLv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao\nDong, et al. 2024. CogCoM: Train Large Vision-\nLanguage Models Diving into Details through Chain\nof Manipulations. arXiv preprint arXiv:2402.04236.\nJie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia\nLi, Hefeng Wu, Xuefeng Xiao, Rui Wang, and\nShilei Wen. 2024.\nDiffusionGPT: LLM-Driven\nText-to-Image Generation System. arXiv preprint\narXiv:2401.10061.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748\u20138763. PMLR.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust Speech Recognition via Large-Scale Weak\nSupervision. In International Conference on Ma-\nchine Learning, ICML 2023, 23-29 July 2023, Hon-\nolulu, Hawaii, USA, pages 28492\u201328518.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nHanoona Rasheed, Muhammad Maaz, Sahal Shaji, Ab-\ndelrahman Shaker, Salman Khan, Hisham Cholakkal,\nRao M Anwer, Erix Xing, Ming-Hsuan Yang, and\nFahad S Khan. 2023.\nGlamm:\nPixel ground-\ning large multimodal model.\narXiv preprint\narXiv:2311.03356.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. Advances in neural informa-\ntion processing systems, 30.\nZhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao\nZhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin.\n2023. PixelLM: Pixel Reasoning with Large Multi-\nmodal Model. arXiv preprint arXiv:2312.02228.\nAnthony Robins. 1995.\nCatastrophic forgetting, re-\nhearsal and pseudorehearsal. Connection Science,\n7(2):123\u2013146.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages\n10684\u201310695.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing\nand Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18, pages\n234\u2013241. Springer.\nLudan Ruan and Qin Jin. 2022. Survey: Transformer\nbased video-language pre-training. AI Open, 3:1\u201313.\nPaul\nK Rubenstein,\nChulayuth\nAsawaroengchai,\nDuc Dung Nguyen, Ankur Bapna, Zal\u00e1n Borsos,\nF\u00e9lix de Chaumont Quitry, Peter Chen, Dalia El\nBadawy, Wei Han, Eugene Kharitonov, et al. 2023.\nAudioPaLM: A Large Language Model That Can\nSpeak and Listen. arXiv preprint arXiv:2306.12925.\nSalesforce. 2022. Ulip.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. 2022. Laion-5b: An open\nlarge-scale dataset for training next generation image-\ntext models. Advances in Neural Information Pro-\ncessing Systems, 35:25278\u201325294.\nChristoph Schuhmann, Andreas K\u00f6pf, Richard Vencu,\nTheo Coombes, and Romain Beaumont. 2022b.\nLaion coco: 600m synthetic captions from laion2b-\nen.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. 2021. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answer-\ning using world knowledge. In European Conference\non Computer Vision, pages 146\u2013162. Springer.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556\u20132565.\nWeizhou Shen, Chenliang Li, Hongzhan Chen, Ming\nYan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei\nHuang. 2024. Small llms are weak tool learners: A\nmulti-llm agent. arXiv preprint arXiv:2401.07324.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. 2020. Textcaps: a dataset for im-\nage captioning with reading comprehension. In Com-\nputer Vision\u2013ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part II 16, pages 742\u2013758. Springer.\nAmanpreet Singh,\nVivek Natarajan,\nMeet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. 2019. Towards vqa models\nthat can read. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 8317\u20138326.\nShezheng Song, Xiaopeng Li, and Shasha Li. 2023.\nHow to Bridge the Gap between Modalities: A Com-\nprehensive Survey on Multimodal Large Language\nModel. arXiv preprint arXiv:2311.07594.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole.\n2021. Score-Based Generative Modeling through\nStochastic Differential Equations. In International\nConference on Learning Representations.\nYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan\nWang, and Deng Cai. 2023.\nPandagpt:\nOne\nmodel to instruction-follow them all. arXiv preprint\narXiv:2305.16355.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang,\nQiying Yu, Zhengxiong Luo, Yueze Wang, Yongming\nRao, Jingjing Liu, Tiejun Huang, et al. 2023a. Gen-\nerative multimodal models are in-context learners.\narXiv preprint arXiv:2312.13286.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,\nXiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. 2024.\nGenerative pretraining in multimodality.\nIn The\nTwelfth International Conference on Learning Repre-\nsentations.\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,\nChunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan\nGui, Yu-Xiong Wang, Yiming Yang, et al. 2023b.\nAligning large multimodal models with factually aug-\nmented rlhf. arXiv preprint arXiv:2309.14525.\nD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128.\nChangli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao\nChen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao\nZhang. 2023a. Salmonn: Towards generic hearing\nabilities for large language models. arXiv preprint\narXiv:2310.13289.\nZineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu,\nChenguang Zhu, and Mohit Bansal. 2023b. CoDi-2:\nIn-Context, Interleaved, and Interactive Any-to-Any\nGeneration. arXiv preprint arXiv:2311.18775.\nZineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng,\nand Mohit Bansal. 2023c. Any-to-Any Generation\nvia Composable Diffusion. In Thirty-seventh Confer-\nence on Neural Information Processing Systems.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Jason Wei, Xuezhi Wang, Hyung Won Chung,\nDara Bahri, Tal Schuster, Steven Zheng, et al. 2022.\nUl2: Unifying language learning paradigms. In The\nEleventh International Conference on Learning Rep-\nresentations.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nInternLM Team. 2023. Internlm: A multilingual lan-\nguage model with progressively enhanced capabili-\nties.\nYi Team. 2023. Yi-VL.\nChangyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun\nWang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei\nLu, Tong Lu, Jie Zhou, et al. 2024. MM-Interleaved:\nInterleaved Image-Text Generative Modeling via\nMulti-modal Feature Synchronizer. arXiv preprint\narXiv:2401.10208.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nChenyu Wang, Weixin Luo, Qianyu Chen, Haonan\nMai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma,\nShenghua Gao, et al. 2024a. Tool-LMM: A Large\nMulti-Modal Model for Tool Agent Learning. arXiv\npreprint arXiv:2401.10727.\nDongsheng Wang, Natraj Raman, Mathieu Sibue,\nZhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong\nPei, Armineh Nourbakhsh, and Xiaomo Liu. 2023a.\nDocLLM: A layout-aware generative language model\nfor multimodal document understanding.\narXiv\npreprint arXiv:2401.00908.\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan,\nWeizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang.\n2024b. Mobile-Agent: Autonomous Multi-Modal\nMobile Device Agent with Visual Perception. arXiv\npreprint arXiv:2401.16158.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022a. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, pages\n23318\u201323340. PMLR.\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi\nHong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,\nLei Zhao, Xixuan Song, et al. 2023b. Cogvlm: Vi-\nsual expert for pretrained language models. arXiv\npreprint arXiv:2311.03079.\nWeiyun Wang, Min Shi, Qingyun Li, Wenhai Wang,\nZhenhang Huang, Linjie Xing, Zhe Chen, Hao Li,\nXizhou Zhu, Zhiguo Cao, et al. 2023c. The all-seeing\nproject: Towards panoptic visual recognition and\nunderstanding of the open world.\narXiv preprint\narXiv:2308.01907.\nWenhui Wang,\nHangbo Bao,\nLi Dong,\nJohan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2022b. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks.\narXiv preprint arXiv:2208.10442.\nWenhui Wang,\nHangbo Bao,\nLi Dong,\nJohan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2023d. Image as a Foreign Language:\nBEiT Pretraining for Vision and Vision-Language\nTasks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n19175\u201319186.\nXinyu Wang, Bohan Zhuang, and Qi Wu. 2024c. Mo-\ndaVerse: Efficiently Transforming Modalities with\nLLMs. arXiv preprint arXiv:2401.06395.\nHaoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, En Yu, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang. 2024. Small Language Model Meets\nwith Reinforced Vision Vocabulary. arXiv preprint\narXiv:2401.12503.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned Language\nModels are Zero-Shot Learners. In International\nConference on Learning Representations.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023a.\nVisual chatgpt:\nTalking, drawing and editing\nwith visual foundation models.\narXiv preprint\narXiv:2303.04671.\nHaoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng\nChen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu\nSun, Qiong Yan, Guangtao Zhai, et al. 2023b. Q-\nbench: A benchmark for general-purpose founda-\ntion models on low-level vision.\narXiv preprint\narXiv:2309.14181.\nJiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming\nYan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen\nLin, Yanwei Fu, et al. 2017. Ai challenger: A large-\nscale dataset for going deeper in image understanding.\narXiv preprint arXiv:1711.06475.\nJiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng\nWan, and Philip S Yu. 2023c.\nMultimodal large\nlanguage models:\nA survey.\narXiv preprint\narXiv:2311.13165.\nPenghao Wu and Saining Xie. 2023. V*: Guided Visual\nSearch as a Core Mechanism in Multimodal LLMs.\narXiv preprint arXiv:2312.14135, 17.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and\nTat-Seng Chua. 2023d. Next-gpt: Any-to-any multi-\nmodal llm. arXiv preprint arXiv:2309.05519.\nTongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan,\nThuy-Trang Vu, and Gholamreza Haffari. 2024. Con-\ntinual Learning for Large Language Models: A Sur-\nvey. arXiv preprint arXiv:2402.01364.\nYusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-\nlor Berg-Kirkpatrick, and Shlomo Dubnov. 2023e.\nLarge-scale contrastive language-audio pretraining\nwith feature fusion and keyword-to-caption augmen-\ntation. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 1\u20135. IEEE.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 5288\u20135296.\nRui Yan, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng\nWang, Xudong Lin, Guanyu Cai, and Jinhui Tang.\n2021. Video-text pre-training with learned regions.\narXiv preprint arXiv:2112.01194.\nSiming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qix-\ning Huang, and Li Erran Li. 2024. ViGoR: Improving\nVisual Grounding of Large Vision Language Models\nwith Fine-Grained Reward Modeling. arXiv preprint\narXiv:2402.06118.\nJinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath\nChanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi,\nand Junzhou Huang. 2022.\nVision-language pre-\ntraining with triple contrastive learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 15671\u201315680.\nLing Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu,\nStefano Ermon, and Bin Cui. 2024. Mastering Text-\nto-Image Diffusion: Recaptioning, Planning, and\nGenerating with Multimodal LLMs. arXiv preprint\narXiv:2401.11708.\nZhen Yang, Yingxue Zhang, Fandong Meng, and Jie\nZhou. 2023a. TEAL: Tokenize and Embed ALL for\nMulti-modal Large Language Models. arXiv preprint\narXiv:2311.04589.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023b.\nMm-react: Prompting chatgpt for multimodal rea-\nsoning and action. arXiv preprint arXiv:2303.11381.\nJiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye,\nMing Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu,\nChenliang Li, Junfeng Tian, et al. 2023a. mplug-\ndocowl: Modularized multimodal large language\nmodel for document understanding. arXiv preprint\narXiv:2307.02499.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,\nMing Yan, Yiyang Zhou, Junyang Wang, An-\nwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023b.\nmplug-owl: Modularization empowers large lan-\nguage models with multimodality. arXiv preprint\narXiv:2304.14178.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei\nLiu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\n2023c. mplug-owl2: Revolutionizing multi-modal\nlarge language model with modality collaboration.\narXiv preprint arXiv:2311.04257.\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen. 2023a. A Survey on\nMultimodal Large Language Models. arXiv preprint\narXiv:2306.13549.\nZhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi,\nDingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xi-\naoshui Huang, Zhiyong Wang, et al. 2023b. Lamm:\nLanguage-assisted multi-modal instruction-tuning\ndataset, framework, and benchmark. arXiv preprint\narXiv:2306.06687.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67\u201378.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C\nBerg, and Tamara L Berg. 2016. Modeling context\nin referring expressions. In Computer Vision\u2013ECCV\n2016: 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part\nII 14, pages 69\u201385. Springer.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu,\nBinh Tang, Brian Karrer, Shelly Sheynin, et al.\n2023a. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning. arXiv preprint\narXiv:2309.02591.\nTianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng\nHan, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao\nZheng, Maosong Sun, et al. 2023b. Rlhf-v: Towards\ntrustworthy mllms via behavior alignment from fine-\ngrained correctional human feedback. arXiv preprint\narXiv:2312.00849.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan\nWang. 2023c. Mm-vet: Evaluating large multimodal\nmodels for integrated capabilities. arXiv preprint\narXiv:2308.02490.\nXumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang,\nJie Zhou, and Jiwen Lu. 2022.\nPoint-bert: Pre-\ntraining 3d point cloud transformers with masked\npoint modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 19313\u201319322.\nYuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xin-\njie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. 2023a.\nOsprey: Pixel Understanding with Visual Instruction\nTuning. arXiv preprint arXiv:2312.10032.\nZhengqing Yuan, Zhaoxu Li, and Lichao Sun. 2023b.\nTinyGPT-V: Efficient Multimodal Large Language\nModel via Small Backbones.\narXiv preprint\narXiv:2312.16862.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, et al. 2023. Mmmu:\nA massive multi-discipline multimodal understand-\ning and reasoning benchmark for expert agi. arXiv\npreprint arXiv:2311.16502.\nRowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu,\nYanpeng Zhao, Mohammadreza Salehi, Aditya Kusu-\npati, Jack Hessel, Ali Farhadi, and Yejin Choi. 2022.\nMerlot reserve: Neural script knowledge through\nvision and language and sound. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16375\u201316387.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022a. GLM-130B:\nAn Open Bilingual Pre-trained Model.\nIn The\nEleventh International Conference on Learning Rep-\nresentations.\nYan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia,\nGuoqiang Wei, Yang Wei, Yuchen Zhang, and Tao\nKong. 2023. What Matters in Training a GPT4-Style\nLanguage Model with Multimodal Inputs?\narXiv\npreprint arXiv:2307.02469.\nYan Zeng, Xinsong Zhang, and Hang Li. 2022b. Multi-\nGrained Vision Language Pre-Training: Aligning\nTexts with Visual Concepts. In International Con-\nference on Machine Learning, pages 25994\u201326009.\nPMLR.\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan,\nPengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a.\nSpeechGPT: Empowering Large Language Models\nwith Intrinsic Cross-Modal Conversational Abilities.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023, pages 15757\u201315773.\nDuzhen Zhang, Wei Cong, Jiahua Dong, Yahan Yu, Xi-\nuyi Chen, Yonggang Zhang, and Zhen Fang. 2023b.\nContinual Named Entity Recognition without Catas-\ntrophic Forgetting. In The 2023 Conference on Em-\npirical Methods in Natural Language Processing.\nDuzhen Zhang, Hongliu Li, Wei Cong, Rongtao Xu,\nJiahua Dong, and Xiuyi Chen. 2023c. Task relation\ndistillation and prototypical pseudo label for incre-\nmental named entity recognition. In Proceedings of\nthe 32nd ACM International Conference on Informa-\ntion and Knowledge Management, pages 3319\u20133329.\nDuzhen Zhang, Yahan Yu, Feilong Chen, and Xiuyi\nChen. 2023d. Decomposing Logits Distillation for\nIncremental Named Entity Recognition. In Proceed-\nings of the 46th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 1919\u20131923.\nDuzhen Zhang, Tielin Zhang, Shuncheng Jia, Qingyu\nWang, and Bo Xu. 2022a. Recent Advances and New\nFrontiers in Spiking Neural Networks. In Proceed-\nings of the Thirty-First International Joint Confer-\nence on Artificial Intelligence, IJCAI 2022, Vienna,\nAustria, 23-29 July 2022, pages 5670\u20135677.\nGe Zhang, Xinrun Du, Bei Chen, Yiming Liang,\nTongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang\nCheng, Chunpu Xu, Shuyue Guo, et al. 2024. CM-\nMMU: A Chinese Massive Multi-discipline Multi-\nmodal Understanding Benchmark. arXiv preprint\narXiv:2401.11944.\nHang Zhang, Xin Li, and Lidong Bing. 2023e. Video-\nLLaMA: An Instruction-tuned Audio-Visual Lan-\nguage Model for Video Understanding. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2023 -\nSystem Demonstrations, Singapore, December 6-10,\n2023, pages 543\u2013553.\nHao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang\nSu, Jun Zhu, Lionel Ni, and Heung-Yeung Shum.\n2022b. DINO: DETR with Improved DeNoising\nAnchor Boxes for End-to-End Object Detection. In\nThe Eleventh International Conference on Learning\nRepresentations.\nJeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas\nGuibas, and Jitendra Malik. 2020. Side-tuning: a\nbaseline for network adaptation via additive side net-\nworks. In Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part III 16, pages 698\u2013714. Springer.\nPan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao\nXu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,\nSongyang Zhang, Haodong Duan, Hang Yan, et al.\n2023f. Internlm-xcomposer: A vision-language large\nmodel for advanced text-image comprehension and\ncomposition. arXiv preprint arXiv:2309.15112.\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao,\nWenqi Shao, Wenwei Zhang, Kai Chen, and Ping\nLuo. 2023g. Gpt4roi: Instruction tuning large lan-\nguage model on region-of-interest. arXiv preprint\narXiv:2307.03601.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022c. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nYanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan\nZhou, Nedim Lipka, Diyi Yang, and Tong Sun.\n2023h. Llavar: Enhanced visual instruction tuning\nfor text-rich image understanding. arXiv preprint\narXiv:2306.17107.\nYoucai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong\nLuo, Yaqian Li, Shilong Liu, et al. 2023i. Recognize\nAnything: A Strong Image Tagging Model. arXiv\npreprint arXiv:2306.03514.\nBingchen Zhao, Haoqin Tu, Chen Wei, and Cihang Xie.\n2024. Tuning LayerNorm in Attention: Towards Ef-\nficient Multimodal LLM Finetuning. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nBo Zhao, Boya Wu, and Tiejun Huang. 2023a. Svit:\nScaling up visual instruction tuning. arXiv preprint\narXiv:2307.04087.\nLiang Zhao, En Yu, Zheng Ge, Jinrong Yang, Hao-\nran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng,\nRunpei Dong, Chunrui Han, et al. 2023b. Chatspot:\nBootstrapping multimodal llms via precise referring\ninstruction tuning. arXiv preprint arXiv:2307.09474.\nMin Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. 2022.\nEGSDE: Unpaired Image-to-Image Translation via\nEnergy-Guided Stochastic Differential Equations. In\nAdvances in Neural Information Processing Systems\n35: Annual Conference on Neural Information Pro-\ncessing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023c. A\nsurvey of large language models.\narXiv preprint\narXiv:2303.18223.\nYang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang,\nJiashi Feng, and Bingyi Kang. 2023d. Bubogpt: En-\nabling visual grounding in multi-modal llms. arXiv\npreprint arXiv:2307.08581.\nJunhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, and\nHuawen Feng. 2024. Beyond Anti-Forgetting: Mul-\ntimodal Continual Instruction Tuning with Positive\nForward Transfer. arXiv preprint arXiv:2401.09181.\nJunhao Zheng, Shengjie Qiu, and Qianli Ma. 2023a.\nLearn or Recall? Revisiting Incremental Learning\nwith Pre-trained Language Models. arXiv preprint\narXiv:2312.07887.\nKaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023b.\nMinigpt-5:\nInterleaved vision-and-language gen-\neration via generative vokens.\narXiv preprint\narXiv:2310.02239.\nBin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,\nHongFa Wang, Yatian Pang, Wenhao Jiang, Junwu\nZhang, Zongwei Li, et al. 2024a. Languagebind: Ex-\ntending video-language pretraining to n-modality by\nlanguage-based semantic alignment. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023a. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nDongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui\nLu, Yukun Zhao, Guoliang Xing, Junfeng Wang, and\nDawei Yin. 2024b. VisLingInstruct: Elevating Zero-\nShot Learning in Multi-Modal Language Models\nwith Autonomous Instruction Optimization. arXiv\npreprint arXiv:2402.07398.\nJinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie\nZhao, Hengshuang Zhao, Xiaohua Wang, and Ying\nShan. 2023b. Vl-gpt: A generative pre-trained trans-\nformer for vision and language understanding and\ngeneration. arXiv preprint arXiv:2312.09251.\nWanrong\nZhu,\nJack\nHessel,\nAnas\nAwadalla,\nSamir Yitzhak Gadre, Jesse Dodge, Alex Fang,\nYoungjae Yu, Ludwig Schmidt, William Yang Wang,\nand Yejin Choi. 2023c. Multimodal c4: An open,\nbillion-scale corpus of images interleaved with text.\narXiv preprint arXiv:2304.06939.\nYichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng\nMou, and Jian Tang. 2024c. LLaVA-Phi: Efficient\nMulti-Modal Assistant with Small Language Model.\narXiv preprint arXiv:2401.02330.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7w: Grounded question answering\nin images. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n4995\u20135004.\nBojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang,\nKam-Fai Wong, and Lei Zhang. 2023. Delta-lora:\nFine-tuning high-rank parameters with the delta of\nlow-rank matrices. arXiv preprint arXiv:2309.02411.\nYongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin\nYang, and Timothy Hospedales. 2024.\nSafety\nFine-Tuning at (Almost) No Cost: A Baseline for\nVision Large Language Models.\narXiv preprint\narXiv:2402.02207.\nA\nRelated Surveys\nPrior to the emergence of LLMs, several surveys\non traditional MM PT have been conducted (Ruan\nand Jin, 2022; Du et al., 2022a; Long et al., 2022;\nChen et al., 2023a). Most of these models entail a\nsubstantial computational cost during the PT phase,\nattributable to end-to-end training using large-scale\nmodels and datasets. As a consequence of not incor-\nporating LLMs, these models suffer from deficien-\ncies in instruction following, ICL, CoT, and inter-\nactive capabilities. Moreover, the training pipeline\nsolely encompasses the PT phase without the inclu-\nsion of an IT stage.\nIn recent times, several surveys have emerged\non MM-LLMs. Yin et al. and Wu et al. exclu-\nsively delve into early VL understanding models.\nHuang et al. place a primary emphasis on visual IT,\nwhile Song et al. focus on modal alignment meth-\nods. Lastly, Cui et al. provide a comprehensive\nreview of the applications of MM-LLMs within the\nrealm of autonomous driving.\nCompared with their works, the main distinc-\ntions are outlined as follows:\n\u2022 We have comprehensively covered nearly all\nMM-LLMs over the past year, totaling around\n120 or more, including not only understand-\ning models but also generative models. Our\ncoverage extends beyond VL modalities to en-\ncompass various modes such as audio and 3D\npoint cloud;\n\u2022 To offer readers a comprehensive understand-\ning of MM-LLMs, we have introduced a gen-\neral model architecture that incorporates any-\nto-any modality transformations, offering a\ndetailed overview of the functional roles and\nimplementation choices for each component;\n\u2022 We have summarized the developmental\ntrends of existing MM-LLMs and provided\nsome training recipes that can enhance effec-\ntiveness;\n\u2022 We have established an open-source website\nfor MM-LLMs researchers, supporting crowd-\nsourced updates and aiming to facilitate col-\nlaboration in the MM-LLMs field. We antic-\nipate that this survey will illuminate future\nresearch in the MM-LLMs domain.\nB\nModality Encoder\nIn the following, we provide a brief introduction to\nsome mainstream modality encoders.\nB.1\nVisual Modality\nNFNet-F6\n(Brock et al., 2021) is a normalizer-\nfree ResNet (He et al., 2016), showcasing an adap-\ntive gradient clipping that allows training on exten-\nsively augmented datasets while achieving SOTA\nlevels of image recognition.\nViT\n(Dosovitskiy et al., 2020) applies the Trans-\nformer (Vaswani et al., 2017) to images by first\ndividing the image into patches. It then undergoes\nlinear projection to flatten the patches, followed by\nencoding via Transformer blocks.\nCLIP ViT\n(Radford et al., 2021) builds con-\nnections between text and images, comprising a\nViT and a text encoder. With a vast amount of\ntext-image pairs, it optimizes ViT by contrastive\nlearning, treating paired text and images as positive\nsamples and others as negative ones.\nEva-CLIP ViT\n(Fang et al., 2023) stabilizes the\ntraining and optimization process of the massive\nCLIP, offering new directions in expanding and\naccelerating the expensive training of MM base\nmodels.\nB.2\nAudio Modality\nC-Former\n(Chen et al., 2023b) employs the\nCIF (Dong and Xu, 2020; Zhang et al., 2022a; Han\net al., 2022, 2023) for sequence transduction and a\nTransformer to extract audio features.\nHuBERT\n(Hsu et al., 2021) is a self-supervised\nspeech representation learning framework based\non BERT (Kenton and Toutanova, 2019), achieved\nby the masked prediction of discrete hidden units.\nIt has the capability to convert continuous speech\nsignals into a sequence of discrete units.\nBEATs\n(Chen et al., 2023g) is an iterative audio\npre-training framework designed to learn Bidirec-\ntional Encoder representations from Audio Trans-\nformers.\nC\nMainstream PEFT Methods\nPEFT entails maintaining the pre-trained LLM in a\nfrozen state while adjusting a small number of ad-\nditional trainable parameters. In the following sec-\ntion, we revisit several representative PEFT meth-\nods, where x and h represent the input and output\nof the original module, and h\u2032 signifies the output\nof this module when attached with PEFT.\nPrefix-tuning\n(Li and Liang, 2021; Lester et al.,\n2021) involves the addition of learnable tokens to\nthe keys and values of the attention module. This\nprocess is formulated as follows:\nh\u2032 = Attn (xWq, [Pk, xWk], [Pv, xWv]) , (6)\nwith Pk, Pv \u2208 Rl\u00d7d representing two sets of prefix\ntokens. [\u00b7, \u00b7] denotes concatenation, and Attn is\ndefined as:\nAttn (Q, K, V) := softmax\n\u0012QKT\n\u221a\nd\n\u0013\nV.\nAdapter\n(Houlsby et al., 2019; He et al., 2021;\nRebuffi et al., 2017; Zhang et al., 2020) is typically\na residual block consisting of a down-projection\nmatrix A, a nonlinear activation function \u03c3(\u00b7), and\nan up-projection matrix B. It can be inserted into\nany layer of the pre-trained LLM, formulated as\nfollows:\nh\u2032 = h + \u03c3(xA)B.\n(7)\nLoRA\n(Hu et al., 2021) is the most commonly\nused PEFT method. It assumes that the change in\nparameters occurs within a low-rank space. Given\na pre-trained matrix W \u2208 Rc\u00d7d, LoRA learns an\nincremental update \u2206W and decomposes \u2206W\ninto a matrix multiplication between two low-rank\nmatrices A \u2208 Rc\u00d7r and B \u2208 Rr\u00d7d, where r \u226a\nmin(c, d). LoRA follows the forward process as\noutlined below:\nh = W x + \u2206W x = W x + ABx.\n(8)\nQLoRA (Dettmers et al., 2023) is a quantized\nLoRA. The underlying principle of QLoRA in-\ncludes the quantization of pre-trained weights to\n4 bits, followed by the execution of PEFT using\nLoRA.\nLayerNorm tuning\n(Zhao et al., 2024) presents\nan efficient strategy to transform LLMs into MM-\nLLMs, which tunes LayerNorm in attention block\nyielding strong MM performance compared with\nfull parameter finetuning or LoRA.\nIn addition to the aforementioned PEFT methods,\nthere are several others, including P-tuning (Liu\net al., 2022), P-tuning v2 (Liu et al., 2021a), Adapt-\nBias (Fu et al., 2022), Compacter (Karimi Ma-\nhabadi et al., 2021),\nAdapterFormer (Chen\net al., 2022a), XTuner (Contributors, 2023), P-\nLoRA (Dong et al., 2024b), MoLE (Chen et al.,\n2024), and Delta-LoRA (Zi et al., 2023).\nD\nRepresentative LLMs\nThe representative LLM Backbones in existing\nMM-LLMs research are as follows:\nFlan-T5\n(Chung et al., 2022) investigates IT\nfor T5 (Raffel et al., 2020), an encoder-decoder\narchitecture using unified text-to-text training for\nall natural language processing issues, exhibiting\nrobust zero-shot and CoT capabilities.\nChatGLM\nis a Chinese-English bilingual dia-\nlogue model,2 optimized by an auto-regressive\nmask infilling objective.\nIt is based on the\nGLM (Du et al., 2022b; Zeng et al., 2022a) archi-\ntecture, optimized for Chinese question answering\nand dialogues.\nInternLM\n(Team, 2023) is a multilingual\ntrillion-parameter foundation model trained on over\na trillion tokens of data. Based on this foundation,\nthe model utilizes high-quality human-annotated\ndialogue data combined with RLHF to respond to\ncomplex instructions during human interactions,\nexhibiting responses that align with human ethics\nand values.\nUL2\n(Tay et al., 2022) is an encoder-decoder\nmodel trained utilizing a mixture of denoisers ob-\njectives, surpassing T5 on numerous benchmarks.\nQwen\n(Bai et al., 2023a) is trained on large-scale\nand diverse datasets, with a primary focus on Chi-\nnese and English. It employs SFT and RLHF tech-\nniques for alignment, resulting in dialogue models\nlike Qwen-Chat.\nChinchilla\n(Hoffmann et al., 2022) is a causal\ndecoder, trained on extensive text data. It posits\nthat model size should double for every doubling\nof training tokens.\nOPT\n(Zhang et al., 2022c) is a GPT-3 (Brown\net al., 2020) clone, striving to release an open-\nsource model that replicates the performance of\nGPT-3.\nPaLM\n(Chowdhery et al., 2023) is a causal de-\ncoder structure with parallel attention and feed-\nforward layers, enabling training speeds up to 15\ntimes faster. Notable changes contain RoPE embed-\ndings, SwiGLU activation, multi-query attention,\nand etc.\n2https://github.com/THUDM/ChatGLM-6B\nLLaMA\n(Touvron et al., 2023a) comprises\ndecoder-only models with efficient causal atten-\ntion.\nLLaMA-2\n(Touvron et al., 2023b) focuses on\nfine-tuning a superior and safer LLaMA-2-Chat\nmodel for conversation generation, incorporating\n40% more training data with grouped-query atten-\ntion and a larger context length.\nVicuna\n(Chiang et al., 2023) is a model built\non top of LLaMA, utilizing user dialogue data ob-\ntained from ShareGPT.com and trained by SFT.\nE\nSOTA MM-LLMs\nIn the following, we will provide a brief introduc-\ntion to the core contributions of some representative\nMM-LLMs.\nFlamingo\n(Alayrac et al., 2022) represents a se-\nries of Visual Language (VL) Models designed for\nprocessing interleaved visual data and text, gener-\nating free-form text as the output.\nBLIP-2\n(Li et al., 2023e) introduces a more\nresource-efficient framework,\ncomprising the\nlightweight Q-Former to bridge modality gaps and\nthe utilization of frozen LLMs. Leveraging LLMs,\nBLIP-2 can be guided for zero-shot image-to-text\ngeneration using natural language prompts.\nLLaVA\n(Liu et al., 2023e) pioneers the trans-\nfer of IT techniques to the MM domain.\nAd-\ndressing data scarcity, LLaVA introduces a novel\nopen-source MM instruction-following dataset cre-\nated using ChatGPT/GPT-4, alongside the MM\ninstruction-following benchmark, LLaVA-Bench.\nMiniGPT-4\n(Zhu et al., 2023a) proposes a\nstreamlined approach where training only one lin-\near layer aligns the pre-trained vision encoder with\nthe LLM. This efficient method enables the repli-\ncation of the exhibited capabilities of GPT-4.\nmPLUG-Owl\n(Ye et al., 2023b) presents a novel\nmodularized training framework for MM-LLMs,\nincorporating the visual context. To assess different\nmodels\u2019 performance in MM tasks, the framework\nincludes an instructional evaluation dataset called\nOwlEval.\nX-LLM\n(Chen et al., 2023b) is expanded to var-\nious modalities, including audio, and demonstrates\nstrong scalability. Leveraging the language trans-\nferability of the Q-Former, X-LLM is successfully\napplied in the context of Sino-Tibetan Chinese.\nVideoChat\n(Li et al., 2023f) pioneers an efficient\nchat-centric MM-LLM for video understanding di-\nalogue, setting standards for future research in this\ndomain and offering protocols for both academia\nand industry.\nInstructBLIP\n(Dai et al., 2023) is trained based\non the pre-trained BLIP-2 model, updating only\nthe Q-Former during MM IT. By introducing\ninstruction-aware visual feature extraction and cor-\nresponding instructions, the model enables the ex-\ntraction of flexible and diverse features.\nPandaGPT\n(Su et al., 2023) is a pioneering\ngeneral-purpose model with the capability to com-\nprehend and act upon instructions across 6 differ-\nent modalities: text, image/video, audio, thermal,\ndepth, and inertial measurement units.\n(PaLI-X\n(Chen et al., 2023h) is trained using\nmixed VL objectives and unimodal objectives, in-\ncluding prefix completion and masked-token com-\npletion. This approach proves effective for both\ndownstream task results and achieving the Pareto\nfrontier in the fine-tuning setting.\nVideo-LLaMA\n(Zhang et al., 2023e) introduces\na multi-branch cross-modal PT framework, en-\nabling LLMs to simultaneously process the vision\nand audio content of a given video while engag-\ning in conversations with humans. This framework\naligns vision with language as well as audio with\nlanguage.\nVideo-ChatGPT\n(Maaz et al., 2023) is a model\nspecifically designed for video conversations, ca-\npable of generating discussions about videos by\nintegrating spatiotemporal vision representations.\nShikra\n(Chen et al., 2023e) introduces a sim-\nple and unified pre-trained MM-LLM tailored for\nReferential Dialogue, a task involving discussions\nabout regions and objects in images. This model\ndemonstrates commendable generalization ability,\neffectively handling unseen settings.\nDLP\n(Jian et al., 2023) proposes the P-Former\nto predict the ideal prompt, trained on a dataset\nof single-modal sentences. This showcases the\nfeasibility of single-modal training to enhance MM\nlearning.\nBuboGPT\n(Zhao et al., 2023d) is a model con-\nstructed by learning a shared semantic space for a\ncomprehensive understanding of MM content. It\nexplores fine-grained relationships among different\nmodalities such as image, text, and audio.\nChatSpot\n(Zhao et al., 2023b) introduces a sim-\nple yet potent method for finely adjusting precise\nreferring instructions for MM-LLM, facilitating\nfine-grained interactions. The incorporation of pre-\ncise referring instructions, consisting of image- and\nregion-level instructions, enhances the integration\nof multi-grained VL task descriptions.\nQwen-VL\n(Bai et al., 2023b) is a multi-lingual\nMM-LLM that supports both English and Chinese.\nQwen-VL also allows the input of multiple images\nduring the training phase, improving its ability to\nunderstand the vision context.\nNExT-GPT\n(Wu et al., 2023d) is an end-to-end,\ngeneral-purpose any-to-any MM-LLM that sup-\nports the free input and output of image, video,\naudio, and text. It employs a lightweight alignment\nstrategy, utilizing LLM-centric alignment in the en-\ncoding phase and instruction-following alignment\nin the decoding phase.\nMiniGPT-5\n(Zheng et al., 2023b) is an MM-\nLLM integrated with inversion to generative vokens\nand integration with Stable Diffusion. It excels in\nperforming interleaved VL outputs for MM gen-\neration. The inclusion of classifier-free guidance\nduring the training phase enhances the quality of\ngeneration.\nLLaVA-1.5\n(Liu et al., 2023d) reports simple\nmodifications to the LLaVA framework, including\napplying an MLP projection and introducing VQA\ndata tailored for academic tasks, along with simple\nresponse formatting prompts. These adjustments\nresult in enhanced capabilities for MM understand-\ning.\nMiniGPT-v2\n(Chen et al., 2023d) is an MM-\nLLM designed as a unified interface for diverse\nVL multi-task learning. To create a single model\nproficient in handling multiple VL tasks, identifiers\nare incorporated for each task during both training\nand inference. This facilitates clear task distinction,\nultimately enhancing learning efficiency.\nCogVLM\n(Wang et al., 2023b) is an open-source\nMM-LLM that bridges the gap between modalities\nvia a trainable visual expert module within the at-\ntention and feedforward layers. This allows for a\ndeep fusion of MM features without compromising\nperformance on NLP downstream tasks.\nDRESS\n(Chen et al., 2023i) introduces a method\nusing natural language feedback to enhance align-\nment with human preferences. DRESS extends\nthe conditional reinforcement learning algorithm to\nintegrate non-differentiable natural language feed-\nback, training the model to generate appropriate\nresponses based on feedback.\nX-InstructBLIP\n(Panagopoulou et al., 2023) in-\ntroduces a cross-modal framework with instruction-\naware representations, scalable enough to em-\npower LLMs to handle diverse tasks across multi-\nple modalities, including image/video, audio, and\n3D. Notably, it achieves this without the need for\nmodality-specific PT.\nCoDi-2\n(Tang et al., 2023b) is a MM genera-\ntion model excelling in modality-interleaved in-\nstruction following, in-context generation, and user-\nmodel interaction by multi-turn conversations. It\nenhances CoDi (Tang et al., 2023c) to process intri-\ncate modality-interleaved inputs and instructions,\ngenerating latent features autoregressively.\nVILA\n(Lin et al., 2023) outperforms in vision\ntasks and shows remarkable reasoning ability while\nmaintaining text-only capabilities. It achieves this\nby harnessing the full capabilities of LLM learning,\nusing the interleaved attributes of image-text pairs,\nand implementing meticulous text data re-blending.\nF\nVL Benchmarks\nThe 18 VL benchmarks presented in Table 2 in-\nclude OKVQA (Schwenk et al., 2022), Icon-\nVQA (Lu et al., 2021), VQAv2 (Goyal et al., 2017),\nGQA (Hudson and Manning, 2019), VizWiz (Gu-\nrari et al., 2018), SQAI: ScienceQA-IMG (Lu\net al., 2022), VQAT: TextVQA (Singh et al., 2019),\nPOPE (Li et al., 2023k), MMEP: MME Per-\nception (Fu et al., 2023), MMEC: MME Cogni-\ntion (Fu et al., 2023), MMB: MMBenchmark (Liu\net al., 2023g), MMBCN: MMBench-Chinese (Liu\net al., 2023g), SEEDI: SEED-Bench (Image) (Li\net al., 2023c), LLaVAW: LLaVA-Bench (In-the-\nWild) (Liu et al., 2023a), MM-Vet (Yu et al.,\n2023c), QBench (Wu et al., 2023b), HM: Hate-\nfulMemes (Kiela et al., 2020), and VSR (Liu et al.,\n2023a).\nG\nTraining Dataset\nThe statistics for MM PT and MM IT dataset are\npresented in Table 3 and Table 4, respectively.\nDataset Name\nX Modality\n#.X\n#.T\n#.X-T\nALIGN (Jia et al., 2021)\nImage\n1.8B\n1.8B\n1.8B\nLTIP (Alayrac et al., 2022)\nImage\n312M\n312M\n312M\nMS-COCO (Lin et al., 2014)\nImage\n124K\n620K\n620K\nVisual Genome (Krishna et al., 2017)\nImage\n108K\n4.5M\n4.5M\nCC3M (Sharma et al., 2018)\nImage\n3.3M\n3.3M\n3.3M\nCC12M (Changpinyo et al., 2021)\nImage\n12.4M\n12.4M\n12.4M\nSBU (Ordonez et al., 2011)\nImage\n1M\n1M\n1M\nLAION-5B (Schuhmann et al., 2022)\nImage\n5.9B\n5.9B\n5.9B\nLAION-400M (Schuhmann et al., 2021)\nImage\n400M\n400M\n400M\nLAION-en (Schuhmann et al., 2022)\nImage\n2.3B\n2.3B\n2.3B\nLAION-zh (Schuhmann et al., 2022)\nImage\n142M\n142M\n142M\nLAION-COCO (Schuhmann et al., 2022b)\nImage\n600M\n600M\n600M\nFlickr30k (Young et al., 2014)\nImage\n31K\n158K\n158K\nAI Challenger Captions (Wu et al., 2017)\nImage\n300K\n1.5M\n1.5M\nCOYO (Byeon et al., 2022)\nImage\n747M\n747M\n747M\nWukong (Gu et al., 2022)\nImage\n101M\n101M\n101M\nCOCO Caption (Chen et al., 2015)\nImage\n164K\n1M\n1M\nWebLI (Chen et al., 2022b)\nImage\n10B\n12B\n12B\nEpisodic WebLI (Chen et al., 2023h)\nImage\n400M\n400M\n400M\nCC595k (Liu et al., 2023e)\nImage\n595K\n595K\n595K\nRefCOCO (Kazemzadeh et al., 2014)\nImage\n20K\n142K\n142K\nRefCOCO+ (Yu et al., 2016)\nImage\n20K\n142K\n142K\nVisual-7W (Zhu et al., 2016)\nImage\n47.3K\n328K\n328K\nOCR-VQA (Mishra et al., 2019)\nImage\n207K\n1M\n1M\nST-VQA (Biten et al., 2022)\nImage\n23K\n32K\n32K\nDocVQA (Mathew et al., 2021)\nImage\n12K\n50K\n50K\nTextVQA (Singh et al., 2019)\nImage\n28.4K\n45.3K\n45.3K\nDataComp (Gadre et al., 2023)\nImage\n1.4B\n1.4B\n1.4B\nGQA (Hudson and Manning, 2019)\nImage\n113K\n22M\n22M\nVGQA (Krishna et al., 2017)\nImage\n108K\n1.7M\n1.7M\nVQAv2 (Goyal et al., 2017)\nImage\n265K\n1.4M\n1.4M\nDVQA (Kafle et al., 2018)\nImage\n300K\n3.5M\n3.5M\nOK-VQA (Schwenk et al., 2022)\nImage\n14K\n14K\n14K\nA-OKVQA (Schwenk et al., 2022)\nImage\n23.7K\n24.9K\n24.9K\nText Captions (Sidorov et al., 2020)\nImage\n28K\n145K\n145K\nM3W (Interleaved) (Alayrac et al., 2022)\nImage\n185M\n182GB\n43.3M (Instances)\nMMC4 (Interleaved) (Zhu et al., 2023c)\nImage\n571M\n43B\n101.2M (Instances)\nObelics (Interleaved) (Lauren\u00e7on et al., 2023)\nImage\n353M\n115M\n141M (Instances)\nMSRVTT (Xu et al., 2016)\nVideo\n10K\n200K\n200K\nWebVid (Bain et al., 2021)\nVideo\n10M\n10M\n10M\nVTP (Alayrac et al., 2022)\nVideo\n27M\n27M\n27M\nAISHELL-1 (Chen et al., 2023b)\nAudio\n\u2013\n\u2013\n128K\nAISHELL-2 (Chen et al., 2023b)\nAudio\n\u2013\n\u2013\n1M\nWaveCaps (Mei et al., 2023)\nAudio\n403K\n403K\n403K\nVSDial-CN (Ni et al., 2024)\nImage, Audio\n120K (Image), 1.2M(Audio)\n120K\n1.2M\nTable 3: The statistics for MM PT datasets. #.X represents the quantity of X, #.T represents the quantity of Text,\nand #.X-T represents the quantity of X-Text pairs, where X can be Image, Video, or Audio.\nDataset Name\nType\nI\u2192O\nSource\nMethod\nMulti-Turn\n#.I/V/A\n#.Dialog Turn\n#.Instance\nMiniGPT-4\u2019s IT (Zhu et al., 2023a)\nSFT\nI+T\u2192T\nCC3M, CC12M\nAuto.\n%\n134M/\u2013/\u2013\n1\n5K\nStableLLaVA (Li et al., 2023i)\nSFT\nI+T\u2192T\nSD (Rombach et al., 2022)\nAuto.+Manu.\n%\n126K/\u2013/\u2013\n1\n126K\nLLaVA\u2019s IT (Zhang et al., 2023h)\nSFT\nI+T\u2192T\nMS-COCO\nAuto.\n\"\n81K/\u2013/\u2013\n2.29\n150K\nSVIT (Zhao et al., 2023a)\nSFT\nI+T\u2192T\nMS-COCO, Visual Genome\nAuto.\n\"\n108K/\u2013/\u2013\n5\n3.2M\nLLaVAR\u2019s IT (Zhang et al., 2023h)\nSFT\nI+T\u2192T\nMS-COCO, CC3M, LAION\nLLaVA+Auto.\n\"\n20K/\u2013/\u2013\n2.27\n174K\nShareGPT4V\u2019s IT (Chen et al., 2023f)\nSFT\nI+T\u2192T\nLCS, COCO, SAM, TextCaps, WikiArt\nAuto.+Manu.\n%\n100K/\u2013/\u2013\n\u2013\n\u2013\nDRESS\u2019s IT (Chen et al., 2023i)\nSFT\nI+T\u2192T\nLLaVA\u2019s IT, VLSafe\nAuto.+Manu.\n\"\n193K/\u2013/\u2013\n\u223c4\n\u2013\nVideoChat\u2019s IT (Li et al., 2023f)\nSFT\nV+T\u2192T\nWebVid\nAuto.\n\"\n\u2013/8K/\u2013\n1.82\n11K\nVideo-ChatGPT\u2019s IT (Maaz et al., 2023)\nSFT\nV+T\u2192T\nActivityNet (Caba Heilbron et al., 2015)\nInherit\n\"\n\u2013/100K/\u2013\n1\n100K\nVideo-LLaMA\u2019s IT (Zhang et al., 2023e)\nSFT\nI/V+T\u2192T\nMiniGPT-4, LLaVA, and VideoChat\u2019s IT\nAuto.\n\"\n81K/8K/\u2013\n2.22\n171K\nInstructBLIP\u2019s IT (Dai et al., 2023)\nSFT\nI/V+T\u2192T\nMultiple (InstructBLIP\u2019s Figure 2)\nAuto.\n%\n\u2013\n\u2013\n\u223c1.6M\nX-InstructBLIP\u2019s IT (Panagopoulou et al., 2023)\nSFT\nI/V/A/3D+T\u2192T\nMultiple (X-InstructBLIP\u2019s Figure 4)\nAuto.\n%\n\u2013\n\u2013\n\u223c1.8M\nMIMIC-IT (Li et al., 2023a)\nSFT\nI/V+T\u2192T\nMultiple\nAuto.\n%\n8.1M/502K/\u2013\n1\n2.8M\nPandaGPT\u2019s IT (Su et al., 2023)\nSFT\nI+T\u2192T\nMiniGPT-4 and LLaVA\u2019s IT\nInherit\n\"\n81K/\u2013/\u2013\n2.29\n160K\nMGVLID (Zhao et al., 2023b)\nSFT\nI+B+T\u2192T\nMultiple\nAuto.+Manu.\n%\n108K/\u2013/\u2013\n\u2013\n108K\nM3IT (Li et al., 2023h)\nSFT\nI/V/B+T\u2192T\nMultiple\nAuto.+Manu.\n%\n\u2013/\u2013/\u2013\n1\n2.4M\nLAMM (Yin et al., 2023b)\nSFT\nI+3D+T\u2192T\nMultiple\nAuto.+Manu.\n\"\n91K/\u2013/\u2013\n3.27\n196K\nBuboGPT\u2019s IT (Zhao et al., 2023d)\nSFT\n(I+A)/A+T\u2192T\nClotho, VGGSS\nAuto.\n%\n5K/\u2013/9K\n\u2013\n9K\nmPLUG-DocOwl\u2019s IT (Ye et al., 2023b)\nSFT\nI/Tab/Web+T\u2192T\nMultiple\nInherit\n%\n\u2013\n\u2013\n\u2013\nT2M (Wu et al., 2023d)\nSFT\nT\u2192I/V/A+T\nWebVid, CC3M, AudioCap\nAuto.\n%\n4.9K/4.9K/4.9K\n1\n14.7K\nMosIT (Wu et al., 2023d)\nSFT\nI+V+A+T\u2192I+V+A+T\nYoutube, Google, Flickr30k, Midjourney, etc.\nAuto.+Manu.\n\"\n4K/4K/4K\n4.8\n5K\nOsprey\u2019s IT (Yuan et al., 2023a)\nSFT\nI+T\u2192T\nMS-COCO, RefCOCO, RefCOCO+, LLaVA\u2019s IT etc. (fine-grained region-text dataset)\nAuto.+Manu.\n\"\n\u2013/\u2013/\u2013\n\u223c4\n724K\nLLaVA-RLHF (Sun et al., 2023b)\nRLHF\nI+T\u2192T\nCollected human preference\nManu.\n%\n\u2013/\u2013/\u2013\n\u2013\n10K\nDRESS\u2019s IT (Chen et al., 2023i)\nRLHF\nI+T\u2192T\nLLaVA\u2019s IT, VLSafe\nAuto.+Manu.\n\"\n33K/\u2013/\u2013\n\u223c4\n\u2013\nRLHF-V\u2019s IT (Yu et al., 2023b)\nRLHF\nI+T\u2192T\nCollected human preference\nManu.\n%\n\u2013/\u2013/\u2013\n\u2013\n1.4K\nVLFeedback (Li et al., 2023g)\nRLHF\nI+T\u2192T\nResponses generated by 12 MM-LLMs\nAuto.\n%\n\u2013/\u2013/\u2013\n\u2013\n80K\nRTVLM (Li et al., 2024a)\nRLHF\nI+T\u2192T\nNew question-image pairs based on publicly available images\nor originally diffusion-generated images (Gallegos et al., 2023)\nAuto.+Manu.\n%\n\u2013/\u2013/\u2013\n\u2013\n5K\nVLGuard\u2019s IT (Zong et al., 2024)\nRLHF\nI+T\u2192T\nSource image data from various datasets\nAuto.\n%\n3K/\u2013/\u2013\n\u2013\n3K\nMMViG (Yan et al., 2024)\nRLHF\nI+T\u2192T\nMS-COCO\nManu.\n%\n16K/\u2013/\u2013\n\u2013\n16K\nTable 4: The statistics for MM IT datasets. I\u2192O: Input to Output Modalities, T: Text, I: Image, V: Video, A: Audio,\nB: Bounding box, 3D: Point Cloud, Tab: Table, and Web: Web page.\n"
  },
  {
    "title": "MaLA-500: Massive Language Adaptation of Large Language Models",
    "link": "https://arxiv.org/pdf/2401.13303.pdf",
    "upvote": "11",
    "text": "MaLA-500: Massive Language Adaptation of Large Language Models\nPeiqin Lin\u22171,2, Shaoxiong Ji\u22173, J\u00f6rg Tiedemann3, Andr\u00e9 F. T. Martins4,5,6, Hinrich Sch\u00fctze1,2\n1Center for Information and Language Processing, LMU Munich\n2Munich Center for Machine Learning\n3University of Helsinki\n4Instituto Superior T\u00e9cnico (Lisbon ELLIS Unit)\n5Instituto de Telecomunica\u00e7\u00f5es\n6Unbabel\nlinpq@cis.lmu.de, shaoxiong.ji@helsinki.fi\nAbstract\nLarge language models have advanced the state\nof the art in natural language processing. How-\never, their predominant design for English or\na limited set of languages creates a substan-\ntial gap in their effectiveness for low-resource\nlanguages. To bridge this gap, we introduce\nMaLA-500, a novel large language model de-\nsigned to cover an extensive range of 534\nlanguages. To train MaLA-500, we employ\nvocabulary extension and continued pretrain-\ning on LLaMA 2 with Glot500-c.\nOur ex-\nperiments on SIB-200 show that MaLA-500\nachieves state-of-the-art in-context learning re-\nsults. We release MaLA-500 at https://\nhuggingface.co/MaLA-LM.\n1\nIntroduction\nLarge Language Models (LLMs), e.g., LLaMA\n(Touvron et al., 2023a,b), Mistral (Jiang et al., 2023,\n2024), and ChatGPT,1 have shown remarkable per-\nformance in natural language understanding and\ngeneration. Follow-up studies (Bang et al., 2023;\nLai et al., 2023; Ahuja et al., 2023a,b) observe\nthat these English-centric LLMs, such as LLaMA\nwith mainly English as the training data, are capa-\nble of handling some high-resource non-English\nlanguages, benefiting from the inclusion of non-\nEnglish language data during pretraining. However,\ntheir applicability to low-resource languages is still\nlimited due to data scarcity and model capability\nin terms of model size.\nRecently, several generative multilingual LLMs,\nsuch as XGLM (Lin et al., 2021), mGPT (Shliazhko\net al., 2022), and BLOOM (Scao et al., 2022), have\nemerged. Notably, the current language coverage\nfor these LLMs is limited to up to 60 languages,\nhighlighting the remaining need for further work\non massively multilingual LLMs.\n*Equal contribution.\n1https://openai.com/blog/chatgpt\nImaniGooghari et al. (2023) have achieved a\nsignificant milestone in the realm of massive lan-\nguage adaptation by extending the language cover-\nage of a small-scale multilingual language model,\nXLM-R (Conneau et al., 2020) - an auto-encoding\nmodel with 278M parameters, from 100 languages\nto an impressive number of 534 languages, and\nintroducing an extended model, Glot500-m with\n395M parameters. ImaniGooghari et al. (2023)\nintroduce the Glot500-c corpora spanning 534 lan-\nguages from 47 language families, and then ap-\nply vocabulary extension and continued pretrain-\ning to create Glot500-m.\nThe introduction of\nGlot500-c significantly mitigates the challenge of\ndata scarcity for low-resource languages. More-\nover, the adaptation method is more favorable than\ntraining from scratch, as it requires fewer com-\nputational resources and emits a smaller carbon\nfootprint. This success serves as a strong motiva-\ntion for our exploration into the massive language\nadaptation of LLMs.\nThis work aims to extend the capabilities of\nLLMs to encompass a wider range of languages.\nExisting works on language adaptation of pre-\ntrained models provide extended coverage across\na wide linguistic spectrum but are limited to rela-\ntively small model sizes - mostly at the hundred\nmillion scale. Our study pushes the boundaries\nby exploring language adaptation techniques for\nLLMs with model parameters scaling up to 10B.\nOur investigation delves into generative LLMs with\na substantial increase in model parameters and their\nin-context learning capabilities that enhance con-\ntextual and linguistic relevance across a diverse\nrange of languages. We address the challenges of\nadapting LLMs to low-resource languages, such as\ndata sparsity, domain-specific vocabulary, and lin-\nguistic diversity. We study continued pretraining of\nopen LLM (i.e., LLaMA 2 (Touvron et al., 2023b)),\nvocabulary extension, and adaptation techniques\n(i.e., LoRA low-rank reparameterization (Hu et al.,\narXiv:2401.13303v1  [cs.CL]  24 Jan 2024\n2022)). We train and release MaLA-500 that cov-\ners more than 500 languages in various domains.\nWe evaluate MaLA-500 on SIB-200 and the results\nshow that MaLA-500 outperforms existing open\nLLMs of close or slightly larger model size. This\nwork broadens the accessibility of LLMs, making\nthem valuable for a more diverse set of language-\nspecific use cases, especially for low-resource ones,\nand addressing the equality issue by removing lan-\nguage barriers for speakers of many languages, es-\npecially those underrepresented languages covered\nby existing LLMs.\n2\nMassive Language Adaptation\nThe principle of massive language adaption of large\nlanguage models accommodates the utilization\nof a massively multilingual corpus (Section 2.1),\nthe strong base LLM (Section 2.2), and the tech-\nnique for effective language adaption (Sections 2.3\nand 2.4).\n2.1\nData\nWe use Glot500-c (ImaniGooghari et al., 2023)\ncovering 534 languages2 as the training data of\nMaLA-500. See \u00a7A for the list of languages with\ntheir data amounts. We sample languages from\nthe imbalanced dataset according to a multinomial\ndistribution, with \u03b1 = 0.3 for vocabulary extension\nand continued pretraining.\n2.2\nModel\nWe choose LLaMA 2 (Touvron et al., 2023b)\nto start continual training. LLaMA series mod-\nels (Touvron et al., 2023a), with model weights\nreleased publicly, have gained popularity in the re-\nsearch community. Despite being English-centric\ncompared to their multilingual counterparts, they\nhave shown remarkable capacity for multiple lan-\nguages (Zhao et al., 2024). We choose the latest\nLLaMA 2, trained on 2 trillion tokens, as our base\nmodel to benefit from its outstanding language ca-\npacity. Our study chooses the 7B model with 32\ntransformer layers.\n2.3\nVocabulary Extension\nThe original LLaMA 2\u2019s 32,000 tokenizer covers\nEnglish and a small fraction of other European lan-\nguages using Latin or Cyrillic scripts. To enhance\nits capability and encoding efficiency for a broader\n2We define languages using the ISO 639-3 code combined\nwith the corresponding written script. For example, \u201ceng_Latn\u201d\nrepresents English written in the Latin script.\nrange of languages, we extend the vocabulary with\nGlot500-c. Specifically, we initially train a multi-\nlingual tokenizer with SentencePiece (Kudo and\nRichardson, 2018) on the sampled Glot500-c with\na vocabulary of 250,000. Subsequently, we merge\nthe trained tokenizer with the original LLaMA 2\ntokenizer by taking the union of their vocabularies.\nAs a result, we obtain the MaLA-500\u2019s tokenizer\nwith a vocabulary size of 260,164. After vocabu-\nlary extension and resizing the embedding layer,\nthe model size becomes 8.6B.\nWe measure the impact of vocabulary extension\non the development set of Glot500-c by analyz-\ning the reduction in segmentation length for each\nlanguage. The results indicate that the effect of\nvocabulary extension varies, ranging from 8% (En-\nglish, eng_Latn) to 88% (Oriya, ori_Orya). Unsur-\nprisingly, vocabulary extension has a larger effect\non languages written in non-Latin scripts than on\nthose in the Latin script. However, for some low-\nresource languages written in the Latin script, e.g.,\nKabiy\u00e8 (kbp_Latn) and Vietnamese (vie_Latn), the\nsegmentation length is shortened by around 50%.\n2.4\nContinued Pretraining\nWe employ continued pretraining for language\nadaptation with low-rank adaptation (LoRA, Hu\net al., 2022) to enable parameter-efficient training,\ngiven the limitation of our computing resources.\nLoRA injects trainable rank decomposition matri-\nces, which approximate the large weight matrices\nwith a lower rank, to the pretrained model weights.\nIt significantly reduces the computational complex-\nity and thus saves the training cost. We continually\ntrain the casual language model to update the rank-\ndecomposition matrices while freezing the model\nweights of pretrained models, allowing the contin-\nually trained language model to learn from new\ndata in new languages without completely losing\nits previous language capacity. Continual train-\ning of large language models requires substantial\ncomputational resources. We adopt efficient dis-\ntributed training setups on supercomputers to make\nthe training process feasible.\n2.5\nTraining\nHardware and Software\nWe train our model on\nthe computing cluster with the theoretical peak per-\nformance of 2 petaflops on GPU nodes. We deploy\ndistributed training on 24 Nvidia Ampere A100\nGPUs. As for software, we utilize the Huggingface\nTransformers (Wolf et al., 2020), PEFT (Parameter-\nEfficient Fine-Tuning)3, and DeepSpeed (Rasley\net al., 2020). We use the ZeRO redundancy opti-\nmizer (Rajbhandari et al., 2020) and maximize the\nbatch size that fits the memory of each GPU. We\nemploy mixed-precision training using the bfloat16\nformat.\nHyperparameters\nThe learning rate is set at 2e-\n4. A weight decay of 0.01 is applied to penalize\nlarge weights and mitigate overfitting. The train-\nable LoRA module targets all linear layers, exclud-\ning the language model head. In our setting, the\nfinal model has 10B parameters in total, in which\n2B parameters are trainable. The LoRA module\nis incorporated with a rank of 8, an alpha value\nof 32, and a dropout rate of 0.05, contributing to\nthe model\u2019s adaptability and regularization during\ntraining. The context window is 2k. We maxi-\nmize the batch size to fit the memory, making a\nglobal batch size of 384. The model undergoes\nthree training epochs. Checkpoints are saved every\n500 steps, and we employ early stopping to select\nthe checkpoint that exhibits the most favorable av-\nerage performance on downstream tasks. In this\nmodel release, we only consider the topic classifi-\ncation task.\nEnvironmental Impacts\nWe train our model on\nCSC\u2019s carbon-neutral data center, with all electric-\nity generated with renewable hydropower,4 and the\nwaste heat is utilized in district heating to further\nreduce CO2 footprint.5\n3\nEvaluation\nWe evaluate the few-shot learning capability of\nMaLA-500 and compare it with other LLMs on\na topic classification dataset of SIB-200 (Adelani\net al., 2023). The classification task involves seven\nclasses, namely science/technology, travel, politics,\nsports, health, entertainment, and geography. Our\nevaluation spans a diverse set of 177 languages, ob-\ntained by intersecting the language sets of SIB-200\nand Glot500-c. In this evaluation, we compute the\naccuracy of 99 samples for each language, with\nshots randomly sampled from the respective train-\ning sets.\nThe evaluated LLM receives a structured prompt,\nwhich is the concatenation of few-shot examples\n3https://huggingface.co/docs/peft/\nindex\n4https://www.csc.fi/\nsustainable-development\n5https://www.csc.fi/sustainability-1\nand the sample intended for prediction. The format\nfor both a few-shot example and the sample to\npredict is defined as follows:\nTopic Classification: science/technology,\ntravel, politics, sports, health, entertain-\nment, geography.\nThe label of [sent] is [label]\nwhere [sent] is the sentence for topic classification,\nand [label] is the ground truth. [label] is included\nif the sample serves as a few-shot example or omit-\nted if it is the sample to predict. The constructed\nprompt is then used as input to the LLM. Subse-\nquently, the evaluated LLM is prompted to estimate\nthe probability of the label over the label set based\non the provided prompt. This evaluation process is\nimplemented using the lm-evaluation-harness.6\n3.1\nComparison across LLMs\nWe compare MaLA-500 with LLaMA 2-7B, mGPT-\n13B, BLOOM-7B1, and XGLM-7.5B, focusing on\nthe 3-shot in-context learning macro-average ac-\ncuracy across languages. As shown in Table. 1,\nMaLA-500 outperforms previous LLMs signifi-\ncantly. Notably, MaLA-500 exhibits a remarkable\n12.16% improvement compared to our base model,\nLLaMA 2, highlighting MaLA-500\u2019s substantial\ncontribution to enhancing the multilingual capacity\nof LLMs.\nFig. 1 provides a detailed performance analysis\nacross languages. Notably, MaLA-500 surpasses\nprevious LLMs in the sense that no language ob-\ntains random results (0-20%) and the number of\nlanguages with very low accuracy (20-40%) is sig-\nnificantly reduced. Moreover, MaLA-500 excels in\nthe number of languages achieving 60% accuracy\nor higher (65 altogether).\nIn our comprehensive analysis of contributing\nfactors, a significant Pearson correlation of 0.53\nis observed between the effect of vocabulary ex-\ntension and the performance gain. This observa-\ntion holds true for languages with both non-Latin\nscripts, such as Kannada (kan_Knda), Malayalam\n(mal_Mlym), and Tigrinya (tir_Ethi), as well as\nLatin scripts, such as Igbo (ibo_Latn) and Yoruba\n(yor_Latn). Furthermore, we note that the corpus\nsize of a language exhibits a weak correlation of\n0.13 with its performance gain. In contrast, the\n6https://github.com/EleutherAI/\nlm-evaluation-harness\nModel\nAccuracy\nLLaMA 2-7B\n41.94\nmGPT-13B\n39.96\nBLOOM-7B1\n44.22\nXGLM-7.5B\n47.53\nMaLA-500\n54.10\nTable 1: 3-shot in-context learning\nmacro-average accuracy (%) on\nSIB-200 of different LLMs. mGPT\nhas no model with around 7B pa-\nrameters, so we choose a larger one\nwith 13B parameters.\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA\n0%\n25%\n50%\n75%\n100%\n0-20\n20-40\n40-60\n60-80\nFigure 1: Detailed results of 3-shot in-context learning on SIB-200. X-axis:\nthe number of languages in different accuracy ranges (%).\ncorpus size of the language family to which a lan-\nguage belongs demonstrates a moderate correlation\nof 0.40.\n3.2\nEffect of Number of Shots\nFig. 2 illustrates the relationship between accu-\nracy and the number of in-context examples (i.e.,\nshots). As the number of in-context shots increases,\nthere is a corresponding rise in accuracy. Notably,\nwith just 1-shot, accuracy exhibits randomness at\n26.08%, indicating 1-shot provides limited infor-\nmation for task learning. The transition from 1 shot\nto 2 shots/3 shots results in a significant improve-\nment, with performances boosted by 20.97% and\n28.02%, respectively. This highlights the effective-\nness of increasing the number of shots. MaLA-500\nachieves its peak performance at approximately\n60% accuracy with 6-10 in-context shots. This\nmay be attributed to the multi-class nature of the\nSIB-200 dataset, necessitating more shots for learn-\ning intricate input-label mappings.\nIn Fig. 3, a more nuanced portrayal of results\naligns with the observations made in Fig. 2. In the\nrealm of 1-shot in-context learning, approximately\n40 languages exhibit erratic results. As the number\nof shots increases, there is a reduction in the num-\nber of languages achieving low accuracy (20-40%),\ncoupled with a growing cohort achieving high ac-\ncuracy (60-80%). Notably, in the case of 10-shot\nin-context learning, 101 (57%) languages surpass\nthe threshold of 60% accuracy.\nFurther examination into individual language\ntrends reveals that some low-resource languages\nrequire more shots to achieve better performance\n(e.g., pes_Arab for Persian) or even exhibit poor\nperformance with 10 shots (e.g., dzo_Tibt for\nDzongkha and ayr_Latn for Central Aymara). In\ncontrast, high-resource languages, such as fra_Latn\nfor French, demonstrate impressive performance\neven with fewer shots, and increasing the number\nof shots results in only marginal improvement.\nNumber of shots\nAccuracy\n0\n20\n40\n60\n80\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 2: In-context learning macro-average accuracy\n(%) on SIB-200 with different number of shots using\nMaLA-500.\n4\nRelated Work\n4.1\nMultilingual Language Models\nLanguage model development has endeavored to\nbroaden the scope of pertaining languages to ad-\ndress multilingual scenarios.\nPretrained multi-\nlingual models have been able to accommodate\nup to a hundred or more languages. Noteworthy\nexamples include mBERT (Devlin et al., 2019),\nwhich supports 104 languages, XLM-R (Conneau\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0%\n25%\n50%\n75%\n100%\n0-20\n20-40\n40-60\n60-80\nFigure 3: Detailed results of in-context learning on SIB-\n200 using MaLA-500. X-axis: the number of languages\nin different accuracy ranges (%). Y-axis: number of\nshots.\net al., 2020) covering 100 languages, mBART (Liu\net al., 2020) designed for 25 languages, mT5 (Xue\net al., 2021) spanning 101 languages, XGLM (Lin\net al., 2021) across 30 languages, GPT-3 cover-\ning 118 languages (93% English), mGPT (Shli-\nazhko et al., 2022) accommodating 60 languages,\nand BLOOM (Scao et al., 2022) supporting 46 lan-\nguages and 13 programming languages.\nSurprisingly, two recent multilingual language\nmodels have surpassed the conventional limit by\nsupporting more than 400 languages. Glot500-\nm (ImaniGooghari et al., 2023) spans 511 lan-\nguages through vocabulary extension and contin-\nued training based on XLM-R. SERENGETI (Ade-\nbara et al., 2022) goes even further by supporting\n517 African languages and language varieties, writ-\nten in five different scripts, employing models in-\nspired by both ELECTRA (Clark et al., 2020) and\nXLM-R. MADLAD (Kudugunta et al., 2023) cov-\ners 419 languages and trains an 8B language model\nfrom scratch with an adapted UL2 objective (Tay\net al., 2022). Our work is concurrent with the MAD-\nLAD400 language model. We distinguish it from:\n1) language coverage. Our work covered more than\n500 languages, a number comparable to that of\nencoder-only models and surpassing MADLAD-\n400 by an additional 100 languages. 2) training\nmethods. We consider continual training to benefit\nfrom the learned knowledge of the original models.\n3) model architecture. We adopt an open model\narchitecture, i.e., LLaMA, while MADLAD uses\nUL2.\n4.2\nLanguage Adaptation\nBefore the advent of LLMs, diverse approaches\nare employed to adapt small-scale multilingual\nlanguage models to new languages. These meth-\nods include using adapters (Pfeiffer et al., 2020;\n\u00dcst\u00fcn et al., 2020; Pfeiffer et al., 2020; Nguyen\net al., 2021; Faisal and Anastasopoulos, 2022; Yong\net al., 2022), vocabulary extension and substitu-\ntion (Chau et al., 2020; Wang et al., 2020; M\u00fcller\net al., 2020, 2021; Pfeiffer et al., 2021; Chen et al.,\n2023b; Downey et al., 2023), leveraging mono-\nlingual corpora (Ebrahimi and Kann, 2021), and\nutilizing bilingual lexicons (Wang et al., 2022).\nWhile language models have been scaled up\nsignificantly, their coverage is limited to a spe-\ncific set of languages. To address this constraint,\nvarious methods have been proposed to expand\nthe applicability of these Large Language Mod-\nels (LLMs) across a broader range of languages,\ncatering to both general-purpose tasks and specific\napplications like machine translation. These meth-\nods also involve vocabulary extension (Cui et al.,\n2023), continued pretraining and instruction-tuning\n(Yong et al., 2022; Cui et al., 2023; Chen et al.,\n2023a), and parallel corpora exploitation (Cahyawi-\njaya et al., 2023; Yang et al., 2023; Zhu et al., 2023;\nXu et al., 2023). Despite these efforts, massive\nlanguage adaptation of LLMs for general-purpose\ntasks across diverse languages remains an area yet\nto be thoroughly explored.\n5\nConclusion\nWe present a pioneering effort in massive lan-\nguage adaptation on LLMs, focusing on extend-\ning LLaMA 7B to our model, MaLA-500. This\nadaptation involves vocabulary extension and con-\ntinued pretraining with LoRA. Our approach leads\nto MaLA-500 achieving state-of-the-art in-context\nlearning capabilities, as demonstrated on the task\nof SIB-200. We release the model weights pub-\nlicly to facilitate future research. This work marks\na substantial advancement in applying LLMs to a\ndiverse range of languages.\nLimitations\nData\nWe do not collect more high-resource data\nand instead use only Glot500-c, which does not put\nfull effort on collecting data for high-resource lan-\nguages. In addition, the flores200-based SIB-200\nevaluation set is included in the training data since\nGlot500-c includes flores200, but the classification\nlabels are not provided.\nModel Size\nOur study limits the maximum model\nsize to 10B parameters. In our future work, we plan\nto expand to a larger model.\nEthical Implications\nLLMs have been known to exhibit biases present in\ntheir training data. When extending LLMs to low-\nresource languages, there is a risk of propagating\nbiases from high-resource languages to underrep-\nresented ones. Careful attention must be paid to\nmitigate bias and ensure fairness in data collection\nand model training. The paper aims to make LLMs\nmore accessible for underrepresented languages.\nStill, there is a risk of creating a digital language\ndivide if certain communities are left out due to\nlimited technological access. Future work would\naddress biases by conducting bias audits on the\ntraining data, debiasing the models during genera-\ntion, and continuously monitoring model outputs.\nAcknowledgements\nThis work was funded by the European Research\nCouncil (grants #740516 and #758969), EU\u2019s\nHorizon Europe Research and Innovation Actions\n(UTTER, contract 101070631), and the European\nUnion\u2019s Horizon Europe research and innovation\nprogramme under grant agreement No 101070350\nand from UK Research and Innovation (UKRI)\nunder the UK government\u2019s Horizon Europe fund-\ning guarantee [grant number 10052546]. The au-\nthors wish to acknowledge CSC \u2013 IT Center for\nScience, Finland, for generous computational re-\nsources. Shaoxiong Ji and Peiqin Lin acknowledge\ntravel support from ELISE (GA no 951847).\nReferences\nIfe Adebara, AbdelRahim Elmadany, Muhammad\nAbdul-Mageed, and Alcides Alcoba Inciarte. 2022.\nSERENGETI: Massively multilingual language mod-\nels for africa. arXiv preprint arXiv:2212.10785.\nDavid Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen,\nNikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Hao-\nnan Gao, and En-Shiun Annie Lee. 2023. SIB-200:\nA simple, inclusive, and big evaluation dataset for\ntopic classification in 200+ languages and dialects.\nCoRR, abs/2309.07445.\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi\nJain, Harshita Diddee, Samuel Maina, Tanuja Ganu,\nSameer Segal, Maxamed Axmed, Kalika Bali, and\nSunayana Sitaram. 2023a. MEGA: multilingual eval-\nuation of generative AI. CoRR, abs/2303.12528.\nSanchit Ahuja, Divyanshu Aggarwal, Varun Gumma,\nIshaan Watts, Ashutosh Sathe, Millicent Ochieng,\nRishav Hada, Prachi Jain, Maxamed Axmed, Ka-\nlika Bali, and Sunayana Sitaram. 2023b. MEGA-\nVERSE: benchmarking large language models across\nlanguages, modalities, models and tasks.\nCoRR,\nabs/2311.07463.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. CoRR, abs/2302.04023.\nSamuel Cahyawijaya, Holy Lovenia, Tiezheng Yu,\nWilly Chung, and Pascale Fung. 2023. Instruct-align:\nTeaching novel languages with to LLMs through\nalignment-based cross-lingual instruction.\nCoRR,\nabs/2305.13627.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual bert, a small treebank, and\na small corpus. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 1324\u20131334. Association\nfor Computational Linguistics.\nPinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Barry\nHaddow, and Kenneth Heafield. 2023a. Monolingual\nor multilingual instruction tuning: Which makes a\nbetter Alpaca. CoRR, abs/2309.08958.\nYihong Chen, Kelly Marchisio, Roberta Raileanu,\nDavid Ifeoluwa Adelani, Pontus Stenetorp, Sebas-\ntian Riedel, and Mikel Artetxe. 2023b. Improving\nlanguage plasticity via pretraining with active forget-\nting. CoRR, abs/2307.01163.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learning\nRepresentations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440\u2013\n8451.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for Chinese LLaMA and\nAlpaca. CoRR, abs/2304.08177.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nC. M. Downey, Terra Blevins, Nora Goldfine, and Shane\nSteinert-Threlkeld. 2023. Embedding structure mat-\nters: Comparing methods to adapt multilingual vo-\ncabularies to new languages. CoRR, abs/2309.04679.\nAbteen Ebrahimi and Katharina Kann. 2021. How to\nadapt your pretrained multilingual model to 1600 lan-\nguages. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n4555\u20134567. Association for Computational Linguis-\ntics.\nFahim Faisal and Antonios Anastasopoulos. 2022.\nPhylogeny-inspired adaptation of multilingual mod-\nels to new languages. CoRR, abs/2205.09634.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nAyyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kar-\ngaran, Silvia Severini, Masoud Jalili Sabet, Nora\nKassner, Chunlan Ma, Helmut Schmid, Andr\u00e9 Mar-\ntins, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2023.\nGlot500: Scaling multilingual corpora and language\nmodels to 500 languages. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1082\u2013\n1117, Toronto, Canada. Association for Computa-\ntional Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nth\u00e9e Lacroix, and William El Sayed. 2023. Mistral\n7b. CoRR, abs/2310.06825.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, et al. 2024.\nMixtral of experts. arXiv preprint arXiv:2401.04088.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66\u201371. Asso-\nciation for Computational Linguistics.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier\nGarcia, Christopher A Choquette-Choo, Katherine\nLee, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, et al. 2023. MADLAD-400: A mul-\ntilingual and document-level large audited dataset.\narXiv preprint arXiv:2309.04662.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Vey-\nseh, Hieu Man, Franck Dernoncourt, Trung Bui, and\nThien Huu Nguyen. 2023.\nChatGPT beyond En-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. CoRR,\nabs/2304.05613.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona T. Diab, Veselin\nStoyanov, and Xian Li. 2021.\nFew-shot learn-\ning with multilingual language models.\nCoRR,\nabs/2112.10668.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation.\nTransac-\ntions of the Association for Computational Linguis-\ntics, 8:726\u2013742.\nBenjamin M\u00fcller, Antonios Anastasopoulos, Beno\u00eet\nSagot, and Djam\u00e9 Seddah. 2021. When being un-\nseen from mbert is just the beginning: Handling new\nlanguages with multilingual language models. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n448\u2013462. Association for Computational Linguistics.\nBenjamin M\u00fcller, Beno\u00eet Sagot, and Djam\u00e9 Seddah.\n2020. Can multilingual language models transfer to\nan unseen dialect? A case study on north african\narabizi. CoRR, abs/2005.00318.\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben\nVeyseh, and Thien Huu Nguyen. 2021. Trankit: A\nlight-weight transformer-based toolkit for multilin-\ngual natural language processing. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: System\nDemonstrations, EACL 2021, Online, April 19-23,\n2021, pages 80\u201390. Association for Computational\nLinguistics.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. MAD-X: an adapter-based frame-\nwork for multi-task cross-lingual transfer. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2020,\nOnline, November 16-20, 2020, pages 7654\u20137673.\nAssociation for Computational Linguistics.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. Unks everywhere: Adapting multi-\nlingual language models to new scripts. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 10186\u201310203. Association\nfor Computational Linguistics.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and\nYuxiong He. 2020. ZeRO: Memory optimizations\ntoward training trillion parameter models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1\u2013\n16. IEEE.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. DeepSpeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505\u20133506.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBLOOM: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mGPT: Few-shot learners go multi-\nlingual. CoRR, abs/2204.07580.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Jason Wei, Xuezhi Wang, Hyung Won Chung,\nDara Bahri, Tal Schuster, Steven Zheng, et al. 2022.\nUL2: Unifying language learning paradigms. In The\nEleventh International Conference on Learning Rep-\nresentations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a.\nLLaMA:\nOpen and efficient foundation language models.\nCoRR, abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur\u00e9lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nAhmet \u00dcst\u00fcn, Arianna Bisazza, Gosse Bouma, and Gert-\njan van Noord. 2020. Udapter: Language adaptation\nfor truly universal dependency parsing. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 2302\u20132315. Associa-\ntion for Computational Linguistics.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2022. Expanding pretrained models to thousands\nmore languages via lexicon-based adaptation.\nIn\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 863\u2013877. Association for Computational\nLinguistics.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and Dan\nRoth. 2020. Extending multilingual BERT to low-\nresource languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 2649\u20132656. Association\nfor Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38\u201345.\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-\nsan Awadalla. 2023. A paradigm shift in machine\ntranslation: Boosting translation performance of\nlarge language models. CoRR, abs/2309.11674.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498.\nWen Yang, Chong Li, Jiajun Zhang, and Chengqing\nZong. 2023. Bigtrans: Augmenting large language\nmodels with multilingual translation capability over\n100 languages. CoRR, abs/2305.18098.\nZheng Xin Yong, Hailey Schoelkopf, Niklas Muen-\nnighoff, Alham Fikri Aji, David Ifeoluwa Adelani,\nKhalid Almubarak, M. Saiful Bari, Lintang Sutawika,\nJungo Kasai, Ahmed Baruwa, Genta Indra Winata,\nStella Biderman, Dragomir Radev, and Vassilina\nNikoulina. 2022. BLOOM+1: adding language sup-\nport to BLOOM for zero-shot prompting. CoRR,\nabs/2212.09535.\nJun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuan-\njing Huang. 2024. LLaMA beyond English: An em-\npirical study on language capability transfer. arXiv\npreprint arXiv:2401.01055.\nWenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan,\nJingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun\nChen, and Lei Li. 2023. Extrapolating large language\nmodels to non-english by aligning languages. CoRR,\nabs/2308.04948.\nA\nLanguages\nThe list of languages of Glot500-c used to train\nMaLA-500 with the number of available sentences\nand language family information for each language\nis available in Tables 2, 3 and 4.\nB\nDetailed Results\nDetailed results of few-shot learning evaluation are\nshown in Tables 5 and 6.\nLang\n|Sent|\nFamily\nLang\n|Sent|\nFamily\nLang\n|Sent|\nFamily\nhbs_Latn\n63411156\nindo1319\nhin_Deva\n7046700\nindo1319\nton_Latn\n1216118\naust1307\nmal_Mlym\n48098273\ndrav1251\nkor_Hang\n6468444\nkore1284\ntah_Latn\n1190747\naust1307\naze_Latn\n46300705\nory_Orya\n6266475\nindo1319\nlat_Latn\n1179913\nindo1319\nguj_Gujr\n45738685\nindo1319\nurd_Arab\n6009594\nindo1319\nsrn_Latn\n1172349\nindo1319\nben_Beng\n43514870\nindo1319\nswa_Latn\n5989369\newe_Latn\n1161605\natla1278\nkan_Knda\n41836495\ndrav1251\nsqi_Latn\n5526836\nindo1319\nbem_Latn\n1111969\natla1278\ntel_Telu\n41580525\ndrav1251\nbel_Cyrl\n5319675\nindo1319\nefi_Latn\n1082621\natla1278\nmlt_Latn\n40654838\nafro1255\nafr_Latn\n5157787\nindo1319\nbis_Latn\n1070170\nindo1319\nfra_Latn\n39197581\nindo1319\nnno_Latn\n4899103\nindo1319\norm_Latn\n1067699\nspa_Latn\n37286756\nindo1319\ntat_Cyrl\n4708088\nturk1311\nhaw_Latn\n1062491\naust1307\neng_Latn\n36122761\nindo1319\nast_Latn\n4683554\nindo1319\nhmo_Latn\n1033636\npidg1258\nfil_Latn\n33493255\naust1307\nmon_Cyrl\n4616960\nmong1349\nkat_Geor\n1004297\nkart1248\nnob_Latn\n32869205\nindo1319\nhbs_Cyrl\n4598073\nindo1319\npag_Latn\n983637\naust1307\nrus_Cyrl\n31787973\nindo1319\nhau_Latn\n4368483\nafro1255\nloz_Latn\n964418\natla1278\ndeu_Latn\n31015993\nindo1319\nsna_Latn\n4019596\natla1278\nfry_Latn\n957422\nindo1319\ntur_Latn\n29184662\nturk1311\nmsa_Latn\n3929084\nmya_Mymr\n945180\nsino1245\npan_Guru\n29052537\nindo1319\nsom_Latn\n3916769\nafro1255\nnds_Latn\n944715\nindo1319\nmar_Deva\n28748897\nindo1319\nsrp_Cyrl\n3864091\nindo1319\nrun_Latn\n943828\natla1278\npor_Latn\n27824391\nindo1319\nmlg_Latn\n3715802\npnb_Arab\n899895\nindo1319\nnld_Latn\n25061426\nindo1319\nzul_Latn\n3580113\natla1278\nrar_Latn\n894515\naust1307\nara_Arab\n24524122\narz_Arab\n3488224\nafro1255\nfij_Latn\n887134\naust1307\nzho_Hani\n24143786\nnya_Latn\n3409030\natla1278\nwls_Latn\n882167\naust1307\nita_Latn\n23539857\nindo1319\ntam_Taml\n3388255\ndrav1251\nckb_Arab\n874441\nindo1319\nind_Latn\n23018106\naust1307\nhat_Latn\n3226932\nindo1319\nven_Latn\n860249\natla1278\nell_Grek\n22033282\nindo1319\nuzb_Latn\n3223485\nturk1311\nzsm_Latn\n859947\naust1307\nbul_Cyrl\n21823004\nindo1319\nsot_Latn\n3205510\natla1278\nchv_Cyrl\n859863\nturk1311\nswe_Latn\n20725883\nindo1319\nuzb_Cyrl\n3029947\nturk1311\nlua_Latn\n854359\natla1278\nces_Latn\n20376340\nindo1319\ncos_Latn\n3015055\nindo1319\nque_Latn\n838486\nisl_Latn\n19547941\nindo1319\nals_Latn\n2954874\nindo1319\nsag_Latn\n771048\natla1278\npol_Latn\n19339945\nindo1319\namh_Ethi\n2862985\nafro1255\nguw_Latn\n767918\natla1278\nron_Latn\n19190217\nindo1319\nsun_Latn\n2586011\naust1307\nbre_Latn\n748954\nindo1319\ndan_Latn\n19174573\nindo1319\nwar_Latn\n2584810\naust1307\ntoi_Latn\n745385\natla1278\nhun_Latn\n18800025\nural1272\ndiv_Thaa\n2418687\nindo1319\npus_Arab\n731992\nindo1319\ntgk_Cyrl\n18659517\nindo1319\nyor_Latn\n2392359\natla1278\nche_Cyrl\n728201\nnakh1245\nsrp_Latn\n18371769\nindo1319\nfao_Latn\n2365271\nindo1319\npis_Latn\n714783\nindo1319\nfas_Arab\n18277593\nuzn_Cyrl\n2293672\nturk1311\nkon_Latn\n685194\nceb_Latn\n18149215\naust1307\nsmo_Latn\n2290439\naust1307\noss_Cyrl\n683517\nindo1319\nheb_Hebr\n18128962\nafro1255\nbak_Cyrl\n2264196\nturk1311\nhyw_Armn\n679819\nindo1319\nhrv_Latn\n17882932\nindo1319\nilo_Latn\n2106531\naust1307\niso_Latn\n658789\natla1278\nglg_Latn\n17852274\nindo1319\ntso_Latn\n2100708\natla1278\nnan_Latn\n656389\nsino1245\nfin_Latn\n16730388\nural1272\nmri_Latn\n2046850\naust1307\nlub_Latn\n654390\natla1278\nslv_Latn\n15719210\nindo1319\nhmn_Latn\n1903898\nlim_Latn\n652078\nindo1319\nvie_Latn\n15697827\naust1305\nasm_Beng\n1882353\nindo1319\ntuk_Latn\n649411\nturk1311\nmkd_Cyrl\n14717004\nindo1319\nhil_Latn\n1798875\naust1307\ntir_Ethi\n649117\nafro1255\nslk_Latn\n14633631\nindo1319\nnso_Latn\n1619354\natla1278\ntgk_Latn\n636541\nindo1319\nnor_Latn\n14576191\nindo1319\nibo_Latn\n1543820\natla1278\nyua_Latn\n610052\nmaya1287\nest_Latn\n13600579\nkin_Latn\n1521612\natla1278\nmin_Latn\n609065\naust1307\nltz_Latn\n12997242\nindo1319\nhye_Armn\n1463123\nindo1319\nlue_Latn\n599429\natla1278\neus_Latn\n12775959\noci_Latn\n1449128\nindo1319\nkhm_Khmr\n590429\naust1305\nlit_Latn\n12479626\nindo1319\nlin_Latn\n1408460\natla1278\ntum_Latn\n589857\natla1278\nkaz_Cyrl\n12378727\nturk1311\ntpi_Latn\n1401844\nindo1319\ntll_Latn\n586530\natla1278\nlav_Latn\n12143980\nindo1319\ntwi_Latn\n1400979\natla1278\nekk_Latn\n582595\nural1272\nbos_Latn\n11014744\nindo1319\nkir_Cyrl\n1397566\nturk1311\nlug_Latn\n566948\natla1278\nepo_Latn\n8737198\narti1236\npap_Latn\n1360138\nindo1319\nniu_Latn\n566715\naust1307\ncat_Latn\n8648271\nindo1319\nnep_Deva\n1317291\nindo1319\ntzo_Latn\n540262\nmaya1287\ntha_Thai\n7735209\ntaik1256\nazj_Latn\n1315834\nturk1311\nmah_Latn\n534614\naust1307\nukr_Cyrl\n7462046\nindo1319\nbcl_Latn\n1284493\naust1307\ntvl_Latn\n521556\naust1307\ntgl_Latn\n7411064\naust1307\nxho_Latn\n1262364\natla1278\njav_Latn\n516833\naust1307\nsin_Sinh\n7293178\nindo1319\ncym_Latn\n1244783\nindo1319\nvec_Latn\n514240\nindo1319\ngle_Latn\n7225513\nindo1319\ngaa_Latn\n1222307\natla1278\njpn_Jpan\n510722\njapo1237\nTable 2: List of languages of Glot500-c (Part I).\nLang\n|Sent|\nFamily\nLang\n|Sent|\nFamily\nLang\n|Sent|\nFamily\nlus_Latn\n509250\nsino1245\nkmb_Latn\n296269\natla1278\nncx_Latn\n162558\nutoa1244\ncrs_Latn\n508755\nindo1319\nzai_Latn\n277632\notom1299\nqug_Latn\n162500\nquec1387\nkqn_Latn\n507913\natla1278\ngym_Latn\n274512\nchib1249\nrmn_Latn\n162069\nindo1319\nndo_Latn\n496613\natla1278\nbod_Tibt\n273489\nsino1245\ncjk_Latn\n160645\natla1278\nsnd_Arab\n488730\nindo1319\nnde_Latn\n269931\natla1278\narb_Arab\n159884\nafro1255\nyue_Hani\n484700\nsino1245\nfon_Latn\n268566\natla1278\nkea_Latn\n158047\nindo1319\ntiv_Latn\n483064\natla1278\nber_Latn\n264426\nmck_Latn\n157521\natla1278\nkua_Latn\n473535\natla1278\nnbl_Latn\n259158\natla1278\narn_Latn\n155882\narau1255\nkwy_Latn\n473274\natla1278\nkmr_Latn\n256677\nindo1319\npdt_Latn\n155485\nindo1319\nhin_Latn\n466175\nindo1319\nguc_Latn\n249044\naraw1281\nher_Latn\n154827\natla1278\niku_Cans\n465011\nmam_Latn\n248348\nmaya1287\ngla_Latn\n152563\nindo1319\nkal_Latn\n462430\neski1264\nnia_Latn\n247406\naust1307\nkmr_Cyrl\n151728\nindo1319\ntdt_Latn\n459818\naust1307\nnyn_Latn\n241992\natla1278\nmwl_Latn\n150054\nindo1319\ngsw_Latn\n449240\nindo1319\ncab_Latn\n240101\naraw1281\nnav_Latn\n147702\natha1245\nmfe_Latn\n447435\nindo1319\ntop_Latn\n239232\ntoto1251\nksw_Mymr\n147674\nsino1245\nswc_Latn\n446378\natla1278\ntog_Latn\n231969\natla1278\nmxv_Latn\n147591\notom1299\nmon_Latn\n437950\nmong1349\nmco_Latn\n231209\nmixe1284\nhif_Latn\n147261\nindo1319\nmos_Latn\n437666\natla1278\ntzh_Latn\n230706\nmaya1287\nwol_Latn\n146992\natla1278\nkik_Latn\n437228\natla1278\npms_Latn\n227748\nindo1319\nsme_Latn\n146803\nural1272\ncnh_Latn\n436667\nsino1245\nwuu_Hani\n224088\nsino1245\ngom_Latn\n143937\nindo1319\ngil_Latn\n434529\naust1307\nplt_Latn\n220413\naust1307\nbum_Latn\n141673\natla1278\npon_Latn\n434522\naust1307\nyid_Hebr\n220214\nindo1319\nmgr_Latn\n138953\natla1278\numb_Latn\n431589\natla1278\nada_Latn\n219427\natla1278\nahk_Latn\n135068\nsino1245\nlvs_Latn\n422952\nindo1319\niba_Latn\n213615\naust1307\nkur_Arab\n134160\nindo1319\nsco_Latn\n411591\nindo1319\nkek_Latn\n209932\nmaya1287\nbas_Latn\n133436\natla1278\nori_Orya\n410827\nkoo_Latn\n209375\natla1278\nbin_Latn\n133256\natla1278\narg_Latn\n410683\nindo1319\nsop_Latn\n206501\natla1278\ntsz_Latn\n133251\ntara1323\nkur_Latn\n407169\nindo1319\nkac_Latn\n205542\nsino1245\nsid_Latn\n130406\nafro1255\ndhv_Latn\n405711\naust1307\nqvi_Latn\n205447\nquec1387\ndiq_Latn\n128908\nindo1319\nluo_Latn\n398974\nnilo1247\ncak_Latn\n204472\nmaya1287\nsrd_Latn\n127064\nlun_Latn\n395764\natla1278\nkbp_Latn\n202877\natla1278\ntcf_Latn\n126050\notom1299\nnzi_Latn\n394247\natla1278\nctu_Latn\n201662\nmaya1287\nbzj_Latn\n124958\nindo1319\ngug_Latn\n392227\ntupi1275\nkri_Latn\n201087\nindo1319\nudm_Cyrl\n121705\nural1272\nbar_Latn\n387070\nindo1319\nmau_Latn\n199134\notom1299\ncce_Latn\n120636\natla1278\nbci_Latn\n384059\natla1278\nscn_Latn\n199068\nindo1319\nmeu_Latn\n120273\naust1307\nchk_Latn\n380596\naust1307\ntyv_Cyrl\n198649\nturk1311\nchw_Latn\n119751\natla1278\nroh_Latn\n377067\nindo1319\nina_Latn\n197315\narti1236\ncbk_Latn\n118789\nindo1319\naym_Latn\n373329\nayma1253\nbtx_Latn\n193701\naust1307\nibg_Latn\n118733\naust1307\nyap_Latn\n358929\naust1307\nnch_Latn\n193129\nutoa1244\nbhw_Latn\n117381\naust1307\nssw_Latn\n356561\natla1278\nncj_Latn\n192962\nutoa1244\nngu_Latn\n116851\nutoa1244\nquz_Latn\n354781\nquec1387\npau_Latn\n190529\naust1307\nnyy_Latn\n115914\natla1278\nsah_Cyrl\n352697\nturk1311\ntoj_Latn\n189651\nmaya1287\nszl_Latn\n112496\nindo1319\ntsn_Latn\n350954\natla1278\npcm_Latn\n187594\nindo1319\nish_Latn\n111814\natla1278\nlmo_Latn\n348135\nindo1319\ndyu_Latn\n186367\nmand1469\nnaq_Latn\n109747\nkhoe1240\nido_Latn\n331239\narti1236\nkss_Latn\n185868\natla1278\ntoh_Latn\n107583\natla1278\nabk_Cyrl\n321578\nabkh1242\nafb_Arab\n183694\nafro1255\nttj_Latn\n106925\natla1278\nzne_Latn\n318871\natla1278\nurh_Latn\n182214\natla1278\nnse_Latn\n105189\natla1278\nquy_Latn\n311040\nquec1387\nquc_Latn\n181559\nmaya1287\nhsb_Latn\n104802\nindo1319\nkam_Latn\n310659\natla1278\nnew_Deva\n181427\nsino1245\nami_Latn\n104559\naust1307\nbbc_Latn\n310420\naust1307\nyao_Latn\n179965\natla1278\nalz_Latn\n104392\nnilo1247\nvol_Latn\n310399\narti1236\nngl_Latn\n178498\natla1278\napc_Arab\n102392\nafro1255\nwal_Latn\n309873\ngong1255\nnyu_Latn\n177483\natla1278\nvls_Latn\n101900\nindo1319\nuig_Arab\n307302\nturk1311\nkab_Latn\n176015\nafro1255\nmhr_Cyrl\n100474\nural1272\nvmw_Latn\n306899\natla1278\ntuk_Cyrl\n175769\nturk1311\ndjk_Latn\n99234\nindo1319\nkwn_Latn\n305362\natla1278\nxmf_Geor\n174994\nkart1248\nwes_Latn\n98492\nindo1319\npam_Latn\n303737\naust1307\nndc_Latn\n174305\natla1278\ngkn_Latn\n97041\natla1278\nseh_Latn\n300243\natla1278\nsan_Deva\n165616\nindo1319\ngrc_Grek\n96986\nindo1319\ntsc_Latn\n298442\natla1278\nnba_Latn\n163485\natla1278\nhbo_Hebr\n96484\nafro1255\nnyk_Latn\n297976\natla1278\nbpy_Beng\n162838\nindo1319\nswh_Latn\n95776\natla1278\nTable 3: List of languages of Glot500-c (Part II).\nLang\n|Sent|\nFamily\nLang\n|Sent|\nFamily\nLang\n|Sent|\nFamily\nalt_Cyrl\n95148\nturk1311\nmny_Latn\n50581\natla1278\ncsy_Latn\n34126\nsino1245\nrmn_Grek\n94533\nindo1319\ngkp_Latn\n50549\nmand1469\nazb_Arab\n33758\nturk1311\nmiq_Latn\n94343\nmisu1242\nkat_Latn\n50424\nkart1248\ncsb_Latn\n33743\nindo1319\nkaa_Cyrl\n88815\nturk1311\nbjn_Latn\n49068\naust1307\ntpm_Latn\n33517\natla1278\nkos_Latn\n88603\naust1307\nacr_Latn\n48886\nmaya1287\nquw_Latn\n33449\nquec1387\ngrn_Latn\n87568\ndtp_Latn\n48468\naust1307\nrmy_Cyrl\n33351\nindo1319\nlhu_Latn\n87255\nsino1245\nlam_Latn\n46853\natla1278\nixl_Latn\n33289\nmaya1287\nlzh_Hani\n86035\nsino1245\nbik_Latn\n46561\nmbb_Latn\n33240\naust1307\najp_Arab\n83297\nafro1255\npoh_Latn\n46454\nmaya1287\npfl_Latn\n33148\nindo1319\ncmn_Hani\n80745\nsino1245\nphm_Latn\n45862\natla1278\npcd_Latn\n32867\nindo1319\ngcf_Latn\n80737\nindo1319\nhrx_Latn\n45716\nindo1319\ntlh_Latn\n32863\narti1236\nrmn_Cyrl\n79925\nindo1319\nquh_Latn\n45566\nquec1387\nsuz_Deva\n32811\nsino1245\nkjh_Cyrl\n79262\nturk1311\nhyw_Cyrl\n45379\nindo1319\ngcr_Latn\n32676\nindo1319\nrng_Latn\n78177\natla1278\nrue_Cyrl\n45369\nindo1319\njbo_Latn\n32619\narti1236\nmgh_Latn\n78117\natla1278\neml_Latn\n44630\nindo1319\ntbz_Latn\n32264\natla1278\nxmv_Latn\n77896\naust1307\nacm_Arab\n44505\nafro1255\nbam_Latn\n32150\nmand1469\nige_Latn\n77114\natla1278\ntob_Latn\n44473\nguai1249\nprk_Latn\n32085\naust1305\nrmy_Latn\n76991\nindo1319\nach_Latn\n43974\nnilo1247\njam_Latn\n32048\nindo1319\nsrm_Latn\n76884\nindo1319\nvep_Latn\n43076\nural1272\ntwx_Latn\n32028\natla1278\nbak_Latn\n76809\nturk1311\nnpi_Deva\n43072\nindo1319\nnmf_Latn\n31997\nsino1245\ngur_Latn\n76151\natla1278\ntok_Latn\n42820\narti1236\ncaq_Latn\n31903\naust1305\nidu_Latn\n75106\natla1278\nsgs_Latn\n42467\nindo1319\nrop_Latn\n31889\nindo1319\nyom_Latn\n74818\natla1278\nlij_Latn\n42447\nindo1319\ntca_Latn\n31852\nticu1244\ntdx_Latn\n74430\naust1307\nmyv_Cyrl\n42147\nural1272\nyan_Latn\n31775\nmisu1242\nmzn_Arab\n73719\nindo1319\ntih_Latn\n41873\naust1307\nxav_Latn\n31765\nnucl1710\ncfm_Latn\n70227\nsino1245\ntat_Latn\n41640\nturk1311\nbih_Deva\n31658\nzpa_Latn\n69237\notom1299\nlfn_Latn\n41632\narti1236\ncuk_Latn\n31612\nchib1249\nkbd_Cyrl\n67914\nabkh1242\ncgg_Latn\n41196\natla1278\nkjb_Latn\n31471\nmaya1287\nlao_Laoo\n66966\ntaik1256\nful_Latn\n41188\natla1278\nhne_Deva\n31465\nindo1319\nnap_Latn\n65826\nindo1319\ngor_Latn\n41174\naust1307\nwbm_Latn\n31394\naust1305\nqub_Latn\n64973\nquec1387\nile_Latn\n40984\narti1236\nzlm_Latn\n31345\naust1307\noke_Latn\n64508\natla1278\nium_Latn\n40683\nhmon1336\ntui_Latn\n31161\natla1278\note_Latn\n64224\notom1299\nteo_Latn\n40203\nnilo1247\nifb_Latn\n30980\naust1307\nbsb_Latn\n63634\naust1307\nkia_Latn\n40035\natla1278\nizz_Latn\n30894\natla1278\nogo_Latn\n61901\natla1278\ncrh_Cyrl\n39985\nturk1311\nrug_Latn\n30857\naust1307\nabn_Latn\n61830\natla1278\ncrh_Latn\n39896\nturk1311\naka_Latn\n30704\natla1278\nldi_Latn\n61827\natla1278\nenm_Latn\n39809\nindo1319\npxm_Latn\n30698\nbook1242\nayr_Latn\n61570\nayma1253\nsat_Olck\n39614\naust1305\nkmm_Latn\n30671\nsino1245\ngom_Deva\n61140\nindo1319\nmad_Latn\n38993\naust1307\nmcn_Latn\n30666\nafro1255\nbba_Latn\n61123\natla1278\ncac_Latn\n38812\nmaya1287\nifa_Latn\n30621\naust1307\naln_Latn\n60989\nindo1319\nhnj_Latn\n38611\nhmon1336\ndln_Latn\n30620\nsino1245\nleh_Latn\n59944\natla1278\nksh_Latn\n38130\nindo1319\next_Latn\n30605\nindo1319\nban_Latn\n59805\naust1307\nikk_Latn\n38071\natla1278\nksd_Latn\n30550\naust1307\nace_Latn\n59333\naust1307\nsba_Latn\n38040\ncent2225\nmzh_Latn\n30517\nmata1289\npes_Arab\n57511\nindo1319\nzom_Latn\n37013\nsino1245\nllb_Latn\n30480\natla1278\nskg_Latn\n57228\naust1307\nbqc_Latn\n36881\nmand1469\nhra_Latn\n30472\nsino1245\nary_Arab\n56933\nafro1255\nbim_Latn\n36835\natla1278\nmwm_Latn\n30432\ncent2225\nhus_Latn\n56176\nmaya1287\nmdy_Ethi\n36370\ngong1255\nkrc_Cyrl\n30353\nturk1311\nglv_Latn\n55641\nindo1319\nbts_Latn\n36216\naust1307\ntuc_Latn\n30349\naust1307\nfat_Latn\n55609\natla1278\ngya_Latn\n35902\natla1278\nmrw_Latn\n30304\naust1307\nfrr_Latn\n55254\nindo1319\najg_Latn\n35631\natla1278\npls_Latn\n30136\notom1299\nmwn_Latn\n54805\natla1278\nagw_Latn\n35585\naust1307\nrap_Latn\n30102\naust1307\nmai_Deva\n54687\nindo1319\nkom_Cyrl\n35249\nural1272\nfur_Latn\n30052\nindo1319\ndua_Latn\n53392\natla1278\nknv_Latn\n35196\nkaa_Latn\n30031\nturk1311\ndzo_Tibt\n52732\nsino1245\ngiz_Latn\n35040\nafro1255\nprs_Arab\n26823\nindo1319\nctd_Latn\n52135\nsino1245\nhui_Latn\n34926\nnucl1709\nsan_Latn\n25742\nindo1319\nnnb_Latn\n52041\natla1278\nkpg_Latn\n34900\naust1307\nsom_Arab\n14199\nafro1255\nsxn_Latn\n51749\naust1307\nzea_Latn\n34426\nindo1319\nuig_Latn\n9637\nturk1311\nmps_Latn\n50645\ntebe1251\naoj_Latn\n34349\nnucl1708\nhau_Arab\n9593\nafro1255\nTable 4: List of languages of Glot500-c (Part III).\nLang\nLLaMA 2\n7B\nmGPT\n13B\nBLOOM\n7B1\nXGLM\n7.5B\nMaLA-500\n1-shot\n2-shot\n3-shot\n4-shot\n5-shot\n6-shot\n7-shot\n8-shot\n9-shot\n10-shot\nace_Latn\n49.49\n45.45\n50.51\n54.55\n34.34\n50.51\n54.55\n59.60\n55.56\n58.59\n62.63\n57.58\n67.68\n47.47\nacm_Arab\n45.45\n51.52\n65.66\n64.65\n34.34\n50.51\n53.54\n52.53\n55.56\n56.57\n57.58\n57.58\n55.56\n76.77\nafr_Latn\n53.54\n47.47\n54.55\n58.59\n34.34\n54.55\n66.67\n66.67\n65.66\n76.77\n71.72\n67.68\n75.76\n48.48\najp_Arab\n51.52\n52.53\n67.68\n64.65\n33.33\n49.49\n49.49\n49.49\n47.47\n48.48\n47.47\n51.52\n56.57\n67.68\nals_Latn\n50.51\n44.44\n46.46\n57.58\n21.21\n57.58\n70.71\n62.63\n63.64\n69.70\n69.70\n67.68\n71.72\n75.76\namh_Ethi\n18.18\n16.16\n13.13\n20.20\n19.19\n38.38\n43.43\n39.39\n43.43\n45.45\n45.45\n39.39\n44.44\n49.49\napc_Arab\n46.46\n55.56\n69.70\n64.65\n25.25\n50.51\n48.48\n47.47\n45.45\n55.56\n51.52\n52.53\n53.54\n55.56\narb_Arab\n46.46\n47.47\n68.69\n61.62\n31.31\n52.53\n49.49\n54.55\n57.58\n56.57\n59.60\n58.59\n55.56\n65.66\nary_Arab\n42.42\n42.42\n67.68\n61.62\n24.24\n46.46\n47.47\n45.45\n44.44\n49.49\n45.45\n46.46\n50.51\n74.75\narz_Arab\n47.47\n54.55\n75.76\n64.65\n34.34\n50.51\n48.48\n51.52\n53.54\n60.61\n55.56\n51.52\n58.59\n35.35\nasm_Beng\n32.32\n47.47\n63.64\n54.55\n22.22\n41.41\n48.48\n47.47\n47.47\n47.47\n49.49\n46.46\n52.53\n64.65\nast_Latn\n60.61\n46.46\n61.62\n64.65\n28.28\n57.58\n67.68\n67.68\n65.66\n70.71\n70.71\n67.68\n76.77\n71.72\nayr_Latn\n35.35\n32.32\n32.32\n34.34\n23.23\n30.30\n38.38\n43.43\n41.41\n47.47\n47.47\n46.46\n44.44\n40.40\nazb_Arab\n40.40\n37.37\n37.37\n41.41\n27.27\n38.38\n38.38\n39.39\n45.45\n51.52\n40.40\n43.43\n45.45\n70.71\nazj_Latn\n43.43\n49.49\n33.33\n44.44\n24.24\n51.52\n59.60\n62.63\n62.63\n71.72\n69.70\n67.68\n69.7\n60.61\nbak_Cyrl\n35.35\n54.55\n34.34\n44.44\n34.34\n49.49\n61.62\n63.64\n63.64\n70.71\n64.65\n63.64\n65.66\n72.73\nbam_Latn\n23.23\n29.29\n22.22\n31.31\n17.17\n29.29\n31.31\n38.38\n33.33\n44.44\n38.38\n37.37\n39.39\n68.69\nban_Latn\n57.58\n52.53\n56.57\n60.61\n25.25\n43.43\n59.60\n62.63\n59.60\n61.62\n63.64\n59.6\n61.62\n64.65\nbel_Cyrl\n35.35\n44.44\n29.29\n45.45\n35.35\n52.53\n64.65\n63.64\n65.66\n66.67\n64.65\n70.71\n69.7\n66.67\nbem_Latn\n29.29\n31.31\n35.35\n38.38\n23.23\n33.33\n42.42\n44.44\n48.48\n46.46\n44.44\n49.49\n47.47\n55.56\nben_Beng\n41.41\n53.54\n60.61\n64.65\n28.28\n48.48\n54.55\n60.61\n60.61\n61.62\n60.61\n55.56\n59.6\n70.71\nbjn_Latn\n53.54\n56.57\n59.60\n64.65\n24.24\n53.54\n62.63\n67.68\n62.63\n67.68\n68.69\n63.64\n65.66\n72.73\nbod_Tibt\n13.13\n13.13\n11.11\n18.18\n22.22\n30.30\n32.32\n38.38\n32.32\n38.38\n34.34\n34.34\n35.35\n70.71\nbos_Latn\n65.66\n54.55\n46.46\n55.56\n27.27\n59.60\n69.70\n64.65\n64.65\n70.71\n75.76\n68.69\n75.76\n72.73\nbul_Cyrl\n57.58\n49.49\n44.44\n65.66\n43.43\n64.65\n64.65\n62.63\n68.69\n70.71\n69.70\n66.67\n70.71\n59.60\ncat_Latn\n61.62\n52.53\n57.58\n66.67\n29.29\n57.58\n68.69\n61.62\n65.66\n68.69\n69.70\n66.67\n72.73\n66.67\nceb_Latn\n61.62\n48.48\n48.48\n49.49\n29.29\n61.62\n69.70\n73.74\n66.67\n80.81\n74.75\n72.73\n74.75\n60.61\nces_Latn\n65.66\n52.53\n44.44\n47.47\n32.32\n54.55\n63.64\n63.64\n69.70\n69.70\n71.72\n65.66\n74.75\n73.74\ncjk_Latn\n35.35\n20.20\n28.28\n29.29\n14.14\n26.26\n29.29\n34.34\n26.26\n40.40\n32.32\n32.32\n42.42\n63.64\nckb_Arab\n25.25\n19.19\n20.20\n20.20\n32.32\n45.45\n54.55\n45.45\n47.47\n51.52\n52.53\n56.57\n54.55\n57.58\ncmn_Hani\n44.44\n54.55\n68.69\n64.65\n40.40\n51.52\n66.67\n64.65\n67.68\n68.69\n69.70\n65.66\n70.71\n49.49\ncrh_Latn\n42.42\n43.43\n42.42\n50.51\n25.25\n55.56\n53.54\n59.60\n52.53\n63.64\n57.58\n52.53\n58.59\n77.78\ncym_Latn\n39.39\n24.24\n29.29\n35.35\n22.22\n46.46\n59.60\n58.59\n60.61\n67.68\n65.66\n62.63\n69.7\n68.69\ndan_Latn\n60.61\n53.54\n53.54\n61.62\n31.31\n52.53\n65.66\n65.66\n67.68\n74.75\n74.75\n71.72\n73.74\n34.34\ndeu_Latn\n59.60\n50.51\n59.60\n65.66\n40.40\n61.62\n71.72\n70.71\n71.72\n77.78\n73.74\n70.71\n78.79\n61.62\ndyu_Latn\n24.24\n29.29\n31.31\n33.33\n15.15\n26.26\n30.30\n30.30\n27.27\n33.33\n33.33\n32.32\n35.35\n50.51\ndzo_Tibt\n16.16\n12.12\n15.15\n17.17\n19.19\n32.32\n28.28\n28.28\n28.28\n29.29\n31.31\n35.35\n32.32\n72.73\nell_Grek\n54.55\n49.49\n32.32\n70.71\n39.39\n57.58\n63.64\n67.68\n67.68\n71.72\n67.68\n68.69\n70.71\n53.54\neng_Latn\n66.67\n48.48\n66.67\n71.72\n44.44\n68.69\n72.73\n71.72\n73.74\n75.76\n76.77\n76.77\n77.78\n76.77\nepo_Latn\n51.52\n43.43\n53.54\n55.56\n31.31\n60.61\n65.66\n74.75\n66.67\n73.74\n67.68\n68.69\n74.75\n55.56\nest_Latn\n44.44\n42.42\n35.35\n61.62\n23.23\n55.56\n60.61\n61.62\n64.65\n67.68\n66.67\n60.61\n66.67\n39.39\neus_Latn\n45.45\n46.46\n58.59\n61.62\n22.22\n49.49\n45.45\n57.58\n54.55\n58.59\n56.57\n53.54\n57.58\n59.60\newe_Latn\n23.23\n26.26\n25.25\n38.38\n21.21\n36.36\n37.37\n38.38\n31.31\n37.37\n42.42\n39.39\n37.37\n56.57\nfao_Latn\n44.44\n40.40\n41.41\n39.39\n25.25\n46.46\n63.64\n58.59\n65.66\n66.67\n63.64\n62.63\n68.69\n70.71\nfij_Latn\n30.30\n30.30\n33.33\n42.42\n17.17\n34.34\n41.41\n45.45\n46.46\n50.51\n46.46\n45.45\n48.48\n39.39\nfin_Latn\n61.62\n48.48\n38.38\n66.67\n28.28\n56.57\n67.68\n62.63\n68.69\n74.75\n72.73\n70.71\n73.74\n56.57\nfon_Latn\n22.22\n25.25\n26.26\n30.30\n20.20\n31.31\n32.32\n28.28\n30.30\n34.34\n31.31\n34.34\n34.34\n75.76\nfra_Latn\n63.64\n53.54\n65.66\n67.68\n39.39\n60.61\n70.71\n64.65\n67.68\n74.75\n72.73\n70.71\n71.72\n55.56\nful_Latn\n35.35\n31.31\n34.34\n44.44\n18.18\n34.34\n44.44\n41.41\n45.45\n49.49\n56.57\n46.46\n58.59\n59.60\nfur_Latn\n55.56\n49.49\n52.53\n58.59\n19.19\n50.51\n57.58\n54.55\n55.56\n65.66\n61.62\n62.63\n58.59\n35.35\ngla_Latn\n30.30\n27.27\n23.23\n34.34\n15.15\n31.31\n40.40\n36.36\n34.34\n36.36\n39.39\n39.39\n43.43\n69.70\ngle_Latn\n41.41\n30.30\n21.21\n28.28\n22.22\n41.41\n51.52\n56.57\n56.57\n58.59\n63.64\n53.54\n61.62\n39.39\nglg_Latn\n56.57\n45.45\n63.64\n65.66\n31.31\n54.55\n69.70\n64.65\n72.73\n75.76\n78.79\n74.75\n81.82\n41.41\ngrn_Latn\n46.46\n46.46\n51.52\n56.57\n29.29\n54.55\n61.62\n63.64\n57.58\n59.60\n63.64\n65.66\n63.64\n62.63\nguj_Gujr\n19.19\n11.11\n63.64\n19.19\n17.17\n40.40\n50.51\n44.44\n51.52\n55.56\n51.52\n50.51\n53.54\n49.49\nhat_Latn\n45.45\n37.37\n42.42\n64.65\n27.27\n52.53\n58.59\n62.63\n63.64\n64.65\n70.71\n60.61\n75.76\n45.45\nhau_Latn\n26.26\n29.29\n27.27\n38.38\n19.19\n49.49\n54.55\n55.56\n54.55\n54.55\n55.56\n53.54\n55.56\n40.40\nheb_Hebr\n42.42\n44.44\n21.21\n26.26\n20.20\n36.36\n35.35\n40.40\n35.35\n31.31\n40.40\n40.4\n35.35\n63.64\nhin_Deva\n51.52\n51.52\n63.64\n70.71\n24.24\n42.42\n53.54\n58.59\n57.58\n61.62\n60.61\n57.58\n56.57\n65.66\nhne_Deva\n44.44\n48.48\n66.67\n64.65\n28.28\n49.49\n52.53\n60.61\n59.60\n59.60\n59.60\n59.6\n55.56\n56.57\nhrv_Latn\n66.67\n56.57\n52.53\n51.52\n30.30\n56.57\n73.74\n67.68\n71.72\n69.70\n76.77\n69.7\n76.77\n74.75\nhun_Latn\n65.66\n51.52\n36.36\n43.43\n33.33\n45.45\n57.58\n59.60\n56.57\n69.70\n61.62\n59.6\n65.66\n63.64\nhye_Armn\n29.29\n13.13\n18.18\n17.17\n26.26\n52.53\n59.60\n60.61\n67.68\n70.71\n66.67\n67.68\n71.72\n71.72\nibo_Latn\n20.20\n27.27\n42.42\n28.28\n31.31\n50.51\n57.58\n55.56\n48.48\n61.62\n58.59\n51.52\n53.54\n66.67\nilo_Latn\n47.47\n49.49\n51.52\n53.54\n19.19\n51.52\n62.63\n65.66\n60.61\n68.69\n72.73\n68.69\n67.68\n73.74\nind_Latn\n63.64\n52.53\n62.63\n71.72\n27.27\n56.57\n72.73\n69.70\n69.70\n77.78\n73.74\n76.77\n77.78\n57.58\nisl_Latn\n48.48\n37.37\n36.36\n34.34\n25.25\n46.46\n56.57\n53.54\n59.60\n54.55\n66.67\n66.67\n67.68\n55.56\nita_Latn\n60.61\n54.55\n61.62\n64.65\n35.35\n59.60\n71.72\n66.67\n67.68\n72.73\n76.77\n71.72\n73.74\n70.71\njav_Latn\n51.52\n58.59\n60.61\n61.62\n31.31\n46.46\n63.64\n63.64\n64.65\n64.65\n69.70\n64.65\n67.68\n73.74\njpn_Jpan\n41.41\n51.52\n64.65\n65.66\n36.36\n56.57\n64.65\n67.68\n66.67\n75.76\n74.75\n69.7\n72.73\n56.57\nkab_Latn\n14.14\n19.19\n17.17\n27.27\n14.14\n31.31\n32.32\n21.21\n26.26\n30.30\n26.26\n25.25\n29.29\n66.67\nkac_Latn\n32.32\n26.26\n34.34\n35.35\n21.21\n34.34\n35.35\n40.40\n39.39\n42.42\n39.39\n41.41\n39.39\n63.64\nkam_Latn\n29.29\n32.32\n32.32\n44.44\n20.20\n29.29\n42.42\n42.42\n39.39\n49.49\n36.36\n40.4\n42.42\n41.41\nkan_Knda\n15.15\n10.10\n59.60\n19.19\n23.23\n45.45\n48.48\n55.56\n56.57\n58.59\n54.55\n55.56\n56.57\n39.39\nkat_Geor\n42.42\n50.51\n20.20\n26.26\n24.24\n45.45\n56.57\n61.62\n59.60\n61.62\n67.68\n62.63\n66.67\n44.44\nkaz_Cyrl\n36.36\n52.53\n35.35\n49.49\n27.27\n47.47\n59.60\n62.63\n62.63\n67.68\n66.67\n64.65\n63.64\n58.59\nkbp_Latn\n22.22\n25.25\n27.27\n26.26\n21.21\n30.30\n30.30\n38.38\n31.31\n39.39\n37.37\n42.42\n41.41\n51.52\nkea_Latn\n51.52\n39.39\n57.58\n57.58\n29.29\n52.53\n58.59\n60.61\n57.58\n65.66\n64.65\n66.67\n70.71\n64.65\nkhm_Khmr\n31.31\n13.13\n30.30\n29.29\n38.38\n53.54\n58.59\n55.56\n58.59\n65.66\n60.61\n63.64\n67.68\n67.68\nkik_Latn\n28.28\n33.33\n34.34\n31.31\n21.21\n41.41\n39.39\n46.46\n43.43\n50.51\n46.46\n45.45\n50.51\n47.47\nkin_Latn\n31.31\n32.32\n49.49\n30.30\n18.18\n32.32\n44.44\n49.49\n50.51\n52.53\n49.49\n48.48\n54.55\n36.36\nkir_Cyrl\n38.38\n50.51\n32.32\n47.47\n29.29\n52.53\n59.60\n61.62\n63.64\n69.70\n60.61\n62.63\n66.67\n71.72\nkmb_Latn\n28.28\n34.34\n34.34\n43.43\n15.15\n28.28\n41.41\n39.39\n37.37\n39.39\n39.39\n44.44\n42.42\n52.53\nkmr_Latn\n30.30\n34.34\n38.38\n42.42\n20.20\n45.45\n52.53\n59.60\n57.58\n58.59\n54.55\n54.55\n62.63\n69.70\nkon_Latn\n34.34\n35.35\n44.44\n46.46\n26.26\n43.43\n46.46\n56.57\n51.52\n50.51\n55.56\n48.48\n53.54\n70.71\nkor_Hang\n48.48\n53.54\n44.44\n60.61\n42.42\n62.63\n67.68\n69.70\n68.69\n76.77\n77.78\n69.7\n74.75\n43.43\nlao_Laoo\n25.25\n10.10\n20.20\n31.31\n31.31\n49.49\n56.57\n61.62\n60.61\n66.67\n63.64\n59.6\n62.63\n58.59\nlij_Latn\n52.53\n51.52\n51.52\n60.61\n23.23\n51.52\n66.67\n64.65\n63.64\n64.65\n68.69\n68.69\n69.7\n54.55\nTable 5: Detailed Results (Part I). For previous LLMs, 3-shot results are presented.\nLang\nLLaMA 2\n7B\nmGPT\n13B\nBLOOM\n7B1\nXGLM\n7.5B\nMaLA-500\n1-shot\n2-shot\n3-shot\n4-shot\n5-shot\n6-shot\n7-shot\n8-shot\n9-shot\n10-shot\nlim_Latn\n49.49\n43.43\n49.49\n50.51\n25.25\n47.47\n60.61\n61.62\n60.61\n66.67\n66.67\n64.65\n71.72\n44.44\nlin_Latn\n43.43\n34.34\n41.41\n46.46\n27.27\n43.43\n46.46\n54.55\n56.57\n57.58\n60.61\n54.55\n55.56\n71.72\nlit_Latn\n38.38\n46.46\n39.39\n45.45\n19.19\n50.51\n58.59\n62.63\n65.66\n64.65\n66.67\n63.64\n65.66\n53.54\nlmo_Latn\n52.53\n48.48\n53.54\n53.54\n19.19\n54.55\n60.61\n54.55\n60.61\n60.61\n60.61\n60.61\n65.66\n63.64\nltz_Latn\n56.57\n48.48\n50.51\n50.51\n32.32\n53.54\n65.66\n63.64\n68.69\n70.71\n70.71\n65.66\n72.73\n62.63\nlua_Latn\n35.35\n27.27\n35.35\n39.39\n19.19\n35.35\n41.41\n51.52\n48.48\n47.47\n46.46\n47.47\n48.48\n41.41\nlug_Latn\n28.28\n27.27\n38.38\n36.36\n18.18\n31.31\n41.41\n40.40\n42.42\n44.44\n44.44\n39.39\n44.44\n46.46\nluo_Latn\n31.31\n27.27\n31.31\n36.36\n17.17\n36.36\n32.32\n41.41\n35.35\n37.37\n37.37\n37.37\n44.44\n72.73\nlus_Latn\n42.42\n36.36\n43.43\n44.44\n20.20\n40.40\n43.43\n47.47\n43.43\n47.47\n47.47\n44.44\n49.49\n66.67\nlvs_Latn\n47.47\n57.58\n44.44\n43.43\n30.30\n59.60\n67.68\n66.67\n64.65\n69.70\n72.73\n68.69\n76.77\n69.70\nmai_Deva\n47.47\n48.48\n62.63\n72.73\n27.27\n51.52\n54.55\n60.61\n54.55\n62.63\n60.61\n54.55\n60.61\n44.44\nmal_Mlym\n19.19\n39.39\n63.64\n25.25\n17.17\n40.40\n52.53\n51.52\n52.53\n52.53\n49.49\n52.53\n57.58\n56.57\nmar_Deva\n34.34\n47.47\n65.66\n48.48\n21.21\n46.46\n56.57\n60.61\n57.58\n55.56\n60.61\n55.56\n60.61\n72.73\nmin_Latn\n51.52\n48.48\n59.60\n55.56\n21.21\n52.53\n59.60\n66.67\n56.57\n67.68\n65.66\n58.59\n65.66\n60.61\nmkd_Cyrl\n56.57\n44.44\n39.39\n55.56\n46.46\n57.58\n61.62\n65.66\n68.69\n71.72\n74.75\n70.71\n72.73\n71.72\nmlt_Latn\n46.46\n43.43\n45.45\n43.43\n20.20\n58.59\n70.71\n68.69\n69.70\n72.73\n74.75\n74.75\n78.79\n68.69\nmon_Cyrl\n25.25\n43.43\n28.28\n40.40\n30.30\n48.48\n55.56\n59.60\n61.62\n65.66\n61.62\n60.61\n64.65\n50.51\nmos_Latn\n24.24\n26.26\n30.30\n35.35\n18.18\n29.29\n36.36\n38.38\n35.35\n39.39\n32.32\n37.37\n40.4\n52.53\nmri_Latn\n35.35\n20.20\n27.27\n28.28\n27.27\n39.39\n41.41\n44.44\n40.40\n48.48\n48.48\n43.43\n48.48\n58.59\nmya_Mymr\n19.19\n49.49\n18.18\n67.68\n18.18\n37.37\n43.43\n41.41\n40.40\n48.48\n48.48\n48.48\n48.48\n43.43\nnld_Latn\n53.54\n47.47\n53.54\n64.65\n38.38\n60.61\n70.71\n74.75\n69.70\n73.74\n77.78\n72.73\n75.76\n75.76\nnno_Latn\n62.63\n49.49\n55.56\n58.59\n26.26\n50.51\n68.69\n62.63\n66.67\n68.69\n69.70\n64.65\n71.72\n66.67\nnpi_Deva\n50.51\n41.41\n62.63\n61.62\n28.28\n51.52\n60.61\n65.66\n62.63\n69.70\n66.67\n63.64\n66.67\n67.68\nnso_Latn\n28.28\n29.29\n32.32\n34.34\n18.18\n37.37\n40.40\n42.42\n39.39\n46.46\n46.46\n43.43\n48.48\n60.61\nnya_Latn\n34.34\n33.33\n40.40\n43.43\n22.22\n37.37\n44.44\n53.54\n54.55\n56.57\n56.57\n56.57\n50.51\n50.51\noci_Latn\n59.60\n49.49\n61.62\n62.63\n26.26\n49.49\n61.62\n61.62\n60.61\n66.67\n65.66\n66.67\n65.66\n66.67\norm_Latn\n22.22\n23.23\n22.22\n23.23\n14.14\n26.26\n29.29\n35.35\n37.37\n32.32\n38.38\n37.37\n38.38\n58.59\nory_Orya\n21.21\n11.11\n60.61\n19.19\n19.19\n37.37\n46.46\n49.49\n52.53\n52.53\n57.58\n51.52\n55.56\n54.55\npag_Latn\n57.58\n47.47\n55.56\n56.57\n29.29\n57.58\n62.63\n69.70\n66.67\n67.68\n67.68\n69.7\n68.69\n57.58\npan_Guru\n19.19\n11.11\n61.62\n22.22\n21.21\n39.39\n48.48\n54.55\n51.52\n55.56\n55.56\n59.6\n50.51\n73.74\npap_Latn\n57.58\n49.49\n54.55\n58.59\n25.25\n59.60\n70.71\n70.71\n69.70\n69.70\n72.73\n73.74\n73.74\n71.72\npes_Arab\n53.54\n44.44\n54.55\n51.52\n30.30\n46.46\n55.56\n58.59\n57.58\n64.65\n64.65\n64.65\n67.68\n68.69\nplt_Latn\n30.30\n30.30\n33.33\n31.31\n20.20\n45.45\n50.51\n52.53\n54.55\n56.57\n60.61\n58.59\n65.66\n74.75\npol_Latn\n61.62\n48.48\n51.52\n53.54\n36.36\n59.60\n69.70\n66.67\n68.69\n72.73\n71.72\n65.66\n68.69\n71.72\npor_Latn\n58.59\n48.48\n58.59\n69.70\n30.30\n58.59\n66.67\n63.64\n66.67\n69.70\n73.74\n67.68\n77.78\n75.76\nprs_Arab\n54.55\n34.34\n48.48\n51.52\n27.27\n49.49\n55.56\n60.61\n60.61\n64.65\n57.58\n61.62\n64.65\n68.69\npus_Arab\n39.39\n32.32\n41.41\n48.48\n19.19\n38.38\n43.43\n48.48\n48.48\n44.44\n47.47\n47.47\n50.51\n72.73\nquy_Latn\n40.40\n37.37\n43.43\n46.46\n23.23\n43.43\n46.46\n54.55\n47.47\n51.52\n52.53\n48.48\n52.53\n74.75\nron_Latn\n64.65\n53.54\n60.61\n57.58\n32.32\n57.58\n63.64\n64.65\n67.68\n71.72\n69.70\n63.64\n72.73\n71.72\nrun_Latn\n29.29\n30.30\n43.43\n33.33\n17.17\n42.42\n52.53\n56.57\n49.49\n55.56\n51.52\n54.55\n54.55\n66.67\nrus_Cyrl\n62.63\n51.52\n53.54\n63.64\n43.43\n60.61\n68.69\n67.68\n67.68\n70.71\n73.74\n68.69\n75.76\n74.75\nsag_Latn\n26.26\n29.29\n34.34\n40.40\n24.24\n37.37\n39.39\n44.44\n44.44\n47.47\n43.43\n45.45\n49.49\n44.44\nsan_Deva\n38.38\n45.45\n52.53\n51.52\n22.22\n41.41\n53.54\n49.49\n42.42\n48.48\n49.49\n42.42\n39.39\n68.69\nscn_Latn\n43.43\n45.45\n52.53\n51.52\n26.26\n60.61\n67.68\n66.67\n66.67\n68.69\n74.75\n70.71\n75.76\n36.36\nsin_Sinh\n22.22\n09.09\n12.12\n16.16\n23.23\n49.49\n51.52\n53.54\n55.56\n60.61\n59.60\n60.61\n57.58\n63.64\nslk_Latn\n63.64\n52.53\n48.48\n49.49\n32.32\n56.57\n62.63\n62.63\n66.67\n69.70\n68.69\n67.68\n68.69\n63.64\nslv_Latn\n65.66\n53.54\n42.42\n49.49\n32.32\n57.58\n64.65\n65.66\n67.68\n68.69\n70.71\n70.71\n74.75\n63.64\nsmo_Latn\n30.30\n30.30\n31.31\n35.35\n24.24\n49.49\n59.60\n60.61\n59.60\n61.62\n65.66\n59.6\n65.66\n76.77\nsna_Latn\n32.32\n26.26\n41.41\n35.35\n21.21\n36.36\n41.41\n43.43\n42.42\n46.46\n47.47\n39.39\n40.4\n68.69\nsnd_Arab\n38.38\n35.35\n34.34\n43.43\n25.25\n42.42\n37.37\n45.45\n40.40\n49.49\n47.47\n45.45\n47.47\n55.56\nsom_Latn\n32.32\n29.29\n26.26\n38.38\n15.15\n31.31\n41.41\n41.41\n47.47\n44.44\n42.42\n45.45\n46.46\n66.67\nsot_Latn\n29.29\n24.24\n35.35\n33.33\n19.19\n40.40\n44.44\n49.49\n50.51\n49.49\n52.53\n51.52\n54.55\n73.74\nspa_Latn\n59.60\n51.52\n66.67\n67.68\n33.33\n58.59\n69.70\n69.70\n67.68\n72.73\n71.72\n73.74\n76.77\n60.61\nsrd_Latn\n45.45\n40.40\n49.49\n54.55\n19.19\n54.55\n65.66\n66.67\n62.63\n68.69\n73.74\n66.67\n72.73\n51.52\nsrp_Cyrl\n62.63\n48.48\n45.45\n59.60\n49.49\n58.59\n64.65\n58.59\n67.68\n69.70\n69.70\n63.64\n69.7\n66.67\nssw_Latn\n25.25\n27.27\n33.33\n29.29\n18.18\n36.36\n39.39\n51.52\n49.49\n48.48\n52.53\n47.47\n48.48\n61.62\nsun_Latn\n53.54\n49.49\n57.58\n60.61\n31.31\n59.60\n68.69\n68.69\n67.68\n67.68\n68.69\n66.67\n73.74\n66.67\nswe_Latn\n65.66\n49.49\n52.53\n61.62\n29.29\n57.58\n66.67\n64.65\n66.67\n72.73\n72.73\n73.74\n75.76\n62.63\nswh_Latn\n43.43\n51.52\n60.61\n70.71\n29.29\n53.54\n61.62\n61.62\n58.59\n62.63\n56.57\n62.63\n66.67\n68.69\nszl_Latn\n51.52\n43.43\n44.44\n41.41\n35.35\n52.53\n56.57\n59.60\n64.65\n63.64\n68.69\n62.63\n71.72\n54.55\ntam_Taml\n29.29\n47.47\n59.60\n64.65\n21.21\n39.39\n47.47\n51.52\n55.56\n65.66\n58.59\n57.58\n60.61\n74.75\ntat_Cyrl\n36.36\n56.57\n34.34\n47.47\n30.30\n50.51\n55.56\n58.59\n59.60\n61.62\n63.64\n63.64\n67.68\n68.69\ntel_Telu\n22.22\n52.53\n59.60\n59.60\n15.15\n36.36\n48.48\n53.54\n44.44\n54.55\n48.48\n49.49\n51.52\n67.68\ntgk_Cyrl\n26.26\n45.45\n32.32\n45.45\n25.25\n39.39\n49.49\n59.60\n61.62\n61.62\n60.61\n59.6\n63.64\n28.28\ntgl_Latn\n62.63\n45.45\n45.45\n50.51\n29.29\n61.62\n70.71\n70.71\n67.68\n76.77\n74.75\n71.72\n77.78\n73.74\ntha_Thai\n49.49\n56.57\n23.23\n67.68\n38.38\n56.57\n62.63\n62.63\n68.69\n69.70\n65.66\n66.67\n73.74\n46.46\ntir_Ethi\n13.13\n14.14\n13.13\n19.19\n17.17\n29.29\n35.35\n29.29\n36.36\n38.38\n38.38\n36.36\n36.36\n72.73\ntpi_Latn\n62.63\n50.51\n56.57\n55.56\n34.34\n63.64\n73.74\n71.72\n67.68\n75.76\n75.76\n74.75\n76.77\n58.59\ntsn_Latn\n33.33\n31.31\n36.36\n36.36\n19.19\n37.37\n36.36\n44.44\n40.40\n40.40\n45.45\n45.45\n45.45\n36.36\ntso_Latn\n34.34\n27.27\n29.29\n30.30\n17.17\n33.33\n38.38\n47.47\n43.43\n45.45\n50.51\n45.45\n46.46\n41.41\ntuk_Latn\n41.41\n43.43\n38.38\n44.44\n25.25\n41.41\n52.53\n54.55\n45.45\n52.53\n53.54\n53.54\n59.6\n68.69\ntum_Latn\n34.34\n31.31\n36.36\n44.44\n24.24\n41.41\n47.47\n53.54\n46.46\n57.58\n51.52\n48.48\n54.55\n46.46\ntur_Latn\n46.46\n50.51\n49.49\n64.65\n21.21\n57.58\n66.67\n66.67\n63.64\n68.69\n69.70\n68.69\n75.76\n74.75\nuig_Arab\n25.25\n17.17\n26.26\n17.17\n19.19\n28.28\n29.29\n27.27\n32.32\n36.36\n37.37\n42.42\n37.37\n57.58\nukr_Cyrl\n55.56\n52.53\n42.42\n60.61\n39.39\n57.58\n59.60\n60.61\n63.64\n65.66\n64.65\n65.66\n64.65\n54.55\numb_Latn\n27.27\n29.29\n30.30\n31.31\n17.17\n36.36\n36.36\n36.36\n34.34\n40.40\n38.38\n36.36\n38.38\n44.44\nurd_Arab\n47.47\n42.42\n63.64\n66.67\n24.24\n53.54\n54.55\n55.56\n50.51\n61.62\n60.61\n53.54\n58.59\n56.57\nuzb_Latn\n39.39\n52.53\n36.36\n37.37\n22.22\n50.51\n56.57\n61.62\n61.62\n68.69\n66.67\n63.64\n70.71\n43.43\nvec_Latn\n52.53\n51.52\n55.56\n63.64\n23.23\n58.59\n65.66\n62.63\n64.65\n69.70\n70.71\n65.66\n73.74\n52.53\nvie_Latn\n62.63\n52.53\n63.64\n68.69\n38.38\n59.60\n65.66\n61.62\n63.64\n69.70\n73.74\n64.65\n73.74\n42.42\nwar_Latn\n56.57\n49.49\n55.56\n57.58\n34.34\n57.58\n69.70\n73.74\n69.70\n78.79\n75.76\n76.77\n73.74\n44.44\nwol_Latn\n38.38\n34.34\n33.33\n35.35\n21.21\n37.37\n36.36\n41.41\n41.41\n41.41\n43.43\n36.36\n51.52\n64.65\nxho_Latn\n32.32\n31.31\n34.34\n36.36\n24.24\n37.37\n43.43\n47.47\n49.49\n55.56\n51.52\n51.52\n51.52\n65.66\nyid_Hebr\n18.18\n23.23\n16.16\n18.18\n17.17\n28.28\n34.34\n39.39\n38.38\n38.38\n43.43\n44.44\n41.41\n75.76\nyor_Latn\n21.21\n27.27\n40.40\n34.34\n20.20\n31.31\n37.37\n41.41\n38.38\n43.43\n44.44\n42.42\n41.41\n70.71\nyue_Hani\n41.41\n53.54\n70.71\n67.68\n42.42\n53.54\n68.69\n62.63\n67.68\n69.70\n64.65\n63.64\n68.69\n75.76\nzsm_Latn\n60.61\n51.52\n60.61\n66.67\n21.21\n58.59\n68.69\n65.66\n71.72\n71.72\n73.74\n72.73\n77.78\n38.38\nzul_Latn\n33.33\n23.23\n35.35\n31.31\n17.17\n36.36\n48.48\n51.52\n49.49\n53.54\n51.52\n52.53\n47.47\n67.68\nall\n41.94\n39.96\n44.22\n47.53\n26.08\n47.05\n54.10\n55.65\n55.34\n59.40\n59.05\n57.26\n60.49\n60.45\nTable 6: Detailed Results (Part II). For previous LLMs, 3-shot results are presented.\n"
  },
  {
    "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion",
    "link": "https://arxiv.org/pdf/2401.13388.pdf",
    "upvote": "9",
    "text": "UNIMO-G: Unified Image Generation through Multimodal Conditional\nDiffusion\nWei Li\u2217, Xue Xu\u2217, Jiachen Liu, Xinyan Xiao\nBaidu Inc., Beijing, China\n{liwei85.2023}@gmail.com\n{xuxue,xiaoxinyan,liujiachen}@baidu.com\nhttps://unimo-ptm.github.io/\nAbstract\nExisting text-to-image diffusion models primar-\nily generate images from text prompts. How-\never, the inherent conciseness of textual de-\nscriptions poses challenges in faithfully syn-\nthesizing images with intricate details, such as\nspecific entities or scenes. This paper presents\nUNIMO-G, a simple multimodal conditional\ndiffusion framework that operates on multi-\nmodal prompts with interleaved textual and\nvisual inputs, which demonstrates a unified\nability for both text-driven and subject-driven\nimage generation. UNIMO-G comprises two\ncore components: a Multimodal Large Lan-\nguage Model (MLLM) for encoding multi-\nmodal prompts, and a conditional denoising\ndiffusion network for generating images based\non the encoded multimodal input. We leverage\na two-stage training strategy to effectively train\nthe framework: firstly pre-training on large-\nscale text-image pairs to develop conditional\nimage generation capabilities, and then instruc-\ntion tuning with multimodal prompts to achieve\nunified image generation proficiency. A well-\ndesigned data processing pipeline involving lan-\nguage grounding and image segmentation is\nemployed to construct multi-modal prompts.\nUNIMO-G excels in both text-to-image genera-\ntion and zero-shot subject-driven synthesis, and\nis notably effective in generating high-fidelity\nimages from complex multimodal prompts in-\nvolving multiple image entities.\n1\nIntroduction\nRecent advancements in text-to-image (T2I) dif-\nfusion models have yielded impressive results in\nthe generation of high-fidelity images from textual\ndescriptions. Various methods, including DALLE-\n2 (Ramesh et al., 2022), Imagen (Saharia et al.,\n2022), Stable Diffusion (Rombach et al., 2022),\nand SD-XL (Podell et al., 2023), have been success-\nful in producing photo-realistic and contextually\n*These authors contributed equally to this work.\nrelevant images based on textual prompts. Never-\ntheless, a fundamental challenge persists due to the\ninherent brevity of textual descriptions, particularly\nwhen intricate details, specific entities, or nuanced\nscenes are involved. Thus, faithfully generating\nimages from general vision-language (VL) inputs\nis essential to improve the controllability of image\ngeneration.\nNumerous studies have explored VL-to-image\ngeneration techniques. Methods such as Dream-\nBooth (Ruiz et al., 2023), Imagic (Kawar et al.,\n2023), SuTI (Chen et al., 2023) and BLIP-\nDiffusion (Li et al., 2023a) emphasize subject-\ndriven generation, where they use both subject im-\nages and textual descriptions as inputs to recon-\ntextualize the subject in a newly described setting.\nThey either fine-tune specific models for a given\nsubject or employ pre-trained subject representa-\ntions. However, their specific training design and\ninput templates hinder their scalability, especially\nin complex scenarios with multiple entities. Ad-\nditionally, studies like FastComposer (Xiao et al.,\n2023) and Subject-Diffusion (Ma et al., 2023) focus\non multiple-entity image generation, integrating\nimage embeddings from image encoders with the\nstandard text conditioning in pre-trained diffusion\nmodels. Nevertheless, these approaches lack the\ncapacity to efficiently process generalized vision-\nlanguage inputs that comprise a mix of textual and\nvisual information in free forms.\nIn this paper, we propose UNIMO-G, a simple\nmultimodal conditional diffusion framework that\noperates on multimodal prompts comprising free-\nform interleaved vision-language inputs. Unlike\ntraditional text-only prompts, multimodal prompts\nencompass various combinations of image entities\nand textual elements, as demonstrated in Figure 1.\nUNIMO-G is designed to faithfully reproduce all\nimage entities, render textual content, and follow\nthe instructions in multimodal prompts. Specifi-\ncally, we leverage the perception capabilities of\narXiv:2401.13388v2  [cs.CV]  25 Jan 2024\nFigure 1: Examples of UNIMO-G for both text-driven and zero-shot subject-driven generation. UNIMO-G can\nperceive free-form interleaved visual-language inputs and faithfully generate images. Particularly, it can generate\nimages from multi-modal prompts with multiple image entities.\nMultimodal Large Language Models (MLLMs) to\nencode multimodal prompts into a unified vision-\nlanguage semantic space. Subsequently, a condi-\ntional diffusion network generates images from\nthese encoded representations.\nTo train UNIMO-G efficiently, we implement a\ntwo-phase strategy. Initially, the model undergoes\npre-training on a large-scale dataset of text-image\npairs, enhancing its proficiency in conditional im-\nage generation. This is followed by a phase of\ninstruction tuning with multimodal prompts, learns\nto generate images that align with the detailed spec-\nifications provided in these prompts. A carefully\ndesigned data processing pipeline, incorporating\nlanguage grounding and image segmentation, is\nemployed to construct these multimodal prompts.\nThis approach enables UNIMO-G to harness rich\nfeatures from the MLLM encoder to generate im-\nages faithfully reproducing the contents across var-\nious contexts.\nUNIMO-G exhibits a comprehensive capabil-\nity for controllable image generation, excelling not\nonly in text-to-image synthesis but also in zero-shot\nsubject-driven generation. It adeptly produces high-\nfidelity images from multimodal prompts, even\nthose containing multiple image entities. To as-\nsess its performance, we conducted evaluations in\nboth text-to-image and subject-driven generation\ncontexts using the MS-COCO and DreamBench\ndatasets, respectively.\nThe results consistently\nhighlight UNIMO-G\u2019s superior performance in\nthese scenarios. Additionally, recognizing Dream-\nBench\u2019s focus on single-subject generation, we in-\ntroduce MultiBench, a new benchmark featuring\nimages with multiple entities. The evaluation on\nMultiBench confirms UNIMO-G\u2019s effectiveness in\nzero-shot multi-entity subject-driven generation.\nIn summary, our contributions in this work can\nbe summarized as follows:\n\u2022 We propose a simple multi-modal conditional\ndiffusion framework that significantly en-\nhances the controllability of image generation\nby supporting multimodal prompts with inter-\nleaved images and text input.\n\u2022 We introduce an effective two-stage train-\ning strategy, empowering zero-shot multi-\nentity subject-driven generation through multi-\nmodal instruction tuning.\n\u2022 UNIMO-G outperforms existing VL-to-image\nmodels in both single and multi-entity subject-\nFigure 2: UNIMO-G consists of an MLLM for mul-\ntimodal perception, and a conditional denoising UNet\nfor image generation. It accepts multimodal prompts\nwith interleaved images and texts, and generate images\nconsistent with the image entities. Orange denotes the\ntrainable modules; Blue denotes the frozen ones.\ndriven generation tasks, especially on the ca-\npabilities of multimodal instruction following.\n2\nMethod\nThe architecture of UNIMO-G, as depicted in Fig-\nure 2, primarily comprises two key components: a\nMultimodal Large Language Model (MLLM) re-\nsponsible for encoding multimodal prompts and a\nconditional denoising diffusion network for image\ngeneration based on the encoded representations.\nIn our study, we employed an in-house Chinese\nMLLM, structurally analogous to MiniGPT-4 (Zhu\net al., 2023). It contains a total of 7 billion pa-\nrameters, underwent pre-training on a vast dataset\ncomprising billions of image-text pairs. This ex-\ntensive training process equips the model with a\nrobust capability to process and interpret complex\nmultimodal data. The training of UNIMO-G is\nconducted in a two-stage process:\n\u2022 Text-to-Image Pre-training: We pre-train the\nconditional denoising diffusion network from\nscratch on large-scale Chinese text-image\npairs. We employ the same U-Net network\narchitecture in Rombach et al. (2022) and con-\ndition it on the text using a cross-attention\nmechanism.\n\u2022 Multi-modal Instruction Tuning: We further\nfine-tune UNIMO-G on millions of pairs of\nmultimodal prompts and images, to improve\nthe capability of faithfully generating images\nfrom multimodal inputs.\nIt is worth noting that during both stages of train-\ning, only the U-Net component is actively trained,\nwith the MLLM parameters frozen. This strategy\nensures that UNIMO-G effectively learns to gen-\nerate images while retaining the perception knowl-\nedge encoded in the pre-trained MLLM.\n2.1\nText-to-Image Pre-training\nPreliminaries\nWe follow the latent diffusion\nmodel (Rombach et al., 2022), utilizing the percep-\ntual compression model (i.e., VQ-VAE) consisting\nof an image encoder E and decoder D to encode\nthe pixel data into the latent space and reverse,\nsuch that D(E(x)) \u2248 x. The diffusion process is\nthen performed on the latent space, which defines\na Markov chain of forward diffusion process q by\ngradually adding Gaussian noise to the initial la-\ntent representations z0 = E(x) over T steps. The\nforward process q(zt|zt\u22121) at each time step t can\nbe expressed as follows:\nq(zt|zt\u22121) = N(zt;\np\n1 \u2212 \u03b2tzt\u22121, \u03b2tI)\nwhere {\u03b2t} is a series of hyper-parameters. Dif-\nfusion models are trained to learn a conditional\nU-Net (Ronneberger et al., 2015) denoiser \u03f5\u03b8 to re-\nverse the diffusion Markov chain, predicting noise\nwith current timestep t, noisy latent zt and gener-\nation condition c. The training loss is the mean\nsquared error (MSE) between the predicted noise\n\u03f5\u03b8(zt, t, c) and the real noise \u03f5 in zt:\nL = Ez0,c,\u03f5\u223cN(0,1),t[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, c)]\u22252\nLarge-scale Chinese text-image pairs are utilized to\ntrain the above denoising objective. The condition\ninformation c is fed into each cross attention block\nof the UNet model as:\nAttn(zt, c) = softmax(Q(zt) \u00b7 K(c)T )\n\u221a\nd\n) \u00b7 V (c)\nwhere Q, K and V denote the query, key and value\nprojections, respectively. d denotes the output di-\nmension of the features. In our model, the condi-\ntion c is encoded by the pre-trained MLLM.\nPre-training Strategies\nTraining a text-to-image\ndiffusion model from scratch presents significant\nchallenges in terms of complexity and resource\nexpenditure. To address these, we introduce an ef-\nfective training schedule to enhance the efficiency\nand performance of model training. This schedule\nencompasses three phases: (1) initial training on a\nsmall image corpus to establish foundational visual\ndistribution knowledge; (2) subsequent expansion\nto large-scale text-image pair training, focusing on\ntext-visual correspondence; (3) culminating in a\nfinal phase of training with a small refined corpus,\ncharacterized by high visual aesthetics and pre-\ncise text-image alignment. In our experiments, the\ntraining of the UNet model is initially conducted us-\ning the CC3M dataset (Sharma et al., 2018). This\ndataset is chosen for its diverse range of visual\nconcepts coupled with straightforward image de-\nscriptions, making it an effective tool for initiating\ntraining from scratch. Subsequently, the model\nundergoes further training with an extensive collec-\ntion of 300M text-image pairs, aimed at broaden-\ning its conceptual understanding and improving\nits alignment to textual descriptions. The final\nphase of training involves fine-tuning the model\nusing a meticulously curated corpus, consisting of\ntens of thousands of high-quality image-text pairs,\ncarefully selected for their superior quality. Based\non the above strategies and our architecture de-\nsigns, we obtain a powerful Chinese text-to-Image\ngeneration model, surpassing open-source mod-\nels like Stable Diffusion and its advanced version\nSD-XL (Podell et al., 2023). We provide detailed\nevaluations of our results in Section 4.2 and imple-\nmentation details in Appendix A.\n2.2\nMultimodal Instruction Tuning\nFollowing the text-to-image pre-training, UNIMO-\nG is indeed capable of generating images from\ninterleaved images and texts, relying on the per-\nception capabilities of MLLM. However, it is im-\nportant to note that the pre-training stage primarily\nfocuses on generating images that are semantically\nconsistent with the input representations. As a re-\nsult, UNIMO-G still face challenges in utilizing\nthe visual features of inputs to faithfully reproduce\nthe contents specified in the image conditions. To\naddress this limitation, we further conduct multi-\nmodal instruction tuning to enhance UNIMO-G\u2019s\nability to faithfully reproduce image contents in\ndiverse contexts.\nMultimodal Prompts\nTo enhance the represen-\ntativeness of text prompts, we introduce a format\nfor multimodal prompts that are composed of in-\nterleaved images and texts. Specifically, entities\nmentioned in text captions can be substituted with\ntheir corresponding images, like \u201c<img>image of\nElon Musk</img> holding his <img>image of\niPhone</img>, takes a selfie on the street\u201d, as\nshown in Figure 2. To create pairs of multimodal\nFigure 3: Overview of our data construction pipeline\nfor multi-modal instruction tuning.\nprompts and images, we have designed a data pro-\ncessing pipeline as illustrated in Figure 3. The\npipeline first generates captions and extracts enti-\nties from the caption by MLLM. Subsequently, it\nacquires the corresponding image segment for each\nentity using a combination of language grounding\nby Grounding DINO (Liu et al., 2023) and image\nsegmentation by SAM (Kirillov et al., 2023). Fur-\nther introduction on the data construction process\nis provided in Section A. With a collection of pairs\nof multimodal prompts and images, UNIMO-G is\ntrained to generate images in accordance with these\nmultimodal instructions.\nVisual-Enhanced Learning\nIn order to better\nharness the visual features of multi-modal input,\nwe introduce an enhancement to the cross-attention\nmechanism between the generated objects and the\ninput image entities. This improvement aims to fos-\nter a more robust and context-aware understanding\nof the relationships between generated content and\nthe visual elements within the input images. As\nstated by Prompt-by-Prompt (Hertz et al., 2022),\nthe cross-attention in text-to-image diffusion mod-\nels can reflect the positions of each generated object\nspecified by the corresponding text token. Simi-\nlarly, the visual features of image entities can also\nbe treated as visual tokens. The cross-attention\nmap between the intermediate feature of the noisy\nlatent zt and the visual token v can be calculated:\nCA(zt, v) = Softmax(Q(zt) \u00b7 K(v)T )\nwhere Q and K denote the query and key projec-\ntions, respectively. For each visual token, we could\nget an attention map of h \u00d7 w, where h and w\nare the spatial dimensions of the latent feature zt.\nThe scores in cross-attention maps represent the\namount of information that flows from a visual\ntoken to a latent pixel. Therefore, we introduce\nan additional loss term that encourages the model\nto ensure that each visual token mainly attends to\nthe image region occupied by the corresponding\nobjects. Specifically, we optimize zt towards the\ntarget that the desired area of the object has large\nvalues by penalizing the L1 deviation between the\nattention maps and the corresponding segmentation\nmaps of the entities:\nLattn = 1\nN\nN\nX\nk=1\n| CA(zt, vk) \u2212 Mk |\nwhere Mk is the segmentation mask of the kth ob-\nject corresponding to its visual token vk. Through\nthis training process, UNIMO-G learns to effec-\ntively harness the visual features of input images\nto faithfully reproduce the corresponding content.\n3\nRelated Work\nText-to-Image Diffusion Generation\nThe incor-\nporation of diffusion models into text-to-image syn-\nthesis represents a notable advancement in com-\nputational creativity (Ho et al., 2020; Song et al.,\n2020; Li et al., 2022). Models like GLIDE (Nichol\net al., 2021) and DALL-E 2 (Ramesh et al., 2022),\nwhich utilize CLIP image embeddings, have sub-\nstantially progressed in producing images that are\nboth diverse and semantically coherent with tex-\ntual inputs. Imagen (Saharia et al., 2022) under-\nscores the importance of language comprehension,\nproposing the integration of a large T5 language\nmodel to enhance semantic representation. The\nLatent Diffusion model (Rombach et al., 2022) ad-\ndresses computational constraints by generating\nimages from text-conditioned, low-dimensional la-\ntent spaces. Our proposed framework builds upon\nthe principles of the Latent Diffusion model, lever-\naging its computational efficiency and scalability.\nSubject-Driven Image Generation\nFollowing\nthe success of generating high quality im-\nages from text descriptions, recent studies have\nexplored subject-driven generation techniques.\nModels like DreamBooth (Ruiz et al., 2023),\ntextual-inversion (Gal et al., 2022), and custom-\ndiffusion (Kumari et al., 2023) use optimization-\nbased methods to embed subjects into diffusion\nmodels. This is achieved by either fine-tuning the\nmodel weights (Ruiz et al., 2023; Kumari et al.,\n2023) or inverting the subject image into a text to-\nken that encodes the subject identity (Gal et al.,\n2022).\nSome works have explored tuning-free\nmethods. ELITE (Wei et al., 2023) and Instant-\nBooth (Shi et al., 2023) project reference images\ninto word embeddings and inject reference image\npatch features into cross-attention layers to enhance\nlocal details. PhotoMaker (Li et al., 2023b) focuses\non the generation of human portraits by extract-\ning a stacked ID embedding from multiple ID im-\nages. Despite impressive results for single-object\ncustomization, their architecture design restricts\ntheir scalability to multiple subject settings. Mod-\nels like Subject-Diffusion (Ma et al., 2023) and\nFastComposer (Xiao et al., 2023) are designed for\nmulti-entity subject-driven generation. They en-\nhance text conditioning within diffusion models by\nincorporating subject embeddings extracted from\nimage encoders.Yet, a prevalent limitation of these\napproaches is their inclination to separate textual\nand visual guidance, thereby constraining the ef-\nficacy of joint modality integration. This issue is\nparticularly pronounced when extending to scenar-\nios with multiple entities.\nGenerating with Multi-modal Language Models\nMultimodal Large Language Models (MLLMs)\nhave significantly broadened the capabilities of\nlanguage models to process various modalities,\nincluding images.\nThese models inherently fa-\ncilitate interleaved vision-language input, effec-\ntively handling multiple images. Notable works\ninclude M-VADER (Weinbach et al., 2022), which\naligns MLLM semantics with the diffusion image\ndecoder via image-caption pair training. Addi-\ntionally, models such as GILL (Koh et al., 2023),\nEmu (Sun et al., 2023), Emu2 (Sun et al., 2023)\nand DreamLLM (Dong et al., 2023) specialize in\ninterleaved vision-language generation, aligning\nthe output space of MLLMs with the diffusion\nimage decoder by leveraging CLIP supervision\nor pre-training on multimodal datasets. However,\nthese methods primarily align at a semantic level\nand may struggle with detailed, subject-driven im-\nage generation. BLIP-Diffusion (Li et al., 2023a)\nsynthesizes images by composing subjects with\nrandom backgrounds, endowing it with zero-shot,\nsubject-driven text-to-image generation capabili-\nties. However, its specific input template and train-\ning process limit scalability for multiple entities.\nKOSMOS-G (Pan et al., 2023), a model closely\nrelated to our work, leverages a MLLM to encode\ninterleaved text-visual inputs. It utilizes the U-Net\nof Stable Diffusion (SD) v1.5 as its image decoder,\nwhich remains frozen throughout the training pro-\ncess. The key component of KOSMOS-G is an\nAlignerNet, trained solely on textual data, to align\nthe output embedding space of the frozen SDv1.5\nU-Net with the MLLM. In contrast, our approach\ncenters on training the U-Net model end-to-end\nspecifically for multimodal diffusion, significantly\nenhancing both fidelity and relevance of generated\nimages in multimodal contexts. Differing from\nembedding alignment-based methodologies, our\ntwo-stage training strategy markedly improves the\nmodel\u2019s capability in following multimodal instruc-\ntions, which results in superior performance in both\ntext-driven and subject-driven generation tasks, par-\nticularly in complex multi-entity scenarios.\n4\nExperiments\nIn this section, we first introduce the implementa-\ntion details and settings of experiments. Then we\npresent the evaluation results in both text-driven\nand subject-driven scenarios. Last, we further ana-\nlyze the results with quantitative ablation studies.\n4.1\nImplementation Details\nUNIMO-G is composed of a 7B-parameter\nMLLM encoder, following the MiniGPT-4 architec-\nture (Zhu et al., 2023), and a 4B-parameter denois-\ning U-Net, totaling approximately 11B parameters.\nThe MLLM is pretrained on a large-scale Chinese\nmultimodal corpus, comprising text, image-caption\npairs, and interleaved image-text data. The U-Net\narchitecture includes 5 downsampling and 5 up-\nsampling blocks with channel sizes [640, 1280,\n2560, 2560], and a cross-attention mechanism with\n4096 dimensions and 16 heads. The image auto-\nencoder, based on the LDM framework, has been\noptimized for our specific image corpus. The detail\ntraining process and data construction are further\nintroduced in Appendix A.\n4.2\nEvaluation Results\nUNIMO-G demonstrates a unified image gener-\nation capability for both text-driven and subject-\ndriven image generation, as shown in Figure 1. In\nthe following, we will evaluate the performance of\nUNIMO-G from different aspects.\nText-to-Image Generation\nFor text-to-image\ngeneration, we used 30,000 captions randomly sam-\npled from the MS-COCO (Lin et al., 2014) valida-\ntion set, translating them into Chinese to align with\nUNIMO-G\u2019s input requirements. Images were gen-\nerated at 512x512 pixels and resized to 256x256 for\nMethods\nFID\nT2I Models\nGLIDE (Nichol et al., 2021)\n12.24\nDALL-E 2 (Ramesh et al., 2022)\n10.39\nSDv1.5 (Rombach et al., 2022)\n9.34\nImagen (Saharia et al., 2022)\n7.27\nSDXL (Podell et al., 2023)\n11.93\nVL2I Models\nGILL (Koh et al., 2023)\n12.20\nEmu (Sun et al., 2023)\n11.66\nKOSMOS-G (Pan et al., 2023)\n10.99\nUNIMO-G\n8.36\nTable 1:\nZero-shot FID-30K comparisons on MS-\nCOCO 256x256.\nFigure 4: Comparison of UNIMO-G and SD-XL by\nhuman evaluation. The mean and standard deviation are\nshown in the figure.\nevaluation using the FID-30k metric, a standard in\nthe field. Our model employs a classifier-free guid-\nance scale of 5.0 and 50 DDIM inference steps for\ndiffusion sampling. As shown in Table 1, UNIMO-\nG significantly surpasses other Vision-Language to\nImage (VL2I) models in performance.\nTo further validate our model, we conducted a\nhuman evaluation comparing UNIMO-G with SD-\nXL (Podell et al., 2023), a leading open-source\nmodel. We established a comprehensive bilingual\nbenchmark, encompassing 312 prompts (162 from\nDrawBench and 150 user queries randomly sam-\npled from online * platforms). The DrawBench\nprompts were filtered to exclude language-specific\nones. All prompts are manually translated and care-\nfully proofread to achieve the final parallel Chinese\nand English set. Three independent evaluators rated\nthe images from UNIMO-G and SD-XL by select-\ning the model they prefer, focusing on aspects of\nimage aesthetics, image-text relevance, and over-\nall quality, respectively. The results demonstrate\n*https://yige.baidu.com/\nMethods\nDINO\nCLIP-I\nCLIP-T\nAvg\nFine-Tuning Methods\nTextual Inversion\n0.569\n0.780\n0.255\n0.535\nDreamBooth\n0.668\n0.803\n0.305\n0.592\nBLIP-Diffusion\n0.670\n0.805\n0.302\n0.592\nZero-Shot Methods\nRe-Imagen\n0.600\n0.740\n0.270\n0.537\nBLIP-Diffusion\n0.594\n0.779\n0.300\n0.558\nKOSMOS-G\n0.694\n0.847\n0.287\n0.609\nUNIMO-G\n0.668\n0.841\n0.329\n0.613\nw/o Tuning\n0.371\n0.717\n0.306\n0.465\nw/o VisualEnh\n0.617\n0.815\n0.329\n0.587\nTable 2: Comparisons of single-entity subject-driven\nimage generation on DreamBench. Avg denotes the\naverage score of DINO, CLIP-I and CLIP-T.\nUNIMO-G\u2019s substantial superiority in all aspects.\nSingle-Entity Subject-Driven Generation\nFor\nsingle-entity subject driven generation, we evalu-\nate UNIMO-G on DreamBench (Ruiz et al., 2023).\nDreamBench comprises 30 subjects with 25 prompt\ntemplates, yielding 750 unique prompts that test\nskills such as re-contextualization, modification,\nand accessorization. We follow prior work to gen-\nerate four images for each prompt, creating a to-\ntal of 3,000 images for a comprehensive assess-\nment. We employed DINO and CLIP-I metrics for\nsubject fidelity evaluation and CLIP-T for image-\ntext relevance assessment. A classifier-free guid-\nance scale of 5.0 and 50 DDIM inference steps\nwere used for sampling. UNIMO-G, accepting a\nsingle image input, utilized the same images as\nKOSMOS-G (Pan et al., 2023) for a consistent\ncomparison. As indicated in Table 2, UNIMO-\nG in a zero-shot setting surpasses other models\nlike Textual Inversion (Gal et al., 2022), Dream-\nBooth (Ruiz et al., 2023), BLIP-Diffusion (Li et al.,\n2023a), and Re-Imagen (Chen et al., 2022), and\nmarginally outperforms KOSMOS-G. Notably, our\nmodel demonstrates a significant improvement in\nbalancing image-text relevance and image fidelity\ncompared to the closely related KOSMOS-G. We\nobserved that existing methods tend to prioritize\nimage information over textual input. This ten-\ndency occasionally leads to a diminished focus on\nsemantic content, favoring subject reconstruction.\nThanks to our two-stage end-to-end learning frame-\nwork, UNIMO-G maintained high image fidelity\nand achieved the highest CLIP-T score for image-\ntext relevance, indicating a strong capability in fol-\nlowing multi-modal instructions.\nFigure 5: Comparison with baselines for multi-entity subject-driven image generation.\nMulti-Entity\nSubject-Driven\nGeneration\nUNIMO-G exhibits exceptional performance in\nzero-shot multi-entity subject-driven generation.\nTo evaluate this capability, we established Multi-\nBench, a novel benchmark specifically designed for\nmulti-entity subject-driven generation assessment.\nMultiBench includes four object categories: living\nobjects (humans and animals), food, wearable\nitems, and toys, each containing 10 different\nobjects.\nWe developed five prompt templates\nfor composing scenarios with 2 and 3 objects,\nresulting in a total of 2,300 distinct prompts.\nDetails are introduced in the Appendix B. For each\nprompt, four images were generated, culminating\nin 9,200 images for an exhaustive evaluation. We\nconducted image similarity analysis using DINO\nand CLIP-I metrics, alongside text relevance\nassessments using CLIP-T. Image similarity was\ndetermined by averaging the similarities between\nthe generated image and each of the two or three\nsubjects. The results, as shown in Table 3, indicate\nthat UNIMO-G outperforms BLIP-Diffusion and\nKOSMOS-G in terms of both image similarity and\ntextual relevance. Some comparison examples are\nshown in Figure 5. This demonstrates UNIMO-G\u2019s\nsuperior capability to accurately capture subject\ninformation from input images and effectively\nfollow multi-modal instructions. More examples\nMethods\nDINO\nCLIP-I\nCLIP-T\nAvg\nBLIP-Diffusion\n0.410\n0.648\n0.249\n0.436\nKOSMOS-G\n0.419\n0.671\n0.283\n0.458\nUNIMO-G\n0.436\n0.665\n0.298\n0.466\nw/o Tuning\n0.235\n0.583\n0.240\n0.353\nw/o VisualEnh\n0.399\n0.631\n0.276\n0.435\nTable 3: Comparisons of multi-entity subject-driven\nimage generation on MultiBench.\nFigure 6: Comparison of UNIMO-G and KOSMOS-G\non MultiBench by human evaluation. The mean and\nstandard deviation are shown in the figure.\nare shown in Figures 8 and 9.\nTo further validate our model, we conducted\na human evaluation comparing UNIMO-G with\nKSOMOS-G by sampling 200 prompts from Multi-\nBench. Three raters are presented with two sets of\nimages generated by UNIMO-G and the compared\nmodel. They are asked to compare these images\nfrom three dimensions of semantic relevance, vi-\nsual faithfulness and image fidelity, and then select\nthe model they prefer, or indifference. Throughout\nthe process, raters are unaware of which model the\nimage is generated from. The results in Figure 6\nshow that human raters greatly prefer UNIMO-G\nover KOSMOS-G on all aspects, which further vali-\ndate the effectiveness of our approach in generating\nhigh-quality, personalized images from free-form\nmultimodal prompts.\n4.3\nAnalysis\nEffectiveness of Multi-modal Instruction Tuning\nUNIMO-G, prior to multi-modal instruction tuning\n(denoted as \u201cw/o Tuning\u201d), also demonstrates the\ncapability to generate images from multi-modal\nprompts based on the MLLM. Nonetheless, it ini-\ntially falls short in accurately reproducing input\nimages. To evaluate the effectiveness of multi-\nmodal instruction tuning, we compared the perfor-\nmance of UNIMO-G with and without this tuning\nin single-entity and multi-entity subject-driven gen-\neration tasks. The comparison results in Table 2\nand Table 3 reveal that multi-modal instruction tun-\ning substantially enhances image similarity metrics\n(DINO and CLIP-I) in both single and multi-entity\nscenarios. This improvement indicates that after\ntuning, UNIMO-G more effectively leverages vi-\nsual features from the inputs, thereby accurately\nreplicating the content defined in the image con-\nditions. Furthermore, UNIMO-G with instruction\ntuning also shows obvious advancements in textual\nrelevance, as evaluated by the CLIP-T metric. This\nindicates that the tuning process not only bolsters\nimage fidelity but also amplifies the model\u2019s ability\nto follow multimodal instructions.\nEffectiveness of Visual Enhancement Learning\nThe incorporation of a visual-enhanced learning\nstrategy during multimodal instructional tuning sig-\nnificantly improves the visual alignment between\ninput and output images. To quantify this effect, we\nconducted an ablation study by omitting the visual\nenhancement component during multimodal tun-\ning (denoted as \u201cw/o VisualEnh\u201d) and assessed its\nimpact on both single-entity and multi-entity gen-\neration tasks. The results, as detailed in Tables 2\nand 3, demonstrate that the visual-enhanced learn-\ning strategy markedly boosts the performance in im-\nage similarity metrics (DINO and CLIP-I), across\nboth single and multi-entity scenarios. Notably, it\nalso improves image-text alignment in multi-entity\nscenarios by reducing entity blending or missing.\n5\nConclusion\nThis paper presents UNIMO-G, a simple multi-\nmodal conditional diffusion framework designed\nto process multimodal prompts that interleave text\nand visual inputs. It demonstrates exceptional pro-\nficiency in text-to-image generation and zero-shot\nsubject-driven synthesis, and is particularly adept\nat producing high-fidelity images from intricate\nmulti-modal prompts with multiple image entities.\nIn comparison to standard text-conditional diffu-\nsion models, UNIMO-G significantly enhances vi-\nsual controllability in image generation. Thanks\nto our two-stage training strategy, UNIMO-G also\noutperforms existing VL-to-image models, espe-\ncially on the ability to follow complex multimodal\ninstructions. Overall, UNIMO-G showcases the\npotential for more nuanced and controlled image\ngeneration processes.\n6\nLimitations\nOur model suffers from some common failures of\ntext-driven and subject-driven generation models,\nsuch as inaccuracies in context synthesis, difficul-\nties in complex composition, and a shortfall in vi-\nsual faithfulness, particularly in multi-entity image\ngeneration tasks. Additionally, there exists an in-\nherent risk associated with the misuse of such tech-\nnology, notably in the creation of deepfakes, which\nraises ethical concerns. Despite the limitations and\nrisks, the proposed framework still demonstrates\nconsiderable promise in facilitating more nuanced\nand controlled processes in image generation.\nReferences\nMinwoo Byeon, Beomhee Park, Haecheon Kim,\nSungjun Lee, Woonhyuk Baek, and Saehoon Kim.\n2022. Coyo-700m: Image-text pair dataset. https:\n//github.com/kakaobrain/coyo-dataset.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 3558\u20133568.\nWenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui,\nXuhui Jia, Ming-Wei Chang, and William W Co-\nhen. 2023.\nSubject-driven text-to-image genera-\ntion via apprenticeship learning.\narXiv preprint\narXiv:2304.00186.\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and\nWilliam W Cohen. 2022.\nRe-imagen: Retrieval-\naugmented text-to-image generator. arXiv preprint\narXiv:2209.14491.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,\nZheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,\nHongyu Zhou, Haoran Wei, et al. 2023. Dreamllm:\nSynergistic multimodal comprehension and creation.\narXiv preprint arXiv:2309.11499.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-\nOr. 2022. An image is worth one word: Personaliz-\ning text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aber-\nman, Yael Pritch, and Daniel Cohen-Or. 2022.\nPrompt-to-prompt image editing with cross attention\ncontrol. arXiv preprint arXiv:2208.01626.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840\u2013\n6851.\nBahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-\nwen Chang, Tali Dekel, Inbar Mosseri, and Michal\nIrani. 2023.\nImagic: Text-based real image edit-\ning with diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6007\u20136017.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen\nLo, et al. 2023. Segment anything. arXiv preprint\narXiv:2304.02643.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.\n2023. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. 2023. Multi-concept\ncustomization of text-to-image diffusion. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1931\u20131941.\nDongxu Li, Junnan Li, and Steven CH Hoi. 2023a. Blip-\ndiffusion: Pre-trained subject representation for con-\ntrollable text-to-image generation and editing. arXiv\npreprint arXiv:2305.14720.\nWei Li, Xue Xu, Xinyan Xiao, Jiachen Liu, Hu Yang,\nGuohao Li, Zhanpeng Wang, Zhifan Feng, Qiaoqiao\nShe, Yajuan Lyu, et al. 2022. Upainting: Unified\ntext-to-image diffusion generation with cross-modal\nguidance. arXiv preprint arXiv:2210.16031.\nZhen Li, Mingdeng Cao, Xintao Wang, Zhongang\nQi, Ming-Ming Cheng, and Ying Shan. 2023b.\nPhotomaker:\nCustomizing realistic human pho-\ntos via stacked id embedding.\narXiv preprint\narXiv:2312.04461.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang\nSu, Jun Zhu, et al. 2023. Grounding dino: Marrying\ndino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499.\nJian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\n2023.\nSubject-diffusion: Open domain personal-\nized text-to-image generation without test-time fine-\ntuning. arXiv preprint arXiv:2307.11410.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. 2021.\nGlide:\nTo-\nwards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint\narXiv:2112.10741.\nXichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. 2023. Kosmos-g: Gen-\nerating images in context with multimodal large lan-\nguage models. arXiv preprint arXiv:2310.02992.\nDustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. 2023. Sdxl: Improving latent\ndiffusion models for high-resolution image synthesis.\narXiv preprint arXiv:2307.01952.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022.\nHierarchical text-\nconditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125, 1(2):3.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages\n10684\u201310695.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing\nand Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18, pages\n234\u2013241. Springer.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch, Michael Rubinstein, and Kfir Aberman. 2023.\nDreambooth: Fine tuning text-to-image diffusion\nmodels for subject-driven generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 22500\u201322510.\nChitwan Saharia, William Chan, Saurabh Saxena,\nLala Li, Jay Whang, Emily L Denton, Kam-\nyar Ghasemipour, Raphael Gontijo Lopes, Burcu\nKaragol Ayan, Tim Salimans, et al. 2022. Photo-\nrealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 35:36479\u201336494.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. 2022. Laion-5b: An open\nlarge-scale dataset for training next generation image-\ntext models. Advances in Neural Information Pro-\ncessing Systems, 35:25278\u201325294.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556\u20132565.\nJing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung.\n2023. Instantbooth: Personalized text-to-image gen-\neration without test-time finetuning. arXiv preprint\narXiv:2304.03411.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020.\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,\nXiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang.\n2023. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222.\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. 2023. Elite: Encod-\ning visual concepts into textual embeddings for cus-\ntomized text-to-image generation. arXiv preprint\narXiv:2302.13848.\nSamuel Weinbach, Marco Bellagente, Constantin\nEichenberg, Andrew Dai, Robert Baldock, Souradeep\nNanda, Bj\u00f6rn Deiseroth, Koen Oostermeijer, Han-\nnah Teufel, and Andres Felipe Cruz-Salinas. 2022.\nM-vader: A model for diffusion with multimodal\ncontext. arXiv preprint arXiv:2212.02936.\nGuangxuan Xiao, Tianwei Yin, William T Freeman,\nFr\u00e9do Durand, and Song Han. 2023. Fastcomposer:\nTuning-free multi-subject image generation with lo-\ncalized attention. arXiv preprint arXiv:2305.10431.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nA\nImplementation Details\nThe training process include text-to-image pre-\ntraining and multi-modal instruction tuning.\nText-to-Image Pre-training\nOur model\u2019s pre-\ntraining involves three stages, each utilizing dis-\ntinct datasets:\n1. Initial Training with CC3M Dataset: The\nCC3M dataset, consists of about 3.3M image-\ndescription pairs, was translated into Chinese\nusing Baidu Translate API *. The model is\ntrained from scratch at 256x256 using the\nAdamW optimizer with a weight decay of\n0.01, a learning rate of 5e-5, and a batch size\nof 40x256 for 100K steps.\n2. Expansion with Large-Scale Chinese Data:\nWe incorporate about 300M internal Chinese\ntext-image pairs from multiple datasets, in-\ncluding LAION-2B (Schuhmann et al., 2022),\nCOYO-700M (Byeon et al., 2022), Concep-\ntual Captions (Changpinyo et al., 2021) and\na series of internal Chinese datasets.\nThe\nEnglish captions are translated into Chinese.\n*https://fanyi.baidu.com\na {living object} wearing {wearing}\na {living object} is playing with {toy}\na {living object} is eating {food}\na {living object} wearing {wearing}, is playing with {toy}\na {living object} wearing {wearing}, is eating {food}\nTable 4: Templates for multi-entity subject-driven gen-\neration in MultiBench.\nThis stage, with a constant learning rate of 5e-\n5, initially training at 256x256 resolution for\n500K steps with a batch size of 40x256, then\nprogresses to 512x512 for 200K steps with a\nbatch size of 12x256.\n3. Refinement with High-Quality Corpus: The\nfinal stage focuses on fine-tuning on high-\nquality corpus, and continues training at\n512x512 and 1024x1024 resolutions. Selec-\ntion criteria include an aesthetic threshold\nabove 6.5 (LAION-Aesthetics V2) and an\nimage-text similarity score over 0.5 (CLIP-\nbase-32), resulting in about 1M pairs. A multi-\nscale training strategy dividing the image size\ninto 5 buckets [0.5, 0.75, 1.0, 1.5, 2.0] sup-\nports various image aspect ratios. The final\nstage uses a learning rate of 1e-5 for 200K\nsteps with a batch size of 3072.\nMulti-modal Instruction Tuning\nWe developed\na multimodal-to-image instruction tuning dataset\nutilizing the 1M high-quality image-text pairs. The\nprocess, depicted in Figure 3, involves: (1) Genera-\ntion of captions and extraction of entity tokens from\ncaptions by the MLLM; (2) Identifying entity detec-\ntion boxes via Grounding DINO (Liu et al., 2023);\n(3) Segmentation and extraction of regions corre-\nsponding to each entity by SAM (Kirillov et al.,\n2023); (4) Randomly substitution of entity tokens\nwith their corresponding image segments. This pro-\ncess effectively transforms the original text-image\npairs into multimodal-image pairs, which are then\nemployed for refining multimodal instruction tun-\ning. We maintain a learning rate of 1e-5, training\nfor 200K steps with a batch size of 3072. To pre-\nserve text-to-image generation capabilities, 10% of\nthe training uses original textual captions.\nB\nImages and Prompts of MultiBench\nMultiBench contains 4 categories of objects, each\nincluding 10 objects. The images in MultiBench\nare shown in Figure 7. Five temples are designed\nto compose multi-modal prompts, as shown in Ta-\nFigure 7: Illustration of images in MultiBench.\nble 4. Some examples generated by UNIMO-G on\nMultiBench are shown in Figure 8 and Figure 9.\nFigure 8: Examples of multi-entity subject-driven image generation on MultiBench by UNIMO-G.\nFigure 9: Examples of multi-entity subject-driven image generation on MultiBench by UNIMO-G.\nFigure 10: Examples of text-to-image generation by UNIMO-G.\n"
  },
  {
    "title": "SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection",
    "link": "https://arxiv.org/pdf/2401.13160.pdf",
    "upvote": "8",
    "text": "SPACTOR-T5: Pre-training T5 Models with Span\nCorruption and Replaced Token Detection\nKe Ye\nGoogle Research\nkkye@google.com\nHeinrich Jiang\nGoogle Research\nheinrichj@google.com\nAfshin Rostamizadeh\nGoogle Research\nrostami@google.com\nAyan Chakrabarti\nGoogle Research\nayanchakrab@google.com\nGiulia DeSalvo\nGoogle Research\ngiuliad@google.com\nJean-Fran\u00e7ois Kagy\nGoogle Research\njfkagy@google.com\nLazaros Karydas\nGoogle Research\nlkary@google.com\nGui Citovsky\nGoogle Research\ngcitovsky@google.com\nSanjiv Kumar\nGoogle Research\nsanjivk@google.com\nAbstract\nPre-training large language models is known to be extremely resource intensive and\noftentimes inefficient, under-utilizing the information encapsulated in the training\ntext sequences. In this paper, we present SPACTOR, a new training procedure\nconsisting of (1) a hybrid objective combining span corruption (SC) and token\nreplacement detection (RTD), and (2) a two-stage curriculum that optimizes the\nhybrid objective over the initial \u03c4 iterations, then transitions to standard SC loss.\nWe show empirically that the effectiveness of the hybrid objective is tied to the\ntwo-stage pre-training schedule, and provide extensive analysis on why this is the\ncase. In our experiments with encoder-decoder architectures (T5) on a variety of\nNLP tasks, SPACTOR-T5 yields the same downstream performance as standard SC\npre-training, while enabling a 50% reduction in pre-training iterations and 40% re-\nduction in total FLOPs. Alternatively, given the same amount of computing budget,\nwe find that SPACTOR results in significantly improved downstream benchmark\nperformance.\n1\nIntroduction\nThe recent emergence of successful large language models (LLMs) is in no small part due to\nthe remarkable effectiveness of self-supervised pre-training on massive text corpora. Pre-trained\nmodels are found to perform strongly on a wide range of downstream tasks including natural\nlanguage understanding (NLU) and generation (NLG) \u2014 through fine-tuning on small task-specific\ndatasets (Wei et al., 2021; Sanh et al., 2022; Chung et al., 2022), or through zero-shot / few-shot\nevaluation, whereby the model is given only task-specific instructions as input, or a handful of\nadditional exemplars to learn from, respectively (Brown et al., 2020).\nOn the one hand, pre-training LLMs using self-supervised objectives frees us from the burden of\ngathering human labels; on the other, the indirect nature of the supervision also means that each batch\nof text provides only weak signals that the model can learn from. Consequently, LLMs need to be\npre-trained on datasets several orders of magnitude larger than the labeled domain specific datasets.\nTherefore, a major bottleneck in developing performant LLMs is the massive computational cost\nincurred at the pre-training phase \u2014 e.g., GPT-3 (175B parameters) (Brown et al., 2020) and PaLM\n(540B parameters) (Chowdhery et al., 2022) need up to tens of thousands of PetaFLOP/s-days of\narXiv:2401.13160v1  [cs.LG]  24 Jan 2024\nThe black dog ran very fast and jumped on the couch.\nThe [M] [M] ran [S0] and jumped [S1].\nThe blue cat ran [S0] and jumped [S1].\nDiscriminator Encoder\n[1, 0, 0, 1, 1, 1, 1, 1, 1]\nDiscriminator Decoder\n[S0] very fast [S1] on the couch. <eos>\nGenerator (Encoder-only)\n(1)\nT5\nFigure 1: The SPACTOR pre-training objective in the first stage. In step (1), the original text is\nrandomly corrupted with span corruption (marked as [S0], [S1], etc, ) and then token-level random\nmasking (marked as [M]). A small auxiliary generator model G is trained to recover [M] only. The\nresulting text is then fed into the T5 discriminator D, whose encoder component learns to predict at\nevery position whether the token is a replaced one, while its decoder component learns to fill in the\nground truth token as in standard span corruption.\ncompute for pre-training, respectively. In order to effectively scale language models towards better\nquality, it is imperative to design more efficient self-supervision strategies under which more useful\nsignals for learning downstream tasks are extracted out of each pre-training iteration on unlabeled\ndata\nIn this paper, we propose SPACTOR (short for \u201cSpan corruption and Token replacement\"), a new pre-\ntraining procedure that significantly improves the efficiency and generalization of T5 models (Raffel\net al., 2020). SPACTOR consists of two ingredients. The first is an augmentation of the span corruption\n(SC) pre-training task with the replaced token detection (RTD) objective proposed in ELECTRA\n(Clark et al., 2020). The second is a two-staged pre-training schedule: after \u03c4 training steps on\nhybrid objectives, we continue pre-training only using the vanilla SC objective. The dual task in\nthe first stage is illustrated in Figure 1. Specifically, starting with a span-corrupted input text, an\nauxiliary generator G replaces a portion of the uncorrupted tokens with plausible tokens. The main\nT5 model (referred to as the discriminator D) is pre-trained to detect replaced tokens with its encoder\ncomponent. Simultaneously, using the same token-replaced input, the discriminator attempts to\ndenoise the SC masks with its decoder.\nFrom a quality standpoint, detecting replaced tokens enforces all token attention (Clark et al., 2020),\nleading to a better text representation. However, the generator G can also inadvertently introduce\nmisleading yet plausible context (albeit trained non-adversarially), resulting in a noisier training\nenvironment for discriminator decoder D.1 As we explain in more detail in Section 3, the advantages\nof RTD are predominantly observed in the initial stages of pre-training. As the training progresses\nhowever, these benefits are eventually overshadowed by the noise introduced to the discriminator\u2019s\nencoder. This phenomenon naturally motivates the two-staged training, which significantly boosts\nthe performance on various downstream tasks. Figure 2 shows examples of these improvements\nwhen \u03c4 equals 120K (1/8 of total iterations) and 250K (1/4 of total iterations) on the SuperGLUE\n(Wang et al., 2019a), SQuAD (Rajpurkar et al., 2016) and CNN/DailyMail (Hermann et al., 2015)\nbenchmarks. These and several other results are discussed in detail in Section 3 and Appendix C.\n1For example, if we have a corrupted sentence \"Last week we travelled to [M], the capital of [S0].\", where\n[M] is Tokyo and [S0] is Japan. The generator G can reasonably produce a different city for the mask [M],\nwhich consequently leads the discriminator to associate it with the capital of Japan due to the use of teacher\nforcing during training.\n2\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n73\n74\n75\n76\n77\n78\nAverage Score\nSuperGLUE\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(a) SuperGLUE\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n87.0\n87.5\n88.0\n88.5\n89.0\nAverage Score\nSQuAD\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(b) SQuAD\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n32.8\n32.9\n33.0\n33.1\n33.2\n33.3\n33.4\n33.5\nAverage Score\nCNN/DailyMail\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(c) CNN/DailyMail\nFigure 2: SPACTOR(\u03c4) performances on SuperGLUE, SQuAD and CNN/DailyMail with respect\nto pre-training FLOPs. Here, we include SPACTOR(250K) and SPACTOR(120K) where the second\npre-training stage (using the span corruption objective only) starts at 250K and 120K training steps\nrespectively. The plots for the remaining tasks are presented in Appendix C.\nFrom the perspective of efficiency, one major benefit of our design is that we do not increase the\ntarget length of the decoder. A naive extension of the ELECTRA approach to the encoder-decoder\narchitecture would require decoding the entire original input sequence rather than only the corrupted\nspans, which is undesirable since the complexity of the decoder\u2019s self-attention is O(L2) for a given\ntarget length L. The additional computational overhead of SPACTOR, on the contrary, mainly comes\nfrom the inference and back-propagation of the generator G (typically much smaller compared to\nthe discriminator D) and a light-weight binary classification head. The cost is only incurred during\nthe first \u03c4 training steps and gets amortized over the rest of the steps. Consequently, SPACTOR\nachieves a 50% reduction in training iterations and a 40% reduction in FLOPs while maintaining task\nperformance, as presented in detail in Section 3.\nThe main contribution of the papers are:\n1. We propose a novel combination of RTD and SC, thus extending ELECTRA to encoder-\ndecoder architecture.\n2. We analyze extensively the interactions between the two objectives, and establish a two-stage\npre-training schedule.\n3. We show that SPACTOR scales well as model size increases, and offers around 40% savings\nin total pre-training compute.\n2\nSPACTOR Method\nIn this section, we first describe in detail the pre-training objective of SPACTOR highlighted in Figure\n1; after that we describe the methodology of two-stage pre-training.\n2.1\nThe Hybrid Pre-training Objective\nGiven an input text composed of a sequence of tokens X = {x0, x1, ..., xN\u22121}, we introduce two\ntypes of masks and apply them sequentially:\nSC masks (Raffel et al., 2020).\nLet Xi,j be the set of consecutive tokens Xi,j\n=\n{xi, xi+1, ..., xj\u22121, xj}. SC selects p disjoint spans Sp = {Xik,jk}p\u22121\nk=0 uniformly at random, with\naverage span length \u00b5 = 3. Each Xik,jk is then replaced with a single sentinel token [Sk]:\n{x0, ..., Xi0,j0, ..., Xik,jk, ..., xN\u22121} \u2212\u2192\n{x0, ..., [S0], ..., [Sk], ..., xN\u22121} .\n(1)\nFor convenience, we denote Xc to be the right hand side of Equation 1.\n3\nMLM masks. For the rest of the tokens Xc \\ {[Sk]}, we continue token level masking by selecting q\nadditional tokens Mq = {xum}q\u22121\nm=0 uniformly at random and replace them with mask [M]:\n{x0, ..., xu0, ..., [Sk], ..., xum, ..., xN\u22121} \u2212\u2192\n{x0, ..., [M], ..., [Sk], ..., [M], ..., xN\u22121} .\n(2)\nWe denote the final corrupted sentence with both masks as XMLM\nc\n.\nNote that we apply MLM masks after SC, to utilize well-established SC algorithm and distributions.\nMLM masks, being at token level, can also be inserted avoiding SC masks naturally.\nThe inputs are now passed to a generator G and a discriminator D. G and D share the same token\nembedder (Clark et al., 2020) and are jointly trained.\nGenerator G. The backbone of G is a bidirectional transformer encoder, mapping each token\nin XMLM\nc\nto contextualized vector representations HG\nd\u00d7n = {hG\n0 , hG\n1 , ..., hG\nn\u22121} where hG\n\u2113 , \u2113 =\n0, ..., n \u2212 1 is a d-dimensional column vector and n = N \u2212 p(\u00b5 \u2212 1) is the length of XMLM\nc\n. We add\na linear projection layer WG\nv\u00d7d that mapping hG\n\u2113 to the v-dimensional embedding space of vocabulary.\nFinally, a softmax is taken to calculate the probabilities of output tokens:\npG\n\u0000x\u2113| XMLM\nc\n\u0001\n= softmax\n\u0000W \u00b7 hG\n\u2113\n\u0001\n,\n(3)\nThe loss function for G is\nLG = E\n X\n\u2113\n\u2212 log pG\n\u0000x\u2113| XMLM\nc\n\u0001\n!\n(4)\nDiscriminator D. D is a T5 model. The encoder input of D is generated by sampling from\ncategorical distribution pG and replacing each [M] in XMLM\nc\nwith plausible token bx. We refer to the\nresulting text as c\nXc, which is used as the encoder input of D.\nThe encoder output of D\u2019, HD\nd\u00d7n = {hD\n0 , hD\n1 , ..., hD\nn\u22121}, is fed into an MLP layer f followed by\nsigmoid to determine whether the given token is the same as the ground truth or is replaced:\npRTD\nD\n(bx\u2113) = exp(f(hD\n\u2113 ))/\n\u0002\n1 + exp(f(hD\n\u2113 ))\n\u0003\n.\n(5)\nThe corresponding loss for RTD is\nLRTD\nD\n= E\n\"n\u22121\nX\n\u2113=0\n\u2212I(bx\u2113 = x\u2113) log pRTD\nD\n(bx\u2113) \u2212 I(bx\u2113 \u0338= x\u2113) log(1 \u2212 pRTD\nD\n(bx\u2113))\n#\n(6)\nOn the other hand, the decoder of D is trained to find the actual tokens behind the SC masks [Sk],\ntaking into account the embedding HD\nd\u00d7n. As in Raffel et al. (2020), we formulate the decoder target\nas the concatenation of SC masks and the ground truth tokens:\nT := [S0] Xi0,j0 ... [S(p \u2212 1)] Xip\u22121,jp\u22121 [EOS].\n(7)\nThis gives the following loss,\nLSC\nD = E\n\"p\u00b5+p+1\nX\ni=1\n\u2212 log pSC\nD\n\u0010\nTi | Ti\u22121, ..., T0; c\nXc\n\u0011#\n.\n(8)\nThe final loss of training is the weighted sum of three terms:\nL = LG + \u03bb1LRTD\nD\n+ \u03bb2LSC\nD ,\n\u03bb1,2 \u2265 0.\n(9)\n2.2\nTwo-staged Pre-training\nAs described in Section 1 and elaborated in Section 3.2.1 below, the existence of MLM masks, plus\nthe imperfection of the generator G itself may provide misleading context c\nXc which obstructs training\nfrom SC. We therefore introduce a one-parameter generalization that after training hybrid objective\nwith \u03c4 iterations, only the discriminator D and shared token embedder are retained, and continue the\nrest of the pre-training with vanilla SC objective.\n4\n3\nExperiments\nIn this section, we begin by describing our experimental setup. To emphasize the stage transition \u03c4\nand the discriminator size M, we explicitly write SPACTORM(\u03c4) in the remaining of the paper. At\ntwo extremes, when \u03c4 = 0 (resp. \u03c4 = \u221e), we train with the SC objective (resp. the hybrid objective)\nexclusively. We then show that the performance gain of SPACTORBase(\u221e) is not sustainable (Section\n3.2.1), and a two-staged pre-training is the natural remedy (Section 3.2.2). With the knowledge\ngained from the Base model up to Section 3.2.3, we extend the experiment to the Large model in\nSection 3.2.4.\n3.1\nSetup\nPre-training procedures. We closely follow the convention of the original T5 paper (Raffel et al.,\n2020), and focus on the T5.1.0 model throughout our experiments. The model is pre-trained on\nColossal Clean Crawled Corpus (C4), a massive English-only web extracted text set. We use the\nSentencePiece tokenizer with 32,000 tokens for preprocessing the raw text corpus, and the Adafactor\noptimizer (Shazeer and Stern, 2018) for model parameter training. Details of the pre-training\nhyper-parameters and their tuning are discussed in Table 5 of Appendix A.1.\nFine-tuning procedure. The weights of the pre-trained discriminator D and the token embedder\nare used to initialize fine-tuning. In accordance with standard practice, we use a constant learning\nrate and train over a sufficiently large number of iterations to ensure that the validation metrics have\nconverged. More details of the fine-tuning hyperparameters can be found in Appendix A.2.\nEvaluation. We use the T5.1.0 model pre-trained with span corruption only (Raffel et al., 2020) as\nbaseline. Table 1 gives a list of representative natural language tasks we evaluate in this paper. For\ntasks having multiple sub-tasks, we treat them independently, and select the best checkpoint based on\nthe maximal value of the average of the corresponding set of metrics. For FLAN instruction-tuning\nin particular, we focus on the benchmark comprised of 27 tasks from BIG-Bench (BBH) (Srivastava\net al., 2022) and 57 tasks from Massive Multitask Language Understanding (MMLU) (Hendrycks\net al., 2021) with direct answers. Here we do not include benchmarks with Chain-of-Thought (Wei\net al., 2022b) as reasoning is an emergent capability of larger models beyond O(10B) scale. We\ncompare the fine-tuning results without using LM adaptation (Lester et al., 2021) to directly reflect\nquality gains. We also exclude tasks involving multilinguality such as WMT translation (e.g., see\nBarrault et al. (2020)), because those tasks are more suitable for mT5 models (Xue et al., 2020).\nTask\nDescription\nNo. Datasets\nReference\nGLUE\nGeneral Language Understanding\n7\nWang et al. (2019b)\nSuperGLUE\nGeneral Language Understanding\n8\nWang et al. (2019a)\nSQuAD\nQA (context)\n1\nRajpurkar et al. (2016)\nCNN/DailyMail\nNews Summarization\n1\nHermann et al. (2015)\nRainbow\nCommonsense Reasoning\n6\nLourie et al. (2021)\nFLAN\nInstruction-tuning\n6\nChung et al. (2022)\nTable 1: List of natural language tasks for fine-tuning.\n3.2\nResults\nWe now present the main experimental results for SPACTORBase(\u03c4) and SPACTORLarge(\u03c4). For the\nformer, we compare \u03c4 = \u221e and \u03c4 < \u221e and emphasize the importance of training stage transition.\nWe also analyze the quantitative gains from both generalizability and efficiency perspective.\n3.2.1\nSingle stage pre-training\nAs motivated in Section 1, jointly pre-training on SC and RTD can be a double-edged sword. This\nis reflected in Figure 3 where we plot the continuous fine-tuning results for SPACTORBase(\u221e) up to\n1M steps. While the added RTD objective enhances performance in the early iterations, the gains\nvanish after around 250K pre-training steps and the model eventually under-performs compared to\nthe baseline.\n5\n0.2\n0.4\n0.6\n0.8\n1.0\npre-training step\n1e6\n73\n74\n75\n76\n77\n78\n79\naverage score\nSuperGLUE\nBaseline\nSpacTor(\n)\n(a) SuperGLUE\n0.2\n0.4\n0.6\n0.8\n1.0\npre-training step\n1e6\n86.5\n87.0\n87.5\n88.0\n88.5\n89.0\naverage score\nSQuAD\nBaseline\nSpacTor(\n)\n(b) SQuAD\nFigure 3: Average score on downstream tasks (y-axis) when continuously fine-tuning along the\npre-training checkpoints (x-axis). The error band illustrates the min-max range over 5 independent\nruns.\nTo gain more insights, we compare validation loss LSC\nD against baseline, when the encoder inputs are\nthe original context Xc or the noisy context c\nXc respectively in Figure 4a. When noisy input c\nXc is\nconsumed, the loss is noticeably inferior compared to using Xc, an indication that replaced tokens in\nfact hurts the validation score of SC.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npre-training step\n1e6\n22000\n24000\n26000\n28000\n30000\n32000\n34000\n36000\n38000\nSpan Corruption Validation Loss\nbaseline\nSpacTor(\n): encoder input Xc\nSpacTor(\n): encoder input Xc\n(a)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\npre-training step\n1e6\n17.0\n16.5\n16.0\n15.5\n15.0\n14.5\n14.0\n13.5\n13.0\n12.5\nCross Entropy Diff (\u00d71e2)\nLoss diff: encoder input Xc\nLinear Regression of the Diff\n(b)\nFigure 4: (Left) Validation loss curve for baseline and SPACTOR(\u221e). (Right) Validation cross-\nentropy loss differences between baseline and SPACTOR(\u221e) evaluated with encoder input Xc. The\ndashed line is the linear regression fits to the data starting at iteration 120K.\nIn Figure 4b, we subtract SPACTORBase(\u221e)\u2019s validation cross entropy against baseline. The gap\nin loss reduces initially, as the generator G produces more correct tokens. An inflection occurs at\naround 200K pre-training steps, after that a reverse trend is developed. The trend is statistically\nsignificant, based on the hypothesis test carried out in Appendix B. This implies the discriminator\nD\u2019s performance on the SC objective is diverging further away from baseline, suggesting that the\ntraining is bottlenecked by noise in the input context c\nXc. The inflection point approximately occurs at\nthe same time as the one happened in Figure 3 \u2014 a qualitative confirmation that downstream metric\ndecay can be attributed to the degradation of span corruption performance during pre-training.\n6\nWe conjecture that RTD helps in early training iterations because discriminator D is still weak, and\ncorrelations of input and target tokens are not yet properly established. Therefore, noise in G does\nnot matter too much. Meanwhile, all token attention enforced by RTD greatly aids the model to\nmaximize the usage of input context, hence boosting the downstream metrics.\n3.2.2\nWith continued pre-training\n\u03c4\nCkpt. FLOPs\nGLUE\nSuperGLUE\nSQuAD\nCNNDM\nRainbow\nMMLU\nBBH\n0\n500K\n1.0\n85.89 \u00b1 0.17 77.33 \u00b1 0.74 88.59 \u00b1 0.05 33.27 \u00b1 0.12 70.14 \u00b1 0.25 50.20 \u00b1 1.47 36.82 \u00b1 0.55\n250K 500K\n1.2\n86.46 \u00b1 0.17 78.26 \u00b1 0.63 88.91 \u00b1 0.11 33.34 \u00b1 0.10 71.60 \u00b1 0.18 51.15 \u00b1 0.80 37.30 \u00b1 0.33\n120K 500K\n1.1\n86.35 \u00b1 0.13 78.23 \u00b1 0.81 88.93 \u00b1 0.11 33.37 \u00b1 0.10 71.34 \u00b1 0.23 51.01 \u00b1 0.40 36.97 \u00b1 0.31\n60K\n500K\n1.05\n86.28 \u00b1 0.22 78.50 \u00b1 0.56 88.95 \u00b1 0.14 33.27 \u00b1 0.08 71.35 \u00b1 0.16 50.67 \u00b1 1.02 36.72 \u00b1 0.30\n0\n1M\n2.0\n86.11 \u00b1 0.17 78.14 \u00b1 0.80 88.90 \u00b1 0.23 33.34 \u00b1 0.10 71.00 \u00b1 0.20 52.79 \u00b1 0.95 37.57 \u00b1 0.77\n250K\n1M\n2.2\n86.48 \u00b1 0.29 78.33 \u00b1 0.76 89.09 \u00b1 0.12 33.47 \u00b1 0.07 72.27 \u00b1 0.29 52.96 \u00b1 0.61 38.18 \u00b1 0.84\n120K\n1M\n2.1\n86.57 \u00b1 0.35 78.16 \u00b1 0.76 88.99 \u00b1 0.14 33.53 \u00b1 0.09 72.14 \u00b1 0.25 52.81 \u00b1 0.57 38.08 \u00b1 0.65\nTable 2: Average score of each downstream tasks for SPACTORBase(\u03c4). When \u03c4 = 0 it becomes the\nbaseline. We present both the mean value and standard deviation across five independent runs. We\nbold numbers for SPACTORBase(\u03c4) with a higher mean than baseline at the same pre-training steps.\nIn the third column, we add the normalized FLOPs where baseline-500K checkpoint is normalized to\nbe 1.0. Details are presented in Section 3.2.3.\nNow we discuss \u03c4 < \u221e. In practice, based on Figure 3 and Figure 4b we compare cases with \u03c4 to be\n60K, 120K or 250K.\nIn Table 2, we summarize the downstream task metrics for baseline and SPACTORBase(\u03c4) fine-tuned\nat 500K / 1M checkpoints. The results show that at 500K checkpoint, SPACTORBase(\u03c4) consistently\noutperforms the baseline by a large margin. For \u03c4 = 250K as an example, the gain is at least one\nstandard deviation, and can reach as large as 3\u03c3 on tasks like GLUE and SQuAD. Except MMLU\nand BBH, SPACTORBase(\u03c4) with only half of the pre-training iterations achieves similar or even\nbetter downstream performances than baseline. When training to 1M, SPACTORBase(\u03c4) retains its\nsuperiority over baseline, even though 75% of the steps are trained with SC only. This implies that\nthe two-staged pre-training, indeed, fixes the decay in performance shown in Figure 3.\nInterestingly, comparing the fine-tuning results at the 500K checkpoint when \u03c4 equals 250K, 120K\nand 60K, we see there is no obvious difference on tasks such as SuperGLUE and SQuAD. For others,\nreducing \u03c4 from 250K to 60K we see a significant drop in the metrics, some of which become even\non par with the baseline. This indicates that 60K iterations is perhaps too early for the transition\nto the second stage of pre-training. For that reason, we do not evaluate SPACTORBase(60K) at 1M\niterations anymore.\nThe breakdown of individual subtasks and their evaluation metrics are described in Appendix D.\n3.2.3\nEfficiency analysis\nComparing downstream tasks at the same number of iterations (i.e. Table 2) is not entirely indicative\nof training efficiency as SPACTORBase(\u03c4) requires more floating point operations (FLOPs) per step\nin the first \u03c4 iterations. Nonetheless, as the analysis in this section shows, SPACTOR achieves a net\nincrease in performance as a function of overall compute cost.\nWe compare the actual compute cost using two approaches. In the first approach, we read sequences\nper second metric using the T5X library (Roberts et al., 2022), a direct reflection of wall clock\ntime. We normalize the value against the baseline to avoid hardware-dependent specifics. In the\nsecond approach, we calculate FLOPs per iteration, a hardware independent quantity. As summarized\nin Table 3, we find that pre-training on SPACTORBase(\u03c4) during the first stage incurs about 37.5%\nmore FLOPs at each iteration than the baseline, which approximately matches the relative value of\nsequence per second.\n7\nExperiment\nSeqs / second\nFLOPs / step\nBaseline\n1.0\n1.6 \u00d7 104 GFLOPs\nSPACTORBase(\u03c4) (1st stage)\n0.7\n2.2 \u00d7 104 GFLOPs\nTable 3: Efficiency analysis of baseline and SPACTORBase(\u03c4) in the first stage (The second stage\ncompute is identical to the baseline). Seqs / second is normalized using the baseline value.\nIn the second column of Table 2, we added the relative FLOPs of each method at a fixed iteration. For\nexample, SPACTORBase(250K) has an overall normalized FLOPs of 0.5 \u00d7 1.375 + 0.5 \u00d7 1.0 \u2248 1.2\nafter 500K iterations. For majority of the benchmarks, the 500K checkpoint is matching or beating\nthe baseline 1M ones with a normalized FLOPs of 2.0. This represent an overall efficiency gain of at\nleast 40%. It is also worth noting that, as the length of the second stage training grows relative to the\nfirst stage, the extra cost of SPACTORBase(\u03c4) is reduced. For example, at 1M iterations the number\nshrinks to 2.2/2 = 1.1.\nTo better illustrate performance as a function of compute, Figure 2 plots average score of SuperGLUE,\nSQuAD and CNN/DailyMail with respect to FLOPs. Not only do we see that SPACTORBase(\u03c4)\nachieves the same average score as baseline-1M with 40% less compute, but that is also outperforms\nbaseline across the majority of compute budgets. In Appendix C we include similar plot for the\nremaining tasks.\n3.2.4\nLarge models\nWe now scale up SPACTOR to T5-Large model (Raffel et al., 2020) of around 700M parameters.\nWe pick transition parameter \u03c4 = 120K and MLM ratio to be 20%, due to the proportional size\nincrease of the generator G. Other hyperparameters such as coefficients \u03bb1,2 (Equation 9) and SC\nconfigurations have stayed the same as before.\n\u03c4\nCkpt. FLOPs\nGLUE\nSuperGLUE\nSQuAD\nCNNDM\nRainbow\nMMLU\nBBH\n0\n500K\n1.0\n88.92 \u00b1 0.27 85.10 \u00b1 0.43 91.30 \u00b1 0.10 34.14 \u00b1 0.02 81.48 \u00b1 0.22 55.59 \u00b1 0.84 40.30 \u00b1 0.30\n120K 500K\n1.06\n89.66 \u00b1 0.19 86.06 \u00b1 0.47 91.36 \u00b1 0.10 34.22 \u00b1 0.18 82.68 \u00b1 0.23 57.78 \u00b1 1.01 42.07 \u00b1 1.44\n0\n1M\n2.0\n89.24 \u00b1 0.17 86.11 \u00b1 0.76 91.52 \u00b1 0.04 34.24 \u00b1 0.08 82.97 \u00b1 0.20 58.72 \u00b1 0.61 42.35 \u00b1 0.72\n120K\n1M\n2.06\n89.90 \u00b1 0.26 86.38 \u00b1 0.80 91.53 \u00b1 0.13 34.27 \u00b1 0.26 83.92 \u00b1 0.32 59.06 \u00b1 0.90 44.22 \u00b1 1.52\nTable 4: Average score of each downstream tasks for SPACTORLarge(\u03c4). \u03c4 = 0 corresponds to the\nbaseline. The mean value and standard deviation across three independent runs. We bold numbers for\nSPACTORLarge(\u03c4) with a higher mean than baseline at the same pre-training steps.\nTable 4 lists fine-tuning results for the same set of benchmarks as Base model. Because of the\nchoice of generator G, the extra compute budget at 500K and 1M checkpoints is now 6% and 3%\nrespectively. Just like previous experiments, we see that SPACTORLarge(\u03c4) consistently outperforms\nthe baseline with a significant margin, measured by standard deviation. For GLUE, SuperGLUE\nand CNN/DailyMail, the 500K checkpoint of SPACTORLarge leads to better or equal downstream\nmetrics compared to 1M checkpoint of baseline, while the rest of the tasks, the former is behind the\nlatter, but the difference is within 1\u03c3. This results in an overall compute saving of 35%. We conclude\nthat SPACTOR method scales well as model size grows, probably because RTD provides purely\ncomplementary information on top of vanilla SC training objective. The breakdown of individual\ntask is given in Appendix D.\n4\nRelated Work\nDai and Le (2015); Ramachandran et al. (2017) introduced language modeling with in-domain data\nto pre-train RNN sequence models. With the invention of transformer architecture (Vaswani et al.,\n8\n2017), pre-training has become a standard paradigm to scale language models beyond O(100B)\nparameters, which often leads to strong performance on natural language tasks.\nAssorted pre-training objectives have been studied in the literature, among which the most popular\nones are causal language modeling (CLM) (Radford et al., 2018, 2019), prefix language modeling\n(PLM) (Liu et al., 2018; Raffel et al., 2020), masked language modeling (MLM) (Devlin et al., 2019).\nIt has been understood that different pre-training objectives correlate with performance in different\ndownstream tasks (Wang et al., 2022); therefore, one naturally curates a mixtures of these objectives\n(Dong et al., 2019; Tay et al., 2022a) such that the pre-trained LLM may inherit strength from them\nall.\nSubsequent work also attempts to improve individual objectives. For MLM as an example, Joshi et al.\n(2020) introduced SPANBERT, which masks contiguous tokens and uses span boundary to assist\nprediction. Inspired by that, Raffel et al. (2020); Lewis et al. (2020) considered a denoising objective\nwhere contiguous tokens are replaced with a single mask token, and showed that it achieves the best\nperformances among other denoising options for encoder-decoder models.\nThe drawback of plain MLM, as well as other variants, is that not all tokens need to be attended to\nin order to figure out the ground truth. The existence of mask token [M] also creates misalignment\nbetween pre-train and downstream tasks. ELECTRA (Clark et al., 2020) rectifies those issues by\njointly training a generator model that fills masked positions with plausible tokens, while the main\nmodel learning to detect which tokens have been replaced (i.e. the RTD loss). The authors showed\nthat ELECTRA significantly reduces the computing cost compared to other larger networks such as\nGPT (Radford et al., 2018) and XLNet (Yang et al., 2019). Further extensions of ELECTRA can be\nfound in Meng et al. (2021, 2022); He et al. (2021); Bajaj et al. (2022).\nBesides its success in BERT models, few works have attempted ELECTRA in T5. This is partially\nbecause RTD by itself is discriminative rather than generative in nature. As described in Section\n2, instead of replacing SC with RTD, we combine them to form a hybrid of pre-training objectives.\nThe hybrid objective is evaluated on each individual input, where RTD learns a text representation\nwhile SC learns token generation. A closely related work that explored hybrid objective is PEGASUS\n(Zhang et al., 2020); We emphasize our difference from PEGASUS in the following aspects: (i)\nPEGASUS de-noises MLM in the encoder. For encoder component, RTD usually brings more benefit\ndue to all token attention (Clark et al., 2020); in addition, leaving MLM mask [M] as model input\nhurts SC more, because token replacement can generate at least a proportion of context correctly; (ii)\nPEGASUS focuses exclusively on text summarization tasks.\nFinally, there has been research on continued pre-training in LLMs, with focus on model adaptation:\neither adapting data (Gururangan et al., 2020), or adapting the training objective (Wang et al., 2022)\ntowards downstream tasks. The continued pre-training used in this paper is neither of the above two\nscenarios, rather it is more akin to curriculum type of training (Bengio et al., 2009; Braun et al.,\n2017): the difficulty of the objective changes as training progresses.\n5\nConclusion and Future Work\nIn this paper, we construct a novel combination of pre-training objectives: span corruption (SC)\n(Raffel et al., 2020) and replaced token detection (RTD) (Clark et al., 2020), which enables the\nlanguage model to learn from two signals simultaneously for every single input.\nIn Section 1 and 3, we argue empirically that RTD and SC cannot be co-trained for long durations\nsince the downstream task performance would deteriorates sharply as pre-training progresses. It is\nthen natural to propose a two-staged pre-training recipe, where after \u03c4 iterations we continue training\nwith SC alone. We show that this approach is highly effective, where the model is able to reach the\nsame performance as baseline with significantly less compute, while outperforming baseline given the\nsame compute budget. Our observation also indicates that high quality data is critical for preserving\nand improving language abilities in later iterations.\nThere are a few limitations in the current scope of the paper. First, one may wonder whether a\ncontinuous pre-training curriculum exists. For example, smoothly varying the \u03bb1, \u03bb2 parameters,\nor MLM masking ratio. Secondly, our results are restricted to encoder-decoder architecture. It is\ninteresting to extend the work to other architectures, and explore the scaling behavior along the lines\nof Wei et al. (2022a); Tay et al. (2022b). We plan to leave those for future work.\n9\nReferences\nAribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H. S., Mehta, S. V., Zhuang, H., Tran, V. Q., Bahri,\nD., Ni, J., et al. (2021). Ext5: Towards extreme multi-task scaling for transfer learning. In\nInternational Conference on Learning Representations.\nBajaj, P., Xiong, C., Ke, G., Liu, X., He, D., Tiwary, S., Liu, T.-Y., Bennett, P., Song, X., and Gao,\nJ. (2022). Metro: Efficient denoising pretraining of large scale autoencoding language models\nwith model generated signals. arXiv preprint arXiv:2204.06644.\nBarrault, L., Biesialska, M., Bojar, O., Costa-juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz,\nR., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C.-k., Ljube\u0161i\u00b4c, N., Monz, C.,\nMorishita, M., Nagata, M., Nakazawa, T., Pal, S., Post, M., and Zampieri, M. (2020). Findings of\nthe 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference\non Machine Translation, pages 1\u201355, Online. Association for Computational Linguistics.\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings\nof the 26th annual international conference on machine learning, pages 41\u201348.\nBraun, S., Neil, D., and Liu, S.-C. (2017). A curriculum learning method for improved noise robust-\nness in automatic speech recognition. In 2017 25th European Signal Processing Conference\n(EUSIPCO), pages 548\u2013552. IEEE.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. In NeurIPS.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\nH. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A.,\nBarnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R.,\nBradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,\nDev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D.,\nLuan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick,\nM., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O.,\nLee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern,\nK., Eck, D., Dean, J., Petrov, S., and Fiedel, N. (2022). Palm: Scaling language modeling with\npathways. CoRR, abs/2204.02311.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani,\nM., Brahma, S., et al. (2022). Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416.\nClark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. (2020). ELECTRA: Pre-training text encoders\nas discriminators rather than generators. In ICLR.\nDai, A. M. and Le, Q. V. (2015). Semi-supervised sequence learning. In Cortes, C., Lawrence, N.,\nLee, D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing\nSystems, volume 28. Curran Associates, Inc.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H.-W.\n(2019). Unified language model pre-training for natural language understanding and generation.\nAdvances in Neural Information Processing Systems, 32.\nGururangan, S., Marasovi\u00b4c, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A.\n(2020). Don\u2019t stop pretraining: Adapt language models to domains and tasks. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pages 8342\u20138360,\nOnline. Association for Computational Linguistics.\nHe, P., Liu, X., Gao, J., and Chen, W. (2021). Deberta: Decoding-enhanced bert with disentangled\nattention. In International Conference on Learning Representations.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021).\nMeasuring massive multitask language understanding. In International Conference on Learning\nRepresentations.\n10\nHendrycks, D. and Gimpel, K. (2016).\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415.\nHermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P.\n(2015). Teaching machines to read and comprehend. Advances in neural information processing\nsystems, 28.\nJoshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. (2020). Spanbert: Im-\nproving pre-training by representing and predicting spans. Transactions of the Association for\nComputational Linguistics, 8:64\u201377.\nLester, B., Al-Rfou, R., and Constant, N. (2021). The power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and\nZettlemoyer, L. (2020). BART: Denoising sequence-to-sequence pre-training for natural lan-\nguage generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for\nComputational Linguistics.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. (2018). Gen-\nerating wikipedia by summarizing long sequences. In International Conference on Learning\nRepresentations.\nLourie, N., Le Bras, R., Bhagavatula, C., and Choi, Y. (2021). Unicorn on rainbow: A universal\ncommonsense reasoning model on a new multitask benchmark. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 35, pages 13480\u201313488.\nMeng, Y., Xiong, C., Bajaj, P., Tiwary, S., Bennett, P., Han, J., and Song, X. (2021). COCO-LM:\nCorrecting and contrasting text sequences for language model pretraining. In Conference on\nNeural Information Processing Systems.\nMeng, Y., Xiong, C., Bajaj, P., Tiwary, S., Bennett, P., Han, J., and Song, X. (2022). Pretraining text\nencoders with adversarial mixture of training signal generators. In International Conference on\nLearning Representations.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improving language under-\nstanding by generative pre-training. OpenAI blog.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.\n(2020). Exploring the limits of transfer learning with a unified text-to-text transformer. In JMLR.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). Squad: 100, 000+ questions for machine\ncomprehension of text. In EMNLP.\nRamachandran, P., Liu, P., and Le, Q. (2017). Unsupervised pretraining for sequence to sequence\nlearning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pages 383\u2013391, Copenhagen, Denmark. Association for Computational Linguistics.\nRoberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B.,\nGaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin,\nJ., Goodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian,\nJ., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp,\nJ., Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N.,\nOmernick, M., Saeta, B., Sepassi, R., Spiridonov, A., Newlan, J., and Gesmundo, A. (2022).\nScaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.\nSanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja,\nA., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani,\nG., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong,\nZ. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry,\nT., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf, T., and Rush, A. M. (2022).\nMultitask prompted training enables zero-shot task generalization. In International Conference\non Learning Representations.\n11\nShazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202.\nShazeer, N. and Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost. In\nDy, J. and Krause, A., editors, Proceedings of the 35th International Conference on Machine\nLearning, volume 80 of Proceedings of Machine Learning Research, pages 4596\u20134604. PMLR.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro,\nA., Gupta, A., Garriga-Alonso, A., et al. (2022). Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D., Schuster, T., Zheng, H. S., Houlsby, N., and\nMetzler, D. (2022a). Unifying language learning paradigms. arXiv preprint arXiv:2205.05131.\nTay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao,\nJ., Chowdhery, A., et al. (2022b). Transcending scaling laws with 0.1% extra compute. arXiv\npreprint arXiv:2210.11399.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and\nPolosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio,\nS., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.\n(2019a). Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems, 32.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2019b). GLUE: A multi-\ntask benchmark and analysis platform for natural language understanding. In International\nConference on Learning Representations.\nWang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C.\n(2022). What language model architecture and pretraining objective work best for zero-shot\ngeneralization? arXiv preprint arXiv:2204.05832.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V.\n(2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou,\nD., Metzler, D., et al. (2022a). Emergent abilities of large language models. Transactions on\nMachine Learning Research.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022b). Chain of\nthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C.\n(2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V. (2019). Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in Neural\nInformation Processing Systems, volume 32. Curran Associates, Inc.\nZhang, J., Zhao, Y., Saleh, M., and Liu, P. (2020). Pegasus: Pre-training with extracted gap-sentences\nfor abstractive summarization. In International Conference on Machine Learning, pages 11328\u2013\n11339. PMLR.\n12\nA\nTraining Hyperparameters\nIn this section, we summarize more details of hyperparameter choices for both pre-training and\nfine-tuning. We only tune those parameters for SPACTORBase(\u03c4) and then choose most of the optimal\nparameters for SPACTORLarge(\u03c4) experiments.\nA.1\nPre-training Hyperparameters\nTo select hyperparameters for T5-Base model, we run SPACTORBase(\u221e) with batch size 2048 to\n250K steps, and then fine-tune the final checkpoints on a subset of downstream tasks (SuperGLUE,\nSQuAD) and select a set of reasonable values based on validation scores. For coefficients \u03bb1,2 in loss\nfunction, i.e. Equation 9, we apply a simple grid search such that \u03bb1,2 \u2208 [1.0, 10.0, 20.0, 50.0]. For\nthe additional token level masking ratio, we experiment with rMLM = [5%, 10%, 15%, 20%, 25%]\nand find that a masking ratio of 15% works the best. Indeed, a ratio that is too small would result in\ngenerator G producing few different tokens from the initial input; while a ratio that is too large leads\nto an overly-corrupted input for the discriminator D, further affecting D from training SC properly.\nWe also experiment with different generator architecture and sizes, in particular, selecting from\nencoder-only or encoder-decoder architecture. It is found that an encoder-only architecture suffices\nand there is no quality degradation using a linear projection layer mapping encoder output to the\nprobability distribution of tokens in the vocabulary. We also compare final downstream performances\nwhen G is a 3-layer, 4-layer, 6-layer model. Same as Clark et al. (2020), when G is around 1/4 - 1/3\nthe size of the encoder of D, the result is optimal.\nThe hyperparameter set is then fixed throughout the remainder of the empirical evaluation across all\ncheckpoints and benchmarks. For T5-Large model, we re-use majority of the hyperparameters except\nscaling generator accordingly and increasing the MLM ratio from 15% to 20%.\nParameter\nT5-Base Value\nT5-Large Value\nDiscriminator Layers\n12\n24\nDiscriminator Num Heads\n12\n16\nDiscriminator Hidden Dimension\n768\n1024\nDiscriminator MLP Size\n3072\n4096\nRTD Head MLP Size\n3072\n4096\nRTD Head MLP Activation\nGELU\nGELU\nGenerator Layers\n4\n6\nGenerator MLP Size\n1024\n2048\nInput Length\n512\n512\nBatch Size\n2048\n2048\nSpan Corruption\n(r = 15%, \u00b5 = 3.0)\n(r = 15%, \u00b5 = 3.0)\nMLM Ratio\n15%\n20%\nWarmup Steps\n\u03ba = 10, 000\n\u03ba = 10, 000\nLearning Rate Schedule\n1.0/\np\nmax(n, \u03ba)\n1.0/\np\nmax(n, \u03ba)\n(\u03bb1, \u03bb2)\n(10.0, 10.0)\n(10.0, 10.0)\nTable 5: Model architecture and pre-training hyperparameters for SPACTOR. The RTD head uses the\nGELU activation initially proposed in (Hendrycks and Gimpel, 2016; Shazeer, 2020).\nA.2\nFine-tuning Hyperparameters\nFor all the tasks except FLAN instruction-tuning, we fix a constant learning rate 1e-3, dropout rate\n0.1, and batch size 128. For FLAN, we use constant learning rate 5e-4, dropout rate 0.05 and batch\nsize 64, following Chung et al. (2022). For the latter we also reset optimizer states since the data\ndistribution is very different from pre-training corpus. We fine-tune for sufficiently long iterations,\ntypically 250K - 300K, to ensure convergence.\n13\nExisting literature sometimes sweeps over a set of fine-tuning hyperparameters to get the optimal\nperformance (for example, see Aribandi et al. (2021)). In our experiment, we found that the relative\nimprovement stays the same regardless of finetuning hyperparameters that we searched, so we believe\na fixed parameter set suffices to compare baseline against SPACTOR.\nB\nStatistical Analysis of Validation Metric\nIn Figure 4b we plot the validation loss difference between SPACTORBase(\u221e) evaluated with vanilla\nSC input text Xc or noisy input text c\nXc generated from G. We now perform linear regression and\nhypothesis testing and argue that the trend is significant.\nWe use simple linear regression where the y variable is the cross-entropy difference, and the x variable\nis training steps starting from 120K. We do not include data before 120K step because they have not\nreached the plateau. Let\ny = \u03b20 \u00b7 x + \u03b21\n(10)\nto be the regression line. The hypothesis testing is\nH0 : \u03b20 = 0,\nHA : \u03b20 \u0338= 0.\n(11)\nThe regression results give \u03b20 = \u22126.90 \u00d7 10\u22125 with p-value 0.01. We therefore conclude that the\nvalidation difference is trending with a high confidence to reject null hypothesis H0.\nC\nAverage Score v.s. Pre-training FLOPs\nIn this section, we include the rest of average score versus pre-training FLOPs plots in Figure 5\nfor SPACTORBase models. While for MMLU the gain is smaller (around 20% FLOPs saving) and\nSPACTORBase gradually converge to baseline, the other tasks we still see substantial gains. On\nthe other hand, whether SPACTORBase(120K) is better than SPACTORBase(250K) is undetermined\nbecause the comparison is task dependent. This implies that the some tasks benefit more from longer\ntraining on the hybrid objective (Equation 9).\nD\nDetailed Eval Metrics\nIn this section, we give detailed metric breakdown of individual sub-tasks for SPACTORBase and\nSPACTORLarge, as a supplement to aggregated results of Table 2 and Table 4.\nSize\n\u03c4\nCkpt\nCoLA\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSST-2\nSTS-B\nBase\n0\n500K\n55.50\n88.22 / 88.11\n93.02 / 90.44\n92.99\n89.41 / 92.12\n81.95\n94.61\n90.05 / 89.91\n250K\n500K\n58.20\n88.35 / 88.20\n93.36 / 90.69\n93.10\n89.36 / 92.08\n83.75\n94.50\n90.34 / 90.09\n120K\n500K\n57.02\n88.34 / 88.19\n92.77 / 89.95\n93.04\n89.33 / 92.06\n82.67\n94.72\n90.59 / 90.40\n0\n1M\n54.73\n88.40 / 88.40\n92.96 / 90.20\n93.12\n89.44 / 92.15\n82.67\n94.84\n90.69 / 90.45\n250K\n1M\n55.98\n88.46 / 88.45\n93.17 / 90.69\n93.23\n89.41 / 92.09\n83.75\n94.72\n90.62 / 90.48\n120K\n1M\n58.06\n88.50 / 88.53\n93.33 / 90.69\n93.26\n89.41 / 92.12\n84.12\n94.72\n90.48 / 90.41\nTable 6: Breakdown of GLUE (Wang et al., 2019b) sub-tasks for SPACTORBase. Each score corre-\nsponds to the median of 5 independent runs. The metrics for each sub-task are: Matthews correlation\ncoefficient for CoLA, matched/mismatched accuracy for MNLI, F1/accuracy for MRPC, accuracy\nfor QNLI, F1/accuracy for QQP, accuracy for RTE, accuracy for SST-2, Pearson correlation coeffi-\ncient/Spearman correlation coefficient for STS-B.\n14\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\nAverage Score\nGLUE\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(a) GLUE\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n64\n66\n68\n70\n72\nAverage Score\nRainbow\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(b) Rainbow\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n35.0\n35.5\n36.0\n36.5\n37.0\n37.5\n38.0\n38.5\nAverage Score\nBBH\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(c) BBH\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\npre-training GFLOPs\n1e10\n47\n48\n49\n50\n51\n52\n53\nAverage Score\nMMLU\nBaseline\nSpacTor(250K)\nSpacTor(120K)\n(d) MMLU\nFigure 5: SPACTOR performances on GLUE, Rainbow, BBH and MMLU with respect to pre-training\nFLOPs for T5-Base model.\nSize\n\u03c4\nCkpt\nCoLA\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSST-2\nSTS-B\nLarge\n0\n500K\n64.14\n90.29 / 90.39\n93.66 / 91.18\n94.82\n90.13 / 92.62\n89.17\n95.76\n91.85 / 91.64\n120K\n500K\n68.28\n90.55 / 90.57\n94.33 / 92.16\n94.93\n90.06 / 92.63\n90.25\n96.22\n91.78 / 91.54\n0\n1M\n63.33\n90.80 / 90.94\n94.16 / 91.91\n95.04\n90.20 / 92.68\n90.61\n96.10\n91.83 / 91.72\n120K\n1M\n67.43\n90.93 / 90.95\n94.41 / 92.16\n95.20\n90.25 / 92.70\n91.34\n96.44\n92.17 / 92.00\nTable 7: Breakdown of GLUE (Wang et al., 2019b) sub-tasks for SPACTORLarge. Each score\ncorresponds to the median of 3 independent runs. The metrics for each sub-task are: Matthews\ncorrelation coefficient for CoLA, matched/mismatched accuracy for MNLI, F1/accuracy for MRPC,\naccuracy for QNLI, F1/accuracy for QQP, accuracy for RTE, accuracy for SST-2, Pearson correlation\ncoefficient/Spearman correlation coefficient for STS-B.\nSize\n\u03c4\nCkpt\nBoolQ\nCB\nCOPA\nMultiRC\nReCoRD\nRTE\nWiC\nWSC\nBase\n0\n500K\n81.99\n96.07 / 96.43\n70.00\n76.15 / 37.88\n77.65 / 78.53\n81.59\n68.97\n83.65\n250K\n500K\n82.32\n93.70 / 94.64\n73.00\n77.09 / 40.19\n77.65 / 78.55\n83.03\n69.44\n85.58\n120K\n500K\n82.72\n95.03 / 96.43\n74.00\n77.04 / 38.93\n77.92 / 78.92\n82.31\n70.22\n84.62\n0\n1M\n82.39\n97.36 / 96.43\n72.00\n77.10 / 39.66\n78.10 / 79.10\n83.03\n69.44\n86.54\n250K\n1M\n82.78\n91.89 / 94.64\n76.00\n77.63 / 41.03\n78.05 / 79.03\n83.03\n69.12\n85.58\n120K\n1M\n82.66\n95.04 / 94.64\n74.00\n77.94 / 41.76\n78.21 / 79.20\n82.67\n69.28\n82.69\nTable 8: Breakdown of SuperGLUE (Wang et al., 2019a) sub-tasks for SPACTORBase. Each score\ncorresponds to the median of 5 independent runs. The metrics for each sub-task are: accuracy for\nBoolQ, average F1/accuracy for CB, accuracy for COPA, F1/Exact Match (EM) for MultiRC, EM/F1\nfor ReCoRD, accuracy for RTE, accuracy for RTE, WiC and WSC.\n15\nSize\n\u03c4\nCkpt\nBoolQ\nCB\nCOPA\nMultiRC\nReCoRD\nRTE\nWiC\nWSC\nLarge\n0\n500K\n87.49\n95.59 / 98.21\n85.00\n83.97 / 52.93\n86.30 / 87.21\n89.89\n72.88\n94.23\n120K\n500K\n87.43\n100.00 / 100.00\n87.00\n84.33 / 54.98\n86.86 / 87.74\n91.94\n74.92\n93.27\n0\n1M\n87.92\n96.23 / 98.21\n90.00\n85.19 / 56.45\n87.31 / 88.17\n90.61\n74.92\n91.35\n120K\n1M\n87.80\n100.00 / 100.00\n87.00\n85.05 / 56.35\n87.43 / 88.29\n90.97\n73.98\n93.27\nTable 9: Breakdown of SuperGLUE (Wang et al., 2019a) sub-tasks for SPACTORLarge. Each score\ncorresponds to the median of 3 independent runs. The metrics for each sub-task are: accuracy for\nBoolQ, average F1/accuracy for CB, accuracy for COPA, F1/Exact Match (EM) for MultiRC, EM/F1\nfor ReCoRD, accuracy for RTE, accuracy for RTE, WiC and WSC.\nSize\n\u03c4\nCkpt\nSQuAD\nCNNDM\n\u03b1NLI CosmosQA HellaSWAG PIQA SocialIQA WinoGrande\nBase\n0\n500K 85.01 / 92.20 41.48 / 19.43 / 38.98 71.28\n74.51\n62.47\n76.28\n69.60\n66.85\n250K 500K 85.34 / 92.42 41.50 / 19.41 / 39.03 71.87\n75.58\n66.80\n76.93\n70.32\n67.96\n120K 500K 85.37 / 92.42 41.54 / 19.47 / 39.04 71.67\n75.08\n66.11\n77.15\n70.01\n68.03\n0\n1M\n85.37 / 92.34 41.51 / 19.48 / 39.02 71.34\n75.31\n64.44\n76.55\n70.83\n67.17\n250K\n1M\n85.63 / 92.57 41.69 / 19.55 / 39.17 72.26\n76.55\n67.19\n77.42\n70.78\n68.35\n120K\n1M\n85.54 / 92.58 41.76 / 19.60 / 39.24 72.39\n76.58\n67.30\n77.20\n71.29\n68.67\nTable 10: Breakdown of SQuAD (Rajpurkar et al., 2016), CNN/DailyMail (Hermann et al., 2015) and\nRainbow (Lourie et al., 2021) sub-tasks for SPACTORBase. Each score corresponds to the median of 5\nindependent runs. The metrics for each sub-task are: EM/F1 for SQuAD, Rouge-1/Rouge-2/Rouge-L\nfor CNN/DailyMail, and accuracy for all the Rainbow tasks.\nSize\n\u03c4\nCkpt\nSQuAD\nCNNDM\n\u03b1NLI CosmosQA HellaSWAG PIQA SocialIQA WinoGrande\nLarge\n0\n500K 88.13 / 94.52 42.39 / 20.23 / 39.82 80.29\n83.62\n85.28\n83.41\n77.23\n79.56\n120K 500K 88.18 / 94.54 42.47 / 20.25 / 39.95 82.11\n84.32\n87.25\n84.44\n77.74\n80.19\n0\n1M\n88.37 / 94.67 42.56 / 20.23 / 40.00 82.05\n84.66\n87.01\n84.28\n78.25\n81.29\n120K\n1M\n88.37 / 94.66 42.47 / 20.32 / 39.95 83.29\n86.06\n88.56\n85.75\n78.61\n81.93\nTable 11: Breakdown of SQuAD (Rajpurkar et al., 2016), CNN/DailyMail (Hermann et al., 2015) and\nRainbow (Lourie et al., 2021) sub-tasks for SPACTORLarge. Each score corresponds to the median of 5\nindependent runs. The metrics for each sub-task are: EM/F1 for SQuAD, Rouge-1/Rouge-2/Rouge-L\nfor CNN/DailyMail, and accuracy for all the Rainbow tasks.\n16\nSize\n\u03c4\nCkpt\nBoolean\nExpressions\nCausal\nJudgement\nDate\nUnderstanding\nDisambi-\nguation QA\nDyck\nLanguages\nFormal\nFallacies\nGeometric\nShapes\nHyper-\nbaton\nBase\n0\n500K\n57.60\n56.68\n38.00\n52.00\n6.80\n56.00\n22.40\n68.80\n250K 500K\n55.20\n55.61\n40.40\n60.00\n3.60\n58.80\n28.40\n66.40\n120K 500K\n54.40\n55.61\n42.80\n56.80\n6.40\n59.60\n21.20\n65.20\n0\n1M\n54.80\n55.61\n40.80\n58.40\n6.40\n60.40\n18.00\n72.00\n250K\n1M\n59.60\n54.55\n44.00\n60.80\n5.20\n60.00\n29.60\n69.20\n120K\n1M\n58.00\n56.15\n40.00\n60.80\n3.60\n60.00\n30.00\n62.00\nSize\n\u03c4\nCkpt\nLogical\nDeduction\n5 Objects\nLogical\nDeduction\n7 Objects\nLogical\nDeduction\n3 Objects\nMovie Reco-\nmmendation\nMultistep\nArithmetic\nTwo\nNavigate\nObject\nCounting\nPenguins\nIn A\nTable\nBase\n0\n500K\n28.00\n26.40\n42.40\n49.20\n1.60\n64.00\n24.80\n32.88\n250K 500K\n32.40\n29.20\n44.00\n47.60\n1.20\n64.40\n23.20\n27.40\n120K 500K\n31.20\n29.60\n45.60\n47.20\n1.20\n64.00\n26.40\n28.77\n0\n1M\n33.20\n25.60\n44.40\n47.20\n1.20\n64.00\n28.00\n32.88\n250K\n1M\n32.40\n32.40\n47.60\n47.60\n1.60\n64.80\n28.40\n30.82\n120K\n1M\n34.00\n29.60\n47.20\n47.60\n1.20\n65.60\n29.60\n30.14\nSize\n\u03c4\nCkpt\nReasoning\nAbout\nColored\nObjects\nRuin\nNames\nSalient\nTranslation\nError\nDetection\nSnarks\nSports\nUnder-\nstanding\nTemporal\nSequences\nTracking\nShuffled\nObjects\n5 Objects\nTracking\nShuffled\nObjects\n7 Objects\nBase\n0\n500K\n4.80\n28.00\n26.00\n53.93\n56.80\n28.40\n22.40\n18.00\n250K 500K\n34.80\n28.00\n27.20\n53.93\n56.80\n28.80\n22.00\n17.20\n120K 500K\n32.80\n28.00\n26.00\n53.93\n56.40\n28.80\n22.40\n18.00\n0\n1M\n35.20\n28.00\n30.00\n54.49\n57.20\n30.40\n21.20\n18.40\n250K\n1M\n32.00\n28.00\n29.20\n53.93\n57.60\n26.40\n21.20\n18.40\n120K\n1M\n34.80\n28.00\n30.40\n53.93\n57.60\n30.00\n21.60\n17.20\nSize\n\u03c4\nCkpt\nTracking\nShuffled\nObjects\n3 Objects\nWeb Of\nLies\nWord\nSorting\nBase\n0\n500K\n37.60\n55.60\n4.00\n250K 500K\n36.40\n54.40\n4.00\n120K 500K\n35.20\n56.80\n4.00\n0\n1M\n38.00\n55.20\n4.00\n250K\n1M\n36.00\n54.40\n4.00\n120K\n1M\n35.60\n54.80\n4.00\nTable 12: Breakdown of 27 BBH (Srivastava et al., 2022) tasks with direct answers for SPACTORBase.\nThe metric are all accuracy.\n17\nSize\n\u03c4\nCkpt\nBoolean\nExpressions\nCausal\nJudgement\nDate\nUnderstanding\nDisambi-\nguation QA\nDyck\nLanguages\nFormal\nFallacies\nGeometric\nShapes\nHyper-\nbaton\nLarge\n0\n500K\n59.60\n59.36\n47.60\n67.20\n2.40\n59.20\n14.80\n72.00\n120K 500K\n60.80\n59.36\n50.80\n66.40\n6.00\n56.80\n32.40\n67.20\n0\n1M\n63.60\n61.50\n54.00\n66.80\n2.40\n60.40\n27.60\n73.60\n120K\n1M\n64.40\n59.89\n54.40\n67.60\n4.80\n57.20\n24.80\n85.60\nSize\n\u03c4\nCkpt\nLogical\nDeduction\n5 Objects\nLogical\nDeduction\n7 Objects\nLogical\nDeduction\n3 Objects\nMovie Reco-\nmmendation\nMultistep\nArithmetic\nTwo\nNavigate\nObject\nCounting\nPenguins\nIn A\nTable\nLarge\n0\n500K\n44.00\n46.00\n56.00\n60.80\n1.20\n59.60\n40.00\n34.93\n120K 500K\n48.00\n50.40\n58.40\n62.00\n1.60\n60.40\n36.80\n35.62\n0\n1M\n44.40\n48.80\n61.60\n54.80\n1.20\n62.40\n42.40\n39.04\n120K\n1M\n52.00\n55.60\n68.80\n62.00\n1.20\n65.20\n37.60\n43.15\nSize\n\u03c4\nCkpt\nReasoning\nAbout\nColored\nObjects\nRuin\nNames\nSalient\nTranslation\nError\nDetection\nSnarks\nSports\nUnder-\nstanding\nTemporal\nSequences\nTracking\nShuffled\nObjects\n5 Objects\nTracking\nShuffled\nObjects\n7 Objects\nLarge\n0\n500K\n41.60\n20.00\n34.40\n53.37\n58.40\n26.80\n17.60\n15.60\n120K 500K\n44.40\n25.60\n28.00\n55.06\n59.20\n37.20\n19.20\n14.80\n0\n1M\n44.80\n25.60\n34.80\n58.99\n58.80\n30.40\n17.60\n14.40\n120K\n1M\n46.40\n24.80\n41.20\n52.25\n57.60\n36.00\n19.20\n14.80\nSize\n\u03c4\nCkpt\nTracking\nShuffled\nObjects\n3 Objects\nWeb Of\nLies\nWord\nSorting\nLarge\n0\n500K\n34.80\n54.00\n4.80\n120K 500K\n35.20\n53.60\n4.00\n0\n1M\n34.00\n53.60\n6.00\n120K\n1M\n33.60\n58.00\n6.00\nTable 13: Breakdown of 27 BBH (Srivastava et al., 2022) tasks with direct answers for SPACTORLarge.\nThe metric are all accuracy.\n18\nSize\n\u03c4\nCkpt\nAbstract\nAlgebra\nAnatomy Astronomy\nBusiness\nEthics\nClinical\nKnowledge\nCollege\nBiology\nCollege\nChemistry\nCollege\nComputer\nScience\nBase\n0\n500K\n36.36\n50.00\n50.00\n63.64\n55.17\n50.00\n50.00\n63.64\n250K 500K\n36.36\n50.00\n56.25\n63.64\n44.83\n50.00\n62.50\n63.64\n120K 500K\n36.36\n50.00\n50.00\n72.73\n48.28\n43.75\n62.50\n54.55\n0\n1M\n45.45\n57.14\n50.00\n72.73\n55.17\n43.75\n62.50\n72.73\n250K\n1M\n45.45\n50.00\n50.00\n63.64\n51.72\n50.00\n62.50\n63.64\n120K\n1M\n36.36\n57.14\n56.25\n72.73\n51.72\n43.75\n62.50\n63.64\nSize\n\u03c4\nCkpt\nCollege\nMathematics\nCollege\nMedicine\nCollege\nPhysics\nComputer\nSecurity\nConceptual\nPhysics\nEcono-\nmetrics\nElectrical\nEngineering\nElementary\nMathematics\nBase\n0\n500K\n36.36\n63.64\n72.73\n45.45\n42.31\n58.33\n50.00\n36.59\n250K 500K\n45.45\n63.64\n72.73\n63.64\n46.15\n50.00\n43.75\n36.59\n120K 500K\n45.45\n63.64\n63.64\n54.55\n42.31\n50.00\n50.00\n39.02\n0\n1M\n45.45\n63.64\n81.82\n54.55\n38.46\n58.33\n56.25\n36.59\n250K\n1M\n45.45\n59.09\n72.73\n63.64\n38.46\n50.00\n50.00\n39.02\n120K\n1M\n45.45\n63.64\n63.64\n54.55\n42.31\n50.00\n50.00\n34.15\nSize\n\u03c4\nCkpt\nFormal\nLogic\nGlobal\nFacts\nHigh\nSchool\nBiology\nHigh\nSchool\nChemistry\nHigh\nSchool\nComputer\nScience\nHigh\nSchool\nEuropean\nHistory\nHigh\nSchool\nGeography\nHigh\nSchool\nGovernment\n& Politics\nBase\n0\n500K\n57.14\n50.00\n40.63\n40.91\n55.56\n55.56\n59.09\n61.90\n250K 500K\n57.14\n50.00\n50.00\n50.00\n55.56\n55.56\n68.18\n61.90\n120K 500K\n57.14\n50.00\n50.00\n50.00\n55.56\n55.56\n63.64\n66.67\n0\n1M\n50.00\n50.00\n43.75\n40.91\n55.56\n61.11\n68.18\n71.43\n250K\n1M\n57.14\n50.00\n43.75\n50.00\n55.56\n61.11\n72.73\n61.90\n120K\n1M\n64.29\n60.00\n46.88\n45.45\n55.56\n55.56\n77.27\n66.67\nSize\n\u03c4\nCkpt\nHigh\nSchool\nMacro-\neconomics\nHigh\nSchool\nMathe-\nmatics\nHigh\nSchool\nMicro-\neconomics\nHigh\nSchool\nPhysics\nHigh\nSchool\nPsy-\nchology\nHigh\nSchool\nStatistics\nHigh\nSchool\nUS\nHistory\nHigh\nSchool\nWorld\nHistory\nBase\n0\n500K\n39.53\n41.38\n46.15\n41.18\n45.00\n47.83\n68.18\n53.85\n250K 500K\n34.88\n41.38\n53.85\n47.06\n48.33\n47.83\n68.18\n53.85\n120K 500K\n34.88\n37.93\n53.85\n47.06\n48.33\n47.83\n68.18\n57.69\n0\n1M\n37.21\n41.38\n53.85\n41.18\n50.00\n47.83\n72.73\n57.69\n250K\n1M\n37.21\n41.38\n53.85\n47.06\n46.67\n47.83\n68.18\n53.85\n120K\n1M\n41.86\n41.38\n53.85\n47.06\n50.00\n43.48\n72.73\n57.69\nTable 14: Breakdown of first 32 of total 57 MMLU (Hendrycks et al., 2021) tasks with direct answers\nfor SPACTORBase. The metric are all accuracy.\n19\nSize\n\u03c4\nCkpt\nHuman\nAging\nHuman\nSexuality\nInter-\nnational\nLaw\nJuris-\nprudence\nLogical\nFallacies\nMachine\nLearning\nManagement\nMarketing\nBase\n0\n500K\n39.13\n50.00\n61.54\n45.45\n55.56\n45.45\n63.64\n68.00\n250K 500K\n39.13\n50.00\n69.23\n36.36\n61.11\n45.45\n63.64\n64.00\n120K 500K\n47.83\n58.33\n61.54\n36.36\n61.11\n45.45\n54.55\n64.00\n0\n1M\n43.48\n58.33\n69.23\n45.45\n66.67\n45.45\n63.64\n68.00\n250K\n1M\n43.48\n50.00\n61.54\n45.45\n61.11\n63.64\n63.64\n68.00\n120K\n1M\n47.83\n50.00\n61.54\n45.45\n72.22\n45.45\n63.64\n72.00\nSize\n\u03c4\nCkpt\nMedical\nGenetics\nMisc.\nMoral\nDisputes\nMoral\nScenarios Nutrition Philosophy\nPrehistory\nProfessional\nAccounting\nBase\n0\n500K\n45.45\n39.53\n50.00\n33.00\n51.52\n35.29\n48.57\n35.48\n250K 500K\n54.55\n38.37\n50.00\n33.00\n51.52\n41.18\n48.57\n35.48\n120K 500K\n63.64\n40.70\n44.74\n33.00\n51.52\n41.18\n45.71\n35.48\n0\n1M\n54.55\n40.70\n50.00\n32.00\n57.58\n38.24\n48.57\n35.48\n250K\n1M\n63.64\n40.70\n50.00\n33.00\n57.58\n41.18\n54.29\n41.94\n120K\n1M\n54.55\n39.53\n47.37\n34.00\n54.55\n41.18\n45.71\n35.48\nSize\n\u03c4\nCkpt Professional\nLaw\nProfessional\nMedicine\nProfessional\nPsychology\nPublic\nRelations\nSecurity\nStudies\nSociology\nUS Foreign\nPolicy\nVirology\nBase\n0\n500K\n35.29\n38.71\n46.38\n75.00\n44.44\n63.64\n63.64\n50.00\n250K 500K\n35.29\n38.71\n44.93\n58.33\n48.15\n63.64\n54.55\n55.56\n120K 500K\n35.29\n35.48\n49.28\n66.67\n40.74\n63.64\n54.55\n55.56\n0\n1M\n35.29\n38.71\n44.93\n66.67\n48.15\n68.18\n63.64\n50.00\n250K\n1M\n33.53\n38.71\n49.28\n58.33\n44.44\n68.18\n63.64\n61.11\n120K\n1M\n33.53\n38.71\n47.83\n66.67\n48.15\n68.18\n63.64\n61.11\nbfSize\n\u03c4\nCkpt\nWorld\nReligions\nBase\n0\n500K\n42.11\n250K 500K\n36.84\n120K 500K\n47.37\n0\n1M\n47.37\n250K\n1M\n47.37\n120K\n1M\n47.37\nTable 15: Breakdown of second 25 of total 57 MMLU (Hendrycks et al., 2021) tasks with direct\nanswers for SPACTORBase. The metric are all accuracy.\n20\nSize\n\u03c4\nCkpt\nAbstract\nAlgebra\nAnatomy Astronomy\nBusiness\nEthics\nClinical\nKnowledge\nCollege\nBiology\nCollege\nChemistry\nCollege\nComputer\nScience\nLarge\n0\n500K\n36.36\n57.14\n56.25\n72.73\n55.17\n43.75\n50.00\n54.55\n120K 500K\n45.45\n57.14\n50.00\n72.73\n62.07\n62.50\n50.00\n63.64\n0\n1M\n45.45\n50.00\n56.25\n72.73\n55.17\n56.25\n50.00\n54.55\n120K\n1M\n54.55\n64.29\n43.75\n72.73\n58.62\n62.50\n50.00\n54.55\nSize\n\u03c4\nCkpt\nCollege\nMathematics\nCollege\nMedicine\nCollege\nPhysics\nComputer\nSecurity\nConceptual\nPhysics\nEcono-\nmetrics\nElectrical\nEngineering\nElementary\nMathematics\nLarge\n0\n500K\n45.45\n59.09\n81.82\n36.36\n42.31\n50.00\n62.50\n39.02\n120K 500K\n54.55\n63.64\n72.73\n54.55\n46.15\n58.33\n62.50\n36.59\n0\n1M\n54.55\n59.09\n81.82\n54.55\n42.31\n50.00\n68.75\n39.02\n120K\n1M\n36.36\n54.55\n90.91\n45.45\n50.00\n58.33\n62.50\n43.90\nSize\n\u03c4\nCkpt\nFormal\nLogic\nGlobal\nFacts\nHigh\nSchool\nBiology\nHigh\nSchool\nChemistry\nHigh\nSchool\nComputer\nScience\nHigh\nSchool\nEuropean\nHistory\nHigh\nSchool\nGeography\nHigh\nSchool\nGovernment\n& Politics\nLarge\n0\n500K\n57.14\n60.00\n46.88\n45.45\n55.56\n72.22\n77.27\n61.90\n120K 500K\n50.00\n60.00\n46.88\n45.45\n66.67\n72.22\n77.27\n66.67\n0\n1M\n64.29\n60.00\n46.88\n45.45\n66.67\n66.67\n81.82\n71.43\n120K\n1M\n50.00\n70.00\n43.75\n36.36\n66.67\n72.22\n81.82\n61.90\nSize\n\u03c4\nCkpt\nHigh\nSchool\nMacro-\neconomics\nHigh\nSchool\nMathe-\nmatics\nHigh\nSchool\nMicro-\neconomics\nHigh\nSchool\nPhysics\nHigh\nSchool\nPsy-\nchology\nHigh\nSchool\nStatistics\nHigh\nSchool\nUS\nHistory\nHigh\nSchool\nWorld\nHistory\nLarge\n0\n500K\n41.86\n48.28\n65.38\n47.06\n63.33\n47.83\n59.09\n57.69\n120K 500K\n44.19\n48.28\n57.69\n47.06\n66.67\n43.48\n68.18\n69.23\n0\n1M\n44.19\n44.83\n65.38\n47.06\n66.67\n52.17\n59.09\n65.38\n120K\n1M\n41.86\n44.83\n69.23\n52.94\n70.00\n56.52\n68.18\n61.54\nTable 16: Breakdown of first 32 of total 57 MMLU (Hendrycks et al., 2021) tasks with direct answers\nfor SPACTORLarge. The metric are all accuracy.\n21\nSize\n\u03c4\nCkpt\nHuman\nAging\nHuman\nSexuality\nInter-\nnational\nLaw\nJuris-\nprudence\nLogical\nFallacies\nMachine\nLearning\nManagement\nMarketing\nLarge\n0\n500K\n47.83\n58.33\n76.92\n54.55\n66.67\n45.45\n72.73\n84.00\n120K 500K\n52.17\n66.67\n76.92\n63.64\n77.78\n36.36\n72.73\n84.00\n0\n1M\n56.52\n66.67\n84.62\n54.55\n72.22\n36.36\n81.82\n84.00\n120K\n1M\n56.52\n66.67\n76.92\n63.64\n72.22\n36.36\n72.73\n88.00\nSize\n\u03c4\nCkpt\nMedical\nGenetics\nMisc.\nMoral\nDisputes\nMoral\nScenarios Nutrition Philosophy\nPrehistory\nProfessional\nAccounting\nLarge\n0\n500K\n63.64\n51.16\n50.00\n32.00\n57.58\n47.06\n48.57\n41.94\n120K 500K\n72.73\n50.00\n55.26\n32.00\n54.55\n52.94\n57.14\n38.71\n0\n1M\n72.73\n52.33\n52.63\n40.00\n51.52\n50.00\n57.14\n51.61\n120K\n1M\n63.64\n52.33\n50.00\n34.00\n54.55\n52.94\n62.86\n45.16\nSize\n\u03c4\nCkpt Professional\nLaw\nProfessional\nMedicine\nProfessional\nPsychology\nPublic\nRelations\nSecurity\nStudies\nSociology\nUS Foreign\nPolicy\nVirology\nLarge\n0\n500K\n33.53\n48.39\n56.52\n66.67\n48.15\n72.73\n63.64\n61.11\n120K 500K\n34.12\n51.61\n55.07\n66.67\n51.85\n63.64\n54.55\n72.22\n0\n1M\n32.94\n58.06\n57.97\n75.00\n44.44\n77.27\n81.82\n61.11\n120K\n1M\n34.12\n54.84\n62.32\n75.00\n55.56\n77.27\n72.73\n66.67\nbfSize\n\u03c4\nCkpt\nWorld\nReligions\nLarge\n0\n500K\n57.89\n120K 500K\n57.89\n0\n1M\n63.16\n120K\n1M\n68.42\nTable 17: Breakdown of second 25 of total 57 MMLU (Hendrycks et al., 2021) tasks with direct\nanswers for SPACTORLarge. The metric are all accuracy.\n22\n"
  },
  {
    "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models",
    "link": "https://arxiv.org/pdf/2401.13311.pdf",
    "upvote": "8",
    "text": "CONTEXTUAL: Evaluating Context-Sensitive Text-Rich Visual\nReasoning in Large Multimodal Models\nRohan Wadhawan*1 Hritik Bansal*1 Kai-Wei Chang1 Nanyun Peng1\n1University of California Los Angeles\ncon-textual.github.io\nAbstract\nRecent advancements in AI have led to the development of large multimodal models\n(LMMs) capable of processing complex tasks involving joint reasoning over text\nand visual content in the image (e.g., navigating maps in public places). This paper\nintroduces CONTEXTUAL, a novel benchmark comprising instructions designed\nexplicitly to evaluate LMMs\u2019 ability to perform context-sensitive text-rich visual\nreasoning. CONTEXTUAL emphasizes diverse real-world scenarios (e.g., time-\nreading, navigation, shopping and more) demanding a deeper understanding of the\ninteractions between textual and visual elements. Our findings reveal a significant\nperformance gap of 30.8% between the best-performing LMM, GPT-4V(ision),\nand human capabilities using human evaluation indicating substantial room for\nimprovement in context-sensitive text-rich visual reasoning. Notably, while GPT-\n4V excelled in abstract categories like meme and quote interpretation, its overall\nperformance still lagged behind humans. In addition to human evaluations, we also\nemployed automatic evaluation metrics using GPT-4, uncovering similar trends in\nperformance disparities. We also perform a fine-grained evaluation across diverse\nvisual contexts and provide qualitative analysis which provides a robust framework\nfor future advancements in the LMM design.\nHuman Evaluation\nGPT-4 Evaluation\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPercentage of Accepted Responses (%)\n21.8\n27.8\n49.3\n80.1\n22.6\n40.2\n47.4\n69.6\nShareGPT-4V-7B\nGemini-Pro-Vision\nGPT-4V\nHumans\n(a) Average performance on the dataset.\nAverage\nTime\nShopping\nNavigation\nAbstract\nMobile App\nWebpage\nInfographics\nMisc. Natural Scenes\n10\n20\n30\n40\n50\n60\n70\n80\n90\nShareGPT-4V-7B\nGemini-Pro-Vision\nGPT-4V\nHumans\n(b) Performance across visual contexts.\nFigure 1: Performance of GPT-4V, Gemini-Pro-Vision, ShareGPT-4V-7B, and Humans on the CONTEXTUAL\ndataset. (a) Human evaluation and an automatic GPT-4 based evaluation of the response correctness. (b)\nFine-grained performance with variation in visual contexts using GPT-4 based evaluation.\n\u2217Equal contribution. Contact rwadhawan7@cs.ucla.edu, hbansal@ucla.edu.\nPreprint.\narXiv:2401.13311v1  [cs.CV]  24 Jan 2024\n1\nIntroduction\nA long-standing goal in AI is to build multimodal assistants that can interact with humans to solve\ndiverse tasks. In this regard, recent development of instruction-tuned large multimodal models\n(LMMs) have resulted in models that can respond to human instructions, posed as questions or\nimperative tasks, over images [1, 2, 3, 4, 5]. Naturally, many images in our surroundings have\ntext within them which provides cues for their comprehensive understanding. The ability to reason\nabout the interactions between the text and visual context in the images powers many real-world\napplications. For example, interpreting text-rich scenes (e.g., navigating maps in public spaces) for\nassisting the visually impaired, and creative understanding of abstract text-rich images (e.g., memes).\nIn prior work, numerous datasets have been proposed to access the text-rich visual reasoning of the\nmodels such as TextVQA [6], STVQA [6], ESTVQA [7]. These datasets are focused at accessing\nthe OCR capability of the models to read the text in the image. As a result, these datasets do not\nrequire the model to capture the visual context in the image to answer the question. In Figure 2(a),\nwe highlight a few examples from the ESTVQA dataset. Here, we show that a high accuracy OCR of\nthe images (e.g, \u2018W 63 ST...Monk\u2019) has sufficient signal to answer the question (e.g., \u2018What is the\nstreet number?\u2019). Though accessing the OCR capability is important, these examples do not test the\nunique potential of the LMMs to jointly reason over the text and visual context in the image.\nTo address this challenge, we propose CONTEXTUAL, a Context-sensitive Text-rich visual reasoning\nbenchmark consisting of 506 challenging instructions for LMM evaluation. We create a diverse set of\ninstructions on text-rich images with the constraint that they should require context-sensitive joint\nreasoning over the textual and visual cues in the image. As shown in Figure 2(b), we show a few\nexamples from our dataset. We observe that the instruction (\u2018Which letters are reflected over the\nwater on the ground?\u2019) cannot be answered even by perfectly capturing the OCR of the text content\nwithin the image (e.g., \u2018RR Water on Pavement...Planes\u2019). While prior work such as MathVista [8]\nand MMMU [9] assess vision-language math reasoning and multi-disclipline (science, art, history)\nunderstanding at college level, our benchmark contains open-ended instructions to test text-rich visual\nreasoning over daily- life natural or digital scenes (e.g., time reading, shopping, navigation). We\nsummarize our novel contributions in comparison to the related works in Table 1. Each instance\nRULE ONE Nothing Is So..Safety\nAngelo's Hamburgers\nW 63 ST..Monk\nWhat is written in the image?\nWhat can we eat here?\nWhat is the street number?\nQuestion\nOCR\nESTVQA\n328A 326\nR R Water on Pavement ... Planes\n1 2 3 ... 12\nInstruction\nOCR\nConTextual\nPinpoint the shop number in front\nof which a scooter is parked.\nWhich letters are reflected over the water on\nthe ground?\nGive the exact steps to change the\ntime to 10:33.\nOCR is\nsufficient to\nanswer the\nquestion\nOCR is not\nsufficient to\nanswer the\ninstruction\n(a)\n(b)\nFigure 2: Comparison between the characteristics of the existing datasets (e.g., ESTVQA) and CONTEXTUAL\ndataset for text-rich visual reasoning. (a) The instances from the prior datasets majorly test the ability of the\nLMM to read. As a result, their questions can be solved by text-only reasoning over the accurate OCR detection.\nHowever, we would want to assess more challenging scenarios with the modern models as they are shown to\nhave enhanced visual perception and reasoning abilities. (b) The instances from CONTEXTUAL are constructed\nsuch that they test the ability of the model to capture the context under which text and visual contents in the\nimage interact with each other. Here, text-only reasoning of the detected OCR is not sufficient to perform well.\ncontains a human-written instruction (question or imperative task), and a human-written ground-truth\nresponse. We create CONTEXTUAL dataset to cover eight real-world scenarios with text in images:\n2\ntime reading, shopping, navigation, abstract scenes, mobile application, webpages, infographics,\nand miscellaneous natural scenes. We present an example from each visual context in Figure 3. The\ndiverse visual nature of these categories enables us to conduct a detailed, nuanced evaluation of the\nmodel\u2019s capabilities (\u00a73.4).\nWe conduct extensive experiments on CONTEXTUAL dataset to assess the reasoning abilities of\n13 foundation models (\u00a73.1). This set includes three augmented LLMs setups (e.g., GPT-4 [10]\nprompted with combinations of image OCR, image layouts, and image captions), two proprietary\nLMMs (e.g., GPT-4V[2], Gemini-Pro-Vision [3]), and eight open LMMs (e.g., LLaVA-1.5 [11],\nShareGPT-4V[12], Idefics [13]). In addition, we perform few-shot experiments for a selected set\nof models (e.g., Gemini-Pro-Vision, Idefics) to analyze the effect of in-context examples on the\nmodel\u2019s performance. Further, we establish a human baseline by asking human annotators to write\nresponses to the dataset instructions. Finally, we perform human and automatic evaluations to assess\nthe correctness of the predicted responses with respect to the ground-truth responses in the dataset\n(\u00a73.2). Overall, we cover a comprehensive set of models and evaluation metrics to track the progress\nin the text-rich visual reasoning (\u00a73.3).\nThrough our human evaluation on randomly selected 280 instances, we find that GPT-4V(ision) is\nthe best performing LMM on the CONTEXTUAL dataset where it achieves 49.3% acceptance rating\nto its generated responses (Figure 1a). Despite this, the performance lags way behind the human\nbaseline of 80.1% which indicates a large gap in the capabilities of the GPT-4V. In addition, we find\nthat the best performing open-model, ShareGPT-4V-7B, achieves 21.8% rating which indicates that\nthe capabilities of open models are way behind the proprietary models on context-sensitive text-rich\nvisual reasoning (\u00a73.3). Our results highlight that the CONTEXTUAL is a challenging dataset for\nmodern LMMs while humans excel on it.\nSince human evaluations are hard to scale and expensive, we also perform automatic evaluation (e.g.,\nGPT-4, GPT-4V, BLEURT [14]) on the complete dataset for all the models (\u00a73.3.1). We present the\nresults with GPT-4 evaluation in Figure 1a along with the fine-grained analysis in Figure 1b. We find\nthat the humans outperform the best performing model GPT-4V by 22.4% highlighting a large gap\nto fill. Notably, GPT-4V outperforms human performance on the abstract category which involves\nunderstanding of visual contexts such as memes and quotes.\nFinally, we conduct a qualitative assessment of model predictions, demonstrating a range of per-\nformance levels. GPT4V and Gemini-Vision-Pro exhibit superior performance, while open-source\nLMMs and the Augmented LLM approach lag. Amongst the models, GPT4V shows the highest\nperformance due to its strong visual perception and context-sensitive text-vision reasoning abilities.\nFurther, qualitatively analyzing model responses for instructions from CONTEXTUAL also helps\nidentify issues like hallucination, lack of grounding the instruction to the image, and fine-grained\nvisual perception associated with poor context-sensitive text-vision reasoning (\u00a74).\nTable 1: Comparison with related works for evaluating large multimodal models for their text-rich visual\nreasoning. We abbreviate Context-sensitive as Consens., Generation as Gen.\nOurs\nLLaVA[1]\nVisIT[15]\nTextVQA[6]\nSTVQA[16]\nDUDE[17]\nInfographicVQA[18]\nSEED [19]\nYear Released\n2024\n2023\n2023\n2019\n2019\n2023\n2021\n2023\nConsens. Text-Rich Visual Reasoning\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nText in Images\n\u2713\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\nNumber of LLM/LMM Models\n13\n3\n10\n-\n-\n9\n-\n15\nNumber of Images\n506\n24\n574\n28.4K\n23K\n5K\n5.4K\n19K\nDiverse Image Sources\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\n\u2717\nQuestion Instructions\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nImperative Instructions\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nInstruction Gen. by Humans\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\nReference Response Gen. by Humans\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\nHuman Evaluation\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\nAutomatic Evaluation\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nHuman-Auto Eval. Correlation\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nHuman performance\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\nAbsolute Score to Models\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nFine-grained Analysis\n\u2713\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n3\nNavigation\nShopping\nMiscellaneous Natural Scenes\nAbstract\nApplication Usage\nInfographic\nWeb Usage\nTime\nGroup gates by direction.\nCalculate the total cost of assembling an outfit\nconsisting of a white dress, heeled sandals,\nand sunglasses.\nDetermine the word obfuscated\nby the outfielder.\nIdentify a common proverb\nportrayed in this image.\nList the exercises where the\ncorresponding illustration\nshowcases a single movement.\nWhich countries have had a non-\nzero containment index that lies\nbetween 85-100?\nWhich items have experienced a price drop?\nComment whether the analog\nclocks are correctly matched to\nthe digital clocks.\nFigure 3: A sample from the 8 visual contexts in CONTEXTUAL that large multimodal models should\nbe able to follow.\nData Sourcing\nOpen-Source\nImages\nData Annotation Pipeline\nOCR\nHeuristic Filter\nSelecting Candidates\nfor Annotation\nConTextual \nDataset\nAnnotation\nCandidates\nAnnotated \nSplit A\nData\nSplit A\nData\nSplit B\nAnnotated \nSplit B\nVerified\nSplit A\nVerified \nSplit B\nImage,\nInstruction,\nResponse,\nMetadata\nMerge\nSplits\nPhase 1\nPhase 2\nFigure 4: Data Collection Pipeline: (1) \u2018Shortlist Images - Utilize manual and automatic methods to shortlist\nimages from the source dataset for annotation. (2) Instruction-Response Creation - Divide the authors into\ntwo distinct groups (Group 1 in green and Group 2 represented in purple), assigning each group the respon-\nsibility of annotating images corresponding to four categories. (3) Data Sample Validation - Scrutinize the\n< image, instruction, response > triplets annotated in the prior stage, with one group cross-validating the\nannotations made by the other group.\n2\nThe CONTEXTUAL Dataset\n2.1\nCollection Guidelines\nWe note that there is a notable gap in the existing benchmarks for text-rich images, which primarily\nevaluate text reading capabilities of LMMs. Our dataset bridges this gap and offers an evaluation\nframework to test the joint reasoning capabilities of the LMMs over the embedded text and the visual\nfeatures in the image (Figure 2). Our dataset encompasses a variety of tasks across diverse natural\nand digital text-rich visual scenarios, thus enabling robust testing.\nBroadly, our benchmark follows these key dataset collection guidelines:\n\u2022 Each sample consists of an < image, instruction, response > triplet, such that the\ninstructions require the models to perform context-sensitive reasoning over the text and\nvisual elements in the image. Specifically, we would avoid creating instructions that could\nbe answered by text-based reasoning (e.g., using LLM) over the detected OCR.\n4\n\u2022 We aim to cover diverse instructions, including questions and imperative tasks. This ensures\nthat the resulting dataset demonstrates a rich variety of instructions.\n\u2022 We aim to create instructions of varied complexity. Specifically, they can make extractive\ninstructions that involve extraction of a specific textual or visual elements (e.g., \u2018Name the\ndrink with banana flavor.\u2019). In addition, they can make instructions that involve reasoning\nover the embedded information (e.g., \u2018count the number of words in the rendered in the blue\ncolor.\u2019).\nIn this work, we establish a taxonomy by categorizing a dataset into eight distinct visual scenarios,\nencompassing real-world and digital human interactions. These scenarios include Shopping (e.g.,\nbuying groceries, clothing, gadgets), Navigation (various modes of transportation and navigation\nsignage), Time (depicting time and dates using clocks and calendars), Web Usage (websites across\ndifferent domains), Application Usage (smartphone applications covering education, games, entertain-\nment, etc.), Infographic (visual information on various topics), Abstract (text-rich images illustrating\nabstract concepts like memes), and Miscellaneous Natural Scenes (miscellaneous human interactions).\nMore details are available in Appendix \u00a7A.\nStatistic\nNumber\nTotal number of samples\n506\n- Images\n506\n- Instructions\n506\n- Responses\n506\nRoot verbs in instructions\n79\nVisual Scenarios\n8\nSamples per Scenario\n- Time\n50\n- Shopping\n50\n- Navigation\n50\n- Abstract\n50\n- Application Usage\n50\n- Web Usage\n50\n- Infographic\n50\n- Miscellaneous Natural Scenes\n156\nAverage Instruction Length\n65\nAverage Response Length\n117\nTable 2: Key Statistics of CONTEXTUAL\nFigure 5: Top 40 Most frequently occurring\nverbs (inner circle) and their top 4 direct\nnouns (outer circle) in the instructions.\n2.2\nData Sources\nCONTEXTUAL comprises images sourced from six different sources. Firstly, we obtain images for the\nTime, Shopping, Navigation & Abstract categories from the LAION-5B [20]. Specifically, we use key-\nword search using CLIP-retrieval UI [21]. A keyword consists of category-specific word(s) + \"text\"\n(e.g., clothes text for shopping, airports text for navigation). Some category-specific words we used\nare: shopping (e.g., grocery, furniture, gadgets, cosmetics, services, clothes), navigation (e.g., street\nsigns, cars, buses, trains, metro, airport, stations, highways, roads), time (e.g., clocks, multiple clocks,\ndigital clocks, timezones, calendars, schedule, watch), and Abstract (e.g., memes, quotes, comic\nstrips, science jokes, math jokes, infographic memes).\nSecondly, we source images for the Application Usage category from the Rico Dataset [22], which\nincludes 66,000 distinct UI screens originating from 9,300 Android apps across 27 categories, each\nof these screens featuring visual, textual, structural, and interactive design attributes essential for\ncontext-sensitive text-rich visual analysis. Thirdly, we scrape the website links made available by the\nOpen WebText Initiative [23] and collect screenshots for the Web Usage category. Open WebText\nis an open-source effort to reproduce OpenAI\u2019s WebText dataset [24]. Lastly, we acquire images\nfrom the test sets of three Visual Question Answering (VQA) datasets, namely InfographicVQA [18],\n5\nSTVQA [16], and ESTVQA [7], and proceed to annotate them with novel instruction-response pairs.\nOriginally, these datasets consists question-and-answer pairs that primarily focus on text recognition\ncapabilities. However, they offer an opportunity to formulate context-sensitive instructions for the\nimages. Consequently, we reannotate these image instances, categorizing them into two groups:\nInfographic, sourced from the InfographicVQA dataset, and Miscellaneous Natural Scenes, sourced\nfrom the STVQA and ESTVQA datasets.\n2.3\nData Annotation\nAfter image sourcing, we proceed with a three-stage annotation process, shown in Figure 4.\nStage 1: Shortlist Images.\nIn this stage, we shortlist images for instruction-response pair annotation.\nThe images that are categorized under Time, Shopping, Navigation, and Abstract undergo manual\nfiltering to guarantee their suitability for annotation. However, for Application Usage, Web Usage,\nInfographic, and Miscellaneous Natural Scenes, we perform heuristic-based filtering. Specifically,\nwe employ a PaddleOCR [25] to detect the text in the image. Subsequently, we select the top 500\nimages with the highest number of words, a subset of which get annotated in our dataset.\nStage 2: Instruction-Response Creation.\nHere, we divided the authors into two groups, namely\nGroup 1 and Group 2, each responsible for annotating four specific categories. The authors strictly\nadhered to the provided annotation guidelines throughout the annotation process. 1\nStage 3: Data Sample Verification.\nIn this final stage, we perform a verification process for each\nsample annotated in Stage 2. To this end, each group is tasked to review the annotations created by\nthe other group. This ensures adherence to guidelines, and filters out low-quality samples. Finally,\nwe end up with a dataset of 506 instances.\nMetadata Annotation\nFine-grained metadata facilitates a comprehensive analysis of a benchmark\nand the performance of models on it. To this end, we provide statistics about the CONTEXTUAL\nbenchmark, as shown in Table 2. We visualize each instruction based on its root verb and the direct\nnoun, as shown in Figure 5. We also annotated each sample to determine whether it is an extractive\ntype and whether it requires mathematical reasoning (Appendix \u00a7E.1).\n2.4\nData Release\nCONTEXTUAL comprises 506 samples spanning eight visual categories (refer to Table 2). To facilitate\nmodel development, we will release a subset of 100 samples from the 506, as validation set, along\nwith their reference responses, while keeping them hidden for the remaining 406 samples. We\nensure that the distribution of validation samples closely mirrors the overall dataset distribution. To\nachieve this, we randomly select 30 samples from the \u2018Miscellaneous Natural Scenes\u2019 category and\n10 samples from the remaining categories, maintaining a proportional representation of each category\nin the validation samples, consistent with the overall benchmark. In this paper, all the results are\nreported on the entire dataset, unless stated otherwise.\n3\nExperiments\nWe conduct a thorough quantitative and qualitative evaluation of the context-sensitive text-rich visual\nreasoning capabilities of the large multimodal models (LMMs) using CONTEXTUAL. Specifically,\nwe assess 10+ foundation models and establish a human performance baseline on our dataset 3.1.\nIn addition, we perform human evaluation and utilize various automatic evaluation methods to rate\nthe predicted responses from the models 3.2. Further, we provide quantitative results on the model\u2019s\nperformance along with human-automatic correlation analysis in \u00a73.3. Additionally, we perform\nfine-grained evaluation of the model\u2019s performances under visual contexts, visual scenes and task\ncomplexity in \u00a73.4. Finally, we provide qualitative analysis in \u00a74.\n1We observe that MTurk workers found this task time-consuming, leading to annotations that would be hard\nto accomplish within a limited budget.\n6\n3.1\nSetup\nAugmented LLMs.\nPrior works [26, 27, 28, 29] have shown that the large language models can\nsolve a wide range of visual reasoning problems by utilizing sensory information from diverse visual\ntools (e.g., image captions, object detectors models). Since our dataset is focused on text-rich visual\nreasoning, it is imperative to understand the extent to which a strong LLM GPT-4 can perform on\nCONTEXTUAL dataset with the OCR information and image captions. To this end, we study this\naugmented setup under three settings: GPT-4 prompted with (a) vanilla OCR, (b) layout-aware OCR,\nand (c) combining layout-aware OCR with image captions. We leverage the PP-OCRv4 model of\nPaddleOCR library [25] for extracting OCR from the images, LATIN prompt [30] inspired OCR text\narrangement implementation to maintain layout-awareness in the OCR, and ShareGPT-4V-7B for the\ndense image captions. We provide the GPT-4 prompt in Appendix \u00a7B.\nLMMs.\nWe evaluate GPT-4V [2] and Gemini-Pro-Vision [3] that are representative proprietary\nLMMs that have achieved state-of-the-art on other visual reasoning benchmarks [31]. In addition,\nwe evaluate a wide range of open LMMs including LLaVA-1.5-13B [11], ShareGPT-4V-7B [12],\nmPLUG-Owl-v2-7B [32, 33], Qwen-VL-Plus [5], InstructBLIP [4], and Idefics-9B [13]. We include\nLLaVAR [34] and BLIVA [35] as they were introduced for enhanced text-rich visual reasoning.\nBy default, we assess the performance of the above models under the zero-shot setting. We perform\nfew-shot evaluation (2-shot, 4-shot, 8-shot) for selected models including augmented LLM (GPT-4\nwith layout-aware OCR and image captions), Gemini-Pro-Vision, and Idefics-9B.\nHumans.\nWe also benchmark the performance of humans on our dataset using Amazon Mechanical\nTurk. The selected annotators that pass an qualification test were asked to write accurate responses\nfor all the instruction-image from the dataset. We provide the screenshot of our annotation interface\nin Appendix \u00a7C.1. We spent $180 on collecting human predictions on our dataset.\n3.2\nEvaluation\nAs discussed in \u00a72, every instance in the CONTEXTUAL dataset consists image, instruction, and a\nreference response in natural language. The goal of the evaluation is to assess the correctness of the\nmodel response for the given instruction for an image.\n3.2.1\nHuman Evaluation\nTo perform a faithful evaluation of the predicted responses, we ask human annotators sourced from\nAmazon Mechanical Turk to rate the predicted response quality given the image, instruction, and\nreference response from our dataset. First, we sample 280 instances from the dataset randomly\nfrom the CONTEXTUAL dataset. Second, we collect the model responses for these instances from\naugmented LLM (GPT-4 with layout-aware OCR and image captions), GPT-4V, Gemini-Pro-Vision,\nLLaVA-1.5-13B, ShareGPT-4V-7B, and humans. In total, we have 1680 predicted responses from\nmodels and humans. Third, we show each model response, without revealing the model identity, to\nthree human annotators independently. Specifically, the human annotators are asked to decide the\npredicted response is acceptable given the reference response, instruction and image from the dataset.\nFinally, we report the acceptance rating (0-100 in percentage) of the responses using the majority vote\namong the three annotator as the final decision for a model prediction. We provide the screenshot of\nour annotation interface in Appendix C.2. We spent $1000 in acquiring human judgments.\n3.2.2\nAutomatic Evaluation\nWhile human evaluation acts as a gold standard, it is hard to scale since it is expensive and time-taking.\nSince our dataset uniquely provides reference response for each instruction, we utilize test a wide\nrange of reference-guided automatic evaluation methods. Specifically, these include (a) prompting\nan LLM GPT-4 with the instruction, reference response and predicted response, (b) prompting an\nLMM GPT-4V with the image, instruction, reference response and predicted response, (c) and other\ntext generation methods like BLEURT [14], Rouge-L [36] and BERTScore [37] that assess the\nsimilarity between the reference response and predicted response. Specifically, GPT-4 and GPT-4V\nare prompted to provide their judgement on the predicted response, same as human evaluation. We\n7\npresent the prompt for GPT-4 based evaluation in Appendix \u00a7D. However, the other text generation\nmethods provide a continuous score 0-1 which is scaled to 0-100.\nThrough our automatic evaluation methods, we evaluate all the model responses on the entire dataset.\nSubsequently, we conduct a correlation analysis between human and automated methods, utilizing the\nsame 1,680 responses from the human evaluation, to assess the efficacy of the automated approaches\n(\u00a73.3.1). Finally, we utilize the GPT-4 automatic evaluation, that achieves the highest correlation with\nhuman judgments, for large-scale evaluation of all the models on the complete dataset (\u00a73.4).\nTable 3: Comparison in the performance of various foundation models (augmented LLM and LMMs) and\nhumans on the CONTEXTUAL dataset. We report the response acceptance rating using human evaluation, auto-\nmatic GPT-4 and GPT-4V based evaluation. In addition, we report standard text generation quality assessment\nmetrics including BLEURT, Rouge-L, and BERTScore. We find that humans outperform existing models on our\ndataset. The best performing LMM is GPT-4V is behind human performance by 30%. The best performance in\na column is highlighted in BLACK while the second best performance is highlighted in UNDERLINE.\nHumans\nGPT-4\nGPT-4V\nBLEURT\nRouge-L\nBERTScore\nGPT-4 w/ Layout-aware OCR + Caption\n17.2\n22.2\n17.6\n41.3\n22.5\n53.9\nGPT-4V [2]\n49.3\n47.4\n45.0\n45.3\n17.3\n52.5\nGemini-Pro-Vision [3]\n27.8\n40.2\n37.1\n42.5\n30.1\n58.4\nLLaVA-1.5-13B [11]\n17.2\n20.6\n17.5\n43.6\n21.7\n54.8\nShareGPT-4V-7B [12]\n21.8\n22.6\n20.6\n44.5\n23.3\n55.8\nHumans\n80.1\n69.6\n68.6\n47.4\n33.6\n59.8\n3.3\nResults\nWe compare the performance of augmented LLM, LMMs, and humans on CONTEXTUAL using\nhuman and automatic evaluation in Table 3. Through our human evaluations, we find that the humans\nperform the best on the dataset with the response acceptance rating of 80.1%. In addition, we observe\nthat the GPT-4V achieves the highest acceptance rating of 49.3% in comparison with all the other\nmodels. However, this rating is quite far from the human performance which indicates that our task\nis quite challenging for the state-of-the-art LMMs while humans are good at it. We find that the\nGPT-4V outperforms Gemini-Pro-Vision by 22% highlighting a large gap in the models text-rich\nvisual reasoning capabilities. Further, we find that augmented LLM approach achieves a very low\nrating of 17.2% which indicates that the dataset instances cannot be solved without precise visual\nperception. Interestingly, we observe that the open-models such as LLaVA-1.5-13B and ShareGPT-\n4V-7B achieve poor acceptance ratings through human evaluations which indicates the presence of a\nlarge gap in their capabilities from proprietary models. This might be attributed to the differences in\nthe model capacity, along with the scale and quality of the pretraining data.\nAs human evaluation is not scalable, we perform automatic evaluation of the model responses\non the entire dataset. In Table 3, we find that the ratings of the human responses outperforms\nthe ones from GPT-4V by 22.2% and 23.6% using GPT-4 and GPT-4V evaluation. Alike human\nevaluation, automatic evaluation with GPT-4 and GPT-4V highlights that the human performance\non the CONTEXTUAL dataset is way higher than the best performing LMM. Interestingly, the gap\nbetween the performance GPT-4V and Gemini-Pro-Vision is 7.2%. In addition, we still observe\na large gap in performance of the proprietary models and open LMMs. We perform fine-grained\nevaluation to understand the gaps in model capabilities along the various quality dimensions in \u00a73.4.\nIn addition, we find that the BLEURT scores for the humans is the highest while GPT-4V achieves the\nhighest score among the LMMs. Interestingly, we find that the open models (LLaVA-1.5, ShareGPT-\n4V) achieve a higher BLEURT score than Gemini-Pro-Vision. We observe similar counter-intuitive\ntrends in our Rouge-L and BERTScore based automatic evaluations. For instance, Rouge-L and\nBERTScore rank open models better than GPT-4V despite considering the human responses to be the\nbest. This counter-intuitive observation might be attributed to the sensitivity of these methods to the\ndifferences in lexical variations in the reference and predicted responses [14].\n3.3.1\nCorrelation Analysis of Automatic Evaluation Methods\nWe measure the correlation between the candidate automatic metrics and human judgments using\nROC-AUC and spearman correlation in Table 4. Specifically, the human judgments are considered\nas gold standard where we assign \u20180\u2019 to unaccepted responses to the instructions and \u20181\u2019 to the\n8\nTable 4: Comparison of the human and automatic evaluation metric using ROC-AUC and spearman correlation.\nWe find that the GPT-4 and GPT-4V based evaluation correlate the most with the humans in both the methods.\nGPT-4\nGPT-4V\nBLEURT\nRougeL\nBERTScore\nROC-AUC\n85.9\n83.9\n72.9\n67.6\n66.8\nSpearman Correlation\n0.71\n0.68\n0.38\n0.29\n0.28\naccepted responses. We find that GPT-4 based evaluation achieves the highest ROC-AUC of 85.9\nand spearman correlation of 0.71 amongst all the automatic evaluation metrics. In addition, we\nobserve that GPT-4V also achieves a high correlation with the human judgments which is close to\nGPT-4. The differences in the judgments of GPT-4 and GPT-4V can be attributed to the difference\nin their respective inputs. Specifically, GPT-4 bases its judgments on the given instruction and the\nreference response, whereas GPT-4V, with access to an input image, may potentially be biased. This\naccess might lead GPT-4V to overlook the reference response and depend on the visual cues from the\ninput image for making judgments in some cases. Finally, we observe that standard text generation\nmetrics achieve a poor ROC-AUC and Spearman correlation in comparison to GPT-4 metrics. This\ncorroborates the findings from the prior research [15] that shows GPT-4 evaluation outperforms\nstandard text generation metrics. As a result, we utilize GPT-4 for automatically evaluate the quality\nof the predicted responses on the CONTEXTUAL dataset at scale.\n3.3.2\nFew-Shot Setting\nHere, we compare the performance of the foundation models on CONTEXTUAL using GPT-4\nevaluation with under the few-shot settings in Figure 6. Specifically, we perform zero-shot, two-shot,\nfour-shot, and eight-shot evaluation for augmented LLM (GPT-4 prompted w/ layout aware OCR and\nimage caption), Gemini-Pro-Vision, and Idefics-9B. We select in-context examples at random from\nour dataset and evaluate the models on the remaining instances.\n0\n2\n4\n8\nNumber of Shots\n0\n10\n20\n30\n40\n50\n60\n70\nAcceptance Rating (GPT-4 Eval)\nHuman Performance\nGPT-4 w/ Layout-aware OCR + Image Caption\nGemini-Pro-Vision\nIdefics-9B\nFigure 6: Few-shot performance on the CONTEX-\nTUAL dataset.\nIn our experiments, we find that the performance\nof all the models does not change drastically\nwith in-context examples. Specifically, we ob-\nserve that Gemini-Pro-Vision response accep-\ntance rating decreases by 5% in the two-shot\nsetting as compared to the zero-shot setting, and,\nincreases monotonically from two-shot to eight-\nshots. In addition, we observe that the perfor-\nmance improvements stagnate for Idefics-9B af-\nter the four in-context examples. Recent studies\nhighlight the instability and sensitivity of LMMs\nin few-shot settings [38]. For instance, a signifi-\ncant accuracy drop was observed in models like\nInstructBLIP in four-shot setting, especially in\ntasks requiring commonsense reasoning. Over-\nall, we highlight that providing few-shot exam-\nples does not elicit context-sensitive text-rich\nvisual reasoning in the foundation models.\n3.4\nFine-Grained Evaluation\nWe compare the fine-grained performance of a wide range of foundation models across different\nvisual contexts using GPT-4 evaluation in Table 5. In our experiments, we find that GPT-4V outshines\nthe baseline models in almost all categories. We observe that the sole exceptions are web usage and\nmiscellaneous natural scenes contexts, where Gemini-Pro-Vision holds the lead. Notably, GPT-4V\noutperforms humans on reasoning over the abstract category, highlighting that it may have been tuned\nto reason over a lot of memes and quotes data. In addition, we observe that all the models struggle\nthe most in the time category while humans ace it, a skill which is could be hard to learn from the\ntraining data. After time reading, the proprietary LMMs underperform on the infographics category\nwhich consists reasoning over data visualizations. Prior work [8, 39] has shown that the existing\nLMMs underperform humans in reasoning over charts.\n9\nTable 5: Fine-grained comparison in the performance of the foundation models (augmented LLM and LMMs)\nand humans on the CONTEXTUAL dataset using GPT-4 evaluation. We abbreviate the average response\nacceptance rating as Avg., Shopping as Shop., Navigation as Nav., Abstract as Abs., Application usage as\nApp., Web Usage as Web, Infographics as Info., Miscellanous natural scenes as NS. We find that the GPT-4V\noutperforms all the model baselines on most of the categories while Gemini-Pro-Vision is the best on Web\nusage and natural scenes. The best performance in a column is highlighted in BLACK while the second best\nperformance is highlighted by UNDERLINE.\n.\nMODELS\nAVG.\nTIME\nSHOP.\nNAV.\nABS.\nAPP.\nWEB.\nINFO.\nMISC. NS.\nAugmented Large Language Models\nGPT-4 w/ OCR\n15.9\n4.0\n10.0\n14.0\n30.6\n8.0\n16.0\n28.6\n16.9\nGPT-4 w/ Layout-aware OCR\n18.2\n8.0\n20.0\n18.0\n34.7\n10.0\n16.0\n16.0\n20.7\nGPT-4 w/ Layout-aware OCR + Caption\n22.2\n6.0\n16.0\n24.0\n57.1\n14.0\n18.0\n8.0\n27.3\nLarge Multimodal Models\nGPT-4V [2]\n47.4\n18.0\n54.0\n48.0\n100.0\n48.0\n42.0\n28.0\n48.0\nGemini-Pro-Vision [3]\n40.2\n16.0\n32.7\n28.6\n65.3\n44.9\n43.8\n20.0\n52.8\nLLaVA-1.5B-13B [11]\n20.8\n4.0\n10.0\n18.0\n44.9\n16.0\n26.0\n4.0\n29.7\nShareGPT-4V-7B [12]\n22.6\n0.0\n16.0\n20.0\n28.6\n20.0\n20.0\n14.0\n37.7\nmPLUG-Owl-v2-7B [32]\n18.6\n4.0\n8.0\n24.0\n32.7\n20.0\n10.0\n12.0\n26.0\nQwen-VL-Plus [5]\n21.8\n4.0\n20.0\n24.0\n53.1\n6.0\n18.0\n14.0\n27.3\nInstructBLIP-Vicuna-7B [4]\n9.7\n2.0\n4.0\n16.0\n20.0\n6.0\n12.0\n2.1\n12.0\nIdefics-9B [13]\n7.7\n4.0\n2.0\n12.0\n12.0\n0.0\n6.0\n2.0\n13.3\nLLaVAR [34]\n14.9\n10.0\n16.0\n6.0\n44.9\n8.0\n10.0\n6.0\n16.7\nBLIVA [35]\n10.3\n2.0\n4.0\n14.0\n24.5\n4.0\n8.0\n4.0\n14.7\nHumans\n69.6\n64.0\n64.0\n73.5\n75.5\n64.0\n58.0\n72.0\n78.0\nFurther, we observe that the best performing open models like LLaVA-1.5-13B and ShareGPT-4V-7B\nachieve an acceptance rating above 25% only in the context of abstract and natural scenes while they\nstruggle with time, shopping, navigation, app (mobile) usage, web usage, and infographics. This can\nbe attributed to the lack of diverse visual contexts in their training data. For instance, COCO dataset\n[40] used for vision-language alignment in the open models, predominantly comprises natural scenes.\nIn addition, we observe that the open models that were specifically introduced for text-rich visual\nreasoning like LLaVAR and BLIVA falter on CONTEXTUAL dataset. This indicates that these models\ncannot reason when the instruction requires them jointly over the text content and visual context in\nthe image. We perform additional fine-grained evaluation in Appendix \u00a7E. Overall, our fine-grained\nanalysis aids in identifying the gaps in the existing models which would inspire the development of\nnext generation LMMs.\n4\nQualitative Examples\nGPT-4V vs Humans.\nGPT-4V performs well on CONTEXTUAL but still has a gap of 30.8%\naccording to human evaluation and 22.2% according to GPT-4 eval. To better understand this gap,\nwe qualitatively analyze GPT-4V\u2019s results compared to human performance. For example, in Figure\n7, we see an instance where GPT-4V provides an incorrect answer. In this case, the model is asked\nto identify the colors of different parts of a space launch vehicle - space shuttle, external tank, and\nrocket thrusters. GPT-4V makes errors in color predictions but can accurately infer the diagram\u2019s\ninformation, revealing a lack of precise visual perception. It wrongly assumes the space shuttle\nis white and hallucinates the red tips of the rockets. We provide more examples in Appendix \u00a7F\n(Figures 25, 29, 33, 34, 48, 51, 52), highlights that GPT-4V\u2019s core issue lies in fine-grained perception\ncoupled with a bias for prior visual knowledge (e.g., assuming the space shuttle is white even when\nit\u2019s represented as yellow). Similar analysis was presented in the prior work [41] where GPT-4V fails\non the perturbed versions of common visual illusions.\nGPT-4V vs. Open LMMs and Augmented LLM.\nWe also qualitatively compare the performance\nof GPT-4V against open-source LMMs and the Augmented LLM approach. To this end, we analyze\nthe best performing open-source LMM and Augmented LLM approach, that is, ShareGPT-4V-7B\nand GPT-4 w/ Layout-aware OCR + Caption, respectively, using an example illustrated in Figure\n8. In this example, the models are instructed to provide the cost of hiring the illustrated washing\nmachine for nine months. To accurately respond to the instruction, a model must first understand\nwhich machine is depicted in the image and then, based on it, extract the appropriate cost for the\nperiod mentioned in the instruction. We observe that GPT-4V can correctly identify the machine and\n10\nalso identify the correct amount. However, ShareGPT-4V-7B and the Augmented LLM approach fail\nto do so.\nThe open-source LMM correctly determines the time period but fails to identify the correct machine,\nhighlighting its weakness in context-sensitive reasoning and fine-grained visual perception. This\nqualitative example is consistent with other instances where open-source LMMs demonstrated inferior\ncontext-sensitive reasoning to GPT-4V (Appendix \u00a74 for more examples). They also show a higher\ntendency to hallucinate, possibly due to a lack of grounding the instruction to the given image or\nreliance on prior knowledge. We refer to Figure 26,27,30,31,38,47 for more examples. In this\nexample, the augmented LLM approach mispredicts the machine as top load and produces a result\nsimilar to that of the LMM. This can be attributed to its inability to reason over text and image jointly\n(refer to Appendix \u00a74 for more examples).\nIn summary, GPT-4V outperformed open-source LMMs and Augmented LLMs on CONTEXTUAL,\nshowcasing superior context-sensitive text-vision reasoning. We qualitative analysis helps in identify-\ning issues like hallucination, lack of grounding the instruction to the image, and fine-grained visual\nperception associated with poor context-sensitive text-vision reasoning.\nInstruction: Specify the colors of the rocket, space shuttle, and\nexternal tank.\nReference Response:\nThe space shuttle is depicted in\nyellow, the rockets are depicted in grey, and the external tank is\ndepicted in red.\nHuman Response: The rocket is gray. The space shuttle is\nyellow. The external tank is red.\nGPT-4V Response: In the image, the solid rocket boosters of\nthe space shuttle are colored in red and light gray, with the\nred at the top and the gray on the bottom. The external tank\nis colored orange. The main body of the space shuttle, also\nknown as the orbiter, is primarily white with black on its\nnose and wing edges. There is also a bit of red on the tail\nsection.\nSummarized Reasoning: During the pre-launch, all components\nare vertical. The solid rocket boosters detach when the shuttle\nascends. Once the shuttle reaches space\u2019s edge, the external tank\nis released. Throughout these phases, the shuttle\u2019s colors remain\nconsistent, i.e., white for the main body that is a typical color for\nit. External tank is orange, which is its natural insulation color.\nThe solid rocket boosters are white with red tops.\nFigure 7: In this instance, GPT-4V provides an incorrect\nresponse to the instruction, despite its logical reasoning. The\nuse of green indicates responses that match the reference,\nwhile red highlights errors in the responses. Additionally, a\nSummarized Reasoning is provided to outline the rationale\nused by GPT-4V to arrive at its answer.\nInstruction: Provide the cost of hiring\nthe machine type depicted in the image\nfor nine months.\nReference Response:\nThe machine\nshown in the image is a front-loading\nmachine, and the rental fee for a period of\n9 months amounts to INR 7200.\nGPT-4V Response: The image displays\na front load washing machine along with\na price list for hiring different types of\nwashing machines. For hiring a front load\nwashing machine for a period of 9 months,\nthe cost would be INR 7,200 according to\nthe given price list.\nShareGPT-4V-7B Response: The cost\nof hiring the washing machine for nine\nmonths is 4500 rupees.\nGPT-4 w/ Layout-aware OCR\n+ Caption Response: The cost of hiring\nthe depicted machine type (Top Load) for\nnine months is 4500 units (currency not\nspecified in the provided text)\nFigure 8: In this example, GPT-4V cor-\nrectly responds to the instruction. How-\never, ShareGPT-4V-7B (best perform-\ning open-source LMM) and GPT-4 w/\nLayout-aware OCR + Caption (Aug-\nmented LLM) produce a wrong response,\ndue to lack of joint reasoning over text\nand image.\n11\n5\nRelated Work\nText-Rich Image Understanding.\nRecently, there has been a grown interest in understanding the\ninteractions between the text and visual elements in the image [42, 43]. To track the progress of the\nmodels in this field, several datasets were introduced like OCRVQA [44], TextVQA [6], DocVQA\n[45], STVQA [16], ESTVQA [7]. These datasets majorly focus on the ability of the models to\naccurately read the text in the documents or natural scene images. In addition, there are datasets such\nas InfographicsVQA [18] and ChartQA [46] that assess model\u2019s ability to reason jointly about the text\nand visual elements in the image. However, they are focused on specific domains (e.g., infographics)\nand limited to question answering. Prior work [47] evaluates the ability of LMMs to perform accurate\nOCR on the existing datasets. In comparison, we propose a new CONTEXTUAL dataset, comprising\na wide range of visual contexts, instruction types (questions and imperative tasks), that aims to test\nthe LMM\u2019s ability to perform precise visual perception and complex reasoning over the visual and\ntext elements of the image.\nVision Language Reasoning Benchmarks.\nHaving high-quality datasets is essential to assess the\nprogress of the fields towards building high utility models for the real-world. Traditionally, vision-\nlanguage learning has focused on tasks such as visual question answering [48, 31] or image captioning\n[49, 40] where the model primarily needs to understand the key objects and their relations. In addition,\nthere are several datasets and benchmarks that evaluate specific skills of the LMMs including math\nskills [8], world knowledge [9], and grade school science diagrams [50]. In comparison, our\nCONTEXTUAL dataset introduces and tests the ability of the large multimodal models to perform\ncontext-sensitive text-rich visual reasoning for real-world scenarios. These include time reading,\nnavigation and transportation in public spaces, meme and quote understanding, and shopping etc.\nLarge Multimodal Models.\nRecently, there has been a huge surge of vision-language generative\nmodels with instruction-following capabilities, termed as large multimodal models (LMMs). These\ninclude proprietary models such as GPT-4V [2] and Gemini-Pro-Vision [3]. These models have\nachieved state-of-the-art performance on the traditional vision-language models. In the open space,\nthe models include LLaVA [1], mPLUG-Owl [32], OpenFlamingo [51], Idefics [13], LLaMA-Adapter\n[52], Idefics [13]. In addition, there are a class of LMMs that focus on enhanced text-rich visual\nreasoning capabilities including LLaVAR [34] and BLIVA [35]. In this work, we compare the\nperformance of LMMs on the CONTEXTUAL dataset. We find that the text-rich visual reasoning\ncapabilities of the proprietary models is way superior than the open models. We also include fine-\ngrained analysis to understand the gaps in the model performance across different visual contexts.\n6\nConclusion\nIn this work, we introduce CONTEXTUAL, a dataset for evaluating the text-rich visual reasoning\nin large multimodal models. Going beyond the prior efforts that focus primarily on the testing\nthe reading skills in the visual contexts, we create novel and challenging instructions from scratch\nthat would require the models to capture the context in which the text is presented in an image. In\naddition, we perform ask humans to solve our dataset and also use human annotators for model\nresponse evaluation. In our experiments, we find that the modern LMMs (proprietary and open\nmodels) struggle to perform on our dataset while humans are good at it. We also perform fine-grained\nevaluation and qualitative analysis to pinpoint the gaps in the model capabilities. In summary, our\ndataset paves a path for assessing the progress on reasoning over text-rich images, a domain with\nsignificant real-world applications.\n7\nAcknowledgement\nThis material is based on research supported by the ECOLE program under Cooperative Agreement\nHR00112390060 with the US Defense Advanced Research Projects Agency (DARPA). We also want\nto thank the UCLA-Amazon Science Hub for Humanity and Artificial Intelligence. Hritik Bansal is\nsupported in part by AFOSR MURI grant FA9550-22-1-0380.\n12\nReferences\n[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[2] OpenAI.\nGpt-4v(ision) system card,\n2023b.\nhttps://openai.com/research/\ngpt-4v-system-card, 2023.\n[3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[4] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023.\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile\nabilities. arXiv preprint arXiv:2308.12966, 2023.\n[6] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi\nParikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019.\n[7] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng\nChan, Anton van den Hengel, and Liangwei Wang. On the general value of evidence, and\nbilingual scene-text visual question answering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10126\u201310135, 2020.\n[8] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao\nCheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical\nreasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\n[9] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\nunderstanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\n[10] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023a, 2023.\n[11] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[12] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua\nLin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint\narXiv:2311.12793, 2023.\n[13] HuggingFace. Introducing idefics: An open reproduction of state-of-the-art visual language\nmodel. https://huggingface.co/blog/idefics, 2023.\n[14] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text\ngeneration. arXiv preprint arXiv:2004.04696, 2020.\n[15] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh\nGardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language\ninstruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023.\n[16] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusinol, Ernest Valveny,\nCV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 4291\u20134301, 2019.\n[17] Jordy Van Landeghem, Rub\u00e8n Tito, \u0141ukasz Borchmann, Micha\u0142 Pietruszka, Pawel Joziak,\nRafal Powalski, Dawid Jurkiewicz, Micka\u00ebl Coustaty, Bertrand Anckaert, Ernest Valveny, et al.\nDocument understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 19528\u201319540, 2023.\n13\n[18] Minesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawa-\nhar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 1697\u20131706, 2022.\n[19] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.\nSeed-\nbench: Benchmarking multimodal llms with generative comprehension.\narXiv preprint\narXiv:2307.16125, 2023.\n[20] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. Advances in\nNeural Information Processing Systems, 35:25278\u201325294, 2022.\n[21] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval\nsystem with them. https://github.com/rom1504/clip-retrieval, 2022.\n[22] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li,\nJeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven\ndesign applications. In Proceedings of the 30th annual ACM symposium on user interface\nsoftware and technology, pages 845\u2013854, 2017.\n[23] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\n[24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[25] paddlepadle. Paddleocr: Multilingual ocr toolkit based on paddlepaddle. https://github.\ncom/PaddlePaddle/PaddleOCR, 2023.\n[26] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun\nZhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language\nmodels. arXiv preprint arXiv:2304.09842, 2023.\n[27] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n[28] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[29] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14953\u201314962, 2023.\n[30] Wenjin Wang, Yunhao Li, Yixin Ou, and Yin Zhang. Layout and task aware instruction prompt\nfor zero-shot document image question answering. arXiv preprint arXiv:2306.00526, 2023.\n[31] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question answering.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n6904\u20136913, 2017.\n[32] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[33] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and\nJingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality\ncollaboration. arXiv preprint arXiv:2311.04257, 2023.\n[34] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.\nLlavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107, 2023.\n14\n[35] Wenbo Hu, Yifan Xu, Y Li, W Li, Z Chen, and Z Tu. Bliva: A simple multimodal llm for better\nhandling of text-rich visual questions. arXiv preprint arXiv:2308.09936, 2023.\n[36] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pages 74\u201381, 2004.\n[37] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.\n[38] Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, and\nMin Zhang. A comprehensive evaluation of gpt-4v on knowledge-intensive visual question\nanswering. arXiv preprint arXiv:2311.07536, 2023.\n[39] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart:\nA universal vision-language pretrained model for chart comprehension and reasoning. arXiv\npreprint arXiv:2305.14761, 2023.\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[41] Tianrui Guan, Fuxiao Liu, Xiyang Wu Ruiqi Xian Zongxia Li, Xiaoyu Liu Xijun Wang, Lichang\nChen Furong Huang Yaser Yacoob, and Dinesh Manocha Tianyi Zhou. Hallusionbench: An\nadvanced diagnostic suite for entangled language hallucination & visual illusion in large vision-\nlanguage models. arXiv e-prints, pages arXiv\u20132310, 2023.\n[42] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisensch-\nlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct:\nScreenshot parsing as pretraining for visual language understanding. In International Confer-\nence on Machine Learning, pages 18893\u201318912. PMLR, 2023.\n[43] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-\ntraining of text and layout for document image understanding. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1192\u20131200,\n2020.\n[44] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa:\nVisual question answering by reading text in images. In 2019 international conference on\ndocument analysis and recognition (ICDAR), pages 947\u2013952. IEEE, 2019.\n[45] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on\ndocument images. In Proceedings of the IEEE/CVF winter conference on applications of\ncomputer vision, pages 2200\u20132209, 2021.\n[46] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A\nbenchmark for question answering about charts with visual and logical reasoning. arXiv preprint\narXiv:2203.10244, 2022.\n[47] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu\nLiu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large\nmultimodal models. arXiv preprint arXiv:2305.07895, 2023.\n[48] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence\nZitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425\u20132433, 2015.\n[49] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo,\nand Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3608\u20133617, 2018.\n15\n[50] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali\nFarhadi. A diagram is worth a dozen images. In Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages\n235\u2013251. Springer, 2016.\n[51] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo: An open-\nsource framework for training large autoregressive vision-language models. arXiv preprint\narXiv:2308.01390, 2023.\n[52] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\nConghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient\nvisual instruction model, 2023.\n16\nA\nDataset Details\nA.1\nVisual Scenarios Description\nIn this section, we outline the constituent elements that make up each visual scenario, as illustrated in\nTable 6.\nCategory\nDescription\nShopping\nPurchasing groceries, clothes, furniture, gadgets, cosmetics, services, and miscella-\nneous products.\nNavigation\nDifferent modes of transportation - passenger vehicles, trucks, buses, trains, and\nairplanes, and navigation signage - streets, roadways, bus stations, train stations, and\nairports.\nTime\nItems showcasing time and dates, including analog clocks, digital clocks, multi-clock\nsetups, calendars, and other miscellaneous time-viewing setups.\nWeb Usage\nWebsites across a variety of domains, like new articles, blogs, sports, and e-commerce\nApp Usage\nSmartphone applications on education, productivity, games, lifestyle, entertainment,\nnews, etc.\nInfographic\nInfographics on local and global information spanning domains of health, sports,\neducation, natural resources, technology, etc.\nAbstract\nMemes, comic strips, and other abstract concepts illustrated through text-rich images.\nMiscellaneous\nNatural Scenes\nMiscellaneous human interactions do not fall into the previous categories.\nTable 6: Descriptions of the eight visual scenarios in CONTEXTUAL.\nA.2\nVisual Scenarios Examples\nIn this section, we provide examples of each visual category in CONTEXTUAL.\n(a) Single Clock\n(b) Multiple Clocks\n(c) Calendar\n(d) Timer\nFigure 9: Examples of the Time visual Scenario\n(a) Grocery\n(b) Furniture\n(c) Clothes\n(d) Gadgets\n(e) Cosmetics\n(f) Services\nFigure 10: Examples of the Shopping visual scenario\n17\n(a) Street\n(b) Car\n(c) Bus\n(d) Train\n(e) Airport\nFigure 11: Examples of the Navigation visual scenario\n(a)\n(b)\n(c)\n(d)\nFigure 12: Examples of the Abstract visual scenario\nB\nAugmented LLM Prompt\nIn this section, we discuss the design and elaborate on the prompts employed for the Augmented LLM\napproach (illustrated in Figure 17, 18, 19). We describe the three distinct prompt formats utilized,\neach differing in the extent of visual information presented. These formats encompass simple OCR of\nthe image, OCR of the image arranged in the layout it appears in the image, and OCR presented in a\nlayout format along with a comprehensive image caption. We prompt GPT4 with the above templates\nthat does not take the image as input. However, the image is included in the illustration for reference\npurposes.\nC\nHuman Annotation Screenshots\nC.1\nHuman Performance Screenshot\nWe present the screenshot of the user interface used for acquiring human responses on the CONTEX-\nTUAL dataset in Figure 20.\nC.2\nHuman Evaluation Screenshot\nWe present the screenshot of the user interface used for human evaluation in Figure 21.\n18\n(a)\n(b)\n(c)\n(d)\nFigure 13: Examples of the Mobile Usage visual scenario\n(a)\n(b)\n(c)\n(d)\nFigure 14: Examples of the Web Usage visual scenario\nD\nGPT-4 Evaluation Prompt\nE\nAdditional Fine-grained Evaluation\nE.1\nTypes of Tasks\nWe compare the performance of the foundation models with varying types of tasks in Figure 22a\nusing GPT-4 evaluation. Specifically, we assess the quality of the responses when the instructions\nrequire the models to extract text or visual elements in the image (e.g., List the exercises where\nthe corresponding illustration showcases a single movement.). There are 285 such instances in the\nCONTEXTUAL dataset. While these tasks require complex perception and reasoning abilities, they do\nnot require additional operations on top of the information already presented in the image explicitly.\nWe observe that the humans achieve 75.1% on such instructions while the proprietary models GPT-4V\nand Gemini-Pro-Vision achieve 38.7% and 40.1%, respectively. This indicates that humans are very\ngood at identify the key information that needs to be extracted to respond to the instructions.\nIn addition, we assess the responses when the instructions require the models to go beyond information\nextraction, and perform math reasoning for the instruction (e.g., What is the total price for the two\ncars listed here?). There are 66 instances in the CONTEXTUAL dataset. We find that humans achieve\n62.1% on such tasks while the proprietary models GPT-4V achieve 27.3%, again highlighting at the\nlarge gap in their math reasoning.\nE.2\nVisual Scenes\nWe compare the performance of the foundation models with varying visual scenes (e.g., natural scenes\nand digital scenes) in Figure 22b. Majorly, shopping, navigation, and misc. natural scenes constitute\nnatural scenes, and web usage, mobile usage, abstract, infographics and time reading constitute\ndigital scenes. We find that humans achieve the highest performance in both the visual scenes i.e.,\n72.5% and 66.2% on natural scenes and digital scenes, respectively. In addition, we observe that\nGPT-4V achieve 47.8% and 48.2% on natural and digital scenes, respectively. Interestingly, we find\nthat Gemini-Pro-Vision and ShareGPT-4V-7B achieve higher performance on the natural scenes than\n19\n(a)\n(b)\n(c)\n(d)\nFigure 15: Examples of the Infographic visual scenario\n(a) Single Clock\n(b) Multiple Clocks\n(c) Calendar\n(d) Miscellaneous\nFigure 16: Examples of the Miscellaneous Natural Scenes visual scenario\nthe digital scenes. It indicates these models may not seen many examples with digital scenes during\ntheir pretraining. Thus, our CONTEXTUAL dataset highlights the gaps in the training data of the\nmodern LMMs.\nF\nDetailed Qualitative Analysis\nIn this section, we qualitatively analyze the responses of the different visual categories.\nF.1\nTime\nThis section provides qualitative analysis of the Time visual scenario and our findings across the\nmodels, as shown in Figures 23, 24, 25, 26.\nF.2\nShopping\nThis section provides qualitative analysis of the Shopping visual scenario and our findings across the\nmodels, as shown in Figures 27, 28, 29, 30..\nF.3\nNavigation\nThis section provides qualitative analysis of the Navigation visual scenario and our findings across\nthe models, as shown in Figures 31, 32, 33, 34.\nF.4\nAbstract\nThis section provides qualitative analysis of the Abstract visual scenario and our findings across the\nmodels, as shown in Figures 35, 36.\nF.5\nApplication Usage\nThis section provides qualitative analysis of the Application Usage visual scenario and our findings\nacross the models, as shown in Figures 37, 38, 39, 40.\n20\nInstruction: Describe the most similar product between the living room and the hip living room.\nReference Response: The orangish-brown colored single-seating sofa is the most similar between\nthe Living Room and the Hip Living Room.\nPrompt:\nYou are OCRGPT, an expert language model at responding to instructions posed for images. You have\nto respond to the instruction using the OCR Text of the image. More specifically, you will be given the\nfollowing:\n1. An instruction: This is a question, an imperative request, or something similar about the image that requires\na response.\n2. OCR Text: Text extracted from the image.\nYou have to respond with the Answer only.\nNOW YOUR TURN:\nInstruction : Provide the price of the upholstered dining set.\nOCR Text:\nBedroom Hip Bedroom From $99 / month From $109 / month Includes 5 items Includes 5 items Living\nRoom Hip Living Room. $59 / month $79 / month Includes 4 items Includes 4 items\nAnswer:\nGPT-4 w/ OCR: Both the Living Room and the Hip Living Room include 4 items.\nFigure 17: Example prompt for Aug LLM with GPT4: OCR provided without layout aware arrange-\nment of it\nF.6\nWeb Usage\nThis section provides qualitative analysis of the Web Usage visual scenario and our findings across\nthe models, as shown in Figures 41, 42, 43, 44.\nF.7\nInfographic\nThis section provides qualitative analysis of the Infographic visual scenario and our findings across\nthe models, as shown in Figures 45, 46, 47, 48.\n21\nInstruction: Describe the most similar product between the living room and the hip living room.\nReference Response: The orangish-brown colored single-seating sofa is the most similar between\nthe Living Room and the Hip Living Room.\nPrompt:\nYou are OCRGPT, an expert language model at responding to instructions posed for images. You have\nto respond to the instruction using the OCR Text of the image. More specifically, you will be given the\nfollowing:\n1. An instruction: This is a question, an imperative request, or something similar about the image that requires\na response.\n2. Layout Aware OCR Text: Text extracted from the image and arranged in the layout as it appears in the\nimage, such that the relative position of among the pieces of text appearing in the image is maintained.\nYou have to respond with the Answer only.\nNOW YOUR TURN:\nInstruction : Provide the cost of hiring the machine type depicted in the image for nine months.\nOCR TEXT:\nBedroom\nHip Bedroom\nFrom $99 / month\nFrom $109 / month\nIncludes 5 items\nIncludes 5 items\nLiving Room\nHip Living Room.\n$59 / month\n$79 / month\nIncludes 4 items\nIncludes 4 items\nAnswer:\nGPT-4 w/ Layout-aware OCR Response: The most similar product between the living room and the hip\nliving room is that both are available on a monthly basis, the living room costs $59/month and the hip\nliving room costs $79/month, both including 4 items.\nFigure 18: Example prompt for Aug LLM with GPT4: OCR in layout aware arrangement\nF.8\nMiscellaneous Natural Scenes\nThis section provides qualitative analysis of the Miscellaneous Natural Scenes visual scenario and\nour findings across the models, as shown in Figures 49, 50, 51, 52.\n22\nFigure 20: User interface of the human response collection.\nFigure 21: User interface of the human evaluation.\n23\nInstruction: Describe the most similar product between the living room and the hip living room.\nReference Response: The orangish-brown colored single-seating sofa is the most similar between\nthe Living Room and the Hip Living Room.\nPrompt:\nYou are OCRImageCaptionGPT, an expert language model at responding to instructions posed for images.\nYou have to respond to the instruction using the OCR Text of the image and the Image Caption. More\nspecifically, you will be given the following:\n1. An instruction: This is a question, an imperative request, or something similar about the image that requires\na response.\n2. Layout Aware OCR Text: Text extracted from the image and arranged in the layout as it appears in the\nimage, such that the relative position of among the pieces of text appearing in the image is maintained.\n3. Image Caption: Natural language description of the contents of the image, like objects, text, etc, and their\ninteraction with one another.\nYou have to respond with the Answer only.\nNOW YOUR TURN:\nInstruction : Provide the cost of hiring the machine type depicted in the image for nine months.\nOCR Text:\nBedroom\nHip Bedroom\nFrom $99 / month\nFrom $109 / month\nIncludes 5 items\nIncludes 5 items\nLiving Room\nHip Living Room.\n$59 / month\n$79 / month\nIncludes 4 items\nIncludes 4 items\nImage Caption: The image is an advertisement for a furniture store, showcasing a variety of furniture items.\nThe items are arranged in a grid-like pattern, with each item occupying its own square. The furniture includes\na couch, a dining table, a chair, a dresser, a bed, and a cabinet. The colors of the furniture range from brown\nto beige, with some items featuring black and white accents. The text in the image indicates that the furniture\nis available for purchase at prices ranging from 295 to 1945. The text also mentions that the furniture is of\n\"\"LUXURY FURNITURE DESIGN\"\". The overall layout and design of the advertisement suggest a focus on\nhigh-quality, stylish furniture.\nAnswer:\nGPT-4 w/ Layout-aware OCR + Caption Response: The most similar product between the Living Room\nand the Hip Living Room is in terms of the quantity of items. Both living room options include 4 items\neach - a couch, a coffee table, and a side table. The relative positioning and type of furniture also seem\nto be similar, although the styles and colors vary.\nFigure 19: Example prompt for Aug LLM with GPT4: OCR in layout aware arrangement + Image\nCaption\n24\nExtractive Reasoning\nMath Reasoning\n0\n10\n20\n30\n40\n50\n60\n70\nPercentage of Accepted Responses (%)\n23.2\n40.1\n38.7\n75.1\n15.2\n21.5\n27.3\n62.1\nShareGPT-4V-7B\nGemini-Pro-Vision\nGPT-4V\nHumans\n(a) Performance on different types of tasks.\nNatural Scenes\nDigital Scenes\n0\n10\n20\n30\n40\n50\n60\n70\nPercentage of Accepted Responses (%)\n28.6\n42.7\n47.8\n72.5\n16.4\n36.8\n48.2\n66.2\nShareGPT-4V-7B\nGemini-Pro-Vision\nGPT-4V\nHumans\n(b) Performance on natural and digital scenes.\nFigure 22: Additional fine-grained evaluation results.\nInstruction: Identify the object on the dial that shows the time 10:10.\nReference Response: The clock at 10:10 features an octopus on its dial.\nHuman Response: The only clock with an object on the dial at 10:10 is the white one with a tall purple\noctopus on it.\nGPT-4V Response: The clock with the image of an octopus on the face is showing the time 10:10.\nGemini Pro-Vision Response: An Octopus\nShareGPT-4V-7B Response: The object on the dial that shows the time 10:10 is an apple.\nWrong response\nLLaVA-1.5-13B Response: The object on the dial that shows the time 10:10 is a clock.\nWrong response\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction does not provide enough specific\ninformation to identify the object on the dial that shows the time 10:10.\nNeed for joint text-vision reasoning\nFigure 23: In this task, GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This can\nbe attributed to their ability for visual perception, text recognition, and context-sensitive text-vision\nunderstanding. Open source LMMs like ShareGPT4V and LLaVA-V-1.5 and GPT-4 w/ Layout-aware\nOCR + Caption Response (Augmented LLM approach) predict the wrong answer.\n25\nInstruction: What time is the sun dial indicating?\nReference Response: The time shown sun dial is between 1pm and 2pm.\nHuman Response: The sun dial Depicts approximately 1:30.\nGPT-4V Response: The sundial in the image is designed to tell time by casting a shadow on its face with the\ngnomon, which is the part sticking out that catches the sunlight. The shadow appears to be falling between\nthe 1 and 2 on the right side of the dial. Given the position of the shadow, it suggests that the time is\napproximately 1:30.\nGemini Pro-Vision Response: The sundial is indicating that it is between 1 and 2 o\u2019 clock.\nShareGPT-4V-7B Response: The sun dial is indicating 12:20.\nWrong response\nLLaVA-1.5-13B Response: The sun dial is indicating that it is 1:20 pm.\nGPT-4 w/ Layout-aware OCR + Caption Response: The image and the OCR text do not provide enough\ninformation to determine the exact time the sundial is indicating.\nNeed for joint text-vision reasoning\nFigure 24: In this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond\nto the instruction. This can be attributed to their ability for visual perception, text recognition, and\ncontext-sensitive text-vision understanding. GPT-4V is also able to explain the reasoning behind its\nresponse correctly. Amongst the open source LMMs, LLaVA-1.5 correctly responds to the instruction,\nsuggesting its ability to perform context-sensitive text-vision reasoning in this case. On the other,\nShareGPT4V and GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM approach)\npredict the wrong answer.\n26\nInstruction: Explain the idea behind using a color scheme for the different numbers in the calendar.\nReference Response: The calendar employs three distinct colors: grey for days prior to the current date,\nyellow highlights for the current day, and blue for the remaining days in the month.\nHuman Response: The grey numbers are the dates that already passed. The yellow color marked\ntoday\u2019s date. The blue color marked tomorrow and future dates..\nGPT-4V Response: The use of a color scheme in a calendar can help users quickly distinguish between\ndifferent types of days or events.For example:\nWeekdays vs. Weekend Days: Standard calendars often utilize color to differentiate between weekdays and\nweekends, with Saturdays and Sundays sometimes colored to stand out from Monday through Friday.\nCurrent Day: The current date might be highlighted in a unique color, as shown in the image where the\nnumber 10 is in orange. This helps users quickly identify the present day without searching.\nPast, Present, and Future Distinguishing: Some calendars use color gradients or different tones to\ndifferentiate between past, present, and future dates.\nPartly correct response but not grounded to this image.\nGemini Pro-Vision Response: The color scheme used in the calendar is to highlight the current day.\nThe current day is highlighted in yellow, making it easy to identify.\nParially correct response\nShareGPT-4V-7B Response: The use of a color scheme for the different numbers in the calendar serves a few\npurposes. First, it adds visual interest and aesthetic appeal to the calendar, making it more visually engaging\nand easier to read. The color-coded numbers can also help users quickly identify specific dates or events, as\nthe distinct colors can be more noticeable and memorable than the traditional black or gray numbers.\nResponse not grounded to this image\nLLaVA-1.5-13B Response: The idea behind using a color scheme for the different numbers in the calendar\nis to make it easier for users to identify and distinguish between specific dates or events. In the image, the\nnumber 10 is highlighted in yellow, which draws attention to that particular day.\nPartly correct response\nGPT-4 w/ Layout-aware OCR + Caption Response: The idea behind using a color scheme for the\ndifferent numbers in the calendar is to create a clear distinction between the types of information\npresented. In this case, the blue color is used for the days of the week and black for the dates. This\nway, users can easily differentiate between these two types of information, enhancing readability and\nunderstanding.\nResponse not grounded to this image\nFigure 25: In this task, all the models fail to ground their response to the image, with the best\nperforming ones being GPT4V, Gemini-Vision-Pro & LLaVA-1.5-13B and ShareGPT4V-7B & GPT-4\nw/ Layout-aware OCR + Caption producing a generic response\n27\nInstruction: How much time remains until the hour and minute hands align?\nReference Response: The hour and the minute hand will align at 12 o\u2019clock, 5 minutes from the\ndepicted time.\nHuman Response: 5 minutes.\nGPT-4V Response: To answer this, we\u2019ll have to calculate the relative speeds of the hour and minute hands\nand determine when they will next align.\nThe hour hand of a clock completes a full cycle every 12 hours, so it moves at a speed of 360 degrees per 12\nhours, which is 30 degrees per hour. The minute hand completes a full cycle every 60 minutes, so it moves at\na speed of 360 degrees per 60 minutes, which is 6 degrees per minute.\nAt 10 o\u2019clock, the hour hand is at the 300-degree mark, and the minute hand is at the 0-degree mark. The\nminute hand will need to \"catch up\" to the hour hand.\nEvery hour, the hour hand moves 30 degrees, while the minute hand moves 360 degrees. The relative speed\nis the difference in their speeds, so the minute hand catches up to the hour hand at a rate of 360 - 30 = 330\ndegrees per hour, or 5.5 degrees per minute (since there are 60 minutes in an hour). We need to find out how\nmany degrees the minute hand needs to catch up from the 10 o\u2019clock position. At 10 o\u2019clock, there is a 300 -\n0 = 300-degree difference between the two hands.\nDoes not ground the task to the image. Incomplete reasoning\nGemini Pro-Vision Response: The minute hand moves 6 degrees per minute, and the hour hand moves 0.5\ndegrees per minute. So, the minute hand moves 1.\nDoes not ground the task to the image. Incomplete reasoning\nShareGPT-4V-7B Response: The hour hand of the clock is pointing to 12, and the minute hand is pointing\nto 3. To determine how much time remains until the hour and minute hands align, we need to calculate the\ndifference between the current time and the time when the hands align.\nCurrent time: 12:03\nHour hand position: 12\nMinute hand position: 3\nTo find the time when the hands align, we need to add the number of minutes to the current hour. In this case,\nthe time when the hands align will be 12:03 + 3 = 12:06.\nSo, there are 3 minutes remaining until the hour and minute hands align.\nHallucination and incorrect reasoning\nLLaVA-1.5-13B Response: The hour hand on the clock is at 12, and the minute hand is at 10. To determine\nthe time remaining until the hour and minute hands align, we need to find the difference between the current\ntime and the next multiple of 12. Since the hour hand moves in increments of 12, the next multiple of 12 will\nbe at 1 o\u2019clock. The time difference between the current time (12:10) and the next multiple of 12 (1:00) is 1\nhour and 50 minutes. This means that there are 1 hour and 50 minutes left until the hour and minute hands\nalign.\nHallucination and incorrect reasoning\nGPT-4 w/ Layout-aware OCR + Caption Response: 30 minutes\nWrong response\nFigure 26: In this task, the closed source LMMs, GPT4V & Gemini-Vision-Pro fail to ground the\ntask to this image. Whereas, the open source LMMs, LLaVA-1.5-13B & ShareGPT4V-7B suffer\nfrom hallucination and incorrect reasoning. Lastly, the GPT-4 w/ Layout-aware OCR + Caption\nResponse (Augmented LLM approach) produces the wrong response.\n28\nInstruction: Provide the cost of hiring the machine type depicted in the image for nine months.\nReference Response: The machine shown in the image is a front-loading machine, and the rental\nfee for a period of 9 months amounts to INR 7200.\nHuman Response: Front load machines cost 7200 for 9 months.\nGPT-4V Response: The image displays a front load washing machine along with a price list for hiring\ndifferent types of washing machines. For hiring a front load washing machine for a period of 9 months, the\ncost would be INR 7,200 according to the given price list.\nGemini Pro-Vision Response: The cost of hiring the machine depicted in the image for nine months is INR\n4500.\nLack of context-sensitive text-vision reasoning\nShareGPT-4V-7B Response: The cost of hiring the washing machine for nine months is 4500 rupees.\nLack of context-sensitive text-vision reasoning\nLLaVA-1.5-13B Response: The cost of hiring the machine type depicted in the image for nine months is\n60,000 rupees.\nHallucination\nGPT-4 w/ Layout-aware OCR + Caption Response: The cost of hiring the depicted machine type (Top\nLoad) for nine months is 4500 units (currency not specified in the provided text)\nFigure 27: In this task, apart from GPT4V, all other models fail to produce the wrong response. This\ncan be attributed to the strong fine-grained visual perception and context-sensitive text-vision\nreasoning abilities of GPT4V.\n29\nInstruction: Describe the most similar product between the living room and the hip living room.\nReference Response: The orangish-brown colored single-seating sofa is the most similar between\nthe Living Room and the Hip Living Room.\nHuman Response: Both living rooms have similar orange chairs.\nGPT-4V Response: The most similar products between the \"Living Room\" and \"Hip Living Room\" sets\nappear to be the armchairs. Both sets feature a single armchair with a similar style and design,\ncharacterized by a mid-century modern aesthetic, with splayed legs and an angled backrest. In the\n\"Living Room\" set, the armchair has an orange-brown color, while in the \"Hip Living Room\" set, the\narmchair has a similar structure but comes in a different color scheme. The two chairs share a modern\nand sleek look that fits well within contemporary interior designs.\nGemini Pro-Vision Response: The most similar product between the living room and the hip living room is\nthe sofa. Both sofas are blue and have a similar shape.\nLack of fine-grained visual perception.\nShareGPT-4V-7B Response: The most similar product between the living room and the hip living room is\nthe couch. Both living room and hip living room couches are designed to provide comfort and serve as a\nfocal point in the room.\nLack of fine-grained visual perception.\nLLaVA-1.5-13B Response: The most similar product between the living room and the hip living room is\nthe couch. Both living rooms feature a couch as a central piece of furniture, providing a comfortable\nseating area for relaxation and socializing. The couches in both rooms have a similar design and serve\nthe same purpose, making them the most comparable items between the two living spaces.\nLack of fine-grained visual perception.\nGPT-4 w/ Layout-aware OCR + Caption Response: The most similar product between the Living Room\nand the Hip Living Room is in terms of the quantity of items. Both living room options include 4 items\neach - a couch, a coffee table, and a side table. The relative positioning and type of furniture also seem\nto be similar, although the styles and colors vary.\nLack of joint text-vision reasoning\nFigure 28: In this task, apart from GPT4V, all other models produce the wrong response. This can be\nattributed to the strong fine-grained visual perception and context-sensitive text-vision reasoning\nabilities of GPT4V. Further, GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM\napproach) shows inability to correctly reason with the provided dense image information as text,\nthereby, highlighting the need for joint vision and text reasoning.\n30\nInstruction: Calculate the maximum money the tour guide can make from the tourists shown in this image.\nReference Response: The picture depicts four potential tourists, one tour guide, and one bus driver.\nConsidering that the tour guide earns between $2 and $5 per person, the highest amount they can earn from\nfour tourists is $20.\nHuman Response: There are four tourists shown in the image. 5 x 4 = $20.\nGPT-4V Response: To calculate the maximum amount of money the tour guide can make from the tourists\nshown in this image in a day, we need to count the number of tourists and multiply that by the maximum rate\nof $5 per person per day. In the image, there are five tourists visible. So, the calculation is as follows: 5\ntourists x $5/tourist/day = $25/day. Therefore, the maximum amount of money the tour guide can make\nfrom the tourists shown in this image per day is $25.\nLack of fine-grained visual perception.\nGemini Pro-Vision Response: The image shows 5 tourists and the tour guide charges $2-5 per person\nper day. So the maximum money the tour guide can make is $25 per day\nLack of fine-grained visual perception.\nShareGPT-4V-7B Response: The tour guide can make a maximum of $25 per day from the tourists shown\nin the image. This is because the image shows five tourists standing next to each other, and the tour\nguide is holding an orange flag. The tour guide\u2019s maximum earnings would be $25 per day, per person,\nconsidering there are five tourists in the group.\nLack of fine-grained visual perception.\nLLaVA-1.5-13B Response:To calculate the maximum money the tour guide can make from the tourists,\nwe need to determine the number of tourists and the cost per person. In the image, there are five people\nstanding in front of the bus, and the tour guide is offering a $25-$5 per person per day deal. Since there\nare five people, the maximum amount of money the tour guide can make is $25 x 5 = $125 per day.\nLack of fine-grained visual perception.\nGPT-4 w/ Layout-aware OCR + Caption Response: The maximum money the tour guide can make\nfrom the five tourists shown in this image is $25 per day ($5 per person per day).\nLack of fine-grained visual perception.\nFigure 29: In this task, all models produce the wrong response. This can be attributed to the lack of\nfine-grained visual perception for this task.\n31\nInstruction:Discuss the features of a camera being held vertically with a single hand.\nReference Response:\nKonica Minolta is the camera held vertically in one hand, specifically the\nBig Mini BM-201 model, priced at $130, C113, \u00a3102, or IDR 1,893,775.\nHuman Response: The camera held by a single hand vertically is a Big Mini and it features a price tag\nof $130.\nGPT-4V Response: The image appears to be a collage of various cameras with a person holding one camera\nvertically in the top right corner. Since we\u2019re focusing on the vertically held camera, here are some general\nfeatures and considerations for using a camera in a vertical orientation with a single hand.\nHallucination\nGemini Pro-Vision Response: The camera being held vertically with a single hand is a Fujifilm Instax\nMini 7s. It is a compact and lightweight instant camera that is perfect.\nWrong response\nShareGPT-4V-7B Response: The camera being held vertically with a single hand is a Canon EOS 5D\nMark IV. This camera is known for ...\nHallucination\nLLaVA-1.5-13B Response: A camera being held vertically with a single hand is typically a compact or\npoint-and-shoot camera ...\nHallucination\nGPT-4 w/ Layout-aware OCR + Caption Response: The image does not specify the features of a camera\nbeing held vertically with a single hand. However, a camera designed for this use would typically be\nlightweight and compact ... Hallucination and need for joint text-vision reasoning\nFigure 30: In this task, all models except Gemini-Vision-Pro, hallucinate. All of them show lack of\nvisual knowledge to solve this task.\n32\nInstruction: Determine the triangular-shaped signs.\nReference Response:\nTriangular-shaped signs are Right Curve, Left Curve, Cross Road, Speed\nBreaker, and School.\nHuman Response: The triangular shaped signs are Right Curve, Left Curve, Cross Road, and Speed\nBreaker, in order from left to right, in the top row. The the second row, the last image is triangular and\nis for School.\nGPT-4V Response: The triangular shaped signs in the image are as follows: 1. The second sign on the top\nrow, indicating a \"RIGHT CURVE.\" 2. The third sign on the top row, indicating a \"LEFT CURVE.\" 3.\nThe fourth sign on the top row, indicating a \"CROSS ROAD.\" 4. The fifth sign on the top row, with an icon\nof a bump, indicating a \"SPEED BREAKER.\" 5. The third sign on the bottom row, with an icon of two\npeople, indicating a \"SCHOOL\" zone.\nGemini Pro-Vision Response: There are 6 triangular-shaped signs: 1. Right curve 2. Left curve 3. Cross\nroad 4. Speed breaker\nPartially correct response and hallucinations.\nShareGPT-4V-7B Response: The triangular-shaped signs in the image are the \"No Parking\" sign and\nthe \"No Entry\" sign..\nHallucinations along with wrong response.\nLLaVA-1.5-13B Response:The triangular-shaped signs in the image are warning signs, which are\ntypically used to convey important information or instructions to drivers and pedestrians ....\nHallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The triangular-shaped signs in the image are: 1. STOP\n2. CAUTION 3. RIGHT CURVE 4. LEFT CURVE 5. CROSS ROAD 6. SPEED BREAKER 7. TURN\nROUND 8. RIGHT CAUTION 9. SPEED LIMIT 10. NO PARKING 11. NO ENTRY 12. SCHOOL 13.\nBARRIER AHEAD\nHallucinations along with correct response.\nFigure 31: In this task GPT4V produces the correct response, Gemini-Vision-Pro produces a partially\ncorrect answer with minor hallucinations. In all other models, either the model hallucinates,\nproduces wrong answer or both.\n33\nInstruction:Itemize the destinations and exits one can take while driving on the left.\nReference Response:\nWhen driving in the left lane, the options for directions and exits include\ntaking Exit 24C for South Quincy on Route 93 or Exits 24 A-B, which lead to South Station or North Concord\n(NH on Route 93), respectively.\nHuman Response: 24A-B S. Station, North 93, Concord NH\nGPT-4V Response: If you\u2019re driving on the left and considering your potential destinations and exits\nbased on the signage in the image, here they are:\nExit 24C: Take this exit if your destination is Quincy; it\u2019s an \"EXIT ONLY\" lane.\nExits 24A-B: Options available from these exits: 24A South Station, 24B North 93, Concord NH\nGemini Pro-Vision Response: Take Exit 24C for Quincy. Take Exit 24A-B for South Station and North\nfor Concord, NH.\nShareGPT-4V-7B Response:While driving on the left, there are several destinations and exits to consider.\nThese include Quincy, South Boston Logan Airport, Concord NH, and S Station. Additionally, there\nare exits for 24A, 24B, 93, and 90. These exits provide drivers with the opportunity to reach various\nlocations and navigate through the city effectively.\nHallucinations\nLLaVA-1.5-13B Response:While driving on the left, there are several destinations and exits available\nfor drivers to take. These include: Exit 24A-B: This exit is for drivers heading towards the south station,\nExit 24C: This exit is for drivers heading towards Quincy, Exit 24D: This exit is for drivers heading\ntowards Concord, NH, Exit 24E: This exit is for drivers heading towards South Boston, Exit 24F: This\nexit is for drivers heading towards Logan Airport. These exits are located under a highway overpass,\nand drivers can choose the appropriate exit based on their desired destination.\nHallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The destinations and exits one can take while\ndriving on the left are: South 93, Concord NH, and South Boston Logan Airport.\nHallucinations\nFigure 32: In this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond to\nthe instruction. This can be attributed to their ability for visual perception, text recognition, and\ncontext-sensitive text-vision understanding. All other models hallucinate their responses.\n34\nInstruction: Specify the colors of the rocket, space shuttle, and external tank.\nReference Response: The space shuttle is depicted in yellow, the rockets are depicted in grey, and\nthe external tank is depicted in red.\nHuman Response: The rocket is gray. The space shuttle is yellow. The external tank is red.\nGPT-4V Response: In the image, the solid rocket boosters of the space shuttle are colored in red and light\ngray, with the red at the top and the gray on the bottom. The external tank is colored orange. The main\nbody of the space shuttle, also known as the orbiter, is primarily white with black on its nose and wing\nedges. There is also a bit of red on the tail section. The image depicts various stages of a space shuttle\nflight sequence from pre-launch to landing.\nHallucinations\nGemini Pro-Vision Response: The rocket is red and white, the space shuttle is white, and the external\ntank is orange.\nHallucinations\nShareGPT-4V-7B Response: The rocket is red, the space shuttle is yellow, and the external tank is\norange.\nPartially correct response with Hallucinations\nLLaVA-1.5-13B Response: The rocket is red, the space shuttle is yellow, and the external tank is white.\nPartially correct response with Hallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The rocket is red and yellow. The space shuttle is\nyellow and red. The image did not specify the color of the external tank.\nHallucinations\nFigure 33: In this task, all models hallucinate, due to lack of grounding the response to the given\nimage & fine-grained visual perception and using prior visual knowledge. Notably, the open-source\nLMMs, LLaVA-1.5-13B & ShareGPT-4V-7B provide partially accurate responses, suggesting a\nreduced reliance on prior visual knowledge, possibly due to not having seen it.\n35\nInstruction: Pick out the airlines immediately in front and behind the one featuring a flying bird in yellow\nand blue.\nReference Response: The airline right in front of the airline with a bird logo is SriLankan Airlines, and the\none situated behind it is Air France.\nHuman Response: Air France and Sri Lankan.\nGPT-4V Response: The airline with a flying bird on its fuselage is \"SriLankan Airlines.\" Directly in\nfront of it, the airplane features the livery of \"Lufthansa,\" and right behind the SriLankan Airlines\naircraft, the airplane with the dark blue tail and the text \"Condor\" represents Condor Airlines.\nLack of fine-grained visual perception\nGemini Pro-Vision Response: In front: Condor, Behind: Sri Lankan\nLack of fine-grained visual perception\nShareGPT-4V-7B Response: The airlines in front of the one with the flying bird are Condor and\nSriLankan. The airline behind the one with the flying bird is Lufthansa.\nLack of fine-grained visual perception\nLLaVA-1.5-13B Response: In the image, the airline with a flying bird logo is Sri Lankan Airlines. The\nairline in front of it is Condor, and the airline behind it is Air France.\nLack of fine-grained visual perception\nGPT-4 w/ Layout-aware OCR + Caption Response: The airlines immediately in front and behind the\none featuring a flying bird, Condor, are Srilankan and Lufthansa respectively.\nLack of fine-grained visual perception\nFigure 34: In this task, all models produce the wrong response, due to lack of fine-grained visual\nperception.\n36\nInstruction: Contextualize the saying with the visual contents of the image.\nReference Response: The saying \"Work smart, not hard\" is illustrated in the accompanying image.\nIt depicts multiple people pushing square wooden blocks across a flat surface, while one person carves a\nwooden block into a sphere and easily transports it across the same surface.\nHuman Response: The person working intelligently turned their square into a circle so they could roll\nit. They aren\u2019t working hard, but intelligently.\nGPT-4V Response: The image depicts a series of individuals pushing cubes across a flat surface, which\nappears to be a difficult and labor-intensive task. In contrast, one individual is shown pushing a\nspherical object\u2014a ball\u2014which naturally rolls and thereby requires much less effort to move. The\nsaying, \"Don\u2019t Work Hard, Work Intelligently,\" encapsulates the idea that the approach to a task can\ngreatly affect the amount of effort required to complete it. The person with the ball has found a more\nefficient method, using a shape that is easier to move, demonstrating intelligence in their approach to\nthe work, as opposed to the others who are working hard but not necessarily in the most effective way\nwith the cube shapes. The image illustrates the principle that smart strategies can often lead to better\nresults than brute force or physical effort alone.\nGemini Pro-Vision Response: The image shows a group of people pushing square blocks. Only one\nperson is pushing a round object, which is much easier to push than a square block.\nShareGPT-4V-7B Response: The image portrays a scene of six men, each engaged in a different activity,\nall working together to move a large white ball. The men are dressed in suits and are using various\ntools, such as shovels and a hammer, to maneuver the ball. The setting appears to be a desert-like\nenvironment, with the men and the ball being the main focus of the image. The phrase \"Dance with the\ndevil, pay the price\" is displayed at the bottom of the image ...\nHallucination\nLLaVA-1.5-13B Response:The image features a painting of a man pushing a large cube on wheels, which\nis a representation of the saying \"Don\u2019t waste your time.\" The man is depicted in various positions,\nincluding pushing the cube, pulling it, and even running away from it ...\nHallucination\nGPT-4 w/ Layout-aware OCR + Caption Response: The saying \"Don\u2019t Work Hard, Work Intelligently\"\nis contextualized by the visual representation of people engaged in a seemingly endless task of moving\nlarge cubes. This serves as a metaphor to highlight the concept behind the saying. The act of pushing,\npulling, and lifting these blocks signifies the traditional \u2019hard work.\u2019 However, the saying places\nemphasis on the need to work \u2019intelligently\u2019 rather than merely putting in immense effort. In the\ncontext of the image, working intelligently might imply finding smarter strategies or techniques to move\nthe cubes, possibly with less effort or more efficiency...\nHallucination\nFigure 35: In this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond to\nthe instruction. This can be attributed to their ability for visual perception, text recognition, and\ncontext-sensitive text-vision understanding. All other models hallucinate their responses.\n37\nInstruction: Comment on the funny aspect of the comic strip.\nReference Response: In this four-panel comic, the lawyer (seal) asks the defendant (killer whale)\nto identify itself, but the whale hesitates because admitting to being a \"killer\" would mean confessing to the\ncrime. In the final panel, the defense lawyer (dolphin) exclaims \"objection, \" evoking humor through this\nconundrum.\nHuman Response: The whale depicted in the strip is a \"Killer\" Whale.\nGPT-4V Response: This comic strip might be considered particularly funny for a few reasons: An-\nthropomorphism: The characters are animals dressed in human clothes and partaking in a court trial,\nwhich is inherently comical. Visual humor: The expressions and body language of the animals add to\nthe comedic effect, as they mimic human courtroom drama. Irony: It\u2019s humorous and ironic because\nwhales obviously cannot speak, let alone participate in a legal procedure, making the whole situation\nabsurdly funny.\nLack of context-sensitive text-vision reasoning\nGemini Pro-Vision Response: The comic strip is funny because it plays on the fact that whales are often\nseen as intelligent and articulate creatures, but in this case, the whale is unable to do so.\nLack of context-sensitive text-vision reasoning\nShareGPT-4V-7B Response: The viewer considers this comic strip to be the funniest thing they have\never seen because it features a humorous and unexpected scenario. In the comic strip, a man and a seal\nare depicted in a conversation, with the seal sitting in a chair and holding a briefcase. The comic strip\u2019s\nunexpected twist, where a seal is dressed in a suit and holding a briefcase, creates a sense of humor and\namusement that the viewer finds entertaining.\nLack of context-sensitive text-vision reasoning\nLLaVA-1.5-13B Response: A viewer might consider this comic strip to be the funniest thing they have\never seen because it features a unique and unexpected combination of elements. The strip includes a\ncourtroom scene with a seal in a suit, a dolphin in a witness box, and a whale in a tie. This unusual and\nabsurd scenario creates a humorous and unexpected twist on a serious topic, which can be seen as a\nform of satire or parody.\nLack of context-sensitive text-vision reasoning\nGPT-4 w/ Layout-aware OCR + Caption Response: The viewer might find this comic strip to be the\nfunniest thing they have ever seen due to the absurdity and humour of the situation: a man and a fish\nhaving a legal discourse. It is unusual and humorous to imagine a fish in a court scene, especially as the\nfish grows a beard and wears a suit in one panel. This surprise element and the incongruity between\nthe characters and the situation possibly make this situation funny to the viewer.\nHallucination and lack of joint text-vision reasoning\nFigure 36: In this task, all models produce the wrong response, due to lack of context-sensitive\ntext-vision reasoning. Specifically, for augmented LLM approach, it is lack of joint text-vision\nreasoning.\n38\nInstruction: How many times has the user drank water today?\nReference Response: 1 times since only the option with \"1\" on it is selected\nHuman Response: Once\nGPT-4V Response: Based on the screenshot provided, it appears that the user has marked one instance of\ndrinking water today. The user seems to be using an application that tracks habits, and the \"Drink\nWater\" card shows one filled circle, indicating one time the user has logged drinking water for the\ncurrent day.\nGemini Pro-Vision Response: The user has not drank any water today.\nLack of fine-grained visual perception\nShareGPT-4V-7B Response: The user has drank water three times today.\nLack of fine-grained visual perception\nLLaVA-1.5-13B Response: The user has drank water three times today.\nLack of fine-grained visual perception\nGPT-4 w/ Layout-aware OCR + Caption Response: The user has drank water 3 times today.\nLack of fine-grained visual perception\nFigure 37: In this task GPT4V produces the correct response, and all other models hallucinate,\npossibly due to lack of fine-grained visual perception ability.\n39\nInstruction: Highlight the settings that are switched off by the user.\nReference Response: Send by Enter and Deny Friend Requests\nHuman Response: Switched off: Send by Enter and Deny Friend Requests\nGPT-4V Response: The settings switched off in this image are \"Send by Enter\" and \"Deny Friend\nRequests.\" These options are indicated by grayed-out toggle switches, while the options that are\nswitched on are indicated by green-colored toggle switches.\nGemini Pro-Vision Response: 1. Send by Enter 2. Deny Friend Requests\nShareGPT-4V-7B Response: In the image, the user has switched off the \"Last Seen\" and \"Everybody\"\nsettings. The \"Last Seen\" setting is turned off, which means the user\u2019s last seen time will not be visible\nto others. The \"Everybody\" setting is also turned off, which means the user\u2019s messages will not be\nvisible to everyone, and the user will not receive messages from everyone.\nHallucinates\nLLaVA-1.5-13B Response: In the image, there is a cell phone with several settings turned off. These\nsettings include the speaker, preloaded videos, and instant video. The speaker setting is turned off,\nwhich means that the user may not be able to hear notifications or ringtones. The preloaded videos\nsetting is also turned off, which means that the user may not be able to watch videos that were pre-\ninstalled on the device. Lastly, the instant video setting is turned off, which means that the user may not\nbe able to use the instant video feature on the device.\nHallucinates\nGPT-4 w/ Layout-aware OCR + Caption Response: The \"Deny Friend Requests\" setting is switched off\nby the user.\nPartially correct response\nFigure 38: In this task, the closed source LMMs, GPT4V & Gemini-Vision-Pro fail to ground the\ntask to this image. Whereas, the open source LMMs, LLaVA-1.5-13B & ShareGPT4V-7B suffer\nfrom hallucination and incorrect reasoning. Notably, the GPT-4 w/ Layout-aware OCR + Caption\nResponse (Augmented LLM approach) produces a partially correct response. On analyzing the\nvisual information provided to GPT4 for reasoning, we find the captions contain information about\n\"Deny Friend Requests\" being set to false. The captions are generated using ShareGPT-4V-7B, but\nwhen given this task, it hallucinates the answer.\n40\nInstruction: What is the difference between the illustration of Romance and Romance II?\nReference Response:\nThe logo for \"Romance\" features a heart with an unshaded interior, while\nthe logo for \"Romance II\" showcases a heart filled with white shading inside.\nHuman Response: Romance II has a filled in heart vs not filled in.\nGPT-4V Response: The illustration of \"Romance\" and \"Romance II\" in the image both feature a heart\nicon, which is a widely recognized symbol of love and affection. The difference between the two appears\nto be simply numerical, possibly indicating a continuation or second set of phrases related to romance\nwithin the language learning app \"Simply Learn Tagalog.\" There\u2019s no visual difference between the\nicons themselves, suggesting the distinction is in content rather than any visual or thematic change.\nIncorrect reasoning\nGemini Pro-Vision Response: The difference between the two is that the first one has one heart, while\nthe second one has two hearts.\nHallucination\nShareGPT-4V-7B Response: The difference between the illustrations of Romance and Romance II lies\nin the design of the heart symbol. In the first illustration, the heart symbol is depicted as a simple,\ntwo-dimensional shape. In contrast, the heart symbol in the second illustration, Romance II, is more\ndetailed and intricately designed.\nPartially correct response but not completely grounded to the image\nLLaVA-1.5-13B Response: The illustration of Romance and Romance II in the app are different in\nterms of the visual representation. Romance is represented by a heart symbol, while Romance II is\nrepresented by a heart with a plus sign on it.\nHallucination\nGPT-4 w/ Layout-aware OCR + Caption Response: The image does not provide distinct illustrations for\n\"Romance\" and \"Romance II\". Only one icon related to romance is described in the image caption - a\nrose. The instruction might be referring to different elements, but based on the provided OCR text and\nimage caption, there is not enough information to distinguish between \"Romance\" and \"Romance II\".\nNeed for joint text-vision reasoning\nFigure 39: In this task, the closed source LMMs, GPT4V produces wrong response due to incorrect\nreasoning. to ground the task to this image. Gemini-Vision-Pro & LLaVA-1.5-13B halluci-\nnate in their responses. GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM\napproach) produces the wrong answer, possibly due to lack of joint text-vision reasoning. However,\nShareGPT4V-7B produces a partially correct response but is unable to ground it completely to the\nimage.\n41\nInstruction:List the words formed by all the grafitti stickers.\nReference Response: Lol, Love, Thanks, Wtf, Ugh, Awesome, Oops, Hot, Dope\nHuman Response: The words are LOL, love, Thanks, WTF, UGH, awesome, oops, HoT and Dope.\nGPT-4V Response: The graffiti stickers each have a different word on them. From what I can discern, the\nwords are: LOVE, THANKS, WIZ, FISH, AWESOME, HOT, DOPE.\nHallucinations along with partial correct response\nGemini Pro-Vision Response: Home, Auto, Wut, Uhh Love,Thanks,Awesome,Dope,Hot\nHallucinations along with partial correct response\nShareGPT-4V-7B Response: Love, Awesome, Dope, Facebook, Google, Android Beam\nHallucinations along with partial correct response\nLLaVA-1.5-13B Response: The words formed by all the grafitti stickers are \"LOVE\".\nPartially correct response\nGPT-4 w/ Layout-aware OCR + Caption Response: The words formed by the graffiti stickers are\n\"GRAFONSNCINER\", \"UTH\", \"AVESOS\".\nHallucinations\nFigure 40: In this task, all models show a varying ability to recognise stylistic text, with GPT4V\nbeing the best and surprisingly GPT-4 w/ Layout-aware OCR + Caption Response (Augmented LLM\napproach) is the worst. This underscores the need of open-ended text recognition models, that can\nunderstand a variety of styles.\n42\nInstruction: Name the country whose portrait has a group of people on the road.\nReference Response: Cuba\nHuman Response: Cuba\nGPT-4V Response: The image with a group of people on the road is associated with \"Material World:\nCuba.\"\nGemini Pro-Vision Response: Bhutan\nWrong response\nShareGPT-4V-7B Response: Mexico\nWrong response\nLLaVA-1.5-13B Response: Bhutan\nWrong response\nGPT-4 w/ Layout-aware OCR + Caption Response: The image does not provide specific information on\nwhich country\u2019s portrait has a group of people on the road.\nLack of joint text-vision reasoning\nFigure 41: In this task GPT4V produces the correct response, and all other models hallucinate,\npossibly due to lack of fine-grained visual perception ability. Specifically, GPT-4 w/ Layout-aware\nOCR + Caption Response (Augmented LLM approach) lacks joint text-vision reasoning ability to\nrespond to this task.\n43\nInstruction: Which story can be read without subscription under Today\u2019s pick?\nReference Response: The story with the title \"Science: It looked like a bizarre alignment of mete-\nors. It was something else\" because it does not have lock sign which other two stories have.\nHuman Response: It looked like a bizarre alignment of meteors. It was something else.\nGPT-4V Response: I\u2019m sorry, but I can\u2019t provide real-time or current information about which specific\narticles are available without a subscription on external websites such as National Geographic. You\nwould need to visit their website directly to see which stories are accessible for free.\nLack of instruction understanding and grounding to the image\nGemini Pro-Vision Response: The story titled \"It looked like a bizarre alignment of meteors. It was\nsomething else\" can be read without a subscription.\nShareGPT-4V-7B Response: The story \"Here\u2019s how wildfires get started and how to stop them\" can be\nread without subscription under Today\u2019s pick.\nLack of context-sensitive text-vision reasoning\nLLaVA-1.5-13B Response: Under Today\u2019s pick, the story \"Here\u2019s how wildfires get started and how\nto stop them\" can be read without subscription. This story is available for full access, while the other\nstories require a subscription to National Geographic.\nLack of context-sensitive text-vision reasoning\nGPT-4 w/ Layout-aware OCR + Caption Response: The stories that can be read without subscription\nunder Today\u2019s Picks are: \"Ancient altar reveals the power of Arab desert merchants\" under the\ncategory \"History & Culture\", \"It looked like a bizarre alignment of meteors. It was something else.\"\nunder the category \"Science\", \"Here\u2019s how wildfires get started-and how to stop them\" under the\ncategory \"Environment\", and \"What was lost in Lahaina, a jewel of the Hawaiian Kingdom\" under\nthe category \"History & Culture\".\nCorrect response with hallucinations\nFigure 42: In this task Gemini-Vision-Pro produces the correct response. GPT4V fails to understands\nthe task and ground it to the give image. The open source LMMs, LLaVA-1.5-13B & ShareGPT4V-7B\nproduce the wrong response, due to lack of context-sensitive text-vision reasoning. GPT-4 w/ Layout-\naware OCR + Caption Response (Augmented LLM approach) hallucinates along with producing the\ncorrect response.\n44\nInstruction: Give the jersey number of the leftmost player on the ground from the white team.\nReference Response: 16\nHuman Response: The jersey number is 16\nGPT-4V Response: I\u2019m sorry, but I can\u2019t provide specific details such as jersey numbers for real\nindividuals in images due to privacy reasons and the limitations you\u2019ve mentioned.\nGemini Pro-Vision Response: The jersey number is 10.\nHallucinates\nShareGPT-4V-7B Response: The leftmost player on the ground from the white team is wearing a jersey\nwith the number 44.\nHallucinates\nLLaVA-1.5-13B Response: The leftmost player on the ground from the white team is wearing a jersey\nwith the number 44.\nHallucinates\nGPT-4 w/ Layout-aware OCR + Caption Response: The image does not provide specific jersey numbers\nfor the players.\nHallucinates\nFigure 43: In this task GPT4V does not produce a response. All other models produce a wrong\nresponse due to lack of strong text recognition capabilities. Specifically, GPT-4 w/ Layout-aware\nOCR + Caption Response (Augmented LLM approach) lacks joint text-vision reasoning capabilities\nto respond to the task.\n45\nInstruction:Retrieve all the words with white font on a red background.\nReference Response: 1. Birmingham, 2. Subscribe\nHuman Response: The words with white font on a red background are: \u2018BIRMINGHAM\u2019 and\n\u2018SUBSCRIBE\u2019.\nGPT-4V Response: The words with white font on a red background are \"Every story. Every day.\" and\n\"First month free.\"\nHallucinates\nGemini Pro-Vision Response: Advertisement, theLede, Every story, Every day, First, Your Birmingham\nnews is on the Lede, SUBSCRIBE\nHallucinations along with partial correct response\nShareGPT-4V-7B Response: Birmingham, Alabama\nHallucinations along with partial correct response\nLLaVA-1.5-13B Response: Birmingham, Alabama, The Lede, Alabama football, Obama, starting, DB,\ndealing, with injury, after free president, Alabama and Auburn fans, sign up today.\nHallucinations along with partial correct response\nGPT-4 w/ Layout-aware OCR + Caption Response: BIRMINGHAM, Advertisement, the, Lede., Every\nstory.Every day, Your, Birmingham, news is, on the Lede., the, Lede, SUBSCRIBE, Birmingham native,\nDaniel Scheinert, and, All at Once\u2019, First month free, ALABAMA FOOTBALL, Alabama starting DB\ndealing with injury after freak accident, Published: Jan. 01, 2018, 3:01 p.m, Kevin Scarbinsky, The\nessential newsletter for Alabama and Auburn fans\nHallucinations along with correct response\nFigure 44: In this task, all models hallucinate, due to lack grounding to the given image.\n46\nInstruction: What is the message corresponding to the eyes symbol?\nReference Response:\nIt says that 61% of the surveyors believe that there won\u2019t be any crowd\nwhere they\u2019re going to eat out.\nHuman Response: 61% of respondents want to know there won\u2019t be crowds where they\u2019re going to eat.\nGPT-4V Response: The eyes symbol corresponds to the message: \"61% of diners feel it\u2019s extremely\nimportant to know there won\u2019t be crowds where they\u2019re going.\" This indicates that a significant number\nof diners prioritize knowledge about the expected crowd levels at dining venues as a factor in their\ncomfort with eating out again in the context of the COVID-19 pandemic.\nGemini Pro-Vision Response: The message corresponding to the eyes symbol is \"Know there won\u2019t be\ncrowds where they\u2019re going\". 61% of diners surveyed said this\nShareGPT-4V-7B Response: The message corresponding to the eyes symbol is \"They\u2019re willing to leave\na higher tip.\" This statement suggests that a significant number of diners are willing to leave a bigger\ntip than the standard tip, which is typically 15-20%.\nWrong response\nLLaVA-1.5-13B Response: The message corresponding to the eyes symbol is that diners are willing\nto pay more for food and beverage items. This indicates that customers are interested in high-quality\ndining experiences and are willing to invest more money in their meals. The eyes symbol, along with the\nother symbols, represents the various aspects of dining preferences and experiences that customers\nvalue.\nHallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The message corresponding to the eye symbol\nis: \"To feel comfortable gathering in public spaces again, diners feel it\u2019s extremely important to See\npeople wearing masks and using hand sanitizer\" with 63% agreement, \"Know there won\u2019t be crowds\nwhere they\u2019re going\" with 61% agreement, and to \"Receive a vaccine for COVID-19\" with also 61%\nagreement.\nHallucinations along with correct response\nFigure 45: In this task, closed-source LMMs GPT-4V and Gemini Pro-Vision correctly respond to\nthe instruction. This can be attributed to their ability for visual perception, text recognition, and\ncontext-sensitive text-vision understanding. All other models hallucinate their responses.\n47\nInstruction: Calculate the difference in the percent of respondents who have confidence and do not have\nconfidence in Obama from the country with a doll in their stamp.\nReference Response: Country is Poland.\nPercent of respondents who approve is 47 and the per-\ncent who do not have confidence is 38. The difference of the two is 9 percent.\nHuman Response: The country is Poland and the difference between the respondents who have and do\nnot have confidence in Obama is 9%.\nGPT-4V Response: To find the country with a doll in its stamp, I\u2019ll first identify that country and then\ncalculate the difference in percentage of respondents who have confidence and do not have confidence\nin Obama from that country. Upon inspection, the country represented by a stamp with a doll on it is\nPoland. According to the data provided: 47% of respondents have a lot of confidence or some confidence\nin Obama\u2019s ability to do the right thing in world affairs. 38% of respondents do not have too much\nconfidence or no confidence at all in Obama\u2019s ability to do the right thing in world affairs. To calculate\nthe difference, we subtract the smaller percentage from the larger percentage: 47% (Confidence) - 38%\n(No confidence) = 9% Therefore, the difference in the percent of respondents who have confidence and\ndo not have confidence in Obama from Poland is 9%.\nGemini Pro-Vision Response: For the country with the doll stamp (Nigeria), 85% of respondents have\nconfidence in Obama, and 13% do not have confidence in\nHallucinations\nShareGPT-4V-7B Response: The country with the highest percentage of people with confidence in\nObama is Great Britain, with 80% of the respondents having confidence. On the other hand, the\ncountry with the lowest percentage of people with confidence in Obama is Pakistan, with only 20% of\nthe respondents having confidence. The difference in the percentages between two countries is 60%.\nHallucinations\nLLaVA-1.5-13B Response: In the image, the country with a doll in their stamp is the United States. The\npercentage of respondents who have confidence in Obama is 63%, while the percentage of respondents\nwho do not have confidence in Obama is 37%. The difference between two percentages is 63 - 37 = 26%.\nHallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The image provided, unfortunately, doesn\u2019t contain\ninformation showing a specific country with a doll in their stamp.\nHallucinations\nFigure 46: In this task, GPT-4V correctly responds to the instruction. This can be attributed to their\nability for math and context-sensitive text-vision understanding. All other models hallucinate\ntheir responses.\n48\nInstruction: Highlight the states where the percent of positive tests is greater than 30%.\nReference Response: WA and MS\nHuman Response: Washington and Mississippi\nGPT-4V Response: If you need to highlight states where the percent of positive tests is greater than\n30% according to the provided image, you can use a graphic editing tool to draw attention to these\nstates. Typically, they would be marked with darker shades on this type of heatmap scale. As I see the\nimage, no state is represented with a shade indicating a positivity rate higher than 30%, but due to\nvariations in screen displays and image quality, please verify using the source data or a more detailed\nversion of the map for accurate information.\nDoes not ground the response to the image\nGemini Pro-Vision Response: The states where the percent of positive tests is greater than 30% are:\nLouisiana, Alabama, Georgia, and Mississippi Hallucinations along with partial correct response\nShareGPT-4V-7B Response: The states where the percent of positive tests is greater than 30% are:\nNew York, New Jersey, Pennsylvania, Maryland, Virginia, Massachusetts, Rhode Island, Delaware,\nConnecticut, Ohio, Indiana, Michigan, Illinois, Minnesota, Wisconsin, Iowa, Nebraska, South Dakota,\nNorth Dakota, West Virginia, Kentucky, Tennessee, Alabama, Georgia, Florida, Arkansas, Oklahoma,\nTexas, Washington, D.C., and Mississippi. Hallucinations along with partial correct response\nLLaVA-1.5-13B Response: In the image, there are several states where the percent of positive tests is\ngreater than 30%. These states include: California: 31.7%, Texas: 30.6%, Florida: 30.5%, New York:\n30.4%, Pennsylvania: 30.3%\nHallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The image doesn\u2019t provide specific information on\nwhich states have a percent of positive tests greater than 30%.\nLack of joint text-vision reasoning abilities\nFigure 47: In this task, the closed source LMMs, GPT4V does not ground the response to the\nimage.Gemini-Vision-Pro, ShareGPT-4V-7B & LLaVA-1.5-13Bhallucinate in their responses. GPT-4\nw/ Layout-aware OCR + Caption Response (Augmented LLM approach) produces the wrong\nanswer, possibly due to lack of joint text-vision reasoning.\n49\nInstruction:How many production methods are used under pour-over/drip category?\nReference Response: 10\nHuman Response: There are 10 methods in the pour-over/drip category.\nGPT-4V Response: In the pour-over/drip category of this coffee chart, there appear to be six production\nmethods used. These methods are as follows: Auto Drip, Cone, Flat Bottom, AeroPress, Woodneck,\nChemex\nWrong response\nGemini Pro-Vision Response: There are 8 production methods listed under pour-over/drip category.\nWrong response\nShareGPT-4V-7B Response: There are two production methods used under the pour-over/drip category:\nFrench press and drip coffee maker.\nWrong response\nLLaVA-1.5-13B Response: There are three production methods used under the pour-over/drip category.\nWrong response\nGPT-4 w/ Layout-aware OCR + Caption Response: The OCR text does not provide specific information\non the number of production methods used under the pour-over/drip category.\nLack of joint text-vision reasoning\nFigure 48: In this task, the all LMMs, produce the wrong response due to lack of fine-grained visual\nperception and possibly poor math reasoning. GPT-4 w/ Layout-aware OCR + Caption Response\n(Augmented LLM approach) produces the wrong answer, possibly due to lack of joint text-vision\nreasoning.\n50\nInstruction: Provide your opinion on whether changing the switch\u2019s current position will activate the\ninstrument.\nReference Response: Despite turning the switch to the On position, the system will remain pow-\nered off due to the absence of a connected power cable.\nHuman Response: It will not. The instrument needs to be plugged in to work.\nGPT-4V Response: The image shows the back panel of an electronic device with a power switch in the\n\u2019OFF\u2019 position. Changing the switch\u2019s current position from \u2019OFF\u2019 to \u2019ON\u2019 is likely to activate the\ninstrument, provided that it is properly connected to a power source and there are no malfunctions\nwith the instrument itself.\nWrong response\nGemini Pro-Vision Response: Flipping the switch to the \"ON\" position will power the instrument.\nWrong response\nShareGPT-4V-7B Response: Based on the image, it is not possible to determine if changing the switch\u2019s\ncurrent position will activate the instrument. The switch is currently in the off position, and the\ninstrument is not in use. To determine if changing the switch\u2019s position will activate the instrument, one\nwould need to observe the instrument\u2019s response to the switch\u2019s movement.\nWrong response\nLLaVA-1.5-13B Response: Based on the image, the switch is currently in the \"off\" position, which\nmeans that the instrument is not activated. To activate the instrument, the switch would need to be\nmoved to the \"on\" position. This would allow the electrical current to flow through the device, enabling\nit to function properly.\nWrong response\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction is ambiguous with not enough visual\ninformation available in the provided OCR text or the image caption regarding a switch. Therefore, it\nis not possible to provide an opinion on whether changing the switch\u2019s current position will activate the\ninstrument.\nLack of joint text-vision reasoning\nFigure 49: In this task, GPT4V produces the correct response, whereas all other LMMs, produce\nthe wrong response due to lack of fine-grained visual perception. GPT-4 w/ Layout-aware OCR +\nCaption Response (Augmented LLM approach) produces the wrong answer, possibly due to lack of\njoint text-vision reasoning.\n51\nInstruction: What is written on a leg of the letter M?\nReference Response: SOHO\nHuman Response: SOHO\nGPT-4V Response: On the leg of the letter \"M\" in the image, it says \"SOHO.\"\nGemini Pro-Vision Response: SOHO\nShareGPT-4V-7B Response: A star is written on the leg of the letter M.\nWrong response\nLLaVA-1.5-13B Response: The word \"Music\" is written on the leg of the letter M.\nWrong response\nGPT-4 w/ Layout-aware OCR + Caption Response: U\nWrong response\nFigure 50: In this task, GPT-4V and Gemini Pro-Vision correctly respond to the instruction. This\ncan be attributed to their ability for text recognition, and context-sensitive text-vision understanding.\nOpen source LMMs like ShareGPT4V and LLaVA-V-1.5 and GPT-4 w/ Layout-aware OCR + Caption\nResponse (Augmented LLM approach) predict the wrong answer.\n52\nInstruction: What would the first line read if we filter the letters in the red?\nReference Response: SAYRCON\nHuman Response: SA YR CON\nGPT-4V Response: The first line reads \"SATYRICONPDX.COM\" with the letters in red. If we filter\nout the letters in red, the remaining letters spell \"SATICONDX.COM\".\nWrong response\nGemini Pro-Vision Response: SATYRICONPDX.COM\nWrong response\nShareGPT-4V-7B Response: If we filter the letters in the red, the first line would read \"SATYRICONP\"\nWrong response\nLLaVA-1.5-13B Response: If we filter the letters in the red, the first line would read \"Satyricon PDX\".\nWrong response\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction can\u2019t be executed on the provided\ninformation because the layout aware OCR text doesn\u2019t indicate any letters in red.\nLack of joint text-vision reasoning\nFigure 51: In this task, all LMMs produce the wrong answer due to lack of fine-grained visual\nperception and context sensitive text-vision reasoning. GPT-4 w/ Layout-aware OCR + Caption\nResponse (Augmented LLM approach) produces the wrong answer, possibly due to lack of joint\ntext-vision reasoning.\n53\nInstruction: Get the number of the boat with three yellow and one red round items hanging from it.\nReference Response: WH97\nHuman Response: 97\nGPT-4V Response: The boat with three yellow and one red round items (which are likely buoys) hanging\nfrom it has the number SS 170.\nHallucinations\nGemini Pro-Vision Response: SS 273\nWrong response\nShareGPT-4V-7B Response: The boat with three yellow and one red round items hanging from it is\nSS138.\nWrong response\nLLaVA-1.5-13B Response: The boat with three yellow and one red round items hanging from it is the\none with the number \"S527\" on it.\nHallucinations\nGPT-4 w/ Layout-aware OCR + Caption Response: The instruction doesn\u2019t specify which boat has\nthree yellow and one red round items hanging from it, so it\u2019s impossible to provide the number of that\nboat based on the provided OCR text and image caption.\nLack of joint text-vision reasoning\nFigure 52: In this task, GPT4V & LLaVA-1.5-13B hallucinate, whereas, Gemini-Vision-Pro &\nShareGPT-4V-7B produce the wrong response. The mispredictions of LMMs is possibly due to lack\nof fine-grained visual perception. GPT-4 w/ Layout-aware OCR + Caption Response (Augmented\nLLM approach) produces the wrong answer, possibly due to lack of joint text-vision reasoning.\n54\n"
  }
]