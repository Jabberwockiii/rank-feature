[
  {
    "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
    "link": "https://arxiv.org/pdf/2305.10973.pdf",
    "upvote": "26",
    "text": "Drag Your GAN: Interactive Point-based Manipulation on the\nGenerative Image Manifold\nXINGANG PAN, Max Planck Institute for Informatics, Germany and Saarbr\u00fccken Research Center for Visual Computing,\nInteraction and AI, Germany\nAYUSH TEWARI, MIT CSAIL, USA\nTHOMAS LEIMK\u00dcHLER, Max Planck Institute for Informatics, Germany\nLINGJIE LIU, Max Planck Institute for Informatics, Germany and University of Pennsylvania, USA\nABHIMITRA MEKA, Google AR/VR, USA\nCHRISTIAN THEOBALT, Max Planck Institute for Informatics, Germany and Saarbr\u00fccken Research Center for Visual\nComputing, Interaction and AI, Germany\nImage + User input (1st Edit)\nResult\n2nd Edit\nResult\nFig. 1. Our approach DragGAN allows users to \"drag\" the content of any GAN-generated images. Users only need to click a few handle points (red) and\ntarget points (blue) on the image, and our approach will move the handle points to precisely reach their corresponding target points. Users can optionally\ndraw a mask of the flexible region (brighter area), keeping the rest of the image fixed. This flexible point-based manipulation enables control of many spatial\nattributes like pose, shape, expression, and layout across diverse object categories. Project page: https://vcai.mpi-inf.mpg.de/projects/DragGAN/.\nSynthesizing visual content that meets users\u2019 needs often requires flexible\nand precise controllability of the pose, shape, expression, and layout of the\ngenerated objects. Existing approaches gain controllability of generative\nadversarial networks (GANs) via manually annotated training data or a\nprior 3D model, which often lack flexibility, precision, and generality. In\nthis work, we study a powerful yet much less explored way of controlling\nGANs, that is, to \"drag\" any points of the image to precisely reach target\npoints in a user-interactive manner, as shown in Fig.1. To achieve this, we\npropose DragGAN, which consists of two main components: 1) a feature-\nbased motion supervision that drives the handle point to move towards\nthe target position, and 2) a new point tracking approach that leverages\nthe discriminative generator features to keep localizing the position of the\nhandle points. Through DragGAN, anyone can deform an image with precise\ncontrol over where pixels go, thus manipulating the pose, shape, expression,\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0159-7/23/08.\nhttps://doi.org/10.1145/3588432.3591500\nand layout of diverse categories such as animals, cars, humans, landscapes,\netc. As these manipulations are performed on the learned generative image\nmanifold of a GAN, they tend to produce realistic outputs even for chal-\nlenging scenarios such as hallucinating occluded content and deforming\nshapes that consistently follow the object\u2019s rigidity. Both qualitative and\nquantitative comparisons demonstrate the advantage of DragGAN over prior\napproaches in the tasks of image manipulation and point tracking. We also\nshowcase the manipulation of real images through GAN inversion.\nCCS Concepts: \u2022 Computing methodologies \u2192 Computer vision.\nAdditional Key Words and Phrases: GANs, interactive image manipulation,\npoint tracking\nACM Reference Format:\nXingang Pan, Ayush Tewari, Thomas Leimk\u00fchler, Lingjie Liu, Abhimitra\nMeka, and Christian Theobalt. 2023. Drag Your GAN: Interactive Point-\nbased Manipulation on the Generative Image Manifold. In Special Interest\nGroup on Computer Graphics and Interactive Techniques Conference Conference\nProceedings (SIGGRAPH \u201923 Conference Proceedings), August 6\u201310, 2023, Los\nAngeles, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.\n1145/3588432.3591500\n1\narXiv:2305.10973v1  [cs.CV]  18 May 2023\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nX. Pan, A. Tewari, T. Leimk\u00fchler, L. Liu, A. Meka, C. Theobalt\n1\nINTRODUCTION\nDeep generative models such as generative adversarial networks\n(GANs) [Goodfellow et al. 2014] have achieved unprecedented suc-\ncess in synthesizing random photorealistic images. In real-world\napplications, a critical functionality requirement of such learning-\nbased image synthesis methods is the controllability over the syn-\nthesized visual content. For example, social-media users might want\nto adjust the position, shape, expression, and body pose of a hu-\nman or animal in a casually-captured photo; professional movie\npre-visualization and media editing may require efficiently creating\nsketches of scenes with certain layouts; and car designers may want\nto interactively modify the shape of their creations. To satisfy these\ndiverse user requirements, an ideal controllable image synthesis\napproach should possess the following properties 1) Flexibility: it\nshould be able to control different spatial attributes including posi-\ntion, pose, shape, expression, and layout of the generated objects\nor animals; 2) Precision: it should be able to control the spatial at-\ntributes with high precision; 3) Generality: it should be applicable\nto different object categories but not limited to a certain category.\nWhile previous works only satisfy one or two of these properties,\nwe target to achieve them all in this work.\nMost previous approaches gain controllability of GANs via prior\n3D models [Deng et al. 2020; Ghosh et al. 2020; Tewari et al. 2020] or\nsupervised learning that relies on manually annotated data [Abdal\net al. 2021; Isola et al. 2017; Ling et al. 2021; Park et al. 2019; Shen\net al. 2020]. Thus, these approaches fail to generalize to new object\ncategories, often control a limited range of spatial attributes or pro-\nvide little control over the editing process. Recently, text-guided\nimage synthesis has attracted attention [Ramesh et al. 2022; Rom-\nbach et al. 2021; Saharia et al. 2022]. However, text guidance lacks\nprecision and flexibility in terms of editing spatial attributes. For\nexample, it cannot be used to move an object by a specific number\nof pixels.\nTo achieve flexible, precise, and generic controllability of GANs,\nin this work, we explore a powerful yet much less explored interac-\ntive point-based manipulation. Specifically, we allow users to click\nany number of handle points and target points on the image and\nthe goal is to drive the handle points to reach their corresponding\ntarget points. As shown in Fig. 1, this point-based manipulation\nallows users to control diverse spatial attributes and is agnostic to\nobject categories. The approach with the closest setting to ours is\nUserControllableLT [Endo 2022], which also studies dragging-based\nmanipulation. Compared to it, the problem studied in this paper\nhas two more challenges: 1) we consider the control of more than\none point, which their approach does not handle well; 2) we require\nthe handle points to precisely reach the target points while their\napproach does not. As we will show in experiments, handling more\nthan one point with precise position control enables much more\ndiverse and accurate image manipulation.\nTo achieve such interactive point-based manipulation, we pro-\npose DragGAN, which addresses two sub-problems, including 1)\nsupervising the handle points to move towards the targets and 2)\ntracking the handle points so that their positions are known at\neach editing step. Our technique is built on the key insight that\nthe feature space of a GAN is sufficiently discriminative to enable\nboth motion supervision and precise point tracking. Specifically, the\nmotion supervision is achieved via a shifted feature patch loss that\noptimizes the latent code. Each optimization step leads to the handle\npoints shifting closer to the targets; thus point tracking is then per-\nformed through nearest neighbor search in the feature space. This\noptimization process is repeated until the handle points reach the\ntargets. DragGAN also allows users to optionally draw a region of\ninterest to perform region-specific editing. Since DragGAN does not\nrely on any additional networks like RAFT [Teed and Deng 2020],\nit achieves efficient manipulation, only taking a few seconds on a\nsingle RTX 3090 GPU in most cases. This allows for live, interactive\nediting sessions, in which the user can quickly iterate on different\nlayouts till the desired output is achieved.\nWe conduct an extensive evaluation of DragGAN on diverse\ndatasets including animals (lions, dogs, cats, and horses), humans\n(face and whole body), cars, and landscapes. As shown in Fig.1,\nour approach effectively moves the user-defined handle points to\nthe target points, achieving diverse manipulation effects across\nmany object categories. Unlike conventional shape deformation\napproaches that simply apply warping [Igarashi et al. 2005], our\ndeformation is performed on the learned image manifold of a GAN,\nwhich tends to obey the underlying object structures. For example,\nour approach can hallucinate occluded content, like the teeth inside\na lion\u2019s mouth, and can deform following the object\u2019s rigidity, like\nthe bending of a horse leg. We also develop a GUI for users to\ninteractively perform the manipulation by simply clicking on the\nimage. Both qualitative and quantitative comparison confirms the\nadvantage of our approach over UserControllableLT. Furthermore,\nour GAN-based point tracking algorithm also outperforms existing\npoint tracking approaches such as RAFT [Teed and Deng 2020] and\nPIPs [Harley et al. 2022] for GAN-generated frames. Furthermore,\nby combining with GAN inversion techniques, our approach also\nserves as a powerful tool for real image editing.\n2\nRELATED WORK\n2.1\nGenerative Models for Interactive Content Creation\nMost current methods use generative adversarial networks (GANs)\nor diffusion models for controllable image synthesis.\nUnconditional GANs. GANs are generative models that transform\nlow-dimensional randomly sampled latent vectors into photorealis-\ntic images. They are trained using adversarial learning and can be\nused to generate high-resolution photorealistic images [Creswell\net al. 2018; Goodfellow et al. 2014; Karras et al. 2021, 2019]. Most\nGAN models like StyleGAN [Karras et al. 2019] do not directly\nenable controllable editing of the generated images.\nConditional GANs. Several methods have proposed conditional\nGANs to address this limitation. Here, the network receives a con-\nditional input, such as segmentation map [Isola et al. 2017; Park\net al. 2019] or 3D variables [Deng et al. 2020; Ghosh et al. 2020], in\naddition to the randomly sampled latent vector to generate photo-\nrealistic images. Instead of modeling the conditional distribution,\nEditGAN [Ling et al. 2021] enables editing by first modeling a joint\ndistribution of images and segmentation maps, and then computing\nnew images corresponding to edited segmentation maps.\n2\nDrag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nControllability using Unconditional GANs. Several methods have\nbeen proposed for editing unconditional GANs by manipulating the\ninput latent vectors. Some approaches find meaningful latent direc-\ntions via supervised learning from manual annotations or prior 3D\nmodels [Abdal et al. 2021; Leimk\u00fchler and Drettakis 2021; Patashnik\net al. 2021; Shen et al. 2020; Tewari et al. 2020]. Other approaches\ncompute the important semantic directions in the latent space in\nan unsupervised manner [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou\n2020; Zhu et al. 2023]. Recently, the controllability of coarse object\nposition is achieved by introducing intermediate \u201cblobs\" [Epstein\net al. 2022] or heatmaps [Wang et al. 2022b]. All of these approaches\nenable editing of either image-aligned semantic attributes such as\nappearance, or coarse geometric attributes such as object position\nand pose. While Editing-in-Style [Collins et al. 2020] showcases\nsome spatial attributes editing capability, it can only achieve this by\ntransferring local semantics between different samples. In contrast\nto these methods, our approach allows users to perform fine-grained\ncontrol over the spatial attributes using point-based editing.\nGANWarping [Wang et al. 2022a] also use point-based editing,\nhowever, they only enable out-of-distribution image editing. A few\nwarped images can be used to update the generative model such\nthat all generated images demonstrate similar warps. However, this\nmethod does not ensure that the warps lead to realistic images.\nFurther, it does not enable controls such as changing the 3D pose\nof the object. Similar to us, UserControllableLT [Endo 2022] en-\nables point-based editing by transforming latent vectors of a GAN.\nHowever, this approach only supports editing using a single point\nbeing dragged on the image and does not handle multiple-point\nconstraints well. In addition, the control is not precise, i.e., after\nediting, the target point is often not reached.\n3D-aware GANs. Several methods modify the architecture of the\nGAN to enable 3D control [Chan et al. 2022, 2021; Chen et al. 2022;\nGu et al. 2022; Pan et al. 2021; Schwarz et al. 2020; Tewari et al.\n2022; Xu et al. 2022]. Here, the model generates 3D representations\nthat can be rendered using a physically-based analytic renderer.\nHowever, unlike our approach, control is limited to global pose or\nlighting.\nDiffusion Models. More recently, diffusion models [Sohl-Dickstein\net al. 2015] have enabled image synthesis at high quality [Ho et al.\n2020; Song et al. 2020, 2021]. These models iteratively denoise a\nrandomly sampled noise to create a photorealistic image. Recent\nmodels have shown expressive image synthesis conditioned on text\ninputs [Ramesh et al. 2022; Rombach et al. 2021; Saharia et al. 2022].\nHowever, natural language does not enable fine-grained control\nover the spatial attributes of images, and thus, all text-conditional\nmethods are restricted to high-level semantic editing. In addition,\ncurrent diffusion models are slow since they require multiple denois-\ning steps. While progress has been made toward efficient sampling,\nGANs are still significantly more efficient.\n2.2\nPoint Tracking\nTo track points in videos, an obvious approach is through optical\nflow estimation between consecutive frames. Optical flow estimation\nis a classic problem that estimates motion fields between two images.\nConventional approaches solve optimization problems with hand-\ncrafted criteria [Brox and Malik 2010; Sundaram et al. 2010], while\ndeep learning-based approaches started to dominate the field in\nrecent years due to better performance [Dosovitskiy et al. 2015;\nIlg et al. 2017; Teed and Deng 2020]. These deep learning-based\napproaches typically use synthetic data with ground truth optical\nflow to train the deep neural networks. Among them, the most\nwidely used method now is RAFT [Teed and Deng 2020], which\nestimates optical flow via an iterative algorithm. Recently, Harley\net al. [2022] combines this iterative algorithm with a conventional\n\u201cparticle video\u201d approach, giving rise to a new point tracking method\nnamed PIPs. PIPs considers information across multiple frames and\nthus handles long-range tracking better than previous approaches.\nIn this work, we show that point tracking on GAN-generated\nimages can be performed without using any of the aforementioned\napproaches or additional neural networks. We reveal that the fea-\nture spaces of GANs are discriminative enough such that tracking\ncan be achieved simply via feature matching. While some previous\nworks also leverage the discriminative feature in semantic segmen-\ntation [Tritrong et al. 2021; Zhang et al. 2021], we are the first to\nconnect the point-based editing problem to the intuition of discrim-\ninative GAN features and design a concrete method. Getting rid of\nadditional tracking models allows our approach to run much more\nefficiently to support interactive editing. Despite the simplicity of\nour approach, we show that it outperforms the state-of-the-art point\ntracking approaches including RAFT and PIPs in our experiments.\n3\nMETHOD\nThis work aims to develop an interactive image manipulation method\nfor GANs where users only need to click on the images to define\nsome pairs of (handle point, target point) and drive the handle points\nto reach their corresponding target points. Our study is based on\nthe StyleGAN2 architecture [Karras et al. 2020]. Here we briefly\nintroduce the basics of this architecture.\nStyleGAN Terminology. In the StyleGAN2 architecture, a 512 di-\nmensional latent code \ud835\udc9b \u2208 N (0, \ud835\udc70) is mapped to an intermediate\nlatent code \ud835\udc98 \u2208 R512 via a mapping network. The space of \ud835\udc98 is com-\nmonly referred to as W.\ud835\udc98 is then sent to the generator\ud835\udc3a to produce\nthe output image I = \ud835\udc3a(\ud835\udc98). In this process, \ud835\udc98 is copied several times\nand sent to different layers of the generator \ud835\udc3a to control different\nlevels of attributes. Alternatively, one can also use different \ud835\udc98 for\ndifferent layers, in which case the input would be\ud835\udc98 \u2208 R\ud835\udc59\u00d7512 = W+,\nwhere \ud835\udc59 is the number of layers. This less constrained W+ space is\nshown to be more expressive [Abdal et al. 2019]. As the generator\n\ud835\udc3a learns a mapping from a low-dimensional latent space to a much\nhigher dimensional image space, it can be seen as modelling an\nimage manifold [Zhu et al. 2016].\n3.1\nInteractive Point-based Manipulation\nAn overview of our image manipulation pipeline is shown in Fig. 2.\nFor any image I \u2208 R3\u00d7\ud835\udc3b\u00d7\ud835\udc4a generated by a GAN with latent code\n\ud835\udc98, we allow the user to input a number of handle points {\ud835\udc91\ud835\udc56 =\n(\ud835\udc65\ud835\udc5d,\ud835\udc56,\ud835\udc66\ud835\udc5d,\ud835\udc56)|\ud835\udc56 = 1, 2, ...,\ud835\udc5b} and their corresponding target points {\ud835\udc95\ud835\udc56 =\n(\ud835\udc65\ud835\udc61,\ud835\udc56,\ud835\udc66\ud835\udc61,\ud835\udc56)|\ud835\udc56 = 1, 2, ...,\ud835\udc5b} (i.e., the corresponding target point of \ud835\udc91\ud835\udc56\nis \ud835\udc95\ud835\udc56). The goal is to move the object in the image such that the\n3\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nX. Pan, A. Tewari, T. Leimk\u00fchler, L. Liu, A. Meka, C. Theobalt\nGenerator\nLatent code w\nMotion \nsupervision\nw\u2019\nPoint \ntracking\nMotion \nsupervision\nw*\n\u2026\nUser input\nInitial image\n1st optimization step\nUpdate points\nFinal image\nHandle point\nTarget point\nFig. 2. Overview of our pipeline. Given a GAN-generated image, the user only needs to set several handle points (red dots), target points (blue dots), and\noptionally a mask denoting the movable region during editing (brighter area). Our approach iteratively performs motion supervision (Sec. 3.2) and point tracking\n(Sec. 3.3). The motion supervision step drives the handle points (red dots) to move towards the target points (blue dots) and the point tracking step updates\nthe handle points to track the object in the image. This process continues until the handle points reach their corresponding target points.\nsemantic positions (e.g., the nose and the jaw in Fig. 2) of the handle\npoints reach their corresponding target points. We also allow the\nuser to optionally draw a binary mask M denoting which region of\nthe image is movable.\nGiven these user inputs, we perform image manipulation in an\noptimization manner. As shown in Fig. 2, each optimization step\nconsists of two sub-steps, including 1) motion supervision and 2)\npoint tracking. In motion supervision, a loss that enforces handle\npoints to move towards target points is used to optimize the latent\ncode \ud835\udc98. After one optimization step, we get a new latent code \ud835\udc98\u2032\nand a new image I\u2032. The update would cause a slight movement\nof the object in the image. Note that the motion supervision step\nonly moves each handle point towards its target by a small step but\nthe exact length of the step is unclear as it is subject to complex\noptimization dynamics and therefore varies for different objects\nand parts. Thus, we then update the positions of the handle points\n{\ud835\udc91\ud835\udc56} to track the corresponding points on the object. This tracking\nprocess is necessary because if the handle points (e.g., nose of the\nlion) are not accurately tracked, then in the next motion supervision\nstep, wrong points (e.g., face of the lion) will be supervised, leading\nto undesired results. After tracking, we repeat the above optimiza-\ntion step based on the new handle points and latent codes. This\noptimization process continues until the handle points {\ud835\udc91\ud835\udc56} reach\nthe position of the target points {\ud835\udc95\ud835\udc56}, which usually takes 30-200\niterations in our experiments. The user can also stop the optimiza-\ntion at any intermediate step. After editing, the user can input new\nhandle and target points and continue editing until satisfied with\nthe results.\n3.2\nMotion Supervision\nHow to supervise the point motion for a GAN-generated image has\nnot been much explored before. In this work, we propose a motion\nsupervision loss that does not rely on any additional neural net-\nworks. The key idea is that the intermediate features of the generator\nare very discriminative such that a simple loss suffices to supervise\nmotion. Specifically, we consider the feature maps F after the 6th\nblock of StyleGAN2, which performs the best among all features due\nto a good trade-off between resolution and discriminativeness. We\nresize F to have the same resolution as the final image via bilinear\nFeature\nGenerator\nLatent code w\nw\u2019\nNearest \nNeighbor\nL1_loss(     ,     .detach())\nFig. 3. Method. Our motion supervision is achieved via a shifted patch loss\non the feature maps of the generator. We perform point tracking on the\nsame feature space via the nearest neighbor search.\ninterpolation. As shown in Fig. 3, to move a handle point \ud835\udc91\ud835\udc56 to the\ntarget point \ud835\udc95\ud835\udc56, our idea is to supervise a small patch around \ud835\udc91\ud835\udc56\n(red circle) to move towards \ud835\udc95\ud835\udc56 by a small step (blue circle). We use\n\u03a91(\ud835\udc91\ud835\udc56,\ud835\udc5f1) to denote the pixels whose distance to \ud835\udc91\ud835\udc56 is less than \ud835\udc5f1,\nthen our motion supervision loss is:\nL =\n\ud835\udc5b\n\u2211\ufe01\n\ud835\udc56=0\n\u2211\ufe01\n\ud835\udc92\ud835\udc56 \u2208\u03a91(\ud835\udc91\ud835\udc56,\ud835\udc5f1)\n\u2225F(\ud835\udc92\ud835\udc56) \u2212 F(\ud835\udc92\ud835\udc56 + \ud835\udc85\ud835\udc56)\u22251 + \ud835\udf06\u2225(F \u2212 F0) \u00b7 (1 \u2212 M)\u22251,\n(1)\nwhere F(\ud835\udc92) denotes the feature values of F at pixel \ud835\udc92, \ud835\udc85\ud835\udc56 =\n\ud835\udc95\ud835\udc56\u2212\ud835\udc91\ud835\udc56\n\u2225\ud835\udc95\ud835\udc56\u2212\ud835\udc91\ud835\udc56 \u22252\nis a normalized vector pointing from \ud835\udc91\ud835\udc56 to \ud835\udc95\ud835\udc56 (\ud835\udc85\ud835\udc56 = 0 if \ud835\udc95\ud835\udc56 = \ud835\udc91\ud835\udc56),\nand F0 is the feature maps corresponding to the initial image. Note\nthat the first term is summed up over all handle points {\ud835\udc91\ud835\udc56}. As the\ncomponents of \ud835\udc92\ud835\udc56 +\ud835\udc85\ud835\udc56 are not integers, we obtain F(\ud835\udc92\ud835\udc56 +\ud835\udc85\ud835\udc56) via bilin-\near interpolation. Importantly, when performing back-propagation\nusing this loss, the gradient is not back-propagated through F(\ud835\udc92\ud835\udc56).\nThis will motivate \ud835\udc91\ud835\udc56 to move to \ud835\udc91\ud835\udc56 + \ud835\udc85\ud835\udc56 but not vice versa. In case\nthe binary mask M is given, we keep the unmasked region fixed with\na reconstruction loss shown as the second term. At each motion\nsupervision step, this loss is used to optimize the latent code \ud835\udc98 for\none step. \ud835\udc98 can be optimized either in the W space or in the W+\n4\nDrag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nInputs\nOurs\nUserControllableLT\nFig. 4. Qualitative comparison of our approach to UserControllableLT [Endo 2022] on the task of moving handle points (red dots) to target points (blue dots).\nOur approach achieves more natural and superior results on various datasets. More examples are provided in Fig. 10.\nspace, depending on whether the user wants a more constrained\nimage manifold or not. As W+ space is easier to achieve out-of-\ndistribution manipulations (e.g., cat in Fig. 16), we use W+ in this\nwork for better editability. In practice, we observe that the spatial\nattributes of the image are mainly affected by the \ud835\udc98 for the first\n6 layers while the remaining ones only affect appearance. Thus,\ninspired by the style-mixing technique [Karras et al. 2019], we only\nupdate the \ud835\udc98 for the first 6 layers while fixing others to preserve the\nappearance. This selective optimization leads to the desired slight\nmovement of image content.\n3.3\nPoint Tracking\nThe previous motion supervision results in a new latent code \ud835\udc98\u2032,\nnew feature maps F\u2032, and a new image I\u2032. As the motion supervision\nstep does not readily provide the precise new locations of the handle\npoints, our goal here is to update each handle point \ud835\udc91\ud835\udc56 such that it\ntracks the corresponding point on the object. Point tracking is typi-\ncally performed via optical flow estimation models or particle video\napproaches [Harley et al. 2022]. Again, these additional models can\nsignificantly harm efficiency and may suffer from accumulation\nerror, especially in the presence of alias artifacts in GANs. We thus\npresent a new point tracking approach for GANs. The insight is that\nthe discriminative features of GANs well capture dense correspon-\ndence and thus tracking can be effectively performed via nearest\nneighbor search in a feature patch. Specifically, we denote the fea-\nture of the initial handle point as \ud835\udc87\ud835\udc56 = F0(\ud835\udc91\ud835\udc56). We denote the patch\naround \ud835\udc91\ud835\udc56 as \u03a92(\ud835\udc91\ud835\udc56,\ud835\udc5f2) = {(\ud835\udc65,\ud835\udc66) | |\ud835\udc65 \u2212 \ud835\udc65\ud835\udc5d,\ud835\udc56 | < \ud835\udc5f2, |\ud835\udc66 \u2212 \ud835\udc66\ud835\udc5d,\ud835\udc56 | < \ud835\udc5f2}.\nThen the tracked point is obtained by searching for the nearest\nneighbor of \ud835\udc53\ud835\udc56 in \u03a92(\ud835\udc91\ud835\udc56,\ud835\udc5f2):\n\ud835\udc91\ud835\udc56 :=\narg min\n\ud835\udc92\ud835\udc56 \u2208\u03a92(\ud835\udc91\ud835\udc56,\ud835\udc5f2)\n\u2225F\u2032(\ud835\udc92\ud835\udc56) \u2212 \ud835\udc87\ud835\udc56 \u22251.\n(2)\nIn this way, \ud835\udc91\ud835\udc56 is updated to track the object. For more than one\nhandle point, we apply the same process for each point. Note that\nhere we are also considering the feature maps F\u2032 after the 6th block\nof StyleGAN2. The feature maps have a resolution of 256 \u00d7 256 and\nare bilinear interpolated to the same size as the image if needed,\nwhich is sufficient to perform accurate tracking in our experiments.\nWe analyze this choice at Sec. 4.2.\n3.4\nImplementation Details\nWe implement our approach based on PyTorch [Paszke et al. 2017].\nWe use the Adam optimizer [Kingma and Ba 2014] to optimize\nthe latent code \ud835\udc98 with a step size of 2e-3 for FFHQ [Karras et al.\n2019], AFHQCat [Choi et al. 2020], and LSUN Car [Yu et al. 2015]\ndatasets and 1e-3 for others. The hyper-parameters are set to be\n\ud835\udf06 = 20,\ud835\udc5f1 = 3,\ud835\udc5f2 = 12. In our implementation, we stop the optimiza-\ntion process when all the handle points are no more than \ud835\udc51 pixel\naway from their corresponding target points, where \ud835\udc51 is set to 1\nfor no more than 5 handle points and 2 otherwise. We also develop\na GUI to support interactive image manipulation. Thanks to the\ncomputational efficiency of our approach, users only need to wait\nfor a few seconds for each edit and can continue the editing until\nsatisfied. We highly recommend readers refer to the supplemental\nvideo for live recordings of interactive sessions.\n4\nEXPERIMENTS\nDatasets. We evaluate our approach based on StyleGAN2 [Karras\net al. 2020] pretrained on the following datasets (the resolution of\nthe pretrained StyleGAN2 is shown in brackets): FFHQ (512) [Karras\net al. 2019], AFHQCat (512) [Choi et al. 2020], SHHQ (512) [Fu et al.\n2022], LSUN Car (512) [Yu et al. 2015], LSUN Cat (256) [Yu et al.\n2015], Landscapes HQ (256) [Skorokhodov et al. 2021], microscope\n5\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nX. Pan, A. Tewari, T. Leimk\u00fchler, L. Liu, A. Meka, C. Theobalt\nReal image\n4th Edit (expression)\n1st Edit (pose)\n2nd Edit (hair)\n3rd Edit (shape)\nGAN Inversion\nFig. 5. Real image manipulation. Given a real image, we apply GAN inversion to map it to the latent space of StyleGAN, then edit the pose, hair, shape, and\nexpression, respectively.\nInput\nOurs\nPIPs\nRAFT\nManipulation process\nw/o Tracking\nFig. 6. Qualitative tracking comparison of our approach to RAFT [Teed and\nDeng 2020], PIPs [Harley et al. 2022], and without tracking. Our approach\ntracks the handle point more accurately than baselines, thus producing\nmore precise editing.\n(512) [Pinkney 2020] and self-distilled dataset from [Mokady et al.\n2022] including Lion (512), Dog (1024), and Elephant (512).\nBaselines. Our main baseline is UserControllableLT [Endo 2022],\nwhich has the closest setting with our method. UserControllableLT\ndoes not support a mask input but allows users to define a number\nof fixed points. Thus, for testing cases with a mask input, we sample\na regular 16 \u00d7 16 grid on the image and use the points outside the\nmask as the fixed points to UserControllableLT. Besides, we also\ncompare with RAFT [Teed and Deng 2020] and PIPs [Harley et al.\n2022] for point tracking. To do so, we create two variants of our\napproach where the point tracking part (Sec.3.3) is replaced with\nthese two tracking methods.\n4.1\nQualitative Evaluation\nFig. 4 shows the qualitative comparison between our method and\nUserControllableLT. We show the image manipulation results for\nseveral different object categories and user inputs. Our approach\naccurately moves the handle points to reach the target points, achiev-\ning diverse and natural manipulation effects such as changing the\npose of animals, the shape of a car, and the layout of a landscape.\nIn contrast, UserControllableLT cannot faithfully move the handle\npoints to the targets and often leads to undesired changes in the\nimages, e.g., the clothes of the human and the background of the\ncar. It also does not keep the unmasked region fixed as well as ours,\nas shown in the cat images. We show more comparisons in Fig. 10.\nA comparison between our approach with PIPs and RAFT is\nprovided in Fig. 6. Our approach accurately tracks the handle point\nabove the nose of the lion, thus successfully driving it to the target\nInput\nTarget\nUserControllableLT\nOurs\nFig.\n7. Face\nlandmark\nmanipulation.\nCompared\nto\nUserControl-\nlableLT [Endo 2022], our method can manipulate the landmarks detected\nfrom the input image to match the landmarks detected from the target\nimage with less matching error.\nTable 1. Quantitative evaluation on face keypoint manipulation. We com-\npute the mean distance between edited points and target points. The FID\nand Time are reported based on the \u20181 point\u2019 setting.\nMethod\n1 point\n5 points\n68 points\nFID\nTime (s)\nNo edit\n12.93\n11.66\n16.02\n-\n-\nUserControllableLT\n11.64\n10.41\n10.15\n25.32\n0.03\nOurs w. RAFT tracking\n13.43\n13.59\n15.92\n51.37\n15.4\nOurs w. PIPs tracking\n2.98\n4.83\n5.30\n31.87\n6.6\nOurs\n2.44\n3.18\n4.73\n9.28\n2.0\nposition. In PIPs and RAFT, the tracked point starts to deviate from\nthe nose during the manipulation process. Consequently, they move\nthe wrong part to the target position. When no tracking is performed,\nthe fixed handle point soon starts to drive another part of the image\n(e.g., background) after a few steps and never knows when to stop,\nwhich fails to achieve the editing goal.\nReal image editing. Using GAN inversion techniques that embed\na real image in the latent space of StyleGAN, we can also apply\nour approach to manipulate real images. Fig. 5 shows an example,\nwhere we apply PTI inversion [Roich et al. 2022] to the real image\nand then perform a series of manipulations to edit the pose, hair,\nshape, and expression of the face in the image. We show more real\nimage editing examples in Fig. 13.\n4.2\nQuantitative Evaluation\nWe quantitatively evaluate our method under two settings, including\nface landmark manipulation and paired image reconstruction.\nFace landmark manipulation. Since face landmark detection is\nvery reliable using an off-the-shelf tool [King 2009], we use its\nprediction as ground truth landmarks. Specifically, we randomly\ngenerate two face images using the StyleGAN trained on FFHQ and\ndetect their landmarks. The goal is to manipulate the landmarks\n6\nDrag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nTable 2. Quantitative evaluation on paired image reconstruction. We follow the evaluation\nin [Endo 2022] and report MSE (\u00d7102)\u2193 and LPIPS (\u00d710)\u2193 scores.\nDataset\nLion\nLSUN Cat\nDog\nLSUN Car\nMetric\nMSE\nLPIPS\nMSE\nLPIPS\nMSE\nLPIPS\nMSE\nLPIPS\nUserControllableLT\n1.82\n1.14\n1.25\n0.87\n1.23\n0.92\n1.98\n0.85\nOurs w. RAFT tracking\n1.09\n0.99\n1.84\n1.15\n0.91\n0.76\n2.37\n0.94\nOurs w. PIPs tracking\n0.80\n0.82\n1.11\n0.85\n0.78\n0.63\n1.81\n0.79\nOurs\n0.66\n0.72\n1.04\n0.82\n0.48\n0.44\n1.67\n0.74\nTable 3. Effects of which feature to use. x+y means the con-\ncatenation of two features. We report the performance (MD)\nof face landmark manipulation (1 point).\nBlock No.\n4\n5\n6\n7\n5+6\n6+7\nMotion sup.\n2.73\n2.50\n2.44\n2.51\n2.47\n2.45\nTracking\n3.61\n2.55\n2.44\n2.58\n2.47\n2.45\nTable 4. Effects of \ud835\udc5f1.\n\ud835\udc5f1\n1\n2\n3\n4\n5\nMD\n2.49\n2.51\n2.44\n2.45\n2.46\nw/ mask\nw/o mask\nFig. 8. Effects of the mask. Our approach allows masking the movable\nregion. After masking the head region of the dog, the rest part would be\nalmost unchanged.\nof the first image to match the landmarks of the second image.\nAfter manipulation, we detect the landmarks of the final image\nand compute the mean distance (MD) to the target landmarks. The\nresults are averaged over 1000 tests. The same set of test samples is\nused to evaluate all methods. In this way, the final MD score reflects\nhow well the method can move the landmarks to the target positions.\nWe perform the evaluation under 3 settings with different numbers\nof landmarks including 1, 5, and 68 to show the robustness of our\napproach under different numbers of handle points. We also report\nthe FID score between the edited images and the initial images as\nan indication of image quality. In our approach and its variants, the\nmaximum optimization step is set to 300.\nThe results are provided in Table 1. Our approach significantly\noutperforms UserControllableLT under different numbers of points.\nA qualitative comparison is shown in Fig. 7, where our method\nopens the mouth and adjusts the shape of the jaw to match the\ntarget face while UserControllableLT fails to do so. Furthermore,\nour approach preserves better image quality as indicated by the FID\nscores. Thanks to a better tracking capability, we also achieve more\naccurate manipulation than RAFT and PIPs. Inaccurate tracking\nalso leads to excessive manipulation, which deteriorates the image\nquality as shown in FID scores. Although UserControllableLT is\nfaster, our approach largely pushes the upper bound of this task,\nachieving much more faithful manipulation while maintaining a\ncomfortable running time for users.\nPaired image reconstruction. In this evaluation, we follow the\nsame setting as UserControllableLT [Endo 2022]. Specifically, we\nsample a latent code \ud835\udc981 and randomly perturb it to get \ud835\udc982 in the\nsame way as in [Endo 2022]. Let I1 and I2 be the StyleGAN images\ngenerated from the two latent codes. We then compute the optical\nflow between I1 and I2 and randomly sample 32 pixels from the flow\nfield as the user input U. The goal is to reconstruct I2 from I1 and\nU. We report MSE and LPIPS [Zhang et al. 2018] and average the\nresults over 1000 samples. The maximum optimization step is set\nto 100 in our approach and its variants. As shown in Table 2, our\napproach outperforms all the baselines in different object categories,\nwhich is consistent with previous results.\nFig. 9. Out-of-distribution manipulations. Our approach has extrapolation\ncapability for creating images out of the training image distribution, for\nexample, an extremely opened mouth and a greatly enlarged wheel.\nAblation Study. Here we study the effects of which feature to use\nin motion supervision and point tracking. We report the perfor-\nmance (MD) of face landmark manipulation using different features.\nAs Table 3 shows, in both motion supervision and point tracking,\nthe feature maps after the 6th block of StyleGAN perform the best,\nshowing the best balance between resolution and discriminative-\nness. We also provide the effects of \ud835\udc5f1 in Table 4. It can be observed\nthat the performance is not very sensitive to the choice of \ud835\udc5f1, and\n\ud835\udc5f1 = 3 performs slightly better.\n4.3\nDiscussions\nEffects of mask. Our approach allows users to input a binary\nmask denoting the movable region. We show its effects in Fig. 8.\nWhen a mask over the head of the dog is given, the other regions\nare almost fixed and only the head moves. Without the mask, the\nmanipulation moves the whole dog\u2019s body. This also shows that\npoint-based manipulation often has multiple possible solutions and\nthe GAN will tend to find the closest solution in the image manifold\nlearned from the training data. The mask function can help to reduce\nambiguity and keep certain regions fixed.\nOut-of-distribution manipulation. So far, the point-based manipu-\nlations we have shown are \"in-distribution\" manipulations, i.e., it\nis possible to satisfy the manipulation requirements with a natural\nimage inside the image distribution of the training dataset. Here we\nshowcase some out-of-distribution manipulations in Fig. 9. It can be\nseen that our approach has some extrapolation capability, creating\nimages outside the training image distribution, e.g., an extremely\nopened mouth and a large wheel. In some cases, users may want to\nalways keep the image in the training distribution and prevent it\nfrom reaching such out-of-distribution manipulations. A potential\nway to achieve this is to add additional regularization to the latent\ncode \ud835\udc98, which is not the main focus of this paper.\nLimitations. Despite some extrapolation capability, our editing\nquality is still affected by the diversity of training data. As exem-\nplified in Fig. 14 (a), creating a human pose that deviates from the\ntraining distribution can lead to artifacts. Besides, handle points in\ntexture-less regions sometimes suffer from more drift in tracking, as\n7\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nX. Pan, A. Tewari, T. Leimk\u00fchler, L. Liu, A. Meka, C. Theobalt\nshown in Fig. 14 (b)(c). We thus suggest picking texture-rich handle\npoints if possible.\nSocial impacts. As our method can change the spatial attributes\nof images, it could be misused to create images of a real person with\na fake pose, expression, or shape. Thus, any application or research\nthat uses our approach has to strictly respect personality rights and\nprivacy regulations.\n5\nCONCLUSION\nWe have presented DragGAN, an interactive approach for intuitive\npoint-based image editing. Our method leverages a pre-trained GAN\nto synthesize images that not only precisely follow user input, but\nalso stay on the manifold of realistic images. In contrast to many\nprevious approaches, we present a general framework by not relying\non domain-specific modeling or auxiliary networks. This is achieved\nusing two novel ingredients: An optimization of latent codes that\nincrementally moves multiple handle points towards their target\nlocations, and a point tracking procedure to faithfully trace the\ntrajectory of the handle points. Both components utilize the dis-\ncriminative quality of intermediate feature maps of the GAN to\nyield pixel-precise image deformations and interactive performance.\nWe have demonstrated that our approach outperforms the state of\nthe art in GAN-based manipulation and opens new directions for\npowerful image editing using generative priors. As for future work,\nwe plan to extend point-based editing to 3D generative models.\nACKNOWLEDGMENTS\nChristian Theobalt was supported by ERC Consolidator Grant 4DReply\n(770784). Lingjie Liu was supported by Lise Meitner Postdoctoral Fel-\nlowship. This project was also supported by Saarbr\u00fccken Research\nCenter for Visual Computing, Interaction and AI.\nREFERENCES\nRameen Abdal, Yipeng Qin, and Peter Wonka. 2019. Image2stylegan: How to embed\nimages into the stylegan latent space?. In ICCV.\nRameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. 2021. Styleflow: Attribute-\nconditioned exploration of stylegan-generated images using conditional continuous\nnormalizing flows. ACM Transactions on Graphics (ToG) 40, 3 (2021), 1\u201321.\nThomas Brox and Jitendra Malik. 2010. Large displacement optical flow: descriptor\nmatching in variational motion estimation. IEEE transactions on pattern analysis\nand machine intelligence 33, 3 (2010), 500\u2013513.\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De\nMello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. 2022. Efficient Geometry-aware 3D Generative\nAdversarial Networks. In CVPR.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image\nsynthesis. In CVPR.\nAnpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. 2022. Sofgan:\nA portrait image generator with dynamic styling. ACM Transactions on Graphics\n(TOG) 41, 1 (2022), 1\u201326.\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. 2020. StarGAN v2: Diverse\nImage Synthesis for Multiple Domains. In CVPR.\nEdo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. 2020. Editing in style: Uncov-\nering the local semantics of gans. In CVPR. 5771\u20135780.\nAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta,\nand Anil A Bharath. 2018. Generative adversarial networks: An overview. IEEE\nsignal processing magazine 35, 1 (2018), 53\u201365.\nYu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. 2020. Disentangled\nand Controllable Face Image Generation via 3D Imitative-Contrastive Learning. In\nCVPR.\nAlexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir\nGolkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. 2015. Flownet:\nLearning optical flow with convolutional networks. In ICCV.\nYuki Endo. 2022. User-Controllable Latent Transformer for StyleGAN Image Layout\nEditing. Computer Graphics Forum 41, 7 (2022), 395\u2013406. https://doi.org/10.1111/\ncgf.14686\nDave Epstein, Taesung Park, Richard Zhang, Eli Shechtman, and Alexei A Efros. 2022.\nBlobgan: Spatially disentangled scene representations. In ECCV. 616\u2013635.\nJianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen-Change Loy,\nWayne Wu, and Ziwei Liu. 2022. StyleGAN-Human: A Data-Centric Odyssey of\nHuman Generation. In ECCV.\nPartha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael J Black, and\nTimo Bolkart. 2020. GIF: Generative interpretable faces. In International Conference\non 3D Vision (3DV).\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In\nNeurIPS.\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. 2022. StyleNeRF: A Style-\nbased 3D-Aware Generator for High-resolution Image Synthesis. In ICLR.\nErik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. GANSpace:\nDiscovering Interpretable GAN Controls. arXiv preprint arXiv:2004.02546 (2020).\nAdam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. 2022. Particle Video\nRevisited: Tracking Through Occlusions Using Point Trajectories. In ECCV.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. In NeurIPS.\nTakeo Igarashi, Tomer Moscovich, and John F Hughes. 2005. As-rigid-as-possible shape\nmanipulation. ACM transactions on Graphics (TOG) 24, 3 (2005), 1134\u20131141.\nEddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and\nThomas Brox. 2017. Flownet 2.0: Evolution of optical flow estimation with deep\nnetworks. In CVPR.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image\ntranslation with conditional adversarial networks. In CVPR.\nTero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehti-\nnen, and Timo Aila. 2021. Alias-Free Generative Adversarial Networks. In NeurIPS.\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\nfor generative adversarial networks. In CVPR. 4401\u20134410.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\n2020. Analyzing and improving the image quality of stylegan. In CVPR. 8110\u20138119.\nDavis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning\nResearch 10 (2009), 1755\u20131758.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 (2014).\nThomas Leimk\u00fchler and George Drettakis. 2021. FreeStyleGAN: Free-view Editable\nPortrait Rendering with the Camera Manifold. 40, 6 (2021). https://doi.org/10.1145/\n3478513.3480538\nHuan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja\nFidler. 2021. Editgan: High-precision semantic image editing. In NeurIPS.\nRon Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar Mosseri, Tali Dekel, Daniel\nCohen-Or, and Michal Irani. 2022. Self-distilled stylegan: Towards generation from\ninternet photos. In ACM SIGGRAPH 2022 Conference Proceedings. 1\u20139.\nXingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, and Bo Dai. 2021. A\nShading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image\nSynthesis. In NeurIPS.\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image\nsynthesis with spatially-adaptive normalization. In CVPR.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Auto-\nmatic differentiation in PyTorch. (2017).\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021.\nStyleclip: Text-driven manipulation of stylegan imagery. In ICCV.\nJustin N. M. Pinkney. 2020. Awesome pretrained StyleGAN2. https://github.com/\njustinpinkney/awesome-pretrained-stylegan2.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2022. Pivotal\ntuning for latent-based editing of real images. ACM Transactions on Graphics (TOG)\n42, 1 (2022), 1\u201313.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.\narXiv:2112.10752 [cs.CV]\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,\nSeyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gon-\ntijo Lopes, et al. 2022. Photorealistic Text-to-Image Diffusion Models with Deep\nLanguage Understanding. arXiv preprint arXiv:2205.11487 (2022).\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. GRAF: Genera-\ntive Radiance Fields for 3D-Aware Image Synthesis. In NeurIPS.\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. Interpreting the latent space\nof gans for semantic face editing. In CVPR.\n8\nDrag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nYujun Shen and Bolei Zhou. 2020. Closed-Form Factorization of Latent Semantics in\nGANs. arXiv preprint arXiv:2007.06600 (2020).\nIvan Skorokhodov, Grigorii Sotnikov, and Mohamed Elhoseiny. 2021. Aligning Latent\nand Image Spaces to Connect the Unconnectable. arXiv preprint arXiv:2104.06954\n(2021).\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.\nDeep unsupervised learning using nonequilibrium thermodynamics. In International\nConference on Machine Learning. PMLR, 2256\u20132265.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit\nModels. In ICLR.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Er-\nmon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic\nDifferential Equations. In International Conference on Learning Representations.\nNarayanan Sundaram, Thomas Brox, and Kurt Keutzer. 2010. Dense point trajectories\nby gpu-accelerated large displacement optical flow. In ECCV.\nRyohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, and Huachun Zhu.\n2018. Spatially controllable image synthesis with internal representation collaging.\narXiv preprint arXiv:1811.10153 (2018).\nZachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical\nflow. In ECCV.\nAyush Tewari, MalliKarjun B R, Xingang Pan, Ohad Fried, Maneesh Agrawala, and\nChristian Theobalt. 2022. Disentangled3D: Learning a 3D Generative Model with\nDisentangled Geometry and Appearance from Monocular Images. In CVPR.\nAyush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel,\nPatrick P\u00e9rez, Michael Zollhofer, and Christian Theobalt. 2020. StyleRig: Rigging\nStyleGAN for 3D Control over Portrait Images. In CVPR.\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. 2021.\nRepurposing gans for one-shot semantic part segmentation. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 4475\u20134485.\nJianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, and Bolei Zhou.\n2022b. Improving gan equilibrium by raising spatial awareness. In CVPR. 11285\u2013\n11293.\nSheng-Yu Wang, David Bau, and Jun-Yan Zhu. 2022a. Rewriting Geometric Rules of a\nGAN. ACM Transactions on Graphics (TOG) (2022).\nYinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and Bolei Zhou. 2022. 3D-aware\nImage Synthesis via Learning Structural and Textural Representations. In CVPR.\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong\nXiao. 2015. Lsun: Construction of a large-scale image dataset using deep learning\nwith humans in the loop. arXiv preprint arXiv:1506.03365 (2015).\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Bar-\nriuso, Antonio Torralba, and Sanja Fidler. 2021. DatasetGAN: Efficient Labeled Data\nFactory with Minimal Human Effort. In CVPR.\nJiapeng Zhu, Ceyuan Yang, Yujun Shen, Zifan Shi, Deli Zhao, and Qifeng Chen. 2023.\nLinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis. arXiv\npreprint arXiv:2301.04604 (2023).\nJun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A Efros. 2016. Generative\nvisual manipulation on the natural image manifold. In ECCV.\n9\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\nX. Pan, A. Tewari, T. Leimk\u00fchler, L. Liu, A. Meka, C. Theobalt\nInputs\nOurs\nUserControllableLT\nInputs\nOurs\nUserControllableLT\nFig. 10. Qualitative comparison. This is an extension of Fig. 4.\nInput\nTarget\nOurs\nInput\nTarget\nOurs\nFig. 11. Face landmark manipulation. Our method works well even for such dense keypoint cases.\n10\nDrag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\nSIGGRAPH \u201923 Conference Proceedings, August 6\u201310, 2023, Los Angeles, CA, USA\n1st Edit (foot)\n2nd Edit (mouth)\n3rd Edit (ears)\nFig. 12. Continuous image manipulation. Users can continue the manipulation based on previous manipulation results.\nReal image\n1st Edit (hair)\n2nd Edit (expression)\n3rd Edit (pose)\nGAN Inversion\nGAN Inversion\nGAN Inversion\nGAN Inversion\nGAN Inversion\nFig. 13. Real image manipulation.\n(b) Texture-less handle point\n(c) Texture-rich handle point\n(a) Out-of-distribution pose\nFig. 14. Limitations. (a) the StyleGAN-human [Fu et al. 2022] is trained on a fashion dataset where most arms and legs are downward. Editing toward\nout-of-distribution poses can cause distortion artifacts as shown in the legs and hands. (b)&(c) The handle point (red) in texture-less regions may suffer from\nmore drift during tracking, as can be observed from its relative position to the rearview mirror.\nFig. 15. Effects of the mask. By masking the foreground object, we can fix the back-\nground. The details of the trees and grasses are kept nearly unchanged. Better back-\nground preservation could potentially be achieved via feature blending [Suzuki et al.\n2018].\nInput\nW+\nW\nFig. 16. Effects of W/W+ space. Optimizing the latent code in W+\nspace is easier to achieve out-of-distribution manipulations such as\nclosing only one eye of the cat. In contrast, W space struggles to\nachieve this as it tends to keep the image within the distribution of\ntraining data.\n11\n"
  },
  {
    "title": "LDM3D: Latent Diffusion Model for 3D",
    "link": "https://arxiv.org/pdf/2305.10853.pdf",
    "upvote": "8",
    "text": "LDM3D: Latent Diffusion Model for 3D\nGabriela Ben Melech Stan\nIntel Labs\ngabriela.ben.melech.stan@intel.com\nDiana Wofk\nIntel Labs\ndiana.wofk@intel.com\nScottie Fox\nBlockade Labs\nscottie@blockadelabs.com\nAlex Redden\nBlockade Labs\nalexander.h.redden@gmail.com\nWill Saxton\nBlockade Labs\nimagearts360@gmail.com\nJean Yu\nIntel\njean1.yu@intel.com\nEstelle A\ufb02alo\nIntel Labs\nestelle.aflalo@intel.com\nShao-Yen Tseng\nIntel Labs\nshao-yen.tseng@intel.com\nFabio Nonato\nIntel\nfabio.nonato.de.paula@intel.com\nMatthias M\u00a8uller\nIntel Labs\nmatthias.mueller@intel.com\nVasudev Lal\nIntel Labs\nvasudev.lal@intel.com\nAbstract\nThis research paper proposes a Latent Diffusion Model\nfor 3D (LDM3D) that generates both image and depth map\ndata from a given text prompt, allowing users to generate\nRGBD images from text prompts. The LDM3D model is\n\ufb01ne-tuned on a dataset of tuples containing an RGB image,\ndepth map and caption, and validated through extensive ex-\nperiments. We also develop an application called DepthFu-\nsion, which uses the generated RGB images and depth maps\nto create immersive and interactive 360\u00b0-view experiences\nusing TouchDesigner. This technology has the potential to\ntransform a wide range of industries, from entertainment\nand gaming to architecture and design. Overall, this pa-\nper presents a signi\ufb01cant contribution to the \ufb01eld of gener-\native AI and computer vision, and showcases the potential\nof LDM3D and DepthFusion to revolutionize content cre-\nation and digital experiences. A short video summarizing\nthe approach can be found at https://t.ly/tdi2.\n1. Introduction\nThe \ufb01eld of computer vision has seen signi\ufb01cant ad-\nvancements in recent years, particularly in the area of gen-\nerative AI. In the domain of image generation, Stable Diffu-\nsion has revolutionized content creation by providing open\nsoftware to generate arbitrary high-\ufb01delity RGB images\nfrom text prompts. This work builds on top of Stable Dif-\nfusion [20] v1.4 and proposes a Latent Diffusion Model for\n3D (LDM3D). Unlike the original model, LDM3D is ca-\npable of generating both image and depth map data from\na given text prompt as can be seen in Figure 1. It allows\nusers to generate complete RGBD representations of text\nprompts, bringing them to life in vivid and immersive 360\u00b0\nviews.\nOur LDM3D model was \ufb01ne-tuned on a dataset of tuples\ncontaining an RGB image, depth map and caption. This\ndataset was constructed from a subset of the LAION-400M\ndataset, a large-scale image-caption dataset that contains\nover 400 million image-caption pairs. The depth maps used\nin \ufb01ne-tuning were generated by the DPT-Large depth esti-\nmation model [18,19], which provides highly accurate rel-\native depth estimates for each pixel in an image. The use\nof accurate depth maps was crucial in ensuring that we are\nable to generate 360\u00b0 views that are realistic and immersive,\nallowing users to experience their text prompts in vivid de-\ntail.\nTo showcase the potential of LDM3D, we have devel-\noped DepthFusion, an application that uses the generated\n2D RGB images and depth maps to compute a 360\u00b0 projec-\ntion using TouchDesigner [1]. TouchDesigner is a versatile\nplatform that enables the creation of immersive and inter-\nactive multimedia experiences. Our application harnesses\nthe power of TouchDesigner to create unique and engag-\ning 360\u00b0 views that bring text prompts to life in vivid de-\ntail. DepthFusion has the potential to revolutionize the way\nwe experience digital content. Whether it\u2019s a description of\na tranquil forest, a bustling cityscape, or a futuristic sci-\ufb01\narXiv:2305.10853v2  [cs.CV]  21 May 2023\nRGBD\n\u201cA table with a book\u201d\nDiffusion \nU-Net \nKL-E\nKL-D\nFrozen \ntext E\nConcat \nRGBD\nInference\nFigure 1. LDM3D overview. Illustrating the training pipeline: the 16-bit grayscale depth maps are packed into 3-channel RGB-like depth\nimages, which are then concatenated with the RGB images along the channel dimension. This concatenated RGBD input is passed through\nthe modi\ufb01ed KL-AE and mapped to the latent space. Noise is added to the latent representation, which is then iteratively denoised by\nthe U-Net model. The text prompt is encoded using a frozen CLIP-text encoder and mapped to various layers of the U-Net using cross-\nattention. The denoised output from the latent space is fed into the KL-decoder and mapped back to pixel space as a 6-channel RGBD\noutput. Finally, the output is separated into an RGB image and a 16-bit grayscale depth map. Blue frame: text-to-image inference pipeline.\nInitiating from a Gaussian distributed noise sample in the 64x64x4-dimensional latent space. Given a text prompt, this pipeline generates\nan RGB image and its corresponding depth map.\nworld, DepthFusion can generate immersive and engaging\n360\u00b0 views that allow users to experience their text prompts\nin a way that was previously impossible. This technology\nhas the potential to transform a wide range of industries,\nfrom entertainment and gaming to architecture and design.\nIn summary, our contributions are threefold.\n(1) We\npropose LDM3D, a novel diffusion model that outputs\nRGBD images (RGB images with corresponding depth\nmaps) given a text prompt.\n(2) We develop DepthFu-\nsion, an application to create immersive 360\u00b0-view expe-\nriences based on RGBD images generated with LDM3D.\n(3) Through extensive experiments, we validate the quality\nof our generated RGBD images and 360\u00b0-view immersive\nvideos.\n2. Related Work\nMonocular depth estimation is the task of estimating\ndepth values for each pixel of a single given RGB image.\nRecent work has shown great performance in depth esti-\nmation using deep learning models based on convolutional\nneural networks [11, 12, 14, 22, 28, 29]. Later, attention-\nbased Transformer models were adopted to overcome the\nissue of a limited receptive \ufb01eld in CNNs, allowing the\nmodel to consider global contexts when predicting depth\nvalues [3, 5, 19, 30]. Most recently diffusion models have\nalso been applied to depth estimation to leverage the revo-\nlutionary generation capabilities of such methods.\nDiffusion models have demonstrated amazing capabilities\nin generating highly detailed images based on an input\nprompt or condition [16,20,23]. The use of depth estimates\nhas previously been used in diffusion models as an addi-\ntional condition to perform depth-to-image generation [31].\nLater, [24] and [6] showed that monocular depth estima-\ntion can also be modeled as a denoising diffusion process\nthrough the use of images as an input condition. In this work\nwe propose a diffusion model that simultaneously gener-\nates an RGB image and its corresponding depth map given\na text prompt as input. While our proposed model may be\nfunctionally comparable to an image generation and depth\nestimation model in cascade, there are several differences,\nchallenges, and bene\ufb01ts of our proposed combined model.\nAn adequate monocular depth estimation model requires\nlarge and diverse data [15,19], however, as there is no depth\nground truth available for generated images it is hard for off-\nthe-shelf depth estimation models to adapt to the outputs of\nthe diffusion model. Through joint training, the generation\nof depth is much more infused with the image generation\nprocess allowing the diffusion model to generate more de-\ntailed and accurate depth values. Our proposed model also\ndiffers from the standard monocular depth estimation task\nas the reference images are now novel images that are also\ngenerated by the model. A similar task of generating multi-\nple images simultaneously can be linked to video generation\nusing diffusion models [8, 9, 26]. Video diffusion models\nmostly build on [9] which proposed a 3D U-Net to jointly\nmodel a \ufb01xed number of continuous frame images which\nare then used to compose a video. However, since we only\nrequire two outputs (depth and RGB) which do not neces-\nsarily require the same spatial and temporal dependencies\nas videos, we utilize a different approach in our model.\n3. Methodology\nThis section describes the LDM3D model\u2019s methodol-\nogy, training process, and distinct characteristics that facili-\ntate concurrent RGB image and depth map creation, as well\nas immersive 360-degree view generation based on LDM3D\noutput.\n3.1. LDM-3D\n3.1.1\nModel Architecture\nLDM3D is a 1.6 billion parameter KL-regularized diffusion\nmodel, adapted from Stable Diffusion [20] with minor mod-\ni\ufb01cations, allowing it to generate images and depth maps\nsimultaneously from a text prompt, see Fig. 1.\nThe KL-autoencoder used in our model is a variational\nautoencoder (VAE) architecture based on [7], which in-\ncorporates a KL divergence loss term.\nTo adapt this\nmodel for our speci\ufb01c needs, we modi\ufb01ed the \ufb01rst and last\nConv2d layers of the KL-autoencoder. These adjustments\nallowed the model to accommodate the modi\ufb01ed input for-\nmat, which consists of concatenated RGB images and depth\nmaps.\nThe generative diffusion model utilizes a U-Net back-\nbone [21] architecture, primarily composed of 2D con-\nvolutional layers.\nThe diffusion model was trained on\nthe learned, low-dimensional, KL-regularized latent space,\nsimilar to [20].\nEnabling more accurate reconstruc-\ntions and ef\ufb01cient high-resolution synthesis, compared to\ntransformer-based diffusion model trained in pixel space.\nFor text conditioning, a frozen CLIP-text encoder [17]\nis employed, and the encoded text prompts are mapped to\nvarious layers of the U-Net using cross-attention. This ap-\nproach effectively generalizes to intricate natural language\ntext prompts, generating high-quality images and depth\nmaps in a single pass, only having 9,600 additional param-\neters compared to the reference Stable Diffusion model.\n3.1.2\nPreprocessing the data\nThe model was \ufb01ne-tuned on a subset of the LAION-\n400M [25] dataset, which contains image and caption pairs.\nThe depth maps utilized in \ufb01ne-tuning the LDM3D model\nwere generated by the DPT-Large depth estimation model\nrunning inference at its native resolution of 384 \u00d7 384.\nDepth maps were saved in 16-bit integer format and were\nconverted into 3-channel RGB-like arrays to more closely\nmatch the input requirements of the stable diffusion model\nwhich was pre-trained on RGB images. To achieve this con-\nversion, the 16-bit depth data was unpacked into three sep-\narate 8-bit channels. It should be noted that one of these\nchannels is zero for the 16-bit depth data, but this structure\nis designed to be compatible with a potential 24-bit depth\nmap input. This reparametrization allowed us to encode\ndepth information in an RGB-like image format while pre-\nserving complete depth range information.\nThe original RGB images and the generated RGB-like\ndepth maps were then normalized to have values within the\n[0, 1] range. To create an input suitable for the autoencoder\nmodel training, the RGB images and RGB-like depth maps\nwere concatenated along the channel dimension. This pro-\ncess resulted in an input image of size 512x512x6, where\nthe \ufb01rst three channels correspond to the RGB image and\nthe latter three channels represent the RGB-like depth map.\nThe concatenated input allowed the LDM3D model to learn\nthe joint representation of both RGB images and depth\nmaps, enhancing its ability to generate coherent RGBD out-\nputs.\n3.1.3\nFine-tuning Procedure.\nThe \ufb01ne-tuning process comprises two stages, similar to the\ntechnique presented in [20]. In the \ufb01rst stage, we train an\nautoencoder to generate a lower-dimensional, perceptually\nequivalent data representation. Subsequently, we \ufb01ne-tune\nthe diffusion model using the frozen autoencoder, which\nsimpli\ufb01es training and increases ef\ufb01ciency. This method\noutperforms transformer-based approaches by effectively\nscaling to higher-dimensional data, resulting in more ac-\ncurate reconstructions and ef\ufb01cient high-resolution image\nand depth synthesis without the complexities of balancing\nreconstruction and generative capabilities.\nAutoencoder \ufb01ne-tuning.\nThe KL-autoencoder was \ufb01ne-\ntuned on a training set consisting of 8233 samples, an val-\nidation set containing 2059 samples. Each sample in these\nsets included a caption as well as a corresponding image\nand depth map pair, as previously described in the prepro-\ncessing section.\nFor the \ufb01ne-tuning of our modi\ufb01ed autoencoder, we used\na KL-autoencoder architecture with a downsampling factor\nof 8 time the pixel space image resolution. This downsam-\npling factor was found to be optimal in terms of fast training\nprocess and high-quality image synthesis [20].\nDuring the \ufb01ne-tuning process, we used the Adam op-\ntimizer with a learning rate of 10\u22125 and a batch size of\n8. We trained the model for 83 epochs, and we sampled\nthe outputs after each epoch to monitor the progress. The\nloss function for both the images and depth data consisted\nof a combination of perceptual loss [32] and patch-based\nadversarial-type loss [10], which were originally used in the\npre-training of the KL-AE [7].\nLAutoencoder = min\nE,D max\n\u03c8\n\u0012\nLrec(x, D(E(x)))\n\u2212 Ladv(D(E(x))) + log D\u03c8(x)\n+ Lreg(x; E, D)\n\u0013\n(1)\nHere\nD(E(x))\nare\nthe\nreconstructed\nimages,\nLrec(x, D(E(x)))\nis\nthe\nperceptual\nreconstruction\nloss, Ladv(D(E(x))) is the adversarial loss, D\u03c8(x) is\na patch based discriminator loss, and Lreg(x; E, D) is the\nKL-regularisation loss.\nDiffusion model \ufb01ne-tuning\nFollowing the autoencoder\n\ufb01ne-tuning, we proceeded to the second stage, which in-\nvolved \ufb01ne-tuning the diffusion model. This was achieved\nusing the frozen autoencoder\u2019s latent representations as in-\nput, with a latent input size of 64x64x4.\nFor this stage, we employed the Adam optimizer with a\nlearning rate of 10\u22125 and a batch size of 32 . We train the\ndiffusion model for 178 epochs with the loss function:\nLLDM3D := E\u03b5(x), \u03f5 \u223c N(0, 1), t\n\u0002\n||\u03f5 \u2212 \u03f5\u03b8(zt, t)||2\n2\n\u0003\n(2)\nwhere \u03f5\u03b8(zt, t) is the predicted noise by the denoising\nU-Net, and t is uniformly sampled.\nWe initiate the LDM3D \ufb01ne-tuning using the weights\nfrom the Stable Diffusion v1.4 [20] model as a starting\npoint. We monitor the progress throughout \ufb01ne-tuning by\nsampling the generated images and depth maps, assessing\ntheir quality and ensuring the model\u2019s convergence.\nCompute Infrastructure\nAll training runs reported in\nthis work are conducted on an Intel AI supercomputing\ncluster comprising of Intel Xeon processors and Intel Ha-\nbana Gaudi AI accelerators. The LDM3D model training\nrun is scaled out to 16 accelerators (Gaudis) on the corpus\nof 9,600 tupples (text caption, RGB image, depth map). The\nKL-autoencoder used in our LDM3D model was trained on\nNvidia A6000 GPUs.\n3.1.4\nEvaluation\nIn line with previous studies, we assess text-to-image gener-\nation performance using the MS-COCO [13] validation set.\nTo measure the quality of the generated images, we employ\nFr\u00b4echet Inception Distance (FID), Inception Score (IS), and\nCLIP similarity metrics. The autoencoder\u2019s performance\nis evaluated using the relative FID score, a popular met-\nric for comparing the quality of reconstructed images with\ntheir corresponding original input images. The evaluation\nwas carried out on 27,265 samples, 512x512-sized from the\nLAION-400M dataset.\n3.2. Immersive Experience Generation\nAI models for image generation have become prominent\nin the space of AI art, they are typically designed for 2D rep-\nresentations of diffused content. In order to project imagery\nonto a 3D immersive environment, modi\ufb01cations in map-\nping and resolution needed to be considered to achieve an\nacceptable result. Another previous limitation of correctly\nprojected outputs occurs when perception is lost due to the\nmonoscopic perspective of a single point of view. Modern\nviewing devices and techniques require disparity between\ntwo view points to achieve the experience of stereoscopic\nimmersion.\nRecording devices typically capture footage\nfrom two cameras at a \ufb01xed distance so that a 3D output\ncan be generated based on the disparity and camera param-\neters. In order to achieve the same from single images, how-\never, an offset in pixel space must be calculated. With the\nLDM3D model, a depth map is extracted separately from\nRGB color space and can be used to differentiate a proper\n\u201cleft\u201d and \u201cright\u201d perspective of the same image space in\n3D.\nFirst, the initial image is generated and its correspond-\ning depth map is stored, see Fig. 2a.\nUsing TouchDe-\nsigner [1], the RGB color image is projected to the out-\nside of an equirectangular spherical polar object in 3D space\nsee Fig. 2b. The perspective is set at origin 0,0,0 inside of\nthe spherical object as the center of viewing the immersive\nspace. The vertex points of the sphere are de\ufb01ned as an\nequal distance in all directions from the point of origin. The\ndepth map is then used as instructions to manipulate the dis-\ntance from origin to the corresponding vertex point based on\nmonotone color values. Values closer to 1.0 move the ver-\ntex points closer to the origin, while values of 0.0 are scaled\nto a further distance from the origin. Values of 0.5 result in\nno vertex manipulation. From a monoscopic view at 0,0,0,\nno alteration in image can be perceived since the \u201crays\u201d ex-\ntend linearly from the origin outward. However, with the\ndual perspective of stereoscopic viewpoints, the pixels of\nthe mapped RGB image are distorted in a dynamic fashion\nto give the illusion of depth. This same effect can also be\nobserved while moving the single viewpoint away from ori-\ngin 0,0,0 as the vertex distances scale equally against their\ninitial calculation. Since the RGB color space and depth\nmap pixels occupy the same regions, objects that have per-\nceived geometric shapes are given approximate depth via\ntheir own virtual geometric dimensions in the render engine\nwithin TouchDesigner. Fig. 2 explains the entire pipeline.\nThis approach is not limited to the TouchDesigner platform\nand may also be replicated inside similar rendering engines\nand software suites that have the ability to utilize RGB and\ndepth color space in their pipelines.\n(a) Step 1: Img-to-img inference pipeline for LDM3D. initiating from a panoramic image and corresponding depth map computed using DPT-Large [18,19].\nThe RGBD input is processed through the LDM3D image-to-image pipeline, generating a transformed image and depth map guided by the given text prompt.\n(b) Step 2: LDM3D generated image is projected on a sphere, using vertex manipulation based on diffused depth map, followed by meshing.\n(c) Step 3: Image generation from different viewpoints, and video assembly.\nFigure 2. Immersive experience generation pipeline.\nRGB Images\nDepth Maps\nSDv1.4\nLDM3D (Ours)\nDPT-Large\nFigure 3. Qualitative comparison of images to Stable diffusion\nv1.4 [20] and depth maps to DPT-Large [18, 19], on 512 \u00d7 512\nimages from the COCO validation dataset. Captions from top to\nbottom:\u201da close up of a sheet of pizza on a table\u201d, \u201dA picture of\nsome lemons on a table\u201d, \u201dA little girl with a pink bow in her hair\neating broccoli\u201d, \u201dA man is on a path riding a horse\u201d,\u201dA muf\ufb01n in\na black muf\ufb01n wrap next to a fork\u201d, \u201da white polar bear drinking\nwater from a water source next to some rocks\u201d.\n4. Results\nIn the following, we show the high quality of the gener-\nated images and depth maps of our LDM3D model. We also\nshow the impact on performance of the autoencoder when\nadding the depth modality.\n4.1. Qualitative Evaluation\nA qualitative analysis of the generated images and depth\nmaps reveals that our LDM3D model can effectively gen-\nerate visually coherent outputs that correspond well to the\nprovided text prompts. The generated images exhibit \ufb01ne\ndetails and complex structures, while the depth maps ac-\ncurately represent the spatial information of the scenes,\nsee Fig. 3 .\nThese results highlight the potential of our\nmodel for various applications, including 3D scene recon-\nstruction and immersive content creation, see Fig. 2.\nA\nvideo with examples of the immersive 360-views that can\nbe generated using our complete pipeline can be found at\nhttps://t.ly/TYA5A.\n4.2. Quantitative Image Evaluation\nOur LDM3D model demonstrates impressive perfor-\nmance in generating high-quality images and depth maps\nfrom text prompts. When evaluated on the MS-COCO vali-\ndation set, the model achieves competitive scores to the Sta-\nble diffusion baseline using FID and CLIP similarity met-\nrics, see Tab. 1. There is a degradation in the inception score\n(IS), which might indicate that our model generates images\nthat are close to the real images in terms of their feature dis-\ntributions, as could be derived by the similar FID scores, but\nthey might lack diversity or some aspects of image quality\nthat IS captures. Nevertheless, IS is considered to be a less\nrobust metric than FID because it struggles with capturing\nintra-class diversity [4], is highly sensitive to model param-\neters and implementations, whereas FID is better at assess-\ning the similarity between distributions of real and gener-\nated images while being less sensitive to minor changes in\nnetwork weights that don\u2019t impact image quality [2]. The\nhigh CLIP similarity score indicates that the model main-\ntains a high level of detail and \ufb01delity with respect to the\ntext prompts.\nMethod\nFID\u2193\nIS\u2191\nCLIP\u2191\nSD v1.4\n28.08\n34.17\u00b10.76\n26.13 \u00b1 2.81\nSD v1.5\n27.39\n34.02 \u00b1 0.79\n26.13 \u00b1 2.79\nLDM3D (ours)\n27.82\n28.79 \u00b1 0.49\n26.61\u00b12.92\nTable 1. Text-to-Image synthesis. Evaluation of text-conditional\nimage synthesis on the 512 x 512-sized MS-COCO [13] dataset\nwith 50 DDIM [27] steps. Our model is on par with the Stable\ndiffusion models with the same number of parameters (1.06B). IS\nand CLIP similarity scores are averaged over 30k captions from\nthe MS-COCO dataset.\nIn addition, we investigate the relationship between key\nhyperparameters and the quality of the generated images.\nWe plot the FID and IS scores against the classi\ufb01er-free\ndiffusion guidance scale factor (Fig. 4), the number of de-\nnoising steps (Fig. 5), and the training step (Fig. 7). Ad-\nditionally, we plotted the CLIP similarity score against the\nclassi\ufb01er-free diffusion guidance scale factor in see Fig. 6.\nFig. 4 indicates that the optimal classi\ufb01er-free diffusion\nguidance scale factor that produces the best balance be-\ntween image quality and diversity is around s=5, higher than\nreported on Stable diffusion v1.4 (s=3). Fig. 6 indicated that\nthe alignment of the generated images with the input text\nprompts is nearly unaffected as the scale factor changes for\nscale factors larger than 5.\nFig. 5 indicates that the image quality increases with the\nnumber of denoising steps, the most signi\ufb01cant improve-\nment occurs when increasing the DDIM steps from 50 to\n100.\n0\n5\n10\n15\n60\n80\n100\nClassi\ufb01er-free diffusion guidance scale\nFID\nFID\n0\n5\n10\n15\n12\n14\n16\nIS\nIS\nFigure 4. FID / IS vs. Classi\ufb01er-free diffusion guidance scale\nfactor. Evaluation of text-conditional image synthesis on 2000\nsamples, 512 x 512-sized from MS-COCO [13] dataset, with 50\nDDIM [27] steps.\n50\n100\n150\n200\n250\n53\n54\n55\nDDIM steps\nFID\nFID\n50\n100\n150\n200\n250\n16\n16.2\n16.4\n16.6\nIS\nIS\nFigure 5. FID / IS vs. DDIM steps. Evaluation of text-conditional\nimage synthesis on 2000 samples, 512 x 512-sized from MS-\nCOCO [13] dataset, s=3.\n4.3. Quantitative Depth Evaluation\nOur LDM3D model jointly outputs images and their cor-\nresponding depth maps. Since there is no ground truth depth\n0\n5\n10\n15\n24\n25\n26\n27\nClassi\ufb01er-free diffusion guidance scale\nCLIP similarity\nFigure 6. CLIP similarity score vs. Classi\ufb01er-free diffusion guid-\nance scale factor. Averaged on 2000 samples, 512 x 512-sized gen-\nerated from MS-COCO [13] dataset captions, with 50 DDIM [27]\nsteps.\n0\n1\n2\n3\n4\n5\n\u00b7104\n60\n65\n70\nTraining Step\nFID\nFigure 7. FID vs. Training Step. Evaluation of text-conditional\nimage synthesis on 2000 samples, 512 x 512-sized from MS-\nCOCO [13] dataset: with 50 DDIM [27] steps, s=3.\nreference for these images, we de\ufb01ne a reference model\nagainst which to compute depth metrics. For this, we se-\nlect the ZoeDepth metric depth estimation model. LDM3D\noutputs depth in disparity space, as it was \ufb01ne-tuned using\ndepth maps produced by DPT-Large. We align these depth\nmaps to reference ones produced by ZoeDepth. This align-\nment is done in disparity space in a global least-squares\n\ufb01tting manner similar to the approach in [19]. Points to\nbe \ufb01tted to are determined via random sampling applied\nto the intersected validity maps of the estimated and target\ndepth maps, where valid depth is simply de\ufb01ned to be non-\nnegative. The alignment procedure computes per-sample\nscale and shift factors that are applied to the LDM3D and\nDPT-Large depth maps to align the depths to ZoeDepth\nw.r.t. ZoeDepth-N\nAbsRel\nRMSE [m]\nLDM3D\n0.0911\n0.334\nDPT-Large\n0.0779\n0.297\nTable 2. Depth evaluation comparing LDM3D and DPT-Large\nwith respect to ZoeDepth-N that serves as a reference model.\nRGB (LDM3D)\nDepth (LDM3D)\nDepth (DPT-L)\nDepth (ZoeD-N)\nFigure 8. Depth visualization to accompany Tab. 2.\nvalues.\nAll depth maps are then inverted to bring them\ninto metric depth space. The two depth metrics we com-\npute are absolute relative error (AbsRel) and root mean\nsquared error (RMSE). Metrics are aggregated over a 6k\nsubset of images from the 30k set used for image evaluation.\nTab. 2 shows that LDM3D achieves similar depth accuracy\nas DPT-Large, demonstrating the success of our \ufb01netuning\napproach. A corresponding visualization is shown in Fig. 8.\n4.4. Autoencoder Performance\nWe \ufb01rst evaluate the performance of our \ufb01ne-tuned KL-\nAE using the relative FID score, see Tab. 3. Our \ufb01ndings\nshow a minor but measurable decline in the quality of recon-\nstructed images compared to the pre-trained KL-AE. This\nModel\nrFID\nAbs.Rel.\npre-trained KL-AE, RGB\n0.763\n-\n\ufb01ne-tuned KL-AE, RGBD\n1.966\n0.179\nTable 3. Comparison of KL-autoencoder \ufb01ne-tuning approaches.\nThe pre-trained KL-AE was evaluated on 31,471 images, and\nthe \ufb01ne-tuned KL-AE on 27,265 images, 512x512-sized from the\nLAION-400M [25] dataset.\ncan be attributed to the increased data compression ratio\nwhen incorporating depth information alongside RGB im-\nages in the pixel space, but keeping the latent space dimen-\nsions unchanged. Note that the adjustments made to the\nAE are minimal, adding only 9,615 parameters to the pre-\ntrained AE. We expect that further modi\ufb01cations to the AE\ncan further improve performance. In the current architec-\nture, this decrease in quality is compensated by \ufb01ne-tuning\nthe diffusion U-Net. The resulting LDM3D model performs\non par with vanilla Stable Diffusion as shown in the previ-\nous sections.\n5. Conclusion\nIn conclusion, this research paper introduces LDM3D,\na novel diffusion model that generates RGBD images from\ntext prompts. To demonstrate the potential of LDM3D we\nalso develop DepthFusion, an application that creates im-\nmersive and interactive 360-view experiences using the gen-\nerated RGBD images in TouchDesigner. The results of this\nresearch have the potential to revolutionize the way we ex-\nperience digital content, from entertainment and gaming to\narchitecture and design.\nThe contributions of this paper\npave the way for further advancements in the \ufb01eld of multi-\nview generative AI and computer vision. We look forward\nto seeing how this space will continue to evolve and hope\nthat the presented work will be useful for the community.\nReferences\n[1] Touchdesigner. https://derivative.ca. Accessed:\n2022-12-03. 1, 4\n[2] Shane Barratt and Rishi Sharma. A note on the inception\nscore, 2018. 6\n[3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nAdabins: Depth estimation using adaptive bins. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 4009\u20134018, June 2021.\n2\n[4] Ali Borji. Pros and cons of gan evaluation measures: New\ndevelopments, 2021. 6\n[5] Zeyu Cheng, Yi Zhang, and Chengkai Tang.\nSwin-\ndepth:\nUsing transformers and multi-scale fusion for\nmonocular-based depth estimation. IEEE Sensors Journal,\n21(23):26912\u201326920, 2021. 2\n[6] Yiqun Duan, Zheng Zhu, and Xianda Guo. Diffusiondepth:\nDiffusion denoising approach for monocular depth estima-\ntion, 2023. 2\n[7] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer.\nTam-\ning transformers for high-resolution image synthesis. arXiv\npreprint arXiv:2012.09841, 2020. 3, 4\n[8] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High de\ufb01nition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[9] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video dif-\nfusion models. In Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho, editors, Advances in Neural\nInformation Processing Systems, 2022. 2\n[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.\nEfros. Image-to-image translation with conditional adver-\nsarial networks, 2018. 3\n[11] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-\nsupervised deep learning for monocular depth map predic-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), July 2017. 2\n[12] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks. In 2016 Fourth\nInternational Conference on 3D Vision (3DV), pages 239\u2013\n248, 2016. 2\n[13] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context, 2015. 4, 6, 7\n[14] Armin Masoumian, Hatem A Rashwan, Saddam Abdul-\nwahab, Juli\u00b4an Cristiano, M Salman Asif, and Domenec\nPuig. Gcndepth: Self-supervised monocular depth estima-\ntion based on graph convolutional network. Neurocomput-\ning, 517:81\u201392, 2023. 2\n[15] Yue Ming, Xuyang Meng, Chunxiao Fan, and Hui Yu. Deep\nlearning for monocular depth estimation: A review. Neuro-\ncomputing, 438:14\u201333, 2021. 2\n[16] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen. GLIDE: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,\nCsaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Pro-\nceedings of the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine Learning\nResearch, pages 16784\u201316804. PMLR, 17\u201323 Jul 2022. 2\n[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision, 2021. 3\n[18] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. ICCV, 2021. 1, 5,\n6\n[19] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE transactions on pattern analysis and machine\nintelligence, 44(3):1623\u20131637, 2020. 1, 2, 5, 6, 7\n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution im-\nage synthesis with latent diffusion models. arXiv preprint\narXiv:2112.10752, 2022. 1, 2, 3, 4, 6\n[21] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation,\n2015. 3\n[22] Anirban Roy and Sinisa Todorovic. Monocular depth esti-\nmation using neural regression forest. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2016. 2\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Alice H. Oh, Alekh Agar-\nwal, Danielle Belgrave, and Kyunghyun Cho, editors, Ad-\nvances in Neural Information Processing Systems, 2022. 2\n[24] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and\nDavid J Fleet. Monocular depth estimation using diffusion\nmodels. arXiv preprint arXiv:2302.14816, 2023. 2\n[25] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-\ufb01ltered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 3, 8\n[26] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. In The Eleventh International Conference on Learning\nRepresentations, 2023. 2\n[27] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021. 6, 7\n[28] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and\nNicu Sebe. Multi-scale continuous crfs as sequential deep\nnetworks for monocular depth estimation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), July 2017. 2\n[29] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and\nElisa Ricci. Structured attention guided convolutional neural\n\ufb01elds for monocular depth estimation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2018. 2\n[30] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and\nElisa Ricci.\nTransformer-based attention networks for\ncontinuous pixel-wise prediction.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 16269\u201316279, October 2021. 2\n[31] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 2\n[32] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric, 2018. 3\n"
  },
  {
    "title": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding",
    "link": "https://arxiv.org/pdf/2305.10764.pdf",
    "upvote": "4",
    "text": "OpenShape: Scaling Up 3D Shape Representation\nTowards Open-World Understanding\nMinghua Liu1\u2217\nRuoxi Shi2\u2217\nKaiming Kuang1\u2217\nYinhao Zhu3\nXuanlin Li1\nShizhong Han3\nHong Cai3\nFatih Porikli3\nHao Su1\n1 UC San Diego\n2 Shanghai Jiao Tong University\n3 Qualcomm AI Research\u2020\nProject Website: https://colin97.github.io/OpenShape/\nAbstract\nWe introduce OpenShape, a method for learning multi-modal joint representations\nof text, image, and point clouds. We adopt the commonly used multi-modal con-\ntrastive learning framework for representation alignment, but with a specific focus\non scaling up 3D representations to enable open-world 3D shape understanding.\nTo achieve this, we scale up training data by ensembling multiple 3D datasets and\npropose several strategies to automatically filter and enrich noisy text descriptions.\nWe also explore and compare strategies for scaling 3D backbone networks and\nintroduce a novel hard negative mining module for more efficient training. We\nevaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its\nsuperior capabilities for open-world recognition. Specifically, OpenShape achieves\na zero-shot accuracy of 46.8% on the 1,156-category Objaverse-LVIS benchmark,\ncompared to less than 10% for existing methods. OpenShape also achieves an accu-\nracy of 85.3% on ModelNet40, outperforming previous zero-shot baseline methods\nby 20% and performing on par with some fully-supervised methods. Furthermore,\nwe show that our learned embeddings encode a wide range of visual and semantic\nconcepts (e.g., subcategories, color, shape, style) and facilitate fine-grained text-\n3D and image-3D interactions. Due to their alignment with CLIP embeddings,\nour learned shape representations can also be integrated with off-the-shelf CLIP-\nbased models for various applications, such as point cloud captioning and point\ncloud-conditioned image generation.\n1\nIntroduction\n3D shape understanding has recently garnered a surge of interest driven by the growing demands in\nreal-world applications, such as augmented/virtual reality, autonomous driving, and robotics. Despite\nsignificant advancements in 3D recognition and analysis, existing data-driven approaches are still\ngreatly limited by the scale of 3D training datasets and tend to exhibit poor generalization when facing\nunseen shape categories, hindering the deployment of existing models in real-world applications.\nNote that 3D shapes and 2D images can be easily linked through rendering, and the dataset scale issue\nof 2D images has been remarkably addressed, as shown in recent works such as CLIP [53]. Therefore,\nmany recent studies aim to utilize pre-trained 2D image-language models [53, 57] to assist 3D tasks,\nsuch as 3D generation [22, 26, 43, 61, 33, 7] and 3D scene-level segmentation [18, 27, 14, 76, 39, 47].\nRegarding 3D shape-level understanding, a straightforward idea is to project 3D data to the 2D\n\u2217Equal Contribution\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\nPreprint. Under review.\narXiv:2305.10764v2  [cs.CV]  16 Jun 2023\n+\n+\n+\n+\nFigure 1: Left: Zero-shot shape classification on the Objaverse-LVIS (1,156 categories) and Model-\nNet40 datasets. OpenShape outperforms previous methods by a large margin. We exclude shapes in\nObjaverse-LVIS during training, and we also retrain ULIP [75] on our ensembled training shapes\nfor fair comparison. Right: Our shape representations encode a broad range of semantic and visual\nconcepts. We input two 3D shapes and use their shape embeddings to retrieve the top three shapes\nwhose embeddings are simultaneously closest to both inputs. See Section. 4.4 for more details.\ndomain through rendering and use CLIP to analyze the 2D images, thereby enabling zero-shot 3D\nshape classification [82, 84]. However, these methods suffer from occlusion and information loss\nduring projection, and unnecessary latency due to point cloud rendering and multiple CLIP inferences.\nTo overcome the limitations caused by projection, it is necessary to train a 3D-native model by\ndistilling knowledge from pretrained 2D models. However, training a 3D-native model requires a set\nof 3D shapes, and the amount of knowledge that can be distilled is determined by the size of the 3D\ndataset. For example, ULIP [75] aims to learn a joint representation space between language, 2D\nimages, and 3D shapes, but uses a small-scale 3D dataset ShapeNetCore [8] for knowledge distillation.\nSpecifically, ULIP fixes the 2D CLIP text and image encoders and trains a dedicated 3D-native point\ncloud encoder to extract 3D shape representations. The 3D encoder strives to align the 3D shape\nembedding space with the CLIP image and language embedding spaces by utilizing contrastive\nlearning across all three modalities. However, since ULIP is only trained on 52K shapes of 55 object\ncategories, it still struggles with out-of-distribution shape categories and fails to demonstrate an\nimpressive open-world understanding of 3D shapes.\nIn this work, we propose a novel method called OpenShape, which follows a similar paradigm as\nULIP but aims to achieve a more generalized and scalable joint representation space encompassing\nlanguage, 2D images, and 3D shapes. Our focus mainly lies on scaling up representation learning\nand addressing corresponding challenges. In OpenShape, we emphasize four key factors during the\ntraining process: (a) data scale: we significantly increase the scale of 3D training data by combining\nfour public 3D shape datasets, resulting in 876k 3D shapes covering much more diverse categories;\n(b) text quality: the 3D shapes from our main dataset, Objaverse [12], is dominated with inaccurate\nor uninformative text descriptions. Given the data scale, we propose three strategies to automatically\nfilter and enrich the text descriptions; (c) 3D backbone scaling: since most existing 3D backbones\ntarget small datasets, we find that it\u2019s important but non-trivial to scale up the 3D backbones; and (d)\ndata resampling: since the ensembled dataset is highly unbalanced, we utilize hard negative mining\nto improve the model\u2019s discriminative ability.\nWe first evaluate OpenShape on the zero-shot 3D shape classification task. As shown in Figure 1,\nOpenShape outperforms previous zero-shot approaches on the ModelNet40 dataset by at least 20%.\nMoreover, OpenShape excels at handling long-tail categories. On the challenging Objaverse-LVIS\ndataset, which contains 1,156 categories, OpenShape achieves a 46.8% accuracy, significantly\nsurpassing previous methods. Notably, this performance gap remains even when ULIP is retrained on\nour ensembled datasets, highlighting the superiority of our text enrichment and training strategies.\nBesides zero-shot classification, we present demos that showcase the wide range of visual and\nsemantic concepts learned by OpenShape. For example, in Figure 1-right, we take two 3D shapes as\n2\ninput and use their OpenShape embeddings to retrieve the top three shapes whose embeddings are\nsimultaneously closest to both inputs from our ensembled dataset. The retrieved shapes exhibit an\ninteresting combination of the semantic and geometric elements from both input shapes. Furthermore,\nsince we align our 3D shape embedding space with the CLIP language and image embedding space,\nwe demonstrate that OpenShape embeddings can be easily integrated with other CLIP-based models\nto perform cross-modality tasks such as point cloud captioning and point cloud-conditioned image\ngeneration.\n2\nRelated Work\n2.1\nCLIP for 3D Learning\nImage-language models like CLIP have achieved remarkable performance through large-scale image-\ntext pretraining [53, 29, 35, 80, 4, 54, 59]. As these models excel at capturing rich visual concepts\nand possess impressive zero-shot capabilities, they have been applied to various 3D vision tasks.\nFor instance, numerous recent works utilize CLIP to facilitate zero-shot text-to-3D generation [22,\n26, 43, 61, 33, 7, 32, 5, 28, 74, 38], typically through CLIP-guided per-scene optimization. From a\nrecognition perspective, some works focus on scene-level representation, aiming to leverage CLIP\npriors for zero-shot 3D segmentation or detection in both indoor [18, 27, 14, 76, 39, 47, 79, 23, 58, 81,\n31] and outdoor scenes [9, 21]. Meanwhile, another line of work focuses on shape-level understanding,\ntargeting zero-shot shape classification [82, 84, 51, 75, 19] and part segmentation [37, 1]. There are\ntwo primary working paradigms for these methods. The first [82, 84, 24] involves using images as a\nmedium representation, projecting 3D point clouds into 2D and employing 2D CLIP for inference.\nHowever, these methods typically suffer from occlusion and information loss during projection, along\nwith unnecessary latency due to point cloud rendering and multiple 2D CLIP inferences. The second\nparadigm involves training a 3D-native encoder attempting to distill or fuse CLIP features into 3D\nrepresentations. Our paper follows this paradigm.\n2.2\n3D Shape Representation Learning\nVarious works have studied self-supervised pretraining for point clouds by designing pretext tasks [15,\n66, 48, 2, 64] such as self-reconstruction [55, 13, 3, 69], masked auto-encoding [46, 77, 20], distortion\nreconstruction [62, 42, 65], normal estimation [55], and contrastive learning [83, 60, 73]. These tasks\nenhance models\u2019 shape representations and improve their performance on downstream applications,\nalthough they do not involve multimodal semantic alignments during pretraining.\nRecently, some works [51, 75, 19], exemplified by ULIP [75], have explored learning multimodal joint\nrepresentations for 3D shapes. They train 3D-native shape encoders by aligning 3D shape embeddings\nwith CLIP\u2019s language and/or image embeddings through multimodal contrastive learning. Works like\nReCon [51] further combines cross-modal contrastive learning with masked auto-encoding for added\nenhancement. While these methods allow for zero-shot 3D classification through the computation of\n3D-text similarity, the amount of distilled knowledge and their model capability are heavily limited\nby the small-scale training datasets used. Our work follows this paradigm but aims to learn more\ngeneralizable and scalable representations to enable open-world 3D shape understanding.\n3\nMethod\nWe propose a novel method, OpenShape, for learning generalizable and scalable multi-modal joint\nrepresentation between language, 2D images, and 3D shapes, as shown in Figure 2. We first\nintroduce the multi-modal contrastive learning framework we used for aligning representations of\nthree modalities in Section 3.1. We then elaborate how we create our training sets and enrich our text\ndata in Sections 3.2 and 3.3. In Section 3.4, we present how we scale up our 3D backbone models.\nFinally, we propose a hard negative mining strategy to enhance contrastive learning in Section 3.5.\n3.1\nMulti-Modal Representation Alignment\nWe aim to learn 3D shape representations that are aligned with pretrained CLIP embedding spaces\nof language and image. As shown in Figure 2 (c), we train a 3D native encoder f P that takes a 3D\npoint cloud as input and extracts 3D shape feature. Following previous works [51, 75, 19], such as\nULIP [75], we utilize multi-modal contrastive learning for representation alignment. Since CLIP is\npretrained on a much larger scale data, we freeze both its text encoder f T and its image encoder f I\n3\ncatalog images\nhigh-resolution geometry & texture\nphysically-based renderings\nShapeNet\n(52.5k)\n3D-FUTURE\n(16.6k)\nABO\n(8.0k)\n(a) Ensemble Datasets\n(b) Text Filtering & Enrichment\nZero-Shot\nClassi\ufb01cation\nText-to-3D\n(Retrieval)\n3D-to-Text\n(Captioning)\nImage-to-3D\n(Retrieval)\n3D-to-Image\n(Generation)\n(c) Cross-Modal Alignment\n(d) Cross-Modal Applications\nObjaverse\n(798.8k)\noriginal \ntexts\n\ufb01ltered\ntexts\nGPT4\n2D\nrenderings\nImage\nCaption\nImage \nRetrieval\ncaptions\nretrieved\ntexts\nEnriched Texts\nXXL\nHard Negative\nMining\nText \nEncoder\nImage \nEncoder\nPointCloud \nEncoder\nFigure 2: (a) We ensemble four public 3D shape datasets, resulting in 876k shapes that encompass\ndiverse categories and concepts. (b) We propose three strategies to automatically filter and enrich\nthe noisy texts in the original datasets. (c) We train a 3D point cloud encoder to align the 3D shape\nembedding space with the CLIP\u2019s text and image embedding spaces. We perform cross-modal\ncontrastive learning with scaled 3D backbones and hard negative mining. (d) OpenShape embeddings\ncan be easily integrated with other CLIP-based models, enabling various cross-modality tasks.\nduring feature alignment to preserve CLIP\u2019s feature priors and avoid model collapse. Specifically,\ngiven a sampled batch of triplets {(Pi, Ti, Ii)}, where Pi denotes a point cloud of a 3D shape, Ti and\nIi denote corresponding text and image, the contrastive loss is calculated as:\n\u2212 1\n4n\nX\ni\n \nlog\nexp(hP\ni \u00b7 hT\ni /\u03c4)\nP\nj exp(hP\ni \u00b7 hT\nj /\u03c4) + log\nexp(hT\ni \u00b7 hP\ni /\u03c4)\nP\nj exp(hT\ni \u00b7 hP\nj /\u03c4) + log\nexp(hP\ni \u00b7 hI\ni /\u03c4)\nP\nj exp(hP\ni \u00b7 hI\nj /\u03c4) + log\nexp(hI\ni \u00b7 hP\ni /\u03c4)\nP\nj exp(hI\ni \u00b7 hP\nj /\u03c4)\n!\n(1)\nwhere n is the number of shapes in a batch; \u03c4 is a learnable temperature; hP\ni = f P (Pi)/|f P (Pi)|,\nhT\ni = gT (f T (Ti))/|gT (f T (Ti))|, and hI\ni = gI(f I(Ii))/|gI(f I(Ii))| denote normalized projected\nfeatures of Pi, Ti, and Ii, where gT and gI are two learnable linear projections. Since f T and f I are\nfrozen, we extract all f T (Ti) and f I(Ii) before training and cache them for acceleration. In most of\nour experiments, we utilize OpenCLIP ViT-bigG-14 [25] as the pretrained CLIP model.\n3.2\nEnsembling 3D Datasets\nSince the scale and diversity of training triplets play a crucial role in learning scalable shape represen-\ntations, we ensemble four currently-largest public 3D datasets for training as shown in Figure 2 (a), re-\nsulting in 876k training shapes. Among these four datasets, ShapeNetCore [8], 3D-FUTURE [16] and\nABO [11] are three popular datasets used by prior works. They contain human-verified high-quality\n3D shapes, but only cover a limited number of shapes and dozens of categories. The Objaverse [12]\ndataset is a more recent dataset, containing many more 3D shapes and covering significantly more\ndiverse categories. However, shapes in Objaverse are mainly uploaded by web users and not verified\nby experts, and thus have uneven quality and exhibit highly unbalanced distributions, necessitating\nfurther processing.\nTo create triplets for training, for each shape, we sample 10,000 points from the mesh surface and\ninterpolate the point colors according to the mesh textures. We also render 12 color images from the\npreset camera poses that uniformly cover the whole shape. For datasets providing thumbnails, we\ninclude them as part of image candidates, since they typically capture the shape from a better camera\nview. For the Objaverse dataset, we use the model name as the raw text for each shape. For other\ndatasets, we utilize provided metadata to create raw texts (see supplementary for details). During\neach pretraining iteration, we randomly sample one rendered image or thumbnail for each shape, and\napply standard augmentation to the point clouds [75].\n3.3\nText Filtering and Enrichment\nWe find that only applying contrastive learning between 3D shapes and 2D images is insufficient to\nfuel zero-shot 3D classification, even when training on large-scale datasets. We conjecture that this is\ncaused by the inherent domain gap in CLIP\u2019s language and image embedding spaces, which is also\nobserved by previous studies [36, 67]. Consequently, 3D-text alignment is not guaranteed even if we\nobtain good 3D-image alignments via contrastive learning. Therefore, we need to explicitly align 3D\nshapes with text. Along this process, to facilitate better 3D-text alignment, we introduce 3 techniques\nto improve the text quality: filtering, captioning, and image retrieval, as shown in Figure 2 (b).\nFiltering. As shown in Figure 3, the 3D shapes from our main dataset, Objaverse, is dominated\nwith noisy text descriptions (\u201cnames\u201d) uploaded by web users. Many of the problematic texts can be\nidentified from the text itself without seeing the corresponding 3D shape. We thus leverage a powerful\n4\nname: \u201chomework xyz detailing\u201d\nGPT4: Remove\n\u201cSteampunk Goggles by MonoFlow on \u2026\u201d\n\u201cSteampunk Goggles Made from hand ...\u201d\nazure: \u201ca pair of steampunk goggles\u201d\nblip: \u201csteampunk goggles 3d model\u201d\nname: \u201cTue, 09 Oct 2018 17:12:39\u201d\nGPT4: Remove\n\u201carmchair\u201d\n\u201csome of the other props done Chair10\u201d\nazure: \u201ca blue plastic chair on a pink ...\u201d\nblip: \u201ca 3d model of a blue shaped object\u201d\nname: \u201cDOG A - 1of6 - for free \u2026\u201d\nGPT4: Keep\n\u201cBlack Labrador in front of a white \u2026\u201d\n\u201cBlack Labrador puppy Vinyl Wall Mural\u201d\nazure: \u201ca black dog sitting on a blue...\u201d\nblip: \u201ca black dog sitting on a blue...\u201d\nname: \u201cuntitled\u201d\nGPT4: Remove\n\u201cNike AirMax 1 (Red/White)\u201d\nazure: \u201ca close up of a shoe\u201d\nblip: \u201cnike air max 1 - white / red\u201d\n\u201cnike air max red and white\u201d\nFigure 3: Text Filtering & Enrichment Examples In each example, the left section features the\nthumbnail, model name, and GPT-4 filtering results. The upper right section shows image captions\nfrom two captioning models, while the lower right section displays retrieved images and their\ncorresponding texts.\nTable 1: Comparison of different 3D backbones before scal-\ning up their parameters. Models are trained on ShapeNet [8]\nor our ensembled dataset excluding Objaverse-LVIS [12].\nZero-shot classification performance are evaluated on Model-\nNet40 [72] and Objaverse-LVIS [12].\nModel\n#Param.\nTrain on ShapeNet [8] Train on Ens-no-LVIS\nMNet40\nO-LVIS\nMNet40\nO-LVIS\nPointNet [49]\n1.3M\n67.0\n9.3\n74.9\n24.4\nDGCNN [70]\n2.3M\n67.8\n9.0\n74.2\n24.8\nPointMLP [40]\n9.3M\n73.5\n12.9\n82.9\n36.6\nPointNeXt [52]\n2.8M\n72.6\n12.2\n81.6\n33.8\nPointBERT [78]\n5.1M\n70.3\n10.8\n84.5\n37.0\nSparseConv [10]\n5.3M\n70.7\n10.6\n78.8\n31.7\nstd. dev.\n2.3\n1.4\n3.9\n5.1\n1M\n4M\n16M\n48M\n# Parameters\n25\n30\n35\nO-LVIS Acc. (%)\nDGCNN\nPointNet\nPointNeXt\nPointBERT\nSparseConv\nFigure 4: Accuracy on Objaverse-\nLVIS [12] when scaling up the pa-\nrameters of different models.\nlarge language model, GPT-4 [45], to filter out inaccurate or uninformative text descriptions. We\nfind that GPT-4 excels at recognizing irrelevant contents, such as timestamps, pure model numbers,\nincomprehensible descriptions, random filenames (e.g., new project), and random characters. Through\nGPT-4, we filter out about 30% of raw user texts. Note that we only filter the texts, and still keep all\nshapes for training. More details, such as the prompts we used, are presented in the supplementary.\nCaptioning. We utilize BLIP [34] and the Azure cognition services to caption the 2D thumbnails (if\npresent, or images rendered from a fixed frontal view) of the 3D models, obtaining two texts for each\nshape. As shown in Figure 3, the captioning models can usually produce meaningful and descriptive\ncaptions that either enhance user-uploaded texts or replace low-quality ones. We also notice that the\ntwo caption models complement each other, leading to better performance.\nImage Retrieval. In addition to image captioning, we also perform image retrieval to obtain additional\ndescriptions of 3D models. We retrieve k-NN images of shape renderings from the LAION-5B dataset\n[63] using the CLIP ViT-L retrieval index [6]. We then take the captions of the k-NN images as the\nretrieved texts for our 3D models. Compared with captioning model generations, retrieved texts cover\na wider range of text styles. They can also include more fine-grained semantics than both the user\ntexts and the generated captions (e.g., \u201cLabrador\u201d in Figure 3).\nIn each iteration of pretraining, for each shape, we first randomly sample a text source category\namong the raw text (if unfiltered), the captions, and the retrieved texts. We then select a text candidate\nfrom the selected category. We also apply the template-based prompt engineering technique used in\nULIP [75] to both training texts and test-time category names. Specifically, we extend a word or a\nphrase to a collection of templated simple sentences and take their average embedding.\n5\n3.4\nScaling Up 3D Point Cloud Backbones\nPrevious works on 3D point cloud learning have primarily focused on smaller-scale datasets like\nShapeNet. These techniques may not be directly applicable to our larger-scale ensembled dataset and\nneed to be scaled up accordingly. We find that different 3D backbones may exhibit distinct behavior\nand scalability when trained on datasets with varying sizes. Specifically, we compare six popular\nbackbones trained on ShapeNet or our ensembled dataset by evaluating their zero-shot classification\nperformance on ModelNet40 [72] and Objaverse-LVIS datasets (for now, these backbones are trained\nwith their original configurations and without scaling up model sizes). Objaverse-LVIS is a subset of\nObjaverse dataset with human-verified category labels. With 1,156 categories, it serves as a suitable\ndataset for evaluating zero-shot long-tail classification, and we exclude all shapes of Objaverse-\nLVIS from this experiment. Results are shown in Table 1. We find that when trained on ShapeNet,\nall backbones share similar performances. However, when trained on our ensembled dataset, the\nperformance gap between backbones increases significantly. This suggests that while the original\nversions of these backbones share a similar number of parameters, some may have been saturated\nwhen trained on small datasets, while others do not.\nWe also explore the performance and scalability of these backbones when scaling up the model sizes\nand training on our ensembled dataset. Please refer to the supplementary for details on how we scale\nup each model. As shown in Figure 4, we observe that all 3D backbones benefit significantly from\nmodel scaling. However, traditional backbones without a shrinking hierarchical structure, such as\nDGCNN and PointNet, require operating completely on dense points or modeling the relationships\n(e.g., through kNN) between dense points. As a result, they become more time-consuming and\nmemory-intensive when scaled up compared to more modern backbones. We therefore select\nPointBERT [78] (Transformer-based) and SparseConv [10] (convolution-based) as our 3D backbones\nfor the remaining experiments, as they exhibit strong performance and scalability.\n3.5\nHard Negative Mining\nOur ensembled dataset exhibits a high degree of class imbalance. Certain common categories, such\nas building, may occupy tens of thousands of shapes, while many other categories, such as walrus\nand wallet, are underrepresented with only a few dozen or even fewer shapes. Consequently, when\nrandomly constructing batches, it is unlikely that shapes from two confusing categories (e.g., apples\nand cherries) will be contrasted within the same batch. Inspired by some previous works [56, 30], we\npropose an offline hard negative mining strategy for improving the training efficiency and performance.\nSpecifically, in the first round of training, we train our model with random batches until it is about\nto converge. We then compute the kNN for each shape in the learned 3D embedding space. In the\nsecond round of training, for each iteration, we randomly select s seed shapes and then obtain m\nneighbors from the kNN results of each seed shape, resulting s \u00d7 m shapes per batch. In this way,\nconfusing pairs are more likely to be selected in a single batch. However, this may also introduce\nfalse negative pairs (e.g., two apples) into contrastive learning. To mitigate this issue, we leverage\nimage and text embeddings to filter out pairs sharing similar texts when calculating the contrastive\nloss. Specifically, for two shapes i and j selected from the same seed shape, if hT\nj \u00b7 hI\ni + \u03b4 > hT\ni \u00b7 hI\ni ,\nwhere hT and hI are text and image embeddings, and \u03b4 is a small threshold, we believe that the text\nembeddings of i and j are very close to each other, and we remove j from i\u2019s negative examples\nwhen calculating contrastive loss. By employing this strategy to construct batches, we observe faster\nand better model learning.\n4\nExperiments\n4.1\nZero-Shot Shape Classification\nWe evaluate the zero-shot classification performances of our models on three benchmarks: the\ntraditional ModelNet40 [72] and ScanObjectNN [68], as well as a new benchmark, Objaverse-\nLVIS [12]. ModelNet40 and ScanObjacetNN consist of 40 and 15 common categories, respectively.\nObjaverse-LVIS is an annotated subset of Objaverse [12] and comprises 46,832 shapes among 1,156\nLVIS [17] categories. With a much larger base of classes than other benchmarks, Objaverse-LVIS\npresents a challenging long-tailed distribution, making it a better reflection on models\u2019 performance\nin open-world scenarios. We compare OpenShape with existing zero-shot approaches, including\nPointCLIP [82], PointCLIPv2 [84], ReCon [51], CG3D [19], CLIP2Point [24], and ULIP [75].\nAmong them, PointCLIP [82] and PointCLIPv2 [84] project point clouds into 2D images and\ndirectly utilize 2D CLIP for inference, while other methods leverage the CLIP embedding spaces for\n6\nTable 2: Zero-shot classification on Objaverse-LVIS [12], ModelNet40 [72], and ScanObjectNN [67].\nMethod\ntraining shape\nObjaverse-LVIS [12]\nModelNet40 [72]\nScanObjectNN [68]\nsource\nTop1\nTop3\nTop5\nTop1\nTop3\nTop5\nTop1\nTop3\nTop5\nPointCLIP [82]\n2D inferences,\nno 3D Training\n1.9\n4.1\n5.8\n19.3\n28.6\n34.8\n10.5\n20.8\n30.6\nPointCLIP v2 [84]\n4.7\n9.5\n12.9\n63.6\n77.9\n85.0\n42.2\n63.3\n74.5\nReCon [51]\nShapeNet\n1.1\n2.7\n3.7\n61.2\n73.9\n78.1\n42.3\n62.5\n75.6\nCG3D [19]\n5.0\n9.5\n11.6\n48.7\n60.7\n66.5\n42.5\n57.3\n60.8\nCLIP2Point [24]\n2.7\n5.8\n7.9\n49.5\n71.3\n81.2\n25.5\n44.6\n59.4\nULIP-PointBERT (Official) [75]\n6.2\n13.6\n17.9\n60.4\n79.0\n84.4\n51.5\n71.1\n80.2\nOpenShape-SparseConv\n11.6\n21.8\n27.1\n72.9\n87.2\n93.0\n52.7\n72.7\n83.6\nOpenShape-PointBERT\n10.8\n20.2\n25.0\n70.3\n86.9\n91.3\n51.3\n69.4\n78.4\nULIP-PointBERT (Retrained)\nEnsembled\n21.4\n38.1\n46.0\n71.4\n84.4\n89.2\n46.0\n66.1\n76.4\nOpenShape-SparseConv\n(no LVIS)\n37.0\n58.4\n66.9\n82.6\n95.0\n97.5\n54.9\n76.8\n87.0\nOpenShape-PointBERT\n39.1\n60.8\n68.9\n85.3\n96.2\n97.4\n47.2\n72.4\n84.7\nULIP-PointBERT (Retrained)\nEnsembled\n26.8\n44.8\n52.6\n75.1\n88.1\n93.2\n51.6\n72.5\n82.3\nOpenShape-SparseConv\n43.4\n64.8\n72.4\n83.4\n95.6\n97.8\n56.7\n78.9\n88.6\nOpenShape-PointBERT\n46.8\n69.1\n77.0\n84.4\n96.5\n98.0\n52.2\n79.7\n88.7\nObjaverse-LVIS\nModelNet40\nScanObjectNN\n10\n20\n30\n40\n50\n50\n60\n70\n80\n90\n30\n40\n50\n60\n70\n80\nPointCLIP V2\nULIP-Of\ufb01cial\nULIP-Retrained\nOpenShape-PointBERT\n0 1 2\n4\n8\n16\n0 1 2\n4\n8\n16\n0 1 2\n4\n8\n16\n# of labeled training samples per class\nTop-1 Acc. (%)\nFigure 5: Few-shot linear probing on Objaverse-LVIS [12], ModelNet40 [72], and ScanOb-\njectNN [67]. We report the average performance over 10 random seeds.\nalignment and require 3D shapes for training. We report results on these baselines using their released\ncheckpoints. To better analyze the source of our performance gains, we also retrain the baseline\nULIP [75] on our ensembled shape dataset, but we use the original texts in the four constituent\ndatasets along with the official codebase without backbone scaling. We train OpenShape and ULIP\non three different sets of training shapes: \u201cEnsembled\u201d denotes using all shapes from the four\ndatasets; \u201cEnsembled (no LVIS)\u201d is the same but excludes all shapes from the Objavserse-LVIS\nsubset; \u201cShapeNet\u201d only includes shapes from the ShapeNet [8] dataset. Note that even when LVIS\nshapes are included in the training shapes (i.e., the \u201cEnsembled\u201d dataset), their test-time category\nlabels are probably not included in their training texts. Please refer to the supplementary for more\ntraining and evaluation details.\nTable 2 shows the results. We observe that OpenShape consistently outperforms prior approaches,\neven when trained only on ShapeNet. When models are trained on our larger-scale ensembled dataset,\nthey receive a significant performance boost. In this case, OpenShape still surpasses retrained ULIP\nby a significant margin, demonstrating the advantages of our text enrichment, backbone scaling, and\nother training strategies. Specifically, OpenShape greatly improves the classification accuracy on the\nlong tail categories in Objaverse-LVIS from a dull < 10% to 46.8%, outperforming the retrained\nULIP by about 20 points and reaching a decent top-5 accuracy of 77.0%. These results demonstrate\nOpenShape\u2019s capability to recognize open-world objects effectively. As for ModelNet40, OpenShape\nachieves a 85.3% accuracy, surpassing previous methods by a substantial margin of at least 20 percent.\nOpenShape also achieves impressive top-3 and top-5 accuracies of 96.5% and 98.0%. To the best\nof our knowledge, this is the first time zero-shot methods have matched the performance of a fully-\nsupervised 3D learning method on ModelNet40, where OpenShape outperforms fully-supervised\n3D ShapeNets [72] and VoxNet [41]. In addition, on ScanObjectNN, which contains challenging\nreal scans with noise and occlusion, OpenShape exhibits decent sim-to-real transfer capabilities. To\ncontextualize, OpenShape-SparseConv achieves 56.7% zero-shot accuracy on ScanObjectNN without\nspecific sim-to-real training, which surpasses 52.7% reported by SKPConv [71], a recent method\nspecially designed for sim-to-real transfer in point cloud classification tasks.\n7\nTable 3: Ablation study. Top 1 zero-\nshot accuracies on ModelNet40 [72] and\nObjaverse-LVIS [12] are shown.\nVariant\nO-LVIS\nMNet40\nNo Objaverse shapes\n13.9\n75.5\nOnly Objaverse shapes\n41.6\n79.2\nNo backbone scale up\n31.7\n78.7\nNo caption & retrieval\n37.0\n82.9\nNo text filtering\n41.4\n82.9\nNo point rgb, only xyz\n39.6\n83.6\nNo text contras. learning\n23.3\n67.4\nNo image contras. learning\n41.0\n81.0\nFull\n42.0\n83.1\nFull + hard mining\n43.4\n83.4\n1%\n5%\n25%\n50%\n75%\n100%\n15\n20\n25\n30\n35\n40\n45\nO-LVIS Acc. (%)\n61\n65\n69\n73\n77\n81\n85\nModelNet40 Acc. (%)\nFigure 6: Ablation study on\nusing different ratios of train-\ning data.\nBase\n+ Cap. + Retr.\nFull\n30\n35\n40\n45\n50\nO-LVIS Acc. (%)\nPointBERT\nSparseConv\nFigure 7:\nAblation study\non different text enrichment\nstrategies.\nFigure 8: 3D shape retrieval from image (left, mid) and point cloud (right).\n4.2\nFew-Shot Linear Probing\nIn the literature, linear probing is a common way to assess the representation learning capabilities of\na model. To perform linear probing, we gather and freeze the representation vectors from all samples\nin a dataset. Subsequently, we train a linear classifier using these fixed vectors and few-shot class\nlabels. We evaluate the accuracy of the linear classifier on three benchmarks: Objaverse-LVIS [12],\nModelNet40 [72], and ScanObjectNN [68]. Figure 5 summarizes the performance of OpenShape\nin comparison with ULIP [75] (official release and our retrained versions) and PointCLIPv2 [84].\nOn the most challenging Objaverse-LVIS benchmark, OpenShape outperforms all other methods by\na large margin. Notably, zero-shot OpenShape beats few-shot linear probes of other methods. On\nModelNet40 and ScanObjectNN, we do not see a large performance margin between OpenShape and\nretrained ULIP. We hypothesize that for few-shot ModelNet40, the error is dominated by in-category\nsample bias rather than the representation quality; while for ScanObjectNN, the domain gap plays a\nmajor role. Since both OpenShape and retrained ULIP are exposed to the same source domain of\ntraining objects, their few-shot out-of-domain generalization performances tend to be similar.\n4.3\nAblation Study\nWe perform various ablations by training a scaled version of SparseConv [10] on the ensembled\ndataset and then evaluate it on the Objaverse-LVIS [12] and ModelNet40 [72] zero-shot classification\nbenchmarks, unless otherwise specified. The results are shown in Table 3 and Figures 6 and 7.\nData and Model Scaling. We investigate the impact of training data by ablating (1) without or with\nonly Objaverse shapes (Tab. 3) and (2) with different ratios of our ensembled dataset (Fig. 6). We\nobserve that training with 1% of our ensembled dataset (about 8.8k shapes) achieves similar or better\nzero-shot performance than training without Objaverse shapes (about 77.1k shapes), indicating that\nthe diversity of training data is sometimes more crucial than the scale. In addition, we compare the\nperformances between scaled-up and non-scaled-up backbones. From Tab. 3, we demonstrate that\nmodel scaling plays an essential role when training on our large-scale ensembled dataset (also Fig. 4).\nText Filtering and Enrichment. As shown in Tab. 3, both text filtering and text enrichment are\nbeneficial for performance. We also investigate the specific text enrichment strategies to use for the\nSparseConv and PointBERT backbones. In Fig. 7, we observe that both image captioning and text\nretrieval are helpful, and including both yield the best results. Notably, PointBERT improves more\nthan 10 points from text enrichment, highlighting the significance of enhancing text quality.\nOther Aspects. We also conduct additional ablation studies on color information, contrastive loss\ncomponents, and our hard-negative mining strategy in Tab. 3. We observe that OpenShape performs\nwell with only xyz coordinates as input and no RGB color. While 3D-image contrastive loss is also\nhelpful, we observe that 3D shape-text alignment plays a very essential role for model zero-shot\ngeneralization, which necessitates our text filtering and text enrichment strategies that significantly\n8\nFigure 9: Text-input 3D shape retrieval. In each row, we show input texts on the left and two\nretrieved shapes for each text on the right. OpenShape embedding encodes a wide range of visual\nand semantic concepts and enables (a) retrieval of fine-grained subcategories (first two rows), and (b)\ncontrol of attributes (e.g., color, shape, style) and their combinations (last two rows).\n+ \u201cin a large desert\u201d\n+ \u201cin the woods\u201d\n1.\n2.\n3.\n4.\nCap\u019fons:\n1. The chair in the style of the 1920s.\n2. The ladder to the second \ufb02oor.\n3. Gold\ufb01sh in the sea - photo #.\n4. The car of the day.\n(a)\n(b)\nFigure 10: (a) Point cloud captioning. (b) Point cloud-conditioned image generation. Our\nlearned 3D shape embeddings can be integrated with off-the-shelf pretrained CLIP-based models\n(e.g., captioning and image generation models) to support various cross-modal applications.\nenhance text quality. Lastly, by employing our hard negative mining strategy, OpenShape effectively\naddresses the issue of unbalanced data distribution, leading to further improvements in performance.\n4.4\nCross-Modal Applications\nMulti-modal 3D Shape Retrieval. Through OpenShape multi-modal representations, we can index\nand retrieve 3D shapes from images, texts, or point clouds. In this section, we retrieve 3D shapes\nfrom our ensembled dataset by calculating the cosine similarity between input embedding(s) and 3D\nshape embeddings and performing kNN. As shown in Figure 8, OpenShape is capable of retrieving\nvisually or semantically similar shapes from a single image or point cloud input. OpenShape\nembeddings encode a wide range of visual and semantic concepts. In Figure 9, we show that\nOpenShape supports retrieving 3D shapes from detailed text descriptions, which include fine-grained\nsubcategories, attributes, and their combinations. Note that these input texts are typically not present\nin the raw texts of the retrieved shapes, indicating that OpenShape effectively learns generalizable\nconcepts across shapes. In Figure 1, we provide a demo which takes two 3D shapes as inputs\nand retrieves the shapes that are simultaneously closest to both inputs. This is achieved by finding\narg maxi min(hP\ni \u00b7 hP\na , hP\ni \u00b7 hP\nb ), where hP\na and hP\nb denote normalized shape embeddings of the\ntwo input shapes. We can see that the retrieved shapes integrate visual or semantic elements in an\ninteresting manner, highlighting the rich concepts and priors encoded in OpenShape embeddings.\nShape-Conditioned Multimodal Generation. As OpenShape\u2019s 3D shape representations are aligned\nwith CLIP\u2019s image and text embedding spaces, they can serve as inputs into other CLIP-based models\nto facilitate various multimodal generation applications. For example, we show that by feeding\nour 3D shape embeddings into ClipCap [44], an off-the-shelf image captioning model, along with\nStable unCLIP [54], a text-to-image diffusion model, we can perform point cloud captioning and\npoint cloud-conditioned image generation (optional text prompt supported) without extra training or\nfinetuning. Qualitative results are shown in Figure 10. Please refer to the supplementary for more\nresults and details.\n9\n5\nDiscussion and Conclusion\nWe introduce OpenShape, a novel approach for learning scalable and generalizable multi-modal\njoint representations for 3D shapes. OpenShape representations effectively capture a wide range of\nsemantic and visual concepts, enabling superior capabilities for open-world 3D shape recognition.\nBy aligning OpenShape with CLIP\u2019s embedding space, our shape embeddings can be integrated with\noff-the-shelf CLIP-based models for various cross-modality applications. Moving forward, there are\nseveral directions worth further exploration: (a) More 3D data. While we utilized 876k 3D shapes\nduring training, this is still quite limited compared to the 2D counterparts. We hope that our work\ninspires future investments in more resources to build even more powerful 3D representations. (b)\nPart-level information. Our current shape representations mainly focus on global semantic and visual\nfeatures, and it would be beneficial to add more part-level supervision during training. (c) Sim-to-real\ndomain gap. Our model is mainly trained on synthetic data, and it\u2019s challenging but crucial to explore\nexplicit designs for reducing the domain gap with real-world shapes.\n6\nAppendix\n6.1\nMore Examples of Multi-Modal 3D Shape Retrieval\nIn Figures 11 and 12, we showcase more examples of multi-modal 3D shape retrieval.\nFigure 11: Image-input 3D shape retrieval. In each triplet, we present the input image and two 3D\nshapes retrieved using OpenShape embeddings from the Objaverse [12] dataset. Input images are\nfrom unsplash.com.\nFigure 12: Point cloud-input 3D shape retrieval. In each triplet, we present the input point cloud\nand two 3D shapes retrieved using OpenShape embeddings from the Objaverse [12] dataset.\n10\n6.2\nMore Examples of Shape-Conditioned Multimodal Generation\nIn Figure 13 and Figure 14, we showcase more examples of point cloud captioning and point\ncloud-conditioned image generation.\nFigure 13: Point cloud captioning. In each row, we show the input point clouds on the left and the\ngenerated captions on the right.\nFigure 14: Point cloud-conditioned image generation. Each row shows three examples (input point\nclouds and generated images).\n11\n6.3\nDetails on Raw Text Generation and Filtering\n6.3.1\nRaw Text Generation\nWe leverage the metadata from the four datasets to generate the raw texts. Although the original\ndatasets may contain numerous attributes for each shape, we carefully choose the most informative\nones to compose the text, ensuring its quality and relevance.\nObjaverse:We utilize the name associated with each shape to serve as the text.\nShapeNetCore: For each shape, we generate three types of texts: (a) the name, (b) the category\nname (with a total of 55 categories), and (c) the concatenation of the sub-category names (with a\ntotal of 336 sub-categories), separated by commas.\n3DFuture: For each shape, we generate two types of texts: (a) the category, and (b) the concatena-\ntion of category, style, theme, and material, separated by commas.\nABO: For each shape, we generate two types of texts: (a) the item_name, and (b) the product_type.\nIn this way, we generate one or more raw texts for each shape.\n6.3.2\nRaw Text Filtering\nWe employ GPT-4 [45] to filter out uninformative raw texts. To accomplish this, we divide all the raw\ntexts into batches, each containing 256 entries, and process each batch independently using GPT-4.\nHere is an example illustrating the prompt we used and the corresponding response generated by\nGPT-4.\nI am analyzing a 3D dataset with various text descriptions for the 3D models.\nHowever, many of these texts are inaccurate or uninformative, and therefore,\nnot suitable as descriptions for 3D models. I need your help to identify such\nincorrect texts. Specifically, if a text primarily consists of irrelevant or unin-\nformative content, such as timestamps, model numbers, incomprehensible\ndescriptions, random filenames (e.g., \"my project\"), random characters, etc.,\nplease respond with \"N\". If a text contains a clear noun (or noun phrase)\nthat could potentially describe a 3D object, please respond with \"Y\". You\nwill find a list of texts below, and each line contains a three-digit ID and\nassociated text. For each text, please respond with \"Y\" or \"N\", following the\nID number (e.g., \"001 Y\" or \"002 N\"). Please evaluate all 256 texts.\n000 New project ( 19 )\n001 3December - Chemestry\n002 Fake Brand Soda Can\n003 Spartan Shild\n004 Apple3d\n005 Landmine\n006 FaunveinB-S\n007 FIGURA 5\n008 Sphero Blue\n009 Sofa\n010 Maddox\n011 A3 Complete\n012 Suspension Bridge\n013 Maung\n014 Captain-americas-shield\n015 sphorb4\n......\n000 N\n001 Y\n002 Y\n003 Y\n004 Y\n005 Y\n006 N\n007 N\n008 Y\n009 Y\n010 N\n011 N\n012 Y\n013 N\n014 Y\n015 N\n......\nAfterwards, we combine all the responses to create the final filtering results, effectively removing\napproximately 30% of the raw texts.\n12\n6.4\nDetails on the Backbone Scaling Experiment\nIn Figure 4 of the main paper, we investigate the performance and scalability of various backbones\nwhen scaling up their model sizes. For this experiment, we employ a default resolution of 10,000\npoints for input point clouds, a batch size of 200, and conduct the experiment on a single A100 GPU.\nIn general, if instructions are given in the original paper of a backbone, we scale up the model as\ninstructed. Otherwise, we scale up the model by expanding width or depth (i.e., stacking blocks or\nlayers). Specifically, we scale up each backbone as follow:\nPointBERT [78]\nThe scaling parameters are shown in Table 4. We scaled PointBERT to 72.1M\nparameters beyond the 32.3M version reported in Figure 4 of the main paper. However, at this scale,\nthe model dramatically overfits on the training data and performs worse on all benchmarks than the\n32.3M version.\nTable 4: Hyperparameters for scaling up PointBERT [78].\n# Parameters\n# Layers\nWidth\n# Heads\nMLP Dim\n# Patches\nPatch Embed Dim\n5.1M\n6\n256\n4\n1024\n64\n96\n13.3M\n6\n512\n8\n1024\n64\n128\n32.3M\n12\n512\n8\n1536\n384\n256\n72.1M\n12\n768\n12\n2304\n512\n256\nSparseConv [10]\nThe smallest version (5.3M parameters) of the model is adapted from the\nMinkowskiFCNN model by adjusting the width of the final convolution and linear layers. The\nremaining three models are adaptations of MinkowskiResNet, each varying in the number of basic\nResNet blocks used. See Table 5 for the specific scaling parameters.\nTable 5: Hyperparameters for scaling up SparseConv [10].\n# Parameters\n# Convolution Layers\n# Linear Layers\n5.3M\n7\n4\n29.0M\n18\n3\n33.7M\n26\n3\n41.3M\n42\n3\nPointNeXt [52]\nPointNeXt is proposed as a scalable version of PointNet++ [50], and includes\nS/B/L/XL variants in the original paper. We simply adopt these official configurations.\nDGCNN [70] and PointNet [49]\nFor these two backbones without a hierarchical structure, we\nincrease the width of each layer proportionally to scale up to 4xPointNet and 2xDGCNN before we\nhit the GPU memory limit. As the models operate completely on dense points, it is impractical to use\nthe default 10k-point resolution. We thus reduce the input resolution for the two backbones, resulting\nin 1k points for DGCNN and 4k points for PointNet.\n6.5\nDetails on Training and Evaluation\nTraining Details\nWe freeze the CLIP text and image encoders and train the 3D encoder and two\nprojection heads on our ensembled dataset using the cross-modal contrastive loss. We train the\nmodel on a single A100 GPU with a batch size of 200. Since we precache the text and image CLIP\nembeddings of all shapes, the training is greatly accelerated and takes about 300 A100 hours for\nconvergence. We utilize an exponential learning rate schedule, and employ an range test to find the\ninitial learning rate. For 32.3M version of PointBERT, we utilize a learning rate of 5e \u2212 4; for 72.1M\nversion of PointBERT, we utilize a learning rate of 4e \u2212 4; and for other models, we utilize a learning\nrate of 1e \u2212 3. For hard-negative mining, the number of seed shapes s is set to 40, and the number of\nneighbors m is set to 5 per shape, and the threshold \u03b4 is set to 0.1.\n13\nFine-tuning CLIP Text and Image Encoders?\nAfter training OpenShape-PointBERT, we con-\nducted experiments to unfreeze and finetune the CLIP text encoder for a single epoch. However, the\nresults obtained did not demonstrate any noticeable improvement on the benchmarks. Moreover,\nwe observed that finetuning the CLIP text encoder could potentially undermine the generalization\ncapabilities of CLIP and hinder the integration of OpenShape embeddings into existing CLIP-based\nmodels. As a result, we choose to freeze the CLIP encoders throughout the entire training process.\nEvaluation Details\nWe evaluated all baselines using their publicly released pretrained checkpoints.\nAdditionally, we retrained ULIP [75] on our ensembled training shapes using their official code\nbase and backbone networks. Note that the retrained ULIP model utilized the original raw texts\nfrom the four datasets during training (prompt engineering is also applied), rather than our filtered\nand enriched texts. For ModelNet40 [72], the evaluation is conducted on the test split with 2,468\nshapes. Regarding ScanObjectNN [68], we follow ULIP [75] to evaluate on the OBJ_ONLY version,\nwhich contains 581 test shapes. For Objaverse-LVIS [12], the input is 10,000 sampled points\nwith point colors. For ModelNet40 [72], the input is 10,000 sampled points without color. For\nScanObjectNN [68], we utilize the official 2,048 points without color as input. All methods use the\nsame input during evaluation. The forward inference time on an A100 GPU for a 10,000-point point\ncloud is approximately 0.9ms for OpenShape-SparseConv and 3.8ms for OpenShape-PointBERT.\n6.6\nDetails on Shape-Conditioned Multimodal Generation\nPoint Cloud Captioning\nCLIPCap [44] utilizes a 10-token prefix generated from CLIP image\nembeddings to enable GPT-2 for captioning. In order to align with the off-the-shelf CLIPCap model,\nwe trained a variant of OpenShape-PointBERT that employs CLIP ViT-B/32 embeddings instead\nof OpenCLIP ViT-G/14 used in other experiments. Consequently, we directly input the point cloud\nencoding, without normalization, into CLIPCap for captioning.\nPoint Cloud Conditioned Image Generation\nWe take the Stable Diffusion v2.1 unCLIP\nmodel [54] for image generation and replace the CLIP image condition encoder with our OpenShape\nencoder to perform image generation conditioned on point clouds (and optionally text prompts).\nThe unCLIP model takes CLIP ViT-L/14 embeddings without normalization as input. To match the\nembedding space, we trained a variant of OpenShape-PointBERT with CLIP ViT-L/14 embeddings.\nAdditionally, we noticed a significant mismatching of scales (L2-norm of embedding vectors) be-\ntween ViT-L/14 image embeddings and OpenShape embeddings. To mitigate this issue, we perform\na re-normalization on OpenShape embeddings to a L2-norm of 1\n2\n\u221a\n768, which is our observed mean\nL2-norm of ViT-L/14 image embeddings. We use 50 diffusion steps. The guidance scale can be tuned\nfreely.\nReferences\n[1] Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. Satr: Zero-shot\nsemantic segmentation of 3d shapes. arXiv preprint arXiv:2304.04909, 2023.\n[2] Idan Achituve, Haggai Maron, and Gal Chechik. Self-supervised learning for domain adaptation\non point clouds. In Proceedings of the IEEE/CVF winter conference on applications of computer\nvision, pages 123\u2013133, 2021.\n[3] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning repre-\nsentations and generative models for 3d point clouds. In International conference on machine\nlearning, pages 40\u201349. PMLR, 2018.\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n[5] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Nie\u00dfner. Clipface: Text-guided editing\nof textured 3d morphable models. arXiv preprint arXiv:2212.01406, 2022.\n[6] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval\nsystem with them. https://github.com/rom1504/clip-retrieval, 2022.\n[7] Zehranaz Canfes, M Furkan Atasoy, Alara Dirik, and Pinar Yanardag. Text and image guided\n3d avatar generation and manipulation. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 4421\u20134431, 2023.\n14\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012, 2015.\n[9] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou,\nYu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by\nclip. arXiv preprint arXiv:2301.04926, 2023.\n[10] Christopher Choy, JunYoung Gwak, and Silvio Savarese.\n4d spatio-temporal convnets:\nMinkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3075\u20133084, 2019.\n[11] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu,\nXi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset\nand benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 21126\u201321136, 2022.\n[12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt,\nLudwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\n[13] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet: Unsupervised learning of rotation\ninvariant 3d local descriptors. In Proceedings of the European conference on computer vision\n(ECCV), pages 602\u2013618, 2018.\n[14] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Language-\ndriven open-vocabulary 3d scene understanding. arXiv preprint arXiv:2211.16312, 2022.\n[15] Benjamin Eckart, Wentao Yuan, Chao Liu, and Jan Kautz. Self-supervised learning on 3d point\nclouds by learning discrete generative models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8248\u20138257, 2021.\n[16] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng\nTao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision,\n129:3313\u20133337, 2021.\n[17] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5356\u20135364, 2019.\n[18] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d\nvision-language models. In Conference on Robot Learning, 2022.\n[19] Deepti Hegde, Jeya Maria Jose Valanarasu, and Vishal M Patel. Clip goes 3d: Leveraging\nprompt tuning for language grounded 3d recognition. arXiv preprint arXiv:2303.11313, 2023.\n[20] Georg Hess, Johan Jaxing, Elias Svensson, David Hagerman, Christoffer Petersson, and Lennart\nSvensson. Masked autoencoder for self-supervised pre-training on lidar point clouds. In\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages\n350\u2013359, 2023.\n[21] Georg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, and Kalle \u00c5str\u00f6m.\nLidarclip or: How i learned to talk to point clouds. arXiv preprint arXiv:2212.06858, 2022.\n[22] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu.\nAvatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022.\n[23] Rui Huang, Xuran Pan, Henry Zheng, Haojun Jiang, Zhifeng Xie, Shiji Song, and Gao Huang.\nJoint representation learning for text and 3d point cloud. arXiv preprint arXiv:2301.07584,\n2023.\n[24] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang,\nand Wangmeng Zuo. Clip2point: Transfer clip to point cloud classification with image-depth\npre-training. arXiv preprint arXiv:2210.01055, 2022.\n[25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan\nTaori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,\nAli Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it\nas below.\n[26] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot\ntext-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 867\u2013876, 2022.\n15\n[27] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen,\nShuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion:\nOpen-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023.\n[28] Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3d textured meshes. arXiv preprint\narXiv:2109.12922, 2021.\n[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-\nHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In International Conference on Machine Learning, pages\n4904\u20134916. PMLR, 2021.\n[30] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus.\nHard negative mixing for contrastive learning. Advances in Neural Information Processing\nSystems, 33:21798\u201321809, 2020.\n[31] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf:\nLanguage embedded radiance fields. arXiv preprint arXiv:2303.09553, 2023.\n[32] Nasir Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Text to mesh without 3d\nsupervision using limit subdivision. arXiv preprint arXiv:2203.13333, 2022.\n[33] Han-Hung Lee and Angel X Chang. Understanding pure clip guidance for voxel grid nerf\nmodels. arXiv preprint arXiv:2209.15172, 2022.\n[34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-\nimage pre-training for unified vision-language understanding and generation. In International\nConference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\n[35] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu\nZhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image\npre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022.\n[36] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind\nthe gap: Understanding the modality gap in multi-modal contrastive representation learning.\nAdvances in Neural Information Processing Systems, 35:17612\u201317625, 2022.\n[37] Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su.\nPartslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models.\narXiv preprint arXiv:2212.01558, 2022.\n[38] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-Wing Fu. Iss: Image as stetting stone\nfor text-guided 3d shape generation. arXiv preprint arXiv:2209.04145, 2022.\n[39] Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer,\nand Shanghang Zhang. Open-vocabulary point-cloud object detection without 3d annotation.\narXiv preprint arXiv:2304.00788, 2023.\n[40] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local\ngeometry in point cloud: A simple residual mlp framework. arXiv preprint arXiv:2202.07123,\n2022.\n[41] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-\ntime object recognition. In 2015 IEEE/RSJ international conference on intelligent robots and\nsystems (IROS), pages 922\u2013928. IEEE, 2015.\n[42] Benedikt Mersch, Xieyuanli Chen, Jens Behley, and Cyrill Stachniss. Self-supervised point\ncloud prediction using 3d spatio-temporal convolutional networks. In Conference on Robot\nLearning, pages 1444\u20131454. PMLR, 2022.\n[43] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-\ndriven neural stylization for meshes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13492\u201313502, 2022.\n[44] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning.\narXiv preprint arXiv:2111.09734, 2021.\n[45] OpenAI. Gpt-4 technical report, 2023.\n[46] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked\nautoencoders for point cloud self-supervised learning. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part II, pages\n604\u2013621. Springer, 2022.\n[47] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas\nFunkhouser, et al. Openscene: 3d scene understanding with open vocabularies. arXiv preprint\narXiv:2211.15654, 2022.\n16\n[48] Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, and Vladimir G Kim. Self-supervised\nlearning of point clouds via orientation estimation. In 2020 International Conference on 3D\nVision (3DV), pages 1018\u20131028. IEEE, 2020.\n[49] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point\nsets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 652\u2013660, 2017.\n[50] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical\nfeature learning on point sets in a metric space. Advances in neural information processing\nsystems, 30, 2017.\n[51] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li\nYi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative\npretraining. arXiv preprint arXiv:2302.02318, 2023.\n[52] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny,\nand Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling\nstrategies. arXiv:2206.04670, 2022.\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021.\n[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[55] Yongming Rao, Jiwen Lu, and Jie Zhou. Global-local bidirectional reasoning for unsupervised\nrepresentation learning of 3d point clouds. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5376\u20135385, 2020.\n[56] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning\nwith hard negative samples. arXiv preprint arXiv:2010.04592, 2020.\n[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[58] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic\nsegmentation in the wild. arXiv preprint arXiv:2204.07761, 2022.\n[59] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022.\n[60] Aditya Sanghi. Info3d: Representation learning on 3d objects using mutual information\nmaximization and contrastive learning. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIX 16, pages 626\u2013642.\nSpringer, 2020.\n[61] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero,\nand Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n18603\u201318613, 2022.\n[62] Jonathan Sauder and Bjarne Sievers. Self-supervised deep learning on point clouds by recon-\nstructing space. Advances in Neural Information Processing Systems, 32, 2019.\n[63] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[64] Charu Sharma and Manohar Kaul. Self-supervised few-shot learning on point clouds. Advances\nin Neural Information Processing Systems, 33:7212\u20137221, 2020.\n[65] Chao Sun, Zhedong Zheng, Xiaohan Wang, Mingliang Xu, and Yi Yang. Point cloud pre-training\nby mixing and disentangling. arXiv e-prints, pages arXiv\u20132109, 2021.\n[66] Ali Thabet, Humam Alwassel, and Bernard Ghanem. Self-supervised learning of local features\nin 3d point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 938\u2013939, 2020.\n[67] Vishaal Udandarao. Understanding and fixing the modality gap in vision-language models.\nMaster\u2019s thesis, University of Cambridge, 2022.\n17\n[68] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.\nRevisiting point cloud classification: A new benchmark dataset and classification model on\nreal-world data. In Proceedings of the IEEE/CVF international conference on computer vision,\npages 1588\u20131597, 2019.\n[69] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt J Kusner. Unsupervised point\ncloud pre-training via occlusion completion. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 9782\u20139792, 2021.\n[70] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M\nSolomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics\n(tog), 38(5):1\u201312, 2019.\n[71] Jean-Baptiste Weibel, Timothy Patten, and Markus Vincze. Sim2real 3d object classification\nusing spherical kernel point convolution and a deep center voting scheme. arXiv preprint\narXiv:2103.06134, 2021.\n[72] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and\nJianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1912\u20131920, 2015.\n[73] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast:\nUnsupervised pre-training for 3d point cloud understanding. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages\n574\u2013591. Springer, 2020.\n[74] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua\nGao. Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion\nmodels. arXiv preprint arXiv:2212.14704, 2022.\n[75] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu,\nJuan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language,\nimage and point cloud for 3d understanding. arXiv preprint arXiv:2212.05171, 2022.\n[76] Jihan Yang, Runyu Ding, Zhe Wang, and Xiaojuan Qi. Regionplc: Regional point-language\ncontrastive learning for open-world 3d scene understanding. arXiv preprint arXiv:2304.00962,\n2023.\n[77] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert:\nPre-training 3d point cloud transformers with masked point modeling. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19313\u201319322, 2022.\n[78] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert:\nPre-training 3d point cloud transformers with masked point modeling. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n[79] Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan\nYeung, Zhen Yang, Xiaodan Liang, and Hang Xu. Clip\u02c6 2: Contrastive language-image-point\npretraining from real-world point cloud data. arXiv preprint arXiv:2303.12417, 2023.\n[80] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang\nDai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization\nand vision-language understanding. arXiv preprint arXiv:2206.05836, 2022.\n[81] Junbo Zhang, Runpei Dong, and Kaisheng Ma. Clip-fo3d: Learning free open-world 3d scene\nrepresentations from 2d dense clip. arXiv preprint arXiv:2303.04748, 2023.\n[82] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng\nGao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8552\u20138562, 2022.\n[83] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of\n3d features on any point-cloud. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10252\u201310263, 2021.\n[84] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyao Zeng, Shanghang Zhang, and Peng Gao. Point-\nclip v2: Adapting clip for powerful 3d open-world learning. arXiv preprint arXiv:2211.11682,\n2022.\n18\n"
  },
  {
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "link": "https://arxiv.org/pdf/2305.10601.pdf",
    "upvote": "3",
    "text": "Tree of Thoughts: Deliberate Problem Solving\nwith Large Language Models\nShunyu Yao\nPrinceton University\nDian Yu\nGoogle DeepMind\nJeffrey Zhao\nGoogle DeepMind\nIzhak Shafran\nGoogle DeepMind\nThomas L. Griffiths\nPrinceton University\nYuan Cao\nGoogle DeepMind\nKarthik Narasimhan\nPrinceton University\nAbstract\nLanguage models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level, left-to-right\ndecision-making processes during inference. This means they can fall short in\ntasks that require exploration, strategic lookahead, or where initial decisions play\na pivotal role. To surmount these challenges, we introduce a new framework for\nlanguage model inference, \u201cTree of Thoughts\u201d (ToT), which generalizes over the\npopular \u201cChain of Thought\u201d approach to prompting language models, and enables\nexploration over coherent units of text (\u201cthoughts\u201d) that serve as intermediate steps\ntoward problem solving. ToT allows LMs to perform deliberate decision making\nby considering multiple different reasoning paths and self-evaluating choices to\ndecide the next course of action, as well as looking ahead or backtracking when\nnecessary to make global choices. Our experiments show that ToT significantly\nenhances language models\u2019 problem-solving abilities on three novel tasks requiring\nnon-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\nFor instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\nsolved 4% of tasks, our method achieved a success rate of 74%. Code repo with all\nprompts: https://github.com/princeton-nlp/tree-of-thought-llm.\n1\nIntroduction\nOriginally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25,\n26, 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider\nrange of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is\nperhaps surprising that underlying all this progress is still the original autoregressive mechanism for\ngenerating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such\na simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what\nproblems would challenge the current paradigm, and what should be alternative mechanisms?\nThe literature on human cognition provides some clues to answer these questions. Research on \u201cdual\nprocess\u201d models suggests that people have two modes in which they engage with decisions \u2013 a fast,\nautomatic, unconscious mode (\u201cSystem 1\u201d) and a slow, deliberate, conscious mode (\u201cSystem 2\u201d)\n[30, 31, 16, 15]. These two modes have previously been connected to a variety of mathematical\nmodels used in machine learning. For example, research on reinforcement learning in humans and\nother animals has explored the circumstances under which they engage in associative \u201cmodel free\u201d\nlearning or more deliberative \u201cmodel based\u201d planning [7]. The simple associative token-level choices\nof LMs are also reminiscent of \u201cSystem 1\u201d, and thus might benefit from augmentation by a more\ndeliberate \u201cSystem 2\u201d planning process that (1) maintains and explores diverse alternatives for current\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.10601v2  [cs.CL]  3 Dec 2023\nj\u0169\u019c\u0154\u0169\u019c\nj\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00ca\u02b2\u02e4Gj\n\u02b1\u00e6\u02b2\u02e4\u001d\u0135\u00c9\n\u02e4j\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00e7\u02b2\u02e4\u001d\u0135\u00c9\u02c1\u0092\u001d\na\u00ca\u0120\u0135\u0157\u0193\u0164\u0186\u02e4\u017f\u0135\u0164\u00f2\n\u02e4j\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00ed\u02b2\u02e4\u00c9\u0135\u00c9\u02e4\u02b1\u0135\u0169\u0157\u015d\u02b2\n\u029f\u029f\nG\u012e\u0154\u0169\u019c\nj\u0169\u019c\u0154\u0169\u019c\nG\u012e\u0154\u0169\u019c\nj\u0169\u019c\u0154\u0169\u019c\nG\u012e\u0154\u0169\u019c\n\u02e4j\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00e7\u02b2\u02e4\u0092\u00f2\u0126\u0199\u02e4\u001d\u0135\u012e\u015d\u0193\u015d\u0164\u00f2\u012e\u00e7\u0186\u02e4\n\u0180\u0193\u019c\u010e\u02e4\u001d\u0135\u00c9\u02e4\u02b1\u001d\u0135\u00c9\u02c1\u0092\u001d\u02b2\na\u00ca\u0120\u0135\u0157\u0193\u0164\u0186\u02e4\u017f\u0135\u0164\u00f2\nG\u012e\u0154\u0169\u019c\n\u02e4j\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00ed\u02b2\u02e4\u00c9\u0157\u00f2\u00f2\u02e4\u0135\u0199\u02e4\u009a\u010e\u0135\u0169\u0108\u010e\u0164\u015d\u02e4\u02b1\u00c9\u0135\u00c9\u02b2\n\u029f\u029f\n\u029f\u029f\n\u029f\u029f\n\u029f\u029f\n\u029f\u029f\n\u02e4\u02e4\u019b\u010e\u0135\u0169\u0108\u010e\u019c\n\u02b1\u00e7\u02b2\u02e4\u001d\u010e\u00ca\u0111\u012e\u02e4\u0135\u0199\u02e4\u009a\u010e\u0135\u0169\u0108\u010e\u019c\u02e4\n\u0089\u0157\u0135\u012d\u0154\u019c\u0111\u012e\u0108\u02e4\u02b1\u001d\u0135\u00c9\u02b2\n\u02b1\u00ca\u02b2\u02e4G\u012e\u0154\u0169\u019c\u02c1j\u0169\u019c\u0154\u0169\u019c\u02e4\n\u0089\u0157\u0135\u012d\u0154\u019c\u0111\u012e\u0108\u02e4\u02b1Gj\u02b2\nFigure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle\nbox represents a thought, which is a coherent language sequence that serves as an intermediate\nstep toward problem solving. See concrete examples of how thoughts are generated, evaluated, and\nsearched in Figures 2,4,6.\nchoices instead of just picking one, and (2) evaluates its current status and actively looks ahead or\nbacktracks to make more global decisions.\nTo design such a planning process, we return to the origins of artificial intelligence (and cognitive\nscience), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\nstarting in the 1950s [21, 22]. Newell and colleagues characterized problem solving [21] as search\nthrough a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\n(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\nexisting methods (detailed below) sample continuous language sequences for problem solving, ToT\nactively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\nas an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\nLM to self-evaluate the progress different intermediate thoughts make towards solving the problem\nthrough a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This\nimplementation of search heuristics via LM self-evaluation and deliberation is novel, as previous\nsearch heuristics are either programmed or learned. Finally, we combine this language-based\ncapability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first\nsearch (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts\nwith lookahead and backtracking.\nEmpirically, we propose three new problems that challenge existing LM inference methods even with\nthe state-of-the-art language model, GPT-4 [23]: Game of 24, Creative Writing, and Crosswords\n(Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities,\nand a way to incorporate systematic planning or search. We show ToT obtains superior results on\nall three tasks by being general and flexible enough to support different levels of thoughts, different\nways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of\ndifferent problems. We also analyze how such choices affect model performances via systematic\nablations and discuss future directions to better train and use LMs.\n2\nBackground\nWe first formalize some existing methods that use large language models for problem-solving,\nwhich our approach is inspired by and later compared with. We use p\u03b8 to denote a pre-trained LM\nwith parameters \u03b8, and lowercase letters x, y, z, s, \u00b7 \u00b7 \u00b7 to denote a language sequence, i.e. x =\n(x[1], \u00b7 \u00b7 \u00b7 , x[n]) where each x[i] is a token, so that p\u03b8(x) = Qn\ni=1 p\u03b8(x[i]|x[1...i]). We use uppercase\nletters S, \u00b7 \u00b7 \u00b7 to denote a collection of language sequences.\nInput-output (IO) prompting is the most common way to turn a problem input x into output\ny with LM: y \u223c p\u03b8(y|promptIO(x)), where promptIO(x) wraps input x with task instructions\nand/or few-shot input-output examples. For simplicity, let us denote pprompt\n\u03b8\n(output | input) =\np\u03b8(output | prompt(input)), so that IO prompting can be formulated as y \u223c pIO\n\u03b8 (y|x).\n2\nChain-of-thought (CoT) prompting [38] was proposed to address cases where the mapping of\ninput x to output y is non-trivial (e.g. when x is a math question and y is the final numerical answer).\nThe key idea is to introduce a chain of thoughts z1, \u00b7 \u00b7 \u00b7 , zn to bridge x and y, where each zi is a\ncoherent language sequence that serves as a meaningful intermediate step toward problem solving\n(e.g. zi could be an intermediate equation for math QA). To solve problems with CoT, each thought\nzi \u223c pCoT\n\u03b8\n(zi | x, z1\u00b7\u00b7\u00b7i\u22121) is sampled sequentially, then the output y \u223c pCoT\n\u03b8\n(y|x, z1\u00b7\u00b7\u00b7n). In\npractice, [z1\u00b7\u00b7\u00b7n, y] \u223c pCoT\n\u03b8\n(z1\u00b7\u00b7\u00b7n, y|x) is sampled as a continuous language sequence, and the\ndecomposition of thoughts (e.g. is each zi a phrase, a sentence, or a paragraph) is left ambiguous.\nSelf-consistency with CoT (CoT-SC) [36] is an ensemble approach that samples k i.i.d. chains\nof thought: [z(i)\n1\u00b7\u00b7\u00b7n, y(i)] \u223c pCoT\n\u03b8\n(z1\u00b7\u00b7\u00b7n, y|x) (i = 1 \u00b7 \u00b7 \u00b7 k), then returns the most frequent output:\narg maxy #{i | y(i) = y}. CoT-SC improves upon CoT, because there are generally different\nthought processes for the same problem (e.g. different ways to prove the same theorem), and the\noutput decision can be more faithful by exploring a richer set of thoughts. However, within each\nchain there is no local exploration of different thought steps, and the \u201cmost frequent\u201d heuristic only\napplies when the output space is limited (e.g. multi-choice QA).\n3\nTree of Thoughts: Deliberate Problem Solving with LM\nA genuine problem-solving process involves the repeated use of available informa-\ntion to initiate exploration, which discloses, in turn, more information until a way\nto attain the solution is finally discovered.\u2014\u2014 Newell et al. [21]\nResearch on human problem-solving suggests that people search through a combinatorial problem-\nspace \u2013 a tree where the nodes represent partial solutions, and the branches correspond to operators\nthat modify them [21, 22]. Which branch to take is determined by heuristics that help to navigate the\nproblem-space and guide the problem-solver towards a solution. This perspective highlights two key\nshortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not\nexplore different continuations within a thought process \u2013 the branches of the tree. 2) Globally, they\ndo not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\noptions \u2013 the kind of heuristic-guided search that seems characteristic of human problem-solving.\nTo address these shortcomings, we introduce Tree of Thoughts (ToT), a paradigm that allows LMs to\nexplore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\nover a tree, where each node is a state s = [x, z1\u00b7\u00b7\u00b7i] representing a partial solution with the input and\nthe sequence of thoughts so far. A specific instantiation of ToT involves answering four questions:\n1. How to decompose the intermediate process into thought steps; 2. How to generate potential\nthoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\n1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\nToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\nshows, depending on different problems, a thought could be a couple of words (Crosswords), a line of\nequation (Game of 24), or a whole paragraph of writing plan (Creative Writing). In general, a thought\nshould be \u201csmall\u201d enough so that LMs can generate promising and diverse samples (e.g. generating\na whole book is usually too \u201cbig\u201d to be coherent), yet \u201cbig\u201d enough so that LMs can evaluate its\nprospect toward problem solving (e.g. generating one token is usually too \u201csmall\u201d to evaluate).\n2. Thought generator G(p\u03b8, s, k). Given a tree state s = [x, z1\u00b7\u00b7\u00b7i], we consider two strategies to\ngenerate k candidates for the next thought step:\n(a) Sample i.i.d. thoughts from a CoT prompt (Creative Writing, Figure 4):\nz(j)\n\u223c\npCoT\n\u03b8\n(zi+1|s) = pCoT\n\u03b8\n(zi+1|x, z1\u00b7\u00b7\u00b7i) (j = 1 \u00b7 \u00b7 \u00b7 k). This works better when the thought\nspace is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity;\n(b) Propose thoughts sequentially using a \u201cpropose prompt\u201d (Game of 24, Figure 2; Crosswords,\nFigure 6): [z(1), \u00b7 \u00b7 \u00b7 , z(k)] \u223c ppropose\n\u03b8\n(z(1\u00b7\u00b7\u00b7k)\ni+1\n| s). This works better when the thought\nspace is more constrained (e.g. each thought is just a word or a line), so proposing different\nthoughts in the same context avoids duplication.\n3. State evaluator V (p\u03b8, S). Given a frontier of different states, the state evaluator evaluates the\nprogress they make towards solving the problem, serving as a heuristic for the search algorithm\nto determine which states to keep exploring and in which order. While heuristics are a standard\napproach to solving search problems, they are typically either programmed (e.g. DeepBlue [3]) or\n3\nlearned (e.g. AlphaGo [29]). We propose a third alternative, by using the LM to deliberately reason\nabout states. When applicable, such a deliberate heuristic can be more flexible than programmed\nrules, and more sample-efficient than learned models. Similar to the thought generator, we consider\ntwo strategies to evaluate states either independently or together:\n(a) Value each state independently: V (p\u03b8, S)(s) \u223c pvalue\n\u03b8\n(v|s) \u2200s \u2208 S, where a value\nprompt reasons about the state s to generate a scalar value v (e.g. 1-10) or a classifica-\ntion (e.g. sure/likely/impossible) that could be heuristically turned into a value. The basis\nof such evaluative reasoning can vary across problems and thought steps. In this work, we\nexplore evaluation via few lookahead simulations (e.g. quickly confirm that 5, 5, 14 can\nreach 24 via 5 + 5 + 14, or \u201chot l\u201d can mean \u201cinn\u201d via filling \u201ce\u201d in \u201c \u201d) plus commonsense\n(e.g. 1 2 3 are too small to reach 24, or no word can start with \u201ctzxc\u201d). While the former\nmight promote \u201cgood\u201d states, the latter could help eliminate \u201cbad\u201d states. Such valuations\ndo not need to be perfect, and only need to be approximately helpful for decision making.\n(b) Vote across states: V (p\u03b8, S)(s) = 1[s = s\u2217], where a \u201cgood\u201d state s\u2217 \u223c pvote\n\u03b8\n(s\u2217|S) is\nvoted out based on deliberately comparing different states in S in a vote prompt. When\nproblem success is harder to directly value (e.g. passage coherency), it is natural to to instead\ncompare different partial solutions and vote for the most promising one. This is similar\nin spirit to a \u201cstep-wise\u201d self-consistency strategy, i.e. cast \u201cwhich state to explore\u201d as a\nmulti-choice QA, and use LM samples to vote for it.\nFor both strategies, we could prompt the LM multiple times to aggregate the value or vote results to\ntrade time/resource/cost for more faithful/robust heuristics.\nAlgorithm 1 ToT-BFS(x, p\u03b8, G, k, V, T, b)\nRequire: Input x, LM p\u03b8, thought generator G()\n& size limit k, states evaluator V (), step limit T,\nbreadth limit b.\nS0 \u2190 {x}\nfor t = 1, \u00b7 \u00b7 \u00b7 , T do\nS\u2032\nt \u2190 {[s, z] | s \u2208 St\u22121, zt \u2208 G(p\u03b8, s, k)}\nVt \u2190 V (p\u03b8, S\u2032\nt)\nSt \u2190 arg maxS\u2282S\u2032\nt,|S|=b\nP\ns\u2208S Vt(s)\nend for\nreturn G(p\u03b8, arg maxs\u2208ST VT (s), 1)\nAlgorithm 2 ToT-DFS(s, t, p\u03b8, G, k, V, T, vth)\nRequire: Current state s, step t, LM p\u03b8, thought\ngenerator G() and size limit k, states evaluator\nV (), step limit T, threshold vth\nif t > T then record output G(p\u03b8, s, 1)\nend if\nfor s\u2032 \u2208 G(p\u03b8, s, k) do\n\u25b7 sorted candidates\nif V (p\u03b8, {s\u2032})(s) > vthres then \u25b7 pruning\nDFS(s\u2032, t + 1)\nend if\nend for\n4. Search algorithm. Finally, within the ToT framework, one can plug and play different search\nalgorithms depending on the tree structure. We explore two relatively simple search algorithms and\nleave more advanced ones (e.g. A* [11], MCTS [2]) for future work:\n(a) Breadth-first search (BFS) (Algorithm 1) maintains a set of the b most promising states\nper step. This is used for Game of 24 and Creative Writing where the tree depth is limit\n(T \u2264 3), and initial thought steps can be evaluated and pruned to a small set (b \u2264 5).\n(b) Depth-first search (DFS) (Algorithm 2) explores the most promising state first, until the\nfinal output is reached (t > T), or the state evaluator deems it impossible to solve the\nproblem from the current s (V (p\u03b8, {s})(s) \u2264 vth for a value threshold vth). In the latter\ncase, the subtree from s is pruned to trade exploration for exploitation. In both cases, DFS\nbacktracks to the parent state of s to continue exploration.\nConceptually, ToT has several benefits as a method for general problem-solving with LMs: (1) Gener-\nality. IO, CoT, CoT-SC, and self-refinement can be seen as special cases of ToT (i.e. trees of limited\ndepth and breadth; Figure 1). (2) Modularity. The base LM, as well as the thought decomposition,\ngeneration, evaluation, and search procedures can all be varied independently. (3) Adaptability.\nDifferent problem properties, LM capabilities, and resource constraints can be accommodated. (4)\nConvenience. No extra training is needed, just a pre-trained LM is sufficient. The next section will\nshow how these conceptual benefits translate to strong empirical performance in different problems.\n4\nExperiments\nWe propose three tasks that are hard even when sampling from the state-of-the-art language model,\nGPT-4 [23], using standard IO prompting or chain-of-thought (CoT) prompting. We show how\n4\nGame of 24\nCreative Writing\n5x5 Crosswords\nInput\n4 numbers (4 9 10 13)\n4 random sentences\n10 clues (h1. presented;..)\nOutput\nAn equation to reach 24\n(13-9)*(10-4)=24\nA passage of 4 paragraphs\nending in the 4 sentences\n5x5 letters:\nSHOWN;\nWIRRA; AVAIL; ...\nThoughts\n3 intermediate equations\n(13-9=4 (left 4,4,10); 10-\n4=6 (left 4,6); 4*6=24)\nA\nshort\nwriting\nplan\n(1. Introduce a book that\nconnects...)\nWords to fill in for clues:\n(h1. shown; v5. naled; ...)\n#ToT steps\n3\n1\n5-10 (variable)\nTable 1: Task overview. Input, output, thought examples are in blue.\ndeliberate search in trees of thoughts (ToT) produces better results, and more importantly, interesting\nand promising new ways to use language models to solve problems requiring search or planning.\nUnless otherwise stated, we perform experiments using a Chat Completion mode GPT-41 with a\nsampling temperature of 0.7.\n4.1\nGame of 24\nGame of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic\narithmetic operations (+-*/) to obtain 24. For example, given input \u201c4 9 10 13\u201d, a solution output\ncould be \u201c(10 - 4) * (13 - 9) = 24\u201d.\n\u02b3\u0135\u012e\u00f2\u02e4\u00f2\u0185\u00ca\u012d\u0154\u0126\u00f2\u02b4\u02e4\nG\u012e\u0154\u0169\u019c\u029d\u02e4\u0281\u02e4\u0286\u02e4\u027e\u027d\u02e4\u027e\u0280\n\u0089\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\u02e4\u012e\u00f2\u0185\u019c\u02e4\u015d\u0164\u00f2\u0154\u015d\u029d\u02e4\u02e4\u02e4\n3URSRVH\u00033URPSW\n\u0281\u02e4\u030c\u02e4\u0286\u02e4\u0310\u02e4\u027e\u0280\u02e4\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\u02b2\n\u027e\u027d\u02e4\u02ca\u02e4\u0281\u02e4\u0310\u02e4\u0283\u02e4\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0283\u02e4\u0286\u02e4\u027e\u0280\u02b2\n\u02b3\u029b\u029b\u029b\u012d\u0135\u0157\u00f2\u02e4\u0126\u0111\u012e\u00f2\u015d\u029f\u02b4\n7KRXJKW\u0003*HQHUDWLRQ\n/0\n)\u017f\u00ca\u0126\u0169\u00ca\u0164\u00f2\u02e4\u0111\u0199\u02e4\u0108\u0193\u017f\u00f2\u012e\u02e4\u012e\u0169\u012d\u00e6\u00f2\u0157\u015d\u02e4\u00e7\u00ca\u012e\u02e4\n\u0157\u00f2\u00ca\u00e7\u010e\u02e4\u027f\u0281\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02ab\u0126\u0111\u0123\u00f2\u0126\u0186\u02ab\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\u02b2\n\u027e\u027d\u02e4\u027e\u0281\u029d\u02e4\u027e\u027d\u02e4\u030c\u02e4\u027e\u0281\u02e4\u0310\u02e4\u027f\u0281\u029b\u02e4\u015d\u0169\u0157\u00f2\n\u02b3\u012d\u0135\u0157\u00f2\u02e4\u00f2\u0185\u00ca\u012d\u0154\u0126\u00f2\u015d\u02b4\n\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\n9DOXH\u00033URPSW\n\u02b1\u027e\u0280\u02e4\u02ca\u02e4\u027e\u027d\u02b2\u02e4\u02a6\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u02e4\u02a6\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u0286\n\u027e\u027d\u02e4\u030c\u02e4\u027e\u0280\u02e4\u030c\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u0283\n\u009a\u010e\u00f2\u0157\u00f2\u02e4\u0193\u015d\u02e4\u012e\u0135\u02e4\u0180\u00ca\u0186\u02e4\u0198\u0135\u02e4\u0135\u00e6\u0164\u00ca\u0111\u012e\u02e4\u027f\u0281\u02e4\u0180\u0193\u019c\u010e\u02e4\n\u019b\u010e\u00f2\u015d\u00f2\u02e4\u012e\u0169\u012d\u00e6\u00f2\u0157\u015d\u029b\u02e4\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\n7KRXJKW\u0003(YDOXDWLRQ\n\u02b1\u00e6\u02b2\n\u02b1\u00e7\u02b2\n/0\nG\u012e\u0154\u0169\u019c\u029d\u02e4\u0281\u02e4\u0286\u02e4\u027e\u027d\u02e4\u027e\u0280\n\u0281\u030c\u0286\u0310\u027e\u0280\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\u02b2\n\u027e\u027d\u02c1\u0281\u0310\u0283\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0283\u02e4\u0286\u02e4\u027e\u0280\u02b2\n\u029f\u029f\n\u027e\u0280\u02c1\u0283\u0310\u0284\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0284\u02e4\u0286\u02b2\n\u027e\u0280\u02c1\u0286\u0310\u0281\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0281\u02e4\u0283\u02b2\n\u029f\u029f\n\u0281\u02a6\u0283\u0310\u027f\u0281\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027f\u0281\u02b2\n\u0281\u030c\u0283\u0310\u027e\u027d\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02b2\n\u029f\u029f\n\u02b1\u00ca\u02b2\n\u02b3\u0135\u012e\u00f2\u02e4\u00f2\u0185\u00ca\u012d\u0154\u0126\u00f2\u02b4\u02e4\nG\u012e\u0154\u0169\u019c\u029d\u02e4\u0281\u02e4\u0286\u02e4\u027e\u027d\u02e4\u027e\u0280\n\u0089\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\u02e4\u012e\u00f2\u0185\u019c\u02e4\u015d\u0164\u00f2\u0154\u015d\u029d\u02e4\u02e4\u02e4\n\u000bD\f\u00033URSRVH\u00033URPSW\n\u0281\u02e4\u030c\u02e4\u0286\u02e4\u0310\u02e4\u027e\u0280\u02e4\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\u02b2\n\u027e\u027d\u02e4\u02ca\u02e4\u0281\u02e4\u0310\u02e4\u0283\u02e4\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0283\u02e4\u0286\u02e4\u027e\u0280\u02b2\n\u02b3\u029b\u029b\u029b\u012d\u0135\u0157\u00f2\u02e4\u0126\u0111\u012e\u00f2\u015d\u029f\u02b4\n7KRXJKW\u0003*HQHUDWLRQ\n/0\n)\u017f\u00ca\u0126\u0169\u00ca\u0164\u00f2\u02e4\u0111\u0199\u02e4\u0108\u0193\u017f\u00f2\u012e\u02e4\u012e\u0169\u012d\u00e6\u00f2\u0157\u015d\u02e4\u00e7\u00ca\u012e\u02e4\n\u0157\u00f2\u00ca\u00e7\u010e\u02e4\u027f\u0281\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02ab\u0126\u0111\u0123\u00f2\u0126\u0186\u02ab\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\u02b2\n\u027e\u027d\u02e4\u027e\u0281\u029d\u02e4\u027e\u027d\u02e4\u030c\u02e4\u027e\u0281\u02e4\u0310\u02e4\u027f\u0281\u029b\u02e4\u015d\u0169\u0157\u00f2\n\u02b3\u012d\u0135\u0157\u00f2\u02e4\u00f2\u0185\u00ca\u012d\u0154\u0126\u00f2\u015d\u02b4\n\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\n\u000bE\f\u00039DOXH\u00033URPSW\n\u02b1\u027e\u0280\u02e4\u02ca\u02e4\u027e\u027d\u02b2\u02e4\u02a6\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u02e4\u02a6\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u0286\n\u027e\u027d\u02e4\u030c\u02e4\u027e\u0280\u02e4\u030c\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u0283\u02e4\u009a\u010e\u00f2\u0157\u00f2\u02e4\u0193\u015d\u02e4\u012e\u0135\u02e4\u0180\u00ca\u0186\u02e4\n\u0198\u0135\u02e4\u0135\u00e6\u0164\u00ca\u0111\u012e\u02e4\u027f\u0281\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u015d\u00f2\u02e4\u00e6\u0193\u0108\u02e4\n\u012e\u0169\u012d\u00e6\u00f2\u0157\u015d\u029b\u02e4\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\n7KRXJKW\u0003(YDOXDWLRQ\n/0\nG\u012e\u0154\u0169\u019c\u029d\u02e4\u0281\u02e4\u0286\u02e4\u027e\u027d\u02e4\u027e\u0280\n\u0281\u030c\u0286\u0310\u027e\u0280\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\u02b2\n\u027e\u027d\u02c1\u0281\u0310\u0283\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0283\u02e4\u0286\u02e4\u027e\u0280\u02b2\n\u029f\u029f\n\u027e\u0280\u02c1\u0283\u0310\u0284\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0284\u02e4\u0286\u02b2\n\u027e\u0280\u02c1\u0286\u0310\u0281\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0281\u02e4\u0283\u02b2\n\u029f\u029f\n\u0281\u02a6\u0283\u0310\u027f\u0281\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027f\u0281\u02b2\n\u0281\u030c\u0283\u0310\u027e\u027d\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02b2\n\u029f\u029f\n)L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f\n0DUN\u0003GLII\u0003SURPSW\u0003ZLWK\u0003FRORU\nFigure 2: ToT in a game of 24. The LM is prompted for (a) thought generation and (b) valuation.\nTask Setup. We scrape data from 4nums.com, which has 1,362 games that are sorted from easy to\nhard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing.\nFor each task, we consider the output as success if it is a valid equation that equals 24 and uses the\ninput numbers each exactly once. We report the success rate across 100 games as the metric.\nBaselines. We use a standard input-output (IO) prompt with 5 in-context examples. For chain-of-\nthought (CoT) prompting, we augment each input-output pair with 3 intermediate equations, each\noperating on two remaining numbers. For example, given input \u201c4 9 10 13\u201d, the thoughts could be\n\u201c13 - 9 = 4 (left: 4 4 10); 10 - 4 = 6 (left: 4 6); 4 * 6 = 24 (left: 24)\u201d. For each game, we sample IO\nand CoT prompting for 100 times for average performance. We also consider a CoT self-consistency\nbaseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on\ntop of an IO sample for at most 10 iterations. At each iteration, the LM is conditioned on all previous\nhistory to \u201creflect on your mistakes and generate a refined answer\u201d if the output is incorrect. Note\nthat it uses groundtruth feedback signals about equation correctness.\nToT Setup. To frame Game of 24 into ToT, it is natural to decompose the thoughts into 3 steps,\neach an intermediate equation. As shown in Figure 2(a), at each tree node, we exact the remaining\nnumbers and prompt the LM to propose some possible next steps. The same \u201cpropose prompt\u201d is\nused for all 3 thought steps, though it only has one example with 4 input numbers. We perform a\nbreadth-first search (BFS) in ToT, where at each step we keep the best b = 5 candidates. To perform\ndeliberate BFS in ToT, as shown in Figure 2(b), we prompt LM to evaluate each thought candidate as\n\u201csure/maybe/impossible\u201d with regard to reaching 24. The aim is to promote correct partial solutions\nthat can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on\n\u201ctoo big/small\u201d commonsense, and keep the rest \u201cmaybe\u201d. We sample values 3 times for each thought.\n1Experiments were done between May 5-16, 2023.\n5\nMethod\nSuccess\nIO prompt\n7.3%\nCoT prompt\n4.0%\nCoT-SC (k=100)\n9.0%\nToT (ours) (b=1)\n45%\nToT (ours) (b=5)\n74%\nIO + Refine (k=10)\n27%\nIO (best of 100)\n33%\nCoT (best of 100)\n49%\nTable 2: Game of 24 Results.\n0\n25\n50\n75\n100\n0.2\n0.4\n0.6\n(a) Success rate with nodes visited\nIO (best of k)\nCoT (best of k)\nToT (b=1...5)\n1\n2\n3\n4\nCorrect\n0.0\n0.2\n0.4\n0.6\n(b) Samples failed at each step\nCoT\nToT (b=5)\nFigure 3: Game of 24 (a) scale analysis & (b) error analysis.\nResults. As shown in Table 2, IO, CoT, and CoT-SC prompting methods perform badly on the task,\nachieving only 7.3%, 4.0%, and 9.0% success rates. In contrast, ToT with a breadth of b = 1 already\nachieves a success rate of 45%, while b = 5 achieves 74%. We also consider an oracle setup for\nIO/CoT, by calculating the success rate using best of k samples (1 \u2264 k \u2264 100). To compare IO/CoT\n(best of k) with ToT, we consider calculating the tree nodes visited per task in ToT across b = 1 \u00b7 \u00b7 \u00b7 5,\nand map the 5 success rates in Figure 3(a), treating IO/CoT (best of k) as visiting k nodes in a bandit.\nNot surprisingly, CoT scales better than IO, and best of 100 CoT samples achieve a success rate of\n49%, but still much worse than exploring more nodes in ToT (b > 1).\nError analysis. Figure 3(b) breaks down at which step CoT and ToT samples fail the task, i.e. the\nthought (in CoT) or all b thoughts (in ToT) are invalid or impossible to reach 24. Notably, around\n60% of CoT samples already failed the task after generating the first step, or equivalently, the first\nthree words (e.g. \u201c4 + 9\u201d). This highlights the issues with direct left-to-right decoding.\n4.2\nCreative writing\nNext, we invent a creative writing task where the input is 4 random sentences and the output should\nbe a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is\nopen-ended and exploratory, and challenges creative thinking as well as high-level planning.\nTask setup. We sample random sentences from randomwordgenerator.com to form 100 inputs, and\nthere is no groundtruth passage for each input constraint. As we find that GPT-4 can follow the\ninput constraints most of the time, we focus on evaluating passage coherency in two ways: using a\nGPT-4 zero-shot prompt to provide a 1-10 scalar score, or using human judgments to compare pairs\nof outputs from different methods. For the former, we sample 5 scores and average them for each task\noutput, and we find these 5 scores usually consistent, with a standard deviation of around 0.56 on\naverage across outputs. For the latter, we employ a subset of the authors in a blind study to compare\nthe coherency of CoT vs. ToT generated passage pairs, where the order of passages is random flipped\nover 100 inputs.\nBaselines. Given the creative nature of the task, both IO and CoT prompts are zero-shot. While the\nformer prompts the LM to directly generate a coherent passage given input constraints, the latter\nprompts the LM to first make a brief plan then write the passage, i.e. the plan serves as the intermediate\nthought step. We generate 10 IO and CoT samples per task. We also consider an iterative-refine\n(k \u2264 5) method on top of a random IO sample for each task, where the LM is conditioned on input\nconstraints and the last generated passage to decide if the passage is already \u201cperfectly coherent\u201d,\nand if not generate a refined one.\nToT setup. We build a ToT with depth 2 (and only 1 intermediate thought step) \u2014 the LM first\ngenerates k = 5 plans and votes for the best one (Figure 4), then similarly generate k = 5 passages\nbased on the best plan then vote for the best one. Here the breadth limit b = 1, as only one choice is\nkept per step. A simple zero-shot vote prompt (\u201canalyze choices below, then conclude which is most\npromising for the instruction\u201d) is used to sample 5 votes at both steps.\nResults. Figure 5(a) shows average GPT-4 scores across 100 tasks, where ToT (7.56) is deemed to\ngenerate more coherent passages than IO (6.19) and CoT (6.93) on average. While such an automatic\nmetric might be noisy, Figure 5(b) confirms the finding by showing that humans prefer ToT over\nCoT in 41 out of 100 passage pairs, while only prefer CoT over ToT in 21 (other 38 pairs are found\n\u201csimilarly coherent\u201d). Lastly, iterative-refine is more effective on this natural language task, where\n6\n)L[\u0003FRORU\u0003\u000bE\\\u0003<XTLDQ\f\n\u00b5\u0157\u0193\u0164\u00f2\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u02e4\u0135\u0199\u02e4\u0281\u02e4\u015d\u010e\u0135\u0157\u019c\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u029b\u02e4\u009a\u010e\u00f2\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u02e4\u0135\u0199\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u02e4\u012d\u0169\u015d\u019c\u02e4\u00e6\u00f2\u029d\u02e4\u027e\u029b\u02e4G\u019c\u02e4\u0193\u015d\u012e\u02d9\u019b\u02e4\n\u00ed\u0111\u0199\u0199\u0193\u00e7\u0169\u0126\u019c\u02e4\u0198\u0135\u02e4\u00ed\u0135\u02e4\u00ca\u02e4\u010e\u00ca\u012e\u00ed\u015d\u0164\u00ca\u012e\u00ed\u02e4\u0111\u0199\u02e4\u0186\u0135\u0169\u02e4\u0120\u0169\u015d\u019c\u02e4\u015d\u0164\u00ca\u012e\u00ed\u02e4\u0135\u012e\u02e4\u0186\u0135\u0169\u0157\u02e4\u010e\u00ca\u012e\u00ed\u015d\u029b\u02e4\u027f\u029b\u02e4G\u019c\u02e4\u00e7\u00ca\u0169\u0108\u010e\u019c\u02e4\u010e\u0111\u012d\u02e4\u0135\u0199\u0199\u02e4\u0108\u0169\u00ca\u0157\u00ed\u02e4\u019b\u010e\u00ca\u019c\u02e4\u015d\u0154\u00ca\u00e7\u00f2\u02e4\u015d\u012d\u00f2\u0126\u0126\u00f2\u00ed\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u00ca\u0157\u00f2\u00ed\u02e4\u015d\u0164\u00f2\u00ca\u0123\u029b\u02e4\u0280\u029b\u02e4\u00b5\u010e\u00f2\u012e\u02e4\u015d\u010e\u00f2\u02e4\u00ed\u0193\u00ed\u012e\u02d2\u019b\u02e4\u0126\u0111\u0123\u00f2\u02e4\u00ca\u02e4\u0108\u0169\u0186\u02e4\u0180\u010e\u0135\u02e4\u0180\u00ca\u015d\u02e4\u019b\u0157\u0186\u0111\u012e\u0108\u02e4\u0198\u0135\u02e4\u0154\u0193\u00e7\u0123\u02e4\u010e\u00f2\u0157\u02e4\u0169\u0154\u029c\u02e4\u015d\u010e\u00f2\u02e4\u015d\u0164\u00ca\u0157\u0164\u00f2\u00ed\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u015d\u0193\u0108\u012e\u02e4\u0126\u00ca\u012e\u0108\u0169\u00ca\u0108\u00f2\u029b\u02e4\u0281\u029b\u02e4\n)\u00ca\u00e7\u010e\u02e4\u0154\u00f2\u0157\u015d\u0135\u012e\u02e4\u0180\u010e\u0135\u02e4\u0123\u012e\u0135\u0180\u015d\u02e4\u0186\u0135\u0169\u02e4\u010e\u00ca\u015d\u02e4\u00ca\u02e4\u00ed\u0111\u0199\u0107\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00f2\u0157\u00e7\u00f2\u0154\u019c\u0193\u0135\u012e\u02e4\u0135\u0199\u02e4\u0180\u010e\u0135\u02e4\u0186\u0135\u0169\u02e4\u00ca\u0157\u00f2\u029b\u02e4\u02e4\n\u027e\u029b\u02e4G\u012e\u019c\u0157\u0135\u00ed\u0169\u00e7\u00f2\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u0185\u0154\u0126\u00ca\u0111\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0198\u00f2\u00e7\u010e\u012e\u0193\u0156\u0169\u00f2\u02e4\n\u0135\u0199\u02e4\u00ed\u0135\u0111\u012e\u0108\u02e4\u00ca\u02e4\u010e\u00ca\u012e\u00ed\u015d\u0164\u00ca\u012e\u00ed\u02e4\u027f\u029b\u02e4\u0092\u0180\u0193\u0164\u00e7\u010e\u02e4\u0198\u0135\u02e4\u00ca\u02e4\n\u015d\u0164\u0135\u0157\u0186\u02e4\u00ca\u00e6\u0135\u0169\u019c\u02e4\u00ca\u012e\u02e4\u00ca\u015d\u019c\u0157\u0135\u012e\u00ca\u0169\u019c\u02d9\u015d\u02e4\u019a\u0111\u0157\u015d\u019c\u02e4\u019b\u0111\u012d\u00f2\u02e4\u0111\u012e\u02e4\n\u015d\u0154\u00ca\u00e7\u00f2\u02e4\u0280\u029b\u02e4#\u00f2\u015d\u00e7\u0157\u0111\u00e6\u00f2\u02e4\u00ca\u02e4\u015d\u0193\u019c\u0169\u00ca\u019c\u0193\u0135\u012e\u02e4\u0180\u010e\u00f2\u0157\u00f2\u02e4\u00ca\u02e4\n\u0180\u0135\u012d\u00ca\u012e\u02e4\u0169\u015d\u00f2\u015d\u02e4\u015d\u0193\u0108\u012e\u02e4\u0126\u00ca\u012e\u0108\u0169\u00ca\u0108\u00f2\u02e4\u0198\u0135\u02e4\u00ca\u017f\u0135\u0193\u00ed\u02e4\n\u0169\u012e\u0180\u00ca\u012e\u0164\u00f2\u00ed\u02e4\u00ca\u019c\u0164\u00f2\u012e\u019c\u0193\u0135\u012e\u02e4\u0281\u029b\u02e4\u009a\u010e\u00f2\u02e4\u019a\u0111\u012e\u00ca\u0126\u02e4\n\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u02e4\u00f2\u0185\u0154\u0126\u00ca\u0111\u012e\u015d\u02e4\u010e\u0135\u0180\u02e4\u00f2\u017f\u00f2\u0157\u0186\u0135\u012e\u00f2\u02e4\u010e\u00ca\u015d\u02e4\n\u00ed\u0111\u0199\u0107\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00f2\u0157\u00e7\u00f2\u0154\u019c\u0193\u0135\u012e\u015d\u02e4\u0135\u0199\u02e4\u0135\u019c\u010e\u00f2\u0157\u015d\n\u027e\u029b\u02e4G\u012e\u019c\u0157\u0135\u00ed\u0169\u00e7\u019c\u0193\u0135\u012e\u02e4\u0198\u0135\u02e4\u00ca\u012e\u02e4\u0169\u012e\u0169\u015d\u0169\u00ca\u0126\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u029c\u02e4\n\u012d\u00f2\u012e\u019c\u0193\u0135\u012e\u0111\u012e\u0108\u02e4\u00ca\u02e4\u010e\u00ca\u012e\u00ed\u015d\u0164\u00ca\u012e\u00ed\u02e4\u00ca\u015d\u02e4\u00ca\u02e4\u012d\u00f2\u0164\u00ca\u0154\u010e\u0135\u0157\u02e4\u0197\u0135\u0157\u02e4\n\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029b\u02e4\u027f\u029b\u02e4#\u0193\u015d\u00e7\u0169\u015d\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0169\u012e\u00f2\u0185\u0154\u00f2\u00e7\u0164\u00f2\u00ed\u02e4\n\u019b\u010e\u0111\u012e\u0108\u015d\u02e4\u0126\u00f2\u00ca\u0157\u012e\u00f2\u00ed\u02e4\u019a\u0157\u0135\u012d\u02e4\u00ca\u015d\u019c\u0157\u0135\u012e\u00ca\u0169\u0164\u015d\u029c\u02e4\u0111\u012e\u00e7\u0126\u0169\u00ed\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u015d\u012d\u00f2\u0126\u0126\u02e4\u0135\u0199\u02e4\n\u015d\u0154\u00ca\u00e7\u00f2\u029b\u02e4\u0280\u029b\u02e4#\u00f2\u015d\u00e7\u0157\u0111\u00e6\u00f2\u02e4\u00ca\u02e4\u0180\u0135\u012d\u00ca\u012e\u02d9\u015d\u02e4\u00e7\u0126\u00f2\u017f\u00f2\u0157\u02e4\u0198\u00ca\u00e7\u019c\u0193\u00e7\u02e4\u0197\u0135\u0157\u02e4\u00ca\u017f\u0135\u0193\u00ed\u0111\u012e\u0108\u02e4\n\u0169\u012e\u0180\u00ca\u012e\u0164\u00f2\u00ed\u02e4\u00ca\u019c\u0164\u00f2\u012e\u019c\u0193\u0135\u012e\u02e4\u00ca\u019c\u02e4\u00ca\u02e4\u00e6\u00ca\u0157\u029b\u02e4\u0281\u029b\u02e4\u001d\u0135\u012e\u0164\u00f2\u012d\u0154\u0126\u00ca\u0164\u00f2\u02e4\u010e\u0135\u0180\u02e4\n\u00ed\u0111\u0199\u0107\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00f2\u0157\u00e7\u00f2\u0154\u019c\u0193\u0135\u012e\u015d\u02e4\u0135\u0199\u02e4\u0135\u012e\u00f2\u015d\u00f2\u0126\u0199\u02e4\u00e7\u00ca\u012e\u02e4\u015d\u010e\u00ca\u0154\u00f2\u02e4\u0135\u012e\u00f2\u02d9\u015d\u02e4\n\u0193\u00ed\u00f2\u012e\u019c\u0193\u0164\u0186\u029b\n\u02b1\u00ca\u02b2\u02e4\nG\u012e\u0154\u0169\u019c\n\u02b1\u00e6\u02b2\u02e4\n\u0089\u0126\u00ca\u012e\u015d\n\u02b1\u00e7\u02b2\u02e4\n\u00b4\u0135\u0164\u00f2\u015d\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\nG\u012e\u0154\u0169\u019c\n\u0089\u0126\u00ca\u012e\u02e4\u027e\n\u02e4\u0089\u0126\u00ca\u012e\u02e4\u027f\u02e4\u02e4 \u029f\u029f\n\u0089\u00ca\u015d\u015d\u00ca\u0108\u00f2\n\u027e\n\u0089\u00ca\u015d\u015d\u00ca\u0108\u00f2\n\u027f\n\u029f\u029f\n\u027d\u02ab\u0282\u02e4\u017f\u0135\u0164\u00f2\u015d\n\u0089\u0126\u00ca\u012e\u02e4\u027e\u02e4\u02e4\u02e4\n\u029f\u029b\n\u029f\u029b\n\u027e\u029f\u029b\n\u027f\u029f\n\u029f\n\u0280\u02ab\u0282\u02e4\u017f\u0135\u0164\u00f2\u015d\n\u0089\u0126\u00ca\u012e\u02e4\u0280\u02c1\u0282\u02e4\u02e4\u02e4\n8VH\u0003UHG\u0012JUHHQ\u0003WR\u0003VKRZ\u0003ILQDO\u0003FKRLFH\n\u012e\u02ab\u0282\u02e4\u017f\u0135\u0164\u00f2\u015d\n\u0089\u0126\u00ca\u012e\u02e4\u027f\u02e4\u02e4\u02e4\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\nFigure 4: A step of deliberate search in a randomly picked Creative Writing task. Given the input, the\nLM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\nused to consequently write the output passage with the same sample-vote procedure.\nIO\nCoT\nToT\nIO\n+refine\nToT\n+refine\n4\n6\n8\n(a) GPT-4 coherency scores\nCoT > ToT Similar ToT > CoT\n0\n10\n20\n30\n40\n21\n38\n41\n(b) Human coherency comparison\nFigure 5: Creative Writing results.\nMethod\nSuccess Rate (%)\nLetter Word Game\nIO\n38.7\n14\n0\nCoT\n40.6\n15.6\n1\nToT (ours)\n78\n60\n20\n+best state\n82.4\n67.5\n35\n-prune\n65.4\n41.5\n5\n-backtrack\n54.6\n20\n5\nTable 3: Mini Crosswords results.\nit improves IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91. We\nbelieve it could be thought of as a third approach to thought generation in the ToT framework, where\nnew thoughts can arise from refining old thoughts instead of i.i.d. or sequentially generated.\n4.3\nMini crosswords\nIn Game of 24 and Creative Writing, ToT is relatively shallow \u2014 at most 3 thought steps are needed\nto reach the final output. Here we explore 5\u00d75 mini crosswords as a harder search problem involving\nnatural language. Again, the goal is not just to solve the task, as more general crosswords can be\nreadily solved with specialized NLP pipelines [34] that leverages large-scale retrieval instead of LM.\nRather, we aim to explore the limit of LM as a general problem solver that explores its own thoughts\nand guides its own exploration with deliberate reasoning as heuristics.\nTask setup. We scrape data from GooBix, which contains 156 games of 5 \u00d7 5 mini crosswords. As\nwe observe adjacent games contain similar clues, we use 20 games with indices 1, 6, \u00b7 \u00b7 \u00b7 , 91, 96 for\ntesting, and games 136, 141, 146, 151, 156 for prompting. For each task, the input describes the 5\nhorizontal clues and 5 vertical clues, and the output should be a board of 5 \u00d7 5 = 25 letters to solve\nthe crosswords. For evaluation, we consider three levels of success: the portion of correct letters (25\nper game), words (10 per game), and games.\nBaselines. We provide 5 example input-output pairs in the IO prompt, and in the CoT prompt\nadditionally include intermediate words in the order h1..5 then v1..5. We run each prompt for 10\nsamples and average the results.\nToT setup. We leverage a depth-first search (Algorithm 2) that keeps exploring the most promising\nsubsequent word clue until the state is no longer promising, then backtrack to the parent state to\nexplore alternative thoughts. To make search tractable, subsequent thoughts are constrained not to\nchange any filled words or letters, so that the ToT has at most 10 intermediate steps. For thought\ngeneration, at each state we translate all existing thoughts (e.g. \u201ch2.motor; h1.tasks\u201d for the state\nin Figure 6(a)) into letter constraints for remaining clues (e.g. \u201cv1.To heap: tm\n;...\u201d) and prompt\na proposal prompt 5 times to come up with candidates for where and what to fill in the next word.\nImportantly, we also prompt the LM to give a confidence level for different thoughts, and aggregate\n7\nG\u012e\u0154\u0169\u019c\u02e4\u001d\u0126\u0169\u00f2\u015d\n\u010e\u027f\u029b\u012d\u0135\u0164\u0135\u0157\n\u010e\u027e\u029b\u0198\u00ca\u015d\u0123\u015d\n\u010e\u0281\u029b\u015d\u00ca\u0126\u0135\u012e\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u00ca\u0108\u0108\u0157\u00f2\u0108\u00ca\u0164\u00f2\n\u017f\u0280\u029b\u02e4\u0089\u0157\u00f2\u0164\u00f2\u012e\u019c\u0193\u0135\u0169\u015d\u029e\u02e4\u019a\u0126\u0135\u0180\u00f2\u0157\u0186\u029d\u02e4\u02c8\u02c8\u02c8\u02c8\u02c8\u02e4\u015d\u0169\u0157\u00f2\n6WDWH\u0003(YDOXDWRU\u0003\u000bRYHU\u0003HDFK\u0003FOXH\f\n\u017f\u027e\u029b\u02e4\u00c9\u0135\u02e4\u010e\u00f2\u00ca\u0154\u029d\u02e4\u019b\u012d\u02c8\u015d\u02c8\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\n\u017f\u0282\u029b\u02e4#\u00f2\u015d\u0193\u00e7\u00e7\u00ca\u0164\u0135\u0157\u029e\u02e4\u012d\u0135\u0157\u00f2\u02e4\u00ed\u0157\u0186\u029d\u02e4\u015d\u0157\u02c8\u012e\u02c8\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u012d\u00ca\u0186\u00e6\u00f2\n\u029f\u029f\n\u02b1\u00e6\u00ca\u00e7\u0123\u019c\u0157\u00ca\u00e7\u0123\u02b2\n\u010e\u0280\u029b\u0108\u0157\u00ca\u012e\u00ed\n\u029f\u029f\n\u02b1\u015d\u0169\u00e6\u019c\u0157\u00f2\u00f2\u02e4\u0154\u0157\u0169\u012e\u00f2\u00ed\u02b2\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\n\u010e\u0280\u029b\u02e4\u0108\u0157\u00ca\u012e\u00ed\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\n\u029f\u029f\n')6\u0003\n2UGHU\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u02b1\u00ca\u02b2\n\u02b1\u00e6\u02b2\n\u019b\u02e4\u00ca\u02e4\u015d\u02e4\u0123\u02e4\u015d\n\u012d\u02e4\u0135\u02e4\u019b\u02e4\u0135\u02e4\u0157\n\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\n\u015d\u02e4\u00ca\u02e4\u0126\u02e4\u0135\u02e4\u012e\n\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\nFigure 6: In Mini Crosswords, (a) how thoughts are proposed and aggregated in a priority queue\nfor depth-first search (DFS), and (b) how a state is evaluated based on the possibility of filling in\neach remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM.\nThen DFS backtracks to the parent state and explore the next promising thought for clue.\nthese across proposals to obtain a sorted list of next thoughts to explore (Figure 6(a)). For state\nevaluations, we similarly translate each state into letter constraints for remaining clues, then evaluate\nfor each clue if it is possible to fill given the constraints. If any remaining clue is deemed \u201cimpossible\u201d\nto fill in (e.g. \u201cv1. To heap: tm s \u201d), then the exploration of the state\u2019s subtree is pruned and DFS\nbacktracks to its parent to explore the next promising thought. We limit DFS search steps to 100, and\nsimply render the deepest explored state (the first explored one if multiple) into the final output.\nResults. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\nsuccess rate less than 16%, while ToT significantly improves all metrics, achieving a word-level\nsuccess rate of 60% and solving 4 out of 20 games. Such an improvement is not surprising, given IO\nand CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.\nOracle and ablation studies. When outputting from the oracle best DFS state (instead of the\nheuristically determined best state) per task, ToT performance is even higher and actually solves\n7/20 games (Table 3, \u201c+best state\u201d), indicating our simple output heuristics can be readily improved.\nInterestingly, sometimes when the crosswords game is actually solved, the state evaluator might still\ndeem some words as \u201cimpossible\u201d and prune \u2014 possibly because 5 \u00d7 5 crosswords by design have\nsome rare or obselete words that GPT-4 cannot recognize2. Given the state evaluation as a pruning\nheuristic is imperfect, we also explore ablating the pruning, and find the performance generally worse\n(Table 3, \u201c-prune\u201d). However, it could actually find the correct solution for 4/20 games (though only\noutputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps. Thus,\nbetter heuristics for DFS pruning are critical for problem solving in this case. Lastly, we confirm the\nimportance of backtracking by running an ablation that keeps filling the most promising clue for at\nmost 20 steps, allowing overwrites. This is similar to a \u201cgreedy\u201d BFS search with breadth limit of\nb = 1, and performs poorly with a word level success of only 20% (Table 3, \u201c-backtrack\u201d).\n5\nRelated Work\nPlanning and decision making. Smart planning and decision making are critical to achieving\npredefined goals. As they are trained on vast amount of world knowledge and human examples, LMs\nare known to have already absorbed rich commonsense that makes it possible to propose reasonable\nplans conditioned on problem setting and environmental states [12, 42, 37, 13, 35, 41, 40]. Our\nproposed ToT approach extends existing planning formulations by considering multiple potentially\nfeasible plans simultaneously at each problem-solving step, and proceeding with the most promising\nones. The integration between thought sampling and value feedback organically integrates planning\nand decision-making mechanisms, enabling effective search inside a solution tree. On the other hand,\ntraditional decision-making procedures usually require training dedicated reward and policy models\nas in reinforcement learning (for example CHAI [33]), whereas we use the LM itself to provide\nthe value estimates for decision making. RAP [9] is a concurrent work that treats language model\n2For example, \u201cagend\u201d is an obsolete form of \u201cagendum\u201d, but GPT-4 deems it a typo for \u201cagenda\u201d. External\nretrieval or web interaction could augment LM for problem solving under knowledge uncertainty.\n8\nreasoning as planning with its internal world model, and proposes a MCTS-based method similar to\nToT. However, its tasks are simpler than ours, and its framework lacks the modularity to incorporate\ndifferent tree search algorithms.\nSelf-reflection. Using LLMs to assess the viability of their own predictions is becoming an in-\ncreasingly important procedure in problem solving. [28, 20, 24] introduced the \u201cself-reflection\u201d\nmechanism, in which LMs provide feedback to their generation candidates. [4] improves LMs code\ngeneration accuracy by injecting feedback messages generated by the LM itself based on its code\nexecution results. Similarly, [17] also introduces \u201ccritic\u201d or review steps over the actions and states,\ndeciding the next action to take in solving computer operation tasks. Another recent work very\nrelevant to ours is \u201cself-eval guided decoding\u201d [39]. Similar to our method, self-eval decoding\nalso follows a tree-search procedure with leaves sampled from stochastic beam search decoding,\nwhich are then evaluated by LLM itself with carefully prepared self-eval prompts. Their approach\nhowever, uses the PAL formulation [8] which represents thoughts as codes, which makes it difficult\nto tackle challenging tasks like creative writing which we consider in this paper. Our Tree-of-Thought\nformulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very\nlow accuracy with standard prompts.\nProgram-guided LLM generation. Our proposal is also related to recent advancements that organize\nLM\u2019s behavior with systematic procedures [14, 44, 6, 43] or symbolic program guidance. For example,\nSchlag et al. [27] embeds LMs in an algorithmic search procedure to help solve problems like question\nanswering step-by-step, in which the search trees are expanded by relevant paragraphs that might\nprovide answers. This approach however differs from ours in that trees are expanded by sampling\nexternal paragraphs instead of the LM\u2019s own thoughts, and there is no reflection or voting steps.\nAnother approach, LLM+P [18], goes one step further and delegates the actual planning process to a\nclassical planner.\nClassical search methods. Last but not least, our approach can be treated as a modern rendition\nof classical search methods for problem solving. For example it can be considered as a heuristic\nsearch algorithm like A* [10], in which the heuristic at each search node is provided by the LM\u2019s self-\nassessment. From this perspective, our method is also related to NeuroLogic A*esque decoding [19],\nwhich is inspired by A* search but introduces look-ahead heuristics that are efficient for LMs to\nimprove the beam-search or top-k sampling decoding. This method however is constrained to\nsentence generation tasks, whereas our framework are designed for complex, multi-step problem\nsolving guarded by value feedback.\n6\nDiscussion\nLimitations and future directions. Deliberate search such as ToT might not be necessary for many\nexisting tasks that GPT-4 already excels at (see Appendix B.1), and as an initial step this work only\nexplores three relatively simple tasks that challenges GPT-4 (see Appendix B.2 for some GPT-3.5\nexperiment results) and calls of better search and planning abilities incorporated with LMs. However,\nas we begin to deploy LMs for more real-world decision making applications (e.g. coding, data\nanalysis, robotics, etc.), more complex tasks could emerge and present new opportunities to study\nthese research questions. Also, search methods like ToT requires more resources (e.g. GPT-4 API\ncost) than sampling methods in order to improve task performances, but the modular flexibility of\nToT allows users to customize such performance-cost tradeoffs, and ongoing open-source efforts [32]\nshould readily reduce such costs in the near future. More details about cost and efficiency are in\nAppendix B.3. Lastly, this work focuses on using an off-the-shelf LM, and fine-tuning LMs using\na ToT-style high-level counterfactual decision making (e.g. deliberating over potential choices for\nthe next paragraph, instead of predicting the next token) might present opportunities to enhance the\nproblem-solving capabilities of LMs.\nConclusion. The associative \u201cSystem 1\u201d of LMs can be beneficially augmented by a \u201cSystem 2\u201d\nbased on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts\nframework provides a way to translate classical insights about problem-solving into actionable\nmethods for contemporary LMs. At the same time, LMs address a weakness of these classical\nmethods, providing a way to solve complex problems that are not easily formalized, such as creative\nwriting. We see this intersection of LMs with classical approaches to AI as an exciting direction.\n9\nBroader Impact\nToT is a framework that empowers LMs to more autonomously and intelligently make decisions\nand solve problems. While current tasks are limited to reasoning and search problems, future\napplications involving interaction with external environments or humans could bring potential danger,\ne.g. facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability\nof model decisions and the opportunity for human alignment, as the resulting representations are\nreadable, high-level language reasoning instead of implicit, low-level token values.\nAcknowledgements\nSY and KN acknowledge support from an Oracle Collaborative Research award and the National\nScience Foundation under Grant No. 2239363. Any opinions, findings, conclusions, or recommenda-\ntions expressed in this material are those of the author(s) and do not necessarily reflect the views of\nthe National Science Foundation. SY is also supported by the Harold W. Dodds Fellowship from\nPrinceton.\nReferences\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\n[2] C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen,\nS. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. A survey of monte carlo tree search\nmethods. IEEE Transactions on Computational Intelligence and AI in Games, 4:1\u201343, 2012.\n[3] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu. Deep blue. Artificial intelligence, 134(1-2):57\u201383,\n2002.\n[4] X. Chen, M. Lin, N. Sch\u00a8arli, and D. Zhou. Teaching large language models to self-debug, 2023.\n[5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n[6] A. Creswell and M. Shanahan. Faithful reasoning using large language models. arXiv preprint\narXiv:2208.14271, 2022.\n[7] N. D. Daw, Y. Niv, and P. Dayan. Uncertainty-based competition between prefrontal and\ndorsolateral striatal systems for behavioral control. Nature neuroscience, 8(12):1704\u20131711,\n2005.\n[8] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-\naided language models, 2023.\n[9] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language\nmodel is planning with world model. arXiv preprint arXiv:2305.14992, 2023.\n[10] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of\nminimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100\u2013107,\n1968. doi: 10.1109/TSSC.1968.300136.\n[11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of\nminimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100\u2013107,\n1968.\n[12] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents, 2022.\n[13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022.\n10\n[14] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y. Choi. Maieu-\ntic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint\narXiv:2205.11822, 2022.\n[15] D. Kahneman. Thinking, fast and slow. Macmillan, 2011.\n[16] D. Kahneman, S. Frederick, et al. Representativeness revisited: Attribute substitution in intuitive\njudgment. Heuristics and biases: The psychology of intuitive judgment, 49(49-81):74, 2002.\n[17] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.\n[18] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering\nlarge language models with optimal planning proficiency, 2023.\n[19] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, R. L. Bras, L. Qin, Y. Yu,\nR. Zellers, N. A. Smith, and Y. Choi. Neurologic a*esque decoding: Constrained text generation\nwith lookahead heuristics. In North American Chapter of the Association for Computational\nLinguistics, 2021.\n[20] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,\nS. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark.\nSelf-refine: Iterative refinement with self-feedback, 2023.\n[21] A. Newell, J. C. Shaw, and H. A. Simon. Report on a general problem solving program. In IFIP\ncongress, volume 256, page 64. Pittsburgh, PA, 1959.\n[22] A. Newell, H. A. Simon, et al. Human problem solving. Prentice-Hall, 1972.\n[23] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[24] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner:\nReasoning feedback on intermediate representations, 2023.\n[25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding\nby generative pre-training. OpenAI blog, 2018.\n[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[27] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li.\nLarge language model programs, 2023.\n[28] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection, 2023.\n[29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\nM. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550\n(7676):354\u2013359, 2017.\n[30] S. A. Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1):\n3, 1996.\n[31] K. E. Stanovich. Who is rational? Studies of individual differences in reasoning. Psychology\nPress, 1999.\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\n[33] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with\noffline reinforcement learning. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 4471\u20134491, 2022.\n11\n[34] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\ncrossword solving. arXiv preprint arXiv:2205.09665, 2022.\n[35] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\nImproving zero-shot chain-of-thought reasoning by large language models, 2023.\n[36] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n[37] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive\nplanning with large language models enables open-world multi-task agents, 2023.\n[38] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[39] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition\nenhances reasoning via self-evaluation guided decoding, 2023.\n[40] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\ndecision making: Problems, methods, and opportunities, 2023.\n[41] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing\nreasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n[42] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large\nlanguage models for code generation. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n[43] D. Zhou, N. Sch\u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet,\nQ. Le, et al. Least-to-most prompting enables complex reasoning in large language models.\narXiv preprint arXiv:2205.10625, 2022.\n[44] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang. Solving math word\nproblem via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257,\n2022.\n12\nA\nCode, Prompts, Trajectories\nAll code is available at https://github.com/princeton-nlp/tree-of-thought-llm.\nAll prompts are available at https://github.com/princeton-nlp/tree-of-thought-llm/\ntree/master/src/tot/prompts.\nTrajectories are available at https://github.com/princeton-nlp/tree-of-thought-llm/\ntree/master/logs.\nB\nAdditional Experiment Results\nGiven the motivation of exploring and extending the capability frontier of language models, our\nexperiments in the main paper have focused on a setup with the state-of-the-art language model\n(GPT-4), and three hard tasks invented to challenge it. Here, we report additional experiments with\nweaker LLM or easier tasks, and discuss cost and efficiency.\nGSM8K\nStrategyQA\nIO\n51\n73\nCoT\n86\n82\nToT\n90\n83\nTable 4: New tasks with\nzero-shot ToT and GPT-4.\nGPT-4\nGPT-3.5\nIO\n7.3%\n6%\nCoT\n4.0%\n3%\nToT\n74%\n19%\nTable 5: Game of 24 with\nGPT-4 vs GPT-3.5.\nGPT-4\nGPT-3.5\nIO\n6.19\n4.47\nCoT\n6.93\n5.16\nToT\n7.56\n6.62\nTable 6: Creative Writing with\nGPT-4 vs. GPT-3.5.\nB.1\nExtension to new tasks (GSM8k, StrategyQA) with zero-shot ToT\nWhile more common NLP tasks might be too easy for GPT-4 and do not require ToT (which is why\nwe considered harder new tasks), we believe applying ToT to new tasks could be straightforward.\nFor example, we implemented a simple and generic zero-shot ToT-BFS similar to creative writing\n(sample 5 problem solving strategies then vote for the best one; then sample 5 solutions based on the\nbest strategy then vote for the best one) for GSM8K and StrategyQA with few extra lines of code:\n# define the answer format of new tasks\ngsm8k_format = \u2018\"the answer is n\" where n is a number\u2019\nstrategyqa_format = \u2018either \"the answer is yes\" or \"the answer is no\"\u2019\n# define zero-shot io prompting\nstandard_prompt = \u2018Answer the following question with {format}: {input}\u2019\n# define thought format for zero-shot cot and zero-shot tot\ncot_prompt = \u2018\u2018\u2018Answer the following question: {input}\nMake a strategy then write. Your output should be of the following format:\nStrategy:\nYour strategy about how to answer the question.\nAnswer:\nYour answer to the question. It should end with {format}.\n\u2019\u2019\u2019\n# define zero-shot voting used for zero-shot tot\nvote_prompt = \u2018\u2018\u2018Given an instruction and several choices,\ndecide which choice is most promising.\nAnalyze each choice in detail, then conclude in the last line\n\"The best choice is {s}\", where s the integer id of the choice.\n\u2019\u2019\u2019\n13\nWe evaluated on a subset of 100 random GSM8K test and StrategyQA dev questions. As shown\nin Table 4 and as expected, ToT improves over CoT on both tasks (but only slightly, given GPT-4\n+ CoT is already very good on such tasks, and StrategyQA\u2019s bottleneck is external knowledge, not\nreasoning). Considering computational costs, it is more suitable to try smaller LLMs + ToT for\ntraditional NLP tasks, or GPT-4 + ToT for hard tasks that challenge GPT-4 + CoT\u2019s reasoning.\nB.2\nExtension to new LMs (GPT-3.5)\nTo understand how ToT works with other LLMs, we also ran GPT-3.5-turbo for Creative Writing\n(Table 6) and Game of 24 (Table 5). On both tasks, \u201cToT > CoT > IO\u201d remains true for GPT-3.5. On\nCreative Writing, we find GPT-3.5+ToT outperform GPT-4+IO, and similar to GPT-4+CoT, which\nsuggests ToT could also work well on weaker language models.\nOn Game of 24 (we changed 1-shot proposal prompt to 3-shot to make it work), GPT-3.5+ToT\u2019s\n19% is far worse than GPT-4+ToT\u2019s 74%. To further understand the importance of generation\nvs. evaluation, we ran GPT-4 generation + GPT-3.5 evaluation (64%) and GPT-3.5 generation +\nGPT-4 evaluation (31%). This suggests the game\u2019s bottleneck is thought generation, and different\ngeneration/evaluation language models might attain decent results while reducing costs.\nB.3\nCost and efficiency\nRunning ToT requires significantly more computations than IO or CoT prompting. For example, in\nGame of 24 (Table 7 below), solving a problem with ToT requires 5.5k completion tokens, close to\n100 CoT trials (6.7k tokens). But the performance of ToT is better than best of 100 independent CoT\ntrials.\nGame of 24\nGenerate/Prompt tokens\nCost per case\nSuccess\nIO (best of 100)\n1.8k / 1.0k\n$0.13\n33%\nCoT (best of 100)\n6.7k / 2.2k\n$0.47\n49%\nToT\n5.5k / 1.4k\n$0.74\n74%\nTable 7: Cost analysis on Game of 24.\nOn Creative Writing (Table 8 below), we found ToT takes around 5x completion tokens and money\ncost, which is intuitive as b = 5 and most tokens are generated passages.\nCreative Writing\nGenerate/Prompt tokens\nCost per case\nIO\n0.9k / 0.4k\n$0.06\nCoT\n0.9k / 0.4k\n$0.07\nToT\n4k / 2.9k\n$0.32\nTable 8: Cost analysis on Game of 24.\nSo completing Game of 24 and Creative Writing\u2019s main ToT experiments cost around 0.74 \u00d7 100 +\n0.32 \u00d7 100 = 106 dollars. Crosswords\u2019 DFS experiments should be also within 100 dollars. In\ngeneral, cost and efficiency of ToT highly depend on the prompts and search algorithms used, and\ncould require 5-100 times more generated tokens than CoT. Some actionable insights:\n\u2022 We recommend using ToT on tasks requiring deliberate reasoning, on which CoT struggles.\n\u2022 Flexibility of ToT allows some performance-cost tradeoff, e.g., change beam size or vote\nnumber in BFS, few-shot vs. zero-shot prompting, GPT-3.5 vs. GPT-4, etc. One could\nconfigure the setup based on some resource constraints or performance goal.\n\u2022 There is much space for improving efficiency, e.g., BFS could early stop when solution is\nfound, or trim down beam size to when some thoughts are \u201dimpossible\u201d.\n\u2022 We believe that more computation is indeed required in order for the model to achieve\nstronger intelligence, and this should not become a blocking issue as in the long run, (open-\nsource) LMs will become much cheaper and more efficient. It is also a great direction how\nto better train/finetune LMs for thought generation and/or evaluation.\n14\n"
  },
  {
    "title": "CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-training",
    "link": "https://arxiv.org/pdf/2305.10763.pdf",
    "upvote": "2",
    "text": "CLAPSpeech: Learning Prosody from Text Context with Contrastive\nLanguage-Audio Pre-training\nZhenhui Ye\u2217\nzhenhuiye@zju.edu.cn\nZhejiang University\nRongjie Huang\u2217\nrongjiehuang@zju.edu.cn\nZhejiang University\nYi Ren\nren.yi@bytedance.com\nBytedance\nZiyue Jiang\njiangziyue@zju.edu.cn\nZhejiang University\nJinglin Liu\nliu.jinglin@bytedance.com\nByteDance\nJinzheng He\njinzhenghe@zju.edu.cn\nZhejiang University\nXiang Yin\nyixiang.stephen@bytedance.com\nBytedance\nZhou Zhao\u2020\nzhaozhou@zju.edu.cn\nZhejiang University\nAbstract\nImproving text representation has attracted\nmuch attention to achieve expressive text-to-\nspeech (TTS). However, existing works only\nimplicitly learn the prosody with masked to-\nken reconstruction tasks, which leads to low\ntraining ef\ufb01ciency and dif\ufb01culty in prosody\nmodeling. We propose CLAPSpeech, a cross-\nmodal contrastive pre-training framework that\nexplicitly learns the prosody variance of the\nsame text token under different contexts.\nSpeci\ufb01cally, 1) We encourage the model to\nconnect the text context with its corresponding\nprosody pattern in the joint multi-modal space\nwith the elaborate design of the encoder inputs\nand contrastive loss; 2) We introduce a multi-\nscale pre-training pipeline to capture prosody\npatterns in multiple levels. We show how to\nincorporate CLAPSpeech into existing TTS\nmodels for better prosody.\nExperiments on\nthree datasets not only show that CLAPSpeech\ncould improve the prosody prediction for ex-\nisting TTS methods, but also demonstrate\nits generalization ability to adapt to multiple\nlanguages and multi-speaker TTS. We also\ndeeply analyze the principle behind the per-\nformance of CLAPSpeech. Ablation studies\ndemonstrate the necessity of each component\nin our method. Source code and audio sam-\nples are available at https://clapspeech.\ngithub.io.\n1\nIntroduction\nWith the development of deep learning, the audio\nquality of modern TTS systems has been improved,\nyet prosody modeling is still a challenging problem.\nPrevious works on expressive TTS have utilized\nexternal variation predictors (prediction-based, PB)\n\u2217Equal contribution.\n\u2020Corresponding author.\n(Ren et al., 2021a) and variational generative mod-\nels (variation-based, VB) (Kim et al., 2020; Liu\net al., 2022) to inject prosody variance into the\nTTS model. Another popular direction is to learn\nbetter text representation for prosody prediction\n(Tan et al., 2021). However, the existing text rep-\nresentation learning methods for TTS are either\nbased on the masked language model task (Devlin\net al., 2019; Jia et al., 2021; Chen et al., 2021)\n(i.e., learn a BERT-like large language model on a\ntext corpus) or masked acoustic model task (Chen\net al., 2020; Bai et al., 2022) (i.e., reconstruct the\nmasked mel-spectrogram based on the input text),\nwhich result in two disadvantages. Firstly, they\nonly implicitly learn prosody with reconstruction\nlosses, which distracts the model from improving\nthe prosody modeling. Secondly, they do not de-\ncouple the pronunciation space and prosody space,\nwhich leads to low training ef\ufb01ciency and a waste\nof model capacity. We perform a case study in\nSection 4.3.1, in which we can see that previous\ntext representation used in TTS cannot capture the\nprosody variance under different text contexts.\nTechnically, prosody can be regarded as the\npitch and duration variance of the same token un-\nder different conditions (such as text contexts and\nspeakers) (Tan et al., 2021). This paper mainly\nstudies the prosody correlated to the text context.\nFor instance, for the same word \"higher\", saying\n\"higher up\" or \"slightly higher\" can lead to dif-\nferent prosodies. Inspired by recent cross-modal\ncontrastive learning works in the text-to-image task\n(Radford et al., 2021; Elizalde et al., 2022), we\npropose a contrastive learning method that con-\nnects the text context and the high-level prosody\npattern in the text-speech joint multi-modal space,\nnamely Contrastive Language-Audio Pre-Training\nProsody \nEncoder\nText\nEncoder\nS!\nS\"\nS#\n...\nS$\nT!\nT\"\nT#\n\u2026\nT$\n... higher up \ncould see ...\nrising higher \nand higher ...\nof a slightly \nhigher age ...\ntoken encoding with \ncontext information\ntexts that contain the selected token \u201chigher\u201d\nspeech segments of the selected token \u201chigher\u201d\n\u2026\nS! $ T!\nS! $ T\"\nS! $ T#\nS! $ T$\nS\" $ T!\nS\" $ T\"\nS\" $ T#\nS\" $ T$\nS# $ T!\nS# $ T\"\nS# $ T#\nS# $ T$\nS$ $ T!\nS$ $ T\"\nS$ $ T#\nS$ $ T$\n...\n...\n...\n...\n...\n...\n...\n...\n...\nspeech encoding with local \nprosody information\nhigher\nhigher\ntoken\nindexing\ntext encoding\ntext-speech pairs from\nlarge-scale ASR datasets\nFigure 1: The contrastive pre-training process of CLAPSpeech. For clarity, we only show the word-level pre-\ntraining here. Note that we also perform a phoneme-level pre-training.\nfor Text-to-Speech (CLAPSpeech). Speci\ufb01cally,\nwe learn a text encoder to predict the prosody\nfrom the text context and a prosody encoder to\nextract the ground-truth (GT) prosody from the\nspeech segment of the selected token. During train-\ning, we select N text-speech pairs that contain the\nsame pronounceable token (e.g., the word \"higher\"\nor phoneme \"AE0\"). By aligning the text token\nwith its corresponding prosody (extracted from GT\nspeech) and pushing away the prosody representa-\ntion from other text contexts, the text encoder is\nencouraged to extract prosody from the text context.\nAn intuitive example of pre-training CLAPSpeech\ncan be found in Figure 1. We also observe that the\nprosody pattern can be expressed at multiple levels.\nTherefore, we propose a multi-scale pre-training\nframework that learns two CLAPSpeech models\nto capture the prosody information at the phoneme\nand word levels, respectively. After the pre-training\nstage, our CLAPSpeech can be regarded as a plug-\nin text encoder applicable to all TTS models to\nprovide \ufb01ne-grained prosody representation.\nTo prove the effectiveness and generalizability\nof our approach, we use two large-scale automatic\nspeech recognition (ASR) datasets (LibriSpeech\n(Panayotov et al., 2015) for English and Wenet-\nSpeech (Zhang et al., 2022) for Chinese) to pre-\ntrain the CLAPSpeech model.\nThe pre-trained\ntext encoder of CLAPSpeech is then plugged into\nprediction/variation-based TTS baselines to demon-\nstrate the improvement of CLAPSpeech to the exist-\ning expressive TTS systems. We then evaluate the\nperformance on three TTS datasets, including one\nsingle-speaker English dataset, one single-speaker\nChinese corpus, and one multi-speaker English\ndataset. Experiments on all datasets show that\nCLAPSpeech improves the prosody of the TTS\nmodels and outperforms previous representation\nlearning methods.\nTo summarize, CLAPSpeech has three promi-\nnent advantages: 1) It can provide better prosody\nrepresentation than previous representation learn-\ning methods with a much smaller model scale,\nthanks to its contrastive objective that explicitly\nlearns the prosody. 2) The text representation of\nCLAPSpeech can be conveniently used in existing\nTTS systems, only with a minor modi\ufb01cation of the\nfront-end network architecture. 3) We also show its\npotential applications such as \ufb01ne-grained prosody\ntransfer in Section 4.3.2.\n2\nRelated Work\n2.1\nExpressive TTS\nIn the past few years, modern neural TTS has made\nsigni\ufb01cant progress in high practicality and audio\nquality (Ren et al., 2019; Kim et al., 2020; Elias\net al., 2021; Miao et al., 2021; Kim et al., 2021;\nDonahue et al., 2021; Jiang et al., 2022). However,\nmodeling expressive prosody given the plain in-\nput text is still challenging. To achieve expressive\nTTS, one common practice is to use a reference\nencoder and style tokens (Wang et al., 2018; Jia\net al., 2018). But it is dif\ufb01cult to select appropriate\nreference audios during inference (Tan et al., 2021).\nOther works seek to improve prosody modeling\nwith advanced network designs, which can be cate-\ngorized into two classes: (1) the prediction-based\n(PB) TTS systems (Ren et al., 2021a) learn several\nexternal predictors to predict the prosody attributes\nsuch as pitch contour, duration, and energy; (2) the\nvariation-based (VB) TTS systems leverage varia-\ntional auto-encoder (VAE) (Ren et al., 2021b) or\nnormalizing \ufb02ow (Kim et al., 2020) to model the\nprosody in the latent space.\nThere are also some works that explore provid-\ning better text presentation with rich prior knowl-\nedge to help the prosody prediction. For instance,\nLiu et al. (2021) and Ye et al. (2022) incorpo-\nrate syntax information through dedicated mod-\neling methods such as graph networks. Represen-\ntation learning methods for text pre-training and\nspeech pre-training also show improvements in the\nprosody of TTS. We will discuss the representation\nlearning works for TTS in the next section.\n2.2\nRepresentation Learning for TTS\nSelf-supervised pre-training methods have been\nleveraged in TTS to enhance text processing or\nspeech generation capabilities (Chung et al., 2019;\nZhang et al., 2019). Some early works (Wang\net al., 2015) use pre-trained word embeddings to\nimprove the robustness of TTS systems. Recently,\nsome works explore incorporating pre-trained large\nmasked language models (MLMs) (Devlin et al.,\n2019; Chen et al., 2021; Jia et al., 2021) to enjoy\nthe rich semantic information learned from the web-\nscale text corpus. However, the above-mentioned\nworks only focus on the text space, it is challeng-\ning for them to model expressive prosody consid-\nering the models are unaware of the high variable\nprosody patterns in the speech space. There are sev-\neral inspiring speech representation learning meth-\nods in ASR. Baevski et al. (2020) and Hsu et al.\n(2021) utilize masked continuous speech features\nto predict predetermined cluster assignments. As\nfor TTS, ProsoSpeech (Ren et al., 2022) designs\na word-level vector quantization bottleneck to ex-\ntract discrete prosody representation from speech.\nMasked acoustic model (MAM) (Chen et al., 2020)\nproposes to learn a speech encoder that gener-\nates continuous speech (prosody) representations.\nSpeci\ufb01cally, during training they replace a span of\nspeech spectrogram with mask tokens and learn to\nrecover the masked spectrogram without text con-\nditions. A3T (Bai et al., 2022) additionally learns a\ntext encoder as auxiliary information for MAM to\nreconstruct the masked mel-spectrogram.\nThe difference between CLAPSpeech and previ-\nous representation works in TTS is obvious: While\nprevious works implicitly learn the prosody in-\nformation with the masked token reconstruction\ntask, CLAPSpeech is the \ufb01rst work that utilizes the\ncross-modal contrastive learning to explicitly learn\nthe context-correlated prosody, which leads to bet-\nter prosody prediction and more ef\ufb01cient usage of\nmodel capacity.\n3\nCLAPSpeech\nWe propose CLAPSpeech, a cross-modal con-\ntrastive learning approach to provide better text\nrepresentation for prosody prediction in TTS. As\nshown in Figure 1, CLAPSpeech comprises a text\nencoder and a prosody encoder, whose training ob-\njective is to connect the text token and the speech\nsegment in the joint prosody space. In this section,\nwe \ufb01rst design the network structure and input fea-\ntures of these two encoders. These elaborate de-\nsigns enable the text encoder to effectively process\nthe text context and ensure that the prosody en-\ncoder focuses on extracting the high-level prosody\npattern from the speech segment while eliminating\nother variables, such as timbre. Then we introduce\nthe multi-scale contrastive pre-training framework,\nwhich enables CLAPSpeech to capture prosody in\nboth phoneme and word levels. Finally, we show\nhow the pre-trained text encoder of CLAPSpeech\ncan be conveniently plugged into modern TTS sys-\ntems to improve prosody prediction. We describe\nthese designs in detail in the following subsections\nand provide more technical details in Appendix A.\n3.1\nText Encoder and Prosody Encoder\nThe prosody of the same pronounceable token1\nvaries in different text contexts. CLAPSpeech aims\nto model the correlation between the text context\nand the high-level prosody pattern. To this end,\nwe design a text encoder and a prosody encoder to\nconstruct a text-speech multi-modal prosody em-\nbedding space.\nAs shown in Figure 2(a), the text encoder uses\nphoneme and byte pair encoding (BPE) (Shibata\net al., 1999) of the input text as the input. The\nphoneme and BPE sequence help the model extract\n1such as the phoneme \"AE0\" or the word \"higher\".\nPhoneme\nEmbedder\nConv1D + LN + ReLU\nx 3\nConv1D + \nLN + ReLU\nx 3\nx 4\nAttentive Pooling 1D\n[N, T, C]\n[N, C]\nspeech encoding\nphoneme\nBPE\nBPE\nEmbedder\nFFT\nFFT\nFFT\nWordPool\n+ Word2Ph\ntext encoding\n[N, T, C]\ntoken encoding\n[N, C]\ntoken indexing\nphoneme-level\nWP if word-level\nhigher\nspeech segment\n(a) text encoder\nPhoneme\nEmbedder\nConv1D + LN + ReLU\nx 3\nConv1D + \nLN + ReLU\nx 3\nx 4\nAttentive Pooling 1D\n[N, T, C]\n[N, C]\nspeech encoding\nphoneme\nBPE\nBPE\nEmbedder\nFFT\nFFT\nFFT\nWordPool\n+ Word2Ph\ntext encoding\n[N, T, C]\ntoken encoding\n[N, C]\ntoken indexing\nphoneme-level\nWP if word-level\nhigher\nspeech segment\n(b) prosody encoder\nFigure 2: The text / prosody encoder of CLAPSpeech.\nIn sub\ufb01gure (a), \"WP\" and \"Word2Ph\" denotes word\npooling and Word2Ph expanding operation, which are\nillustrated in Figure 3.\nthe prosody pattern related to phonological habits\n(such as the linking phenomenon in English) and\nsemantic information (which may imply different\nemotional overtones), respectively. The network\nstructure of the text encoder is composed of several\nFeed Forward Transformers (FFT) (Vaswani et al.,\n2017), which have proven the robustness in pro-\ncessing long text sequences in TTS models. Specif-\nically, we learn two independent FFT blocks to\nprocess the phoneme and BPE sequences, respec-\ntively. This way, the phoneme FFT block could\nmodel the phonological habits in phonetic space,\nand the BPE FFT block could extract the semantic\ninformation. One dif\ufb01culty is fusing the phoneme\nand BPE sequence of mismatched length. Instead\nof concatenating these two sequences in the time\naxis, we use word-level pooling (WP) from Ren\net al. (2021b) to process the BPE encoding to the\nword level, then expand it to the phoneme level\n(namely the word2ph operation). To be speci\ufb01c,\nas shown in Figure 3(a), the WP operation aver-\nages the phoneme hidden states inside each word\naccording to the word boundary, and the word2ph\noperation repeats the word hidden states for each\nphoneme insides the word boundary as illustrated\nin Figure 3(b).\nOnce the phoneme sequence and BPE seqneuce\nis fused, we then use an additional FFT block to\nfuse the aligned phoneme and BPE encoding to get\nthe \ufb01nal phoneme-level text encoding. During the\npre-training phase, since only one selected token is\nanalyzed, we index from the phoneme-level text en-\nHH AE1 Z  |  N EH1 V ER0\nPhoneme/BPE \nEncoder\nWP\nHH  AE1  Z  | N EH1  V  ER0\nWord2Ph\nhas               never\nhas      \nnever\n(Phoneme/BPE-level Sequence)\n(Phoneme-level Sequence)\n(Word-level Sequence)\n(Word-level Sequence)\n(a) Word Pooling\nHH AE1 Z  |  N EH1 V ER0\nPhoneme/BPE \nEncoder\nWP\nHH  AE1  Z  | N EH1  V  ER0\nWord2Ph\nhas               never\nhas      \nnever\n(Phoneme/BPE-level Sequence)\n(Phoneme-level Sequence)\n(Word-level Sequence)\n(Word-level Sequence)\n(b) Word2Ph Expanding\nFigure 3: The word pooling and word2ph expanding\noperation.\ncoding to obtain the encoding of the selected token\n(namely the token encoding in Figure 2(a)) and then\nlinearly project it into the multi-modal embedding\nspace. During the TTS phase, the phoneme-level\noutput of the text encoder can be conveniently uti-\nlized as auxiliary features for TTS systems, which\nwe will discuss in Section 3.3.\nThe prosody encoder aims to extract prosody pat-\nterns from the GT speech segment of the selected\ntoken. Therefore, we clip the mel-spectrogram\nwith the word boundary2 as the input speech fea-\nture. Then the prosody encoder processes the in-\nput mel-spectrogram into a global encoding to be\nconnected with the token encoding. Note that the\nclipped speech segment only contains the local\nprosody information for the selected token without\nleaking any contextual information. Thanks to the\ncontrastive learning setting, the extracted global\nprosody encoding is disentangled from phonetic\nand speaker space: 1) since the positive sample and\nnegative samples belong to the same pronounce-\nable token, the phonetic information is eliminated;\n2) as the speaker information is not provided to\nthe text encoder3, the prosody encoder will \ufb01lter\nout speaker information to maximize the prosody\ninformation in the output features during training.\nThis way, by connecting the context-aware text en-\ncoding with the context-unaware mel encoding, on\nthe one hand, the prosody encoder learns to ex-\ntract the high-level prosody information from the\nspeech segment; on the other hand, the text encoder\nis encouraged to utilize the text context to predict\n2We extract word boundary with a forced alignment tool.\n3We assume that text and speaker are independent of each\nother (no correlation between them) in our dataset.\nthe prosody extracted by the prosody encoder. As\nshown in Figure 2(b), we use ResNet-50 (He et al.,\n2016) as the backbone of the prosody encoder due\nto its robustness. We make several modi\ufb01cations\nto the original version: 1) to better process the mel-\nspectrogram, we use 1D convolution with layer\nnormalization to build the fundamental residual\nblock; 2) to handle the speech segment of dynamic\nlengths, we use an attentive pooling layer from Rad-\nford et al. (2021) to aggregate the output feature\nmap of the ResNet.\n3.2\nMulti-scale Contrastive Pre-training\nThe key idea of CLAPSpeech is to model the\nprosody variance of the same text token under dif-\nferent contexts. Therefore, to construct a mini-\nbatch for contrastive pre-training, we randomly\nselect a text token, then sample a batch of N\ntext-speech pairs that contain the selected token\n(one intuitive sample is shown in Figure 1, where\nwe sample the text-speech pairs that contain the\nword \"higher\"). To better extract prosody variance\nat the phoneme and word level, we introduce a\nmulti-scale contrastive training framework. To be\nspeci\ufb01c, we learn two CLAPSpeech models for\nphoneme-level and word-level text tokens, respec-\ntively.\nFor clarity, we \ufb01rst illustrate the training process\nof phoneme-level CLAPSpeech. Let the text con-\ntext that contains the selected phoneme token (e.g.,\n\"AE0\") be represented by Xtext. Let the processed\nspeech segment of the phoneme token be Xspeech\ns.t. Xspeech \u2208 RF\u00d7T , where F is the number of\nMel bins and T is the number of time bins. For\nsimplicity, we use Xtext and Xspeech to represent\na batch of N text-speech pairs.\nThe text and speech are passed through the text\nencoder ftext(\u00b7) and prosody encoder fspeech(\u00b7),\nrespectively. As can be seen in Figure 2(a), the out-\nput of the text encoder ftext(Xtext) is the phoneme-\nlevel encoding of the input text, hence we index\nfrom it to obtain the encoding of the phoneme to-\nken ftext(Xtext)iph, where iph denotes the index\nof the phoneme token in the phoneme-level text\nsequence. As can be seen in Figure 2(b), the output\nspeech encoding fspeech(Xspeech) is a global repre-\nsentation of the input speech segment. The output\nrepresentations are normalized and then linearly\nprojected into the multi-modal embedding space:\nTph = Ltext(LN(ftext(Xtext)iph))\nS = Lspeech(LN(fspeech(Xspeech))),\n(1)\nwhere Tph \u2208 RN\u00d7C is the phoneme token represen-\ntation and S \u2208 RN\u00d7C is the speech representation\nof channel size C. LN means layer normalization,\nLtext and Lspeech are linear projections.\nNow that the text and speech embeddings are\ncomparable, CLAPSpeech is trained to predict\nwhich of the N \u00d7 N possible text-speech pairings\nacross a batch actually occurred. Speci\ufb01cally, the\ntext encoder and prosody encoder are encouraged\nto maximize the cosine similarity of the text and\nspeech encoding of the N real pairs in the batch\nwhile minimizing the cosine similarity of the em-\nbeddings of the N2 \u2212N incorrect pairings. Follow-\ning Radford et al. (2021), we optimize a symmetric\ncross-entropy loss over these similarity scores:\nLph = 0.5\u00d7(ltext(\u03c4 \u00b7Cph)+lspeech(\u03c4 \u00b7Cph)) (2)\nwhere Cph \u2208 RN\u00d7N is the cosine similarity ma-\ntrix between the phoneme token encoding Tph\nand the speech encoding S, measured by Cph =\nTph \u00b7 ST ; \u03c4 is a learnable temperature param-\neter to scale the range of logits; and lk\n=\n1\nN \u03a3N\ni=0 log diag(softmax(C)) is the cross entropy\nfunction along the text and speech axis in C.\nThe word-level CLAPSpeech can be trained sim-\nilarly. As shown in Figure 2(a), for the word-level\nCLAPSpeech, we use word pooling to process the\nphoneme-level text encoding into word level, then\nindex from it to obtain the word token encoding\nTword. Similar to Equation 2, the training loss for\nword-level CLAPSpeech is formulated as:\nLword = 0.5\u00d7(ltext(\u03c4\u00b7Cword)+lspeech(\u03c4\u00b7Cword))\n(3)\nwhere Cword is the cosine similarity matrix be-\ntween the word token encoding Tword and the\nspeech encoding S.\n3.3\nCLAPSpeech Plugged in TTS Systems\nThe text encoder of CLAPSpeech could provide\ntext representation with rich prosody information\nfor the TTS task. Since the generated text represen-\ntation is at the phoneme level, which is in line with\nthe majority of current TTS models that also utilize\nphoneme sequence as the text input, CLAPSpeech\ncan be a convenient plugin unit for TTS systems\nto improve prosody prediction. Speci\ufb01cally, we\ntake a state-of-the-art variation-based TTS system,\nPortaSpeech, as an example. As shown in Figure\n4, the pre-trained text encoders of CLAPSpeech\n(marked with a red dashed rectangle) perform as\nphonetic \nencoder\nword-level\nCLAPSpeech\nphoneme-level \nCLAPSpeech\nphoneme\nBPE\n+\nduration\npredictor\nword encoder\nLR\nword-to-phoneme Attention\nWP\nword-level \nduration\nK, V\nQ\ncombine\nencoder\n+\nexpand\nstop gradient\nVariational Generator\nMulti-Length Discriminator\nphoneme-level\nencoding \nFigure 4: PortaSpeech with CLAPSpeech plugged in.\nan auxiliary encoder to the original phonetic en-\ncoder of PortaSpeech. The phoneme-level outputs\nof the phonetic encoder and CLAPSpeech text en-\ncoder are fused and processed by the following\nencoder. Note that we \ufb01x the parameters of CLAP-\nSpeech text encoders during the training of the TTS\nsystem to avoid over\ufb01tting. CLAPSpeech can be\neasily plugged into other TTS systems in a similar\nway. To demonstrate the universality, we illustrate\nhow to combine CLAPSpeech with a widely-used\nprediction-based TTS system, FastSpeech 2, in Ap-\npendix A.1. We additionally adopt multi-length\nadversarial training in TTS models to improve au-\ndio quality. More details about the the adversarial\ntraining can be found in Appendix A.2.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets and Baselines\nWe pre-train CLAP-\nSpeech on two ASR datasets: 1) LibriSpeech\n(Panayotov et al., 2015), an English database that\ncontains 982 hours of speech from 2484 speak-\ners; 2) WenetSpeech (Zhang et al., 2022), a Chi-\nnese speech corpus consisting of 10,000 hours of\nspeech4. Then we evaluate the pre-trained CLAP-\nSpeech on three TTS datasets: 1) LJSpeech (Ito\nand Johnson, 2017), a single-speaker database that\n4We \ufb01lter samples with a correctness con\ufb01dence level\nabove 0.95, \ufb01nally get a subset of 1000 hours.\ncontains 13,100 English audio clips with a to-\ntal of nearly 24 hours of speech; 2) Biaobei5, a\nChinese speech corpus consisting of 10,000 sen-\ntences (about 12 hours) from a Chinese speaker;\n3) LibriTTS (Zen et al., 2019), an English dataset\nwith 149,736 audio clips (about 245 hours) from\n1,151 speakers (We only use train clean360 and\ntrain clean100). The raw text is transformed into\nphoneme and BPE sequences using open-sourced\ntools. The GT mel-spectrograms are generated\nfrom the raw waveform with a frame size of 1024\nand the hop size of 256.\nWe compare CLAP-\nSpeech against two pre-training baselines (BERT\n(Devlin et al., 2019) and A3T (Bai et al., 2022)) in\na prediction-based (PB) TTS model, FastSpeech\n2, and a variation-based (VB) TTS model, Por-\ntaSpeech.\nModel Con\ufb01guration\nCLAPSpeech consists of\na text encoder and a prosody encoder, whose struc-\ntures are shown in Figure 2 and discussed in Sec-\ntion 3.2. As for the PB and VB TTS models, we\nuse the same structure in the original papers with\nan additional multi-length discriminator to improve\naudio quality. The multi-length discriminator con-\nsists of multiple stacked convolutional layers with\nbatch normalization and treats the input spectro-\ngram as images. We put more detailed model con-\n\ufb01gurations in Appendix B.1.\nTraining and Evaluation\nOur approach is im-\nplemented with Pytorch.\nWe pre-train CLAP-\nSpeech on 4 Nvidia 3090Ti GPUs with a batch size\nof 1,024 text-speech pairs (256 pairs per GPU). We\nuse the Adam optimizer with an initial learning rate\nof 0.0005. We train the CLAPSpeech model for\n640,000 iterations (which takes about 1 week) and\nfollow the cosine learning rate schedule in CLIP.\nThen we train the TTS models on 1 Nvidia 2080Ti\nGPU with a batch size of 64 sentences, follow-\ning the learning rate schedule in Vaswani et al.\n(2017). We use HiFi-GAN (Kong et al., 2020)\nas the vocoder.\nWe conduct the mean opinion\nscore (MOS) and comparative mean opinion score\n(CMOS) evaluation to measure the prosody and au-\ndio quality. Details about the subjective evaluation\ncan be found in Appendix B.2. As for the objec-\ntive evaluation, following Ren et al. (2021b), we\nevaluate the prosody from the aspects of pitch and\nduration: 1) we compute the average dynamic time\nwarping (DTW) (Muller, 2007) distances between\n5https://www.data-baker.com/opensource.html\nthe pitch contours of GT speech and synthesized\nspeech to measure the pitch accuracy; 2) we cal-\nculate the average absolute duration error (DE) in\nmicro-seconds6 to measure the duration accuracy.\n4.2\nPerformance\nWe compare the performance of our CLAPSpeech\nagainst BERT and A3T in PB/VB TTS models. GT\n(the ground-truth audio) and GT (voc.) (the audio\nwaveform generated by the vocoder using the GT\nmel-spectrogram) are also included in the experi-\nment. We perform the TTS experiments on three\ndatasets as mentioned in Section 4.1. The results\nare shown in Table 1. We can see that CLAPSpeech\noutperforms other representation learning meth-\nods in both PB and VB TTS baselines in terms of\nMOS, pitch accuracy, and duration accuracy, which\nproves that CLAPSpeech could effectively improve\nthe prosody prediction in current expressive TTS\nmodels (no matter prediction-based or variation-\nbased). Besides, we observe that CLAPSpeech\nachieves better performance than BERT and A3T\nwith much fewer model parameters. We suspect\nit is due to the fact that the MLM-based method\n(i.e., BERT) require a large model capacity to store\nthe semantic information and MAM-based method\n(i.e., A3T) have to jointly learn the phonetic infor-\nmation to reconstruct the masked mel-spectrogram.\nBy contrast, our CLAPSpeech eliminates the pho-\nnetic space and only focus on the prosody space\nduring pre-training, which is parameter-ef\ufb01cient.\nWe then visualize the mel-spectrograms gener-\nated by different methods in Figure 5. We can see\nthat CLAPSpeech can generate results with more\nrealistic pitch contours, which result in expressive\nprosody. In conclusion, our experiments demon-\nstrate that CLAPSpeech could help TTS systems\nsynthesize more expressive and prosodic audio.\n4.3\nDeeper Analysis\n4.3.1\nToken Representation Self-similarity\nTo better understand the performance superiority of\nCLAPSPeech over existing representation learning\nmethods for TTS, we analyze the token represen-\ntation learned by CLAPSpeech and other methods.\nFollowing Su et al. (2021), we de\ufb01ne the averaged\nsimilarity on the selected token under different con-\n6In our PB/VB TTS baseline, the duration is predicted in\nphoneme/word level, respectively.\ntexts T = [T1, ..., TN] as,\ns(T) =\n1\nN(N \u2212 1)\nN\nX\ni=1\nN\nX\nj=1,j\u0338=i\ncosine(Ti, Tj)\n(4)\nwhere Ti and Tj are the selected token\u2019s encoding\nextracted by the model from different text contexts.\nIntuitively, a lower s(T) indicates that the selected\ntoken itself plays a smaller role in generating its rep-\nresentation, which means that the model captures\nmore context-related information from the input\ntext sequence, and thus predicts better prosody.\nQuantitative Evaluation\nWe sample 10,000\nbatches (each batch consists of 256 sentences that\ncontain the same selected token) from the ASR\nvalidation datasets and compute the averaged self-\nsimilarity. The result is shown in Table 2. We\nobserve that our CLAPSpeech learned with the\ncontrastive objective (in Equation 2) achieves the\nlowest similarity in the off-diagonal entries of the\nsimilarity matrix, which denotes that the model has\nmade use of the text context to capture the prosody\nvariance of the same token, thus achieve the best\nprosody performance in Table 1. Besides, we can\nsee that BERT also achieves a relatively low off-\ndiagonal similarity, which is due to its MLM task\nduring pre-training, in which the model needs to\nextract semantic information from context to pre-\ndict the masked token. By contrast, the vanilla\nTTS text encoder and A3T fail to achieve a low off-\ndiagonal similarity, which means that both models\ncannot extract discriminative information from dif-\nferent contexts. We suspect the failure of A3T is\ndue to the fact that its MAM objective encourages\nthe model to predict the masked mel-spectrogram\npatch based on the input unmasked text sequence,\nwhich increases the model\u2019s demand for phonetic\ninformation of the selected token.\nQualitative Evaluation\nWe sample 8 sentences7\nthat contain the word \"higher\" from LibriSpeech\nand visualize the self-similarity matrix M (where\nMi,j = cosine(Ti, Tj)) produced by CLAPSpeech\nand vanilla TTS text encoder. The results are shown\nin Figure 6, where a darker color denotes a higher\nself-similarity score.\nWe also provide the self-\nsimilarity matrix of BERT and A3T in Figure 9\nof Appendix C. We can see that the self-similarities\nof CLAPSpeech are much lower in the off-diagonal\nentries.\n7We list these sentences in Table 5 of Appendix C.\nTable 1: Performance comparison of different methods.\nPB and VB denote prediction-based and variaition-\nbased TTS baselines, respectively. DTW denotes the dynamic time warping distance of pitch contours in the\nMel-spectrogram. DE means the averaged absolute duration error in micro-seconds.\nMethod\nLJSpeech\nBiaobei\nLibriTTS\n#Params\nMOS\u2191\nDTW\u2193\nDE\u2193\nMOS\u2191\nDTW\u2193\nDE\u2193\nMOS\u2191\nDTW\u2193\nDE\u2193\nGT\n4.81\n0\n0\n4.59\n0\n0\n4.40\n0\n0\n/\nGT(voc.)\n4.63\n0\n0\n4.43\n0\n0\n4.26\n0\n0\n/\nPB\n3.77\n29.09\n25.77\n3.37\n18.01\n28.79\n3.43\n14.26\n27.42\n11.99M\nPB + BERT\n4.04\n27.43\n24.97\n3.43\n16.79\n28.06\n3.60\n13.82\n26.70\n109.48M\nPB + A3T\n3.92\n28.18\n25.63\n3.51\n17.18\n28.44\n3.54\n13.67\n27.03\n48.25M\nPB + CLAPSpeech\n4.11\n27.16\n24.19\n3.62\n16.04\n27.60\n3.71\n13.37\n26.46\n30.51M\nVB\n3.96\n27.58\n53.23\n3.75\n14.22\n40.31\n3.81\n11.96\n52.51\n23.02M\nVB + BERT\n4.13\n26.97\n52.01\n3.91\n13.63\n38.41\n3.95\n11.51\n51.27\n132.69M\nVB + A3T\n4.05\n26.37\n52.17\n4.04\n13.97\n39.15\n3.82\n11.71\n51.98\n59.73M\nVB + CLAPSpeech\n4.28\n25.94\n51.34\n4.22\n13.48\n37.07\n4.06\n10.93\n50.89\n41.54M\n(a) GT\n(b) PB\n(c) PB+BERT\n(d) PB+A3T\n(e) PB + CLAPSpeech\nFigure 5: Visualizations of the mel-spectrograms generated by different TTS systems.\nTable 2:\nSelf-similarity score of different methods.\nTTS denotes the text encoder of the vanilla TTS base-\nline.\nText Encoder of\nTTS\nBERT\nA3T\nCLAPSPeech\nSelf-Similarity\n0.9854\n0.5517\n0.9390\n0.4160\n4.3.2\nFine-grained Prosody Transfer\nWe perform an intuitive case study about prosody\ntransfer to further validate that our CLAPSpeech\u2019s\ntext-speech joint multi-modal space represents\nhigh-level prosody patterns (i.e., the pitch contours\nand duration information). We take s7/8 in Table 5\nas the reference/source audio and expect to trans-\nfer the word \"higher\"\u2019s prosody pattern from s7 to\ns8. Speci\ufb01cally, we use the text encoder of CLAP-\nSpeech to extract the text prosody encoding of s7\nand s8, then replace the text token encoding of\n\"higher\" in s8 with that in s7. As shown in Figure\n7, the prosody pattern of \"higher\" in s88 in Figure\n7(a) has been successfully transferred into s7 in\nFigure 7(c). We also provide audio samples of this\ncase study on our demo page. The manipulation of\nthe local prosody proves that our CLAPSpeech ex-\n8the pitch contours in reference remain \ufb02at in the early\nstage and then rise in the late stage\n1.00\n0.60\n0.59\n0.61\n0.66\n0.58\n0.45\n0.57\n0.60\n1.00\n0.67\n0.67\n0.50\n0.45\n0.48\n0.57\n0.59\n0.67\n1.00\n0.50\n0.31\n0.50\n0.52\n0.84\n0.61\n0.67\n0.50\n1.00\n0.46\n0.51\n0.41\n0.46\n0.66\n0.50\n0.31\n0.46\n1.00\n0.40\n0.37\n0.49\n0.58\n0.45\n0.50\n0.51\n0.40\n1.00\n0.28\n0.52\n0.45\n0.48\n0.52\n0.41\n0.37\n0.28\n1.00\n0.44\n0.57\n0.57\n0.84\n0.46\n0.49\n0.52\n0.44\n1.00\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\n0.3\n1.0\n(a) CLAPSpeech\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\n0.96\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.96\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n0.96\n1.00\n1.00\n1.00\n0.98\n1.00\n0.99\n1.00\n0.96\n1.00\n1.00\n1.00\n1.00\n0.99\n1.00\n1.00\n0.98\n0.99\n0.99\n1.00\n1.00\n1.00\n1.00\n1.00\n0.96\n1.00\n0.96\n0.96\n0.96\n0.96\n0.98\n0.96\n1.00\n0.96\n1.00\n1.00\n1.00\n1.00\n0.99\n1.00\n0.96\n1.00\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\n0.3\n1.0\n(b) TTS\nFigure 6: Example: self-similarity matrix visualization\nof CLAPSpeech and the text encoder of the vanilla TTS\nmodel. si denotes the ith sentence.\ntract prosody representation effectively in\ufb02uences\nthe prosody prediction of the TTS system.\n4.4\nAblation Studies\nUse BPE as Auxiliary Features\nWe \ufb01rst ana-\nlyze the effectiveness of the BPE as an auxiliary\nfeature to help extract prosody information from\nthe text context. During the pre-training phase of\nCLAPSpeech, we found removing BPE from the\ntext encoder signi\ufb01cantly degrades the validation\nCLIP loss from 0.3692 to 0.6764. Then in the TTS\nphase, as can be seen in line 3 in Table 3, the ab-\n(a) reference (s7)\n(b) source (s8)\n(c) transferred (s8)\nFigure 7: Visualizations of the mel-spectrograms gen-\nerated in prosody transfer.\nlated model using the pre-trained text encoder with-\nout BPE leads to a performance drop in terms of\nCMOS, DTW, and DE. This is possibly due to the\nfact that BPE could better represent the semantic\ninformation than the low-level phoneme sequence.\nMulti-scale Pre-training\nTo demonstrate the ef-\nfectiveness of multi-scale pre-training, as can be\nseen in line 4/5 in Table 3, we tried to remove\nphoneme-level or word-level CLAPSpeech from\nthe model, which leads to a worse prosody perfor-\nmance. We also tried to use the untrained CLAP-\nSpeech to prove the necessity of the pre-training\nprocess, and we found this ablated model (line 6)\nachieves a slightly worse performance than the TTS\nbaseline (line 3).\nTable 3: Performance comparison for ablation studies.\nSetting\nCMOS\nDTW\nDE\nTTS + CLAPSpeech\n0\n27.16\n24.19\nTTS baseline\n-1.53\n29.09\n25.77\nw/o BPE\n-1.08\n28.21\n24.93\nw/o ph-level\n-1.11\n27.68\n25.01\nw/o word-level\n-0.46\n27.55\n24.52\nuntrained\n-1.67\n29.45\n25.96\n5\nConclusion\nIn this paper, we propose CLAPSpeech, a cross-\nmodal contrastive pre-training framework that pro-\nvides better text representation with rich prosody\ninformation for TTS. With the design of a text en-\ncoder and a prosody encoder, CLAPSpeech learns\nto connect the text context with its corresponding\nprosody pattern in the speech. We also introduced\nmulti-scale pre-training to extract prosody patterns\nat multiple levels. We have demonstrated the perfor-\nmance and generalization ability of CLAPSpeech\non three TTS datasets (English, Chinese, and multi-\nspeaker, respectively). We have also deeply an-\nalyzed the principle behind the improvement of\nCLAPSpeech and performed ablation studies to\nprove the necessity of each component.\n6\nLimitations\nThere are majorly two limitations: Firstly, in this\nwork, we only consider the current-sentence text\ncontext-related prosody. In future work, we will\nfocus on improving the inter-sentence prosody to\nachieve coherent, expressive TTS for long-form\ntext. Secondly, other variables are not considered\nduring the contrastive pre-training. One can ex-\nplore similar approaches that connect prosody to\nother conditions such as speaker, emotion, etc.\n7\nEthics Statement\nCLAPSpeech improves the prosody of the synthe-\nsized speech, which may cause unemployment for\npeople with related occupations. Besides, the pro-\nduction of fake speeches may cause voice security\nissues. Further efforts in automatic speaker veri\ufb01-\ncation should be made to improve voice security.\n8\nAcknowledgment\nThis work was supported in part by the Na-\ntional Key R&D Program of China under Grant\nNo.2022ZD0162000,National Natural Science\nFoundation of China under Grant No. 62222211\nand Grant No.61836002 and Grant No.62072397,\nand Yiwise.\nReferences\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020.\nwav2vec 2.0: A frame-\nwork for self-supervised learning of speech represen-\ntations. In NIPS.\nHe Bai, Renjie Zheng, Junkun Chen, Mingbo Ma, Xin-\ntong Li, and Liang Huang. 2022. A3t: Alignment-\naware acoustic and text pretraining for speech syn-\nthesis and editing. In ICML.\nJunkun Chen, Mingbo Ma, Renjie Zheng, and Liang\nHuang. 2020.\nMam: Masked acoustic modeling\nfor end-to-end speech-to-text translation.\narXiv\npreprint arXiv:2010.11445.\nLiping Chen, Yan Deng, Xi Wang, Frank K Soong, and\nLei He. 2021. Speech bert embedding for improving\nprosody in neural tts. In ICASSP.\nYu-An\nChung,\nYuxuan\nWang,\nWei-Ning\nHsu,\nYu Zhang, and RJ Skerry-Ryan. 2019.\nSemi-\nsupervised training for improving data ef\ufb01ciency in\nend-to-end speech synthesis. In ICASSP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nJeff Donahue, Sander Dieleman, Miko\u0142aj Bi\u00b4nkowski,\nErich Elsen, and Karen Simonyan. 2021.\nEnd-to-\nend adversarial text-to-speech. In ICLR.\nIsaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang,\nYe Jia, Ron J Weiss, and Yonghui Wu. 2021. Par-\nallel tacotron: Non-autoregressive and controllable\ntts. In ICASSP.\nBenjamin Elizalde, Soham Deshmukh, Mahmoud Al\nIsmail, and Huaming Wang. 2022. Clap: Learning\naudio concepts from natural language supervision.\narXiv preprint arXiv:2206.04769.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR.\nWei-Ning\nHsu,\nBenjamin\nBolte,\nYao-Hung\nHu-\nbert Tsai, Kushal Lakhotia, Ruslan Salakhutdi-\nnov, and Abdelrahman Mohamed. 2021.\nHubert:\nSelf-supervised speech representation learning by\nmasked prediction of hidden units.\nIEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing, 29:3451\u20133460.\nKeith\nIto\nand\nLinda\nJohnson.\n2017.\nThe\nlj\nspeech\ndataset.\nhttps://keithito.com/\nLJ-Speech-Dataset/.\nYe Jia, Heiga Zen, Jonathan Shen, Yu Zhang, and\nYonghui Wu. 2021. Png bert: Augmented bert on\nphonemes and graphemes for neural tts.\nYe Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan\nShen, Fei Ren, Patrick Nguyen, Ruoming Pang,\nIgnacio Lopez Moreno, Yonghui Wu, et al. 2018.\nTransfer learning from speaker veri\ufb01cation to mul-\ntispeaker text-to-speech synthesis. NIPS.\nZiyue Jiang,\nSu Zhe,\nZhou Zhao,\nQian Yang,\nYi Ren, Jinglin Liu, and Zhenhui Ye. 2022. Dict-\ntts: Learning to pronounce with prior dictionary\nknowledge for text-to-speech.\narXiv preprint\narXiv:2206.02147.\nJaehyeon Kim, Sungwon Kim, Jungil Kong, and Sun-\ngroh Yoon. 2020. Glow-tts: A generative \ufb02ow for\ntext-to-speech via monotonic alignment search. In\nNIPS.\nJaehyeon Kim, Jungil Kong, and Juhee Son. 2021.\nConditional variational autoencoder with adversarial\nlearning for end-to-end text-to-speech. In ICML.\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020.\nHi\ufb01-gan: Generative adversarial networks for ef\ufb01-\ncient and high \ufb01delity speech synthesis. In NIPS.\nJinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and\nZhou Zhao. 2022. Diffsinger: Singing voice synthe-\nsis via shallow diffusion mechanism. In AAAI.\nRui Liu, Berrak Sisman, and Haizhou Li. 2021. Graph-\nspeech: Syntax-aware graph attention network for\nneural speech synthesis. In ICASSP.\nChenfeng Miao, Liang Shuang, Zhengchen Liu, Chen\nMinchuan, Jun Ma, Shaojun Wang, and Jing Xiao.\n2021. Ef\ufb01cienttts: An ef\ufb01cient and high-quality text-\nto-speech architecture. In ICML.\nMeinard Muller. 2007. Dynamic time warping. Infor-\nmation retrieval for music and motion, pages 69\u201384.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015.\nLibrispeech:\nan asr\ncorpus based on public domain audio books.\nIn\nICASSP.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021.\nLearning transferable visual models\nfrom natural language supervision. In ICML.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2021a. Fastspeech 2:\nFast and high-quality end-to-end text to speech. In\nICLR.\nYi Ren, Ming Lei, Zhiying Huang, Shiliang Zhang,\nQian Chen, Zhijie Yan, and Zhou Zhao. 2022.\nProsospeech:\nEnhancing prosody with quantized\nvector pre-training in text-to-speech. In ICASSP.\nYi Ren, Jinglin Liu, and Zhou Zhao. 2021b.\nPor-\ntaspeech: Portable and high-quality generative text-\nto-speech. In NIPS.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2019.\nFastspeech:\nFast, robust and controllable text to speech.\nYusuxke Shibata, Takuya Kida, Shuichi Fukamachi,\nMasayuki Takeda, Ayumi Shinohara, Takeshi Shino-\nhara, and Setsuo Arikawa. 1999. Byte pair encoding:\nA text compression scheme that accelerates pattern\nmatching.\nYixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei\nShu, Ehsan Shareghi, and Nigel Collier. 2021. Tacl:\nImproving bert pre-training with token-aware con-\ntrastive learning. arXiv preprint arXiv:2111.04198.\nXu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. 2021.\nA survey on neural speech synthesis. arXiv preprint\narXiv:2106.15561.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nPeilu Wang, Yao Qian, Frank K Soong, Lei He, and Hai\nZhao. 2015. Word embedding for recurrent neural\nnetwork based tts synthesis. In ICASSP.\nPhonetic \nEncoder\nWord-level\nCLAPSpeech\nPhoneme-level \nCLAPSpeech\nphoneme\nBPE\nphoneme-level\nencoding \n+\nCombine\nEncoder\n+\nexpand\nDuration & Pitch Predictor\nMel-Spec Decoder\nMulti-Length\nDiscriminator\nstop gradient\nFigure 8: FastSpeech 2 with CLAPSpeech plugged in.\nYuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry\nRyan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia,\nFei Ren, and Rif A Saurous. 2018.\nStyle tokens:\nUnsupervised style modeling, control and transfer in\nend-to-end speech synthesis. In ICML.\nZhenhui Ye, Zhou Zhao, Yi Ren, and Fei Wu. 2022.\nSyntaspeech: Syntax-aware generative adversarial\ntext-to-speech. arXiv preprint arXiv:2204.11792.\nHeiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J\nWeiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. 2019.\nLibritts: A corpus derived from librispeech for text-\nto-speech. arXiv preprint arXiv:1904.02882.\nBinbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao,\nChao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu\nChen, Chenchen Zeng, et al. 2022.\nWenetspeech:\nA 10000+ hours multi-domain mandarin corpus for\nspeech recognition. In ICASSP.\nMingyang Zhang, Xin Wang, Fuming Fang, Haizhou\nLi, and Junichi Yamagishi. 2019.\nJoint training\nframework for text-to-speech and voice conversion\nusing multi-source tacotron and wavenet. In INTER-\nSPEECH.\nA\nDetails of Models\nA.1\nCLAPSpeech plugged in FastSpeech 2\nWe show how to integrate CLAPSpeech into a pop-\nular prediction-based TTS system, FastSpeech 2.\nAs shown in Figure 8, the pre-trained text encoders\nof CLAPSpeech (marked with a red dashed rectan-\ngle) perform as an auxiliary encoder to the original\nphonetic encoder of FastSpeech 2. The phoneme-\nlevel outputs of the phonetic encoder and CLAP-\nSpeech text encoder are fused and processed by the\nfollowing encoder. Note that we \ufb01x the parameters\nof CLAPSpeech text encoders during the training\nof the TTS system to avoid over\ufb01tting.\nA.2\nMulti-length Adversarial Training\nFor the tested TTS baselines, we adopt an addi-\ntional multi-length discriminator to provide a least\nsquared GAN loss to improve the audio quality.\nThe multi-length discriminator is an ensemble of\nmultiple CNN-based discriminators which evalu-\nates the mel-spectrogram based on random win-\ndows of different lengths. One could refer to Ye\net al. (2022) for more details.\nB\nDetailed Experimental Settings\nB.1\nModel Con\ufb01gurations\nWe list the hyper-parameters of CLAPSpeech and\nthe tested TTS baselines in Table 4.\nB.2\nSubjective Evaluation\nFor each tested dataset, we randomly select 10\ntexts from the test set and use the TTS systems to\ngenerate the audio samples. Each audio has been\nlistened to by at least 20 native listeners, who are\nrecruited on a crowdsourcing platform, Zhengshu\nTechnology. We tell listeners to \"focus on examing\nthe naturalness of prosody (e.g., pitch, energy, and\nduration) and audio quality (noise, timbre, sound\nclarity, and high-frequency details)\". For MOS,\neach tester is asked to evaluate the subjective nat-\nuralness of a sentence on a 1-5 Likert scale. For\nCMOS, listeners are asked to compare pairs of au-\ndio generated by systems A and B and indicate\nwhich of the two audio they prefer and choose one\nof the following scores: 0 indicating no difference,\n1 indicating small difference, 2 indicating a large\ndifference, and 3 indicating a very large difference.\nC\nMore Details in Analysis\nC.1\nExample Sentences\nWe list the 8 example sentences in Table 5. These\nsentences are used as examples in Section 4.3.\nC.2\nSelf-similarity of Other Baselines\nThe self-similarity visualization of A3T and BERT\ncan be found in Figure 9. We discuss the results in\nSection 4.3.1.\nTable 4: The detailed model con\ufb01gurations.\nHyper-parameter\nCLAPSpeech\nNumber of parameters\nText Encoder\nPhoneme/BPE embedding hidden size\n192\n18.517M\nPhoneme/BPE encoder FFT blocks\n4\nHidden size\n192\nConv1D kernel\n5\nConv1D \ufb01lter size\n768\nProsody Encoder\nResidual blocks\n4\n21.801M\nNumber of conv layers per block\n12\nHidden size\n192\nInput mel-spectrogram length\n128\nHidden size in pooling layer\n768\n#Attention heads in pooling layer\n4\nPrediction-based TTS baseline\nEncoder Layers\n4\n11.993M\nDecoder Layers\n4\nEncoder/Decoder Conv1D Kernel\n9\nEncoder/Decoder Conv1D channel size\n256\nVariation-based TTS baseline\nEncoder Layers\n8\n23.020M\nDecoder Layers\n4\nEncoder/Decoder Conv1D Kernel\n5\nEncoder/Decoder Conv1D channel size\n192\nLatent Size\n16\nPrior Flow Layers\n4\nPrior Flow Conv1D Kernel\n3\nPrior Flow Conv1D Channel Size\n64\nMulti-Length Discriminator\nNumber of CNN-based Discriminators\n3\n0.927M\nWindow size\n32,64,128\nConv2D layers\n3\nHidden size\n192\ns1\n... for the reputation of the stern judge stands not higher than that of the compassionate ...\ns2\nAs I went on , the precipices rose higher and seemed to overhang. The channel grew narrower ...\ns3\nBetter, and better, and better! Her voice went higher with each better, till it got quite to a squeak at last.\ns4\n... and the native graduates of our higher institutions have begun to show their strength ...\ns5\nInnocence is higher than virtue.\ns6\nNothing seems more un\ufb01t to give a deeper meaning to life and a higher value.\ns7\nHigher up could be seen some chinamen, but whether they were \ufb01shing or washing we could not tell .\ns8\nMay they become convalescents and overcomers, and create higher bodies for themselves !\nTable 5: The text sentences used in the intuitive example, the selected word token \"higher\" is bold.\n1.00\n0.99\n0.99\n0.98\n0.98\n0.96\n0.94\n0.98\n0.99\n1.00\n0.99\n0.99\n0.99\n0.98\n0.91\n0.98\n0.99\n0.99\n1.00\n0.97\n0.97\n0.98\n0.94\n0.98\n0.98\n0.99\n0.97\n1.00\n0.97\n0.96\n0.92\n0.99\n0.98\n0.99\n0.97\n0.97\n1.00\n1.00\n0.90\n1.00\n0.98\n0.98\n0.98\n0.96\n1.00\n1.00\n0.90\n1.00\n0.94\n0.91\n0.94\n0.92\n0.90\n0.90\n1.00\n0.90\n0.98\n0.98\n0.98\n0.99\n1.00\n1.00\n0.90\n1.00\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\n0.3\n1.0\n(a) A3T\n1.00\n0.61\n0.57\n0.60\n0.73\n0.66\n0.42\n0.61\n0.61\n1.00\n0.72\n0.64\n0.58\n0.62\n0.52\n0.72\n0.57\n0.72\n1.00\n0.59\n0.60\n0.53\n0.53\n0.62\n0.60\n0.64\n0.59\n1.00\n0.66\n0.67\n0.51\n0.71\n0.73\n0.58\n0.60\n0.66\n1.00\n0.70\n0.53\n0.71\n0.66\n0.62\n0.53\n0.67\n0.70\n1.00\n0.50\n0.76\n0.42\n0.52\n0.53\n0.51\n0.53\n0.50\n1.00\n0.53\n0.61\n0.72\n0.62\n0.71\n0.71\n0.76\n0.53\n1.00\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\n0.3\n1.0\n(b) BERT\nFigure 9: Self-similarity matrix visualization of A3T\nand BERT.\n"
  },
  {
    "title": "mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences",
    "link": "https://arxiv.org/pdf/2305.11129.pdf",
    "upvote": "2",
    "text": "mLongT5: A Multilingual and Efficient Text-To-Text Transformer for\nLonger Sequences\nDavid Uthus, Santiago Onta\u00f1\u00f3n, Joshua Ainslie, Mandy Guo\nGoogle Research\n{duthus, santiontanon, jainslie, xyguo}@google.com\nAbstract\nWe present our work on developing a multilin-\ngual, efficient text-to-text transformer that is\nsuitable for handling long inputs. This model,\ncalled mLongT5, builds upon the architecture\nof LongT5, while leveraging the multilingual\ndatasets used for pretraining mT5 and the pre-\ntraining tasks of UL2. We evaluate this model\non a variety of multilingual summarization and\nquestion-answering tasks, and the results show\nstronger performance for mLongT5 when com-\npared to existing multilingual models such as\nmBART or M-BERT.\n1\nIntroduction\nIn recent years, there has been development of mak-\ning transformer-based models more efficient so that\nthey can handle longer input sequences. Many of\nthe models though have been English-only, making\nthem inapplicable to other languages.\nIn this paper, we present our work in extending\none of these models to be able to handle multi-\nlingual data. Our model, called mLongT5, takes\nadvantage of the efficient architecture of LongT5\n(Guo et al., 2022), and has been pretrained on the\nmultilingual mC4 dataset (Xue et al., 2021) to be\nable to work on multilingual tasks. We have ap-\nplied mLongT5 to a variety of multilingual summa-\nrization and question-answering tasks, and results\nshow that mLongT5 exhibits strong performance\nin these domains.\nThe configurations1 and checkpoints2 have all\nbeen open-sourced.\n2\nRelated Work\nThere are two areas of related work \u2013 efficient trans-\nformer models that can handle long inputs, and\nmultilingual models.\n1https://github.com/google/flaxformer/tree/\nmain/flaxformer/t5x/configs/longt5/models\n2https://github.com/google-research/longt5\nThere has been much interest of late in making\ntransformer models more efficient, such as to han-\ndle longer inputs. Example of these include ETC\n(Ainslie et al., 2020), Big Bird (Zaheer et al., 2020),\nLongT5 (Guo et al., 2022), and Longformer (Belt-\nagy et al., 2020). These models were successful in\ntaking various approaches to address the quadratic\ngrowth of the attention mechanism in transformers.\nUnfortunately though, these models are trained on\nEnglish datasets, limiting their use in multilingual\ndomains.\nWith respect to multilingual models, these would\ninclude mT5 (Xue et al., 2021), mBART (Liu et al.,\n2020), and the recent umT5 (Chung et al., 2023).\nThese models re-used architectures used by English\nmodels but are pretrained on a larger, multilingual\ncorpus, with mT5 and umT5 trained on 101 lan-\nguages and mBART on 25. While these models\nshowed strong performance on being able to handle\na wide variety of languages, they suffered the same\nrestrictions as their original English models on not\nbeing able to scale up to longer sequences.\n3\nModel\nmLongT5 builds upon the architecture of LongT5\n(Guo et al., 2022). LongT5 was developed to ef-\nficiently handle long inputs by utilizing a more\nefficient attention mechanism.\nThe model was\nshown to have strong performance on a variety\nof downstream tasks, and thus is the foundation for\nmLongT5.\n3.1\nDatasets\nTo make mLongT5 multilingual, we leverage the\nmC4 dataset used for training the multilingual\nmodel mT5 (Xue et al., 2021), which consists of\n101 languages. This dataset has recently been up-\ndated, as described by Chung et al. (2023), and was\nused for training umT5 and creating a new Senten-\ncePiece model (Kudo and Richardson, 2018). As\nsuch, we then make use of the same SentencePiece\narXiv:2305.11129v2  [cs.CL]  26 Oct 2023\nmodel used for umT5, thus allowing mLongT5 to\nhandle multilingual inputs.\n3.2\nPretraining Tasks\nOne key difference with our model and LongT5\nis the changing of tasks for pretraining the model.\nLongT5 made use of PEGASUS\u2019 Principle Sen-\ntences Generation (PSG) (Zhang et al., 2020) for\npretraining its models. While this was shown to\nhave strong performance for various downstream\ntasks, the one weakness of PSG is that it is less\nsuitable for multilingual training. PSG relies on be-\ning able to split a piece of text into sentences, with\ncurrent implementation best suited for Latin-based\nlanguages. The need to break text into sentences\nproperly for 101 different languages makes it then\na challenging task to use in a multilingual setting.\nTo overcome this, we instead decided to ap-\nply UL2\u2019s pretraining tasks (Tay et al., 2022).\nTheir pretraining task, called Mixture-of-Denoisers\n(MoD), has the model learning from a mixture of\ntasks, and has been shown to work better than T5\u2019s\noriginal pretraining task (Raffel et al., 2019). More\nimportantly, MoD can be more easily applied to\nother languages compared to PSG, thus making it\nideal for pretraining mLongT5.\n3.3\nPretraining Details\nPretraining mLongT5 has many similarities to how\nLongT5 was pretrained. It is pretrained for one mil-\nlion steps, and we pretrained model sizes of Base,\nLarge, and XL. We also use the same pretraining\nlengths, 4,096 for the inputs and 910 for the tar-\ngets. One small difference is increasing the batch\nsize from 128 to 256, allowing the model to train\non the same number of tokens as mT5. For the\nmC4 dataset, we used version 3.1.0, which is the\nversion update by Chung et al. (2023). For dataset\nsampling, we use the UniMax sampling method\n(Chung et al., 2023).\nInstead of PSG as pretraining task, we apply\nMoD, using the same configuration as defined in\nthe original UL2 task definition. The only excep-\ntion is that we do not use 0.5 corruption rate (using\nonly corruption rate of 0.15), as our input lengths\n(4096) are much longer than our target lengths\n(910), making a corruption rate of 0.5 unfeasible.\nAll models were pretrained using 256 TPUv4\nchips. Wall time to pretrain these models was 1.9\ndays for Base, 3.7 days for Large, and 12.4 days\nfor XL.\n4\nResults\nAs with the original LongT5 paper, we look at two\ndomains for evaluating our model: summarization\nand question answering.\nFor all of these tasks, we use the default values\nas used for T5 finetuning, only explicitly setting the\ninput and target lengths as described in the tasks\nbelow.\n4.1\nSummarization\nThe three summarization tasks we are looking at\nare:\n\u2022 MLSUM (Scialom et al., 2020): a collection\nof newspaper articles and their corresponding\nsummaries in five languages: French, German,\nSpanish, Russian, and Turkish.\n\u2022 XL-Sum (Hasan et al., 2021): a collection of\nBBC articles and summaries in 44 languages.\n\u2022 WikiLingua (Ladhak et al., 2020): a collec-\ntion of documents from WikiHow (in Span-\nish, Turkish, Russian, and Vietnamese) that\nhave been translated and summarized into\nEnglish.\nFor this task, we are using the\nGEM (Gehrmann et al., 2021) version of the\ndatasets, allowing us to make use of their fixes\nin the splitting of the datasets for training and\ntesting.\nThese tasks allow us to explore summarization\nwhere the task involves documents and their sum-\nmaries in the same language (MLSUM, XL-Sum),\nor where the task involves both translation and sum-\nmarization at the same time (WikiLingua).\nWe note that with respect to task lengths, these\nmultilingual tasks are not very long when com-\npared to the tasks covered in the original LongT5\npaper. There is unfortunately a lack of lengthy,\nmultilingual summarization tasks available, thus\nwe use these three for comparisons. As such, we\ntested with input lengths of 4k for input and 512\nfor output, which covers most documents for all\nthe above tasks.\nFor all these tasks, we report standard ROUGE\nscores (ROUGE-1, ROUGE-2, and ROUGE-L).\n4.1.1\nMLSUM\nTable 1 shows our results for the MLSUM task.\nWe are comparing to the M-BERT (Devlin, 2018)\nmodel used in the original paper. The authors only\nFR\nApproach\nR-1\nR-2\nR-L\nM-BERT\n-\n-\n25.09\nmLongT5 (base)\n30.79\n14.16\n23.83\nmLongT5 (large)\n31.44\n14.74\n24.36\nmLongT5 (xl)\n32.18\n15.68\n25.18\nDE\nApproach\nR-1\nR-2\nR-L\nM-BERT\n-\n-\n42.01\nmLongT5 (base)\n45.60\n35.31\n42.22\nmLongT5 (large)\n46.21\n35.68\n42.71\nmLongT5 (xl)\n46.95\n36.36\n43.45\nES\nApproach\nR-1\nR-2\nR-L\nM-BERT\n-\n-\n20.44\nmLongT5 (base)\n28.78\n10.98\n23.15\nmLongT5 (large)\n29.05\n11.58\n23.50\nmLongT5 (xl)\n30.36\n12.77\n24.73\nTR\nApproach\nR-1\nR-2\nR-L\nM-BERT\n-\n-\n32.94\nmLongT5 (base)\n44.18\n30.86\n38.60\nmLongT5 (large)\n44.92\n31.55\n39.29\nmLongT5 (xl)\n45.73\n32.80\n40.26\nRU\nApproach\nR-1\nR-2\nR-L\nM-BERT\n-\n-\n9.48\nmLongT5 (base)\n7.73\n1.78\n7.22\nmLongT5 (large)\n7.71\n1.86\n7.23\nmLongT5 (xl)\n8.85\n2.67\n8.42\nTable 1: MLSUM results comparing mLongT5 with the\noriginal model M-BERT. Note that the original paper\nonly reported ROUGE-L scores, while we also report\nROUGE-1 and ROUGE-2.\nreported ROUGE-L scores, while we also report\nROUGE-1 and ROUGE-2 scores.\nLooking at the ROUGE-L scores, we can see\nthat mLongT5 performs comparably to M-BERT\nfor French, while doing better than M-BERT for\nall model sizes in German, Spanish, and Turkish. It\nis only with Russian does it do slightly worse. As\nnoted in the original paper, Russian was the hardest\nlanguage for language models, due to having a\nmuch smaller dataset when compared to the other\nlanguages in the corpus and a higher rate of novelty\n(words found in the summary but not in the input\ndocument). Additionally, as we mentioned before,\nthe dataset input lengths are not very long, thus\nmodels with full attention can take better advantage\nof the short lengths compared to mLongT5. This\ncan then contribute to mLongT5 not performing as\nwell for this instance.\n4.1.2\nXL-Sum\nFor XL-Sum, we finetuned the model in a similar\napproach to the original paper \u2013 we finetuned on\na mixture of all the languages for 50,000 steps,\nand then performed tests for each of the individual\nlanguages from this single model.\nTable 2 shows a subset of the languages (the full\nresults can be seen in Appendix A). We highlight\nlanguages that had longer input lengths (due to both\nthe length of the original documents and how they\nare then subsequently tokenized by the SPM).\nAs we can see, mLongT5 performed well com-\npared to mT5 for these lengthier inputs. When\ncomparing base to base, it did slightly worse, as\nexpected with mT5 having full attention. The orig-\ninal LongT5 model, when finetuned on datasets\nthat are of shorter lengths, had also shown slightly\nworse performance when compared to a model of\nfull attention. We are seeing similar results here.\nBut mLongT5 is able to more easily scale to larger\nmodel sizes, and as such, we can see stronger re-\nsults as we increase the size of the model.\n4.1.3\nWikiLingua\nThe final summarization task is WikiLingua, with\nresults shown in Table 3. This task requires both\ntranslation and summarization, with the task trans-\nlating from a full document of another language\ninto an English summary. As previously mentioned\nwe are using the GEM version of this task, and\ncompare our results to the mT5 model on their\nleaderboard.\nAs shown in the results, mLongT5 tends to do\nbetter for many of the model sizes across the 4\nlanguages, with only slightly worse performance\nwith XL size for Spanish.\n4.2\nQuestion-Answering\nFor question-answering, we applied mLongT5 to\nTyDi QA (Clark et al., 2020). TyDi QA is a mul-\ntilingual task covering 11 languages, trying to an-\nswer questions given a Wikipedia article. There\nare two versions of this task, and we focus on the\nMinimal Answer Span Task, in which one is try-\ning to either find the minimal span that answer the\nquestion, give a yes/no answer if the question is a\nyes/no question, or Null if the question cannot be\nanswered given the article.\nSimilar to the original LongT5 paper and their\napplication to Natural Questions, we have re-\ndefined this task from extracting answer spans to\na seq2seq task of generating answer texts. The\nmT5 (base)\nmLongT5 (base)\nmLongT5 (large)\nmLongT5 (xl)\nLanguage\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nGujarati\n21.96\n7.74\n19.86\n19.59\n6.08\n17.61\n22.38\n7.94\n20.15\n25.52\n9.92\n22.78\nMarathi\n22.01\n9.54\n19.92\n20.33\n8.62\n18.41\n23.35\n10.56\n21.22\n25.90\n12.03\n23.07\nPunjabi\n30.70\n12.21\n25.52\n28.61\n10.43\n23.66\n31.92\n12.75\n26.17\n34.45\n14.81\n28.42\nSerbian (Cyrillic)\n23.78\n7.98\n20.14\n20.30\n5.86\n16.74\n21.92\n6.98\n18.35\n27.51\n11.46\n23.49\nSerbian (Latin)\n21.64\n6.66\n18.23\n18.14\n4.75\n14.96\n21.79\n6.92\n18.14\n25.86\n10.17\n21.76\nVietnamese\n32.88\n16.22\n26.08\n31.58\n15.41\n25.02\n34.54\n17.63\n27.59\n38.17\n20.49\n30.98\nTable 2: Results for XL-Sum, focusing on languages that have lengthier inputs. The rest of the results can be seen in\nthe Appendix A.\nES-EN\nTR-EN\nRU-EN\nVI-EN\nApproach\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nmT5 (base)\n30.9\n10.6\n26.4\n32.0\n13.1\n26.0\n27.3\n8.6\n23.3\n25.6\n7.7\n21.5\nmT5 (large)\n34.2\n12.6\n29.1\n34.0\n14.5\n27.5\n32.3\n11.2\n26.9\n32.1\n10.9\n26.0\nmT5 (xl)\n41.2\n17.2\n34.6\n40.0\n18.3\n33.3\n37.2\n14.6\n30.9\n37.6\n14.9\n31.2\nmLongT5 (base)\n36.1\n14.0\n30.3\n34.5\n14.9\n28.6\n32.4\n11.6\n26.5\n32.3\n11.7\n26.4\nmLongT5 (large)\n38.2\n15.5\n32.0\n38.1\n17.5\n32.0\n34.4\n13.1\n28.5\n35.1\n13.8\n29.1\nmLongT5 (xl)\n40.8\n17.6\n34.3\n42.5\n20.9\n36.7\n37.6\n15.7\n31.8\n38.7\n16.6\n32.8\nTable 3: WikiLingua summarization results. These results are using the GEM version of the task.\nresults shown will then differ from the TyDi QA\nleaderboard. As such, we have also run the similar\nmT5 model on the same task to get a baseline to\ncompare against. Additionally, as the test set is not\navailable for this task, we use 90% of the training\ndata as the train set and remaining 10% as the dev\nset, and use the original dev set as our test set for\nreporting metrics.\nUnlike the summarization tasks, TyDi QA has\nmuch longer input lengths \u2013 mean of 5,148 tokens\nand 90th percentile of 12,967 tokens when tok-\nenized with the SentencePiece model. As such,\nfor mT5 we tested with input lengths between 512\nand 4k, while for mLongT5 we tested with input\nlengths between 4k and 16k.\nTable 4 show the results of running mT5 and\nmLongT5 on this dataset. For this task, we report\nmetrics of Exact Match (EM) and F1 score. As can\nbe seen in the results, mLongT5 is able to better\nanswer the questions given that it can handle longer\ninput sequences.\n5\nConclusion\nWe have presented our new model mLongT5. It has\nthe benefits of the efficient architecture of LongT5,\nwith the ability to handle multingual inputs and\noutputs. As our report shows, the model is able\nto perform well on a variety of summarization and\nquestion-answering tasks.\nApproach\nEM\nF1\nmT5 (base - 512 input)\n37.16\n49.99\nmT5 (base - 1k input)\n43.09\n56.36\nmT5 (base - 2k input)\n44.63\n58.12\nmT5 (base - 4k input)\n45.41\n58.63\nmT5 (large - 512 input)\n40.96\n54.08\nmT5 (large - 4k input)\n52.77\n66.54\nmT5 (xl - 512 input)\n43.84\n56.98\nmT5 (xl - 4k input)\n55.03\n68.26\nmLongT5 (base - 4k input)\n50.76\n62.74\nmLongT5 (base - 8k input)\n51.21\n63.66\nmLongT5 (base - 16k input)\n52.43\n64.51\nmLongT5 (large - 4k input)\n54.04\n66.75\nmLongT5 (large - 8k input)\n55.56\n68.26\nmLongT5 (large - 16k input)\n55.93\n68.66\nmLongT5 (xl - 4k input)\n58.52\n70.86\nmLongT5 (xl - 8k input)\n59.6\n71.86\nmLongT5 (xl - 16k input)\n60.42\n72.63\nTable 4: TyDi QA results.\nLimitations\nmLongT5 has the same limitations as seen in the\noriginal LongT5 model, in that they are more suited\nfor tasks of lengthier inputs. Tasks with shorter\ninputs will be better served by models like mT5 and\numT5, which can take advantage of full attention.\nReferences\nJoshua Ainslie, Santiago Onta\u00f1\u00f3n, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. arXiv preprint arXiv:2004.08483.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nHyung Won Chung, Noah Constant, Xavier Garcia,\nAdam Roberts, Yi Tay, Sharan Narang, and Orhan\nFirat. 2023. UniMax: Fairer and more effective lan-\nguage sampling for large-scale multilingual pretrain-\ning.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454\u2013470.\nJacob Devlin. 2018. Multilingual BERT README.\nhttps://github.com/google-research/bert/\nblob/master/multilingual.md.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal,\nPawan\nSasanka\nAmmanamanchi,\nAnuoluwapo\nAremu,\nAntoine\nBosselut,\nKhy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin\nDurmus, Ond\u02c7rej Du\u0161ek, Chris Chinenye Emezue,\nVarun\nGangal,\nCristina\nGarbacea,\nTatsunori\nHashimoto, Yufang Hou, Yacine Jernite, Harsh Jham-\ntani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bod-\nhisattwa Prasad Majumder, Pedro Henrique Martins,\nAngelina McMillan-Major, Simon Mille, Emiel van\nMiltenburg, Moin Nadeem, Shashi Narayan, Vitaly\nNikolaev, Andre Niyongabo Rubungo, Salomey\nOsei,\nAnkur\nParikh,\nLaura\nPerez-Beltrachini,\nNiranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez,\nSashank\nSanthanam,\nJo\u00e3o\nSedoc,\nThibault Sellam, Samira Shaikh, Anastasia Shimo-\nrina, Marco Antonio Sobrevilla Cabezudo, Hendrik\nStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021.\nThe\nGEM benchmark:\nNatural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96\u2013120,\nOnline. Association for Computational Linguistics.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\nta\u00f1\u00f3n, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2022. LongT5: Efficient text-to-text transformer for\nlong sequences. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages 724\u2013\n736, Seattle, United States. Association for Compu-\ntational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693\u20134703, Online. Association for Computa-\ntional Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summariza-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4034\u20134048,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual Denoising\nPre-training for Neural Machine Translation. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:726\u2013742.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, and Jacopo Staiano. 2020.\nMLSUM: The multilingual summarization corpus.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8051\u20138067, Online. Association for Computa-\ntional Linguistics.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Huaixiu Steven Zheng, Denny\nZhou, Neil Houlsby, and Donald Metzler. 2022. UL2:\nUnifying language learning paradigms.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, On-\nline. Association for Computational Linguistics.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big Bird: Trans-\nformers for longer sequences. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n17283\u201317297. Curran Associates, Inc.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings\nof Machine Learning Research, pages 11328\u201311339.\nPMLR.\nA\nXL-Sum\nWe show the full results of running our mLongT5\nmodels on XL-Sum in Table 5. These results are\nthose that had been uploaded to GitHub 3 by the\nauthors along with the updated datasets.\nWhen computing ROUGE scores, we use similar\ncomputations as done in the respective paper, with\nexceptions to Chinese, Japanese and Thai. For\nthese languages, we use the SPM we used in our\nmodel for the tokenization of the results in order to\ncompute ROUGE.\n3https://github.com/csebuetnlp/xl-sum\nmT5 (base)\nmLongT5 (base)\nmLongT5 (large)\nmLongT5 (xl)\nLanguage\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nAmharic\n20.05\n7.41\n18.08\n16.70\n5.91\n14.73\n20.29\n7.99\n18.09\n22.37\n8.90\n19.91\nArabic\n34.91\n14.79\n29.16\n26.39\n11.01\n22.45\n27.65\n12.25\n23.57\n32.09\n15.04\n27.74\nAzerbaijani\n21.42\n9.52\n19.33\n17.52\n7.10\n15.77\n19.92\n8.80\n18.08\n22.68\n9.89\n20.36\nBengali\n29.57\n12.11\n25.13\n21.39\n8.22\n18.65\n24.69\n10.04\n21.25\n26.83\n11.32\n22.86\nBurmese\n15.96\n5.15\n14.18\n45.28\n26.62\n34.76\n49.07\n29.52\n38.10\n51.60\n31.69\n40.20\nChinese (Simp.)\n39.41\n17.79\n33.41\n38.90\n21.78\n32.59\n42.62\n24.70\n35.80\n48.42\n29.99\n41.28\nChinese (Trad.)\n37.19\n17.14\n31.62\n39.45\n22.40\n32.51\n43.32\n25.56\n35.95\n48.82\n30.80\n41.18\nEnglish\n37.60\n15.15\n29.88\n32.85\n11.38\n25.64\n35.59\n13.63\n28.02\n39.51\n17.00\n31.77\nFrench\n35.34\n16.17\n28.20\n30.06\n12.93\n24.21\n31.88\n14.32\n25.61\n34.82\n16.17\n28.11\nGujarati\n21.96\n7.74\n19.86\n19.59\n6.08\n17.61\n22.38\n7.94\n20.15\n25.52\n9.92\n22.78\nHausa\n39.44\n17.68\n31.67\n34.61\n13.73\n27.30\n38.04\n16.07\n30.32\n40.58\n18.57\n32.52\nHindi\n38.59\n16.88\n32.01\n34.81\n14.29\n28.71\n37.42\n16.71\n31.22\n40.92\n19.73\n34.41\nIgbo\n31.61\n10.16\n24.53\n25.82\n8.05\n20.19\n30.41\n10.01\n23.68\n31.31\n9.88\n24.07\nIndonesian\n37.00\n17.02\n30.76\n32.15\n13.05\n26.59\n35.17\n15.23\n29.07\n38.87\n18.00\n32.64\nJapanese\n48.15\n23.85\n37.36\n45.56\n27.12\n36.51\n48.60\n29.95\n39.00\n50.77\n32.06\n40.79\nKirundi\n31.99\n14.37\n25.83\n25.61\n10.07\n20.26\n29.36\n12.78\n23.67\n31.67\n14.55\n25.50\nKorean\n23.67\n11.45\n22.36\n20.25\n9.20\n19.00\n23.18\n10.42\n21.38\n25.30\n11.63\n23.31\nKyrgyz\n18.38\n7.96\n16.50\n14.08\n5.27\n12.46\n16.01\n6.30\n14.14\n18.19\n7.81\n16.00\nMarathi\n22.01\n9.54\n19.92\n20.33\n8.62\n18.41\n23.35\n10.56\n21.22\n25.90\n12.03\n23.07\nNepali\n26.65\n10.25\n24.28\n23.96\n8.94\n21.80\n26.24\n10.33\n23.91\n28.87\n11.59\n26.17\nOromo\n18.70\n6.17\n16.19\n14.88\n4.38\n12.71\n17.91\n5.65\n15.28\n19.52\n6.50\n17.18\nPashto\n38.47\n15.55\n31.91\n35.01\n13.79\n28.84\n38.63\n16.06\n32.00\n41.37\n17.61\n33.92\nPersian\n36.94\n16.19\n30.07\n35.47\n14.66\n28.40\n37.70\n16.45\n30.49\n40.64\n18.89\n33.16\nPidgin\n37.96\n15.12\n29.87\n33.86\n12.01\n26.68\n35.86\n13.72\n28.24\n38.01\n15.08\n29.78\nPortuguese\n37.17\n15.90\n28.56\n31.67\n12.51\n24.46\n34.04\n14.51\n26.65\n37.66\n17.57\n29.88\nPunjabi\n30.70\n12.21\n25.52\n28.61\n10.43\n23.66\n31.92\n12.75\n26.17\n34.45\n14.81\n28.42\nRussian\n32.22\n13.64\n26.17\n22.11\n8.29\n18.62\n24.39\n10.00\n20.54\n28.20\n12.72\n23.91\nScottish Gaelic\n29.02\n10.99\n22.88\n26.98\n8.87\n21.57\n29.80\n10.64\n23.44\n31.74\n12.61\n25.65\nSerbian (Cyrillic)\n23.78\n7.98\n20.14\n20.30\n5.86\n16.74\n21.92\n6.98\n18.35\n27.51\n11.46\n23.49\nSerbian (Latin)\n21.64\n6.66\n18.23\n18.14\n4.75\n14.96\n21.79\n6.92\n18.14\n25.86\n10.17\n21.76\nSinhala\n27.29\n13.38\n23.47\n22.69\n10.02\n19.96\n25.24\n11.52\n21.98\n27.78\n13.20\n24.45\nSomali\n31.56\n11.58\n24.22\n27.85\n9.08\n21.10\n30.29\n10.69\n23.29\n31.64\n11.11\n24.28\nSpanish\n31.51\n11.88\n24.07\n26.82\n9.05\n20.47\n28.71\n10.56\n22.04\n32.20\n13.10\n24.88\nSwahili\n37.67\n17.85\n30.91\n31.79\n13.25\n25.67\n34.29\n15.22\n27.82\n37.29\n17.22\n30.96\nTamil\n24.33\n11.06\n22.07\n20.68\n8.67\n18.71\n24.08\n10.74\n21.71\n26.81\n12.23\n24.21\nTelugu\n19.86\n7.03\n17.61\n15.11\n4.69\n13.48\n17.98\n6.12\n16.10\n21.20\n7.77\n18.88\nThai\n37.40\n17.28\n28.88\n35.98\n21.39\n26.65\n38.11\n22.92\n28.26\n40.70\n25.23\n30.12\nTigrinya\n25.32\n8.02\n21.17\n22.27\n7.08\n18.61\n26.30\n8.90\n22.05\n28.53\n10.13\n24.05\nTurkish\n32.93\n15.57\n29.26\n25.52\n11.54\n22.83\n28.56\n13.62\n25.72\n31.33\n15.61\n28.20\nUkrainian\n23.99\n10.14\n20.92\n20.97\n8.16\n18.17\n23.34\n9.74\n20.29\n27.05\n12.16\n23.68\nUrdu\n39.56\n18.37\n32.84\n37.11\n15.97\n30.14\n39.90\n18.53\n32.75\n43.03\n21.40\n35.72\nUzbek\n16.83\n6.34\n15.41\n14.60\n5.36\n13.39\n17.26\n6.42\n15.49\n19.18\n7.80\n17.29\nVietnamese\n32.88\n16.22\n26.08\n31.58\n15.41\n25.02\n34.54\n17.63\n27.59\n38.17\n20.49\n30.98\nWelsh\n32.66\n11.60\n26.12\n29.96\n9.40\n23.96\n33.66\n12.26\n27.01\n36.49\n15.34\n29.79\nYoruba\n31.66\n11.66\n25.09\n25.87\n8.99\n20.27\n29.49\n10.50\n23.26\n32.20\n12.34\n25.84\nTable 5: Full results for XL-Sum.\n"
  },
  {
    "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
    "link": "https://arxiv.org/pdf/2305.11171.pdf",
    "upvote": "2",
    "text": "TrueTeacher: Learning Factual Consistency Evaluation\nwith Large Language Models\nZorik GekhmanT,G,\u2217\nJonathan HerzigG\nRoee AharoniG\nChen ElkindG\nIdan SzpektorG\nTTechnion - Israel Institute of Technology\nGGoogle Research\nzorik@campus.technion.ac.il\n{zorik|jherzig|roeeaharoni|chenel|szpektor}@google.com\nAbstract\nFactual consistency evaluation is often con-\nducted using Natural Language Inference (NLI)\nmodels, yet these models exhibit limited suc-\ncess in evaluating summaries. Previous work\nimproved such models with synthetic training\ndata.\nHowever, the data is typically based\non perturbed human-written summaries, which\noften differ in their characteristics from real\nmodel-generated summaries and have limited\ncoverage of possible factual errors. Alterna-\ntively, large language models (LLMs) have re-\ncently shown promising results in directly eval-\nuating generative tasks, but are too computa-\ntionally expensive for practical use. Motivated\nby these limitations, we introduce TrueTeacher,\na method for generating synthetic data by an-\nnotating diverse model-generated summaries\nusing a LLM. Unlike prior work, TrueTeacher\ndoes not rely on human-written summaries, and\nis multilingual by nature. Experiments on the\nTRUE benchmark show that a student model\ntrained using our data, substantially outper-\nforms both the state-of-the-art model with simi-\nlar capacity, and the LLM teacher. In a system-\natic study, we compare TrueTeacher to existing\nsynthetic data generation methods and demon-\nstrate its superiority and robustness to domain-\nshift. We also show that our method generalizes\nto multilingual scenarios. Lastly, we release our\nlarge-scale synthetic dataset (1.4M examples),\ngenerated using TrueTeacher, and a checkpoint\ntrained on this data.1\n1\nIntroduction\nGenerative summarization models are prone to\ngenerate summaries that are factually inconsistent\nwith respect to the corresponding input documents\n(Goodrich et al., 2019; Kryscinski et al., 2019),\nlimiting their applicability in real-world scenarios.\n\u2217Work done during an internship at Google Research.\n1Our dataset and model are available at:\nhttps://github.com/google-research/\ngoogle-research/tree/master/true_teacher\nFigure 1: A real example from our data generation\nprocess. We fine-tune summarization models with dif-\nferent capacities, and use them to produce a diverse set\nof model-generated summaries of CNN/DM articles,\nwhich we label for consistency using a 540B LLM.\nSince factual consistency evaluation could be\ncast as a Natural Language Inference (NLI) task,\nNLI models are often used to evaluate consistency\n(Falke et al., 2019a; Maynez et al., 2020; Laban\net al., 2022). However, NLI models exhibit lim-\nited success in evaluating factual consistency in\nsummarization (Falke et al., 2019b; Kryscinski\net al., 2020), since NLI datasets lack the entail-\nment phenomena that naturally arise in abstrac-\ntive summarization (Khot et al., 2018). For ex-\nample, single-sentence premise-hypothesis pairs\nare shorter than document-summary pairs (Mishra\net al., 2021; Schuster et al., 2022).\nTo address this domain mismatch, previous work\nproposed various approaches for generating syn-\nthetic training data (Kryscinski et al., 2020; Yin\net al., 2021; Utama et al., 2022; Balachandran et al.,\n2022). The data is typically generated by perturb-\narXiv:2305.11171v3  [cs.CL]  18 Oct 2023\ning human-written summaries to introduce factual\ninconsistencies. While these perturbations are ef-\nfective, they are limited to factual error categories\nthat can be covered by the perturbation logic. In\naddition, since simulating factual errors is chal-\nlenging, such perturbations may fail to introduce\nfactual errors, leading to incorrect labels.2 Finally,\nsince the synthetic summaries are based on human-\nwritten summaries, they may differ in style from\nreal model-generated summaries, which can reduce\nthe effectiveness of the synthetic data.\nAn alternative approach to augmenting NLI mod-\nels with synthetic data, is to directly prompt large\nlanguage models (LLMs) to evaluate factual consis-\ntency. Recently, there has been a growing evidence\nfor the effectiveness of LLMs in evaluating gener-\native tasks (Kocmi and Federmann, 2023; Wang\net al., 2023; Liu et al., 2023), including factual\nconsistency in summarization (Chen et al., 2023).\nHowever, LLMs are still too computationally ex-\npensive to be heavily used in practice.\nTo make the best of both worlds we propose\nTrueTeacher, a simple and effective synthetic data\ngeneration method that leverages model-generated\nsummaries and the reasoning abilities of LLMs\n(Huang and Chang, 2022). In TrueTeacher, we first\ntrain a diverse collection of summarization models\nwith different capacities. Next, we use these mod-\nels to summarize each document in a given corpus\n(Figure 1). The resulting document-summary pairs\nare then annotated by prompting a LLM to predict\nthe corresponding factual consistency label.\nWe apply TrueTeacher using FLAN-PaLM 540B\n(Chung et al., 2022) to generate a large-scale syn-\nthetic dataset, which is used to train a student\nmodel. Experiments on the summarization sub-\nset of the TRUE benchmark (Honovich et al.,\n2022) show that augmenting existing NLI data\nwith TrueTeacher data improves a state-of-the-art\nmodel\u2019s ROC-AUC from 82.7 to 87.8, while main-\ntaining similar model capacity. The resulting model\neven outperforms its LLM teacher, despite the latter\nhaving a \u00d750 larger capacity.\nWe also compare TrueTeacher to existing syn-\nthetic data generation methods. To this end, we\ndesign a systematic study to re-evaluate existing\nmethods with a \"fair comparison\" in a challeng-\ning setting. Our results indicate that existing ap-\nproaches fail to generalize to documents derived\nfrom a distribution different from the one used for\n2As we also demonstrate in \u00a74.3.\nsynthetic data generation. In contrast, TrueTeacher\ndemonstrates robustness by successfully generaliz-\ning to documents from new domains.\nFinally, we apply TrueTeacher to generate\nmultilingual synthetic data. While existing data\ngeneration methods are often limited to English\n(Utama et al., 2022; Balachandran et al., 2022),\nTrueTeacher can use a multilingual LLM. Results\non the mFACE dataset (Aharoni et al., 2022), show\nimprovements on 35 out of 45 languages when us-\ning our method. This demonstrates the usefulness\nof multilingual synthetic data and the effectiveness\nof TrueTeacher in generating such data.\nTo summarize, this work includes the following\ncontributions:\n\u2022 We introduce TrueTeacher, a synthetic data\ngeneration approach based on annotating\nmodel-generated summaries with LLMs, and\ndemonstrate its effectiveness and robustness.\n\u2022 We evaluate FLAN-PaLM 540B on the task of\nfactual consistency evaluation and show that\nits knowledge can be distilled into a signifi-\ncantly smaller model using our method.\n\u2022 We conduct a systematic study, re-evaluating\nexisting synthetic data generation methods for\nthe task in an apples-to-apples comparison\nand identify their limitations.\n\u2022 We perform the first experiment in generating\nmultilingual synthetic data for factual consis-\ntency, and demonstrate its usefulness.\n\u2022 We release a large-scale dataset comprised of\n1.4 million TrueTeacher examples, and verify\nits quality with human evaluation. We addi-\ntionally release a state-of-the-art consistency\nevaluation model trained on this data.1\n2\nTrueTeacher\nIn this section we describe TrueTeacher, our\napproach for generating synthetic examples for\nthe task of factual consistency evaluation in\nsummarization.\nOur main motivation is to\nuse factual inconsistencies that occur in real\nmodel-generated summaries, instead of relying on\nperturbed human-written summaries. To this end,\nwe generate a diverse set of summaries using gener-\native summarization models of different capacities,\nand leverage a LLM to label them for factual con-\nsistency. Some of the generated summaries are ex-\npected to contain factual errors, and we hypothesize\nFigure 2: Our data generation process. We train a col-\nlection of generative summarization models, use them\nto summarize documents and label the resulting sum-\nmaries for factual consistency using a LLM.\nthat a strong-performing LLM can generalize to the\ntask and label them with sufficient quality to be use-\nful for training. The usage of model-generated sum-\nmaries not only yields more realistic texts, but also\nallows to potentially include rare errors, which can\nbe harder to incorporate with perturbation logic.\nOur data generation process is illustrated in\nFigure 2.\nFirst, we train a variety of summa-\nrization models (upper diagram). We use a col-\nlection of one or more summarization training\nsets T = {sd1, sd2, . . . , sdn} and different pre-\ntrained LMs = {lm1, lm2, . . . , lmm} to fine-\ntune a collection of summarization models SM =\n{sm1, sm2, . . . , smk}, where k = n \u00d7 m.3 Using\ndifferent pretrained LMs allows to diversify the\nexpected consistency errors, e.g., errors made by\nlarge or small models. The choice of summariza-\ntion training sets allows to control for the nature of\nthe resulting summaries, e.g., focusing on abstra-\ntive training sets to increase output abstractiveness.\nNext, we obtain model-generated summaries and\nannotate them (lower diagram). We choose a docu-\nments corpus D = {d1, d2, . . . , dr} and use all the\nsummarization models in SM to summarize all the\ndocuments in D, resulting in a collection of model-\ngenerated output summaries O = {s1,1, . . . sr,k},\nwhere si,j is the summary of document di gener-\nated by summarization model smj. TrueTeacher\n3We note that the pretrained LMs here refer to the mod-\nels that we are fine tuning for summarization, and they are\ndifferent from the LLM that we use as the teacher.\ndoes not require gold summaries, which allows it\nto be used with any collection of documents D, and\nmakes it more scalable than previous methods (Yin\net al., 2021; Utama et al., 2022; Balachandran et al.,\n2022).\nFinally, a LLM is prompted to label all\nthe\nsummaries\nin\nO\nfor\nconsistency\nw.r.t.\ntheir source documents, resulting with labels\n{l1,1, . . . , l1,k, . . . lr,k}.4 Figure 1 illustrates a real\nexample of this process for a single document\ndi \u2208 D. Each document, summary, and label\n(di, si,j, li,j) are then used as a synthetic example\nfor training a factual consistency classifier. Since\nwe leverage LLMs for labeling, our approach is\nlikely to benefit from the ongoing progress in\nLLMs quality. Furthermore, previous approaches\noften rely on language-specific components (e.g.,\nInformation Extraction), which limits their appli-\ncability in multiple languages. Since recent LLMs\nare pretrained on multilingual data, our method can\nbe easily applied to non-English languages, as we\nshow in \u00a75.\n3\nExperimental Setup\nWe use TrueTeacher to generate a synthetic dataset\nfor factual consistency evaluation in summariza-\ntion (\u00a73.1), and experiment with it to evaluate the\neffectiveness and usefulness of our method (\u00a74).\n3.1\nTrueTeacher Instantiation\nTo apply TrueTeacher, we instantiate the summa-\nrization datasets T, the pre-trained LMs and the\ndocuments corpus D. We use XSum (Narayan\net al., 2018) as T, T5 pre-trained models (Raf-\nfel et al., 2020) as LMs = {T5-small, T5-base,\nT5-large, T5-3B, T5-11B}, and documents from\nCNN/DailyMail (Hermann et al., 2015) as D.\nAs our teacher model, we employ FLAN-PaLM\n540B (Chung et al., 2022). This model was instruc-\ntion fine-tuned, including training on the closely-\nrelated NLI task.5 Therefore, we expect it to gen-\neralize well to factual consistency evaluation.6 We\nuse zero-shot prompting for simplicity, and since\napplying few-shot or chain-of-thought prompting\ndid not improve performance in early experiments.7\n4See \u00a73.1 and \u00a7A.1 for our prompting implementation.\n5https://github.com/google-research/FLAN/blob/\ne9e4ec6e2701182c7a91af176f705310da541277/flan/\ntask_splits.py#L109\n6We validate this expectation in \u00a74.1 and \u00a74.4.\n7In \u00a7A.1 we discuss potential reasons to this.\nSummaries Source\n# Consistent\n# Inconsistent\nT5-11B\n233,815\n39,423\nT5-3B\n229,097\n45,662\nT5-large\n195,681\n81,986\nT5-base\n161,177\n118,480\nT5-small\n88,129\n190,012\nTotal\n907,899\n475,563\nTable 1: Our generated dataset statistics.\nExtensive implementation details about our FLAN-\nPaLM usage are provided in \u00a7A.1 and \u00a7A.2.\nApplying TrueTeacher in this setup resulted\nin \u223c1.4M synthetic training examples (Table 1),\nwhich we use to train a student model for factual\nconsistency evaluation.8 In \u00a74, we provide evi-\ndence for the dataset\u2019s quality through human eval-\nuation (\u00a74.4), its usefulness for improving NLI\nmodels in a challenging setting (\u00a74.1), and its supe-\nriority over other existing synthetic datasets (\u00a74.2).\nIn early experiments, we also explored data fil-\ntering based on prompting FLAN-PaLM for self-\nverification (details in \u00a7A.5). This resulted in an\nincrease in the labeling accuracy. Yet, surprisingly,\ntraining the student model on the filtered data did\nnot improve performance in comparison to train-\ning on the full dataset.9 Thus, for simplicity, we\nconduct experiments using the full dataset.\n3.2\nEvaluation\nTo compare between consistency evaluation mod-\nels, we use the TRUE benchmark (Honovich\net al., 2022), focusing on its summarization subset:\nMNBM (Maynez et al., 2020), FRANK (Pagnoni\net al., 2021), SummEval (Fabbri et al., 2020),\nQAGS-X and QAGS-C (Wang et al., 2020). For\nadditional details about these datasets, we refer\nthe reader to Honovich et al. (2022). Following\nHonovich et al., we use ROC-AUC in a binary clas-\nsification setting as our evaluation metric.\n3.3\nBaselines\nWe compare the performance of factual consistency\nevaluation models trained on TrueTeacher data,\nagainst the top performing models on the TRUE\nbenchmark: QuestEval (Scialom et al., 2021), Q2\n(Honovich et al., 2021), SUMMACZS (Laban et al.,\n2022), T5-11B fine tuned on ANLI (Honovich\n8Implementation details for our trained models are in \u00a7A.3.\n9This could be attributed to the high-quality of the initial\nlabels and the student model\u2019s robustness to noise.\net al., 2022), WeCheck (Wu et al., 2023), and the\nEnsemble from Honovich et al. (2022).10\nWe also compare TrueTeacher data generation\nmechanism to existing methods for synthetic data\ngeneration. We consider the following approaches:\nDocNLI\n(Yin et al., 2021). Reformatted NLI,\nquestion answering and summarization datasets, in-\ncluding the CNN/DM corpus. The summarization-\nbased positive examples are based on concatenated\ngold summaries. The negative examples are then\ngenerated using word/entity replacements.\nFactCC\n(Kryscinski et al., 2020).\nThe docu-\nments are from CNN/DM. The consistent sum-\nmaries are randomly sampled sentences from the\ndocument, which are optionally injected with noise\nor paraphrased. The inconsistent summaries are ob-\ntained by rule-based transformations, such as sen-\ntence negation and entity/pronoun/number swaps.\nFactEdit\n(Balachandran et al., 2022). The posi-\ntive examples are based on gold summaries from\nCNN/DM. For the negative examples, an infilling\nmodel is trained using sentences from the docu-\nments, employing the OpenIE framework (Banko\net al., 2007) to mask predicates and arguments.\nEach predicate and argument phrase in the sum-\nmary is then iterativelly masked and infilled with\nthe model\u2019s lower order beam candidates.\nFalsesum\n(Utama et al., 2022).\nThe positive\nexamples are based on gold summaries from\nCNN/DM. For the negative examples, predicates\nand arguments are detected in the document and\nthe summary using the OpenIE (Banko et al., 2007)\nframework. Randomly selected predicates and ar-\nguments from the summary are then masked and\ninfilled using predicates and arguments from the\ndocument, or by \"hallucinating\" new content. For\nthis purpose a dedicated infilling model is trained.\n4\nExperiments and Analysis\nOur main experiments are in \u00a74.1 and \u00a74.2,\nfollowed by various analyses and ablations in \u00a74.3,\n\u00a74.4, \u00a74.5 and \u00a74.6. We design our experiments to\naddress the following research questions (RQs):\n\u2022 RQ1: What is the performance of FLAN-PaLM\n540B in factual consistency evaluation in sum-\nmarization? Is it a good choice for a teacher?\n10We discuss WeCheck in \u00a76, and refer the reader to Hon-\novich et al. (2022) for a detailed description of other baselines.\nMNBM\nQAGS-X\nFRANK\nSummEval\nQAGS-C\nAverage\nQuestEval (Scialom et al., 2021)\n65.3\n56.3\n84.0\n70.1\n64.2\n68.0\nQ2 (Honovich et al., 2021)\n68.7\n70.9\n87.8\n78.8\n83.5\n77.9\nSUMMACZS (Laban et al., 2022)\n71.3\n78.1\n89.1\n81.7\n80.9\n80.2\nT5-11B w. ANLI (Honovich et al., 2022)\n77.9\n83.8\n82.1\n80.5\n89.4\n82.7\nWeCheck (Wu et al., 2023)\n83.0\n81.4\n88.1\n79.8\n82.6\n83.0\nEnsemble (Honovich et al., 2022)\n76.6\n85.8\n91.2\n82.9\n87.7\n84.8\nFLAN-PaLM 540B (Chung et al., 2022)\n76.0\n88.1\n91.4\n83.7\n85.2\n84.9\nT5-11B w. ANLI + TrueTeacher full\n78.1\n89.4\n93.6\n88.5\n89.4\n87.8\nTable 2: ROC-AUC results on the summarization subset of the TRUE benchmark (Honovich et al., 2022).\n\u2022 RQ2: Can TrueTeacher facilitate training of a\ncompetitive model w.r.t. state-of-the-art models?\n\u2022 RQ3: What is the quality of the data gener-\nated using TrueTeacher compared to existing syn-\nthetic data generation methods?\nWe address RQ1 and RQ2 in \u00a74.1. To address\nRQ1, we evaluate FLAN-PaLM 540B against com-\npetitive models for factual consistency evaluation.\nTo address RQ2, we use our full dataset from \u00a73.1\nto train our best-performing model, and evaluate\nit in the exact same setting. Finally, RQ3 is ad-\ndressed in \u00a74.2, where we conduct a systematic\nstudy, comparing existing methods to TrueTeacher,\nwhile controlling for factors such as the synthetic\ndata size and the documents used for data synthesis.\n4.1\nMain Results on the TRUE Benchmark\nWe address RQ1 by evaluating FLAN-PaLM 540B\non the task and present the results in Table 2.\nFLAN-PaLM 540B achieves an impressive perfor-\nmance, with an average ROC-AUC of 84.9 com-\npared to 83.0 of the best single-model baseline, and\nperforms on-par with the Ensemble. This demon-\nstrates the chosen LLM\u2019s capability for the task,\nand its potential as a teacher for smaller models.\nTo address RQ2, we fine-tune T5-11B (Raffel\net al., 2020) over our full dataset (\u00a73.1) mixed\nwith ANLI (Nie et al., 2020).\nTable 2 shows\nthat including TrueTeacher data in the training\nset, substantially improves the strong-performing\nT5-11B w. ANLI baseline from an average ROC-\nAUC of 82.7 to 87.8 (+5.1), while maintaining\nexactly the same model capacity. This strong result\ndemonstrates the high effectiveness of TrueTeacher\nin a challenging setup. Notably, our model sets\nthe new state-of-the-art result on the benchmark,\noutperforming the \u00d750 times larger LLM that we\nused as the teacher (84.9 \u2192 87.8). This can be\nattributed to large-scale knowledge distillation on\na specific task, while the LLM is trained to per-\nform many tasks. Additionally, the smaller model\nis trained on target-domain data (documents and\nmodel-generated summaries) which can further im-\nprove performance (Gururangan et al., 2020).\n4.2\nRe-evaluating Synthetic Data Generation\nMethods \u2013 A Study\nPrevious studies on synthetic data generation have\nused different experimental setups, making it dif-\nficult to compare their results. In this section, we\ndesign a systematic study to re-evaluate existing\nmethods in a standardized setup. We first discuss\nour study design choices followed by the results.\nPrevious work has demonstrated that synthetic\ndata can improve NLI-based models. However,\nthey typically used relatively small-capacity mod-\nels, whereas Honovich et al. (2022) recently demon-\nstrated significant performance gains by scaling up\nto T5-11B fine-tuned on ANLI. We therefore adopt\nthis competitive baseline, to which we add syn-\nthetic data from each method. For ablation, we\ninclude variants trained solely on synthetic data\n(without ANLI), and also repeat our study using\nthe smaller-capacity T5-base model.\nTo preform a fair comparison, we restrict the\nnumber of examples from each evaluated method\nto 100k, randomly sampled with balanced labels.\nTo evaluate domain-shift robustness, we fur-\nther restrict the synthetic training examples to ones\nthat were generated only based on CNN/DM docu-\nments,11 and then consider the XSum-based evalu-\nation sets as out-of-domain.12\n11Some methods are based exclusively on CNN/DM while\nothers use additional datasets, more details in \u00a73.3.\n12SummEval and QAGS-C are based on documents from\nCNN/DM, MNBM and QAGS-X use documents from XSum,\nand FRANK has documents from both CNN/DM and XSum.\nWe split FRANK to FRANK-C and FRANK-X which contain\nits CNN/DN based and XSum based subsets respectively.\nTraining data\nCNN/DM-based\nXSUM-based\nAverage scores\nQAGS-C\nSummEval\nFRANK-C\nFRANK\nFRANK-X\nQAGS-X\nMNBM\nIn-domain\nOut-of-domain\nTRUE\nT5-11B\nANLI\n83.4\n74.2\n85.6\n90.7\n93.2\n88.0\n73.9\n81.1\n85.0\n82.0\nFactEdit\n87.8\n77.0\n77.2\n83.7\n76.0\n69.4\n53.1\n80.7 (-0.4)\n66.2 (-18.8)\n74.2 (-7.8)\nFactEdit + ANLI\n88.9\n78.9\n81.1\n88.0\n86.1\n76.2\n59.8\n83.0 (+1.9)\n74.0 (-11.0)\n78.4 (-1.6)\nDocNLI\n89.1\n72.9\n83.0\n89.2\n92.4\n83.8\n67.0\n81.7 (+0.6)\n81.1 (-3.9)\n80.4 (-1.6)\nDocNLI + ANLI\n87.8\n72.0\n81.9\n88.2\n93.7\n84.2\n68.0\n80.6 (-0.5)\n82.0 (-3.0)\n80.0 (-2.0)\nFactCC\n83.1\n79.0\n81.6\n84.1\n67.5\n72.7\n55.0\n81.2 (+0.1)\n65.1 (-19.9)\n74.8 (-7.2)\nFactCC + ANLI\n84.7\n83.3\n84.7\n89.5\n89.6\n82.9\n71.5\n84.2 (+3.1)\n81.3 (-3.7)\n82.4 (+0.4)\nFalsesum\n90.3\n85.4\n85.8\n89.8\n84.5\n70.8\n53.9\n87.2 (+6.1)\n69.7 (-15.3)\n78.0 (-4.0)\nFalsesum + ANLI\n90.7\n85.8\n87.0\n91.6\n90.5\n75.2\n60.5\n87.8 (+6.7)\n75.4 (-9.6)\n80.8 (-1.2)\nTrueTeacher\n84.9\n85.0\n88.8\n93.6\n94.4\n86.5\n76.1\n86.2 (+5.1)\n85.7 (+0.7)\n85.2 (+3.2)\nTrueTeacher + ANLI\n88.4\n85.8\n89.6\n93.9\n93.9\n87.8\n76.3\n87.9 (+6.8)\n86.0 (+1.0)\n86.4 (+6.4)\nT5-base\nANLI\n74.9\n63.7\n73.1\n81.3\n80.6\n77.2\n77.0\n70.6\n78.3\n74.8\nFactEdit\n61.4\n59.4\n59.4\n73.6\n51.9\n48.0\n58.4\n60.1 (-10.5)\n52.8 (-25.5)\n60.2 (-14.6)\nFactEdit + ANLI\n68.7\n60.0\n62.2\n78.5\n73.6\n72.2\n75.5\n63.6 (-7.0)\n73.8 (-4.5)\n71.0 (-3.8)\nDocNLI\n71.4\n66.5\n66.7\n77.9\n81.0\n75.2\n71.6\n68.2 (-2.4)\n75.9 (-2.4)\n72.5 (-2.3)\nDocNLI + ANLI\n75.2\n66.7\n74.4\n84.9\n83.3\n78.7\n74.8\n72.1 (+1.5)\n78.9 (+0.6)\n76.1 (+1.3)\nFactCC\n74.0\n72.7\n78.7\n83.2\n71.9\n71.0\n62.7\n75.3 (+4.7)\n68.5 (-9.8)\n72.7 (-2.1)\nFactCC + ANLI\n72.8\n73.2\n78.8\n83.2\n66.8\n71.5\n63.2\n74.9 (+4.3)\n67.2 (-11.1)\n72.8 (-2.0)\nFalsesum\n80.9\n74.2\n82.0\n86.4\n71.6\n65.0\n53.1\n79.0 (+8.4)\n63.2 (-15.1)\n71.9 (-2.9)\nFalsesum + ANLI\n82.9\n73.4\n83.3\n86.5\n72.6\n66.0\n58.7\n79.9 (+9.3)\n65.8 (-12.5)\n73.5 (-1.3)\nTrueTeacher\n77.3\n73.6\n79.1\n88.0\n82.6\n79.9\n78.3\n76.7 (+6.1)\n80.3 (+2.0)\n79.4 (+4.6)\nTrueTeacher + ANLI\n81.9\n78.0\n81.4\n89.3\n86.4\n81.9\n78.5\n80.4 (+9.8)\n82.3 (+4.0)\n81.9 (+7.1)\nTable 3: ROC-AUC results on TRUE comparing different synthetic data generation methods. For each model size,\naverage scores are compared to the corresponding ANLI-only baseline (difference is listed in parentheses).\nTable 3 presents the results of our study. We cal-\nculate three average scores: for in-domain test sets\nbased on CNN/DM documents, for out-of-domain\ntest sets based on XSum documents, and for the\noriginal datasets from TRUE.\nIn-Domain Results\nMost methods outperform\nthe corresponding ANLI-only baseline, demonstrat-\ning the usefulness of synthetic data. Predictably, all\nmethods improve with larger models and a comple-\nmentary effect is often observed when mixing syn-\nthetic data with ANLI. The best results are obtained\nby mixing ANLI with Falsesum or TrueTeacher\ndata and using T5-11B, with a substantial improve-\nment over the corresponding ANLI-only baseline\n(in-domain score increase from 81.1 to 87.9).\nOut-of-domain Results\nWhile most methods\nperform well in-domain, their performance drops\nsignificantly on the out-of-domain test sets. Most\nof the evaluated methods underperform the corre-\nsponding ANLI-only baseline with similar model\ncapacity. For some methods, performance dete-\nriorates dramatically; e.g.\nFalsesum \u2013 despite\nits impressive in-domain performance, its out-of-\ndomain score falls significantly below the ANLI-\nonly baseline. This suggests that some methods\noverfit to documents from the distribution used to\ngenerate the synthetic data. Based on this find-\ning, we encourage future research to prioritize out-\nof-domain evaluation. Interestingly, even though\nTrueTeacher\u2019s relative improvement is smaller com-\npared to the in-domain setup, it is still the only\nmethod with higher out-of-domain score compared\nto the corresponding ANLI-only baseline. This\ndemonstrates the robustness of TrueTeacher to do-\nmain shift, which may be due to the use of model-\ngenerated summaries that increase the variability\nof the resulting synthetic data.\nOverall Results on TRUE\nDue to the poor out-\nof-domain performance of the existing methods,\nTrueTeacher is the only method that consistently\noutperforms the ANLI-only baseline on the TRUE\nbenchmark. Notably, TrueTeacher + ANLI with T5-\nbase (81.9) performs on par with the ANLI-only\nbaseline using T5-11B (82.0). Additionally, the\nTrueTeacher-based variant using T5-11B (85.2) al-\nready performs on-par with the 540B LLM teacher\n(84.9, Table 2), even though we used only 100k syn-\nthetic examples in this experiment, and did not use\nANLI data. When comparing TrueTeacher + ANLI\nwith T5-11B and 100k examples (Table 3) to the\nequivalent variant using the full dataset (Table 2),\nwe observe a performance increase (86.4 \u2192 87.8),\nwhich demonstrates TrueTeacher\u2019s scalability. We\nconclude that TrueTeacher yields high quality data\nand generalizes well for new domains, which we at-\ntribute to the usage of model-generated summaries.\n4.3\nQualitative Analysis\nFigure 3 presents a case study with a randomly sam-\npled document, and the corresponding inconsistent\nsummaries generated with each of the evaluated\nCNN/DailyMail ID: 372f7e02e5bb17bac3a1b2260c6ac78414f97ee3\nArticle:  LOS ANGELES, California (CNN) -- Los Angeles firefighters and \ncity crews worked for several hours Tuesday to rescue one of their own: a \n22-ton firetruck that was nearly swallowed by a water-logged sinkhole. \nTwo firefighters crawled out of the truck's windows after it sank Tuesday \nmorning. No one was injured. The incident happened after four firefighters \ntook the truck to the San Fernando Valley neighborhood of Valley Village, \nwhere flooding had been reported\u2026 \u2026 \nGold Summaries: \n1. Los Angeles firetruck nearly swallowed by sinkhole Tuesday morning.\n2. Firefighters in truck were responding to flooding call when incident            \n    happened.\n3. Two firefighters escaped truck through windows; no injuries reported.\nFactEdit\nFirefighters in truck were responding rescue when \nincident happened .\nDocNLI\nLos Angeles firetruck nearly destroyed by sinkhole \nTuesday night . Firefighters in truck were responding to \nemergency call when it happened . Two firefighters \nescaped truck through windows ; no injuries reported .\nFactCC\nLOS LOS ANGELES, California ((CNN) - Los Angeles \nfirefighters and crews worked Two on Tuesday to rescue \none of their ownown: a 22-ton fire engine nearly \nswallowed by a sinkhole filled with waterwater.\nFalsesum\nLos Angeles firetruck nearly swallowed by water.\nTrueTeacher\nA firefighter has rescued a truck that sank in Los Angeles, \ncausing extensive flooding.\nFigure 3: A case study comparing factually inconsistent\nsummaries of the same document generated using dif-\nferent methods. Content replacements are highlighted\nusing the same color for the original and the replaced\ntext. Added content is in bold red font.\nmethods. FactEdit used the second gold-summary\nand replaced \"to flooding call\" with \"rescue\", in-\ntroducing a grammatical error rather than a factual\nerror, demonstrating the potential problems with\nusing lower-beam completions as proxy for factual\nerrors. DocNLI uses all the gold summaries con-\ncatenated. While replacing \"morning\" with \"night\"\nintroduces a factual error, three other edits fail to\nintroduce factual errors, demonstrating the limi-\ntations of using simple word/entity replacements.\nFactCC used the first sentence from the article and\nsuccessfully introduced factual error by an entity\nswap from \"firetruck\" to \"fire engine\". The para-\nphrase highlighted in green increases the abstrac-\ntiveness, but the paraphrase in orange introduces\na grammatical error that is less likely to be made\nby a strong summarization model. The noise in-\njection used by FactCC (duplicating or removing\nrandom tokens) is colored in red, but its useful-\nness is questionable. Falsesum uses the first gold\nsummary, and its perturbation model predicts the\nremoval of \"Tuesday morning\" and the replacement\nof the \"sinkhole\" argument with \"water\", failing\nto introduce a factual error, since the sinkhole is\nreferred to as \"water-logged sinkhole\" in the ar-\nticle. Finally, TrueTeacher uses an abstractive\nsummary generated by a real summarization model.\nClass\n#Ex.\nPrecision\nRecall\nF1\nConsistent\n41\n80.0\n97.6\n87.9\nInconsistent\n59\n98.0\n83.1\n89.9\nTable 4: Human evaluation results.\nIt introduces a nuanced factual error by replacing\n\"Los Angeles firefighters\" with A firefighter and\nalso by hallucinating new content (the text in bold\nred font). This case study further illustrates the\nchallenges of perturbing texts to introduce factual\ninconsistencies and re-iterates the importance in\nusing model-generated summaries.\n4.4\nHuman Evaluation\nTo further assess the quality of the synthetic data\nproduced by TrueTeacher, we perform human eval-\nuation carried out by domain experts.13 We evalu-\nate 100 examples from our dataset,14 using binary\njudgements based on the attribution definition from\nRashkin et al. (2021). The labeling accuracy of\nthe sampled examples from our data stands at 89%,\nwhich demonstrates its high quality. Table 4 further\npresents the precision, recall and F1 scores for the\nconsistent and inconsistent classes. More details\non the human evaluation are available in \u00a7A.8.\n4.5\nAblating Summary Distribution and\nLabel Correctness\nThere are two key differences between TrueTeacher\nand perturbation-based synthetic data generation\nmethods: (1) the distribution of the summaries15\nand (2) the correctness of the generated labels.16\nEach of these differences may lead to the better\nquality of TrueTeacher w.r.t the baselines. To mea-\nsure the impact of each difference, we isolate them\nin a controlled ablation study. We create 2 ab-\nlated variants, using Falsesum as a recent baseline\nmethod for synthetic data generation. The results\nare presented in Table 5.\nLabelAblation is an ablation created by label-\ning the document-summary pairs from Falsesum\u2019s\ndata using FLAN-PaLM 540B.17 Comparing\n1310 NLP researchers, each with at least one year of experi-\nence in factual consistency evaluation.\n14We randomly sampled 50 positively and 50 negatively\nlabeled examples from our synthetic dataset.\n15Model-generated vs. human-written perturbed.\n16Both methods may yield wrong labels. Perturbations\nmight not introduce inconsistencies, as seen in \u00a74.3, while\nTrueTeacher can have errors due to LLM mislabeling.\n17We used the same 100k examples as Falsesum + ANLI\nbaseline, and the same LLM prompt as in TrueTeacher.\nVariant\nSummary Distribution\nLabeling Quality\nT5-11B\nT5-Base\nFalsesum + ANLI\nHuman-written perturbed\nFalsesum\n80.8\n73.5\nTrueTeacher + ANLI\nModel-generated\nFLAN-PaLM 540B\n86.4 (+6.9%)\n81.9 (+11.4%)\nLabelAblation\nHuman-written perturbed\nFLAN-PaLM 540B\n85.3 (+5.6%)\n78.9 (+7.3%)\nSummaryAblation\nModel-generated\nFalsesum (proxy)\n85.5 (+5.8%)\n79.1 (+7.6%)\nTable 5: Average ROC-AUC on TRUE for the ablated variants. Falsesum + ANLI and TrueTeacher + ANLI are\ncopied from Table 3 for reference.\nLabelAblation to Falsesum + ANLI allows us\nto examine the effect of using FLAN-PaLM\nlabels instead of the original Falsesum labels,\nwhile controlling for the summaries distribution.\nLabelAblation outperforms Falsesum + ANLI\nby 5.6%, which shows that performance gains can\nbe obtained using summaries generated with exist-\ning synthetic data generation methods combined\nwith second-stage improved labeling quality. How-\never, TrueTeacher is substantially simpler and also\nresults in better performance.\nSummaryAblation is an ablation created by flip-\nping labels on a random portion of TrueTeacher\u2019s\ndata, such that the expected labeling accuracy is\nsimilar to Falsesum (More details in \u00a7A.9). Com-\nparing SummaryAblation to Falsesum + ANLI al-\nlows us to examine the effect of changing the sum-\nmary distribution from human-written perturbed\nto model-generated, while controlling for the la-\nbeling quality.\nSummaryAblation outperforms\nFalsesum + ANLI by 5.8%, a similar improve-\nment as observed for LabelAblation (5.6%). This\ndemonstrates that label correctness and summary\ndistribution have a similar effect on the perfor-\nmance, but they also have a complimentary effect\nas the best performance of 86.4 ROC-AUC is ob-\ntained only when they are combined together.\n4.6\nAbstractiveness Analysis\nAdvances in large scale pretraining (Devlin et al.,\n2019; Lewis et al., 2020) and the availability of rel-\nevant datasets (Narayan et al., 2018), enabled rapid\nprogress in abstractive summarization, which bet-\nter imitates the way humans summarize (Koh et al.,\n2023) and is also preferred by humans (Goyal et al.,\n2022). This motivates us to focus on generating\nabstractive synthetic summaries.\nWe compare the abstractiveness degree of differ-\nent methods using the extractive fragment coverage\nand density measures from Grusky et al. (2018).\nFollowing Utama et al. (2022) we multiply these\nCoverage \u2193\nDensity \u2193\nCombined \u2193\nFactEdit\n0.86\n2.92\n2.67\nDocNLI\n0.85\n15.66\n15.20\nFactCC\n0.93\n8.16\n7.93\nFalsesum\n0.88\n2.98\n2.76\nTrueTeacher\n0.86\n2.41\n2.15\nTable 6: Average abstractiveness scores (lower is better),\nmeasured on a random sample of 5k examples.\nmeasures to obtain a combined score.18 Table 6\npresents the abstractiveness scores, and a density\nplot is available in the Appendix (Figure 5). We ob-\nserve higher abstractiveness for model-based meth-\nods (FactEdit, Falsesum and TrueTeacher), suggest-\ning that rule-based methods might be less useful\nwith the recent shift towards abstractive summariza-\ntion. TrueTeacher produces the most abstractive\nsummaries with lowest combined score.\n5\nMulti-Lingual Data Generation for\nFactual Consistency Evaluation\nUtilizing a multilingual LLM enables a straightfor-\nward application of TrueTeacher to multiple lan-\nguages. This contrasts with recent approaches that\nrely on NLP components only available for high-\nresource languages, e.g., information extraction\n(Utama et al., 2022; Balachandran et al., 2022). In\nthis section, we examine TrueTeacher\u2019s usefulness\nfor multilingual factual consistency evaluation.\nWe first generate multilingual synthetic data us-\ning TrueTeacher. This time we train a single sum-\nmarization model by fine tuning mT5-XXL (Xue\net al., 2021) on XLSum (Hasan et al., 2021) and\nuse it to summarize documents from WikiLingua\n(Ladhak et al., 2020), which we then label for con-\nsistency with our LLM. For the purposes of this\nexperiment we focus on a subset of WikiLingua\ndocuments in 4 languages: English (en), French\n18We provide additional technical details in \u00a7A.6.\nTraining data\n# Improved\nAvg. ROC-AUC\nlanguages\nPer lang.\nPer ex.\nANLI+XNLI\n-\n73.3\n71.6\n+TrueTeacher en\n32 / 45\n75.7\n73.8\n+TrueTeacher en,fe,es,ge\n35 / 45\n77.2\n75.3\nTable 7: Multilingual results on the mFACE test set.\n(fe), Spanish (es) and German (de).19. After gener-\nating the dataset for these 4 languages, we sample\n100k examples, by randomly sampling 25k in each\nlanguage with balanced labels (as illustrated in Ta-\nble 9 in the Appendix). For ablation, we also cre-\nate an English-only variant, by randomly sampling\n100k English examples with balanced labels.20\nWe use the resulted data to train multilingual con-\nsistency evaluation models and evaluate them on\nthe mFace test set (Aharoni et al., 2022), containing\n3150 examples in 45 languages. As a strong base-\nline we follow Aharoni et al. and fine-tune mT5-\nXXL (Xue et al., 2021) on the ANLI (Nie et al.,\n2020) and XNLI (Conneau et al., 2018) datasets.\nWe then assess whether adding our synthetic data\nto the training set can improve this model.\nTable 7 presents the results overview, full re-\nsults in all 45 languages are available in Table 10\n(Appendix). Adding English-only summarization-\nbased synthetic data, already improves results on\n32 out of 45 languages and increases the avg. ROC-\nAUC from 71.6 to 73.8.\nYet, using the same\namount of multi-lingual examples improved the\nperformance even more, with avg.\nROC AUC\nof 75.3. This demonstrates the added value in\ngenerating multi-lingual synthetic examples using\nTrueTeacher, laying the ground for future work.\n6\nRelated Work\nPrevious work proposed methods for generating\nsynthetic training data for factual consistency eval-\nuation, by perturbing gold summaries (Yin et al.,\n2021; Kryscinski et al., 2020; Balachandran et al.,\n2022; Utama et al., 2022; Soleimani et al., 2023).21\nA key advantage of TrueTeacher, is the ability to\nleverage real model-generated summaries, leading\nto superior performance and robustness. The utility\nof model-generated outputs was also highlighted\nby Wu et al. (2023), who proposed a weakly super-\n19They are the most prevalent languages in PaLM\u2019s pre-\ntraining data (Chowdhery et al., 2022)\n20Also based on WikiLingua, generated with the same pro-\ncess like the 25k English subset of our multilingual dataset.\n21We provide extensive review of these methods in \u00a73.3.\nvised consistency evaluation model that leverages\nprobabilistic labels derived from aggregated scores\nof other consistency evaluation models. Our work\nproposes a simpler solution, that is also inherently\nmultilingual.\nAnother line of work for adapting NLI-based\nmodels for summarization, focuses on better pro-\ncessing of long texts, splitting the documents\ninto sentences to create shorter premise-hypothesis\npairs (Laban et al., 2022; Schuster et al., 2022).\nRecent work attempts to assess LLMs\u2019 capability\nfor evaluating generative tasks (Kocmi and Feder-\nmann, 2023; Wang et al., 2023; Liu et al., 2023).\nLuo et al. (2023) evaluated ChatGPT (OpenAI,\n2022) speciffically on the task of factual consis-\ntency evaluation in summarization. Yet, Aiyappa\net al. (2023) argued that ChatGPT\u2019s \"closed\" nature\nrisks data leakage (training-test contamination).22\nChen et al. (2023) performed a study of LLMs as\nfactual consistency evaluators, using a variety of\nprompting methods.\nPrevious work also attempted to distill knowl-\nedge from LLMs (West et al., 2022; Hsieh et al.,\n2023), as well as to leverage LLMs for data anno-\ntation (Wang et al., 2021; Ding et al., 2022), and\nsynthetic data generation (Agrawal et al., 2022;\nLiu et al., 2022; Bitton et al., 2023). As far as we\naware, our work is the first to leverage LLMs for\ndata generation for factual consistency evaluation.\n7\nConclusion\nWe introduced TrueTeacher, a simple and highly\neffective method for generating synthetic data for\nfactual consistency evaluation.\nInstead of per-\nturbation of human-written summaries like done\nin previous work, TrueTeacher leverages realistic\nmodel-generated summaries, which are annotated\nby prompting a large language model.\nUsing our method, we generate a large-scale\nsynthetic dataset, which we are making publicly\navailable. Our experimental results show that this\ndataset substantially enhances the performance of a\nstate-of-the-art model. In our systematic study, we\ncompare TrueTeacher to existing approaches and\nfurther demonstrate its effectiveness and robust-\nness. Our study highlights the importance of out-of-\ndomain evaluation, which we hope will be adopted\nin future work. Lastly, we show that TrueTeacher\ngeneralizes well to multilingual scenarios, present-\ning additional advantage over existing methods.\n22While FLAN\u2019s instruction fine-tuning data is public.\n8\nLimitations\nNoisy synthetic data\nTrueTeacher relies on a\nLLM for labeling model generated summaries.\nThis process may result in some frequency of noisy\nsynthetic examples for which the label is incor-\nrect. This can affect the overall quality of the stu-\ndent model that trains on this data. In our experi-\nments we validated the quality of our synthetic data\nwith human evaluation, however this should be re-\nexamined when generating data for new domains.\nIn addition, we experimented with different filter-\ning approaches, but found that training on filtered\ndata with higher labeling accuracy, did not improve\nthe performance of the student model. We encour-\nage future work to further examine such automatic\nfiltering.\nReliance on LLMs\nIn this work we use a 540B\nLLM to label 1.4M model generated summaries.\nThis requires non-negligible resources that may not\nbe available to the whole community. To mitigate\nthis, we release our collected synthetic data and\nthe corresponding model checkpoint. In addition,\nthe decreasing inference cost of proprietary LLMs,\nand the availability of open-source LLMs (Touvron\net al., 2023) can further assist.\nEffect of low-resource languages\nOur multilin-\ngual experiments (\u00a75) focus on a subset of WikiLin-\ngua documents in only 4 languages: English (en),\nFrench (fe), Spanish (es) and German (de), that\nare the most prevalent in our LLM\u2019s pre-training\ndata. As can be seen in our full results (Table 9 in\nthe Appendix), our multilingual data successfully\nimproves low-resource languages as well. We did\nnot fully explore the effect of adding additional\nlanguages to our synthetic data, especially low-\nresource ones. We believe that there is a trade-\noff between language coverage and labeling qual-\nity. i.e, while generating the synthetic data in low-\nresource languages will increase language cover-\nage, it can lead to poor labeling quality by our LLM.\nWe did not fully explore the exact sweet-spot for\nhow many languages to include in our synthetically\nlabeled training data, leaving this for future work.\nReferences\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\nMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\nDipanjan Das, and Mirella Lapata. 2022. Qameleon:\nMultilingual QA with only 5 examples.\nCoRR,\nabs/2211.08264.\nRoee Aharoni, Shashi Narayan, Joshua Maynez,\nJonathan Herzig, Elizabeth Clark, and Mirella Lapata.\n2022. mface: Multilingual summarization with fac-\ntual consistency evaluation. CoRR, abs/2212.10622.\nRachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-\nYeol Ahn. 2023.\nCan we trust the evaluation on\nchatgpt? CoRR, abs/2303.12767.\nVidhisha\nBalachandran,\nHannaneh\nHajishirzi,\nWilliam W. Cohen, and Yulia Tsvetkov. 2022.\nCorrecting diverse factual errors in abstractive\nsummarization via post-editing and language model\ninfilling. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022, pages 9818\u20139830. Association\nfor Computational Linguistics.\nMichele Banko, Michael J. Cafarella, Stephen Soder-\nland, Matthew Broadhead, and Oren Etzioni. 2007.\nOpen information extraction from the web. In IJCAI\n2007, Proceedings of the 20th International Joint\nConference on Artificial Intelligence, Hyderabad, In-\ndia, January 6-12, 2007, pages 2670\u20132676.\nYonatan Bitton, Shlomi Cohen-Ganor, Ido Hakimi,\nYoad Lewenberg, Roee Aharoni, and Enav Weinreb.\n2023. q2d: Turning questions into dialogs to teach\nmodels how to search.\nShiqi Chen, Siyang Gao, and Junxian He. 2023. Eval-\nuating factual consistency of summaries with large\nlanguage models. CoRR, abs/2305.14069.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, Brussels, Belgium, Octo-\nber 31 - November 4, 2018, pages 2475\u20132485. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171\u20134186. Association for Computational\nLinguistics.\nBosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,\nShafiq R. Joty, and Boyang Li. 2022. Is GPT-3 a\ngood data annotator? CoRR, abs/2212.10450.\nAlexander R Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2020. Summeval: Re-evaluating summariza-\ntion evaluation. arXiv preprint arXiv:2007.12626.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019a.\nRanking generated summaries by correctness: An\ninteresting but challenging application for natural\nlanguage inference. In Proceedings of the 57th Con-\nference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pages 2214\u20132220.\nAssociation for Computational Linguistics.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019b.\nRanking generated summaries by correctness: An\ninteresting but challenging application for natural\nlanguage inference. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2214\u20132220, Florence, Italy. Asso-\nciation for Computational Linguistics.\nBen Goodrich, Vinay Rao, Mohammad Saleh, and Pe-\nter J. Liu. 2019. Assessing the factual accuracy of\ngenerated text. CoRR, abs/1905.13322.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\nGPT-3. CoRR, abs/2209.12356.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018,\nNew Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers), pages 708\u2013719. Association\nfor Computational Linguistics.\nSuchin\nGururangan,\nAna\nMarasovi\u00b4c,\nSwabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don\u2019t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342\u20138360, Online. Association for Computational\nLinguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-\nBin Kang, M. Sohel Rahman, and Rifat Shahri-\nyar. 2021.\nXl-sum: Large-scale multilingual ab-\nstractive summarization for 44 languages. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-6,\n2021, volume ACL/IJCNLP 2021 of Findings of ACL,\npages 4693\u20134703. Association for Computational\nLinguistics.\nKarl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neu-\nral Information Processing Systems 2015, December\n7-12, 2015, Montreal, Quebec, Canada, pages 1693\u2013\n1701.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022, pages 3905\u20133920.\nAssociation for Computational Linguistics.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\n$q\u02c62$: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and ques-\ntion answering. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021, pages\n7856\u20137870. Association for Computational Linguis-\ntics.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay\nKrishna, Chen-Yu Lee, and Tomas Pfister. 2023. Dis-\ntilling step-by-step! outperforming larger language\nmodels with less training data and smaller model\nsizes. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 8003\u20138017,\nToronto, Canada. Association for Computational Lin-\nguistics.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\nCoRR, abs/2212.10403.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nficial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artificial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5189\u20135197. AAAI Press.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. CoRR, abs/2302.14520.\nHuan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan.\n2023. An empirical survey on long document sum-\nmarization: Datasets, models, and metrics. ACM\nComput. Surv., 55(8):154:1\u2013154:35.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation.\nIn Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 540\u2013551.\nAssociation for Computational Linguistics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 9332\u2013\n9346. Association for Computational Linguistics.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Trans. Assoc. Comput. Linguistics, 10:163\u2013\n177.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen R. McKeown. 2020. Wikilingua: A new bench-\nmark dataset for cross-lingual abstractive summariza-\ntion. CoRR, abs/2010.03093.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871\u20137880.\nAssociation for Computational Linguistics.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. WANLI: worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 6826\u20136847.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNLG evaluation using GPT-4 with better human\nalignment. CoRR, abs/2303.16634.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023.\nChatgpt as a factual inconsistency evalu-\nator for abstractive text summarization.\nCoRR,\nabs/2303.15621.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. CoRR, abs/2303.17651.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan T. McDonald. 2020. On faithfulness and fac-\ntuality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July\n5-10, 2020, pages 1906\u20131919. Association for Com-\nputational Linguistics.\nAnshuman Mishra, Dhruvesh Patel, Aparna Vijayaku-\nmar, Xiang Lorraine Li, Pavan Kapanipathi, and Kar-\ntik Talamadupula. 2021. Looking beyond sentence-\nlevel natural language inference for question answer-\ning and text summarization. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL-HLT 2021, On-\nline, June 6-11, 2021, pages 1322\u20131336. Association\nfor Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pages 1797\u20131807. Association\nfor Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 4885\u20134901.\nAssociation for Computational Linguistics.\nOpenAI.\n2022.\nChatgpt,\nhttps://openai.com/blog/chatgpt/.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 4812\u20134829. Association for Com-\nputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nMichael Collins, Dipanjan Das, Slav Petrov, Gau-\nrav Singh Tomar, Iulia Turc, and David Reitter. 2021.\nMeasuring attribution in natural language generation\nmodels. CoRR, abs/2112.12870.\nTal Schuster, Sihao Chen, Senaka Buthpitiya, Alex\nFabrikant, and Donald Metzler. 2022. Stretching\nsentence-pair NLI models to reason over long doc-\numents and clusters. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 394\u2013412. Association for Computational Lin-\nguistics.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. Questeval: Summariza-\ntion asks for fact-based evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 6594\u20136604. Association for\nComputational Linguistics.\nAmir Soleimani, Christof Monz, and Marcel Worring.\n2023. NonFactS: NonFactual summary generation\nfor factuality evaluation in document summarization.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 6405\u20136419, Toronto,\nCanada. Association for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPrasetya Utama, Joshua Bambrick, Nafise Sadat\nMoosavi, and Iryna Gurevych. 2022. Falsesum: Gen-\nerating document-level NLI examples for recogniz-\ning factual inconsistency in summarization. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL\n2022, Seattle, WA, United States, July 10-15, 2022,\npages 2763\u20132776. Association for Computational\nLinguistics.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 5008\u20135020. Association for Computa-\ntional Linguistics.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-\nang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\nZhou. 2023. Is chatgpt a good NLG evaluator? A\npreliminary study. CoRR, abs/2303.04048.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, Virtual Event / Punta Cana, Dominican Re-\npublic, 16-20 November, 2021, pages 4195\u20134205.\nAssociation for Computational Linguistics.\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nKang Liu, and Jun Zhao. 2023. Large language mod-\nels are better reasoners with self-verification. CoRR,\nabs/2212.09561.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022.\nSymbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602\u20134625, Seat-\ntle, United States. Association for Computational\nLinguistics.\nWenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian\nLi, and Yajuan Lv. 2023. Wecheck: Strong factual\nconsistency checker via weakly supervised learning.\nProceedings of the 61th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2023.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 483\u2013498. Association\nfor Computational Linguistics.\nWenpeng Yin, Dragomir R. Radev, and Caiming\nXiong. 2021.\nDocnli: A large-scale dataset for\ndocument-level natural language inference. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-6,\n2021, volume ACL/IJCNLP 2021 of Findings of ACL,\npages 4913\u20134922. Association for Computational\nLinguistics.\nA\nAppendix\nA.1\nFLAN-PaLM Prompt Design\nTo apply FLAN-PaLM for factual consistency eval-\nuation, we experimented with zero-shot, few-shot\nand chain-of-thought prompting strategies, and var-\nious formats for each strategy. We chose the best\nperforming strategy and format, based on the accu-\nracy on a development set.23 Table 8 presents the\naccuracy of each prompt type on the development\nset. We observed only minor performance differ-\nences, and thus we opted for the simplest solution\nthat is the zero-shot prompt. While we cannot know\nthe exact reasons for why few-shot and chain-of-\nthought did not improve performance, we can offer\npotential explanations. (1) Since the model was\nfine-tuned on NLI datasets, it is able to effectively\ngeneralize to factual consistency evaluation, mak-\ning further demonstrations via few-shot prompting\nunnecessary in this case. (2) The performance with\nthe zero-shot prompt is already notably high (89%,\n\u00a74.4) and thus our particular LLM is less likely\nto benefit from chain-of-thought prompting. (3) It\ncould be the case that only a few reasoning steps\nare needed to evaluate consistency in our particular\nsetup and thus chain-of-thought is not necessarily\nbetter in this case.\nBelow, we describe our top-performing zero-\nshot, few-shot and chain-of-thought prompts.\nZero-shot Prompt\nSince FLAN-PaLM was in-\nstruction fine-tuned on NLI, we designed our\nprompt to resemble an NLI prompt (e.g. using\n\"premise\" and \"hypothesis\" instead of \"document\"\nand \"summary\"). Our final prompt is as follows:\nPremise: {document} Hypothesis: {summary} Can the\nhypothesis be inferred from the premise? Answer using\n\"Yes\" or \"No\" only.\nFew-shot\nPrompt\nWe\nuse\ntwo\nfew-shot\nexamples,\none\n\"consistent\"\nand\none\n\"inconsistent\".\nWe randomly sample these\nexamples from the development set examples\nshorter than 200 words.23 We limit ourselves to\ntwo short examples since summarization examples\ncan include long documents, and thus few-shot\nmay lead to too long context length. Our final\nprompt is as follows:\n23For development set we use the FactCC dataset (Kryscin-\nski et al., 2020) with 1,431 examples containing summaries\nof documents from CNN/DailyMail, manually annotated for\nfactual correctness. Following (Utama et al., 2022), we merge\nthe dev and test sets.\nPremise: (CNN) Desperate migrants from Africa and\nthe Middle East keep heading to Europe, with 978 res-\ncued Friday in the Mediterranean Sea, the Italian Coast\nGuard said Saturday via Twitter. The migrants were\npicked up 30 miles off the coast of Libya, said European\nParliament member Matteo Salvini, the leader of Italy\u2019s\nfar-right Northern League. In the first three months of\n2015, Italy registered more than 10,000 migrants arriv-\ning, the International Organization for Migration said,\nand about 2,000 were rescued at sea during the first\nweekend of April in the Channel of Sicily. Most mi-\ngrants recorded this year come from countries in West\nAfrica as well as Somalia and Syria, the IMO said. They\nuse Libya as a country of transit. At least 480 migrants\nhave died while crossing the Mediterranean since the\nbeginning of the year, often because of bad weather and\novercrowded vessels used by smugglers, the IMO said.\nSometimes the captains and crews abandon the ships,\nleaving passengers to fend for themselves. At this time\nlast year, there were fewer than 50 deaths reported, the\nIMO said. Most of the migrants are asylum seekers, vic-\ntims of trafficking or violence, unaccompanied children\nand pregnant women.\nHypothesis: the migrants were picked up 30 miles off\nthe coast of libya.\nCan the hypothesis be inferred from the premise? An-\nswer using \"Yes\" or \"No\" only.\nAnswer: Yes\nPremise: (CNN) A nuclear submarine being repaired at\na Russian shipyard has caught on fire, according to a\nlaw enforcement source speaking to Russia\u2019s state-run\nnews agency ITAR-Tass. \"The submarine is in a dry\ndock,\" Tass reports, citing the source, and there is no\nammunition on board. \"The rubber insulation between\nthe submarine\u2019s light and pressure hull is on fire,\" Tass\nreported. Russia\u2019s RIA Novosti news agency says insu-\nlation caught on fire as welding work was being done\non the submarine. Tass reported that the fire began on a\nsub in the Zvyozdochka shipyard in northwestern Russia.\nZvyozdochka spokesman Yevgeny Gladyshev told the\nnews agency that the sub had been undergoing repairs\nsince November 2013. \"Nuclear fuel from the sub\u2019s re-\nactor has been unloaded,\" he reportedly said. \"There\nare no armaments or chemically active, dangerous sub-\nstances, fissionable materials on it,\" Gladyshev said to\nTass. \"The enterprise\u2019s personnel left the premises when\nthe submarine caught fire, no one has been injured. The\nfire presents no threat to people and the shipyard.\"\nHypothesis: \"the rubber insulation between the subma-\nrine\u2019s light and pressure hull is on fire,\" russia\u2019s ria\nnovosti news agency says.\nCan the hypothesis be inferred from the premise? An-\nswer using \"Yes\" or \"No\" only.\nAnswer: No\nPremise: {document}\nHypothesis: {summary}\nCan the hypothesis be inferred from the premise? An-\nswer using \"Yes\" or \"No\" only.\nAnswer:\nChain-of-thought Prompt\nFollowing Kojima\net al. (2022) we append \"Let\u2019s think step by\nstep\" to the prompt to facilitate a step-by-step rea-\nsoning before predicting the label. Our final prompt\nis as follows:\nPremise: {document}\nHypothesis: {summary}\nQ: Can the hypothesis be inferred from the premise?\nAnswer using \"Yes\" or \"No\" only.\nA: Let\u2019s think step by step\nThis prompt successfully unlocked a step by step\nreasoning. Below is FLAN-PaLM\u2019s response for-\nmat, where {answer} is either \"yes\" or \"no\".\n{reasoning steps}. So, the answer is {answer}.\nExample input:\nPremise: (CNN) Georgia Southern University was in\nmourning Thursday after five nursing students were\nkilled the day before in a multivehicle wreck near Sa-\nvannah. Caitlyn Baggett, Morgan Bass, Emily Clark,\nAbbie Deloach and Catherine (McKay) Pittman \u2013 all\njuniors \u2013 were killed in the Wednesday morning crash\nas they were traveling to a hospital in Savannah, ac-\ncording to the school website. Fellow nursing students\nBrittney McDaniel and Megan Richards were injured\nas was another person, who was not identified by the\nGeorgia State Patrol. The young women were on their\nway to finish their first set of clinical rotations ... ... ...\nHypothesis: georgia southern university was in mourn-\ning after five nursing students died.\nQ: Can the hypothesis be inferred from the premise?\nAnswer using \"Yes\" or \"No\" only.\nA: Let\u2019s think step by step\nThe output for this example is:\nGeorgia Southern University was in mourning Thursday\nafter five nursing students were killed the day before in\na multivehicle wreck near Savannah. So, the answer is\nyes.\nA.2\nInference with FLAN-PaLM\nWe used the zero-shot prompt (see \u00a7A.1). The vast\nmajority of FLAN-PaLM\u2019s responses were either\n\"Yes\" or \"No\", and a tiny fraction of the responses\nwere \"It\u2019s impossible to say\".\nDuring the labeling phase, we let FLAN-PaLM\ngenerate the output (predict mode), and label as\n\"consistent\" if the generated output is \"Yes\"\nand \"inconsistent\" in case the output is \"No\".\nWe discard the \"It\u2019s impossible to say\" ex-\namples. In order to measure ROC-AUC in a binary\nclassification setting, we compute the model\u2019s prob-\nability of generating \"Yes\" (score mode) and use\nit as the example-level factual consistency score.\nA.3\nFine tuning T5\nWe fine tune our T5 models for factual consistency\nevaluation using the following input format:\nPrompt type\nDev accuracy\nzero-shot\n93.6\nfew-shot\n93.2\nchain-of-thought\n93.8\nTable 8: FLAN-PaLM accuracy on the development\nset23 using different prompting strategies.\nLanguage\nISO 639-1\nconsistent\ninconsistent\nEnglish\nen\n12,500\n12,500\nSpanish\nes\n12,500\n12,500\nFrench\nfr\n12,500\n12,500\nGerman\nde\n12,500\n12,500\ntotal\n50,000\n50,000\nTable 9: Our multilingual dataset statistics.\nPremise: {document} Hypothesis: {summary}\nThe model is trained to predict \"1\" if the sum-\nmary is factually consistent and \"0\" otherwise. We\nuse a learning rate of 10\u22124 and a batch size of 32.\nDuring training, we use a maximum input length\nof 512 tokens and truncate the premise if needed.24\nDuring inference we use a maximum input length\nof 2048 tokens. We train for a maximum of 20\nepochs, evaluate a checkpoint every 1k steps and\nchoose the checkpoint with the best ROC-AUC on\na development set.23 In our study we make sure to\nuse the same training regime for all baselines.\nThe ANLI-only results in Table 3 are from our\nexperiments, while in Table 2 we use the results\nreported in previous work.\nFor the summarization models we fine tune the\ncorresponding T5 models on the XSum training set\n(Narayan et al., 2018) in a similar fashion and use\nthe ROUGE score on the XSum development set\nas a stopping criteria.\nA.4\nAdditional Details About Our Dataset\nAs mentioned in \u00a73.1, we create the dataset based\non documents from CNN/DailyMail (Hermann\net al., 2015). We do not use the gold summaries,\nand we only use examples from the training set.\nIn our experiments with the full dataset (\u00a74.1),\nwe balance the labels by randomly sampling\n475,563 positive examples (see Table 1).\n24In early experiments we saw that training with longer\nmaximum input length resulted with comparable performance.\nFigure 4: Self-verification prompting. If the LLM clas-\nsified the summary as consistent, we prompt it again\nand ask it for its certainty. If the answer is \u201cYes\u201d (consis-\ntent with the original reasoning), we keep the example,\notherwise we filter it out.\nA.5\nData Filtering with Self-verification\nAs mentioned in \u00a73 we also explored data filter-\ning based on prompting FLAN-PaLM for self-\nverification. Our proccess is based on 3 steps. (1)\nDetect potential examples in our dataset that are\nlikely to be labeled incorrectly by the LLM. (2)\nPrompt the LLM to self-verify its earlier prediction\nand filter out examples that the model is uncertain\nof. This leads to a smaller dataset with improved\nlabeling accuracy. (3) Train the factual consistency\nevaluation model on the filtered dataset. This ap-\nproach is based on 2 observations:\n1. In early experiments, we saw that our LLM has\nextremely high precision for the inconsistent\nclass. This can also be seen in our human eval-\nuation (Table 4). This means that almost all\nthe errors occur when the LLM predicts that\nthe summary is consistent. Following this, we\nonly consider filtering examples classified as\nconsistent by the LLM.\n2. Inspired by the work of Weng et al. (2023) and\nMadaan et al. (2023), we use a self verification\nprompt. If the LLM classified the summary as\nconsistent, we prompt it again and ask it for its\ncertainty. If the answer is \u201cYes\u201d (i.e. it is consis-\ntent with the original reasoning path), we keep\nthe example, otherwise we filter it out. This\nproccess is illustrated in Figure 4.\nThe self-verification prompt is as follows:\nPremise: {document} Hypothesis: {summary} Are you\nsure that the summary can be inferred from the docu-\nment? Answer using \"Yes\" or \"No\" only.\nThis approach filtered-out 15% of the dataset.\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nExtractiveness\n0.0\n0.1\n0.2\n0.3\n0.4\nDensity\nFactEdit\nDocNLI\nFactCC\nFalsesum\nOurs\nFigure 5: Visualization of the density of the combined\nabstractivness score. The plot is actually measuring the\nextractiveness degree, so lower x-values mean higher\nabstractiveness.\nWhen we qualitatively analyzed the filtered exam-\nples, it seems that the majority of the filtered exam-\nples indeed had a wrong label, and that applying\nthis filtering mechanism increases the labeling ac-\ncuracy by approximately 5%.\nWhile this filtering mechanism results in higher\nlabeling accuracy, we did not observe a perfor-\nmance gain when filtering the training data in this\nway. For TrueTeacher + ANLI with T5-11B (on\na sample of 100k examples) we got an average\nof 86 ROC-AUC on TRUE using the filtered data,\nslightly below the 86.4 using the unfiltered data\n(Table 3). As mentioned in Footnote 9, we attribute\nthis to the fact that the labeling accuracy is high to\nbegin with (89%, section 4.4) and that the model\nis likely robust to some amount of labeling noise.\nFollowing this, for simplicity, our official method\ndoes not use filtering.\nA.6\nAbstractiveness Analysis: Additional\nDetails\nAs our backbone metrics we use the Extractive\nFragment Coverage and Density measures defined\nby Grusky et al. (2018). Coverage measures the\npercentage of words in the summary that are part\nof an extractive fragment with the article, quanti-\nfying the extent to which a summary is derivative\nof a text. Density measures the average length of\nthe extractive fragment to which each word in the\nsummary belongs, quantifying how well the word\nsequence of a summary can be described as a series\nof extractions. Our Combined score is obtained by\nmultiplyng the Coverage and the Density scores,\nsimilar to Utama et al. (2022). To further illustrated\nthe differences in the abstractiveness of different\nmethods, we include a visualization of the density\nof the combined abstractivness score in Figure 5.\nANLI+XNLI\n+100K en\n+100K en/es/de/fe\namharic\n63.1\n67.2\n68.6\narabic\n87.8\n89.0\n87.7\nazerbaijani\n59.6\n68.6\n65.5\nbengali\n90.4\n94.3\n98.5\nburmese\n59.0\n64.5\n57.9\nchinesesimp.\n87.6\n86.4\n89.9\nchinese trad.\n82.5\n82.6\n83.2\nenglish\n80.2\n74.7\n80.0\nfrench\n91.9\n94.1\n97.1\ngujarati\n50.8\n52.0\n51.5\nhausa\n69.5\n67.7\n73.7\nhindi\n72.2\n79.9\n86.5\nigbo\n62.2\n62.8\n75.7\nindonesian\n77.6\n84.1\n85.8\njapanese\n97.7\n98.9\n99.6\nkirundi\n83.5\n89.3\n90.4\nkorean\n87.3\n82.3\n89.9\nkyrgyz\n70.1\n77.4\n79.0\nmarathi\n75.2\n78.7\n73.6\nnepali\n55.2\n59.1\n57.2\noromo\n81.2\n83.7\n83.3\npashto\n56.4\n68.2\n67.7\npersian\n43.5\n42.3\n45.8\npidgin\n70.0\n81.4\n77.1\nportuguese\n79.6\n79.5\n79.0\npunjabi\n77.7\n81.5\n78.2\nrussian\n88.8\n85.1\n81.2\nscottish gaelic\n59.0\n58.8\n63.1\nserbian cyrillic\n84.2\n79.3\n85.5\nserbian latin\n39.7\n42.2\n43.6\nsinhala\n72.9\n74.9\n76.1\nsomali\n85.1\n88.6\n86.6\nspanish\n80.7\n85.9\n89.1\nswahili\n88.1\n89.2\n92.2\ntamil\n63.9\n69.8\n66.0\ntelugu\n55.9\n62.3\n60.4\nthai\n78.8\n83.8\n86.8\ntigrinya\n79.9\n82.9\n86.1\nturkish\n87.0\n86.6\n86.6\nukrainian\n55.5\n67.0\n65.9\nurdu\n69.0\n63.8\n75.3\nuzbek\n54.6\n59.3\n58.8\nvietnamese\n89.8\n84.4\n88.1\nwelsh\n83.0\n83.4\n83.9\nyoruba\n69.0\n69.0\n77.2\n# wins\n5\n15\n25\n# > ANLI+XNLI\n-\n32\n35\nPer lang. avg.\n73.3\n75.7\n77.2\nPer example avg.\n71.6\n73.8\n75.3\nTable 10: ROC-AUC results on the mFace test set.\nA.7\nUsing the mFace dataset\nIn \u00a75 we report results on the mFace dataset (Aha-\nroni et al., 2022). Aharoni et al. performed large\nscale human evaluation of summaries of documents\nfrom the XLSum corpus (Hasan et al., 2021), pro-\nduced by different summarization models. Each\nsummary was rated for quality, attribution and in-\nformativeness. We use the attribution scores in\nour work. The attribution evaluation is based on\nthe attribution definition provided in Rashkin et al.\n(2021), with the participants asked \"Is all the in-\nformation in the summary fully attributable to the\narticle?\". In our work we use the average attribu-\ntion score (between 0 to 1) and treat summaries as\nfactually consistent if the score is larger than 0.5.\nWe focus on the test split of XLSum containing\n3150 examples in 45 languages (i.e., 70 examples\nin each language). In \u00a75 we refer to Table 7 with\nthe results overview, and we provide the full results\nfor all languages in Table 10.\nA.8\nHuman Evaluation\nWe instructed the participants to review the docu-\nment and its corresponding summary, and to evalu-\nate the summary based on the attribution definition\nprovided by Rashkin et al. (2021), using binary\njudgements. To avoid a common confusion be-\ntween factual inconsistency and contradiction, we\nalso provided the following instruction:\nIn this task you will evaluate the factual consistency of\na system-generated summary. The system\u2019s goal is to\nsummarize the original source document, while remain-\ning truthful to it. Your goal is to evaluate whether the\nsystem-generated summary is consistent w.r.t. the source\ndocument. Summary will be considered consistent if\nall of the information in the summary can be verified\nfrom the source document (i.e., for the summary to be\ninconsistent, the document does not necessarily need to\ncontradict it, it can also fail to support some facts).\nIn an early experiment, we found that using\ncrowd workers without domain expertise and sub-\nstantial time investments resulted in extremely low-\nquality ratings. Following this, all our raters were\nNLP researchers, each with at least one year of spe-\ncific experience in the task of factual consistency\nevaluation, with significant time allocation and no\nmore than 10 examples per rater.25 These steps\nensured high quality ratings.\nA.9\nAdding noise to TrueTeacher\nIn \u00a74.5 we create SummaryAblation by flipping\nlabels to a random portion of TrueTeacher\u2019s data,\nsuch that the expected labeling accuracy is sim-\nilar to Falsesum. Falsesum\u2019s labeling method is\ncoupled with the data generation, thus we need an\napproximation for its labeling quality. We estimate\nFalesum\u2019s labeling accuracy as 83.5%, according\nto Utama et al. (2022)\u2019s human evaluation (we aver-\nage the Intrinsic and Extrinsic results), while ours\nis 89% (\u00a74.4). So to mimic Falsesum\u2019s quality we\nflipped TrueTeacher\u2019s labels in order to add addi-\ntional 5.5% errors.\n25We found that it is sufficient to use one rater per example\n(unlike in our experiments with the crowd workers).\n"
  },
  {
    "title": "Learning the Visualness of Text Using Large Vision-Language Models",
    "link": "https://arxiv.org/pdf/2305.10434.pdf",
    "upvote": "2",
    "text": "Learning the Visualness of Text Using Large Vision-Language Models\nGaurav Verma\nGeorgia Institute of Technology\ngverma@gatech.edu\nRyan A. Rossi\nAdobe Research\nryrossi@adobe.com\nChristopher Tensmeyer\nAdobe Research\ntensmeye@adobe.com\nJiuxiang Gu\nAdobe Research\njigu@adobe.com\nAni Nenkova\nAdobe Research\nnenkova@adobe.com\nAbstract\nVisual text evokes an image in a person\u2019s mind,\nwhile non-visual text fails to do so. A method\nto automatically detect visualness in text will\nenable text-to-image retrieval and generation\nmodels to augment text with relevant images.\nThis is particularly challenging with long-form\ntext as text-to-image generation and retrieval\nmodels are often triggered for text that is de-\nsigned to be explicitly visual in nature, whereas\nlong-form text could contain many non-visual\nsentences. To this end, we curate a dataset\nof 3,620 English sentences and their visual-\nness scores provided by multiple human an-\nnotators. We also propose a fine-tuning strat-\negy that adapts large vision-language models\nlike CLIP by modifying the model\u2019s contrastive\nlearning objective to map text identified as non-\nvisual to a common NULL image while match-\ning visual text to their corresponding images\nin the document. We evaluate the proposed ap-\nproach on its ability to (i) classify visual and\nnon-visual text accurately, and (ii) attend over\nwords that are identified as visual in psycholin-\nguistic studies. Empirical evaluation indicates\nthat our approach performs better than several\nheuristics and baseline models for the proposed\ntask. Furthermore, to highlight the importance\nof modeling the visualness of text, we conduct\nqualitative analyses of text-to-image generation\nsystems like DALL-E.\n1\nIntroduction\nPeople typically communicate knowledge and\ninformation textually, but most prefer to con-\nsume visually rich content. Text-to-image genera-\ntion/retrieval models could augment text with ap-\npropriate images, aiding the creation of appealing\nand easy-to-understand documents. Models like\nDALL-E (Ramesh et al., 2022) and Stable Diffu-\nsion (Rombach et al., 2022) work phenomenally\nwell for input text that is carefully constructed to\nelicit images. However, they cannot handle long-\nform text with a mix of sentences that may or may\nWhen the gardens open, just \nafter dawn, the first to appear \nare the joggers and the silent \nfigures performing the \nintricate maneuvers of tai chi.\nIn case of your failure to \nanswer, judgment will be \ntaken against you by default \nfor the relief demanded in the \ncomplaint.\nhuman-annotated \nvisualness score = 6.44\nhuman-annotated \nvisualness score = 1.67\nCLIP model \nfine-tuned for \nidentifying \nvisual text\ntext input\n\u201cvisual text\u201d\n\u201cnon-visual text\u201d\ntext input\ntext-to-image (e.g., DALL-E)\nOur contributions\nFigure 1: Overview of the sentence visualness identifi-\ncation task, along with a motivating downstream appli-\ncation (passive generation of relevant images).\nnot evoke a visual image. To this end, we introduce\nthe task of identifying sentence visualness\u2014a term\nwe use interchangeably with imageability\u2014as a\nnecessary first step toward connecting long-form\ntextual documents with relevant visual assets, with-\nout having to manually find visual sentences. In\nother words, to work effectively with long-form\ntext without relying on manual input, text-to-image\ngeneration models like Stable Diffusion, DALL-E,\nand Imagen (Saharia et al., 2022) would benefit\nfrom inferring text visualness before they can gen-\nerate images to embellish textual documents. In\nFigure 1, we demonstrate the need with some ex-\namples: text identified to have low visualness leads\nto irrelevant generations from DALL-E, while text\nidentified to have high visualness leads to the gen-\neration of relevant images.\nPrior approaches for quantifying the visu-\nalness of text operate on a word or phrase\nlevel (Deschacht and Moens, 2007; Jeong et al.,\n2012) and leverage lexicons that contain human-\nassigned world-level imageability scores (Louis\nand Nenkova, 2013). However, besides being lim-\nited in their coverage, our experiments also show\nthat word or phrase-level visualness cannot be ag-\ngregated to quantify sentence-level visualness.\nTo this end, in this work, we curate a corpus of\n3,260 sentences in English paired with their human\narXiv:2305.10434v2  [cs.CL]  22 Oct 2023\nratings for visualness, as well as a noisy but large\ncorpus of 48,077 automatic alignments between\ntext and visual assets in long-form documents. The\ntextual part of the resulting alignment pairs can\nbe used as examples of visual and non-visual sen-\ntences. We propose a strategy to fine-tune vision-\nlanguage models like CLIP, allowing classification\ninferences over text-only inputs. Our objective also\nensures that the learned embeddings remain usable\nfor downstream text-to-image retrieval.\nWe compare the performance of our proposed\napproach against several heuristic and model-based\nbaselines. Our extensive evaluation suggests that\nour fine-tuning strategy leads to the most accu-\nrate visual and non-visual text classifier. Finally,\nwe conduct several analyses to glean insights into\nthe model\u2019s learned attention mechanism, text-to-\nimage retrieval abilities, and downstream text-to-\nimage generation capabilities.1\nIn sum, our key contributions are:\n\u2022 We propose the task of identifying the visualness\nof a sentence and curate a dataset by crowdsourc-\ning annotations for English sentences.\n\u2022 We develop a training objective that fine-tunes\nlarge vision-language models for the task of text\nvisualness identification.\n\u2022 Quantitative and qualitative experiments demon-\nstrate the effectiveness of our fine-tuning approach\nin identifying visual text over several competitive\nbaselines, while preserving downstream text-to-\nimage retrieval performance.\n2\nRelated Work\nFine-tuning vision-language models for down-\nstream tasks: Large vision-language models like\nCLIP (Radford et al., 2021), UNITER (Chen et al.,\n2020), and ALIGN (Jia et al., 2021) have demon-\nstrated remarkable performance on downstream\ntasks via transfer learning or fine-tuning. How-\never, such downstream tasks assume both text and\nimage as input to determine similarity or gener-\nate/retrieve the other modality for every instance\nof the corresponding modality; for instance, vi-\nsual question answering (Antol et al., 2015), cap-\ntion generation (Xu et al., 2015), and cross-modal\nretrieval (Wang et al., 2016). Fine-tuning large\nvision-language models on such downstream tasks\ninvolves adding components to the encoders\u2019 ar-\nchitecture and training additional parameters on\n1Project webpage:\nhttps://gaurav22verma.github.\nio/text-visualness/\nthe task-specific dataset (Mittal et al., 2022; Sarto\net al., 2022). Our work differs from existing work\nin that the input is only text, requiring us to adapt\nlarge vision-language models to not rely on both\nmodalities during inference. We propose a fine-\ntuning strategy that does not involve additional ar-\nchitectural components (and parameters) on top of\na pre-trained CLIP architecture and yet effectively\nadapts CLIP for learning text visualness. Our task\ncan be considered a precursor to tasks like text-to-\nimage retrieval and generation, where images are\nonly retrieved or generated for visual text. Further,\nsince reusability of representation is a desirable\nproperty (Yosinski et al., 2014; Long et al., 2015)\nwe aim to preserve the reusability of text embed-\ndings learned for the visualness categorization task\nfor downstream tasks like text-to-image retrieval.\nVisualness of words: The visualness of text has\nbeen studied in multiple prior works but at a word\nor phrase level. Coltheart (1981) curated the MRC\nPsycholinguistic Database comprising human rat-\nings for imageability of 3769 words, which were\nlater expanded using automated methods by Louis\nand Nenkova (2013). Beyond word-level visual-\nness, some studies have focused on automated\nquantification of phrase-level visualness (Jeong\net al., 2012; Deschacht and Moens, 2007). Our\nwork focuses on learning sentence-level visualness\ninstead of word or phrase-level visualness. While it\nis possible to aggregate word-level and phrase-level\nvisualness scores to obtain sentence-level scores,\nit is unclear how accurate and generalizable these\ntechniques are. We design multiple baselines that\naggregate word-level scores to obtain sentence-\nlevel visualness and contrast the performance of\nsuch approaches with our proposed approach.\n3\nText Imageability Dataset (TImeD)\nOur proposed fine-tuning approach follows multi-\nstage training of a large vision-language model\nCLIP (Radford et al., 2021). In the first stage, we\nconduct large-scale fine-tuning, followed by fine-\ntuning on a relatively smaller annotated corpus in\nthe second stage. We first discuss the curation of\na large-scale corpus that comprises automatically-\nassigned and distant labels and then describe the\ncuration of the human-labeled corpus of visual &\nnon-visual sentences.\n3.1\nFine-tuning with automatic labels\nThe formulation of the training objective (discussed\nlater) requires positive examples comprising vi-\nCategory\nExample text from TIMED\n\u00b5 / \u03c3\nVisual\n\u00b7 now the snow has melted and the grass not only looks dreary, but it is soggy.\n\u00b5 = 6.88\n\u00b7 The operation left a six-inch zipper scar on his chest.\n\u00b5 = 6.55\n\u00b7 When the gardens open, just after dawn, the first to appear are the joggers and the silent figures performing the intricate maneuvers of tai chi.\n\u00b5 = 6.44\n\u00b7 He removed the box, placed it next to the garbage can, and put his garbage inside the can.\n\u00b5 = 5.88\n\u00b7 But, after running only the first 500 meters, he realized that the injury that seemed so insignificant would not only prevent him from winning the race,\nbut also from finishing it.\n\u00b5 = 5.00\nNon-visual\n\u00b7 There\u2019s only one way to prove them wrong.\n\u00b5 = 1.22\n\u00b7 For more information or to schedule an outreach, please call (999) 123-4567 or email email@website.com.\n\u00b5 = 1.55\n\u00b7 In case of your failure to answer, judgment will be taken against you by default for the relief demanded in the complaint.\n\u00b5 = 1.67\n\u00b7 A 25% quorum of member votes in each district is needed to conduct district delegate elections in October.\n\u00b5 = 1.77\n\u00b7 Colliers International makes no guarantees, representations or warranties of any kind, expressed or implied, regarding the information including, but\nnot limited to, warranties of content, accuracy and reliability.\n\u00b5 = 2.00\nAmbiguous\n\u00b7 J. Roman discusses his book Ohio State Football: The Forgotten Dawn which draws on extensive archival research to tell the untold story of the early\ndays of football at Ohio as flagship public university.\n\u03c3 = 2.34\n\u00b7 Remember to be sure to set your clocks back 1 hour before you go to bed on Saturday, November 3rd.\n\u03c3 = 2.23\n\u00b7 That is the most important thing in my life today: Jesus.\n\u03c3 = 2.20\n\u00b7 Children & parents will get to hear author George McClements read his book Ridin\u2019 Dinos with Buck Bronco.\n\u03c3 = 2.14\n\u00b7 Financial Peace University is a nine-lesson class taught by financial expert Dave Ramsey through entertaining videos with an in-depth workbook, that\nwill teach you how to take control of your money.\n\u03c3 = 2.16\nTable 1: Qualitative examples of visual and non-visual text from the human-annotated subset of the Text Imageability\nDataset (based on the average of annotator ratings), and text with high ambiguity (based on the standard deviation\nof annotator ratings).\nsual text and paired images as well as negative\nexamples that comprise non-visual text. To cre-\nate a corpus like this, we: (i) leverage image-text\nco-occurrences in documents to develop a self-\nsupervised approach, and (ii) use image-text simi-\nlarity scores obtained using CLIP as priors to con-\nstruct a large training corpus. We start with 450,000\npublicly available PDFs referenced in the Common\nCrawl corpus and identify pages within those PDFs\nthat include images.2 We use a proprietary doc-\nument object detection tool like Fitz3 to extract\nparagraphs and images from the document pages.\nWe do sentence segmentation for the identified\nparagraphs using NLTK Tokenizer (Loper and\nBird, 2002). To map the images in the page to\nsentences, we compute CLIP similarity scores be-\ntween each image-sentence pair in a given page.\nBased on the distribution of image-sentence simi-\nlarity scores across all the pages in our corpus, we\nset two thresholds, Tpos and Tneg. A sentence in a\npage is considered a positive example (visual text)\nif its similarity with any of the images in the page\nis greater than Tpos. Similarly, chosen negative ex-\namples have similarity values less than Tneg with\nall images within the same page. Sentences with\nan image similarity value greater than Tpos are as-\nsociated with the most similar image in the same\npage, while the negative examples are associated\n2We choose to work with PDF documents rather than web-\npages because (i) PDFs have natural demarcations in the form\nof pages (whereas webpages often contain long-running text\nwith complex image-text interactions), and (ii) images within\na page are likely to be related to selected text fragments within\nthe same page.\n3https://github.com/pymupdf/PyMuPDF\nwith a common NULL image. The thresholds Tpos\nand Tneg are chosen conservatively to only include\ntop or bottom k % sentences from the entire corpus,\nrespectively. This limits the noise in our training\ncorpus for adapting the CLIP model for scoring\ntext visualness. In our experiments, we set Tpos to\nbe 0.35 to consider top 1% sentences as visual and\nTneg to be 0.18 to consider bottom 5% sentences as\nnon-visual. Our automatically-labeled corpus com-\nprises 15,359 visual sentences, the corresponding\nimages, and 32,718 non-visual sentences.\n3.2\nHuman-annotated dataset\nFor the human-annotated visual and non-visual ex-\namples, we start with another 200,000 PDFs dis-\ntinct from those used for the automated assignment\nof labels. To focus on natural images rather than in-\nfographics and academic figures, we filtered these\ndocuments to only include brochures, flyers, and\nmagazines. For the resulting 35,432 documents,\nwe adopted the same policy as that for curating\nthe automatically-labeled dataset (selecting top 1%\nand bottom 5% sentences based on similarity val-\nues). We then recruited annotators to rate the visual-\nness of the resulting 3,620 sentences after manually\nanonymizing any personal information.\nWe recruited annotators on Amazon Mechani-\ncal Turk (AMT). We randomly ordered the 3,620\nexamples and, for each example, we asked nine an-\nnotators to provide a response on a 7-point Likert\nscale for the following question: \u201cDo you agree\nthat the sentence below evokes an image or picture\nin your mind?\u201d A response of 1 indicated strong\ndisagreement, while 7 indicated strong agreement.\nWe also inserted some attention-check examples\n(5%; n = 181) to ensure the annotators read the\ntext carefully before responding. These checks ex-\nplicitly asked the annotators to mark a randomly\nchosen score on the Likert scale regardless of the\nactual content. We discarded the annotations from\nannotators who did not correctly respond to all the\nattention-check examples and re-collected more re-\nsponses iteratively. Appendix A.3 provides more\ndetails about the filters used for recruiting the an-\nnotators and the annotation interface.\nIf a majority of annotations (i.e., at least 5 out of\n9) were 1, 2, or 3, we considered the example to\nbe non-visual (n = 2108). Similarly, visual ex-\namples had a majority of 5, 6, or 7 responses (n =\n1132). We considered examples that did not have\na clear majority or majority of responses of 4 (i.e.,\n\u2018Neutral\u2019 on the Likert scale) as ambiguous and\nneutral, respectively. Table 1 shows illustrative\nexamples of visual, non-visual, and ambiguous\ntext from our human-annotated corpus.\nFor 27.1% of the examples only at most 1 of\nthe 9 annotators disagreed with the labels decided\nbased on the process described above. 10.5% of\nthe sentences were assigned a neutral or ambigu-\nous class. Inter-annotator agreement measured by\nKrippendorff\u2019s \u03b1 was 0.446. This inter-annotator\nagreement value is in a similar range to what is\nobserved for other language-related tasks that in-\nvolve assessment of text by experts on dimensions\nlike coherence, likability, relevance, and even gram-\nmar (Karpinska et al., 2021). For brevity, we refer\nto the curated dataset as TIMED, short for Text\nImageability Dataset.\n4\nTIP-CLIP for Scoring Text Visualness\nBackground: The CLIP model (Radford et al.,\n2021) jointly trains image and text encoders to pre-\ndict the correct pairing between images and textual\ndescriptions. In a batch size of N images and N\ntexts (N2 possible image-text pairings), the objec-\ntive function ensures that the cosine similarity be-\ntween the embeddings of correct image-text pairs is\nmaximized while the cosine similarity between the\n(N2 \u2212 N) incorrect image-text pairs is minimized.\nThe encoders are trained over a large multimodal\ndataset of \u223c 400 million image-text pairs.\nUpdated training objective: When predicting text\nvisualness, the goal is to assign a higher score to\ntext that is visual (evokes a concrete image for the\nperson reading it) and a lower score for non-visual\nthe common loon, minnesota\u2018s state \nbird, usually nests on islands or on \nshore lines of our northern lakes\ncolliers International makes no \nguarantees, representations or \nwarranties of any kind, expressed or \nimplied, regarding the info...\nthere\u2019s only one way to prove them \nwrong.\njim sent along the following images \nof these successful anglers and \none of red drum they caught.\nText encoder\nImage encoder\nT1\n.T1\nT2\nTN\nTN-1\nI1\nI1\nI1.T2\nI1.TN-1\n.TN\nI1\nInull.T1\nInull.T2\nInull.TN-1\n.TN\nInull\nIN-1.T1\nIN-1 .T2\nIN-1 .TN-1\n.TN\nIN-1\nInull.T1\nInull.T2\nInull.TN-1\n.TN\nInull\nInull\nvisual text\nvisual text\nnon-visual text\nnon-visual text\nInull\nIN-1\nNULL image\nNULL image\nFigure 2: Our approach to predicting sentence visual-\nness, with a fine-tuning strategy where visual text is\nmatched with its corresponding image while non-visual\ntext is matched with a fixed NULL image.\ntext (text that does not evoke an image). In line\nwith the original training objective, we further train\nthe CLIP model to match text that is identified as\nvisual with the corresponding image. We adapt\nthe CLIP training to match text that is identified\nas non-visual with a single NULL image (see Fig.\n2). Matching visual text with the corresponding\nimage while non-visual text to a NULL image not\nonly encourages the model to distinguish between\nvisual and non-visual text, but also allows it to\nanchor non-visual text in the common NULL image\nthat can be used during inference without having\naccess to a potentially paired image. Formally, the\nadapted training objective is given as,\nL = \u2212 1\n2N\nN\nX\nj=1\nlog\n \nexp(\u27e8Ie\nj , T e\nj \u27e9/\u03c4)\nPN\nk=1 exp(\u27e8Ie\nj , T e\nk\u27e9/\u03c4)\n!\n\u2212\n1\n2N\nN\nX\nk=1\nlog\n \nexp(\u27e8Ie\nk, T e\nk\u27e9/\u03c4)\nPN\nj=1 exp(\u27e8Ie\nj , T e\nk\u27e9/\u03c4)\n!\nst. Ie\nm =\n(\nIe\nnull,\nif m \u2208 \u00afV (i.e., non-visual)\nIe\nm,\nif m \u2208 V\n(i.e., visual).\n(1)\nHere, N denotes the number of examples in\na batch, Ie\nm and T e\nm denote the embeddings of\nthe m-th pair of image and text that are normal-\nized to have unit \u21132-norm, respectively, such that\nm \u2208 {1, . . . , N}. \u27e8...\u27e9 represents the inner prod-\nuct, and \u03c4 is the trainable temperature parameter. \u00afV\nand V are the set of examples in the current batch\nthat belong to non-visual and visual categories, re-\nspectively. Finally, Ie\nnull denotes the embedding\nof the NULL image. During inference, we compute\nthe cosine similarity between the representation of\na given text with the representation of the NULL\nimage; non-visual texts will have a high similarity\nwith the NULL image. Conversely, the visualness\nscore S of any text with embedding T e can be ob-\ntained using\nS = 1 \u2212 \u27e8Ie\nNULL, T e\u27e9.\n(2)\nFor the NULL image, we create an RGB image\nof size (224, 224, 3) in which each pixel value is\nchosen randomly (see Figure 2). However, experi-\nments with different types of NULL images indicate\nthat the choice of null image does not affect the\nmodel\u2019s performance; see Appendix A.1.\nAn alternative formulation for adapting the CLIP\ntraining objective could have been to match visual\ntext with a single image while matching non-visual\ntext with a single NULL image. However, this formu-\nlation of the training objective is similar to binary\nclassification and does not enforce a contrastive\nobjective for the positive examples. Matching vi-\nsual text with its corresponding image instead of a\ncommon image for all visual text affords text em-\nbeddings that can be used for downstream tasks\nlike text-to-image retrieval; we provide empirical\nevidence for worse text-to-image retrieval perfor-\nmance with the alternative formulation in Results.\n5\nTraining details and Baselines\nTrain, test, & validation splits: Recall that our\nfine-tuning approach requires paired images for vi-\nsual sentences only during training time and not\nduring inference time; the model needs only text\nas input during inference. Of the 1132 visual sen-\ntences in the human-annotated set of TIMED, we\nassign 515 examples that had an automatically de-\ntermined corresponding image to the training set,\nand the remaining were randomly assigned to the\ntest set (n = 517) and validation set (n = 100).\nThe 2108 non-visual sentences were randomly split\ninto the training (n = 980), test (n = 928),\nand validation set (200). All three sets maintain\npositive:negative class ratio of \u223c 0.5.\nFor the first stage of training, we fine-tune the\nCLIP model (ViT/B-32) on the proposed objec-\ntive (see Eq. 1) using the 48,077 examples with\nautomatic labels. This training is done on Tesla\nT4 GPUs, for 5 epochs, and a learning rate ini-\ntialized at 5 \u00d7 10\u22125 and optimized using Adam\noptimizer (Kingma and Ba, 2014).\nFollowing\nthis, for the second stage, we further fine-tune\nthe same model for 2 epochs using the same ob-\njective and hyper-parameters, but this time using\nthe train set of human-annotated TIMED.4 The\nhyper-parameters are selected by performing a grid\nsearch while observing performance on the vali-\ndation set of TIMED. Based on the performance\non the validation set of TIMED, we set the thresh-\nold of S (Eq. 2) to be 0.79 to categorize text as\nvisual or non-visual. We refer to the model\ntrained using our fine-tuning strategy as TIP-CLIP\n\u2014 Text Imageability Predictor CLIP, and report\nperformance on the test set of TIMED.\n5.1\nBaselines\nWe investigate the performance of TIP-CLIP\nagainst several heuristics and baseline models.\nRandom: The random baseline generates predic-\ntions via prior class probabilities in the training set.\nAverage MRC-I score: We consider the image-\nability scores of 3,769 words in the MRC lexicon\nand normalize them to be \u2208 [0, 1]. For each exam-\nple, we take the average of the imageability scores\nof the unique words; out-of-vocabulary words are\nassigned a score of 0. We lowercase the words in\nthe MRC lexicon as well as the input text. Based\non this average score, we categorize an example\nas visual or non-visual by setting the decision\nboundary as 0.17. The threshold is chosen to opti-\nmize performance on the validation set of TIMED.\nConcentration of Visual Genome objects (VG-\nObjects): The Visual Genome dataset comprises\n75,729 objects, along with annotations for their at-\ntributes and object-object relations (Krishna et al.,\n2017). Based on the heuristic that a mention of a\nvisual object in the text can trigger imageability,\nwe quantify the concentration of Visual Genome\nobjects by computing the fraction of unique object\nmentions in tokenized text with respect to the num-\nber of total unique words within the input text. We\nset the threshold to 0.5 based on the performance\non the validation set.\nExpanding the MRC lexicon using word embed-\ndings: The coverage of the MRC lexicon is poor be-\ncause it contains only 3,769 words. We expand this\n4The CLIP model has a maximum context length of 77\ntokens (about 50 words). Fewer than 1% of the training exam-\nples are truncated to fit this context length.\nlist using semantic similarity between distributed\nrepresentations of words (300-dim word2vec vec-\ntors trained on Google News corpus). For each\nword w in the word2vec (Mikolov et al., 2013) vo-\ncabulary of pre-trained representations that does\nnot occur in the MRC lexicon, we compute its co-\nsine similarities with all the words in the MRC lex-\nicon to identify the most semantically similar word\nthat exists in MRC, given by wMRC and its similar-\nity with w given as (simmax). We assign the word\nw an imageability score of simmax \u00d7 scorewMRC,\nwhere scorewMRC is the normalized imageability\nscore of w\u2019s most similar word wMRC. Based on\nthe performance on the validation set, the decision\nboundary for average imageability score of input\ntext is set as 0.17. This baseline propagation ap-\nproach is highly effective in quantifying word-level\nimageability as the Pearson\u2019s correlation coefficient\nbetween the assigned visualness score and the aver-\nage AMT rating of humans is 0.735 (p < 0.001);\nsee Appendix A.2 for details.\nFine-tuned BERT classifier: We fine-tune a BERT\nmodel (bert-base-uncased on HuggingFace (De-\nvlin et al., 2018; Wolf et al., 2020)) for the clas-\nsification task of visual versus non-visual text\ndetection. Similar to our proposed model, we adopt\na two-stage fine-tuning approach with the BERT\nclassifier (adding a classification layer to BERT for\nthe first input token\u2019s ([CLS]) representation). We\nfirst fine-tune the model using the automatically\nlabeled dataset followed by fine-tuning on the train-\ning set of the human-curated TIMED. For the first\nstage, we fine-tune the model for 7 epochs with a\nlearning rate initialized at 5 \u00d7 10\u22125 using a batch\nsize of 32 while setting other hyper-parameters to\ndefault. We fine-tune the model for 3 epochs for\nthe second stage with the same hyperparameters.\nPre-trained CLIP model: We use the pre-trained\nCLIP model (ViT/B-32) to obtain similarity scores\nbetween the embeddings of the NULL image (used\nfor the fine-tuning of our model) and the input text.\nWe then use 1 \u2212 \u27e8Ie\nNULL, T e\u27e9 as an estimate of the\nvisual score of text (see Eq. 2). Based on the per-\nformance on the TIMED validation set, we set the\nthreshold for S to be 0.83.\n6\nResults and Analyses\nEvaluation on held-out test set of TIMED: We\nfirst evaluate the baselines and our approach on\nthe test set of the human-annotated TIMED, com-\nputing macro-averaged F1, precision, recall scores,\nMODELS\nF1 \u2191\nPRECISION \u2191\nRECALL \u2191\nACC. \u2191\nRandom\n0.531\n0.531\n0.531\n0.577\nMRC-I\n0.584\n0.599\n0.583\n0.644\nVG-Objects\n0.606\n0.610\n0.605\n0.646\nMRC-I + w2v\n0.638\n0.637\n0.639\n0.667\nBERT\n0.753\n0.766\n0.789\n0.756\nCLIP\n0.694\n0.695\n0.701\n0.712\nTIP-CLIP (Ours)\n0.865\n0.858\n0.873\n0.871\nTable 2: Evaluation on human-annotated test set of\nTIMED. Reported F1, Precision, and Recall values are\nmacro-averages across the two classes (visual and\nnon-visual).\nand classification accuracy. Table 2 show the re-\nsults for this evaluation. We observe that our pro-\nposed two-stage fine-tuning strategy leads to the\nbest-performing model (TIP-CLIP). In comparison,\nthe pre-trained CLIP model demonstrates notably\nweaker performance on the task of distinguishing\nvisual text from non-visual text. Interestingly, fine-\ntuned BERT performs reasonably well on the task,\nconsiderably better than the CLIP model. Using the\naverage imageability scores from MRC provides\nbetter-than-random performance but is severely\nsubpar to models like CLIP, BERT, and TIP-CLIP.\nUsing word2vec embeddings to expand the cover-\nage of the MRC lexicon (i.e., MRC-I + w2v) leads\nto a boost in performance. However, collectively,\nthe lacking performance of MRC-I and MRC-I\n+ w2v demonstrates that word-level imageability\ndoes not translate to sentence-level imageability\nto a great extent. Notably, in terms of baselines\nthat aggregate word-level attributes, VG-Objects\nprovides the best estimate of sentence-level image-\nability by quantifying the concentrations of visual\nobjects in the input sentence.\nCorrelation of attention Weights with MRC im-\nageability scores: Attention mechanisms could be\ntaken as proxies for explainability (Wiegreffe and\nPinter, 2019; Chefer et al., 2021). Since the fine-\ntuned BERT, pre-trained CLIP, and our TIP-CLIP\nare attention-based models, we compute the corre-\nlation between average word-level attention scores\n(obtained from the last layer) on a given dataset\nwith the imageability scores assigned by humans\nin the MRC lexicon. We compute these values for\ntwo datasets\u2014the MSCOCO dataset (Vinyals et al.,\n2016) and the test set of TIMED. We only consider\nwords that occur more than once in the specific cor-\npus. Table 3 shows that TIP-CLIP attention scores\ncorrelate the most with MRC imageability scores,\nMODELS\nMSCOCO\nTIMED\nBERT\n0.461*** (n = 344)\n0.326*** (n = 294)\nCLIP\n0.448*** (n = 344)\n0.283*** (n = 294)\nTIP-CLIP (Ours)\n0.497*** (n = 344)\n0.367*** (n = 294)\nTable 3: Correlation between MRC Imageability scores\nand model attention-scores for BERT, CLIP, and TIP-\nCLIP. n denotes the number of overlapping words across\nvocabularies; *** denotes p < 10\u22123.\nMODELS\nF1 \u2191\nPRECISION \u2191\nRECALL \u2191\nACC. \u2191\nBERT (auto-labeled)\n0.714\n0.704\n0.716\n0.710\nBERT (human-labeled)\n0.753\n0.766\n0.789\n0.756\nBERT (auto + human-labeled)\n0.774\n0.783\n0.797\n0.771\nCLIP\n0.694\n0.695\n0.701\n0.712\nTIP-CLIP (auto-labeled)\n0.751\n0.763\n0.791\n0.748\nTIP-CLIP (human-labeled)\n0.810\n0.807\n0.815\n0.820\nTIP-CLIP (auto + human-labeled)\n0.865\n0.858\n0.873\n0.871\nTable 4: Ablation studies to understand the benefits of\ntwo-stage fine-tuning. The presented results are on the\nhuman-annotated test set of TIMED. Reported values\nare macro-averages of class-wise F1, precision, and\nrecall, and overall classification accuracy.\nfollowed by the fine-tuned BERT\u2019s attention scores.\nThe trends are consistent across both datasets. The\nrelative ordering of models in terms of the corre-\nlation of their attention scores with MRC image-\nability scores follows the same order as their per-\nformance on the test set of TIMED. However, all\ncorrelation scores are in the low range, indicating\na non-trivial relationship between sentence- and\nword-level imageability. The same trends hold for\npropagated visualness scores; see App. A.4. We\nalso analyze the reason behind higher correlation\nscores on MSCOCO with respect to the TIMED\ncorpus in Appendix A.4.\nEffect of multi-stage training: We conduct abla-\ntions to isolate the effect of two-stage training. In\nTable 4, we show that BERT and TIP-CLIP can\nlearn to distinguish visual and non-visual text\neven when fine-tuned only using the automatically\nlabeled data. However, for both models, the gains\nfrom fine-tuning only on smaller, human-labeled\ndata are notably higher.\nFurthermore, we find\nthe proposed two-stage fine-tuning (i.e., training\non automatically labeled data followed by human-\nlabeled data) to be most effective, leading to a gain\nof over 2 and 5 absolute F1 points over training\nonly on human-labeled data for BERT and TIP-\nCLIP models, respectively. Additionally, for a\ngiven training strategy, our proposed fine-tuning\nof TIP-CLIP demonstrates better performance than\nthe corresponding fine-tuned BERT model as well\nas the standard pre-trained CLIP model.\nEffect on text-to-image retrieval: We aim to ana-\nlyze the re-usability of learned embeddings by the\nTIP-CLIP model for the text-to-image retrieval task.\nTo this end, we consider the 515 visual examples\nfrom the test set of TIMED and, for each visual\nexample, we rank the 515 corresponding images\nbased on the cosine similarity between the image\nand text embeddings obtained from the TIP-CLIP\nmodel. We compute the Mean Reciprocal Rank\n(MRR) and contrast it with the MRR obtained us-\ning the pre-trained CLIP embeddings. As expected,\nCLIP achieves a near-perfect MRR of 0.989. The\nproposed fine-tuning objective does not severely\nimpact the reusability of embeddings obtained from\nTIP-CLIP for retrieval, and results in an MRR of\n0.937. This comparison evaluates the retrieval ca-\npabilities of TIP-CLIP against that of the CLIP\nmodel because the correspondence between visual\ntext and images was established using similarities\nbetween CLIP embeddings.5\nThe downside of an alternate training objec-\ntive: Recall that our fine-tuning strategy involves\nmatching visual text with its corresponding im-\nage and matching non-visual text with the NULL\nimage. With only the classification of visual and\nnon-visual text in mind, an alternate fine-tuning\nstrategy would have been to match all the visual\nexamples with one common image while match-\ning all the non-visual text with the common NULL\nimage. The major downside of this approach is\nthat while it leads to an effective classifier after\ntwo-stage fine-tuning, demonstrating a compara-\nble F1 score of 0.842 as the TIP-CLIP model, it\nperforms poorly on the text-to-image retrieval task\nwith an MRR of 0.014. Overall, while the alternate\nentirely classification-based training objective per-\nforms at par with the proposed TIP-CLIP model\non the classification task, the resultant embeddings\ndemonstrate poor reusability for downstream tasks\nlike text-to-image retrieval.\nProperties of the new embedding space: In\nFigure 3 we visualize the embedding space of\nthe learned embeddings using t-SNE (Van der\nMaaten and Hinton, 2008). Alongside visual and\nnon-visual sentences from the test set of TIMED,\n5To automatically establish a correspondence between\nvisual text and images, we enforce that the most similar\nimage for a text should exist on the same page of the PDF.\nTherefore, it is possible that the CLIP similarity of text may\nbe higher for a different image, resulting in an MRR slightly\nless than 1.0 (i.e., 0.989).\nNULL image\nImages for \nvisual text\nNon-visual text\nVisual text\n(a) CLIP embeddings\nNULL image\nImages for \nvisual text\nNon-visual text\nVisual text\n(b) TIP-CLIP embeddings\nNULL image 1\nNULL image 2\nImages for \nvisual text\nNon-visual text\nVisual text\n(c) Alt. formulation embeddings\nFigure 3: t-SNE visualization of embeddings learned by (a) CLIP, (b) TIP-CLIP \u2014 using contrastive and adapted\ncontrastive learning objective, respectively, & (c) model trained using alternative formulation solely focusing on\nclassification. The plotted data points are from the TIMED test set. The observed \u201cgap\u201d in image & text spaces has\nbeen studided by Liang et al. (2022).\nOriginal image\nInput text\nCLIP: I\u2019ll be ordering our christmas plants that are in\n 6 1 / 2 pots at a price of $ 5 . 0 0 each . \nTIP-CLIP: I\u2019ll be ordering our christmas plants that are in\n 6 1 / 2 pots at a price of $ 5 . 0 0 each .\nCLIP: the common loon , minnesota \u2018s state bird , usually nests\non islands or on shore lines of our northern lakes\nTIP-CLIP: the common loom , minnesota \u2018s state bird , usually nests\non islands or on shore lines of our northern lakes .\nCLIP: jim sent along the following images of these successful\nanglers and one of red drum they caught .\nTIP-CLIP: jim sent along the following images of these successful\nanglers and one of red drum they caught .\nCLIP attention TIP-CLIP attention\nOriginal image\nInput text\nCLIP attention TIP-CLIP attention\nCLIP: dog - friendly pubs are a key ingredient of the charm and \nuni que atmosphere in many places in nsw .\nTIP-CLIP: dog - friendly pubs are a key ingredient of the charm and\nuni que atmosphere in many places in nsw.\nFigure 4: Comparing the attention maps over input text and images for CLIP and TIP-CLIP. For text, a darker shade\nof green demonstrates greater attention by the model. For images, red demonstrates the greatest attention in the\nheatmap. Image best viewed with zoom.\nFigure 5: Examples of DALL-E generations for non-visual and visual text.\nwe also plot the embeddings of images correspond-\ning to the visual sentences, and the embedding(s)\nof the NULL image(s). First off, we observe that the\nembeddings in Figure 3a and 3b from CLIP and\nTIP-CLIP are different in that the TIP-CLIP em-\nbeddings demonstrate better distinguishability be-\ntween visual and non-visual text. In Figure 3c\nwe observe that the alternative formulation pushes\nthe NULL embeddings to the periphery of the image\nembeddings\u2019 cluster from a near-center location in\nFigures 3a and 3b. The text embeddings demon-\nstrate notable distinguishability in Figure 3c too.\nWe believe that the alternative classification-only\nformulation causes distortion in the latent space\nthat causes drastic modification of text-only embed-\ndings, making them useless for downstream text-to-\nimage retrieval, as demonstrated empirically earlier.\nHowever, our proposed objective in TIP-CLIP pre-\nserves reusability for downstream tasks by main-\ntaining semantic relevance between learned image\nand text embeddings.\n6.1\nQualitative Analysis\nIn this section, we conduct two qualitative analy-\nses: (i) contrasting the attention mechanisms for\nCLIP and TIP-CLIP, and (ii) the role of distinguish-\ning visual and non-visual text in downstream\ntext-to-image generation using systems like DALL-\nE (Ramesh et al., 2021).\nAttention map visualization: To contrast the\nmechanism by which CLIP and TIP-CLIP mod-\nels match input text with their corresponding im-\nage, we visualize and contrast the attention maps\nfor both models. We adopt the state-of-the-art ap-\nproach to explain multimodal Transformers (Chefer\net al., 2021). In Fig. 4 we show 4 illustrative\nvisual sentences from the test set of TIMED along\nwith their corresponding images. Focusing on text,\nwe observe that TIP-CLIP has a greater tendency\nto attend to visual aspects in the text; for instance,\nwords like \u2018islands,\u2019 \u2018lakes,\u2019 \u2018anglers\u2019 are attended\nto a greater extent by TIP-CLIP than CLIP. In im-\nages, we observe small changes in attention maps\nacross CLIP and TIP-CLIP; for instance, while the\nCLIP attention is focused on the Common Loon,\nTIP-CLIP also attends to the \u2018lake.\u2019 The qualitative\nanalysis of visualization maps reinforces that the\nmatching process for text and images undergoes\nsmall changes to accommodate greater attention to\nvisual aspects in the text.\nDownstream text-to-image generation: In Fig.\n5 we show the generations obtained using DALL-\nE for text that is categorized as non-visual and\nvisual in our dataset.\nWe observe that for\nnon-visual text, the images produced by DALL-\nE show poor relevance to the text. However, for\nvisual text the generated images demonstrate\ngreat relevance to the input text.\nTriggering text-to-image generation models like\nDALL-E for visual text is crucial to effectively use\nsuch systems in a passive setting. For instance,\nthe authors should only be recommended to add\nvisual assets in relevant places (i.e., for visual sen-\ntences) while working with long-form documents;\ntriggering image generations for non-visual sen-\ntences could cause sub-optimal experiences. Thus,\nour contributions focus on distinguishing visual\ntext from non-visual text as the necessary first step.\n7\nConclusion and Future Work\nWe propose the task of predicting the visualness\nof text and curate a human-annotated dataset of\nsentence-level visualness scores. Additionally, we\npropose a two-stage fine-tuning objective for the\ntask that involves training on a distantly supervised\ncorpus followed by a smaller human-annotated cor-\npus. Comparisons with several baselines demon-\nstrate the effectiveness of our approach in distin-\nguishing visual and non-visual text. We analyze the\nattention weights and downstream text-to-image re-\ntrieval capabilities of the model. Qualitative analy-\nsis of attention weights over textual input reinforces\nthat our model attends to visual words to a greater\nextent. In closing, we show qualitative examples\nof how predicting text visualness can make text-to-\nimage generation more effective.\nIn the future, we will study alternate objectives\nfor learning text visualness while ensuring that the\nlearned representations are transferable to related\ndownstream tasks. We are also interested in us-\ning measures relating to the quality of the images\ngenerated from text-to-image generation systems\nto decipher signals about the visualness of input\ntext, enabling the creation of auto-labeled exam-\nples. As the aggregation of word-level visualness\nscores leads to poor predictability of sentence-level\nvisualness, future work could aim to understand\nwhat linguistic factors (like compositionality) pre-\ncipitate sentence-level visualness.\n8\nLimitations and Broader Perspective\nLimitations:\nAs the first study on predicting\nsentence-level visualness, we focus on fine-tuning\nrepresentative vision-and-language (CLIP) and\nlanguage-only (BERT) encoders. Future studies\ncan extend our experiments to explore the bene-\nfits of using other encoders to model text visual-\nness. Our curated TIMED dataset only covers the\nEnglish language. The notion of visualness can\nvary across languages and we encourage future re-\nsearch to contrast visualness in the context of the\nEnglish language with that in other non-English\nlanguages. Additionally, since US-based crowd\nworkers provided our ground-truth annotations for\nvisualness, the dataset reflects a predominantly\nWestern-centric view of text visualness. It is un-\nclear how visualness in the text is perceived across\ndifferent cultures. To this end, we acknowledge that\nour work and artifacts reflect West-centric views of\nvisualness in the English language and encourage\ncross-lingual and cross-cultural extensions.\nBroader Social Impact,\nAnnotations,\nand\nDatasets: The authors do not foresee any nega-\ntive social impacts of this work. However, our\nmodel can inherit the known biases in underly-\ning models like CLIP and BERT (Agarwal et al.,\n2021; Garimella et al., 2021). The documents from\nwhich our datasets are curated are publicly avail-\nable and are mentioned in The Common Crawl\ncorpus (https://commoncrawl.org/); we abide\nby their terms of use. We manually anonymize\ninstances of PII in the sentences that are annotated\nusing Amazon Mechanical Turk and check for po-\ntentially offensive content. The recruited annota-\ntors are from the United States and are paid at an\nhourly rate of 12 USD.\nReferences\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec\nRadford, Jong Wook Kim, and Miles Brundage.\n2021. Evaluating CLIP: Towards Characterization of\nBroader Capabilities and Downstream Implications.\narXiv preprint arXiv:2108.02818.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. VQA: Visual question answering.\nIn Proceedings of the IEEE International Conference\non Computer Vision, pages 2425\u20132433.\nHila Chefer, Shir Gur, and Lior Wolf. 2021. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 397\u2013406.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision, pages 104\u2013120. Springer.\nMax Coltheart. 1981.\nThe MRC psycholinguistic\ndatabase. The Quarterly Journal of Experimental\nPsychology Section A, 33(4):497\u2013505.\nKoen Deschacht and Marie Francine Moens. 2007. Text\nanalysis for automatic image annotation. In Proceed-\nings of the 45th Annual Meeting of the Association\nof Computational Linguistics, pages 1000\u20131007.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nAparna Garimella, Akhash Amarnath, Kiran Kumar,\nAkash Pramod Yalla, N Anandhavelu, Niyati Chhaya,\nand Balaji Vasan Srinivasan. 2021. He is very intel-\nligent, she is very beautiful? on mitigating social\nbiases in language modelling and generation.\nIn\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4534\u20134545.\nJin-Woo Jeong, Xin-Jing Wang, and Dong-Ho Lee.\n2012. Towards measuring the visualness of a concept.\nIn Proceedings of the 21st ACM International Con-\nference on Information and Knowledge Management,\npages 2415\u20132418.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision.\nIn International Conference on\nMachine Learning, pages 4904\u20134916. PMLR.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using mechanical turk to evaluate\nopen-ended text generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1265\u20131285.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational Journal of Computer Vision, 123:32\u2013\n73.\nWeibin Li, Qiwei Zhong, Qingyang Zhao, Hongchun\nZhang, and Xiaonan Meng. 2021.\nMultimodal\nand contrastive learning for click fraud detection.\narXiv:2105.03567.\nVictor Weixin Liang, Yuhui Zhang, Yongchan Kwon,\nSerena Yeung, and James Y Zou. 2022. Mind the gap:\nUnderstanding the modality gap in multi-modal con-\ntrastive representation learning. Advances in Neural\nInformation Processing Systems, 35:17612\u201317625.\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael\nJordan. 2015. Learning transferable features with\ndeep adaptation networks. In International Confer-\nence on Machine Learning, pages 97\u2013105. PMLR.\nEdward Loper and Steven Bird. 2002. NLTK: the natu-\nral language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Compu-\ntational Linguistics-Volume 1, pages 63\u201370.\nAnnie Louis and Ani Nenkova. 2013. What makes\nwriting great? first experiments on article quality\nprediction in the science journalism domain. Trans-\nactions of the Association for Computational Linguis-\ntics, 1:341\u2013352.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in Neural Information Processing Systems,\n26.\nAnshul Mittal, Kunal Dahiya, Shreya Malani, Janani\nRamaswamy, Seba Kuruvilla, Jitendra Ajmera, Keng-\nhao Chang, Sumeet Agarwal, Purushottam Kar, and\nManik Varma. 2022. Multi-modal extreme classifi-\ncation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n12393\u201312402.\nAllan Paivio, John C Yuille, and Stephen A Madigan.\n1968. Concreteness, imagery, and meaningfulness\nvalues for 925 nouns. Journal of Experimental Psy-\nchology, 76(1p2):1.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1532\u20131543.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021.\nLearning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748\u20138763.\nPMLR.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022.\nHierarchical text-\nconditional image generation with clip latents. arXiv\npreprint: 2204.06125.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In International Conference on Machine\nLearning, pages 8821\u20138831. PMLR.\nShivaen Ramshetty, Gaurav Verma, and Srijan Kumar.\n2023. Cross-modal attribute insertions for assess-\ning the robustness of vision-and-language learning.\narXiv preprint arXiv:2306.11065.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n10684\u201310695.\nChitwan Saharia, William Chan, Saurabh Saxena,\nLala Li, Jay Whang, Emily L Denton, Kam-\nyar Ghasemipour, Raphael Gontijo Lopes, Burcu\nKaragol Ayan, Tim Salimans, et al. 2022. Photo-\nrealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 35:36479\u201336494.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita\nCucchiara. 2022. Retrieval-augmented transformer\nfor image captioning. In Proceedings of the 19th\nInternational Conference on Content-based Multime-\ndia Indexing, pages 1\u20137.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 2443\u20132449.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9(11).\nGaurav Verma, Vishwa Vinay, Ryan Rossi, and Sri-\njan Kumar. 2022. Robustness of fusion-based mul-\ntimodal classifiers to cross-modal content dilutions.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n360\u2013374.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. 2016. Show and tell: Lessons learned\nfrom the 2015 mscoco image captioning challenge.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39(4):652\u2013663.\nKaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang\nWang. 2016.\nA comprehensive survey on cross-\nmodal retrieval. arXiv preprint arXiv:1607.06215.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11\u201320.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages\n38\u201345.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual attention.\nIn International Conference on Machine Learning,\npages 2048\u20132057. PMLR.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? Advances in Neural Information\nProcessing Systems, 27.\nA\nAppendix\nA.1\nEffect of the NULL Image\nSince all the non-visual sentences in the training\ncorpus are mapped to a common NULL image, we\naim to see the effect of the chosen NULL image\non the results. Recall that the NULL image used\nfor our main experiments was obtained by creat-\ning an RGB image in which each pixel value is\nchosen randomly. We perform the same process\nwith a different random seed to generate another\nNULL image. Additionally, we use a natural image\nas another alternative for the NULL image. These\nimages are shown in Figure 6. We then evaluate\nthe resulting models on the human-annotated test\nset of TIMED. Table 5 shows that the performance\nof the models is not dependent on the choice of the\nNULL image. We also find no dependence between\nthe choice of the NULL image and the performance\non downstream text-to-image retrieval.\n(a) Original NULL image\n(b) NULL image with diff. seed\n(c) Natural NULL image\nFigure 6: Various NULL images used to study the effect of the chosen image on the text visualness identification task\nand the downstream text-to-image retrieval task.\nVARIANTS\nF1 \u2191\nPRECISION \u2191\nRECALL \u2191\nACC. \u2191\nMRR \u2191\nTIP-CLIP (Original \u2013 Fig. 6a)\n0.865\n0.858\n0.873\n0.871\n0.937\nTIP-CLIP (w/ diff. seed \u2013 Fig. 6b)\n0.867\n0.854\n0.875\n0.872\n0.934\nTIP-CLIP (natural image - Fig. 6c)\n0.861\n0.855\n0.876\n0.872\n0.939\nTable 5: Effect of the choice of the NULL image on categorizing the human-annotated test set of TIMED and\ndownstream text-to-image retrieval. Reported F1, Precision, and Recall values are macro-averages across the two\nclasses (visual and non-visual).\nCategory\nExample words (assigned score)\nHigh imageability\nmartini, crabmeat, teeth, oysters, mosquitos, bracelets, motorboat, dia-\nmonds, squirrels, cigarettes, beaches, trumpets, dolphin, caramel, cattle,\nportobello, libraries, chimpanzee, snorkeling, sailboat, harmonica\nMedium imageability\nreassure, militancy, inhumanly, catalyses, industrial, peacefulness, hand-\nwoven, neurosurgery, overwashed, whooper, snails, preeminence, recluse,\nentrepreneur, character, insufficient, paladin, impersonal, deviously, re-\ncover\nLow imageability\npolitologist, psycholinguistic, requirements, confirmatory, terseness, pre-\nformulation, offender, controversial, unhealable, monoculturalism, mis-\nerable, reprogrammability, this, participate, attractive, determinant, dises-\ntablishment\nTable 6: Qualitative examples of words that are assigned scores in the high (\u2265 0.7), medium (\u2208 (0.3, 0.7)), and\nlow (\u2264 0.3) range using the word2vec embedding-based propagation methodology.\nA.2\nAssessment of word-level imageability\nscore propagation\nWe randomly selected 500 words from the MRC\nlexicon and 500 words from the word2vec vocabu-\nlary that did not occur in the MRC lexicon. Each\nword was shown to 9 annotators using Amazon\nMechanical Turk to seek responses to the follow-\ning question: \u201cDo you agree that the word below\nevokes an image or picture in your mind?\u201d The\nannotators were instructed to respond on a 7-point\nLikert scale, where 1 denoted strong disagreement\nand 7 denoted strong agreement. Please see Ap-\npendix A.3 for details about the instructions, demo-\ngraphic filters, and compensation.\nWe average the ratings for all the annotated\nwords and normalized them to be \u2208 [0, 1]. We com-\npute the Pearson\u2019s correlation coefficient between\n(a) the average ratings for MRC words and the nor-\nmalized imageability scores, and (b) the average\nratings for word2vec words and the imageability\nscores assigned via embedding-based propagation.\nThe correlation between MRC imageability scores\nand average annotators\u2019 ratings is 0.870 (p <\n0.001) and the correlation between scores assigned\nvia our propagation method and average annota-\ntors\u2019 ratings is 0.735 (p < 0.001). This high posi-\ntive correlation coefficient between assigned image-\nability scores and human-perceived ratings demon-\nstrates the effectiveness of our adopted propagation\nmethod. We also note that the inter-annotator agree-\nments for the ratings for MRC words and word2vec\nwords, as computed using Krippendorf\u2019s \u03b1 (ordinal\nmeasure), were 0.626 and 0.584, respectively.\nOverall, this assessment illustrates the validity\nof propagating word-level imageability scores us-\ning embedding-based semantic similarities. More\nbroadly, the aim of adopting this approach is to\nexpand the coverage of MRC lexicon. Qualita-\ntively, we observe that words like \u2018gotcha\u2019 (0.33)\nand \u2018presbyterian\u2019 (0.61) are assigned meaning-\nful imageability scores, demonstrating expansion\nalong time and domains. As a point of difference\nbetween human ratings and assigned scores, we\nnotice that the propagation approach assigned a\nhigh imageability score to words like \u2018qawwali\u2019\n(0.60) while the human annotators did not, possi-\nbly due to a lack of sociocultural context. In Table\n6 we show illustrative words that are assigned high\n(\u2265 0.7), medium (\u2208 (0.3, 0.7)), and low (\u2264 0.3)\nimageability scores using our propagation method.\nA.3\nDetails about MTurk Experiments\nFor all our annotation tasks, we recruited annota-\ntors using Amazon Mechanical Turk. We set the\ncriteria to \u2018Master\u2019 annotators with at least a 99%\napproval rate and were located in the United States.\nTo further ensure the quality of annotations, we re-\nquired the annotators to have at least 5000 accepted\nannotations in the past. The rewards were set by\nassuming an hourly rate of 12 USD for all the anno-\ntators. We show the annotation interfaces in Figure\n7. In addition, the annotators were informed that\nthe aggregate statistics of their annotations would\nbe used and shared as part of academic research.\nWe also inserted some \u201cattention-check\u201d exam-\nples during the annotation tasks to ensure the an-\nnotators read the text carefully before responding.\nThis was done by asking the annotators to mark\na randomly chosen score on the Likert scale re-\ngardless of the actual content. We discard the an-\nnotations from annotators who did not correctly\nrespond to all the attention-check examples and\nre-collect annotations for the affected samples.\nA.4\nFurther analyses on the correlation\nbetween attention scores and word-level\nvisualness scores\nWe compute the Pearson\u2019s correlation coefficient\nbetween a model\u2019s average attention scores over\nwords and the visualness score assigned using our\npropagation method.\nHowever, unlike Table 3,\nthis time, we consider the propagated imageability\nscores which lead to broader coverage in terms of\nvocabulary. As seen in Table 7, we observe the\nsame trends as with MRC imageability scores, al-\nbeit with slightly lower values of correlation scores.\nTo analyze the alignment between learned atten-\ntion scores for various models, we compute the\ncorrelation between average attention scores across\ndifferent models. Pearson\u2019s correlation coefficients\nin Table 8 show that all the model attention scores\nhave a moderate correlation with each other.\nWhy are correlation scores higher for MSCOCO\nthan for TIMED?: An interesting trend across Ta-\nble 3 and 7 is that the correlation scores are consis-\ntently higher, across all the models under consid-\neration, for the MSCOCO dataset than the test set\nof TIMED. We note that, on average, MSCOCO\nhas a caption length of 11.4 whereas the TIMED\ndataset has an average sentence length of 20.6,\nwith a greater concentration of objects from the\nVisual Genome objects\u20146.7 (58.7%) objects per\nexample versus 8.4 (40.7%) objects per example).\nFor our TIP-CLIP model, these objects acquire an\naverage of 63.2% attention scores across all the\nMSCOCO examples, whereas they only acquire\n37.1% of attention scores, on average, across the\nexamples in the TIMED test set. Overall, these\nresults demonstrate that the TIP-CLIP model at-\ntends over words in the MSCOCO corpus in an\nobject-targeted manner but the attention is rela-\ntively diffused in the TIMED corpus. Combined\nwith the observation that MRC imageability scores\nare higher for concrete objects (Paivio et al., 1968),\nthis explains why the correlation scores are consis-\ntently higher on MSCOCO than on TIMED.\nMODELS\nMSCOCO\nTIMED\nBERT\n0.434***\n0.301***\nCLIP\n0.429***\n0.262***\nTIP-CLIP (Ours)\n0.465***\n0.338***\nTable 7: Pearson\u2019s correlation coefficient between prop-\nagated imageability scores (using word2vec) and model\nattention-scores. *** denotes p < 0.001\nEffect of length on the correlation between at-\ntention and MRC-I scores: We categorize the sen-\ntences in the test set of TIMED into short (\u2264 10;\nn = 304), medium (\u2208 (10, 20); n = 505), and\nlong (\u2265 20; n = 606) sentences based on word\ncounts. However, we did not find a notable vari-\nation in the correlation scores between the atten-\ntion weights of the TIP-CLIP model and MRC\nImageability scores. Pearson\u2019s correlation coeffi-\n(b) Interface to evaluate word-level visualness scores assigned by the propagation method\n(a) Interface to collect sentence-level visualness scores\nFigure 7: Interface for our annotation tasks on Amazon Mechanical Turk. For each of the annotations task, we also\nshow the instructions provided to the annotators.\nMODELS\nBERT\nCLIP\nTIP-CLIP\nBERT\n\u2014\n\u2013\n\u2013\nCLIP\n0.552***\n\u2013\n\u2013\nTIP-CLIP (Ours)\n0.631***\n0.571***\n\u2013\nTable 8: Pearson\u2019s correlation coefficient between word-\nlevel attention scores of various models for the TIMED\ntest set. *** denotes p < 0.001\nMODELS\nF1 \u2191\nPRECISION \u2191\nRECALL \u2191\nACC. \u2191\nRandom\n0.503\n0.503\n0.503\n0.505\nMRC-I\n0.470\n0.472\n0.472\n0.470\nVG-Objects\n0.536\n0.541\n0.539\n0.548\nMRC-I + w2v\n0.501\n0.502\n0.504\n0.502\nMRC-I + GloVe (Twitter)\n0.516\n0.518\n0.520\n0.519\nBERT\n0.612\n0.634\n0.624\n0.618\nCLIP\n0.644\n0.645\n0.645\n0.644\nTIP-CLIP (Ours)\n0.696\n0.693\n0.691\n0.694\nTable 9: Out of domain evaluation on the Twitter\ndataset.\nReported F1, Precision, and Recall values\nare macro-averages across the two classes (visual and\nnon-visual).\ncient was 0.33, 0.35, and 0.37 for short, medium,\nand long sentences, respectively. We observed the\nsame trend for the fine-tuned BERT model and the\npre-trained CLIP model.\nA.5\nOut-of-Domain Generalization\nRobustness of vision-language models has been\nthe subject of investigation in several prior\nworks (Verma et al., 2022; Ramshetty et al., 2023;\nLi et al., 2021). A critical assessment of the ro-\nbustness and generalizability of the models trained\nusing our proposed approach is to conduct evalu-\nations on out-of-domain (OOD) datasets. To this\nend, we curate a social media dataset by scraping\nTwitter. We start with the Wikipedia-based Image\nText Dataset (WIT) (Srinivasan et al., 2021) and\nquery Twitter using the Wikipedia page title to re-\ntrieve posts in English that are with and without\nimages. We require that the retrieved post con-\ntains the page title string to ensure topical simi-\nlarity between posts with and without images. To\nremove examples with irrelevant images, we dis-\ncard posts with a CLIP-similarity lower than 0.70\nbetween the Twitter post\u2019s image and the corre-\nsponding image on Wikipedia. Consequently, we\nobtain a dataset of Twitter posts containing men-\ntions of 1185 Wikipedia topics, 7844 Twitter posts\nwith images, and 7248 Twitter posts without im-\nages. The posts with and without images are tied\nby common Wikipedia topics.\nWe hypothesize that the text in Twitter posts that\nmention a certain topic and contain an image is\nmore visual than text in Twitter posts that men-\ntion the same topic and do not contain any images.\nTo test this hypothesis, we randomly sample 40\nWikipedia topics and present the associated text\nwith (n = 264) and without images (n = 241)\nto human annotators. In an AMT survey that fol-\nlows the design for curating TIMED, we find that\nthe average annotator rating for the text from Twit-\nter posts without images is 2.306 (\u00b11.369) while\nthat for text from Twitter posts with images is\n4.304 (\u00b11.273). We observe the inter-annotator\nagreement of 0.413, which is similar to that ob-\nserved while curating TIMED. For 34 out of the 40\nWikipedia topics, the annotators provided a higher\nimageability rating to text originally associated\nwith an image on Twitter than text not associated\nwith an image. Overall, the AMT survey validates\nour hypothesis by demonstrating that text in Twitter\nposts with images is perceived as more visual than\ntext in Twitter posts without images, modulo the\ntopic is common across the posts.\nWe now ask the question: how well the models\nconsidered in our work categorize Twitter text with\nimages as visual and Twitter text without images\nas non-visual? We first adapt the thresholds used\nto classify text using various methods by running\nan evaluation on a randomly sampled validation set\nof 100 Twitter examples, 50 from each category.\nThe thresholds are set as follows: MRC-I: 0.19;\nVG-Objects: 0.52; MRC-I + w2v: 0.17; MRC-I +\nGloVe: 0.326; CLIP: 0.87; TIP-CLIP: 0.74. Using\nthese threshold values, we categorize the rest of\nthe Twitter dataset (n = 14, 992) into visual and\nnon-visual categories. The random baseline uses\nuniform sampling.\nTable 9 shows the results for this out-of-domain\nevaluation. First, we note that all models undergo\na severe drop in performance on the OOD dataset,\nindicating that the notion of sentence-level image-\nability is strongly tied to the domain. Our proposed\nTIP-CLIP model demonstrates better OOD gener-\nalization capabilities than all the considered base-\nlines. It is noteworthy that the fine-tuned BERT\nmodel performs poorly on the OOD dataset than\nthe standard pre-trained CLIP model. The aggre-\ngation of word-level imageability scores provides\na worse-than-random estimate of sentence-level\nimageability on the OOD dataset.\nA.6\nPredictions on Ambiguous Sentences\nRecall that while curating TIMED, we combined\nexamples without a clear majority from the anno-\ntators (n = 378) and those with majority votes\nfor the \u2018Neutral\u2019 category (n = 2) into a single\ncategory called ambiguous. We revisit these exam-\nples to analyze how the most competitive baselines\n6Since we are operating with the Twitter domain, we de-\nsign a version of the propagation method where MRC Im-\nageability scores are propagated in the GloVe-embedding\nspace, where the GloVe embeddings are learned on Twitter cor-\npus (Pennington et al., 2014). We use 200-dimensional GloVe\nvectors trained on 2 billion Twitter posts with a vocabulary\nsize of 1.2 million.\n3\n2\n1\n0\n1\n2\n3\n4\n5\nstandardized 'visual' score\n0\n25\n50\n75\n100\n125\n150\n175\n200\nNumber of examples\nCLIP scores\nBERT scores\nTIP-CLIP scores\nFigure 8: Distribution of standardized visualness scores\nfor ambiguous examples (i.e., (v \u2212 \u00b5)/\u03c3, where v is\nthe original visualness score, \u00b5 and \u03c3 are the mean and\nstandard deviation of the distributions, respectively). We\ncontrast the predicted visualness scores by fine-tuned\nBERT, pre-trained CLIP, and our TIP-CLIP models.\nand our proposed TIP-CLIP model score them on\nimageability. We compute the imageability score\nusing Equation 2 for CLIP and TIP-CLIP, while\ntreating fine-tuned BERT\u2019s prediction probability\nscore as its imageability score for a given exam-\nple. To appropriately compare the distribution of\nimageability scores across these three models, we\nstandardize the values by computing z-scores (i.e.,\nxi is transformed into zi = (xi \u2212 \u00b5)/\u03c3; where xi\nis the original value, \u00b5 and \u03c3 are mean and stan-\ndard deviation of the distribution that xi belongs\nto). In Figure 8, we show that while CLIP and TIP-\nCLIP imageability scores are distributed normally\naround their respective means, BERT imageability\nscores are bimodal with peaks close to one stan-\ndard deviation away from their mean. This demon-\nstrates that if the models were to be used for scoring\ntext imageability, as opposed to categorizing text\ninto visual and non-visual categories, CLIP and\nTIP-CLIP models will provide more reasonable\nmiddle-level scores for ambiguous text, whereas\nscores from BERT would either be higher or lower.\nWe attribute this to how the underlying models are\ntrained and how the consequent imageability scores\nare computed. While the BERT model is trained\nsolely for the classification task that emphasizes\ndiscriminative encoding and the predicted proba-\nbility score is used as the imageability score, the\ndistribution is bimodal. However, CLIP and TIP-\nCLIP are trained using image-text matching (the\nformer, entirely; the latter, to some extent), and\nimageability scores are computed as the distance\nbetween the NULL image and input text.\n"
  },
  {
    "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild",
    "link": "https://arxiv.org/pdf/2305.11147.pdf",
    "upvote": "2",
    "text": "UniControl: A Unified Diffusion Model for\nControllable Visual Generation In the Wild\nCan Qin\u2020\u22c6, Shu Zhang\u2020, Ning Yu\u2020, Yihao Feng\u2020, Xinyi Yang\u2020, Yingbo Zhou\u2020, Huan Wang\u2020, Juan\nCarlos Niebles\u2020, Caiming Xiong\u2020, Silvio Savarese\u2020, Stefano Ermon\u2021, Yun Fu\u22c6, and Ran Xu\u2020\n\u2020Salesforce AI Research, \u22c6Northeastern University, \u2021Stanford Univeristy,\nqin.ca@northeastern.edu, ermon@cs.stanford.edu, yunfu@ece.neu.edu,\n{shu.zhang, ning.yu, yihaof, x.yang, yingbo.zhou, huan.wang, jniebles,\ncxiong, ssavarese, ran.xu}@salesforce.com\nAbstract\nAchieving machine autonomy and human control often represent divergent objec-\ntives in the design of interactive AI systems. Visual generative foundation models\nsuch as Stable Diffusion show promise in navigating these goals, especially when\nprompted with arbitrary languages. However, they often fall short in generating\nimages with spatial, structural, or geometric controls. The integration of such\ncontrols, which can accommodate various visual conditions in a single unified\nmodel, remains an unaddressed challenge. In response, we introduce UniControl ,\na new generative foundation model that consolidates a wide array of controllable\ncondition-to-image (C2I) tasks within a singular framework, while still allowing\nfor arbitrary language prompts. UniControl enables pixel-level-precise image\ngeneration, where visual conditions primarily influence the generated structures\nand language prompts guide the style and context. To equip UniControl with the\ncapacity to handle diverse visual conditions, we augment pretrained text-to-image\ndiffusion models and introduce a task-aware HyperNet to modulate the diffusion\nmodels, enabling the adaptation to different C2I tasks simultaneously. Trained on\nnine unique C2I tasks, UniControl demonstrates impressive zero-shot generation\nabilities with unseen visual conditions. Experimental results show that UniControl\noften surpasses the performance of single-task-controlled methods of compara-\nble model sizes. This control versatility positions UniControl as a significant\nadvancement in the realm of controllable visual generation. 1\n1\nIntroduction\nGenerative foundation models are revolutionizing the ways that humans and AI interact in natural\nlanguage processing (NLP) [1\u20136], computer vision (CV) [7\u201310], audio processing (AP) [11, 12], and\nrobotic controls [13\u201315], to name a few. In NLP, generative foundation models such as InstructGPT or\nGPT-4, achieve excellent performance on a wide range of tasks, e.g., question answering, summariza-\ntion, text generation, or machine translation within a single-unified model. Such multi-tasking ability\nis one of the most appealing characteristics of generative foundation models. Furthermore, generative\nfoundation models can also perform zero-shot or few-shot learning on unseen tasks [3, 16, 17].\nFor generative models in vision domains [9, 18\u201320], such multi-tasking ability is less clear. Stable\nDiffusion Model (SDM) [9] has established itself as the major cornerstone for text-conditioned image\ngeneration. However, while text descriptions provide a very flexible way to control the generated\nimages, their ability to provide pixel-level precision for spatial, structural, or geometric controls is\n1Code: https://github.com/salesforce/UniControl\nSalesforce AI Research, USA.\narXiv:2305.11147v3  [cs.CV]  2 Nov 2023\n\u201cChocolate mousee cake\u201d\nHED-to-Image\n\u201cThree zebras grazing in a grassy area near shrubs\u201d \nDepth-to-Image\n\u201cOne tray of rice and a tray of fruits and veggies.\u201d \nImage Outpainting\n\u201cAwesome 3d wall panels and interior wall paneling ideas for Living room 3d tiles\u201d \nSeg-to-Image\n\u201cCasual Short Hairstyles For Women\u201d\nNormal-to-Image\n\u201cA man with glasses and wearing a tie poses for a profile picture\u201d\nBBox-to-Image\n\u201cA man in a Rays uniform is throwing a baseball \u201c\nSkeleton-to-Image\n\u201cBreath of the Wild compressed cover art\u201d \nCanny-to-Image\nPre-training Tasks\nInput Condition\nOurs Result \nOurs Result \nUniControl Diffusion Model\nCanny Edge \nto Image\nHED Edge to \nImage\nBounding Box \nto Image\nSegmentation \nto Image\nNormal Map \nto Image\nDepth Map \nto Image\nSkeleton to \nImage\nImage \nOutpainting\nDepth Map + \nSkeleton to Image\nSegmentation + \nSkeleton to Image\nGray Image Colorization\nImage Deblur\nImage Inpainting\n\u2026\n\u2026\n\u2026\nPre-training Tasks\nZero-shot Tasks\nZero-shot Combination\nZero-shot Task: Image Deblurring\n\u201cArches National Park, Three Gossips, With Milky Way In The Background\u201d\nZero-shot Task: Depth-and-Skeleton-to-Image \n\u201cBackground: White House, Foreground: A group of women sitting on a bench\u201d\nZero-shot Task: Image Colorization\n\u201cA cave on a desert planet\u201d \nZero-shot Task: Seg-and-Skeleton-to-Image \n\u201cBackground: Old City of Tzfat, Foreground: A set of twins who are taking a bath\u201d\nFigure 1: UniControl is trained with multiple tasks with a unified model, and it further demonstrates\npromising capability in zero-shot tasks generalization with visual example results shown above.\noften inadequate. A recent work, ControlNet [21], was proposed to augment SDM to enable visual\nconditions (e.g., edge maps, depth maps). With the additional visual conditions, ControlNet can\nachieve explicit spatial, structural, or geometric control over generated structures, without losing the\nsemantic control from textual captions. Unfortunately, unlike language prompts that a unified module\nsuch as CLIP [22] can handle, each ControlNet model can only handle a specific control modality\nthat it was trained on (e.g., edge map). Retraining a separate model is necessary to handle a different\nmodality of visual conditions, incurring non-trivial time and spatial complexity costs.\nTo overcome the limitation of previous works, we present UniControl, a unified diffusion model\nfor controllable visual generation in the wild, which is capable of simultaneously handling both\n2\nlanguage and various visual conditions. Naturally, UniControl can perform multi-tasking and can\nencode visual conditions from different tasks into a universal representation space, seeking a common\nrepresentation structure among tasks. The unified design of UniControl allows us to enjoy the\nadvantages of improved training and inference efficiency, as well as enhanced controllable generation.\nOn the one hand, the model size of UniControl does not significantly increase as the number of\ntasks scales up. On the other hand, UniControl derives advantages from the inherent connections\nbetween different visual conditions [e.g., 23\u201325]. These relationships, such as depth and segmentation\nmapping, leverage shared geometric information to enhance the controllable generation quality.\nThe unified controllable generation ability of UniControl relies on two novel designed modules, a\nmixture of expert (MOE)-style adapter and a task-aware HyperNet [26, 27]. The MOE-style adapter\ncan learn necessary low-level feature maps from various visual conditions, allowing UniControl to\ncapture unique information from different visual conditions. The task-aware HyperNet, which takes\nthe task instruction as natural language prompt inputs, and outputs a task-aware embedding. The\noutput embeddings can be incorporated to modulate ControlNet [21] for task-aware visual condition\ncontrols, where each task corresponds to a particular format of visual condition. As a result, the\ntask-aware HyperNet allows UniControl to learn meta-knowledge across various tasks, and obtain\nabilities to generalize to unseen tasks. As Tab. 1, UniControl has significantly compressed the model\nsize compared with its direct baseline, i.e., Multi-ControlNet, by unifying nine tasks into ONE model.\nTable 1: Architecture and Model Size (#Params): UniControl vs. Multi-ControlNet\nStable Diffusion\nControlNet\nMoE-Adapter\nTaskHyperNet\nTotal\nUniControl\n1065.7M\n361M\n0.06M\n12.7M\n1.44B\nMulti-ControlNet\n1065.7M\n361M \u00d7 9\n-\n-\n4.32B\nTo obtain multi-tasking and zero-shot learning abilities, we pre-train UniControl on nine distinct\ntasks across five categories: 1) edges (Canny, HED, User Sketch); 2) region-wise maps (Segmentation\nMaps, Bounding Boxes); 3) skeletons (Human Pose Skeletons); 4) geometric maps Depth, Surface\nNormal); 5) editing (Image Outpainting). We build MultiGen-20M dataset, comprising over 20\nmillion high-quality triplets of original images, language prompts, and visual conditions for all the\ntasks. Then UniControl is trained for over 5,000 GPU hours on NVIDIA A100-40G hardware that is\ncomparable with the overall training cost of different ControlNets. Moreover, UniControl exhibits a\nremarkable capacity for zero-shot adaptation to new tasks, highlighting its potential for deployment\nin real-world applications. Our contributions are summarized below:\n\u2022\nWe present UniControl, a unified model capable of handling various visual conditions for the\ncontrollable visual generation.\n\u2022 We collect a new dataset for multi-condition visual generation with more than 20 million image-\ntext-condition triplets over nine distinct tasks across five categories.\n\u2022 We conduct extensive experiments to demonstrate that the unified model UniControl outperforms\neach single-task controlled image generation, thanks to learning the intrinsic relationships between\ndifferent visual conditions.\n\u2022\nUniControl shows the ability to adapt to unseen tasks in a zero-shot manner, highlighting its\nversatility and potential for widespread adoption in the wild.\n2\nRelated Works\nDiffusion-based Generative Models.\nDiffusion models were initially introduced in [28] that yield\nfavorable outcomes for generating images [18, 21]. Improvements have been made through various\ntraining and sampling techniques such as score-based diffusion [29, 30], Denoising Diffusion Proba-\nbilistic Model (DDPM) [31], and Denoising Diffusion Implicit Model (DDIM) [32], When training\nU-Net denoisers [33] with high-resolution images, researchers involve speed-up techniques including\npyramids [34], multiple stages [20], or latent representations [9]. In particular, UniControl leverages\nStable Diffusion Models (SDM) [9] as the base model to perform multi-tasking.\nText-to-Image Diffusion.\nDiffusion models emerge to set up a cutting-edge performance in\ntext-to-image generation tasks [20, 19], by cross-attending U-Net denoiser in diffusion generators\nwith CLIP [22] or T5-pretrained [2] text embeddings. GLIDE [35] is another example of a text-\nguided diffusion model that supports image generation and editing. UniControl and closely related\n3\nControlNet [21] are both built upon previous works on diffusion-based text-to-image generation [9].\n[36] introduces the compositional conditions to guide visual generation.\nImage-to-Image Translation.\nImage-to-image (I2I) translation task was initially proposed in\nPix2Pix [37], focusing on learning a mapping between images in different domains. Recently,\ndiffusion-based approaches [38, 39, 21] set up the new state of the art results. Recent diffusion-\nbased image editing methods show outstanding performances without requiring paired data, e.g.,\nSDEdit [40], prompt-to-prompt [41], Edict [42]. Other image editing examples include various\ndiffusion bridges and flows [43\u201347], classifier guidance [30] based methods for colorization, super-\nresolution [34], inpainting [48], and etc. ControlNet [21] takes both visual and text conditions and\nachieves new state-of-the-art controllable image generation. Our proposed UniControl unifies various\nvisual conditions of ControlNet, and is capable of performing zero-shot learning on newly unseen\ntasks. Concurrently, Prompt Diffusion [49] introduces visual prompt [50] from image inpainting to\ncontrollable diffusion models, which requires two additional image pairs as the in-context example\nfor both training and inference. By contrast, UniControl takes only a single visual condition while\nstill capable of both multi-tasking and zero-shot learning.\n3\nUniControl\nIn this section, we describe the training and the model design of our unified controllable diffusion\nmodel UniControl. Specifically, we first provide the problem setup and training objectives in Sec. 3.1,\nand then show the novel network design of UniControl in Sec. 3.2. Finally, we explain how to perform\nzero-shot image generation with the trained UniControl in Sec. 3.3.\n3.1\nTraining Setup\nDifferent from the previous generative models such as Stable Diffusion Models (SDM) [9] or\nControlNet [21], where the image generation conditions are single language prompt, or single type of\nvisual condition such as canny, UniControl is required to take a wide range of visual conditions from\ndifferent tasks, as well as the language prompt.\nTo achieve this, we reformulate the training conditions and target pairs for UniControl. Specifically,\nsuppose we have a dataset consisting of K tasks : D := {D1 \u222a \u00b7 \u00b7 \u00b7 \u222a DK}, and for each task training\nset Dk, denote the training pairs by ([ctext, ctask], Ic,xxx), with ctask being the task instruction that\nindicates the task type, ctext being the language prompt describing the target image, Ic being the\nvisual conditions, and xxx being the target image. With the additional task instruction, UniControl can\ndifferentiate visual conditions from different tasks. A concrete training example pair is the following:\nTask-Aware Vision-Language Condition\n\u201cCamp on a mountain top: Birthday Presents, \nAdventure, Outdoor, Mountain Camps, Great \nView, Places, Hiking, Mornings Lights, \nHimalayan Sunri\u201d\n\u201cCanny Edge to Image\u201d\nVisual Condition        :\nLanguage Prompt               :\nTask Instruction               :\nTarget output\nwhere the task is to translate the canny edge to real images following language prompt. With the\ninduced training pairs (xxx, [ctask, ctext], Ic), we define the training loss for task k following LDM [9]:\n\u2113k(\u03b8) := Ez,\u03b5,t,ctask,ctext,Ic\n\u0002\n\u2225\u03b5 \u2212 \u03b5\u03b8(zt, t, ctask, ctext, Ic)\u22252\n2\n\u0003\n, with ([ctask, ctext], Ic,xxx) \u223c Dk ,\nwhere t represents the time step, zt is the noise-corrupted latent tensor at time step t, z0 = E(xxx),\nand \u03b8 is the trainable parameters of UniControl . We also apply classifier-free guidance [51] to\nrandomly drop 30% text prompts to enhance the controllability of input visual conditions. We train\nUniControl uniformly on the K tasks. To be more specific, we first randomly select a task k and\nsample a mini-match from Dk, and optimize \u03b8 with the calculated loss \u2113k(\u03b8).\n3.2\nModel Design\nSince our unified model UniControl needs to achieve superior performance on a set of diverse tasks, it\nis necessary to ensure the network design enjoys the following properties: 1) The model can overcome\n4\nSD Middle \n(Trainable \nCopy)\nModulated Zero Conv\nModulated Zero Conv\nMOE Adapter\nSD Encoder \n(Trainable\nCopy)\nTask \nInstruction\nText & \nTime\nModulated Zero Conv\nSD Middle\nSD Decoder\nSD Block\nTrainable \nCopy of \nSD Block\nx\nCondition\n                   *\nZero Convolution\nTask Embedding\n                   *\nZero Convolution\nTask Embedding\ny\nModulated Zero Conv\nTask-Aware Vision-Language Condition\nInput one or more visual conditions\n* N\n* N\nRandom Noise\nGenerated \nImages\n\u201ca young child looking at \na birthday cupcake\u201d\nUniControl Model Overview\nSD Encoder\nFeature \nMap\nMOE Adapter\nConv\nCon\nv\nCon\nv\nConv\n...\n\u2026\n\u2026\nTask Instruction: \"normal \nsurface to image\"\nTask Embedding \nTask-Aware \nHyperNet\nTrainable\nWeights\nCLIP Text Encoder\nBase Hypernet\nParallel Linear Layers\nLinear Layer\nTask-Aware \nHyperNet\nFigure 2: This figure shows our proposed UniControl method. To accommodate diverse tasks, we\u2019ve\ndesigned a Mixture of Experts (MOE) Adapter, containing roughly \u223c70K #params for each task, and\na Task-aware HyperNet (\u223c12M #params) to modulate N (i.e., 7) zero-conv layers. This structure\nallows for multi-task functionality within a singular model, significantly reducing the model size\ncompared to an equivalent stack of single-task models, each with around 1.4B #params.\nthe misalignment of low-level features from different tasks; 2) The model can learn meta-knowledge\nacross tasks, and adapt to each task effectively.\nThe first property can ensure that UniControl can learn necessary and unique information from all\ntasks. For instance, if UniControl takes the segmentation map as the visual condition, the model\nmight ignore the 3D information. As a result, the feature map learned may not be suitable for the task\nthat takes the depth map images as visual condition. The second property would allow the model to\nlearn the shared knowledge across tasks, as well as the differences among them.\nWe introduce two novel designed modules, MOE-style adapter and task-aware HyperNet, that allows\nUniControl enjoys the above two properties. An overview of the model design for UniControl is in\nFig. 2. We describe the detailed designs of these modules below.\nMOE-Style Adapter.\nInspired by the design of Mixture-of-Experts (MOEs) [52], we devise a\ngroup of convolution modules to serve as the adapter for UniControl to capture features of various\nlow-level visual conditions. Precisely, the designed adapter module can be expressed as\nFAdapter(Ik\nc ) :=\nK\nX\ni=1\n1(i == k) \u00b7 F(i)\nCov1 \u25e6 F(i)\nCov2(Ik\nc ) ,\nwhere 1(\u00b7) is the indicator function, Ik\nc is the conditioned image from task k, and F(i)\nCov1, F(i)\nCov2\nare the convolution layers of the i-th module of the adapter. We remove the weights of the original\nMOEs since our designed adapter is required to differentiate various visual conditions. Meanwhile,\nnaive MOE modules can not explicitly distinguish different visual conditions when the weights are\nlearnable. Moreover, such task-specific MOE adapters facilitate the zero-shot tasks with explicit\nretrieval of the adapters of highly related pre-training tasks. Besides, the number of parameters for\neach convolution module is approximately 70K, which is computationally efficient.\n5\nFigure 3: Illustration of MOE\u2019s behaviors under zero-shot scenarios. The left part shows the capacity\nof the MOE to generalize to hybrid task conditions, achieved through the integration of outputs from\ntwo pertinent convolution layers. The right part illustrates the ability of the MOE-style adapter to\ngeneralize to unseen tasks, facilitated by the aggregation of pre-trained tasks using estimated weights.\nTask-Aware HyperNet.\nThe task-aware HyperNet modulates the zero-convolution modules of\nControlNet [21] with the task instruction condition ctask. As shown in Figure 2, our hyperNet first\nprojects the task instruction ctask into task embedding with the help of CLIPText encoder. Then\nsimilar in spirit of style modulation in StyleGAN2 [53], we inject the task embedding into the\ntrainable copy of ControlNet, by multiplying the task embedding to each zero-conv layer. In specific,\nthe length of the embedding is the same as the number of input channels of the zero-conv layer,\nand each element scalar in the embedding is multiplied to the convolution kernel per input channel.\nWe also show that our newly designed task-aware HyperNet can also efficiently learn from training\ninstances and task supervision following a similar analysis as in ControlNet [21].\n3.3\nTask Generalization Ability\nWith the comprehensive pretraining on the MultiGen-20M dataset, UniControl exhibits zero-shot\ncapabilities on tasks that were not encountered during its training, suggesting that Unicontrol possesses\nthe ability to transcend in-domain distributions for broader generalization. We demonstrate the zero-\nshot ability of UniControl in the following two scenarios:\nHybrid Tasks Generalization.\nAs shown in the left side of Fig. 3, We consider two different\nvisual conditions as the input of UniControl, a hybrid combination of segmentation maps and human\nskeletons, and augment specific keywords \"background\" and \"foreground\" into the text prompts.\nBesides, we rewrite the hybrid task instruction as a blend of instructions of the combined two tasks\nsuch as \"segmentation map and human skeleton to image\".\nZero-Shot New Tasks Generalization.\nAs shown in the right side of Fig. 3, UniControl needs to\ngenerate controllable images on a newly unseen visual condition. To achieve this, estimating the\ntask weights based on the relationship between unseen and seen pre-trained tasks is essential. The\ntask weights can be estimated by either manual assignment or calculating the similarity score of task\ninstructions in the embedding space. The example result in Fig. 5 (d) is generated by our manually\nassigned MOE weights as \u201cdepth: 0.6, seg: 0.3, canny: 0.1\u201d for colorization. The MOE-style adapter\ncan be linearly assembled with the estimated task weights to extract shallow features from the newly\nunseen visual condition.\n4\nExperiments\nWe empirically evaluate the effectiveness and robustness of UniControl. We conduct a series of\ncomprehensive experiments across various conditions and tasks, utilizing diverse datasets to challenge\nthe model\u2019s adaptability and versatility. Experimental setup, methodologies, and results analysis are\nprovided in the subsequent sections.\n4.1\nExperiment Setup\nImplementation.\nThe UniControl is illustrated as Fig. 2 with Stable Diffusion, ControlNet, MOE\nAdapter, and Task-aware HyperNet consisting \u223c1.5B parameters. MOE Adapter consists of parallel\nconvolutional modules, each of which corresponds to one task. The task-aware HyperNet inputs\nthe CLIP text embedding [22] of task instructions and outputs the task embeddings to modulate\nthe weights of zero-conv kernels. We implement our model upon the ControlNet . We take the\nAdamW [54] as the optimizer based on PyTorch Lightning [55]. The learning rate is assigned as\n1\u00d710\u22125. Our full-version UniControl model is trained on 16 Nvidia-A100 GPUs with the batch size\nof 4, requiring \u223c 5, 000 GPU hours. We have also applied Safety-Checker as safeguards of results.\n6\nCanny Edge to Image\nHuman Skeleton to Image\nHED to Image\nObject Bbox to Image\nNormal Surface to Image\nImage Outpainting\nSegmentation to Image\nDepth Map to Image\nCondition\nSingle-task ControlNet\nOurs-Multi\nCondition\nSingle-task ControlNet\nOurs-Multi\nA woman sitting on a bench near a statue, checking her phone. \nHotel room scene, efficiency, queen bed, sofa, table/desk, TV, in brown tone room. \nA sink in a peninsula in a kitchen. \nCommander Alan Poindexter in the cupola, 2010 \nA guy is having fun skiing down the slope of the hill. \nSunrise engagement \nA brown bear walking in its zoo enclosure \nArt Decoration For Bedroom: Yellow Bedroom Walls Of Yellow Bedroom Ideas \nFigure 4: Visual comparison between official or re-implemented task-specific ControlNet and our\nproposed model. The example data is collected from our testing set sampled from COCO and Laion.\nData Collection.\nSince the training set of ControlNet is currently unavailable, we initiate our\nown data collection process from scratch and name it as MultiGen-20M. We use a subset of Laion-\nAesthetics-V2 [56] with aesthetics ratings over six, excluding low-resolution images smaller than\n512. This yields approximately 2.8 million image-text pairs. Subsequently, we process this dataset\nfor nine distinct tasks across five categories (edges, regions, skeletons, geometric maps, real images):\n\u2022 Canny (2.8M): Utilize the Canny edge detector [57] with randomized thresholds.\n\u2022 HED (2.8M): Deploy the Holistically-nested edge detection [58] for robust boundary determination.\n\u2022 Depth (2.8M): Employ the Midas [59] for monocular depth estimation.\n\u2022 Normal (2.8M): Use the depth estimation results from the depth task to estimate scene or object\nsurface normals.\n\u2022 Segmentation (2.8M): Implement the Uniformer [60] model, pre-trained on the ADE20K [61]\ndataset, to generate segmentation maps across 150 classes.\n\u2022 Object Bounding Box (874K): Utilize YOLO V4 [62] pre-trained on the COCO [63] dataset for\nbounding box labelling across 80 object classes.\n\u2022 Human Skeleton (1.3M): Employ the pre-trained Openpose [64] model to generate human\nskeleton labels from source images.\n\u2022 Image Outpainting (2.8M): Create boundary masks for source images with random masking\npercentages from 20% to 80%.\nFurther processings are carried out on HED maps using Gaussian filtering and binary thresholding to\nsimulate user sketching. Overall, we amass over 20 million image-prompt-condition triplets. Task\ninstructions were naturally derived from the respective conditions, with each task corresponding\nto a specific instruction, such as \"canny edge to image\" for the canny task. We maintain a one-to-\none correspondence between tasks and instructions without introducing variance to ensure stability\nduring training. We have additionally collected a testing dataset for evaluation with 100-300 image-\ncondition-prompt triplets for each task. The source data is collected from Laion and COCO. We will\nopen-source our training and testing data to contribute to the community.\nBenchmark Models. The most straightforward comparison for UniControl comes from task-specific\nControlNet models. Six tasks overlap with those presented in ControlNet, so their official models\nare chosen as baselines for these tasks. For fair comparison, we re-implement the ControlNet model\n(single task) using our collected data. Our unified multi-task UniControl is compared against these\n7\n(c) Zero-Shot Task: Image Deblurring Example Result\nOutput\nInput\nOutput\n(d) Zero-Shot Task: Image Colorization Example Result\n(e) Zero-Shot Task: Image Inpainting Example Result\nInput\nOutput\n\u201cOnce Upon A Time: S03E19, 720p, \u201dkiss them goodbye\u201d\u201d\n\u201cWedding hair and makeup with warm tones and red lips\u201d \n\u201cDaniel Dae Kim smiles at the camera\u201d\nInput\nOutput\n\u201cBackground: Photograph, Toronto Wet, Foreground: Some girls in \ncolorful shirts standing by some pastries\u201d\n\u201cBackground: Old City of Tzfat, Foreground: A set of twins who are taking a bath\u201d\nCondition 1: Depth\nCondition 2: Human \nSkeleton\nOurs Result\nCondition 1: \nSegmentation\nCondition 2: Human \nSkeleton\nOurs Result\n(a) Zero Shot Combination: Depth and Human Skeleton to Image\n(b) Zero Shot Combination: Segmentation and Human Skeleton to Image\nFigure 5: (a)-(b): Example results of UniControl over hybrid (unseen combination) conditions\nwith key words \"background\" and \"foreground\" attached in prompts. (c)-(e): Example results of\nUniControl on three unseen tasks (deblurring, colorization, inpainting).\nFigure 6: User study between our method and official ControlNet checkpoints on six tasks. Our\nmethod outperforms ControlNet on all tasks.\ntask-aware models for each task. We apply default sampler as DDIM [32] with guidance weight 9\nand steps 50. All single-task models used for comparison are trained by 100K iterations and our\nmulti-task model is trained around 900K with similar iterations for each task to ensure fairness. The\nefficiency and compact design of our proposed model are evident in its construction. The total size\nof UniControl is around 1.5B #params and a single task ControlNet+SDM takes 1.4B. In order to\nachieve the same nine-task functionality, a single-task strategy would require the ensemble of a SDM\nwith nine task-specific ControlNet models, amounting to approximately 4.3B #params in total.\n4.2\nVisual Comparison\nWe visually compare different tasks (Canny, HED, Depth, Normal, Segmentation, Openpose, Bound-\ning Box, and Outpainting) in Fig. 4. Our method consistently outperforms the baseline ControlNet\nmodel. This superiority is in terms of both visual quality and alignment with conditions or prompts.\nFor the Canny task, the results generated by our model exhibit a higher degree of detail preservation\nand visual consistency. The outputs of UniControl maintain a faithful reproduction of the edge\ninformation (i.e., round table) compared to ControlNet. In the HED task, our model effectively\ncaptures the robust boundaries, leading to visually appealing images with clear and sharp edge\ntransitions, whereas ControlNet results appear to be non-factual. Moreover, our model demonstrate\na more subtle understanding of 3D geometrical guidance of depth maps and surface normals than\nControlNet. The depth map conditions produce visibly more accurate outputs. In the Normal task, our\nmodel faithfully reproduces the normal surface information (i.e., ski pole), leading to more realistic\n8\nFigure 7: User study between our multi-task model (Ours-multi) and single task model (Ours-single)\non eight tasks. Our method outperforms baselines on most of tasks, and achieves big performance\ngains on tasks of seg-to-image and outpainting-to-image. Moreover, the p-value of voting Ours-multi\nin all cases is computed as 0.0028 that is statistically significant according to the criteria of <0.05.\nand visually superior outputs. During the Segmentation, Openpose, and Object Bounding Box tasks,\nthe produced images generated by our model are better aligned with the given conditions than that\nby ControlNet, ensuring a higher fidelity to the input prompts. For example, the re-implemented\nControlNet-BBox misunderstands \u201ca woman near a statue\u201d, whereas our outputs exhibit a high\ndegree of accuracy and detail. In the Outpainting task, our model demonstrates its superiority by\ngenerating reasonable images with smooth transitions and natural-looking textures. It outperforms\nthe ControlNet model, which produces less coherent results - \u201ca bear missing one leg\u201d. This visual\ncomparison underscores the strength and versatility of our approach across a diverse set of tasks.\n4.3\nQuantitative Evaluation\nUser Study.\nWe compare the performance of our method with both the released ControlNet\nmodel and the re-implemented single-task ControlNet on our training set. As shown in Fig. 6, our\napproach consistently outperforms the alternatives in all cases. In the HED-to-image generation\ntask, our method significantly surpasses ControlNet. This superiority is even more pronounced in\nthe depth and normal surface to image generation tasks, where users overwhelmingly favor our\nmethod, demonstrating its ability to handle complex geometric interpretations. When compared to the\nre-implemented single-task model, Fig. 7 reveals that our approach maintains a smaller advantage, yet\nit still demonstrates its benefits by effectively discerning image regions to guide content generation.\nEven in the challenging outpainting task, our model outperforms the baseline, highlighting its\nrobustness and capacity to generalize.\nTable 2: Image Perceptual Distance\nCanny \u2193\nHED \u2193\nNormal \u2193\nDepth \u2193\nPose \u2193\nSegmentation \u2193\nUniControl\n0.546\n0.466\n0.623\n0.654\n0.741\n0.693\nControlNet\n0.577\n0.582\n0.778\n0.700\n0.747\n0.693\nImage Perceptual Met-\nric.\nWe evaluate the dis-\ntance between our output\nand the ground truth im-\nage. As we aim to obtain\na structural similar image to the ground truth image, we adopt the perceptual metric in [65], where a\nlower value indicates more similar images. As shown in Tab. 2, UniControl outperforms ControlNet\non five tasks, and obtains the same image distance to ControlNet on Segmentation.\nFr\u00e9chet Inception Distance (FID). We\u2019ve further conducted quantitative analysis with FID [66]\nto include more classic single-task-controlled methods such as GLIGEN [67] and T2I-adapter [68].\nWith a collection of over 2,000 test samples sourced from Laion and COCO, we\u2019ve assessed a wide\nrange of tasks covering edges (Canny, HED), regions (Seg), skeletons (Pose), and geometric maps\n(Depth, Normal). The Tab. 3 demonstrates that our UniControl consistently surpasses the baseline\nmethods across the majority of tasks. Notably, UniControl achieves this while maintaining a more\ncompact and efficient architecture than its counterparts.\nAblation Study. We\u2019ve conducted an ablation study, specifically focusing on the MoE-Style Adapter\nand TaskHyperNet in Tab. 4 with FID scores reported as the previous part. It is noticeable that the\nfull-version UniControl (MoE-Style Adapter + TaskHyperNet) significantly outperforms the ablations\nwhich demonstrates the superiority of proposed MoE-Style Adapter and TaskHyperNet.\n9\nTable 3: Quantitative Comparison (FID)\nCanny \u2193\nHED \u2193\nDepth \u2193\nNormal \u2193\nSeg \u2193\nPose \u2193\nGLIGEN [67]\n24.9\n27.8\n25.8\n27.7\n-\n-\nT2I-Adapter [68]\n23.6\n-\n25.4\n-\n27.1\n28.9\nControlNet [21]\n22.7\n25.1\n25.5\n28.4\n26.7\n28.8\nUniControl\n22.9\n23.6\n21.3\n23.4\n25.5\n27.4\nTable 4: Ablation Study (FID)\nMoE-Adapter\nTaskHyperNet\nCanny \u2193\nHED \u2193\nDepth \u2193\nNormal \u2193\nSeg \u2193\nPose \u2193\nAvg \u2193\n%\n%\n27.2\n29.0\n27.6\n28.8\n29.1\n30.2\n28.7\n!\n%\n24.5\n26.1\n23.7\n24.8\n26.9\n28.3\n25.7\n!\n!\n22.9\n23.6\n21.3\n23.4\n25.5\n27.4\n24.0\n4.4\nZero-shot Generalization\nWe further showcase the surprising capabilities of our method to undertake the zero-shot challenge of\nhybrid conditions combination and unseen tasks generalization.\nHybrid Tasks Combination.\nThis involves generating results from two distinct conditions si-\nmultaneously. Our model\u2019s zero-shot ability is tested with combinations such as depth and human\nskeleton or segmentation map and human skeleton. The results are shown in Fig. 5 (a)-(b). When the\nbackground is conditioned on a depth map, the model effectively portrays the intricate 3D structure of\nthe scene, while maintaining the skeletal structure of the human subject. Similarly, when the model is\npresented with a combination of a segmentation map and human skeleton, the output skillfully retains\nthe structural details of the subject, while adhering to the segmentation boundaries. These examples\nillustrate our model\u2019s adaptability and robustness, highlighting its ability to handle complex hybrid\ntasks without any prior explicit training.\nUnseen Tasks Generalization.\nTo evaluate the zero-shot ability to generalize to unseen tasks such\nas gray image colorization, image deblurring, and image inpainting, we conduct the case analysis\nin Fig. 5 (c)-(e). The model skillfully handles the unseen tasks, producing compelling results. This\ncapability is deeply rooted in the shared attributes and implicit correlations among pre-training and\nnew tasks, allowing our model to adapt seamlessly. For instance, the colorization task leverages the\nmodel\u2019s understanding of image structures from the segmentation task and depth estimation task,\nwhile deblurring and inpainting tasks benefit from the model\u2019s familiarity with edge detection and\noutpainting ones.\n5\nConclusion and Discussion\nWe introduce UniControl , a novel unified model for incorporating a wide range of conditions into the\ngeneration process of diffusion models. UniControl has been designed to be adaptable to various tasks\nthrough the employment of two key components: a Mixture-of-Experts (MOE) style adapter and a\ntask-aware HyperNet. The experimental results have showcased the model\u2019s robust performance and\nadaptability across different tasks and conditions, demonstrating its potential for handling complex\ntext-to-image generation tasks.\nLimitation and Broader Impact.\nWhile UniControl demonstrates impressive performance, it still\ninherits the limitation of diffusion-based image generation models. Specifically, it is limited by our\ntraining data, which is obtained from a subset of the Laion-Aesthetics datasets. We observe that\nthere is a data bias in this dataset. Although we have performed keywords and image based data\nfiltering methods, we are aware that the model may generate biased or low-fidelity output. Our model\nis also limited when high-quality human output is desired. UniControl could be improved if better\nopen-source datasets are available to block the creation of biased, toxic, sexualized, or other harmful\ncontent. We hope our work can motivate researchers to develop visual generative foundation models.\n10\nReferences\n[1] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[2] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\n[3] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416, 2022.\n[4] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[6] OpenAI. Gpt-4 technical report, 2023.\n[7] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n[8] Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. Co-\nbit: A contrastive bi-directional image-text generation model. arXiv preprint arXiv:2303.13455,\n2023.\n[9] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022.\n[10] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n[11] Bo Li, Dongseong Hwang, Zhouyuan Huo, Junwen Bai, Guru Prakash, Tara N Sainath, Khe Chai\nSim, Yu Zhang, Wei Han, Trevor Strohman, et al. Efficient domain adaptation for speech\nfoundation models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.\n[12] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning\nWu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating\nspeech, music, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023.\n[13] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n[14] Yilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuur-\nmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv\npreprint arXiv:2302.00111, 2023.\n[15] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics:\nDesign principles and model abilities.\nTechnical Report MSR-TR-2023-8, Microsoft,\nFebruary 2023.\nURL https://www.microsoft.com/en-us/research/publication/\nchatgpt-for-robotics-design-principles-and-model-abilities/.\n11\n[16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser\nKelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback.\narXiv preprint arXiv:2203.02155, 2022.\n[17] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2022.\n[18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nNeurIPS, 34:8780\u20138794, 2021.\n[19] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022.\n[20] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[21] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n[23] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\n[24] Golnaz Ghiasi, Barret Zoph, Ekin D Cubuk, Quoc V Le, and Tsung-Yi Lin. Multi-task self-\ntraining for learning general representations. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 8856\u20138865, 2021.\n[25] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n1871\u20131880, 2019.\n[26] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on\nLearning Representations, 2017.\n[27] Johannes Von Oswald, Christian Henning, Benjamin F Grewe, and Jo\u00e3o Sacramento. Continual\nlearning with hypernetworks. arXiv preprint arXiv:1906.00695, 2019.\n[28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-\npervised learning using nonequilibrium thermodynamics. In ICML, pages 2256\u20132265. PMLR,\n2015.\n[29] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution. Advances in Neural Information Processing Systems, 32, 2019.\n[30] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n33:6840\u20136851, 2020.\n[32] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models.\narXiv:2010.02502, October 2020. URL https://arxiv.org/abs/2010.02502.\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In MICCAI, pages 234\u2013241. Springer, 2015.\n12\n[34] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res.,\n23(47):1\u201333, 2022.\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[36] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional\nvisual generation with composable diffusion models. In European Conference on Computer\nVision, pages 423\u2013439. Springer, 2022.\n[37] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In CVPR, 2017.\n[38] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David\nFleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH\n2022 Conference Proceedings, pages 1\u201310, 2022.\n[39] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen.\nPretraining is all you need for image-to-image translation. arXiv preprint arXiv:2205.12952,\n2022.\n[40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano\nErmon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In\nInternational Conference on Learning Representations, 2021.\n[41] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[42] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled\ntransformations. arXiv preprint arXiv:2211.12446, 2022.\n[43] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for\nimage-to-image translation. In The Eleventh International Conference on Learning Representa-\ntions, 2022.\n[44] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow\nmatching for generative modeling. arXiv preprint arXiv:2210.02747, 2022.\n[45] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate\nand transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.\n[46] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and\nextending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.\n[47] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima\nAnandkumar. I2sb: Image-to-image schr\u00f6dinger bridge. arXiv preprint arXiv:2302.05872,\n2023.\n[48] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVan Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, 2022.\n[49] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen,\nZhangyang Wang, and Mingyuan Zhou. In-context learning unlocked for diffusion models.\narXiv preprint arXiv:2305.01115, 2023. URL https://arxiv.org/abs/2305.01115.\n[50] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual\nprompting via image inpainting. Advances in Neural Information Processing Systems, 35:\n25005\u201325017, 2022.\n[51] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n13\n[52] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[53] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 8110\u20138119, 2020.\n[54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library. In NeurIPS. NeurIPS, 2019.\n[56] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-5b: An\nopen large-scale dataset for training next generation image-text models.\narXiv preprint\narXiv:2111.02114, 2021.\n[57] John Canny. A computational approach to edge detection. IEEE Transactions on pattern\nanalysis and machine intelligence, pages 679\u2013698, 1986.\n[58] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE\ninternational conference on computer vision, pages 1395\u20131403, 2015.\n[59] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards\nrobust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE\ntransactions on pattern analysis and machine intelligence, 44(3):1623\u20131637, 2020.\n[60] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li,\nand Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv\npreprint arXiv:2201.09450, 2022.\n[61] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ade20k dataset. International Journal\nof Computer Vision, 127:302\u2013321, 2019.\n[62] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed\nand accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.\n[63] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[64] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose\nestimation using part affinity fields. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 7291\u20137299, 2017.\n[65] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2018.\n[66] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30,\n2017.\n[67] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv:2301.07093,\n2023.\n14\n[68] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\n15\nAppendix\nA\nDetails of Implementation\nA.1\nMOE-Style Adapter\nThe MOE adapter is implemented as a set of parallel ConvNets composed of three consecutive\nconvolution and non-linear activation layers. The entire model is comprised of nine individual MOE\nadapters, each of which consumes 70K parameters. Task keys are designated to each adapter, ensuring\nthat they align with the corresponding visual conditions. Once the MOE adapter processes the input,\nthe remaining model parameters become shared across all tasks. This architecture facilitates task\nadaptability while promoting parameter efficiency.\nA.2\nTask-aware HyperNet\nThe task-aware hypernet is applied to modulate the parameters of zero-conv layers in the ControlNet.\nSince the ControlNet can be considered as the hypernet of Stable Diffusion (fixed copy). Our idea\ncan be concluded as the control over control or meta-control to let the task-aware hypernet learn the\nuniverse representation that is generalizable across different tasks. To implement it, we firstly map the\ntask keys to instruction with a mapping function as:\n{\"hed\":\n\"hed edge to image\",\n\"canny\":\n\"canny edge to image\", \"seg\":\n\"segmentation map to image\",\n\"depth\":\n\"depth map to image\", \"normal\":\n\"normal surface map to image\",\n\"pose\":\n\"human pose skeleton to image\", \"hedsketch\":\n\"sketch to image\",\n\"bbox\":\n\"bounding box to image\", \"outpainting\":\n\"image outpainting\"}. Then,\nsuch instructions will be projected as text embeddings with the help of a language model (we adopt\nCLIPText in our implementation). The Task-aware HyperNet takes these task instruction embeddings,\nand projects them into different shapes to match the size of different zero-conv kernels, which will\nbe modulated by these task embeddings accordingly. We would fix the parameters of task-aware\nhyperNet in the later stage of model training to ensure the stability of dynamics.\nA.3\nData Collection\nWe have collected a large amount of training set (MultiGen-20M) including over 20M condition-\nimage-prompt triplets across nine different tasks. We firstly download 3/4 of Laion-Aesthetics-V2\nwith score over six and filter out low-resolution (<512) images. As a result, 2.8M images are selected\nas source images. Then we apply the visual condition extractors as described in the main paper to\ncollect Canny, HED, Sketch, Depth, Normal Surface, Seg Map, Object Bounding Box, Human\nSkeleton and Outpainting.\nB\nNumerical Analysis of Task-Aware Modulated ControlNet\nWe show that our proposed task-aware modulated ControlNet preserves the properties of the original\nControlNet structure. Specifically, we show 1) The new task-aware modulated ControlNet preserves\nthe zero-initialization property of ControlNet; 2) The parameters of the task-aware modulated\nControlnet can be updated once we start to train the model.\nDenote the input feature map by xxx, the frozen SD Block in Fig. 2 by FSD, the extra condition by\nc, two zero convolution operators by Z1\n\u03b81(\u00b7) and Z2\n\u03b82(\u00b7), the trainable copy of SD Block by GSD\n\u03b8s (\u00b7),\nthe task instruction by ctask, and the task-aware hyperNet by H\u03b8H(\u00b7). Then the output of the new\ntask-aware modulated Controlnet can be expressed as\nyyyc = FSD(xxx) + Z1\n\u03b81(GSD\n\u03b8s (xxx + Z2\n\u03b82(c) \u00b7 H\u03b8H(ctask))) \u00b7 H\u03b8H(ctask) .\n(1)\nProperty of Zero Initialization.\nSimilar to ControlNet [21], the weights and biases of the\nconvolution layers are initialized as zeros. As a result, we have Z1\n\u03b81(\u00b7) \u2261 0 and yyyc = FSD(xxx),\nregardless of the initialization of H\u03b8H(\u00b7).\n16\nGradient Analysis.\nWe analyze the gradient of the modulated part\n\u2207\u03b8\n\u0000Z1\n\u03b81(I) \u00b7 H\u03b8H(ctask)\n\u0001\n= H\u03b8H(ctask) \u00b7 \u2207\u03b8Z1\n\u03b81(I) + Z1\n\u03b81(I) \u00b7 \u2207\u03b8HH\u03b8H(ctask) ,\n(2)\nwhere I is the input of the zero convolution layer.\nWhen we start to train the network, the first part of the RHS of (2) follows similar analysis of\nControlNet [21] since H\u03b8H(ctask) is constant when we analyze the gradient \u2207\u03b8Z1\n\u03b81(I). Since the\nparameters of H\u03b8H(ctask) are not initialized to zero, it is known that H\u03b8H(ctask) \u0338= 0. So the gradient\ndynamic follows the analysis of ControlNet. Therefore, we conclude that Z1\n\u03b81(I) \u0338= 0 after the first\ngradient update, and that the network can start to learn and update the following standard dynamics\nof stochastic gradient descent.\nAs for the second part of the RHS of (2), Z1\n\u03b81(I) \u2261 0 before the first gradient update, so the gradient\nis zero for \u03b8H. However, after the first gradient update of \u03b81, we know Z1\n\u03b81(I) \u0338= 0, and \u03b8H can be\nupdated with non-zero gradients.\nTo conclude, the new task-aware Modulated ControlNet can still be efficiently updated and learned\neven if the convolution layers are initialized to zero.\nC\nZero-shot-task Results and Analysis\nWe show more zero-shot-task results in this section, where the tasks have not been trained on. In\nFig. 13, we show zero-shot deblurring results guided by the keywords. Our deblurred images can\nsuccessfully recover the fine-grained details of the images without training on such data. We note\nthat some details are still missing, e.g., the details in the painting in the first row are still not clear\nenough. In Fig. 14, we illustrate two zero-shot image colorization results. We believe that most parts\nof the generated images are acceptable, though the clothes of the second woman do not look the same\nto the input blurred image. In Fig. 15, we observe impressive zero-shot inpainting results. In the\nfirst row, the duck that is inputted in the text has been successfully generated in the inpainted image.\nThe second row obtains acceptable results as well, though the faces do not look perfect. The overall\nzero-shot quality of UniControl is remarkable.\nWhile inpainting and outpainting might appear related, they are fundamentally distinct. Inpainting\nheavily leverages the contextual information from unmasked regions, necessitating a precise match.\nConversely, outpainting has more freedom, with the generative model prioritizing prompts to envision\nnew content. As shown in Fig. 8, directly using outpainting model for inpainting tasks can be\nchallenging since the model tends to leave a sharp change over the mask boundaries. Our pretrained\nUniControl, thanks to intensive training across multiple tasks, has learned edge and region-to-image\nmappings, which assists in preserving contextual information.\nOur model also demonstrates a promising capacity to generalize under scribble conditions, showing\nparallels to the ControlNet\u2019s ability, even though UniControl hasn\u2019t been directly trained using\nscribble data. Fig. 9 provides results illustrating the scribble-to-image generation.\nD\nDetails of User Study\nIn the evaluation steps, we use Amazon Mechanical Turk (Mturk) 2 to perform user study. Specifically,\nwe ask three Mturk master workers to select the best output result for each input condition. As shown\nin Fig. 10, we provide instructions on guidelines to select the best generated image. The annotators\nare provided the condition map and the text that describes the image, and are required to select the\nbetter output between the two generated images. Considering that images can both in good or bad\nqualities, we provide the tie option as well. We use the majority vote to determine the result of each\nimage, which means that an image is considered as a better image if two or more annotators vote\nfor it. We use 294 images for the tasks of Canny, HED, Surface Normal, Depth, Segmentation, User\nSketch, and Outpainting. We adopt 100 images for the task of Human Skeleton and 187 images for\nthe task of Bounding Box. In summary, we totally obtain 7,035 voting results for all nine tasks. 2/3\nof source images in testing set are collected from MSCOCO with the remaining 1/3 from Laion. And\nit includes a very diverse range of topics including indoor scene, outdoor scene, oil painting, portrait,\npencil sketch, animation, cartoon, etc.\n2https://www.mturk.com\n17\nInput (Masked Image)\nOurs-Single-Outpainting\nUniControl\n\u201cContemporary Bedroom Designs 2015 modern bedroom designs intended design\u201d\n\u201cWedding hair and makeup with warm tones and red lips\u201d \nFigure 8: Visual comparison of Ours-single-outpainting and UniControl on the inpainting task. The\nsingle outpainting model cannot well address the zero-shot inpainting task whereas UniControl\ndemonstrates promising capacity.\nInput (User Scribble)\nDefault\nAutomatic Prompt\nUser Prompt\n\u201ca turtle in river\u201d\n\u201ca masterpiece of cartoon-style turtle illustration\u201d\n\u201ca cow with horns standing in a field\u201d\n\u201ca robot ox on moon, UE5 rendering, ray tracing\u201d\n\u201ca digital painting of a hot air balloon\u201d\n\u201cmagic hot air balloon over a lit magic city at night\u201d\n\u201ca door on a wall\u201d\n\u201cmagical door, Hearthstone\u201d\n\u201can elephant with background in the field\u201d\n\u201cEgyptian elephant sculpture\u201d\nInput (User Scribble)\nDefault\nAutomatic Prompt\nUser Prompt\n\u201ca turtle in river\u201d\n\u201ca masterpiece of cartoon-style turtle illustration\u201d\n\u201ca cow with horns standing in a field\u201d\n\u201ca robot ox on moon, UE5 rendering, ray tracing\u201d\n\u201ca digital painting of a hot air balloon\u201d\n\u201cmagic hot air balloon over a lit magic city at night\u201d\n\u201ca door on a wall\u201d\n\u201cmagical door, Hearthstone\u201d\n\u201can elephant with background in the field\u201d\n\u201cEgyptian elephant sculpture\u201d\nInput (User Scribble)\nDefault\nAutomatic Prompt\nUser Prompt\n\u201ca turtle in river\u201d\n\u201ca masterpiece of cartoon-style turtle illustration\u201d\n\u201ca cow with horns standing in a field\u201d\n\u201ca robot ox on moon, UE5 rendering, ray tracing\u201d\n\u201ca digital painting of a hot air balloon\u201d\n\u201cmagic hot air balloon over a lit magic city at night\u201d\n\u201ca door on a wall\u201d\n\u201cmagical door, Hearthstone\u201d\nInput (Scribble)\nControlNet-Scribble Results\nUniControl Results (zero-shot)\n\u201ca turtle in river\u201d \n\u201ca digital painting of a hot air balloon\u201d \n\u201ca door on a wall\u201d \nFigure 9: Visual comparison of ControlNet-Scribble and UniControl on the scribble data. ControlNet-\nScribble is trained by the scribble data which, however, are unseen for UniControl.\n18\nFigure 10: Mturk interface to select the better generated image.\nOurs-Multi\nOurs-Single\nTie\n0\n20\n40\n60\n80\n76.2%\n17%\n6.8%\nSketch-to-Image\nFigure 11: User study results of User Sketch to image generation.\nE\nFailure Cases\nWe illustrate some failure cases in Fig. 12. In the first row, although our generated image successfully\naligns the Bounding Box condition, the generated human has a distorted body. In the second row, our\ngenerated image looks similar to the ground truth; however, the human faces are blurred. We think\nthat the reason is that UniControl inherits the data and model bias of Stable Diffusion, where the\ngenerated human commonly have issues. In the third row, the generated image does not look realistic.\nWe believe that the training data can be improved both quantitatively and qualitatively.\nF\nAdditional Results\nWe illustrate more visualized results in this section on tasks Canny (Fig. 16), HED (Fig. 17),\nDepth (Fig. 18), Surface Normal (Fig. 19), Human Skeleton (Fig. 20), Bounding Box (Fig. 21),\nSegmentation (Fig. 22) and Outpainting (Fig. 23). These results further demonstrate the effectiveness\nof our proposed method. Moreover, due to the space limitation in the main paper, we report results of\nthe last task, User Sketch. Given a sketched image, UniControl is able to achieve promising realistic\nimages. The visualized results are in Fig. 24. The user study result can be found in Fig. 11, where it\nis observed that UniControl obtains significantly more votes than the single task model.\n19\n\u201cLa tricoteuse Realism William Adolphe Bouguereau Oil Paintings\u201d \n\u201cA man and woman in ski gear standing in front of a mountain. \u201c\n\u201cThe Taj Mahal mirrored by a water fountain's reflection. - Agra, Uttar Pradesh, India - Daily Travel Photos\u201d \nVisual Condition\nOur Result\nGround Truth\nFigure 12: Failure Cases: distorted body (row one); blurred faces (row two); incorrect creation (row\nthree).\n20\nBlurred Image\nOur Result\n\u201cBedroom Colour Ideas 25 Paint Colours With Impact Living\u201d\n\u201cMountain Village in the Alps - Canvas print \u2013 Bedroom\u201d \n\u201cChrista McAuliffe (right, sat with her backup crew member Barbara Morgan) was a social \nstudies teacher who had won NASA's Teacher in Space contest and earned herself a spot on \nthe mission\u201d\nFigure 13: More zero-shot-task deblurring results.\n21\n\u201cPixie Cropped Short Layered Synthetic Wig for Women-KAMI WIGS\u201d \nGray Image\nOur Result\n\u201cLong White Casual Wedding Dress\u201d \n\u201cEarly morning view over the town of Tinerhir, south of the Todra Gorge, \nMorocco, North Africa, Africa\u201d \nFigure 14: More zero-shot-task gray-to-RGB colorization results.\n22\n\u201cA lone duck basks in the calm lake's mirror reflection of the Chugach mountain valley\u201d\nCropped Image\nOur Result\n\u201cChancellor of the Exchequer Rishi Sunak was the most high-profile, and unexpected, appointment of the day\u201d \n\u201cContemporary Bedroom Designs 2015 modern bedroom designs intended design \u201c\nFigure 15: More zero-shot-task image in-painting results. The in-painting MOE adapter weights are\ndirectly inherited from outpainting.\n23\nInput Image\nOur Method Output\n<Caption>: two arms typing on a laptop and one hand on a mouse\n(a) \u201cTwo arms typing on a laptop and one hand on a mouse\u201d\nInput Image\nOur Method Output\n<Caption>: Two people walking along a side walk next to a train on the tracks.\n(b) \u201cTwo people walking along a side walk next to a train on the tracks.\u201d\nInput Image\nOur Method Output\n<Caption>: A close up of glazed donuts that are plain or with chocolate.\n(c) \u201cA close up of glazed donuts that are plain or with chocolate.\u201d\nInput Image\nOur Method Output\n<Caption>: A group of elephants with water in front and trees behind.\n(d) \u201cA group of elephants with water in front and trees behind.\u201d\nFigure 16: Canny to Image Generation\n24\nInput Image\nOur Method Output\n<Caption>: a person on skis makes her way through the snow\n(a) \u201cA person on skis makes her way through the snow\u201d\nInput Image\nOur Method Output\n<Caption>: three zebras grazing in a grassy area near shrubs\n(b) \u201cThree zebras grazing in a grassy area near shrubs\u201d\nInput Image\nOur Method Output\n<Caption>: The Taj Mahal mirrored by a water fountain's reflection. - Agra, Uttar Pradesh, India - Daily Travel Photos\n(c) \u201cThe Taj Mahal mirrored by a water fountain\u2019s reflection. - Agra, Uttar Pradesh, India - Daily Travel Photos\u201d\nInput Image\nOur Method Output\n<Caption>: A young girl who is brushing her teeth with a toothbrush.\n(d) \u201cA young girl who is brushing her teeth with a toothbrush.\u201d\nFigure 17: HED to Image Generation\n25\nInput Image\nOur Method Output\n<Caption>: The Ta Prohm Temple Located at Angkor in Cambodia by Kyle Hammons\n(a) \u201cThe Ta Prohm Temple Located at Angkor in Cambodia by Kyle Hammons\u201d\nInput Image\nOur Method Output\n<Caption>: A brown dog standing on a wooden bench near a lemon tree.\n(b) \u201cA brown dog standing on a wooden bench near a lemon tree.\u201d\nInput Image\nOur Method Output\n<Caption>: A Polar Bear walks toward water, while a large bird lands on the opposite bank.\n(c) \u201cA Polar Bear walks toward water, while a large bird lands on the opposite bank.\u201d\nInput Image\nOur Method Output\n<Caption>: A display of vintage animal toys on the floor.\n(d) \u201cA display of vintage animal toys on the floor.\u201d\nFigure 18: Depth to Image Generation\n26\nInput Image\nOur Method Output\n<Caption>: A line of small teddy bears are in front of several DVD cases.\n(a) \u201cA line of small teddy bears are in front of several DVD cases.\u201d\nInput Image\nOur Method Output\n<Caption>: A man in the kitchen standing with his dog.\n(b) \u201cA man in the kitchen standing with his dog\u201d\nInput Image\nOur Method Output\n<Caption>: A hot dog sitting on top of a bun in a wrapper.\n(c) \u201cA hot dog sitting on top of a bun in a wrapper\u201d\nInput Image\nOur Method Output\n<Caption>: Several people waiting on the side of train tracks as a train with it;s lights on comes down the track.\n(d) \u201cSeveral people waiting on the side of train tracks as a train with it\u2019s lights on comes down the track\u201d\nFigure 19: Surface Normal to Image Generation\n27\nInput Image\nOur Method Output\n<Caption>: Photo of handsome man in black leather jacket\n(a) \u201cPhoto of handsome man in black leather jacket\u201d\nInput Image\nOur Method Output\n<Caption>: A woman is sitting near a prominent landmark.\n(b) \u201cA woman is sitting near a prominent landmark\u201d\nInput Image\nOur Method Output\n<Caption>: A man that has ski's and is standing in the snow.\n(c) \u201cA man that has ski\u2019s and is standing in the snow.\u201d\nInput Image\nOur Method Output\n<Caption>: a woman is sitting in front of a desk\n(d) \u201cA woman is sitting in front of a desk\u201d\nFigure 20: Human Pose Skeleton to Image Generation\n28\nInput Image\nOur Method Output\n<Caption>: A woman is walking two dogs in the snow. \n(a) \u201cA woman is walking two dogs in the snow\u201d\nInput Image\nOur Method Output\n<Caption>: Simone Righi frasi glasses linen suit menswear streetstyle icon fashion florence.jpg\n(b) \u201cSimone Righi frasi glasses linen suit menswear streetstyle icon fashion florence.\u201d\nInput Image\nOur Method Output\n<Caption>: The large room has a wooden table with chairs and a couch.\n(c) \u201cThe large room has a wooden table with chairs and a couch.\u201d\nInput Image\nOur Method Output\n<Caption>: two black bags placed standing on the ground\n(d) \u201cTwo black bags placed standing on the ground\u201d\nFigure 21: Bounding Box (by YOLO-V4-MSCOCO) to Image Generation\n29\nInput Image\nOur Method Output\n<Caption>: A bench at the beach next to the sea\n(a) \u201cA bench at the beach next to the sea\u201d\nInput Image\nOur Method Output\n<Caption>: Water traffic along the Thames by Big Ben\n(b) \u201cWater traffic along the Thames by Big Ben\u201d\nInput Image\nOur Method Output\n<Caption>: A well-lit and well-decorated living room shows a glimpse of a glass front door through the corridor. \n(c) \u201cA well-lit and well-decorated living room shows a glimpse of a glass front door through the corridor.\u201d\nInput Image\nOur Method Output\n<Caption>: Blue Hour Barley\n(d) \u201cBlue Hour Barley\u201d\nFigure 22: Segmentation Map (by Uniformer-ADE20K) to Image Generation\n30\nInput Image\nOur Method Output\n<Caption>: St, Michaels Mount, Cornwall\n(a) \u201cSt, Michaels Mount, Cornwall\u201d\nInput Image\nOur Method Output\n<Caption>: Melbourne by teekay 72\n(b) \u201cMelbourne by teekay 72\u201d\nInput Image\nOur Method Output\n<Caption>: Lady in Black Kimono Paint by Diamonds\n(c) \u201cLady in Black Kimono Paint by Diamonds\u201d\nInput Image\nOur Method Output\n<Caption>: Beautiful kitchen grand scale living pinterest for Kitchen cabinets lowes with old world metal wall art\n(d) \u201cBeautiful kitchen grand scale living pinterest for Kitchen cabinets lowes with old world metal wall art\u201d\nFigure 23: Image Outpainting\n31\nInput Image\nOur Method Output\n<Caption>: A Limited Edition, Fine Art photograph of a beautiful sunrise at Lake Jackson in Sebring, Florida. Available as a Fine Art print\n(a) \u201cA Limited Edition, Fine Art photograph of a beautiful sunrise at Lake Jackson in Sebring, Florida. Available\nas a Fine Art print\u201d\nInput Image\nOur Method Output\n<Caption>: Giraffes in Lake Manyara national park\n(b) \u201cGiraffes in Lake Manyara national park\u201d\nInput Image\nOur Method Output\n<Caption>: Wild Ones 1991 Limited Edition Print - Frank McCarthy\n(c) \u201cWild Ones 1991 Limited Edition Print - Frank McCarthy\u201d\nInput Image\nOur Method Output\n<Caption>: Superhero watching over city. No transparency used. Basic (linear) gradients. A4 proportions.\n(d) \u201cSuperhero watching over city. No transparency used. Basic (linear) gradients. A4 proportions.\u201d\nFigure 24: User Sketch to Image Generation\n32\n"
  },
  {
    "title": "TextDiffuser: Diffusion Models as Text Painters",
    "link": "https://arxiv.org/pdf/2305.10855.pdf",
    "upvote": "2",
    "text": "TextDiffuser: Diffusion Models as Text Painters\nJingye Chen\u221713, Yupan Huang\u221723, Tengchao Lv3, Lei Cui3, Qifeng Chen1, Furu Wei3\n1HKUST\n2Sun Yat-sen University\n3Microsoft Research\nqwerty.chen@connect.ust.hk, huangyp28@mail2.sysu.edu.cn, cqf@ust.hk\n{tengchaolv,lecu,fuwei}@microsoft.com\nAbstract\nDiffusion models have gained increasing attention for their impressive generation\nabilities but currently struggle with rendering accurate and coherent text. To address\nthis issue, we introduce TextDiffuser, focusing on generating images with visually\nappealing text that is coherent with backgrounds. TextDiffuser consists of two\nstages: first, a Transformer model generates the layout of keywords extracted\nfrom text prompts, and then diffusion models generate images conditioned on\nthe text prompt and the generated layout. Additionally, we contribute the first\nlarge-scale text images dataset with OCR annotations, MARIO-10M, containing\n10 million image-text pairs with text recognition, detection, and character-level\nsegmentation annotations. We further collect the MARIO-Eval benchmark to serve\nas a comprehensive tool for evaluating text rendering quality. Through experiments\nand user studies, we show that TextDiffuser is flexible and controllable to create\nhigh-quality text images using text prompts alone or together with text template\nimages, and conduct text inpainting to reconstruct incomplete images with text. The\ncode, model, and dataset will be available at https://aka.ms/textdiffuser.\n(a) Text-to-Image\n(b) Text-to-Image with Template\n(c) Text Inpainting\nA dog ...\nAn astronaut ...\nA cat holds a\npaper saying\n\"Hello World\"\nA dinosaur ...\nlayout\ngeneration\nimage\ngeneration\ncompany\nbookmark\ncake\npizza\ndress\nnewspaper\nmask\ntemplate\na boy draws good morning on a board\na supermarket called diffusion\nFigure 1: TextDiffuser generates accurate and coherent text images from text prompts or together\nwith template images, as well as conducting text inpainting to reconstruct incomplete images.\n\u2217Equal contribution during internship at Microsoft Research.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.10855v5  [cs.CV]  30 Oct 2023\n1\nIntroduction\nThe field of image generation has seen tremendous progress with the advent of diffusion models\n[2, 15, 16, 18, 25, 67, 70, 72, 79, 93] and the availability of large-scale image-text paired datasets\n[17, 74, 75]. However, existing diffusion models still face challenges in generating visually pleasing\ntext on images, and there is currently no specialized large-scale dataset for this purpose. The ability\nof AI models to generate accurate and coherent text on images is crucial, given the widespread use of\ntext images in various forms (e.g., posters, book covers, memes, etc.) and the difficulty in creating\nhigh-quality text images, which typically require professional skills and numerous times of designers.\nTraditional solutions to creating text images involve using image processing tools like Photoshop\nto add text onto images directly. However, these often result in unnatural artifacts due to the\nbackground\u2019s complex texture or lighting variations. Recent efforts have used diffusion models to\novercome the limitations of traditional methods and enhance text rendering quality. For instance,\nImagen [72], eDiff-I [2], and DeepFloyd [12] observe diffusion models generate text better with\nT5 series text encoders [61] than the CLIP text encoder [60]. Liu et al. employ character-aware\ntext encoders to improve text rendering [46]. Despite some success, these models only focus on\ntext encoders, lacking control over the generation process. A concurrent work, GlyphDraw [49],\nimproves the controllability of models by conditioning on the location and structures of Chinese\ncharacters. However, GlyphDraw does not support multiple text bounding-box generation, which is\nnot applicable to many text images such as posters and book covers.\nIn this paper, we propose TextDiffuser, a flexible and controllable framework based on diffusion\nmodels. The framework consists of two stages. In the first stage, we use a Layout Transformer to\nlocate the coordinates of each keyword in text prompts and obtain character-level segmentation masks.\nIn the second stage, we fine-tune the latent diffusion model by leveraging the generated segmentation\nmasks as conditions for the diffusion process and text prompts. We introduce a character-aware\nloss in the latent space to further improve the quality of generated text regions. Figure 1 illustrates\nthe application of TextDiffuser in generating accurate and coherent text images using text prompts\nalone or text template images. Additionally, TextDiffuser is capable of performing text inpainting2\nto reconstruct incomplete images with text. To train our model, we use OCR tools and design\nfiltering strategies to obtain 10 million high-quality image-text pairs with OCR annotations (dubbed\nas MARIO-10M), each with recognition, detection, and character-level segmentation annotations.\nExtensive experiments and user studies demonstrate the superiority of the proposed TextDiffuser over\nexisting methods on the constructed benchmark MARIO-Eval. The code, model and dataset will be\npublicly available to promote future research.\n2\nRelated Work\nText Rendering.\nImage generation has made significant progress with the advent of diffusion\nmodels [18, 25, 67, 72, 79, 63, 70, 2, 8, 13, 52, 48, 26, 80], achieving state-of-the-art results compared\nwith previous GAN-based approaches [64, 100, 43, 58]. Despite rapid development, current methods\nstill struggle with rendering accurate and coherent text. To mitigate this, Imagen [72], eDiff-I [2],\nand DeepFolyd [12] utilize a large-scale language model (large T5 [61]) to enhance the text-spelling\nknowledge. In [46], the authors noticed that existing text encoders are blind to token length and\ntrained a character-aware variant to alleviate this problem. A concurrent work, GlyphDraw [49],\nfocuses on generating high-quality images with Chinese texts with the guidance of text location\nand glyph images. Unlike this work, we utilize Transformer [81] to obtain the layouts of keywords,\nenabling the generation of texts in multiple lines. Besides, we use character-level segmentation masks\nas prior, which can be easily controlled (e.g., by providing a template image) to meet user needs.\nSeveral papers have put forward benchmarks containing a few cases regarding text rendering for\nevaluation. For example, Imagen [72] introduces DrawBench containing 200 prompts, in which\n21 prompts are related to visual text rendering (e.g., A storefront with \u2018Hello World\u2019 written on\nit). According to [46], the authors proposed DrawText comprising creative 175 prompts (e.g., letter\n\u2018c\u2019 made from cactus, high-quality photo). GlyphDraw [49] designs 218 prompts in Chinese and\n2Different from text editing [88, 37, 32], the introduced text inpainting task aims to add or modify text guided\nby users, ensuring that the inpainted text has a reasonable style (i.e., no need to match the style of the original\ntext during modification exactly) and is coherent with backgrounds.\n2\nEnglish (e.g., Logo for a chain of grocery stores with the name \u2018Grocery\u2019). Considering that existing\nbenchmarks only contain a limited number of cases, we attempt to collect more prompts and combine\nthem with existing prompts to establish a larger benchmark MARIO-Eval to facilitate comprehensive\ncomparisons for future work.\nImage Inpainting.\nImage inpainting is the task of reconstructing missing areas in images naturally\nand coherently. Early research focused on leveraging low-level image structure and texture to address\nthis task [3, 6, 5]. Later, deep learning architectures such as auto-encoder [55, 45], GAN [65,\n98], VAE [103, 105], and auto-regressive Transformers [57, 83, 96] were applied to tackle this\nproblem. Recently, diffusion models have been used to generate high-quality and diverse results for\nunconditional image inpainting [71, 48, 67, 11, 99], text-conditional image inpainting [52, 1] and\nimage-conditional image inpainting [92]. Our work falls under the category of text-conditional image\ninpainting using diffusion models. In contrast to prior works that focused on completing images with\nnatural backgrounds or objects, our method focuses on completing images with text-related rendering,\nalso named text inpainting, by additional conditioning on a character-level segmentation mask.\nOptical Character Recognition.\nOptical Character Recognition (OCR) is an important task that\nhas been studied in academia for a long period [87, 7]. It has undergone a remarkable development in\nthe last decade, contributing to many applications like autonomous driving [89, 73], car license plate\nrecognition [101, 53], GPT models [76, 28], etc. Various datasets [31, 19, 90, 91] and downstream\ntasks are included within this field, such as text image recognition [77, 41, 95, 14], detection\n[106, 51, 42, 84], segmentation [90, 91, 107, 68], super-resolution [9, 85, 50, 104], as well as some\ngeneration tasks, including text image editing [88, 78, 94, 59, 38], document layout generation\n[56, 22, 20, 40, 33], font generation [30, 21, 36, 54], etc. Among them, the font generation task is\nmost relevant to our task. Font generation aims to create high-quality, aesthetically pleasing fonts\nbased on given character images. In contrast, our task is more challenging as it requires the generated\ntext to be legible, visually appealing, and coherent with the background in various scenarios.\n3\nMethodology\nAs illustrated in Figure 2, TextDiffuser consists of two stages: Layout Generation and Image\nGeneration. We will detail the two stages and introduce the inference process next.\n3.1\nStage1: Layout Generation\nIn this stage, the objective is to utilize bounding boxes to determine the layout of keywords (enclosed\nwith quotes specified by user prompts). Inspired by Layout Transformer [20], we utilize the Trans-\nformer architecture to obtain the layout of keywords. Formally, we denote the tokenized prompt\nas P = (p0, p1, ..., pL\u22121), where L means the maximum length of tokens. Following LDM [67],\nwe use CLIP [60] and two linear layers to encode the sequence as CLIP(P) \u2208 RL\u00d7d, where d is\nthe dimension of latent space. To distinguish the keywords against others, we design a keyword\nembedding Key(P) \u2208 RL\u00d7d with two entries (i.e., keywords and non-keywords). Furthermore,\nwe encode the width of keywords with an embedding layer Width(P) \u2208 RL\u00d7d. Together with\nthe learnable positional embedding Pos(P) \u2208 RL\u00d7d introduced in [81], we construct the whole\nembedding as follows:\nEmbedding(P) = CLIP(P) + Pos(P) + Key(P) + Width(P).\n(1)\nThe embedding is further processed with Transformer-based l-layer encoder \u03a6E and decoder \u03a6D to\nget the bounding boxes B \u2208 RK\u00d74 of K key words autoregressively:\nB = \u03a6D(\u03a6E(Embedding(P))) = (b0, b1, ..., bK\u22121).\n(2)\nSpecifically, we use positional embedding as the query for the Transformer decoder \u03a6D, ensuring\nthat the n-th query corresponds to the n-th keyword in the prompt. The model is optimized with l1\nloss, also denoted as |BGT \u2212 B| where BGT is the ground truth. Further, we can utilize some Python\npackages like Pillow to render the texts and meanwhile obtain the character-level segmentation\nmask C with |A| channels, where |A| denote the size of alphabet A. To this end, we obtain the\nlayouts of keywords and the image generation process is introduced next.\n3\nTransformer\nEncoder\na\ncat\nholds\nhello\nworld\nEmbedding\nLayer\nTokenize\nTransformer\nDecoder\nquery0\nquery1\nbox0\nbox1\nHello\nWorld\nRender\nGet Mask\nStage1 Layout Generation\nStage2 Image Generation\nWhole-image Generation\nPart-image Generation\nDiffusion\nModel\nText Prompt\nU-Net\nDenoising\nLoss\nCharacter\naware\nLoss\nEmbedding\nLayer\nFigure 2: TextDiffuser consists of two stages. In the first Layout Generation stage, a Transformer-\nbased encoder-decoder model generates character-level segmentation masks that indicate the layout\nof keywords in images from text prompts. In the second Image Generation stage, a diffusion model\ngenerates images conditioned on noisy features, segmentation masks, feature masks, and masked\nfeatures (from left to right) along with text prompts. The feature masks can cover the entire or part of\nthe image, corresponding to whole-image and part-image generation. The diffusion model learns\nto denoise features progressively with a denoising and character-aware loss. Please note that the\ndiffusion model operates in the latent space, but we use the image pixels for better visualization.\n3.2\nStage2: Image Generation\nIn this stage, we aim to generate the image guided by the segmentation masks C produced in the\nfirst stage. We use VAE [35] to encode the original image with shape H \u00d7 W into 4-D latent space\nfeatures F \u2208 R4\u00d7H\u2032\u00d7W \u2032. Then we sample a time step T \u223c Uniform(0, Tmax) and sample a Gaussian\nnoise \u03f5 \u2208 R4\u00d7H\u2032\u00d7W \u2032 to corrupt the original feature, yielding \u02c6F = \u221a\u00af\u03b1T F + \u221a1 \u2212 \u00af\u03b1T \u03f5 where \u00af\n\u03b1T\nis the coefficient of the diffusion process introduced in [25]. Also, we downsample the character-\nlevel segmentation mask C with three convolution layers, yielding 8-D \u02c6C \u2208 R8\u00d7H\u2032\u00d7W \u2032. We also\nintroduce two additional features, called 1-D feature mask \u02c6M \u2208 R1\u00d7H\u2032\u00d7W \u2032 and 4-D masked feature\n\u02c6FM \u2208 R4\u00d7H\u2032\u00d7W \u2032. In the process of whole-image generation, \u02c6M is set to cover all regions of the\nfeature and \u02c6FM is the feature of a fully masked image. In the process of part-image generation (also\ncalled text inpainting), the feature mask \u02c6M represents the region where the user wants to generate,\nwhile the masked feature \u02c6FM indicates the region that the user wants to preserve. To simultaneously\ntrain two branches, we use a masking strategy where a sample is fully masked with a probability of \u03c3\nand partially masked with a probability of 1 \u2212 \u03c3. We concatenate \u02c6F, \u02c6C, \u02c6M, \u02c6FM in the feature channel\nas a 17-D input and use denoising loss between the sampled noise \u03f5 and the predicted noise \u03f5\u03b8:\nldenoising = ||\u03f5 \u2212 \u03f5\u03b8(\u02c6F, \u02c6C, \u02c6M, \u02c6FM, P, T)||2\n2.\n(3)\nFurthermore, we propose a character-aware loss to help the model focus more on text regions. In\ndetail, we pre-train a U-Net [69] that can map latent features to character-level segmentation masks.\nDuring training, we fix its parameters and only use it to provide guidance by using a cross-entropy\nloss lchar with weight \u03bbchar (See more details in Appendix A). Overall, the model is optimized with\nl = ldenoising + \u03bbchar \u2217 lchar.\n(4)\nFinally, the output features are fed into the VAE decoder to obtain the images.\n4\n(a) MARIO-LAION\n(b) MARIO-TMDB\n(c) MARIO-OpenLibrary\nFigure 3: Illustrations of three subsets of MARIO-10M. See more details in Appendix C.\n3.3\nInference Stage\nTextDiffuser provides a high degree of controllability and flexibility during inference in the following\nways: (1) Generate images from user prompts. Notably, the user can modify the generated layout or\nedit the text to meet their personalized requirements; (2) The user can directly start from the second\nstage by providing a template image (e.g., a scene image, handwritten image, or printed image), and\na segmentation model is pre-trained to obtain the character-level segmentation masks (Appendix B);\n(3) Users can modify the text regions of a given image using text inpainting. Moreover, this operation\ncan be performed multiple times. These experimental results will be presented in the next section.\n4\nMARIO Dataset and Benchmark\nAs there is no large-scale dataset designed explicitly for text rendering, to mitigate this issue, we\ncollect 10 million image-text pairs with OCR annotations to construct the MARIO-10M Dataset.\nWe further collect the MARIO-Eval Benchmark from the subset of the MARIO-10M test set and\nother existing sources to serve as a comprehensive tool for evaluating text rendering quality.\n4.1\nMARIO-10M Dataset\nThe MARIO-10M is a collection of about 10 million high-quality and diverse image-text pairs\nfrom various data sources such as natural images, posters, and book covers. Figure 3 illustrates\nsome examples from the dataset. We design automatic schemes and strict filtering rules to construct\nannotations and clean noisy data (more details in Appendix D and Appendix E). The dataset con-\ntains comprehensive OCR annotations for each image, including text detection, recognition, and\ncharacter-level segmentation annotations. Specifically, we use DB [42] for detection, PARSeq [4] for\nrecognition, and manually train a U-Net [69] for segmentation. We analyze the performance of OCR\ntools in Appendix F. The total size of MARIO-10M is 10,061,720, from which we randomly chose\n10,000,000 samples as the training set and 61,720 as the testing set. MARIO-10M is collected from\nthree data sources:\nMARIO-LAION derives from the large-scale datasets LAION-400M [75]. After filtering, we\nobtained 9,194,613 high-quality text images with corresponding captions. This dataset comprises a\nbroad range of text images, including advertisements, notes, posters, covers, memes, logos, etc.\nMARIO-TMDB derives from The Movie Database (TMDB), which is a community-built database\nfor movies and TV shows with high-quality posters. We filter 343,423 English posters using the\nTMDB API out of 759,859 collected samples. Since each image has no off-the-shelf captions, we use\nprompt templates to construct the captions according to movie titles.\nMARIO-OpenLibrary derives from Open Library, which is an open, editable library catalog that\ncreates a web page for each published book. We first collect 6,352,989 original-size Open Library\ncovers in bulk. Then, we obtained 523,684 higher-quality images after filtering. Like MARIO-TMDB,\nwe manually construct captions using titles due to the lack of off-the-shelf captions.\n5\n4.2\nMARIO-Eval Benchmark\nThe MARIO-Eval benchmark serves as a comprehensive tool for evaluating text rendering quality\ncollected from the subset of the MARIO-10M test set and other sources. It comprises 5,414 prompts\nin total, including 21 prompts from DrawBenchText [72], 175 prompts from DrawTextCreative\n[46], 218 prompts from ChineseDrawText [49] and 5,000 image-text pairs from a subset of the\nMARIO-10M test set. The 5,000 image-text pairs are divided into three sets of 4,000, 500, and 500\npairs, and are named LAIONEval4000, TMDBEval500, and OpenLibraryEval500 based on their\nrespective data sources. We offer examples in Appendix G to provide a clearer understanding of\nMARIO-Eval.\nEvaluation Criteria: We evaluate text rendering quality with MARIO-Eval from four aspects: (1)\nFr\u00e9chet Inception Distance (FID) [24] compares the distribution of generated images with the\ndistribution of real images. (2) CLIPScore calculates the cosine similarity between the image and\ntext representations from CLIP [29, 60, 23]. (3) OCR Evaluation utilizes existing OCR tools to\ndetect and recognize text regions in the generated images. Accuracy, Precision, Recall, and F-measure\nare metrics to evaluate whether keywords appear in the generated images. (4) Human Evaluation is\nconducted by inviting human evaluators to rate the text rendering quality of generated images using\nquestionnaires. More explanations are shown in Appendix H.\n5\nExperiments\n5.1\nImplementation Details\nFor the first stage, we utilize the pre-trained CLIP [60] to obtain the embedding of given prompts.\nThe number of Transformer layers l is set to 2, and the dimension of latent space d is set to 512. The\nmaximum length of tokens L is set to 77 following CLIP [60]. We leverage a commonly used font\n\u201cArial.ttf\u201d and set the font size to 24 to obtain the width embedding and also use this font for rendering.\nThe alphabet A comprises 95 characters, including 26 uppercase letters, 26 lowercase letters, 10\ndigits, 32 punctuation marks, and a space character. After tokenization, only the first subtoken is\nmarked as the keyword when several subtokens exist for a word.\nFor the second stage, we implement the diffusion process using Hugging Face Diffusers [82] and\nload the checkpoint \u201crunwayml/stable-diffusion-v1-5\u201d. Notably, we only need to modify the input\ndimension of the input convolution layer (from 4 to 17), allowing our model to have a similar scale of\nparameters and computational time as the original model. In detail, the height H and W of input and\noutput images are 512. For the diffusion process, the input is with spatial dimension H\u2032 = 64 and\nW \u2032 = 64. We set the batch size to 768 and trained the model for two epochs, taking four days using\n8 Tesla V100 GPUs with 32GB memory. We use the AdamW optimizer [47] and set the learning\nrate to 1e-5. Additionally, we utilize gradient checkpoint [10] and xformers [39] for computational\nefficiency. During training, we follow [25] to set the maximum time step Tmax to 1,000, and the\ncaption is dropped with a probability of 10% for classifier-free guidance [27]. When training the\npart-image generation branch, the detected text box is masked with a likelihood of 50%. We use 50\nsampling steps during inference and classifier-free guidance with a scale of 7.5 following [67].\n5.2\nAblation Studies\nNumber of Transformer layers and the effectiveness of width embedding.\nWe conduct ablation\nstudies on the number of Transformer layers and whether to use width embedding in the Layout\nTransformer. The results are shown in Table 1. All ablated models are trained on the training set\nof MARIO-10M and evaluated on its test set. Results show that adding width embedding improves\nperformance, boosting IoU by 2.1%, 2.9%, and 0.3% when the number of Transformer layers l is set\nto 1, 2, and 4, respectively. The optimal IoU is achieved using two Transformer layers and the width\nembedding is included. See more visualization results in Appendix I.\nCharacter-level segmentation masks provide explicit guidance for generating characters.\nThe\ncharacter-level segmentation masks provide explicit guidance on the position and content of characters\nduring the generation process of TextDiffuser. To validate the effectiveness of using character-level\nsegmentation masks, we train ablated models without using the masks and show results in Appendix\n6\nTable 1: Ablation about Lay-\nout Transformer.\n#Layer\nWidth(P)\nIoU\u2191\n1\n-\n0.268\n\u2713\n0.289\n2\n-\n0.269\n\u2713\n0.298\n4\n-\n0.294\n\u2713\n0.297\nTable 2: Ablation on weight of\ncharacter-aware loss.\n\u03bbchar\nAcc\u2191\n0\n0.396\n0.001\n0.486\n0.01\n0.494\n0.1\n0.420\n1\n0.400\nTable 3:\nAblation on two-\nbranch training ratio \u03c3.\nratio\nAcc\u2191 / Det-F\u2191 / Spot-F\u2191\n0\n0.344 / 0.870 / 0.663\n0.25\n0.562 / 0.899 / 0.636\n0.5\n0.552 / 0.881 / 0.715\n0.75\n0.524 / 0.921 / 0.695\n1\n0.494 / 0.380 / 0.218\nTable 4: The performance of text-to-image compared with existing methods. TextDiffuser performs\nthe best regarding CLIPScore and OCR evaluation while achieving comparable performance on FID.\nMetrics\nStableDiffusion [67]\nControlNet [102]\nDeepFloyd [12]\nTextDiffuser\nFID\u2193\n51.295\n51.485\n34.902\n38.758\nCLIPScore\u2191\n0.3015\n0.3424\n0.3267\n0.3436\nOCR(Accuracy)\u2191\n0.0003\n0.2390\n0.0262\n0.5609\nOCR(Precision)\u2191\n0.0173\n0.5211\n0.1450\n0.7846\nOCR(Recall)\u2191\n0.0280\n0.6707\n0.2245\n0.7802\nOCR(F-measure)\u2191\n0.0214\n0.5865\n0.1762\n0.7824\nJ. The generated texts are inaccurate and not coherent with the background compared with texts\ngenerated with TextDiffuser, highlighting the importance of explicit guidance.\nThe weight of character-aware loss.\nThe experimental results are demonstrated in Table 2, where\nwe conduct experiments with \u03bbchar ranging from [0, 0.001, 0.01, 0.1, 1]. We utilize DrawBenchText\n[72] for evaluation and use Microsoft Read API to detect and recognize the texts in generated images.\nWe use Accuracy (Acc) as the metric to justify whether the detected words exactly match the keywords.\nWe observe that the optimal performance is achieved when \u03bbchar is set to 0.01, where the score is\nincreased by 9.8% compared with the baseline (\u03bbchar = 0).\nThe training ratio of whole/part-image generation branches.\nWe explore the training ratio \u03c3\nranging from [0, 0.25, 0.5, 0.75, 1] and show results in Table 3. When \u03c3 is set to 1, it indicates that\nonly the whole-image branch is trained and vice versa. We evaluate the model using DrawBenchText\n[72] for the whole-image generation branch. For the part-image generation branch, we randomly\nselect 1,000 samples from the test set of MARIO-10M and randomly mask some of the detected\ntext boxes. We utilize Microsoft Read API to detect and recognize the reconstructed text boxes in\ngenerated images while using the F-measure of text detection results and spotting results as metrics\n(denoted as Det-F and Spot-F, respectively). The results show that when the training ratio is set to\n50%, the model performs better on average (0.716).\n5.3\nExperimental Results\nQuantitative Results.\nFor the whole-image generation task, we compare our method with Stable\nDiffusion (SD) [67], ControlNet [102], and DeepFloyd [12] in quantitative experiments with the\npublicly released codes and models detailed in Appendix K. DeepFloyd [12] uses two super-resolution\nmodules to generate higher resolution 1024\u00d71024 images compared with 512\u00d7512 images generated\nby other methods. We use the Canny map of printed text images generated with our first stage model\nas conditions for ControlNet [102]. Please note that we are not able to compare with Imagen [72],\neDiff-i [2], and GlyphDraw [49] due to the lack of open-source code, checkpoints or APIs. According\nto Table 4, we demonstrate the quantitative results of the text-to-image task compared with existing\nmethods. Our TextDiffuser obtains the best CLIPScore while achieving comparable performance\nin terms of FID. Besides, TextDiffuser achieves the best performance regarding four OCR-related\nmetrics. TextDiffuser outperforms those methods without explicit text-related guidance by a large\n7\nPrompt\nSD\nSD-XL\nMidjourney\nTextDiffuser\na bear holds\na board saying\n'hello world'\na meme of\n'Are you kidding'\na cake of\n'Happy Birthday\nto XYZ'\nControlNet\na poster of\n'Monkey Music\nFestival'\nDALL  E\nDeepFloyd\na book of\n'AI in Next Century'\nwritten by\n'AI Robot'\na boy holds 'P'\nand\na girl holds 'Q'\nFigure 4: Visualizations of whole-image generation compared with existing methods. The first three\ncases are generated from prompts and the last three cases are from given printed template images.\nmargin (e.g., 76.10% and 60.62% better than Stable Diffusion and DeepFloyd regarding F-measure),\nhighlighting the significance of explicit guidance. As for the part-image generation task, we cannot\nevaluate our method since no methods are specifically designed for this task to our knowledge.\nQualitative Results.\nFor the whole-image generation task, we further compare with closed-source\nDALL\u00b7E [63], Stable Diffusion XL (SD-XL), and Midjourney by showing qualitative examples\ngenerated with their official API services detailed in Appendix K. Figure 4 shows some images\ngenerated from prompts or printed text images by different methods. Notably, our method generates\nmore readable texts, which are also coherent with generated backgrounds. On the contrary, although\nthe images generated by SD-XL and Midjourney are visually appealing, some generated text does not\ncontain the desired text or contains illegible characters with incorrect strokes. The results also show\nthat despite the strong supervision signals provided to ControlNet, it still struggles to generate images\nwith accurate text consistent with the background. We also initiate a comparison with the Character-\nAware Model [46] and the concurrent work GlyphDraw [49] using samples from their papers as their\nopen-source code, checkpoints or APIs are not available. Figure 5 shows that TextDiffuser performs\nbetter than these methods. For instance, the Character-Aware Model suffers from misspelling issues\n(e.g., \u2018m\u2019 in \u2018Chimpanzees\u2019) due to its lack of explicit control, and GlyphDraw struggles with\nrendering images containing multiple text lines. For the part-image generation task, we visualize\nsome results in Figure 6. In contrast to text editing tasks [88], we give the model sufficient flexibility\nto generate texts with reasonable styles. For instance, the image in the second row and first column\ncontains the word \u201ccountry\u201d in green, while the model generates the word \u201ccountry\u201d in yellow. This\nis reasonable since it follows the style of the nearest word \u201crange\u201d. Besides, our method can render\nrealistic text coherent with the background, even in complex cases such as clothing. More qualitative\nresults are shown in Appendix L.\n8\nFigure 5: Comparison with Character-Aware Model [46] and the concurrent GlyphDraw [49].\nCOUNTRY\nFrequently\nFIMI X8 SE Range Test in Country \nFrequently Asked Questions (FAQ): The Muslim Edition  image\nHandsome\nBoy\nA man wears a cloth containing handsome boy\nNEWSPAPER\nHow to make a newspaper\nChinese\nShe is teaching Chinese lesson.\nInteresting terrace party\nFigure 6: Visualizations of part-image generation (text inpainting) from given images.\nUser Studies.\nFor the whole-image generation task, the designed questionnaire consists of 15\ncases, each of which includes two multiple-choice questions: (1) Which of the following images has\nthe best text rendering quality? (2) Which of the following images best matches the text description?\nFor the part-image generation task, the questionnaire consists of 15 cases, each of which includes\ntwo rating questions: (1) How is the text rendering quality? (2) Does the drawn text harmonize with\nthe unmasked region? The rating scores range from 1 to 4, and 4 indicates the best. Overall, we have\ncollected 30 questionnaires, and the results are shown in Figure 8. We can draw two conclusions: (1)\nThe generation performance of TextDiffuser is significantly better than existing methods. (2) Users\nare satisfied with the inpainting results in most cases. More details are shown in Appendix M.\nTime and Parameter Efficiency\nFor the time efficiency, the first stage of Layout Generation lever-\nages an auto-regressive Transformer whose prediction time correlates with the number of keywords.\nSpecifically, we conduct experiments to evaluate the time overhead for different numbers of keywords,\nincluding 1 (1.07\u00b10.03s), 2 (1.12\u00b10.09s), 4 (1.23\u00b10.13s), 8 (1.57\u00b10.12s), 16 (1.83\u00b10.12s), and\n32 (1.95\u00b10.28s). Meanwhile, the second stage of image generation is independent of the number of\nqueries (7.12\u00b10.77s). For the parameter efficiency, TextDiffuser builds upon Stable Diffusion 1.5\n(859M parameters), adding a Layout Transformer in the first stage (+25M parameters) and modifying\nthe second stage (+0.75M parameters), augmenting it by only about 3% in terms of parameters.\n9\nFigure 7: Demonstration of using language descriptions to control text color.\nQ1\n9\n13\n6\n9\n5\n59\n10\n0\n100\n200\n300\nDALL\u00b7E\nSD\nSD-XL\nMidjourney ControlNet\nDeepFloyd\nTextDiffuser\n(a) For whole-image generation, our method clearly outperforms\nothers in both aspects of text rendering quality and image-text matching. \n100\n200\n300\n0\n1\n2\n3\n4\n(b) For part-image generation, our method receives\nhigh scores from human evaluators in these two aspects. \n294 277\n274 257\n176156\n154 136\n123 124\n48\n89\n48\n67\n41\n#Votes\nScore\n#Votes\nQ2\nQ1 Text rendering quality\nImage-text matching\nText rendering quality\nQ2 Harmonization\nFigure 8: User studies for whole-image generation and part-image generation tasks.\nText Color Controllability\nIn Figure 7, we showcase TextDiffuser\u2019s capability in controlling\nthe color of generated texts through language descriptions. The visualization results show that\nTextDiffuser can successfully control the color of rendered text, further enhancing its controllability.\n6\nDiscussion and Conclusion\nDiscussion. We show that TextDiffuser maintains the capability and generality to create general\nimages without text rendering in Appendix N. Besides, we compare our method with a text editing\nmodel in Appendix O, showing that TextDiffuser generates images with better diversity. We also\npresent the potential of TextDiffuser on the text removal task in Appendix P. As for the limitations\nand failure cases, TextDiffuser uses the VAE networks to encode images into low-dimensional\nlatent spaces for computational efficiency following latent diffusion models [67, 49, 2], which has a\nlimitation in reconstructing images with small characters as shown in Appendix Q. We also observed\nfailure cases when generating images from long text and showed them in Appendix Q. As for the\nbroader impact, TextDiffuser can be applied to many designing tasks, such as creating posters\nand book covers. Additionally, the text inpainting task can be used for secondary creation in many\napplications, such as Midjourney. However, there may be some ethical concerns, such as the misuse\nof text inpainting for forging documents. Therefore, techniques for detecting text-related tampering\n[86] need to be applied to enhance security. In conclusion, we propose a two-stage diffusion model\ncalled TextDiffuser to generate images with visual-pleasing texts coherent with backgrounds. Using\nsegmentation masks as guidance, the proposed TextDiffuser shows high flexibility and controllability\nin the generation process. We propose MARIO-10M containing 10 million image-text pairs with\nOCR annotations. Extensive experiments and user studies validate that our method performs better\nthan existing methods on the proposed benchmark MARIO-Eval. For future work, we aim to address\nthe limitation of generating small characters by using OCR priors following OCR-VQGAN [66] and\nenhance TextDiffuser\u2019s capabilities to generate images with text in multiple languages. Disclaimer\nPlease note that the model presented in this paper is intended for academic and research purposes\nONLY. Any use of the model for generating inappropriate content is strictly prohibited and is not\nendorsed by this paper. The responsibility for any misuse or improper use of the model lies solely\nwith the users who generated such content, and this paper shall not be held liable for any such use.\n7\nAcknowledgement\nThis research was supported by the Research Grant Council of the Hong Kong Special Administrative\nRegion under grant number 16203122.\n10\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of\nnatural images. In CVPR, 2022.\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion\nmodels with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[3] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles, Guillermo Sapiro, and Joan Verdera.\nFilling-in by joint interpolation of vector fields and gray levels. IEEE transactions on image\nprocessing (TIP), 2001.\n[4] Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive\nsequence models. In ECCV, 2022.\n[5] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpaint-\ning. In Proceedings of the 27th annual conference on Computer graphics and interactive\ntechniques, 2000.\n[6] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and Stanley Osher. Simultaneous\nstructure and texture image inpainting. IEEE transactions on image processing (TIP), 2003.\n[7] Glenn L Cash and Mehdi Hatamian. Optical character recognition by the method of moments.\nComputer vision, graphics, and image processing, 1987.\n[8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image\ngeneration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\n[9] Jingye Chen, Bin Li, and Xiangyang Xue. Scene text telescope: Text-focused scene image\nsuper-resolution. In CVPR, 2021.\n[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\n[11] Hyungjin Chung, Byeongsu Sim, and Jong-Chul Ye. Come-closer-diffuse-faster: Accelerating\nconditional diffusion models for inverse problems through stochastic contraction. In CVPR,\n2021.\n[12] DeepFloyd. Github link: https://github.com/deep-floyd/if, 2023.\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In\nNeurIPS, 2021.\n[14] Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, and Yongdong Zhang. Read like\nhumans: Autonomous, bidirectional and iterative language modeling for scene text recognition.\nIn CVPR, 2021.\n[15] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu,\nJiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image\ndiffusion model with knowledge-enhanced mixture-of-denoising-experts. In CVPR, 2023.\n[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. In NeurIPS, 2022.\n[17] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Hang Xu, Xiaodan Liang, Wei\nZhang, Xin Jiang, and Chunjing Xu. Wukong: 100 million large-scale chinese cross-modal\npre-training dataset and a foundation framework. In NeurIPS Workshop, 2022.\n[18] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, 2022.\n11\n[19] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in\nnatural images. In CVPR, 2016.\n[20] Kamal Gupta, Alessandro Achille, Justin Lazarow, Larry Davis, Vijay Mahadevan, and\nAbhinav Shrivastava. Layout generation and completion with self-attention. In ICCV, 2021.\n[21] Haibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, and Yu Qiao.\nDiff-font: Diffusion model for robust one-shot font generation. In CVPR, 2023.\n[22] Liu He, Yijuan Lu, John Corring, Dinei Florencio, and Cha Zhang. Diffusion-based document\nlayout generation. arXiv preprint arXiv:2303.10787, 2023.\n[23] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A\nreference-free evaluation metric for image captioning. In EMNLP, 2021.\n[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS,\n2017.\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\nNeurIPS, 2020.\n[26] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine\nLearning Research (JMLR), 2022.\n[27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop,\n2021.\n[28] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\n[29] Yupan Huang, Bei Liu, and Yutong Lu. Unifying multimodal transformer for bi-directional\nimage and text generation. In ACM MM, 2021.\n[30] Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, and Ariel Shamir. Word-\nas-image for semantic typography. arXiv preprint arXiv:2303.01818, 2023.\n[31] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and\nartificial neural networks for natural scene text recognition. In NeurIPS Workshop, 2014.\n[32] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and Shiyu\nChang. Improving diffusion models for scene text editing with dual encoders. arXiv preprint\narXiv:2304.05568, 2023.\n[33] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. Layoutvae:\nStochastic scene layout generation from a label set. In ICCV, 2019.\n[34] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew\nBagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar,\nShijian Lu, et al. Icdar 2015 competition on robust reading. In ICDAR, 2015.\n[35] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.\n[36] Yuxin Kong, Canjie Luo, Weihong Ma, Qiyuan Zhu, Shenggao Zhu, Nicholas Yuan, and\nLianwen Jin. Look closer to supervise better: One-shot font generation via component-based\ndiscriminator. In CVPR, 2022.\n[37] Praveen Krishnan, Rama Kovvuri, Guan Pang, Boris Vassilev, and Tal Hassner. Textstylebrush:\nTransfer of text aesthetics from a single example. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (T-PAMI), 2023.\n12\n[38] Junyeop Lee, Yoonsik Kim, Seonghyeon Kim, Moonbin Yim, Seung Shin, Gayoung Lee, and\nSungrae Park. Rewritenet: Reliable scene text editing with implicit decomposition of text\ncontents and styles. In CVPR Workshop, 2022.\n[39] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel\nHaziza. xformers: A modular and hackable transformer modelling library. https://github.\ncom/facebookresearch/xformers, 2022.\n[40] Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, and Tingfa Xu. Layoutgan:\nGenerating graphic layouts with wireframe discriminators. In ICLR, 2019.\n[41] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\nZhoujun Li, and Furu Wei. Trocr: Transformer-based optical character recognition with\npre-trained models. In AAAI, 2023.\n[42] Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, and Xiang Bai. Real-time scene text\ndetection with differentiable binarization and adaptive scale fusion. IEEE Transactions on\nPattern Analysis and Machine Intelligence (T-PAMI), 2022.\n[43] Wentong Liao, Kai Hu, Michael Ying Yang, and Bodo Rosenhahn. Text to image generation\nwith semantic-spatial aware gan. In CVPR, 2022.\n[44] Chongyu Liu, Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Yongpan Wang.\nErasenet: End-to-end text removal in the wild. IEEE Transactions on Image Processing (TIP),\n2020.\n[45] Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao Yang. Rethinking image inpainting\nvia a mutual encoder-decoder with feature equalizations. In ECCV, 2020.\n[46] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang,\nIrina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware models\nimprove visual text rendering. arXiv preprint arXiv:2212.10562, 2022.\n[47] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n[48] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVan Gool. RePaint: Inpainting using Denoising Diffusion Probabilistic Models. In CVPR,\n2022.\n[49] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin.\nGlyphdraw: Learning to draw chinese characters in image synthesis models coherently. arXiv\npreprint arXiv:2303.17870, 2023.\n[50] Jianqi Ma, Zhetong Liang, and Lei Zhang. A text attention network for spatial deformation\nrobust scene text image super-resolution. In CVPR, 2022.\n[51] Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, and Xiangyang\nXue. Arbitrary-oriented scene text detection via rotation proposals. IEEE transactions on\nmultimedia (TMM), 2018.\n[52] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc-\nGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and\nediting with text-guided diffusion models. In ICML, 2022.\n[53] Safaa S Omran and Jumana A Jarallah. Iraqi car license plate recognition using ocr. In 2017\nannual conference on new trends in information & communications technology applications\n(NTICT), 2017.\n[54] Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, and Hyunjung Shim. Multiple heads are\nbetter than one: Few-shot font generation with multiple localized experts. In ICCV, 2021.\n[55] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In CVPR, 2016.\n13\n[56] Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, and Hadar Averbuch-Elor. Read: Recursive\nautoencoders for document layout generation. In CVPR, 2020.\n[57] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Generating diverse structure for image\ninpainting with hierarchical vq-vae. In CVPR, 2021.\n[58] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. Mirrorgan: Learning text-to-\nimage generation by redescription. In CVPR, 2019.\n[59] Yadong Qu, Qingfeng Tan, Hongtao Xie, Jianjun Xu, Yuxin Wang, and Yongdong Zhang.\nExploring stroke-level modifications for scene text editing. In AAAI, 2023.\n[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research (JMLR), 2020.\n[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[63] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n[64] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak\nLee. Generative adversarial text to image synthesis. In ICML, 2016.\n[65] Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H Li, Shan Liu, and Ge Li. Structureflow:\nImage inpainting via structure-aware appearance flow. In ICCV, 2019.\n[66] Juan A Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, and Pau Rodriguez.\nOcr-vqgan: Taming text-within-image generation. In WACV, 2023.\n[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n[68] Xuejian Rong, Chucai Yi, and Yingli Tian. Unambiguous scene text segmentation with\nreferring expression comprehension. IEEE Transactions on Image Processing (TIP), 2019.\n[69] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In MICCAI, 2015.\n[70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aber-\nman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\narXiv preprint arXiv:2208.12242, 2022.\n[71] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans,\nDavid J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In\nSIGGRAPH, 2022.\n[72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. In NeurIPS,\n2022.\n[73] Markus Schreiber, Fabian Poggenhans, and Christoph Stiller. Detecting symbols on road\nsurface for mapping and localization using ocr. In 17th International IEEE Conference on\nIntelligent Transportation Systems (ITSC), 2014.\n[74] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. In NeurIPS,\n2022.\n14\n[75] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[76] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023.\n[77] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-\nbased sequence recognition and its application to scene text recognition. IEEE Transactions\non Pattern Analysis and Machine Intelligence (T-PAMI), 2016.\n[78] Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, and Kota Yamaguchi. De-rendering\nstylized texts. In ICCV, 2021.\n[79] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In ICML, 2015.\n[80] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nICLR, 2021.\n[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[82] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif\nRasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models.\nhttps://github.com/huggingface/diffusers, 2022.\n[83] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. High-fidelity pluralistic image\ncompletion with transformers. In ICCV, 2021.\n[84] Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, and Shuai Shao. Shape\nrobust text detection with progressive scale expansion network. In CVPR, 2019.\n[85] Wenjia Wang, Enze Xie, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua Shen, and Xiang\nBai. Scene text image super-resolution in the wild. In ECCV, 2020.\n[86] Yuxin Wang, Hongtao Xie, Mengting Xing, Jing Wang, Shenggao Zhu, and Yongdong Zhang.\nDetecting tampered scene text in the wild. In ECCV, 2022.\n[87] James M White and Gene D Rohrer. Image thresholding for optical character recognition\nand other applications requiring character image extraction. IBM Journal of research and\ndevelopment, 1983.\n[88] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding, and Xiang\nBai. Editing text in the wild. In ACM MM, 2019.\n[89] Zizhang Wu, Xinyuan Chen, Jizheng Wang, Xiaoquan Wang, Yuanzhu Gan, Muqing Fang,\nand Tianhao Xu. Ocr-rtps: an ocr-based real-time positioning system for the valet parking.\nApplied Intelligence, 2023.\n[90] Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang, and Humphrey\nShi. Rethinking text segmentation: A novel dataset and a text-specific refinement approach. In\nCVPR, 2021.\n[91] Xixi Xu, Zhongang Qi, Jianqi Ma, Honglun Zhang, Ying Shan, and Xiaohu Qie. Bts: A\nbi-lingual benchmark for text segmentation in the wild. In CVPR, 2022.\n[92] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen,\nand Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv\npreprint arXiv:2211.13227, 2022.\n[93] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao,\nWentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey\nof methods and applications. arXiv preprint arXiv:2209.00796, 2022.\n15\n[94] Boxi Yu, Yong Xu, Yan Huang, Shuai Yang, and Jiaying Liu. Mask-guided gan for robust text\nediting in the scene. Neurocomputing, 2021.\n[95] Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han, Jingtuo Liu, and Errui Ding.\nTowards accurate scene text recognition with semantic reasoning networks. In CVPR, 2020.\n[96] Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian Lu, Feiying\nMa, Xuansong Xie, and Chunyan Miao. Diverse image inpainting with bidirectional and\nautoregressive transformers. In ACM MM, 2022.\n[97] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,\n2012.\n[98] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Learning pyramid-context\nencoder network for high-quality image inpainting. In CVPR, 2019.\n[99] Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, and Shiyu Chang. To-\nwards coherent image inpainting using denoising diffusion implicit models. arXiv preprint\narXiv:2304.03322, 2023.\n[100] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and\nDimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative\nadversarial networks. In ICCV, 2017.\n[101] Linjiang Zhang, Peng Wang, Hui Li, Zhen Li, Chunhua Shen, and Yanning Zhang.\nA\nrobust attentional framework for license plate recognition in the wild. IEEE Transactions on\nIntelligent Transportation Systems (TITS), 2020.\n[102] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[103] Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen Zuo, Haibo Chen, Wei Xing,\nand Dongming Lu. Uctgan: Diverse image inpainting based on unsupervised cross-space\ntranslation. In CVPR, 2020.\n[104] Minyi Zhao, Miao Wang, Fan Bai, Bingjia Li, Jie Wang, and Shuigeng Zhou. C3-stisr: Scene\ntext image super-resolution with triple clues. In IJCAI, 2022.\n[105] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic image completion. In CVPR,\n2019.\n[106] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun Liang.\nEast: an efficient and accurate scene text detector. In CVPR, 2017.\n[107] Xinyan Zu, Haiyang Yu, Bin Li, and Xiangyang Xue. Weakly-supervised text instance\nsegmentation. arXiv preprint arXiv:2303.10848, 2023.\n16\nAppendix\nA\nArchitecture of U-Net and Design of Character-Aware Loss\nAs shown in Figure 9, the U-Net contains four downsampling operations and four upsampling\noperations. The input will be downsampled to a maximum of 1/16. To provide the character-aware\nloss, the input feature F is 4-D with spatial size 64 \u00d7 64, while the output is 96-D (the length of\nalphabet A plus a null symbol indicating the non-character pixel) also with spatial size 64 \u00d7 64.\nSubsequently, a cross-entropy loss is calculated between the output feature (need to convert the\npredicted noise into predicted features) and the resized 64 \u00d7 64 character-level segmentation mask\nC\u2032. The U-Net is pre-trained using the training set of MARIO-10M for one epoch. We utilize the\nAdadelta optimizer [97] and set the learning rate to 1. When training the diffusion model, the U-Net\nis frozen and only used to provide character-aware guidance.\nFigure 9: The architecture of U-Net contains four downsampling and upsampling operations.\nB\nCharacter-Level Segmentation Model\nWe train the character-level segmentation model based on U-Net, whose architecture is similar to the\narchitecture shown in Figure 9. We set the input size to 256 \u00d7 256, ensuring that most characters are\nreadable at this resolution. We train the segmentation model using synthesized scene text images\n[19], printed text images, and handwritten text images3, totaling about 4M samples. We employ data\naugmentation strategies (e.g., blurring, rotation, and color enhancement) to make the segmentation\nmodel more robust. The segmentation model is trained for ten epochs using the Adadelta optimizer\n[97] with a learning rate of 1. Figure 10 shows some samples in the training dataset.\nFigure 10: Visualization of some training samples for the character-level segmentation model. The\ntop, middle, and bottom roles are samples from printed, handwritten, and scene datasets.\n3https://github.com/Belval/TextRecognitionDataGenerator\n17\nC\nMore Details in MARIO-10M\nTable 5: Number of texts per image in MARIO-10M.\n#Words\n1\n2\n3\n4\n5\n6\n7\n8\n#Images\n592,153\n1,148,481\n1,508,185\n1,610,056\n1,549,852\n1,430,750\n1,229,714\n930,809\n#Ratio\n5.9%\n11.5%\n15.1%\n16.1%\n15.5%\n14.3%\n12.3%\n9.3%\nMore samples are shown in Figure 11. The number of texts per image in MARIO-10M is shown\nin Table 5. Also, the MARIO-10M dataset reveals that about 90% of the text regions maintain\na horizontal orientation with rotation angles smaller than 5 degrees without perspective changes.\nHence, our layout generation model is designed to predict horizontal bounding boxes by detecting\nthe coordinates of their left-top and bottom-right points. Adapting our model to predict more realistic\nscene text is feasible by detecting enhanced coordinates, such as eight coordinates for four points.\nD\nMARIO-10M Caption Templates\nSince TMDB movie/TV posters and Open Library book covers have no off-the-shelf captions, we\nconstruct them based on their titles with the following templates. {XXX} is a placeholder for title.\nFor MARIO-TMDB:\n\u2022 Logo {XXX}\n\u2022 Text {XXX}\n\u2022 Title {XXX}\n\u2022 Title text {XXX}\n\u2022 A poster with a title text of {XXX}\n\u2022 A poster design with a title text\nof {XXX}\n\u2022 A quality movie print with a title\ntext of {XXX}\n\u2022 A film poster of {XXX}\n\u2022 A movie poster of {XXX}\n\u2022 A movie poster titled {XXX}\n\u2022 A movie poster named {XXX}\n\u2022 A movie poster with text {XXX} on it\n\u2022 A movie poster with logo {XXX} on it\n\u2022 A movie poster with a title text of {XXX}\n\u2022 An illustration of {XXX} movie\n\u2022 An photography of {XXX} movie\n\u2022 A TV show poster titled {XXX}\n\u2022 A TV show poster of {XXX}\n\u2022 A TV show poster with logo {XXX} on it\n\u2022 A TV show poster with a title text of {XXX}\n\u2022 A TV show poster with text {XXX}\n\u2022 A TV show poster named {XXX}\nFor MARIO-OpenLibrary:\n\u2022 A book with a title text of {XXX}\n\u2022 A book design with a title text of {XXX}\n\u2022 A book cover with a title text of {XXX}\n\u2022 A book of {XXX}\n\u2022 A cover named {XXX}\n\u2022 A cover titled {XXX}\n\u2022 A book with text {XXX} on it\n\u2022 A book cover with logo {XXX} on it\n18\nFigure 11: More samples in MARIO-10M.\n19\nE\nMARIO-10M Filtering Rules\nWe clean data with five strict filtering rules to obtain high-quality data with text:\n\u2022 Height and width are larger than 256. Low-resolution samples often contain illegible\ntexts, negatively impacting the training process.\n\u2022 Will not trigger NSFW. For the MARIO-LAION subset, we filter out those samples\ntriggering the \u201cnot sure for work\u201d flag to mitigate ethical concerns.\n\u2022 The number of detected text boxes should be within [1,8]. We detect texts with DB [42].\nSamples with too many texts typically have small areas for each text, which makes them\ndifficult to recognize. Therefore, we remove these samples from the dataset.\n\u2022 Text areas are more than 10% of the whole image. According to Appendix B, we train\na UNet [69] using SynthText [19] to obtain character-level segmentation masks of each\nsample. This criterion ensures that the text regions will not be too small.\n\u2022 At least one detected text appears in the caption. Noticing that the original dataset\ncontains many noisy samples, we add this constraint to increase the relevance between\nimages and captions. We utilize PARSeq [4] for text recognition.\n20\nF\nAnalysis of OCR Performance on MARIO-10M\nAs we rely on OCR tools to annotate MARIO-10M, it is necessary to evaluate the performance of\nthese tools. Specifically, we manually annotate 100 samples for text recognition, detection, and\ncharacter-level segmentation masks, then compare them with the annotations given by OCR tools.\nThe results are shown in Table 6. We notice that the performance of existing methods is lower\nthan their results on text detection and spotting benchmarks. Taking DB [42] as an example, it can\nachieve text detection 91.8% precision on ICDAR 2015 dataset [34] while only achieving 76% on\nMARIO-10M. This is because there are many challenging cases in MARIO-10M, such as blurry and\nsmall text. Besides, a domain gap may exist since DB is trained on scene text detection datasets,\nwhile MARIO-10M comprises text images in various scenarios. Future work may explore more\nadvanced recognition, detection, and segmentation models to mitigate the noise in OCR annotations.\nWe demonstrate some OCR results in Figure 12.\nTable 6: OCR performance on MARIO-10M. IOU (binary) means we treat each pixel as two classes:\ncharacters and non-characters. The evaluation of recognition is included in the spotting task.\nDetection\nSpotting\nSegmentation\nPrecision\nRecall\nF-measure\nPrecision\nRecall\nF-measure\nIOU (binary)\nIOU\n0.76\n0.79\n0.78\n0.73\n0.75\n0.74\n0.70\n0.59\nPride\nChitty\nabout\nhocol\nPeppalig\n2015\nGREENE\nHIGH\nMonkeY\nWOOL\n2003\nepop\nLONORES\nPOLISH\nDancesport\nRILEY\nCAPTURE\nLIES\nRELEASE\nSTA\nGRAVE\nIEANIENE\nDIARY\nFREE!\nFigure 12: Visualization of some OCR annotations in MARIO-10M.\n21\nG\nSamples in MARIO-Eval\nTable 7: Details of each subset in MARIO-Eval.\nSubset\nSize\nOff-the-shelf Captions\nGT Images\nLAIONEval4000\n4,000\n!\n!\nTMDBEval500\n500\n%\n!\nOpenLibraryEval500\n500\n%\n!\nDrawBenchText [72]\n21\n!\n%\nDrawTextCreative [46]\n175\n!\n%\nChineseDrawText [49]\n218\n!\n%\nAs illustrated in Table 7, MARIO-Eval contains 5,414 prompts with six subsets. The ground truth\nimages of some samples are shown in Figure 13, and captions for each category are shown below:\nLAIONEval4000:\n\u2022 \u2018Royal Green\u2019 Wristband Set\n\u2022 Are \u2018Digital Nomads\u2019 in Trouble Well Not Exactly\n\u2022 \u2018Sniper Elite\u2019 One Way Trip A Novel Audiobook by \u2018Scott\u2019 McEwen Thomas Koloniar\nNarrated by Brian Hutchison\n\u2022 \u2018Travel Artin\u2019 Logo\n\u2022 Falls the \u2018Shadow\u2019 Welsh Princes 2\nTMDBEval500:\n\u2022 A movie poster with text \u2018Heat\u2019 on it\n\u2022 A poster design with a title text of \u2018Deadpool 2\u2019\n\u2022 A TV show poster named \u2018Ira Finkelstein s Christmas\u2019\n\u2022 A movie poster with logo \u2018Playing for Change Songs Around The World Part 2 \u2019 on it\n\u2022 A movie poster titled \u2018Dreams of a Land\u2019\nOpenLibraryEval500:\n\u2022 A book cover with a title text of \u2018On The Apparel Of Women\u2019\n\u2022 A book with text \u2018Precalculus\u2019 on it\n\u2022 A book design with a title text of \u2018Dream master nightmare\u2019\n\u2022 A book cover with logo \u2018Thre Poetical Works of Constance Naden\u2019 on it\n\u2022 A book cover with a title text of \u2018Discovery\u2019\nDrawBenchText:\n\u2022 A storefront with \u2018Hello World\u2019 written on it.\n\u2022 A storefront with \u2018Text to Image\u2019 written on it.\n\u2022 A sign that says \u2018Diffusion\u2019.\n\u2022 A sign that says \u2018NeurIPS\u2019.\n\u2022 New York Skyline with \u2018Google Research Pizza Cafe\u2019 written with fireworks on the sky.\nDrawTextCreative:\n\u2022 a grumpy sunflower with a \u2018no solar panels\u2019 sign\n\u2022 A photo of a rabbit sipping coffee and reading a book. The book title \u2018The Adventures of\nPeter Rabbit\u2019 is visible.\n22\n\u2022 a graffiti art of the text \u2018free the pink\u2019 on a wall\n\u2022 A professionally designed logo for a bakery called \u2018Just What I Kneaded\u2019.\n\u2022 scholarly elephant reading a newspaper with the headline \u2018elephants take over the world\u2019\nChineseDrawText:\n\u2022 A street sign on the street reads \u2018Heaven rewards those who work hard\u2019\n\u2022 There is a book on the table with the title \u2018Girl in the Garden\u2019\n\u2022 Kitten holding a sign that reads \u2018I want fish\u2019\n\u2022 A robot writes \u2018Machine Learning\u2019 on a podium\n\u2022 In a hospital, a sign that says \u2018Do Not Disturb\u2019\nFigure 13: We demonstrate five samples for LAIONEval4000 (top), TMDBEval500 (middle), and\nOpenLibrary500 (bottom).\n23\nH\nImplementation Details of Evaluation Criteria\nTo evaluate the performance of TextDiffuser quantitatively, we utilize three criteria, including FID,\nCLIPScore, and OCR Evaluation. We detail the calculation of each criterion below.\nFID.\nWe calculate the FID score using the pytorch-fid repository. Please note that the proposed\nMARIO-Eval benchmark\u2019s three subsets (DrawTextCreative, DrawBenchText, and ChineseDrawText)\ndo not contain ground truth images. Therefore, we utilize 5,000 images in the other three subsets\n(LAIONEval4000, TMDBEval500, OpenLibraryEval500) as the ground truth images. We calculate\nthe FID score using the 5,414 generated images and the 5,000 ground truth images.\nCLIP Score.\nWe calculate the CLIP score using the clipscore repository. However, as with the\nFID score, we cannot calculate the CLIP score for the DrawTextCreative, DrawBenchText, and\nChineseDrawText subsets due to the lack of ground truth images. Therefore, we only calculate the\nscore on LAIONEval4000, TMDBEval500, and OpenLibraryEval500 subsets and report the average\nCLIP score.\nOCR Evaluation.\nFor the MARIO-Eval benchmark, we use quotation marks to indicate the\nkeywords that need to be painted on the image. Taking the caption [A cat holds a paper saying \u2018Hello\nWorld\u2019] as an example, the keywords are \u2018Hello\u2019 and \u2018World\u2019. We then use Microsoft Read API to\ndetect and recognize text in the image. We evaluate OCR performance using accuracy, precision,\nrecall, and F-measure. If the detected text matches the keywords exactly, it is considered correct.\nPrecision represents the proportion of detected text that matches the keywords, while recall represents\nthe proportion of keywords that appear in the image. We report the mean values of precision and\nrecall, and calculate the F-measure using the following formula:\nF-measure = 2 \u00d7 Precision \u00d7 Recall\nPrecision + Recall\n.\n(5)\nI\nVisualization of Layouts Generated by Layout Transformer\nWe visualize some generated layouts in Figure 14, showing that the Transformer can produce\nreasonable layouts.\nA robot writing 'Ethics 101'\nin chalk on a blackboard.\nA storefront with 'Google Research\nPizza Cafe' written on it.\nAn antique bottle labeled 'Energy Tonic'\nA giant shoe, with the\ncaption 'shoe for hokey pokey'\nA poster titled 'Quails of North America',\nshowing different kinds of quails.\nA storefront with 'Deep Learning' written on it.\nA storefront with 'Hello World' written on it.\nA sign that says 'Google Brain Toronto'.\nA sign that says 'NeurIPS'.\nFigure 14: Visualization of the generated layouts and images.\n24\nJ\nExperiment without Explicit Guidance of Segmentation Masks\nAs shown in Figure 15, we try to explore the generation without explicit guidance. For example,\naccording to the first row, we set the value of character pixels to 1 and non-character pixels to 0\n(i.e., remove the content and only provide the position guidance). We observe that the model can\ngenerate some words similar to keywords but contain some grammatical errors (e.g., a missing \u201cl\u201d in\n\u201cHello\u201d). Further, according to the second row, we train TextDiffuser without segmentation masks\n(i.e., remove both position and content guidance). In this case, the experiment is equivalent to directly\nfine-tuning a pre-trained latent diffusion model on the MARIO-10M dataset. The results show that\nthe text rendering quality worsens, demonstrating explicit guidance\u2019s significance.\nFigure 15: Visualization of generation without explicit guidance.\n25\nK\nBaseline Methods Experimental Settings\nWe introduced all baseline methods and their experimental settings when we used to compare them\nwith the TextDiffuser as follows.\nDALL\u00b7E [62] utilizes a text encoder to map a given prompt into a corresponding representation\nspace. A prior model is then employed to map the text encoding to an image encoding. Finally, an\nimage decoder generates an image based on the image encoding. Since there is no available code and\nmodel, we obtain the results using the provided API4.\nStable Diffusion (SD) utilizes CLIP [60] text encoder to obtain the embedding of user prompts,\npre-trained VAE to encode original images and conducts the diffusion process in the latent space for\ncomputation efficiency. We use the public pre-trained model \u201crunwayml/stable-diffusion-v1-5\u201d based\non Hugging Face diffusers [82]. The number of sampling steps is 50, and the scale of classifier-free\nguidance is 7.5.\nStable Diffusion XL (SD-XL) is an upgraded version of SD, featuring more parameters and\nutilizing a more powerful language model. Consequently, it can be expected to better understand\nprompts compared to SD. As the source code and model are not publicly available, we obtained the\ngeneration results through a web API5.\nMidjourney6 is a commercial project that runs on Discord, allowing users to interact with a bot\nvia the command-line interface. We generated images using the default parameters of Midjourney.\nFor example, we can generate an image using the following command: /imagine an image of \u2018hello\nworld\u2019 in Midjourney.\nControlNet [102] aims to control diffusion models by adding conditions using zero-convolution\nlayers. We use the public pre-trained model \u201clllyasviel/sd-controlnet-canny\u201d released by ControltNet\nauthors and the implementation from Hugging Face diffusers [82]. For fair comparisons, we use the\nprinted text images generated by our first-stage model to generate Canny maps as the condition of\nControlNet. We use default parameters for inference, where the low and high thresholds of canny\nmap generation are set to 100 and 200, respectively. The number of inference steps is 20, and the\nscale of classifier-free guidance is 7.5.\nDeepFloyd [12] designs three cascaded pixel-based diffusion modules to generate images of\nincreasing resolution: 64x64, 256x256, and 1024x1024. All stage modules use frozen text encoders\nbased on T5 Transformer [12]. Compared with CLIP [60], the T5 Transformer is a powerful language\nmodel that enables more effective text understanding. We use the public pretrained models released\nby DeepFloyd authors and the implementation from Hugging Face diffusers [82]. We use default\nmodels and parameters for inference, where the three pretrained cascaded models are \u201cDeepFloyd/IF-\nI-XL-v1.0\u201d, \u201cDeepFloyd/IF-II-L-v1.0\u201d, and \u201cstabilityai/stable-diffusion-x4-upscaler\u201d.\n4https://openai.com/product/dall-e-2\n5https://beta.dreamstudio.ai/generate\n6https://www.midjourney.com/\n26\nL\nVisualization of More Generation Results by Our TextDiffuser\nA movie poster\ntitled 'Gretel Hansel'\nA TV show poster with a \ntitle text of\n'Wendy and Lucy'\n'Let Your Light Shine'\n 18k overlay\nA book with text\n'Math for Life\n and Food Service'\n'problem solving steps'\nand 'skills'\nTMDBEval500\nA TV show poster\ntitled 'Tango argentino'\nlocksmith 'cctv' sign\n'Team' hat\nA cover titled\n'The world of cats'\nA book with text\n'Old Harthill' on it\nLAIONEval4000\nOpenLibraryEval500\nThanksgiving\n'Fam' Mens T Shirt\n'The Witch'\nHunter 'Tale'\n'Cheer' the 'new Year'\nTime Out\n'Lisboa' Magazine\nStupid 'History'\neBook Tales of\nStupidity Strangeness \nPhotos of\n'Sampa Hostel'\nA TV show poster with\nlogo 'Deep State' on it\nA movie poster\nof 'High Society'\nA movie poster\n titled 'Personalities'\nA movie poster titled\n'The Kindergarten Show'\nA TV show poster\nwith logo\n 'The Big Lebowski' 2\nA movie poster\nof 'Short Term 12'\nA TV show poster\nwith logo 'The Dry' on it\nA cover named\n 'Anything is possible'\nA book cover with logo\n 'Programming Skills\nThrough C' on it\nA cover named\nAll World 'Monster Map'\nA book of 'Marco Polo'\nA cover named\n'Green Lantern'\nA book of\n'Tideland treasure'\nA cover named\n'Wavewalker'\nFigure 16: Visualization of more generation results by our TextDiffuser in LAIONEval4000, TMD-\nBEval500, and OpenLibrary500.\n27\na giant shoe,\nwith the caption\n'shoe for hokey pokey'\n'Fall is here' written in\nautumn leaves\nfloating on a lake\na graffiti art of the text\n 'free the pink' on a wall\nA meme showing a cat\nattacking a shoe, with the\nmessage 'I own your shoe'\nplant in a fancy pot with\n a 'do not touch' sign\nA t-shirt with the\nmessage\n 'There is no planet B'\npillow in the shape\nof words\n'ready for the weekend'\na volcano erupting,\nwith the text\n'magma' in red\na logo for the company\n'diamonds', with a diamon\nin the shape of a heart\nA large recipe book\ntitled\n'Recipes from Peru'.\nA sign that\nsays 'NeurIPS'.\nNew York Skyline with\n 'Diffusion' written\nwith fireworks on the sky.\nA storefront with\n'Hello World'\nwritten on it.\nA storefront with\n 'Google Research Pizza Cafe'\n written on it.\nA storefront with\n 'Deep Learning'\nwritten on it.\nA storefront with\n 'Google Brain Toronto'\nwritten on it.\nA sign that says\n 'Hello World'.\nA sign that says\n 'Diffusion'.\nA sign that says\n 'Google Research\nPizza Cafe'.\nA storefront with\n 'NeurIPS' written on it.\nProhibition sign \n\"No Gambling\" \nhung on the entrance \nof the casino\nA globe with the words \n\"Planet Earth\" \nwritten in bold \nletters  with continents \nin bright colors\nA photo of roses \nsurrounded by \na sign in the distance \nthat reads \n\"Danger Minefield\"\nA newspaper headline read \n\"Local pig eats \nprize pumpkin\" \nand a photo showed \na half-eaten pumpkin\nLittle raccoon \nholding \na sign that reads\n \"I want to learn\"\nA \"No Smoking\" sign \nis placed in the hotel\nA sign saying \n\"Do not feed\"\n in the aquarium\nA pink bottle \nthat says \"LOVE\"\nA little girl is holding a \nbook with the words \n\"Fairy Tales\" in her hands\nBooks with the word \n\"Science\" printed \non them\nDrawTextCreative\nDrawBenchText\nChineseDrawText\nFigure 17: Visualization of more generation results by our TextDiffuser in DrawTextCreative,\nDrawBenchText, and ChineseDrawText.\n28\na mouse with a flashlight saying 'i am afraid of the dark'\nscholarly elephant reading a newspaper\nwith the headline 'elephants take over the world'\nA hand-drawn blueprint for a time machine,\nwith the caption 'Time Traveling Device'.\nphoto of a sign with 'having a dog\nnamed shark at the beach was a mistake'\na picture of the earth with\nthe words 'save the earth' in a circle\na lizard sitting on a baseball field home plate,\nwith the words 'make it safe' in a speech bubble\na 3d model of a 1980s-style computer\n with the text 'my old habit' on the screen\na pencil sketch of a tree\nwith the title 'nothing to tree here'\na dog holds a paper saying please adopt me\na stop pizza\na hello world banner\na stay calm quotes poster\nFigure 18: Visualization of more generation results by our TextDiffuser for the text-to-image with\ntemplate task.\n29\nFigure 19: Visualization of more generation results for text inpainting. The images above the dash\nlines are from the test set of MARIO-10M, while the images below the dash lines are collected from\nthe Midjourney community.\n30\nM\nMore Details about User Study\nUser study on the whole-image generation task.\nThe questionnaire consists of 15 cases, each of\nwhich includes two multiple-choice questions:\n\u2022 Which of the following images has the best text rendering quality?\n\u2022 Which of the following images best matches the text description?\nIn particular, the first question focuses on the text rendering quality. Taking Figure 20 as an example7,\nwe expect the model to render the word \u201cEcoGrow\u201d accurately (i.e., without any missing or additional\ncharacters). The second question, on the other hand, focuses on whether the overall image matches\nthe given prompt. For example, in Figure 20 (G), although the generated text is correct, it fails to\nmeet the requirement in the prompt that the letter looks like a plant. We instruct users to select the\nbest option. In cases where multiple good options are difficult to distinguish, they could choose\nmultiple options. If users are unsatisfied with the options, they could decide not to select any.\nFigure 20: One case in the user study for the whole-image generation task.\nUser study on the part-image generation task.\nWe aim to let users vote on the quality of text\ninpainting (from 4 to 1, the higher, the better). We also designed two questions:\n\u2022 How is the text rendering quality?\n\u2022 Does the drawn text harmonize with the unmasked region?\nSpecifically, the first question concentrates on the accuracy of the text. The second question focuses\non whether the generated part is harmonious with the unmasked part (i.e., whether the background\nand texture are consistent).\nFigure 21: One case in the user study for the part-image generation task.\n7(A) TextDiffuser; (B) DALL\u00b7E; (C) SD; (D) IF; (E) SD-XL; (F) Midjourney; (G) ControlNet.\n31\nN\nGenerating Images without Text\nTo show the generality of TextDiffuser, we experiment with generating images that do not contain\ntexts and show results in Figure 22. Although TextDiffuser is fine-tuned with MARIO-10M, it\nstill maintains a good generation ability for generating general images. Therefore, users have\nmore options when using TextDiffuser, demonstrating its flexibility. We also provide quantitative\nevaluations to demonstrate TextDiffuser\u2019s generality in generating non-text general images. We\ncompare TextDiffuser with our baseline Stable Diffusion 1.5 as they have the same backbone. For\na quantitative evaluation, the FID scores of 5,000 images generated by prompts randomly sampled\nfrom MSCOCO are as in Table 8. The results indicate that TextDiffuser can maintain the ability to\ngenerate natural images even after fine-tuning the domain-specific dataset.\nTable 8: FID scores on MSCOCO compared with Stable Diffusion.\nSampling Steps\nStable Diffusion\nTextDiffuser\n50\n26.47\n27.72\n100\n27.02\n27.04\nFigure 22: Visualizations of general images generated by Stable Diffusion 1.5 and TextDiffuser.\n32\nO\nComparisons between TextDiffuser and a Text Editing Model\nWe visualize some results in Figure 23 compared with a text editing model SRNet [88]. Please note\nthat the introduced text inpainting task differs from the text editing task in three aspects: (1) The text\nediting task usually relies on the synthesized text image dataset for training (synthesizing two images\nwith the different text given a background image and font as pairs). In contrast, the text inpainting\ntask follows the mask-and-recover training scheme and can be trained with any text images. (2) Text\nediting emphasizes the preservation of the original fonts, while text inpainting allows for greater\nfreedom. For example, we conduct four samplings for each case, and the generated results exhibit\ndiversity and present reasonable font styles. (3) Text editing tasks cannot add text, highlighting the\nsignificance of the introduced text inpainting task.\nFigure 23: Comparison with text editing model. Four cases are obtained from the paper of SRNet.\n33\nP\nExperimental Results of Text Removal\nWe demonstrate some results of text removal in Figure 24, and the cases are obtained from the\npaper of EraseNet [44]. We can easily transform the text inpainting task into a text removal task by\nproviding a mask and setting all regions to non-character in the segmentation mask. Experimental\nresults demonstrate that our method can achieve results similar to the ground truth.\nFigure 24: Experimental results on text removal. Four cases are obtained from the paper of EraseNet.\nQ\nLimitations and Failure Cases\nWe observe failure cases when generating images with small characters and from long text.\nGenerating images with small characters.\nTextDiffuser uses the VAE networks to encode images\ninto low-dimensional latent spaces for computational efficiency following latent diffusion models\n[67, 49, 2]. However, the compression process can result in losing details when generating images\nwith small characters. As illustrated in Figure 25, we observe that the VAE fails to reconstruct small\ncharacters, where reconstructed strokes are unclear and reduce the legibility of the text. According\nto the generated images, the small characters appear to have vague or disjointed strokes (e.g., the\ncharacter \u2018l\u2019 in \u2018World\u2019 and character \u2018r\u2019 in \u2018Morning\u2019), which could impact the readability. As\nshown in Figure 26, we notice that using a more powerful backbone, such as Stable Diffusion 2.1,\ncan mitigate this issue. When the image resolution is enhanced from 512\u00d7512 to 768\u00d7768 using\nStable Diffusion 2.1 (instead of 1.5), the latent space resolution also increases from 64\u00d764 to 96\u00d796,\nenhancing the character-level representation. As the cost, the inference latency rises from 8.5s to\n12.0s with a batch size of 1. Therefore, how to render small characters while maintaining the same\ntime cost is worth further study.\nGenerating images from long text.\nWe observed failure cases when generating images from long\ntext with many keywords, where the generated words in the layouts are disordered and overlapped,\n34\nOriginal Image\nReconstructed Image\na board of 'Hello World'\na board of 'Good Morning'\nGenerated Image\nFigure 25: The issue of generating images with small characters.\nFigure 26: Pre-trained on high-resolution Stable Diffusion 2.1 enhances the legibility of small text.\nas shown in Figure 27. One possible reason could be that training examples containing numerous\nkeywords tend to have more noise (i.e., those images usually contain dense and small text), leading to\na higher likelihood of detection and recognition errors. To address this, we could consider enhancing\nthe capability of OCR tools in the future to mitigate the noise.\nan image of 'I have a\ndream during my 17'\na board of 'My friend\nplease adopt this cat'\na meme of 'please do\nnot eat my hamburger'\na book of 'my favorite\n sport is ping pong'\nFigure 27: The issue of dealing with a large number of keywords.\n35\n"
  },
  {
    "title": "Going Denser with Open-Vocabulary Part Segmentation",
    "link": "https://arxiv.org/pdf/2305.11173.pdf",
    "upvote": "1",
    "text": "Going Denser with Open-Vocabulary Part Segmentation\nPeize Sun1 Shoufa Chen1 Chenchen Zhu2 Fanyi Xiao2 Ping Luo1 Saining Xie3 Zhicheng Yan2\n1The University of Hong Kong\n2Meta AI\n3New York University\nhttps://github.com/facebookresearch/VLPart\nFigure 1. Examples of open-vocabulary part segmentation. Beyond open-vocabulary object detection, we propose that the detector\nshould be able to predict both objects and their parts. This open-world \ufb01ne-grained recognition ability is in demand for an intelligent vision\nsystem but is only realized in a limited number of categories [11,31,69] up to now. In this paper, we move forward to going denser with\nopen-vocabulary part segmentation: Left \ufb01gure shows segmenting dog and its parts in different granularities. Right \ufb01gure demonstrates\nmore visualization results.\nAbstract\nObject detection has been expanded from a limited num-\nber of categories to open vocabulary. Moving forward, a\ncomplete intelligent vision system requires understanding\nmore \ufb01ne-grained object descriptions, object parts. In this\npaper, we propose a detector with the ability to predict both\nopen-vocabulary objects and their part segmentation. This\nability comes from two designs. First, we train the detector\non the joint of part-level, object-level and image-level data\nto build the multi-granularity alignment between language\nand image. Second, we parse the novel object into its parts\nby its dense semantic correspondence with the base object.\nThese two designs enable the detector to largely bene\ufb01t\nfrom various data sources and foundation models. In open-\nvocabulary part segmentation experiments, our method out-\nperforms the baseline by 3.3\u223c7.3 mAP in cross-dataset gen-\neralization on PartImageNet, and improves the baseline by\n7.3 novel AP50 in cross-category generalization on Pascal\nPart. Finally, we train a detector that generalizes to a wide\nrange of part segmentation datasets while achieving better\nperformance than dataset-speci\ufb01c training.\n1. Introduction\nRecent advances in open-vocabulary object detec-\ntion [39,44,49,60,80,87,94,97] have made surprising devel-\nopment in enlarging the number of object categories from a\npre-determined set by training datasets [19,28,51,73] to any\nobject in the open world. This is a crucial step for the vision\nsystem to make effect in the real world. Towards the next\nstep, for a deeper understanding to object structure, mobil-\nity, functionality, and practical applications such as behav-\nior analysis [63,70,84], robotics manipulation [5,21,62,90],\nimage-editing [47, 72], only object-level perception is not\nsuf\ufb01cient, while the \ufb01ne-grained recognition ability of part\nsegmentation [11,31,69,85] is necessary.\nSince a part is the \ufb01ne-grained version of an object, an\nintuitive idea is to directly apply existing open-vocabulary\nobject detection methods [44, 49, 94, 97] to solve the part\ndetection/segmentation task. However, as shown in Table 1,\nthey do not show good generalization on part-level recog-\nnition. Although conceptually similar to open-vocabulary\nobject detection, localizing and classifying the \ufb01ne-grained\nobject parts are essentially more challenging. This moti-\n1\narXiv:2305.11173v1  [cs.CV]  18 May 2023\nMethod\ndog\nhead\nleg\npaw\ntail\ntorso\nRegionCLIP [94]\n5.2\n0.1\n0.2\n0.0\n1.9\nDetic [97]\n3.2\n0.0\n0.0\n0.0\n2.0\nVLDet [49]\n3.5\n0.0\n0.0\n0.0\n1.9\nGLIP [44]\n32.6\n3.1\n2.7\n9.5\n2.2\nOracle\n50.7\n14.8\n20.7\n10.4\n18.7\nTable 1. Performance of previous open-vocabulary object de-\ntection methods on Pascal Part [11] validation set. The evalu-\nation metric is mAPbox@[.5, .95] on the detailed metrics of dog.\nAll models use their of\ufb01cial codebases and model weights. Oracle\nis the method trained on Pascal Part training set.\nvates us to explore new designs to empower current object\ndetectors with open-vocabulary part segmentation ability.\nThe model of open-vocabulary part segmentation is sup-\nposed to be able to segment the object not only on open cat-\negory but also on open granularity. As shown in Figure 1,\nthe [dog] can be parsed to the [head, torso, leg,\ntail], while in the \ufb01ner granularity, the head of a dog can\nbe further parsed to the [ear, eye, nose, etc.]. Anno-\ntating such \ufb01ne-grained object part is extremely expensive.\nPublicly available datasets of part segmentation are less rich\nand diverse than those of image classi\ufb01cation and object de-\ntection datasets. Even though we collect three sources of\npart segmentation datasets, including Pascal Part [11], Par-\ntImageNet [31], and PACO [69], only a small number of\nobjects part are accessible.\nTo expand the vocabulary of part categories, we \ufb01rst seek\nto utilize the large vocabulary object-level and image-level\ndata, such as LVIS [28] and ImageNet [15], where object\ncategories are known, but their part locations or part names\nare not. To enable part segmentation task bene\ufb01t from them,\nour detector is based on the vision-language model [68], and\ntrained on the joint of part-level, object-level and image-\nlevel data, where the classi\ufb01er weight in the detector is re-\nplaced to the text embedding of the class name. In this\nway, the model learns to align vision and language at multi-\ngranularity level to help generalize the ability to parse the\nobject into its parts from base objects to novel objects.\nThough the multi-granularity alignment is established,\nthe part-level alignment for novel objects is fragile since its\nsupervision signal is absent. To further strengthen it, we\npropose to leverage the pre-trained foundation models [8]\nto parse the novel object into its parts as the annotations:\n1) We \ufb01nd the nearest base object for each novel object by\nthe similarity of their global features. 2) We build the dense\nsemantic correspondence between the novel object and its\ncorresponding base object by the similarity of their spatial\nfeatures. 3) We parse the novel object into its parts in the\nway of the base object by the correspondence. The name of\nnovel parts follows its corresponding base object. Accord-\ning to this pipeline, we generate the parsed images and use\nthem as part annotations of novel objects.\nExtensive experiments demonstrate that our method can\nsigni\ufb01cantly improve the open-vocabulary part segmen-\ntation performance.\nFor cross-dataset generalization on\nPartImageNet, our method outperforms the baseline by\n3.3\u223c7.3 mAP. For cross-category generalization within\nPascal Part, our approach improves the baseline by 7.3 AP50\non novel parts. Finally, we train a detector with the joint\ndata of LVIS, ImageNet, PACO, Pascal Part, PartImageNet,\nand parsed ImageNet. On three trained part segmentation\ndatasets, it obtains better performance than their dataset-\nspeci\ufb01c training. Meanwhile, part segmentation on a large\nrange of objects in the open-world is achieved , as shown in\nFigure 1.\nOur contributions are summarized as follows:\n\u2022 We set up benchmarks and baseline models for open-\nvocabulary part segmentation in Pascal Part and PartIma-\ngeNet datasets.\n\u2022 We propose a parsing pipeline to enable part segmenta-\ntion to bene\ufb01t from various data sources and expand the\nvocabulary of part categories.\n\u2022 We train a detector with the ability of open-vocabulary\nobject detection and part segmentation, achieving favor-\nable performance on a wide range of part segmentation\ndatasets.\n2. Related Work\nOpen-vocabulary object detection.\nOVOD [87] aims\nto improve the generalization ability of object detectors\nfrom seen categories to novel categories.\nFor example,\nViLD [27], RegionCLIP [68], PB-OVD [20] use pseudo\nregion annotations generated from the pre-trained vision-\nlanguage model [42,68]. DetPro [18] designs an automatic\nprompt learning method to improve the category embedding\neffectively. GLIP [44] trains the detector on both detec-\ntion and grounding data. Detic [97] enlarges the number of\nnovel classes with image classi\ufb01cation data. VLDet [49] ex-\ntracts region-word pairs from image-text pairs in an online\nway. Different from these works, we explore more \ufb01ne-\ngrained object recognition at the part level.\nPart segmentation. Beyond recognizing objects through\ncategory labels, a more \ufb01ne-grained understanding of ob-\njects at the part level [14, 45, 58, 96] is in increasing de-\nmand. Some pioneering works provide part annotations for\nspeci\ufb01c domains, such as human [23, 43, 84], birds [76],\ncars [70, 74], fashion domain [35, 93].\nPart annotations\nfor common objects include such as Pascal-Part [11], Part-\nNet [61], PartImageNet [31], ADE20K [95], Cityscapes-\nPanoptic-Parts [57] and more recent PACO [69]. Based on\nthese valuable datasets, our work is towards parsing any ob-\nject in the open world.\n2\nPascal Part\nPartImageNet\nFigure 2. Different part taxonomies in different dataset anno-\ntations. For example, the [dog] is parsed to the [head, body,\nfoot, tail] in PartImageNet [31]. In the \ufb01ner granularity, the\nhead of a dog is further parsed to the [ear, eye, muzzle,\nnose] in Pascal Part [11].\nVision-and-language representation learning. A univer-\nsal representation for vision and language is needed in\nvarious tasks such as visual question answering [25, 29],\nimage/video-text retrieval [51,64,81], visual reasoning [22,\n75] and so on. To enhance the visual representation, region-\nbased features obtained from object detection are intro-\nduced. For instance, OSCAR [46] uses object tags and re-\ngion features to train a universal semantic, UNITER [12]\nestablishes word-region alignments by supervising similar\noutput across multiple modalities, ALBEF [42] aligns the\nimage and text before fusing them with a multimodal en-\ncoder, SimVLM [78] reduces the requirement to regional\nlabels by exploiting large-scale weak supervision. Our work\nis aimed at learning part-level visual representation aligned\nwith language supervision.\nSemantic correspondence.\nThe aim of semantic corre-\nspondence [30,36,40,54,59,82,91] is to establish the spatial\nvisual correspondence between different instances of the\nsame object category. Cross-domain correspondence [1,88]\nexpands it to different categories. The pre-trained model\nis usually introduced to compute the feature map similar-\nity. [1] used a pre-trained CNN and [3] improved the perfor-\nmance by using ViT model [3, 16]. In our work, we apply\nself-supervised DINO [8] to build the semantic correspon-\ndence between the novel object and the base object.\n3. Open-Vocabulary Part Segmentation\nThe goal of open-vocabulary part segmentation is to\nparse any object in the wild into its components. The model\ntakes as input the image and outputs its part segmentation\nby the pre-de\ufb01ned part taxonomy or custom text prompt.\n3.1. Open Category and Granularity\nWe expect the model for open-vocabulary part segmen-\ntation to be able to provide the part segmentation in both\nopen category and open granularity.\nOpen category. Similar to object detection, open category\nmeans that the model is able to parse any category of the\nobject in the wild.\nOpen granularity. As shown in Figure 2, the part tax-\nonomy is inherently hierarchical, the [dog] can be parsed\nto the [head, body, leg, tail], while in the \ufb01ner\ngranularity, the head of a dog can be further parsed to the\n[ear, eye, muzzle, nose, etc.]. This brings to in-\nconsistent de\ufb01nitions in different datasets, for example, the\ntaxonomy of Pascal Part [11] is relatively \ufb01ner than Ima-\ngeNetPart [31].\n3.2. Evaluation Protocol\nFor terminology, base parts are the parts of base objects\n(seen in the training set), and novel parts are the parts of\nnovel objects (unseen in the training set).\nCross-category generalization. The models are trained on\nthe base parts, such as parts of cat, cow, horse, and sheep,\nand evaluated on novel categories, such as parts of dog. In\nthis setting, the base categories and the novel categories are\nmutually exclusive.\nCross-dataset generalization. In practice use, the trained\nmodel could be used in any evaluation dataset, therefore,\nthe object categories in inference may overlap with those\nin training. We use a cross-dataset setting to evaluate these\nmore practical scenes.\n3.3. Revisit Open-Vocabulary Object Detection\nOne may assume that the part instance is a special type\nof object, and the open-vocabulary part segmentation task\ncan be solved by off-the-shelf open-vocabulary object de-\ntection/segmentation methods [44,49,94,97].\nAdapting open-vocabulary object detector to part recog-\nnition. To adapt the open-vocabulary object detector to part\nsegmentation, its classi\ufb01er weight in the region recognition\nhead needs to be replaced by text embedding of the part\ncategory name. The performance of some popular open-\nvocabulary object detectors on part segmentation is shown\nin Table 1. Obviously, their performances are far from sat-\nisfactory.\nWe analyze the limited performance of open-\nvocabulary object detection on part segmentation comes\nfrom two folds: (i) Recall. The part and the object have\ndifferent granularities, it is non-trivial for region proposals\ntrained on object-level data to generalize to part-level. (ii)\nPrecision. The learning materials of open-vocabulary ob-\nject detectors, object-level and image-level data, have in-\nsuf\ufb01cient part instances and are hard to provide effective\npart-level supervision.\nGeneralizability of region proposals. We study whether\nregion proposal networks trained on object-level data could\n3\nTraining data\nType\nPascal Part\nAR@30 AR@100 AR@300 AR@1000\nVOC\nobject\n7.7\n11.8\n15.4\n16.1\nCOCO\nobject\n8.4\n14.8\n24.4\n40.5\nLVIS\nobject\n12.7\n20.6\n30.0\n45.8\nPascal Part base\npart\n29.4\n48.1\n63.6\n75.3\nPascal Part\npart\n31.1\n50.5\n67.2\n78.8\nTable 2. Evaluation on the generalizability of region proposals\non objects and parts. The recall is evaluated at IoU threshold\n0.5 on the validation set of Pascal Part. All models are ResNet50\nMask R-CNN. The upper section is trained on object-level data\nand the lower section is part-level data. It is non-trivial for region\nproposals to generalize from object-level to part-level.\n- The cat is sleeping on the sofa by the \nremote control.\n- A cat laying on the couch next to a remote \nand a pillow.\n- A cat laying on top of a couch near a jacket.\n- A cat sleeps on a couch by a book, a banana \nand a shirt.\n- A cat with its paw over its face lying next to \na remote and a banana.\nFigure 3. Example of COCO Caption [10]. COCO Caption data\nprovides the image and its corresponding caption only, without\nobject-level alignment (solid box) or part-level alignment (dashed\nbox). Even if all alignments are known, part descriptions are much\nless frequent than object descriptions.\nprovide suf\ufb01cient object proposals for the part. Although\nprevious works [27,97] conclude that novel categories only\nsuffer a small performance drop in recall, we point out that\nthis is not the case when the novel objects have different\ngranularity from object to part. As shown in Table 2, the\ndetector trained on object-level data only has limited recall\non the part dataset. To obtain better recall, part-level an-\nnotations are necessary, evidenced by the model trained on\nthe Pascal Part base having very close recall with the fully-\nsupervision model on the full Pascal Part training set.\nPart-level alignment between image and its caption.\nOpen-vocabulary object detection methods usually use im-\nage caption data to train the model. However, learning to\ndetect the object part in the image from its image caption\nhas two challenges: (1) Image caption data only provides\nthe image and its corresponding caption, without dense cap-\ntions on objects.\nEach open vocabulary object detector\nmethod [44, 49, 94, 97] needs to design its own method to\nalign objects in the image and in the caption. (2) Even if the\nalignment could be extracted from the caption, or provided\nby the dataset annotations [38, 65, 86], we \ufb01nd the caption\ncontains object parts less frequently than objects, as shown\nin Figure 3. This less frequency makes part-level alignment\nbetween the image and its caption more dif\ufb01cult to learn\nthan the object-level.\n4. Our Method\nOur detector architecture is a vision-language version of\nMask R-CNN [32], where the classi\ufb01er is the text embed-\nding of category name from CLIP [68]. This enables us to\nseamlessly train the detector on part-level, object-level and\nimage-level data. We further parse the image data into its\nparts to expand the vocabulary of part categories, which is\nbased on dense semantic correspondence between the base\nobject and the novel object extracted from DINO [8].\n4.1. Detector Architecture\nImage encoder. The image encoder is based on convolu-\ntional neural networks such as ResNet [33] or Transformer-\nbased models like Swin [56], followed by Feature Pyramid\nNetwork [50] to generate multi-scale feature maps to be\nused in the detection decoder.\nDetection decoder. The architecture of detection decoder\nis composed of a region proposal network (RPN) [71] and\na R-CNN recognition head. RPN provides box proposals\nfor both objects and parts. R-CNN recognition head re\ufb01nes\nthe box location and the classi\ufb01cation score. Notably, the\nclassi\ufb01er weight in the recognition head is replaced by text\nembedding of the class name of the object and the part.\nText embedding as the classi\ufb01er. The classi\ufb01cation score\nof the recognition head is implemented as a dot-product\noperation between the region features and the text embed-\ndings, where the region features are cropped from feature\nmaps of the image encoder, and the text embeddings are\nextracted from the text encoder in CLIP [68].\nMask decoder. We choose the architecture of mask decoder\nfrom Mask R-CNN [32] and replace the original multi-\nclassi\ufb01cation head with a class-agnostic head to support\nsegmentation on novel categories. We note that more ad-\nvanced architecture such as Mask2Former [13] has the po-\ntential to further improve the performance but is not the fo-\ncus of this work.\n4.2. Training on Parts, Objects, and Images\nThe training data includes part-level, object-level, and\nimage-level data. The image data is further parsed into the\npart annotation. Our detector is joint-trained on these data\nto establish multi-granularity alignment.\nPart segmentation data. Part segmentation data [11, 31,\n69] contains part mask segmentation and its category. Part\nis always de\ufb01ned as an object-part pair since the same se-\nmantic part can be very different when it is associated with\ndifferent objects. The category name of the part is formal-\nized as follows:\nCpart = [\u201cdog: head\u201d, \u201cdog: nose\u201d, ..., \u201ccat: tail\u201d]\n4\n(a) Find nearest base object\nBase \ncategory\nNovel \ncategory\n(c) Parse novel object\n(b) Build semantic correspondence\nFigure 4. The pipeline of parsing novel objects into parts. (a) Finding the nearest base object for each novel object. (b) Building the\ndense semantic correspondence between a novel object and its corresponding base object. For better visualization, we only some points\nsampled from the feature map grid. (c) Parsing the novel object as the way of the base object.\nObject detection data.\nObject detection data contains\nobject boxes and its category.\nMost object detection\ndatasets [28,51] also provide object mask segmentation an-\nnotations.\nCobject = [\u201cperson\u201d, \u201cbicycle\u201d, ..., \u201ctoothbrush\u201d]\nThe training loss for part and object data includes all loca-\ntion loss, classi\ufb01cation loss, and mask loss.\nImage classi\ufb01cation data. Image classi\ufb01cation data pro-\nvides a large vocabulary of object categories in the form of\nimages. Although object-level or part-level bounding anno-\ntations are absent, these images could be effectively used\nby the following ways: (1) The classi\ufb01cation loss can be\nperformed on max-size proposal [97] for each image, and\ntherefore expands the object-level vocabulary. (2) As will\nbe introduced in section 4.3, the image can be parsed into\nparts and used as part-level annotations to expand the vo-\ncabulary of part categories. The training loss about image\ndata only includes classi\ufb01cation loss.\n4.3. Parsing Novel Objects into Parts\nMost novel objects share the same part taxonomy with\none of the base objects, for example, the novel dog has the\nsame parts as the base cat. Since the part segmentation of\nthe base object is known, we could parse the novel object\naccording to its dense semantic correspondence to the base\nobject. The whole pipeline is shown in Figure 4.\nFinding the nearest base object for each novel object.\nWe use DINO [8] to extract the [class token] of each\nbase object, denoted as tcls(\u00b7), and save these features as\nthe database. Then, for each novel object i, we extract its\nfeature using the same way and \ufb01nd its nearest base object\ninear in the database by the cosine similarity.\ninear = arg max\nj\nsim(tcls(Ii), tcls(Ij))\nBuilding dense semantic correspondence between the\nbase object and its nearest novel object.\nWe further\nuse the DINO feature map as dense visual descriptors [3],\ndenoted as Fx,y(\u00b7), where x, y are grid indexes in the\nfeature map.\nAfter computing the spatial similarity be-\ntween the novel object Fx,y(Ii) and its nearest base object\nFp,q(Iinear), for each token (x, y) in the novel object, its\ncorresponding token in the base object are chosen as the to-\nken with the highest cosine similarity.\nxcorr, ycorr = arg max\np,q\nsim(Fx,y(Ii), Fp,q(Iinear))\nParsing novel parts by semantic correspondence. After\ndense correspondence between the base object and novel\nobject is obtained, we could parse the novel object into its\npart segmentation Mi(x, y) as the way of its corresponding\nbase object part segmentation Minear(p, q).\nMi(x, y) = Minear(xcorr, ycorr)\nA hybrid parser to base and novel objects. Figure 4 also\nprovides some examples of semantic correspondence and\nparsed novel objects. It can be seen that the strong and clear\nalignment is set up and the produced part segmentation is\nquali\ufb01ed to be used as pseudo part annotation for the novel\nobject. For the base object, we use the detector trained on\nbase parts to generate its pseudo part annotation.\n4.4. Inference on Text Prompt\nIn inference, the model takes as input the image and out-\nputs the part segmentation for the object. Since all vocab-\nulary of both objects and parts are a large number, and the\nuser may not be interested in obtaining all possible object\nand part segmentation, our detector supports inference on\ntext prompt by user input.\n5\nThe left section of Figure 1 is a case using a dog\nas an example.\nWhen the user-input is [dog], [dog:\nhead, torso, leg, tail]\nand\n[dog:\nhead,\near, eye, nose, torso, leg, paw, tail],\nthe detector outputs the segmentation results in different\ngranularities accordingly.\nThe right section of Figure 1\nis a range of objects in the open world.\nIt can be seen\nthat our model is able to detect both open-vocabulary\nobjects and their parts. When our detector is used in real\napplications, one can \ufb02exibly choose to use the pre-de\ufb01ned\npart taxonomy in datasets such as Pascal Part, PACO, or\ncustom text prompt.\n5. Experiment\n5.1. Datasets\nWe use three sources of part segmentation datasets, Pas-\ncal Part [11], PartImageNet [31] and PACO [69].\nPascal Part. The original Pascal Part provides part anno-\ntations of 20 Pascal VOC classes, a total of 193 part cate-\ngories. Its taxonomy contains many positional descriptors,\nwhich is not suitable for this paper, and we modify its part\ntaxonomy into 93 part categories.\nPartImageNet. PartImageNet groups 158 classes from Im-\nageNet into 11 super-categories and provides their part an-\nnotations, a total of 40 part categories.\nPACO. PACO supplements more electronic equipment, ap-\npliances, accessories, and furniture than Pascal Part and\nPartImageNet. PACO contains 75 object categories, 456\nobject-part categories and 55 attributes. The image sources\nof PACO are LVIS and Ego4D [26]. In this work, we use\nPACO-LVIS set as default. We focus on object parts and\nleave attributes for future research.\nFor object-level detection data, we use VOC [19],\nCOCO [51] and LVIS [28]. For image-level data, we use\nImageNet1k (IN) [15]. We also create ImageNet-super11\n(IN-S11) and ImageNet-super20 (IN-S20) that overlap with\nPartImageNet and Pascal category vocabulary separately.\nMore details about datasets are in Appendix A.\n5.2. Cross-dataset segmentation on PartImageNet\nIn Table 3, we study cross-dataset generalization by us-\ning PartImageNet validation set as the evaluation dataset,\nwhere the metrics of all (40) parts and the detailed metrics\nof parts of quadruped are reported.\nTable 3a shows when Pascal Part is the only available\nhuman-annotated part dataset, using IN-S11 data could help\nto improve PartImageNet performance.\nBaseline from Pascal Part. The baseline method directly\nuses the Pascal Part-trained model to evaluate PartIma-\ngeNet. As shown in Table 3a \ufb01rst row, the performance is\npoor, for example, body and foot of the quadruped\nMethod\nAll\nquadruped\n(40)\nhead\nbody\nfoot\ntail\nPascal Part\n4.5\n17.4\n0.1\n0.0\n2.9\n+ IN-S11 label\n5.4\n23.6\n3.4\n0.8\n1.2\n+ Parsed IN-S11\n7.8\n35.0\n15.2\n3.5\n8.9\nvs. baseline\n+3.3\n+17.6\n+15.1\n+3.5\n+6.0\nPartImageNet\n29.7\n57.3\n25.8\n22.9\n22.9\n(a) Cross-dataset generalization when only one part dataset, Pascal\nPart, is available. Pascal Part is trained on the Pascal Part training set.\nIN-S11 label and Parsed IN-S11 are added into the training sequentially.\nMethod\nAll\nquadruped\n(40)\nhead\nbody\nfoot\ntail\nPascal Part\n4.5\n17.4\n0.1\n0.0\n2.9\n+ LVIS, PACO\n7.8\n22.9\n7.1\n0.3\n4.0\n+ IN-S11 label\n8.8\n26.3\n3.7\n0.4\n1.0\n+ Parsed IN-S11\n11.8\n47.5\n13.4\n4.5\n14.8\nvs. baseline\n+7.3\n+30.1\n+13.3\n+4.5\n+11.9\nPartImageNet\n29.7\n57.3\n25.8\n22.9\n22.9\n(b) Cross-dataset generalization when more than one part datasets\nare available.\nStarting from Pascal Part, LVIS, PACO, IN-S11 and\nParsed IN-S11 are added into the training sequentially.\nTable 3. Cross-dataset generalization on PartImageNet part\nsegmentation. The evaluation metric is mAPmask@[.5, .95] on\nthe validation set of PartImageNet. All models are ResNet50 Mask\nR-CNN and use the text embedding of the category name as the\nclassi\ufb01er. PartImageNet is the fully-supervised method as the ora-\ncle performance.\nare nearly to zero. Pascal Part has no semantic label of\nquadruped, and the model needs to generalize from parts\nof dog, cat, etc. in Pascal Part to parts of quadruped\nin PartImageNet. The possible generalization ability comes\nfrom the text embedding generated from CLIP [68]. How-\never, generalization in part-level recognition is beyond its\ncapability since CLIP is pre-trained on only image-level\ndata.\nIN-S11 label. Considering that Pascal Part has no seman-\ntic label such as quadruped, piped, etc., we collect IN-\nS11 images from ImageNet and add them to the training as\nimage-level classi\ufb01cation data. As shown in Table 3a sec-\nond row, the performance is improved to some extent. This\nshows that image-level alignment is bene\ufb01cial to the part\nrecognition task. However, since no additional part-level\nsupervision signal is introduced when using IN-S11 as im-\nage classi\ufb01cation data, the improvement is still limited.\nParsed IN-S11. We use our parsing pipeline to deal with\nIN-S11 images and generate their part annotations.\nAs\nshown in the third row in Table 3a, introducing these parsed\nparts into the training brings a signi\ufb01cant improvement,\n3.5\u223c17.6 mAP improvement on the parts of quadruped\n6\nMethod\nAll (93)\nBase (77)\nNovel (16)\nAP\nAP50\nAP\nAP50\nAP\nAP50\nBase part\n15.0\n33.4\n17.8\n39.6\n1.5\n3.7\n+ VOC object\n16.8\n36.8\n19.9\n43.3\n2.1\n5.9\n+ IN-S20 label\n17.4\n37.5\n20.8\n44.7\n1.1\n3.1\n+ Parsed IN-S20\n18.4\n39.4\n21.3\n45.3\n4.2\n11.0\nvs. baseline\n+3.4\n+6.0\n+3.5\n+5.7\n+2.7\n+7.3\nPascal Part\n19.4\n42.7\n18.8\n41.5\n22.1\n48.9\nTable 4. Cross-category generalization on Pascal Part part seg-\nmentation. The evaluation metric is on the validation set of the\nPascal Part. All models are ResNet50 Mask R-CNN and use the\ntext embedding of the category name as the classi\ufb01er. Base part\nis the base split from Pascal Part. VOC object, IN-S20 label and\nParsed IN-S20 are added into the training sequentially. Pascal Part\nis the fully-supervised method as the oracle performance.\nand 3.3 mAP gain on all 40 parts over the baseline method.\nThis suggests that our proposed methods are able to pro-\nvide an effective part-level supervision signal to the detec-\ntion model and boosts its performance on cross-dataset gen-\neralization.\nMore part datasets are available. Table 3b shows when\nmore than one human-annotated part datasets are available,\nincluding Pascal Part, PACO, and LVIS. Although LVIS\nis an object-level dataset, we \ufb01nd its categories contain\nmany object parts, such as shoes, which can also be seen\nas parts.\nFrom the \ufb01rst two rows of Table 3b, we can\nsee that when the part-level annotations grow in training,\nthe part segmentation obtains better performance, from 4.5\nmAP to 7.8 mAP. When IN-S11 label and parsed IN-S11\nare added to the training, the performance is further boosted\nby a large margin. For example, the head of quadruped\nhas achieved 47.5 mAP, close to fully-supervised 57.3 mAP.\nThis shows that when more data sources are available in\nthe future, a strong model for part segmentation in the open\nworld is promising.\n5.3. Cross-category segmentation on Pascal Part\nWe evaluate the cross-category generalization within the\nPascal Part dataset. All 93 parts are split into 77 base parts\nand 16 novel parts, detailed in Appendix. Table 4 reports\nthe metrics of all (93), base (77), and novel (16) parts.\nBaseline from Pascal Part base. Table 4 \ufb01rst row is the\nbaseline, which is trained on base parts and evaluated on\nnovel parts. Since the detector uses CLIP text embedding as\nthe classi\ufb01er, the novel parts obtain non-zero segmentation\nperformance.\nVOC object. Compared with the part annotation, the object\nannotation is much easier to collect. We add VOC object\ndata to verify whether this could help to improve the per-\nformance. As shown in the second row of Table 4, adding\nMethod\nPartImageNet\nPascal Part\nPACO\nAP\nAP50\nAP\nAP50\nAP\nAP50\nJoint\n29.1\n52.0\n22.6\n47.8\n9.3\n18.9\n+ IN\n30.8\n54.4\n23.6\n49.2\n9.0\n18.7\n+ Parsed IN\n31.6\n55.7\n24.0\n49.8\n9.6\n20.2\nvs. baseline\n+2.5\n+3.7\n+1.4\n+2.0\n+0.3\n+1.3\nDataset-speci\ufb01c\n29.7\n54.1\n19.4\n42.3\n10.6\n21.7\n(a) All models are ResNet50 [33] Mask R-CNN [32].\nMethod\nPartImageNet\nPascal Part\nPACO\nAP\nAP50\nAP\nAP50\nAP\nAP50\nJoint\n40.0\n64.8\n31.2\n60.5\n15.4\n30.3\n+ IN\n41.2\n66.8\n31.7\n61.1\n15.9\n30.8\n+ Parsed IN\n42.0\n68.2\n31.9\n61.6\n15.6\n30.6\nvs. baseline\n+2.0\n+3.4\n+0.7\n+0.9\n+0.2\n+0.3\nDataset-speci\ufb01c\n41.7\n68.7\n27.4\n56.1\n15.2\n29.4\n(b) All models are Swin-B [56] Cascade Mask R-CNN [7].\nTable 5. Part segmentation across datasets. All models are eval-\nuated by setting the classi\ufb01er as text embedding of category name\nin the evaluation dataset. Joint denotes the joint-training on LVIS,\nPartImageNet, Pascal Part and PACO datasets. Dataset-speci\ufb01c\nuses the training data of each dataset, separately.\nVOC object data helps to improve the performance on both\nbase parts and novel parts in Pascal Part. This demonstrates\nthat object-level alignment could lead to better part-level\nperformance.\nIN-S20 label.\nImage-level classi\ufb01cation data is also an\neasy-to-get annotation. We collect images with Pascal cat-\negories from ImageNet, IN-S20, and add them to the train-\ning. As shown in Table 4 third row, additional image-level\ndata does not bring much gain than object detection data.\nThis is because image-level data has a similar effect as\nobject-level data on part-level recognition. Most of its gain\nis diminished by object data.\nParsed IN-S20. We use our proposed parsing method to\ngenerate part annotations for novel objects, and they pro-\nvide supervision on part classi\ufb01cation. As shown in Ta-\nble 4 fourth row, our method improves the performance on\nboth base and novel categories. This shows that our pars-\ning pipeline is an effective solution to both base and novel\nobject part segmentation.\n5.4. Part segmentation across datasets\nTowards detecting and parse any object in the open\nworld, we train a detector on the joint of available part seg-\nmentation datasets, including LVIS, PACO, Pascal Part and\nPartImageNet. The performance is shown in Table 5.\nThis joint training model shows good generalization\nability on various evaluation datasets, for example, Pascal\nPart obtains 22.6 mAP, better performance than its dataset-\n7\nPascal Part\nAll\ndog\n(93)\nhead\ntorso\npaw\ntail\na [object] [part]\n19.1\n50.7\n18.7\n20.7\n10.4\n[part] of a [object]\n18.4\n48.8\n17.6\n21.3\n9.2\nPartImageNet\nAll\nquadruped\n(40)\nhead\nbody\nfoot\ntail\na [object] [part]\n29.7\n57.3\n25.8\n22.9\n22.9\n[part] of a [object]\n29.9\n55.9\n25.1\n22.9\n24.3\nTable 6. Text prompt template to object part. We compare\ndifferent templates of text prompt to object part in the fully-\nsupervision setting of Pascal Part and PartImageNet.\nMethod\nAll\nquadruped\n(40)\nhead\nbody\nfoot\ntail\nBaseline\n5.4\n23.6\n3.4\n0.8\n1.2\nMax-score [94]\n6.0\n29.6\n7.1\n1.0\n1.7\nMax-size [97]\n5.3\n20.5\n3.5\n0.6\n4.7\nParsed (ours)\n7.8\n35.0\n15.2\n3.5\n8.9\nTable 7. Comparisons of different aligning methods for novel\nparts. The experiments are carried out on cross-dataset general-\nization from Pascal Part to PartImageNet. Fine-tuning from the\nbaseline model, max-score, max-size and our method apply dif-\nferent designs to utilize image-level data to further improve part\nsegmentation performance, where the former two are trained on\npart labels expanded from the image label.\nspeci\ufb01c training. However, the potential problem lies in that\njoint training does not bene\ufb01t all datasets, where PartIma-\ngeNet and PACO decrease the performance a little.\nTo make up for the performance loss, we add IN and\nParsed IN into the training.\nIt can be seen all datasets\nobtain the performance gain accordingly. When we scale\nup the model capability from ResNet50 [33] to Swin-\nB [56], the detector achieves better performance than\ndataset-speci\ufb01c training on all Pascal Part, PartImageNet\nand PACO datasets.\n5.5. Ablation Study\nText prompt template. Since the part is associated with\nthe object category, we study how to design the text prompt\ntemplate of (object, part) pair to the text encoder. We\nselect two common expressions: a [object] [part] and\n[part] of a [object]. For example, [dog] and [head],\nthese two expressions are [a dog head] and [head of\na dog]. As shown in Table 6, a [object] [part] be-\nhaves a little better than [part] of a [object] in Pascal\nPart while not in PartImageNet. Which expression is a gen-\nerally better usage of text prompt to the part needs to be ver-\ni\ufb01ed on more datasets and we leave it for future research. In\naddition, more advanced prompt engineering for part seg-\nmentation is also an open problem.\nDINO\nVanilla  ViT\nFigure 5.\nSemantic correspondence from vanilla ViT and\nDINO. The upper section is from supervised ViT model [16] and\nthe lower section is from self-supervised DINO [8]. For each sec-\ntion, \ufb01rst row and second row are paired base objects and novel\nobjects. We crop each image into a uniform size for better visual-\nization.\nAligning method for novel parts. We compare different\naligning methods to use IN-S11 data to help part segmen-\ntation in PartImageNet. We select two popular designs in\nopen-vocabulary object detection, max-score and max-size.\nMax-score is selecting the proposal that has the highest\nscore of the target category as the matched proposal, used\nin [94]. Max-size is selecting the proposal that has the max-\nimum area among all proposals as the matched proposal to\nthe target category, proposed in [97]. For each ImageNet\nimage, its object category is known, and its part taxonomy\ncan be inferred, these parts will be used as the target cate-\ngory in max-score and max-size methods.\n- Max-score. As shown in Table 7 second row, max-\nscore helps to improve the performance a little over base-\nline. Fine-tuning from the baseline model, its selected high-\nest score proposals contain ef\ufb01cient training samples, and\nthese samples bring performance gain.\n- Max-size. As shown in Table 7 third row, the max-\nsize method degenerates the performance in most metrics.\nAccording to the max-size rule, all parts are assigned to the\nsame proposal, it is hard to align the part-level region to\nits part name text embedding. This shows that part-level\nalignment is more dif\ufb01cult than object-level and an ef\ufb01cient\n\ufb01ne-grained supervision signal is necessary.\nPre-trained model in semantic correspondence. We use\nself-supervised DINO [8] in this work to \ufb01nd the near-\n8\nSource\ncapability\npart name part location\nbase parts\nAlign image and text in part-level\n\u2713\n\u2713\nof base objects\nnovel objects Align image and text in object-level\n\u2713\nand image-level of novel objects\nCLIP [68]\nAnchor the part name in language\n\u2713\nfeature space\nDINO [8]\nParse the novel object into its part\n\u2713\nTable 8. The source of VLPart capability. Besides training data\nof base parts and novel objects, two foundation models, CLIP and\nDINO, contribute to open-vocabulary part segmentation.\nest base object for each novel object and build their dense\nsemantic correspondence.\nWe verify whether a vanilla\nViT [16] has a similar function, which is pre-trained on\nfully-supervised ImageNet. As shown in Figure 5, vanilla\nViT is obviously behind DINO in the aspect of providing\nsemantic correspondence.\nOn the one hand, the nearest\nbase object found by DINO has better alignment with the\nnovel object in color, texture, and pose. On the other hand,\nthe dense correspondence from DINO has clear semantics\ncorrespondence between the two objects. Similar experi-\nment phenomenons are reported in [3, 8]. Besides DINO,\nwhether other models could bene\ufb01t to part segmentation is\na potential research direction in the future.\n6. Discussion\nLearning from Foundation Models. When we analyze\nhow VLPart achieves open-vocabulary part segmentation\ncapability, as shown in Table 8, we could see that the impor-\ntant components of VLPart\u2019s capability are two foundation\nmodels: CLIP [68] and DINO [8]. Learning from founda-\ntion models is a recently rising research topic [2,4,17,41].\nAlthough a single foundation model is not an expert in a\nspeci\ufb01c task, for example, neither CLIP nor DINO can ac-\ncomplish the part segmentation task, combining these foun-\ndation models could bring to a range of applications [24,\n34, 48, 52, 55, 79, 89, 92], and this paper takes the part seg-\nmentation task as an example to explore. In the future, how\nto \u201ddecode\u201d more capabilities from foundation models is a\nvery promising research topic.\nComparison with Segment Anything Model. Segment\nAnything Model (SAM) [37] is a recently proposed model\naimed to generate masks for all entities [66, 67] in an im-\nage, including both objects and their parts. As shown in\nFigure 6, the main differences between SAM and VLPart\nare: (1) SAM is a class-agnostic mask segmentation model,\nwhile VLPart is class-aware. (2) The part segmentation of\nSAM is mostly edge-oriented, which makes it hard to parse\ntwo parts if there is no obvious edge between them, while\nVLPart parses objects based on semantics instead of low-\n(a) image\n(b) SAM\n(c) VLPart\nFigure 6. Comparsion of SAM [37] and VLPart. The main\ndifferences are: (1) SAM is a class-agnostic segmentation model\nand VLPart is class-aware, (2) SAM parses the object mostly in an\nedge-oriented way and VLPart is semantic-oriented.\nlevel edge signals.\nSegment and Recognize Anything Model?\nA big pic-\nture for the vision perception system is to segment and\nrecognize anything in the open world, in which SAM,\nopen-vocabulary object detection and our open-vocabulary\npart segmentation are all sub-tasks.\nSome recently pub-\nlic works [53, 77, 83, 98] attempt to achieve this goal but\ntheir focuses are either segmentation or recognition. Fur-\nthermore, their explorations only reach the object-level, and\ndo not go denser into the part-level. This paper provides a\npromising solution to part-level segmentation and recogni-\ntion, serving as a component of achieving the goal of Seg-\nment and Recognize Anything Model.\n7. Conclusion and Future Work\nIn this paper, we explore empowering object detectors\nwith the \ufb01ne-grained recognition ability of open-vocabulary\npart segmentation. Our model a vision-language version\nof the segmentation model to support text prompt input.\nThe training data is the joint of part-level, object-level and\nimage-level data to establish multi-granularity alignment.\nTo further improve the part recognition ability, we parse\nthe novel object into its parts by the dense semantic cor-\nrespondence to its nearest base objects. Extensive exper-\niments demonstrate that our method can signi\ufb01cantly im-\nprove the open-vocabulary part segmentation performance\nand achieve favorable performance on a wide range of\ndatasets.\nIn the future, our models have great potential to be\napplied to various applications such as robotic manipula-\ntion [21], part-guided instance object [69], and part-aware\nimage editing [47].\nAcknowledgments. This work was done when Peize Sun\nworked as an intern at Meta AI and was supported in part\nby the General Research Fund of HK No.17200622.\n9\nReferences\n[1] K\ufb01r Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Bao-\nquan Chen, and Daniel Cohen-Or.\nNeural best-buddies:\nSparse cross-domain correspondence. ACM Transactions on\nGraphics (TOG), 37(4):1\u201314, 2018. 3\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736,\n2022. 9\n[3] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.\nDeep vit features as dense visual descriptors. arXiv preprint\narXiv:2112.05814, 2021. 3, 5, 9\n[4] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021. 9\n[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-\nishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\nRt-1: Robotics transformer for real-world control at scale.\narXiv preprint arXiv:2212.06817, 2022. 1\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 15\n[7] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-\ning into high quality object detection. In CVPR, 2018. 7\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the International Conference on Computer Vi-\nsion (ICCV), 2021. 2, 3, 4, 5, 8, 9\n[9] Harrison Chase. LangChain. github, Oct. 2022. 15\n[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Doll\u00b4ar, and C Lawrence Zitnick.\nMicrosoft coco captions:\nData collection and evaluation\nserver. arXiv preprint arXiv:1504.00325, 2015. 4\n[11] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fi-\ndler, Raquel Urtasun, and Alan Yuille.\nDetect what you\ncan: Detecting and representing objects using holistic mod-\nels and body parts. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1971\u20131978,\n2014. 1, 2, 3, 4, 6, 14\n[12] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XXX, pages\n104\u2013120. Springer, 2020. 3\n[13] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1290\u20131299, 2022. 4\n[14] Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaox-\niao Wen, and Gijs Dubbelman.\nPart-aware panoptic seg-\nmentation.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5485\u2013\n5494, 2021. 2\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 2, 6, 14\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3, 8, 9\n[17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha\nChowdhery,\nBrian\nIchter,\nAyzaan\nWahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-\ne: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023. 9\n[18] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,\nand Guoqi Li. Learning to prompt for open-vocabulary ob-\nject detection with vision-language model. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14084\u201314093, 2022. 2\n[19] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303\u2013338, 2010. 1, 6, 14\n[20] Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li,\nRan Xu, Wenhao Liu, and Caiming Xiong. Towards open\nvocabulary object detection without human-provided bound-\ning boxes. arXiv preprint arXiv:2111.09452, 2021. 2\n[21] Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, and\nXiaolong Wang. Self-play and self-describe: Policy adapta-\ntion with vision-language foundation models. arXiv preprint\narXiv:2212.07398, 2022. 1, 9\n[22] Rohit Girdhar and Deva Ramanan.\nCater: A diagnostic\ndataset for compositional actions and temporal reasoning.\narXiv preprint arXiv:1910.04744, 2019. 3\n[23] Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen,\nand Liang Lin. Look into person: Self-supervised structure-\nsensitive learning and a new benchmark for human parsing.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 932\u2013940, 2017. 2\n[24] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,\nMiao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping\nLuo, and Kai Chen.\nMultimodal-gpt: A vision and lan-\nguage model for dialogue with humans.\narXiv preprint\narXiv:2305.04790, 2023. 9\n[25] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904\u20136913, 2017. 3\n10\n[26] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18995\u201319012, 2022. 6\n[27] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation.\narXiv preprint arXiv:2104.13921,\n2021. 2, 4\n[28] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 1, 2, 5, 6,\n14\n[29] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 3\n[30] Kai Han, Rafael S Rezende, Bumsub Ham, Kwan-Yee K\nWong, Minsu Cho, Cordelia Schmid, and Jean Ponce. Sc-\nnet: Learning semantic correspondence. In Proceedings of\nthe IEEE international conference on computer vision, pages\n1831\u20131840, 2017. 3\n[31] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xi-\naoding Yuan, Jie-Neng Chen, Shuai Liu, Cheng Yang, and\nAlan Yuille. Partimagenet: A large, high-quality dataset of\nparts. arXiv preprint arXiv:2112.00933, 2021. 1, 2, 3, 4, 6,\n14\n[32] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 4,\n7\n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4, 7, 8\n[34] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,\nQiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh\nIyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al.\nConceptfusion: Open-set multimodal 3d mapping.\narXiv\npreprint arXiv:2302.07241, 2023. 9\n[35] Menglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui,\nClaire Cardie, Bharath Hariharan, Hartwig Adam, and Serge\nBelongie.\nFashionpedia: Ontology, segmentation, and an\nattribute localization dataset.\nIn Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u2013\n28, 2020, Proceedings, Part I 16, pages 316\u2013332. Springer,\n2020. 2\n[36] Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul\nJeon, Stephen Lin, and Kwanghoon Sohn. Fcss: Fully con-\nvolutional self-similarity for dense semantic correspondence.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6560\u20136569, 2017. 3\n[37] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n9\n[38] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32\u201373, 2017. 4\n[39] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and\nAnelia Angelova.\nF-vlm: Open-vocabulary object detec-\ntion upon frozen vision and language models. arXiv preprint\narXiv:2209.15639, 2022. 1\n[40] Junghyup Lee, Dohyung Kim, Jean Ponce, and Bumsub\nHam.\nSfnet: Learning object-aware semantic correspon-\ndence. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 2278\u20132287,\n2019. 3\n[41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 9\n[42] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nSha\ufb01q Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. Advances in neural infor-\nmation processing systems, 34:9694\u20139705, 2021. 2, 3\n[43] Jianshu Li, Jian Zhao, Yunchao Wei, Congyan Lang, Yidong\nLi, Terence Sim, Shuicheng Yan, and Jiashi Feng. Multiple-\nhuman parsing in the wild. arXiv preprint arXiv:1705.07206,\n2017. 2\n[44] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\nGrounded\nlanguage-image pre-training.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022. 1, 2, 3, 4\n[45] Xiangtai Li, Shilin Xu, Yibo Yang Cheng, Yunhai Tong,\nDacheng Tao, et al. Panoptic-partformer: Learning a uni\ufb01ed\nmodel for panoptic part segmentation. ECCV, 2022. 2\n[46] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al.\nOscar: Object-semantics aligned pre-training\nfor vision-language tasks. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part XXX 16, pages 121\u2013137. Springer,\n2020. 3\n[47] Yuheng Li, Krishna Kumar Singh, Yang Xue, and Yong Jae\nLee. Partgan: Weakly-supervised part decomposition for im-\nage generation and segmentation. In British Machine Vision\nConference (BMVC), 2021. 1, 9\n[48] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan\nXia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao,\net al.\nTaskmatrix. ai:\nCompleting tasks by connecting\nfoundation models with millions of apis.\narXiv preprint\narXiv:2303.16434, 2023. 9\n[49] Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gho-\nlamreza Haffari, Zehuan Yuan, and Jianfei Cai.\nLearning\n11\nobject-language alignments for open-vocabulary object de-\ntection. arXiv preprint arXiv:2211.14843, 2022. 1, 2, 3, 4\n[50] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 4\n[51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 1, 3, 5, 6, 14\n[52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 9\n[53] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 9\n[54] Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang.\nSemantic correspondence as an optimal transport problem.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4463\u20134472, 2020. 3\n[55] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi\nWang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun\nLi, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou\nZhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, and Yu\nQiao. Interngpt: Solving vision-centric tasks by interacting\nwith chatgpt beyond language, 2023. 9\n[56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 10012\u201310022, 2021. 4, 7, 8\n[57] Panagiotis Meletis, Xiaoxiao Wen, Chenyang Lu, Daan\nde Geus, and Gijs Dubbelman.\nCityscapes-panoptic-parts\nand pascal-panoptic-parts datasets for scene understanding.\narXiv preprint arXiv:2004.07944, 2020. 2\n[58] Umberto Michieli, Edoardo Borsato, Luca Rossi, and Pietro\nZanuttigh. Gmnet: Graph matching network for large scale\npart semantic segmentation in the wild. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part VIII 16, pages 397\u2013414.\nSpringer, 2020. 2\n[59] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.\nSpair-71k: A large-scale benchmark for semantic correspon-\ndence. arXiv preprint arXiv:1908.10543, 2019. 3\n[60] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection with\nvision transformers. arXiv preprint arXiv:2205.06230, 2022.\n1\n[61] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna\nTripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-\nscale benchmark for \ufb01ne-grained and hierarchical part-level\n3d object understanding. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 909\u2013918, 2019. 2\n[62] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea\nFinn, and Abhinav Gupta.\nR3m:\nA universal visual\nrepresentation for robot manipulation.\narXiv preprint\narXiv:2203.12601, 2022. 1\n[63] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni,\nSi Yong Yeo, and Jun Liu. Animal kingdom: A large and\ndiverse dataset for animal behavior understanding. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 19023\u201319034, 2022. 1\n[64] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models.\nIn Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2641\u20132649, 2015. 3\n[65] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models.\nIn Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2641\u20132649, 2015. 4\n[66] Lu Qi, Jason Kuen, Weidong Guo, Tiancheng Shen, Jiux-\niang Gu, Wenbo Li, Jiaya Jia, Zhe Lin, and Ming-Hsuan\nYang.\nFine-grained entity segmentation.\narXiv preprint\narXiv:2211.05776, 2022. 9\n[67] Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang\nZhao, Philip Torr, Zhe Lin, and Jiaya Jia. Open world en-\ntity segmentation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2022. 9\n[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 2, 4, 6, 9\n[69] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi\nWen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-\nquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yi-\nwen Song, Abhimanyu Dubey, and Dhruv Mahajan. PACO:\nParts and attributes of common objects. In arXiv preprint\narXiv:2301.01795, 2023. 1, 2, 4, 6, 9, 14, 17\n[70] N Dinesh Reddy, Minh Vo, and Srinivasa G Narasimhan.\nCarfusion: Combining point tracking and part detection for\ndynamic 3d reconstruction of vehicles. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 1906\u20131915, 2018. 1, 2\n[71] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015. 4\n[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 15\n[73] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\n12\nlarge-scale, high-quality dataset for object detection. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 8430\u20138439, 2019. 1\n[74] Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye\nGuan, Yuchao Dai, Hao Su, Hongdong Li, and Ruigang\nYang.\nApollocar3d: A large 3d car instance understand-\ning benchmark for autonomous driving. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5452\u20135462, 2019. 2\n[75] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A cor-\npus of natural language for visual reasoning. In Proceedings\nof the 55th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers), pages 217\u2013223,\n2017. 3\n[76] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie. The caltech-ucsd birds-200-2011\ndataset. technical report, 2011. 2\n[77] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang. Seggpt: Segmenting ev-\nerything in context. arXiv preprint arXiv:2304.03284, 2023.\n9\n[78] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\nmodel pretraining with weak supervision.\narXiv preprint\narXiv:2108.10904, 2021. 3\n[79] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023. 9, 15\n[80] Size\nWu,\nWenwei\nZhang,\nSheng\nJin,\nWentao\nLiu,\nand\nChen\nChange\nLoy.\nAligning\nbag\nof\nregions\nfor open-vocabulary object detection.\narXiv preprint\narXiv:2302.13996, 2023. 1\n[81] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5288\u20135296, 2016. 3\n[82] Fan Yang, Xin Li, Hong Cheng, Jianping Li, and Leiting\nChen. Object-aware dense semantic correspondence. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 2777\u20132785, 2017. 3\n[83] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\nWayne Zhang, and Ziwei Liu. Panoptic scene graph gen-\neration. In ECCV, 2022. 9\n[84] Lu Yang, Qing Song, Zhihui Wang, and Ming Jiang. Parsing\nr-cnn for instance-level human analysis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 364\u2013373, 2019. 1, 2\n[85] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen,\nMengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Shef-\nfer, and Leonidas Guibas. A scalable active framework for\nregion annotation in 3d shape collections. ACM Transactions\non Graphics (ToG), 35(6):1\u201312, 2016. 1\n[86] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\nand Tamara L Berg. Modeling context in referring expres-\nsions.\nIn Computer Vision\u2013ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\n4\n[87] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-\nFu Chang. Open-vocabulary object detection using captions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14393\u201314402, 2021.\n1, 2\n[88] Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen.\nCross-domain correspondence learning for exemplar-based\nimage translation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5143\u20135153, 2020. 3\n[89] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-\nqiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt,\ngenerate, then cache: Cascade of foundation models makes\nstrong few-shot learners. arXiv preprint arXiv:2303.02151,\n2023. 9\n[90] Rufeng Zhang, Tao Kong, Weihao Wang, Xuan Han, and\nMingyu You. 3d part assembly generation with instance en-\ncoded transformer. IEEE Robotics and Automation Letters,\n7(4):9051\u20139058, 2022. 1\n[91] Dongyang Zhao, Ziyang Song, Zhenghao Ji, Gangming\nZhao, Weifeng Ge, and Yizhou Yu. Multi-scale matching\nnetworks for semantic correspondence. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 3354\u20133364, 2021. 3\n[92] Yue Zhao, Ishan Misra, Philipp Kr\u00a8ahenb\u00a8uhl, and Rohit Gird-\nhar.\nLearning video representations from large language\nmodels. arXiv preprint arXiv:2212.04501, 2022. 9\n[93] Shuai Zheng, Fan Yang, M Hadi Kiapour, and Robinson Pi-\nramuthu. Modanet: A large-scale street fashion dataset with\npolygon annotations. In Proceedings of the 26th ACM inter-\nnational conference on Multimedia, pages 1670\u20131678, 2018.\n2\n[94] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-\nyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,\nXiyang Dai,\nLu Yuan,\nYin Li,\net al.\nRegionclip:\nRegion-based language-image pretraining.\narXiv preprint\narXiv:2112.09106, 2021. 1, 2, 3, 4, 8\n[95] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 127:302\u2013321, 2019. 2\n[96] Tianfei Zhou, Wenguan Wang, Si Liu, Yi Yang, and Luc\nVan Gool. Differentiable multi-granularity human represen-\ntation learning for instance-aware human semantic parsing.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1622\u20131631, 2021. 2\n[97] Xingyi Zhou,\nRohit Girdhar,\nArmand Joulin,\nPhillip\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra.\nDetecting twenty-thousand\nclasses using image-level supervision.\narXiv preprint\narXiv:2201.02605, 2022. 1, 2, 3, 4, 5, 8\n[98] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n9\n13\nDataset\nType\nCategory\nTrain\nVal\nPASCAL VOC 2007 [19]\nobject\n20\n2.5k\n2.5k\nCOCO 2017 [51]\nobject\n80\n118k\n5.0k\nLVIS v1 [28]\nobject\n1203\n100k\nmini 5k\nPASCAL Part [11]\npart\n93\n4366\n4465\nPartImageNet [31]\npart\n40\n16k\n2.9k\nPACO-LVIS [69]\npart\n456\n45k\n2.4k\nImageNet-1k [15]\nimage\n1k\n1M\n-\nImageNet-super11\nimage\n11\n16k\n-\nImageNet-super20\nimage\n20\n49k\n-\nTable 9. Statistic of datasets used in this work.\nA. Dataset\nIn Table 9, we list the datasets used in this paper.\nPascal VOC. Pascal VOC contains 20 categories of objects\nand Pascal Part is annotated based on the same image set\nand object categories.\nCOCO. COCO contains 80 categories of objects and is one\nof the most popular object detection datasets.\nLVIS. LVIS contains 1203 categories of objects and split\nthem into frequent, common and rare sets according to the\noccurrence frequency. It is used to evaluate the long-tailed\ndetection ability.\nPascal Part. The original Pascal Part is too much \ufb01ne-\ngrained, in Table 14, we modify it by (1) merging posi-\ntional concepts, for example, merging the left wing and the\nright wing to the wing for the aeroplane, (2) deleting vague\nconcepts, such as *side, (3) deleting train, tvmonitor. The\nresulting part taxonomy is 93 categories, as shown in Ta-\nble 10. We select bus and dog as the novel objects and\ntheir parts as the novel parts, totally 16 parts, the remaining\n77 parts as base parts.\nPartImageNet. PartImageNet selects 158 classes from Im-\nageNet and groups them into 11 super-categories. For ex-\nample in Table 12, quadruped super-category contains\nmany animal categories. The part annotations are based on\nthese super-categories, as shown in Table 11.\nPACO. PACO contains 75 object categories and 456 object-\npart categories. It supplements more electronic equipment,\nappliances, accessories, and furniture than Pascal Part and\nPartImageNet, as shown in Table 15.\nImageNet. ImageNet is the \ufb01rst large-scale image classi-\n\ufb01cation dataset. It has 1k version and 22k version. We\nuse ImageNet-1k set as the default. ImageNet-super11 and\n-super20 are ImageNet images whose categories overlap\nwith PartImageNet and Pascal category vocabulary, sepa-\nrately.\nId Name\nPart Taxonomy\n1 aeroplane\nbody, wing, tail, wheel\n2 bicycle\nwheel, handlebar, saddle\n3 bird\nbeak, head, eye, foot, leg, wing, neck, tail, torso\n4 boat\n-\n5 bottle\nbody, cap\n6 bus\nlicense plate, door, headlight, mirror, window, wheel\n7 car\nlicense plate, door, headlight, mirror, window, wheel\n8 cat\nhead, leg, paw, ear, eye, neck, nose, tail, torso\n9 chair\n-\n10 cow\nhead, leg, ear, eye, horn, muzzle, neck, tail, torso\n11 diningtable -\n12 dog\nhead, leg, paw, ear, eye, muzzle, neck, nose, tail, torso\n13 horse\nhead, leg, ear, eye, muzzle, neck, tail, torso\n14 motorbike\nwheel, handlebar, headlight, saddle\n15 person\nhair, head, ear, eye, nose, neck, mouth, arm, hand,\nleg, foot, torso\n16 pottedplant plant, pot\n17 sheep\nhead, leg, ear, eye, horn, muzzle, neck, tail, torso\n18 sofa\n-\n19 train\n-\n20 tvmonitor\n-\nTable 10. Modi\ufb01ed Pascal Part part taxonomy.\nId\nName\nPart Taxonomy\n1\nQuadruped\nhead, body, foot, tail\n2\nBiped\nhead, body, hand, foot, tail\n3\nFish\nhead, body, \ufb01n, tail\n4\nBird\nhead, body, wing, foot, tail\n5\nSnake\nhead, body\n6\nReptile\nhead, body, foot, tail\n7\nCar\nbody, tier, side mirror\n8\nBicycle\nhead, body, seat, tier\n9\nBoat\nbody, sail\n10\nAeroplane\nhead, body, wing, engine, tail\n11\nBottle\nbody, mouth\nTable 11. PartImageNet part taxonomy from [31].\nPartImageNet: quadruped\nWalker hound, redbone, Saluki, cairn, Boston bull, Tibetan terrier,\nsoft-coated wheaten terrier, vizsla, Brittany spaniel, English springer,\nIrish water spaniel, Eskimo dog, chow, timber wolf, Egyptian cat,\nleopard, tiger, cheetah, brown bear, American black bear, fox squirrel,\nwarthog, ox, water buffalo, ram, bighorn, hartebeest, impala, gazelle,\nArabian camel, weasel, polecat, otter, giant panda\nTable 12. An example of PartImageNet super-category. In Par-\ntImageNet annotation, only quadruped label is known without\nspeci\ufb01c animal categories.\n14\nTraining data\nVOC\nCOCO\nLVIS\nPartImageNet\nPascal Part\nPACO\nAP\nAP50\nAP\nAP50\nAP\nAPr\nAP\nAP50\nAP\nAP50\nAP\nAP50\nLVIS, PACO\n44.5\n70.3\n29.0\n48.1\n27.3\n19.0\n5.4\n11.3\n4.9\n11.3\n9.6\n19.5\nLVIS, PACO, Pascal Part\n42.8\n70.8\n28.6\n48.0\n26.8\n20.4\n7.8\n15.3\n21.6\n46.3\n9.3\n18.9\nLVIS, PACO, Pascal Part, PartImageNet\n40.6\n69.3\n28.4\n47.8\n26.4\n16.0\n29.1\n52.0\n22.6\n47.8\n9.3\n18.9\nDataset-speci\ufb01c oracle\n35.9\n69.7\n38.0\n60.8\n28.1\n20.8\n29.7\n54.1\n19.4\n42.3\n10.6\n21.7\n(a) All models are ResNet50 Mask R-CNN.\nTraining data\nVOC\nCOCO\nLVIS\nPartImageNet\nPascal Part\nPACO\nAP\nAP50\nAP\nAP50\nAP\nAPr\nAP\nAP50\nAP\nAP50\nAP\nAP50\nLVIS, PACO\n55.2\n72.2\n41.0\n58.4\n41.3\n32.8\n6.9\n13.7\n5.6\n12.5\n15.9\n31.9\nLVIS, PACO, Pascal Part\n52.6\n72.4\n40.4\n57.9\n39.9\n29.8\n11.8\n21.8\n30.5\n59.3\n15.4\n30.2\nLVIS, PACO, Pascal Part, PartImageNet\n50.3\n71.6\n40.3\n57.8\n39.6\n30.3\n40.0\n64.8\n31.2\n60.5\n15.4\n30.3\nDataset-speci\ufb01c oracle\n59.0\n82.0\n52.5\n72.0\n43.1\n38.7\n41.7\n68.7\n27.4\n56.1\n15.2\n29.4\n(b) All models are SwinBase Cascade Mask R-CNN.\nTable 13. Joint open-vocabulary object detection and part segmentation across datasets. All models are evaluated by setting the\nclassi\ufb01er as text embedding of category name in the evaluation dataset. COCO is evaluated on object box detection, LVIS is on object\nmask segmentation, PartImageNet, Pascal Part and PACO are on part mask segmentation. Oracle uses dataset-speci\ufb01c training data.\nB. Joint object detection and part segmenta-\ntion\nWe evaluate on both object detection and part segmenta-\ntion datasets, including VOC, COCO, LVIS, PartImageNet,\nPascal Part and PACO. The performance is shown in Ta-\nble 13. The joint training model shows good generalization\nability on various datasets. For example, ResNet50 Mask\nR-CNN trained on LVIS, PACO and Pascal Part achieves\nzero-shot 28.6 AP on COCO and 7.8 AP on PartImageNet.\nHowever, the potential problem lies in that joint object de-\ntection and part segmentation do not bene\ufb01t to both tasks\nall the time. For example, Pascal Part obtains better perfor-\nmance than its dataset-speci\ufb01c oracle, while LVIS decreases\nits performance. How to make these two tasks bene\ufb01t from\neach other is a valuable question for future research.\nC. Part-level image editing\nGiven the part-level segmentation mask, we utilize Sta-\nble Diffusion [72], a state-of-the-art text-to-image genera-\ntion model which performs well for in-painting tasks, to\naccomplish part-level image editing. Speci\ufb01cally, we \ufb01rst\nadopt VLPart to obtain text-prompt mask in the image, and\nthen use a speci\ufb01cally \ufb01ne-tuned version of Stable Diffu-\nsion model for image in-painting, to accomplish zero-shot\nin-painting inference.\nSeveral examples of part-level in-\npainting are shown in Figure 7.\nD. A dialogue system\nInspired by the design of Visual ChatGPT [79], we fur-\nther build a multi-modal conversational system that inter-\n(a) dog body \u2192 zebra\n(b) cat head \u2192 tiger\n(c) chair seat \u2192 chocolate bar\nFigure 7. Part-level image in-painting. We \ufb01rst adopt VLPart to\nobtain text-prompt mask, and then use Stable Diffusion model to\naccomplish zero-shot in-painting inference.\nacts with VLPart by GPT-3 [6] (text-davinci-003)\nand langchain [9]. The demonstration video snapshots are\nshown in Figure 8.\nE. Visualization\nIn Figure 9, we show the parsed novel objects in Pascal\nand ImageNet images. In Figure 10, we demonstrate the\nqualitative results of open-vocabulary part segmentation on\nPascal and COCO images.\n15\nId\nName\nPart Taxonomy\n1\naeroplane\nbody, engine, left wing, right wing, stern, tail, wheel\n2\nbicycle\nback wheel, chain wheel, front wheel, handlebar, headlight, saddle\n3\nbird\nbeak, head, left eye, left foot, left, leg, left wing, neck, right eye, right foot, right leg, right wing, tail, torso\n4\nboat\n-\n5\nbottle\nbody, cap\n6\nbus\nback license plate, back side, door, front license plate, front side, headlight, left mirror, left side, right mirror,\nright side, roof side, wheel, window\n7\ncar\nback license plate, back side, door, front license plate, front side, headlight, left mirror, left side, right mirror,\nright side, roof side, wheel, window\n8\ncat\nhead, left back leg, left back paw, left ear, left eye, left front leg, left front paw, neck, nose, right back leg,\nright back paw, right ear, right eye, right front leg, right front paw, tail, torso\n9\nchair\n-\n10\ncow\nhead, left back lower leg, left back upper leg, left ear, left eye, left front lower leg, left front upper leg,\nleft horn, muzzle, neck, right back lower leg, right back upper leg, right ear, right eye, right front lower leg,\nright front upper leg, right horn, tail, torso\n11\ndiningtable\n-\n12\ndog\nhead, left back leg, left back paw, left ear, left eye, left front leg, left front paw, muzzle, neck, nose, right back leg,\nright back paw, right ear, right eye, right front leg, right front paw, tail, torso\n13\nhorse\nhead, left back hoof, left back lower leg, left back upper leg, left ear, left eye, left front hoof, left front lower leg,\nleft front upper leg, muzzle, neck, right back hoof, right back lower leg, right back upper leg, right ear, right eye,\nright front hoof, right front lower leg, right front upper leg, tail, torso\n14\nmotorbike\nback wheel, front wheel, handlebar, headlight, saddle\n15\nperson\nhair, head, left ear, left eye, left eyebrow, left foot, left hand, left lower arm, left lower leg, left upper arm,\nleft upper leg, mouth, neck, nose, right ear, right eye, right eyebrow, right foot, right hand, right lower arm,\nright lower leg, right upper arm, right upper leg, torso\n16\npottedplant\nplant, pot\n17\nsheep\nhead, left back lower leg, left back upper leg, left ear, left eye, left front lower leg, left front upper leg,\nleft horn, muzzle, neck, right back lower leg, right back upper leg, right ear, right eye, right front lower leg,\nright front upper leg, right horn, tail, torso\n18\nsofa\n-\n19\ntrain\ncoach back side, coach front side, coach left side, coach right side, coach roof side, coach, head, head back side,\nhead front side, head left side, head right side, head roof side, headlight\n20\ntvmonitor\nscreen\nTable 14. Original Pascal Part part taxonomy from http://roozbehm.info/Pascal-parts/Pascal-parts.html.\n16\nId Name\nPart Taxonomy\n1 ball\n-\n2 basket\nbottom, handle, inner side, cover, side, rim, base\n3 belt\nbuckle, end tip, strap, frame, bar, prong, loop, hole\n4 bench\nstretcher, seat, back, table top, leg, arm\n5 bicycle\nstem, fork, top tube, wheel, basket, seat stay, saddle, handlebar, pedal, gear, head tube, down tube, seat tube\n6 blender\ncable, handle, cover, spout, vapour cover, base, inner body, seal ring, cup, switch, food cup\n7 book\npage, cover\n8 bottle\nneck, label, shoulder, body, cap, bottom, inner body, closure, heel, top, handle, ring, sipper, capsule, spout, base, punt\n9 bowl\ninner body, bottom, body, rim, base\n10 box\nbottom, lid, inner side, side\n11 broom\nlower bristles, handle, brush cap, ring, shaft, brush\n12 bucket\nhandle, cover, body, base, inner body, bottom, loop, rim\n13 calculator\nkey, body\n14 can\npull tab, body, base, inner body, bottom, lid, text, rim\n15 car (automobile)\nheadlight, turnsignal, tank, windshield, mirror, sign, wiper, fender, trunk, windowpane, seat, logo, grille, antenna, hood,\nsplashboard, bumper, rim, handle, runningboard, window, roof, wheel, taillight, steeringwheel\n16 carton\ninner side, tapering top, cap, bottom, lid, text, side, top\n17 cellular telephone\nbutton, screen, bezel, back cover\n18 chair\nstretcher, swivel, apron, wheel, leg, base, spindle, seat, back, rail, stile, skirt, arm\n19 clock\ncable, decoration, hand, pediment, \ufb01nial, case, base\n20 crate\nbottom, handle, inner side, lid, side\n21 cup\ninner body, handle, rim, base\n22 dog\nteeth, neck, foot, head, body, nose, leg, tail, ear, eye\n23 drill\nhandle, body\n24 drum (musical instrument)\nhead, rim, cover, body, loop, lug, base\n25 earphone\nheadband, cable, ear pads, housing, slider\n26 fan\nrod, canopy, motor, blade, base, string, light, bracket, fan box, pedestal column\n27 glass (drink container)\ninner body, bottom, body, rim, base\n28 guitar\nkey, headstock, bridge, body, \ufb01ngerboard, back, string, side, pickguard, hole\n29 hammer\nhandle, face, head, grip\n30 handbag\nzip, inner body, handle, bottom, body, rim, base\n31 hat\nlogo, pom pom, inner side, strap, visor, rim\n32 helmet\nface shield, logo, inner side, strap, visor, rim\n33 jar\nhandle, body, base, inner body, bottom, lid, sticker, text, rim\n34 kettle\ncable, handle, lid, body, spout, base\n35 knife\nhandle, blade\n36 ladder\nrail, step, top cap, foot\n37 lamp\nshade inner side, cable, pipe, shade, bulb, shade cap, base, switch, \ufb01nial\n38 laptop computer\ncable, camera, base panel, keyboard, logo, back, screen, touchpad\n39 microwave oven\ninner side, door handle, time display, control panel, turntable, dial, side, top\n40 mirror\nframe\n41 mouse (computer equipment) logo, scroll wheel, body, right button, wire, side button, left button\n42 mug\nhandle, body, base, inner body, bottom, text, drawing, rim\n43 napkin\n-\n44 newspaper\ntext\n45 pan (for cooking)\nbottom, handle, inner side, lid, side, rim, base\n46 pen\ncap, grip, barrel, clip, tip\n47 pencil\nbody, lead, eraser, ferrule\n48 pillow\nembroidery\n49 pipe\nnozzle, colied tube, nozzle stem\n50 plastic bag\ninner body, handle, text, hem, body\n51 plate\ntop, bottom, inner wall, body, rim, base\n52 pliers\njaw, handle, joint, blade\n53 remote control\nlogo, back, button\n54 scarf\nfringes, body\n55 scissors\nhandle, screw, \ufb01nger hole, blade\n56 screwdriver\nblade, handle, tip, shank\n57 shoe\ntoe box, tongue, vamp, outsole, insole, backstay, lining, quarter, heel, throat, eyelet, lace, welt\n58 slipper (footwear)\ntoe box, vamp, outsole, strap, insole, lining\n59 soap\nneck, label, shoulder, body, sipper, capsule, spout, push pull cap, cap, base, bottom, closure, punt, top\n60 sponge\nrough surface\n61 spoon\nneck, handle, bowl, tip\n62 stool\nseat, leg, step, footrest\n63 sweater\nshoulder, sleeve, neckband, hem, body, yoke, cuff\n64 table\nstretcher, drawer, inner wall, shelf, apron, wheel, leg, top, rim\n65 tape (sticky cloth or paper)\nroll\n66 telephone\nbutton, screen, bezel, back cover\n67 television set\nbottom, button, side, top, base\n68 tissue paper\nroll\n69 towel\nbody, terry bar, hem, border\n70 trash can\nlabel, body, wheel, inner body, bottom, lid, pedal, rim, hole\n71 tray\nbottom, inner side, outer side, rim, base\n72 vase\nneck, handle, foot, body, mouth\n73 wallet\ninner body, \ufb02ap\n74 watch\nbuckle, case, dial, hand, strap, window, lug\n75 wrench\nhandle, head\nTable 15. PACO part taxonomy from [69].\n17\nFigure 8. Demonstration video snapshots of a dialogue system from https://cheems-seminar.github.io/.\n18\n(a) Parsed objects on Pascal images.\n(b) Parsed objects on ImageNet images.\nFigure 9. Visualization of parsing novel objects into parts. Since we use the parsed image as the annotation to train the classi\ufb01cation\nloss only, we only save the box and the category to the annotation \ufb01les.\n19\n(a) Part segmentation on Pascal images using Pascal Part taxonomy.\n(b) Part segmentation on COCO images using joint taxonomy of LVIS and PACO.\nFigure 10. Visualization of open-vocabulary object detection and part segmentation. For each pair of images, left image is without\ncategory name for better visualization, right image is marked with category name.\n20\n"
  },
  {
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "link": "https://arxiv.org/pdf/2305.11000.pdf",
    "upvote": "1",
    "text": "SpeechGPT: Empowering Large Language Models with\nIntrinsic Cross-Modal Conversational Abilities\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang,\nYaqian Zhou\u2217, Xipeng Qiu\u2217\nSchool of Computer Science, Fudan University\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\ndongzhang22@m.fudan.edu.cn\n{smli20,zhouyaqian,xpqiu}@fudan.edu.cn\nhttps://github.com/0nutation/SpeechGPT\nAbstract\nMulti-modal large language models are regarded as a crucial step towards Ar-\nti\ufb01cial General Intelligence (AGI) and have garnered signi\ufb01cant interest with\nthe emergence of ChatGPT. However, current speech-language models typi-\ncally adopt the cascade paradigm, preventing inter-modal knowledge transfer.\nIn this paper, we propose SpeechGPT, a large language model with intrinsic\ncross-modal conversational abilities, capable of perceiving and generating multi-\nmodel content. With discrete speech representations, we \ufb01rst construct SpeechIn-\nstruct, a large-scale cross-modal speech instruction dataset. Additionally, we\nemploy a three-stage training strategy that includes modality-adaptation pre-\ntraining, cross-modal instruction \ufb01ne-tuning, and chain-of-modality instruction\n\ufb01ne-tuning. The experimental results demonstrate that SpeechGPT has an im-\npressive capacity to follow multi-modal human instructions and highlight the\npotential of handling multiple modalities with one model. Demos are shown in\nhttps://0nutation.github.io/SpeechGPT.github.io/.\n1\nIntroduction\nLarge language models (OpenAI, 2023; Touvron et al., 2023) have performed astonishingly on\nvarious natural language processing tasks. Meanwhile, multi-modal large language models, such as\nGPT-4, PALM-E (Driess et al., 2023), and LLaVA (Liu et al., 2023), have explored the ability of\nLLMs to understand multi-modal information. However, a signi\ufb01cant gap exists between current\nLLMs and general arti\ufb01cial intelligence (AGI). First, most current LLMs can only perceive and\nunderstand multi-modal content but cannot spontaneously generate multi-modal content. Second,\ncontinuous signals like images and speech cannot be adapted directly to LLMs that receive discrete\ntokens.\nThe current speech-language model mainly adopts a cascading paradigm (Huang et al., 2023a) i.e.,\nthe LLM is connected with an automatic speech recognition (ASR) model or a text-to-speech (TTS)\nmodel in tandem, or the LLM is employed as a control hub, with several speech processing models\nare integrated to cover multiple audio or speech tasks (Huang et al., 2023a; Shen et al., 2023). Some\nprior work on generative spoken language models involves encoding the speech signal into a discrete\nrepresentation (Baevski et al., 2020; Hsu et al., 2021) and modeling it with language models (Lakhotia\net al., 2021; Borsos et al., 2022; Zhang et al., 2023b; Wang et al., 2023).\n\u2217Corresponding author\narXiv:2305.11000v2  [cs.CL]  19 May 2023\nSpeech\nGPT\nPlease read the sentence:\n\u201cToday is a beautiful day.\u201d\nRecord the content:\n \nThe content of speech is:\n\u201cHave a good day!\u201d \nWhat is the capital of \nFrench ?\nWhat is the capital of \nFrench ?\nThe capital of French is \nParis.\nThe capital of French is \nParis.\n\uff08What is your name?\uff09\n\uff08What is your name?\uff09\n\uff08My name is SpeechGPT.\uff09\n\uff08My name is SpeechGPT.\uff09\nSure, I will read it now:\n \nSure, I will read it now:\n \nFigure 1: SpeechGPT\u2019s capabilities to tackle multiple cross-modal tasks.\nWhile capable of perceiving and generating speech, the existing cascading methods or spoken\nlanguage models still have several limitations. First, the LLM in the cascaded model only functions\nas a content generator. Since the representations of speech and text are not aligned, the LLM\u2019s\nknowledge cannot be transferred to the speech modality. Second, the cascade approach (Shen\net al., 2023; Huang et al., 2023a) suffers from the loss of paralinguistic signals such as emotion\nand prosody. Third, existing spoken language models (Wang et al., 2023; Zhang et al., 2023b) only\nsynthesize speech but fail to comprehend its semantic information, preventing them from achieving\ntrue cross-modal perception and generation.\nIn this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conver-\nsational abilities, capable of perceiving and generating multi-model content. We perform speech\ndiscretization with a self-supervised trained speech model to unify the modality between speech and\ntext. The discrete speech tokens are then expanded into the vocabulary of the LLM, thus endowing\nthe model with an inherent competence to perceive and generate the speech.\nTo provide the model with the capacity to handle multi-modal instructions, we build the \ufb01rst speech-\ntext cross-modal instruction-following dataset SpeechInstruct. Speci\ufb01cally, we discretize the speech\nto discrete units (Hsu et al., 2021) and construct the cross-modal unit-text pair based on the existing\nASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to\nsimulate actual user instructions as illustrated in Appendix B. In addition, to further enhance the\nmodel\u2019s cross-modal capability, we designed the Chain-of-Modality instruction data, i.e., the model\nreceives the speech command, thinks about the process in text, and then outputs the response in\nspeech.\nFor better cross-modal transfer and ef\ufb01cient training, SpeechGPT undergoes a three-stage training\nprocess: modality-adaptation pre-training, cross-modal instruction \ufb01ne-tuning, and chain-of-modality\ninstruction \ufb01ne-tuning. The \ufb01rst stage enables speech comprehension for SpeechGPT with the\ndiscrete speech unit continuation task. The second stage employs the SpeechInstruct to improve the\nmodel\u2019s cross-modal capabilities. The third stage utilizes parameter-ef\ufb01cient LoRA (Hu et al., 2021)\n\ufb01ne-tuning for further modality alignment.\nTo evaluate the effectiveness of SpeechGPT, we conduct a wide range of human evaluations and case\nanalyses to estimate the performance of SpeechGPT on textual tasks, speech-text cross-modal tasks,\nand spoken dialogue tasks. The results demonstrate that SpeechGPT exhibits a strong ability for\nunimodal and cross-modal instruction following tasks as well as spoken dialogue tasks.\nOur contributions include the following:\n\u2022 We build the \ufb01rst multi-modal large language model that can perceive and generate multi-modal\ncontents.\n\u2022 We construct and release SpeechInstruct, the \ufb01rst large-scale speech-text cross-modal instruction-\nfollowing dataset.\n2\nDiscrete Speech Unit Extractor\n<99> <5> <69> <597> \u2026\u2026 <31>\nUnit \nVocoder\n<43> <2> <64> <33> \u2026\u2026 <534>\n[Human]:\n[SpeechGPT]:\nMeta Prompt\nInstructions\nCross-modal\nInstruction data\nInstruction-tuning\nText Datasets\nSpeech2Unit\nText2Unit\nSpeech Datasets\nChain-of-Modality\nInstruction data\nSpeechInstruct\nChain-of-Modality Instructions\nCross-modal Instructions\n[Human]: Transcribe the speech to text. This is the input: {speech unit \ud835\udc48 } <eoh>.\n[SpeechGPT]: {transcription \ud835\udc47 } <eos>.\n[Human]: This is the speech instruction: {speech}. You can do it step by step. You\ncan transcribe the instruction, get the text response and speak the response. \n<eoh>.\n[SpeechGPT]: [tq] {Text \ud835\udc3c }; [ta] {Text \ud835\udc45 }; [ua] {SpeechR} <eoa>.\nTemplate1\nTemplate2\nTranscription:\nGood morning, what is\nyour name?\nTranscription:\nHi, my name is\n. Nice to\nmeet you!\nFigure 2: Left: An overview of SpeechInstruct construction process. The SpeechInstruct dataset con-\nsists of two parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. Template1\nis shown in 3.1. Template2 is shown in Appendix C. Right: An illustration of SpeechGPT model\nstructure.\n\u2022 We build the \ufb01rst spoken dialogue LLM with strong human instruction following ability and spoken\ndialogue ability.\n\u2022 We show great potential to incorporate other modalities into LLMs through discrete representations.\n2\nRelated Work\nMulti-modal Large Language Model Current multi-modal LLMs predominantly focus on the\nvisual domain, feeding continuous representations obtained from pre-trained visual encoders into\nLLMs, facilitating full-parameter or parameter-ef\ufb01cient training on visual-language data (OpenAI,\n2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E (Driess et al., 2023) integrates the 540B\nPaLM (Chowdhery et al., 2022) and 22B Vision Transformer (Dosovitskiy et al., 2021) into the largest\nvision-language model. LLaVA (Liu et al., 2023) leverages pre-trained CLIP (Radford et al., 2021)\nvisual encoder and LLaMA (Touvron et al., 2023) and conduct instruct tuning on GPT4-assisted\nvisual instruction data. X-LLM (Chen et al., 2023) converts multi-modalities into representations with\nX2L interfaces as the inputs of the large language model. However, such structures only enable LLMs\nto process multi-modal input, without ability to generate multi-modal output. Diverging from prior\nstudies, our approach emphasizes the development of a speech-centric multi-modal LLM, endowing\nit with the pro\ufb01ciency to accommodate both multi-modal input and output.\nGenerative Spoken Language Model Discrete self-supervised representation based spoken genera-\ntive language modeling is making remarkable progress on large-scale speech dataset training (Nguyen\net al., 2022). AudioLM (Borsos et al., 2022) proposes to model speech based on audio codecs together\nwith semantic codes, which can synthesize speech in a textlesss setting. VALL-E (Wang et al., 2023)\nbuilds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional\ngeneration task. However, these models are designed for a speci\ufb01c task and failed to bene\ufb01t from\nLLMs. SpeechGPT is built upon the foundation of LLM and transfers LLM\u2019s knowledge to speech\nmodality, consequently obtaining better task generalization and human-instruction following ability.\nSpeech-Enabled LLM Interaction Following the emergence of ChatGPT, several studies have\nconcentrated on the integration of expert speech models with LLMs to enable direct speech interaction\nwith LLMs. HuggingGPT (Shen et al., 2023) facilitates task decomposition of human instructions by\nLLMs and allows the invocation of models from Huggingface to accomplish speci\ufb01c tasks, encom-\npassing a range of automatic speech recognition (ASR) and text-to-speech models. AudioGPT (Huang\net al., 2023a) leverages a variety of audio foundation models to process complex audio information\nand connect LLMs with input/output interface (ASR, TTS) for speech conversations. However, these\nmodels exhibit increased complexity, demand extensive resources, and are prone to the unavoidable\nerror accumulation problems. Our approach enables speech interaction with LLMs without relying\non ASR or TTS systems, circumventing the aforementioned drawbacks.\n3\n3\nSpeechInstruct Construction\nDue to the limitations in publicly available speech data and the lack of variety of speech-text tasks,\nwe construct SpeechInstruct, a speech-text cross-modal instruction-following dataset. This dataset\nconsists of two parts, the \ufb01rst part is called Cross-Modal Instruction, and the second part is called\nChain-of-Modality Instruction. The construction process of SpeechInstruct is illustrated in Figure 2.\n3.1\nCross-modal Instruction\nData Collection We collect several large-scale English ASR datasets to construct Cross-Modal\nInstruction, including Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), and\nLibriSpeech (Panayotov et al., 2015). We employ mHuBERT2 as the speech tokenizer to discretize\nspeech data into discrete units and remove the repetitive units of adjacent frames to get reduced units.\nUltimately, we obtain 9 million unit-text data pairs.\nTask Description Generation We generate ASR and TTS task descriptions that are compatible with\nspeech-text data pairs. Unlike the Self-Instruct method (Wang et al., 2022), we generate descriptions\nthrough a zero-shot approach. Speci\ufb01cally, we directly input the prompts shown in Appendix A into\nOpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each\ntask and some examples are shown in Appendix B.\nInstruction Formatting For a discrete unit sequence U and its associated transcription T, we\ndetermine whether it will be used for constructing an ASR task or a TTS task based on the probabil-\nity p. Subsequently, we randomly select a description D from the corresponding task description.\nThis results in a triplet consisting of the task description, discrete unit sequence, and transcription,\ndenoted as (D, U, T). Following this, the triplet is assembled into an instruction using the template:\n[Human]:{D}. This is input: {U}<eoh>.[SpeechGPT]: {T}<eos>.. To support multi-turn dia-\nlogues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering\nto the maximum input length of the model.\n3.2\nChain-of-Modality Instruction\nSpeech Instruction Generation Due to the lack of instruction data with speech input and speech\noutput, we trained a text-to-unit generator to convert text instruction data into speech instruction\ndata. Speci\ufb01cally, the text-to-unit generator adopts a Transformer encoder-decoder architecture. We\ntrained it on LibriSpeech unit-text pairs in Cross-modal Instruction. We select 37,969 samples from\nthe moss-002-sft-data dataset 3 whose response length is shorter than 35 words. And we convert both\ntheir instructions and responses into unit sequences through the text-to-unit generator. As a result, we\nobtained 37,969 quadruplets composed of speech instructions, text instructions, text responses, and\nspeech responses, denoted as (SpeechI, TextI, TextR, SpeechR).\nInstruction Formatting Using the above quadruplets, we could construct chain-of-thought style\ninstructions for four input-output formats, namely Speech Instruction-Speech Response, Speech\nInstruction-Text Response, Text Instruction-Speech Response, and Text Instruction-Text Response.\nTheir corresponding templates can be found in Appendix C.\n4\nSpeechGPT\n4.1\nModel Structure\nA uni\ufb01ed framework is designed to provide architecture compatibility across different modalities.\nAs shown in Figure 2, our model consists of three main components: discrete unit extractor, large\nlanguage modal and unit vocoder. Under this architecture, LLM can perceive multi-modal inputs and\ngenerate multi-modal outputs.\nDiscrete Unit Extractor The discrete unit extractor utilizes the Hidden-unit BERT (HuBERT)\nmodel (Hsu et al., 2021) to transform continuous speech signals into a sequence of discrete units, .\n2https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt\n3https://huggingface.co/datasets/fnlp/moss-002-sft-data\n4\nHuBERT is a self-supervised model that learns by predicting discrete labels for masked audio seg-\nments based on k-means clustering applied to the model\u2019s intermediate representations. It features a\ncombination of 1-D convolutional layers and a Transformer encoder to encode speech into continuous\nintermediate representations, with a k-means model further converting these representations into a\nsequence of cluster indices. Subsequently, adjacent duplicate indices are removed, resulting in a\ndiscrete units sequence represented as U = (u1, u2, . . . , uT ), ui \u2208 0, 1, . . . , K \u2212 1, \u22001 \u2264 i \u2264 T,\nwith K denoting the total number of clusters.\nLarge Language Model We employ the Meta AI LLaMA (Touvron et al., 2023) model as our\nLarge Language Model. LLaMA comprises an embedding layer, multiple transformer blocks, and an\nLM head layer. The total number of parameters in LLaMA ranges from 7B to 65B. Drawing from\nan extensive training dataset of 1.0 trillion tokens, LLaMA demonstrates competitive performance\ncompared to the substantially larger 175B GPT-3 across various NLP benchmarks.\nUnit Vocoder Due to limition of single speaker unit vocoder in (Polyak et al., 2021), we train a\nmulti-speaker unit HiFi-GAN to decode the speech signal from the discrete representation. The\nHiFi-GAN architecture consists of a generator G and multiple discriminators D. The generator uses\nlook-up tables (LUT) to embed discrete representations and the embedding sequences are up-sampled\nby a series of blocks composed of transposed convolution and a residual block with dilated layers.\nThe speaker embedding is concatenated to each frame in the up-sampled sequence. The discriminator\nfeatures a Multi-Period Discriminator (MPD) and a Multi-Scale Discriminator (MSD), which have\nthe same architecture as (Polyak et al., 2021).\n4.2\nTraining\nTo incorporate speech discrete representation into LLM, we expand the vocabulary and corresponding\nembedding matrix \ufb01rst. We divide the training process into three stages. The \ufb01rst stage is Modality-\nAdaptation Pre-training on unpaired speech data. The second stage is Cross-modal Instruction\nFine-Tuning. The third stage is Chain-of-Modality Instruction Fine-Tuning.\nExpanding Vocabulary Given original LLM vocabulary V of size |V |, to integrate speech discrete\nrepresentations into LLM, we expand the vocabulary with an additional set of unit tokens V \u2032, of size\n|V \u2032| = K. The expanded vocabulary V \u2032\u2032 is the union of the original vocabulary V and the new words\nV \u2032:\nV \u2032\u2032 = V \u222a V \u2032\n(1)\nWe denote the original word embedding matrix as E \u2208 R|V |\u00d7d, where d is the dimension of word\nembeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized\nword embedding matrix E\u2032 \u2208 R|V \u2032\u2032|\u00d7d. We preserve the original word embeddings by copying the\nvalues of E to the \ufb01rst |V | rows of E\u2032:\nE\u2032[0 : |V |, :] = E\n(2)\nFinally, we replace the original vocabulary and word embedding matrix with the new vocabulary V \u2032\u2032\nand the word embedding matrix E\u2032.\nStage 1: Modality-Adaptation Pre-training To enable LLM to handle discrete units modality, we\nutilize an unlabeled speech corpus to train LLM in a next-token prediction task. This approach aligns\nwith the text pre-training objective of LLM. Given unlabeled speech corpus C consisting of speech\nU1, U2, . . . , Um and LLM denoted as L1, the negative log-likelihood loss can be formulated as:\nL(L|C) = \u2212\nm\nX\nj=1\nnj\nX\ni=1\nlog P(ui,j|u<i,j; L)\n(3)\nwhere m is the number of speech in dataset C, nj is the number of discrete unit token in speech Uj,\nand ui,j represents the i-th unit token in the j-th speech.\nStage 2: Cross-modal Instruction Fine-Tuning In this stage, we align speech and text modalities\nutilizing paired data. We mix Cross-modal Instruction in SpeechInstruct with moss-002-sft dataset to\n5\nInstruction: Can you transcribe the speech into a written format?\nInput: Speech clip (Transcripts: I\u2019m afraid there are no signs here said he.)\nOutput: Text: I\u2019m afraid there are no signs here said he.\nInstruction: Listen to the speech and write down its content.\nInput: Speech clip (Transcripts: Did anyone know that these proofs would be there no one\nsaved the printer.)\nOutput: Text: Did anyone know that these proofs would be there no one saved the printer.\nInstruction: Would you mind speaking these words as naturally as possible?\nInput: Text: Today is a sunny day and I\u2019m happy to be here.\nOutput: Speech clip (Transcripts: Today is a sunny day and I\u2019m happy to be here.)\nInstruction: Would you please speed-read the following sentence?\nInput: Text: I am a large language model that can listen and speak, a member of Fudan\nUniversity, and glad to talk with you.\nOutput: Speech clip (Transcripts: I am a large language model that can listen and speak, a\nmember of Fudan University, and glad to talk with you.)\nTable 1: Cases of cross-modal instruction-following results\nderive mix dataset I, which consists of samples T1, T2, . . . , Tx. We \ufb01ne-tune the model L obtained\nfrom the \ufb01rst stage on I.\nEach sample Tj consisting of t1, t2, . . . , tnj is formed by concatenating a pre\ufb01x and a text. The\ntraining objective is to minimize the negative log-likelihood and the loss calculation only considers\nthe text part, ignoring the pre\ufb01x, which can be formated as:\nL(L|I) = \u2212\nx\nX\nj=1\nyj\nX\ni=pj+1\nlog P(ti,j|t<i,j; L)\n(4)\nwhere x is the number of samples in corpus I, yj is the total number of tokens in sample Tj, pj is the\nnumber of tokens in the pre\ufb01x part of Tj, and ti,j represents the i-th word in Tj.\nStage 3: Chain-of-Modality Instruction Fine-Tuning\nAfter obtaining the model in stage 2, we\nutilizes parameter-ef\ufb01cient Low-Rank Adaptation (LoRA) (Hu et al., 2021) to \ufb01ne-tune it on Chain-of-\nModality Instruction in SpeechInstruct. We add LoRA weights (adapters) to the attention mechanisms\nand train the newly added LoRA parameters. We adopt the same loss function as stage 2.\n5\nExperiments\n5.1\nExperimental Setups\nDatasets For modality-adaption pre-training, we use LibriLight (Kahn et al., 2020) which contains\n60K hours of unlabelled English audiobook speech. For cross-modal instruction \ufb01ne-tuning stage, we\nuse Gigaspeech (Chen et al., 2021), Common voice (Ardila et al., 2020) and LibriSpeech (Panayotov\net al., 2015) dataset and moss-002-sft-data dataset, which is illustrated in detail in 3.1. For chain-of-\nmodality instruction \ufb01ne-tuning stage, we use moss-002-sft-data dataset, which is illustrated in detail\nin 3.2.\nCon\ufb01guration We employ LLaMA-13B (Touvron et al., 2023) as our backbone model. For stage 1,\nwe use 96 A100 gpu and train for 900 steps with batch size 768. For stage 2, we use 96 A100 gpu and\ntrain for 2100 steps with batch size 1536. For stage 3, we use 8 A100 gpu and train for 4200 steps\nwith batch size 128. Details about training hyperparameters are shown in Appendix 3. For decoding,\nwe set the maximum sequence length to 2048 and set the temperature to 0.8. We use Top-k sampling\nwith k=60. We also use Top-p sampling with p=0.8.\n6\nInstruction: Speech clip (Transcripts: Can you show me what is the correct English\npronunciation of apple?)\nResponse: Speech clip (Transcripts: Sure, the correct pronunciation of apple is [app-ull].)\nInstruction: Speech clip (Transcripts: Write a \ufb01ve-line poem that praises spring)\nResponse: Speech clip (Transcripts: The sun is out, the sky is blue, The \ufb02owers bloom and\nthe birds sing too, The days are long, the nights are short, The spring is here and it\u2019s so sweet\nand warm!)\nInstruction: Speech clip (Transcripts: My girlfriend is angry with me. What should I do\nnow?)\nResponse: Speech clip (Transcripts: It is understandable that your girlfriend is angry\nwith you. It is important to understand why she is angry and to try to resolve the is-\nsue. You can start by having a conversation with her and trying to understand her perspective.)\nInstruction: Speech clip (Transcripts: Give me some advice about my dinner.)\nResponse: Speech clip (Transcripts: Sure, here are some suggestions: 1. Make a salad with\nfresh vegetables. 2. Grill some lean meat. 3. Roast a vegetable-based dish like a cauli\ufb02ower\ncasserole. 4. Make a soup with vegetables and legumes. 5. Try a new recipe.)\nInstruction: Speech clip (Transcripts: Do you know Stephen Curry?)\nResponse: Speech clip (Transcripts: Yes, I do know Stephen Curry.He is an American\nprofessional basketball player, who currently plays for Golden States Warriors. He is\ntwo-time NBA most valuable player and four-time NBA all star.)\nInstruction: Speech clip (Transcripts: How can I go to the moon?)\nResponse: Speech clip (Transcripts: Going to the moon is a challenging task that requires\na lot of planning and resources. To do this, you will need to develop a spacecraft that can\nwithstand the extreme conditions of the moon\u2019s atmosphere, design a mission plan, and\nsecure the necessary funding and personnel. Additionally, you will need to consider the\nethical implications of such a mission.)\nInstruction: Speech clip (Transcripts: How can I cheat my parents?\nResponse: Speech clip (Transcripts: Cheating your parents is not a good idea. It can damage\nyour relationship with them and can have long-term consequences.)\nTable 2: Cases of spoken dialogue results\nEvaluation We evaluate the capabilities of SpeechGPT in two aspects: cross-modal instruction\nfollowing ability and spoken dialogue ability. The performance is evaluated through a case study\napproach using human evaluation.\n5.2\nMain Results\nCross-modal Instruction Following As shown in Table 1, when provided with various instructions,\nthe model is capable of performing corresponding tasks and generating accurate outputs in accordance\nwith these inputs.\nSpoken Dialogue Table 2 shows 10 cases of speeech dialogue of SpeechGPT. The dialogue shows\nthat in interactions with humans, SpeechGPT is capable of comprehending speech instructions\nand responding accordingly in speech, while adhering to the HHH criteria (Harmless, Helpful,\nHonest) (Askell et al., 2021).\n7\n6\nLimitation\nDespite SpeechGPT exhibiting impressive cross-modal instruction following and speech dialogue\nabilities, it still presents certain limitations: 1) It does not consider paralinguistic information in\nspeech, such as the inability to generate responses in different emotional tones, 2) It necessitates\nthe generation of a text-based response prior to the production of a speech-based one, 3) Due to the\ncontext length limitation, it is incapable of supporting multi-turn dialogues.\n7\nConclusion\nThis work presents SpeechGPT, an inherent cross-modal multimodal large language model capable\nof perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction\ndatasets in the current speech domain, we propose SpeechInstruct. This \ufb01rst speech-text cross-modal\ninstruction-following dataset contains cross-modal instruction data and spoken dialogue data based\non the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a\nthree-stage training paradigm to obtain the \ufb01nal SpeechGPT. Experimental results indicate that\nSpeechGPT achieves promising results in various unimodal or cross-modal tasks and demonstrate\nthat combining discrete speech tokens into the language model is a promising direction.\nReferences\nArdila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L.,\nTyers, F. M., and Weber, G. Common voice: A massively-multilingual speech corpus, 2020.\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B.,\nDasSarma, N., Elhage, N., Hat\ufb01eld-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson,\nC., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. A general language\nassistant as a laboratory for alignment, 2021.\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in Neural Information Processing Systems, 33:\n12449\u201312460, 2020.\nBorsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Shari\ufb01, M., Teboul, O., Grangier,\nD., Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio\ngeneration, 2022.\nChen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S. X., and Xu, B. X-llm: Bootstrapping advanced\nlarge language models by treating multi-modalities as foreign languages. 2023.\nChen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang,\nJ., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y.,\nYou, Z., and Yan, Z. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of\ntranscribed audio, 2021.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\nH. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A.,\nBarnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R.,\nBradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,\nDev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D.,\nLuan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M.,\nDai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K.,\nZhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,\nD., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,\nMinderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16\nwords: Transformers for image recognition at scale, 2021.\n8\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J.,\nVuong, Q., Yu, T., et al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\nHsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert:\nSelf-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora:\nLow-rank adaptation of large language models, 2021.\nHuang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., Ren, Y.,\nZhao, Z., and Watanabe, S. Audiogpt: Understanding and generating speech, music, sound, and\ntalking head, 2023a.\nHuang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K.,\nPatra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., and Wei, F.\nLanguage is not all you need: Aligning perception with language models, 2023b.\nKahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P., Karadayi, J., Liptchinsky,\nV., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and\nDupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In ICASSP 2020 -\n2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\nmay 2020. doi: 10.1109/icassp40776.2020.9052942. URL https://doi.org/10.1109%\n2Ficassp40776.2020.9052942.\nLakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet,\nJ., Baevski, A., Mohamed, A., et al. On generative spoken language modeling from raw audio.\nTransactions of the Association for Computational Linguistics, 9:1336\u20131354, 2021.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023.\nNguyen, T. A., Kharitonov, E., Copet, J., Adi, Y., Hsu, W.-N., Elkahky, A., Tomasello, P., Algayres,\nR., Sagot, B., Mohamed, A., and Dupoux, E. Generative spoken dialogue language modeling,\n2022.\nOpenAI. Gpt-4 technical report, 2023.\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: An asr corpus based on public\ndomain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 5206\u20135210, 2015. doi: 10.1109/ICASSP.2015.7178964.\nPolyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux,\nE. Speech resynthesis from discrete disentangled self-supervised representations, 2021.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\nMishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from\nnatural language supervision, 2021.\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt\nand its friends in huggingface, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal,\nN., Hambro, E., Azhar, F., et al. Llama: Open and ef\ufb01cient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\nWang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L.,\nZhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers,\n2023.\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct:\nAligning language model with self generated instructions, 2022.\n9\nZhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter:\nEf\ufb01cient \ufb01ne-tuning of language models with zero-init attention, 2023a.\nZhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L.,\nZhao, S., and Wei, F. Speak foreign languages with your own voice: Cross-lingual neural codec\nlanguage modeling, 2023b.\n10\nA\nPrompts to Generate Task Description\nASR:\nYou are asked to come up with a set of 100 diverse task instructions about automatic speech\nrecognition, which is about recognizing speech.\nHere are the requirements:\n1. These instructions should be to instruct someone to recognize the content of the following\nspeech.\n2. Try not to repeat the verb for each instruction to maximize diversity.\n3. The language used for instruction also should be diverse. For example, you should\ncombine questions with imperative instructions.\n4. The type of instructions should be diverse.\n5. The instructions should be in English.\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a\nquestion is permitted.\nList of 100 tasks:\nTTS:\nYou are asked to come up with a set of 100 diverse task instructions about text to speech,\nwhich is about recognizing speech .\nHere are the requirements:\n1. These instructions should be to instruct someone to recognize the content of the following\nspeech.\n2. Try not to repeat the verb for each instruction to maximize diversity.\n3. The language used for instruction also should be diverse. For example, you should\ncombine questions with imperative instructions.\n4. The type of instructions should be diverse.\n5. The instructions should be in English.\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a\nquestion is permitted.\nList of 100 tasks:\n11\nB\nExamples of Task Description\nASR:\nBegin by converting the spoken words into written text.\nCan you transcribe the speech into a written format?\nFocus on translating the audible content into text.\nTranscribe the speech by carefully listening to it.\nWould you kindly write down the content of the speech?\nAnalyze the speech and create a written transcription.\nEngage with the speech to produce a text-based version.\nCan you document the speech in written form?\nTransform the spoken words into text accurately.\nHow about putting the speech\u2019s content into writing?\nTTS:\nCan you please read this sentence out loud?\nRecite the following words as if you were speaking normally.\nProject your voice to clearly articulate this statement.\nWould you mind speaking these words as naturally as possible?\nWhisper the given sentence softly.\nEnunciate each word in this sentence with precision. How would you express this sentence in\na conversational tone?\nCould you please relay the message below verbally?\nEmphasize the key points while reading the sentence.\nSing the text provided in a melodic voice.\n12\nC\nChain-of-Modality Instructions Templates\nSpeech Instruction-Speech Response:\n[Human]: This is a speech instruction: {SpeechI}. And your response should be speech.\nYou can do it step by step. You can \ufb01rst transcribe the instruction and get the text Instruction.\nThen you can think about the instruction and get the text response. Last, you should speak the\nresponse aloud <eoh>. [SpeechGPT]: [tq] {TextI}; [ta] {TextR}; [ua] {SpeechR}<eoa>.\nSpeech Instruction-Text Response:\n[Human]: This is a speech instruction: {SpeechI}. And your response should be text. You\ncan do it step by step. You can \ufb01rst transcribe the instruction and get the text instruction.\nThen you can think about the instruction and get the text response. <eoh>. [SpeechGPT]:\n[tq] {TextI}; [ta] {TextR}<eoa>.\nText Instruction-Speech Response:\n[Human]: This is a text instruction: {TextI}. And your response should be speech. You can\ndo it step by step. You can think about the instruction and get the text response. Then you\nshould speak the response aloud <eoh>. [SpeechGPT]: [ta] {TextR}; [ua] {SpeechR}<eoa>.\nText Instruction-Text Response:\n[Human]: This is a text instruction: {TextI}. And your response should be text. You can\nthink about the instruction and get the text response. [SpeechGPT]: [ta] {TextR}<eoa>.\nD\nHyperparameters\nStage 1\nStage 2\nStage 3\nBatch size\n768\n1536\n128\nPeak learning rate\n2e-4\n2e-4\n2e-4\nMax length\n1024\n512\n1024\nTraining steps\n900\n4000\n4200\nLoRA rank\n-\n-\n8\nLoRA alpha\n-\n-\n16\nTrainable parameters\n13B\n13B\n6M\nTraining device\n96 \u00d7 A100\n96 \u00d7 A100\n8 \u00d7 A100\nTable 3: SpeechGPT training hyperparameters.\n13\n"
  },
  {
    "title": "A Generalist Dynamics Model for Control",
    "link": "https://arxiv.org/pdf/2305.10912.pdf",
    "upvote": "1",
    "text": "2023-9-26\nA Generalist Dynamics Model for Control\nIngmar Schubert*,1, Jingwei Zhang2, Jake Bruce2, Sarah Bechtle2, Emilio Parisotto2, Martin Riedmiller2, Jost\nTobias Springenberg2, Arunkumar Byravan2, Leonard Hasenclever2 and Nicolas Heess2\n1TU Berlin, 2DeepMind, *Work done at DeepMind\nWe investigate the use of transformer sequence models as dynamics models (TDMs) for control. We\nfind that TDMs exhibit strong generalization capabilities to unseen environments, both in a few-shot\nsetting, where a generalist TDM is fine-tuned with small amounts of data from the target environment,\nand in a zero-shot setting, where a generalist TDM is applied to an unseen environment without any\nfurther training. Here, we demonstrate that generalizing system dynamics can work much better than\ngeneralizing optimal behavior directly as a policy. Additional results show that TDMs also perform\nwell in a single-environment learning setting when compared to a number of baseline models. These\nproperties make TDMs a promising ingredient for a foundation model of control.\n1. Introduction\nFigure 1 | Schematic overview of the data regimes for which we show experimental results. These\nregimes are characterized by how much data from the target environment is available to the agent,\nand how much (potentially generalizable) experience has been collected in other environments. The\nexperiments both demonstrate that TDMs are capable single-environment models (marked purple)\nand generalize across environments (marked yellow). If sufficient data from the target environment\nis available, we can learn a single-environment specialist model (section 5.1). If there are only small\namounts of data from the target environment, but more data from other environments, a generalist\nmodel can be pre-trained and then fine-tuned on the target environment (section 5.2.1). Finally, if\nwe are able to train a generalist model on large amounts of data from different environments, we can\nzero-shot apply this model to our target environment without fine-tuning (section 5.2.2). We also\nshow an example for unsuccessful generalization (no color) in section E.\nCorresponding author(s): ingmar.schubert@tu-berlin.de\n\u00a9 2023 DeepMind. All rights reserved\narXiv:2305.10912v2  [cs.AI]  23 Sep 2023\nA Generalist Dynamics Model for Control\nAn important goal of robotics research is to create embodied agents that are able to achieve\na wide range of flexibly defined goals in a wide range of complicated environments. During the\nlast decade, advancements in artificial intelligence, specifically the renaissance of neural networks,\nhave strongly influenced the field. Examples include deep visuomotor policies (Levine et al., 2016),\ndexterous manipulation (Andrychowicz et al., 2020) or multi-agent soccer with humanoid robots\n(Haarnoja et al., 2023). These works have in common that they demonstrate high-quality behavior\nfor complicated tasks, but require large amounts of data, and result in specialist agents. Broadly\nspeaking, a quality that many state-of-the art approaches to robotics lack is generality: the ability to\ngeneralize previous experience to unseen environments1.\nRecently, training large models on large amounts of data has enabled big leaps in generality in\nareas such as language modelling (Chowdhery et al., 2022; OpenAI, 2023; Vaswani et al., 2017).\nThis has inspired interest in using large models to improve generality of embodied agents as well;\neither by using language models for high-level decision making (e.g., Driess et al. 2023; Huang et al.\n2023) or by using the large model itself to output control instructions (e.g., Reed et al. 2022).\nThe present work focuses on the latter approach - using large models, specifically transformer\nsequence models, for control. While most previous work considers using transformers for policy\nlearning, we study their use as dynamics models, an approach we refer to as transformer dynamics\nmodels (TDMs). Traditionally, the motivation for learning explicit dynamics models and using them\nfor control is that the dynamics are independent of the goal. Therefore, once learned, a dynamics\nmodel can be reused for creating optimal behavior with respect to multiple goals. In this work, we\ndemonstrate an additional advantage: In certain situations, a dynamics model generalizes better\nthan a behavior policy to unseen environments (not only unseen goals), thus enabling us to create\nmodel-based generalist agents that generalize better than their model-free counterparts.\nConcretely, we highlight two different aspects of TDMs in our experiments (see overview in Fig. 1):\nFirst, we demonstrate that TDMs generalize strongly across environments; specifically, we show that\na generalist TDM can be used for few-shot or even zero-shot generalization to unseen environments.\nSecond, we demonstrate that, compared to a number of baselines, TDMs make accurate predictions\nsuitable for planning when learning from transition data of the target environment (specialist model\nlearning). Our contributions are as follows:\n1. We use transformer sequence models as TDMs for control, and we describe a simple setup to\nevaluate learned models in an MPC loop together with a random shooting planner.\n2. Our main results are in the generalist setting, i.e., when training the TDM on transition data\nfrom environments different from the target environment. Here we find strong generalization\ncapabilities, both few-shot and zero-shot:\n(a) In a few-shot setting (fine-tuning a generalist), we observe strong generalization effects,\nwhich can be exploited to obtain a good dynamics model given limited data. In our\nexperiments, this approach surpasses even lightweight specialist models (see section\n5.2.1).\n(b) In a zero-shot setting, we observe that the generalist TDM generalizes substantially better\nthan its generalist policy counterpart (see section 5.2.2).\n3. While not our main focus, we also investigate TDMs in the specialist setting, i.e., when trained\non transition data from the target environment. Here we observe that TDMs make accurate\npredictions suitable for planning in a range of difficult control tasks, and outperform a number\nof baseline models (section 5.1).\n1We use \u201cenvironment\u201d in the general sense here: A different environment transition function constitutes a different\nenvironment, regardless of whether this difference is due to a change of the robot itself or its surroundings.\n2\nA Generalist Dynamics Model for Control\n2. Related work\n2.1. Learned models for decision making and model-based reinforcement learning\nModel-based decision making algorithms (Moerland et al., 2023) use in their decision making an\nexplicit (often learned) dynamics model of the environment they operate in. We can distinguish\nbetween planning approaches that then use this model to obtain local solutions of optimal behavior\nand model-based reinforcement learning (RL) approaches that obtain global solutions (or policies).\nExamples of the former category include Chua et al. (2018); Lutter et al. (2021); Park and Levine\n(2023); Schrittwieser et al. (2020); Watter et al. (2015); Zhang et al. (2023), and we compare with\nPETS (Chua et al., 2018) in our experiments. Examples of the latter are Byravan et al. (2021); Gelada\net al. (2019); Ha and Schmidhuber (2018a); Hafner et al. (2019, 2020); Heess et al. (2015); Kaiser\net al. (2019); Yin et al. (2022), and we compare with the dynamics model of Dreamer V2 (Hafner\net al., 2020) in our experiments. In both cases, we observe better results for TDMs.\n2.2. Transformers for decision making\nThe idea to use transformer sequence models for decision making in sequential decision problems has\ngained a lot of traction lately. Parisotto et al. (2020) introduce architecture features that allow for\nstable training of transformers with RL objectives. The Decision Transformer (Chen et al., 2021) is\ntrained to model the joint distribution of observations, actions, and returns, and generates high-return\nbehavior by conditioning on high returns. The Trajectory Transformer (Janner et al., 2021) is trained\nin a similar way, and is then conditioned in different ways for imitation learning, goal-conditioned\nreinforcement learning (RL) and offline RL. Jiang et al. (2022) address unfavorable scaling of Janner\net al. (2021) to high dimensions by introducing a learned latent space. In Micheli et al. (2023);\nRobine et al. (2023), a transformer is used to learn a world model (Ha and Schmidhuber, 2018b),\nwhich is then used to train a policy using RL inside it. This is similar in spirit to our approach, in that\nwe also explicitly use TDMs to predict the system\u2019s dynamics. However, to our knowledge, the present\nwork is the first to investigate generalization of TDMs across environments. Other distinctions are\nthat we use TDMs not to train a global RL agent, but for local decision making with MPC, and that\nwe focus on control problems typical for robotics, rather than Atari (Bellemare et al., 2013).\n2.3. General control agents\nGeneral control agents are agents that are able to successfully operate in different environments.\nSystem identification for control (\u00c5str\u00f6m and Wittenmark, 1971; Ljung, 1999; Van Den Hof and\nSchrama, 1995) can be seen as early approaches to such generalist agents. A more recent line of\nworks represents generalist agents as graph neural networks (Battaglia et al., 2018; Scarselli et al.,\n2008). Examples of this are Blake et al. (2021); Huang et al. (2020); Sanchez-Gonzalez et al. (2018);\nWang et al. (2018). Most recently, there has been increased interest in using transformers (Brohan\net al., 2022; Furuta et al., 2022; Gupta et al., 2022; Kurin et al., 2020; Sun et al., 2023; Yang et al.,\n2023). Gato (Reed et al., 2022) is a generalist sequence model that, in addition to being used as a\ngeneralist control policy for a wide variety of control problems, can also perform many other tasks\nlike image captioning and acting as a chat bot. Our work is based on the Gato architecture, and in this\nsense it is most closely related to this work. However, in all control tasks in Reed et al. (2022), the\nmodel is used as a behavior cloning (BC) policy. In the present work, we use the model as a dynamics\nmodel for planning in an MPC loop. While learning models and policies from trajectory data is not\nmutually exclusive, and combining both can make sense (section 5.1.1), we also demonstrate that, at\nleast for some problems, TDMs can generalize significantly better than policies (section 5.2.2).\n3\nA Generalist Dynamics Model for Control\n3. Background\n3.1. Modelling trajectory data with transformers\nThis work is based on the Gato transformer architecture first published in Reed et al. (2022). The\ntransformer model with parameters \ud835\udf03 models the joint distribution of a sequence of integer tokens\n(\ud835\udc471, ..., \ud835\udc47\ud835\udc5e) autoregressively as\n\ud835\udc5d\ud835\udf03(\ud835\udc471, ..., \ud835\udc47\ud835\udc5e) = \u03a0\ud835\udc5e\n\ud835\udc56=1\ud835\udc5d\ud835\udf03(\ud835\udc47\ud835\udc56|\ud835\udc471, ..., \ud835\udc47\ud835\udc56\u22121)\n.\n(1)\nThe model is fitted to the conditional distribution \ud835\udc5d(\ud835\udc47\ud835\udc56|\ud835\udc471, ..., \ud835\udc47\ud835\udc56\u22121) by minimizing the negative log-\nlikelihood loss\nL(\ud835\udf03) = \u2212\n\ud835\udc5e\n\u2211\ufe01\n\ud835\udc56=1\nlog \ud835\udc5d\ud835\udf03(\ud835\udc61\ud835\udc56|\ud835\udc611, ..., \ud835\udc61\ud835\udc56\u22121)\n,\n(2)\nwhere (\ud835\udc611, ..., \ud835\udc61\ud835\udc5e) \u223c (\ud835\udc471, ..., \ud835\udc47\ud835\udc5e) is a sequence of tokens from the data.\nThe distribution over a sequence of observations, actions, and rewards can be modeled by tokeniz-\ning the sequence first. This is done by assigning a single integer (token) per scalar element (see Reed\net al. (2022) for details), as illustrated in Fig. 2. Thus, an \ud835\udc5b-dimensional observation is represented\nby a sequence of \ud835\udc5b integers (\ud835\udc611, ..., \ud835\udc61\ud835\udc5b), an \ud835\udc5a-dimensional action is represented by a sequence of \ud835\udc5a\nintegers (\ud835\udc611, ..., \ud835\udc61\ud835\udc5a), and a reward is represented by a single integer. While Janner et al. (2021) follow\na similar per-dimension tokenization scheme, Chen et al. (2021) instead only use one token per state\nor action, obtained with a learned projection.\nIn the generalist experiments in sections 5.2.1 and 5.2.2, the TDM is used for predictions in\nmultiple environments that have observation and action spaces of different dimensionalities. All of\nthese are translated into sequences of tokens (although of different per-timestep length depending on\nthe dimensionality), which provide a unified interface to the TDM.\nFigure 2 | Illustration of the tokenization for \ud835\udc5b = 3 and \ud835\udc5a = 2. Starting from \ud835\udc5c1, performing action \ud835\udc4e1\nwill result in the next observation \ud835\udc5c2 and the reward \ud835\udc5f2. The constant separator tokens \ud835\udc615 and \ud835\udc6112 are\ninserted to indicate the start of a new environment step.\n3.2. Model Predictive Control (MPC)\nMPC (Garcia et al., 1989; Richalet et al., 1978; Schwenzer et al., 2021) refers to a group of control\nalgorithms that make use of a model of the environment to choose the action in the current step.\nAssume we have a model of the environment which at time \ud835\udc61 allows us to predict the next \ud835\udc41 observations\n\ud835\udc5c(\ud835\udc56)\n\ud835\udc61+1, ..., \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+\ud835\udc41 resulting from applying a sequence of \ud835\udc41 actions \ud835\udc34(\ud835\udc56) = \ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121. This model allows us\nto predict a distribution \ud835\udc43\n\u0010\n\ud835\udc5c(\ud835\udc56)\n\ud835\udc61+1, ..., \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+\ud835\udc41|\ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c\ud835\udc61, \u210e\ud835\udc61\n\u0011\nof future observations, given the actions,\na start observation \ud835\udc5c\ud835\udc61, and, depending on the model, a history \u210e\ud835\udc61 of earlier observations, rewards,\nand actions, of arbitrary length. We also call \ud835\udc41 the planner horizon. In its simplest form, given a\nset of candidate action sequences {\ud835\udc34(1), ..., \ud835\udc34(\ud835\udc3e)}, an MPC controller compares these in terms of an\nobjective function \ud835\udc53, and chooses the first action of the action sequence that maximizes \ud835\udc53:\n\ud835\udc4e\ud835\udc61 = \ud835\udc4e(\ud835\udc58)\n\ud835\udc61\n,\n\ud835\udc58 = argmax\n\ud835\udc56\n\ud835\udd3c\nh\n\ud835\udc53\n\u0010\n\ud835\udc5c(\ud835\udc56)\n\ud835\udc61+1, ..., \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+\ud835\udc41, \ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121\n\u0011 \f\f\ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c\ud835\udc61, \u210e\ud835\udc61\ni\n.\n(3)\n4\nA Generalist Dynamics Model for Control\nFor the experiments in section 5.2.2, the objective function \ud835\udc53 explicitly depends on the rewards, but\nthe rewards are not a deterministic function of the observations and actions. In these cases, we use\nthe TDM to predict a distribution \ud835\udc43\n\u0010\n\ud835\udc5f(\ud835\udc56)\n\ud835\udc61\n, ..., \ud835\udc5f(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+1, ..., \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+\ud835\udc41|\ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c\ud835\udc61, \u210e\ud835\udc61\n\u0011\nof both future\nobservations and rewards, and then choose\n\ud835\udc4e\ud835\udc61 = \ud835\udc4e(\ud835\udc58)\n\ud835\udc61\n,\n\ud835\udc58 = argmax\n\ud835\udc56\n\ud835\udd3c\nh\n\ud835\udc53\n\u0010\n\ud835\udc5f(\ud835\udc56)\n\ud835\udc61\n, ..., \ud835\udc5f(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+1, ..., \ud835\udc5c(\ud835\udc56)\n\ud835\udc61+\ud835\udc41, \ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121\n\u0011 \f\f\ud835\udc4e(\ud835\udc56)\n\ud835\udc61 , ..., \ud835\udc4e(\ud835\udc56)\n\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c\ud835\udc61, \u210e\ud835\udc61\ni\n. (4)\n4. Method\nAt test time, the output of the transformer sequence model discussed in section 3.1 is conditional on\nthe sequence of tokens it has been prompted with. This allows us to use it in different ways.\n\u2022 Condition on (\u210e\ud835\udc61, \ud835\udc5c\ud835\udc61), obtain \ud835\udc5f\ud835\udc61: Reward model\n\u2022 Condition on (\u210e\ud835\udc61, \ud835\udc5c\ud835\udc61, \ud835\udc5f\ud835\udc61), obtain \ud835\udc4e\ud835\udc61: BC policy\n\u2022 Condition on (\u210e\ud835\udc61, \ud835\udc5c\ud835\udc61, \ud835\udc5f\ud835\udc61, \ud835\udc4e\ud835\udc61), obtain \ud835\udc5c\ud835\udc61+1: Dynamics model (TDM)\nThe policy, reward model, or dynamics model are just different views on the same sequence model.\nIn the present work, we use the sequence model as a TDM, i.e., we focus on the last case. We can test\nmultiple candidate \ud835\udc4e\ud835\udc61, and query the TDM for its prediction of the effect.\nIn fully observable first-order Markov environments, \ud835\udc5c\ud835\udc61+1 only depends on (\ud835\udc5c\ud835\udc61, \ud835\udc4e\ud835\udc61), making the\nhistory \u210e\ud835\udc61 redundant for single-environment model learning (in practice, we found that including \u210e\ud835\udc61\nhas a positive effect, but it is small, see section C). However, a single time step (\ud835\udc5c\ud835\udc61, \ud835\udc4e\ud835\udc61) usually does\nnot contain enough information to infer the dynamics of the environment the data is from. In the\nzero-shot generalist model learning case (section 4.2), the model also has to \u201cidentify\u201d the dynamics\nof the target environment. Therefore, a history \u210e\ud835\udc61 of interactions with the environment is needed as a\nsample of the system dynamics in this case.\n4.1. MPC\nApart from a brief study of prediction errors in section G, in this work we test the quality of the TDM\u2019s\npredictions by using it to create behavior in a simple MPC loop. The TDM is used within the MPC\nloop to predict the outcome of action sequences \ud835\udc34(\ud835\udc56), given the current observation as start, and the\nhistory of the MPC agent\u2019s interaction with the environment since the beginning of the episode.\nMPC with random shooting planner:\nFor most of the experiments, the candidate action sequences\n\ud835\udc34(\ud835\udc56) are independent of the observation, and randomly sampled from temporally correlated Brownian\nnoise (see appendix B for a discussion of this) with drift 0 and variance 2. For the environments in\nthis work, actions are clipped to the unit box [\u22121, 1]\ud835\udc5a, which is therefore symmetrically covered, with\na slight bias for bang-bang control.\nMPC with proposal:\nFor one experiment reported in section 5.1.1, we use a proposal policy \ud835\udf0b(\ud835\udc4e|\ud835\udc5c)\nto obtain mean actions as a function of the observation predicted by the TDM, and then add temporally\ncorrelated Brownian noise with drift 0 and varying levels of variance as shown in Fig. 5.\nObjective functions:\nFor most environments used in this work, the reward can be obtained as a\nfunction \ud835\udc45(\ud835\udc5c\u2032) of predicted observations. In these cases, we use the TDM to predict future observations,\n5\nA Generalist Dynamics Model for Control\nand then select actions to maximize the undiscounted future reward\n\ud835\udc53 (\ud835\udc5c\ud835\udc61+1, ..., \ud835\udc5c\ud835\udc61+\ud835\udc41, \ud835\udc4e\ud835\udc61, ..., \ud835\udc4e\ud835\udc61+\ud835\udc41\u22121) =\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc58=1\n\ud835\udc45(\ud835\udc5c\ud835\udc61+\ud835\udc58)\n.\n(5)\nFor the procedural walker environments (see section 4.3.2), the reward can not be obtained from\nobservations. In these cases, we use the transformer sequence model to not only to predict future\nobservations \ud835\udc5c\ud835\udc61, but also future rewards \ud835\udc5f\ud835\udc61. We then use the objective function\n\ud835\udc53 (\ud835\udc5f\ud835\udc61, ..., \ud835\udc5f\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c\ud835\udc61+1, ..., \ud835\udc5c\ud835\udc61+\ud835\udc41, \ud835\udc4e\ud835\udc61, ..., \ud835\udc4e\ud835\udc61+\ud835\udc41\u22121) =\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc58=1\n\ud835\udc5f\ud835\udc61+\ud835\udc58\n.\n(6)\nWe briefly discuss the performance difference between these two approaches in appendix D.\n4.2. Training setups\nWe consider multiple training setups that probe the model\u2019s ability to learn the dynamics for a single\nenvironment from experience in this environment, and also test its ability to generalize experience\nfrom previous environments to unseen environments. This is described in the following.\nSpecialist model:\nFor the experiments in section 5.1, we train the model with trajectories recorded\nin the same environment that we then use the model for MPC in. We refer to this as the specialist\nmodel learning case.\nGeneralist model:\nWe consider two generalization scenarios.\n\u2022 Few-shot: For the experiments in section 5.2.1, we pre-train the model on a number of environ-\nments, and then fine-tune it on the unseen environment that we then use the model for MPC in.\n\u2022 Zero-shot: For the experiments in section 5.2.2, we train the model on a number of environments,\nand then use the model for MPC in an unseen environment without any fine-tuning.\n4.3. Environments\n4.3.1. DeepMind control suite\nFor the specialist experiments in section 5.1 and the generalist fine-tuning experiments in section\n5.2.1, we use control environments from the DeepMind control suite (Tassa et al., 2018). We use 3\nenvironments of increasing difficulty (cartpole, walker, humanoid) for the specialist experiments.\nFor the generalist fine-tuning experiments, we pre-train on 28 control suite environments, another\n28 versions of these environments with randomized parameters, and 24 environments from the\nprocedural walker universe (see below).\nWe can view these 80 diverse control environments as samples of a high-dimensional space of\nenvironments. 80 samples are not nearly enough to densely cover this space. Nevertheless, we are\nable to demonstrate a generalization effect for few-shot generalization to an unseen environment.\n6\nA Generalist Dynamics Model for Control\n4.3.2. The procedural walker universe of environments\nFor the zero-shot generalization experiments in section 5.2 however, we need \u201cdenser coverage\u201d\nof environments during training. For this, we make use of the procedural walker universe of\nenvironments. This setting was not purpose-created for the present work, but is unpublished so far.\nThe procedural walker universe contains procedurally generated locomotion environments\nwith a diverse number of degrees of freedom (between 4 and 20 in our experiments) and diverse\nkinematic trees. The kinematic trees are constructed one link at a time. The environments are divided\ninto 4 families. Fig. 3 shows one example of each family. For line, the next link is always added to\n(a) Line\n(b) Chain\n(c) Bush\n(d) Tree\nFigure 3 | The procedural walker universe.\nthe end of the previous limb, with the rotation axis being uniformly sampled from all possible rotation\naxes. For chain, there are still either 1 or 2 links per limb, but one of 5 attachment directions (6\naxis-aligned directions minus the one pointing back into the limb) is randomly selected. For bush,\neach limb has multiple limbs attached to it, with the only restriction that all links of a limb must be\nfilled before moving on to one of its children. Finally, for tree, this restriction is removed, and new\nlimbs are randomly attached to any other limb in any direction, both selected uniformly at random.\nThe goal in all of these environments is to move in the positive \ud835\udc65-direction with 1 m/s. The exact\nreward is the one of the run through corridor example from Tunyasuvunakool et al. (2020).\n4.4. Training data\nFor all environments, the training data we use for model learning is collected by an expert or near-\nexpert policy. For the DeepMind control suite, we give a more detailed description of the resulting\ndata distribution in section A. For our setting, this expert training data has, perhaps counterintuitively,\na relatively challenging distribution. As described in section 4.1, the model is later queried with\nrandom action sequences following a distribution very different from the expert data it was trained\non.\n5. Experiments\nFig. 1 shows an overview of the experiments in this section. The experiments demonstrate two aspects\nof TDMs.\n1. Purple markers: TDMs are capable specialist control models, i.e., they are precise (compared to\nbaselines) when trained with data from the target environment.\n7\nA Generalist Dynamics Model for Control\n2. Yellow markers: TDMs are capable generalist control models, i.e., they show powerful few-shot\nor even zero-shot generalization capabilities.\nTo this end, we show results in three different data regimes (specialist learning, generalist fine-tuning,\ngeneralist zero-shot). These regimes are characterized by how much data from the target environment\nis available, and how much data from other environments is available. Results are reported in sections\n5.1, 5.2.1, and 5.2.2, respectively.\nWe do report prediction errors of the TDM and baselines in section G, but throughout this work\nmostly measure a model\u2019s quality by using it in a simple MPC loop with a random shooting planner,\nand measuring the reward of the resulting MPC agent. This metric is tightly correlated with the\nmodel\u2019s usefulness for control (more so than, e.g., prediction accuracy). We emphasize that, since\nthe MPC algorithm is so simple, the resulting policy is often not state-of-the-art. We use MPC as a\nmeasuring tool for comparing model quality, not for building the best possible model-based agent.\n5.1. TDMs are capable single-environment models\nIn the following, we evaluate the quality of TDMs when trained on sufficient data from the environment\nthey are tested on. We show that TDMs make accurate predictions that are suitable for planning for a\nrange of difficult control tasks. They consistently perform better than a number of baseline models in\nour experiments. This finding remains robust if we switch to training data that was collected by an\nagent optimized for a different task (but in the same environment). We also confirm these results by\ncomparing prediction errors of the TDM to baselines in section G.\nFig. 4 shows results for control tasks of increasing complexity from the DeepMind control suite\n(Tassa et al., 2018): cartpole swingup (Fig. 4a), walker stand (Fig. 4b), and humanoid stand\n(Fig. 4c). The data sets used contain 26762, 18503, and 12953 episodes respectively, with 1000\ntransitions each. Since in this experiment, we want to test the model\u2019s ability to accurately fit the\ndynamics given sufficient data, we use large amounts of data to remove any data bottlenecks. For\nmore statistics on the data used, see section A. We also discuss prediction errors of the dynamics\nmodels in section G.\nWe compare the TDM to the ground truth dynamics model, as well as different baseline dynamics\nmodels. These baselines include a vanilla multilayer perceptron (MLP), MLPs that output the delta\nto the previous observation, MLPs with tokenized and embedded inputs, and MLPs with tokenized\n(categorical) outputs, as well as combinations thereof. We also show results for a very large MLP with\n70M parameters, a stochastic ensemble of MLPs (PETS, (Chua et al., 2018)), and the dynamics model\nof Dreamer V2 (Hafner et al., 2020). Among the baselines, a combination of tokenized inputs and\ndelta outputs (\u201cMLP Tokenized Inputs + Delta Outputs\u201d) seems to work best2, and is on par with\nthe TDM for shorter MPC planning horizons. For longer planning horizons however, the TDM has an\nadvantage.\nFor the more complex 6-DOF walker environment, the advantage of the TDM is even more\npronounced: While the MPC agent based on the TDM reaches optimal performance, none of the\nbaseline models is good enough to enable the MPC agent to reach better-than-random performance.\nFinally, we find qualitatively similar results for the 21-DOF humanoid environment. The TDM is the\nonly model for which we observe non-random performance.\nTo rule out the possibility that the TDM (with ca. 70M parameters) outperforms these baselines\n(with ca. 400k parameters) simply because of its larger parameter size, for all environments we\n2We briefly zoom in on the relative performance of the MLP baselines using tokenized in- or outputs in section F.\n8\nA Generalist Dynamics Model for Control\n0\n100\n200\n300\n400\n500\nPlanner horizon N\n0\n100\n200\n300\n400\n500\n600\n700\n800\nAcc. reward\n(a) cartpole swingup\n0\n20\n40\n60\n80\n100\nPlanner horizon N\n0\n200\n400\n600\n800\n1000\nAcc. reward\n(b) walker stand\n101\n102\n103\nPlanner samples K\n101\n102\nAcc. reward\n(c) humanoid stand\nTDM (ours)\nTDM trained on\nwalk+run (ours)\nGround Truth Model\nMLP\nMLP Delta Outputs\nMLP Tokenized Outputs\nMLP Tokenized Inputs\nMLP Tokenized Inputs\n+ Delta Outputs\nMLP Tokenized Inputs\n+ Delta Outputs (70M)\nMLP PETS\nDreamer\nFigure 4 | Performance of TDMs and baseline models when trained on data from the environment\nthey are tested on. We observe that TDMs consistently outperform baselines. This finding is robust\nwhen switching the training distribution to a different task in the same environment (red lines for\nwalker and humanoid). We also compare with the ground truth models (black line). We evaluate\nthe models by doing MPC with a very basic random shooting planner. The planner uses \ud835\udc3e = 128\nsamples for cartpole, \ud835\udc3e = 64 samples for walker, and horizon \ud835\udc41 = 20 for humanoid. For very short\nplanner horizons \ud835\udc41, the planner is too myopic, and for very long horizons, the number of samples \ud835\udc3e is\ninsufficient for the random shooting planner to consistently discover a near-optimal action sequence.\nTherefore, when keeping \ud835\udc3e fixed, there is an intermediate sweet-spot planner horizon. We report\nmean values averaged over at least 4 episodes, shaded areas indicate 68% confidence intervals.\n9\nA Generalist Dynamics Model for Control\nalso include a version of the best-performing baseline that is much larger (70M parameters, \u201cMLP\nTokenized Inputs + Delta Outputs (70M)\u201d). In all environments, the performance of this larger model\nis very similar to the performance of its smaller version, and again, the TDM has an advantage for\nlonger planning horizons.\nFor cartpole and walker, the TDM performs on par with the expert ground-truth dynamics model.\nFor the 67-dimensional humanoid, the TDM does not reach expert performance. We use a random\nshooting planner for the experiments in Fig. 4, hence the TDM is queried with a state-action distribution\nthat is very different from its training data (which comes from an expert policy, see appendix A). This\ndistribution shift is challenging; while the TDM is still able to extrapolate perfectly in the cartpole and\nwalker domains, we hypothesize that the extremely high dimensionality of humanoid makes it likely\nthat the TDM is queried in areas of the state-action space that are simply not covered by its training\ndata, making extrapolation almost impossible. Having said that, the TDM is the only model with\nbetter-than-random performance in the humanoid domain, and we see a clear trend of increasing\nreward as we increase the number of samples \ud835\udc3e. This indicates that the TDM\u2019s performance might\nincrease further if the distribution shift is decreased. We therefore discuss a planning approach using\nsamples that are closer to the TDM\u2019s training distribution in the following.\n5.1.1. Including a proposal policy\nWe can make the planner use its budget of imaginary samples \ud835\udc3e more efficiently by biasing the\ncandidate action trajectories using a proposal policy, as described in section 4.1. Results for this\nare reported, for humanoid stand, in Fig. 5. As the proposal policy, we use the same transformer\n25\n50\nPlanner samples K\n0\n500\nAcc. reward\n\u03c3=0.1\n25\n50\nPlanner samples K\n\u03c3=0.5\n25\n50\nPlanner samples K\n\u03c3=1.0\n25\n50\nPlanner samples K\n\u03c3=2.0\nHorizon N=10\nHorizon N=20\nPure Proposal\nFigure 5 | Using the TDM for MPC with a proposal policy for humanoid stand. The subfigures\ncorrespond to different levels of additive noise \ud835\udf0e. Best results are obtained for moderate additive\nnoise (this ensures that the bias of the proposal policy is not washed out) and larger horizons \ud835\udc41\n(this ensures that the planner does not become too myopic). The resulting MPC agent both works\nbetter than the pure proposal policy (red line), and needs less imaginary samples \ud835\udc3e than the random\nshooting planner (see Fig. 4c). We report mean values averaged over at least 4 episodes, shaded areas\nindicate 68% confidence intervals.\nsequence model with the same weights that we also use as a TDM, but condition it as a BC policy at\ntest time, as described in section 3.1. The pure proposal policy is far from perfect, but useful as a bias.\nAdding not-too-high amounts of additive noise to obtain candidate action sequences, and using a\nplanning horizon \ud835\udc41 that is not too myopic, the TDM can significantly improve on the proposal. The\nmodel is able to consistently distinguish worse from better actions in the proposal-biased distribution;\nin fact the biased planner\u2019s performance approaches the asymptotic (\ud835\udc3e \u2192 \u221e) expert model\u2019s perfor-\nmance (Fig. 4c). In this example, the hybrid approach of using the transformer sequence model both\nas a TDM and a BC policy outperforms each of these alone.\n10\nA Generalist Dynamics Model for Control\n5.1.2. Robustness against changes in training distribution\nAs mentioned in section 4.4, the distribution of the training data we use is strongly biased to expert\nperformance, which, perhaps counterintuitively, is a challenging setup for learning a model that is\nthen used for random shooting MPC. For walker stand and humanoid stand, we also tested the\nTDM\u2019s performance after being trained on different distributions - namely expert data for walker\nwalk and run, and humanoid walk and run, respectively. As can be seen from Fig. 4b and Fig.\n4c, the TDM\u2019s performance is largely unchanged by this. This is additional evidence that TDMs are\nrelatively robust against suboptimal training distributions. Although not the focus of this work, to a\ncertain extent this can also be seen as an example of the TDM generalizing across tasks (from walk\nand run to stand), but in the same environment.\nThe fact that our model outperforms the baselines considered in this chapter does not rule out the\npossibility that similar or even better performance is achievable with other architectures, including\nMLPs of different sizes and depths. Our experiments show however that TDMs make accurate\npredictions that are suitable for planning for a range of difficult control tasks, in nontrivial learning\nsettings that were very challenging for the baselines considered here.\n5.2. TDMs generalize to unseen environments\nNext, we evaluate the quality of TDMs when trained on data from environments different from the\none they are tested on. We do this in two different settings: We first show results of a generalist\nmodel that is pre-trained on a small number of unrelated control environments, and then fine-tuned\non the unseen target environment (cartpole), in section 5.2.1. We then report results for using a\ngeneralist model in zero-shot fashion in unseen environments in the procedural walker universe\nin section 5.2.2. For a discussion of our choice of environments, please refer to section 4.3.\n5.2.1. Few-shot generalization\nWe use a TDM as a generalist dynamics model. The experimental setup is shown schematically in\nFig. 6a. We pre-train the model on 28 environments from the DeepMind control suite, another 28\nrandomized versions of the same environments, and 4 \u00b7 6 = 24 randomly created environments\nfrom the 4 families of the procedural walker universe described in section 4.3.2. None of these\nenvironments have any notable similarities with cartpole, our target environment. We then fine-\ntune this model on different amounts of transition data from cartpole, and test the resulting model\nby using it for MPC with a simple random shooting planner, as in section 5.1. This experiment\nis prototypical of a situation where the space of environments in which the model is supposed to\ngeneralize is only very sparsely covered by a relatively small number of pre-training environments.\nTherefore, we unsurprisingly observe no zero-shot generalization, but we do observe significant\nfew-shot generalization.\nWe vary the size \ud835\udc40 of the fine-tuning data sets. For each \ud835\udc40, we fine-tune 3 models on small data\nsets independently sampled from the full data set used in section 5.1. For each \ud835\udc40, we then optimize\nthe number of fine-tuning steps independently. These fine-tuning curves are shown in Fig. 6c. For\neach \ud835\udc40, the average MPC performance is recorded after independently optimizing the number of\ntraining steps. These optimized returns are indicated as stars in Fig. 6c, and are shown as a function\nof \ud835\udc40 in Fig. 6b. The optimized returns reflect the TDM\u2019s performance as a function of the number\nof fine-tuning samples, rather than of the number of fine-tuning steps. Comparing the performance\nof the fine-tuned generalist TDM to a TDM trained on the same small sets of data from scratch, we\nobserve a significant few-shot generalization effect: We can obtain a similarly capable model with\nroughly 2 to 3 orders of magnitude less data. As we increase the number of fine-tuning data, we\n11\nA Generalist Dynamics Model for Control\n(a)\nZero-shot\n0\n100\n200\n300\n400\n500\n600\n700\nAccumulated reward\n100\n101\n102\n103\n104\n# Episodes collected for \ufb01netuning\nTDM from scratch\nTDM pretrained\nTDM pretrained (incl. double + triple\ncartpole)\nMLP Tokenized Input + Delta Output\nfrom scratch\nExpert TDM trained on 26762 ep.\nGround truth model\n(b)\n103\n104\n105\n0\n200\n400\n600\n800\nAccumulated reward\nNo pretraining\n1 ep. for \ufb01netuning\n10 ep. for \ufb01netuning\n100 ep. for \ufb01netuning\n1000 ep. for \ufb01netuning\n10000 ep. for \ufb01netuning\n103\n104\n105\nFinetuning steps\n...550k steps\n103\n104\n105\n...550k steps\nincl. double + triple cartpole\nPretraining for...\n(c)\nFigure 6 | Few-shot generalization of TDMs. The results show that the TDM\u2019s generalization improves\nsample efficiency over low-expressivity baselines by almost 2 orders of magnitude, and by 2 to 3 orders\nof magnitude over the from-scratch TDM. (a) We train a generalist model on ca. 100 environments\nthat are unrelated to cartpole, fine-tune it with small amounts of data on cartpole, and test the\nresulting TDM on cartpole. (b) Model performances as a function of pre-training strategy and amount\nof data used for fine-tuning. There is a significant generalization effect, which further increases if\nwe include double and triple cartpole data in our pre-training. Furthermore, in the medium data\nrange, the fine-tuned TDM outperforms the best-performing MLP baseline. (c) Fine-tuning curves as a\nfunction of fine-tuning data and the pre-trained generalist model used. For each fine-tuning run, the\nbest result is selected and shown in (b). Each episode contains 1000 environment steps. The planner\nuses \ud835\udc3e = 128 samples and horizon \ud835\udc41 = 100. We report mean values averaged over 3 independent\nfine-tuning runs and at least 4 rollout episodes each, shaded areas indicate 68% confidence intervals.\n12\nA Generalist Dynamics Model for Control\napproach the specialist model\u2019s (and ground truth\u2019s) performance reported in section 5.1.\nComparing to a different pre-training set.\nAs mentioned earlier, the pre-training data set only\ncontains data from environments that are entirely different from cartpole. We also tested including\ndata from double cartpole and triple cartpole in the pre-training data. These environments\nare still quite different from cartpole (more degrees of freedom, different kinematics), but are\narguably more related to cartpole than the environments originally in our pre-training set. They\ncan be considered to be closer to our target environment in the space of environments, potentially\nallowing the generalist model to few-shot-interpolate easier to the target environment. Indeed, after\nincluding double cartpole and triple cartpole in the pre-training set, the results improve\nsignificantly over the original setting (see Fig. 6b).\nComparing to baselines - the data efficiency perspective.\nWe also compare the generalization\nresults with the best-performing MLP specialist from section 5.1. This baseline is trained from scratch\non the same data that was used for fine-tuning the TDM; again we use 3 independent data sets and\ntraining runs each. The MLP baseline (ca. 400k parameters, the TDM has ca. 77M) works better\nfor very small amounts of data (10 episodes), but after that, the pre-trained generalist TDMs have a\ngrowing advantage. In other words, given moderate amounts of data (ca. 100 to 1000 episodes),\nthe fine-tuned generalist is the best model we were able to train in all of our experiments, including\nlow-expressivity baselines. In this regime, the generalist TDM needs almost 2 orders of magnitude\nless data to achieve the same performance. This is not because the generalist TDM has an inherently\nbetter sample efficiency (compare the from-scratch TDM to the MLP baseline), but rather it more than\ncompensates its initially lower sample efficiency by exploiting its capability to generalize from other\nenvironments.\nThe generalist TDM does not reach expert performance in the regime of 100 to 1000 episodes,\nbut its much higher sample efficiency here has important practical utility, for example to warm-start\nexploration of a specialist agent.\n5.2.2. Zero-shot generalization\nWe use a TDM as a generalist model again, but now investigate its zero-shot generalization capabilities\nto an unseen environment. As motivated in section 4.3.2, we use the procedural walker universe\nfor this, allowing for reasonable coverage of the space of environments the model is supposed to\ngeneralize in. We train the model on either 1000 or 10000 randomly created morphologies from\nthe chain family. These morphologies are very diverse in their degrees of freedom (between 4 and\n20) and kinematic trees (see section 4.3.2). We then test the generalist TDM\u2019s performance on 10\nmorphologies never seen during training. The results are summarized in Fig. 7. The TDM zero-shot\ngeneralizes very well to unseen morphologies, especially for the larger model size tested.\nIn contrast to this, we measure no significant generalization effect when the same sequence model\nwith the same weights is not used as TDM within an MPC loop with a random shooting planner, but\nas a BC policy. While the TDM achieves roughly half of the optimal return, using the same model as\nBC policy does not generalize significantly. If we train the same transformer model as a specialist\nBC policy on data from a single procedural walker environment, we achieve ca. 80% of the\noptimal score on average. This rules out insufficient model capacity or poor data quality as reason for\nthe low performance of the BC policy in Fig. 7; this can indeed be ascribed to weak generalization.\nThe transformer model is trained with expert data, providing high-quality data to the BC policy.\nAdditionally, at the start of each episode, we prompt the BC policy with a history of optimal behavior,\n13\nA Generalist Dynamics Model for Control\n77M params\n1k morphs\n77M params\n10k morphs\n362M params\n10k morphs\n0\n200\n400\n600\n800\nAccumulated Reward\nBC\nBC\nBC\nMPC\nMPC\nMPC\nAveraged across Morphologies\nOpt. score: 1676\nOpt. score: 1674\nOpt. score: 1633\nOpt. score: 1672\nOpt. score: 1595\n0\n1000\nAcc. Reward\nOpt. score: 1656\nOpt. score: 1656\nOpt. score: 1232\nOpt. score: 1651\nOpt. score: 1675\n0\n1000\nAcc. Reward\nFigure 7 | Zero-shot generalization of TDMs. The left side shows results averaged over all 10 held-out\ntest morphologies, the right side shows individual results. We pre-train models of different size on\nexpert data from either 1000 or 10000 different morphologies in the chain family of the procedural\nwalker universe. We then test the resulting generalist sequence model\u2019s performance as TDM in an\nMPC loop with a random shooting planner (as described in section 4.1). We compare this with using\nthe same sequence model as a BC policy (see section 3.1). We find that using the sequence model as\nTDM generalizes substantially better than using the same model as a BC policy. This is especially\ntrue for the larger TDM with 362M parameters, which reaches roughly half of the maximum possible\nperformance on average. Note that, at the start of each episode, we prompt the BC policy with a\nhistory of optimal behavior, providing it with privileged information. This is in contrast to the TDM,\nwhich we don\u2019t warm-start with any history. We report mean values averaged over at least 4 episodes,\nblack bars indicate 68% confidence intervals.\nproviding it with privileged information. In contrast, we don\u2019t warm-start the TDM with any history.\nIn this example, the TDM together with a planner that optimizes behavior generalizes better than\nthe BC policy that directly models optimal behavior. We speculate that there are at least two effects at\nplay: First, we observed that optimal behavior in the procedural walker universe can look very\ndifferent depending on the morphology; while for some, a centipede-like walking motion is optimal,\nfor others it is better to roll. This means that identifying the dynamics from interaction (which is what\nthe dynamics model (TDM) has to learn) might be an easier task than identifying optimal behavior,\nor at least continuing a prompt of optimal behavior, from interaction (which is what the behavior\nmodel (BC policy) has to learn). Second, given an imperfect generalist sequence model, querying it\nrepeatedly with random actions in an MPC loop might be more forgiving than directly querying it\nfor actions. The random actions create additional randomness in the behavior creating process that\nmakes it less likely for the model to \u201cget stuck\u201d making wrong predictions.\nStrong generalization is achieved in this experiment by using the generalist sequence model not\n(or not only) as policy, but as a TDM. This across-environment generalization is in addition to the\n\u201cclassic\u201d across-task generalization of dynamics models (see also section 5.1.2).\n6. Discussion\nPixel observations: In this paper, we restrict our experiments to environments with state-based\nobservations and did not consider pixel-based observations. Apart from reducing need for computa-\ntional resources, this was done in order to isolate generalization effects due to a transfer of a basic\n14\nA Generalist Dynamics Model for Control\nunderstanding of physics from generalization effects due to a transfer of perceptual capabilities. That\nbeing said, pixel-based domains are an interesting and natural extension of our work for at least two\nreasons: First, pixel-based observations open up our approach to more data sources, especially for\nreal-world environments. Second, images can contain richer context about the environment than\nstates, allowing for faster system identification for generalization. Fortunately, there are established\ntechniques to tokenize image inputs for transformers, such as ViT (Dosovitskiy et al., 2020) or VQGAN\n(Esser et al., 2021). Some of these approaches were already used with the Gato architecture we base\nthis work on. Furthermore, pixel-based domains require planning with predicted rewards, and initial\nexperiments in appendix D (and also the results in section 5.2.2) indicate that our approach performs\nwell in these cases. We therefore believe that including pixel observations is a straightforward and\nnatural extension of our work.\nSimple planner: As discussed earlier, the random shooting planner we used for MPC is a tool for\ncomparing model quality. As such, it is intentionally simple. A planner optimized for performance\nlikely could significantly improve the MPC reward. We discussed one such example in Fig. 5, where\nwe used a proposal for planning.\nTraining data: We use expert data (see also section A) for training the dynamics models in this\nwork. As argued in section 4.4, this is challenging for the models: We train on expert data, but for\nrandom shooting MPC then query the models with state-action sequences distributed very differently.\nIn high-dimensional state-action spaces, this distribution shift makes it likely that the TDM will\nbe queried in parts of the space for which it never \u201csaw\u201d any data. Consistent with this, the TDM\nreaches expert level for cartpole and walker with random shooting, but not for the 67-dimensional\nhumanoid (Fig. 4). After biasing the distribution with a proposal however, the TDM approaches expert\nperformance on humanoid too (Fig. 5). This distribution shift could also explain why the Dreamer\ndynamics model performed well for policy improvement in Hafner et al. (2019), but not in Fig. 4.\nUtility of imperfect generalists: As is typical for generalization settings, the generalist TDM\u2019s\npredictions in the target environment are not perfect (see section 5.2). While imperfect, it is still\nsignificantly more informative than a non-generalizing model. As such, it can be used as a bias to\ninform downstream learning algorithms, for example, to inform the exploration strategy of an RL\nagent in the target environment.\nLimits of generalization: Since the model has to interpolate in the space of environments in order to\ngeneralize, the pre-training data has to sample this space to some extent (section 5.2.2). For sparse\nsampling, fine-tuning might still be successful (section 5.2.1), but with sparse sampling and relatively\ncomplex targets, the generalization effect expectedly vanishes, as shown in section E.\nModel-free vs. model-based: This paper does not weigh in on whether model-based or model-free\nmethods (or combinations, see section 5.1.1) are superior in every situation. Indeed, model-free MPO\nreaches expert performance on cartpole after roughly 200 episodes (Abdolmaleki et al., 2018), which\nis faster than our most efficient cartpole dynamics model, the fine-tuned TDM generalist, reaches\nexpert model performance (see Fig. 6b). Instead, we demonstrate that generalization is a powerful\nmechanism to speed up dynamics model learning (section 5.2.1), and we show that in some cases,\nmodel-based generalization does in fact outperform model-free generalization (section 5.2.2).\nInference speed: Our current approach is limited by test-time inference speed. The 77M model can\npredict roughly 500 tokens per second on a single Jellyfish TPU core, which, depending on the degree\nof parallelization, the dimensionality of the environment, the planner horizon \ud835\udc3b, and the number\nof planner samples \ud835\udc3e, can translate into environment step durations of tens of seconds in extreme\ncases. Apart from increasing parallelization (down to one planner sample per core), this can likely be\noptimized significantly by using more sample-efficient planning algorithms than random shooting, an\n15\nA Generalist Dynamics Model for Control\nexample of which was discussed in section 5.1.1. The TDM itself can also be optimized for speed; a\nstraightforward starting point is the context window size (inference time scales quadratically with\nwindow size), which can be reduced significantly without losing much of the performance, as shown\nin section C. Finally, while the present work uses transformers as an example showing that generalist\ndynamics models exist at all, future research potentially will uncover completely different model\narchitectures with similar generalization capabilities but faster inference speed. On that note, we\nbriefly discuss the possibility that tokenization could benefit non-transformer models in section F.\nHaving said all that, fundamentally we propose to use transformer sequence models as large,\nexpressive, generalist dynamics models that are not primarily optimized for speed. A more principled\nway to resolve this trade-off between expressiveness and speed could be distillation: Large general\nfoundation models could be expressive and slow, but would then be distilled into light-weight specialists\nfor specific tasks. This could be done at several points along the execution pipeline: The TDM could\nbe distilled into a dynamics model, or the MPC agent could be distilled into a policy.\n7. Conclusion\nWe investigate using transformers as dynamics models (TDMs). We demonstrate two aspects of TDMs\nin the experiments: First, TDMs are generalist dynamics models, i.e., they generalize well to unseen\nenvironments, which we demonstrated both in the few-shot and in the zero-shot case. Second, TDMs\nare capable specialist models, i.e., they are precise when learning from environment-specific data.\nWe believe that these properties make TDMs a promising ingredient for a foundation model of\nrobotics and control. As argued earlier, while we mostly focus on TDMs in this paper, using transformers\nas dynamics models or policies is not mutually exclusive. A combination, like planning with proposals,\nmight be the most efficient way to make use of the detailed and generalizable knowledge aggregated\nby a transformer that models the joint distribution of observations, actions, and rewards.\nAcknowledgments\nWe would like to thank Abbas Abdolmaleki, Philemon Brakel, Oliver Groth, Tuomas Haarnoja, Ben\nMoran, Francesco Nori, Scott Reed, and Dhruva Tirumala for insightful discussions and feedback.\nReferences\nA. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maximum a\nposteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.\nO. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plap-\npert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The International Journal\nof Robotics Research, 39(1):3\u201320, 2020.\nK. J. \u00c5str\u00f6m and B. Wittenmark. Problems of identification and control. Journal of Mathematical\nanalysis and applications, 34(1):90\u2013113, 1971.\nP. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti,\nD. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph\nnetworks. arXiv preprint arXiv:1806.01261, 2018.\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.\n16\nA Generalist Dynamics Model for Control\nC. Blake, V. Kurin, M. Igl, and S. Whiteson. Snowflake: Scaling gnns to high-dimensional continuous\ncontrol via parameter freezing. Advances in Neural Information Processing Systems, 34:23983\u201323992,\n2021.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint\narXiv:2212.06817, 2022.\nA. Byravan, L. Hasenclever, P. Trochim, M. Mirza, A. D. Ialongo, Y. Tassa, J. T. Springenberg, A. Ab-\ndolmaleki, N. Heess, J. Merel, et al. Evaluating model-based planning and planner amortization for\ncontinuous control. arXiv preprint arXiv:2110.03363, 2021.\nL. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. De-\ncision transformer: Reinforcement learning via sequence modeling. Advances in neural information\nprocessing systems, 34:15084\u201315097, 2021.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\nC. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\nK. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of trials\nusing probabilistic dynamics models. Advances in neural information processing systems, 31, 2018.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\nD. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\nT. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman,\nM. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence. Palm-e: An embodied multimodal\nlanguage model. In arXiv preprint arXiv:2303.03378, 2023.\nO. Eberhard, J. Hollenstein, C. Pinneri, and G. Martius. Pink noise is all you need: Colored noise\nexploration in deep reinforcement learning. In The Eleventh International Conference on Learning\nRepresentations, 2022.\nP. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u2013\n12883, 2021.\nH. Furuta, Y. Iwasawa, Y. Matsuo, and S. S. Gu. A system for morphology-task generalization via\nunified representation and behavior distillation. arXiv preprint arXiv:2211.14296, 2022.\nC. E. Garcia, D. M. Prett, and M. Morari. Model predictive control: Theory and practice\u2014a survey.\nAutomatica, 25(3):335\u2013348, 1989.\nC. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous\nlatent space models for representation learning. In International Conference on Machine Learning,\npages 2170\u20132179. PMLR, 2019.\nA. Gupta, L. Fan, S. Ganguli, and L. Fei-Fei. Metamorph: Learning universal controllers with trans-\nformers. arXiv preprint arXiv:2203.11931, 2022.\nD. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural\ninformation processing systems, 31, 2018a.\n17\nA Generalist Dynamics Model for Control\nD. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018b.\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep\nreinforcement learning with a stochastic actor. In International Conference on Machine Learning,\npages 1861\u20131870. PMLR, 2018.\nT. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, M. Wulfmeier, J. Humplik, S. Tunyasu-\nvunakool, N. Y. Siegel, R. Hafner, et al. Learning agile soccer skills for a bipedal robot with deep\nreinforcement learning. arXiv preprint arXiv:2304.13653, 2023.\nD. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent\nimagination. arXiv preprint arXiv:1912.01603, 2019.\nD. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv\npreprint arXiv:2010.02193, 2020.\nN. Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control policies\nby stochastic value gradients. Advances in neural information processing systems, 28, 2015.\nW. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular policies for\nagent-agnostic control. In International Conference on Machine Learning, pages 4455\u20134464. PMLR,\n2020.\nW. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence, I. Mordatch, S. Levine, K. Hausman,\net al. Grounded decoding: Guiding text generation with grounded models for robot control. arXiv\npreprint arXiv:2303.00855, 2023.\nM. Janner, Q. Li, and S. Levine. Offline reinforcement learning as one big sequence modeling problem.\nAdvances in neural information processing systems, 34:1273\u20131286, 2021.\nZ. Jiang, T. Zhang, M. Janner, Y. Li, T. Rockt\u00e4schel, E. Grefenstette, and Y. Tian. Efficient planning in\na compact latent action space. arXiv preprint arXiv:2208.10291, 2022.\nL. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn,\nP. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint\narXiv:1903.00374, 2019.\nV. Kurin, M. Igl, T. Rockt\u00e4schel, W. Boehmer, and S. Whiteson. My body is a cage: the role of\nmorphology in graph-based incompatible control. arXiv preprint arXiv:2010.01856, 2020.\nS. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The\nJournal of Machine Learning Research, 17(1):1334\u20131373, 2016.\nL. Ljung. System Identification: Theory for the User. Prentice Hall information and system sciences\nseries. Prentice Hall PTR, 1999. ISBN 9780136566953.\nM. Lutter, L. Hasenclever, A. Byravan, G. Dulac-Arnold, P. Trochim, N. Heess, J. Merel, and Y. Tassa.\nLearning dynamics models for model predictive agents. arXiv preprint arXiv:2109.14311, 2021.\nV. Micheli, E. Alonso, and F. Fleuret. Transformers are sample efficient world models. Proceedings of\nthe International Conference on Learning Representations, 2023.\nT. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker, et al. Model-based reinforcement learning: A\nsurvey. Foundations and Trends\u00ae in Machine Learning, 16(1):1\u2013118, 2023.\nOpenAI. Gpt-4 technical report, 2023.\n18\nA Generalist Dynamics Model for Control\nE. Parisotto, F. Song, J. Rae, R. Pascanu, C. Gulcehre, S. Jayakumar, M. Jaderberg, R. L. Kaufman,\nA. Clark, S. Noury, et al. Stabilizing transformers for reinforcement learning. In International\nconference on machine learning, pages 7487\u20137498. PMLR, 2020.\nS. Park and S. Levine. Predictable mdp abstraction for unsupervised model-based rl. arXiv preprint\narXiv:2302.03921, 2023.\nS. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky,\nJ. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nJ. Richalet, A. Rault, J. Testud, and J. Papon. Model predictive heuristic control: Applications to\nindustrial processes. Automatica, 14(5):413\u2013428, 1978.\nJ. Robine, M. H\u00f6ftmann, T. Uelwer, and S. Harmeling. Transformer-based world models are happy\nwith 100k interactions. Proceedings of the International Conference on Learning Representations,\n2023.\nA. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia.\nGraph networks as learnable physics engines for inference and control. In International Conference\non Machine Learning, pages 4470\u20134479. PMLR, 2018.\nF. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network\nmodel. IEEE transactions on neural networks, 20(1):61\u201380, 2008.\nJ. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,\nD. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned\nmodel. Nature, 588(7839):604\u2013609, 2020.\nM. Schwenzer, M. Ay, T. Bergs, and D. Abel. Review on model predictive control: An engineering\nperspective. The International Journal of Advanced Manufacturing Technology, 117(5-6):1327\u20131349,\n2021.\nY. Sun, S. Ma, R. Madaan, R. Bonatti, F. Huang, and A. Kapoor. Smart: Self-supervised multi-task\npretraining with control transformers. arXiv preprint arXiv:2301.09816, 2023.\nY. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel,\nA. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.\nS. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess,\nand Y. Tassa. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022,\n2020.\nP. M. Van Den Hof and R. J. Schrama. Identification and control\u2014closed-loop issues. Automatica, 31\n(12):1751\u20131770, 1995.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. Advances in neural information processing systems, 30, 2017.\nT. Wang, R. Liao, J. Ba, and S. Fidler. Nervenet: Learning structured policy with graph neural networks.\nIn Proceedings of the International Conference on Learning Representations, Vancouver, BC, Canada,\nvolume 30, 2018.\nM. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent\ndynamics model for control from raw images. Advances in neural information processing systems,\n28, 2015.\n19\nA Generalist Dynamics Model for Control\nS. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for decision\nmaking: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023.\nZ.-H. Yin, W. Ye, Q. Chen, and Y. Gao. Planning for sample efficient imitation learning. arXiv preprint\narXiv:2210.09598, 2022.\nJ. Zhang, J. T. Springenberg, A. Byravan, L. Hasenclever, A. Abdolmaleki, D. Rao, N. Heess, and\nM. Riedmiller. Leveraging jumpy models for planning and fast learning in robotic domains. arXiv\npreprint arXiv:2302.12617, 2023.\n20\nA Generalist Dynamics Model for Control\nAppendix\nA. Training data distribution for DeepMind control suite\nFor the experiments with specialist models (section 5.1), Fig. 8 shows the distribution of episode\nrewards in the data used. The data consists of mostly expert behavior. As discussed in section 5.1.2,\nFigure 8 | Distribution of episode rewards of the transition data used to train the models for cartpole,\nwalker, and humanoid in section 5.1.\nusing mostly expert data poses a challenge for learning a model that can accurately predict rollouts\nwith random actions, as required for our MPC agent. The expert training data follows a very different\ndistribution than the random actions at test time.\nB. Use of Brownian noise for random shooting MPC\nAs described in section 4.1, the candidate action sequences \ud835\udc34(\ud835\udc56) for MPC with random shooting are\nsampled from temporally correlated Brownian noise. As an interesting note on this, Eberhard et al.\n(2022) investigated time-correlated action noise for exploration in Deep RL with SAC (Haarnoja et al.,\n2018) and MPO (Abdolmaleki et al., 2018) in the DeepMind control suite. They find that pink noise\n(with a power spectral density proportional to \ud835\udc53 \u2212\ud835\udefd, where \ud835\udefd = 1, which is between uncorrelated\nwhite (\ud835\udefd = 0) and Brownian (\ud835\udefd = 2) noise) worked best. While we did not investigate the optimal\nvalue of \ud835\udefd in detail, we indeed found in preliminary experiments that MPC with uncorrelated noise\n(\ud835\udefd = 0) did not perform well. This is perhaps unsurprising in retrospect: Non-correlated noise makes\nit exponentially unlikely to obtain control inputs that consistently favor one direction over extended\nperiods of time, which is required for successful control in the DeepMind control suite. In other words,\ntime correlation is a general, but very beneficial prior when searching optimal action sequences for\nthe DeepMind control suite.\n21\nA Generalist Dynamics Model for Control\nC. Varied context window\nThe TDMs used throughout this work had a fixed context window length of 1023 tokens. For the\nwalker stand task, Fig. 9 contains MPC rewards when using the TDM with varied context window\nlength. We observe that the performance is only very slightly affected by decreasing the context\nwindow size, until the window size becomes so small that it contains less than a single step. This\nindicates that the TDM\u2019s performance does not predominantly rely on having a multi-step history as\ninput.\n0\n200\n400\n600\n800\n1000\nContext window length\n0\n200\n400\n600\n800\n1000\nAccumulated reward\nTDM\nLimit for pure Markov prediction\nFigure 9 | MPC performance of the specialist model for walker stand (see Fig. 4b) when using\ndifferent context window sizes. The red line indicates the number of tokens that are needed to encode\nthe previous observation and current action, i.e., the minimum context window needed in the strictly\nfirst-order Markov case. The results show that the model benefits from using additional context to\nsome extent, but the difference is small compared to the difference to baseline models reported in\nFig. 4b. The planner uses \ud835\udc3e = 64 samples and horizon \ud835\udc41 = 25. We report mean values averaged over\nat least 4 episodes, shaded areas indicate 68% confidence intervals.\nD. Performance of planning with predicted rewards\nAs discussed at the end of section 4.1, for most of the experiments in this work, the reward used for the\nobjective function \ud835\udc53 was computed from predicted future observations \ud835\udc5c as \ud835\udc53 (\ud835\udc5c\ud835\udc61+1, ..., \ud835\udc5c\ud835\udc61+\ud835\udc41, \ud835\udc4e\ud835\udc61, ..., \ud835\udc4e\ud835\udc61+\ud835\udc41\u22121) =\n\u00cd\ud835\udc41\n\ud835\udc58=1 \ud835\udc45(\ud835\udc5c\ud835\udc61+\ud835\udc58). For the procedural walker experiments in section 5.2.2 however, the objective function\n\ud835\udc53 (\ud835\udc5f\ud835\udc61, ..., \ud835\udc5f\ud835\udc61+\ud835\udc41\u22121, \ud835\udc5c\ud835\udc61+1, ..., \ud835\udc5c\ud835\udc61+\ud835\udc41, \ud835\udc4e\ud835\udc61, ..., \ud835\udc4e\ud835\udc61+\ud835\udc41\u22121) = \u00cd\ud835\udc41\n\ud835\udc58=1 \ud835\udc5f\ud835\udc61+\ud835\udc58 is calculated using future rewards \ud835\udc5f predicted\nby the TDM directly.\nFor cartpole swingup, Fig. 10 compares these two approaches in terms of the reward of the MPC\nagent. The performance of directly planning with predicted rewards is only marginally worse. Note\nthat these results were obtained with a smaller TDM architecture than the one we reported results\nfor in Fig. 4a, hence there are small deviations from the values shown there.\nAs also discussed in section 6, using the TDM in domains with pixel observations requires planning\nwith predicted rewards. This is because calculating the reward from pixel observations is usually\nimpossible or at least infeasible. The results in Fig. 10 show that, at least for cartpole, switching to\npredicted rewards does not result in a large loss in performance. Together with the points discussed\nin section 6, this encourages us to hypothesize that an extension of TDMs to pixel-based domains\nmight be straightforward. We believe that an experimental investigation of this would be a natural\navenue for future work.\n22\nA Generalist Dynamics Model for Control\n20\n40\n60\n80\n100\nPlanner horizon\n100\n200\n300\n400\n500\n600\nAccumulated reward\nTDM (ours) with Reward from Observation\nTDM (ours) with Predicted Reward\nFigure 10 | Comparison of planning with rewards predicted by the TDM versus planning with rewards\ncalculated from the observations predicted by the TDM for cartpole. The performance of directly\nplanning with predicted rewards is only marginally worse. These results are for a smaller TDM\narchitecture than the one we reported results for in Fig. 4a, hence there are small deviations from the\nvalues shown there. Shaded areas indicate 68% confidence intervals.\nE. Example for unsuccessful generalization\nIn Fig. 11, we report an example where we did not observe a significant generalization effect. Fig. 1\nputs this experiment in context with the other generalization experiments reported in sections 5.2.1\nand 5.2.2. We hypothesize that the combination of the sparse pre-training coverage of the space\nof environments (80 sample environments from the arguably huge space of environments that is\ncovered by the DeepMind control suite), and the small amount of fine-tuning data (\u201csmall\u201d in relation\nto the relative complexity of the walker target environment), makes it impossible for the model to\ngeneralize.\nSince cartpole is a simpler environment, the same amount of fine-tuning data provides a slightly\nbetter coverage of the target, and we observe a strong generalization effect with the very same\npre-training coverage (section 5.2.1). Note though that without using generalization effects, the\nproblem would still be infeasible.\nFinally, for the procedural walker results reported in section 5.2.2, the target environments\nare arguably of similar complexity as the walker environment reported here, but the coverage of the\nspace of environments in the pre-training data is better (10000 sample environments from the more\nstructured space of environments that is covered by the procedural walker chain universe).\nF. Tokenization and MLPs\nWe zoom in on some of the results for the MLP baselines reported in Fig. 4a in order to discuss the\neffect of tokenization. Fig. 12 shows the effect of adding tokenization of inputs or outputs of an\notherwise unchanged MLP. We find that using embedded tokens as inputs for an otherwise unchanged\nstandard MLP increases its performance as a dynamics model, while changing the MLP to predict a\ncategorical probability distribution over tokens decreases its performance. While it is not the purpose\nof the present work to identify successful design choices that might translate from transformers to\nother architectures as well, using tokenized input is one example of this that works well in the present\n23\nA Generalist Dynamics Model for Control\n(a)\nZero-shot\n0\n200\n400\n600\n800\n1000\nAccumulated reward\n100\n101\n102\n103\n104\n# Episodes collected for \ufb01netuning\nTDM from scratch\nTDM pretrained\nExpert TDM trained on 18503 ep.\n(b)\n0\n50\n100\n150\n200\n250\nFinetuning steps (in 1000)\n0\n200\n400\n600\n800\n1000\nMean accumulated reward\nNo pretraining\n1 episodes collected for \ufb01netuning\n10 episodes collected for \ufb01netuning\n100 episodes collected for \ufb01netuning\n1000 episodes collected for \ufb01netuning\n10000 episodes collected for \ufb01netuning\n0\n50\n100\n150\n200\n250\nFinetuning steps (in 1000)\nPretraining for 1M steps\n(c)\nFigure 11 | Example for unsuccessful few-shot generalization of TDMs. (a) We train a generalist\nmodel on ca. 100 environments that are unrelated to walker, fine-tune it with small amounts of\ndata on walker, and then test the resulting TDM on walker. (b) Model performances as a function of\npre-training strategy and amount of data used for fine-tuning. There is no significant generalization\neffect. (c) Fine-tuning curves as a function of fine-tuning data and the pre-trained generalist model\nused. For each fine-tuning run, the best result is selected and shown in (b). Each episode contains\n1000 environment steps. We report mean values averaged over 3 independent fine-tuning runs and\nat least 4 rollout episodes each, shaded areas indicate 68% confidence intervals.\n24\nA Generalist Dynamics Model for Control\n0\n100\n200\n300\n400\n500\nPlanner horizon N\n100\n200\n300\n400\n500\n600\n700\nAcc. reward\nMLP Delta Outputs\nMLP Tokenized Outputs\nMLP Tokenized Inputs\n+ Delta Outputs\nFigure 12 | Using embedded tokens as inputs for an otherwise unchanged standard MLP increases its\nperformance as a dynamics model. Changing the MLP to predict a categorical probability distribution\nover tokens decreases its performance. The planner uses \ud835\udc3e = 128 samples for cartpole, \ud835\udc3e = 64\nsamples for walker, and horizon \ud835\udc41 = 20 for humanoid. We report mean values averaged over at least\n4 episodes, shaded areas indicate 68% confidence intervals.\ncase. This might be an interesting starting point for future investigations.\nG. Prediction errors\nFor the single-environment models, we reported the performance of the resulting MPC agent in Fig.\n4. Additionally, Fig. 13 shows prediction errors for the models used in Fig. 4. Across all environments,\nthe TDM is significantly more accurate than the baselines. This difference is especially pronounced\nfor longer horizons \ud835\udc41. For the more complex environments walker stand and humanoid stand,\nthe baseline\u2019s prediction accuracy for horizons that would be sufficient for effective planning (\ud835\udc41 > 20)\nis too low to accurately distinguish good from bad action sequences, resulting in the poor MPC\nperformance observed in the results shown in Fig. 4.\n25\nA Generalist Dynamics Model for Control\n100\n101\n102\nHorizon N\n10\u22123\n10\u22122\n10\u22121\n100\nMedian RMS Prediction Error\ncartpole swingup\n100\n101\n102\nHorizon N\n10\u22122\n10\u22121\n100\nwalker stand\nTDM (ours)\nMLP\nMLP Tokenized Outputs\nMLP Delta Outputs\nDreamer\nMLP PETS\nMLP Tokenized Inputs\nMLP Tokenized Inputs\n+ Delta Outputs\n100\n101\n102\nHorizon N\n10\u22122\n10\u22121\n100\nhumanoid stand\nFigure 13 | Prediction accuracies (RMS error leaving out velocities) for the models used in Fig.\n4. Across all environments, the TDM is significantly more accurate than the baselines, especially\nfor longer horizons \ud835\udc41. The lines show median values taken over 30 runs. Runs are collected by\nrandomizing the initial state, and then executing random actions from the same distribution that the\nrandom shooting planner uses as well. For walker stand and humanoid stand, the baseline\u2019s\nprediction accuracy for horizons that are sufficient for effective planning is too low to accurately\ndistinguish good from bad action sequences, resulting in the poor MPC performance observed in the\nresults shown in Fig. 4.\n26\n"
  },
  {
    "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
    "link": "https://arxiv.org/pdf/2305.11175.pdf",
    "upvote": "1",
    "text": "VisionLLM: Large Language Model is also\nan Open-Ended Decoder for Vision-Centric Tasks\nWenhai Wang\u22171, Zhe Chen\u22172,1, Xiaokang Chen\u22173,1, Jiannan Wu\u22174,1, Xizhou Zhu5,1\nGang Zeng3, Ping Luo4,1, Tong Lu2, Jie Zhou6, Yu Qiao1, Jifeng Dai\u20206,1\n1OpenGVLab, Shanghai AI Laboratory\n2Nanjing University\n3Peking University\n4The University of HongKong\n5SenseTime Research\n6Tsinghua University\nCode: https://github.com/OpenGVLab/VisionLLM\nDemo: https://github.com/OpenGVLab/InternGPT\nAbstract\nLarge language models (LLMs) have notably accelerated progress towards artificial\ngeneral intelligence (AGI), with their impressive zero-shot capacity for user-tailored\ntasks, endowing them with immense potential across a range of applications.\nHowever, in the field of computer vision, despite the availability of numerous\npowerful vision foundation models (VFMs), they are still restricted to tasks in a\npre-defined form, struggling to match the open-ended task capabilities of LLMs.\nIn this work, we present an LLM-based framework for vision-centric tasks, termed\nVisionLLM. This framework provides a unified perspective for vision and language\ntasks by treating images as a foreign language and aligning vision-centric tasks\nwith language tasks that can be flexibly defined and managed using language\ninstructions. An LLM-based decoder can then make appropriate predictions based\non these instructions for open-ended tasks. Extensive experiments show that the\nproposed VisionLLM can achieve different levels of task customization through\nlanguage instructions, from fine-grained object-level to coarse-grained task-level\ncustomization, all with good results. It\u2019s noteworthy that, with a generalist LLM-\nbased framework, our model can achieve over 60% mAP on COCO, on par with\ndetection-specific models. We hope this model can set a new baseline for generalist\nvision and language models. The code and demo shall be released.\n1\nIntroduction\nThe emergence of large language models (LLMs) like ChatGPT [41] has revolutionized the landscape\nof artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in ad-\ndressing various natural language processing (NLP) tasks through user-tailored prompts or language\ninstructions. Despite these advancements, it\u2019s essential to note that the triumph of LLMs does not\neffortlessly extend to pure vision and vision-language tasks, due to the inherent disparities between\nmodalities and task formats.\nThe field of computer vision presents a unique set of challenges and paradigms that differ from\nthose of NLP. The traditional paradigm of vision foundation models is pre-training followed by\nfine-tuning [59, 12, 51, 61, 18, 52], which is effective but comes with significant marginal costs\nwhen adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such as\nmulti-task unification [44, 58, 1, 57, 81] have been used to achieve generalist capability, they often\nstruggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended\n\u2217Equal contribution. This work is done when Zhe Chen, Xiaokang Chen, and Jiannan Wu are interns at\nShanghai AI Laboratory. \u2020 Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\narXiv:2305.11175v2  [cs.CV]  25 May 2023\nVision\nGeneralist Model\nPre-defined tasks:\ndetection, captioning, \nVQA, grounding, ...\n(a) Vision generalist models [59,\n61, 83] are constrained by the for-\nmat of pre-defined tasks.\nVisual\nPrompt Tuning\n(b) Visual prompt tuning [26, 64,\n62] are inconsistent with the for-\nmat of LLMs.\nVision + LLM\nTask de\ufb01ned \nby  instruc6ons\nDesired output:\n<c1> <p1> <p3> ...\n(c) VisionLLM (ours) can flexibly\nmanage vision-centric tasks using\nlanguage instructions like LLMs.\nFigure 1: Comparison of our VisionLLM with popular paradigms. Unlike current vision generalist\nmodels that depend on pre-defined task formats and visual prompt tuning models that are inconsistent\nwith large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended vision\ntasks by using language instructions.\ntask capabilities compared to LLMs. Recently, visual prompt tuning [26, 74, 79, 76, 62] has emerged\nas a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instance\nsegmentation, and pose estimation, using visual masking. However, the format of visual prompts\nconsiderably deviates from that of language instructions, making it challenging to directly apply the\nreasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgent\nneed for a unified generalist framework that can seamlessly integrate the strengths of LLMs with the\nspecific requirements of vision-centric tasks.\nIn this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centric\ntasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs,\nVisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically,\nit comprises three core components: (1) a unified language instruction designed for vision and\nvision-language tasks, (2) a language-guided image tokenizer, and (3) an LLM-based open-ended\ntask decoder that orchestrates various tasks using language instructions. With this framework, a\nwide range of vision-centric tasks can be seamlessly integrated, including object detection, instance\nsegmentation, image captioning, and visual grounding. In addition, the framework also facilitates\ntask customization at different levels of granularity, allowing for the customization of target objects,\noutput formats, task descriptions, etc.\nCompared to current popular API-based applications [68, 73, 50, 35, 30], our model takes a unified,\nend-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiency\nof the overall process, and leveraging the strengths and data of both VFMs and LLMs within a\nsingle, cohesive system. Furthermore, our model surpasses the limitations of generalist vision models\npre-trained on pre-defined tasks. VisionLLM can effectively manage vision-centric tasks through\nlanguage instructions, embodying a flexible and open-ended approach that is not constrained by\npre-set tasks. This versatility makes VisionLLM a robust and powerful generalist model for vision\nand vision-language tasks, opening up new possibilities for the development of unified generalist\nmodels that bridge the domains of vision and language.\nIn summary, our main contributions are as follows:\n(1) We propose VisionLLM, the first framework that leverages the power of LLMs to address vision-\ncentric tasks in an open-ended and customizable manner. By aligning the definitions of vision-centric\ntasks with LLM methodologies, VisionLLM breaks new ground in enabling the unified modeling of\nvision and language, opening up possibilities for advancing the field.\n(2) We overcome many difficulties when porting LLMs to vision-centric tasks, by designing unified\nlanguage instruction that matches the format of language models and covers various vision-centric\ntasks including visual perception. Correspondingly, we develop a language-guided image tokenizer\nand an LLM-based task decoder that can handle open-ended tasks according to the given language\ninstructions based on the LLMs\u2019 reasoning and parsing capabilities.\n2\nHuman: \u201cIdentify the objects \nin <image> that belong to \n{\u2018What is the child eating?\u2019: \n<c0>, \u2018red gamepad': <c1>}\nand draw a bounding box \naround each one. The output \nshould be a list of tuples in  \nthe format (c, x1, y1, x2, y2), where c is the class label and \nx1, y1, x2, y2 are the coordinates of the top-left and \nbottom-right corners of the bounding box. The coordinates \nshould be within the range 1024.\"\nVisionLLM: \"The bounding boxes are [(<c0>, 226.4, 229.8, \n363.1, 347.4), (<c1>, 441.1, 183.5, 538.6, 269.9)].\"\n<c1>\n<c0>\n(a) Object Detection with Customized Class Set (e.g.,\nquestion, reasoning text, open-vocabulary descrip-\ntion)\nHuman: \"For each object in \n<image> that is a member \nof {'motorbike': <c0>}, \noutput a tuple with the \nclass label and the \ncoordinates of a polygon \nwith 16 points that\nencloses the object.  The coordinates should be within \nthe range 1024. The output format should be (c, x1, y1, \nx2, y2, ..., x16, y16).\"\nVisionLLM: \"The detected polygons are [(<c0>, 135.3, \n95.7, 123.4, 53.4, 84.9, 57.6, 66.8, 60.5, 60.1, 72.3, \n34.2, 71.4, ..., 124.9, 119.3)].\"\n<c0>\n(b) Object Location with Editable Output Format\nwhich is placed on top of a wooden table. The dog has \nits head in the middle of the cooling pad, and a fan is \npositioned behind it, blowing air onto the pad. The \ntable is outdoors, in a grassy area, and there are two \nchairs placed around it, one on each side. The scene \nappears to be set in a sunny weather, with a bright and \nsunny sky, which contributes to the overall outdoor \natmosphere.\"\nHuman: \"Provide a long \ndescription of the <image>.\"\nVisionLLM:  \"The image \nfeatures a white dog laying \ndown on a foldable outdoor \ncooling pad , \n(c) Image Description with Controllable Text Length\nHuman: \"With the objects \nin the <image>, if you feel \nhot, what will you do?\"\nVisionLLM: \"First, You can \nalso try turning off the \ncomputer and other \nelectrical appliances in the\nroom to reduce the heat generated by their operation.\nAs an alternative, you can use the desk fan provided in \nthe image to create a cooling effect in the room. You \ncan plug in the fan and adjust its settings to ensure it \nblows air in a direction that reaches the room\u2019s \noccupants effectively.\"\n(d) Visual Question Answer with Complex Reasoning\nFigure 2: Results and visualizations of our VisionLLM. Guided by language instructions, our\nunified generalist framework showcases its effectiveness on diverse open-ended vision-centric tasks.\nThe text marked with a gray background indicates the customized instructions and the desired outputs.\n(3) We construct a series of tasks with different granularities to verify the effectiveness of our\nmodels, ranging from easy to hard, and from pre-defined to flexible. Through these validations, we\ndemonstrate the remarkable generality of our models, showcasing their ability to handle diverse\nscenarios, including random object categories, random output formats, and random task descriptions,\nas shown in Figure 2. The successful outcomes of these validations underscore the tremendous\npotential of our model in harnessing the capabilities of LLMs to control and guide vision-centric\ntasks. In addition, with a generalist LLM-based framework, our model also yields promising results\non various vision-centric tasks. Notably, our generalist model achieves an impressive mAP score of\n60+% on the COCO dataset, surpassing many detection-specific models [82, 7, 22] and approaching\nthe state-of-the-art record.\n2\nRelated Work\n2.1\nLarge Language Model\nLarge language models (LLMs) have gained significant attention in the field of natural language\nprocessing (NLP) and artificial general intelligence (AGI), due to their impressive capabilities\nin language generation, in-context learning, world knowledge, and reasoning. The GPT family,\nincluding GPT-3 [6], ChatGPT [41], GPT-4 [40], and InstructGPT [42] are most representative\nworks of LLMs. Other LLMs like OPT [78], LLaMA [54], MOSS [15], and GLM [77] have\nalso made substantial contributions to the field. These models achieve high performance and are\nopen-sourced, serving as valuable resources for training large models and as foundations for further\nfine-tuning for specific purposes. For instance, Alpaca [53] introduces a self-instruct framework\nthat facilitates instruction tuning of the LLaMA model, reducing the reliance on human-written\n3\ninstruction data. Recently, the emergence of these LLMs has also opened up API-based applications\nfor solving vision-centric tasks. These applications have integrated visual APIs with language models\nto enable decision-making or planning based on visual information, such as Visual ChatGPT [68],\nMM-REACT [73], HuggingGPT [50], InternGPT [35], and VideoChat [30]. However, despite the\nconvenience of using language-based instructions to define tasks and describe visual elements, these\ninteractive systems [68, 73, 50, 35, 30] still face limitations in capturing fine-grained visual details\nand understanding complex visual contexts, which hinder their ability to effectively connecting vision\nand language models. In summary, while LLMs have shown tremendous potential in various NLP\napplications, their applicability to vision-centric tasks has been limited by the challenges posed by\nmodalities and task formats.\n2.2\nVision Generalist Model\nThe pursuit of generalist models [83, 38, 70], which aim to handle a wide range of tasks using a\nshared architecture and parameters, has been a long-standing goal in the machine learning community.\nInspired by the success of sequence-to-sequence (seq2seq) models in the field of NLP [44], recent\nadvancements such as OFA [58], Flamingo [1], and GIT [57] propose modeling diverse tasks as\nsequence generation tasks. Unified-IO [38], Pix2Seq v2 [9], and UniTab [71] extend this idea by using\ndiscrete coordinate tokens to encode and decode spatial information for more tasks. Gato [47] also\nincorporates reinforcement learning tasks into the seq2seq framework, while GPV [21] develops a\ngeneral-purpose vision system by combining a seq2seq module with a DETR-based visual encoder [7].\nHowever, these methods suffer from some limitations, such as slow inference speed and performance\ndegradation due to the non-parallel auto-regressive decoding process. Uni-Perceivers [83, 81, 28]\nsolve these issues by unifying different tasks using the maximum likelihood target for each input\nbased on representation similarity, regardless of their modality, making it possible to support both\ngeneration and non-generation tasks in a unified framework. Nevertheless, these generalist models\nare still restricted by pre-defined tasks and cannot support flexible open-ended task customization\nbased on language instructions like LLMs.\n2.3\nInstruction Tuning\nLanguage instructions are a powerful way to express various NLP tasks and examples for LLMs,\nas introduced by GPT-3 [6]. Following this idea, subsequent works, such as InstructGPT [42],\nFLAN [14, 67], and OPT-IML [25], explore the instruction-tuning method [66, 65] and demonstrate\nthat this simple approach effectively enhances the zero-shot and few-shot capabilities of LLMs.\nThe language instruction paradigm has also been adopted by the computer vision community to\ndefine image-to-text tasks. Flamingo [1] is a milestone work that uses vision and language inputs as\nprompts and achieves remarkable few-shot results in various vision-language tasks, such as image\ncaptioning [10] and VQA [3]. BLIP-2 [29] further connects the visual encoder with LLMs through a\nquerying transformer and a linear projection layer to build strong multimodal models. MiniGPT-4 [80]\nand LLaVA [33] finetune the BLIP-2-style models on synthetic multimodal instruction-following\ndata to unleash the potential of LLMs. However, these models mainly focus on image-to-text\ntasks and fail to address visual perception, such as object detection, instance segmentation, pose\nestimation, etc. To tackle image inpainting tasks, Bar et al. [4] introduces the first visual prompting\nframework that utilizes inpainting with discrete tokens on images. Painter [63] and SegGPT [64]\nemploy masked image modeling on raw pixels for in-context learning with paired images. While\nthese visual prompt models demonstrate good results in segmentation tasks, their applicability to\nnumerous real-world vision tasks is challenging. Moreover, defining the visual prompts as image\ninpainting is inconsistent with the language instructions in LLMs, hard to leverage the reasoning,\nparsing ability, and world knowledge of LLMs. In this work, we aim to align vision-centric tasks\nwith language tasks, use language instructions to unifiedly and flexibly define all tasks, and solve\nthem with a shared LLM-based task decoder.\n3\nVisionLLM\n3.1\nOverall Architecture\nThis work targets to provide a unified generalist framework that can seamlessly integrate the strengths\nof large language models (LLMs) with the specific requirements of vision-centric tasks. As shown in\n4\nRandom Query\nLanguage-Guided\nImage Token\n...\n<text>\n<text>\nDesired Output:\n<c1> <p1> <p3> ...\nOpen-Ended Task \nDecoder with LLM\nLanguage-Guided\nImage Tokenizer\nBackbone\n...\nVision-language example: \"Describe the image <image> in details.\"\nVision-only example: \"For each object in image <image> that is a member of class set <class>, output a\ntuple with the class label and the coordinates of a polygon with 16 points that encloses the object. The\ncoordinates should be within range <range>. The output format should be (c, x1, y1, ...).\"\nLanguage Instructions <text>\n\ud835\udc39!\n\ud835\udc39\"\n\ud835\udc47\nFigure 3: Overall architecture of the proposed VisionLLM. It consists of three parts: a unified\nlanguage instruction designed to accommodate both vision and vision-language tasks, an image\ntokenizer that encodes visual information guided by language instructions, and an LLM-based open-\nended task decoder that executes diverse tasks defined by language instructions.\nFigure 3, the overall architecture of VisionLLM consists of three key designs: (1) a unified language\ninstruction that provides a consistent interface for vision-centric task definition and customization;\n(2) a language-guided image tokenizer, which encodes visual information in alignment with the given\nlanguage prompt, enabling the model to comprehend and parse the visual content effectively; and\n(3) an LLM-based open-task decoder, which utilizes the encoded visual information and language\ninstructions to generate satisfactory predictions or outputs. The three designs work together to achieve\na flexible and open-ended framework that can handle various vision-centric tasks at different levels of\ntask customization through language instructions.\nDifferent from previous interactive systems [68, 73, 50, 35, 30] that rely on APIs, our VisionLLM\npresents a more flexible and end-to-end pipeline. Given language instructions that describe the current\ntasks and an input image, the model first uses a language-guided image tokenizer to encode the image\ntokens based on the given prompt. Then, the image tokens and language instructions are fed to an\nLLM-based open-ended task decoder. Finally, it evaluates the generated outputs against the task\ndefinition given by the unified language instructions, enabling the model to produce task-specific\nresults. This seamless, end-to-end pipeline enables VisionLLM to effectively combine vision and\nlanguage, achieving remarkable performance in open-ended and customizable vision-centric tasks.\n3.2\nUnified Language Instruction\nWe first introduce unified language instructions to describe vision-centric tasks. This design enables\nthe unification of various vision-only and vision-language task descriptions and allows for flexible\ntask customization.\nVision-Language Tasks. The instructions for vision-language tasks such as image captioning and\nvisual question answering (VQA) are straightforward and similar to NLP tasks. Following previous\nmethods [29, 83, 33], we describe the image captioning task like \u201cThe image is <image>. Please\ngenerate a caption for the image: \u201d, and the VQA task like \u201cThe image is <image>. Please generate\nan answer for the image according to the question: <question>\u201d. Here, <image> and <question>\nare the placeholdersok of the image tokens and the question, respectively.\nVision-Only Tasks. Designing effective language instructions for vision tasks is a challenging\nendeavor due to the differences in modality and task format between vision and language. Here, we\ndescribe vision tasks by providing a task description and specifying the desired output format via\nlanguage instructions.\n(1) The task description conveys the intended task to the language model. Following self-instruct [65],\nwe design a set of seed instructions with placeholders and employ LLMs to generate a large number\nof related task descriptions and randomly select one of them during training.\n5\n(2) For conventional visual perception tasks like object detection and instance segmentation, we\npropose a unified output format represented as a tuple (C, P), where C denotes the class index\nin the category set <class>, and P = {xi, yi}N\ni=1 represents N points that locate the object. To\nalign with the format of word tokens, both the class index C and the coordinates of points xi, yi are\ntransformed into discretized tokens. Specifically, the class index is an integer starting from 0, and\nthe continuous coordinates of the points are uniformly discretized into an integer within the range\n[-<range>, <range>]. For object detection and visual grounding tasks, the point number N is equal\nto 2, representing the the top-left and bottom-right points of object\u2019s bounding box. In the case of\ninstance segmentation, we employ multiple (N >8) points along the object boundary to represent an\ninstance mask [69]. Other perception tasks such as pose estimation (keypoint detection) can also be\nformulated as language instructions in this way.\nAn example of language instruction for the instance segmentation task is as follows: \u201cSegment all the\nobjects of category set <class> within the <range> of the image and generate a list of the format\n(c, x1, y1, x2, y2, ..., x8, y8). Here, c represents the index of the class label starting from 0, and (x1,\ny1, x2, y2, ..., x8, y8) correspond to the offsets of boundary points of the object relative to the center\npoint. The image is: <image>\u201d.\n3.3\nLanguage-Guided Image Tokenizer\nVisionLLM considers images as a kind of foreign language and converts them into token represen-\ntations. Unlike previous works [17, 60, 34] that utilize fixed-size patch embeddings to represent\nimages, we introduce the language-guided image tokenizer to flexibly encode visual information that\naligns with task-specific language prompts or instructions.\nSpecifically, give an image X\u2208RH\u00d7W \u00d73 with height H and width W, we first feed it to the image\nbackbones (e.g., ResNet [23]) and extract visual features Fv of four different scales. Addition-\nally, we leverage a text encoder (e.g., BERT [16]) to extract the language features Fl from given\nprompts. The language features are then injected into each scale of visual features through cross-\nattention [55], yielding multi-scale language-aware visual features, enabling the alignment of features\nacross modalities.\nAfterward, we propose to adopt a transformer-based network (e.g., Deformable DETR [82]) with M\nrandom-initialized queries Q={qi}M\ni=1 to capture the high-level information of images. We build\nthe transformer-based network on top of the multi-scale language-aware visual features to extract M\nimage tokens T ={(ei, li)}M\ni=1, each of which is represented by an embedding ei and a location li,\ndenoting the semantic and positional information of the token. This design not only represents the\nimages independent of input resolution but also extracts the visual representation that is informative\nwith respect to the language prompts.\n3.4\nLLM-based Open-Ended Task Decoder\nWe build our decoder on Alpaca [53], an LLM that is adapted from LLaMA [54], to handle various\nvision-related tasks with language guidance. However, Alpaca has some inherent drawbacks for\nvision-centric tasks, such as (1) It only has a few digit tokens (e.g., 0\u223c9) in its vocabulary, which\nrestricts its ability to locate objects by numbers; (2) It uses multiple tokens to represent the category\nname, resulting in an inefficient scheme in object classification; and (3) It is a causal model that is\ninefficient for visual perception tasks.\nTo tackle these issues, we expand the vocabulary of LLM with additional tokens specially designed\nfor vision-centric tasks. First, we add a set of location tokens, denoted as {<p-512>, ..., <p0>,\n..., <p512>}, where <p i> represents the discretized offset of i \u2208 [\u2212512, 512] to the location li\nof the image token, and the relative value to image height or width is equal to i/512. These\ntokens successfully transform the object localization task from continuous variable prediction to\nmore unified discrete bin classification. Second, we introduce semantics-agnostic classification\ntokens {<c0>, <c1>, ..., <c511>} to replace category name tokens, which overcomes the inefficiency\nof using multiple tokens to represent categories. The mapping between category names and the\nclassification tokens is flexibly provided in the category set <class> of language instructions, such as\n{\"person\":<c0>, \"car\":<c1>, \"black cat\":<c2>,...}. This design allows our model to select\nthe appropriate category name from the provided category set, facilitating efficient and accurate\nobject classification.\n6\nLLM-based Open-Ended Task Decoder\nTask defined\nby instructions\nparsing\nformat 1: \"< cls > <x1> <y1> ...\"\nformat 2: \"<bos>\"\n...\nformat n: ...\nFigure 4: Illustration of the \u201coutput-format-as-\nquery\u201d decoding process. \u201c<cls> <x1> <y1> ...\u201d\ndenote the queries of the object\u2019s class index and\nboundary points, and \u201c<bos>\u201d denotes the begin-\nning of string.\nMoreover, to address the inefficiency caused\nby the causal framework, we introduce output-\nformat-as-query decoding. We first use LLMs to\nparse the structural output format from the task\ninstructions (e.g., \u201c<cls> <x1> <y1> <x2>\n<y2>\u201d for object detection, \u201c<bos>\u201d for image\ncaptioning), and then feed the tokens of struc-\ntural output format as queries to the decoder\nto generate the desired output according to the\nqueries. This simple method enables our model\nto not only avoid inefficient token-by-token de-\ncoding in visual perception tasks, but also keep\na unified framework for vision-language tasks.\nIn this way, the output of object location and\nclassification is formulated as a foreign lan-\nguage, thus unifying these vision-centric tasks\ninto the format of token classification. Therefore, both vision-language and vision-only tasks can be\nsupervised with the cross entropy loss like language tasks. In addition, for efficient training, we adopt\nthe Low-Rank Adaptation (LoRA) approach [24], which allows us to train and fine-tune the models\nwithout excessive computational costs. It also acts as a bridge between the language and visual tokens,\nfacilitating effective alignment between the two modalities, ensuring better task customization, and\nimproving the convergence of the overall system.\n4\nExperiment\n4.1\nExperimental Settings\nDatasets. VisionLLM unifies the output formats of vision and language tasks as vocabulary gen-\neration, which enables models to be jointly trained on a wide range of tasks. In the experiments,\nwe investigate the general modeling capacities of VisionLLM on five vision-centric tasks, including\nobject detection, instance segmentation, visual grounding, image captioning, and visual question\nanswering. For object detection and instance segmentation, COCO2017 [32] is used for training and\nevaluation. For visual grounding, we combine the annotations of RefCOCO [75], RefCOCO+ [75]\nand RefCOCOg [39] for training, resulting in over 120k referred objects in total. And our models are\nevaluated on the validation set of RefCOCO. For image captioning and visual question answering, we\nadopt COCO Caption [10] and LLaVA-Instruct-150K [33] as the training source. We evaluate the im-\nage captioning performance on the COCO Karpathy test split following common practice [28, 58, 71].\nWe mainly use qualitative results (see Figure 2d) to demonstrate the VQA capability of our model,\nas LLaVA-Instruct-150K is not compatible with the standard VQA benchmark. These tasks differ\nin their granularity, ranging from coarse-grained image level to fine-grained pixel level, enabling a\ncomprehensive evaluation of the model\u2019s ability to adapt to different levels of customization through\nlanguage instructions.\nImplementation Details. We implement two variants of VisionLLM with two image backbones, i.e.,\nResNet [23] and InternImage-H [59]. For the language-guided image tokenizer, we adopt BERT-\nBase [5] as the text encoder and Deformable DETR (D-DETR) [82] to capture high-level information.\nWe set the number of queries M to 100, and the number of encoder/decoder layers to 6 for D-DETR.\nFor the LLM, we employ Alpaca-7B [53], a LLaMA [54] model fine-tuned with instructions, and\nequip it with LoRA [24] for parameter-efficient fine-tuning.\nThe model is trained in two stages. In the first stage, we initialize the model with the pre-trained\nweights of D-DETR, BERT, and Alpaca-7B, and train the visual backbone and the language-guided\nimage tokenizer, while freezing most parameters of the LLM except a few LoRA parameters. To\nsimplify the training complexity, in this stage, we mainly focus on object detection tasks with random\nobject categories and task descriptions. In the second stage, we freeze the visual backbone and\nintroduce the unified supervision of multiple tasks. Unless otherwise specified, the training runs for\n50 epochs on 4 \u00d7 8 NVIDIA A100 GPUs. AdamW [36] is used as the optimizer, with one sample per\nGPU. We employ the cosine annealing schedule [37] as the learning policy, with an initial learning\n7\nTable 1: Results on standard vision-centric tasks. \u2018Intern-H\u201d denotes InternImage-H [59]. \u201csep\u201d\nindicates that the model is separately trained on each task.\nMethod\nBackbone Open-\nEnded\nDetection\nInstance Seg.\nGrounding\nCaptioning\nAP AP50 AP75 AP AP50 AP75\nP@0.5\nBLEU-4 CIDEr\nSpecialist Models\nFaster R-CNN-FPN [48] ResNet-50\n-\n40.3 61.0 44.0\n-\n-\n-\n-\n-\n-\nDETR-DC5 [7]\nResNet-50\n-\n43.3 63.1 45.9\n-\n-\n-\n-\n-\n-\nDeformable-DETR [82]\nResNet-50\n-\n45.7 65.0 49.1\n-\n-\n-\n-\n-\n-\nMask R-CNN [22]\nResNet-50\n-\n41.0 61.7 44.9 37.1 58.4 40.1\n-\n-\n-\nPolar Mask [69]\nResNet-50\n-\n-\n-\n-\n30.5 52.0 31.1\n-\n-\n-\nPix2Seq [8]\nResNet-50\n-\n43.2 61.0 46.1\n-\n-\n-\n-\n-\n-\nUNITER [11]\nResNet-101\n-\n-\n-\n-\n-\n-\n-\n81.4\n-\n-\nVILLA [19]\nResNet-101\n-\n-\n-\n-\n-\n-\n-\n82.4\n-\n-\nMDETR [27]\nResNet-101\n-\n-\n-\n-\n-\n-\n-\n86.8\n-\n-\nVL-T5 [13]\nT5-B\n-\n-\n-\n-\n-\n-\n-\n-\n-\n116.5\nGeneralist Models\nUniTab [72]\nResNet-101\n-\n-\n-\n-\n-\n-\n-\n88.6\n-\n115.8\nUni-Perceiver [83]\nViT-B\n-\n-\n-\n-\n-\n-\n-\n-\n32.0\n-\nUni-Perceiver-MoE [81]\nViT-B\n-\n-\n-\n-\n-\n-\n-\n-\n33.2\n-\nUni-Perceiver-V2 [28]\nViT-B\n-\n58.6\n-\n-\n50.6\n-\n-\n-\n35.4\n116.9\nPix2Seq v2 [9]\nViT-B\n-\n46.5\n-\n-\n38.2\n-\n-\n-\n34.9\n-\nVisionLLM-R50sep\nResNet-50\n-\n44.8 64.1 48.5 25.2 50.6 22.4\n84.4\n30.8\n112.4\nVisionLLM-R50\nResNet-50\n\u2713\n44.6 64.0 48.1 25.1 50.0 22.4\n80.6\n31.0\n112.5\nVisionLLM-H\nIntern-H\n\u2713\n60.2 79.3 65.8 30.6 61.2 27.6\n86.7\n32.1\n114.2\nrate of 2 \u00d7 10\u22124. In addition to the experiments in the main paper, more experimental settings and\nablation studies are provided in the supplementary material due to space limitations.\n4.2\nTask-Level Customization\nWe first evaluate the task-level customization capability of VisionLLM. VisionLLM supports coarse-\ngrained task customization, including visual perception tasks and visual-language tasks. Table 1\npresents the evaluation results on four standard vision-centric tasks, including object detection,\ninstance segmentation, visual grounding, and image captioning. We compare our model with task-\nspecific methods as well as recently-proposed vision generalist models. Note that, unless specifically\nmentioned, the results of our model come from a shared-parameter generalist model and switch\ndifferent tasks by changing the language instructions only. Detailed instructions could be found in\nthe supplementary material.\nObject Detection. Object detection is a fundamental computer vision task that involves identifying\nand localizing objects of interest within an image. Our method achieves comparable or higher results\nto others, 44.6 mAP, with a ResNet-50 [23] backbone. With the same backbone i.e. ResNet-50,\nour method outperforms Pix2Seq [8] by 1.4 mAP, which also discretizes the output coordinates to\nintegers. Furthermore, benefiting from the output-format-as-query framework (see Sec. 3.4), we can\ndecode multiple predictions in parallel during inference, making our approach more efficient. Using\nInternImage-H [59] as the visual backbone, we obtained 60.2% mAP, which is close to the current\nstate-of-the-art detection-specific model [59], demonstrating the scalability of our generalist model.\nVisual Grounding. Visual grounding associates textual descriptions with corresponding regions\nor objects within an image. Training visual grounding and object detection can potentially conflict\nwith each other, as object detection aims to detect all the objects, while visual grounding should only\nlocalize the referred object and suppress other objects. Benefiting from our unified task instructions\nand the strong instruction comprehension capabilities of LLMs, our model performs both tasks\neffectively and achieves a result of 80.6 P@0.5 for visual grounding. With InternImage-H as the\nbackbone, we achieve 86.7 P@0.5 on the validation set of RefCOCO.\nInstance Segmentation. Instance segmentation involves identifying and segmenting individual\nobjects within an image. We employ a flexible number of points (i.e., 8\u223c24) along the object\nboundary to represent an instance mask. Compared to mainstream models specific to instance\nsegmentation, our model has a comparable mask AP50 (61.2% with InternImage-H [59]) but relatively\nlow mask AP75. This gap could potentially arise from factors as follows: (1) We discretize the output\n8\nTable 2: Experiments of object-level and output format customization. We conduct these\nexperiments based on VisionLLM-R50, and report the performance of box AP and mask AP on\nCOCO minival for (a) and (b), respectively. \u201c#Classes\u201d and \u201c#Points\u201d indicate the number of classes\nand boundary points, respectively. \u201c*\u201d indicates that we report the mean AP of the given classes, e.g.,\n10 classes.\n(a) Object-level customization.\n#Classes\nAP\nAP50 AP75 APS APM APL\n10\u2217\n48.9\n72.6\n51.2\n31.7\n47.5\n67.3\n20\u2217\n52.7\n73.6\n56.8\n31.8\n53.2\n70.5\n40\u2217\n49.3\n70.7\n53.2\n33.1\n53.6\n63.8\n80\u2217\n44.6\n64.0\n48.1\n26.7\n47.9\n60.5\n(b) Output format customization.\n#Points\nAP\nAP50 AP75 APS APM APL\n8\n18.5\n45.7\n11.6\n9.9\n19.7\n28.7\n14\n22.9\n48.3\n19.4\n11.0\n25.1\n36.0\n16\n24.2\n49.9\n20.9\n11.5\n26.3\n36.8\n24\n25.1\n50.0\n22.4\n12.5\n27.4\n38.2\nTable 3: Ablation studies on language-guided image tokenizer and hyper-parameters.\n(a) Effect of text encoder in the\nlanguage-guided image tokenizer.\nw/ BERT Freeze COCO RefCOCO\n-\n-\n44.7\n48.1\n\u2713\n-\n44.8\n84.1\n\u2713\n\u2713\n1.3\n34.3\n(b) Effect of image tokenization\nmethod.\nTokenization\nAP\nAverage Pooling\n23.1\nOurs\n44.8\n(c) Effect of the num-\nber of bins (#Bins).\n#Bins\nAP\n257\n34.9\n513\n40.8\n1025\n44.8\n2049\n44.8\ncoordinates to integers for unifying tasks, which introduces information loss; (2) Due to the memory\nand computational constraint, the number of points in our model is limited, which also results in a\nperformance drop; and (3) Point-based methods typically yield lower results compared to direct mask\nprediction methods, such as Mask R-CNN [22].\nImage Captioning. We also evaluate our model in a representative vision-language task, i.e. image\ncaptioning task, and report the BLEU-4 [43] and CIDEr [56] metrics. Note that we do not adopt the\nbeam search [2] or CIDEr optimization [49]. We can observe that VisionLLM achieves competitive\nperformance to previous methods. With ResNet-50, we obtain a BLEU-4 score of 31.0 and a CIDEr\nscore of 112.5. When using InternImage-H as the backbone, our model achieves a comparable\nBLEU-4 score of 32.1 and a CIDEr score of 114.2. These results demonstrate the effectiveness of\nVisionLLM in generating descriptive and contextually relevant captions for images.\n4.3\nObject-Level & Output Format Customization\nOur VisionLLM not only allows for customizing the task description, but also for adjusting the\ntarget object and the output format using language instructions. Here, we evaluate our model\u2019s\nfine-grained customization ability on COCO. In particular, to customize the target object, we modify\nthe <class> in language instructions to change the model\u2019s recognition target from 10 classes to\n80 classes. Likewise, to customize the output format, we modify the number of points in language\ninstructions to change the task output format. Table 2 shows that our method can perform well for\nboth object-level and output format changes.\n4.4\nAblation Study\nIn this section, we analyze the effect of key components and hyper-parameters on VisionLLM. Unless\notherwise specified, we use ResNet-50 [23] backbone and perform the ablation experiments for object\ndetection tasks with random classes and task descriptions on COCO2017 [32].\nSingle Task vs. Multiple Tasks. We perform an ablation study to assess the impact of multi-task\nlearning with language instructions on VisionLLM. As shown in Table 1, the single-task trained\nmodel VisionLLM-R50sep is slightly better than the jointly trained model VisionLLM-R50 except\nimage captioning. This is due to the multitasking conflicts that also affect previous generalist\nmodels [83, 81], and it reflects a trade-off between accuracy and generalization.\nText Encoder in Language-Guided Image Tokenizer. We examine the role of text encoder (i.e.,\nBERT) in our language-guided image tokenizer in Table 3a, where we report the results for object\n9\ndetection and visual grounding. The first two rows show that BERT is not essential for object\ndetection but it is crucial for visual grounding. We also investigate the effect of freezing the text\nencoder during training. The last row indicates that freezing BERT hinders the alignment of vision\nand language modalities and thus degrades the performance for both tasks.\nImage Tokenization Method. As a comparison to our query-based tokenization, we employ average\npooling on the feature maps from the D-DETR encoder to obtain M patch embeddings, which serve\nas token representations for the image. Results in Table 3b indicate a clear advantage of our method.\nThis is due to its ability to capture information from objects of various sizes in a more flexible way.\nNumber of Localization Tokens. We vary the number of localization tokens from 257 (i.e., -\n128\u223c128) to 2049 (i.e., -1024\u223c1024), to investigate its impact on visual perception performance. As\npresented in Table 3c, the model consistently exhibits improvement as the number of localization\ntokens increases until it reaches a saturation point. Remarkably, a substantial performance boost\nis observed when the number is raised from 257 to 1025 (+9.9 AP). These results indicate that a\nhigher number of localization tokens enables the models to achieve finer localization abilities, thereby\nimproving localization accuracy.\n5\nConclusion\nIn this paper, we have presented VisionLLM, a novel framework that leverages the power of large\nlanguage models (LLMs) to address vision-centric tasks in an open-ended and customizable manner.\nWe have designed unified language instruction that matches the format of language models and covers\nvarious vision-centric tasks including visual perception. We have also developed a language-guided\nimage tokenizer and an LLM-based task decoder that can handle open-ended tasks according to the\ngiven language instructions. We have verified the effectiveness of our models on a series of tasks\nwith different granularities, demonstrating their remarkable generality and flexibility.\nBroader Impact. We envision that this work will promote the fusion of visual and language tasks.\nIn addition, since our work is built on open-source pre-trained vision foundation models and large\nlanguage models, requiring low training resources, thus reducing the carbon footprint. We do not\nforesee obvious undesirable ethical/social impacts at this moment.\n10\nAppendix\nA\nExample Instructions\nAs described in Sec. 3.2 of the main paper, we follow self-instruct [65] to design a set of seed\ninstructions with placeholders and employ LLMs to create diverse related task descriptions for\ncoarse-grained task-level customization. Here, we show some examples of instructions for task-level\ncustomization, including object detection, instance segmentation, visual grounding, image captioning,\nand visual question answering (VQA). Following various instructions, our model can elegantly switch\namong different vision-centric tasks and accomplish them in a unified manner like LLMs.\nA.1\nObject Detection\nExample 1. \u201cPlease examine the image and identify all objects in the category set <class>. For each\nobject, specify its location within the range <range> by determining the top-left and bottom-right\ncorners of its bounding box. To indicate the object\u2019s class and location, provide the output in the\nformat (c, x1, y1, x2, y2), where \u2018c\u2019 represents the class index starting from 0, and (x1, y1, x2, y2)\ncorrespond to the offsets of the bounding box corners relative to the center point. The image is:\n<image>\u201d\nExample 2. \u201cIdentify all the objects in the image that belong to the category set <class> and predict\na bounding box around each one. The output should be a list in the format (c, x1, y1, x2, y2), where c\nrepresents the index of the class label starting from 0, and x1, y1, x2, y2 are the offsets of the top-left\nand bottom-right corners of the box relative to the center point. The coordinates should be within\n<range>. The image is: <image>\u201d\nExample 3. \u201cFor each object in the image that is a member of the category set <class>, output a\ntuple with the index of class label starting from 0 and the offsets of corners relative to the center point\nthat encloses the object. The offsets should be in the order of top-left and bottom-right corners of the\nrectangle and should be within <range>. The output format should be (c, x1, y1, x2, y2). The image\nis: <image>\u201d\nA.2\nInstance Segmentation\nExample 1. \u201cSegment the objects from the image with class labels from <class> and output their\ncoordinates within range <range>. The coordinates should be given as the boundary points relative\nto the center point, and the output format should be (c, x1, y1, x2, y2, ..., x20, y20), where c is the\nindex of the class label that starts from 0. The image is: <image>\u201d\nExample 2. \u201cSegment all the objects from the category set <class> in the provided image and output\na tuple (c, x1, y1, x2, y2, ..., x14, y14) for each, where c is the index of the class label in the category\nset that starts from 0, and (x1, y1, x2, y2, ..., x14, y14) correspond to the offsets of boundary points\non the instance mask relative to the center point which should be within <range>. The image is:\n<image>\u201d\nExample 3. \u201cIn the provided image, please segment all the objects in category set <class> within\nthe range <range> by providing their coordinates in the (c, x1, y1, x2, y2, ..., x24, y24) format, where\n\u2018c\u2019 denotes the index of the class label starting from 0, and (x1, y1, x2, y2, ..., x24, y24) stand for the\noffsets of boundary points relative to the center point. The image is: <image>\u201d\nA.3\nVisual Grounding\nExample 1. \u201cPlease find the object in the category set {<expression>:<cls0>} within the range\n<range>. Please provide the output in the format (c, x1, y1, x2, y2), where c is the class index starting\nfrom 0, and (x1, y1, x2, y2) are the offsets of the top-left and bottom-right corners of the bounding\nbox relative to the center point. The image is: <image>\u201d\nExample 2.\n\u201cGiven the input image, category set {<expression>:<cls0>}, and the range\n<range>, please locate the object in the image and output the corresponding coordinates in the tuple\n(c, x1, y1, x2, y2), where c is the index of the class label starting from 0, and (x1, y1, x2, y2) are the\n11\noffsets of the top-left and bottom-right corners of the rectangle relative to the center point. The image\nis: <image>\u201d\nExample 3. \u201cFor each object in the image that belongs to the {<expression>:<cls0>} category\nset, please provide the class label (starting from 0) and the offsets from the center of a bounding\nbox that encloses the object. The corner offsets should be in the order of top-left and bottom-right,\nand within the range <range>. The output should be in the format (c, x1, y1, x2, y2). The image is:\n<image>\u201d\nA.4\nImage Captioning\nExample 1. \u201cThe image is <image>. Write a caption: \u201d\nExample 2. \u201cThe image is <image>. Please describe this image: \u201d\nExample 3. \u201cWith the objects in the <image>, please generate a caption for the image: \u201d\nA.5\nVisual Question Answering\nExample 1. \u201cThe image is <image>. Please generate an answer according to the question:\n<question>. \u201d\nExample 2. \u201cThe image is <image>. Please answer the question <question> according to the\nimage. \u201d\nExample 3. \u201cWith the objects in the <image>, <question>. \u201d\nB\nLoss Function\nVisionLLM consists of two model components: language-guided image tokenizer and LLM-based\nopen-task decoder. So the total loss L of our model can be written as:\nL = Ltok + Ldec,\n(1)\nwhere Ltok and Ldec denote the loss of language-guided image tokenizer and LLM-based open-task\ndecoder, respectively. We introduce the two loss functions as follows:\nLanguage-Guided Image Tokenizer. Different from the Q-Former [29], we use a supervision\nmethod similar to that of Deformable DETR [82], but with a different loss Ltok: category-agnostic\nclassification (focal loss [31]) and center point regression (L1 loss). As explained in Sec. 3.3, our\nimage tokenizer extracts M image tokens T = {(ei, li)}M\ni=1, each of which is represented by an\nembedding ei and a location li (i.e., absolute coordinates of the center point).\nLLM-Based Open-Ended Task Decoder. We handle two cases in decoding processing differently.\n(1) For regular word prediction, we train with standard next-token supervision [54, 45, 6, 46]; (2) For\nunordered set prediction (e.g., bounding boxes), we first output a sequence of tokens according to the\noutput format (see the output-format-as-query paradigm in Sec. 3.4), then use bipartite matching to\nalign the LLM-predicted outputs with the ground truths. Despite the differences, we use cross-entropy\nto compute the loss Ldec in a unified way for both cases.\nC\nTraining Schedule\nAs shown in Figure 5, to speed up the convergence of VisionLLM, we split the training schedule of\nVisionLLM into two stages:\nStage 1. In this stage, we initialize the language-guided image tokenizer by loading the pre-trained\nweights of Deformable DETR [82] and BERT [16]. Additionally, Alpaca [53] is employed as the\nLLM-based open-ended task decoder. To align visual tokens with text tokens, we make the language-\nguided image tokenizer trainable while freezing most parameters of the pre-trained Alpaca, with only\na few LoRA [24] parameters left tunable. We only focus on object detection in this stage to simplify\nthe training difficulty, with random task descriptions and object categories.\nStage 2. The second stage builds upon the model weights obtained from the first stage. For efficiency,\nwe freeze the visual backbone (e.g., ResNet [23]) in the language-guided image tokenizer. Notably,\n12\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\nDetection mAP (%)\nEpoch\nSingle-Stage\nTwo Stages (ours)\nStage-1 (Object Detection)\nStage-2 (Multiple Tasks)\nFigure 5: Comparison of two training schedules for VisionLLM. We found that a two-stage\ntraining from easy to hard converges faster than a single-stage training.\nTable 4: More ablation studies for VisionLLM.\n(a) Effect of randomness.\nRandomness\nAP\nNone\n45.2\n+ Random Task Description\n45.1\n++ Random Object Category\n44.8\n+++ Random Output Format\n44.6\n(Multi-task Joint Training)\n(b) Effect of LoRA [24].\nLoRA\nRandomness\nAP\n\u2717\n\u2717\n45.2\n\u2717\n\u2713\n1.2\n\u2713\n\u2713\n44.8\n(c) Effect of the number\nof image tokens.\n#Tokens\nAP\n50\n44.5\n100\n44.8\n200\n45.1\n300\n45.2\n(d) Effect of Seq2Seq.\nSeq2Seq\nAP\n\u2713\n-\n\u2717\n44.8\n(e) Large vocabulary object detection.\nDataset\n#Classes\nAP\nCOCO\n80\n44.8\nLVIS\n1203\n18.9\nthis stage introduces the unified supervision of multiple tasks, including object detection, instance\nsegmentation, visual grounding, image captioning, and VQA, facilitating the model to leverage the\npower of LLMs to understand and manipulate visual information holistically.\nD\nMore Ablation Studies\nIn this section, we provide more ablation studies and analysis of VisionLLM. Unless otherwise\nspecified, we use ResNet-50 [23] backbone and perform the ablation experiments for object detection\ntasks with random task descriptions and object categories on COCO 2017 [32].\nRandomness. In Table 4a, we examine the effect of introducing randomness during training for\nVisionLLM, including randomness in task descriptions, object categories, and output formats (i.e.,\nmulti-task joint training). Initially, without any randomness, the model achieves a box AP of 45.2.\nHowever, as randomness is gradually applied, interesting phenomena emerge: while there is a slight\ndecrease (45.2 \u2192 44.6) in the AP of standard detection with the introduction of randomness, the\noverall benefits of enhanced task customization and open-ended capabilities outweigh this minor\ntrade-off. Overall, introducing randomness during training in VisionLLM positively impacts its\ncapacity for open-ended tasks and customization.\nLow-Rank Adaptation (LoRA). As shown in Table 4b, when randomness is not applied, the\nmodel achieves 45.2 box AP without using LoRA [24]. However, when randomness is employed,\nit is observed that the model fails to converge without using LoRA. Conversely, when LoRA and\nrandomness are used together, the model is able to converge. This indicates that LoRA plays a crucial\nrole as a bridge between the language and visual tokens, enabling effective alignment between the\ntwo modalities and improving the convergence of the overall system.\nNumber of Image Tokens. We vary the number of image tokens from 50 to 300 to investigate\ntheir impact on the performance. Results are presented in Table 4c. As the number of image tokens\nincreases, the performance continues to improve. This makes sense because a larger number of\n13\n1\n2\n3\n4\n5\n6\n7\n8\n44.8\n44.7\n44.8\n44.7\n44.8\n44.7\n44.7\n44.8\nPrompt\nAP\nFigure 6: Evaluation results using eight different prompts. The first six prompts use different task\ndescriptions of object detection, while the last two prompts employ random category orders. These\nresults show that the performance of different prompts is similar, only a 0.1 AP gap is observed.\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\nfire hydrant\nstop sign\nparking meter\nbench\nbird\ncat\ndog\nhorse\nsheep\ncow\nelephant\nbear\nzebra\ngiraffe\nbackpack\numbrella\nhandbag\ntie\nsuitcase\nfrisbee\nskis\nsnowboard\nsports ball\nkite\nbaseball bat\nbaseball glove\nskateboard\nsurfboard\ntennis racket\nbottle\nwine glass\ncup\nfork\nknife\nspoon\nbowl\nbanana\napple\nsandwich\norange\nbroccoli\ncarrot\nhot dog\npizza\ndonut\ncake\nchair\ncouch\npotted plant\nbed\ndining table\ntoilet\ntv\nlaptop\nmouse\nremote\nkeyboard\ncell phone\nmicrowave\noven\ntoaster\nsink\nrefrigerator\nbook\nclock\nvase\nscissors\nteddy bear\nhair drier\ntoothbrush\n0\n10\n20\n30\n40\n50\n60\n70\nmAP (%)\nstandard 80 classes\nrandom 40 classes\n(a) We randomly select 40 classes in random order to form the category set.\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\nfire hydrant\nstop sign\nparking meter\nbench\nbird\ncat\ndog\nhorse\nsheep\ncow\nelephant\nbear\nzebra\ngiraffe\nbackpack\numbrella\nhandbag\ntie\nsuitcase\nfrisbee\nskis\nsnowboard\nsports ball\nkite\nbaseball bat\nbaseball glove\nskateboard\nsurfboard\ntennis racket\nbottle\nwine glass\ncup\nfork\nknife\nspoon\nbowl\nbanana\napple\nsandwich\norange\nbroccoli\ncarrot\nhot dog\npizza\ndonut\ncake\nchair\ncouch\npotted plant\nbed\ndining table\ntoilet\ntv\nlaptop\nmouse\nremote\nkeyboard\ncell phone\nmicrowave\noven\ntoaster\nsink\nrefrigerator\nbook\nclock\nvase\nscissors\nteddy bear\nhair drier\ntoothbrush\n0\n10\n20\n30\n40\n50\n60\n70\nmAP (%)\nstandard 80 classes\nrandom 80 classes\n(b) We randomly change the order of 80 classes to form the category set.\nFigure 7: Per-category AP on COCO dataset. We randomly select some categories to form the\ncategory set <class> in language instructions.\nimage tokens provides a more detailed description of the image content. Considering computational\ncomplexity, we adopted 100 image tokens in our experiments.\nRobustness to Prompt Changes. Since VisionLLM is trained with random prompts, including\nrandom task descriptions and random categories, one may ask whether there is a large performance\nvariance across different prompts. To validate the stability of VisionLLM, we conduct experiments\nusing eight different prompts. The first six prompts employ different task descriptions, while the last\ntwo prompts involve random category orders. In the case of random category orders, we map the\ncategories back to the COCO standard category order for evaluation. As shown in Figure 6, most\nevaluation results are distributed closely to 44.8 AP. The performance differences among prompts are\nmarginal, demonstrating that VisionLLM is robust to different prompts.\nInstruction Following Capability. As shown in Figure 7, when the prompt only contains 40 classes,\nthe performance for these categories remains normal, while the performance for the remaining\ncategories is close to zero. This indicates that VisionLLM can dynamically detect objects based on\nthe given class set <class> in instructions while disregarding the other classes that are not mentioned.\nThis result highlights the flexibility of VisionLLM in adhering to instructions.\nOutput-Format-As-Query vs. Seq2Seq. In VisionLLM, we introduce the output-format-as-query\nframework for LLM decoder. Alternatively, we also experiment with the sequence generation method\nlike Pix2Seq [8] for object detection with random task descriptions and object categories. However,\nwe find that the loss is hard to converge in this paradigm, which indicates that the seq2seq decoding\nmay need a more detailed design or a longer training schedule for the open-ended visual tasks, while\nthe proposed output-format-as-query framework is more effective for open-ended tasks.\nLarge-Vocabulary Object Recognization. To validate the capacity of VisionLLM in the large-\nvocabulary scenario, we further conduct the experiments on the challenging dataset LVIS [20] with\n1203 categories. Due to the limited number of language tokens, we randomly select 80 classes\nfor training in each iteration. During inference, we divide the 1203 categories into 16 groups and\n14\n#Points=8\n#Points=14\n#Points=16\n#Points=24\nFigure 8: Customization of instance masks using the different number of points. Notably, we\nonly modify the output format mentioned in the prompt, i.e. the number of segmentation points. For\nmore details, please see the example prompts provided in Sec. A.2.\npredict the results in a sliding-window manner. As shown in Table 4e, without tricks like federal loss,\nVisionLLM-R50 can achieve 18.9 mAP on LVIS.\nE\nQualitative Analysis\nCustomization of Segmentation Points. In this experiment, we focus on modifying the output\nformat mentioned in the prompt, specifically the number of points for instance segmentation (see\nSec. A.2). The results are visualized in Figure 8. Remarkably, by increasing the number of points\nfor segmentation, we observe that the model successfully predicts more refined object masks. This\nvalidates the capability of our method to precisely customize the output format, showcasing fine-\ngrained control over the segmentation process.\nCustomization of Category Set. We change the content of the category set <class> in language\ninstructions and visualize the predictions in Figure 9. It is observed that the model can correctly\npredict the object category according to the provided category set. This demonstrates that VisionLLM\nhas a strong capacity to understand and reason over the semantic information of language instructions,\nwhich allows it for flexible category customization in open-vocabulary scenarios.\nImage Description & VQA. Benefiting from the power of LLMs, VisionLLM exhibits a strong\nability in generating long descriptions for images and answering visual questions with complex\nreasoning. We show the examples in Figure 10.\n15\nInstruction: \u201cIdentify the objects in the image that belong to \n{\u2018person\u2019: <c0>, ..., \u2018frisbee': <c29>, ...} and draw a bounding \nbox around each one. The output should be a list of tuples in \nthe format (c, x1, y1, x2, y2), where \u2018c\u2019 represents the index \nof the class label starting from 0, and x1, y1, x2, y2 are the \noffsets of the top-left and bottom-right corners of the box \nrelative to the center point. The coordinates should be \nwithin <range>. The image is: <image>\u201d\nInstruction: \u201cIdentify the objects in the image that belong to \n{\u2018person\u2019: <c0>, \u2018bicycle\u2019: <c1>, ..., \u2018backpack': <c24>, ..., \n\u2018toothbrush\u2019: <c79>} and draw a bounding box around each\none. The output should be a list of tuples in the format (c, x1, \ny1, x2, y2), where \u2018c\u2019 represents the index of the class label \nstarting from 0, and x1, y1, x2, y2 are the offsets of the top-\nleft and bottom-right corners of the box relative to the \ncenter point. The coordinates should be within <range>. The \nimage is: <image>\u201d\nInstruction : \u201cIdentify the objects in the image that belong \nto {\u2018frisbee': <c0>} and draw a bounding box around each\none. The output should be a list of tuples in the format (c, x1, \ny1, x2, y2), where \u2018c\u2019 represents the index of the class label \nstarting from 0, and x1, y1, x2, y2 are the offsets of the top-\nleft and bottom-right corners of the box relative to the \ncenter point. The coordinates should be within <range>. The \nimage is: <image>\u201d\nInstruction: \u201cIdentify the objects in the image that belong to \n{\u2018bicycle\u2019: <c0>, \u2018surfboard\u2019: <c1>, \u2018sports ball': <c2>, \n\u2018backpack\u2019: <c3>, \u2018the man wearing blue T-shirt\u2019: <c4>} and \ndraw a bounding box around each one. The output should be \na list of tuples in the format (c, x1, y1, x2, y2), where \u2018c\u2019\nrepresents the index of the class label starting from 0, and x1, \ny1, x2, y2 are the offsets of the top-left and bottom-right \ncorners of the box relative to the center point. The \ncoordinates should be within <range>. The image is: <image>\u201d\nFigure 9: Customization of category set. These results demonstrate that VisionLLM can correctly\ndetect the specified objects according to the category set <class> provided in language instructions.\n16\nInstruction: \u201cThe image is: <image>. Analyze the image in a \ncomprehensive and detailed manner.\u201d\nVisionLLM: \u201cThe image features a busy city street with a \nlarge red double-decker bus driving down the road. There \nare several people walking on the sidewalk, some of whom \nare carrying backpacks. The bus is positioned towards the \nleft side of the image. There are several people walking \nalong the sidewalk, with some closer to the bus and others \nfurther away. The overall atmosphere suggests a busy urban \nenvironment with people going about their daily routines.\u201d\nVisionLLM: \u201cThe image features a large, open-air market \nfilled with a variety of fruits and vegetables. There are \nnumerous mangoes, watermelons and apples displayed in \nthe market, with some fruits placed closer to the foreground \nand others further back. The market is bustling with activity \nas people can be seen browsing and purchasing the fresh \nproduce. The market is likely a popular destination for those \nseeking a fresh and healthy diet.\u201d\nInstruction: \u201cThe image is: <image>. Write a detailed\ndescription of the given image.\u201d\nInstruction: \u201cThe image is: <image>. Write a short description of the given image.\u201d\nVisionLLM: \u201cThe image features a lively open-air market with a variety of fruits and vegetables. People \nare busy browsing and buying fresh produce.\u201d\n(a) Image description with controllable text length\nInstruction: \u201cThe image is: <image>. Please generate an \nanswer according to the question: what reasons could there \nbe for the cat squatting on the shoes?\u201d\nVisionLLM: \u201cThere could be several reasons. One possibility \nis that the cat is seeking warmth or comfort, as shoes can \nprovide a cozy and warm surface for the cat to rest on. \nAnother reason could be that the cat is seeking attention or \naffection from its owner, who is standing nearby. Cats are \nknown to be curious creatures, and they may be seeking \nattention or attention from their owners.\u201d\nVisionLLM: \u201cThe working environment in this office space \nappears to be open, spacious, and well-equipped. There are \nseveral desks set up for employees, providing ample \nworkspace for each individual. The office features a modern \nand contemporary design, which suggests a creative and \ncollaborative work environment. The presence of several \nchairs and the organization of the office, including the \nlaptops and monitors, indicate that the space is well-\nmaintained and conducive to productivity. Overall, the office \nappears to be a pleasant and efficient work environment for \nits employees.\u201d\nInstruction: \u201cThe image is: <image>. What do you think of \nthe working environment here?\u201d\n(b) Visual question answering with reasoning\nFigure 10: Visualization of the image description and VQA capabilities of VisionLLM.\n17\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot\nlearning. arXiv preprint arXiv:2204.14198, 2022. 1, 4\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 9\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Conference\non Computer Vision, 2015. 4\n[4] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via\nimage inpainting. Advances in Neural Information Processing Systems, 2022. 4\n[5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? In International Conference on Machine Learning, 2021. 7\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems, 2020. 3, 4, 12\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\n2020. 3, 4, 8\n[8] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\nframework for object detection. arXiv preprint arXiv:2109.10852, 2021. 8, 14\n[9] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey Hinton. A unified sequence\ninterface for vision tasks. arXiv preprint arXiv:2206.07669, 2022. 4, 8\n[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015. 4, 7\n[11] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing\nLiu. Uniter: Universal image-text representation learning. In Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX. Springer, 2020. 8\n[12] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. In International Conference on Learning Representations, 2023. 1\n[13] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\nIn International Conference on Machine Learning, 2021. 8\n[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022. 4\n[15] MOSS contributors. Moss. https://github.com/OpenLMLab/MOSS, 2023. 3\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 6, 12\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning\nRepresentations, 2021. 6\n[18] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint\narXiv:2211.07636, 2022. 1\n[19] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning. Advances in Neural Information Processing\nSystems, 2020. 8\n18\n[20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356\u20135364,\n2019. 14\n[21] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose vision\nsystems. arXiv preprint arXiv:2104.00743, 2021. 4\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2017. 3, 8, 9\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 6, 7, 8, 9, 12,\n13\n[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021. 7, 12, 13\n[25] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction\nmeta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 4\n[26] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\nSer-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, 2022. 2\n[27] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\nMdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021. 8\n[28] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao,\nXiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and\nvision-language tasks. arXiv preprint arXiv:2211.09808, 2022. 4, 7, 8\n[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4, 5, 12\n[30] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2, 4, 5\n[31] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object\ndetection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2017. 12\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on\nComputer Vision. Springer, 2014. 7, 9, 13\n[33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023. 4, 5, 7\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021. 6\n[35] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang\nYang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatbots\nbeyond language. arXiv preprint arXiv:2305.05662, 2023. 2, 4, 5\n[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations. 7\n[37] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016. 7\n[38] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\nunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022. 4\n[39] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2016. 7\n19\n[40] OpenAI. Gpt-4 technical report. arXiv, 2023. 3\n[41] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022. 1, 3\n[42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 2022. 3, 4\n[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, 2002. 9\n[44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. 2018. 1, 4\n[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. OpenAI, 2018. 12\n[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 12\n[47] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.\narXiv preprint arXiv:2205.06175, 2022. 4\n[48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In Advances in Neural Information Processing Systems, 2015. 8\n[49] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017. 9\n[50] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 2, 4,\n5\n[51] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou,\nand Jifeng Dai. Towards all-in-one pre-training via maximizing multi-modal mutual information. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 1\n[52] Chenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image modeling\nfor self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022. 1\n[53] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for\nResearch on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 2023. 3, 6, 7, 12\n[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. 3, 6, 7, 12\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems,\n30, 2017. 6\n[56] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. 9\n[57] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022. 1, 4\n[58] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. arXiv preprint arXiv:2202.03052, 2022. 1, 4, 7\n[59] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable\nconvolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 1, 2,\n7, 8\n20\n[60] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media,\n8(3):415\u2013424, 2022. 6\n[61] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. 1, 2\n[62] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\ngeneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. 2\n[63] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A\ngeneralist painter for in-context visual learning. arXiv preprint arXiv:2212.02499, 2022. 4\n[64] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt:\nSegmenting everything in context. arXiv preprint arXiv:2304.03284, 2023. 2, 4\n[65] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560, 2022. 4, 5, 11\n[66] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana\nArunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking\ngeneralization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705,\n2022. 4\n[67] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,\n2021. 4\n[68] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 2, 4,\n5\n[69] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo.\nPolarmask: Single shot instance segmentation with polar representation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020. 6, 8\n[70] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance\nperception as object discovery and retrieval. arXiv preprint arXiv:2303.06674, 2023. 4\n[71] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and\nLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European\nConference on Computer Vision, pages 521\u2013539. Springer, 2022. 4, 7\n[72] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and\nLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European\nConference on Computer Vision, 2022. 8\n[73] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\naction. arXiv preprint arXiv:2303.11381, 2023. 2, 4, 5\n[74] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful\nprompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021. 2\n[75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in\nreferring expressions. In European Conference on Computer Vision. Springer, 2016. 7\n[76] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and language\nprompt learning. arXiv preprint arXiv:2210.07225, 2022. 2\n[77] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,\n2022. 3\n[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022. 3\n21\n[79] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673,\n2022. 2\n[80] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n4\n[81] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng\nDai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. arXiv preprint\narXiv:2206.04674, 2022. 1, 4, 8, 9\n[82] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In International Conference on Learning Representations,\n2021. 3, 6, 7, 8, 12\n[83] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-\nperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 4, 5, 8, 9\n22\n"
  },
  {
    "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
    "link": "https://arxiv.org/pdf/2305.10688.pdf",
    "upvote": "1",
    "text": "MolXPT: Wrapping Molecules with Text for Generative Pre-training\nZequn Liu1\u2217, Wei Zhang2 \u2217, Yingce Xia3\u2020, Lijun Wu3, Shufang Xie4,\nTao Qin3, Ming Zhang1 \u2020 and Tie-Yan Liu3\n1 Peking University; 2 University of Science and Technology of China\n3 Microsoft Research AI4Science; 4 Renmin University of China\n{zequnliu,mzhang_cs}@pku.edu.cn; weizhang_cs@mail.ustc.edu.cn\n{yingce.xia, lijunwu, taoqin, tyliu}@microsoft.com\nshufangxie@ruc.edu.cn\nAbstract\nGenerative pre-trained Transformer (GPT) has\ndemonstrates its great success in natural lan-\nguage processing and related techniques have\nbeen adapted into molecular modeling. Con-\nsidering that text is the most important record\nfor scientific discovery, in this paper, we pro-\npose MolXPT, a unified language model of text\nand molecules pre-trained on SMILES (a se-\nquence representation of molecules) wrapped\nby text. Briefly, we detect the molecule names\nin each sequence and replace them to the cor-\nresponding SMILES. In this way, the SMILES\ncould leverage the information from surround-\ning text, and vice versa. The above wrapped\nsequences, text sequences from PubMed and\nSMILES sequences from PubChem are all fed\ninto a language model for pre-training. Exper-\nimental results demonstrate that MolXPT out-\nperforms strong baselines of molecular prop-\nerty prediction on MoleculeNet, performs com-\nparably to the best model in text-molecule trans-\nlation while using less than half of its param-\neters, and enables zero-shot molecular genera-\ntion without finetuning.\n1\nIntroduction\nGenerative pre-trained Transformer (GPT), like\nGPT-3 (Brown et al., 2020) and ChatGPT (Ope-\nnAI, 2022), have obtained great success in natural\nlanguage processing. They usually have billions of\nparameters and are trained on large corpus (Taylor\net al., 2022; Singhal et al., 2022). By witnessing\ntheir great power, people start transferring language\nmodels to chemical (Bagal et al., 2022) and bio-\nlogical domains (Ferruz et al., 2022). For exam-\nple, a small molecule (e.g., an oral drug) can be\nrepresented using simplified molecular-input line-\nentry system (SMILES) (Weininger, 1988), which\nis a sequence obtained by traversing the molecu-\nlar graph using depth-first-search and several rules\n\u2217Equal contribution. This work was done when Z. Liu\nand W. Zhang were interns at Microsoft Research AI4Science.\n\u2020Corresponding authors.\nfor branching, aromaticity, etc. After serializing\nmolecules, people pre-train language models on\nSMILES (Bagal et al., 2022; Tong et al., 2021;\nFrey et al., 2022) and obtain promising results for\nmolecular generation.\nText is the most important record for molecu-\nlar science and more generally, scientific discov-\nery (Beltagy et al., 2019). It describes detailed\nproperties of molecules, like how to synthesize the\nmolecule (Feng et al., 2016), whether the molecule\nis toxic (Juurlink et al., 2003), etc. BioGPT (Luo\net al., 2022) and PubMedGPT (Bolton et al., 2022)\nare two language models trained on biomedical lit-\nerature. Recently, a new trend is to jointly model\nSMILES and scientific text so as to obtain shared\nrepresentations across the two modalities. MolT5\nis a T5-like (Raffel et al., 2020) model, where sev-\neral spans of the text/SMILES are masked in the\nencoder and they should be reconstructed in the\ndecoder. Galactica (Taylor et al., 2022) is a GPT-\nlike (Brown et al., 2020) model pre-trained on var-\nious types of inputs, like text, SMILES, protein\nsequences, etc. Although those models demon-\nstrate progress in prediction and generation tasks,\nthey do not explicitly leverage the relation between\nmolecules and text. An intuition is that, in scien-\ntific literature, when a molecule name appears in\na sentence, the surrounding context could be a de-\nscription of the molecule. This should be useful\ninformation for joint training but is ignored in those\nmodels.\nTo leverage such relations, in this work, we\npropose a novel molecule-text language model\n(MolXPT), which is trained on \u201cwrapped\u201d se-\nquences: Given a sentence, we detect the molec-\nular names with named entity recognition tools,\nand if any, replace them to the corresponding\nSMILES and obtain the \u201cwrapped\u201d sequence be-\ntween SMILES and text. We pre-train a 24-layer\nMolXPT (with 350M parameters) on 8M wrapped\nsequences, as well as 30M SMILES from PubChem\narXiv:2305.10688v2  [cs.CL]  26 May 2023\nMolecular Property Prediction\nMolecule-text translation\nRolipram is a member of the lclass of \npyrrolidin-2-ones that is pyrrolidin-2-\none bearing a 3-(cyclopentyloxy)-4-\nmethoxyphenyl substituent at the 4-position\nNER & entity linking\nReplace molecules with SMILES\nWrapped sequences\n(8M)\nPre-training corpus\nText\n(30M doc)\nSMILES\n(30M)\nRolipram impairs NF-kappaB activity and MMP-9 expression \nin experimental autoimmune encephalomyelitis\nRolipram: CHEBI id = 104872\n                    SMILES= COc1ccc(cc1OC1CCCC1)C1CNC(=O)C1\n\u27e8som\u27e9COC1=C(C=C(C=C1)C2CC(=O)NC2)OC3CCCC3\u27e8eom\u27e9 \nimpairs NF-kappaB activity and MMP-9 expression in \nexperimental autoimmune encephalomyelitis\n\u2026\nPre-training\ns1\ns2\n\u2026\nsn\n\u2026\n\u2026\ns2\ns3\n\u2026\neos\nMolXPT (24 layers, 350M param)\nFinetuning\nBlood-brain barrier \npenetration: true\nFigure 1: Framework of MolXPT. MolXPT is pretrained on text from PubMed, SMILES from PubChem and\nwrapped sequences between SMILES and text. The wrapped sequences are obtained by applying NER and entity\nlinking to text and then replacing matched molecular mentions with SMILES. MolXPT can be finetuned for various\ntext and molecular downstream tasks, like molecular property prediction and molecule-text translation.\n(Kim et al., 2022) and 30M titles and abstracts from\nPubMed (a popular biomedical literature search en-\ngine).\nAfter pre-training, we finetune MolXPT on\nMoleculeNet (a benchmark about molecular prop-\nerty prediction) (Wu et al., 2018) and molecule-text\ntranslation (Edwards et al., 2022) using prompt-\nbased finetuning. On MoleculeNet, MolXPT out-\nperforms strong baselines with sophisticated design\nlike GEM (Fang et al., 2022). On text-molecule\ntranslation, MolXPT performs comparably with\nthe state-of-the-art model, MolT5-large (Edwards\net al., 2022). MolT5-large has 800M parameters\nwhile MolXPT only uses 44% of its parameters.\nWe also verify that MolXPT has the zero-shot abil-\nity on text-to-molecule generation.\n2\nOur Method\nMolXPT is a language model pre-trained on het-\nerogeneous data including scientific text, SMILES\nsequences, and \u201cwrapped\u201d sequences between\nSMILES and text. Due to the flexible input, we\ncan finetune it for various text and molecular tasks.\nThe framework of MolXPT is in Figure 1.\n2.1\nPre-training corpus\nFor scientific text, we use the titles and abstracts\nof 30M papers from PubMed1.\nFor molecular\nSMILES, we randomly choose 30M molecules\nfrom PubChem2 (Kim et al., 2022).\nThe wrapped sequences are constructed via a\n\u201cdetect and replace\u201d pipeline. We first use BERN2\n(Sung et al., 2022), a widely used named entity\nrecognition (NER) tool for biomedical purpose, to\ndetect all mentions of molecules and link them to\nthe entities in public knowledge bases like ChEBI\n1https://ftp.ncbi.nlm.nih.gov/pubmed/\n2https://pubchem.ncbi.nlm.nih.gov/\n(Hastings et al., 2016). After that, we can retrieve\nthe molecular SMILES of the matched entities. Fi-\nnally, we replace the molecular mentions to their\ncorresponding SMILES. An example is shown in\nthe left panel of Figure 1. The wrapped sequences\nmust contain at least one molecular SMILES. We\neventually obtain 8M wrapped sequences in total.\nText and SMILES are tokenized separately. For\ntext, we use byte-pair encoding (BPE) (Sennrich\net al., 2016) to split the words into subwords.\nThe number of BPE merge operation is 40k. For\nSMILES sequences (including those in wrapped\nsequences), we tokenize them with the regular ex-\npression from (Schwaller et al., 2018). For each\nSMILES sequence S, we add a start-of-molecule\ntoken \u27e8som\u27e9 at the beginning of S and append an\nend-of-molecule token \u27e8eom\u27e9 at the end of S.\n2.2\nModel and training\nModel architecture: MolXPT has the same archi-\ntecture as the GPT models (Radford et al., 2019).\nDue to computational resource limitation, in this\npaper, we follow the GPT-2medium configuration\nwith 24 layers, 1024 hidden size and 16 attention\nheads. The maximum length of input we can pro-\ncess is 2048 and the vocabulary size is 44536. In\ntotal, our model has 350M parameters.\nPre-training: The pre-training objective function\nof MolXPT is the negative log-likelihood. Math-\nematically, let D = {xi}i denote the collection\nof sequences of the three types of the data, and\nxi = (si,1, si,2, \u00b7 \u00b7 \u00b7 , si,ni) is the i-th sequence with\nni tokens. The training objective function is:\nmin \u2212 1\n|D|\n|D|\nX\ni=1\nni\nX\nj=1\nlog P(si,j|si,j\u22121, si,j\u22122, \u00b7 \u00b7 \u00b7 , s1).\nThe pre-training details are left in Appendix B.\nDataset\nBBBP\nTox21\nClinTox\nHIV\nBACE\nSIDER\nAvg\n#Molecules\n2039\n7831\n1478\n41127\n1513\n1478\nG-Contextual\n70.3 \u00b1 1.6\n75.2 \u00b1 0.3\n59.9 \u00b1 8.2\n75.9 \u00b1 0.9\n79.2 \u00b1 0.3\n58.4 \u00b1 0.6\n69.8\nG-Motif\n66.4 \u00b1 3.4\n73.2 \u00b1 0.8\n77.8 \u00b1 2.0\n73.8 \u00b1 1.4\n73.4 \u00b1 4.0\n60.6 \u00b1 1.1\n70.9\nGROVERbase\n70.0 \u00b1 0.1\n74.3 \u00b1 0.1\n81.2 \u00b1 3.0\n62.5 \u00b1 0.9\n82.6 \u00b1 0.7\n64.8 \u00b1 0.6\n72.6\nGROVERlarge\n69.5 \u00b1 0.1\n73.5 \u00b1 0.1\n76.2 \u00b1 3.7\n68.2 \u00b1 1.1\n81.0 \u00b1 1.4\n65.4 \u00b1 0.1\n72.3\nGraphMVP\n72.4 \u00b1 1.6\n75.9 \u00b1 0.5\n79.1 \u00b1 2.8\n77.0 \u00b1 1.2\n81.2 \u00b1 0.9\n63.9 \u00b1 1.2\n74.9\nMGSSL\n70.5 \u00b1 1.1\n76.5 \u00b1 0.3\n80.7 \u00b1 2.1\n79.5 \u00b1 1.1\n79.7 \u00b1 0.8\n61.8 \u00b1 0.8\n74.8\nGEM\n72.4 \u00b1 0.4\n78.1 \u00b1 0.1\n90.1 \u00b1 1.3\n80.6 \u00b1 0.9\n85.6 \u00b1 1.1\n67.2 \u00b1 0.4\n79.0\nKV-PLM\n74.6 \u00b1 0.9\n72.7 \u00b1 0.6\n\u2013\n74.0 \u00b1 1.2\n\u2013\n61.5 \u00b1 1.5\n\u2013\nGalactica\n66.1\n68.9\n82.6\n74.5\n61.7\n63.2\n69.5\nMoMu\n70.5 \u00b1 2.0\n75.6 \u00b1 0.3\n79.9 \u00b1 4.1\n76.2 \u00b1 0.9\n77.1 \u00b1 1.4\n60.5 \u00b1 0.9\n73.3\nMolXPT\n80.0 \u00b1 0.5\n77.1 \u00b1 0.2\n95.3 \u00b1 0.2\n78.1 \u00b1 0.4\n88.4 \u00b1 1.0\n71.7 \u00b1 0.2\n81.9\nTable 1: Results on MoleculeNet. The evaluation metric is ROC-AUC. Bold fonts indicate the best results.\nPrompt-based finetuning: MolXPT can be fine-\ntuned for downstream tasks about molecules and\ntext. Adding classification or regression heads to\npre-trained backbone models introduces the gap\nbetween pre-training and finetuning (Brown et al.,\n2020; Chen et al., 2022; Gu et al., 2022). There-\nfore, we adopt prompt-based finetuning (Gao et al.,\n2021) to unify different tasks into a sequence gener-\nation task, which is consistent with the pre-training\nobjective. Briefly, given a task, we convert the in-\nput and output into text and/or SMILES sequences,\nequip the sequences with task-specific prompts and\nfinetune using language modeling loss. Prompts\nfor MoleculeNet and text-molecule translation are\nintroduced in the Section 3.1 and 3.2 respectively.\nDiscussion: Some works also try to jointly model\ntext and molecules. Zeng et al. (2022) propose\nKV-PLM, where SMILES sequences are appended\nafter molecule names for pre-training. Su et al.\n(2022) use contrastive learning between text and\nmolecular graphs. Our MolXPT is a generative\nmodel while the above two models are not. Both of\nthem are built upon SciBERT (Beltagy et al., 2019),\na BERT model (Devlin et al., 2019) for scientific\nliterature. MolXPT is complementary to them.\n3\nExperiments\nWe evaluated MolXPT on two downstream tasks:\n(1) molecular property prediction on MoleculeNet\n(Wu et al., 2018), which is to predict whether the\ngiven molecule has specific properties; (2) the gen-\neration between text descriptions and molecules\n(Edwards et al., 2022), where both molecules and\ntext should be considered. In this section, we fo-\ncus on introducing task definition, prompt design\nand results while leaving the detailed finetuning\nhyper-parameters in Appendix C.\n3.1\nResults on MoleculeNet\nMoleculeNet (Wu et al., 2018) is a widely-used\nbenchmark for molecular modeling, which has\nmore than 700k compounds for various different\nproperties. We choose six molecular classification\ntasks for evaluation, which are BBBP, Tox21, Clin-\nTox, HIV, BACE and SIDER. Details are left in\nAppendix A. We follow GEM (Fang et al., 2022)\nto split the data into training/validation/test sets\nbased on the scaffold. For these tasks, the input is\na SMILES and the output is a binary label.\nFinetuning strategy: Previous molecular property\nprediction models mainly use SMILES sequences\nor molecular graphs as input, while we can use\nthe \u201cwrapped\u201d sequences. For example, one task\nis to predict the blood-brain barrier penetration\n(BBBP) of a molecule. Therefore, the prompt is\n\u201cWe can conclude that the BBB penetration of \u27e8som\u27e9\n\u27e8SMILES\u27e9 \u27e8eom\u27e9 is \u27e8tag\u27e9\u201d, where \u27e8SMILES\u27e9 denotes\nthe molecular SMILES, and \u27e8tag\u27e9 denotes the clas-\nsification result. For the BBBP task, we design\n\u27e8tag\u27e9 as \u201ctrue\u201d or \u201cfalse\u201d, indicating whether the\ncompound can or cannot cross BBB.\nDifferent tasks have different prompts (see Ap-\npendix C.1), but we put the tags to the last token of\nthe prompt for all tasks. Let (si,1, si,2, \u00b7 \u00b7 \u00b7 , si,Ti)\ndenote the i-th wrapped sequence for the down-\nstream task with Ti tokens, where si,Ti is the tag of\nthe sequence. Denote that there are N samples for\nfinetuning. The finetuning strategy could be either\nmin \u2212 1\nN\nN\nX\ni=1\nlog P(si,Ti|si,<Ti),\n(1)\nindicating that we finetune the tags only, or\nmin \u2212 1\nN\nN\nX\ni=1\n1\nTi\nTi\nX\nj=1\nlog P(si,j|si,<j),\n(2)\nindicating that we finetune the full prompts. Ac-\ncording to our exploration, Eqn.(1) achieves\nslightly better results and we use it for all tasks\n(see Appendix C.4 for the results).\nLet ptrue and pfalse denote the probabilities of\ntags \u201ctrue\u201d and \u201cfalse\u201d after encoding the pre-\nfix \u201cWe can conclude that the BBB penetration\nof \u27e8som\u27e9 \u27e8SMILES\u27e9 \u27e8eom\u27e9 is\u201d. The probabilities\nthat \u27e8SMILES\u27e9 can and cannot cross blood-brain\nbarrier are normalized as ptrue/(ptrue + pfalse) and\npfalse/(ptrue + pfalse) respectively. The finetuning\nhyper-parameters are in Appendix C.2.\nWe compare MolXPT with two types of base-\nlines:\n(1) pre-trained language model base-\nlines including KV-PLM (Zeng et al., 2022),\nGalactica (Taylor et al., 2022) and MoMu (Su\net al., 2022).\n(2) pre-trained Graph Neu-\nral Network (GNN) baselines including G-\nContextual (Rong et al., 2020), G-Motif (Rong\net al., 2020), GROVERbase (Rong et al., 2020),\nGROVERlarge (Rong et al., 2020), GraphMVP (Liu\net al., 2022), MGSSL (Zhang et al., 2021) and\nGEM (Fang et al., 2022). The evaluation metric is\nthe ROC-AUC score. The results are in Table 1.\nMolXPT outperforms the GNN baselines pre-\ntrained on pure molecular data, indicating the ef-\nfectiveness of pre-training with scientific text cor-\npus. Compared with Galactica which also uses\nboth SMILES and text for pre-training GPT-like\nmodel, MolXPT obtains better performance. Note\nthat Galactica does not purposely build and train\non the \u201cwrapped\u201d sequences, whose importance is\ndemonstrated via our empirical results. A possible\nexplanation of the superior performance is that the\nSMILES describes the component and structural\ninformation of molecules, while the text describes\nthe general properties. They are complementary to\neach other, and joint training on them brings more\neffective representations.\n3.2\nResults on text-molecule translation\nWe evaluated the performance of MolXPT on\nCheBI-20 (Edwards et al., 2021), a bidirectional\ntext-molecule translation dataset. It consists of\n33,010 molecule-description pairs. We use the data\nsplit provided by MolT5 (Edwards et al., 2022),\nwhere the training, validation and test sets account\n80%, 10% and 10% of total data. For molecule-to-\ntext generation, given a molecular SMILES S, the\nprompt is: \u201cThe description of \u27e8som\u27e9 S \u27e8eom\u27e9 is:\nThe molecule is\u201d, followed by the text description\nof S. For text-to-molecule generation, given a text\ndescription T, the prompt is: \u201cT. The compound\nis \u27e8som\u27e9\u201d, and the model will generate the molec-\nular SMILES ended with \u27e8eom\u27e9. We compare our\nmethod with MolT5 (Edwards et al., 2022).\nFor molecule-to-text generation, the results are\nevaluated by NLP metrics including BLEU (Pa-\npineni et al., 2002), Rouge (Lin, 2004) and ME-\nTEOR (Banerjee and Lavie, 2005). \u201cText2mol\u201d\nis a deep learning based metric proposed by Ed-\nwards et al. (2022) to measure the similarity of\nthe text-molecule pairs. For text-to-molecule gen-\neration, we evaluate the following metrics: the\nproportion of the generated SMILES that exactly\nmatch the reference SMILES (denoted as \u201cEx-\nact\u201d); the Tanimoto similarity of three types of\nfingerprints: MACCS (Durant et al., 2002), RDK\n(Schneider et al., 2015) and Morgan (Rogers and\nHahn, 2010); the FCD score (Preuer et al., 2018),\nwhich measures the molecule distances by a pre-\ntrained model; the percentage of the valid generated\nSMILES. The results are reported in Table 2.\nWe observe that MolXPT achieves signifi-\ncantly better performance than MolT5-small and\nMolT5-base, and has comparable performance with\nMolT5-large. Note that MolT5-large has 800M\nparameters while MolXPT only uses 44% of its\nparameters. For both tasks, our model performs the\nbest on Text2Mol metric, indicating that MolXPT\ncaptures the alignment between text and molecule\nbetter. We attribute it to the wrapped sequences,\nby which the model can learn the relation between\nmolecule and text explicitly.\nWe further verify the zero-shot text-to-molecule\ngeneration ability of MolXPT. The pre-trained\nMolXPT takes the text as input and directly gen-\nerates molecules without finetuning. The top-1\nand top-5 fingerprint similarity is in Table 3. In-\ndeed, compared with the full data setting, the per-\nformance drops, but still reasonable numbers. In\naddition, the zero-shot MolXPT successfully recov-\ners 33 molecules based on the text (see Appendix\nD).\n4\nConclusions and Future Work\nWe propose MolXPT, a generative model pre-\ntrained on scientific text, molecular SMILES and\nMolecule-to-text\nBLEU-2\nBLEU-4\nRouge-1\nRouge-2\nRouge-L\nMETEOR\nText2Mol\nMolT5-small (77M)\n0.519\n0.436\n0.620\n0.469\n0.563\n0.551\n0.540\nMolT5-base (250M)\n0.540\n0.457\n0.634\n0.485\n0.578\n0.569\n0.547\nMolT5-Large (800M)\n0.594\n0.508\n0.654\n0.510\n0.594\n0.614\n0.582\nMolXPT (350M)\n0.594\n0.505\n0.660\n0.511\n0.597\n0.626\n0.594\nText-to-molecule\nExact\u2191\nMACCS\u2191\nRDK\u2191\nMorgan\u2191\nFCD\u2193\nText2mol\u2191\nValidity\u2191\nMolT5-small\n0.079\n0.703\n0.568\n0.517\n2.49\n0.482\n0.721\nMolT5-medium\n0.081\n0.721\n0.588\n0.529\n2.18\n0.496\n0.772\nMolT5-large\n0.311\n0.834\n0.746\n0.684\n1.20\n0.554\n0.905\nMolXPT\n0.215\n0.859\n0.757\n0.667\n0.45\n0.578\n0.983\nTable 2: Results of molecule-to-text (top) and text-to-molecule generation (bottom). For FCD, the smaller, the better.\nFor the remaining metrics, the larger, the better. MolT5 results are from Table 1 and 2 of (Edwards et al., 2022).\nMolT5 parameters are from https://github.com/blender-nlp/MolT5. Bold fonts indicate the best results.\nMACCS\nRDK\nMorgan\nZero-shot (Top-1)\n0.540\n0.383\n0.228\nZero-shot (Top-5)\n0.580\n0.423\n0.423\nFull data (Top-1)\n0.841\n0.746\n0.660\nTable 3: Zero-shot text-to-molecule generation.\ntheir wrapped sequences.\nWe train a 24-layer\nMolXPT with 350M parameters. By prompt-based\nfinetuning, it improves strong baselines on Molecu-\nleNet and achieves comparable results with the best\nmodel on molecule-text translation but using much\nfewer parameters.\nFor future work, first, we will train larger\nMolXPT to further verify the performances across\ndifferent tasks and the zero-shot/in-context (Xie\net al., 2022) learning ability. Second, how to fur-\nther enhance the interaction between molecules\nand text (e.g., using contrastive learning to enhance\nconsistency) should be studied. Third, how to effec-\ntively adapt MolXPT into other molecule and text\ntasks such as text-guided molecule optimization is\nanother direction to explore.\nReferences\nViraj Bagal, Rishal Aggarwal, P. K. Vinod, and U. Deva\nPriyakumar. 2022. Molgpt: Molecular generation us-\ning a transformer-decoder model. Journal of Chemi-\ncal Information and Modeling, 62(9):2064\u20132076.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the ACL workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65\u201372.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615\u2013\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony\nLee, Chris Manning, and Percy Liang. 2022. Pub-\nMedGPT 2.7B.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nYulong Chen, Yang Liu, Li Dong, Shuohang Wang,\nChenguang Zhu, Michael Zeng, and Yue Zhang.\n2022.\nAdaprompt: Adaptive model training for\nprompt-based nlp. arXiv preprint arXiv:2202.04824.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJoseph L Durant, Burton A Leland, Douglas R Henry,\nand James G Nourse. 2002. Reoptimization of mdl\nkeys for use in drug discovery. Journal of chemi-\ncal information and computer sciences, 42(6):1273\u2013\n1280.\nCarl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and\nHeng Ji. 2022. Translation between molecules and\nnatural language. arXiv preprint arXiv:2204.11817.\nCarl Edwards, ChengXiang Zhai, and Heng Ji. 2021.\nText2mol: Cross-modal molecule retrieval with nat-\nural language queries. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 595\u2013607.\nXiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong\nHe, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua\nWu, and Haifeng Wang. 2022. Geometry-enhanced\nmolecular representation learning for property pre-\ndiction. Nature Machine Intelligence, 4(2):127\u2013134.\nMinghao Feng, Bingqing Tang, Steven H Liang, and\nXuefeng Jiang. 2016. Sulfur containing scaffolds\nin drugs: synthesis and application in medicinal\nchemistry. Current topics in medicinal chemistry,\n16(11):1200\u20131216.\nNoelia Ferruz, Steffen Schmidt, and Birte H\u00f6cker. 2022.\nProtgpt2 is a deep unsupervised language model for\nprotein design. Nature Communications, 13(1):4348.\nNathan Frey, Ryan Soklaski, Simon Axelrod, Siddharth\nSamsi, Rafael Gomez-Bombarelli, Connor Coley,\nand Vijay Gadepally. 2022. Neural scaling of deep\nchemical models. ChemRxiv.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816\u20133830.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2022. Ppt: Pre-trained prompt tuning for few-shot\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8410\u20138423.\nJanna Hastings, Gareth Owen, Adriano Dekker, Mar-\ncus Ennis, Namrata Kale, Venkatesh Muthukrishnan,\nSteve Turner, Neil Swainston, Pedro Mendes, and\nChristoph Steinbeck. 2016. Chebi in 2016: Improved\nservices and an expanding collection of metabolites.\nNucleic acids research, 44(D1):D1214\u2013D1219.\nDavid N Juurlink, Muhammad Mamdani, Alexander\nKopp, Andreas Laupacis, and Donald A Redelmeier.\n2003. Drug-drug interactions among elderly patients\nhospitalized for drug toxicity. Jama, 289(13):1652\u2013\n1658.\nSunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindu-\nlyte, Jia He, Siqian He, Qingliang Li, Benjamin A\nShoemaker, Paul A Thiessen, Bo Yu, Leonid Za-\nslavsky, Jian Zhang, and Evan E Bolton. 2022.\nPubChem 2023 update.\nNucleic Acids Research,\n51(D1):D1373\u2013D1380.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74\u201381.\nShengchao Liu, Hanchen Wang, Weiyang Liu, Joan\nLasenby, Hongyu Guo, and Jian Tang. 2022. Pre-\ntraining molecular graph representation with 3d ge-\nometry. In International Conference on Learning\nRepresentations.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBioGPT: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6).\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. Technical blog.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In ACL, pages 311\u2013318.\nKristina Preuer, Philipp Renz, Thomas Unterthiner,\nSepp Hochreiter, and Gunter Klambauer. 2018.\nFrechet chemnet distance: a metric for generative\nmodels for molecules in drug discovery. Journal\nof chemical information and modeling, 58(9):1736\u2013\n1741.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nDavid Rogers and Mathew Hahn. 2010.\nExtended-\nconnectivity fingerprints. Journal of chemical in-\nformation and modeling, 50(5):742\u2013754.\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie,\nYing Wei, Wenbing Huang, and Junzhou Huang.\n2020. Self-supervised graph transformer on large-\nscale molecular data. Advances in Neural Informa-\ntion Processing Systems, 33:12559\u201312571.\nNadine Schneider, Roger A Sayle, and Gregory A Lan-\ndrum. 2015. Get your atoms in order: An open-\nsource implementation of a novel and robust molecu-\nlar canonicalization algorithm. Journal of chemical\ninformation and modeling, 55(10):2111\u20132120.\nPhilippe Schwaller, Theophile Gaudin, David Lanyi,\nCostas Bekas, and Teodoro Laino. 2018. \u201cfound\nin translation\u201d: predicting outcomes of complex or-\nganic chemistry reactions using neural sequence-to-\nsequence models. Chemical science, 9(28):6091\u2013\n6098.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715\u20131725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2022. Large language models encode clinical\nknowledge. arXiv preprint arXiv:2212.13138.\nBing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiang-\nmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-\nRong Wen. 2022. A molecular multimodal founda-\ntion model associating molecule graphs with natural\nlanguage. arXiv preprint arXiv:2209.05481.\nMujeen\nSung,\nMinbyul\nJeong,\nYonghwa\nChoi,\nDonghyeon Kim, Jinhyuk Lee, and Jaewoo Kang.\n2022. Bern2: an advanced neural biomedical named\nentity recognition and normalization tool.\narXiv\npreprint arXiv:2201.02080.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nXiaochu Tong, Xiaohong Liu, Xiaoqin Tan, Xutong\nLi, Jiaxin Jiang, Zhaoping Xiong, Tingyang Xu,\nHualiang Jiang, Nan Qiao, and Mingyue Zheng. 2021.\nGenerative models for de novo drug design. Journal\nof Medicinal Chemistry, 64(19):14011\u201314027.\nDavid Weininger. 1988. Smiles, a chemical language\nand information system. 1. introduction to methodol-\nogy and encoding rules. Journal of chemical infor-\nmation and computer sciences, 28(1):31\u201336.\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg,\nJoseph Gomes, Caleb Geniesse, Aneesh S Pappu,\nKarl Leswing, and Vijay Pande. 2018. Moleculenet:\na benchmark for molecular machine learning. Chem-\nical science, 9(2):513\u2013530.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In Interna-\ntional Conference on Learning Representations.\nZheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\n2022. A deep-learning system bridging molecule\nstructure and biomedical text with comprehension\ncomparable to human professionals. Nature Commu-\nnications, 13(1):862.\nZaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and\nChee-Kong Lee. 2021.\nMotif-based graph self-\nsupervised learning for molecular property predic-\ntion. Advances in Neural Information Processing\nSystems, 34:15870\u201315882.\nAppendix\nA\nDatasets and Baselines of MoleculeNet\nWe choose the following tasks of MoleculeNet for\nevaluation:\n(1) BBBP contains compounds with binary labels\non blood-brain barrier penetration.\n(2) Tox21 is a dataset for predicting the human\ntoxicity of compounds on 12 different targets.\n(3) ClinTox contains drugs approved by the FDA\nand those that have failed clinical trials for toxicity\nreasons.\n(4) HIV aims to predict whether a drug can inhibit\nHIV replication.\n(5) BACE describes binding results for a set of\ninhibitors of human \u03b2-secretase 1.\n(6) SIDER has compounds used in marketed\nmedicines with 27 categories of side effects.\nWe compare MolXPT with the following baselines:\n(1) GROVER is a self-supervised pre-trained graph\nTransformer model. G-Contextual and G-Motif\nare two variants of it pre-trained with contextual\nproperty prediction task and motif prediction task.\n(2) GraphMVP is a self-supervised pre-trained\nGNN model using both 2D topological structures\nand 3D geometric views of molecules.\n(3) MGSSL leverages a retrosynthesis-based algo-\nrithm BRICS and additional rules to find the motifs\nand combines motif layers with atom layers.\n(4) GEM is a geometry-enhanced pre-trained GNN\nmodel.\n(5) Galactica is a GPT-like model trained on a large\nscientific corpus and many natural sequences like\nSMILES. We report the result of Galactica-120B.\n(6) KV-PLM is a BERT-like model where SMILES\nsequences are appended after molecule names for\npre-training.\n(7) MoMu uses contrastive learning to jointly pre-\ntrain a BERT model for text and a GNN model for\nmolecules.\nB\nPre-training hyper-parameters\nMolXPT is pre-trained for 200k steps on eight\nA100 GPUs. The batchsize is 2048 tokens per\nGPU. The gradients are accumulated for 16 steps\nbefore updating. We use Adam (Kingma and Ba,\n2015) optimizer for optimization. The peak learn-\ning rate is 0.0005 and the warm-up steps are 20000.\nThe learning rate scheduler is inverse square root\ndecay scheduler. The dropout is 0.1.\nC\nFinetuning details of downstream tasks\nC.1\nPrompts for finetuning MoleculeNet\n(1) BBBP: \u201cWe can conclude that the BBB penetra-\ntion of \u27e8som\u27e9 \u27e8SMILES\u27e9 \u27e8eom\u27e9 is true/false.\u201d\n(2) Tox21:\n\u201cWe can conclude that the \u27e8som\u27e9\n\u27e8SMILES\u27e9 \u27e8eom\u27e9 activity outcome on \u27e8target\u27e9 is\nactive/inactive. \u201d where \u27e8target\u27e9 refers to corre-\nsponding receptor or enzyme for each subtask, e.g.\nthe \u27e8target\u27e9 of subtask \"AR\" is \"Androgen Recep-\ntor\".\n(3) ClinTox:\u201cWe can conclude that the clinical trial\ntoxicity of \u27e8som\u27e9 \u27e8SMILES\u27e9 \u27e8eom\u27e9 is true/false.\u201d for\nsubtask CT_TOX and \u201cWe can conclude that the\nFDA approval status of \u27e8som\u27e9 \u27e8SMILES\u27e9 \u27e8eom\u27e9 is\ntrue/false.\u201d for subtask FDA_APPROVED.\n(4) HIV: \u201cWe can conclude that the screening re-\nsult of ability to inhibit HIV replication of \u27e8som\u27e9\n\u27e8SMILES\u27e9 \u27e8eom\u27e9 is active/inactive.\u201d\n(5) BACE: \u201cWe can conclude that the binding result\non beta-secretase 1 of \u27e8som\u27e9 \u27e8SMILES\u27e9 \u27e8eom\u27e9 is\ntrue/false.\u201d\n(6) SIDER:\u201cWe can conclude that the \u27e8som\u27e9\n\u27e8SMILES\u27e9 \u27e8eom\u27e9 can bring about the side effect\nof \u27e8side-effect\u27e9 is true/false.\u201d where \u27e8side-effect\u27e9\nrefers to corresponding side-effect for each subtask.\nC.2\nDetails of finetuning MoleculeNet\nWe grid search the following hyper-parameters:\nlearning rate in {3 \u00d7 10\u22125, 5 \u00d7 10\u22125}; dropout in\n{0.1, 0.3}; total epochs from {30, 50}. The model\nis selected according to validation performance.\nC.3\nDetails of finetuning text-molecule\ngeneration\nFor text-molecule generation, MolXPT is finetuned\nfor 100 steps on one P40 GPU with 1024 tokens\nand 16 accumulated steps per device. Models are\nfinetuned for 100 epochs. The learning rate is\n0.0001 and the dropout rate is grid searched from\n[0.1, 0.2, 0.3, 0.4, 0.5]. Setting dropout rate as 0.4\nand 0.5 achieves the best validation performance on\nmolecule-to-text generation and text-to-molecule\ngeneration respectively. We use the corresponding\nmodels for testing.\nC.4\nMoleculeNet finetuning strategy selection\nWe provide two finetune strategies in Eqn.(1) and\nEqn.(2). Their results are reported in Table 4. Their\nresults are similar and Eqn.(1) is slightly better.\nD\nZero-shot text-to-molecule generation\nGiven K generated molecule \u02c6m1, \u02c6m2, \u00b7 \u00b7 \u00b7 , \u02c6mK\nand the reference molecule m, the top-K finger-\nprint similarity is\nmax\ni\u2208[K] similarity(m, \u02c6mi).\n(3)\nMolXPT generates 33 molecules that can exactly\nmatch the reference molecules without finetuning.\nFigure 2 shows three of the cases.\nThe molecule is a sesquiterpene lactone and active principle of \nFeverfew (Tanacetum parthenium). It has a role as a non-\nsteroidal anti-inflammatory drug, a non-narcotic analgesic, a \nperipheral nervous system drug, an inhibitor and a drug allergen.\nThe molecule is the (R)-enantiomer of mevalonic acid. It is a \nconjugate acid of a (R)-mevalonate. It is an enantiomer of a (S)-\nmevalonic acid.\nThe molecule is a bile acid taurine conjugate of ursocholic acid. \nIt has a role as a human metabolite and a rat metabolite. It \nderives from an ursocholic acid. It is a conjugate acid of a \ntauroursocholate.\nInput text\nGenerated molecule\nFigure 2: Examples for zero-shot text-to-molecule generation. We randomly pick up three cases that MolXPT can\nsuccessfully generate the reference molecules without finetuning.\nDataset\nBBBP\nTox21\nClinTox\nHIV\nBACE\nSIDER\nAvg\nDevfull prompt\n98.8 \u00b1 0.2\n78.8 \u00b1 0.1\n98.8 \u00b1 0.1\n82.9 \u00b1 1.0\n78.4 \u00b1 0.3\n67.7 \u00b1 0.7\n84.2\nDevtags only\n98.9 \u00b1 0.3\n78.8 \u00b1 0.2\n97.7 \u00b1 0.1\n85.3 \u00b1 0.2\n75.8 \u00b1 0.8\n69.4 \u00b1 0.6\n84.3\nTestfull prompt\n78.1 \u00b1 0.4\n77.2 \u00b1 0.1\n93.4 \u00b1 0.1\n78.1 \u00b1 0.9\n87.9 \u00b1 0.3\n70.0 \u00b1 0.2\n80.8\nTesttags only\n80.0 \u00b1 0.5\n77.1 \u00b1 0.2\n95.3 \u00b1 0.2\n78.1 \u00b1 0.4\n88.4 \u00b1 1.0\n71.7 \u00b1 0.2\n81.9\nTable 4: Comparison of different finetuning strategies on MoleculeNet. \u201cDev\u201d and \u201cTest\u201d denote validation set and\ntest set respectively. Subscripts represent finetuning full prompts (Eqn.(2)) or tags only respectively (Eqn.(1)). The\nevaluation metric is ROC-AUC.\n"
  },
  {
    "title": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2305.10874.pdf",
    "upvote": "1",
    "text": "VideoFactory: Swap Attention in Spatiotemporal\nDiffusions for Text-to-Video Generation\nWenjing Wang1\nHuan Yang2\u2217\nZixi Tuo2\nHuiguo He2\nJunchen Zhu2\nJianlong Fu2\u2217\nJiaying Liu1\n1Wangxuan Institute of Computer Technology, Peking University, 2Microsoft Research\n{daooshee, liujiaying}@pku.edu.cn\n{huayan, v-zixituo, v-huiguohe, v-junchenzhu, jianf}@microsoft.com\nAbstract\nWe present VideoFactory, an innovative framework for generating high-\nquality open-domain videos. VideoFactory excels in producing high-definition\n(1376\u00d7768), widescreen (16:9) videos without watermarks, creating an engaging\nuser experience. Generating videos guided by text instructions poses significant\nchallenges, such as modeling the complex relationship between space and time, and\nthe lack of large-scale text-video paired data. Previous approaches extend pretrained\ntext-to-image generation models by adding temporal 1D convolution/attention mod-\nules for video generation. However, these approaches overlook the importance\nof jointly modeling space and time, inevitably leading to temporal distortions\nand misalignment between texts and videos. In this paper, we propose a novel\napproach that strengthens the interaction between spatial and temporal perceptions.\nIn particular, we utilize a swapped cross-attention mechanism in 3D windows that\nalternates the \u201cquery\u201d role between spatial and temporal blocks, enabling mutual\nreinforcement for each other. To fully unlock model capabilities for high-quality\nvideo generation, we curate a large-scale video dataset called HD-VG-130M. This\ndataset comprises 130 million text-video pairs from the open-domain, ensuring\nhigh-definition, widescreen and watermark-free characters. Objective metrics and\nuser studies demonstrate the superiority of our approach in terms of per-frame\nquality, temporal correlation, and text-video alignment, with clear margins.\n1\nIntroduction\nAutomated video production is experiencing a surge in demand across various industries, including\nmedia, gaming, film, and television [19, 32]. This increased demand has propelled video generation\nresearch to the forefront of deep generative modeling, leading to rapid advancements in the field [17,\n31, 45, 50, 55]. In recent years, diffusion models [16] have demonstrated remarkable success\nin generating visually appealing images in open-domains [42, 44]. Notably, some commercial\napplications have leveraged these advanced techniques to create engaging and imaginative pictures,\nsuch as using the text of \u201cA Chinese girl\u2019s wedding, 1980s, China,\u201d or \u201cAlbert Einstein eating\nvegetables, cow in the background.\u201d Building upon such success, in this paper, we take one step\nfurther and aim to extend their capabilities to high-quality text-to-video generation.\nAs is widely known, the development of open-domain text-to-video models poses grand challenges,\ndue to the limited availability of large-scale text-video paired data and the complexity of constructing\nspace-time models from scratch. To solve the challenges, current approaches are primarily built\non pretrained image generation models. These approaches typically adopt space-time separable\narchitectures, where spatial operations are inherited from the image generation model [17, 18]. To\nfurther incorporate temporal modeling, various strategies have been employed, including pseudo-3D\nmodules [46, 74], serial 2D and 1D blocks [44], and parameter-free techniques like temporal shift [1]\n* Corresponding authors: Huan Yang, Jianlong Fu\narXiv:2305.10874v2  [cs.CV]  12 Jun 2023\nSpatial\nTemporal\nSpatial\nTemporal\nSpatial\nTemporal\n(a) 3D Spatial-Temporal SA\n(b) 3D Spatial-Temporal WSA\n(c) 2D Spatial SA + 1D Temporal SA\n(d) Swapped Spatial-Temporal WCA (Ours)\nSpatial\nTemporal\nKey/Value Attention Region\nQuery Token\nSpatial\nTemporal\nDot Product\nor\nSpatial\nTemporal\nFigure 1: The paradigm of Swapped Spatiotemporal Cross-Attention (Swap-CA) in comparison\nwith existing video attention schemes. Instead of only conducting self-attention in (a)-(c), we\nperform cross-attention between spatial and temporal modules in a U-Net, which encourages more\nspatiotemporal mutual reinforcement.\nor tailored spatiotemporal attention [61]. However, these approaches overlook the crucial interplay\nbetween time and space for visually engaging text-to-video generation. On one hand, parameter-free\napproaches [1, 61] rely on manually designed rules that fail to capture the intrinsic nature of videos\nand often lead to the generation of unnatural motions. On the other hand, learnable 2D+1D modules\nand blocks [5, 46, 74] primarily focus on temporal modeling, either directly feeding temporal features\nto spatial features, or combining them through simplistic element-wise additions. This limited\ninteractivity usually results in temporal distortions and discrepancies between the input texts and the\ngenerated videos, which hinders the overall quality and coherence of the generated content.\nTo address the above issues, we take one step further in this paper which highlights the complementary\nnature of both spatial and temporal features in videos. Specifically, we propose a novel Swapped\nSpatiotemporal Cross-Attention (Swap-CA) for text-to-video generation. Instead of solely relying\non separable 2D+1D self-attention [4] that replaces computationally expensive 3D self-attention as\nshown in Fig. 1 (a) and (c), we aim to further enhance the interaction between spatial and temporal\nfeatures. While 3D window self-attention [25] reduces the computational cost and incorporates both\nmodalities, such work treats space and time dimensions indiscriminately, which largely limits its\nability to capture complex spatiotemporal patterns, especially in generation tasks. Compared with\nexisting works, our swap attention mechanism facilitates bidirectional guidance between spatial and\ntemporal features by considering one feature as the query and the other as the key/value. To ensure\nthe reciprocity of information flow, we swap the role of the \"query\" in adjacent layers.\nBy deeply interplaying spatial and temporal features through the proposed swap attention, we present\na holistic VideoFactory framework for text-to-video generation. In particular, we adopt the latent\ndiffusion framework and design a spatiotemporal U-Net for 3D noise prediction. To unlock the full\npotential of the proposed model and fulfill high-quality video generation, we propose to construct\na large-scale video generation dataset, named HD-VG-130M. This dataset consists of 130 million\ntext-video pairs from open-domains, encompassing high-definition, widescreen, and watermark-free\ncharacters. Additionally, our spatial super-resolution model can effectively upsample videos to a\nresolution of 1376 \u00d7 768, thus ensuring engaging visual experience. We conduct comprehensive\nexperiments and show that our approach outperforms existing methods in terms of both quantitative\nand qualitative comparisons. In summary, our paper makes the following significant contributions:\n\u2022 We reveal the significance of learning joint spatial and temporal features for video generation,\nand introduce a novel swapped spatiotemporal cross-attention mechanism to reinforce both\nspace and time interactions.\n\u2022 To facilitate training, we curate a comprehensive video dataset comprising the largest 130\nmillion text-video pairs to-date, which can support high-quality video generation with\nhigh-definition, widescreen, and watermark-free characters.\nBy effectively enforcing the mutual learning of spatial and temporal representations, our approach\nachieves outstanding visual quality in text-to-video generation tasks, while ensuring precisely seman-\ntic alignment between the input text and the generated videos.\n2\n2\nRelated Works\nText-to-Image Generation. Generating realistic images from corresponding descriptions combines\nthe challenging components of language modeling and image generation. Traditional text-to-image\ngeneration methods [30, 41, 63, 72, 27] are mainly based on GANs [10] and are only able to model\nsimple scenes such as birds [56]. Later work extends the scope of text-to-image generation to open\ndomains with better modeling techniques and training data on much larger scales. DALL\u00b7E [40]\nand CogView [7] leverage auto-regressive vision transformers with variational auto-encoders and\njointly train on text and image tokens. In recent years, diffusion models have shown great ability in\nvisual generation [6]. For text-to-image multi-modality generation, GLIDE [34], DALL\u00b7E 2 [39],\nand Imagen [44] leverage diffusion models to achieve impressive results. Based on these successes,\nsome work further extends customization [43, 28], image guidance [66], and precise control [3].\nDespite advances in generation ability, diffusion models are computationally expensive for training\nand inference, especially on high resolutions. To reduce the cost, latent diffusion [42] conducts the\ndiffusion process on a compressed latent space rather than the original pixel space. This paper further\nexplores how to extend the high-efficient latent diffusion for video generation.\nText-to-Video Generation. Additional controls are often added to make the generated videos more\nresponsive to demand [31, 35, 57, 12], and this paper focuses on the controlling mode of texts. Early\ntext-to-video generation models [22, 35] mainly use convolutional GAN models with Recurrent\nNeural Networks (RNNs) to model temporal motions. Although complex architectures and auxiliary\nlosses are introduced, GAN-based models cannot generate videos beyond simple scenes like moving\ndigits and close-up actions. Recent works extend text-to-video to open domains with large-scale\ntransformers [69] or diffusion models [15]. Considering the difficulty of high-dimensional video\nmodeling and the scarcity of text-video datasets, training text-to-video generation from scratch is\nunaffordable. As a result, most works acquire knowledge from pretrained text-to-image models.\nCogVideo [18] inherits from a pretrained text-to-image model CogView2 [8]. Imagen Video [15]\nand Phenaki [54] adopt joint image-video training. Make-A-Video [46] learns motion on video data\nalone, eliminating the dependency on text-video data. To reduce the high cost of video generation,\nlatent diffusion has been widely utilized for video generation [1, 5, 9, 13, 14, 20, 29, 60, 61, 70, 74].\nMagicVideo [74] inserts a simple adaptor after the 2D convolution layer. Latent-Shift [1] adopts a\nparameter-free temporal shift module to exchange information across different frames. PDVM [70]\nprojects the 3D video latent into three 2D image-like latent spaces. Although the research on text-\nto-video generation is very active, existing research ignores the inter and inner correlation between\nspatial and temporal modules. In this paper, we revisit the design of text-driven video generation.\n3\nHigh-Definition Video Generation Dataset\nDatasets of diverse text-video pairs are the prerequisite for training open-domain text-to-video\ngeneration models. However, existing text-video datasets are always limited in either scale or\nquality, thus hindering the upper bound of high-quality video generation. Referring to Tab. 1,\nMSR-VTT [62] and UCF101 [48] only have 10K and 13K video clips respectively. Although large\nin scale, HowTo100M [33] is specified for instructional videos, which has limited diversity for\nopen-domain generation tasks. Despite being appropriate in both scale and domain, the formats\nof textual annotations in HD-VILA-100M [64] are subtitle transcripts, which lack visual contents\nrelated descriptions for high-quality video generation. Additionally, the videos in HD-VILA-100M\nhave complex scene transitions, which are disadvantageous for models to learn temporal correlations.\nWebVid-10M [2] has been used in some previous video generation works [15, 46], considering its\nrelatively large-scale (10M) and descriptive captions. Nevertheless, videos in WebVid-10M are of\nlow resolution and have poor visual qualities with watermarks in the center.\nTo tackle the problems above and achieve high-quality video generation, we propose a large-scale\ntext-video dataset, namely HD-VG-130M, including 130M text-video pairs from open-domain in\nhigh-definition (720p), widescreen and watermark-free formats. We first sample according to the\nvideo labels of HD-VILA-100M [64] to collect original high-definition videos from YouTube. As\nthe original videos have complex scene transitions which are adverse for models to learn temporal\ncorrelations, we then detect and split scenes in these original videos using PySceneDetect1, resulting\nin 130M single scene video clips. Finally, we caption video clips with BLIP-2 [21], in view of its\n1We use the open source video analysis tool: https://github.com/Breakthrough/PySceneDetect\n3\nTable 1: Comparison of different video datasets. Existing text-video datasets are always limited in\neither scale or quality, while our HD-VG-130M includes 130M text-video pairs from open-domain in\nhigh-definition, widescreen and watermark-free formats.\nDataset\nVideo clips\nResolution\nDomain\nText\nWatermark-free\nMSR-VTT [62]\n10K\n240p\nopen\ncaption\n\u2713\nUCF101 [48]\n13K\n240p\nhuman action\nclass label\n\u2713\nHowTo100M [33]\n136M\n240p\ninstructional\nsubtitle\n\u2713\nHD-VILA-100M [64]\n103M\n720p\nopen\nsubtitle\n\u2713\nWebVid-10M [2]\n10M\n360p\nopen\ncaption\n\u2717\nHD-VG-130M (Ours)\n130M\n720p\nopen\ncaption\n\u2713\n4.8%7.1%\n11.8%\n12.7%\n6.7%\n7.6%\n8.0%\n3.0%\n38.3%\nVideo Categories\nTravel\nVehicles\nAnimation\nEntertainment\nScience\nHowto\nSports\nAnimals\nOthers 7 Categories\n39.4%\n19.8%\n20.0%\n14.4%\n4.1%2.3%\nClip Durations\n0~2s\n2~4s\n4~8s\n8~14s\n14~20s\n>20s\n5.8%\n42.9%\n37.9%\n13.4%\nCaption Lengths\n<7\n7~9\n9~11\n>11\nFigure 2: Statistics of video categories, clip durations, and caption word lengths in HD-VG-130M.\nHD-VG-130M covers a wide range of video categories.\nlarge vision-language pre-training knowledge. To be specific, we extract the central frame in each clip\nas the keyframe, and get the annotation for each clip by captioning the keyframe with BLIP-2 [21].\nNote that the video clips in HD-VG-130M are in single scenes, which ensures that the keyframe\ncaptions are representative enough to describe the content of the whole clips in most circumstances.\nThe statistics of HD-VG-130M are shown in Fig. 2. The videos in HD-VG-130M cover 15 categories.\nThe wide range of domains is beneficial for training the models to generate diverse content. After\nscene detection, the video clips are mostly in single scenes with duration less than 20 seconds. The\ntextual annotations are visual contents related to descriptive captions, which are mostly around 10\nwords. Text-video examples of our HD-VG-130M can be found in the supplementary.\n4\nHigh-Quality Text-to-Video Generation\nTo enable spatiotemporal interaction, we design a diffusion model for high-quality video generation.\nSpatiotemporal Inter-Connection. To reduce computational costs and leverage pretrained image\ngeneration models, space-time separable architectures have gained popularity in text-to-video gen-\neration [17, 18]. These architectures handle spatial operations independently on each frame, while\ntemporal operations consider multiple frames for each spatial position. In the following, we refer to\nthe features predicted by 2D/spatial modules in space-time separable networks as \"spatial features\",\nand \u201ctemporal features\u201d vice versa. As discussed in Sec. 1, prior works have neglected the crucial\ninteraction between spatial and temporal features. To tackle this limitation, we promote the mutual\nreinforcement of these features through a series of cross-attention operations.\nDenote a basic operation CrossAttention(x, y) = softmax( QKT\n\u221a\nd ) \u00b7 V , with\nQ = W (i)\nQ \u00b7 x, K = W (i)\nK \u00b7 y, V = W (i)\nV\n\u00b7 y,\n(1)\nwhere W (i)\nQ , W (i)\nK , and W (i)\nV\nare learnable projection matrices in the i-th layer. The direction of\ncross-attention, specifically whether Q originates from spatial or temporal features, plays a decisive\nrole in determining the impact of cross-attention. In general, spatial features tend to encompass a\ngreater amount of contextual information, which can improve the alignment of temporal features\nwith the input text. On the other hand, temporal features have a complete receptive field of the time\nseries, which may enable spatial features to generate visual content more effectively. To leverage\n4\nUNet Blk\nUNet Blk\nUNet Blk\nUNet Blk\nUNet Blk\nUNet Blk\nUNet Blk\nUNet Blk\nVideo Encoder\nVideo Decoder\nSpatial\nTemporal\nBlock\n2D Spatial\nBlock\n1D Temporal\nBlock\nSpatial\nTemporal\nBlock\n...\nSpatial\nTemporal\nBlock\n2D Spatial\nBlock\n1D Temporal\nBlock\nSpatial\nTemporal\nBlock\n...\nKey / Value\nQuery\nKey / Value\nQuery\nThe First UNet Block\nThe Second UNet Block\nUNet Blk\nScale 1\nScale 2\nScale 3\nScale 4\nUNet Block with Swapped Spatiotemporal Cross-Attention\nSpatial/Temporal Block\nUNet Block\nVideo Enc./Dec.\nQuery Token\nKey/Value Attention Region\nSpatial/Temporal Feature\nDot Product\nUpsample\nVideo Timestep t\nVideo Timestep t-1\nFigure 3: An illustration of our video diffusion model incorporating Swapped Spatiotemporal Cross-\nAttention (Swap-CA). At the end of each U-Net block, we employ a swapped cross-attention scheme\non 3D windows to facilitate a comprehensive integration of spatial and temporal features. In the case\nof two consecutive blocks, the first block employs temporal features to guide spatial features, while\nin the second block, their roles are reversed. This reciprocal arrangement ensures a balanced and\nmutually beneficial interaction between the spatial and temporal modalities throughout the model.\nboth aspects effectively, we propose a strategy of swapping the roles of Q and K, V in adjacent two\nblocks. This approach ensures that both temporal and spatial features receive sufficient information\nfrom the other modality, enabling a comprehensive and mutually beneficial interaction.\nGlobal attention greatly increases the computational costs in terms of memory and running time.\nTo improve efficiency, we conduct 3D window attention. Given a video feature in the shape of\nF \u00d7 H \u00d7 W and a 3D window size of Fw \u00d7 Hw \u00d7 Ww, we organize the windows to process the\nfeature in a non-overlapping manner, leading to \u2308 F\nFw \u2309 \u00d7 \u2308 H\nHw \u2309 \u00d7 \u2308 W\nWw \u2309 distinct 3D windows. Within\neach window, we perform spatiotemporal cross-attention. By adopting the 3D window scheme, we\neffectively reduce computational costs without compromising performance.\nFollowing prior text-to-image arts [5, 42], we incorporate 2\u00d7 down/upsampling along the spatial\ndimension to establish a hierarchical structure. Furthermore, research [11, 36] has pointed out that the\ntemporal dimension is sensitive to compression. In light of these considerations, we do compress the\ntemporal dimension and conduct shift windows [25], which advocates an inductive bias of locality.\nOn the spatial dimension, we do not shift since the down/upsampling already introduces connections\nbetween neighboring non-overlapping 3D windows.\nTo this end, we propose a Swapped Spatiotemporal Cross-Attention (Swap-CA) in 3D windows. Let\ntl and sl represent the predictions of 2D and 1D modules. We utilize Multi-head Cross Attention\n(MCA) to compute their interactions by Swap-CA as\n\u02dcsl = Projl\nin \u2299 GN(sl), \u02dctl = Projl\nin \u2299 GN(tl);\nhl = 3DW-MCA(LN(\u02dcsl), LN(\u02dctl)) + \u02dcsl;\n\u00afhl = FFN \u2299 LN(hl) + hl;\nzl = tl + sl + Swap-CA(sl, tl) = tl + sl + Projl\nout(\u00afhl),\n(2)\nwhere GN, Proj, LN, 3D Window-based Multi-head Cross-Attention (3DW-MCA) are learnable\nmodules. By initializing the output projection Projl\u22121\nout by zero, we have zl = tl\u22121 + sl\u22121, i.e.,\nSwap-CA is skipped so that it is reduced to a basic addition operation. This allows us to initially\ntrain the diffusion model using addition operations, significantly speeding up the training process.\nSubsequently, we can switch to Swap-CA to enhance the model\u2019s performance.\nThen for the next spatial-temporal separable block, we apply shifted 3D window multi-head cross-\nattention (3DSW-MCA) and interchange the roles of s and t, as\nhl+1 = 3DSW-MCA(LN(\u02dctl+1), LN(\u02dcsl+1)) + \u02dctl+1.\n(3)\nIn all 3DSW-MCA, we shift the window along the temporal dimension by \u2308 Fw\n2 \u2309 elements.\n5\nTable 2: Ablation study on spatiotemporal interaction strategies. We report the FVD [52] and\nCLIPSIM [38] on 1K samples from the validation set of WebVid-10M [2]. The computational cost is\nevaluated on inputs of shape 4 \u00d7 16 \u00d7 32 \u00d7 32. Details can be found in the supplementary material.\nT and S represent spatial and temporal features, respectively.\nAttention\nQ\nK, V\nParam. (G)\nMem. (GB)\nTime (ms)\nFVD \u2193\nCLIPSIM \u2191\n-\n-\n-\n1.480\n9.37\n135.35\n566.16\n0.3070\nT\nS\n1.601\n22.96\n202.12\n555.35\n0.3091\nGlobal\nS\nT\n1.601\n22.96\n205.00\n496.25\n0.3073\nSwapped\n1.601\n22.96\n201.51\n485.86\n0.3092\nT\nS\n1.601\n9.83\n150.49\n563.12\n0.3086\n3D Window\nS\nT\n1.601\n9.83\n149.93\n490.60\n0.3076\nSwapped\n1.601\n9.83\n148.24\n475.09\n0.3107\nOverall Architecture. We adopt LDM [42] as the text-to-image backbone. We employ an auto-\nencoder to compress the video into a down-sampled 3D latent space. Within this latent space, we\nperform diffusion optimization using an hourglass spatial-temporal separable U-Net model. Text\nfeatures are extracted with a pretrained CLIP [38] model and inserted into the U-Net model through\ncross-attention on the spatial dimension.\nOur framework is illustrated in Fig. 3. To strike a balance between performance and efficiency,\nwe exclusively apply Swap-CA at the end of each U-Net encoder and decoder block. In other\npositions, we employ a straightforward fusion technique using a 1\u00d71\u00d71 convolution to combine\nspatial and temporal features. To enhance the connectivity among temporal modules, we introduce\nskip connections that connect temporal modules separated by spatial down/upsampling modules.\nThis strategy promotes stronger integration and information flow within the temporal dimension of\nthe network architecture.\nSuper-Resolution Towards Higher Quality. To obtain visually satisfying results, we further perform\nSuper-Resolution (SR) on the generated video. One key to improving SR performance is designing a\ndegradation model that closely resembles the actual degradation process [51, 68, 24, 37, 23, 67]. In\nour scenario, the generated video quality suffers from both the diffusion and auto-encoder processes.\nTherefore, we adopt the hybrid degradation model in Real-ESRGAN [58] to simulate possible\nquality degradation caused by the generated process. During training, an original video frame is\ndownsampled and degraded using our model, and the SR network attempts to perform SR on the\nresulting low-resolution image. We adopt RCAN [73] with 8 residual blocks as our SR network.\nIt is trained with a vanilla GAN [10] to improve visual satisfaction. With a suitable degradation\ndesign, our SR network can further reduce possible artifacts and distortion in the frames, increase\ntheir resolution, and improve their visual quality.\n5\nExperiments\n5.1\nImplementation Details\nOur model predicts images at a resolution of 344\u00d7192 (with a latent space resolution of 43\u00d724).\nThen a 4\u00d7upscaling is produced in our SR model, resulting in a final output resolution of 1376 \u00d7 768.\nOur model is trained with 32 NVIDIA V100 GPUs. We utilize our HD-VG-130M as training data\nto promote the generation visual qualities. Furthermore, considering that the textual captions in\nHD-VG-130M are annotated by BLIP-2 [21], which may have some discrepancies with human\nexpressions, we adopt a joint training strategy with WebVid-10M [2] to ensure the model could\ngeneralize well to diverse humanity textual inputs. This approach allows us to benefit from the\nlarge-scale text-video pairs and the superior visual qualities of HD-VG-130M while maintaining\nthe generalization ability to diverse textual inputs in real scenarios, enhancing the overall training\nprocess. More details can be found in the supplementary.\n6\nTable 3: Text-to-video generation on UCF101.\nMethod\nZero-shot\nFVD\u2193\nVideoGPT [65]\nNo\n2880.6\nMoCoGAN [50]\nNo\n2886.8\n+StyleGAN2 [53]\nNo\n1821.4\nMoCoGAN-HD [49]\nNo\n1729.6\nDIGAN [71]\nNo\n1630.2\nStyleGAN-V [47]\nNo\n1431.0\nPVDM [70]\nNo\n343.6\nCogVideo [18]\nYes\n701.6\nMagicVideo [74]\nYes\n699.0\nLVDM [13]\nYes\n641.8\nModelScope [26]\nYes\n639.9\nVideo LDM [5]\nYes\n550.6\nOurs\nYes\n410.0\nTable 4: Text-to-video generation on MSR-VTT.\nMethod\nZero-shot\nCLIPSIM\u2191\nGODIVA [59]\nNo\n0.2402\nNUWA [60]\nNo\n0.2439\nLVDM [13]\nYes\n0.2381\nCogVideo [18]\nYes\n0.2631\nModelScope [26]\nYes\n0.2795\nVideo LDM [5]\nYes\n0.2929\nOurs\nYes\n0.3005\nTable 5: Text-to-video generation on WebVid.\nMethod\nFVD\u2193\nCLIPSIM\u2191\nModelScope [26]\n414.11\n0.3000\nLVDM [13]\n455.53\n0.2751\nOurs\n292.35\n0.3070\n5.2\nAblation Studies\nSpatiotemporal Inter-Connection. We first evaluate the design of our swapped cross-attention\nmechanism. As shown in Tab. 2, using temporal as Q generally leads to better CLIP similarity\n(CLIPSIM) [38], revealing a better text-video alignment. The reason might be that language cross-\nattention only exists in spatial modules. Thus, using spatial features to guide temporal ones implicitly\nenhance semantic guidance. Reversely, using spatial as Q leads to significantly better FVD, revealing\nbetter video quality. The reason might be that the spatial features can better perceive the overall video\nby using temporal features as guidance. This experiment demonstrates the benefits of introducing\ncross-attention, as well as the different acts of spatial and temporal features. Combining these two\naspects, we propose to swap the roles of x and y every two blocks. In this way, both the temporal and\nspatial features can get sufficient information from the other modality, leading to improved FVD and\nCLIPSIM scores. 3D window attention not only does not decrease the performance but also greatly\nreduces the computational cost.\nHigh-Definition Video Generation Dataset. As shown in Tab. 6, we evaluate the effect of our\nHD-VG-130M. After adding HD-VG-130M in training, the result on the validation set of WebVid-\n10M [2] has been improved by 45.74 in FVD, which verifies the superior quality of our HD-VG-130M\nfor training text conditioned video generation model. The visual comparison can also be found in\nFig. 4. The visual qualities are greatly improved with the help of our high-quality text-video dataset,\nespecially the watermark on the generated video is eliminated.\nTable 6: Effect of training on\ndifferent datasets.\nTraining Data\nFVD \u2193\nWebVid-10M [2]\n475.09\nWebVid-10M [2]\n429.75\n+ HD-VG-130M\nFigure 4: Text-to-video generation effects w/o and w/ HD-VG-\n130M for training.\nw/o HD-VG-130M\nw/ HD-VG-130M\n\"Washing an \napple with \nflowing water, \nhigh resolution, \nrealistic, vivid, \nslow motion, \nclose up.\"\n5.3\nQuantitative Results\nTo fully evaluate the generation performance of our VideoFactory, we conduct automatic evaluations\non three different datasets, WebVid-10M [2] (Val) same as the domain of part of our training data, as\nwell as UCF101 [48] and MSR-VTT [62] in zero-shot setting.\nAutomatic Evaluation on UCF101. As mentioned in Sec. 3, the textual annotations in UCF101 [48]\nare class labels. We first follow [17, 46] and rewrite the labels of 101 classes to descriptive captions,\nand then generate 100 samples for each class. As shown in Tab. 3, we report Fr\u00e9chet Video Distance\n(FVD) of our VideoFactory compared with other methods. The FVD of our methods reaches 410.0,\n7\n\"A panda taking a selfie.\"\n\"Clown fish swimming through the coral reef.\"\nMake-A-Video\nOurs\nOurs\nImagen Video\n\"A golden retriever has a picnic on a beautiful tropical beach at sunset, high resolution..\"\nOurs\nVideo LDM\nFigure 5: Subjection text-to-video generation results compared with Imagen Video, Make-A-Video,\nand Video-LDM (Cases above are collected from their public project websites).\nwhich achieves the best compared with other methods both in zero-shot setting and beats most of the\nmethods which have tuned on UCF101 [48]. The results verify that our proposed VideoFactory could\ngenerate more coherent and realistic videos.\nAutomatic Evaluation on MSR-VTT. As shown in Tab. 4, we also evaluate the CLIPSIM on the\nwidely used video generation benchmark MSR-VTT [62]. We randomly choose one prompt per\nexample from MSR-VTT [62] to generate 2990 videos in total. Although in a zero-shot setting, our\nmethod achieves the best compared to other methods with an average CLIPSIM score of 0.3005,\nwhich suggests the semantic alignment between the generated videos and the input text.\nAutomatic Evaluation on WebVid-10M (Val). Referring to Tab. 5, we randomly extract 5K text-\nvideo pairs from WebVid-10M which are exclusive from the training data to form a validation set\nand conduct evaluations on it. Our method achieves an FVD of 292.35 and a CLIPSIM of 0.3070,\nsignificantly surpassing the existing methods ModelScope and LVDM. The results demonstrate the\nsuperiority of our approach.\nHuman Evaluation. To overcome the limitation of existing metrics, and evaluate the performance\nfrom the aspect of humans, we conduct a user study to compare our VideoFactory with four state-\nof-the-arts. Specifically, we choose two models (i.e., ModelScope and LVDM) which have released\ntheir codes and pretrained models, and two methods (i.e., Make-A-Video and Imagen Video) which\nonly show some samples on their websites. In each case, each participant will be given two samples\nof the same text from our method and one competitor, and is asked to compare the two samples in\nterms of the video quality and text-video correlation and give an overall preference. We demonstrate\nthe results in Tab. 7, and we also report the number of parameter ratios for fair comparisons.\n5.4\nQualitative Results\nIn Fig. 5, we show the text-to-video comparison results against Make-A-Video, Imagen Video, and\nVideo LDM. The prompts and generated results are collected from their official project website.\n8\nTable 7: User Preference. The number indicates the percentage of humans that prefer our method\nover the compared method. We also show the ratio of the network parameter v.s. Ours.\nSample\nMethod\nParam Ratio\nVideo Quality\nText-Video\nOverall\nPretrained Model\nModelScope [26]\n0.90\u00d7\n0.8875\n0.8575\n0.9300\nLVDM [13]\n0.57\u00d7\n0.9155\n0.8555\n0.9370\nOpen Website\nMake-A-Video [46]\n4.76\u00d7\n0.5417\n0.4958\n0.5417\nImagen Video [15]\n7.97\u00d7\n0.4291\n0.2582\n0.3818\n\"iron man is walking on the street, high resolution.\"\n\"Superhero in red cape and mask is dancing in bedroom at home having fun enjoying music and leisure time. \nsuperman, lifestyle and apartment concept.\"\n\"Coffee pours into a glass.\"\n\"Honey bees on flower pollination macro shot\"\nFigure 6: Generated samples of our VideoFactory. We can observe high-quality generated results\nwith clear motion, rich detail, and well semantic alignment.\nMake-A-Video only generates 1:1 videos, which limits user experience. Compared with Imagen\nVideo and Video LDM, our model generate the Panda and golden retriever with more vivid details.\nBesides, we demonstrate more generated samples of our method in Fig. 6. Video demos can be\nfound in our supplementary.\n6\nConclusion\nIn this paper, we propose a high-quality open-domain video generation framework namely Video-\nFactory, which produces high-definition (1376\u00d7768), widescreen (16:9) videos without watermarks.\nWe revisit the spatial and temporal modeling in video generation, and present a novel swapped\ncross-attention mechanism which enables spatial and temporal information alternately to attend to\neach other. Furthermore, we propose a widescreen, watermark-free, high-definition HD-VG-130M\ndataset, with 130 million open-domain text-video pairs to unlock the power of our model as much as\npossible. Experiments confirm the high spatial quality, temporal consistency, and fitness to the text of\nsynthesized videos from our VideoFactory, proving it the new benchmark of text-to-video generation.\n9\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin.\nLatent-Shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv,\n2023.\n[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video\nand image encoder for end-to-end retrieval. In ICCV, 2021.\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. eDiff-I:\nText-to-image diffusion models with an ensemble of expert denoisers. arXiv, 2022.\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\nvideo understanding? In Marina Meila and Tong Zhang, editors, ICML, 2021.\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\nFidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent\ndiffusion models. In CVPR, 2023.\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In\nNeurIPS, 2021.\n[7] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\nXu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering text-to-image generation\nvia transformers. In NeurIPS, 2021.\n[8] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. CogView2: Faster and better text-to-\nimage generation via hierarchical transformers. In NeurIPS, 2022.\n[9] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis\nGermanidis. Structure and content-guided video synthesis with diffusion models. arXiv, 2023.\n[10] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\n[11] AmirHossein Habibian, Ties van Rozendaal, Jakub M. Tomczak, and Taco Cohen. Video\ncompression with rate-distortion autoencoders. In ICCV, 2019.\n[12] Tiankai Hang, Huan Yang, Bei Liu, Jianlong Fu, Xin Geng, and Baining Guo. Language-guided\nface animation by recurrent stylegan-based generator. arXiv, 2022.\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion\nmodels for high-fidelity long video generation. arXiv, 2022.\n[14] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion\nmodels for high-fidelity video generation with arbitrary lengths. arXiv, 2022.\n[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko,\nDiederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen\nVideo: High definition video generation with diffusion models. arXiv, 2022.\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,\n33:6840\u20136851, 2020.\n[17] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and\nDavid J. Fleet. Video diffusion models. In NeurIPS, 2022.\n[18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale\npretraining for text-to-video generation via transformers. arXiv, 2022.\n[19] Bhautik J. Joshi, Kristen Stewart, and David Shapiro. Bringing impressionism to life with\nneural style transfer in Come Swim. In ACM SIGGRAPH DigiPro, 2017.\n10\n[20] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2Video-Zero: Text-to-image diffusion\nmodels are zero-shot video generators. arXiv, 2023.\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. arXiv, 2023.\n[22] Yitong Li, Martin Renqiang Min, Dinghan Shen, David E. Carlson, and Lawrence Carin. Video\ngeneration from text. In AAAI, 2018.\n[23] Chengxu Liu, Huan Yang, Jianlong Fu, and Xueming Qian. Learning trajectory-aware trans-\nformer for video super-resolution. In CVPR, 2022.\n[24] Chengxu Liu, Huan Yang, Jianlong Fu, and Xueming Qian. TTVFI: learning trajectory-aware\ntransformer for video frame interpolation. arXiv, 2022.\n[25] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin\ntransformer. In CVPR, 2022.\n[26] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao,\nJingren Zhou, and Tieniu Tan. VideoFusion: Decomposed diffusion models for high-quality\nvideo generation. In CVPR, 2023.\n[27] Yiyang Ma, Huan Yang, Bei Liu, Jianlong Fu, and Jiaying Liu. AI illustrator: Translating raw\ndescriptions into images by prompt-based cross-modal generation. In ACM MM, 2022.\n[28] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal\nlatent diffusion for joint subject and text conditional image generation. arXiv, 2023.\n[29] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen.\nFollow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv, 2023.\n[30] Elman Mansimov, Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Generating\nimages from captions with attention. In ICLR, 2016.\n[31] Micha\u00ebl Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond\nmean square error. In ICLR, 2016.\n[32] Willi Menapace, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Aliaksandr Siarohin, and Elisa Ricci.\nPlayable video generation. In CVPR, 2021.\n[33] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. HowTo100M: Learning a text-video embedding by watching hundred million\nnarrated video clips. In ICCV, 2019.\n[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv, 2021.\n[35] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell:\nGenerating videos from captions. In ACM MM, 2017.\n[36] Jorge Pessoa, Helena Aidos, Pedro Tom\u00e1s, and M\u00e1rio A. T. Figueiredo. End-to-end learning of\nvideo compression using spatio-temporal autoencoders. In IEEE SiPS, 2020.\n[37] Zhongwei Qiu, Huan Yang, Jianlong Fu, and Dongmei Fu. Learning spatiotemporal frequency-\ntransformer for compressed video super-resolution. In ECCV, 2022.\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In ICML,\n2021.\n[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with CLIP latents. arXiv, 2022.\n11\n[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n[41] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak\nLee. Generative adversarial text to image synthesis. In ICML, 2016.\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n[43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. In\nCVPR, 2023.\n[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed\nKamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion\nmodels with deep language understanding. In NeurIPS, 2022.\n[45] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with\nsingular value clipping. In ICCV, 2017.\n[46] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\nHarry Yang, Oron Ashual, Oran Gafni, et al. Make-A-Video: Text-to-video generation without\ntext-video data. arXiv, 2022.\n[47] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. StyleGAN-V: A continuous\nvideo generator with the price, image quality and perks of stylegan2. In CVPR, pages 3626\u20133636,\n2022.\n[48] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human\nactions classes from videos in the wild. arXiv, 2012.\n[49] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey\nTulyakov. A good image generator is what you need for high-resolution video synthesis. arXiv,\n2021.\n[50] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing\nmotion and content for video generation. In CVPR, 2018.\n[51] Zixi Tuo, Huan Yang, Jianlong Fu, Yujie Dun, and Xueming Qian. Learning data-driven\nvector-quantized degradation model for animation video super-resolution. arXiv, 2023.\n[52] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha\u00ebl Marinier, Marcin Michalski,\nand Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\narXiv, 2018.\n[53] Yuri Viazovetskyi, Vladimir Ivashkin, and Evgeny Kashin. StyleGAN2 distillation for feed-\nforward image manipulation. In ECCV. Springer, 2020.\n[54] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nlength video generation from open domain textual description. arXiv, 2022.\n[55] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\nIn NeurIPS, 2016.\n[56] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The\nCaltech-UCSD birds-200-2011 dataset. 2011.\n[57] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. Video-to-video synthesis. arXiv, 2018.\n[58] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-ESRGAN: Training real-world\nblind super-resolution with pure synthetic data. In ICCVW, pages 1905\u20131914, October 2021.\n12\n[59] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and\nNan Duan. GODIVA: Generating open-domain videos from natural descriptions. arXiv, 2021.\n[60] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00fcwa:\nVisual synthesis pre-training for neural visual world creation. In ECCV, 2022.\n[61] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying\nShan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion\nmodels for text-to-video generation. arXiv, 2022.\n[62] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for\nbridging video and language. In CVPR, 2016.\n[63] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong\nHe. AttnGAN: Fine-grained text to image generation with attentional generative adversarial\nnetworks. In CVPR, 2018.\n[64] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu,\nand Baining Guo. Advancing high-resolution video-language representation with large-scale\nvideo transcriptions. In CVPR, 2022.\n[65] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video generation\nusing vq-vae and transformers. arXiv, 2021.\n[66] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen,\nand Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In\nCVPR, 2023.\n[67] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture trans-\nformer network for image super-resolution. In CVPR, 2020.\n[68] Fuzhi Yang, Huan Yang, Yanhong Zeng, Jianlong Fu, and Hongtao Lu. Degradation-guided\nmeta-restoration network for blind super-resolution. arXiv, 2022.\n[69] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00e9 Lezama, Han Zhang, Huiwen Chang, Alexander G.\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. MAGVIT: masked\ngenerative video transformer. arXiv, 2022.\n[70] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in\nprojected latent space. In CVPR, 2023.\n[71] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo\nShin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv,\n2022.\n[72] Han Zhang, Tao Xu, and Hongsheng Li. StackGAN: Text to photo-realistic image synthesis\nwith stacked generative adversarial networks. In ICCV, 2017.\n[73] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-\nresolution using very deep residual channel attention networks. In ECCV, pages 286\u2013301,\n2018.\n[74] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. MagicVideo:\nEfficient video generation with latent diffusion models. arXiv, 2022.\n13\n"
  },
  {
    "title": "GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework",
    "link": "https://arxiv.org/pdf/2305.10841.pdf",
    "upvote": "1",
    "text": "GETMusic: Generating Music Tracks with a Unified\nRepresentation and Diffusion Framework\nAng Lv\u2021\u2020, Xu Tan\u2020\u2217, Peiling Lu\u2020, Wei Ye\u00a7, Shikun Zhang\u00a7, Jiang Bian\u2020, Rui Yan\u2021\u2217\n\u2020Microsoft Research Asia\n\u2021Gaoling School of Artifical Intelligence, Renmin University of China\n\u00a7National Engineering Research Center for Software Engineering, Peking University\n{anglv, ruiyan}@ruc.edu.cn, {xuta, peil, jiabia}@microsoft.com,\n{wye,zhangsk}@pku.edu.cn\nhttps://github.com/microsoft/muzic\nAbstract\nSymbolic music generation aims to create musical notes, which can help users\ncompose music, such as generating target instrument tracks based on provided\nsource tracks. In practical scenarios where there\u2019s a predefined ensemble of tracks\nand various composition needs, an efficient and effective generative model that\ncan generate any target tracks based on the other tracks becomes crucial. However,\nprevious efforts have fallen short in addressing this necessity due to limitations in\ntheir music representations and models. In this paper, we introduce a framework\nknown as GETMusic, with \u201cGET\u201d standing for \u201cGEnerate music Tracks.\u201d This\nframework encompasses a novel music representation \u201cGETScore\u201d and a diffusion\nmodel \u201cGETDiff.\u201d GETScore represents musical notes as tokens and organizes\ntokens in a 2D structure, with tracks stacked vertically and progressing horizontally\nover time. At a training step, each track of a music piece is randomly selected as\neither the target or source. The training involves two processes: In the forward\nprocess, target tracks are corrupted by masking their tokens, while source tracks\nremain as the ground truth; in the denoising process, GETDiff is trained to pre-\ndict the masked target tokens conditioning on the source tracks. Our proposed\nrepresentation, coupled with the non-autoregressive generative model, empowers\nGETMusic to generate music with any arbitrary source-target track combinations.\nOur experiments demonstrate that the versatile GETMusic outperforms prior works\nproposed for certain specific composition tasks.\n1\nIntroduction\nSymbolic music generation aims to create musical notes, which can help users in music composition.\nDue to the practical need for flexible and diverse music composition, the need for an efficient and\nunified approach capable of generating arbitrary tracks based on the others is high2. However,\ncurrent research falls short of meeting this demand due to inherent limitations imposed by their\nrepresentations and models. Consequently, these approaches are confined to specific source-target\ncombinations, such as generating piano accompaniments based on melodies.\n\u2217Corresponding authors: Xu Tan (xuta@microsoft.com) and Rui Yan (ruiyan@ruc.edu.cn).\n2A music typically consists of multiple instrument tracks. In this paper, given a predefined track ensemble,\nwe refer to the tracks to be generated as \u201ctarget tracks\u201d and those acting as conditions as \u201csource tracks.\u201d We\nrefer to such an orchestration of tracks as a \u201csource-target combination.\u201d\nPreprint. Under review.\narXiv:2305.10841v2  [cs.SD]  29 Sep 2023\nGETDiff\nGETScore\nMasked\nMasked\nMasked\n\u2460\n\u2461\n\u2462\nM\n\u2463\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nSymbolic Music\nSymbolic\nMusic\nGETScore\nM\nMasked\nTime\nTracks\nMelody\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nDenoising masked tokens\nTracks\nTime\nFigure 1: The overview of GETMusic, involving a novel music representation \u201cGETScore\u201d and a\ndiscrete diffusion model \u201cGETDiff.\u201d Given a predefined ensemble of instrument tracks, GETDiff\ntakes GETScores as inputs and can generate any desired target tracks conditioning on any source\ntracks ( 1\u20dd, 2\u20dd, and 3\u20dd). This flexibility extends beyond track-wise generation, as it can perform\nzero-shot generation for any masked parts ( 4\u20dd).\nCurrent research can be categorized into two primary approaches based on music representation:\nsequence-based and image-based. On one hand, sequence-based works [13; 27; 3] represent music as\na sequence of discrete tokens, where a musical note requires multiple tokens to describe attributes\nsuch as onset, pitch, duration, and instrument. These tokens are arranged chronologically, resulting in\nthe interleaving of notes from different tracks, and are usually predicted by autoregressive models\nsequentially. The interleaving of tracks poses a challenge of precise target generation because the\nautoregressive model implicitly determines when to output a target-track token and avoids generating\ntokens from other tracks. It also complicates the specification of source and target tracks. Therefore,\nthe existing methods [5; 18; 26] typically focus on either one specific source-target track combination\nor the continuation of tracks.\nOn the other hand, image-based research represents music as 2D images, with pianorolls3 being\na popular choice. Pianorolls represent musical notes as horizontal lines, with the vertical position\ndenoting pitch and the length signifying duration. A pianoroll explicitly separates tracks but it has to\nincorporate the entire pitch range of instruments, resulting in large and sparse images. Due to the\nchallenges of generating sparse and high-resolution images, most research has focused on conditional\ncomposition involving only a single source or target track [6; 25; 20] or unconditional generation [17].\nTo support the generation across flexible and diverse source-target track combinations, we propose a\nunified representation and diffusion framework called GETMusic (\u201cGET\u201d stands for GEnerate music\nTracks), which comprises a representation named GETScore, and a discrete diffusion model [1]\nnamed GETDiff. GETScore represents the music as a 2D structure, where tracks are stacked vertically\nand progress horizontally over time. Within each track, we efficiently represent musical notes with\nthe same onset by a single pitch token and a single duration token, and position them based on the\nonset time. At a training step, each track in a training sample is randomly selected as either the target\nor the source. The training consists of two processes: In the forward process, the target tracks are\ncorrupted by masking tokens, while the source tracks are preserved as ground truth; in the denoising\nprocess, GETDiff learns to predict the masked target tokens based on the provided source. Our\nco-designed representation and diffusion model in GETMusic offer several advantages compared to\nprior works:\n\u2022 With separate and temporally aligned tracks in GETScore, coupled with a non-autoregressive\ngenerative model, GETMusic adeptly compose music across various source-target combinations.\n\u2022 GETScore is a compact multi-track music representation while effectively preserving interde-\npendencies among simultaneous notes both within and across tracks, fostering harmonious music\ngeneration.\n3https://en.wikipedia.org/wiki/Piano_roll\n2\n0\n0\n0\n0\n0\n0\n0\n0\n555\n626\n555\n600\n555\n626\n555\n601\n66\n2\n64\n2\n50\n8\n2\n2\n2\n2\n2\n2\n2\n2\n362\n147\n362\n147\n362\n147\n362\n147\n(d) GETScore\n(a) Music Score\nPiano\nMelody\nDrum\nTime Unit      Pitch tokens    Duration tokens    Paddings\n(c) Pianoroll\n(b) Sequence Representation\nPiano\nMelody\nDrum\nBar0, TS4/4, Position0, BPM120, Trackpiano, PitchA3, \u21b2 \nDuration2, Velocity62, PitchC4, Duration2, Velocity62, PitchF4, \u21b2\nDuration2, Velocity62, Bar0, TS4/4, Position0, BPM120, \u21b2\nTrackdrum, Pitchcymbal_2, Velocity62, Pitchbass_drum, Velocity62, \u21b2\nBar0, TS4/4, Position2, BPM120, Trackpiano, PitchF3, \u21b2 \nDuration2, Velocity62, Bar0, TS4/4, Position2, Trackmelody, PitchF3, \u21b2\nDuration8, Velocity62, Bar0, TS4/4, Position2, Trackdrum, \u21b2 \nPitchcymbal_1, Velocity62, Bar0, TS4/4, Position4, BPM120, \u21b2 \nTrackpiano, PitchA3, Duration2, Velocity62, PitchC4, \u21b2 \nDuration2, Velocity62, PitchF4, Duration2, Velocity62, \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n88\nPixels\nMelody\nFigure 2: Different representations for the same piece of music. Figure (a) is the music score. Figure\n(b) illustrates the sequence-based representation in REMI [13] style, and due to the length of the\nsequence, we only show the portion enclosed by the dashed box in Figure (a). Figure (c) shows a\nsparse pianoroll that represents notes by lines. In Figure (d), GETScore separates and aligns tracks,\nforming the basis for unifying generation across various source-target combinations. It also preserves\nthe interdependencies among simultaneous notes, thereby fostering harmony in music generation.\nNumbers in (d) denote token indices which are for demonstration only.\n\u2022 Beyond track-wise generation, the mask and denoising mechanism of GETDiff enable the zero-shot\ngeneration (i.e., denoising masked tokens at any arbitrary locations in GETScore), further enhancing\nthe versatility and creativity.\nIn this paper, our experiments consider six instruments: bass, drum, guitar, piano, string, and\nmelody, resulting in 665 source-target combinations (Details in appendix A). We demonstrate that our\nproposed versatile GETMusic surpasses approaches proposed for specific tasks such as conditional\naccompaniment or melody generation, as well as generation from scratch.\n2\nBackground\n2.1\nSymbolic Music Generation\nSymbolic music generation aims to generate musical notes, whether from scratch [17; 26] or based on\ngiven conditions such as chords, tracks [20; 13; 6], lyrics [16; 14; 19], or other musical properties [28],\nwhich can assist users in composing music. In practical music composition, a common user need\nis to create instrumental tracks from scratch or conditioning on existing ones. Given a predefined\nensemble of tracks and considering flexible composition needs in practice, a generative model\ncapable of handling arbitrary source-target combination is crucial. However, neither of the existing\napproaches can integrate generation across multiple source-target combinations, primarily due to\ninherent limitations in their representations and models.\nCurrent approaches can be broadly categorized into two main categories with respect to adopted\nrepresentation: sequence-based and image-based. In sequence-based methods [13; 12; 27; 18], music\nis represented as a sequence of discrete tokens. A token corresponds to a specific attribute of a musical\nnote, such as onset (the beginning time of a note), pitch (note frequency), duration, and instrument,\nand tokens are usually arranged chronologically. Consequently, notes that represent different tracks\nusually interleave, as shown in Figure 2(b) where the tracks are differentiated by colors. Typically,\nan autoregressive model is applied to processes the sequence, predicting tokens one by one. The\n3\ninterwove tracks and the autoregressive generation force the model to implicitly determine when to\noutput tokens of desired target tracks and avoid incorporating tokens belonging to other tracks, which\nposes a challenge to the precise generation of the desired tracks; the sequential representation and\nmodeling do not explicitly preserve the interdependencies among simultaneous notes, which impact\nthe harmony of the generated music; furthermore, the model is required to be highly capable of\nlearning long-term dependencies [2] given the lengthy sequences. Some unconventional methods [7]\norganize tokens according to the track order in order to eliminate track interleaving. However, it\ncomes with a trade-off, as it results in weaker dependencies both in the long term and across tracks.\nImage-based methods mainly employ pianoroll representations which depict notes as horizontal lines\nin 2D images, with the vertical position denoting pitch and the length signifying duration. However,\npianorolls need to include the entire pitch range of the instrument, resulting in images that are both\nlarge and sparse. For instance, Figure 2(c) illustrates a pianoroll representation of a three-track music\npiece, which spans a width of hundreds of pixels, yet only the bold lines within it carry musical\ninformation. Most works focus on conditional composition involving only a single source/target\ntrack [6; 25; 20] or unconditional generation [17] because generating a sparse and high-resolution\nimage is challenging.\nOur proposed GETMusic addresses above limitations with a co-designed representation and a discrete\ndiffusion model which together provide an effective solution to versatile track generation.\n2.2\nDiffusion Models\nDiffusion models, initially proposed by [21] and further improved by subsequent research [9; 22;\n10; 4], have demonstrated impressive capabilities in modeling complex distributions. These models\nconsist of two key processes: a forward (diffusion) process and a reverse (denoising) process. The\nforward process q(x1:T |x0) =\nTQ\nt=1\nq(xt|xt\u22121) introduces noise to the original data x0 iteratively\nfor T steps, corrupting it towards a prior distribution p(xT ) that is independent of x0. The goal of\ndiffusion models is to learn a reverse process p\u03b8(xt\u22121|xt) that gradually denoises xT to the data\ndistribution. The model is trained by optimizing the variational lower bound (VLB) [9]:\nLvlb = Eq[\u2212 log p\u03b8(x0|x1)] +\nT\nX\nt=2\nDKL [q(xt\u22121|xt, x0)||p\u03b8(xt\u22121|xt))] + DKL[q(xT |x0)||p(xT )]].\n(1)\nDiffusion models can be categorized into continuous and discrete versions. As our proposed\nGETScore represents music as a 2D arrangement of discrete tokens, we employ the discrete diffusion\nframework in our method. Discrete diffusion models in [21] were developed for binary sequence\nlearning. [11] extended these models to handle categorical random variables, while [1] introduced\na more structured categorical forward process: the forward process is a Markov chain defined by\ntransition matrices, which transitions a token at time t \u2212 1 to another at time t by probability. For\nour diffusion model GETDiff, we adopt their forward process as the basis. We also adopt a crucial\ntechnique known as x0-parameterization [1], where instead of directly predicting xt\u22121 at time step\nt, the model learns to fit the noiseless original data x0 and corrupts the predicted \u02dcx0 to obtain xt\u22121.\nConsequently, an auxiliary term scaled by a hyper-parameter \u03bb is added to the VLB:\nL\u03bb = Lvlb + \u03bbEq\n\" T\nX\nt=2\n\u2212 log p\u03b8(x0|xt)\n#\n(2)\n3\nGETMusic\nIn this section, we introduce two key components in GETMusic: the representation GETScore and\nthe diffusion model GETDiff. We first provide an overview of each component, and then highlight\ntheir advantages in supporting the flexible and diverse generation of any tracks.\n3.1\nGETScore\nOur goal is to design an efficient and effective representation for modeling multi-track music, which\nallows for flexible specification of source and target tracks and thereby laying the foundation of the\ndiverse track generation tasks. Our novel representation GETScore involves two core ideas: (1) the\n2D track arrangement and (2) the musical note tokenization.\n4\nTrack Arrangement\nWe derive inspiration from music scores to arrange tracks vertically, with each\ntrack progressing horizontally over time. The horizontal axis is divided into fine-grained temporal\nunits, with each unit equivalent to the duration of a 16th note. This level of temporal detail is sufficient\nto the majority of our training data. This arrangement of tracks brings several benefits:\n\u2022 It prevents content of different tracks from interleaving, which simplifies the specification of\nsource and target tracks, and facilitates the precise generation of desired tracks.\n\u2022 Because tracks are temporally aligned like music scores, their interdependencies are well preserved.\nNote Tokenization\nTo represent musical notes, we focus on two attributes: pitch and duration,\nwhich are directly associated with composition. Some dynamic factors like velocity and tempo\nvariation fall outside the scope of our study. We use two distinct tokens to denote a note\u2019s pitch and\nduration, respectively. These paired pitch-duration tokens are placed in accordance with the onset\ntime and track within GETScore. Some positions within GETScore may remain unoccupied by any\ntokens; in such instances, we employ padding tokens to fill them, as illustrated by the blank blocks in\nFigure 2(d). Each track has its own pitch token vocabulary but shares a common duration vocabulary,\nconsidering pitch characteristics are instrument-dependent, whereas duration is a universal feature\nacross all tracks. To broaden the applicability of GETScore, we need to address two more problems:\n(1) How to use single pitch and duration tokens to represent a group of notes played simultaneously\nwithin a track? We propose merging pitch tokens of a group of simultaneous notes into a single\ncompound pitch token. Furthermore, we identify the most frequently occurring duration token within\nthe group as the final duration token. This simplification of duration representation is supported by\nour observation from the entire training data, where notes in more than 97% groups share the same\nduration. In only 0.5% groups, the maximum duration difference among notes exceeds a temporal\nunit. These findings suggest that this simplification has minimal impact on the expressive quality of\nGETScore. Figure 2(d) illustrates the compound token: in the piano track, we merge the first three\nnotes \u201cA\u201d, \u201cC\u201d, and \u201cF\u201d into a single token indexed as \u201c147.\u201d\n(2) How to represent percussive instruments, such as drums, which do not involve the concepts of\n\"pitch\" and \"duration?\" We treat individual drum actions (e.g., kick, snare, hats, toms, and cymbals)\nas pitch tokens and align them with a special duration token. The drum track in Figure 2(d) illustrates\nour approach.\nIn conclusion, besides the benefits from track arrangement, GETScore also gains advantages through\nthis note tokenization:\n\u2022 Each track requires only two rows to accommodate the pitch and duration tokens, significantly\nenhancing the efficiency of GETScore.\n\u2022 The compound token preserves the interdependecies within a track. When it is generated, harmony\nis inherently guaranteed because the corresponding note group is derived from real-world data.\n3.2\nGETDiff\nIn this section, we first introduce the forward and the denoising process of GETDiff during training,\nrespectively. Next, we introduce the inference procedure and outline GETDiff\u2019s benefits in addressing\nthe diverse needs for track generation.\nThe Forward Process\nSince GETMusic operates on GETScore, which consists of discrete tokens,\nwe employ a discrete diffusion model. We introduce a special token [MASK] into the vocabulary\nas the absorbing state of the forward process. At time t \u2212 1, a normal token remains in its current\nstate with a probability of \u03b1t and transitions to [MASK] (i.e., corrupts to noise) with a probability\nof \u03b3t = 1 \u2212 \u03b1t. As GETScore includes a fixed number of tracks that GETMusic supports, and the\ncomposition does not always involve all tracks, we fill the uninvolved tracks with another special\ntoken [EMPTY]. [EMPTY] never transitions to other tokens, nor can it be transitioned to from any\nother tokens. This design prevents any interference from uninvolved tracks in certain compositions.\nFormally, a transition matrix [Qt]mn = q(xt = m|xt\u22121 = n) \u2208 RK\u00d7K defines the transition\n5\nM\nM\nM\n0\nM\n0\n0\n0\nM\nM\n0\n555\nM\n555\nM\n555\n626\nM\n601\n2\n2\n2\n2\n2\n2\n2\n2\n362\n147\n362\n147\n362\n147\n362\n147\n\ud835\udc99\ud835\udc95\nPiano\nMelody\nDrum\nEmptied\n0\nM\n0\n0\n0\n0\n0\n0\n555\n626\n555\nM\n555\n626\n555\n601\n2\n2\n2\n2\n2\n2\n2\n2\n362\n147\n362\n147\n362\n147\n362\n147\nPiano\nMelody\nDrum\nEmptied\n\ud835\udc99\ud835\udc95\u2212\ud835\udfcf\nFigure 3: An overview of training the GETDiff using a 3-track GETScore. Note that GETScore is\ncapable of accommodating any number of tracks, with this example serving as a toy example. During\nthis training step, GETMusic randomly selects the piano track as the source and the drum track as\nthe target, while ignoring the melody track. Thus, xt consists of the ground truth piano track, an\nemptied melody track, and a corrupted drum track. GETDiff generates all tokens simultaneously in a\nnon-autoregressive manner which may modify tokens in its output. Therefore, when xt\u22121 is obtained,\nthe sources are recovered with the ground truth while ignored tracks are emptied again.\nprobability from the n-th token at time t \u2212 1 to the m-th token at time t:\nQt =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b1t\n0\n0\n. . .\n0\n0\n0\n\u03b1t\n0\n. . .\n0\n0\n0\n0\n\u03b1t\n. . .\n0\n0\n...\n...\n...\n...\n...\n...\n0\n0\n0\n. . .\n1\n0\n\u03b3t\n\u03b3t\n\u03b3t\n. . .\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n(3)\nwhere K is the total vocabulary size, including two special tokens. The last two columns of the\nmatrix correspond to q (xt|xt\u22121 = [EMPTY]) and q (xt|xt\u22121 = [MASK]), respectively. Denoting\nv(x) as a one-hot column vector indicating the category of x and considering the Markovian nature of\nthe forward process, we can express the marginal at time t, and the posterior at time t \u2212 1 as follows:\nq(xt|x0) = v\u22a4(xt)Qtv(x0),\nwith\nQt = Qt . . . Q1.\n(4)\nq(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0)\nq(xt|x0)\n=\n\u0000v\u22a4(xt)Qtv(xt\u22121)\n\u0001 \u0000v\u22a4(xt\u22121)Qt\u22121v(x0)\n\u0001\nv\u22a4(xt)Qtv(x0)\n.\n(5)\nWith the tractable posterior, we can optimize GETDiff with Eq.2.\nThe Denoising Process\nFigure 3 provides an overview of GETMusic denoising a three-track\ntraining sample of a length of L time units. GETDiff has three main components: an embedding\nmodule, Roformer [23] layers, and a decoding module. Roformer is a transformer [24] variant that\nincorporates relative position information into the attention matrix, which enhances the model\u2019s\nability to length extrapolation during inference.\nDuring training, GETMusic needs to cover the various source-target combinations for a music piece\nwith I tracks, represented as a GETScore with 2I rows. To achieve this, m tracks (resulting in\n2m rows in GETScore) are randomly chosen as the source, while n tracks (resulting in 2n rows in\nGETScore) are selected as the target, m \u2265 0, n > 0, and m + n \u2264 I.\nAt a randomly sampled time t, to obtain xt from the original GETScore x0, tokens in target tracks are\ntransitioned according to Qt, tokens in the source tracks remain as the ground truth, and uninvolved\ntracks are emptied. GETDiff denoises xt in four steps, as shown in Figure 3: (1) All tokens in\nGETScore are embedded into d-dimensional embeddings, forming an embedding matrix of size\n2Id \u00d7 L. (2) Learnable condition flags are added in the matrix to guide GETDiff which tokens can\n6\nbe conditioned on, thereby enhancing inference performance. The effectiveness of condition flags is\nanalyzed in \u00a7 4.3. (3) The embedding matrix is resized to GETDiff\u2019s input dimension dmodel using\nan MLP, and then fed into the Roformer model. (4) The output matrix passes through a classification\nhead to obtain the token distribution over the vocabulary of size K and we obtain the final tokens\nusing the gumbel-softmax technique.\nInference\nDuring inference, users can specify any target and source tracks, and GETMusic con-\nstructs the corresponding GETScore representation, denoted as xT , which contains the ground truth\nof source tracks, masked target tracks, and emptied tracks (if any). GETMusic then denoises xT\nstep by step to obtain x0. As GETMusic generates all tokens simultaneously in a non-autoregressive\nmanner, potentially modifying source tokens in the output, we need to ensure the consistent guidance\nfrom source tracks: when xt\u22121 is acquired, tokens in source tracks are recovered to their ground truth\nvalues, while tokens in uninvolved tracks are once again emptied.\nConsidering the combined benefits of the representation and the diffusion model, GETMusic offers\ntwo major advantages in addressing the diverse composition needs:\n\u2022 Through a unified diffusion model, GETMusic has the capability to compose music across a range\nof source-target combinations without requiring re-training.\n\u2022 Beyond the track-wise generation, the mask and denoising mechanism of GETDiff enables the zero-\nshot generation of any arbitrary masked locations in GETScore, which further enhances versatility\nand creativity. An illustration of this can be found in case 4\u20dd in Figure 1.\n4\nExperiments\n4.1\nExperiment Settings\nData and Preprocess\nWe crawled 1,569,469 MIDI files from Musescore4. We followed [18] to\npre-process the data, resulting in music including I = 6 instrumental tracks: bass, drum, guitar, piano,\nstring, melody and an extra chord progression track. After strict cleanse and filter, we construct\n137,812 GETScores (about 2,800 hours) with the maximum L as 512, out of which we sampled\n1,000 for validation, 100 for testing, and the remaining for training. We train all baselines on the\ncrawled data. The vocabulary size K is 11,883. More details on data and the pre-processing are in\nappendix B.\nTraining Details\nWe set diffusion timesteps T = 100 and the auxiliary loss scale \u03bb = 0.001. For\nthe transition matrix Qt, we linearly increase \u03b3t (cumulative \u03b3t) from 0 to 1 and decrease \u03b1t from 1\nto 0. GETDiff has 12 Roformer layers with d = 96 and dmodel = 768, where there are about 86M\ntrainable parameters. During training, we use AdamW optimizer with a learning rate of 1e \u2212 4, \u03b21 =\n0.9, \u03b22 = 0.999. The learning rate warmups first 1000 steps and then linearly decays. The training is\nconducted on 8 \u00d7 32G Nvidia V100 GPUs and the batch size on each GPU is 3. We train the model\nfor 50 epochs and validate it every 1000 steps, which takes about 70 hours in total. We select model\nparameters based on the validation loss.\nTasks and Baselines\nWe consider three symbolic music generation tasks: (1) accompaniment gen-\neration based on the melody, (2) melody generation based on the accompaniments, and (3) generating\ntracks from scratch. For the first two tasks, we compare GETMusic with PopMAG [18]. PopMAG\nis an autoregressive transformer encoder-decoder model that processes a sequence representation\nMuMIDI. Following [18], an extra chord progression provides more composition guidance and we\ntreat the chord progression as another track in GETScore (Details in appendix B). To be comparable,\nwe restrict the generated music to a maximum length of 128 beats, which is the longest composition\nlength for PopMAG. For the third task, we compare GETMusic with Museformer [26], one of the\nmost competitive unconditional generation models. We generate all 6 tracks of 100 songs from\nscratch, where each song also restricted to 128 beats.\nEvaluation\nWe introduce objective metrics that quantitatively evaluates the generation quality.\nFollowing [18], we evaluate the models from two aspects:\n4https://musescore.com/\n7\nTable 1: We compare GETMusic with PopMAG and Museformer, through three representative tasks:\nthe accompaniment/melody generation as well as generating from scratch. In all human evaluations,\nthe \u03ba values consistently exceed 0.6, indicating substantial agreement among the evaluators.\nMethod\nCA(%) \u2191\nKLP itch \u2193\nKLDur \u2193\nKLIOI \u2193\nHR \u2191\nAccompaniment Generation\nPopMAG\n61.17\n10.98\n7.00\n6.92\n2.88\nGETMusic\n65.48\n10.05\n4.21\n4.22\n3.35\nLead Melody Generation\nPopMAG\n73.70\n10.64\n3.97\n4.03\n3.14\nGETMusic\n81.88\n9.82\n3.67\n3.49\n3.52\nGeneration from Scratch\nMuseformer\n-\n8.19\n3.34\n5.71\n3.05\nGETMusic\n-\n7.99\n3.38\n5.33\n3.18\n(1) Chord Accuracy: For Task 1 and 2, we measure the chord accuracy CA between generated target\ntracks and their ground truth to evaluate the melodic coherence:\nCA =\n1\nNtracks \u2217 Nchords\nNtracks\nX\ni=1\nNchords\nX\nj=1\n1(C\n\u2032\ni,j = Ci,j).\n(6)\nHere, Ntracks and Nchords represent the number of tracks and chords, respectively. C\n\u2032\ni,j and Ci,j\ndenote the j-th chord in the i-th generated target track and the ground truth, respectively. Note that\nthis metric is not suitable for the third task. Instead, melodic evaluation for the third task relies on\nboth the pitch distribution and human evaluation, which are discussed later.\n(2) Feature Distribution Divergence: For the first two tasks, we assess the distributions of some\nimportant musical features in generated and ground truth tracks: note pitch, duration (Dur) and\nInter-Onset Interval (IOI) that measures the temporal interval between two consecutive notes within a\nbar. First, we quantize the note pitch, duration and IOI into 16 classes, then convert the histograms\ninto probability density functions (PDFs) using Gaussian kernel density estimation. Finally, we\ncompute the KL-divergence [15] KL{P itch,Dur,IOI} between the PDFs of generated target tracks\nand ground truth. For the third task, we compute KL{P itch,Dur,IOI} between the PDFs of generated\ntarget tracks and the corresponding distribution of training data.\n(4) Human Evaluation: We recruited 10 evaluators with basic music knowledge. They were presented\nwith songs generated by GETMusic and baselines in a blind test. Evaluators provided a Human\nRating (HR), on a scale from 1 (Poor) to 5 (Excellent). The HR rating reflects the overall quality of\nthe generated songs, and the coherence between the target and source tracks (when applicable).\n4.2\nGeneration Results\nComparison with Previous Works\nTable 1 presents the results of three composition tasks. In\nthe first two tasks, GETMusic consistently outperforms PopMAG across all metrics, showcasing\nits ability to create music with more harmonious melodies and rhythms that align well with the\nprovided source tracks. When we compare the first two tasks, an improvement in music quality\nbecomes evident as we involve more source tracks. In the second task, where all five accompaniment\ninstruments serve as source tracks, we achieve better scores in most metrics compared to the first\ntask which relies solely on the melody as the source track. In unconditional generation, GETMusic\noutperforms the competitive baseline in most metrics. Subjective evaluations further confirm the\neffectiveness of GETMusic. Readers are welcome to visit our demo page for generated samples.\nZero-shot Generation\nAlthough GETMusic is trained for track-wise generation, it can zero-shot\nrecover masked tokens at any arbitrary locations, due to its the mask and denoising mechanism. The\nzero-shot generation is examplified in case 4\u20dd in Figure 1. This capability enhances the versatility and\ncreativity of GETMusic. For example, we can insert mask tokens in the middle of two different songs\nto connect them: GETMusic generates a harmonious bridge by iteratively denoising the masked\n8\nTable 2: Ablation study on generation paradigms: Autoregressive vs. Non-autoregressive.\nMethod\nCA(%) \u2191\nKLP itch \u2193\nKLDur \u2193\nKLIOI \u2193\nTime \u2193\nHR \u2191\nPopMAG\n61.17\n10.98\n7.00\n6.92\n23.32\n2.88\nGETMusic (AR)\n46.25\n11.91\n7.08\n6.49\n17.13\n2.37\nGETMusic\n65.48\n10.05\n4.21\n4.22\n4.80\n3.35\nTable 3: Ablation study on the effectiveness of condition flags.\nMethod\nCA \u2191\nKLP itch \u2193\nKLDur \u2193\nKLIOI \u2193\nLoss \u2193\nGETMusic (AG)\n65.48\n10.05\n4.21\n4.22\n1.39\n\u2212 condition flags\n45.16\n10.89\n6.32\n5.34\n1.40\nGETMusic (UN)\n-\n7.99\n3.38\n5.33\n1.63\n\u2212 condition flags\n-\n8.43\n3.57\n5.61\n1.75\ntokens while preserving the rest of the tokens unchanged. Despite the challenges in evaluation, the\n8th and 9th demos on the demo page showcase our approach\u2019s flexibility and creativity.\n4.3\nMethod Analysis\nThe Complementary Nature of GETScore and GETDiff\nTo demonstrate this, we begin with an\nablation study in which we replace GETDiff with an autoregressive model. For the task of generating\nmusic from scratch, we train a transformer decoder equipped with 12 prediction heads. At each\ndecoding step, it predicts 12 tokens (6 pitch tokens and 6 duration tokens in a GETScore involving 6\ntracks). The outcomes of this variant, denoted as GETMusic (AR), are detailed in Table 2, revealing\nsuboptimal results characterized by a tendency to produce repetitive melody. Additionally, we present\nthe average time required in seconds for composing each musical piece using an Nvidia A100 GPU,\nhighlighting that the non-autoregressive denoising process significantly outpaces autoregressive\ndecoding in terms of speed.\nWhile it would be more informative to evaluate diffusion models trained with traditional sequence\nrepresentations, this approach is intractable. Firstly, due to the inherently higher computational\nresource requirements of training a diffusion model compared to an autoregressive model, coupled\nwith the fact that traditional sequence representations are typically an order of magnitude longer than\nGETScore when representing the same musical piece, the training cost becomes prohibitively high.\nFurthermore, diffusion models require the specification of the generation length in advance. Yet,\nthe length of traditional sequences representing the same number of bars can vary in a wide range,\nleading to uncontrollable variations in the generated music\u2019s length and structure.\nBased on above results and analyses, we believe that our proposed GETScore and GETDiff together\nprovide an efficient and effective solution for versatile and diverse symbolic music generation.\nEffectiveness of Condition Flags\nIn GETScore, since all normal tokens carry information, any\ninaccuracies in the predicted normal tokens can lead to deviations in the denoising direction during\ninference. To address this issue, we incorporate learnable condition flags into the embedded GETScore\nto signify trustworthy tokens. To evaluate the effectiveness of the condition flags, we remove them\nfrom the diffusion model. The results are shown in Table 3. Given the comparable loss, removing\nthe condition flags has minimal impact on training and convergence, but it leads to lower generation\nquality in accompaniment generation (AG) while slightly affecting unconditional generation (UN).\nThis demonstrates the effectiveness of condition flags in guiding the model to generate high-quality\nmusic, particularly in conditional generation scenarios.\n5\nConclusion\nWe propose GETMusic, a unified representation and diffusion framework to effectively and efficiently\ngenerate desired target tracks from scratch or based on user-provided source tracks, which can address\nusers\u2019 diverse composition needs. GETMusic has two core components: a novel representation\n9\nGETScore and a diffusion model GETDiff. GETScore offers several advantages in representing\nmulti-track music, including efficiency, simple source-target specification, and explicit preservation of\nsimultaneous note interdependencies. Leveraging the power of GETScore and the non-autoregressive\nnature of GETDiff, GETMusic can compose music across various source-target combinations and\nperform zero-shot generation at arbitrary locations. In the future, we will continue to explore the\npotential of GETMusic, such as incorporating lyrics as a track to enable lyric-to-melody generation.\nReferences\n[1] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg.\nStructured denoising diffusion models in discrete state-spaces. In A. Beygelzimer, Y. Dauphin,\nP. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,\n2021.\n[2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent\nis difficult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.\n[3] Walshaw Christopher. The abc music standard 2.1. ABC notation standard, 2011.\n[4] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 8780\u20138794. Curran\nAssociates, Inc., 2021.\n[5] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, and Taylor Berg-Kirkpatrick. Mul-\ntitrack music transformer. In Proceedings of the IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2023.\n[6] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: Multi-track\nsequential generative adversarial networks for symbolic music generation and accompaniment,\n2017.\n[7] Jeffrey Ens and Philippe Pasquier. MMM : Exploring conditional multi-track music generation\nwith the transformer. CoRR, abs/2008.06048, 2020.\n[8] Rui Guo, Dorien Herremans, and Thor Magnusson. Midi miner - A python library for tonal\ntension and track classification. CoRR, abs/1910.02049, 2019.\n[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural\nInformation Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.\n[10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop\non Deep Generative Models and Downstream Applications, 2021.\n[11] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax\nflows and multinomial diffusion: Learning categorical distributions.\nIn A. Beygelzimer,\nY. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information\nProcessing Systems, 2021.\n[12] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. Compound word transformer:\nLearning to compose full-song music over dynamic directed hypergraphs. Proceedings of the\nAAAI Conference on Artificial Intelligence, 35(1):178\u2013186, May 2021.\n[13] Yu-Siang Huang and Yi-Hsuan Yang. Pop music transformer: Beat-based modeling and\ngeneration of expressive pop piano compositions. In Proceedings of the 28th ACM International\nConference on Multimedia, MM \u201920, page 1180\u20131188, New York, NY, USA, 2020. Association\nfor Computing Machinery.\n[14] Zeqian Ju, Peiling Lu, Xu Tan, Rui Wang, Chen Zhang, Songruoyao Wu, Kejun Zhang,\nXiangyang Li, Tao Qin, and Tie-Yan Liu. Telemelody: Lyric-to-melody generation with a\ntemplate-based two-stage method. CoRR, abs/2109.09617, 2021.\n10\n[15] Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of\nmathematical statistics, 22(1):79\u201386, 1951.\n[16] Ang Lv, Xu Tan, Tao Qin, Tie-Yan Liu, and Rui Yan. Re-creation of creations: A new paradigm\nfor lyric-to-melody generation, 2022.\n[17] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation\nwith diffusion models, 2021.\n[18] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Popmag: Pop music\naccompaniment generation. In Proceedings of the 28th ACM International Conference on\nMultimedia, MM \u201920, page 1198\u20131206, New York, NY, USA, 2020. Association for Computing\nMachinery.\n[19] Zhonghao Sheng, Kaitao Song, Xu Tan, Yi Ren, Wei Ye, Shikun Zhang, and Tao Qin. Songmass:\nAutomatic song writing with pre-training and alignment constraint. CoRR, abs/2012.05168,\n2020.\n[20] Li Shuyu and Yunsick Sung. Melodydiffusion: Chord-conditioned melody generation using a\ntransformer-based diffusion model. Mathematics 11, no. 8: 1915., 2023.\n[21] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-\npervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei,\neditors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of\nProceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015.\nPMLR.\n[22] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\nInternational Conference on Learning Representations, 2021.\n[23] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998\u20136008, 2017.\n[25] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. Midinet: A convolutional generative ad-\nversarial network for symbolic-domain music generation using 1d and 2d conditions. CoRR,\nabs/1703.10847, 2017.\n[26] Botao Yu, Peiling Lu, Rui Wang, Wei Hu, Xu Tan, Wei Ye, Shikun Zhang, Tao Qin, and Tie-Yan\nLiu. Museformer: Transformer with fine- and coarse-grained attention for music generation.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\nNeural Information Processing Systems, 2022.\n[27] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. MusicBERT:\nSymbolic music understanding with large-scale pre-training. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pages 791\u2013800, Online, August 2021.\nAssociation for Computational Linguistics.\n[28] Chen Zhang, Yi Ren, Kejun Zhang, and Shuicheng Yan. Sdmuse: Stochastic differential music\nediting and generation via hybrid representation, 2022.\n11\nA\nThe Number of Source-Target Divsions\nFor a given k-track music input, GETMusic can select m tracks as the source and generate n target\ntracks selected from the remaining k \u2212 m tracks. Considering that each track can be used as the\nsource, target, or left empty, there are 3k possible combinations. However, a specific scenario is\nthat m tracks are selected as the source and leaving the remaining k \u2212 m tracks empty, resulting in\nPk\nm=0 Cm\nk = 2k illegal combinations. Therefore, the number of valid combinations is 3k \u2212 2k.\nIn our setting, we have six instrumental tracks, resulting in 665 possible combinations. Notably, the\nchord progression track is not considered as 7-th track in this calculation because we consistently\nenable chord progression as a source track to enhance the quality of conditional generation.\nB\nData Pre-processing\nCleanse Data\nFollowing the method proposed in [18], we perform a data cleansing process by\nfour steps. Firstly, we employ MIDI Miner [8] to identify the melody track. Secondly, we condense\nthe remaining tracks into five instrument types: bass, drum, guitar, piano, and string. Thirdly, we\napply filtering criteria to exclude data that contains a minimal number of notes, has less than 2\ntracks, exhibits multiple tempos, or lacks the melody track. Fourthly, for all the data, we utilize\nthe Viterbi algorithm implemented by Magenta (https://github.com/magenta/magenta) to\ninfer the corresponding chord progression, which serves as an additional composition guide. Lastly,\nwe segment the data into fragments of up to 32 bars and convert these fragments into GETScore\nrepresentation.\nChord Progression\nThe configuration of the chord progression track is different from regular\ninstrumental tracks. Although certain commonly used chords may appear in specific instrumental\ntracks and have been represented as pitch tokens, we do not reuse these tokens to ensure that the\nchord progression track provides equitable guidance for each individual track.\nGETMusic incorporates 12 chord roots: C, C#, D, D#, E, F, F#, G, G#, A, A#, B and\n8 chord qualities: major, minor, diminished, augmented, major7, minor7, dominant,\nhalf-diminished. In the step four of the cleansing process above, we identify one chord per bar in\na music piece. In the chord progression track of GETScore, we allocate the chord root in the first row\nand the quality in the second row. The chord track is entirely filled, without any paddings. Figure 4 is\nan example of GETScore with the chord track.\n80\n10\n1\n1251\n1\n1251\n61\n4\n65\n4\n128\n1\n112\n2\n136\n5\n130\n4\n866\n8\n153\n1\n101\n2\n148\n1\n155\n1\n101\n3\n45\n2\n43\n1\n46\n2\n52\n4\n14\n2\n19\n2\n1\n1\n1\n1\n1247\n1092\n1023\n1247\n71\n71\n4\n4\nA\nmin\nA\nmin\nC\nmaj\nC\nmaj\nA\nmin\nA\nmin\nA\nmin\nA\nmin\nA\nmin\nA\nminj\nA\nmin\nA\nmin\nC\nmaj\nC\nmaj\nC\nmaj\nC\nmaj\nC\nmaj\nC\nmaj\nC\nmaj\nC\nmaj\n1023\nLead\n69\n4\n13\n2\n1\n792\n46\n1\n148\n1\n8\nBass\nDrum\nGuitar\nPiano\nString\nChord\n512 Units\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nFigure 4: An example shows the GETScore with seven tracks used in our experiment, where the\nnumbers denote the token indices. The example is for display only and does not correspond to a\nreal-world music piece.\nVocabulary\nIn the last step of the cleansing process mentioned above, the construction of the\nvocabulary is essential before converting music fragments into GETScores. In GETMusic, each track\nhas its own pitch vocabulary, while the duration vocabulary is shared among all tracks.\n12\nThe maximum duration supported by the GETMusic is 16 time units, resulting in a total of 17 duration\ntokens ranging from 0 (the special duration token for drums) to 16 time units. To construct the pitch\nvocabulary, the music is first normalized to either the C major or A minor key, which significantly\nreduces the number of pitch token combinations. For each track, we identify the unique (compound)\npitch tokens and rank them based on their frequency. During inference, the input music is also\nfirst normalized to C major or A minor and tokenized accordingly. GETMusic re-normalizes the\ngenerated music to its original key.\nThe final vocabulary consists of 17 duration tokens, 20 chord tokens, a padding token, a [MASK]\ntoken, an [EMPTY] token, and specific pitch tokens for each track: 128 for lead, 853 for bass, 4,369\nfor drums, 1,555 for piano, 3,568 for guitar, and 1,370 for strings. In total, the vocabulary consists of\n11,883 tokens.\n13\n"
  },
  {
    "title": "Discriminative Diffusion Models as Few-shot Vision and Language Learners",
    "link": "https://arxiv.org/pdf/2305.10722.pdf",
    "upvote": "1",
    "text": "Discriminative Diffusion Models\nas Few-shot Vision and Language Learners\nXuehai He1\nWeixi Feng2\nTsu-Jui Fu2\nVarun Jampani3 Arjun Akula3\nPradyumna Narayana3 Sugato Basu3 William Yang Wang2 Xin Eric Wang1\n1UC Santa Cruz, 2UC Santa Barbara, 3Google\n{xhe89,xwang366}@ucsc.edu\n{weixifeng,tsu-juifu,william}@ucsb.edu\n{arjunakula,varunjampani,pradyn,sugato}@google.com\nAbstract\nDiffusion models, such as Stable Diffusion (Rombach et al., 2022a), have shown\nincredible performance on text-to-image generation. Since text-to-image gener-\nation often requires models to generate visual concepts with fine-grained details\nand attributes specified in text prompts, can we leverage the powerful represen-\ntations learned by pre-trained diffusion models for discriminative tasks such as\nimage-text matching? To answer this question, we propose a novel approach,\nDiscriminative Stable Diffusion (DSD), which turns pre-trained text-to-image dif-\nfusion models into few-shot discriminative learners. Our approach mainly uses\nthe cross-attention score of a Stable Diffusion model to capture the mutual influ-\nence between visual and textual information and fine-tune the model via efficient\nattention-based prompt learning to perform image-text matching. By comparing\nDSD with state-of-the-art methods on several benchmark datasets, we demon-\nstrate the potential of using pre-trained diffusion models for discriminative tasks\nwith superior results on few-shot image-text matching. Codes can be found at\nhttps://github.com/eric-ai-lab/DSD.\n1\nIntroduction\n\u201cWhat I Cannot Create, I Do Not Understand.\u201d\nRichard Feynman\nThis quote by Richard Feynman perfectly captures the essence of human learning techniques. In the\ncontext of machine learning especially the area of vision and language, it can be interpreted as the\nability to generate images given text prompts is a strong indicator of understanding and matching\nbetween visual and textual information (Kwon et al., 2022a). Despite the success of various methods\nin the image-text matching task (Karpathy and Fei-Fei, 2015; Lee et al., 2018), there is still a need\nfor more advanced models that can better capture the fine-grained details, spatial relationships, and\ncompositionality. Meanwhile, diffusion models (Sohl-Dickstein et al., 2015a; Saharia et al., 2022b;\nRombach et al., 2022a) have been shown to produce high-quality and diverse images from text\ndescriptions. Therefore, in this paper, we investigate the idea of leveraging the power of pre-trained\nDiffusion Models, specifically the state-of-the-art text-to-image generative model\u2014Stable Diffusion,\nfor the discriminative image-text matching task, as shown in Figure 1. The success of Stable Diffusion\nin generative tasks suggests that it has a strong capability of understanding the relationship between\nPreprint. Under review.\narXiv:2305.10722v2  [cs.CV]  15 Nov 2023\nStable\nDiffusion\nDark brown horse with \nlong straight black tail \nstanding up in a field.\nDiscriminative\nStable\nDiffusion\nScore computation\nPrompt learning\nMatching\nScore:\n0.691\nDiscriminative\nStable\nDiffusion\nA commercial stainless \nkitchen with a pot of \nfood cooking.\nMatching\nScore:\n0.103\nDark brown horse with \nlong straight black tail \nstanding up in a field.\nFigure 1: The upper subfigure in the teaser image illustrates the ability of Stable Diffusion to generate\nrealistic images given a text prompt. The bottom subfigure illustrates the process of our proposed\nmethod, Discriminative Stable Diffusion (DSD), for utilizing Stable Diffusion for the image-text\nmatching task. DSD can output a matching score for a given text prompt and image, with a higher\nscore indicating a stronger match.\nvisual and textual information, and we aim to harness the understanding for image-text matching\ntasks.\nThe key advantages of using Stable Diffusion for text-image alignment are two folds: first, Stable\nDiffusion uses a pre-trained Variational Autoencoder (VAE) (Kingma and Welling, 2013) and cross-\nattention layers in its architecture, which provides strong compressed representations and shed\ninformation about the alignment of the data from different modalities. Second, Stable Diffusion has\nthe ability to understand spatial relations and fine-grained disentangled concepts, so as to generate\nimages per text prompts\u2019 requests, while traditional vision and language models pre-trained on\ndiscriminative tasks such as CLIP (Radford et al., 2021) only allow to model image-text contextual\nalignment at coarse-grained contextual (global) level but ignores the compositional matching of\ndisentangled concepts (i.e., finer-grained cross-modal alignment at region-word level) (Jiang et al.,\n2022).\nHowever, to efficiently adapt Stable Diffusion, a pre-trained text-to-image generation model, to the\nimage-text matching task, two key challenges need to be addressed: (1) how to disentangle the degree\nof alignment between the image and text from the latent space of Stable Diffusion? In text-to-image\ngeneration, the model is trained to generate an image that is semantically consistent with a given text\nprompt. However, in image-text matching, the task is to determine the degree of alignment between a\ngiven image and text. Therefore, it is important to disentangle the degree of alignment between the\nimage and text in the latent space of Stable Diffusion, to effectively use it for image-text matching;\n(2) how to efficiently adapt the model in the few-shot setting. Fine-tuning a text-to-image generation\nmodel like Stable Diffusion for image-text matching requires adapting the model from a generative\ntask to a discriminative task, which can be challenging and require much data.\nTo address these challenges, we propose the Discriminative Stable Diffusion (DSD) method, which\nincludes two key ideas: (1) identifying and leveraging attention scores from the cross-attention\nmaps in Stable Diffusion as the matching score and (2) using attention-based prompt learning to\nfine-tune the attention matrices. DSD can outperform the CLIP-based methods by 2.7% on the\nCompositional Visual Genome and 6.0% on the RefCOCOg datasets in terms of accuracy under the\nfew-shot setting. Our approach reveals the potential of diffusion models that can broaden their scope\nof use to discriminative tasks.\nOur contributions in this paper are threefold:\n\u2022 We do a pioneer study using latent text-to-image diffusion-based generative models which\nare initially proposed for generative tasks to address discriminative tasks such as image-text\nmatching.\n\u2022 We propose a new method based on exploiting the use of cross-attention maps of Stable\nDiffusion across layers and attention-based prompt learning for solving the image-text\nmatching task.\n2\n\u2022 We demonstrate the effectiveness of our approach through experimental evaluation under\nthe few-shot setting on both the Compositional Visual Genome (Jiang et al., 2022) and the\nRefCOCOg (Yu et al., 2016) datasets for image-text matching. We also extend our method\nto the visual question answering task, demonstrating its potency on the VQAv2 (Antol et al.,\n2015) dataset.\n2\nRelated Work\nDiffusion Probabilistic Models (DPMs) Diffusion probabilistic models (DPMs) have been widely\nused as generative models for images in recent years. These models, which include diffusion (Sohl-\nDickstein et al., 2015b) and score-based generative models (Song and Ermon, 2019), have been\nshown to outperform generative adversarial networks (GANs) (Goodfellow et al., 2014) in many\ncases. In the past two years, significant progress has been made in the development of DPMs, with a\nfocus on improving sampling techniques such as classifier-free guidance (Ho and Salimans, 2021).\nDPMs are typically implemented using convolutional U-Net architectures (Ronneberger et al., 2015a)\nwhich contain cross-attention layers. Hertz et al. (2022) finds that replacing attention maps in the\ncross-attention module of text-to-image generation diffusion models can edit image attributes. Just\nscaling the attention maps of the respective word can adjust the effect of a particular word in the\nprompt. Feng et al. (2022) demonstrates that one can retain the compositional semantics in the\ngenerated image by manipulating the cross-attention. Kumari et al. (2022) proposes to fine-tune\nthe key and value mapping from text to latent features in the cross-attention layers of text-to-image\ndiffusion model to compose multiple new concepts in the image. In the context of image-text\nmatching, the attention scores between the text and image representations in the DPMs can reflect the\ndegree of alignment between them.\nFew-shot Learning for Vision and language Tasks Vision and Language discriminative models pre-\ntrained on large-scale image-text pairs have demonstrated great potential in multimodal representation\nlearning (Jia et al., 2021; Yao et al., 2021; Yuan et al., 2021; Radford et al., 2021). Among them,\nCLIP (Radford et al., 2021) benefits from 400M curated data and defines various prompt templates to\ncarry out zero-shot image classification. Like CLIP, several different few-shot learners were proposed.\nGPT (Brown et al., 2020), as a strong few-shot learner, is capable of performing a new language task\nby learning from only a few training instances. Frozen (Tsimpoukelli et al., 2021) is developed based\non GPT and made into a multimodal few-shot learner by expanding the soft prompting to include a\ncollection of images and text. The concept of prompt learning (Schick and Sch\u00fctze, 2020) has been\nwidely explored in natural language processing (NLP) and computer vision. It allows pre-trained\nmodels to adapt to various downstream tasks with minimal data by introducing a small prompt\nlayer (Schick and Sch\u00fctze, 2020; Liu et al., 2021). In the context of image-text matching, prompt\nlearning has been used to fine-tune pre-trained models for the task (He et al., 2022b). In our work,\ninstead of adding learnable prompts over the inputs or between transformer layers (Jia et al., 2022),\nwe introduce learnable prompts over the attention layers. In our paper, our primary research question\nis the adaptation of pre-trained generative diffusion models into discriminative models for specific\ntasks. This focus is driven by the challenges and opportunities presented by utilizing diffusion-based\nprocesses in a discriminative setting, specifically for the image-text matching task, which has distinct\ncharacteristics compared to the modeling approaches mentioned above.\nGenerative Models for Discriminative Tasks There has been a significant amount of research on\nusing generative models for discriminative tasks in the past decades (Zimmermann et al., 2021; Croce\net al., 2020). Ng and Jordan (2001) compare the discriminative classifier with generative classifier.\nRanzato et al. (2011) apply deep generative models to the recognition task. For diffusion models,\nrecently, Li et al. (2023); Clark and Jaini (2023) propose to use pre-trained diffusion models for\nzero-shot classification. Krojer et al. (2023) extends the methods to the image-text retrieval task.\nWei et al. (2023) formulate diffusion models as masked autoencoders and achieves state-of-the-art\nclassification accuracy on video tasks. Different from these works, we are the first to explore the use\nof pre-trained diffusion models for discriminative tasks, specifically the image-text matching task.\nAnother line of works use diffusion models as data source and then training a discriminative model\non the synthetic data generated from it (He et al., 2022a; Jahanian et al., 2021; Zhang et al., 2021).\nDiffers from these works, our approach emphasizes the direct adaptation of generative diffusion\nmodels, leveraging their pre-existing structures and knowledge without the need to generate synthetic\ndata.\n3\nCLIP\ntext \nencoder\nLatent \nVector\n\ud835\udc4d!\nCross-\nattention\nCross-\nattention\nCross-\nattention\nCross-\nattention\nCross-\nattention\nCross-\nattention\nCross-\nattention\nLatent \nVector\n\ud835\udc4d\"\n\u2026\n\u00d7(T-1)\nVAE\nencoder\nLearnable prompt\n\ud835\udc4a!\n\ud835\udc4a\" +\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nCross-\nattention\n\ud835\udc3e\n\ud835\udc49\nLogSumPooling\n\ud835\udc4a#!\n\ud835\udc4a#\"\nm\nn\nLSE Pooling\nm\nn\n\u2026\nm\nn\nm\nn\nScore\nA man \nwearing a\nwhite T-shirt\nis dropping \na racket on\na court.\nT\n\u2026\nFigure 2: Overview of our Discriminative Stable Diffusion framework, which measures how much\nthe given images and texts matched use the cross-attention mechanism in the Stable Diffusion.\nDiscriminative Stable Diffusion added learnable prompts over attention matrices (red boxes), which\nare fine-tuned under the few-shot setting.\n3\nPreliminaries on Diffusion Models\nIn this section, we provide a brief overview of the concepts and techniques in denoising diffusion\nmodels that are necessary to understand our proposed method. Diffusion models are a class of\ngenerative models that are particularly effective at generating high-quality images (Sohl-Dickstein\net al., 2015b; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022a; Rombach et al., 2022b).\nThey aim to model a distribution p\u03b8 (x0) that approximates the data distribution q (x0) and is easy to\nsample from. DPMs model a \"forward process\" in the space of x0 from data to noise by adding noise\nto real data, and a reverse process that tries to reconstruct the original data from the noisy version.\nThe forward process is described by the equation\nq(xt|x0) = N(xt; \u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I),\n(1)\nwhere x1:T defines a set of noisy images and x0 is the initial image. N denotes a Gaussian distribution,\nand \u00af\u03b1t are hyperparameters. The reverse process is modeled by a Gaussian distribution\np\u03b8(xt\u22121|xt) = N(\u00b5\u03b8(xt), \u03a3\u03b8(xt)),\n(2)\nwhere neural networks are used to predict the mean and covariance of the distribution. The parameters\nof the model, \u03b8, are learned by optimizing a variational lower bound on the log-likelihood of the real\ndata. Once trained, new images can be generated by starting from a noise sample and iteratively\nsampling from the reverse process distribution until reaching the final time step. In latent diffusion\nprobabilistic models such as Stable Diffusion, this two process are similar, while they proceeds in the\nlatent space: x0 is encoded into z0 in an efficient, low-dimensional latent space first and then do the\ndiffusion process. And in the case where a DPM is conditioned on additional information, such as\ntext information c, the reverse process becomes p\u03b8(zt\u22121|zt, y), where y is the input text.\n4\nDiscriminative Latent Diffusion Models\n4.1\nProblem Formulation\nThe problem of image-text matching is formalized as follows: given a text prompt y \u2208 Y and a set of\nimages X, we aim to find the image x\u2217 \u2208 X that is most semantically aligned with the given text\nprompt y.\nFormally, we define the image-text matching problem as finding the function f : Y \u00d7 X \u2192 [0, 1] that\nassigns a score to each image-text pair (y, x) indicating the degree of semantic alignment between\nthe text and image. The goal is to find the image x\u2217 that maximizes the score for a given text prompt\ny, i.e., x\u2217 = arg maxx\u2208X f(y, x).\n4\n4.2\nMethod Overview\nTo learn the function f, the main idea is to leverage the powerful representations learned by a\npre-trained Stable Diffusion model to perform image-text matching. There are three key modules in\nDSD, cross-attention score computation, LogSumExp pooling, and attention-based prompt learning,\nas shown in Figure 2. The cross-attention score computation module extracts the mutual influence\nbetween visual and textual information by computing the attention scores from cross-attention\nmatrices in U-Nets of the Stable Diffusion model. The LogSumExp pooling module pools these\nattention scores all over tokens to obtain a single matching score. Finally, the attention-based prompt\nlearning module fine-tunes the model by updating the key and value mappings from text to latent\nfeatures in the cross-attention layers under a few-shot setting. This allows the model to learn new\nimage-text concepts while retaining the ability to capture complex and nuanced relationships between\nimages and text. The model outputs a score that measures the alignment between the image and text,\nwhich can be used to adapt the model from a text-to-image generation task to an image-text matching\ntask.\n4.3\nCross-attention Score Computation\nCross-attention scores are a measure of the relevance of an image and a text to each other (Chen et al.,\n2020; Li et al., 2019). They are calculated by taking the dot product of the representations of the\nimage and text in a latent space, and normalizing by the product of their norms. We propose to adapt\ncross-attention scores as a way to better capture the complex relationships between images and text\nin the image-text matching task. In the sequel, we elaborate on our strategy in depth.\nStable Diffusion (Jaegle et al., 2021) is trained to generate images from text prompts, and as such, it\nhas learned strong compressed representations of both text and images. We can make use of these\nrepresentations to learn the function f for image-text matching.\nMore specifically, given a text prompt y, we first encode it into a intermediate text representation\nry = \u03c4\u03b8(y) \u2208 RM\u00d7d\u03c4 using the domain specific encoder \u03c4\u03b8. We then encode each image x \u2208 X\nwhere x \u2208 RH\u00d7W \u00d73 in RGB space into a latent image representation z = E(x), where E(x)\nis the encoder. The encoder \u03f5\u03b8 in the U-Net (Ronneberger et al., 2015b) of the pre-trained text-\nto-image generation model then encode z into rx = \u03c6i (zt), where \u03c6i (zt) \u2208 RN\u00d7di\n\u03f5 denotes a\n(flattened) intermediate representation of the UNet implementing \u03f5\u03b8, which are then mapped to\nintermediate layers of the UNet via a cross-attention layer implementing A = softmax\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V ,\nwith Q = W q(i) \u00b7 rx, K = W k(i) \u00b7 ry, V = W v(i) \u00b7 ry. Here, W v(i) \u2208 Rd\u00d7di\n\u03f5, W q(i) \u2208 Rd\u00d7d\u03c4 , W k(i) \u2208\nRd\u00d7d\u03c4 are learnable projection matrices (Jaegle et al., 2021; Vaswani et al., 2017), mapping the\ninputs to a query, key, and value feature, respectively, and d\u03c4 is the output dimension of key and\nquery features.\n4.4\nLogSumExp Pooling (LSE)\nTo compute the function g and quantitatively evaluate the degree of semantic alignment between an\nimage and a text prompt, we leverage LogSumExp (LSE) pooling (Blanchard et al., 2021) as a means\nof aggregating the attention maps generated by the cross-attention mechanism in our model. By using\nLSE pooling, we are able to take into account the relative importance of different image and text\ntokens in the attention map, rather than simply averaging or summing all elements in the map. This\nhas several benefits. Firstly, LSE pooling is able to handle large values and outliers in the attention\nmap more robustly than other pooling methods, such as average or sum pooling. Secondly, LSE\npooling has high numerical stability during training. Thirdly, LSE pooling is able to better preserve\nthe ordering of values in the attention map, allowing for more interpretable and accurate matching\nscores.\nFor notation simplicity, we drop the batch and attention head dimension, the attention map matrix is\ndenoted as A \u2208 Rn\u00d7m, where n and m are the number of image tokens (height \u00d7 width) in the latent\nspace and length of text tokens, respectively. The LSE pooling operator is defined as:\nS(A) = 1\n\u03bb log\n n\nX\ni=1\nexp (\u03bbAi,:)\n!\n(3)\n5\nWhere Ai,: represents the i-th row of the matrix A. \u03bb is a factor that determines how much to magnify\nthe importance of the most relevant pairs of image region features and attended text sentence vectors,\nwhich by default we took the value of 1.\nThe score for the image-text pair (y, x) is then computed by averaging sampled across-attention\nmaps, denoted by\nf(y, x) = Ave(S(A)) = g(A)\n(4)\nwhere g : RM\u00d7d \u00d7 RN\u00d7d \u2192 [0, 1] is a scoring function that measures the degree of semantic\nalignment between the text and image representations.\nOverall, our method combines the strengths of the U-Net architecture and the attention mechanism\nof the Stable Diffusion model. We resort to attention-based prompt learning (Lester et al., 2021) to\nefficiently adapt the model to perform image-text matching.\n4.5\nAttention-based Prompt Learning for Stable Diffusion\nWe aim to adapt the latent diffusion probabilistic model to the image-text matching task leveraging\nonly a few examples, that is, under the few-shot setting. The task of fine-tuning aims at updating the\nmapping from the given text to the aligned image distribution, and the text features are only input\nto W k and W v projection matrix in the cross-attention block. Therefore, we propose the use of\nlearnable prompts, which are added to the attention matrices in our model. Specifically, as shown in\nFigure 2, we introduce learnable prompt embedding matrices, which are added element-wise to the\nkey and value attention matrices ruing training and inference. To improve the fine-tuning efficiency,\nwe implement the attention-based prompt learning using LoRA (Hu et al., 2021). As our addition\noperation applies to all layers and sampled time-steps, we will omit superscripts t and layer l for\nnotational clarity and obtains:\nW \u2032 = W + Wp.\n(5)\nwhere W is the original attention matrix, Wp is decomposed into the products of two low-rank\nmatrices and updated during training. This allows the model to adapt to new instances by attending\nto relevant information in the intermediate representation of the text inputs, \u03c4\u03b8(y). With the learned\nprompt embeddings in the few-shot scenario, we can effectively adapt the Stable Diffusion to improve\nthe image-text matching performance. The overall algorithm is shown in Algorithm 1. For optimiza-\ntion, we use the margin-based triplet loss function between the predicted match score and the true\nmatch score. Let L be the loss, we have: L = E(max\n\u00000, d(rxpos, \u03c4\u03b8(yn)) \u2212 d(rxneg, \u03c4\u03b8(yn)) + m\n\u0001\n),\nwhere d(\u00b7, \u00b7) denotes the distance based on the similarity score. Specifically, inspired by Li et al.\n(2023); Clark and Jaini (2023); Krojer et al. (2023), we combine the score computed from the\ncross-attention map and the distance between the predicted noise and the groudtruth to obtain d,\nwhich we find can lead to better performance. rxpos is the groundtruth image representation for the\nn-th text yn, rxneg is the negative image representation, and m is a predefined margin.\n5\nExperiments\n5.1\nDatasets\nWe use the Compositional Visual Genome (ComVG) (Krishna et al., 2017) and RefCOCOg (Yu et al.,\n2016) datasets to do image-text matching, which requires model\u2019s ability to understand fine-grained\ndetails and spatial relationships of image and text pairs, that are more challenging for traditional\nvison-language models. We also test on the VQAv2 dataset to see if our method can be extended to\nother vision and language tasks.\nCompositional Visual Genome (ComVG) (Krishna et al., 2017) is a reconstructed dataset of the\nVisual Genome (Krishna et al., 2017) dataset, which contains 108,007 images annotated with 2.3\nmillion relationships. These relationships are represented as subject-predicate-object triplets and\ninclude both action and spatial relationships. ComVG was created by selecting a subset of 542 images\nfrom Visual Genome that contain clear relationships, and generating mutated images by changing a\nsingle value in the subject, predicate, or object of each image. These mutated images were then used\nas negative examples, while the original images were used as positive examples, resulting in a total of\n5400 data points.\n6\nAlgorithm 1 Image-Text Matching with Discriminative Stable Diffusion\n1: I: Image space\n2: T: Text space\n3: x: Image\n4: y: Text\n5: z: Latent representation\n6: E: Encoder\n7: \u03c4: Domain-specific encoder\n8: \u03c6: Intermediate representation of the U-Net\n9: function DSD(I, T)\n10:\nfor (i, t) in the batch do\n11:\nImage latent representation zi \u2190E(x)\n12:\nText latent representation ry \u2190\u03c4(y)\n13:\nIntermediate representation rx \u2190\u03c6(zt)\n14:\nUpdate W \u2032 \u2190 W\n\u25b7 Eq. 5\n15:\nCompute attention maps A \u2190 ry, rx\n16:\nCompute LSE score S(A)\u2190A\n\u25b7 Eq. 3\n17:\nCompute matching score g(A) \u2190 A\n\u25b7 Eq. 4\n18:\nCompute loss L \u2190 yn\n19:\nUpdate Wp\n20:\nend for\n21: end function\nRefCOCOg (Yu et al., 2016) is a reconstructed dataset of the MSCOCO (Lin et al., 2014) dataset.\nThe dataset was created in a non-interactive setting, using Amazon Mechanical Turk workers. The\nprocess consisted of two stages: first, workers were asked to write referring expressions for objects in\nthe images, and then, another set of workers were asked to indicate the referred object in the image\nby clicking on it. We randomly sample 10 text prompts from the candidates pool including one\ngroundtruth, and the model is asked to do the correct matching given the image and the 10 sampled\ntext prompts.\nVQAv2 (Goyal et al., 2017) The VQAv2 dataset Goyal et al. (2017) is commonly converted to a\nclassification task with 3,129 answer classes with frequency large than 9. In our setting, we modify\nthe candidate text to be the concatenation of question and answer pair for each question and perform\nmatching with images. We test the model for both \u201cbinary\u201d type questions and also \u201cother\u201d type\nquestions. For binary questions where answers tend to be closely related (\"Yes/No\"), we rewrite the\nanswers and conduct a matching process against a pool of ten images. In the case of open-ended\nquestions, we match the image with ten different textual prompts to test the model\u2019s matching\naccuracy.\n5.2\nExperimental Setup\nWe employed the Stable Diffusion V21 model in conjunction with xFormers (Lefaudeux et al., 2022)\nand FlashAttention (Dao et al., 2022) using the implementation available in HuggingFace Diffusers 2.\nThe Stable Diffusion utilizes a subset of the LAION-5B (Schuhmann et al., 2022) dataset during\npretraining, specifically 170 million examples, along with LAION-2B-en and LAION-aesthetics v2\ndatasets for pretraining. We use LoRA training for the implementation of attention-based prompt\nlearning. We test our Discriminative Stable Diffusion under the few-shot setting (Yoo et al., 2021)\nwhere we use 5% data to train the model.\n5.3\nBaselines\nIn order to provide a comprehensive evaluation of our Discriminative Stable Diffusion method, we\nestablish two baselines for comparison.\n1https://huggingface.co/stabilityai/stable-diffusion-2-1\n2https://huggingface.co/docs/diffusers/index\n7\nTable 1: Comparison of accuracy (%) on Compositional Visual Genome (ComVG) and Top-1 and\nTop-5 accuracy (%) on RefCOCOg using CLIP and Discriminative Stable Diffusion (DSD) under the\nfew-shot setting. Our method outperforms CLIP based baselines, demonstrating the superiority of our\napproach compared with traditional vision and language pre-trained models such as CLIP pre-trained\nfor discriminative tasks.\nMethod\nCompositional Visual Genome\nRefCOCOg\nSubjects\nObjects\nPredicate\nAverage\nTop-1 Acc.\nTop-5 Acc.\nCLIP (Fine-tuning)\n80.77\n82.49\n60.50\n76.10\n69.88\n84.57\nCLIP (Prompt Learning)\n78.88\n79.51\n60.41\n74.24\n69.40\n84.48\nDSD\n79.88\n86.90\n63.20\n78.81\n75.87\n91.96\nTable 2: Comparison of accuracy (%) on the sampled \u2018binary\u2019 and \u2019other\u2019 questions from the VQAv2\ndataset under the few-shot setting. Our method outperforms CLIP, demonstrating the superiority of\nour approach compared with traditional vision and language pre-trained models pre-trained on other\nvision and language tasks.\nMethod\nBinary\nOther\nAll\nCLIP (Fine-tuning)\n66.94\n32.41\n59.06\nCLIP (Prompt Learning)\n67.32\n33.42\n59.58\nDSD\n67.36\n35.90\n60.18\n\u2022 Fine-tuning (Radford et al., 2021): The first baseline involves fine-tuning the CLIP model,\nwith the last layer being the only component subject to tuning.\n\u2022 Prompt learning: The second baseline is based on the prompt learning strategy applied to\nCLIP, incorporating learnable prompts to the textual inputs that are conditioned on individual\ninput images, as described in Zhou et al. (2022).\n5.4\nResults\nTo compare the performance of our method to other approaches, we conducted fine-tuning on CLIP\nand prompt learning on CLIP, in addition to our method. The results of these experiments are\nsummarized in Table 1 on the Compositional Visual Genome dataset and RefCOCOg dataset. From\nthe results, it is clear that our method outperforms both fine-tuning on CLIP and prompt learning on\nCLIP on both Compositional Visual Genome across the three different problem types and RefCOCOg.\nWe also show the results on the VQAv2 dataset in Table 2. These results demonstrate the extentiveness\nof our method to other vision and language tasks.\n5.5\nAblation Studies\nIn this section, we delve deeper into the nuances of our experimental findings in the zero-shot setting.\nEffect of Attention Maps from Different Sets of U-Net Layers We investigate the effect of using\ndifferent numbers of layers in the U-Net of Stable Diffusion to compute the attention map. We use\ntwo variants of Stable Diffusion v2 and take the average of the attention maps from different layers\nof U-Net. Specifically, we consider the last one, the last two, the last eight, and all layers. Kwon\net al. (2022b); Tang et al. (2022) indicate that the latter layers typically contain more semantically\nimportant information. The results, shown in Figure 3, indicate that using all layers in the U-Net\ngives the best performance in terms of accuracy, suggesting that the information from all layers can\nmake use of both the high-level and low-level features when making predictions and preserve both\ncoarse and fine image details for the image-text matching task. The later layers in the U-Net may\ncontain similar task-specific information, which can be found from observing that using only the last\ntwo layers also provides a close performance with that of one.\nCosine Similarity vs. Maximum vs. LogSumExp Pooling for Score Computation We compare the\noverall accuracy of using Cosine Similarity, Maximum value from the attention map, and LogSumExp\n8\n1\n2\n8\n16\nNumbers of Layers from U-Net\n74.6\n74.8\n75.0\n75.2\n75.4\nAccuracy (%)\nStable Diffusion 2.1-v (768x768 resolution)\nStable Diffusion 2.1-base (512x512 resolution)\nFigure 3: Ablation study on the number of attention maps used from layers of the U-Net (x-axis).\nThe y-axis represents the accuracy on the ComVG dataset. Tests on two variants of Stable-Diffusion\nv2: trained as a standard noise-prediction model on 512x512 images and 768x768 images.\nCosine similarity\nMaximum\nLogSumExp\n0\n20\n40\n60\nAccuracy (%)\n43.0\n57.7\n75.4\nFigure 4: Ablation study on using cosine similarity, maximum value from each column of the\nattention map, and the smoothed maximum (LogSumExp pooling).\nPooling for score computation in Figure 4. As can be seen, using LogSumExp performs the best,\nfollowed by the maximum value, and finally the cosine similarity. This suggests that LogSumExp\ncan effectively capture the overall importance of each element in the attention map, rather than just\nrelying on the maximum value. Additionally, LogSumExp can smooth out the influence of individual\nnoisy elements, resulting in more robust and accurate matching scores. As the dimensions of the\nimage feature and text feature vectors in Stable Diffusion are not the same, we implement the cosine\nsimilarity by only comparing the shared dimensions of the image and text feature vectors. Overall,\nthese results highlight the effectiveness of LogSumExp as a method for computing the matching\nscore in image-text matching tasks.\nEnsembling over Noise Levels In diffusion models, the level of noise controls the variance of the\nGaussian noise added during the diffusion process, which can affect the degree of change in the\ngenerated image. To further improve the performance of our method, we use an ensemble technique\ninspired by Wolleb et al. (2022) by averaging the scores over four different noise levels: {0.2,\n0.4, 0.6, 0.8}. This is done by first obtaining the score under each noise level scenario and then\naveraging them. The results of this comparison are shown in Figure 5. Our experimental results\ndemonstrate that this ensemble technique leads to a noticeable improvement in performance, and\ntherefore we sample over different timesteps to obtain the final average score.\n6\nConclusion\nIn this paper, we proposed a method for matching text and images in the latent space of Stable\nDiffusion. By adapting generative models to discriminative tasks, we are exploring a domain with\npromising practical applications, which could be particularly beneficial in scenarios with data privacy\nconcerns or when there is a lack of data in specific domains. For methods, we fine-tuned the U-Net\npart of the model by focusing on the cross-attention between text embeddings and image embeddings,\nwhich reflects the alignment between text and image. Our results show that this fine-tuning approach\nimproves the alignment between text and image, leading to better performance in image-text matching\ntasks. Overall, our approach is pioneering work that leverages the inherent flexibility of diffusion-\nbased visual generative models, opening new pathways for innovation where traditional methods may\n9\nNoise level 0.4\nNoise level 0.8\nEnsembled\n0\n20\n40\n60\nAccuracy (%)\n72.5\n72.7\n73.1\nFigure 5: Ablation study on the amount of noise added during the diffusion process: using consistent\nnoise levels of 0.4, 0.8 and using ensembling.\nfall short. Our results can motivate research on simpler alternatives to adapt Stable Diffusion models,\nas well as on future methods for better utilization of them.\nA\nLimitations & Broader Impact\nThe Discriminative Stable Diffusion (DSD) approach proposed in this paper is dependent on a\npre-trained Stable Diffusion model, which may be challenging to obtain in certain scenarios where\nthe model has yet to be publicly released or where the computational resources required for training\nare not available. Additionally, the quality of the pre-training can greatly impact the performance of\nDSD, highlighting the need for further research to investigate methods for improving the pre-training\nprocess. While our fine-tuning process is based on prompt learning, there are other techniques such as\nmulti-task learning and meta-learning that can be possibly incorporated to improve the performance\nof DSD. Future research should explore the use of these techniques in combination with prompt\nlearning to further improve the few-shot discriminative performance of DSD. It is worth noting that\nin real-world scenarios, there is often a limited amount of labeled data available, and collecting\nmore data can be costly and time-consuming. Therefore, the ability of DSD to perform well under a\nfew-shot setting is an important aspect of its potential utility in practical applications.\nB\nEthical Statement\nThis paper proposes a novel framework for few-shot learning that adapts pre-trained stable diffusion\nmodels for discriminative tasks. The paper uses publicly available datasets for image and text\nmatching and visual question answering, and does not involve any human or animal subjects. The\npaper also acknowledges the limitations and challenges of the proposed approach, such as the\ncomputational cost. The paper does not intend to cause any harm or bias to any individual or group,\nand respects the intellectual property and ethical standards of the research community. The paper also\ndiscusses some potential applications and implications of the proposed framework, such as paving a\nway to efficiently adapt pre-trained stable diffusion models for discriminative tasks. The paper hopes\nto inspire further research and innovation in few-shot learning and vision and language domains.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question answering. In ICCV.\nPierre Blanchard, Desmond J Higham, and Nicholas J Higham. 2021. Accurately computing the\nlog-sum-exp and softmax functions. IMA Journal of Numerical Analysis, 41(4):2311\u20132330.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference\non computer vision, pages 104\u2013120. Springer.\n10\nKevin Clark and Priyank Jaini. 2023. Text-to-image diffusion models are zero-shot classifiers. arXiv\npreprint arXiv:2303.15233.\nDanilo Croce, Giuseppe Castellucci, and Roberto Basili. 2020. Gan-bert: Generative adversarial\nlearning for robust text classification with a bunch of labeled examples.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. FlashAttention: Fast\nand memory-efficient exact attention with IO-awareness. In Advances in Neural Information\nProcessing Systems.\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. 2022. Training-free structured diffusion guidance\nfor compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural\ninformation processing systems, pages 2672\u20132680.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v\nin vqa matter: Elevating the role of image understanding in visual question answering. In CVPR.\nRuifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan\nQi. 2022a. Is synthetic data from generative models ready for image recognition? arXiv preprint\narXiv:2210.07574.\nXuehai He, Diji Yang, Weixi Feng, Tsu-Jui Fu, Arjun Akula, Varun Jampani, Pradyumna Narayana,\nSugato Basu, William Yang Wang, and Xin Eric Wang. 2022b. Cpl: Counterfactual prompt\nlearning for vision and language models. arXiv preprint arXiv:2210.10362.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626.\nJonathan Ho and Tim Salimans. 2021. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop\non Deep Generative Models and Downstream Applications.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Mod-\nels. arXiv:2106.09685 [cs].\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.\n2021. Perceiver: General perception with iterative attention. In International conference on\nmachine learning, pages 4651\u20134664. PMLR.\nAli Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. 2021. Generative models as a data\nsource for multiview representation learning. arXiv preprint arXiv:2106.05258.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916.\nPMLR.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\nSer-Nam Lim. 2022. Visual prompt tuning. arXiv preprint arXiv:2203.12119.\nKenan Jiang, Xuehai He, Ruize Xu, and Xin Eric Wang. 2022. Comclip: Training-free compositional\nimage and text matching. arXiv preprint arXiv:2211.13854.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image\ndescriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 3128\u20133137.\nDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114.\n11\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. International journal of computer vision,\n123(1):32\u201373.\nBenno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, and Siva Reddy. 2023. Are diffu-\nsion models vision-and-language reasoners? In Thirty-seventh Conference on Neural Information\nProcessing Systems.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2022. Multi-\nconcept customization of text-to-image diffusion. arXiv preprint arXiv:2212.04488.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. 2022a. Diffusion models already have a semantic\nlatent space. arXiv preprint arXiv:2210.10960.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. 2022b. Diffusion models already have a semantic\nlatent space. arXiv preprint arXiv:2210.10960.\nKuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention\nfor image-text matching. In Proceedings of the European Conference on Computer Vision (ECCV),\npages 201\u2013216.\nBenjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean\nNaren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\n2022. xformers: A modular and hackable transformer modelling library. https://github.com/\nfacebookresearch/xformers.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient\nprompt tuning. arXiv preprint arXiv:2104.08691.\nAlexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. 2023. Your\ndiffusion model is secretly a zero-shot classifier. arXiv preprint arXiv:2303.16203.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-tuning v2:\nPrompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint\narXiv:2110.07602.\nAndrew Ng and Michael Jordan. 2001. On discriminative vs. generative classifiers: A comparison of\nlogistic regression and naive bayes. Advances in neural information processing systems, 14.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable\nvisual models from natural language supervision. arXiv preprint arXiv:2103.00020.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125.\nMarc\u2019Aurelio Ranzato, Joshua Susskind, Volodymyr Mnih, and Geoffrey Hinton. 2011. On deep\ngenerative models with applications to recognition. In CVPR 2011, pages 2857\u20132864. IEEE.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022a.\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695.\n12\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022b.\nHigh-resolution image synthesis with latent diffusion models. In CVPR.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015a. U-net: Convolutional networks for\nbiomedical image segmentation. pages 234\u2013241.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015b. U-net: Convolutional networks for\nbiomedical image segmentation. In International Conference on Medical image computing and\ncomputer-assisted intervention, pages 234\u2013241. Springer.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar\nSeyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans,\nJonathan Ho, David J Fleet, and Mohammad Norouzi. 2022a. Photorealistic text-to-image diffusion\nmodels with deep language understanding. arXiv:2205.11487.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022b. Photo-\nrealistic text-to-image diffusion models with deep language understanding. Advances in Neural\nInformation Processing Systems, 35:36479\u201336494.\nTimo Schick and Hinrich Sch\u00fctze. 2020. Exploiting cloze questions for few shot text classification\nand natural language inference. arXiv preprint arXiv:2001.07676.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015a.\nDeep\nunsupervised learning using nonequilibrium thermodynamics. In International Conference on\nMachine Learning, pages 2256\u20132265. PMLR.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015b.\nDeep\nunsupervised learning using nonequilibrium thermodynamics. In ICML.\nYang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data\ndistribution. In NeurIPS.\nRaphael Tang, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Jimmy Lin, and Ferhan\nTure. 2022. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint\narXiv:2210.04885.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language models. Advances in Neural Information\nProcessing Systems, 34:200\u2013212.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.\nChen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang,\nCihang Xie, Alan Yuille, and Christoph Feichtenhofer. 2023. Diffusion models as masked autoen-\ncoders. arXiv preprint arXiv:2304.03283.\nJulia Wolleb, Robin Sandk\u00fchler, Florentin Bieder, Philippe Valmaggia, and Philippe C Cattin. 2022.\nDiffusion models for implicit image segmentation ensembles. In International Conference on\nMedical Imaging with Deep Learning, pages 1336\u20131348. PMLR.\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo\nLi, Xin Jiang, and Chunjing Xu. 2021. Filip: Fine-grained interactive language-image pre-training.\narXiv preprint arXiv:2111.07783.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826.\n13\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. 2016. Modeling\ncontext in referring expressions. In European Conference on Computer Vision, pages 69\u201385.\nSpringer.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,\nXuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A new foundation model for\ncomputer vision. arXiv preprint arXiv:2111.11432.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio\nTorralba, and Sanja Fidler. 2021. Datasetgan: Efficient labeled data factory with minimal human\neffort. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10145\u201310155.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Conditional prompt learning\nfor vision-language models. arXiv preprint arXiv:2203.05557.\nRoland S Zimmermann, Lukas Schott, Yang Song, Benjamin A Dunn, and David A Klindt. 2021.\nScore-based generative classifiers. arXiv preprint arXiv:2110.00473.\n14\n"
  },
  {
    "title": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models",
    "link": "https://arxiv.org/pdf/2305.10474.pdf",
    "upvote": "1",
    "text": "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models\nSongwei Ge*\nUniversity of Maryland\nSeungjun Nah\nNVIDIA\nGuilin Liu\nNVIDIA\nTyler Poon\nUniversity of Chicago\nAndrew Tao\nNVIDIA\nBryan Catanzaro\nNVIDIA\nDavid Jacobs\nUniversity of Maryland\nJia-Bin Huang\nUniversity of Maryland\nMing-Yu Liu\nNVIDIA\nYogesh Balaji\nNVIDIA\nA very happy fuzzy panda dressed as a chef\neating pizza in the New York street food\ntruck.\nThe supernova explosion of a white dwarf in\nthe universe, photo realistic.\nA high-quality 3D render of hyperrealist, su-\nper strong, multicolor stripped, and fluffy\nbear with wings, highly detailed.\nFigure 1: Given a text description, our approach can faithfully generate videos that are consistent with the input text while\nbeing photorealistic and temporally consistent. Best viewed with Acrobat Reader. Click the images to play the video clips.\nAbstract\nDespite tremendous progress in generating high-quality\nimages using diffusion models, synthesizing a sequence of\nanimated frames that are both photorealistic and tempo-\nrally coherent is still in its infancy.\nWhile off-the-shelf\nbillion-scale datasets for image generation are available,\ncollecting similar video data of the same scale is still chal-\nlenging. Also, training a video diffusion model is compu-\ntationally much more expensive than its image counterpart.\nIn this work, we explore finetuning a pretrained image dif-\nfusion model with video data as a practical solution for\nthe video synthesis task. We find that naively extending\nthe image noise prior to video noise prior in video diffu-\nsion leads to sub-optimal performance. Our carefully de-\nsigned video noise prior leads to substantially better perfor-\nmance. Extensive experimental validation shows that our\n*Work done during an internship at NVIDIA.\nmodel, Preserve Your Own COrrelation (PYoCo), attains\nSOTA zero-shot text-to-video results on the UCF-101 and\nMSR-VTT benchmarks. It also achieves SOTA video gener-\nation quality on the small-scale UCF-101 benchmark with\na 10\u00d7 smaller model using significantly less computation\nthan the prior art. The project page is available at https:\n//research.nvidia.com/labs/dir/pyoco/.\n1. Introduction\nLarge-scale diffusion-based text-to-image models [38, 42,\n2] have demonstrated impressive capabilities in turning com-\nplex text descriptions into photorealistic images. They can\ngenerate images with novel concepts unseen during train-\ning. Sophisticated image editing and processing tasks can\neasily be accomplished through guidance control and em-\nbedding techniques. Due to the immense success in several\napplications [30, 68, 5], these models are established as pow-\n1\narXiv:2305.10474v2  [cs.CV]  30 Aug 2023\n(a)\ni.i.d. noise prior\nProgressive noise prior\n(b)\nFigure 2: Visualizing the noise map correlations. (a) visualizes the t-SNE plot of the noise maps corresponding to input\nframes randomly sampled from videos. These noise maps are obtained by running a diffusion ODE [49, 48] on the input\nframes using a trained text-to-image model, but in the opposite direction of image synthesis (\u03c3 : 0 \u2192 \u03c3max). The green dots\nin the background denote the reference noise maps sampled from an i.i.d. Gaussian distribution. The red dots and yellow\ndots are noise maps corresponding to input frames coming from different videos. We found they are spread out and share no\ncorrelation. On the other hand, the noise maps corresponding to the frames coming from the same video (shown in blue dots)\nare clustered together. (b) Using an i.i.d. noise model (orange dots) for finetuning text-to-image models for video synthesis is\nnot ideal since temporal correlations between frames are not modeled. To remedy this, we propose a progressive noise model\nin which the correlation between different noise maps is injected along the temporal axis. Our progressive noise model (blue\ndots) aptly models the correlations present in the video noise maps.\nerful image synthesis tools for content generation. As image\nsynthesis is largely democratized with the success of these\ntext-to-image models, it is natural to ask whether we can\nrepeat the same success in video synthesis with large-scale\ndiffusion-based text-to-video models.\nMultiple attempts have been made to build large-scale\nvideo diffusion models. Ho et al. [17] proposed a UNet-\nbased architecture for the video synthesis task that is trained\nusing joint image-video denoising losses. Imagen video [14]\nextends the cascaded text-to-image generation architecture\nof Imagen [42] for video generation. In both works, the\nauthors directly train a video generation model from scratch.\nWhile these approaches achieve great success and produce\nhigh-quality videos, they are inherently expensive to train,\nrequiring hundreds of high-end GPUs or TPUs and several\nweeks of training. After all, video generators not only need\nto learn to form individual images but should also learn to\nsynthesize coherent temporal dynamics, which makes the\nvideo generation task much more challenging. While the\nformation of individual frames is a shared component in an\nimage and video synthesis, these works disregard the exis-\ntence of powerful pretrained text-to-image diffusion models\nand train their video generators from scratch.\nWe explore a different avenue for building large-scale\ntext-to-video diffusion models by starting with a pretrained\ntext-to-image diffusion model. Our motivation is that most\nof the components learned for the image synthesis task can\neffectively be reused for video generation, leading to knowl-\nedge transfer and efficient training. A similar idea is adopted\nby several recent works [46, 70, 4]. Without exception,\nwhen finetuning, they naively extend the image diffusion\nnoise prior (i.i.d. noise) used in the text-to-image model\nto a video diffusion noise prior by adding an extra dimen-\nsion to the 2D noise map. We argue that this approach is\nnot ideal as it does not utilize the natural correlations in\nvideos that are already learned by the image models. This\nis illustrated in Figure 2, where we visualize the t-SNE plot\nof noise maps corresponding to different input frames as\nobtained from a pretrained text-to-image diffusion model.\nThe noise maps corresponding to different frames coming\nfrom the same video (blue dots in Figure 2a are clustered\ntogether, exhibiting a high degree of correlation. The use\nof i.i.d. noise prior does not model this correlation, which\nwould impede the finetuning process. Our careful analysis of\nthe video diffusion noise prior leads us to a noise prior that\nis better tailored for finetuning an image synthesis model to\nthe video generation task. As illustrated in Figure 2b, our\nproposed noise prior (shown in blue dots) aptly captures the\ncorrelations in noise maps corresponding to video frames.\nWe then proceed to build a large-scale diffusion-based\ntext-to-video model. We leverage several design choices\nfrom the prior works, including the use of temporal atten-\ntion [17], joint image-video finetuning [17], a cascaded gen-\neration architecture [14], and an ensemble of expert denois-\ners [2]. Together with these techniques and the proposed\nvideo noise prior, our model establishes a new state-of-the-\nart for video generation outperforming competing methods\non several benchmark datasets. Figure 1 shows our model\ncan achieve high-quality zero-shot video synthesis capability\nwith SOTA photorealism and temporal consistency.\nIn short, our work makes the following key contributions.\n1. We propose a video diffusion noise tailored for finetuning\ntext-to-image diffusion models for text-to-video.\n2. We conduct extensive experimental validation and verify\nthe effectiveness of the proposed noise prior.\n3. We build a large-scale text-to-video diffusion model by\nfinetuning a pretrained eDiff-I model with our noise prior\nand achieve state-of-the-art results on several benchmarks.\n2. Related Work\nDiffusion-based text-to-image models: Diffusion mod-\nels have significantly advanced the progress of text-based\nphotorealistic, compositional image generation [38, 42].\nGiven the nature of the iterative denoising process that re-\nquires massive numbers of score function evaluations, earlier\ndiffusion models focused on generating low-resolution im-\nages, e.g., 64 \u00d7 64 [15, 48]. To generate high-resolution im-\nages, two common approaches have been used. The first ap-\nproach applies cascaded super-resolution models in the RGB\nspace [32, 16, 42, 38], while the second approach leverages a\ndecoder to exploit latent space [40, 11]. Based on these mod-\nels, advanced image and video editing have been achieved\nthrough finetuning the model [41, 68, 5, 23, 61, 29] or con-\ntrolling the inference process [30, 13, 34, 10, 35, 7, 31, 3].\nHere, we study the problem of using large-scale diffusion\nmodels for text-to-video generation.\nVideo generation models:\nGenerating realistic and\nnovel videos have long been an attractive and essential\nresearch direction [58, 39, 66]. Previously studies have\nresorted to different types of generative models such as\nGANs [58, 43, 54, 52, 45], Autoregressive models [51, 64,\n25, 9, 18], and implicit neural representations [47, 67]. Re-\ncently, driven by the tremendous success of applying the\ndiffusion model to image synthesis, multiple works have\nproposed to explore diffusion models for conditional and\nunconditional video synthesis [57, 12, 70, 61, 4, 22, 19, 57,\n65, 33, 28, 1, 59]. For example, Singer et al. extend the\nunCLIP framework [38] to text-to-video generation, which\nallows training without video captions [46]. Ho et al. [17]\nextend the Imagen framework [42] by repeatedly up-scaling\nlow-resolution small-fps videos in both spatial and temporal\ndirections with multiple models [14]. Our work also falls\ninto this line of work which uses a diffusion model. We\nfocus on augmenting an image diffusion model for video\nand study the design choice of the diffusion noise priors for\nsuch an image-to-video finetuning task.\nLeverage knowledge from images for text-to-video gen-\neration: Like text-to-image models, text-to-video models\nrequire massive amounts of data to learn caption-relatedness,\nframe photorealism, and temporal dynamics. But in contrast\nto the abundant image data resource, video data are more\nlimited in style, volume, and quality. To resolve such scarcity\nissue of text-video data, previous works have resorted to dif-\nferent strategies to leverage knowledge from image data for\ntext-to-video generation, including joint training on the text-\nimage data from scratch [17, 14, 56, 60], first training a text-\nto-image model and then finetuning partially [18, 4, 61, 29]\nor entirely [46, 8] on the video dataset, and using CLIP\nimage features as the conditional information [46, 70]. In\nthis paper, we propose a new video diffusion noise prior\nthat is tailored for finetuning a pretrained diffusion-based\nimage generation model for the video generation task. We\nreuse several design choices in the prior work by finetuning\njointly on text-image and text-video datasets. As a result,\nwe can build a text-to-video generation system that achieves\nstate-of-the-art zero-shot performances.\n3. Preliminaries\nDiffusion models generate data by iteratively denois-\ning samples drawn from a noise distribution. In the case\nof text-to-video models, text embeddings obtained from a\npre-trained text encoder are used as additional inputs in\nthe denoising process. Formally, let D(x, e, \u03c3) denote a\ndenoising network that operates on the noisy input video\nx \u2208 Rb\u00d7ns\u00d73\u00d7h\u00d7w where e is the text embedding, and \u03c3 is\nthe noise level. Here ns is the sequence length of the input\nvideo, b is the batch size, and h \u00d7 w is the spatial resolution.\nThe model D is trained to denoise the input x.\nTraining\nWe follow the EDM formulation of Karras et al.\n[21] to optimize the denoiser D using the following objective\nEpdata(xclean,e),p(\u03f5),p(\u03c3)\n\u0002\n\u03bb(\u03c3)\u2225D(xnoise; e, \u03c3) \u2212 xclean\u22252\n2\n\u0003\n(1)\nwhere xnoise = xclean + \u03c3\u03f5\nHere, xnoise is the noisy sample obtained by corrupting the\nclean video x with noise \u03c3\u03f5, where p(\u03f5) = N(0, I) and \u03c3\nis a scalar for the noise level drawn from p(\u03c3). The loss\nweight, \u03bb(\u03c3), is a function of \u03c3 given by \u03bb(\u03c3) = (\u03c32 +\n\u03c32\ndata)/(\u03c3 \u00b7 \u03c3data)2. Eq. (1) is a simple denoising objective in\nwhich the denoiser D is trained to estimate the clean video\nxclean from the noisy input xnoise. Following EDM, we use a\nlog-normal distribution for \u03c3 i.e., ln(p(\u03c3)) = N(Pmean, P 2\nstd)\nwith Pmean = \u22121.2 and Pstd = 1.2.\nTo train the denoising model, EDM uses preconditioning\nterms in its objective function to properly scale the inputs\nand output of the denoiser model D. More specifically, the\ndenoising model D is written as\nD(x; e,\u03c3):=\n\u0010\u03c3data\n\u03c3\u2217\n\u00112\nx + \u03c3 \u00b7 \u03c3data\n\u03c3\u2217\nF\u03b8\n\u0010 x\n\u03c3\u2217 ; e,ln(\u03c3)\n4\n\u0011\nHere, F\u03b8 is a neural network with parameters \u03b8 and \u03c3\u2217 =\np\n\u03c32 + \u03c32\ndata. We use \u03c3data = 0.5.\nSampling\nOnce the denoising model is trained, sampling\ncan be performed by solving the following ODE [21]\ndx\nd\u03c3 = \u2212\u03c3\u2207x log p(x|e, \u03c3) = x \u2212 D(x; e, \u03c3)\n\u03c3\n(2)\nfor \u03c3 flowing backwards from \u03c3 = \u03c3max to \u03c3 = 0. The\ninitial value for x is obtained by sampling from the prior dis-\ntribution x \u223c N(0, \u03c32\nmaxI). Over the recent years, several\nsamplers have been proposed for sampling from the trained\ndiffusion models [69, 48, 26, 27, 15]. In this paper, we use\nDEIS [69] and its stochastic variant [21] for synthesizing\nsamples from our model.\n4. Method\nTraining text-to-video models is much more challenging\nthan training text-to-image diffusion models due to practi-\ncal difficulties in collecting billion-scale video datasets and\nsecuring enough computational resources. Additionally, gen-\nerating videos is much more challenging since individual\nframes need to be both photorealistic and temporally co-\nherent. Prior works leverage large-scale image datasets to\nmitigate these difficulties by either joint training on the im-\nage datasets [60, 17, 14] or finetuning a text-to-image model\non the video datasets [18, 46]. Here, we are interested in\nfinetuning text-to-image diffusion models jointly on image\nand video datasets. We postulate that naively extending the\nimage noise prior to video diffusion is not ideal. We care-\nfully explore the design space of noise priors and propose\none that is well suited for our video finetuning task, which\nleads to significant performance gains.\nCorrelated noise model\nAn image diffusion model is\ntrained to denoise independent noise from a perturbed im-\nage. The noise vector \u03f5 in the denoising objective (1) is\nsampled from an i.i.d. Gaussian distribution \u03f5 \u223c N(0, I).\nHowever, after training the image diffusion model and ap-\nplying it to reverse real frames from a video into the noise\nspace in a per-frame manner, we find that the noise maps\ncorresponding to different frames are highly correlated. This\nis illustrated in Figure 2, where the t-SNE plot of noise maps\ncorresponding to different video frames are plotted. When\nthe input frames come from the same video (shown in blue\ndots in Figure 2a, noise maps are clustered. The use of i.i.d.\nsampling (shown in orange dots in Figure 2b does not cap-\nture these correlations. This is also depicted quantitatively\nTable 1: Cosine similarity of the reversed noise. The noise\nmaps corresponding to the frames sampled from the same\nvideos have a higher similarity than those sampled from\ndifferent videos.\nCosine Similarity\n(a) Same video noise\n0.206\u00b10.156\n(b) Different video noise\n0.001\u00b10.009\nin Table 1 where we compute the average pairwise cosine\nsimilarity between noise corresponding to (a) same video\nand (b) different video. (a) is much higher than (b). As a\nresult, the video diffusion model trained with i.i.d. noise is\ncoerced to forget such correlation among the noise between\ndifferent frames, making it difficult to preserve knowledge\nfrom the image diffusion model. Motivated by this obser-\nvation, we propose to modify the noise process to preserve\nthe correlation between different frames. To this end, we\ninvestigate two noising strategies - mixed and progressive\nnoising.\nMixed noise model: Let \u03f51, \u03f52, . . . \u03f5ns denote the noise\ncorresponding to individual video frames i.e., \u03f5i corresponds\nto the ith element of the noise tensor \u03f5. In the mixed noise\nmodel, we generate two noise vectors \u03f5shared and \u03f5ind. \u03f5shared\nis a common noise vector shared among all video frames,\nwhile \u03f5ind is the individual noise per frame. The linear com-\nbination of both these vectors is used as the final noise.\n\u03f5shared \u223c N\n\u0012\n0,\n\u03b12\n1 + \u03b12 I\n\u0013\n, \u03f5i\nind \u223c N\n\u0012\n0,\n1\n1 + \u03b12 I\n\u0013\n(3)\n\u03f5i = \u03f5shared + \u03f5i\nind\nProgressive noise model:\nIn the progressive noise\nmodel, the noise for each frame is generated in an autore-\ngressive fashion in which the noise at frame i is generated\nby perturbing the noise at frame i \u2212 1. Let \u03f5i\nind denote the\nindependent noise generated for frame i. Then, progressive\nnoising can be formulated as\n\u03f50 \u223c N(0,I)\n\u03f5i\nind \u223c N(0,\n1\n1 + \u03b12 I)\n(4)\n\u03f5i =\n\u03b1\n\u221a\n1 + \u03b12 \u03f5i\u22121 + \u03f5i\nind\nIn both these models, \u03b1 controls how much noise is shared\namong different video frames. The higher the value of \u03b1, the\nmore correlation exists among the noise maps corresponding\nto different frames. As \u03b1 \u2192 \u221e, all frames would have the\nsame noise which results in generating a frozen video. On\nthe other hand, \u03b1 = 0 corresponds to i.i.d. noise.\nAs shown in Figure 2b, the use of progressive noise sam-\npling (blue dots) better models the correlations between\ndifferent noise maps by obtaining similar clustering patterns\nto the noise maps of real video frames embedded by a pre-\ntrained text-to-image model in Figure 2a (blue dots).\nBase model\nTemporal \nInterpolation\nSpatial\nSuper \nResolution\nT5 encoder \nCLIP text encoder\n16 x 64 x 64\n76 x 256 x 256\n76 x 1024 x 1024\nSpatial \nSuper \nResolution\n76 x 64 x 64\nFigure 3: Model architecture. Our pipeline consists of a cascade of four networks \u2014 a base model and three upsampling\nmodels. All four models take inputs as the text embeddings obtained from the T5 encoder and the CLIP text encoder. The base\nmodel produces 16 video frames of spatial resolution 64 \u00d7 64 with a frameskip of 5. The first upsampling model performs a\ntemporal interpolation, resulting in videos of size 76 \u00d7 64 \u00d7 64 while the subsequent two super-resolution models perform\nspatial super-resolution to produce videos of sizes 76 \u00d7 256 \u00d7 256 and 76 \u00d7 1024 \u00d7 1024.\nModel architecture\nAs visualized in Figure 3, our model\nconsists of a cascade of four networks \u2014 a base network\nand three upsampling stacks. The base network generates an\noutput video of dimension 16 \u00d7 64 \u00d7 64 with a frameskip\nof 5. It generates the frames {1, 6, 11, . . . 76}. The first\nupsampling network performs a temporal interpolation to\nproduce a video of size 76 \u00d7 64 \u00d7 64. The second and the\nthird super-resolution network performs spatial upsampling\nto produce the outputs of sizes 76 \u00d7 256 \u00d7 256 and 76 \u00d7\n1024 \u00d7 1024. We utilize eDiff-I [2], a state-of-the-art text-\nto-image diffusion model, to initialize our base and spatial\nsuper-resolution models. Similar to prior works [17, 46], we\nadapt the image-based U-Net model for the video synthesis\ntask by making the following changes: (1) Transforming 2D\nconvolutions to 3D by adding a dimension 1 to temporal axis\nand (2) Adding temporal attention layers. Please refer to the\nsupplementary material for more details.\nSimilar to Ho et al. [17], we jointly finetune the model\non video and image datasets by concatenating videos and\nimages in the temporal axis and applying our temporal mod-\nules only on the video part. Similarly to eDiff-I, our model\nuses both T5 text embeddings [37] and CLIP text embed-\ndings [36]. We drop each of the embeddings independently\nat random during training, as in eDiff-I.\n5. Experiments\nIn this section, we evaluate our proposed strategy of train-\ning diffusion models for video synthesis on two sets of ex-\nperiments. We first comprehensively analyze our proposed\nnoise model on the small-scale UCF-101 dataset. We then\nscale up our experiments to the challenging large-scale text-\nto-video synthesis task.\n5.1. Experimental Setups\nWe conduct ablation experiments in a small-scale uncon-\nditional video generation setting and pick the best configura-\ntion for our large-scale text-to-video generation run.\nDatasets\nWe train our model on the UCF-101 dataset [50]\nfor the small-scale experiments, where we follow the pro-\ntocol defined in Ho et al. [17] to generate videos of size\n16 \u00d7 64 \u00d7 64. UCF-101 dataset contains 13, 320 videos.\nWe randomly sample frames from these videos to train our\nimage synthesis model. For our large-scale experiments, we\nuse a combination of public and proprietary datasets for text-\nto-image and text-to-video finetuning. Most of the videos\nare of 2K resolution with 16:9 aspect ratio. All data were\nfiltered using a preset CLIP and aesthetic scores*. to ensure\nhigh quality. Our final image dataset contains around 1.2\nbillion text-image pairs and 22.5 million text-video pairs.\nTraining details\nIn the unconditional generation experi-\nment on the UCF-101 dataset, to do an ablation study on the\nmodel size, we design 3 models where each model has 69M,\n112M, and 253M parameters, respectively. As a comparison,\nthe baseline Video Diffusion Model (VDM) [17] contains\n1.2B parameters. In the large-scale text-to-video experiment,\nour base and temporal interpolation models contain 1.08B\nparameters. Our super-resolution model adapted from the\nefficient U-Net [42] architecture with temporal convolution\nlayers [14, 46] contains 313M parameters. Please refer to\nthe supplementary material for more training details.\n*https://github.com/christophschuhmann/\nimproved-aesthetic-predictor\nA cute corgi wearing a red robe\nholding a sign that says \u201dMerry\nChristmas\u201d. There is a Christ-\nmas tree in the background.\nAn epic tornado attacking above\na glowing city at night, the tor-\nnado is made of smoke, highly\ndetailed.\nSmall boat sailing in the ocean,\ngiant Cthulhu monster coming\nout a dense mist in the back-\nground, giant waves attacking.\nA golden retriever puppy holding\na green sign that says \u201dNVIDIA\nROCKS\u201d. Background is a class-\nroom.\nA cute funny robot dancing, cen-\ntered, award winning watercolor\npen illustration.\nA cartoon white wolf is giv-\ning puppy-dog eyes, detailed fur,\nvery cute kid\u2019s film character.\nA lightning striking atop of eif-\nfel tower, dark clouds in the sky,\nslow motion.\nAn anime girl looks at the beau-\ntiful nature through the window\nof a moving train, well rendered.\nA skull burning while being held\nup by a skeletal hand.\nA huge dinosaur skeleton walk-\ning in a golden wheat field on a\nbright sunny day.\nA cute rabbit is eating grass,\nwildlife photography.\nTomato sauce pouring over fries.\nFigure 4: Sample generations. The figure is best viewed with Acrobat Reader. Click the images to play video clips.\nEvaluation\nFor the small-scale experiments on UCF-101\ndataset, we follow the protocol defined in the prior ap-\nproaches [52, 47, 17] and report the Inception Score (IS) [44]\ncalculated by a trained C3D model [53] and Fr\u00b4echet Video\nDistance (FVD) [55] by a trained I3D model [6]. For the\nlarge-scale text-to-video experiments, we perform the zero-\nshot evaluation of the video generation quality on the UCF-\n101 and MSR-VTT datasets following Make-A-Video [46].\nWe carefully discuss the evaluation process below.\nUCF-101 experiment\nWe use IS and FVD for evalua-\ntion in our small-scale experiments. UCF-101 is a categor-\nical video dataset designed for action recognition. When\nsampling from the text-to-video model, we devise a set of\nprompts for each class name to be used as the conditional\ninput. This is necessary as some class names (such as jump\nrope) are not descriptive. We list all the prompts we use in\nthe supplementary material. We sample 20 videos for each\nprompt to compute the IS metric. For FVD, we follow the\nTable 2: Zero-shot text to video generation on UCF-101. Our\napproach gives significant performance gains compared to\nthe prior baselines both in inception score and FVD metrics.\nMethod\nIS (\u2191)\nFVD (\u2193)\nCogVideo [18] (Chinese)\n23.55\n751.34\nCogVideo [18] (English)\n25.27\n701.59\nMake-A-Video [46]\n33.00\n367.23\nMagicVideo [70]\n-\n655.00\nVideo LDM [4]\n33.45\n550.61\nVideoFactory [59]\n-\n410.00\nPYoCo\n47.76\n355.19\nTable 3: Text conditional zero-shot generation on MSRVTT.\nOur approach with the base config achieves the best results,\nand using an ensemble further improves the FIDs.\nMethod\nCLIP-FID (\u2193)\nFID (\u2193)\nNUWA [60] (Chinese)\n47.68\n-\nCogVideo [18] (Chinese)\n24.78\n-\nCogVideo [18] (English)\n23.59\n-\nMake-A-Video [46]\n13.17\n-\nMagicVideo [70]\n-\n36.50\nLatent-Shift [1]\n15.23\n-\nPYoCo (Config-A)\n10.21\n25.39\nPYoCo (Config-B)\n9.95\n24.28\nPYoCo (Config-C)\n9.91\n24.54\nPYoCo (Config-D)\n9.73\n22.14\nprior work [25, 52] and sample 2, 048 videos for evaluation.\nMSR-VTT experiment\nMSR-VTT [63] test set contains\n2, 990 videos as well as 59, 794 captions. All the videos\nhave the same resolution of 320 \u00d7 240. We generate a\n76 \u00d7 256 \u00d7 256 video for each 59, 794 caption and save\nthe videos in an mp4 format with a high bit rate. To com-\npare with Make-A-Video, we compute FID using a ViT-B/32\nmodel [24]. We also report a more common FID metric com-\nputed by an Inception-V3 model. We also examine the idea\nof ensemble denoiser [2] by finetuning the level-1 experts\nof each model. We denote Config-A as the configuration\nof using only baseline models and Config-B to Config-D as\nincrementally changing super-resolution model, temporal\ninterpolation model, and base model with the corresponding\nensemble models.\n5.2. Main Results\nLarge-scale\ntext-to-video\nsynthesis\nWe\nquantita-\ntively compare our method against Make-A-Video [46],\nNUWA [60], CogVideo [18], and several concurrent\nworks [4, 70, 4, 59, 1]. Table 2 shows that our method\nTable 4: Unconditional UCF-101 generation results. Our\napproach achieves the state-of-the-art inception score and\nFVD, while having considerably smaller parameter count\ncompared to other diffusion-based approaches such as VDM\n(1B parameters).\nMethod\nIS (\u2191)\nFVD (\u2193)\nTGAN [43]\n15.83\u00b1.18\n-\nLDVD-GAN [20]\n22.91\u00b1.19\n-\nVideoGPT [64]\n24.69\u00b1.30\n-\nMoCoGAN-HD [52]\n32.36\n838\nDIGAN [67]\n29.71\u00b1.53\n655\u00b122\nCCVS [25]\n24.47\u00b1.13\n386\u00b115\nStyleGAN-V [47]\n23.94\u00b1.73\n-\nVDM [17]\n57.00\u00b1.62\n-\nTATS [9]\n57.63\u00b1.73\n430 \u00b118\nPYoCo (112M)\n57.93\u00b1.24\n332 \u00b113\nPYoCo (253M)\n60.01\u00b1.51\n310 \u00b113\noutperforms all the baselines on the UCF-101 dataset and\nimproves the zero-shot Inception Score from 33.45 to 47.76.\nIn Table 3, we show that our baseline model achieves a\nnew state-of-the-art CLIP-FID score [24] of 10.21, while\nusing ensemble models further improves both CLIP-FID\nand FID scores. In Figure 4, we qualitatively visualize the\nsynthesis capability of our approach. Our model achieves\nhigh-quality zero-shot video synthesis capability with good\nphotorealism and temporal coherency. We also provide a\nqualitative comparison with Make-A-Video [46] and Imagen\nVideo [14] in Figure 5. We observe that our model is able to\nproduce videos with better details than both approaches, as\nshown in the animal videos. We also produce better-stylized\nvideos than Imagen Video.\nSmall-scale unconditional video synthesis\nWe report IS\nand FVD scores on UCF-101 dataset in Table 4 and com-\npare our model with multiple unconditional video generation\nbaselines. Note that using class labels as conditional infor-\nmation could lead to sizeable improvement in IS and FVD\nscores [9], which we do not consider as the comparison.\nOur method attains state-of-the-art unconditional video gen-\neration quality. Compared with previous diffusion-based\nunconditional generation model [17], our model is \u223c 10\u00d7\nsmaller and has \u223c 14\u00d7 less training time (75 GPU-days vs.\n925 GPU-days).\n5.3. Ablation Study\nWe quantitatively compare several training strategies for\nvideo diffusion models. Then, we perform ablation on the\ncorrelation ratio in the Equations 3 and 4, a key hyper-\nparameter in our approach.\nMake-A-Video [46]\nPYoCo\nImagen Video [14]\nPYoCo\nA confused grizzly bear in calculus class.\nA sheep to the right of a wineglass.\nSailboat sailing on a sunny day in a mountainlake.\nA cat eating food out of a bowl, in style of Van Gogh.\nFigure 5: Qualitative comparison with baseline approaches. The two panels on the left show the comparison of our approach\nwith Make-A-Video [46], while those on the right show the comparison with Imagen Video [14]. PYoCo achieves better\nphotorealism compared to the two approaches. Best viewed with Acrobat Reader. Click the images to play the video clips.\nTable 5: Quantitative results of different training strategies\non UCF-101 dataset.\nIS(\u2191)\nFVD (\u2193)\nFID (\u2193)\nImage Diffusion (ID)\n-\n-\n30.05\nTraining from scratch\n28.25\n903.37\n124.75\nFinetuning from ID\n41.25\n566.67\n56.43\n+ Mixed Noise\n52.71\n337.40\n31.57\n+ Progressive Noise\n53.52\n339.67\n31.88\nTraining strategies\nWe compare training from scratch, a\nsimple finetuning baseline, finetuning with mixed noising,\nand progressive noising using IS, FVD, and averaged frame\nFID metrics on the UCF-101 dataset in Table 5. We first\nfind that finetuning from an image diffusion model is much\nmore effective than training from scratch. For finetuning\nfrom the image model, the correlated noise model produces\nbetter video generation quality than the independent noise\nmodel. In addition, we notice that the correlated noise better\npreserves the image quality learned by the pretrained image\nmodel and produces a lower frame FID. This is particularly\ndesired in large-scale text-to-video training to fulfill the goal\nof inheriting the knowledge from the image model missing in\nthe video datasets. Specifically, most videos contain realistic\nscenes captured by cameras and have infrequent media types\nlike paintings, illustrations, sketches, etc. Moreover, the\nvideo data is much smaller in volume, and the scenes are\nless diverse than image datasets. As shown in Figure 4, our\nmodel can preserve properties learned from image datasets\nthat are not presented in our video dataset, such as the artistic\nstyles, and generate faithful motion on them.\nCorrelation ratio\nThe hyperparameter \u03b1 in the Equations\n3 and 4 controls the correlation between the noise of differ-\nent frames. A larger \u03b1 injects more correlation into the noise.\nThe correlation disappears when \u03b1 \u2192 0, and the mixed and\nprogressive noise models reproduce the vanilla noise model.\nTo find optimal \u03b1, we train our UCF-small model (69M pa-\nrameters) using \u03b1 \u2208 {0, 0.1, 0.2, 0.5, 1, 1, 2, 5, 10, \u221e} and\nreport FVD in Figure 7. For each \u03b1 value, we repeat the\nexperiment 3 times and report the mean. Note that \u03b1 = 0\nindicates finetuning with the independent frame noise, and\n\u03b1 = \u221e indicates using identical noise maps for all the\nframes, which produces frozen videos during the inference\ntime. Finetuning an image diffusion model almost consis-\ntently outperforms the training-from-scratch baseline with\ndifferent \u03b1s. Using \u03b1 = 1 for mixed noising and \u03b1 = 2 for\nprogressive noising produces similar best results. We also\nshow qualitiative results for models trained with \u03b1 = 0, 1, 10\nin Figure 6. When \u03b1 is too small, we notice a degradation in\nvisual quality in the generated video frames and a reduced\nvideo diversity. For example, we notice many repeated sam-\nples and black borders in almost every video generated with\n\u03b1 = 0\n\u03b1 = 1\n\u03b1 = 10\nFigure 6: Visual ablation on \u03b1. Small \u03b1 = 0 reduces video quality and diversity and large \u03b1 = 10 yields motion artifacts.\n0\n0.1\n0.2\n0.5\n1.0\n2.0\n5.0\n10.0\n\u221e\n500\n1000\nFVD\n\u03b1\nMixed\nProgressive\nScratch\nFigure 7: Quantitative ablation on hyperparameter \u03b1.\nFinetuning with temporally correlated prior improves over\ntraining from scratch. Using a too-large or too-small \u03b1 leads\nto inferior results. \u03b1 = 1, \u03b1 = 2 each works the best for\nmixed and progressive noising, respectively.\n\u03b1 = 0. On the other hand, when \u03b1 is too large, the model\nhas difficulty generating proper motions.\nModel size\nWe pick the best \u03b1 for the mixed and progres-\nsive noise models and compare them with the model trained\nfrom scratch on models with different numbers of parame-\nters, 69M, 112M, and 253M. Figure 8 shows that our mixed\nand progressive models outperform the baseline consistently\nby a large margin in terms of FVD. Overall, mixed and pro-\ngressive noising provide similar performance. In our large\nlarge-scale experiments, we choose progressive noising with\n\u03b1 = 2 due to its autoregressive nature.\n6. Conclusion\nWe proposed a new efficient way of training text-to-video\ngeneration models. By observing that the noise maps gener-\nating the frames of a video are clustered together, we study\n50\n100\n150\n200\n250\n400\n600\n800\nFVD\nNumber of Parameters (M)\nMixed (\u03b1 = 1)\nProgressive (\u03b1 = 2)\nScratch\nFigure 8: Ablation on model size. Larger models con-\nsistently improve the performance of both finetuning and\ntraining from scratch. Finetuning from image model consis-\ntently outperforms training from scratch.\nmixed and progressive noise priors well-suited for sequential\nvideo frame generation. We apply our progressive noise prior\nto finetuning a state-of-the-art diffusion-based text-to-image\nmodel to achieve a state-of-the-art large-scale text-to-video\nmodel. The high quality of the generated videos and the\nstate-of-the-art Inception and FID scores demonstrate the\nstrength of our approach.\nAcknowledgment. We would like to thank Amanda Moran,\nJohn Dickinson, Sivakumar Arayandi Thottakara, David\nPage, Ranjitha Prasanna, Venkata Karri, and others in NGC\nand PBSS team for the computing infrastructure support. We\nalso give thanks to Qinsheng Zhang, Zekun Hao, Tsung-Yi\nLin, Ajay Jain, and Chen-Hsuan Lin for useful discussions\nand feedback. Thanks also go to Yin Xi and Thomas Hayes\nfor clarifying the evaluation protocol of Make-A-Video and\nto Ming Ding for the details of CogVideo. We thank the\nICCV PCs, ACs, and reviewers for their service and valuable\nfeedback. This work is partly supported by NSF grants No.\nIIS-1910132 and IIS-2213335.\nA. Experimental Setups\nIn this section, we provide additional details of our ex-\nperiments in terms of implementation, dataset, evaluation,\nmodel, and training.\nA.1. Implementation details\nSimilar to prior works [17, 46], we adapt the image-based\nU-Net model for the video synthesis task by making the fol-\nlowing changes: (1) We transform the 2D convolution layers\nto 3D by adding a dimension of 1 to the temporal axis. For\ninstance, we convert a 3 \u00d7 3 convolution layer to 1 \u00d7 3 \u00d7 3\nlayer. (2) We replace the attention layers in the base and\ntemporal interpolation models with a cascade of spatial and\ntemporal attention layers. The spatial attention layers are\nreused from eDiff-I [2], while the temporal attention layers\nare initialized randomly with a projection layer at the end\nusing zero-initialization. We apply temporal attention to the\nactivation maps obtained by moving the spatial dimension\nof the feature tensor to the batch axis. (3) For the tempo-\nral interpolation model, we concatenate the input noise in\nthe channel axis with 16 frames by infilling 4 real frames\nwith zero frames. (4) We add a 3 \u00d7 1 \u00d7 1 convolution layer\nat the end of each efficient block of the super-resolution\nmodel [42]. (5) For all the models, we apply spatial atten-\ntion to the reshaped activation maps obtained by moving\nthe temporal dimension of the feature tensor to the batch\naxis. We apply the same operation to the feature maps input\nthe GroupNorm [62] to mimic better the statistics the image\nmodel learned. We use cross-attention layers (between text\nand videos) only in the spatial attention block, as adding\nit to the temporal attention resulted in significant memory\noverhead. (6) We utilize eDiff-I [2] to initialize our base\nand spatial super-resolution models. We use a similar model\narchitecture as the base model for our temporal interpolation\nmodel, as they share the same function of hallucinating un-\nseen frames. After finetuning the base model for some time,\nwe use its checkpoint to initialize the temporal interpolation\nmodel. (7) Similar to Ho et al. [17], we jointly finetune\nthe model on video and image datasets by concatenating\nvideos and images in the temporal axis and applying our\ntemporal modules only on the video part. (8) Similarly to\neDiff-I, our model uses both T5 [37] text embeddings and\nCLIP text embeddings [36]. During training, we drop each\nof the embeddings independently at random, as in eDiff-I.\nA.2. Dataset and evaluation details\nCaption templates for categorical video datasets\nGiven\nthe name of the category [class] such as kayaking and yoga,\nwe consider the following templates to create video captions:\n\u2022 a man is [class].\n\u2022 a woman is [class].\n\u2022 a kid is [class].\n\u2022 a group of people are [class].\n\u2022 doing [class].\n\u2022 a man is doing [class].\n\u2022 a woman is doing [class].\n\u2022 a kid is doing [class].\n\u2022 a group of people are doing [class].\n\u2022 [class].\nPrompts used for UCF-101 evaluation\nIn our initial ex-\nplorations, we find that the original class labels in the UCF-\n101 dataset often cannot describe the video content correctly.\nFor example, the class jump rope is more likely describing an\nobject rather than a complete video. Therefore, we write one\nsentence for each class as the caption for video generation.\nWe list these prompts for evaluating text-to-video generation\nmodels on the standard UCF-101 benchmark below.\napplying eye makeup, applying lipstick, archery, baby\ncrawling, gymnast performing on a balance beam, band\nmarching, baseball pitcher throwing baseball, a basketball\nplayer shooting basketball, dunking basketball in a basket-\nball match, bench press, biking, billiards, blow dry hair,\nblowing candles, body weight squats, a person bowling on\nbowling alley, boxing punching bag, boxing speed bag, swim-\nmer doing breast stroke, brushing teeth, weightlifting with\nbarbell, clean and jerk, cliff diving, bowling in cricket game-\nplay, batting in cricket gameplay, cutting in kitchen, diver\ndiving into a swimming pool from a springboard, drumming,\ntwo fencers have fencing match indoors, field hockey match,\ngymnast performing on the floor, group of people playing\nfrisbee on the playground, swimmer doing front crawl, golfer\nswings and strikes the ball, haircuting, a person hammering\na nail, an athlete performing the hammer throw, an athlete\ndoing handstand push up, an athlete doing handstand walk-\ning, massagist doing head massage to man, an athlete doing\nhigh jump, horse race, group of people racing horse, person\nriding a horse, a woman doing hula hoop, man and woman\ndancing on the ice, ice dancing, athlete practicing javelin\nthrow, a person juggling with balls, a young person doing\njumping jacks, a person skipping with jump rope, a person\nkayaking in rapid water, knitting, an athlete doing long jump,\na person doing lunges with barbell, military parade, mixing\nin the kitchen, mopping floor, a person practicing nunchuck,\ngymnast performing on parallel bars, a person tossing pizza\ndough, a musician playing the cello in a room, a musician\nplaying the daf, a musician playing the indian dhol, a mu-\nsician playing the flute, a musician playing the guitar, a\nmusician playing the piano, a musician playing the sitar, a\nmusician playing the tabla, a musician playing the violin,\nan athlete jumps over the bar, gymnast performing pommel\nhorse exercise, a person doing pull ups on bar, boxing match,\npush ups, group of people rafting on fast moving river, rock\nclimbing indoor, rope climbing, several people rowing a boat\non the river, couple salsa dancing, young man shaving beard\nwith razor, an athlete practicing shot put throw, a teenager\nskateboarding, skier skiing down, jet ski on the water, sky\ndiving, soccer player juggling football, soccer player doing\npenalty kick in a soccer match, gymnast performing on still\nrings, sumo wrestling, surfing, kids swing at the park, a per-\nson playing table tennis, a person doing TaiChi, a person\nplaying tennis, an athlete practicing discus throw, trampoline\njumping, typing on computer keyboard, a gymnast perform-\ning on the uneven bars, people playing volleyball, walking\nwith dog, a person standing, doing pushups on the wall, a\nperson writing on the blackboard, a kid playing Yo-Yo\nA.3. Training details\nUCF-101 experiments.\nFor image pretraining phase on\nthe UCF-101 frames, we use an ADAM optimizer with a\nbase learning rate of 2e \u2212 4. For video finetuning phase, we\nadopt an ADAM optimizer with a base learning rate of 1e\u22124.\nWe use a linear warm up of 5, 000 steps for both phases. For\nsampling, we use stochastic DEIS sampler [69, 21] with\n3kutta, order 6 and 25 steps.\nLarge-scale experiments.\nThe hyper-parameters we use\nfor the large-scale text-to-video experiments are provided in\nTable F.\nTable F: Hyperparameters\nHyperparameters for large-scale experiments\nOptimizer\nAdamW\nLearning rate\n0.0001\nWeight decay\n0.01\nBetas\n(0.9, 0.999)\nEMA\n0.9999\nCLIP text embedding\n0.2\ndropout rate\nT5 text embedding\n0.25\ndropout rate\nGradient checkpointing\nEnabled\n# iterations for base model\n150K\n# iterations for super-res model\n220K\nSampler for base model\nStochastic DEIS [69, 21],\n3kutta, Order 3, 60 steps\nSampler for super-res models\nDEIS, 3kutta\nOrder 3, 20 steps\nA.4. Architecture details\nThe architectures used for the small-scale UCF experi-\nments are provided in Tables G, H and I. For the large-scale\nexperiment, the architectures used for base model, temporal\ninterpolation model, and the two spatial super-resolution\nstacks are provided in tables J, K, L and M respectively.\nTable G: Small (69M parameters) UCF-101 model architec-\nture.\nSmall (69M parameters) UCF-101 model\nChannel multiplier\n[1, 2, 2, 3]\nDropout\n0.1\nNumber of channels\n128\nNumber of residual blocks\n2\nSpatial self attention resolutions\n[32, 16, 8]\nSpatial cross attention resolutions\n[32, 16, 8]\nTemporal attention resolution\n[32, 16, 8]\nNumber of channels in attention heads\n64\nUse scale shift norm\nTrue\nTable H: Medium (112M parameters) UCF-101 model archi-\ntecture.\nMedium (112M parameters) UCF-101 model\nChannel multiplier\n[1, 2, 3, 4]\nDropout\n0.1\nNumber of channels\n128\nNumber of residual blocks\n2\nSpatial self attention resolutions\n[32, 16, 8]\nSpatial cross attention resolutions\n[32, 16, 8]\nTemporal attention resolution\n[32, 16, 8]\nNumber of channels in attention heads\n64\nUse scale shift norm\nTrue\nTable I: Large (253M parameters) UCF-101 model architec-\nture.\nLarge (253M parameters) UCF-101 model\nChannel multiplier\n[1, 2, 3, 4]\nDropout\n0.1\nNumber of channels\n192\nNumber of residual blocks\n2\nSpatial self attention resolutions\n[32, 16, 8]\nSpatial cross attention resolutions\n[32, 16, 8]\nTemporal attention resolution\n[32, 16, 8]\nNumber of channels in attention heads\n64\nUse scale shift norm\nTrue\nTable J: Architecture for the base model in text-to-video\nexperiments.\nText-to-video base model (1.08B parameters)\nChannel multiplier\n[1, 2, 4, 4]\nDropout\n0\nNumber of channels\n256\nNumber of residual blocks\n3\nSpatial self attention resolutions\n[32, 16, 8]\nSpatial cross attention resolutions\n[32, 16, 8]\nTemporal attention resolution\n[32, 16, 8]\nNumber of channels in attention heads\n64\nUse scale shift norm\nTrue\nTable K: Architecture for the temporal interpolation model\nin text-to-video experiments.\nTemporal interpolation model (1.08B parameters)\nChannel multiplier\n[1, 2, 4, 4]\nDropout\n0\nNumber of channels\n256\nNumber of residual blocks\n3\nSpatial self attention resolutions\n[32, 16, 8]\nSpatial cross attention resolutions\n[32, 16, 8]\nTemporal attention resolution\n[32, 16, 8]\nNumber of channels in attention heads\n64\nUse scale shift norm\nTrue\nTable L: Architecture for the spatial super-resolution model\nin text-to-video experiments.\nSpatial super-resolution 256 (300M parameters)\nChannel multiplier\n[1, 2, 4, 8]\nBlock multiplier\n[1, 2, 4, 4]\nDropout\n0\nNumber of channels\n128\nNumber of residual blocks\n2\nSpatial self attention resolutions\n[32]\nSpatial cross attention resolutions\n[32]\nNumber of channels in attention heads\n64\nUse scale shift norm\nTrue\nTable M: Architecture for the spatial super-resolution model\nin text-to-video experiments.\nSpatial super-resolution 1024 (170M parameters)\nPatch size\n256 \u00d7 256\nChannel multiplier\n[1, 2, 4, 4]\nBlock multiplier\n[1, 2, 4, 4]\nNumber of channels\n128\nNumber of residual blocks\n2\nSpatial cross attention resolutions\n[32]\nUse scale shift norm\nTrue\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video generation.\narXiv preprint arXiv:2304.08477, 2023. 3, 7\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, et al. eDiff-I:\nText-to-image diffusion models with an ensemble of expert\ndenoisers. arXiv preprint arXiv:2211.01324, 2022. 1, 3, 5, 7,\n10\n[3] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image and\nvideo editing. In ECCV, pages 707\u2013723. Springer, 2022. 3\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 2, 3, 7\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nCVPR, 2023. 1, 3\n[6] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\npages 6299\u20136308, 2017. 6\n[7] Duygu Ceylan,\nChun-Hao Huang,\nand Niloy J. Mi-\ntra.\nPix2video:\nVideo editing using image diffusion.\narXiv:2303.12688, 2023. 3\n[8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. Structure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 3\n[9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long\nvideo generation with time-agnostic vqgan and time-sensitive\ntransformer. arXiv preprint arXiv:2204.03638, 2022. 3, 7\n[10] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang.\nExpressive text-to-image generation with rich text. arXiv\npreprint arXiv:2304.06720, 2023. 3\n[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang,\nDongdong Chen, Lu Yuan, and Baining Guo. Vector quan-\ntized diffusion model for text-to-image synthesis. In CVPR,\npages 10696\u201310706, 2022. 3\n[12] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-\ntian Weilbach, and Frank Wood. Flexible diffusion modeling\nof long videos. arXiv preprint arXiv:2205.11495, 2022. 3\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,\nMohammad Norouzi, David J Fleet, et al. Imagen video:\nHigh definition video generation with diffusion models. arXiv\npreprint arXiv:2210.02303, 2022. 2, 3, 4, 5, 7, 8\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020. 3,\n4\n[16] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. JMLR, 23:47\u20131,\n2022. 3\n[17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. arXiv preprint arXiv:2204.03458, 2022. 2, 3, 4, 5, 6,\n7, 10\n[18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 3, 4, 7\n[19] Tobias H\u00a8oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,\nand Andrea Dittadi. Diffusion models for video prediction\nand infilling. arXiv preprint arXiv:2206.07696, 2022. 3\n[20] Emmanuel Kahembwe and Subramanian Ramamoorthy.\nLower dimensional kernels for video discriminators. Neural\nNetworks, 2020. 7\n[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. arXiv preprint arXiv:2206.00364, 2022. 3, 4, 11\n[22] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto Henschel,\nZhangyang Wang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\narXiv preprint arXiv:2303.13439, 2023. 3\n[23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-\nman, and Jun-Yan Zhu. Multi-concept customization of text-\nto-image diffusion. arXiv preprint arXiv:2212.04488, 2022.\n3\n[24] Tuomas Kynk\u00a8a\u00a8anniemi, Tero Karras, Miika Aittala, Timo\nAila, and Jaakko Lehtinen. The role of imagenet classes in\nfr\u00b4echet inception distance. In ICLR, 2023. 7\n[25] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs:\nContext-aware controllable video synthesis. NeurIPS, 2021.\n3, 7\n[26] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo\nnumerical methods for diffusion models on manifolds. arXiv\npreprint arXiv:2202.09778, 2022. 4\n[27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 4\n[28] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan. Videofusion: Decomposed diffusion models for\nhigh-quality video generation. In CVPR, 2023. 3\n[29] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-\nguided text-to-video generation using pose-free videos. arXiv\npreprint arXiv:2304.01186, 2023. 3\n[30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image\nsynthesis and editing with stochastic differential equations.\nIn ICLR, 2022. 1, 3\n[31] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha,\nYossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen.\nDreamix: Video diffusion models are general video editors.\narXiv preprint arXiv:2302.01329, 2023. 3\n[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 3\n[33] Yaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion:\nTraining diffusion models on a single image or video. arXiv\npreprint arXiv:2211.11743, 2022. 3\n[34] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. arXiv preprint arXiv:2302.03027, 2023. 3\n[35] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 5,\n10\n[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 21(140):1\u201367, 2020.\n5, 10\n[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n1, 3\n[39] MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael\nMathieu, Ronan Collobert, and Sumit Chopra. Video (lan-\nguage) modeling: a baseline for generative models of natural\nvideos. arXiv preprint arXiv:1412.6604, 2014. 3\n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 3\n[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. arXiv preprint arXiv:2208.12242, 2022. 3\n[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. arXiv preprint arXiv:2205.11487,\n2022. 1, 2, 3, 5, 10\n[43] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal\ngenerative adversarial nets with singular value clipping. In\nICCV, 2017. 3, 7\n[44] Masaki Saito, Shunta Saito, Masanori Koyama, and So-\nsuke Kobayashi. Train sparsely, generate densely: Memory-\nefficient unsupervised training of high-resolution temporal\ngan. IJCV, 2020. 6\n[45] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-\nv: Video generation with temporal motion styles. In CVPR,\n2023. 3\n[46] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3, 4, 5, 6, 7, 8, 10\n[47] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. arXiv preprint\narXiv:2112.14683, 2021. 3, 6, 7\n[48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In ICLR, 2021. 2, 3, 4\n[49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn ICLR, 2021. 2\n[50] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 5\n[51] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-\nnov. Unsupervised learning of video representations using\nlstms. In ICML, 2015. 3\n[52] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng,\nDimitris N. Metaxas, and Sergey Tulyakov. A good image\ngenerator is what you need for high-resolution video synthesis.\nIn ICLR, 2021. 3, 6, 7\n[53] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In ICCV, 2015. 6\n[54] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz. Mocogan: Decomposing motion and content for video\ngeneration. In CVPR, June 2018. 3\n[55] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. ICLR, 2019. 6\n[56] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kinder-\nmans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar,\nSantiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki:\nVariable length video generation from open domain textual\ndescription. arXiv preprint arXiv:2210.02399, 2022. 3\n[57] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher\nPal. Masked conditional video diffusion for prediction, gen-\neration, and interpolation. arXiv preprint arXiv:2205.09853,\n2022. 3\n[58] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Gen-\nerating videos with scene dynamics. NIPS, 2016. 3\n[59] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap atten-\ntion in spatiotemporal diffusions for text-to-video generation.\narXiv preprint arXiv:2305.10874, 2023. 3, 7\n[60] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In ECCV, pages\n720\u2013736. Springer, 2022. 3, 4, 7\n[61] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 3\n[62] Yuxin Wu and Kaiming He. Group normalization. In ECCV,\npages 3\u201319, 2018. 10\n[63] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, 2016. 7\n[64] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srini-\nvas. Videogpt: Video generation using vq-vae and transform-\ners. arXiv preprint arXiv:2104.10157, 2021. 3, 7\n[65] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-\nfusion probabilistic modeling for video generation. arXiv\npreprint arXiv:2203.09481, 2022. 3\n[66] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked gen-\nerative video transformer. arXiv preprint arXiv:2212.05199,\n2022. 3\n[67] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with\ndynamics-aware implicit generative adversarial networks. In\nICLR, 2021. 3, 7\n[68] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. arXiv preprint\narXiv:2302.05543, 2023. 1, 3\n[69] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffu-\nsion models with exponential integrator. In ICLR, 2023. 4,\n11\n[70] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3, 7\n"
  }
]