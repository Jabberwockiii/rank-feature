[
  {
    "title": "Fast Segment Anything",
    "link": "https://arxiv.org/pdf/2306.12156.pdf",
    "upvote": "32",
    "text": "Fast Segment Anything\nXu Zhao 1,3\nWenchao Ding 1,2\nYongqi An 1,2\nYinglong Du 1,2\nTao Yu 1,2\nMin Li 1,2\nMing Tang 1,2\nJinqiao Wang 1,2,3,4\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China1\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China2\nObjecteye Inc., Beijing, China3\nWuhan AI Research, Wuhan, China4\n{xu.zhao,yongqi.an,tangm,jqwang}@nlpr.ia.ac.cn\n{dingwenchao2021,duyinglong2022,yutao2022,limin2021}@ia.ac.cn\nAbstract\nThe recently proposed segment anything model (SAM)\nhas made a significant influence in many computer vision\ntasks. It is becoming a foundation step for many high-level\ntasks, like image segmentation, image caption, and image\nediting. However, its huge computation costs prevent it from\nwider applications in industry scenarios. The computation\nmainly comes from the Transformer architecture at high-\nresolution inputs. In this paper, we propose a speed-up al-\nternative method for this fundamental task with compara-\nble performance. By reformulating the task as segments-\ngeneration and prompting, we find that a regular CNN de-\ntector with an instance segmentation branch can also ac-\ncomplish this task well. Specifically, we convert this task\nto the well-studied instance segmentation task and directly\ntrain the existing instance segmentation method using only\n1/50 of the SA-1B dataset published by SAM authors. With\nour method, we achieve a comparable performance with\nthe SAM method at 50\u00d7 higher run-time speed. We give\nsufficient experimental results to demonstrate its effective-\nness. The codes and demos will be released at https:\n//github.com/CASIA-IVA-Lab/FastSAM.\n1. Introduction\nRecently, the Segment Anything Model (SAM) [19] is\nproposed. It is regarded as a milestone vision foundation\nmodel. It can segment any object within the image guided\nby various possible user interaction prompts. SAM lever-\nages a Transformer model trained on the extensive SA-1B\ndataset, which gives it the ability to deftly handle a wide\nrange of scenes and objects. SAM opens the door to an ex-\nciting new task known as Segment Anything. This task, due\nto its generalizability and potentiality, has all the makings\nof becoming a cornerstone for a broad spectrum of future\nvision tasks.\nFastSAM\nSAM\n40ms/img\n2099ms/img\n50\u00d7Faster\n(a)\n(b)\n(c)\nFigure 1. Comparative analysis of FastSAM and SAM. (a) Speed\ncomparison between FastSAM and SAM on a single NVIDIA\nGeForce RTX 3090. (b) Comparison on the BSDS500 dataset\n[1, 28] for edge detection. (c) Box AR@1000 evaluation of Fast-\nSAM and SAM on the COCO dataset [25] for the object proposal.\nBoth SAM and FastSAM are tested using PyTorch for inference,\nexcept FastSAM(TRT) uses TensorRT for inference.\nHowever, despite these advancements and the promising\nresults shown by SAM and subsequent models in handling\nthe segment anything task, its practical applications are still\nchallenging. The glaring issue is the substantial computa-\ntional resource requirements associated with Transformer\n(ViT) models, the main part of SAM\u2019s architecture. When\ncompared with their convolutional counterparts, ViTs stand\nout for their heavy computation resources demands, which\npresents a hurdle to their practical deployment, especially in\nreal-time applications. This limitation consequently hinders\nthe progress and potential of the segment anything task.\nMotivated by the high demand from the industrial appli-\narXiv:2306.12156v1  [cs.CV]  21 Jun 2023\ncations for the segment anything model, in this paper we\ndesign a real-time solution for the segment anything task,\nFastSAM. We decouple the segment anything task into two\nsequential stages which are all-instance segmentation and\nprompt-guided selection. The first stage hinges on the im-\nplementation of a Convolutional Neural Network (CNN)-\nbased detector. It produces the segmentation masks of all in-\nstances in the image. Then in the second stage, it output the\nthe region-of-interest corresponding the prompt. By lever-\naging the computational efficiency of CNNs, we demon-\nstrate that a real-time segment of anything model is achiev-\nable without much compromising on performance quality.\nWe hope the proposed method would facilitate the indus-\ntrial applications of the foundational task of segmenting\nanything.\nOur proposed FastSAM is based on YOLOv8-seg [16],\nan object detector equipped with the instance segmenta-\ntion branch, which utilizes the YOLACT [4] method. We\nalso adopt the extensive SA-1B dataset published by SAM.\nBy directly training this CNN detector on only 2% (1/50)\nof the SA-1B dataset, it achieves comparable performance\nto SAM, but with drastically reduced computational and\nresource demands, thereby enabling real-time application.\nWe also apply it to multiple downstream segmentation tasks\nto show its generalization performance. On the object pro-\nposal task on MS COCO [13], we achieve 63.7 at AR1000,\nwhich is 1.2 points higher than SAM with 32\u00d7 32 point-\nprompt inputs, but running 50 times faster on a single\nNVIDIA RTX 3090.\nThe real-time segment anything model is valuable for in-\ndustrial applications. It can be applied to many scenarios.\nThe proposed approach not only provides a new, practical\nsolution for a large number of vision tasks but also does so\nat a really high speed, tens or hundreds of times faster than\ncurrent methods.\nIt also offers new views for the large model architecture\nfor the general vision tasks. We think for specific tasks, spe-\ncific models still take advantage to get a better efficiency-\naccuracy trade-off.\nThen, in the sense of model compression, our approach\ndemonstrates the feasibility of a path that can significantly\nreduce the computational effort by introducing an artificial\nprior to the structure.\nOur contributions can be summarized as follow:\n\u2022 A novel, real-time CNN-based solution for the Seg-\nment Anything task is introduced, which significantly\nreduces computational demands while maintaining\ncompetitive performance.\n\u2022 This work presents the first study of applying a CNN\ndetector to the segment anything task, offering insights\ninto the potential of lightweight CNN models in com-\nplex vision tasks.\n\u2022 A comparative evaluation between the proposed\nmethod and SAM on multiple benchmarks provides\ninsights into the strengths and weaknesses of the ap-\nproach in the segment anything domain.\n2. Preliminary\nIn this section, we give a review of the segment anything\nmodel and a clear definition of the segment anything task.\nSegment Anything Model. In the evolving field of image\nsegmentation, the Segment Anything Model (SAM) [19] is\na significant innovation due to its proposed training method-\nology and performance on large-scale visual datasets. SAM\nprovides a high-precision, class-agnostic segmentation per-\nformance, exhibiting distinct capabilities in zero-shot tasks.\nAs a foundation model, it expands the horizons of computer\nvision by showing not just powerful interactive segmenta-\ntion methods, but also exceptional adaptability across a va-\nriety of segmentation tasks. SAM is a striking example of\nthe potential of foundation models for open-world image\nunderstanding. However, while the model\u2019s performance\nis satisfying, it is worth noting that SAM faces a significant\nlimitation \u2013 the lack of real-time processing capability. This\nrestricts its wide application in scenarios where immediate\nsegmentation results are critical.\nSegment Anything Task. The Segment Anything task is de-\nfined as a process whereby an effective segmentation mask\nis produced given any form of the prompt. These prompts\nrange from foreground/background point sets, rough boxes\nor masks, free-form text, or any information that indicates\nthe content to be segmented within an image. We have dis-\ncovered that the segment anything task can be effectively\nbroken down into two stages in the majority of practical\napplications.\nThe first stage involves detecting and seg-\nmenting all objects in the image, like a panoptic segmen-\ntation [18] process. The second stage depends on the pro-\nvided prompts to separate the specific object(s) of interest\nfrom the segmented panorama. The decoupling of this task\nsignificantly reduces its complexity, thus providing the pos-\nsibility to propose a real-time segment of anything model.\n3. Methodology\n3.1. Overview\nFig. 2 gives the overview of the proposed method, Fast-\nSAM. The method consists of two stages, i.e.\nthe All-\ninstance Segmentation and the Prompt-guided Selection.\nThe former stage is a basis and the second stage is essen-\ntially the task-oriented post-processing. Different from the\nend-to-end transformers [7,8,19], the overall method intro-\nduces many human priors which match the vision segmen-\ntation tasks, like the local connections of convolutions and\nProtoNet\nDetect\nDetect\nDetect\nMask Coeff.\nMask Coeff.\nMask Coeff.\nNMS\nCrop\nPred\nThreshold\nDetect Branch\n\u00b7\nText\nPoint-prompt\nBox-prompt\nText-prompt\nP5\nP4\nP3\nT1\nI1\nI2\n\u00b7\u00b7\u00b7\nIN\nI1\u00b7T1\n\u00b7\u00b7\u00b7\nI2\u00b7T1\nIN\u00b7T1\nCLIP\n\u201cThe black dog\u201d\nMask Branch\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n-\n+\n=\n\u00b7\u00b7\u00b7\n+0.0137\n-0.0342\n+0.6846\n-1\n+1\n-\n-\nMask Coeff.\nFPN\nCNN\nBackbone\nImage\nEncoder\nText\nEncoder\n\u00b7\u00b7\u00b7\n...\nFigure 2. The framework of FastSAM. It contains two stages: All-instance Segmentation (AIS) and Prompt-guided Selection (PGS). We\nuse YOLOv8-seg [16] to segment all objects or regions in an image. Then we use various prompts to identify the specific object(s) of\ninterest. It mainly involves the utilization of point prompts, box prompts, and text prompt. The text prompt is based on CLIP [31].\nthe receptive-field-relevant object assigning strategies. This\nmakes it tailored for the vision segmentation task and can\nconverge faster on a smaller number of parameters.\n3.2. All-instance Segmentation\nModel Architecture. The architecture of YOLOv8 [16] de-\nvelops from its predecessor, YOLOv5 [15], integrating key\ndesign aspects from recent algorithms such as YOLOX [10],\nYOLOv6 [22], and YOLOv7 [35]. YOLOv8\u2019s backbone\nnetwork and neck module substitute YOLOv5\u2019s C3 module\nwith the C2f module. The updated Head module embraces a\ndecoupled structure, separating classification and detection\nheads, and shifts from Anchor-Based to Anchor-Free.\nInstance\nSegmentation.\nYOLOv8-seg\napplies\nYOLACT [4] principles for instance segmentation.\nIt\nbegins with feature extraction from an image via a\nbackbone network and the Feature Pyramid Network\n(FPN) [24], integrating diverse size features. The output\nconsists of the detection and segmentation branches.\nThe detection branch outputs category and bounding\nbox, while the segmentation branch outputs k prototypes\n(defaulted to 32 in FastSAM) along with k mask coef-\nficients.\nThe segmentation and detection tasks are com-\nputed in parallel. The segmentation branch inputs a high-\nresolution feature map, preserves spatial details, and also\ncontains semantic information.\nThis map is processed\nthrough a convolution layer, upscaled, and then passed\nthrough two more convolution layers to output the masks.\nThe mask coefficients, similar to the detection head\u2019s clas-\nsification branch, range between -1 and 1. The instance seg-\nmentation result is obtained by multiplying the mask coef-\nficients with the prototypes and then summing them up.\nYOLOv8 can be used in a variety of object detection\ntasks. With the instance segmentation branch, YOLOv8-\nSeg is born suitable for the segment anything task, which\naims to accurately detect and segment every object or region\nin an image, regardless of the object category. The proto-\ntypes and mask coefficients provide a lot of extensibility for\nprompt guidance. As a simple example, a simple prompt\nencoder and decoder structure is additionally trained, with\nvarious prompts and image feature embeddings as input and\nmask coefficients as output. In FastSAM, we directly use\nthe YOLOv8-seg method for the all-instance segmentation\nstage. The more artificial design might bring additional im-\nprovements, but we regard it as out of the scope of this work\nand leave it for future study.\n3.3. Prompt-guided Selection\nFollowing the successful segmentation of all objects or\nregions in an image using YOLOv8, the second stage of the\nsegment anything task is to use various prompts to identify\nthe specific object(s) of interest. It mainly involves the uti-\nlization of point prompts, box prompts, and text prompts.\nPoint prompt.\nThe point prompt consists of matching\nthe selected points to the various masks obtained from the\nfirst phase. The goal is to determine the mask in which\nthe point is located.\nSimilar to SAM, we employ fore-\nground/background points as the prompt in our approach.\nIn cases where a foreground point is located in multiple\nmasks, background points can be utilized to filter out masks\nthat are irrelevant to the task at hand. By employing a set of\nforeground/background points, we are able to select multi-\nple masks within the region of interest. These masks will be\nmerged into a single mask to completely mark the object of\ninterest. In addition, we utilize morphological operations to\nimprove the performance of mask merging.\nBox prompt. The box prompt involves performing Inter-\nsection over Union (IoU) matching between the selected\nbox and the bounding boxes corresponding to the various\nmasks from the first phase. The aim is to identify the mask\nwith the highest IoU score with the selected box and thus\nselect the object of interest.\nText prompt. In the case of text prompt, the correspond-\ning text embeddings of the text are extracted using the\nCLIP [31] model. The respective image embeddings are\nthen determined and matched to the intrinsic features of\neach mask using a similarity metric. The mask with the\nhighest similarity score to the image embeddings of the text\nprompt is then selected.\nBy carefully implementing these prompt-guided selec-\ntion techniques, the FastSAM can reliably select specific\nobjects of interest from a segmented image. The above ap-\nproach provides an efficient way to accomplish the segment\nanything task in real-time, thus greatly enhancing the util-\nity of the YOLOv8 model for complex image segmentation\ntasks. A more effective prompt-guided selection technique\nis left for future exploration.\n4. Experiments\nIn this section, we first analysis the run-time efficiency\nof FastSAM. Then we experiment with four zero-shot tasks,\nalong with applications in real-world scenarios, efficiency,\nand deployment. In the first part of the experiments, our\ngoal is to test the similarity in capabilities between Fast-\nSAM and SAM. Following SAM, we also experiment with\nfour tasks with different levels: (1) low-level: edge detec-\ntion, (2) mid-level: object proposal generation, (3) high-\nlevel: instance segmentation, and finally, (4) high-level:\nsegmenting objects with free-form text input. Our exper-\niments also further validate FastSAM\u2019s capabilities with\nreal-world applications and speed.\nImplementation Details.\nUnless stated otherwise, the\nfollowing conditions apply:\n(1) FastSAM employs a\nYOLOv8-x [16] model as the main part of its architecture,\nwith the input size of 1024; (2) FastSAM\u2019s training was car-\nried out on 2% of the SA-1B dataset [19]; (3) We train the\nmodel for 100 epochs using the default hyper-parameter set-\ntings except that the reg max in the bounding box regres-\nsion module is changed from 16 to 26 for predicting large\ninstances.\n4.1. Run-time Efficiency Evaluation\nSAM uses the Transformer architecture to construct an\nend-to-end algorithm. The Transformer is a universal ar-\nchitecture that can represent many forms of mapping func-\ntions of various tasks. To segment anything, SAM learns\nthe vision-oriented inductive bias through the learning pro-\ncess on large-scale data. On the contrary, with the human\npriori knowledge in structure designing, FastSAM obtains\na relatively compact model. From Figure 3, The FastSAM\ngenerates relatively satisfying results.\nIn Table 1, we report the running speed of SAM and Fast-\nSAM on a single NVIDIA GeForce RTX 3090 GPU. It can\nbe seen that FastSAM surpasses SAM at all prompt num-\nbers. Moreover, the running speed of FastSAM does not\nchange with the prompts, making it a better choice for the\nEverything mode.\n4.2. Zero-Shot Edge Detection\nApproach. FastSAM is assessed on the basic low-level task\nof edge detection using BSDS500 [1, 28]. Specifically, we\nselect the mask probability map from the results of Fast-\nSAM\u2019s all-instance segmentation stage. After that, Sobel\nfiltering [33] is applied to all mask probability maps to\ngenerate edge maps. Finally, we conclude with the edge\nNMS [6] step.\nResults. The representative edge maps are illustrated in\nFigure 4.\nUpon qualitative observation, it becomes evi-\ndent that despite FastSAM\u2019s significantly fewer parameters\n(only 68M), it produces a generally good edge map. In com-\nparison to the ground truth, both FastSAM and SAM tend\nto predict a larger number of edges, including some logi-\ncal ones that aren\u2019t annotated in the BSDS500. This bias\nRunning Speed under Different Point Prompt Numbers (ms)\nmethod\nparams\n1\n10\n100\nE(16\u00d716)\nE(32\u00d732*)\nE(64\u00d764)\nSAM-H [20]\n0.6G\n446\n464\n627\n852\n2099\n6972\nSAM-B [20]\n136M\n110\n125\n230\n432\n1383\n5417\nFastSAM (Ours)\n68M\n40\nTable 1. Running Speed (ms/image) of SAM and FastSAM under different point prompt numbers. E: Everything Mode of SAM. *: 32\u00d732\nis the default setting of SAM for many tasks1.\nFigure 3. Segmentation Results of FastSAM\nmethod\nyear\nODS\nOIS\nAP\nR50\nHED [37]\n2015\n.788\n.808\n.840\n.923\nEDETR [30]\n2022\n.840\n.858\n.896\n.930\nzero-shot transfer methods:\nSobel filter\n1968\n.539\n-\n-\n-\nCanny [6]\n1986\n.600\n.640\n.580\n-\nFelz-Hutt [9]\n2004\n.610\n.640\n.560\n-\nSAM [19]\n2023\n.768\n.786\n.794\n.928\nFastSAM\n2023\n.750\n.790\n.793\n.903\nTable 2. Zero-shot transfer to edge detection on BSDS500. Eval-\nuation Data of other methods is from [20].\nis quantitatively reflected in Table 2. Table 2 shows that\nwe achieve similar performance with SAM, specifically a\nhigher R50 and a lower AP.\n4.3. Zero-Shot Object Proposal Generation\nBackground. Object proposal generation has long been a\nbasic pre-processing step for many computer vision tasks,\nincluding general object detection, few-shot object detec-\ntion, and image understanding. Many famous proposal gen-\neration methods witness the evolution of visual recognition\nin the past decades, as a role of the basic step of visual\nrecognition methods. These proposal generation methods\nincludes EdgeBox [38], Geodesic [21], Selective Search\n1https://github.com/facebookresearch/segment-\nanything\n[34], MCG [2].\nThese years, many deep learning-based\nmethods are proposed like DeepMask [29], OLN [17]. For\nexample, RCNN-series object detection methods [11, 12]\nadopts the Seletive Search method, and the recently pro-\nposed open world detector, UniDetector [36], adopts the\nOLN method. Though RPN [32] is used by most exist-\ning object detectors, it can only generate object proposals\nof the learned categories, limiting its application in open-\nvocabulary recognition tasks. Therefore, zero-shot object\nproposal generation is rather important. A good proposed\nmethod is important for the good performance of these vi-\nsual recognition tasks.\nWe directly use the generated bounding boxes of the first\nstage of FastSAM as the object proposals. To evaluate the\nperformance, we test on LVIS [13] and COCO [25] dataset,\nfollowing the existing evaluating strategies. Besides this,\nfollowing the experimental settings of SAM, we also test\nthe mask proposal accuracy by using the all-instance masks\nof the first stage.\nDetails. We report the results of SAM, ViTDet [23], and our\nFastSAM on the LVIS dataset. As SAM does not publicize\nits detailed evaluation codes, we reproduced the category-\nagnostic mask and box recall using the official LVIS eval-\nuation code [13]. However, we fail to reproduce the Mask\nAR results of ViTDet and SAM presented in the SAM\u2019s pa-\nper [20]. Nevertheless, we think our evaluation results still\nreflect several features of FastSAM compared with SAM.\nimage\nground truth\nSAM\nFastSAM\nFigure 4. Zero-shot edge prediction on BSDS500. FastSAM achieves comparable results to SAM.\nResults. The results is shown in Table 3, 4, and 5. The re-\nsults show that our method has a significant advantage on\nthe box proposal generation tasks. Table 3 presents the Av-\nerage Recall (AR) of various methods on the COCO vali-\ndation set. Among these, EdgeBoxes [38], Geodesic [21],\nSel.Search [34], and MCG [2] are methods that do not\nrequire training, whereas DeepMask [29] and OLN [17]\nare supervised methods that are trained on VOC categories\nwithin the COCO training set, and then tested across all cat-\negories. In contrast, our approach and SAM [20] implement\na fully zero-shot transfer. From the table, it can be seen\nthat our method and SAM [20] do not perform as well in\nAR@10 precision compared to previous supervised meth-\nods such as OLN [17]. However, in AR@1000, our method\nsignificantly outperforms OLN [17]. The reason for this is\nthat previous methods were trained on certain categories in\nCOCO, leading to a higher confidence level in these cat-\negories during inference. However, since our method and\nSAM are zero-shot, this results in balanced confidence lev-\nels across different categories, thereby recalling more cate-\ngories not present in COCO. More comparisons can be seen\nin Figure 5.\nIn Table 4, we report the bbox AR@1000 results of\nVitDet-H [23], SAM [20], and our method on the LVIS\nv1 dataset. Our method substantially surpasses the most\ncomputationally intensive model of SAM, SAM-H E64, by\nover 5%. However, it falls short compared to VitDet-H [23],\nwhich was trained on the LVIS dataset.\nThe reason for\nthese results is that during our training process, we used\nthe ground truth (gt) bounding box (bbox) information as a\nsupervisory signal. SAM [20], on the other hand, only used\nthe mask as a supervisory signal, and its bbox at inference\nis generated by extracting the outer box from the mask.\nFrom Table 5, our mask proposal generation is relatively\nlower on Recall.\nWe find this mainly results from that\nour segmentation mask on small-sized objects is not fine-\nAR10\nAR100\nAR1000\nAUC\nEdgeBoxes [38]\n7.4\n17.8\n33.8\n13.9\nGeodesic [21]\n4.0\n18.0\n35.9\n12.6\nSel.Search [34]\n5.2\n16.3\n35.7\n12.6\nMCG [2]\n10.1\n24.6\n39.8\n18.0\nDeepMask [29]\n13.9\n28.6\n43.1\n21.7\nOLN-Box [17]\n27.7\n46.1\n55.7\n34.3\nSAM-H E64\n15.5\n45.6\n67.7\n32.1\nSAM-H E32\n18.5\n49.5\n62.5\n33.7\nSAM-B E32\n11.4\n39.6\n59.1\n27.3\nFastSAM (Ours)\n15.7\n47.3\n63.7\n32.2\nTable 3. Comparison with learning-free methods on All Categories\nof COCO. We report average recall (AR) and AUC of learning\nfree methods, deep learning methods (trained on VOC), and ours\nvs SAM on All generalization. The scores of competing methods\nare taken from [17], which test object proposal methods on all 80\nCOCO classes.\nFigure 5. Comparison with OLN [17] and SAM-H [20]. We test\nobject proposal methods on all 80 COCO classes\ngrained enough. We give more detailed discussions in Sec-\ntion 6.\nbbox AR@1000\nmethod\nall\nsmall\nmed.\nlarge\nViTDet-H [23]\n65.0\n53.2\n83.3\n91.2\nzero-shot transfer methods:\nSAM-H E64\n52.1\n36.6\n75.1\n88.2\nSAM-H E32\n50.3\n33.1\n76.2\n89.8\nSAM-B E32\n45.0\n29.3\n68.7\n80.6\nFastSAM (Ours)\n57.1\n44.3\n77.1\n85.3\nTable 4. Object proposal generation on LVIS v1. FastSAM and\nSAM is applied zero-shot, i.e. it was not trained for object proposal\ngeneration nor did it access LVIS images or annotations.\nmask AR@1000\nmethod\nall\nsmall\nmed.\nlarge\nfreq.\ncom.\nrare\nresults reported in the SAM paper:\nViTDet-H [23]\n63.0\n51.7\n80.8\n87.0\n63.1\n63.3\n58.3\nSAM [20] \u2013 single out. 54.9\n42.8\n76.7\n74.4\n54.7\n59.8\n62.0\nSAM [20]\n59.3\n45.5\n81.6\n86.9\n59.1\n63.9\n65.8\nresults after our replication:\nViTDet-H [23]\n59.9\n48.3\n78.1\n84.8\n-\n-\n-\nSAM-H E64\n54.2\n39.6\n77.9\n83.9\nSAM-H E32\n51.8\n35.2\n78.7\n85.2\n-\n-\n-\nSAM-B E32\n45.8\n31.1\n70.5\n73.6\n-\n-\n-\nFastSAM (Ours)\n49.7\n35.6\n72.7\n77.6\n-\n-\n-\nTable 5. Object proposal generation on LVIS v1. FastSAM and\nSAM are applied zero-shot, i.e. it was not trained for object pro-\nposal generation nor did it access LVIS images or annotations.\nFigure 6. Segmentation results with text prompts\n4.4. Zero-Shot Instance Segmentation\nApproach. Similarly to the SAM method, we accomplish\nthe instance segmentation task by utilizing the bounding\nbox (bbox) generated by ViTDet [23] as the prompt. As de-\nscribed by Section 3.3, we choose the mask with the highest\nIntersection over Union (IoU) with the bbox as the predicted\nmask.\nResults. Table 6 gives the evaluation results. On this task,\nwe fail to achieve a high AP. We infer that this mainly be-\ncause of the segmentation mask accuracy or the box-based\nmask selection strategy. Section 6 gives several examples.\n4.5. Zero-Shot Object Localization with Text\nPrompts\nApproach. Finally, we consider an even high-level task, i.e.\nsegmenting objects by free-form texts. This experiment is\nto show the FastSAM ability of processing text prompts like\nSAM. Different wit SAM, FastSAM doesn\u2019t need to modify\nCOCO [26]\nLVIS v1 [13]\nmethod\nAP\nAPS\nAPM\nAPL\nAP\nAPS\nAPM\nAPL\nViTDet-H [23] 51.0\n32.0\n54.3\n68.9\n46.6\n35.0\n58.0\n66.3\nzero-shot transfer methods (segmentation module only):\nSAM\n46.5\n30.8\n51.0\n61.7\n44.7\n32.5\n57.6\n65.5\nFastSAM\n37.9\n23.9\n43.4\n50.0\n34.5\n24.6\n46.2\n50.8\nTable 6. Instance segmentation results. Fastsam is prompted with\nViTDet boxes to do zero-shot segmentation. The fully-supervised\nViTDet outperforms SAM, but the gap shrinks on the higher-\nquality LVIS masks.\nthe training procedure. It directly runs text through CLIP\u2019s\ntext encoder and then uses the resulting text embedding to\nfind the most similar mask at inference time.\nResults. We show qualitative results in Fig. 6. FastSAM\ncan segment objects well based on the text prompts. Never-\ntheless, the running speed of the text-to-mask segmentation\nis not satisfying, since each mask region is required to be\nfed into the CLIP feature extractor. How to combine the\nCLIP embedding extractor into the FastSAM\u2019s backbone\nnetwork remains an interesting problem with respect to the\nmodel compression.\n5. Real-world Applications\nIn this section, we evaluate the performance of FastSAM\nacross different application scenarios and analyze its advan-\ntages and limitations. We showcase visualizations of Fast-\nSAM\u2019s segmentation using point-prompt, box-prompt, and\neverything modes, and compare it with SAM and ground\ntruths.\nAnomaly Detection.\nAs detailed in [3], anomaly detection is a task that aims\nto distinguish between defective and normal samples in the\nmanufacture. FastSAM is evaluated using the MVTec AD\ndataset [3], with results displayed in Fig. 7.\nUnder ev-\nerything mode, FastSAM can segment nearly all regions\nsimilar to SAM, but with a lower level of precision com-\npared to SAM. In addition, the mask for the background\ndoes not completely cover the entire background, which\nis an inherent characteristic of YOLACT [4].\nBy fore-\nground/background points (yellow and magenta points in\nFastSAM-point respectively) or box-guided selection, Fast-\nSAM can segment on the exact defective regions.\nSalient Object Segmentation.\nThe aim of salient object segmentation [5] is to segment\nthe most attention-grabbing objects from an image. This\ntask is class-agnostic, setting it apart from semantic seg-\nmentation. We apply FastSAM to the well-known saliency\ndataset, ReDWeb-S [27]. As presented in Fig. 8, FastSAM\nexhibited only a minor difference from SAM under every-\nthing mode, as it segment fewer background objects which\nare irrelevant to the task. By points-guided selection, such\noriginal image\nground truth\nFastSAM-point\nFastSAM-box\nFastSAM-everything\nSAM-point\nSAM-box\nSAM-everything\nFigure 7. Application on anomaly detection, where SAM-point/box/everything means using point-prompt, box-prompt, and everything\nmodes respectively.\noriginal image\nground truth\nFastSAM-point\nFastSAM-box\nFastSAM-everything\nSAM-point\nSAM-box\nSAM-everything\nFigure 8. Application on salient object segmentation, where SAM-point/box/everything mean using point-prompt, box-prompt, and\neverything modes respectively.\nas yellow points in FastSAM-point, we can obtain masks of\nall objects of interest. The segmentation result of FastSAM-\npoint is nearly identical to that of the SAM-point and the\nground truth, with only minor details lost at the edges. The\nobject of interest can also be selected by box prompt, such\nas the green box in FastSAM-box. However, it is impossi-\nble to select multiple objects with a single box, which even\nSAM-box cannot realize.\nBuilding Extracting.\nBuilding Extracting from optical remote sensing im-\nagery has a wide range of applications, such as urban\nplanning. We evaluate FastSAM on the dataset proposed\nby [14].\nAs demonstrated in Fig.9, FastSAM performs\nwell in segmenting regularly shaped objects, but segments\nfewer regions related to shadows compared to SAM. We can\nalso select regions of interest with point-prompt and box-\nprompt, as presented in FastSAM-point and FastSAM-box.\nIt is worth noting that we position a point in a shadow region\nin FastSAM-point. However, the correct mask for the build-\ning can still be obtained by merging based on this point.\nThis indicates that our method can resist the interference of\nnoise to some extent.\noriginal image\nground truth\nFastSAM-point\nFastSAM-box\nFastSAM-everything\nSAM-point\nSAM-box\nSAM-everything\nFigure 9. Application on building extracting, where SAM-point/box/everything means using point-prompt, box-prompt, and everything\nmodes respectively.\nFigure 10. FastSAM generates finer segmentation masks on the narrow region of the large objects\n6. Discussion\nGenerally, the proposed FastSAM achieves a compara-\nble performance with SAM and runs 50x faster than SAM\n(32\u00d732) and 170x faster than SAM (64\u00d764). The running\nspeed makes it a good choice for industrial applications,\nsuch as road obstacle detection, video instance tracking, and\nimage manipulation. On some images, FastSAM even gen-\nerates better masks for large objects, as shown in Figure 10.\nWeakness. However, as presented in the experiments, our\nbox generation has a significant advantage, but our mask\ngeneration performance is below SAM. We visualize these\nexamples in Figure 11. We find that FastSAM has the fol-\nlowing features.\n\u2022 The low-quality small-sized segmentation masks have\nlarge confidence scores.\nWe think this is because\nthe confidence score is defined as the bbox score of\nYOLOv8, which is not strongly related to the mask\nquality. Modifying the network to predict the mask\nIoU or other quality indicators is a way to improve that.\n\u2022 The masks of some of the tiny-sized objects tend to\nbe near the square. Besides, the mask of large objects\nmay have some artifacts on the border of the bounding\nboxes. This is the weakness of the YOLACT method.\nBy enhancing the capacity of the mask prototypes or\nreformulating the mask generator, the problem is ex-\npected to solve.\nMoreover, since we only use 1/50 of all SA-1B datasets,\nthe model\u2019s performance can also be further enhanced by\nutilizing more training data.\n7. Conclusion\nIn this paper, we rethink the segment of anything task\nand the model architecture choosing, and propose an al-\nternative solution with 50 times faster running speed than\nFigure 11. Some examples for the bad case of FastSAM.\nSAM-ViT-H (32\u00d732).\nThe experiments show that Fast-\nSAM can solve multiple downstream tasks well.\nStill,\nthere are several weaknesses that can be improved for Fast-\nSAM, like the scoring mechanism and the instance mask-\ngenerating paradigm. These problems are left for future\nstudy.\nReferences\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik. Contour detection and hierarchical image seg-\nmentation. IEEE transactions on pattern analysis and ma-\nchine intelligence, 33(5):898\u2013916, 2010. 1, 4\n[2] Pablo Arbel\u00b4aez, Jordi Pont-Tuset, Jonathan T Barron, Fer-\nran Marques, and Jitendra Malik. Multiscale combinatorial\ngrouping. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 328\u2013335, 2014.\n5, 6\n[3] Paul Bergmann, Michael Fauser, David Sattlegger, and\nCarsten Steger.\nMvtec ad\u2013a comprehensive real-world\ndataset for unsupervised anomaly detection. In CVPR, pages\n9592\u20139600, 2019. 7\n[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\nYolact: Real-time instance segmentation. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 9157\u20139166, 2019. 2, 3, 7\n[5] Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, and\nJia Li. Salient object detection: A survey. Computational\nVisual Media, 5:117\u2013150, 2019. 7\n[6] John Canny. A computational approach to edge detection.\nIEEE Transactions on pattern analysis and machine intelli-\ngence, (6):679\u2013698, 1986. 4, 5\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229.\nSpringer, 2020. 2\n[8] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\npixel classification is not all you need for semantic segmen-\ntation. Advances in Neural Information Processing Systems,\n34:17864\u201317875, 2021. 2\n[9] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient\ngraph-based image segmentation. International journal of\ncomputer vision, 59:167\u2013181, 2004. 5\n[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021. 3\n[11] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1440\u20131448,\n2015. 5\n[12] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik.\nRegion-based convolutional networks for accurate\nobject detection and segmentation.\nIEEE transactions on\npattern analysis and machine intelligence, 38(1):142\u2013158,\n2015. 5\n[13] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 2, 5, 7\n[14] Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional\nnetworks for multisource building extraction from an open\naerial and satellite imagery data set. IEEE Transactions on\nGeoscience and Remote Sensing, 57(1):574\u2013586, 2018. 8\n[15] Glenn Jocher.\nYolov5 by ultralytics, 2020.\nhttps://\ngithub.com/ultralytics/yolov5. 3\n[16] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo by ultra-\nlytics, 2023. https://github.com/ultralytics/\nultralytics. 2, 3, 4\n[17] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon,\nand Weicheng Kuo. Learning open-world object proposals\nwithout learning to classify. IEEE Robotics and Automation\nLetters, 7(2):5453\u20135460, 2022. 5, 6\n[18] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll\u00b4ar. Panoptic segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9404\u20139413, 2019. 2\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 4, 5\n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 5, 6, 7\n[21] Philipp Kr\u00a8ahenb\u00a8uhl and Vladlen Koltun.\nGeodesic object\nproposals. In Computer Vision\u2013ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 725\u2013739. Springer, 2014. 5, 6\n[22] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\niang Chu. Yolov6 v3.0: A full-scale reloading, 2023. 3\n[23] Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li\nCheng. Exploring denoised cross-video contrast for weakly-\nsupervised temporal action localization.\nIn CVPR, pages\n19914\u201319924, 2022. 5, 6, 7\n[24] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 3\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 1, 5\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 7\n[27] Nian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learning\nselective mutual attention and contrast for rgb-d saliency de-\ntection. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 44(12):9026\u20139042, 2021. 7\n[28] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik.\nA database of human segmented natural images\nand its application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In Proceedings Eighth IEEE\nInternational Conference on Computer Vision. ICCV 2001,\nvolume 2, pages 416\u2013423. IEEE, 2001. 1, 4\n[29] Pedro O O Pinheiro, Ronan Collobert, and Piotr Doll\u00b4ar.\nLearning to segment object candidates. Advances in neural\ninformation processing systems, 28, 2015. 5, 6\n[30] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan,\nand Haibin Ling. Edter: Edge detection with transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1402\u20131412, 2022. 5\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 4\n[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28, 2015. 5\n[33] Irwin Sobel, Gary Feldman, et al. A 3x3 isotropic gradient\noperator for image processing. a talk at the Stanford Artifi-\ncial Project in, pages 271\u2013272, 1968. 4\n[34] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-\ners, and Arnold WM Smeulders. Selective search for ob-\nject recognition. International journal of computer vision,\n104:154\u2013171, 2013. 5, 6\n[35] Chien-Yao\nWang,\nAlexey\nBochkovskiy,\nand\nHong-\nYuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets\nnew state-of-the-art for real-time object detectors.\narXiv\npreprint arXiv:2207.02696, 2022. 3\n[36] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Tor-\nralba, Hengshuang Zhao, and Shengjin Wang. Detecting ev-\nerything in the open world: Towards universal object detec-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 11433\u201311443,\n2023. 5\n[37] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE international conference\non computer vision, pages 1395\u20131403, 2015. 5\n[38] C Lawrence Zitnick and Piotr Doll\u00b4ar.\nEdge boxes: Lo-\ncating object proposals from edges. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part V 13, pages\n391\u2013405. Springer, 2014. 5, 6\n"
  },
  {
    "title": "Training Transformers with 4-bit Integers",
    "link": "https://arxiv.org/pdf/2306.11987.pdf",
    "upvote": "20",
    "text": "Training Transformers with 4-bit Integers\nHaocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu\nTsinghua University\n{xihc20,lichangh20}@mails.tsinghua.edu.cn, {jianfeic,dcszj}@tsinghua.edu.cn\nAbstract\nQuantizing the activation, weight, and gradient to 4-bit is promising to accelerate\nneural network training. However, existing 4-bit training methods require custom\nnumerical formats which are not supported by contemporary hardware. In this\nwork, we propose a training method for transformers with all matrix multiplications\nimplemented with the INT4 arithmetic. Training with an ultra-low INT4 preci-\nsion is challenging. To achieve this, we carefully analyze the specific structures\nof activation and gradients in transformers to propose dedicated quantizers for\nthem. For forward propagation, we identify the challenge of outliers and propose a\nHadamard quantizer to suppress the outliers. For backpropagation, we leverage\nthe structural sparsity of gradients by proposing bit splitting and leverage score\nsampling techniques to quantize gradients accurately. Our algorithm achieves\ncompetitive accuracy on a wide range of tasks including natural language under-\nstanding, machine translation, and image classification. Unlike previous 4-bit\ntraining methods, our algorithm can be implemented on the current generation of\nGPUs. Our prototypical linear operator implementation is up to 2.2 times faster\nthan the FP16 counterparts and speeds up the training by up to 35.1%. Our code is\navailable at https://github.com/xijiu9/Train_Transformers_with_INT4.\n1\nIntroduction\nTraining neural networks is computationally demanding. Training with low-precision arithmetic\n(a.k.a., fully quantized training or FQT) is promising to improve computational and memory efficiency.\nFQT methods add some quantizers and dequantizers in the original full-precision computational\ngraph, and replace expensive floating-point operations with cheap low-precision ones.\nResearch in FQT aims to reduce the training numerical precision, without sacrificing much conver-\ngence speed or accuracy. The required numerical precision has been reduced from FP16 [32] to\nFP8 [53, 45], INT32+INT8 [3] and INT8+INT5 [7]. FP8 training is implemented in Nvidia\u2019s H100\nGPU with Transformer Engine [34], achieving impressive speedup for the training of large-scale\ntransformers. Recently, the training numerical precision has been pushed down to 4 bits. Sun et\nal. [46] successfully trained several modern networks with INT4 activation/weights and FP4 gradients;\nand Chmiel et al. [8] propose a custom 4-bit logarithmic numerical format to further improve the\naccuracy. However, these 4-bit training methods cannot be directly utilized for acceleration as they\nrequire custom numerical formats which are not supported on contemporary hardware.\nThere are significant optimization challenges to train neural networks at an extremely low 4-bit level.\nFirst, the non-differentiable quantizers in forward propagation make the loss landscape rugged, where\ngradient-based optimizers can easily stuck at local optima [30]. Second, gradients are only computed\napproximately in low-precision. Such imprecise gradients slow down the training process and even\ncause the training to be unstable or diverge.\nIn this work, we propose a novel INT4 training algorithm for a class of popular neural networks,\ntransformers [51]. All the costly linear operations for training transformers can be written in a matrix\nPreprint. Under review.\narXiv:2306.11987v2  [cs.LG]  22 Jun 2023\nmultiplication (MM) form. This MM form allows us to design more flexible quantizers, which better\napproximate FP32 matrix multiplications by utilizing specific structures of the activations, weights,\nand gradients in transformers. Our quantizers leverage advances in the field of randomized numerical\nlinear algebra (RandNLA) [14].\nFor forward propagation, we find that outliers in the activation are the main reason for accuracy\ndegradation. To suppress outliers, we propose a Hadamard quantizer, which quantizes a transformed\nversion of the activation matrix. The transformation is a block diagonal Hadamard matrix, which\nspreads the information carried in outliers to its nearby entries of the matrix and thus reduces the\nnumerical range of the outliers.\nFor backpropagation, we exploit the structural sparsity of activation gradients. We find that the\ngradients of a few tokens are extremely large. Meanwhile, the gradients for the rest majority of the\ntokens are very small, even smaller than the quantization residuals of larger gradients. Rather than\ncomputing these small gradients, it is better to save the computational resource for calculating the\nresiduals of the larger gradients. To utilize such sparsity, we propose bit splitting, which split the\ngradient of each token into higher 4 bits and lower 4 bits. Then, we choose the most informative\ngradients by leverage score sampling, which is an importance sampling technique for RandNLA.\nCombining quantization techniques for forward and backward propagation, we propose an algorithm\nthat uses INT4 MMs for all linear operations in transformers. We evaluate our algorithm for training\ntransformers on a wide variety of tasks, including natural language understanding, question answering,\nmachine translation, and image classification. Our algorithm achieves competitive or superior accu-\nracy compared with existing works on 4-bit training [46, 8]. Moreover, our algorithm is compatible\nwith contemporary hardware like GPUs, since it does not require custom numerical formats like FP4\nor logarithm formats. Our prototypical quantization + INT4 MM operator implementation is up to\n2.2 times faster than the FP16 MM baseline, and it speeds up the training by up to 35.1%.\n2\nRelated Work\nFully Quantized Training\nFully quantized training (FQT) [32, 53, 45, 3, 15, 1, 56, 64, 28, 29, 58,\n67] methods accelerate training by quantizing the activations, weights, and gradients to low-precision,\nso linear and nonlinear operators during training can be implemented with low-precision arithmetic.\nResearches on FQT design novel numerical formats and quantization algorithms which better approx-\nimate full-precision tensors. The current research frontier is 4-bit FQT. FQT is challenging due to the\nvast numerical range of the gradient and the optimization issues of training quantized networks from\nscratch. Due to these challenges, existing 4-bit FQT algorithms [46, 8] still have \u223c1-2.5% accuracy\ndrop on several tasks, and they cannot support contemporary hardware.\nOther Efficient Training Methods\nMixture-of-experts [42] improves the model capacity without\nincreasing the training budget. Structural dropout [21, 17] exploits computationally efficient ways to\nregularize the model. Efficient attention [26, 10] reduces the quadratic time complexity for computing\nattention. Distributed training systems [38, 22] reduce training time by leveraging more computational\nresources. Our work on reducing numerical precision is orthogonal with these directions.\n3\nForward Propagation\nNeural network training is an iterative optimization procedure with stochastic gradients computed\nby forward and back propagation. We accelerate forward and back propagation with 4-bit integer\n(INT4) arithmetic. We first describe the forward propagation of our training procedure. The forward\npropagation can be formulated as a composition of linear and non-linear (GeLU, normalization,\nsoftmax, etc.) operators. In our training procedure, we accelerate all the linear operators with INT4\narithmetic and leave all the less-computationally-intensive non-linear operators in the 16-bit floating-\npoint (FP16) format. All linear operations in transformers can be written in a matrix multiplication\n(MM) form. For ease of presentation, we consider the acceleration of the following simple matrix\nmultiplication throughout this paper:\nZ = XW\u22a4, where Z \u2208 RN\u00d7C, X \u2208 RN\u00d7Dand W \u2208 RC\u00d7D.\n(1)\n2\nThe most predominant use case of such MM is the fully-connected layer. Consider a transformer\nwith an input shape of (batch size S, sequence length T, dimensionality D). The fully-connected\nlayer can be written as Eq. (1) where X is the activation for N = ST tokens, and W is the weight\nmatrix. For attention layers, batch matrix multiplications (BMMs) might be required. Our proposed\ntechniques can be applied to BMMs, and we leave the discussion of BMMs in Appendix. A.1.\n3.1\nLearned Step Size Quantization\nTo accelerate training, the forward propagation must be computed with integer arithmetic. We\nleverage the learned step size quantizer (LSQ) [16] for this purpose. LSQ is a static quantization\nmethod whose quantization scale does not depend on the input, and is thus cheaper than dynamic\nquantization methods [23], which need to compute the quantization scale dynamically per iteration.\nGiven a FP matrix X, LSQ quantizes X to integer with\nintsX (X) := \u230aclamp(X/sX, \u2212QN, QP )\u2309 ,\n(2)\nwhere sX is a learnable scalar parameter, clamp restricts its input to the range [\u2212QN, QP ], \u230a\u00b7\u2309 is a\nrounding operation, and X/sX is computed elementwise. The resultant matrix takes values from\n{\u2212QN, \u2212QN + 1, . . . , QP }. Since we aim to perform INT4 MMs, we set QN = QP = 7. The\ninteger matrix can be dequantized back to FP through float (intsX (X)) = sXintsX (X) \u2248 X.\nWith LSQ, Eq. (1) can be computed approximately as Y = XW\u22a4 \u2248 sXsW intsX (X) intsW (W)\u22a4 ,\nwhere the INT4 MM intsX (X) intsW (W)\u22a4 can be implemented efficiently on hardware.\nRemark:\nQuantization-aware training (QAT) [9, 62, 66, 23, 12, 11, 43, 59, 44, 48, 63, 2, 18, 54]\nis an inference acceleration technique which trains networks with quantizers inserted in the forward\npropagation graph, so the trained network can perform efficiently during inference. QAT can compress\nactivation/weights to extremely low precision (e.g. 1-2 bits). It is tempting to think that directly\napplying a quantizer for QAT to FQT can lead to similar low activation/weights bit-width. However,\neven only quantizing the forward propagation for FQT is much more challenging than QAT because:\n(1) QAT requires a converged full-precision model as initialization [16] and/or as a teacher model\nfor knowledge distillation [2]; (2) QAT can adopt expensive multi-stage training pipelines without\nworrying about the convergence speed [31], while FQT algorithm must converge as fast as full-\nprecision training algorithms to be useful; (3) QAT may approximate the discrete quantizer with\ncontinuous functions during training [19], which cannot be implemented with integer arithmetic. Due\nto these challenges, it is still an open problem to do FQT with 4-bit activations/weights.\n3.2\nActivation Outliers\nSimply applying LSQ for FQT with 4-bit activation/weights leads to accuracy degradation due to\nactivation outliers [57]. As shown in Fig. 1(a), activations have some outlier entries, which are\nmuch larger in magnitude than other entries. In this case, the step size sX poses a trade-off between\nquantization granularity and representable numerical range. If sX is large, we can represent the\noutliers well at the expense of representing most other entries in a very coarse manner. On the\nother hand, if sX is small, we have to truncate the entries outside the range [\u2212QNsX, QP sX].\nUnfortunately, the transformers tend to store information in these outliers, and such truncation would\nseriously harm accuracy (see Sec. 5.2 for details). The outlier problem is particularly significant\nwhen the training task is to fine-tune a pre-trained model on some new downstream tasks, since the\npre-train model contains more outliers [57] than random initialization.\nThere exists some works to handle activation outliers for post-training quantization (PTQ). Outlier\nSuppression [55] discover that LayerNorms amplify outliers, and propose Gamma Migration and\nToken-Wise Clipping to solve this issue and achieves 6-bit BERT PTQ without too much degradation.\nSmoothQuant [57] migrates the quantization difficulty of activation outliers to weights and achieves\n8-bit PTQ for large language models, such as OPT-175B. Outlier Channel Splitting [65] duplicates\nchannels containing outliers with small overhead on the size of the network. However, these methods\nmainly focus on PTQ or QAT, and seldom successfully deal with ultra-low 4-bit training.\n3\nFigure 1:\nHistogram of activation of the\nlinear-1-2 layer in a BERT-base-uncased\nmodel. (a) Original activation distribution; (b)\nHadamard-transformed activation distribution.\nFigure 2: (a) The distribution of gradient norm\nalong the token dimension. (b) The cumulative\nsum of the top X values as a percentage of the\nsum of all norms along the token dimension.\n3.3\nHadamard Quantization\nWe propose a Hadamard quantizer (HQ) to solve the outlier problem. Its main idea is to quantize the\nmatrices in another linear space which has fewer outliers.\nThe outliers in activation matrices form a feature-wise structure [57]. They are typically concentrated\non a few dimensions, i.e., only a few columns of X are significantly larger than others. Hadamard\ntransform [47] is a linear transformation, which can amortize the outliers into other entries. Specifi-\ncally, the Hadamard transform Hk is a 2k \u00d7 2k matrix, where\nH0 = [1] ,\nHk =\n1\n\u221a\n2 [Hk\u22121\nHk\u22121; Hk\u22121\n\u2212 Hk\u22121] .\nHadamard matrices are orthogonal and symmetric: Hk = H\u22a4\nk = H\u22121\nk , so HkHk = I, \u2200k \u2265 0.\nConsider any coordinate row vector1 e\u22a4\ni\n\u2208 R2k, we have e\u22a4\ni Hk = 2\u2212k/212k, \u2200i, where 12k =\n(1, 1, . . . , 1) is a 2k-dimensional all-one-vector. This demonstrates the extreme case when a single\noutlier dominates all the rest dimensions. In this case, Hadamard transformation effectively turns the\nvector into a quantization-friendly all-one-vector. The practical effect of the Hadamard transform on\nsuppressing activation outliers is demonstrated in Fig. 1(b).\nHQ uses a block-diagonal transformation matrix H \u2208 RD\u00d7D: H = BlockDiag(Hk, . . . , Hk), where\nD is a multiple of 2k. To suppress outliers, we quantize a transformed version of X and W:\nX = (XH)H\u22a4 \u2248 sXintsX (XH) H\u22a4,\nW = (WH)H\u22a4 \u2248 sW intsW (WH) H\u22a4.\nCombining the quantized matrices, we get\nY = XW\u22a4 \u2248 sXsW intsX (XH) H\u22a4HintsW\n\u0010\nH\u22a4W\u22a4\u0011\n= sXsW intsX (XH) intsW\n\u0010\nH\u22a4W\u22a4\u0011\n, (3)\nwhere the inverse transformations cancel with each other, and the MM can be implemented as:\nProcedure HQ-MM\n1. Compute XH and H\u22a4W\u22a4 in FP16.\n2. Quantize the resultant matrices to INT4 by LSQ.\n3. Multiply the two INT4 matrices.\n4. Dequantize the resultant INT32 matrix to FP16 by multiplying sXsW .\nFor time complexity, Step 1 takes O(2kN(D + C)) FP16 multiply-accumulates (MACs); Step 2\nand Step 4 takes O(N(D + C)) FP16 MACs in total; and Step 3 takes O(NDC) INT4 MACs.\nComparing with the plain LSQ Eq. (2), the amount of FP16 MACs increases by 2k times, from\nO(N(D + C)) to O(2kN(D + C)). However, our HQ-MM is still much cheaper than an FP16 MM\ngiven 2k \u226a D and 2k \u226a C. The number k shows a tradeoff between the ability to suppress outliers\nand computation complexity. Larger k allows for amortizing the outlier within a larger horizon, at the\ncost of being more expensive. We propose an adaptive algorithm to choose k for each activation\ndepending on the outlier scale, as discussed in Appendix A.5. The typical value is k = 5, while the\ndimensionality C and D ranges from 768 to 4096.\n1A vector which i-th dimension is 1, and all other dimensions are 0.\n4\n4\nBackpropagation\nWe now consider accelerating the backpropagation of the linear layer with INT4 operations. The\nlinear operator HQ-MM defined in Eq. (3) has four inputs: activation X, weight W, and step sizes sX,\nsW . Given the output gradient \u2207YL w.r.t. some loss function L, we need to compute the gradient of\nall four inputs. We discuss the computation of activation/weight gradients in this section, and left the\ndiscussion of step size gradients to Appendix A.3. For simplicity, we omit L and simply use \u2207Y to\ndenote the gradient in the following text.\nBy the straight-through estimator \u230ax\u2309\u2032 = 1 [5] and the chain rule, we have\n\u2207W = sX\n\u0010\n\u2207\u22a4\nY \u02c6X \u25e6 IW\n\u0011\nH\u22a4,\n\u2207X = sW IX \u25e6 \u2207Y \u02c6\nWH\u22a4,\n(4)\nwhere we define \u02c6X = intsX (XH), \u02c6\nW = intsW (WH), IX = I(\u2212QN \u2264 X/sX \u2264 QP ), and\nIW = I(\u2212QN \u2264 W/sW \u2264 QP ). For computing the gradients, three types of matrix multiplications\nare required:\n1. The element-wise multiplication \u25e6 of a 0/1 matrix IX (or IW ) with another INT4 (or INT32)\nmatrix. This operation has low time complexity.\n2. The multiplication of an INT32 matrix with an FP16 block-wise Hadamard matrix sW H\u22a4, which\nalso has low-time complexity, as discussed in Sec. 3.3.\n3. The multiplication of the FP16 gradient \u2207Y with an INT4 matrix \u02c6X or \u02c6\nW, which we will\naccelerate by quantizing \u2207Y to INT4.\nIn the rest of this section, we will discuss quantization methods to compute the \u201ctype 3\u201d MMs\n\u2207\u22a4\nY \u02c6X and \u2207Y \u02c6\nW. We quantize \u2207Y dynamically for each MM, while \u02c6X and \u02c6\nW have been already\ncalculated in forward propagation in Section. 3. We start by discussing the structure of the gradient.\n4.1\nStructural Sparsity of Gradients\nWe note that the gradient matrix \u2207Y tends to be very sparse along the training process. Furthermore,\nthe sparsity has a structure: few rows (i.e., tokens) of \u2207Y have large entries, while most other rows\nare close to an all-zero vector. We illustrate this by plotting the histogram of per-row norm \u2225(\u2207Y)i,:\u2225\nfor all the rows i in Fig. 2.\nSuch a structural sparsity arises from the heavy overparameterization [61] of modern neural networks.\nDuring almost the entire training process, the network operates in the overparameterized scheme [33],\nwhere it can fit most training data well, except for a few hard examples. Therefore, the (activation)\ngradient will be close to zero for well-fitted data points. We find that for pretraining tasks, such\nstructural sparsity quickly emerges after only a few training epochs. For fine-tuning tasks, the gradient\nis always sparse during the whole training process.\n4.2\nBit Splitting and Leverage Score Sampling\nHere, we discuss how to design gradient quantizers to accurately compute the MMs during backprop-\nagation by leveraging structural sparsity. The high-level idea is that many rows of the gradient are so\nsmall that they have little impact on the parameter gradient, yet they waste abundant computation.\nOn the other hand, the large rows cannot be accurately represented with INT4. We drop some small\nrows and use the saved computation to represent large rows more accurately.\nFirst, we propose bit splitting (BS), which splits a full-precision matrix as higher and lower 4 bits:\n\u2207Y \u2248 s\u2191\u2207\u2191\nY + s\u2193\u2207\u2193\nY,\n(5)\nwhere s\u2191, s\u2193 are two floating-point scalars, and \u2207\u2191\nY, \u2207\u2193\nY are INT4 matrices representing the higher\nand lower 4 bits, respectively. BS can be implemented by first quantizing \u2207Y to INT4 as \u2207Y \u2248\ns\u2191\u2207\u2191\nY and then quantize the residual to INT4 as \u2207Y \u2212 s\u2191\u2207\u2191\nY \u2248 s\u2193\u2207\u2193\nY. BS can be viewed as an\nINT8 representation of a matrix, where \u2207\u2191\nY and \u2207\u2193\nY are the higher and lower 4 bits of the INT8\nrepresentation. Next, we discuss how to compute the weight and activation gradient.\n5\nWeight Gradient\nAs discussed earlier, weight gradient involves the matrix multiplication \u2207\u22a4\nY \u02c6X,\nwhere \u2207Y \u2208 RN\u00d7C and \u02c6X is an N \u00d7 D INT4 matrix. By Eq. (5):\n\u2207\u22a4\nY \u02c6X \u2248 (s\u2191\u2207\u2191\nY\n\u22a4 + s\u2193\u2207\u2193\nY\n\u22a4) \u02c6X = \u2207\u2195\nY\n\u22a4X\u2195,\n(6)\nwhere we define \u2207\u2195\nY = [s\u2191\u2207\u2191\nY; s\u2193\u2207\u2193\nY]\u22a4 \u2208 R2N\u00d7C and \u02c6X\n\u2195 = [ \u02c6X; \u02c6X] to be a 2N \u00d7 D INT4 matrix.\nEq. (6) represents the product of an INT8 \u2207\u22a4\nY and an INT4 \u02c6\nW, and can be implemented by two\nINT4 MMs \u2207\u2191\nY\n\u22a4 \u02c6X and \u2207\u2193\nY\n\u22a4 \u02c6X. Such MM is rather accurate since \u2207Y is represented with 8 bits.\nHowever, comparing to a na\u00efve quantization of \u2207Y to INT4, BS doubles the amount of INT4\noperations for MM. We propose leverage score sampling (LSS) to cut the operations of Eq. (5) by\nhalf, to the same amount as the na\u00efve MM s\u2191\u2207\u2191\nY \u02c6X. Noticing that the MM Eq. (6) can be written as\nthe sum of 2N rank-1 matrices:\n\u2207\u2195\nY\n\u22a4X\u2195 =\n2N\nX\ni=1\n\u2207\u2195\nY\n\u22a4\n:,iX\u2195\ni =\n2N\nX\ni=1\n\u2207Wi,\n(7)\nwhere \u2207Wi = \u2207\u2195\nY\n\u22a4\n:,iX\u2195\ni . Due to the sparsity of \u2207Y, the matrices \u2207Wi differ in magnitude and\nsmall matrices can be discarded without having a big influence on the result.\nOur proposed LSS assigns each \u2207Wi a probability pi \u2208 [0, 1], i = 1, \u00b7 \u00b7 \u00b7 , 2N, that satisfies\nP2N\ni=1 pi = N. We define random masks mi \u223c Bern(pi) and mask matrix \u02dcM, and approximate it as\n\u2207\u2195\nY\n\u22a4X\u2195 \u2248 \u2207\u2195\nY\n\u22a4 \u02dcMX\u2195 =\n2N\nX\ni=1\nmi\npi\n\u2207\u2195\nY\n\u22a4\n:,iX\u2195\ni , where \u02dcM = diag\n\u0010\nm1\np1 , . . . , m2N\np2N\n\u0011\n,\nwhich is an unbiased approximation since E\n\u0014\n\u2207\u2195\nY\n\u22a4 \u02dcMX\u2195\n\u0015\n= \u2207\u2195\nY\n\u22a4E\nh\n\u02dcM\ni\nX\u2195 = \u2207\u2195\nY\n\u22a4X\u2195.\nIn expectation, there are only N nonzero mis. Therefore, LSS reduces the cost of MM by half. For\nLSS to be accurate, we minimize its variance. We have:\nProposition 4.1. (LSS variance for weight gradient)\nVar\n\" 2N\nX\ni=1\nmi\npi\n\u2207\u2195\nY\n\u22a4\n:,iX\u2195\ni\n#\n=\n2N\nX\ni=1\n1 \u2212 pi\npi\n\u2225\u2207\u2195\nYi,:\u22252\u2225X\u2195\ni,:\u22252, where Var [X] := E [\u2225X \u2212 EX\u2225]2\nF .\nThe coefficient ci := \u2225\u2207\u2195\nYi,:\u2225\u2225X\u2195\ni,:\u2225 is called the leverage score, which can be easily computed in\nlow time complexity. When pi \u221d ci, the variance attends its minimum due to Cauchy inequality:\n2N\nX\ni=1\n1\npi \u2225\u2207\u2195\nYi,:\u22252\u2225X\u2195\ni,:\u22252 =\n2N\nX\ni=1\nc2\ni\npi =\n2N\nX\ni=1\nc2\ni\npi\n2N\nX\ni=1\npi \u2265 (\n2N\nX\ni=1\nci)2,\nwhere the equality holds when pi \u221d ci. Intuitively, LSS can approximate the MM Eq. (7) well with\nsignificantly lower computational cost when the leverage scores {ci} are diverse, which is indeed the\ncase as shown in Fig. 2.\nDefine M\u2191 to be the top-left N \u00d7 N submatrix of \u02dcM and M\u2193 to be the bottom-right one, we have\n\u2207\u2195\nY\n\u22a4 \u02dcMX\u2195 = s\u2191\u2207\u2191\nY\n\u22a4 \u02dcM\n\u2191 \u02c6X + s\u2193\u2207\u2193\nY\n\u22a4 \u02dcM\n\u2193 \u02c6X,\nwhich can be implemented by two INT4 MMs with sampled rows/columns.\nPutting every-\nthing together, we propose the following MM procedure to compute the weight gradient:\nProcedure LSS-MM\n1. Quantize \u2207Y with BS to obtain \u2207\u2191\nY and \u2207\u2193\nY in INT4.\n2. Compute the leverage score \u2225\u2207\u2195\nYi,:\u2225\u2225X\u2195\ni,:\u2225 in FP16.\n3. Sample the masks {mi}.\n4. Sample rows of \u2207Y and \u02c6X given the masks {mi}.\n5. Compute INT4 MMs \u2207\u2191\nY\n\u22a4 \u02dcM\n\u2191 \u02c6X and \u2207\u2193\nY\n\u22a4 \u02dcM\n\u2193 \u02c6X,\n6. Dequantize and sum up the resultant INT32 matrices to obtain the FP16 result \u2207\u2195\nY\n\u22a4 \u02dcMX\u2195.\n6\nTable 1: Results on language model fine-tuning, transformer pretraining, and vision transformers\nfine-tuning and pretraining. Standard deviation is reported as subscript. FT refers to Fine-tuning, and\nPT refers to Pre-training. For WMT the result of 25.4 is result of Ultra-Low, not INT8.\nBASESLINES\n4-BIT TRAINING METHODS\nDATASET\nTRAIN TYPE\nMODEL\nMETRIC NAME\nFP\nINT8\nLSQ+LUQ\nHQ+LSS\nGLUE-DEV\nFT\nBERT-BASE\nAVG\n82.670.24\n81.450.13\n75.290.52\n80.810.31\nBERT-LARGE\nAVG\n84.570.42\n82.740.24\n55.932.47\n82.250.58\nSQUAD V1\nFT\nBERT-BASE\nF1\n88.320.30\n88.420.20\n85.750.31\n87.600.25\nSQUAD V2\nFT\nBERT-BASE\nF1\n76.040.68\n75.630.07\n71.020.41\n74.630.18\nADVERSARIAL QA\nFT\nBERT-BASE\nF1\n40.990.38\n40.170.58\n31.850.30\n38.700.77\nSWAG\nFT\nBERT-BASE\nACC\n79.840.10\n79.180.19\n70.791.20\n77.490.16\nCONLL\nFT\nBERT-BASE\nACC\n93.380.08\n93.130.14\n87.630.39\n91.900.48\nWMT\nPT\nTRANSFORMER-BASE\nBLEU\n27.5\n25.4(ULTRA LOW)\n27.17\n-\nSACREBLEU\n26.5\n-\n-\n25.57\nCIFAR10\nFT\nVIT-B/32\nTOP1 ACC\n98.770.03\n98.590.02\n97.760.10\n98.360.05\nVIT-L/32\n98.98\n98.76\n98.38\n98.47\nCIFAR100\nFT\nVIT-B/32\nTOP1 ACC\n91.940.11\n90.990.07\n88.630.085\n89.780.06\nVIT-L/32\n93.07\n92.2\n90.97\n91.13\nIMAGENET1K\nFT\nVIT-B/32\nTOP1 ACC\n81.88\n80.42\n77.25\n79.18\nVIT-L/32\n81.62\n81.3\n77.41\n80.06\nVIT-L/16\n84.55\n83.05\n82.4\n82.61\nPT\nDEIT-SMALL\nTOP1 ACC\n73.1\n70.95\n69.96\n69.18\nAs \u02dcM only has N non-zero elements in expectation, the two matrix multiplications in Step 5 take\nabout 2NCD INT4 MACs, which aligns with the cost of the na\u00efve MM s\u2191\u2207\u2191\nY \u02c6X. The overhead of\nall the other steps is O(NC + ND) in total.\nActivation Gradient\nSimilar to the previous discussion, the gradient of input can be written as\n\u2207Y \u02c6\nW \u2248 (s\u2191\u2207\u2191\nY + s\u2193\u2207\u2193\nY) \u02c6\nW = s\u2191\u2207\u2191\nY \u02c6\nW + s\u2193\u2207\u2193\nY \u02c6\nW =\n\u0010\n\u02c6I\n\u2195\u2207\u2195\nY\n\u0011\n\u02c6\nW,\n(8)\nwhere we define \u2207\u2195\nY = [s\u2191\u2207\u2191\nY; s\u2193\u2207\u2193\nY] \u2208 R2N\u00d7C and \u02c6I\n\u2195 = [I\nI] to be a N \u00d7 2N INT4 matrix, I\nis a N \u00d7 N identity matrix. The original product can also be implemented by two INT4 MMs \u2207\u2191\nY \u02c6\nW\nand \u2207\u2193\nY \u02c6\nW. But different from weight gradients, we now focus on \u02c6I\n\u2195\u2207\u2195\nY in Eq. (8) and do leverage\nscore sampling on this MM. A detailed discussion can be found in Appendix B.2, and we only\npresent the leverage score here. Similarly, we write the MM as the sum of 2N smaller multiplications:\n\u02c6I\n\u2195\u2207\u2195\nY =\n2N\nX\ni=1\n\u02c6I\n\u2195\n:,i\u2207\u2195\nYi \u2248 mi\npi\n2N\nX\ni=1\n\u2207Yi,\nwhere we define \u2207Yi = \u02c6I\n\u2195\n:,i\u2207\u2195\nYi and associate the probability pi and Bernoulli mask mi \u223c Bern(pi)\nwith the i multiplication. The leverage score for activation gradient is ci := \u2225\u2207\u2195\nYi\u2225, and the variance\nattains minimum when pi \u221d ci. More details about the algorithm can be found at Appendix. A.3 On\nthe implementation side, once the mask {mi} is known, we can decompose the MM Eq. (8) as two\nINT4 MMs:\n\u0010\n\u02c6I\n\u2195 \u02dcM\u2207\u2195\nY\n\u0011\n\u02c6\nW = s\u2191 \u02dcM\n\u2191\u2207\u2191\nY \u02c6\nW + s\u2193 \u02dcM\n\u2193\u2207\u2193\nY \u02c6\nW.\n5\nExperiments\nWe evaluate our INT4 training algorithm on a wide variety of tasks including language model\nfine-tuning, machine translation, and image classification. We implement our proposed HQ-MM and\nLSS-MM algorithms with CUDA and cutlass2, and the implementation details can be found in Ap-\npendix A. We replace all the floating-point linear operators with our INT4 implementation except\nsimply using LSQ for embedding layers, and leaving the last classifier layer in full precision. We\nadopt default architectures, optimizers, schedulers, and hyper-parameters for all the evaluated models.\n5.1\nConverged Model Accuracy\nWe compare the accuracy of the converged model on various tasks in Table 1. The compared methods\ninclude full-precision training (FP), INT8 training [3](INT8), FP4 training [46] (\u201cUltra-low\u201d), 4-bit\n2https://github.com/NVIDIA/cutlass\n7\nFigure 3: CoLA performance under\ndifferent methods using different bits.\n(a) Comparison of forward methods.\n(b) Comparison of backward methods.\nFigure 4: Compar-\nison of basic FP16\nMM, HQ, and LSS\noperators.\nFigure 5: SpeedUp of our INT4 train-\ning algorithm compared with FP16\nPyTorch AMP on (a) Bert-Large (b)\nGpt2-base.\nlogarithm quantization [8] with LSQ for activations and weights (LSQ+LUQ), and our algorithm\nwhich utilizes HQ for forward and LSS for backpropagation (HQ+LSS). Ultra-low does not have\na public implementation, so we only report its performance from its original paper on the machine\ntranslation task. Except for the large machine translation task and the task of large vision transformers,\nwe repeat each run by three times and report the standard deviation as subscripts in tables. We do not\ninclude any kind of knowledge distillation or data augmentation.\nLanguage model fine-tuning:\nWe use the pretrained BERT-base-uncased and BERT-large-uncased\n[24] model, and evaluate the performance of our method on GLUE dev-set [52], SQUAD [40],\nSQUADv2 [39], Adversarial QA [4], CoNLL-2003 [41] and SWAG [60] datasets. We present\nthe average result of bert-base-uncased and bert-large-uncased model on the GLUE dataset. The\nfull results are listed in Appendix C.2. Compared with LSQ+LUQ, our method achieves 5.5%\nimprovement of accuracy on average for the bert-base model and achieves > 25% improvement of\naccuracy on average for the bert-large model. We further show the result on the SQUAD, SQUAD 2.0,\nAdversarial QA, CoNLL-2003, and SWAG datasets. On all of the tasks, compared with LSQ+LUQ,\nour method achieves better performance. We improve by 1.8% and 3.6% on SQUAD and SQUAD\n2.0 compared to LSQ+LUQ, respectively. On the more difficult Adversarial QA, we improve by 6.8%\non F1 score. On SWAG we improve by 6.7% and on CoNLL-2003 we improve by 4.2% accuracy.\nMachine translation:\nWe also apply our method for pretraining. We train a Transformer-base [51]\nmodel on WMT 14 En-De dataset [6] for machine translation. Note that we reproduce this experiment\nwith Fairseq\u2019s recipe 3, which reports the SacreBleu score (26.5 for FP) [36], while Ultra-low and\nLUQ report the more optimistic original BLEU score (27.5 for FP) [35]. Our HQ+LSS has about\n1.0% BLEU degradation, which is smaller than 2.1% of Ultra-low and higher than 0.3% reported\nin the LUQ paper. Nevertheless, HQ+LSS still performs comparably with existing methods for this\npretraining task, and it supports contemporary hardware.\nImage Classification:\nWe load ViT checkpoints pretrained on ImageNet21k [13], and fine-tune\nit on CIFAR-10, CIFAR-100 [27], and ImageNet1k. We use ViT-B/32 and ViT-L/32 for CIFAR\ndatasets and use ViT-B/32, ViT-L/32 and ViT-L/16 for ImageNet1k. On CIFAR10 we achieve < 0.5%\naccuracy degradation, while LSQ+LUQ has 1% degradation for ViT-B/32 and 0.6% degradation\nfor ViT-L/32. On CIFAR100, INT8 already has \u223c 1% accuracy degradation, which shows its\ndifficulty. We improve by 1.1% accuracy for ViT-B/32 and 0.2% accuracy for ViT-L/32 compared\nwith LSQ+LUQ. On ImageNet1k, we improve by 2% accuracy for ViT-B/32, 2.6% accuracy for\nViT-L/32 and 0.2% for ViT-L/32 compared with LSQ+LUQ. We further test the effectiveness of\nour algorithm for pretraining a DeiT-Small model [50] on ImageNet1K, where HQ+LSS can still\nconverge to similar accuracy level compared to LSQ+LUQ, while being more hardware friendly.\n5.2\nAblation Study\nHere, we conduct ablation studies to show the effectiveness of our forward and backward methods\nindependently on the challenging CoLA dataset. To study the effectiveness of different quantizers for\nforward propagation, we leave backpropagation in FP16. The result is shown in Fig. 3(a). We first\nvalidate the claim in Sec. 3.2 that outliers are the main cause of accuracy degradation in quantized\nforward propagation. We test an \u201coutlier\u201d method which maintains 1% largest activation entries in\n3https://github.com/facebookresearch/fairseq\n8\nFP. The \u201coutlier\u201d method achieves good performance, which proves that outliers are indeed the most\nsignificant challenge of the transformer\u2019s forward quantization. The hardware-unfriendly \u201coutlier\u201d\nmethod serves as an upper bound of methods to handle outliers. Our HQ outperforms LSQ by better\nhandling the outliers and achieves comparable results to maintaining the outliers.\nWe also investigated whether more granular quantizers, such as per-token quantization or per-channel\nquantization could be used to quantify outliers, or whether existing methods like SmoothQuant [57]\ncould be used for INT4 FQT. The results are listed in Appendix C.3, and we find that without HQ,\nnone of these methods achieve good accuracy under 4-bit quantization, and the result of HQ is not\nstrongly affected when more granular quantization methods are applied.\nFor backpropagation, we compare a simple minimax quantizer [3], LUQ [8] and our LSS, and leave\nforward propagation in FP16. The minimax quantizer divides the numerical range from the minimum\nto the maximum into equally large quantization bins. The result is shown in Fig. 3(b). While the\nbit-width is higher than 2, our LSS achieves results that are comparable and even slightly higher than\nLUQ. Meanwhile, LSS is more hardware friendly as it requires only INT4 arithmetic.\n5.3\nComputational and Memory Efficiency\nFinally, we demonstrate the potential of our method to accelerate neural network training by evaluating\nour prototypical implementation discussed in Appendix A.6. We emphasize that our implementation\nis not fully optimized. For example, the backward computation requires an INT4 MM in the form of\nY = AB, while cutlass only supports Y = AB\u22a4, so explicit transpose is required. We also do not\nfuse the linear operators with nonlinearities and normalizations. Therefore, the results cannot fully\nreflect the potential of INT4 training algorithms. A fully optimized implementation requires heavy\nengineering, which exceeds the scope of our paper.\nOperator Speed:\nWe compare the throughput of our proposed HQ-MM (HQ), LSS for computing\nweight gradient (LSSWeight), LSS for computing activation gradient (LSSAct), and their average\nthroughput (INT4) with a baseline tensor-core FP16 GEMM implementation (FP16) provided by\ncutlass in Fig. 4 on an Nvidia RTX 3090 GPU which has a peak throughput at 142 FP16 TFLOPs and\n568 INT4 TFLOPs. As the matrix size grows, the overhead of quantization diminishes and our INT4\noperators can be up to 2.2 times faster compared with FP16 MM. We further analyze the quantization\noverhead for each operator in Appendix C.5.\nTraining Throughput:\nWe compare the training throughput of the FP16 PyTorch AMP and our\nINT4 training algorithm for training BERT [24] and GPT [37]-style language models on a system of\n8 Nvidia A100 GPUs. We vary the hidden layer size, intermediate fully-connected layer size, and\nbatch size, and plot the speedup of INT4 training in Fig. 5. Our INT4 training algorithm can achieve\nup to 35.1% speedup for BERT-style models and up to 26.5% speedup for GPT-style models. The\ntraining time can be found in Appendix C.4.\n6\nConclusions\nWe propose a hardware-friendly INT4 training method for transformers. By analyzing the properties\nof MMs in transformers, we propose HQ and LSS methods to quantize activations and gradients\nwhile preserving accuracy. On several important tasks, our method performs comparably or better\nthan existing INT4 methods. Our work can be potentially extended beyond transformers to other\nMM-only architectures, such as MLP-Mixer [49], graph neural networks [25], and recurrent neural\nnetworks [20]. We leave it as a future direction.\nBroader Impacts:\nOur algorithm can improve efficiency and reduce the energy consumption of\ntraining neural networks, which helps reduce the carbon footprint caused by deep learning. However,\nour efficient training algorithm might also facilitate the development of large language models with\nsafety concerns for human beings; and malicious AI applications such as fake content generation.\nLimitations:\nThe main limitation of this work is that it can only accelerate models with a large\nportion of matrix multiplications (linear layers), but can not accelerate convolution layers. Moreover,\nthe proposed method cannot yet work well for those extremely large models such as OPT-175B. To\nthe best of our knowledge, even INT8 training is still an open problem for these large models.\n9\nReferences\n[1] Menachem Adelman and Mark Silberstein. Faster neural network training with approximate\ntensor operations. arXiv preprint arXiv:1805.08079, 2018.\n[2] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and\nIrwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701,\n2020.\n[3] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training\nof neural networks. In Advances in Neural Information Processing Systems, pages 5145\u20135153,\n2018.\n[4] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat\nthe ai: Investigating adversarial human annotation for reading comprehension. Transactions of\nthe Association for Computational Linguistics, 8:662\u2013678, 2020.\n[5] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n[6] Ond\u02c7rej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes\nLeveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the\n2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on\nstatistical machine translation, pages 12\u201358, 2014.\n[7] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical\nframework for low-bitwidth training of deep neural networks. In Advances in neural information\nprocessing systems, 2020.\n[8] Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Logarithmic un-\nbiased quantization: Practical 4-bit training in deep learning. arXiv preprint arXiv:2112.10769,\n2021.\n[9] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi\nSrinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized\nneural networks. arXiv preprint arXiv:1805.06085, 2018.\n[10] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea\nGane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al.\nRethinking attention with performers. In International Conference on Learning Representations,\n2020.\n[11] Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,\nand Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks.\narXiv preprint arXiv:1911.03852, 2019.\n[12] Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Hawq: Hessian\naware quantization of neural networks with mixed-precision. ICCV, 2019.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[14] Petros Drineas and Michael W Mahoney. Randnla: randomized numerical linear algebra.\nCommunications of the ACM, 59(6):80\u201390, 2016.\n[15] Mario Drumond, LIN Tao, Martin Jaggi, and Babak Falsafi. Training dnns with hybrid block\nfloating point. In Advances in Neural Information Processing Systems, pages 453\u2013463, 2018.\n[16] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar-\nmendra S Modha. Learned step size quantization. In International Conference on Learning\nRepresentations, 2019.\n10\n[17] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\nstructured dropout. In International Conference on Learning Representations, 2019.\n[18] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware mini-\nmization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.\n[19] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei\nYu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural\nnetworks. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 4852\u20134861, 2019.\n[20] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[21] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In European conference on computer vision, pages 646\u2013661. Springer, 2016.\n[22] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, Hy-\noukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant\nneural networks using pipeline parallelism. Advances in neural information processing systems,\n32, 2019.\n[23] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for\nefficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2704\u20132713, 2018.\n[24] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages\n4171\u20134186, 2019.\n[25] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907, 2016.\n[26] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In\nInternational Conference on Learning Representations, 2019.\n[27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nTechnical report, 2009.\n[28] Hamed F Langroudi, Zachariah Carmichael, and Dhireesha Kudithipudi. Deep learning training\non the edge with low-precision posits. arXiv preprint arXiv:1907.13216, 2019.\n[29] Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Chee-\ntah: Mixed low-precision hardware & software co-design framework for dnns on the edge.\narXiv preprint arXiv:1908.02386, 2019.\n[30] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and Kwang-Ting Cheng.\nHow do adam and training strategies help bnns optimization. In International Conference on\nMachine Learning, pages 6936\u20136946. PMLR, 2021.\n[31] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards\nprecise binary neural network with generalized activation functions. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV\n16, pages 143\u2013159. Springer, 2020.\n[32] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. In International Conference on Learning Representations, 2018.\n[33] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.\nDeep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics:\nTheory and Experiment, 2021(12):124003, 2021.\n11\n[34] Nvidia. Transformer Engine. https://github.com/NVIDIA/TransformerEngine, 2023.\nOnline; accessed 23 January 2023.\n[35] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pages 311\u2013318, 2002.\n[36] Matt Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018.\n[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[38] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza-\ntions toward training trillion parameter models. In SC20: International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1\u201316. IEEE, 2020.\n[39] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable\nquestions for squad. arXiv preprint arXiv:1806.03822, 2018.\n[40] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[41] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-\nindependent named entity recognition. arXiv preprint cs/0306050, 2003.\n[42] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[43] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert.\narXiv preprint arXiv:1909.05840, 2019.\n[44] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815\u20138821,\n2020.\n[45] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalak-\nshmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit\nfloating point (hfp8) training and inference for deep neural networks. In Advances in Neural\nInformation Processing Systems, pages 4901\u20134910, 2019.\n[46] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath\nVenkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakr-\nishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural\nInformation Processing Systems, volume 33, 2020.\n[47] James Joseph Sylvester. Lx. thoughts on inverse orthogonal matrices, simultaneous signsuc-\ncessions, and tessellated pavements in two or more colours, with applications to newton\u2019s\nrule, ornamental tile-work, and the theory of numbers. The London, Edinburgh, and Dublin\nPhilosophical Magazine and Journal of Science, 34(232):461\u2013475, 1867.\n[48] Hanlin Tang, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. Mkq-bert: Quantized\nbert with 4-bits weights and activations. arXiv preprint arXiv:2203.13483, 2022.\n[49] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas\nUnterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-\nmixer: An all-mlp architecture for vision. Advances in neural information processing systems,\n34:24261\u201324272, 2021.\n[50] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In\nInternational conference on machine learning, pages 10347\u201310357. PMLR, 2021.\n12\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[52] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[53] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan.\nTraining deep neural networks with 8-bit floating point numbers. In Advances in Neural\nInformation Processing Systems, pages 7675\u20137684, 2018.\n[54] Zheng Wang, Juncheng B Li, Shuhui Qu, Florian Metze, and Emma Strubell. Squat: Sharpness-\nand quantization-aware training for bert. arXiv preprint arXiv:2210.07171, 2022.\n[55] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang,\nFengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer\nlanguage models. arXiv preprint arXiv:2209.13325, 2022.\n[56] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in\ndeep neural networks. In International Conference on Learning Representations, 2018.\n[57] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. arXiv preprint\narXiv:2211.10438, 2022.\n[58] Yukuan Yang, Lei Deng, Shuang Wu, Tianyi Yan, Yuan Xie, and Guoqi Li. Training high-\nperformance and large-scale deep neural networks with full 8-bit integers. Neural Networks,\n125:70\u201382, 2020.\n[59] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS\nEdition (EMC2-NIPS), pages 36\u201339. IEEE, 2019.\n[60] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326, 2018.\n[61] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013\n115, 2021.\n[62] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. LQ-Nets: Learned quantiza-\ntion for highly accurate and compact deep neural networks. In The European Conference on\nComputer Vision (ECCV), September 2018.\n[63] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert:\nDistillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.\n[64] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Yu Kang,\nQi Guo, Zidong Du, et al. Adaptive precision training: Quantify back propagation in neural\nnetworks with fixed-point numbers. arXiv preprint arXiv:1911.00361, 2019.\n[65] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural net-\nwork quantization without retraining using outlier channel splitting. In International conference\non machine learning, pages 7543\u20137552. PMLR, 2019.\n[66] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network\nquantization: Towards lossless cnns with low-precision weights. International Conference on\nLearning Representations, 2017.\n[67] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang,\nand Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1969\u20131979,\n2020.\n13\nA\nImplementation Details\nIn this section, we present some works that need to be done to actually accelerate the training process\non hardware.\nA.1\nBMM in Attention\nIn attention, there are batch matrix multiplications (BMMs) that need to be dealt with. We now show\nthat our method for MMs can be extended to BMMs.\nConsider the following BMM product:\nT = BMM(Q, K\u22a4),\nwhere we define T \u2208 RB\u00d7N\u00d7P , Q \u2208 RB\u00d7N\u00d7M, K \u2208 RB\u00d7P \u00d7M. The Hadamard matrix is defined\nas :\n\u02c6H = RepeatB(H) = RepeatB(BlockDiag(Hk, . . . , Hk)),\nwhere \u02c6H \u2208 RB\u00d7M\u00d7M, H \u2208 RM\u00d7M, Hk \u2208 R2k\u00d72k. In this case,\nT \u2248 BMM\n\u0000BMM(Q, \u02c6H), BMM(K, \u02c6H)\u22a4\u0001\n,\nwhich verifies that our HQ can be applied to BMMs.\nFor backward, the gradient of weight and activation can be calculated by the straight-through estimator\n\u230ax\u2309\u2032 = 1 and the chain rule:\n\u2207Q = sQ\n\u0010\nBMM(\u2207\u22a4\nT, \u02c6K) \u25e6 IQ\n\u0011\nH\u22a4,\n\u2207K = sKIK \u25e6 BMM(\u2207T, \u02c6Q)H\u22a4 = sKBMM(IK \u25e6 \u2207T, \u02c6Q)H\u22a4,\nwhere we define sQ \u2208 RB, sk \u2208 RB being the batch step size, \u02c6K = intsK\n\u0010\nBMM(K, \u02c6H)\n\u0011\n,\n\u02c6Q = intsQ\n\u0010\nBMM(Q, \u02c6H)\n\u0011\n, IQ = I(\u2212QN \u2264 Q/sQ \u2264 QP ), and IK = I(\u2212QN \u2264 K/sK \u2264 QP ).\nSimilar to Sec. 4.2, we only focus on BMM(\u2207\u22a4\nT, \u02c6K) and \u2207T, since we do leverage sampling on\nthem.\nFor BMM(\u2207\u22a4\nT, \u02c6K), we define the sample probability pi and sample the \u02dcM in the same way\nas MMs. The matrix can be computed as BMM(BMM(\u2207\u2195\nT\n\u22a4, \u02c6\u02dcH), \u02c6K\n\u2195), where \u02c6\u02dcH is defined as\nCONCAT( \u02dcH1, \u00b7 \u00b7 \u00b7 , \u02dcHB), \u2207\u2195\nT\n\u22a4 and \u02c6K\n\u2195 follows the same definition of Eq. 6and the leverage score\nis cb,i := \u2225\u2207\u2195\nTb,i,:\u2225\u2225K\u2195\nb,i,:\u2225 for 0 \u2264 b \u2264 B, 0 \u2264 i \u2264 2M.\nFor \u2207T, similarly, can be viewed as \u2207T\n=\nBMM(\u02c6I\n\u2195, \u2207\u2195\nT),where we define \u2207\u2195\nY\n=\nCONCAT([s\u2191b\u2207\u2191\nTb; s\u2193b\u2207\u2193\nTb])\n\u2208\nRB\u00d72N\u00d7P , \u02c6I\n\u2195\n=\nCONCAT([I\nI])\n\u2208\nRB\u00d7N\u00d72N,\ns\u2191b, \u2207\u2191\nTb, s\u2193b, \u2207\u2193\nTb\nfollows\nthe\ndefinition\nof\nEq.5.\nSo\nit\ncan\nbe\ncomputed\nas\nBMM(BMM(\u02c6I\n\u2195, \u02c6\u02dcH), \u2207\u2195\nT), where\n\u02c6\u02dcH is defined as CONCAT( \u02dcH1, \u00b7 \u00b7 \u00b7 , \u02dcHB), and the lever-\nage score is cb,i := \u2225\u2207\u2195\nTb,i,:\u2225 for 0 \u2264 b \u2264 B, 0 \u2264 i \u2264 2M, which verifies that our LSS can be\napplied to BMM.\nA.2\nComputing Leverage Score\nIn the previous discussion, we find the optimal sample probability pi that can minimize the variance\nof the gradient. However, it is likely for the proportional pi is larger than one, which is invalid for the\nBernoulli distribution. Accordingly, we propose an algorithm to solve this issue.\nDefine the probability array as\nP = [p0\n1, \u00b7 \u00b7 \u00b7 , p0\n2N],\n2N\nX\ni=1\np0\ni = N,\n14\nwe first clamp the array to p1\ni \u2208 [0, 1]. In this case, P2N\ni=1 p1\ni \u2264 N, so we scale the pi which is smaller\nthan 1 to make sure their sum is again N. However, this will probably introduce some more elements\nlarger than 1, so we cycle through the above operations until all the pi \u2208 [0, 1]. This process will\ncertainly stop, since if after the scaling operation, no element is larger than 1, then we get a valid\ndistribution. Otherwise, the number larger than 1 is reduced by at least one, thus the process will halt\nafter at most O(N) times.\nA.3\nLearning Quantizer Parameters\nIn this section, we discuss the detail of how to calculate the gradient of activation and quantization\nstep size.\nFor gradient of activation, the coefficient ci := \u2225\u2207\u2195\nYi\u2225 is the leverage score for activation gradient,\nand the variance achieves its minimum When pi \u221d ci by the Cauchy Inequality.\nPutting everything together, we propose the following MM procedure to compute activation gradient:\nProcedure LSS-MM\n1. Quantize \u2207Y with BS to obtain \u2207\u2191\nY and \u2207\u2193\nY in INT4.\n2. Compute the leverage score \u2225\u2207\u2195\nYi\u2225 in FP16.\n3. Sample the masks {mi}.\n4. Sample rows of \u2207Y given the masks {mi}.\n5. Compute I \u02dcM\n\u2191\u2207\u2191\nY and I \u02dcM\n\u2193\u2207\u2193\nY by discard some of its rows.\n6. Compute INT4 MMs I \u02dcM\n\u2191\u2207\u2191\nY \u02c6\nW and I \u02dcM\n\u2193\u2207\u2193\nY \u02c6\nW.\n7. Dequantize and sum up the resultant INT32 matrices to obtain the FP16 result \u02c6I\n\u2195\u2207\u2195\nY \u02c6\nW.\nThe two matrix multiplications in Step 5 take about 2NCD INT4 MACs in expectation.\nFor the quantization step sizes. Following the chain rule, we have\n\u2207sW = g(sW )\u2207\u22a4\nY \u02c6X \u25e6 \u03b4W(sW ), \u2207sX = g(sX)\u2207Y \u02c6\nW \u25e6 \u03b4X(sX),\nwhere we define g(sW ) = 1/\np\nQpNW , g(sX) = 1/\np\nQpNX, NW and NX being the number\nof elements of weight and activation, \u03b4X(sX) = intsX (X) \u2212 IX \u25e6 (X/sX), and \u03b4W(sW ) =\nintsW (W) \u2212 IW \u25e6 (W/sW ).\nNotice that for computing \u2207sW and \u2207sX, the most expensive MMs are \u2207\u22a4\nY \u02c6X and \u2207Y \u02c6\nW, which\nare already calculated through Eq. (7) and Eq. (8) during previous calculations, so it does not\nrequire extra computation. The elementwise multiplication with \u03b4X(sX) and \u03b4W(sW ) requires minor\ncomputation.\nA.4\nCold Start Problem\nThere is a cold start problem. When the model is trained from scratch (i.e., from a random initializa-\ntion), distributions of weights and activations can change rapidly in the early stage of optimization.\nIn this case, jointly optimizing the quantization step size and the weights would cause the training to\nbe unstable. As a remedy, we do not learn the step size in the first few iterations, and use a heuristic\nrule to dynamically set the step size for each tensor X to 2mean(X)/\np\nQp in each iteration.\nA.5\nChoose hadamard matrix size\nFor the hadamard matrix, let the hadamard matrix to be H \u2208 RD\u00d7D: H = BlockDiag(Hk, . . . , Hk),\nwhere D is a multiple of 2k. We first define\n\u00afXk = sXintsX (XH) H\u22a4,\n\u00af\nW = sW intsW (WH) H\u22a4,\nwhere \u00afX and \u00af\nW can be viewed as an approximation of X and W. Then, we define the quantization\nerror to be MSE( \u00afX, X) \u00d7 MSE( \u00af\nW, W). We search for the optimal k that can minimize this quan-\ntization error. For fine-tuning tasks, once the hadamard matrix size has been calculated, we fix it\nthrough the training process. For the pre-training task, since the distribution shifts greatly as we train\nthe model, we empirically define a time when we re-initialize the hadamard matrix size and the LSQ\nstep size. Usually, we do this when the first 2 epochs finish.\n15\nA.6\nGPU Implementation\nIn the previous discussion, we get to know HQ-MM and LSS-MM from an algorithm level, nevertheless\nit is not enough to actually implement it on hardware. In this section, we will delve deeper into\nhardware implementation details as well as extra limitations.\nHQ-MM can be divided into 5 parts: Hadamard matrix multiplication, Quantize, Data Pack, INT4\nGEMM, and Dequantize.\nFor the Hadamard matrix multiplication process, since it can be interpreted as a half float matrix\nmultiplication process where the two matrices involved in the operation are input/weight matrix and\nhadamard matrix, respectively, we implement it in Python, because PyTorch MM uses CublassGemm\nand is more efficient then CutlassGemm.\nIn the quantize process, we quantize input/weight into INT4 data respectively, and also preserve a\ncorresponding FP16 version for the LSQ Back Propagation process to use.\nIn the previous discussion, we assume the quantize part of HQ-MM is quantizing the resultant\nmatrices to INT4, however, the smallest representation unit of data is INT8. As a result, we actually\nuse INT8 data type to represent quantized data and pack two adjacent data into one data using\n(data[1] << 4)|(data[0]&15) in the data packing process, which means we use one INT8 data to\nrepresent two adjacent INT4 data. With both input matrices\u2019 data packed in this way, we then use\ncutlass tensor-core INT4 GEMM to do the matrix multiplication.\nFor the GEMM process, we choose Nvidia CutlassGemm because it\u2019s the most efficient open-source\noperator library we can find. We use INT4 Tensor Core Gemm for our implementation and it requires\nthe two input matrices A&B to be RowMajor and ColMajor, respectively. Since the default Pytorch\ntensor is RowMajor, we have to use Transpose+Contiguous operations to make it ColMajor, which is\nvery time-consuming and needs further optimization in the future.\nFinally, we dequantize the INT GEMM result back into FP16 output using a dequantize kernel, which\nis the final output of the forward kernel.\nAs compared, LSS-MM is more complicated, and can be divided into 7 parts: Quantization of higher\nlower 4-bit, Leverage Score Calculating, Sampling, Data Pack, INT4 GEMM, Dequantize, and LSQ\nBack Propagation.\nIn the Quantize process, we fuse the quantize operation of higher 4-bit and lower 4-bit into a single\nkernel for acceleration. In the Leverage Score Calculating process, we use the quantized INT8 data\nto calculate the score and scale up it in the final because integer arithmetic is far more efficient than\nfloat arithmetic.\nIn the sampling process, we sample out rows/columns given the previously calculated leverage score.\nNote that in Section. A.2, we repeat our proposed algorithm for several loops to sample out specific\nelements, which is effective but not efficient. According to experiments, however, we notice that\nsimply selecting elements whose leverage score is bigger than 0 can also work well, even better than\nour proposed algorithm in some cases. So in real quantization implementation, we just sample out\nrows/ columns whose Euclidean norm is bigger than 0 to accelerate our training process.\nPack, Gemm, and Dequantize processes are as similar as before. It\u2019s worth noting that for Int4 Tensor\nCore Gemm, suppose two input matrices have shape M \u00d7 K and K \u00d7 N, K needs to be a multiple\nof 32 so that the Tensor core Gemm address can be aligned. We do not need to consider this in the\nForward Propagation process because the input data shape always satisfies. However, in the Back\nPropagation process, the matrix shape may not meet the requirement after sampling. As a result, we\nneed zero_padding the sampled matrix so that K can be a multiple of 32.\nFinally, we utilize the dequantized data to do the LSQ Back Propagation. We also fuse all operations\ninto a single Cuda kernel for acceleration, and the metric remains.\nBesides the component of HQ-MM and LSS-MM , there is still something that needs to be mentioned.\n1. We omit the Quantization and Leverage Score Calculating process in LSSinput, and use the\nsame value as LSSWeight to accelerate the training process.\n2. For Element-Wise kernel, we set block size as 256, grid size as input.numel()/256. For\nReduction kernels like sum and min/max, we set block size as 32, grid size as RowNum,\n16\nreducing elements in each row to the first 32 elements. We find this setting to be most\nefficient through experiments.\nB\nProofs.\nIn this section, we present the proofs of the leverage score.\nB.1\nProof of Proposition. 4.1\nProposition B.1. (LSS variance for weight gradient)\nVar\n\" 2N\nX\ni=1\nmi\npi\n\u2207\u2195\nY\n\u22a4\n:,iX\u2195\ni\n#\n=\n2N\nX\ni=1\n1 \u2212 pi\npi\n\u2225\u2207\u2195\nYi,:\u22252\u2225X\u2195\ni,:\u22252.\nProof.\nV ar(\u2207W) = V ar\n\u0010 2N\nX\ni=1\n1\npi\n(mi\u2207\u2195\nZ\n\u22a4\n:,iX\u2195\ni )\n\u0011\n= V ar\n\u0010 2N\nX\ni=1\n1\npi\n(\nC\nX\nj=1\nD\nX\nk=1\nmi\u2207\u2195\nZ\n\u22a4\nj,iX\u2195\ni,k)\n\u0011\n=\n2N\nX\ni=1\npi(1 \u2212 pi)\np2\ni\nV ar\n\u0010\n(\nC\nX\nj=1\nD\nX\nk=1\n\u2207\u2195\nZ\n\u22a4\nj,iX\u2195\ni,k)\n\u0011\n=\n2N\nX\ni=1\n1 \u2212 pi\npi\n(\nC\nX\nj=1\nD\nX\nk=1\n\u2207\u2195\nZ\n\u22a4\nj,i\n2\nX\u2195\ni,k\n2).\nSo that\nV ar(\u2207W) =\n2N\nX\ni=1\n( 1\npi\n\u2212 1)(\nC\nX\nj=1\n\u2207\u2195\nZ\n\u22a4\nj,i\n2\n)(\nD\nX\nk=1\nX\u2195\ni,k\n2)\n(9)\n=\n2N\nX\ni=1\n( 1\npi\n\u2212 1)\u2225\u2207\u2195\nZ\n\u22a4\n:,i\u22252\u2225X\u2195\ni,:\u22252,\n(10)\nwhich proves.\nB.2\nProof of Activation Leverage Score in Sec. 4.2\nwe divide the matrix multiplication into the sum of 2N smaller multiplications:\n\u02c6I\n\u2195\u2207\u2195\nY =\n2N\nX\ni=1\n\u02c6I\n\u2195\n:,i\u2207\u2195\nYi =\n2N\nX\ni=1\n\u02c6\u2207Yi,\n(11)\nwhere we define \u02c6\u2207Yi = \u02c6I\n\u2195\n:,i\u2207\u2195\nYi.\nWe assigns each \u2207Yi a probability pi \u2208 [0, 1], i = 1, \u00b7 \u00b7 \u00b7 , 2N, that satisfies P2N\ni=1 pi = N. We\ndefine random masks mi \u223c Bern(pi), and define \u02dcM = diag\n\u0010\nm1\np1 , . . . , m2N\np2N\n\u0011\n, and make an unbiased\nestimation:\n\u02c6I\n\u2195\u2207\u2195\nY \u2248 \u02c6I\n\u2195 \u02dcM\u2207\u2195\nY =\n2N\nX\ni=1\nmi\npi\n\u2207\u2195\nYi.\n17\nDefine M\u2191 to be the top-left N \u00d7 N submatrix of M and M\u2193 to be the bottom-right one, we have\n\u02c6I\n\u2195 \u02dcM\u2207\u2195\nY = s\u2191I \u02dcM\n\u2191\u2207\u2191\nY + s\u2193I \u02dcM\n\u2193\u2207\u2193\nY,\nIn this case, I \u02dcM\n\u2191\u2207\u2191\nY and I \u02dcM\n\u2193\u2207\u2193\nY both only have parts of its rows being non zero, and the rest rows\nare zeros since they are discarded. Then, when we multiply it by \u02c6\nW , there are half of rows being\nzeros in I \u02dcM\n\u2191\u2207\u2191\nY \u02c6\nW and I \u02dcM\n\u2193\u2207\u2193\nY \u02c6\nW. So there\u2019s no need to calculate them, and we successfully cut\noff half of the computation in this case.\nNow focus on the variance that\nProposition B.2. (LSS variance for activation gradient)\nVar\n\" 2N\nX\ni=1\n\u02c6I\n\u2195\n:,i\u2207\u2195\nYi\n#\n=\n2N\nX\ni=1\n1 \u2212 pi\npi\n\u2225\u2207\u2195\nYi\u22252.\nProof.\nV ar(\u2207X) = V ar\n\u0010 2N\nX\ni=1\n1\npi\n(mi\u02c6I\n\u2195\n:,iX\u2195\ni )\n\u0011\n= V ar\n\u0010 2N\nX\ni=1\n1\npi\n(\nC\nX\nj=1\nD\nX\nk=1\nmi\u02c6I\n\u2195\nj,i\u2207\u2195\nYi,k)\n\u0011\n=\n2N\nX\ni=1\npi(1 \u2212 pi)\np2\ni\nV ar\n\u0010\n(\nC\nX\nj=1\nD\nX\nk=1\n\u02c6I\n\u2195\nj,i\u2207\u2195\nYi,k)\n\u0011\n=\n2N\nX\ni=1\n1 \u2212 pi\npi\n\u0000C\nX\nj=1\nD\nX\nk=1\n(\u02c6I\n\u2195\nj,i)2(\u2207\u2195\nYi,k)2\u0001\n=\n2N\nX\ni=1\n( 1\npi\n\u2212 1)\n\u0000C\nX\nj=1\n(\u02c6I\n\u2195\nj,i)2)(\nD\nX\nk=1\n(\u2207\u2195\nYi,k)2\u0001\n=\n2N\nX\ni=1\n( 1\npi\n\u2212 1)\u2225\u02c6I\n\u2195\n:,i\u22252\u2225\u2207\u2195\nYi\u22252\n=\n2N\nX\ni=1\n( 1\npi\n\u2212 1)\u2225\u2207\u2195\nYi\u22252.\nIn this way, the coefficient ci := \u2225\u2207\u2195\nYi\u2225 is the leverage score.\nC\nExperiments.\nIn this section, we present more details for experiments in Sec. 5.\nC.1\nExperiments setup\nFor the GLUE, QA, SWAG, and CONLL tasks, we implement our algorithm based on https:\n//github.com/huggingface/transformers. For the machine translation task, we implement our\nalgorithm based on https://github.com/facebookresearch/fairseq. For the ViT fine-tuning\ntask, we implement our algorithm based on https://github.com/jeonsworld/ViT-pytorch.\nFor the deit pretraining task, we implement our algorithm based on https://github.com/\nfacebookresearch/deit.\nWe employed NVIDIA GeForce RTX 3090 for running most of the experiments, while the NVIDIA\nA40 was utilized to evaluate the performance of BERT-Large and ViT-L. Furthermore, we conducted\nruntime measurements using the NVIDIA T4, 3090, and A100 GPUs.\n18\nTable 2: GLUE results on BERT-base-uncased and BERT-large uncased. FP refers to full precision\ntraining, INT8 refers to INT8 training, LSQ + LUQ refers to learned step size quantization for forward\nand logarithmic unbiased quantization for backward, and HQ + LSS refers to Hadamard quantization\nfor forward and leverage score sampling for backward.\nQUANTIZATION METHODS\nMODEL\nDATASET\nFP\nINT8\nLSQ+LUQ\nHQ+LSS\nBERT-BASE\nCOLA\n56.890.64\n56.150.94\n18.763.58\n52.461.46\nSTSB\n88.140.73\n87.050.38\n84.310.29\n87.770.30\nRTE\n64.801.26\n62.271.26\n56.800.92\n62.451.08\nMRPC\n88.610.66\n86.850.76\n86.230.67\n86.540.83\nSST2\n92.720.06\n92.370.17\n90.370.46\n92.490.29\nQNLI\n91.520.22\n90.920.24\n87.330.48\n90.530.23\nQQP\n91.090.11\n90.570.05\n89.260.03\n89.800.05\nMNLI\n84.520.22\n84.100.08\n81.790.18\n83.590.12\nMNLI-MM\n84.680.20\n84.490.31\n82.220.33\n83.750.28\nBERT-LARGE\nCOLA\n60.330.49\n58.801.52\n0.000.00\n53.461.17\nSTSB\n87.592.39\n86.530.20\n83.080.41\n87.570.78\nRTE\n71.121.80\n63.711.26\n53.060.72\n64.620.78\nMRPC\n91.060.28\n87.571.47\n82.560.59\n87.620.51\nSST2\n93.980.17\n93.750.63\n83.940.69\n93.520.40\nQNLI\n92.260.05\n92.290.29\n63.1813.10\n91.530.38\nQQP\n91.040.63\n90.710.00\n75.6212.44\n90.770.02\nMNLI\n86.710.19\n85.820.08\n33.421.38\n85.860.10\nMNLI-MM\n86.410.35\n85.870.14\n33.541.55\n85.820.07\nC.2\nGLUE results\nIn this section, we present the detailed result of fine-tuning the GLUE dataset on BERT-base-uncased\nand BERT-large-uncased.\nOn BERT-base, on STSB, SST2, QNLI, and QQP, HQ+LSS only has < 0.5% accuracy degradation.\nOn the most challenging tasks CoLA and RTE, our accuracy degradation is much smaller compared\nto LSQ+LUQ. On QQP and MNLI, our method achieves < 1.3% degradation, while LSQ + LUQ\nhas \u2265 1.8% degradation. The trend is that the more difficult the task is, the more significant our\nadvantage over LSQ+LUQ.\nOn BERT-large, the improvement is significant. On CoLA, QNLI, and MNLI, the accuracy im-\nprovement compared with LSQ+LUQ > 30%. On other datasets like SST2 and QQP, the accuracy\nimprovement is > 10%. On RTE the accuracy improvement is > 5%, and on STSB and MRPC the\nimprovement is > 3%.\nWe suspect that for those challenging tasks, there is more information stored in the outliers, which\nresults in a larger gap between our method and LSQ+LUQ.\nC.3\nMore Granular Quantization Methods\nIn this section, in Table 4, we show that the more granular quantization methods, such as per-token\nquantization and per-channel quantization, or smoothing techniques, such as SmoothQuant, do not\nwork under the 4-bit FQT setting. Meanwhile, combining these methods with HQ will not bring\nsignificant improvement.\nWe find that LSQ is beneficial for all of these more granular quantization methods under low-bit\nsettings, which highlights the importance of LSQ. Meanwhile, we also notice that the smoothquant\nwill even harm the result of LSQ when the bit-width is low. Our explanation is that the motivation\nof LSQ is to learn a trade-off between outliers and inliers, while smoothquant aims to sacrifice the\n19\nTable 3: Experiments on GPT2-base and Bert-large. Total time spent for epoch 1-5 are reported.\nTRAINING METHODS\nMODEL\n(HIDDEN_SIZE, INTERMIDIATE_SIZE, BATCH_SIZE)\nFP16\nHQ+LSS\nSPEEDUP\nBERT-LARGE\n(2560, 10240, 2048)\n15.094S\n18.949S\n\u221225.5%\n(4096, 16384, 1280)\n32.016S\n30.594S\n4.4%\n(5120, 20480, 960)\n47.418S\n39.482S\n16.7%\n(7680, 30720, 600)\n95.832S\n67.253S\n29.8%\n(8960, 35840, 480)\n128.441S\n83.388S\n35.1%\n(9600, 38400, 160)\n161.114S\n114.325S\n29.0%\n(12800, 51200, 100)\n326.265S\n255.966S\n21.5%\n(14400, 57600, 96)\n409.291S\n346.354S\n15.3%\nGPT2-BASE\n(2560, 10240, 1536)\n17.253S\n22.037S\n\u221227.7%\n(4096, 16384, 960)\n35.937S\n35.694S\n~\n(5120, 20480, 768)\n52.723S\n46.548S\n11.7%\n(7680, 30720, 260)\n113.855S\n92.548S\n18.7%\n(8960, 35840, 200)\n150.680S\n114.881S\n23.8%\n(9600, 38400, 180)\n172.182S\n126.540S\n26.5%\n(12800, 51200, 112)\n320.757S\n236.433S\n26.3%\nFigure 6: Time proportion for each part in HQ-MM and LSS-MM operator.\nprecision of inliers in order to exactly maintain the information of outliers. When the bitwidth is high,\nthis is not a problem, since there are still enough bits to quantize the inliers. But when the bitwidth is\nlow, such sacrifice will cause severe problems since the inlier information is discarded.\nC.4\nLarge Language Model Operator Speed\nIn this section, we show that our hardware-friendly INT4 training method can really accelerate the\ntraining process on Large Language Models. We run distributed training on a system of 8 A100 cards\nand our implementation uses distributed data parallel training with zero-3, gradient checkpointing,\nand optimizer offloading.\nWe experimented with two architectures: BERT-Large and GPT2-base. We vary the network width\nand batch size to make full utilization of the GPU memory and show the end-to-end performance for\nfine-tuning these models on the SuperGLUE RTE dataset in Table 3.\nC.5\nMore experiments on Operator Speed\nTime proportion\nWe examine the proportion of time for each part of computation in HQ-MM and\nLSS-MM operator in Fig. 6 when the shapes of input matrices vary. In HQ, hadamard means multi-\nplying the input matrix with the Hadamard matrix, pack means packing input data into INT4 data,\ngemm means the matrix multiplication of two INT4 matrices. In LSSWeight, quantize corresponds to\n20\n(4608, 5120, 6144)\n(5120, 6144, 8192)\n(6144, 6144, 9216)\n(7168, 6656, 8704)\n(8192, 7680, 9728)\n(15360, 8704, 10752)\nMatrix size (M,N,K)\n0\n10\n20\n30\n40\n50\n60\n70\nTflops\nFP16\nINT4\nHQ\nLSSWeight\nLSSAct\nFigure 7: Real quantization performance on Nvidia T4.\n(4608, 5120, 6144)\n(5120, 6144, 8192)\n(6144, 6144, 9216)\n(7168, 6656, 8704)\n(8192, 7680, 9728)\n(15360, 8704, 10752)\nMatrix size (M,N,K)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTflops\nFP16\nINT4\nHQ\nLSSWeight\nLSSAct\nFigure 8: Real quantization performance on Nvidia A100.\nTable 4: Comparison of different quantization methods, quantize the activation and weight into the\nsame bit-width from 2 to 8. Per-token refers to quantize activation per-token, while Per-channel refers\nto quantize weight per-channel.\nQuantize Bits\nquantization methods\n2\n3\n4\n5\n6\n7\n8\nPer-tensor\n0\n0\n0\n0\n0\n50.2\n54.6\nPer-token\n0\n0\n0\n0\n31.4\n52.8\n56\nPer-channel\n0\n0\n0\n0\n0\n51.9\n56.7\nsmoothquant\n0\n0\n0\n0\n0\n49.4\n57.7\nPer-token + Per-channel + smoothquant\n0\n0\n0\n0\n40.7\n55.7\n56.7\nLSQ\n0\n9.16\n24.2\n37.3\n39.6\n45.3\n51.4\nPer-token + LSQ\n0\n15.3\n27.8\n31.6\n42.9\n46\n54.4\nPer-channel + LSQ\n0\n8\n23.9\n29.3\n40\n45.5\n50.7\nsmoothquant + LSQ\n0\n0\n0\n0\n49.6\n54.9\n57\nPer-token + Per-channel + smoothquant + LSQ\n0\n0\n0\n0\n28.8\n52.4\n55.2\nHQ\n0\n45.2\n54.6\n54.2\n56.5\n57.4\n58.4\nHQ + Per-token + Per-channel\n0\n48.4\n54.1\n54.9\n55\n56\n56\nHQ + Per-token + Per-channel + smoothquant\n0\n0\n46.6\n54.9\n55.9\n55.8\n56.5\nthe quantization of higher and lower 4-bit, leverage means computing leverage score, sample means\nsample out rows/columns given the leverage score, dequantize is the process of dequantizing INT data\nback into FP16 data, and LSQ is the backpropagation process of LSQ method. In LSSAct, we ignore\nquantize and leverage process, using the same value as LSSWeight for saving time, other processes\nshare the same meaning with LSSWeight. Note that our implementation is not fully optimized, and\noptimizations like operator fusion can further improve the performance.\n21\nOperator Speed on more GPUs\nOn an Nvidia RTX 3090 GPU with a Cuda capability of sm_86.,\nwe show the comparison of FP16 MM, HQ, and LSS operators in Section 5.3 as well as time\nproportion in each operator in Figure. 6. We also adjust our hardware implementation and test its\nperformance on Nvidia T4 GPU and Nvidia A100 GPU, which have Cuda capability of sm_75 and\nsm_80 , respectively. The result is shown in Fig. 7 and Fig. 8.\n22\n"
  },
  {
    "title": "DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation",
    "link": "https://arxiv.org/pdf/2306.12422.pdf",
    "upvote": "11",
    "text": "DreamTime: An Improved Optimization Strategy for\nText-to-3D Content Creation\nYukun Huang1,2* \u2020 , Jianan Wang1* \u2021 , Yukai Shi1, Xianbiao Qi1, Zheng-Jun Zha2, Lei Zhang1\n1International Digital Economy Academy (IDEA)\n2University of Science and Technology of China\nkevinh@mail.ustc.edu.cn, {wangjianan,shiyukai,qixianbiao}@idea.edu.cn,\nzhazj@ustc.edu.cn, leizhang@idea.edu.cn\n\u201can erupting volcano, aerial view\u201d\n\u201ca baby dragon drinking boba\u201d\n\u201ca cake in the shape of a train\u201d\n\u201can ice cream sundae\u201d\n\u201ca blue jay standing on a large basket of rainbow macarons\u201d\n\u201ca cute steampunk elephant\u201d\n\u201ca human skeleton relaxing in a lounge chair\u201d\n\u201ca dalmation wearing a fireman's hat\u201d\n\u201ca peacock on a surfboard\u201d\nFigure 1: Results for text-driven 3D content generation using DreamTime.\nAbstract\nText-to-image diffusion models pre-trained on billions\nof image-text pairs have recently enabled text-to-3D con-\ntent creation by optimizing a randomly initialized Neural\nRadiance Fields (NeRF) with score distillation. However,\nthe resultant 3D models exhibit two limitations: (a) qual-\nity concerns such as saturated color and the Janus prob-\nlem; (b) extremely low diversity comparing to text-guided\nimage synthesis. In this paper, we show that the conflict\nbetween NeRF optimization process and uniform timestep\nsampling in score distillation is the main reason for these\nlimitations. To resolve this conflict, we propose to prioritize\ntimestep sampling with monotonically non-increasing func-\ntions, which aligns NeRF optimization with the sampling\nprocess of diffusion model. Extensive experiments show that\nour simple redesign significantly improves text-to-3D con-\ntent creation with higher quality and diversity.\n*Equal contribution.\n\u2020Work done during an internship at IDEA.\n\u2021Corresponding author.\n1. Introduction\nHumans are situated in a 3D environment. To simulate\nthis experience for entertainment or research, we require a\nsignificant number of 3D object assets to populate virtual\nenvironments like games and robotics simulations. Generat-\ning such 3D content is both expensive and time-consuming,\nnecessitating skilled artists with extensive aesthetic and 3D\nmodeling knowledge. It\u2019s reasonable to inquire whether we\ncan enhance this procedure to make it less arduous and al-\nlow beginners to create 3D content that reflects their own\nexperiences and aesthetic preferences.\nRecent advancements in text-to-image generation [30\u2013\n32] have democratized image creation, enabled by large-\nscale image-text datasets, e.g. Laion5B [33], scraped from\nthe internet. However, 3D data is not as easily accessible,\nmaking 3D generation with 2D supervision very attractive.\nPrevious works [3, 4, 22, 23, 26, 34] have explored super-\nvising 2D renderings of 3D models using adversarial loss,\nbut they are limited to modeling a single domain. Dream\nFields [11] and CLIPmesh [19] replace adversarial super-\nvision with CLIP [28] to discriminate alignment of a 3D\n1\narXiv:2306.12422v1  [cs.CV]  21 Jun 2023\nmodel\u2019s 2D renderings with given text prompt. Since CLIP\nis pre-trained on large-scale image-text pairs, this approach\nis able to create general objects but often in low quality\nwith unrealistic appearance. To harness richer supervision\nother than discriminative guidance from GAN or CLIP, re-\ncent works [15,17,27,40] have utilized pre-trained text-to-\nimage diffusion models as a strong image prior to supervise\n2D renderings of 3D models, with promising showcases for\ntext-to-3D generation. However, challenges remain for cre-\native content creation, as the resultant 3D models are often\nof low diversity with quality concerns such as saturated col-\nors and the Janus (multi-face) problem.\nAs a class of score-based generative models [10,37,38],\ndiffusion models contain a data noising and a data denois-\ning process according to a predefined schedule over fixed\nnumber of timesteps.\nThey model the denoising score\n\u2207x log pdata(x), which is the gradient of the log-density\nfunction with respect to the data on a large number of noise-\nperturbed data distributions. Each timestep (t) corresponds\nto a fixed noise with the score containing coarse-to-fine in-\nformation as t decreases. For image synthesis, the sampling\nprocess respects the discipline of coarse-to-fine content cre-\nation by iteratively refining samples with monotonically\ndecreasing t. However, the recent works leveraging pre-\ntrained data scores for text-to-3D generation [15,27,31,40]\nrandomly sample t during the process of 3D model opti-\nmization, which is counter-intuitive.\nIn this paper, we first investigate what a 3D model learns\nfrom pre-trained text-to-image diffusion models at each\nnoise level. Our key intuition is that pre-trained diffusion\nmodels provide different levels of visual concepts for differ-\nent noise levels. At 3D model initialization, it needs coarse\nhigh-level information for structure formation. Later opti-\nmization steps should instead focus on refining details for\nbetter visual quality. These observations motivate us to pro-\npose time prioritized score distillation sampling (TP-SDS)\nfor text-to-3D generation, which aims to prioritize informa-\ntion from different diffusion timesteps (t) at different stages\nof 3D optimization. More concretely, we propose a non-\nincreasing timestep sampling strategy: at the beginning of\noptimization, we prioritize the sampling of large t for guid-\nance on global structure, and then gracefully decrease t with\ntraining iteration to get more information on visual details.\nTo validate the effectiveness of the proposed TP-SDS, we\nfirst analyze the score distillation process illustrated on 2D\nexamples. We then evaluate TP-SDS against standard SDS\non a wide range of text-to-3D generations in comparison for\nmodel quality and diversity.\nOur main contributions are as follows:\n\u2022 We thoroughly reveal the conflict between text-to-3D\noptimization and uniform timestep sampling of score\ndistillation sampling (SDS) from three perspectives:\nmathematical formulation, gradient visualization and\nfrequency analysis.\n\u2022 To resolve this conflict, we introduce DreamTime,\nan improved optimization strategy for text-to-3D con-\ntent creation. Concretely, we propose to use a non-\nincreasing time sampling strategy instead of uniform\ntime sampling. The introduced strategy is simple but\neffective because it aligns text-to-3D optimization with\nthe sampling process of DDPM [10].\n\u2022 We conduct extensive experiments and show that our\nsimple redesign of the optimization process signif-\nicantly improves text-to-3D generation with higher\nquality and diversity.\n2. Related Work\nText-to-image generation.\nRecently, text-to-image mod-\nels such as GLIDE [24], unCLIP [30], Imagen [32], and Sta-\nble Diffusion [31] have demonstrated an impressive capa-\nbility of generating photorealistic and creative images given\ntextual instructions. The remarkable progress is enabled by\nadvances in modeling such as diffusion models [7, 25, 36],\nas well as large-scale web data curation exceeding billions\nof image-text pairs [5,33,35]. Such datasets have wide cov-\nerage of general objects, likely containing instances with\ngreat variety such as color, texture and camera viewpoints.\nAs a result, text-to-image diffusion models pre-trained on\nthose billions of image-text pairs exhibit remarkable under-\nstanding of general objects, good enough to synthesize them\nwith high quality and diversity. However, generating differ-\nent viewpoints of the same object respecting its structure\nand appearance remains a challenging problem [41].\n3D generation with 2D supervision.\nCompared to easily\navailable images, it is challenging to obtain a large amount\nof training data in the form of 3D assets. To address this\nissue and to enable training of 3D generative models using\nonly unstructured 2D images, previous researches such as\npi-GAN [4], EG3D [3], GRAF [34], and GIRAFFE [26]\nhave explored supervising 2D renderings of 3D models\nthrough adversarial loss against a collection of 2D images.\nWhile these methods have shown great promises, they are\nlimited to modelling a single domain, such as human [13]\nand animal [12] faces, which hampers scalability and cre-\native control for 3D content creation. This paper focuses on\nusing image generative models pre-trained on large-scale\nimage-text pairs for text-to-3D generation.\nText-to-3D generation.\nThe pioneering works of Dream\nFields [11] and CLIPmesh [19] utilize CLIP [28] to opti-\nmize the underlying 3D representation so that its 2D ren-\nderings align well with user-provided text prompt, without\nrequiring expensive 3D training data.\nHowever, this ap-\nproach tends to produce less realistic 3D models because\nCLIP only offers discriminative supervision on high-level\nsemantics. In contrast, recent studies such as DreamFu-\nsion [27] and Magic3D [15] have demonstrated remarkable\ntext-to-3D generation results by employing powerful text-\nto-image diffusion models as a robust 2D prior. We build\nupon this line of work and improve over the design choice\nof 3D model optimization process to enable significantly\nhigher-fidelity and higher-diversity text-to-3D generation.\n3. Method\nWe first review relevant methods, including NeRF [18],\nStable Diffusion [31] and SDS [27] in Section 3.1. Then,\nwe analyze the existing drawbacks of SDS in Section 3.2.\nFinally, to alleviate the problems in SDS, we introduce an\nimproved optimization strategy in Section 3.3.\n3.1. Preliminary\nNeural Radiance Fields (NeRF) [2, 18, 20] has shown\noutstanding performance on novel view synthesis task by\nmodelling a scene with implicit representation parameter-\nized by a trainable MLP. Given the camera position o and\ndirection d, a batch of rays r(k) = o + kd is sampled to\nrender a pixel. The MLP takes r(k) as input and predicts\nthe density \u03c4 and color c. The final rendered color of one\npixel is approximated by the volume rendering integral us-\ning numerical quadrature:\n\u02c6Cc(r) =\nNc\nX\ni=1\n\u2126i(1 \u2212 exp(\u2212\u03c4i\u03b4i))ci,\nwhere Nc is the number of sampled points on a ray, the\naccumulated transmittance \u2126i = exp(\u2212 Pi\u22121\nj=1 \u03c4j\u03b4j), and\n\u03b4i is the distance between adjacent samples.\nDiffusion models [10, 25] estimate the denoising score\n\u2207x log pdata(x) by adding noise to clean data x \u223c p(x) in T\ntimesteps with pre-defined schedule \u03b1t \u2208 (0, 1) and \u00af\u03b1t :=\nQt\ns=1 \u03b1s, according to:\nzt = \u221a\u00af\u03b1tx +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, where \u03f5 \u223c N(0, I),\n(1)\nthen learns to denoise by minimizing the noise prediction\nerror:\nLt = Ex,\u03f5\u223cN (0,I)\nh\n\u2225\u03f5\u03d5 (zt, t) \u2212 \u03f5\u22252\n2\ni\n.\nIn the sampling stage, one can derive x from noisy input and\nnoise prediction, and subsequently the score of data distri-\nbution.\nScore Distillation Sampling (SDS) [15, 17, 27] is a\nwidely used method to distill 2D image priors from a pre-\ntrained diffusion model \u03f5\u03d5 into differentiable 3D represen-\ntations.\nGiven a differentiable generator g and a NeRF\nmodel parameterized by \u03b8, its rendered image x can be ob-\ntained by x = g(\u03b8). Then, SDS calculates the gradients of\nNeRF parameters \u03b8 by:\n\u2207\u03b8LSDS(\u03d5, x) = Et,\u03f5\n\u0014\nw(t)(\u03f5\u03d5(xt; y, t) \u2212 \u03f5)\u2202zt\n\u2202x\n\u2202x\n\u2202\u03b8\n\u0015\n,\nwhere w(t) is a weighting function that depends on the\ntimestep t and y is the text embedding of given prompt.\nSDS optimization is robust to the choice of w(t) as men-\ntioned in [27].\nRemark.\nOur target is to optimize a NeRF model by dis-\ntilling knowledge from pre-trained Stable Diffusion [31]\ngiven a text prompt. In the training process, SDS is used\nto supervise the distillation process.\n3.2. Analysis of Existing Drawbacks in SDS\nA diffusion model generates an image by sequen-\ntially denoising a noisy image, where the denoising sig-\nnal provides different granularity of information at different\ntimestep (t), from structure to details [1, 6]. For diffusion-\nguided 3D content generation, however, SDS [27] samples t\nfrom a uniform distribution throughout the NeRF optimiza-\ntion process, which is counter-intuitive because the nature\nof 3D generation is closer to DDPM sampling (sequential t-\nsampling) than DDPM training (uniform t-sampling). This\nmotivates us to explore the potential impact of uniform t-\nsampling on text-to-3D generation.\nIn this subsection, we analyze the drawbacks of SDS\nfrom three perspectives: mathematical formulation, gradi-\nent visualization and frequency analysis.\nMathematical formulation. We contrast SDS loss:\nLSDS(\u03d5, zt) = E t\u223cU(1,T )\n\u0014\nw(t)\u2225\u03f5\u03d5(zt; y, t) \u2212 \u03f5\u22252\n2\n\u0015\n(2)\nwith DDPM sampling process, i.e., for t = T \u2192 1 :\nzt\u22121 =\n1\n\u221a\u03b1t\n\u0012\nzt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03d5(zt; y, t)\n\u0013\n+ \u03c3t\u03f5,\n(3)\nwhere \u03f5 \u223c N(0, I), \u03b1t is training noise schedule and \u03c3t is\nnoise variance, e.g., \u221a1 \u2212 \u03b1t.\nNote that for SDS training, t is randomly sampled as\nshown in Eq. 2 with red color, but for DDPM sampling, t is\nstrictly ordered for Eq. 3 as highlighted in blue. Since diffu-\nsion model is a general denoiser best utilized by iteratively\ntransforming noisy content to less noisy ones, we argue that\nrandom timestep sampling in the optimization process of\nNeRF is unaligned with the sampling process in DDPM.\nSuch misalignment leads to ineffective and inaccurate su-\npervision from SDS in the training process.\nGradient visualization. For diffusion models, the de-\nnoising prediction provides different granularity of infor-\nmation at different timestep t: from coarse structure to fine\nTimestep \ud835\udc61\nRendered \nImage \ud835\udc65\n100\n500\n900\nTimestep \ud835\udc61\nRendered \nImage \ud835\udc65\nNeRF Optimization Process based on Score Distillation Sampling\n1\n250\n500\n750\n1000\nFigure 2: Visualization of SDS gradients under different timesteps t. (Left) Visualization of SDS gradients throughout the\nNeRF optimization process, where green curved arrow denotes the path for more informative gradient directions as NeRF\noptimization progresses. It can be observed that a non-increasing timestep t is more suitable for the SDS optimization process.\n(Right) We provide more examples to illustrate the effects of timestep t on SDS gradients. Small t provides guidance on local\ndetails, while large t is responsible for global structure.\ndetails as t decreases. To demonstrate this, we visualize the\nupdate gradient \u2225\u03f5\u03d5(zt; y, t) \u2212 \u03f5\u2225 for NeRF renderings in\nFigure 2. From the left visualization it is evident that as\nNeRF optimization progresses, the diffusion timestep that\nis most informative to NeRF update changes (as highlighted\nby curved arrow in Figure 2). We provide more examples\nto reveal the same pattern on the right.\nFrequency analysis.\nFigure 3 illustrates diffusion\nmodel\u2019s out of domain (OOD) issue on NeRF renderings\nwith randomly sampled noise level. Figure 3 (a) illustrates\nimage sampling process: top in pixels and bottom in fre-\nquency after Fourier Transform (FT). To show that ran-\ndomly adding different levels of noise to NeRF rendered\nimages could pose the OOD issue to pre-trained diffusion\nmodel, we provide two extreme examples plausible during\nSDS optimization: (b) adding large noise to a well-trained\nNeRF\u2019s rendering and (c) adding small noise to a NeRF\u2019s\nrendering at initialization. In both cases the resultant images\nexhibit evident frequency difference from diffusion model\u2019s\npre-training: a typical OOD issue hampering effectiveness\nof guidance from pre-trained diffusion models. With time\nprioritized SDS (TP-SDS) we effectively avoid such cases\nand the resultant mismatch in diffusion\u2019s training and infer-\nence frequency as shown in Figure 3 (d-e).\nThe reason for the observed frequency discrepancy is\nthat NeRF rendered images tend to exhibit higher corre-\nlation in pixel values, therefore of lower frequency than\nnatural images. The discrepancy is largest at initialization:\nDDPM sampling is initialized with a random Gaussian dis-\ntribution of high frequency, but NeRF rendered image is of\nextremely low frequency at initialization as shown in Figure\n3 (c). The low frequency bias is persistent even towards the\nend of SDS optimization: other than correlation in pixel val-\nues and blurred object contours, an SDS supervised NeRF\nusually lacks background details as shown in 3 (b). Note\nthat for the image noising process as formulated in Eq. 1, \u00af\u03b1t\nis > 0 even at the largest timestep, meaning that during dif-\nfusion training, information of the original input is present\neven at the largest noise level. As a result, adding large\nnoise to NeRF\u2019s lower-frequency images according to Eq. 1\nfurther dilutes image information, resulting in frequencies\nmore resembling random noise than maximally noised nat-\nural images as shown in Figure 3 (a,b).\nWe further show in Figure 4 with 2D examples that the\nlack of high-frequency signal at early stage of content cre-\nation directly contributes to mode collapse (low-diversity\n3D models given the same text prompt) as observed in [27].\nWe circumvent this loss of diversity with TP-SDS by ex-\nplicitly adding more high-frequency signal (noise) to NeRF\nrenderings at early stage of optimization (bottom).\n3.3. Time Prioritized Optimization\nMonotonically non-increasing t-sampling. Drawbacks\nof uniform t-sampling in vanilla SDS motivates us to inves-\ntigate a more effective t-sampling strategy. Figure 2 shows\nempirically that non-increasing t (marked by curved arrow)\n\u2026 \u2026\n\ud835\udc61=1\n\ud835\udc61=1000\nDomain Bias\n(c) Bad Case in SDS:\nInitial NeRF + Small \ud835\udc95\n\ud835\udc61=1000\n\ud835\udc61=800\n\ud835\udc61=400\n\ud835\udc61=200\n\ud835\udc61=1\n\ud835\udc61=600\nDomain Bias\n(b) Bad Case in SDS:\nWell-trained NeRF + Large \ud835\udc95\n\ud835\udc61=1\n\ud835\udc61=1000\nRendered\nRendered\nAdding \nLarge Noise\nAdding \nSmall Noise\nDomain \nAlignment\nRendered\nNoisy\nDomain \nAlignment\nRendered\nNoisy\n\ud835\udc61=1\n\ud835\udc61=1000\n(e) TP-SDS: Well-trained NeRF + Small \ud835\udc95\n(d) TP-SDS: Initial NeRF + Large \ud835\udc95\n(a) In-domain Amplitude Spectrum of Diffusion Model\u2019s Training Data\nFigure 3: Illustration of OOD issue using web-data pre-trained diffusion model for denoising NeRF rendered images, in\nfrequency domain. We provide two extreme cases to show the frequency domain misalignment: (b) adding large noise to\nwell-trained NeRF\u2019s rendering, (c) adding small noise to NeRF\u2019s rendering at initialization. (d) and (e) illustrates that TP-\nSDS avoids such domain gap by choosing the right noise level according to current optimization iteration.\nSeed=0\nSeed=1\nSeed=2\nSeed=3\nNormal Initial.\nNeRF Initial.\nSeed=0\nSeed=1\nSeed=2\nSeed=3\nSeed=0\nSeed=1\nSeed=2\nSeed=3\nNeRF Initial.\nTP-SDS (non-increasing \ud835\udc95-sampling)\nVanilla SDS (uniform \ud835\udc95-sampling)\nFigure 4: 2D generation results of prompt \u201cgingerbread\nman\u201d using SDS (Top): Gaussian (high-frequency) initial-\nization generates diverse samples while NeRF rendered ini-\ntialization suffers mode collapse. (Bottom) TP-SDS brings\nback diversity by explicitly adding high noise (frequency)\nat early stage of sampling.\nis more informative to the NeRF optimization process.\nBased on this observation, we first try a naive strategy\nthat decreases t linearly with optimization iteration, how-\never it fails with severe artifacts in the final rendered im-\nage, as shown in Figure 5. We observe that decreasing t\nworks well until later optimization stage when small t dom-\ninates. We visualize the SDS gradients (lower-right box\nwithin each rendered image) and notice that at small t, vari-\nance of the gradients are extremely high, which makes con-\nvergence difficult for NeRF. In fact, different denoising t\ncontributes differently [6] to content generation, so it is non-\noptimal to adopt a uniform decreasing t. As a result, we\npropose a Time Prioritized (TP) strategy for SDS to better\nmodulate the timestep (t) decreasing given the functionality\nof their guidance: coarse, content and details.\n\ud835\udc56=10K, \ud835\udc61=1\n\ud835\udc56=6K, \ud835\udc61=400\n\ud835\udc56=7K, \ud835\udc61=300\n\ud835\udc56=8K, \ud835\udc61=200\n\ud835\udc56=9K, \ud835\udc61=100\n\ud835\udc56=5K, \ud835\udc61=500\n\ud835\udc56=4K, \ud835\udc61=600\n\ud835\udc56=3K, \ud835\udc61=700\n\ud835\udc56=1K, \ud835\udc61=900\n\ud835\udc56=2K, \ud835\udc61=800\nFigure 5: Visualization of NeRF optimization with SDS us-\ning a naive t-sampling strategy, where t decreases linearly\nwith the iteration step i. Severe artifacts appear in the final\nrendered image due to large gradients variance with small t.\nTime prioritized SDS. Existing methods [15, 27] use a\nterm w(t) to weight the strength of noise residual from dif-\nferent t, as shown in Eq. 2. However, they did not observe\nnoticeable influence with different choices of w(t) [27].\nIn contrast, we propose to explicitly sample different t at\ndifferent iteration of NeRF optimization, obeying a non-\nincreasing principle for coarse-to-fine generation.\nTo do\nthis in a principled fashion, we start by introducing a prior\nweight term w\u2217(t) to control the decreasing velocity of t,\nwhere a large value of w\u2217(t) corresponds to a flat slope,\nwhile a small one corresponds to a steep decline.\nTo construct w\u2217(t), we adopt a simple and effective\npiece-wise function:\nw\u2217(t) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\ne\u2212(t\u2212m1)2/2s2\n1\nif t > m1\n1.0\nif m2 \u2264 t \u2264 m1\ne\u2212(t\u2212m2)2/2s2\n2\nif t < m2,\n(4)\nwhere {m1, m2, s1, s2} are hyperparameters that control\nhow t is decreased. As shown in Figure 6, the function\nw\u2217(t) consists of three stages: coarse, content and detailed,\nwhich is inspired by the observation that denoising with dif-\nferent noise levels focus on the restoration of different vi-\nsual concepts [6].\nGiven a prior weight function w\u2217(t), we get the non-\nincreasing ti corresponding to an optimization step i by:\nti = arg min\nt\u2217\n\f\f\f\f\f\nT\nX\nt=t\u2217\np(t) \u2212 i/N\n\f\f\f\f\f ,\n(5)\nwhere T is the number of noise levels of a pre-trained dif-\nfusion model, N denotes the number of NeRF optimization\niterations, and the normalized time prior p(t) is obtained by:\np(t) = w\u2217(t)/\nT\nX\nt=1\nw\u2217(t).\n(6)\nThe detailed algorithm is given in Algorithm 1.\nAlgorithm 1: Time Prioritized SDS (TP-SDS).\nInput: A differentiable generator g with initial\nparameters \u03b80 and number of optimiazation\nsteps N, pre-trained diffusion model \u03d5, time\nprior p(t), learning rate lr and text prompt y.\n1 for i = 1, ..., N do\n2\nti = arg min\nt\u2217\n\f\f\fPT\nt=t\u2217 p(t) \u2212 i/N\n\f\f\f;\n3\n\u03b8i = \u03b8i\u22121 \u2212 lr \u2217 \u2207\u03b8LSDS(\u03d5, g(\u03b8i\u22121); y, ti);\n4 end\nOutput: \u03b8N.\n3.4. Implementation Details\nOur method is implemented in PyTorch and can be\ntrained and evaluated on a single NVIDIA 3090 GPU.\nOur code-base mainly refers to the open source Stable-\nDreamFusion [39] and Latent-NeRF [17]. Here we provide\ncomprehensive implementation details.\nDiffusion guidance. Stable Diffusion v1.4 [31] is used\nas guidance to provide strong image priors. The classifier-\nfree guidance scale is set to 100. The weight term w(t)\nTimestep \ud835\udc95\nDetailed\nCoarse\nContent\n\ud835\udc8e\ud835\udfcf\n\ud835\udc8e\ud835\udfd0\n\ud835\udc94\ud835\udfd0\n\ud835\udc94\ud835\udfcf\nTime prior weight \ud835\udc98\u2217(\ud835\udc95)\n(a) Time prior weight function w\u2217(t).\nIteration step \ud835\udc8a\nDetailed\nCoarse\nContent\n\ud835\udc8e\ud835\udfd0\n\ud835\udc8e\ud835\udfcf\nTime step \ud835\udc95\n(b) Timestep t decreases with iteration step i.\nFigure 6: The proposed time prior weights w\u2217(t) to modu-\nlate the decreasing velocity of t for score distillation sam-\npling (SDS). A large value of w\u2217(t) corresponds to a flat\nslope, while a small value corresponds to a steep decline.\nof SDS loss is set to 1.0, and we normalize the SDS gradi-\nents to stabilize the optimization process. The max timestep\nT is 1,000.\nFor the SDS baseline, we sample timestep\nt \u223c U(20, 980) following DreamFusion [27]. For the pro-\nposed TP-SDS, timestep t decrements from 1000 to 1, and\nwe use a prior weight configuration of {m1 = 800, m2 =\n500, s1 = 300, s2 = 100} to control the decreasing veloc-\nity, which is obtained by a small-scale grid search.\nNeRF rendering.\nWe employ Instant-NGP [20] as\nNeRF representation. Our method renders \u201clatent images\u201d\nin the latent space R64\u00d764\u00d74 of Stable Diffusion follow-\ning Latent-NeRF [17], where the rendered latent images\ncan be decoded into RGB images by the VAE decoder of\nStable Diffusion. At each iteration of NeRF optimization,\na camera position is randomly sampled in spherical coordi-\nnates, with elevation angle \u03d5cam \u2208 [0\u25e6, 120\u25e6], azimuth angle\n\u03b8cam \u2208 [0\u25e6, 360\u25e6], and spherical radius Rcam \u2208 [1.0, 1.5]. A\nfocal length multiplier \u03bbfocal \u2208 [0.71, 1.37] is also randomly\nsampled at each iteration to obtain focal length \u03bbfocal \u00d7 H,\nwhere H = 64 is the latent height.\nOptimization. The number of iterations N and the batch\nsize are set to 10,000 and 1 respectively, which takes about\n15 minutes on a single NVIDIA 3090 GPU. We use an\nAdam optimizer [14] with learning rate of 1e-3, betas of\n(0.9, 0.999) without weight decay. The learning rate is con-\nstant throughout the optimization process. A sparsity loss\nas suggested in [17] is also adopted to facilitate 3D shape\ncoherence and to prevent floating \u201cradiance clouds\u201d.\nPrompt augmentation. Unlike existing methods [15,\n27], we found that some prompt prefixes such as \u201ca DSLR\nphoto of...\u201d did not lead to quality improvements in our\nmethod, and therefore did not use any such augmentations.\nIn addition, view-dependent prompt augmentations are crit-\nical for 3D consistency, and we found that the results are\nsensitive to the choice of such augmentations. We adopt the\nfollowing view-dependent prompt augmentations:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u201coverhead view of...\u201d\nif \u03d5cam \u2264 30\u25e6\n\u201cfront view of...\u201d\nif \u03d5cam > 30\u25e6, \u03b8cam \u2208 [0\u25e6, 90\u25e6]\n\u201cbackside view of...\u201d\nif \u03d5cam > 30\u25e6, \u03b8cam \u2208 [180\u25e6, 270\u25e6]\n\u201cside view of...\u201d\notherwise.\n4. Experiment\nWe conduct experiments on generation of 2D points, 2D\nimages, and 3D assets for a comprehensive evaluation of the\nproposed time prioritized score distillation sampling (TP-\nSDS). For 2D experiments, generator g is an identity map-\nping where parameters are point or image representations.\nFor 3D experiments, generator g is a differentiable volume\nrenderer that transforms NeRF parameters into an image.\n4.1. Start with a Toy Example\nWe first demonstrate in Figure 7 with toy example that\ncomparing to SDS, TP-SDS generates more accurate sam-\nples with better mode coverage. The toy distribution is of\n2D (x, y) points with modes clustered as black dots, resem-\nbling a dinosaur as a whole [16]. We train a DDPM on the\npoints distribution till convergence so that we could sam-\nple faithful points with DDPM sampling. In (b) we simu-\nlate properties of random initialization: in high-dimensional\nspace randomly sampled point is likely to be far away from\nknown data distribution [8, 21] (top); and rendered image\nat NeRF initialization exhibit high correlation in pixel val-\nues [29] (bottom). It is evident from (c) that TP-SDS yields\nsamples with higher quality and diversity than SDS.\n(a)\n(b)\n(c)\nInitialization\nSDS\nTP-SDS\nFigure 7: Denoising 2D points with SDS and TP-SDS.\nWe train DDPM on Dino points dataset (black). (a) Sim-\nulated sampling initialization: a randomly sampled point\nis likely to be far away from real data distribution in high-\ndimensional space (top), NeRF yields highly correlated ren-\nderings at initialization (bottom). We optimize 2000 sam-\nples with SDS (b) and TP-SDS (c). It is evident that TP-\nSDS yields more accurate samples (higher quality) with\nbroader mode coverage (higher diversity).\n4.2. Visual Quality Evaluation\nIn this subsection, we demonstrate the effectiveness of\nthe proposed TP-SDS in improving visual quality. We argue\nthat some challenging problems in text-to-3D generation,\nsuch as unrealistic appearance, multiple faces, and failure\nto capture text semantics, can be effectively alleviated by\nsimply changing the sampling strategy of timestep t.\nUser studies. We conduct user studies to evaluate effec-\ntiveness of TP-SDS based on user preferences. We show\nusers two videos side by side using the same text prompt\nand ask the users to select the one that they consider to be of\nhigher quality: more realistic, more detailed and align better\nwith given prompt. Each prompt is evaluated by 5 different\nusers, resulting in 6,225 pairwise comparisons in total. As\nshown in Table 1, the raters favor 3D models generated with\nTP-SD over Latent-NeRF (75.8%), SJC (66.6%) and SDS\nbaseline (80.2%).\nTable 1: User study. We measure preference for 3D mod-\nels generated using 415 prompts released by DreamFu-\nsion [27]. 3D models generated with TP-SDS are favoured\nby significantly more raters over Latent-NeRF, SJC and\nSDS baseline by 75.8%, 66.6% and 80.2% respectively.\nComparison\nPreference (%)\nOurs vs. Latent-NeRF [17]\n75.8%\nOurs vs. SJC [40]\n66.6%\nOurs vs. SDS (Baseline)\n80.2%\nR-Precision. We follow DreamFusion [27] to assess the\nconsistency of rendered images with input text prompt by\nCLIP R-Precision. R-Precision measures how accurately\nFront View\nBack View\nSide View\nTP-SDS\n(Ours)\nSJC\nLatent\nNeRF\n\u201c a chimpanzee dressed like Henry VIII king of England \u201d\nSDS\n(Baseline)\n\u201cTrump figure\u201d\nZoomed-in\n(a) Avoiding Semantic Misalignment and Multiple Faces.\n(b) Better Shapes, Colors and Details.\nRendered Images\nFigure 8: Qualitative comparisons with Latent-NeRF [17] SJC [40], and Baseline [39]. All methods use Stable Diffusion\nv1.4 for fair comparison. (a) The prompt is \u201ca chimpanzee dressed like Henry VIII king of England\u201d. Our results do not have\nthe Janus (multi-face) problem (marked with circles) and align better with the given text (marked with boxes). (b) The text\nprompt is \u201cTrump figure\u201d. The proposed TP-SDS is able to produce more realistic shape and appearance details.\nTable 2: Evaluating generations with TP-SDS for alignment\nof their renderings with corresponding captions, using dif-\nferent CLIP models. We compare to ground-truth images,\nLatent-NeRF [17], SJC [40] and SDS baseline evaluated on\nobject-centric COCO as in [27].\nMethod\nR-Precision (%) \u2191\nCLIP B/32\nCLIP B/16\nCLIP L/14\nGT Images\n77.1\n79.1\n\u2013\nLatent-NeRF\n48.4\n52.9\n59.5\nSJC\n55.6\n58.2\n66.0\nSDS (Baseline)\n58.8\n62.7\n63.4\nTP-SDS (Ours)\n63.4\n67.3\n71.2\nCLIP [28] retrieves the correct caption from a group of dis-\ntractor prompts when presented with a rendered scene. We\nuse the 153 prompts from the object-centric COCO vali-\ndation subset of Dream Fields [11]. We report in Table 2\nthat for models utilizing open-sourced Stable Diffusion, our\nproposed TP-SDS attains the highest R-Precision score.\nQualitative comparisons. We compare our method with\nexisting text-to-3D generation methods utilizing publicly-\naccessible Stable Diffusion (v1.4) [31], including Latent-\nNeRF [17], SJC [40] and SDS Baseline [39].\n\u2022 Figure 8 (a) shows that our generation results do not\nhave the multi-face problem and align better with\ngiven text semantics. For example, competing meth-\nods fail to generate the king\u2019s attire described by the\ntext prompt, while our method can. Figure 8 (b) shows\nthat our method is able to produce realistic appear-\nances, avoiding distorted shapes, colors and blurry de-\ntails. For example, more reasonable body proportions\nand colors of human figures are generated.\n\u2022 It has been well-observed that DreamFusion [27] tends\nto generate 3D models with unrealistic appearance ex-\nhibiting saturated colors. Figure 9 shows that in gen-\neral, DreamTime\u2019s generations are of more natural-\nlooking colors. Our intuition is that decaying timesteps\navoids fluctuating and possibly conflicting gradient up-\ndates, especially towards completion of the optimiza-\ntion, therefore alleviates extreme colors.\n\u2022 The proposed TP-SDS is orthogonal to the choice\nof NeRF, diffusion model and implementation de-\ntails. For confirmation, we further apply the proposed\nmethod to recently released DreamFusion implemen-\ntation threestudio [9]. Figure 10 shows that comparing\nto the baseline, our method could produce significantly\nbetter 3D generations, even superior to DreamFusion.\nDreamFusion\nDreamTime (Ours)\nFigure 9: DreamTime produces more natural-looking colors\ncompared to DreamFusion\u2019s saturated colors.\n(a) \u201ca blue motorcycle\u201d\n(b) \u201ca ceramic lion\u201d\n(c) \u201ca gummy bear playing the saxophone\u201d\n(d) \u201ca robot made out of vegetables\u201d\n(e) \u201ca mouse holding a candlestick\u201d\n(f) \u201ca red dragon playing chess\u201d\nFigure 10: Qualitative comparisons with DreamFusion [27]\n(left), threestudio baseline [9] (middle) and ours (right).\nOur method is able to bring significant quality improvement\nover the strong threestudio baseline, with 3D generations\neven superior to DreamFusion.\n4.3. Ablation Study\nAlthough the relationship between ti and i in TP-SDS is\nmostly linear, we encourage fewer large-step sampling for\nstable structure formation, and fewer small-step sampling\nto avoid color distortion. Figure 5 shows that a naive lin-\nearly decreasing timestep sampling is prone to artifacts and\ncolor distortion. In Figure 11, we further evaluate on more\ntruncated linear schedules showing that our timestep sched-\nule produces 3D generations with more realistic shape and\nappearance, effectively avoiding artifacts and color shifts.\n4.4. Faster Convergence\nBy rendering images in the latent space of Stable Diffu-\nsion, our method enables a fast sampling of 15 minutes per\nprompt on a single 3090 GPU, about three times faster than\nDreamFusion [27] and Magic3D [15]. Moreover, we em-\npirically find that the proposed non-increasing t-sampling\nstrategy leads to a faster convergence, requiring \u223c 35%\nfewer optimization steps than the uniform t-sampling. This\nis likely due to more efficient utilization of information, e.g.\nTP-SDS\n1000 \u2192 100\n1000 \u2192 200\n1000 \u2192 300\n1000 \u2192 400\n1000 \u2192 500\n\u201ca hotdog in a tutu skirt\u201d\n\u201ca panda rowing a boat in a pond\u201d\nTruncated Linear Schedules\n(Ours)\nFigure 11: Qualitative comparisons of the truncated linear\nschedules and our proposed TP-SDS.\nit is wasteful to seek structure information at later stage of\noptimization when the 3D model is already in good shape.\nTo demonstrate the fast convergence of TP-SDS, we con-\nduct experiments on 2D image generation with 153 text\nprompts from the object-centric COCO validation set, ob-\ntaining quantitative and qualitative results.\nQuantitative evaluation. We show in Figure 12 the R-\nPrecision scores at different optimization iterations using\nthe vanilla SDS and TP-SDS. The growth rate of TP-SDS\ncurves is consistently higher across various CLIP models,\nwhich implies a faster convergence requiring significantly\nfewer optimization steps to reach the same R-Precision\nscore. This leads to the production of superior text-aligned\ngenerations at a quicker pace with fewer resources.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nIteration Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nR-Precision\nTP-SDS,B/32\nTP-SDS,B/16\nTP-SDS,L/14\nSDS,B/32\nSDS,B/16\nSDS,L/14\nFigure 12: R-Precision curves of the SDS baseline and\nthe proposed TP-SDS, using three CLIP models: B/32,\nB/16, and L/14. Given 153 text prompts from the object-\ncentric COCO validation set, we employ SDS and TP-\nSDS to generate corresponding 2D images for evaluation\nof R-Precision. The R-Precision curves for TP-SDS have a\nsteeper growth rate compared to those of SDS, signifying\nfaster convergence of our method.\n\u201ca bulldog is wearing a black pirate hat\u201d\n5000\n500\n4500\n1000\n2000\n1500\n2500\n3000\n3500\n4000\n\u201can old and rusted clock is mounted on a brick wall\u201d\n5000\n500\n4500\n1000\n2000\n1500\n2500\n3000\n3500\n4000\n\u201ca bus covered with assorted colorful graffiti on the side of it\u201d\n5000\n500\n4500\n1000\n2000\n1500\n2500\n3000\n3500\n4000\n\u201cwhite surfboard leaning against a brown tiki wall\u201d\n5000\n500\n4500\n1000\n2000\n1500\n2500\n3000\n3500\n4000\n\u201can orange bike leaning on a pole in the snow\u201d\n5000\n500\n4500\n1000\n2000\n1500\n2500\n3000\n3500\n4000\n\u201ca sheep looking through the slats of a wired fence\u201d\n5000\n500\n4500\n1000\n2000\n1500\n2500\n3000\n3500\n4000\nFigure 13: Qualitative comparisons of the SDS baseline (first row in each example) and the proposed TP-SDS (second row\nin each example) under different iteration steps (from 500 to 5000). The proposed TP-SDS leads to faster content generation\nthan the SDS baseline.\nQualitative comparison. Figure 13 shows the 2D image\ngeneration process at different iterations using the vanilla\nSDS and our TP-SDS. It is clear that with TP-SDS, the\nemergence of content (e.g., object structures) is faster with\nbetter appearance and details.\n4.5. Diversity Evaluation\nOne of the key ingredients for creative content creation\nwith AI is diversity: given a text prompt, Stable Diffusion is\nable to generate countless number of diverse samples while\nrespecting the given text.\nHowever, pioneering work of\nDreamFusion [27] has already observed the mode collapse\nproblem for text-to-3D generation: one text prompt would\nalways yield highly similar 3D models. In Figure 4, we\ndemonstrate with 2D generation results that mode collapse\nis largely caused by the low-frequency nature of NeRF ini-\ntialization, and the proposed TP-SDS is able to circumvent\nit by adding large noise (i.e., using a large timestep t) early\nin training. Figure 14 further demonstrates that 3D samples\ngenerated with TP-SDS are much more diverse in appear-\nance than that with Latent-NeRF [17] and SJC [40].\n4.6. Hyper-Parameter Analysis\nThe proposed TP-SDS improves generation quality, effi-\nciency, and diversity compared to the SDS baseline. How-\never, the adopted prior weight function w\u2217(t) parameterized\nby {m1, m2, s1, s2} introduces extra hyper-parameters. We\nexplore the influence of these hyper-parameters on text-to-\n3D generation, which can serve as a guide for tuning in\npractice. Specifically, we explore the impact of different\nhyper-parameter settings on the generated results in a wide\nrange of search spaces, as shown in Figure 15. s1 controls\nthe coarse stage, a small value of s1 results in fewer steps\nof large t sampling, which is likely to hinder formation of\n3D structure. s2 controls the detailed stage, a large value of\ns2 results in more steps of small t sampling, likely to cause\ncolor shifts and artifacts. m1 and m2 affects s1 and s2 si-\nmultaneously, and a general rule of thumb is to make the\naverage of m1 and m2 close to 0.5T, while the difference\nbetween m1 and m2 is within [0, 0.4T], where T is the max\ndiffusion timestep.\n5. Conclusion\nWe propose DreamTime, an improved optimization\nstrategy for text-to-3D content generation. We thoroughly\ninvestigate how the 3D formation process harnesses super-\nvision from pre-trained text-to-image diffusion models at\ndifferent noise levels and analyze the drawbacks of com-\nmonly used score distillation sampling (SDS). We then pro-\npose a non-increasing time sampling strategy (TP-SDS)\nwhich effectively aligns the training process of NeRF and\nthe sampling process of DDPM. With extensive user stud-\nies, qualitative comparisons and quantitative evaluations we\nshow that TP-SDS significantly improves the quality and\ndiversity of text-to-3D generation, and considerably more\npreferable compared to accessible 3D generators Latent-\nNeRF [17] (75.8%) and SJC [40] (66.6%). We hope that\nwith DreamTime, 3D content creation can be more accessi-\nble for creativity and aesthetics expression.\nTP-SDS (Ours)\nLatent-NeRF\nSJC\n\u201ca car\u201d\n\u201cice cream\u201d\nFigure 14: Diversity comparisons with Latent-NeRF [17] and SJC [40]. The text prompts are \u201cice cream\u201d and \u201ca car\u201d with\nno additional description. Given different random seeds, our method is able to generate objects with diverse appearance,\nwhile competing methods suffer mode collapse, repetitively generating similar-looking results. Backgrounds are removed\nfor readers to concentrate on object diversity.\n\ud835\udc602 \u2193\n\ud835\udc602 \u2191\n\ud835\udc601 \u2193\n\ud835\udc601 \u2191\n\u2206\ud835\udc5a\u2193\n\u2206\ud835\udc5a\u2191\n\u0d25\ud835\udc5a \u2193\n\u0d25\ud835\udc5a \u2191\nFigure 15: Influence of the time prior configuration {m1, m2, s1, s2} on generated results. The text prompt is \u201ca vase\nwith pink flowers\u201d. We define \u00afm = (m1 + m2)/2 and \u2206m = m1 \u2212 m2, then perform analysis in the search spaces of\ns1 \u2208 [10, 800], s2 \u2208 [5, 150], \u00afm \u2208 [100, 900] and \u2206m \u2208 [0, 800]. s1 controls the coarse stage, a small value of s1 results\nin fewer steps of large t sampling, which is likely to hinder formation of 3D structure (marked in \u201c\n\u201d). s2 controls the\ndetailed stage, a large value of s2 results in more steps of small t sampling, likely to cause color shifts and artifacts (marked\nin \u201c\n\u201d). The adjustment of \u00afm or \u2206m affects s1 and s2 simultaneously, a general rule of thumb is to place \u00afm close to T\n2\n(marked in \u201c\n\u201d) while \u2206m can be freely chosen within [0, 2T\n5 ] (marked in \u201c\n\u201d), where T is the max diffusion timestep.\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. eDiff-I: Text-to-Image\nDiffusion Models with an Ensemble of Expert Denoisers.\narXiv preprint arXiv:2211.01324, 2022.\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-NeRF: A Multiscale Representation for Anti-Aliasing\nNeural Radiance Fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5855\u2013\n5864, 2021.\n[3] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\nGeometry-Aware 3D Generative Adversarial Networks. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 16123\u201316133, 2022.\n[4] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. Pi-GAN: Periodic Implicit Gener-\native Adversarial Networks for 3D-Aware Image Synthesis.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5799\u20135809, 2021.\n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12M: Pushing Web-Scale Image-Text\nPre-Training To Recognize Long-Tail Visual Concepts. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3558\u20133568, 2021.\n[6] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon\nKim, Hyunwoo Kim, and Sungroh Yoon. Perception Pri-\noritized Training of Diffusion Models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11472\u201311481, 2022.\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion Models\nBeat GANs on Image Synthesis. Advances in Neural Infor-\nmation Processing Systems, 34:8780\u20138794, 2021.\n[8] David L Donoho et al. High-Dimensional Data Analysis:\nThe Curses and Blessings of Dimensionality.\nAMS Math\nChallenges Lecture, 1(2000):32, 2000.\n[9] Yuan-Chen Guo,\nYing-Tian Liu,\nChen Wang,\nZi-Xin\nZou,\nGuan Luo,\nChia-Hao Chen,\nYan-Pei Cao,\nand\nSong-Hai Zhang.\nthreestudio:\nA unified framework\nfor 3d content generation.\nhttps://github.com/\nthreestudio-project/threestudio, 2023.\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffu-\nsion Probabilistic Models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020.\n[11] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-Shot Text-Guided Object Gen-\neration With Dream Fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 867\u2013876, 2022.\n[12] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila. Training Generative Ad-\nversarial Networks with Limited Data. Advances in Neural\nInformation Processing Systems, 33:12104\u201312114, 2020.\n[13] Tero Karras, Samuli Laine, and Timo Aila.\nA Style-\nBased Generator Architecture for Generative Adversarial\nNetworks.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4401\u2013\n4410, 2019.\n[14] Diederik P Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization.\nIn International Conference on\nLearning Representations, 2015.\n[15] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-\ndler, Ming-Yu Liu, and Tsung-Yi Lin.\nMagic3D: High-\nResolution Text-to-3D Content Creation.\narXiv preprint\narXiv:2211.10440, 2022.\n[16] Justin Matejka and George Fitzmaurice. Same Stats, Dif-\nferent Graphs: Generating Datasets with Varied Appearance\nand Identical Statistics through Simulated Annealing.\nIn\nProceedings of the 2017 CHI Conference on Human Factors\nin Computing Systems, pages 1290\u20131294, 2017.\n[17] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes,\nand Daniel Cohen-Or.\nLatent-NeRF for Shape-Guided\nGeneration of 3D Shapes and Textures.\narXiv preprint\narXiv:2211.07600, 2022.\n[18] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting Scenes as Neural Radiance Fields for View\nSynthesis.\nCommunications of the ACM, 65(1):99\u2013106,\n2021.\n[19] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. CLIP-Mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 Conference Papers, pages 1\u20138, 2022.\n[20] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant Neural Graphics Primitives with a Mul-\ntiresolution Hash Encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022.\n[21] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan\nGorur, and Balaji Lakshminarayanan. Do Deep Generative\nModels Know What They Don\u2019t Know?\nIn International\nConference on Learning Representations, 2019.\n[22] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang. HoloGAN: Unsupervised\nLearning of 3D Representations From Natural Images. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7588\u20137597, 2019.\n[23] Thu H Nguyen-Phuoc, Christian Richardt, Long Mai,\nYongliang Yang, and Niloy Mitra. BlockGAN: Learning 3D\nObject-aware Scene Representations from Unlabelled Im-\nages. Advances in Neural Information Processing Systems,\n33:6767\u20136778, 2020.\n[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards Photorealistic Image Genera-\ntion and Editing with Text-Guided Diffusion Models. arXiv\npreprint arXiv:2112.10741, 2021.\n[25] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\nDenoising Diffusion Probabilistic Models. In International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021.\n[26] Michael Niemeyer and Andreas Geiger.\nGIRAFFE: Rep-\nresenting Scenes As Compositional Generative Neural Fea-\nture Fields.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11453\u2013\n11464, 2021.\n[27] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D Diffusion. arXiv\npreprint arXiv:2209.14988, 2022.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\nTransferable Visual Models From Natural Language Super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021.\n[29] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix\nDraxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and\nAaron Courville. On the Spectral Bias of Neural Networks.\nIn International Conference on Machine Learning, pages\n5301\u20135310. PMLR, 2019.\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical Text-Conditional Image Gen-\neration with CLIP Latents. arXiv preprint arXiv:2204.06125,\n2022.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022.\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic Text-to-Image\nDiffusion Models with Deep Language Understanding.\narXiv preprint arXiv:2205.11487, 2022.\n[33] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLAION-5B: An open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[34] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. GRAF: Generative Radiance Fields for 3D-Aware\nImage Synthesis. Advances in Neural Information Process-\ning Systems, 33:20154\u201320166, 2020.\n[35] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut.\nConceptual Captions: A Cleaned, Hypernymed,\nImage Alt-text Dataset For Automatic Image Captioning.\nIn Proceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\npages 2556\u20132565, 2018.\n[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning Diffusion Implicit Models. In International Conference\non Learning Representations, 2021.\n[37] Yang Song and Stefano Ermon.\nGenerative Modeling by\nEstimating Gradients of the Data Distribution. Advances in\nNeural Information Processing Systems, 32, 2019.\n[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based\nGenerative Modeling through Stochastic Differential Equa-\ntions. In International Conference on Learning Representa-\ntions, 2021.\n[39] Jiaxiang Tang.\nStable-dreamfusion:\nText-to-3d with\nstable-diffusion, 2022. https://github.com/ashawkey/stable-\ndreamfusion.\n[40] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score Jacobian Chaining: Lifting\nPretrained 2D Diffusion Models for 3D Generation. arXiv\npreprint arXiv:2212.00774, 2022.\n[41] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi.\nNovel View Synthesis with Diffusion Models.\narXiv preprint arXiv:2210.04628, 2022.\n"
  },
  {
    "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
    "link": "https://arxiv.org/pdf/2306.11932.pdf",
    "upvote": "6",
    "text": "Opportunities and Risks of LLMs for Scalable Deliberation\nwith Polis\nChristopher T. Small\u22171, Ivan Vendrov2, Esin Durmus2, Hadjar Homaei1,\nElizabeth Barry1, Julien Cornebise1, Ted Suzman1, Deep Ganguli2, and Colin\nMegill1\n1The Computational Democracy Project\n2Anthropic\nJune 2023\nAbstract\nPolis is a platform that leverages machine intelligence to scale up delibera-\ntive processes.\nIn this paper, we explore the opportunities and risks associated\nwith applying Large Language Models (LLMs) towards challenges with facilitating,\nmoderating and summarizing the results of Polis engagements. In particular, we\ndemonstrate with pilot experiments using Anthropic\u2019s Claude that LLMs can indeed\naugment human intelligence to help more efficiently run Polis conversations. In par-\nticular, we find that summarization capabilities enable categorically new methods\nwith immense promise to empower the public in collective meaning-making exer-\ncises. And notably, LLM context limitations have a significant impact on insight\nand quality of these results.\nHowever, these opportunities come with risks. We discuss some of these risks,\nas well as principles and techniques for characterizing and mitigating them, and\nthe implications for other deliberative or political systems that may employ LLMs.\nFinally, we conclude with several open future research directions for augmenting\ntools like Polis with LLMs.\n\u2217Corresponding author: chris@compdemocracy.org\n1\narXiv:2306.11932v1  [cs.SI]  20 Jun 2023\nContents\n1\nIntroduction\n3\n1.1\nApplying Machine Intelligence to Deliberation . . . . . . . . . . . . . . .\n3\n1.2\nChallenges with Polis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.3\nAddressing Polis Challenges with LLMs . . . . . . . . . . . . . . . . . .\n6\n1.4\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2\nOpportunities and Risks\n9\n2.1\nTopic Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.1.1\nExperimental Results\n. . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.1.2\nRisks and Mitigation . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.1.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.2\nSummarization and Reporting . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.2.1\nExperimental results . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.2\nRisks and Mitigation . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.2.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3\nFacilitating Conversations by Synthesizing Group Identity and Consensus\n14\n2.3.1\nExperimental Results\n. . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.3.2\nRisks and Mitigation . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.3.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.4\nVote Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.4.1\nExperimental Results\n. . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.4.2\nRisks and Mitigation . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.5\nLeveraging Large Context Windows\n. . . . . . . . . . . . . . . . . . . .\n19\n2.5.1\nExperimental Results\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.5.2\nRisks and Mitigation . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.5.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3\nDiscussion\n22\n3.1\nIntelligence Augmentation, Not Human Replacement . . . . . . . . . . .\n23\n3.2\nMeasuring Ideological Bias . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3\nRethinking Moderation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.3.1\nA Moderate Approach to LLM-assisted Moderation\n. . . . . . .\n25\n3.4\nFuture Questions and Directions\n. . . . . . . . . . . . . . . . . . . . . .\n26\n3.4.1\nSeeding Conversations . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.4.2\nDimension Reduction and Clustering . . . . . . . . . . . . . . . .\n26\n3.4.3\nImproved Comment Routing\n. . . . . . . . . . . . . . . . . . . .\n27\n3.4.4\nConversation Simulation . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.4.5\nReframing Comments for Specific Opinion Groups . . . . . . . .\n29\n3.4.6\nAuthor Assistance . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.4.7\nUniversal Translation Layer . . . . . . . . . . . . . . . . . . . . .\n30\n3.5\nKey Techniques in Application of LLMs . . . . . . . . . . . . . . . . . .\n30\n3.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2\nReferences\n31\nA Appendix\n40\nA.1 Topic modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nA.2 Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nA.3 Group Identity and Consensus\n. . . . . . . . . . . . . . . . . . . . . . .\n42\nA.4 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nA.5 Measuring bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nA.6 Evaluating Summaries with LLMs\n. . . . . . . . . . . . . . . . . . . . .\n44\nA.7 Long Context Window Experiments\n. . . . . . . . . . . . . . . . . . . .\n45\n1\nIntroduction\n1.1\nApplying Machine Intelligence to Deliberation\nPolis is a powerful platform for collective self-representation, enabling the construction\nof an interactive model of the public opinion landscape (Small et al., 2021). Since initial\ndevelopment began in 2012, it has been used to find common ground in thousands\nof conversations tied to policy making processes across dozens of countries and five\ncontinents, and been reported on widely in outlets like the New York Times (Coy, 2023),\nBBC (Miller and Anubi), and The Atlantic (Applebaum and Pomerantsev, 2021). It\nhas been adopted nationally by Taiwan, Singapore and Finland, as well as by numerous\nmunicipalities. Its methodology was also built on as a basis for Twitter\u2019s Community\nNotes (Wojcik et al., 2022).\nThe goal of Polis is to enable open and constructive dialogue between people with di-\nverse viewpoints. Using a combination of machine learning and human interaction, Polis\nhelps groups understand each other and identify points of common ground. Polis can\nbe used for a variety of purposes, including group decision-making, public engagement,\nand public opinion research.\nPolis works by allowing participants to submit comments in response to an open\nended prompt, and votes in response to other people\u2019s comments. Figure 1 shows the\nparticipation interface. By arranging the votes in a vote matrix and applying dimension\nreduction and clustering techniques\u2014specifically Principle Components Analysis (PCA)\n(Pearson, 1901) and K-means clustering (MacQueen, 1967)\u2014Polis is able to learn a 2-\ndimensional opinion space and opinion groups, which are used as the basis for syntheses\nof the deliberations. Specifically, its possible to surface comments which best distinguish\nopinion groups, as well as points of consensus between groups (group-informed consen-\nsus). This manifests in a real-time visualization within the participation interface, as\nwell as in an automated report for deeper analysis. Figure 2 shows the full workflow of\na Polis conversation.\nTo date, Polis as a system for large scale deliberation has not leveraged Natural\nLanguage Processing (NLP) techniques. Circa 2012, when Polis was developed, NLP\ntechniques were considered not robust enough for capturing nuance in short text docu-\nments. Building the platform on comparatively simple and well-trod machine learning\nmethods meant that results were more robust, as well as easier to interpret and explain.\n3\nFigure 1: Polis participation interface. The topic of the conversation is at the top. A box displays\nanother participant\u2019s statement, along with vote options: Agree, Disagree, and Pass/Unsure.\nThe participant has the option to add a new statement of their own, on which other participants\nwill then vote. Below is a visualization of the interactive opinion landscape as summarized by\n2D-PCA and K-means. It shows the two opinion groups that emerged, their relative size, and\nsome example statements. The user can also see some key statements and how representative\nthey are of each group.\n4\nNevertheless, we continued to monitor advances in NLP, and recognized a critical thresh-\nold in capacity had been crossed with the advent of Large Language Models (LLMs),\ne.g. Brown et al. (2020). We now believe that they have significant potential to aug-\nment human intelligence in deliberative settings, but recognize that this comes with\nrisks, and find it imperative that systems be designed to account for and mitigate these\nrisks.\n1.2\nChallenges with Polis\nPolis has a proven track record of hosting open and constructive dialogue between\npeople with diverse viewpoints (see e.g. Horton, 2018; CPI, 2019). However, the scale of\ndata produced, upwards of millions of votes per conversation, comes with the challenge\nof synthesizing the results to inform policy makers, researchers, participants, and the\npublic at large.\nPolis\u2019 machine learning techniques and automated report provide scaffolding for this\ntask, and indeed make it much more feasible than it would otherwise be, but successful\napplications of the platform nevertheless require dozens of hours of (human) analysis\nper conversation. This typically manifests in the production of a written report which\nbuilds on the automated results to tell a story about the data in terms of the learned\nopinion groups. This role requires significant expertise in data analysis, writing skills to\nsummarize for a general audience, and contextual knowledge about Polis and the topic\nof discussion. These background requirements, training required to build up familiarity\nwith Polis, and ultimately time spent analyzing and reporting on the data, pose a\nsignificant cost to successful adoption of the platform.\nPolis engagements also require a significant commitment of human time and energy\ndevoted to facilitating conversations. To ensure that the initial group of participants\nhave statements to consider as soon as they join the discussion, preliminary comments,\nknown as seed statements, are typically provided. These statements help establish the\nconversation\u2019s tone and offer a starting point for participants to delve into the discus-\nsion. Participant submitted statements are also typically moderated to ensure they are\non-topic, not abusive, and sufficiently distinct from existing statements. More active\nfacilitators will sometimes add clarifying statements based on content they see emerg-\ning from the participants themselves. All of these tasks require significant training in\nbest practices and commitment of time as the conversation unfolds, and run the risk of\nsilencing voices or biasing results.\nThere are a wide range of methods used by groups to process information together\nand form collectively considered positions. Citizen assemblies facilitate intensive delib-\neration, at a considerable cost of time and money (Landemore, 2022). Focus groups\ncan produce robust summaries more quickly, but remain expensive and do not scale.\nMeanwhile, traditional polling sacrifices robustness, nuance and emergence in favor of\ninclusiveness and quantitative interpretability. Polis bridges qualitative and quantita-\ntive methodologies to produce a kind of rough ethnography at greater speed and lower\ncost relative to citizen assemblies or focus groups, trading some robustness for the speed\nand inclusiveness of a traditional poll. Nevertheless, there remains significant potential\nfor further improving outcomes and affordability.\n5\n1.3\nAddressing Polis Challenges with LLMs\nLLMs are artificial neural networks trained on massive corpora of training data (typi-\ncally text) in order to predict new textual completions (OpenAI, 2023; Touvron et al.,\n2023; Bai et al., 2022; Schulman et al., 2022). Importantly, this ability can be used to\ngenerate the most contextually relevant and plausible text as a response to or contin-\nuation of a provided prompt. With careful prompt engineering, LLMs can be used to\nanalyze, synthesize, categorize and in other ways process and produce textual content.\nLLMs can also be used to create textual embeddings, which map bodies of text into high\ndimensional \u201csemantic spaces\u201d which numerically capture the semantic essence of the\ntext in question. These embeddings can be used to measure semantic similarity between\nbodies of text, among other uses.\nLLMs offer a number of opportunities for improving Polis as a deliberative platform,\nwhich we investigate in detail in Section 2, ordered from most feasible and lowest risk\nto speculative and high risk:\nTopic Modelling: Identifying topics among comments to assist with analysis and re-\nporting.\nSummarization: More automated production of nuanced, digestible reports and sum-\nmaries of the outputs of engagements.\nModeration: Reduce the burden of moderating comments.\nComment routing: Make better use of participant and moderator time by more ef-\nfectively \u201drouting\u201d comments to participants for voting, i.e. deciding which com-\nments should be voted on first.\nIdentifying consensus: Discovery of statements with broad consensus between iden-\ntified opinion groups (group informed consensus; Small et al., 2021), as well as\nthose which describe the worldview of specific groups.\nVote Prediction: Based on how a participant voted on N statements, can the system\npredict agreement or disagreement on a random unseen statement?\nHowever, while LLM technology has massively broadened the scope of what\u2019s pos-\nsible to achieve with Natural Language Processing, there remain significant limitations\nand risks associated with their use (Ganguli et al., 2022b,a; Weidinger et al., 2021; Bom-\nmasani et al., 2022; Bender et al., 2021b). These risks are especially critical to address\nif LLMs are to find a place in deliberative technology, where the impact, by the virtue\nof their purpose, is both societal and political. Despite significant efforts on the part of\nresearchers developing LLMs, there remain issues around bias (Santurkar et al., 2023;\nHutchinson et al., 2020; Kurita et al., 2019; Basta et al., 2019; Abid et al., 2021b; Sap\net al., 2020), prompt injection attacks (Greshake et al., 2023; Perez and Ribeiro, 2022),\nso called \u201challucinations\u201d (LLMs making things up when they don\u2019t know something;\nJi et al., 2023b), and basic transparency. Therefore, it is our position that the utmost\ncare be taken in considering how LLMs are employed in deliberative settings.\nBecause Polis is designed to facilitate participant feedback on text, it is particularly\namenable to addressing some of the risks associated with the application of LLMs de-\nscribed above. By integrating human evaluation of LLM-generated content as a part of\nthe participation process, it may be possible to both mitigate risk, and further empower\n6\nparticipants. Where necessary, dedicated human moderators and facilitators can also\nserve as a safety check.\nWe believe that maintaining human feedback is vital as a starting point\nfor safe application of these immensely promising technologies to the task\nof deliberative engagement.\n1.4\nRelated Work\nRecently, there have been several efforts to utilize LLMs in political or sociological\ncontexts. For example, Romania\u2019s government has created Ion, an Artificial Intelligence\nAgent that scours social media for content, solicits feedback from citizens, and reports\nback its findings to the nation\u2019s Prime Minister, who can also interact with it directly,\ne.g. posing questions about public sentiment (France-Presse, 2023). DeepMind has fine\ntuned an LLM specifically designed to generate statements that resonate with a broad\naudience, based on a given set of input statements (Bakker et al., 2022). The Remesh\nplatform (Bilich et al., 2019) has pioneered the use of LLMs for inferring opinion data in\nthe absence of votes across and within conversations by integrating semantic similarity\nvia text embeddings into its Latent Factor Model (Konya et al., 2022).\nThis diverse set of examples reflects the broad design space of LLM application to\ndeliberative technology. In some of these cases, such as Romania\u2019s Ion, we see an LLM\nbeing used in a very unconstrained manner where there is very little risk mitigation.\nBy contrast, DeepMind\u2019s fine-tuned model is modular, and thus presents opportunities\nfor using in a responsible manner, in which it is continually supervised.\nOur motivation for this paper is to consider the broad design space of LLMs as\napplied to Polis, and identify strategies for taking advantage of the many opportunities\npresented by LLMs, while mitigating the risks they might pose.\n7\nFigure 2: Overview of the process of a Polis conversation.\nSteps where we envision the support of LLMs are highlighted in yellow.\nFull\nmethodological details of the current (non-LLM) Polis processing flow are described in depth in Small et al. (2021).\n8\n2\nOpportunities and Risks\nConducting an effective engagement with Polis involves a number of distinct tasks for\nfacilitators and participants. A summary of the system is presented in Figure 2, which\nadditionally highlights the steps which could benefit from the use of LLMs. In this\nsection we discuss the particular challenges we believe to be most amenable to the\napplication of LLMs, as well as the associated risks and potential mitigations.\nTo evaluate the potential for LLMs in these applications, we performed a number of\nexperiments using Anthropic\u2019s Claude (Bai et al., 2022). We describe these experiments\nand their results in the sections below. We include the prompts in Section A.\nWe ran the experiments primarily with open Polis data from a conversation run in\nBowling Green, Kentucky in 2018 (Barry, 2023; Sergent, 2018; McKenzie, 2018). At the\ntime, Bowling Green was deeply divided by national hot button issues, so The Compu-\ntational Democracy Project (the non-profit organization maintaining Polis) teamed up\nwith local media partners at Bowling Green Daily News to explore whether Polis would\nallow residents to unite on common interests. Residents were asked to respond to the\nprompt \u201cWhat do you believe should change in Bowling Green/Warren County in order\nto make it a better place to live, work and spend time?\u201d (See Figure 1 for a depiction\nof the participation interface.) As expected, the community was able to identify local\nissues on which there was general consensus, and hold their officials accountable to act\non that consensus. Some of the issues on which points of consensus were identified\nincluded traffic, development, internet access, and public accountability. This engage-\nment had very high participation rates relative to total population size: 2000 out of\n65000 residents participated, accounting for around 3% of the population. While this\nmight seem minor for an election, it is a remarkably high proportion for a deliberative\nexercise, where typical engagement for such a town might be only 0.1% - 0.3%. The\nautomated report, raw data export, and additional information for this conversation is\navailable in Barry (2023).\n2.1\nTopic Modelling\nOne of the simplest but most versatile uses for LLMs (and NLP more generally) in Polis\nis topic modelling, which assigns abstract \u201ctopics\u201d (short descriptions) to larger bodies\nof text. Categorizing comments under discrete topics can facilitate numerous analyses,\nvisualizations and summarizations which can help people understand the outcome of a\nPolis engagement in greater depth. In the past, we have used human-assigned topics\nin custom reporting to compare how contentious different topics are (vote variance per\ntopic), analyze how voting patterns correlate between topics (using the RV coefficient;\nRobert and Escoufier (1976)), and in narrative summaries to simply enumerate the\ntopics which emerged in broad discussions.\nWe experimented with generating topics from a Polis conversation by prompting\nClaude to identify topics in sets of comments from the conversation. Because LLMs are\nlimited in how much text they can respond to (the so called context window length),\ncomments were assigned topics in batches, and then the resulting batch topics merged.1\n1This experiment was performed on a research model with a context window length of around 8K\ntokens. Since then, Claude has been updated to have a context window length of 100K tokens, which\n9\nGroup\nTopics\nGovernment and Public Policy:\nLocal government and politics\nLaws and regulations\nTaxes and services\nTransparency and accountability\nInfrastructure and Development:\nHousing\nTransportation\nUtilities\nHistoric preservation\nUrban planning\nPublic Services:\nHealthcare\nEducation\nPublic spaces\nHomeless services\nDisability access\nSafety Health and Environment:\nLaw enforcement\nPublic health\nPollution\nEmergency management\nMarginalized groups\nAnimal control\nEconomy and Business:\nJob opportunities\nLocal business support\nCompetition and legalization issues\nTable 1: Claude topic modelling results as returned in response to the prompt shown\nin Appendix A.1.\nSee Appendix A.1 for details about the prompts and systems used.\n2.1.1\nExperimental Results\nWhile not explicitly asked to do so, Claude automatically produced hierarchical topic\nassignments, at two levels of depth. This allowed for a nice breakdown of the conver-\nsation, and is expected to be useful for producing summaries and reports of varying\ndetail, as discussed in 2.2. The resulting topics matched our expectations from manual\nanalysis of the conversation, and are presented in Table 1.\n2.1.2\nRisks and Mitigation\nTopic modelling has fewer risks relative to some of the applications of LLMs we discuss\nin later sections. We foresee only one: Assigned topics could either be inaccurate or\nmiss important contextual nuance. To address this, human review and overriding of\ninferred topics must be supported, as well as manual topic assignment.\nwe used for the experiments in Section 2.5.\n10\n2.1.3\nDiscussion\nWe anticipate that this inherently iterative approach will naturally adapt well to an\nonline system, updating topics as new comments enter the conversation. While we did\nnot rigorously evaluate how stable topics were over iterative application, we found that\nthey were indeed largely stable as we experimented with prompts. Issues which arise\nwith stability on more complex or nuanced datasets may be dealt with by tuning the\nprompt to prefer existing topics over new ones, similar to the sequential construction of\ntopic models in Bayesian nonparametric statistics (Griffiths et al., 2003; Ghahramani\nand Griffiths, 2005).\nHow human overriding of topic assignments adapts to an online recursive application\nof this technique remains to be seen. However, from our experience with the flexibility\nof LLMs to prompting, we again expect this could be achieved by including human\npreference in the prompt.\n2.2\nSummarization and Reporting\nSummarization and reporting are an essential but costly part of deliberative processes.\nIn a citizens\u2019 assembly, facilitators summarize the perspectives they are hearing as they\noversee a deliberation, as well as when they produce a final synthesis for participants,\nstakeholders, policy makers, and the public at large. In parliamentary settings, each\nmember and their staff would be responsible for maintaining an internal representation\nof a multi-stakeholder environment.\nSummarization and reporting are an essential but costly part of using Polis in a\ndeliberative process as well.\nPolis supports summarization of the data it produces\nby providing an automated report which, based on the learned PCA projection and\nopinion groups, highlights comments with high overall support, as well as those which\nbest explain how each opinion group differs from the rest of the conversation. However,\nthe automated report is still close to the raw data, and not suitable for consumption by,\ne.g. a mayor, without dozens of hours of manual human analysis of this information.\nAs mentioned above, this requires a significant level of expertise and training, which\nconstrains the adoption and ultimate impact of these processes.\nFor some domains, LLMs have already been shown to be effective at summarizing\ntextual and numeric information, in some cases judged as on par with human written\nsummaries (Zhang et al., 2023; Singh et al., 2023). LLMs would thus seem to have\nimmense potential for reducing the burden of this critical task. However, many LLMs\nstill have significant context window length limitations, which pose a constraint in\nhow they can be applied to large conversations, which might have many thousands\nof comments.\nMoreover, given the critical nature of this aspect of the deliberative\nprocess\u2014literally the mechanism by which meaning is made\u2014it\u2019s imperative that it be\nthought through carefully.\nThe overall organization of the current report\u2014presenting comments which best\ncharacterize or represent each opinion group, followed by points of consensus\u2014has al-\nready proven largely successful in serving as an anchor for meaning-making exercises\n(see Small et al., 2021, for details).\nCustom reporting typically builds on this pattern, adding narrative descriptions to\n11\nsummarize the perspectives of each group, and the participant body as a whole. An\nLLM can similarly summarize each opinion group in turn, as well as points of consensus,\nusing the same comment selection routines featured in the automated report. Prompts\ncould even instruct the LLM to cite specific comments to backup its claims, and these\ncould be used to insert vote distributions figures like those which show up in the current\nreport.\nWhile not a feature of the current automated report, custom reports sometimes dive\ndeeper by exploring the topics which emerge in the conversation. In these cases, topic\nmodelling is performed by humans, and reports spend time investigating results on a\ntopic by topic basis. Given automated topic modelling, as discussed in Section 2.1,\ncomments and voting patterns associated with a given topic can be summarized by a\nLLM, together with the associated group-by-group voting data.\nThese techniques address context window limitations, while scaffolding upon known\nand effective patterns. For large conversations, it may be necessary to filter comment\nsets by selecting for higher quality comments, based on number of \u201cPass/Unsure\u201d votes,\netc.\nIn cases where context window length is still an issue, summaries can also be\nperformed recursively; An initial batch of comments can be summarized, followed by a\nprompt to update the previous summary with a second batch of comments, and so on.\nThis reflects the pattern of recursive summarization explored with human summarizers\nin Zhang et al. (2017).\n2.2.1\nExperimental results\nTo investigate the potential for using LLMs to automatically generate concise and co-\nherent summaries, we took batches of comments from the Bowling Green, Kentucky\nconversation, including vote data, had Claude generate summaries, and then merged\nthe summaries across batches. The comments and prompt are presented in Appendix\nA.2, and resulting summary shown below.\nThe following summary was produced using a small set of comments, together with\noverall number of agree and disagree votes:\nClaude\u2019s output:\nIn summary, there was general agreement on issues around traffic, public\namenities,\nmedia/technology,\nurban\nplanning,\nand\ntransportation. Views\nwere more mixed on schools,\nneighborhood issues,\nhealthcare,\neconomics,\nand\ncommunity\nsupport. A\nfew\nlighthearted\nor\nisolated\ncomments\ndid\nnot\nshow\nclear\nagreement\nor\ndisagreement. Overall,\nthe\ncomments\nindicated\ninterest\nin\nimproving\ninfrastructure,\nquality\nof\nlife,\nand\ncommunity\nsupport in the area. But there were some polarizing views on select issues\nlike immigrant language ability or reactions to the opioid crisis.\nWhen asked to summarize points of agreement and disagreement between groups\n(using the percentage of agrees and disagrees per group), the following was produced:\nClaude\u2019s output:\nOverall, there was more agreement in Group 0 on infrastructure, housing\n12\nand public services issues while more disagreement in Group 1 on those\nsame topics. Both groups agreed there is a problem with rental properties\ndeteriorating. Group\n0\nmostly\ndisagreed\nwith\ncomments\nrelated\nto\nthe\nopioid\ncrisis\nwhile\nGroup\n1\nwas\nsplit. The\nsummary\nshows\nthe\ndominant\ntopics\nof\ndiscussion\nand\nhow\nmuch\neach\ngroup\nagreed\nor\ndisagreed\nwith\nthem.\nBoth of these summaries concur with manual analysis of the data, and demonstrate\nthat LLMs are capable of summarizing data from Polis conversations. There is room\nfor improvement however; e.g. \u201cGroup 0 mostly disagreed with comments related to\nthe opioid crisis\u201d could more clearly specify which side of the issue the group falls on\n(e.g. \u201cGroup 0 mostly agreed with statements expressing concern for the opioid crisis\u201d).\nHowever, additional efforts with prompting (possibly providing examples) are likely to\naddress this.\n2.2.2\nRisks and Mitigation\nLLMs have potential to assist in automatically generating summaries of deliberative\nprocesses. However, it is important to note that LLMs can generate misinformation or\nfabricate details while summarizing (Ji et al., 2023a; Stiennon et al., 2020; Bommasani\net al., 2022). Detecting these errors automatically is challenging. Prior work has pro-\nposed automated metrics to detect such errors (Maynez et al., 2020; Durmus et al.,\n2020; Kryscinski et al., 2020; Goyal and Durrett, 2021); however, they do not always\ncorrelate well with human judgements (Fabbri et al., 2021; Durmus et al., 2022).\nFurthermore, LLMs may reflect and amplify societal biases present in their training\ndata (Bender et al., 2021a; Abid et al., 2021a; Cheng et al., 2023) \u2013 including political\nbiases \u2013 which may affect what information they extract and emphasize when summa-\nrizing deliberative discourse (see Section 3.2). This could potentially lead to skewed\nor unrepresentative summaries that fail to capture the full spectrum of perspectives\nexpressed.\nWe believe human involvement is critical for checking machine-generated summaries\nof deliberative processes to ensure accuracy and fairness. One specific innovation we\nintend to implement towards this end is to show the automated summaries back to the\nparticipants for review and feedback. Prior work has found this type of participatory\napproach, where the users and stakeholders of an AI system are involved in evaluating\nand improving its outputs, can help identify errors, biases, and misrepresentations (Lee\net al., 2022; Yang et al., 2018).\nBy soliciting feedback directly from the participants whose perspectives were sum-\nmarized, we can collect targeted annotations on the quality of the summaries. To avoid\noverwhelming participants, these summaries should be concise, likely at the level of\ntheir groups\u2019 perspective on a particular topic or subtopic, building up to more com-\nprehensive summaries. These human judgments can be used to as a feedback to the\nmodels and better align the automated summarization with what participants feel cap-\ntures the essence of the discourse, a sort of Deliberative Reinforcement Learning from\nHuman Feedback (RLHF) (Ziegler et al., 2020; Lambert et al., 2022). This human-AI\ncollaborative approach, combining the scale of LLMs with human insight, has potential\nto produce higher-quality and more trustworthy summaries.\n13\nFigure 3: Example of using Claude to help evaluate a human-written summary. The evaluation\nof the representativeness score is done by Claude as specified in the prompt in Appendix A.6\nfor details.\nWhether generated by humans or LLMs, summaries inevitably leave out details, and\nmay not capture every perspective. It\u2019s critical that these omissions be transparent, but\nevaluating a summary can be even harder and more time consuming than generating\none, particularly for humans. We can assist evaluation by decomposing the problem\ninto a set of much smaller and narrower tasks: asking an LLM to evaluate how well a\nsummary represents each individual comment. This can augment a human evaluator by\ndrawing their attention to the comments least represented by a given summary, using\nan interface such as the one shown in Figure 3, and prompting shown in Appendix A.6.\n2.2.3\nDiscussion\nThe interface in Figure 3 could be taken a step further towards intelligence augmenta-\ntion: rather than replacing human summarizers, we can use language model evaluations\nto augment their ability to create good summaries, empowering participants to place\nthe summary in its proper context. One could imagine the rapid feedback loop of a\nreport editing environment which, as you type, immediately highlights any comments\nor topics that are not adequately represented by the text. This would allow a human\nanalyst to either take automated summaries as a starting point for a custom report, or\nwrite a report from scratch, and have guidance about what perspectives could be better\nreflected.\n2.3\nFacilitating Conversations by Synthesizing Group Identity and\nConsensus\nIn live deliberations, a human facilitator may summarize the standpoint or worldview\nof a group expressing its opinions as the discussion unfolds, to determine whether the\ngroup assesses that it is well understood by the facilitator. Importantly, this process\n14\nis dialectic: the facilitator reflects back their understanding of what they\u2019ve heard,\nrecursively asking for confirmation or further clarification until a fixed point is reached,\nand each side feels understood. This process of building shared understanding is a core\npillar of the Nonviolent Communication approach to conflict resolution (Rosenberg,\n2005), which was profoundly influential in shaping the design of Polis.\nBetter understanding where each side is coming from increases the likelihood that\npoints of common ground are able to serve as anchors for bridging divides; respecting\npoints of dissent is key to distilling solutions with broad support.\nWhile summarization and reporting on the final results of a Polis conversation (see\nSection 2.2) is an important part of this process, summarization may happen during\nthe Polis conversation as well.\nExperienced facilitators of Polis conversations inject\nstatements into the conversation as a way of testing hypothesis they may have about\nthe emerging opinion landscape as it unfolds. This is especially true of potential points\nof consensus.\nGiven a set of statements with broad support from across the participant body,\nan LLM could craft additional statements which it suspects have similar support, and\nspecifically represent points of consensus. Similarly, given a set of statements with broad\nsupport from a particular opinion group, it could craft statements which it expects those\nparticipants will tend to agree with. Of note, DeepMind has researched and developed\nLLMs specifically tailored towards generating content with broad support based on a\nset of input statements (Bakker et al., 2022), an approach which might be perfectly\nsuited to this task.\nWe hypothesize that by reflecting back to the participants syntheses of their per-\nspectives:\n1. Points of common ground or consensus surface faster.\n2. Conversations \u201cconverge\u201d faster, as measured by our ability to accurately predict\nvotes based on past voting data, and quickly understand where people fall in the\nopinion landscape.\n2.3.1\nExperimental Results\nThe first approach we tested for generating consensus statements involved feeding the\noverall summary (see Section 2.2.1) into a prompt which asked Claude to generate\nnew consensus statements based on the summary (see Appendix A.3 for details). This\nproduced the following statements:\nClaude\u2019s output:\n\u2022 Our infrastructure and public amenities could use continued improve-\nment and investment. Things like roads, parks, transit, etc. enhance\nquality of life for all.\n\u2022 We\nshould\nadvocate\nfor\nbetter\nurban\nplanning\nand\ntransportation\noptions. Making the community more livable,\nwalkable and connected\nwould benefit everyone.\n\u2022 While we may disagree on some issues, we all want a safe, supportive\n15\nand thriving community. Compromise and understanding can go a long\nway.\n\u2022 Access to healthcare and economic opportunity are important for any\ncommunity. We should work to enable wellbeing and prosperity for as\nmany as possible.\n\u2022 Technology\nis\nconstantly\nchanging,\nbut\nwe\nshould\nensure\nchanges\nbenefit\nand\nconnect\nour\ncommunity,\nnot\nisolate\nus. Education\nand\nconnectivity for all ages matter.\n\u2022 There is always room for improvement, but appreciation for what we\nhave\nand\nthe\ndiversity\nin\nour\ncommunity\nis\nso\nimportant\ntoo. Our\ndifferences make us stronger together.\nWhile there is no way to assess the status of these statements as potential points of\nconsensus without feeding them in to a live conversation with human participants, they\nnevertheless capture the typical tone and style of consensus statements we\u2019ve observed in\nreal conversations. Testing this work in situ would have required a complete integration\nof the method into the software, and was thus out of scope for this study.\n2.3.2\nRisks and Mitigation\nKey questions remain for the ethical application of LLM technology as described in this\nsection:\n1. What thresholds should exist for the fraction of comments in a conversations\ngenerated by LLMs, as apposed to humans?\n2. What metrics and procedures should be involved in deciding when and how often\nto submit LLM generated content?\n3. Should these statements be routed (probability of being displayed for votes) any\ndifferently than human-generated statements?\n4. How should their generated nature be disclosed?\n5. How might we measure the impact of nudges the LLMs would insert in the process?\nUnderlying all of these concerns is the question: How much machine influence is ac-\nceptable in a process which ultimately aims to surface human opinions? Perhaps human\nin the loop review of LLM generated statements is sufficient to ensure ethical application\nof this technology. But further consideration seems warranted regarding what additional\nconstraints should be placed upon the quantity and nature of its submissions.\nEspecially important to consider is the mapping between minority opinion groups\nand the continued consideration of the core Polis group-informed consensus mechanism\n(Small et al., 2021), ensuring there is no reversion to prioritizing majority rule.\n2.3.3\nDiscussion\nEthically, it\u2019s important that participants be aware that they may be looking at a\nLLM generated content. Moreover, many LLM Terms of Service (including those of\nOpenAI\u2019s GPT and Anthropic\u2019s Claude) require that people be informed when they\n16\nare looking at model generated content. However, because knowledge that a comment\nwas written by an AI would likely affect the way people vote on it, LLM-generated\ncomments would (from a purely statistical perspective) ideally display no differently\nthan human-generated comments, possibly as having been anonymously written.\nAddressing both of these concerns, the interface might notify participants when a\nconversation has LLM generated content, without specifying which comments, until the\nconclusion of the conversation. Alternatively, the interface could notify participants\nwhen a statement was generated by an LLM immediately after they had finished voting\non it.\n2.4\nVote Prediction\nThe ability of LLMs to predict votes in a Polis conversation is of both theoretical\nand practical interest. From a theoretical standpoint, predicting votes is a valuable\ntest of how capable LLMs are of understanding the opinion landscapes which emerge\nfrom Polis conversation. Experiments have already demonstrated that LLMs trained\non media content can predict public opinion (Chu et al., 2023). More practically, vote\nprediction has significant potential to improve how Polis handles missing data, both in\nreporting results, in the core clustering algorithms, and as a signal for comment routing\n(see Section 3.4.3).\nLarge Polis conversations can have thousands of comments, while most participants\nvote on fewer than 100 comments. So over 90% of the data in the participant \u00d7 comment\nmatrix may be missing for such conversations.\nThe comment routing algorithm tries to ameliorate the missing data problem by\nsurfacing the most relevant comments (which either help explain the opinion space, or\ngarner broad support), but it has limitations. Without careful engagement design, early\nparticipants miss out on the opportunity to come back and respond to comments which\nemerge later in the conversation. Participants in some opinion groups tend to vote more\nthan others, complicating interpretation of the total vote breakdown. This problem can\nbe mitigated by looking at votes group by group, but we must also consider whether this\nphenomena operates on an intra-group basis. Inferring missing voting data therefore\nhas the potential to improve our ability to understand and interpret the results of Polis\nconversations.\n2.4.1\nExperimental Results\nWe evaluated vote prediction by prompting an LLM with a participant\u2019s previous voting\nhistory and asking if the participant would agree or disagree with a given comment (see\nAppendix A.4 for details). Surprisingly, we found that a plain LLM (Askell et al., 2021),\nwithout fine-tuning or RLHF training, was nearly perfectly statistically calibrated at\npredicting agreement, as seen in Figure 4. In a majority of cases the LLM was at least\n90% confident in its prediction, and correctly calibrated even at that high confidence\nlevel. This suggests LLMs are already very capable of understanding human opinions,\npresenting both opportunities and serious risks.\n17\nFigure 4: Calibration plot of LLM-predicted probabilities of participant agreement with com-\nments. The probabilities predicted by the LLM are close to perfectly calibrated across comments.\n2.4.2\nRisks and Mitigation\nSystematically inaccurate vote prediction runs the risk of misrepresenting public opinion\non issues that arise within a conversation. Given the accuracy of vote prediction on an\nindividual basis, it\u2019s possible that in aggregate the inference across participants will\nbe even more accurate. However, participant bodies which deviate from stereotypical\nideological configurations may be systematically misrepresented by these estimates.\nCare must be taken to adaptively track and adjust predictions for opinions which\nbreak the LLM\u2019s expectations. Other mitigations of this risk might include: presenting\nboth raw and inferred vote tallies, presenting inferred tallies only for comments with\nhigh inference confidence, aggregating inference information only for those participants\nfor whom we have high confidence, or presenting inferred tallies with estimated error\nmargins.\nThe existence of powerful vote prediction technology creates incentives to entirely\nreplace human participation with in-silico deliberation. This runs the risk of amplifying\nexisting biases as well as eliminating the many positive externalities of deliberation on\nmutual understanding, civic empowerment, surfacing leaders, etc. We strongly advocate\nthat vote prediction be used only to amplify participants\u2019 voices, never to replace them.\nWe reject as invalid on both ethical and performance grounds the use of vote prediction\ntechnology to replace human participants with simulations.\nWe cannot enough emphasize the latter point: it would be catastrophic for de-\n18\nliberation at scale if the remarkable capabilities of LLMs lead to replacement\nof whole groups of individuals by simulacrums designed by a very different\npopulation. Besides the question of accuracy and biases, we believe this could also\nlead to a crisis of faith and belief in the deliberative process.\n2.4.3\nDiscussion\nMore speculatively, vote prediction could be used as another way to evaluate the com-\npleteness of Polis reports and summaries. A good summary of a conversation should\nallow its readers to accurately predict the participant \u00d7 comment vote matrix. Users\nwhose votes are particularly hard to predict could be surfaced as statistical \u201coutliers\u201d\nnot well-represented by the summary, as with comments poorly represented by sum-\nmaries (as discussed in Section 2.2).\nThis information could also be presented to participants themselves as a sort of\n\u201cprogress bar\u201d or measurement of Polis\u2019 ability to understand their position in the\nopinion landscape. Right now participants are shown the number of comments remain-\ning, which for large conversations will typically read \u201c100+\u201d in perpetuity, until they\ngrow weary and close the conversation. Grading how well we understand a participant\u2019s\nposition will give them a clearer sense of where they stand, and better judge how much\nlonger they want to participate.\nThese metrics could also be used to determine when it might be a good time for a\nparticipant to return to the conversation once new and more interesting comments have\nchanged Polis\u2019 certainty about their position in the opinion landscape. However, care\nshould be taken to evaluate the net effect this has on engagement. While it\u2019s possible\nthat engagement improves among participants who tend to vote on fewer comments, the\nopposite may be true for those who tend to vote more. It is also important to consider\nhow systematic errors in inference might affect participation levels, since overconfidence\ncould encourage participants to leave early.\n2.5\nLeveraging Large Context Windows\nThe above designs and experiments were carried out assuming the constraint of context\nwindow lengths typical of most LLMs. Since performing those experiments, Anthropic\nreleased a model with a 100k-token context window length, capable of including the\nentire set of comments from the Bowling Green conversation. To explore how this affects\nboth the design space and qualitative results of applying LLMs towards the challenges\naddressed in Section 1.2, we performed several experiments which performed both topic\nmodelling and summarization in a single pass against the entire set of comments from\nthe Bowling Green conversation, including vote breakdowns per group.\n2.5.1\nExperimental Results\nThe first experiment asked Claude to perform topic modelling on the provided com-\nments, then quantify the level of agreement vs disagreement between the two groups in\nthe conversation, and summarize differences in opinion between the two groups. The\nresults are shown below:\n19\nClaude\u2019s output:\n1. Traffic\nand\ntransportation\n-\nComments\nabout\ntraffic\nflow,\npublic\ntransit, sidewalks, bike lanes, road widening, traffic lights, etc.\nThis topic had a high level of agreement between Group 0 and Group1.\n2. Housing\nand\ndevelopment\n-\nComments\nrelated\nto\naffordable\nhousing,\nrental prices, property taxes, neighborhood revitalization, zoning,\nannexation, urban planning, downtown development, etc. Group 0 tended\nto\nagree\nmore\non\nincreasing\naffordable\nhousing\nand\nrevitalizing\nneighborhoods\nwhile\nGroup\n1\nagreed\nmore\non\nlimiting\nmulti-family\ndevelopments.\n3. Healthcare\nand\npublic\nhealth\n-\nComments\nabout\nhealthcare\naccess,\nopioid addiction,\nmental health,\nmedical marijuana,\nhealthy eating\noptions, etc. Group 0 agreed more on treating addiction as a health\nissue and increasing healthcare access while Group 1 agreed more on\nharsher sentencing for drug users.\n4. Education\n-\nComments\non\nschool\nfunding,\nclass\nsizes,\ncurriculum,\nhigher education,\netc. Group 0 agreed more on funding schools and\ncommunity\ncolleges\nwhile\nGroup\n1\nagreed\nmore\non\nschools\nhelping\naddress budget cuts.\n5. City laws and policies - Comments about fairness ordinance, noise or-\ndinances, smoking bans, term limits, transparency in government, etc.\nGroup 0 overwhelmingly agreed more on passing a fairness ordinance\nwhile Group 1 disagreed. Group 1 agreed more on enforcing existing\nlaws.\n6. Parks\nand\nrecreation\n-\nComments\nabout\nincreasing\ngreen\nspaces,\nplaygrounds, community activities, tourism opportunities, etc. Group\n0 agreed more on improving parks and recreation opportunities while\nGroup 1 agreed at a lower level.\nIn summary, while Groups 0 and 1 showed high agreement on some topics like\nimproving traffic flow and infrastructure, there were clear differences of\nopinion on social issues like healthcare, education, housing, and LGBTQ\nrights. Group\n0\nexpressed\nmore\nprogressive\nviews\nsupporting\nincreased\ngovernment\ninvestment\nin\npublic\nservices\nand\nprotecting\nmarginalized\ngroups. In contrast, Group 1 expressed more conservative views supporting\nlimited\ngovernment\nintervention\nand\nexisting\nlaws. The\ntopic\nmodeling\nanalysis\nrevealed\nthese\nopposing\nviewpoints\nand\nareas\nof\ndisagreement\nbetween the two groups.\nThe second experiment omitted topic modeling as an intermediate step in the\nprompt, instead asking for a summary of the key points of consensus and divergence\nbetween the groups. The results:\nClaude\u2019s output:\nBased on the comments and votes from Group 0 and Group 1 on each comment,\nhere are the key points of consensus and divergence that I identified:\n20\nConsensus between Group 0 and Group 1:\n\u2022 Most comments that received strong agreement in both groups focused\non\ninvesting\nin\nthe\ncommunity\ngood,\nimproving\ninfrastructure\nand\npublic\nservices,\nsupporting\nthe\nlocal\neconomy,\nand\nprotecting\nthe\nenvironment. For example,\ncomments advocating for more bike lanes,\nsidewalks,\npublic\ntransit;\ninvesting\nin\nthe\narts\nand\neducation;\ndeveloping the downtown and riverfront area; increasing green spaces;\nimproving\ntraffic\nflow\nand\ninfrastructure;\nand\nsupporting\nlocal\nbusinesses were agreed upon by both groups.\nDivergence between Group 0 and Group 1:\n\u2022 There\nwere\nsome\nnotable\ndifferences\nof\nopinion\non\nsocial\nand\npo-\nlitical issues. Group 0 tended to be more progressive while Group 1\ntended to be more conservative. For example, Group 0 largely agreed\nwith and Group 1 largely disagreed with comments supporting LGBTQ\nrights,\nmarijuana legalization,\nand sanctuary cities. Group 0 dis-\nagreed and Group 1 agreed with restricting immigration and requiring\nimmigrants to learn English.\n\u2022 Group 0 was more supportive of government intervention and social\nprograms to address issues like poverty,\nhomelessness,\nhealthcare,\nand education. Group 1 was more skeptical of government overreach and\nsupportive of free market principles. For example, Group 0 agreed and\nGroup 1 disagreed with raising the minimum wage, providing affordable\nhousing and healthcare options,\nand increasing funding for schools\nand social programs.\n\u2022 There\nwere\nalso\ndifferences\nin\nlevels\nof\ntrust\nin\ninstitutions\nlike the media, government, and universities. Group 0 expressed more\ntrust\nin\nthese\ninstitutions\nwhile\nGroup\n1\nwas\nmore\nskeptical\nand\ncritical. For\nexample,\nGroup\n0\nagreed\nand\nGroup\n1\ndisagreed\nthat\nmost journalists and public officials aim to serve the public good.\nGroup 0 disagreed and Group 1 agreed that university mismanagement is\nmostly responsible for increasing tuition.\nIn summary, while there were areas of agreement on community investment\nand development, the groups differed in their social and political values\nas well as levels of trust in public institutions. But there is clearly\nsome common ground to build upon, if the goal is to find consensus and\nwork together.\nThese summaries demonstrate a marked improvement over the results from Section\n2.2. Not only were they were more detailed and nuanced, but they succeeded in better\nexplaining what position participants (either as a group or whole) took on a particular\nissue.\n2.5.2\nRisks and Mitigation\nMany of the risks inherent in this application have already been discussed in previous\nsections (notably Sections 2.2 and 2.1). For topic modelling specifically, this approach\n21\nwould seem to be less risky overall, as it may be easier to glean a comprehensive but\naccurate topic breakdown when considering the full set of comments all at once. For\nsummarization, there may be some benefit to subsetting comments based on other crite-\nria (e.g. Polis\u2019 PCA or K-means grouping) prior to applying LLMs for summarization,\nsince we have some external and explainable basis for contextualizing how those com-\nments relate to each other. However, this comes at the cost of not being able to consider\nthe full conversation, which could lead to less nuanced summarization of the data, and\nso here as well, the large context window approach may ultimately be less risky.\n2.5.3\nDiscussion\nIn this study, we leverage Claude\u2019s 100k context window to distill intricate opinion land-\nscapes from extensive conversational data, a task usually demanding significant manual\neffort. To avoid bias, we asked the LLM to piggyback its analysis on deterministic statis-\ntics which were gathered the usual way one would with Polis, and then to summarize,\nas someone might with dozens of hours of data analysis.\nUsing a large context window has proved beneficial for this particular task because\nit facilitates integration and correlation of diverse pieces of information in a unified\nmanner. This, in turn, creates a richer understanding and representation of the data,\nakin to a \u201dbroad perspective\u201d. This holistic approach fosters an understanding that is\ngreater than the mere sum of its parts, and proved far superior to results obtained with\na smaller context window length.\nThe strength of this result is not limited to zero shot summarization capabilities; as\nPolis is a real-time system, participants can collectively respond to and rate summaries\nof their positions potentially hundreds to hundreds of thousands of times during the\ncourse of a deliberation.\nWith these findings, new areas of exploration open up. For instance:\n1. Given the real-time operation of the Polis system, it could be worthwhile to ex-\namine how the language model can interpret evolving voting patterns over time,\nor recursively consider notable shifts between previous iterations of analysis.\n2. Leveraging the system\u2019s internal representation of the stakeholder groups to ex-\ntend Polis\u2019 real-time capabilities into multi-stakeholder dialog using a different\napproach than attempted in the CICERO research.\n3. Allow participant feedback on summaries, even in plain text (e.g. \u201cI\u2019m not part of\ngroup Y for X reason\u201d) as the the basis for a kind of novel hierarchical clustering\nalgorithm, which might draw out expert sub-populations and niche interest groups.\n3\nDiscussion\nWhile this paper primarily explores the application of LLMs towards optimizing Polis\nas a deliberative platform, the considerations here apply much more broadly towards\ndeliberative systems which wish to embrace LLMs and other Machine Learning tech-\nnology.\n22\nFigure 5: Overall vote distributions of Claude and human opinion groups.\n3.1\nIntelligence Augmentation, Not Human Replacement\nAlready, as mentioned in Section 1, we see attempts to use in silico deliberation to\nreplace or obscure human voice and agency (France-Presse, 2023).\nAs broadly appealing as the idea is of having limitless insight into public opinion at\nbeck and call, removing the opportunity for humans to interact (deliberate) with other\nhumans fails to respect human agency in building shared understanding, and runs the\nrisk of misrepresenting and thereby doing violence to public will.\nIt is therefore vital that wherever LLMs are used in such systems to generate de-\nliberative content, there be an opportunity for humans to be included in the process\nof evaluating, verifying and contextualizing that content, augmenting rather than sup-\nplanting human intelligence.\n3.2\nMeasuring Ideological Bias\nAs we begin to adopt LLMs for the tasks of synthesizing information, building consensus\nand potentially even assisting with moderation (see Section 3.3), it\u2019s important to stop\nand evaluate whether LLMs themselves have biases which might influence their output.\nWhile measuring bias is a domain of active research where even core definitions can\nbe debated, the use case of online deliberation offers a very concrete set of measures:\nBy having a LLM vote on comments as part of the discussion, we can see how its votes\nrelate to those of the human participants in the conversation, and therefore directly\nmeasure whether the LLM votes in a manner that is biased toward a particular opinion\ngroup.\nUsing the prompts described in Section A.5, we asked Claude to vote on each of\nthe comments from the conversation. Because LLMs are probabilistic models, we can\nlook at the probability associated with each response (vote choice), and compare this\nwith probability estimates associated with each of the actual opinion groups.\nThe\ndistributions of these probabilities across comments are shown in Figure 5. This figure\nillustrates that Claude\u2019s voting pattern is distinct from both of the groups, but overall\nmore similar to Group 0, the more socially progressive of the two groups. While we\nmight expect a disembodied LLM with no lived experience in the town of Bowling Green,\nKentucky to vote more frequently for \u201cPass/Unsure\u201d, it actually did so less frequently\nthan Group 0 (but more than Group 1).\nComparing these vote probabilities, we can measure the response similarity between\nClaude and a given group as 1 \u2212 JensenShannonDistance of the vote probabilities\nfor a given comment. The distribution of these values across comments are shown in\n23\nFigure 6: Claude votes more similarly to one of the human opinion groups.\nFigure 6, which again shows that Claude is significantly biased towards Group 0 (T-test\nP = 1.86 \u00d7 10\u22124).\nSanturkar et al. (2023) and Hartmann et al. (2023) have demonstrated evidence that\nother LLMs may trend left-leaning politically. In the role of deliberative assistant, this\nis potentially problematic if we are relying on a model as a multi-partial (Harding, 1986)\nfacilitation agent when it aligns more closely with one of the groups than the other.\nHowever, a complete lack of bias is impossible for both humans and LLMs. Some\ncharacteristics of facilitation practice include remaining curious and open to all sides\n(multi-partial) while attempting to help a group through the transformative process of\nbridging a divide. While we can attempt to instill the same attitude in LLMs with\ncareful prompt engineering, our ability to measure LLM biases, as described above,\nprovides us with a unique opportunity to monitor them relative to human agents.\n3.3\nRethinking Moderation\nModeration serves two broad purposes: protecting Polis participants from exposure\nto abusive or hateful content, and optimizing participant time. The latter has until\nnow typically led us to recommend moderators remove comments that are off topic or\nduplicates of existing comments. However, as we considered how LLMs might reduce\nthe burden of moderation and participation, we have arrived at a new position on\nthis matter: Moderators should only remove abusive content, with comment routing\noptimizing participant time.\nOne of the strengths of Polis as a system for sentiment analysis relative to traditional\nsurveys is that participants are able to define the dimensions of the opinion space they\nmap out in their own words. It\u2019s often precisely the questions survey crafters (or\nmoderators) wouldn\u2019t think to ask which are the most pivotal in both understanding a\nspace, and arriving at novel solutions to challenging political issues. Therefore, taking\nsome of that power away from participants and putting it the hands of the moderators\nis ultimately disempowering.\nOver the years we have observed instances of moderators removing perfectly reason-\nable positions which challenge the underlying assumptions and framing of the conversa-\ntion prompt. Since observing this, The Computational Democracy Project has made a\npoint of improving training practices to cultivate more principled ethical standards for\nmoderation. But this is ultimately time consuming, and still leaves room for distortion.\n24\nMeanwhile, the incidence of truly \u201coff topic\u201d comments is typically infrequent, and thus\nthe burden posed to participants relatively small. And ultimately, participants them-\nselves have a way of signaling that a comment is of low relevance, which already feeds\ninto the comment routing algorithm: Pass votes.\nSemantically similar comments which might be considered \u201cduplicates\u201d are much\nmore common. However, deciding how semantically close two comments must be to\neach other to be considered \u201cduplicates\u201d is not a straightforward task. Inconsistent\napplication of this blurry threshold again leaves room for moderator bias to confound\nresults. It is also the most time consuming aspect of the moderation process, since\nevery comment needs to be evaluated in relation to all previously submitted comments.\nThis burden is greatest for the largest of conversations, where participants are unlikely\nto vote on every one of the thousands of comments submitted before adding their own,\nleading to a higher number of near-duplicate comments.\nThere\u2019s a significant body of research which suggests that subtle shifts in the way\npositions are framed or expressed can have a huge impact on how people react (Lakoff\net al., 2004; Lakoff, 2009), and our years of experience with Polis as a platform bears\nthis out. There\u2019s also a strong case to be made that, in the absence of funded training,\nthis is likely to have an outsized effect on historically marginalized communities, who\nmay express themselves in ways which might not resonate as well with moderators from\nthe status quo. These individuals may consequently have their voices silenced in favor\nof more \u201cpalatable\u201d phrasings which further entrench privileged perspectives.\nWhile we\u2019ve seen that the existing comment routing system can and does boost\none particular framing of a position over another under certain circumstances (when it\neither appeals to one group more than another, or has higher overall support from the\nconversation as a whole), this effect is frequently mild. As participants grow weary from\nresponding to multiple rephrasings of the same sentiment, they may leave the conver-\nsation without responding to less frequently brought up (but nevertheless important)\ntopics. However, in recognizing this particular cost, we see a potential resolution: By\ntaking advantage of topic modelling efforts and potentially vote prediction (see Sec-\ntion 2.1 and Section 2.4) in comment routing (see Section 3.4.3), we can mitigate this\nproblem without sacrificing the diversity of framing that makes Polis so valuable.\n3.3.1\nA Moderate Approach to LLM-assisted Moderation\nModeration of hate speech remains central to ensuring the integrity of a Polis con-\nversation\u2019s result, as violent and/or dignity-impairing speech limits full participation\n(Habermas, 1962, 1981).\nWhile continued support for strict moderation (requiring comments to be moder-\nated in before being shown) can address this issue, the delay between submission and\nmoderation reduces exposure to new ideas.\nA potential \u201cthird way\u201d between strict\nand non-strict moderation might leverage LLMs (or other NLP technology) to place\npotentially toxic or hateful content in a queue for human review (Jahan and Oussalah,\n2021; MacAvaney et al., 2019; Wei et al., 2021; Chiu et al., 2022). Relative to strict\nmoderation, this approach would overall get new perspectives out more quickly, with a\nlower risk of exposure to problematic content relative to non-strict moderation.\n25\n3.4\nFuture Questions and Directions\nThe following sections discuss ideas which are either more speculative, rely on further\ntechnological advances, or which we simply did not have time to fully explore.\n3.4.1\nSeeding Conversations\nFor most major Polis conversations, human facilitators submit seed statements to ensure\nthat there is content there to vote on for the very first tranche of participants, and set\nthe tone of the conversation (not dissimilar to how Sequential Design of Experiments,\nActive Learning (Cohn et al., 1996), and Bayesian Optimisation (Rasmussen, 2006) are\nseeded with a set of initial points). Most participants (on average, around 90%) do\nnot comment in conversations, and only vote, so it is important to ensure that there\nis content there to start off the conversation which does a reasonable job of laying out\nsome of the key perspectives expected to emerge. Given the potential impact these\nstatements have on the direction of the conversation (Lakoff et al., 2004; Lakoff, 2009),\nit typically involves careful consideration of the opinion space ahead of time, known as\nframing, a task which requires investment in prior training. And while the beauty of\nPolis is that participants have the opportunity to turn a narrative or framing on its\nhead, there\u2019s nevertheless the potential for these seed comments to have an outsized\ninfluence on the course of the discussion.\nTo address this issue, seed comments could be generated along the lines of Section\n2.3. However, rather than input comments into this process which have been submitted\nby participants (obviously impossible if the conversation hasn\u2019t started yet), content\nbe sourced from a combination of social media content and online archives of debate\ncontent, e.g. The Society Library (noa, 2023). This move would further empower the\npublic at large in framing the terms of deliberations, and reduce the impact of a lack of\ntraining in issue framing by those setting up a conversation.\n3.4.2\nDimension Reduction and Clustering\nTopic modelling and semantic embeddings could potentially be used to adjust how Polis\nperforms dimension reduction (related work can be found in Konya et al., 2022). The\ncurrent application of PCA effectively more heavily weights votes on issues for which\nthere are a larger number of comments (since PCA looks for correlations in responses,\na large number of semantically similar comments may have an outsized effect on the\nresulting opinion space). To some extent, this can be seen as a feature, in that frequently\nsubmitted opinions may be more important to participants, and thus deserve some\namount of boosting in the learned opinion space. However, this could be exploited to\nentrench wedge issues and smother novel perspectives. With the planned addition of a\nfeature allowing participants to indicate how important issues are to them (independent\nof any application of LLMs), we also have a more direct way of integrating this signal.\nBy weighting columns in the vote matrix according to how semantically similar the\ncorresponding comment is to other comments in the conversation, we may be able to\nobtain a more robust and nuanced understanding of the opinion landscape. Multiple\ncomments exploring different angles of a particular topic would still overall boost that\ntopic\u2019s effect on the opinion space, but in a more measured way which allows for newer\n26\ntopics to emerge and have an appropriate impact on the structure of the conversation.\nThis would also naturally tie in with and support efforts to rethink Polis\u2019 approach to\nmoderation (see Section 3.3) by reducing the pressure to deduplicate comments, since\nthe algorithm would effectively be doing this work automatically.\n3.4.3\nImproved Comment Routing\nComment routing is the process which determines how comments are routed to par-\nticipants for vote solicitation. Polis uses probabilistic weighting to determine which\ncomments to route to participants, with comments more likely to be routed if they help\nexplain position in the 2D PCA projection (opinion space), have higher overall agree-\nment ratios, low pass ratios, or are new to the conversation. This system maximizes use\nof participant time, improves engagement, and ultimately produces information which\nis more informative and easier to analyze (Small et al., 2021).\nOne thing that is not currently accounted for in this system is topicality. By mapping\ncomments to discrete topics, as described in Section 2.1, weighting can be shifted to\nboost newly emerging topics. However, LLMs can also be used to create embeddings of\ncomments, for a more continuous notion of topicality. As discussed in Section 3.3, we\nwould like for comment routing to be able to take over the task of handling duplicate\ncomments from moderators. While we found in our work that topic embeddings failed to\ncapture the directionality of sentiment (i.e. A and \u00acA embeddings are almost identical),\nthis can be disambiguated by combining the topic embedding with either raw vote data,\nassociated PCA loadings, or feedback from an LLM.\nAn even more nuanced approach to comment routing might incorporate vote pre-\ndiction (see Section 2.4). For example, we can improve utilization of participant time\nby boosting comments for which the LLM is less certain of the answer.\nMore gen-\nerally, incorporating uncertainty into comment routing is rooted in the fields of Ac-\ntive Learning (Cohn et al., 1996), also known as Sequential Design of Experiment in\nstatistics, Bayesian Optimisation using Gaussian Processes (Rasmussen, 2006), Explo-\nration/Exploitation in Reinforcement Learning and Bandits Algorithms Sutton and\nBarto (1998). We can build on this bedrock of methods with the added capabilities\nbrought by LLMs.\nFor larger conversations, it would be prohibitively expensive to\nquery the LLM for every unanswered question, after every new vote, so some care\nwould need to be taken in using this approach judiciously.\nHowever, vote prediction holds the potential to improve comment routing more\nindirectly by relaxing a key constraint on how it functions. In particular, it has been a\nlong standing assumption that comment routing should be implemented so that at any\ngiven point in time, the relative probability of receiving a particular comment is the same\nfor all individuals in the conversation. The reason is that if we started differentiating\ncomment routing on a participant by participant basis, there is the chance that there\nwill be comments which are routed to participants in one opinion group more frequently\nthan another, which confounds interpretation of conversation wide totals.\nIf we are able to infer missing data, and present more robust estimates of conversation-\nwide tallies in reporting, this frees up the comment routing algorithm to be more tailored\non a participant by participant basis. Aside from routing comments based on uncer-\ntainty in how participants will respond, this also unlocks the potential for considering\n27\nhow the order of comments affects state of mind (as evidenced by, e.g. Lakoff et al.,\n2004; Lakoff, 2009), and in particular, propensity to either polarize or find common\nground.\nGoing into the details of how precisely the weighting algorithm should be modified\nto account for this information is outside the scope of this paper, as it\u2019s a complex\nproblem deserving of a comprehensive treatment (and which will likely benefit from\nfeatures currently in progress, such as allowing participants to indicate which comments\nare most important to them). We nevertheless anticipate that a combination of these\nideas could vastly improve the system, and reduce the burden placed on moderators\nand participants.\n3.4.4\nConversation Simulation\nIn Section 2.4.2 we strongly denounced the substitution of machine simulations for\nhuman participation when gauging public opinion. However, there are valid use cases\nfor simulated deliberation, where results are not assumed to be an accurate reflection\nof the public will.\nSynthetic data are commonly used in software engineering for testing system behav-\nior and scalability. Ad-hoc, fixed datasets reflecting particular scenarios are often used\nfor this purpose, but by virtue of being fixed, are limited in their ability to explore the\nstate space and discover edge cases. For these reasons, generative testing and property-\nbased testing have emerged as solutions which involve stochastically generating data to\nmore thoroughly explore the possible state space (Claessen and Hughes, 2000; Pacheco\nand Ernst, 2007).\nThis technique could also be used as a deliberative or sociological \u201ctest bench\u201d for\nrapidly and cheaply exploring how Polis conversations might unfold in specific situations.\nFollowing Park et al. (2022), an entire Polis conversation could be simulated using an\nLLM-generated collection of participants\u2014each with unique backgrounds, expertise,\nand ideological alignments\u2014who comment and vote in response to a prompt.\nThe\nsocial dynamic of a simulation can thus be controlled to evaluate the behaviour of Polis\nin any number of scenarios, from extreme polarization, to high ideological uniformity.\nThis functionality could be used for fine-tuning new features, training facilitators,\nor building intuition about how conversations might unfold in particular situations, in\norder to formulate hypotheses that are then tested in the real world.\nHowever, we\nagain stress that it must not be used to replace human deliberation or make any social\ninference, but rather to allow researchers to iterate and experiment more quickly and\ncost-effectively on deliberation technologies, with all the appropriate caveats as to how\nperformance on a simulation carries to the real world.\nIt is possible to point here to a deeper and more structural problem in the innovation\nof technology intended for the advancement of democratic behavior: certain countries\nand cities are willing to serve as a test bench, but these so called \u201ddemocracy labs\u201d have\nlimited bandwidth and political will that must be fought for. Reducing the cycle time\nfor innovation in political technology is a laudable goal.\n28\n3.4.5\nReframing Comments for Specific Opinion Groups\nOn very divided topics of conversation, where differing ideological perspectives frustrate\neach side\u2019s ability to understand the other, LLMs may be able to re-frame individual\ncomments or summaries in a way that better resonates with the other side. This gen-\nerally fits within the context of conversation facilitation (see Section 2.3). Our initial\nexperimentation with this idea produced very sub-optimal results (omitted for the sake\nof brevity), but it\u2019s possible that more careful prompt engineering or finely tuned models\ncould help participants build shared understanding. However, the implications of this\nwork should be carefully considered, in particular the potential for erasure of historically\ndisenfranchised voices and perspectives.\n3.4.6\nAuthor Assistance\nEven before the advent of LLMs, NLP technology has been capable of providing nuanced\nstylistic feedback to authors based on their own writing (e.g. grammarly.com). LLMs\nhave taken this ability to another level, able to imitate any style it might be familiar\nwith from its training corpus, from Shakespeare to Thoreau.\nWe can therefore consider how stylistic or editorial suggestions might improve de-\nliberative applications like Polis, alongside risks of distorting the meaning of people\u2019s\ncommunications by forcing them towards imaginary, oppressive norms and erasing ver-\nnacular expression. For example, suggestions might be made based on the following:\n\u2022 Whether a comment has compound ideas which would be more effective in the\nsystem broken up into separate comments (avoiding a common issue where com-\npound ideas elicit more passes when participants agree with one point, but not\nothers)\n\u2022 Notify the participant when their ideas seem unclear or abusive, and thus at risk\nof being moderated out\n\u2022 How to make the comment more likely to be approved of globally, and thus have\na higher chance of emerging as group informed consensus\n\u2022 How to make the comment more appealing towards a specific opinion group,\nwhether the authors own, or another\n\u2022 Generally offer suggestions for improving grammar or fixing typos\nBoth the risks and rewards associated with this class of features lies in the potential\nfor an LLM to steer a discussion away from the specific framing that naturally comes\nto participants.\nPolis has an ethnographic disposition; the idea is to elicit people\u2019s\nstandpoint in their own words and phrasing. The risk is that important perspectives or\ntones might get erased, e.g. any group being biased away from a mode of speech which\nmost resonates with them and other members of their community. The potential benefit\nthough is that it may be easier for people to understand each other, and come to con-\nsensus. Care must be taken in considering how to do this without nudging participants\n29\ntowards an artificial consensus. Of these potential interventions, LLM-generated sug-\ngestions about how to break up compound ideas into separate comments may therefore\nbe the safest.\n3.4.7\nUniversal Translation Layer\nAs the technology improves, we expect that LLMs will provide a much improved univer-\nsal translation layer (including nuance, domain specificity and intent) which will have\nprofound implications for multi-language dialogue (see for example Vincent (2022)).\nConsider a real-time dialogue among many countries of the EU happening at a scale\nwhere human translators are not available, except to randomly spot check the LLM\nfor quality assurance. Such a capability has the potential to address the linguistic in-\njustice of people marginalized from political representation due to language barriers or\nlanguage hegemony.\n3.5\nKey Techniques in Application of LLMs\nThis paper has been the result of collaborations between The Computational Democracy\nProject, which broadly applies computational and machine learning methods towards\nsolving problems of participatory democracy, and Anthropic, which works to build more\nreliable, steerable and interpretable AI systems. Working together, we identified the\nfollowing LLM usage patterns that we hope may help anyone looking to employ LLMs\nfor deliberative technology:\n\u2022 Iterative or recursive compilation of information to get around context\nwindow limits: This was for example key in our approach to topic-modelling, and\nwe anticipate may be useful in aspects of summarization and content generation\nwith LLMs constrained by context window length.\n\u2022 Using probability distributions instead of single answers: When answering\nbasic prompts for e.g. voting or moderation options, this technique gives a much\nmore robust picture of how confident the LLM is about possible responses. This\ncan help bridge LLMs with the broad existing probabilistic approach to machine\nlearning (See e.g. the textbook trio Murphy, 2012, 2022, 2023). However, most\ncommercial APIs, including Claude\u2019s, do not provide these probabilities at the\ntime of writing.\n\u2022 Chain of thought reasoning: Asking LLMs to explain themselves proved valu-\nable for improving their results and understanding their behavior.\n3.6\nConclusion\nThe advent of technologies like Polis provides a powerful platform for collective self-\nrepresentation, facilitating the construction of an interactive model of the public opinion\nlandscape.\nHistorically, elected representatives have served as a conduit for public opinion,\n\u201csummarizing\u201d, in a manner of speaking, the voices of their constituents. This pro-\ncess often occurs through qualitative conversations in town halls and private meetings\n30\nwith industry stakeholders. These representatives function as embodied information\ncompression, distilling broad public sentiment into digestible messages.\nIn parallel,\nresearch institutions like Pew Research conduct independent public opinion polling.\nThis involves surveying a segment of the population, summarizing their responses, and\nthereby providing another form of representation.\nIn both scenarios, there exists a summarizing mechanism that mediates between a\npopulation and an end result, whether it be a politician\u2019s soundbite, an article informed\nby research, or, in this case a Polis report. This inherent property of representative\ndemocracy allows for the coexistence of heterogeneous, often conflicting summaries \u2013\ntextual intermediaries that encapsulate a diverse range of public will.\nThe advent of transformer-based Large Language Models (LLMs) introduces an-\nother layer to this dynamic. With their attention mechanisms, it may eventually be\npossible to use these models to formally quantify aspects of the summarization process\nin unprecedented ways, though currently there is debate about whether these mecha-\nnisms can be relied on for interpretability (Jain and Wallace, 2019; Wiegreffe and Pinter,\n2019). However, our experiments in 2.2.2 suggest that LLMs can dramatically reduce\nthe cost of evaluating summaries and identifying omissions, increasing accountability\nand transparency relative to existing summarization systems.\nThe capability of LLMs to create coherent summaries from a multitude of indi-\nvidual participant-written statements further enhances the process facilitated by Polis.\nThis introduces a feedback loop that hasn\u2019t been available to citizens who are being\n\u201csummarized\u201d by other means. This potentially recursive self-summarization could be\nrevolutionary, providing continuous, adaptive feedback and resulting in a more accurate\nrepresentation of a population\u2019s views by the population itself.\nIn essence, the integration of LLMs in platforms like Polis can introduce a new era in\npublic self-representation. It can ensure that the voices previously ignored or drowned\nout in the process are heard, thus making democracy more inclusive, adaptive, and\nreflective of its constituents.\nIn conclusion, the use of LLMs can significantly improve the productivity and ef-\nficiency of Polis discussions. LLMs can generate prompts to guide the conversation,\nidentify patterns and trends in the discussion, and provide a real-time summary to the\nhuman facilitator. In doing all of this, they have the potential to significantly reduce\nthe burden of human labor required to facilitate this kind of engagement, therefore\nmaking it possible to roll them out more frequently and in more diverse contexts. How-\never, the details of how this is accomplished are vital, and it is imperative for safety\nand legitimacy that, as a baseline, there remain an oversight role for humans in the\nloop wherever LLMs are employed in this setting. Additional research exploring safe-\nguards and metrics for gauging outcomes is also absolutely necessary to ensure positive\noutcomes.\nReferences\nThe Society Library, Mar. 2023. URL https://www.societylibrary.org.\nA. Abid, M. Farooqi, and J. Zou. Persistent anti-muslim bias in large language models.\nIn Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES\n31\n\u201921, page 298\u2013306, New York, NY, USA, 2021a. Association for Computing Machinery.\nISBN 9781450384735. doi: 10.1145/3461702.3462624. URL https://doi.org/10.\n1145/3461702.3462624.\nA. Abid, M. Farooqi, and J. Zou.\nLarge language models associate Muslims with\nviolence.\nNature Machine Intelligence, 3(6):461\u2013463, June 2021b.\nISSN 2522-\n5839. doi: 10.1038/s42256-021-00359-2. URL https://www.nature.com/articles/\ns42256-021-00359-2. Number: 6 Publisher: Nature Publishing Group.\nA. Applebaum and P. Pomerantsev.\nHow to Put Out Democracy\u2019s Dumpster Fire.\nThe Atlantic, Mar. 2021. URL https://www.theatlantic.com/magazine/archive/\n2021/04/the-internet-doesnt-have-to-be-awful/618079/. Section: Ideas.\nA. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph,\nB. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion,\nK. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, and\nJ. Kaplan. A General Language Assistant as a Laboratory for Alignment, Dec. 2021.\nURL http://arxiv.org/abs/2112.00861. arXiv:2112.00861 [cs].\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort,\nD. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-\nShowk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston,\nS. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. Mc-\nCandlish, C. Olah, B. Mann, and J. Kaplan.\nTraining a Helpful and Harmless\nAssistant with Reinforcement Learning from Human Feedback, Apr. 2022.\nURL\nhttp://arxiv.org/abs/2204.05862. arXiv:2204.05862 [cs].\nM. A. Bakker, M. J. Chadwick, H. R. Sheahan, M. H. Tessler, L. Campbell-Gillingham,\nJ. Balaguer, N. McAlesse, A. Glaese, J. Aslanides, M. M. Botvinick, and C. Sum-\nmerfield. Fine-tuning language models to find agreement among humans with diverse\npreferences, Nov. 2022. URL https://arxiv.org/abs/2211.15006.\nE. Barry.\nTownhall meeting in Kentucky turns tables on polarization, 2023.\nURL\nhttps://compdemocracy.org/Case-studies/2018-kentucky/.\nC. Basta, M. R. Costa-juss`a, and N. Casas. Evaluating the Underlying Gender Bias in\nContextualized Word Embeddings, Apr. 2019. URL http://arxiv.org/abs/1904.\n08783. arXiv:1904.08783 [cs].\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of\nstochastic parrots: Can language models be too big?\nIn Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page\n610\u2013623, New York, NY, USA, 2021a. Association for Computing Machinery. ISBN\n9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/\n3442188.3445922.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the Dangers of\nStochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, pages\n32\n610\u2013623, New York, NY, USA, Mar. 2021b. Association for Computing Machinery.\nISBN 978-1-4503-8309-7.\ndoi: 10.1145/3442188.3445922.\nURL https://dl.acm.\norg/doi/10.1145/3442188.3445922.\nJ. Bilich, M. Varga, D. Masood, and A. Konya. Faster Peace via Inclusivity: An Efficient\nParadigm to Understand Populations in Conflict Zones. AI for Social Good workshop\nat NeurIPS, page 6, 2019.\nR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bern-\nstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castel-\nlon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doum-\nbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn,\nT. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto,\nP. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain,\nD. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W.\nKoh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee,\nJ. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchan-\ndani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman,\nA. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou,\nJ. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren,\nF. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. R\u00b4e, D. Sadigh, S. Sagawa, K. Santhanam,\nA. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tram`er, R. E. Wang,\nW. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia,\nM. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the\nopportunities and risks of foundation models, 2022.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, and A. Askell. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\nM. Cheng, E. Durmus, and D. Jurafsky. Marked personas: Using natural language\nprompts to measure stereotypes in language models, 2023.\nK.-L. Chiu, A. Collins, and R. Alexander. Detecting Hate Speech with GPT-3, Mar.\n2022. URL http://arxiv.org/abs/2103.12407. arXiv:2103.12407 [cs].\nE. Chu, J. Andreas, S. Ansolabehere, and D. Roy. Language Models Trained on Media\nDiets Can Predict Public Opinion, Mar. 2023. URL http://arxiv.org/abs/2303.\n16779. arXiv:2303.16779 [cs].\nK. Claessen and J. Hughes.\nQuickCheck: a lightweight tool for random testing of\nHaskell programs. In Proceedings of the fifth ACM SIGPLAN international conference\non Functional programming, ICFP \u201900, pages 268\u2013279, New York, NY, USA, Sept.\n2000. Association for Computing Machinery. ISBN 978-1-58113-202-1. doi: 10.1145/\n351240.351266. URL https://dl.acm.org/doi/10.1145/351240.351266.\nD. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models.\nJournal of artificial intelligence research, 4:129\u2013145, 1996.\n33\nP. Coy. Opinion | Can A.I. and Democracy Fix Each Other? The New York Times, Apr.\n2023.\nISSN 0362-4331.\nURL https://www.nytimes.com/2023/04/05/opinion/\nartificial-intelligence-democracy-chatgpt.html.\nCPI.\nBuilding\nConsensus\nand\nCompromise\non\nUber\nin\nTaiwan,\nSept.\n2019.\nURL\nhttps://www.centreforpublicimpact.org/case-study/\nbuilding-consensus-compromise-uber-taiwan.\nE. Durmus, H. He, and M. Diab. FEQA: A question answering evaluation framework\nfor faithfulness assessment in abstractive summarization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages 5055\u20135070,\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.454. URL https://aclanthology.org/2020.acl-main.454.\nE. Durmus, F. Ladhak, and T. Hashimoto. Spurious correlations in reference-free evalu-\nation of text generation. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 1443\u20131454, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.acl-long.102. URL https://aclanthology.org/2022.acl-long.102.\nA. R. Fabbri, W. Kry\u00b4sci\u00b4nski, B. McCann, C. Xiong, R. Socher, and D. Radev. Sum-\nmeval: Re-evaluating summarization evaluation, 2021.\nA.\nFrance-Presse.\nRomania\nPM\nunveils\nAI\n\u2018adviser\u2019\nto\ntell\nhim\nwhat\npeople\nthink\nin\nreal\ntime.\nThe\nGuardian,\nMar.\n2023.\nISSN\n0261-\n3077.\nURL\nhttps://www.theguardian.com/world/2023/mar/02/\nromania-ion-ai-government-honorary-adviser-artificial-intelligence-pm-nicolae-ciuca.\nD. Ganguli, D. Hernandez, L. Lovitt, N. DasSarma, T. Henighan, A. Jones, N. Joseph,\nJ. Kernion, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, N. El-\nhage, S. E. Showk, S. Fort, Z. Hatfield-Dodds, S. Johnston, S. Kravec, N. Nanda,\nK. Ndousse, C. Olsson, D. Amodei, D. Amodei, T. Brown, J. Kaplan, S. Mc-\nCandlish, C. Olah, and J. Clark.\nPredictability and Surprise in Large Genera-\ntive Models.\nIn 2022 ACM Conference on Fairness, Accountability, and Trans-\nparency, pages 1747\u20131764, June 2022a. doi: 10.1145/3531146.3533229. URL http:\n//arxiv.org/abs/2202.07785. arXiv:2202.07785 [cs].\nD. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez,\nN. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma,\nD. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Her-\nnandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-\nJohnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and\nJ. Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behav-\niors, and Lessons Learned, Nov. 2022b. URL http://arxiv.org/abs/2209.07858.\narXiv:2209.07858 [cs].\nZ. Ghahramani and T. L. Griffiths. Infinite latent feature models and the Indian buffet\nprocess. May 2005.\n34\nT. Goyal and G. Durrett.\nAnnotating and modeling fine-grained factuality in sum-\nmarization.\nIn Proceedings of the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language Technolo-\ngies, pages 1449\u20131462, Online, June 2021. Association for Computational Linguis-\ntics.\ndoi:\n10.18653/v1/2021.naacl-main.114.\nURL https://aclanthology.org/\n2021.naacl-main.114.\nK. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz. Not what\nyou\u2019ve signed up for: Compromising Real-World LLM-Integrated Applications with\nIndirect Prompt Injection, May 2023.\nURL http://arxiv.org/abs/2302.12173.\narXiv:2302.12173 [cs].\nT. Griffiths, M. Jordan, J. Tenenbaum, and D. Blei. Hierarchical topic models and the\nnested chinese restaurant process. Advances in neural information processing systems,\n16, 2003.\nJ. Habermas. The Structural Transformation of the Public Sphere. Polity Press, Eng-\nland, 1962. ISBN 978-0-262-58108-0.\nJ. Habermas.\nThe Theory of Communicative Action.\n1981.\nURL https:\n//en.wikipedia.org/w/index.php?title=The_Theory_of_Communicative_\nAction&oldid=1134581345. Page Version ID: 1134581345.\nS. Harding.\nThe Science Question in Feminism.\nCornell University Press,\n1986.\nURL\nhttps://www.cornellpress.cornell.edu/book/9780801418808/\nthe-science-question-in-feminism/#bookTabs=1.\nJ. Hartmann, J. Schwenzow, and M. Witte. The political ideology of conversational AI:\nConverging evidence on ChatGPT\u2019s pro-environmental, left-libertarian orientation,\nJan. 2023. URL https://papers.ssrn.com/abstract=4316084.\nC.\nHorton.\nThe\nsimple\nbut\ningenious\nsystem\nTaiwan\nuses\nto\ncrowdsource\nits\nlaws.\nMIT\nTechnology\nReview,\nAug.\n2018.\nURL\nhttps://www.technologyreview.com/2018/08/21/240284/\nthe-simple-but-ingenious-system-taiwan-uses-to-crowdsource-its-laws/.\nB. Hutchinson, V. Prabhakaran, E. Denton, K. Webster, Y. Zhong, and S. Denuyl.\nSocial Biases in NLP Models as Barriers for Persons with Disabilities. pages 5491\u2013\n5501. Association for Computational Linguistics, July 2020. doi: 10.18653/v1/2020.\nacl-main.487.\nM. S. Jahan and M. Oussalah. A systematic review of Hate Speech automatic detection\nusing Natural Language Processing, May 2021. URL http://arxiv.org/abs/2106.\n00742. arXiv:2106.00742 [cs].\nS. Jain and B. C. Wallace.\nAttention is not Explanation, May 2019.\nURL http:\n//arxiv.org/abs/1902.10186. arXiv:1902.10186 [cs].\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and\nP. Fung.\nSurvey of hallucination in natural language generation.\nACM Comput.\n35\nSurv., 55(12), mar 2023a.\nISSN 0360-0300.\ndoi: 10.1145/3571730.\nURL https:\n//doi.org/10.1145/3571730.\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and\nP. Fung. Survey of Hallucination in Natural Language Generation. ACM Computing\nSurveys, 55(12):248:1\u2013248:38, Mar. 2023b. ISSN 0360-0300. doi: 10.1145/3571730.\nURL https://doi.org/10.1145/3571730.\nA. Konya, Y. L. Qiu, M. P. Varga, and A. Ovadya. Elicitation Inference Optimization\nfor Multi-Principal-Agent Alignment. NuerIPS, 2022. URL https://openreview.\nnet/pdf?id=tkxnRPkb_H.\nW. Kryscinski, B. McCann, C. Xiong, and R. Socher. Evaluating the factual consis-\ntency of abstractive text summarization. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 9332\u20139346, On-\nline, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.750. URL https://aclanthology.org/2020.emnlp-main.750.\nK. Kurita, N. Vyas, A. Pareek, A. W. Black, and Y. Tsvetkov.\nMeasuring Bias in\nContextualized Word Representations, June 2019.\nURL http://arxiv.org/abs/\n1906.07337. arXiv:1906.07337 [cs].\nG. Lakoff. The Political Mind: A Cognitive Scientist\u2019s Guide to Your Brain and Its\nPolitics. Penguin Books, New York, NY, reprint edition edition, June 2009. ISBN\n978-0-14-311568-7.\nG. Lakoff, H. Dean, and D. Hazen. Don\u2019t Think of an Elephant!: Know Your Values and\nFrame the Debate\u2013The Essential Guide for Progressives. Chelsea Green Publishing,\nWhite River Junction, Vt, first edition edition, Sept. 2004. ISBN 978-1-931498-71-5.\nN. Lambert, L. Castricato, L. von Werra, and A. Havrilla. Illustrating Reinforcement\nLearning from Human Feedback (RLHF). Hugging Face Blog, 2022. URL https:\n//huggingface.co/blog/rlhf.\nH. Landemore.\nOpen Democracy:\nReinventing Popular Rule for the Twenty-First\nCentury.\nPrinceton University Press, Princeton, Mar. 2022.\nISBN 978-0-691-\n21239-5. URL https://press.princeton.edu/books/hardcover/9780691181998/\nopen-democracy.\nM. Lee, M. Srivastava, A. Hardy, J. Thickstun, E. Durmus, A. Paranjape, I. Gerard-\nUrsin, X. L. Li, F. Ladhak, F. Rong, R. E. Wang, M. Kwon, J. S. Park, H. Cao,\nT. Lee, R. Bommasani, M. Bernstein, and P. Liang.\nEvaluating human-language\nmodel interaction, 2022.\nS. MacAvaney, H.-R. Yao, E. Yang, K. Russell, N. Goharian, and O. Frieder. Hate speech\ndetection: Challenges and solutions. PLOS ONE, 14(8):e0221152, Aug. 2019. ISSN\n1932-6203. doi: 10.1371/journal.pone.0221152. URL https://journals.plos.org/\nplosone/article?id=10.1371/journal.pone.0221152. Publisher: Public Library\nof Science.\n36\nJ. B. MacQueen.\nSome methods for classification and analysis of multivariate\nobservations.\nIn Proceedings of the Fifth Berkeley Symposium on Mathemat-\nical Statistics and Probability, Volume 1:\nStatistics, volume 5.1, pages 281\u2013\n298. University of California Press, 1967.\nURL https://projecteuclid.org/\nebooks/berkeley-symposium-on-mathematical-statistics-and-probability/\nProceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/\nchapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/\nbsmsp/1200512992.\nJ. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and factual-\nity in abstractive summarization. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 1906\u20131919, Online, July 2020. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.173. URL\nhttps://aclanthology.org/2020.acl-main.173.\nJ. McKenzie.\nTesting Tech for Consensus in a Purple Town | Civicist, Mar. 2018.\nURL https://web.archive.org/web/20210414093745/https://civichall.org/\ncivicist/testing-tech-consensus-purple-town/. Cached archive.\nC. Miller and M. Anubi.\nHow to fix democracy.\nURL https://www.bbc.co.uk/\nprogrammes/p0crtyww.\nK. P. Murphy. Machine learning: a probabilistic perspective. Adaptive computation and\nmachine learning series. MIT Press, Cambridge, MA, 2012. ISBN 9780262018029.\nK. P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL\nprobml.ai.\nK. P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023.\nURL http://probml.github.io/book2.\nOpenAI. GPT-4 Technical Report, Mar. 2023. URL http://arxiv.org/abs/2303.\n08774. arXiv:2303.08774 [cs].\nC. Pacheco and M. D. Ernst. Randoop: feedback-directed random testing for Java. In\nCompanion to the 22nd ACM SIGPLAN conference on Object-oriented programming\nsystems and applications companion, OOPSLA \u201907, pages 815\u2013816, New York, NY,\nUSA, Oct. 2007. Association for Computing Machinery. ISBN 978-1-59593-865-7. doi:\n10.1145/1297846.1297902. URL https://doi.org/10.1145/1297846.1297902.\nJ. S. Park, L. Popowski, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Social\nsimulacra: Creating populated prototypes for social computing systems. In In the\n35th Annual ACM Symposium on User Interface Software and Technology (UIST\n\u201922), UIST \u201922, New York, NY, USA, 2022. Association for Computing Machinery.\nISBN 9781450393201. doi: 10.1145/3526113.3545616. URL https://doi.org/10.\n1145/3526113.3545616.\nK. Pearson.\nOn lines and planes of closest fit to systems of points in space.\nThe London, Edinburgh, and Dublin Philosophical Magazine and Journal of Sci-\nence,\n2(11):559\u2013572,\n1901.\ndoi:\n10.1080/14786440109462720.\nURL https:\n37\n//doi.org/10.1080/14786440109462720.\nPublisher:\nTaylor & Francis\neprint:\nhttps://doi.org/10.1080/14786440109462720.\nF. Perez and I. Ribeiro. Ignore Previous Prompt: Attack Techniques For Language\nModels, Nov. 2022.\nURL http://arxiv.org/abs/2211.09527.\narXiv:2211.09527\n[cs].\nC. E. Rasmussen. Gaussian processes for machine learning. 2006.\nP. Robert and Y. Escoufier. A Unifying Tool for Linear Multivariate Statistical Meth-\nods: The RV- Coefficient. Journal of the Royal Statistical Society. Series C (Ap-\nplied Statistics), 25(3):257\u2013265, 1976. ISSN 0035-9254. doi: 10.2307/2347233. URL\nhttps://www.jstor.org/stable/2347233. Publisher: [Wiley, Royal Statistical So-\nciety].\nM. B. Rosenberg.\nNonviolent communication : a language of life.\nEncinitas, CA :\nPuddleDancer Press, 2005. ISBN 978-1-892005-03-8. URL http://archive.org/\ndetails/isbn_9781892005038.\nS. Santurkar, E. Durmus, F. Ladhak, C. Lee, P. Liang, and T. Hashimoto.\nWhose\nOpinions Do Language Models Reflect?, Mar. 2023. URL http://arxiv.org/abs/\n2303.17548. arXiv:2303.17548 [cs].\nM. Sap, S. Gabriel, L. Qin, D. Jurafsky, N. A. Smith, and Y. Choi. Social Bias Frames:\nReasoning about Social and Power Implications of Language, Apr. 2020. URL http:\n//arxiv.org/abs/1911.03891. arXiv:1911.03891 [cs].\nJ. Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. Ceron Uribe, L. Fe-\ndus, L. Metz, M. Pokorny, R. Gontijo Lopes, S. Zhao, A. Vijayvergiya, E. Sigler,\nA. Perelman, C. Voss, M. Heaton, J. Parish, D. Cummings, R. Nayak, V. Balcom,\nD. Schnurr, T. Kaftan, C. Hallacy, N. Turley, N. Deutsch, V. Goel, J. Ward, A. Kon-\nstantinidis, W. Zaremba, L. Ouyang, L. Bogdonoff, J. Gross, D. Medina, S. Yoo,\nT. Lee, R. Lowe, D. Mossing, J. Huizinga, R. Jiang, C. Wainwright, D. Almeida,\nS. Lin, M. Zhang, K. Xiao, K. Slama, S. Bills, A. Gray, J. Leike, J. Pachocki,\nP. Tillet, S. Jain, G. Brockman, N. Ryder, A. Paino, Q. Yuan, C. Winter, B. Wang,\nM. Bavarian, I. Babuschkin, S. Sidor, I. Kanitscheider, M. Pavlov, M. Plappert,\nN. Tezak, H. Jun, W. Zhuk, V. Pong, L. Kaiser, J. Tworek, A. Carr, L. Weng,\nS. Agarwal, K. Cobbe, V. Kosaraju, A. Power, S. Polu, J. Han, R. Puri, S. Jain,\nB. Chess, C. Gibson, O. Boiko, E. Parparita, A. Tootoonchian, K. Kosic, and\nC. Hesse. ChatGPT: Optimizing Language Models for Dialogue, Nov. 2022. URL\nhttps://openai.com/blog/chatgpt/.\nD. Sergent.\nFirst-ever civic assembly gives residents chance to be heard.\nBowl-\ning\nGreen\nDaily\nNews,\nFeb.\n2018.\nURL\nhttps://www.bgdailynews.com/\nnews/first-ever-civic-assembly-gives-residents-chance-to-be-heard/\narticle_0a17254e-a8bb-5f4f-884f-9d0617ab9c08.html.\nauthor email:\ndser-\ngent@bgdailynews.com.\n38\nC. Singh, J. X. Morris, J. Aneja, A. M. Rush, and J. Gao. Explaining Patterns in Data\nwith Language Models via Interpretable Autoprompting, Jan. 2023.\nURL http:\n//arxiv.org/abs/2210.01848. arXiv:2210.01848 [cs, q-bio, stat].\nC. Small, M. Bjorkegren, T. Erkkil\u00a8a, L. Shaw, and C. Megill. Polis: Scaling Delib-\neration by Mapping High Dimensional Opinion Spaces. Recerca: Revista de Pen-\nsament i An`alisi, 26(2), 2021.\ndoi:\nhttps://doi.org/10.6035/recerca.5516.\nURL\nhttps://www.proquest.com/docview/2610037205. Publisher: Universitat Jaume\nI Servei de Comunicacio i Publicacions.\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei,\nand P. F. Christiano. Learning to summarize with human feedback. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 3008\u20133021. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1f89885d556929e98d3ef9b86448f951-Paper.pdf.\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Cambridge,\nMA: MIT Press, 1998.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere,\nN. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample.\nLLaMA: Open and Efficient Foundation Language Models, Feb. 2023. URL http:\n//arxiv.org/abs/2302.13971. arXiv:2302.13971 [cs].\nJ. Vincent. Google plans giant AI language model supporting world\u2019s 1,000 most spo-\nken languages, Nov. 2022. URL https://www.theverge.com/2022/11/2/23434360/\ngoogle-1000-languages-initiative-ai-llm-research-project.\nB. Wei, J. Li, A. Gupta, H. Umair, A. Vovor, and N. Durzynski. Offensive Language\nand Hate Speech Detection with Deep Learning and Transfer Learning, Aug. 2021.\nURL http://arxiv.org/abs/2108.03305. arXiv:2108.03305 [cs].\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng,\nM. Glaese, B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton,\nC. Biles, A. Birhane, J. Haas, L. Rimell, L. A. Hendricks, W. Isaac, S. Legassick,\nG. Irving, and I. Gabriel. Ethical and social risks of harm from Language Models,\nDec. 2021. URL http://arxiv.org/abs/2112.04359. arXiv:2112.04359 [cs].\nS. Wiegreffe and Y. Pinter. Attention is not not Explanation, Sept. 2019. URL http:\n//arxiv.org/abs/1908.04626. arXiv:1908.04626 [cs].\nS. Wojcik, S. Hilgard, N. Judd, D. Mocanu, S. Ragain, K. Coleman, M. B. F.\nHunzaker,\nand J. Baxter.\nBirdwatch:\nCrowd Wisdom and Bridging Al-\ngorithms\ncan\nInform\nUnderstanding\nand\nReduce\nthe\nSpread\nof\nMisinfor-\nmation,\nOct.\n2022.\nURL\nhttps://github.com/twitter/birdwatch/blob/\na7b8c8a1492eb930267f84578e7ebebefe8e8aef/birdwatch_paper_2022_10_27.\npdf.\n39\nQ. Yang, J. Suh, N.-C. Chen, and G. Ramos. Grounding interactive machine learning\ntool design in how non-experts actually build models. In Proceedings of the 2018\nDesigning Interactive Systems Conference, DIS \u201918, page 573\u2013584, New York, NY,\nUSA, 2018. Association for Computing Machinery. ISBN 9781450351980. doi: 10.\n1145/3196709.3196729. URL https://doi.org/10.1145/3196709.3196729.\nA. X. Zhang, L. Verou, and D. Karger.\nWikum: Bridging Discussion Forums and\nWikis Using Recursive Summarization. In Proceedings of the 2017 ACM Conference\non Computer Supported Cooperative Work and Social Computing, CSCW \u201917, pages\n2082\u20132096, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4335-0. doi: 10.1145/\n2998181.2998235.\nURL http://doi.acm.org/10.1145/2998181.2998235.\nevent-\nplace: Portland, Oregon, USA.\nT. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto.\nBenchmarking Large Language Models for News Summarization, Jan. 2023. URL\nhttp://arxiv.org/abs/2301.13848. arXiv:2301.13848 [cs].\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano,\nand G. Irving. Fine-Tuning Language Models from Human Preferences, Jan. 2020.\nURL http://arxiv.org/abs/1909.08593. arXiv:1909.08593 [cs, stat].\nA\nAppendix\nWe provide here more details about the prompts used for the experiments of each section.\nThe actual code has been developed using an internal API of Claude which provides\nmore details than the external API, such as probability distribution over the output\ncontent. It is therefore somewhat difficult to share the code. However, given that the\npurpose of this article is more qualitative than quantitative, we hope that the limited\nreproducibility provided by the prompts below should be sufficient to understand the\nmain points.\nWe use the models available in the public Claude API as of March 15th, 2023, with\nthe exception of the large context window experiments, which used the more recent\n100k-token context window LLM as of June 12th, 2023.\nA.1\nTopic modelling\nFirst pass topic modeling prompt (50 comments at a time due to context window limi-\ntations):\nPrompt:\nHuman: I want you to do topic modeling on the given comments. Print the\ndetected topics line by line.\nHere are the comments:\n{comments}\n40\nAssistant: Sure,\nI can do that. Here are the main topics I detected in\nthese comments:\nWe then ask the LLMs to merge the topics generated in the previous step:\nPrompt:\nHuman: I want you merge the given lists of topics into smallest set of\ntopics that are comprehensive.\nHere are the lists of topics:\n{topics}\nAssistant: Sure, I can do that. Here is my attempt at merging the list of\ntopics you have provided:\nThe full response to this was as follows:\nClaude\u2019s output:\nGROUP\n0: Local\nGovernment\nand\nPolicy\n: Government\naccountability,\nwaste\nreduction,\ntransparency,\nPlanning and zoning,\nPermits,\nLaws and regula-\ntions on businesses, landlords/tenants, traffic etc., Taxes and services,\nAnnexed areas and districts\nGROUP 1: Infrastructure and Development : Traffic and transportation in-\ncluding\nroad\ninfrastructure,\ntraffic\nflow,\ntransit,\nparking,\nsidewalks\netc.,\nUnderground\nutilities\nand\npower\nlines,\nHousing\nincluding\nafford-\nable housing and housing regulations, Historic preservation, Urban sprawl,\nDowntown\ndevelopment\nincluding\nrevitalizing\ndowntown,\nattracting\nbusi-\nnesses and tenants\nGROUP 2: Public Services : Healthcare including access to healthcare, opi-\noid\ncrisis,\nmental\nhealth\nservices\netc.,\nEducation\nincluding\nschools,\nafter-school programs,\ncurriculum,\nfunding etc.,\nPublic spaces like li-\nbraries,\nparks,\nrecreation\ncenters,\nswimming\npools\netc.,\nUtilities\nin-\ncluding internet access,\nelectricity,\ncable etc.,\nHomeless shelters and\nservices, Accessibility and disability access\nGROUP\n3: Safety,\nHealth\nand\nQuality\nof\nLife\n: Law\nenforcement\ninclud-\ning traffic enforcement, noise ordinances, loitering etc., Public health\nincluding healthy lifestyle promotion,\nfood access,\npollution etc.,\nAn-\nimal control including ordinances on chaining dogs,\nstray animals etc.,\nEmergency\nmanagement\nincluding\nweather\nemergency\nplanning\nand\nshelters,\nEnvironmental\nissues\nlike\nrecycling,\nsustainability,\nsolar\npower\netc.,\nSupport for marginalized groups like LGBTQ+ people, immigrants, disabled\npeople etc.\nGROUP 4: Economy and Business : Job opportunities, workforce development\nand\nfair\nwages,\nSupport\nfor\nlocal\nbusinesses\nlike\nsmall\nbusinesses,\nfarmers, downtown businesses etc., Lack of competition in industries like\ngrocery stores, Legalization of marijuana\n41\nBased on these lists, here is a comprehensive list of topics:\nGovernment\nand\nPublic\nPolicy: Local\ngovernment\nand\npolitics,\nLaws\nand\nregulations, Taxes and services, Transparency and accountability Infras-\ntructure\nand\nDevelopment: Housing,\nTransportation,\nUtilities,\nHistoric\npreservation, Urban planning Public Services: Healthcare, Education, Pub-\nlic\nspaces,\nHomeless\nservices,\nDisability\naccess\nSafety,\nHealth\nand\nEnvironment: Law\nenforcement,\nPublic\nhealth,\nPollution,\nEmergency\nman-\nagement,\nMarginalized\ngroups,\nAnimal\ncontrol\nEconomy\nand\nBusiness: Job\nopportunities, Local business support, Competition and legalization issues\nA.2\nSummarization\nSummarizing main points and consensus by providing comments and number of agree\nand disagree votes overall:\nPrompt:\nHuman: In each line, I provide you with comments and percentage of votes\nthat agreed and disagreed with them for Group 0 and Group 1. I want you\nto do topic modeling on the given comments. Print the detected topics line\nby line. At the end, generate an overall summary of the comments. In the\nsummary, make sure to include information and quantification on how much\nagreement versus disagreement there was across Group 0 and Group 1 for\ndifferent topics.\nHere are the comments:\n{comments}\nAssistant: Sure,\nI can do that. Here are the main topics I detected in\nthese comments:\nComments for the first experiment were passed in in the form\nInput:\nComment: {comment-text} == Number of agree votes:{agree-count} == Number of\ndisagree votes:{disagree-count}\nFor the second experiment, they were passed in the form\nInput:\nComment: {comment-text} == Group 0 percentage agreement:{g0-agree-percent}%\n== Group 0 percentage disagreement:{g0-disagree-percent}% == Group 1 per-\ncentage agreement:{g1-agree-percent}% == Group 1 percentage disagreement:\n{g1-disagree-percent}%\nA.3\nGroup Identity and Consensus\n42\nPrompt:\nGiven the following summary of various comments from a platform, can you\ngenerate novel comments that are likely to get consensus:\nSummary: In summary, there was general agreement on issues around traffic,\npublic\namenities,\nmedia/technology,\nurban\nplanning,\nand\ntransportation.\nViews were more mixed on schools, neighborhood issues, healthcare, eco-\nnomics, and community support. A few lighthearted or isolated comments did\nnot show clear agreement or disagreement. Overall, the comments indicated\ninterest in improving infrastructure, quality of life, and community sup-\nport in the area. But there were some polarizing views on select issues\nlike immigrant language ability or reactions to the opioid crisis.\nA.4\nPrediction\nModel used for prediction: 170B parameter plain LLM, no fine-tuning or RLHF. We\npredict user votes using the preceding vote history, using at most the last 30 votes due\nto context window limitations.\nAn example prompt used for prediction follows - in this case with 2 votes in the\nhistory.\nPrompt:\nWhat\nfollows\nis\nthe\nhistory\nof\na\nparticular\nuser\u2019s\nvotes\non\nvarious\ncomments submitted to a digital town hall.\nEach vote is commented as follows.\nComment: {comment-text}\nVote: {Agree|Disagree}\n<history>\nComment: Local\nschools\nshould\nhelp\nmake\nup\nthe\npension\nfund\ncrisis\nby\nlearning to live with budget cuts.\nVote: Disagree\nComment: University management bears more responsibility than federal and\nstate education funding cuts for tuition rising so much over the years.\nVote: Disagree\nComment: Officials\nthat\nuse\nsocial\nmedia\naccounts\nin\na\nprofessional\ncapacity should not be allowed to block constituents except under rare\nconditions like being threatened.\nVote:\nA.5\nMeasuring bias\nThe following prompt was used to ask Claude to vote on comments:\n43\nPrompt:\nHuman: The goal of the town hall was to give members of the community\na framework in which they could articulate and share concerns, prompted\nby the topic: \u2019What do you believe should change in Bowling Green/Warren\nCounty in order to make it a better place to work, live, and spend time?\u2019.\nFor this topic, do you agree/disagree/pass the following comment:\n\"{comment-text}\"\nOptions:\n(A) Pass\n(B) Disagree\n(C) Agree\nAssistant: If had to select one of the options, my answer would be (\nA.6\nEvaluating Summaries with LLMs\nThe following prompt was used to evaluate how well a summary represents a comment:\nPrompt:\nHuman: I\u2019m going to ask you to evaluate whether a comment by a digital\ntown hall participant is represented in a summary of the town hall.\nHere is the summary:\n<summary>\n{summary}\n</summary>\nHere is the comment:\n<comment>\n{comment}\n</comment>\nHow well is the comment represented in the summary? Would someone who had\nread the summary gain new information as a result of reading the comment?\nPlease choose one of the following options:\n(1) The comment is not represented at all; the summary ignored the comment\nentirely.\n(2)\nThe\nsummary\ncontains\nsome\nmaterial\nrelevant\nto\nthe\ncomment\nbut\nis\nmissing most of the content.\n(3) The summary substantially represents the comment but is still missing\nsomething important.\n(4) The summary covers most of the information in the comment,\nbut is\nmissing some nuance or detail.\n(5) The summary covers all the information in the comment.\n44\nPlease respond with one of these options.\nAssistant: All things considered, the best answer is (\nA.7\nLong Context Window Experiments\nThe following prompt was used for summarization with topic modelling as an explicit\nprompt instruction:\nPrompt:\nHuman: Perform topic modeling on the provided comments and print the de-\ntected topics line-by-line. At the end, summarize the key topics discussed\nand quantify the level of agreement vs disagreement between Group 0 and\nGroup 1. Specifically, note which topics had high agreement or disagree-\nment. Provide an overview of the difference in opinion between the two\ngroups based on the topic modeling analysis.\nHere\nare\nthe\ncomments: Comment: {comment-text}\n==\nGroup\n0\npercentage\nagreement: {g0-agree-percent}%\n==\nGroup\n0\npercentage\ndisagreement: {g0-\ndisagree-percent}% == Group 1 percentage agreement: {g1-agree-percent}% ==\nGroup 1 percentage disagreement: {g1-disagree-percent}%\n....... [MORE COMMENTS] .......\nAssistant: Sure,\nI can do that. Here are the main topics I detected in\nthese comments:\nFor summarization without explicit topic modelling as an intermediary step in the\nprompt:\nPrompt:\nHuman: Given the comments and agreement/disagreement votes for each com-\nment\nfrom\ntwo\ngroups\n(Group\n0\nand\nGroup\n1),\nsummarize\nthe\nkey\npoints\nof consensus and divergence between Group 0 and Group 1. Identify what\nopinions the two groups have consensus on versus where their views differ.\nHere\nare\nthe\ncomments: Comment: {comment-text}\n==\nGroup\n0\npercentage\nagreement: {g0-agree-percent}%\n==\nGroup\n0\npercentage\ndisagreement: {g0-\ndisagree-percent}% == Group 1 percentage agreement: {g1-agree-percent}% ==\nGroup 1 percentage disagreement: {g1-disagree-percent}%\n....... [MORE COMMENTS] .......\nAssistant: Sure, I can do that.\n45\n"
  },
  {
    "title": "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations",
    "link": "https://arxiv.org/pdf/2306.12059.pdf",
    "upvote": "5",
    "text": "Published as a conference paper at ICLR 2024\nEQUIFORMERV2: IMPROVED EQUIVARIANT TRANSFORMER\nFOR SCALING TO HIGHER-DEGREE REPRESENTATIONS\nYi-Lun Liao1\nBrandon Wood2\nAbhishek Das2\u2039\nTess Smidt1\u2039\n1Massachusetts Institute of Technology\n2FAIR, Meta\n\u2039Equal contribution\n{ylliao,tsmidt}@mit.edu\n{bmwood,abhshkdz}@meta.com\nhttps://github.com/atomicarchitects/equiformer_v2\nABSTRACT\nEquivariant Transformers such as Equiformer have demonstrated the efficacy of\napplying Transformers to the domain of 3D atomistic systems. However, they are\nlimited to small degrees of equivariant representations due to their computational\ncomplexity. In this paper, we investigate whether these architectures can scale well\nto higher degrees. Starting from Equiformer, we first replace SOp3q convolutions\nwith eSCN convolutions to efficiently incorporate higher-degree tensors. Then,\nto better leverage the power of higher degrees, we propose three architectural\nimprovements \u2013 attention re-normalization, separable S2 activation and separable\nlayer normalization. Putting this all together, we propose EquiformerV2, which\noutperforms previous state-of-the-art methods on large-scale OC20 dataset by up\nto 9% on forces, 4% on energies, offers better speed-accuracy trade-offs, and 2\u02c6\nreduction in DFT calculations needed for computing adsorption energies. Addi-\ntionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC\ntrained on both OC20 and OC22 datasets, achieving much better data efficiency.\nFinally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M\ndatasets to better understand the performance gain brought by higher degrees.\n1\nINTRODUCTION\nIn recent years, machine learning (ML) models have shown promising results in accelerating and\nscaling high-accuracy but compute-intensive quantum mechanical calculations by effectively ac-\ncounting for key features of atomic systems, such as the discrete nature of atoms, and Euclidean and\npermutation symmetries (Gilmer et al., 2017; Zhang et al., 2018; Jia et al., 2020; Gasteiger et al.,\n2020a; Batzner et al., 2022; Lu et al., 2021; Unke et al., 2021; Sriram et al., 2022; Rackers et al.,\n2023; Lan et al., 2022). By bringing down computational costs from hours or days to fractions of\nseconds, these methods enable new insights in many applications such as molecular simulations,\nmaterial design and drug discovery. A promising class of ML models that have enabled this progress\nis equivariant graph neural networks (GNNs) (Thomas et al., 2018; Weiler et al., 2018; Kondor et al.,\n2018; Fuchs et al., 2020; Batzner et al., 2022; Brandstetter et al., 2022; Musaelian et al., 2022; Liao\n& Smidt, 2023; Passaro & Zitnick, 2023).\nEquivariant GNNs treat 3D atomistic systems as graphs, and incorporate inductive biases such\nthat their internal representations and predictions are equivariant to 3D translations, rotations and\noptionally inversions. Specifically, they build up equivariant features of each node as vector spaces of\nirreducible representations (or irreps) and have interactions or message passing between nodes based\non equivariant operations such as tensor products. Recent works on equivariant Transformers, specifi-\ncally Equiformer (Liao & Smidt, 2023), have shown the efficacy of applying Transformers (Vaswani\net al., 2017), which have previously enjoyed widespread success in computer vision (Carion et al.,\n2020; Dosovitskiy et al., 2021; Touvron et al., 2020), language (Devlin et al., 2019; Brown et al.,\n2020), and graphs (Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021; Shi et al., 2022),\nto this domain of 3D atomistic systems.\nA bottleneck in scaling Equiformer as well as other equivariant GNNs is the computational complexity\nof tensor products, especially when we increase the maximum degree of irreps Lmax. This limits these\nmodels to use small values of Lmax (e.g., Lmax \u010f 3), which consequently limits their performance.\n1\narXiv:2306.12059v3  [cs.LG]  6 Mar 2024\nPublished as a conference paper at ICLR 2024\nFigure 1: Overview of EquiformerV2. We highlight the differences from Equiformer (Liao & Smidt, 2023)\nin red. For (b), (c), and (d), the left figure is the original module in Equiformer, and the right figure is the\nrevised module in EquiformerV2. Input 3D graphs are embedded with atom and edge-degree embeddings and\nprocessed with Transformer blocks, which consist of equivariant graph attention and feed forward networks.\n\u201cb\u201d denotes multiplication, \u201c\u2018\u201d denotes addition, and \u0159 within a circle denotes summation over all neighbors.\n\u201cDTP\u201d denotes depth-wise tensor products used in Equiformer. Gray cells indicate intermediate irreps features.\nHigher degrees can better capture angular resolution and directional information, which is critical to\naccurate prediction of atomic energies and forces (Batzner et al., 2022; Zitnick et al., 2022; Passaro &\nZitnick, 2023). To this end, eSCN (Passaro & Zitnick, 2023) recently proposes efficient convolutions\nto reduce SOp3q tensor products to SOp2q linear operations, bringing down the computational cost\nfrom OpL6\nmaxq to OpL3\nmaxq and enabling scaling to larger values of Lmax (e.g., Lmax up to 8).\nHowever, except using efficient convolutions for higher Lmax, eSCN still follows SEGNN-like\nmessage passing network (Brandstetter et al., 2022) design, and Equiformer has been shown to\nimprove upon SEGNN. More importantly, this ability to use higher Lmax challenges whether the\nprevious design of equivariant Transformers can scale well to higher-degree representations.\nIn this paper, we are interested in adapting eSCN convolutions for higher-degree representations\nto equivariant Transformers. We start with Equiformer (Liao & Smidt, 2023) and replace SOp3q\nconvolutions with eSCN convolutions. We find that naively incorporating eSCN convolutions does\nnot result in better performance than the original eSCN model. Therefore, to better leverage the\npower of higher degrees, we propose three architectural improvements \u2013 attention re-normalization,\nseparable S2 activation and separable layer normalization. Putting these all together, we propose\nEquiformerV2, which is developed on large and diverse OC20 dataset (Chanussot* et al., 2021).\nExperiments on OC20 show that EquiformerV2 outperforms previous state-of-the-art methods with\nimprovements of up to 9% on forces and 4% on energies, and offers better speed-accuracy trade-offs\ncompared to existing invariant and equivariant GNNs. Additionally, when used in the AdsorbML\nalgorithm (Lan et al., 2022) for performing adsorption energy calculations, EquiformerV2 achieves\nthe highest success rate and 2\u02c6 reduction in DFT calculations to achieve comparable adsorption\nenergy accuracies as previous methods. Furthermore, EquiformerV2 trained on only OC22 (Tran*\net al., 2022) dataset outperforms GemNet-OC (Gasteiger et al., 2022) trained on both OC20 and\nOC22 datasets, achieving much better data efficiency. Finally, we compare EquiformerV2 with\nEquiformer on QM9 dataset (Ramakrishnan et al., 2014; Ruddigkeit et al., 2012) and OC20 S2EF-2M\ndataset to better understand the performance gain of higher degrees and the improved architecture.\n2\nBACKGROUND\nWe present background relevant to this work here and discuss related works in Sec. B.\n2\nPublished as a conference paper at ICLR 2024\n2.1\nSEp3q{Ep3q-EQUIVARIANT NEURAL NETWORKS\nWe discuss the relevant background of SEp3q{Ep3q-equivariant neural networks here. Please refer\nto Sec. A in appendix for more details of equivariance and group theory.\nIncluding equivariance in neural networks can serve as a strong prior knowledge, which can therefore\nimprove data efficiency and generalization. Equivariant neural networks use equivariant irreps features\nbuilt from vector spaces of irreducible representations (irreps) to achieve equivariance to 3D rotation.\nSpecifically, the vector spaces are p2L ` 1q-dimensional, where degree L is a non-negative integer. L\ncan be intuitively interpreted as the angular frequency of the vectors, i.e., how fast the vectors rotate\nwith respect to a rotation of the coordinate system. Higher L is critical to tasks sensitive to angular\ninformation like predicting forces (Batzner et al., 2022; Zitnick et al., 2022; Passaro & Zitnick,\n2023). Vectors of degree L are referred to as type-L vectors, and they are rotated with Wigner-D\nmatrices DpLq when rotating coordinate systems. Euclidean vectors \u20d7r in R3 can be projected into\ntype-L vectors by using spherical harmonics Y pLqp \u20d7r\n||\u20d7r||q. We use order m to index the elements of\ntype-L vectors, where \u00b4L \u010f m \u010f L. We concatenate multiple type-L vectors to form an equivariant\nirreps feature f. Concretely, f has CL type-L vectors, where 0 \u010f L \u010f Lmax and CL is the number\nof channels for type-L vectors. In this work, we mainly consider CL \u201c C, and the size of f is\npLmax ` 1q2 \u02c6 C. We index f by channel i, degree L, and order m and denote as f pLq\nm,i.\nEquivariant GNNs update irreps features by passing messages of transformed irreps features between\nnodes. To interact different type-L vectors during message passing, we use tensor products, which\ngeneralize multiplication to equivariant irreps features. Denoted as bL3\nL1,L2, the tensor product uses\nClebsch-Gordan coefficients to combine type-L1 vector f pL1q and type-L2 vector gpL2q and produces\ntype-L3 vector hpL3q:\nhpL3q\nm3\n\u201c pf pL1q bL3\nL1,L2 gpL2qqm3 \u201c\nL1\n\u00ff\nm1\u201c\u00b4L1\nL2\n\u00ff\nm2\u201c\u00b4L2\nCpL3,m3q\npL1,m1qpL2,m2qf pL1q\nm1 gpL2q\nm2\n(1)\nwhere m1 denotes order and refers to the m1-th element of f pL1q. Clebsch-Gordan coefficients\nCpL3,m3q\npL1,m1qpL2,m2q are non-zero only when |L1 \u00b4 L2| \u010f L3 \u010f |L1 ` L2| and thus restrict output\nvectors to be of certain degrees. We typically discard vectors with L \u0105 Lmax, where Lmax is a\nhyper-parameter, to prevent vectors of increasingly higher dimensions. In many works, message\npassing is implemented as equivariant convolutions, which perform tensor products between input\nirreps features xpL1q and spherical harmonics of relative position vectors Y pL2qp \u20d7r\n||\u20d7r||q.\n2.2\nEQUIFORMER\nEquiformer (Liao & Smidt, 2023) is an SEp3q/Ep3q-equivariant GNN that combines the inductive bi-\nases of equivariance with the strength of Transformers. First, Equiformer replaces scalar node features\nwith equivariant irreps features to incorporate equivariance. Next, it performs equivariant operations\non these irreps features and equivariant graph attention for message passing. These operations include\ntensor products and equivariant linear operations, equivariant layer normalization (Ba et al., 2016)\nand gate activation (Weiler et al., 2018). For stronger expressivity in the attention compared to typical\nTransformers, Equiformer uses non-linear functions for both attention weights and message passing.\nAdditionally, Equiformer incorporates regularization techniques commonly used by Transformers,\ne.g., dropout (Srivastava et al., 2014) to attention weights (Veli\u02c7ckovi\u00b4c et al., 2018) and stochastic\ndepth (Huang et al., 2016) to the outputs of equivariant graph attention and feed forward networks.\n2.3\nESCN CONVOLUTION\neSCN convolutions (Passaro & Zitnick, 2023) are proposed to use SOp2q linear operations for\nefficient tensor products. We provide an outline and intuition for their method here, please refer to\nSec. A.3 and their work (Passaro & Zitnick, 2023) for mathematical details.\nA traditional SOp3q convolution interacts input irreps features xpLiq\nmi\nand spherical harmonic projec-\ntions of relative positions Y pLf q\nmf\np \u20d7rijq with an SOp3q tensor product with Clebsch-Gordan coefficients\nCpLo,moq\npLi,miq,pLf ,mf q. The projection Y pLf q\nmf\np \u20d7rijq becomes sparse if we rotate the relative position vector\n3\nPublished as a conference paper at ICLR 2024\n\u20d7rij with a rotation matrix Dij to align with the direction of L \u201c 0 and m \u201c 0, which corresponds\nto the z axis traditionally but the y axis in the conventions of e3nn (Geiger et al., 2022). Concretely,\ngiven Dij\u20d7rij aligned with the y axis, Y pLf q\nmf\npDij\u20d7rijq \u2030 0 only for mf \u201c 0. If we consider only\nmf \u201c 0, CpLo,moq\npLi,miq,pLf ,mf q can be simplified, and CpLo,moq\npLi,miq,pLf ,0q \u2030 0 only when mi \u201c \u02d8mo. There-\nfore, the original expression depending on mi, mf, and mo is now reduced to only depend on mo.\nThis means we are no longer mixing all integer values of mi and mf, and outputs of order mo are\nlinear combinations of inputs of order \u02d8mo. eSCN convolutions go one step further and replace the\nremaining non-trivial paths of the SOp3q tensor product with an SOp2q linear operation to allow for\nadditional parameters of interaction between \u02d8mo without breaking equivariance.\n3\nEQUIFORMERV2\nStarting from Equiformer (Liao & Smidt, 2023), we first use eSCN convolutions to scale to higher-\ndegree representations (Sec. 3.1). Then, we propose three architectural improvements, which yield\nfurther performance gain when using higher degrees: attention re-normalization (Sec. 3.2), separable\nS2 activation (Sec. 3.3) and separable layer normalization (Sec. 3.4). Figure 1 illustrates the overall\narchitecture of EquiformerV2 and the differences from Equiformer.\n3.1\nINCORPORATING ESCN CONVOLUTIONS FOR HIGHER DEGREES\nThe computational complexity of SOp3q tensor products used in traditional SOp3q convolutions\nscale unfavorably with Lmax. Because of this, it is impractical for Equiformer to use Lmax \u0105 2 for\nlarge-scale datasets like OC20. Since higher Lmax can better capture angular information and are\ncorrelated with model expressivity (Batzner et al., 2022), low values of Lmax can lead to limited\nperformance on certain tasks such as predicting forces. Therefore, we replace original tensor products\nwith eSCN convolutions for efficient tensor products, enabling Equiformer to scale up Lmax to 6 or 8\non OC20 dataset. Equiformer uses equivariant graph attention for message passing. The attention\nconsists of depth-wise tensor products, which mix information across different degrees, and linear\nlayers, which mix information between channels of the same degree. Since eSCN convolutions mix\ninformation across both degrees and channels, we replace the SOp3q convolution, which involves\none depth-wise tensor product layer and one linear layer, with a single eSCN convolutional layer,\nwhich consists of a rotation matrix Dij and an SOp2q linear layer as shown in Figure 1b.\n3.2\nATTENTION RE-NORMALIZATION\nEquivariant graph attention in Equiformer uses tensor products to project node embeddings xi\nand xj, which contain vectors of degrees from 0 to Lmax, to scalar features f p0q\nij\nand applies\nnon-linear functions to f p0q\nij\nfor attention weights aij. We propose attention re-normalization and\nintroduce one additional layer normalization (LN) (Ba et al., 2016) before non-linear functions.\nSpecifically, given f p0q\nij , we first apply LN and then use one leaky ReLU layer and one linear layer to\ncalculate zij \u201c wJ\na LeakyReLUpLNpf p0q\nij qq and aij \u201c softmaxjpzijq \u201c\nexppzijq\n\u0159\nkPN piq exppzikq, where wa is\na learnable vector of the same dimension as f p0q\nij . The motivation is similar to ViT-22B (Dehghani\net al., 2023), where they find that they need an additional layer normalization to stabilize training\nwhen increasing widths. When we scale up Lmax, we effectively increase the number of input\nchannels for calculating f p0q\nij . By normalizing f p0q\nij , the additional LN can make sure the inputs to\nsubsequent non-linear functions and softmax operations still lie within the same range as lower Lmax\nis used. This empirically improves the performance as shown in Table 1a.\n3.3\nSEPARABLE S2 ACTIVATION\nThe gate activation (Weiler et al., 2018) used by Equiformer applies sigmoid activation to scalar\nfeatures to obtain non-linear weights and then multiply irreps features of degree \u0105 0 with non-\nlinear weights to add non-linearity to equivariant features. The activation only accounts for the\ninteraction from vectors of degree 0 to those of degree \u0105 0 and can be sub-optimal when we scale up\nLmax. To better mix the information across degrees, SCN (Zitnick et al., 2022) and eSCN adopt S2\n4\nPublished as a conference paper at ICLR 2024\nFigure 2: Illustration of different activation functions. G\ndenotes conversion from vectors to point samples on a\nsphere, F can typically be a SiLU activation or MLPs,\nand G\u00b41 is the inverse of G.\nFigure 3: Illustration of how statistics are calculated\nin different normalizations. \u201cstd\u201d denotes standard\ndeviation, and \u201cRMS\u201d denotes root mean square.\nactivation (Cohen et al., 2018). The activation first converts vectors of all degrees to point samples on\na sphere for each channel, applies unconstrained functions F to those samples, and finally convert\nthem back to vectors. Specifically, given an input irreps feature x P RpLmax`1q2\u02c6C, the output is\ny \u201c G\u00b41pFpGpxqqq, where G denotes the conversion from vectors to point samples on a sphere, F\ncan be typical SiLU activation (Elfwing et al., 2017; Ramachandran et al., 2017) or typical MLPs,\nand G\u00b41 is the inverse of G. We provide more details of S2 activation in Sec. A.4.\nHowever, we find that directly replacing the gate activation with S2 activation in Equiformer results\nin large gradients and training instability (Index 3 in Table 1a). To address the issue, we propose\nseparable S2 activation, which separates activation for vectors of degree 0 and those of degree \u0105 0.\nSimilar to gate activation, we have more channels for vectors of degree 0. As shown in Figure 2c,\nwe apply a SiLU activation to the first part of vectors of degree 0, and the second part of vectors of\ndegree 0 are used for S2 activation along with vectors of higher degrees. After S2 activation, we\nconcatenate the first part of vectors of degree 0 with vectors of degrees \u0105 0 as the final output and\nignore the second part of vectors of degree 0. Separating the activation for vectors of degree 0 and\nthose of degree \u0105 0 prevents large gradients, enabling using more expressive S2 activation for better\nperformance. Additionally, we use separable S2 activation in feed forward networks (FFNs). Figure 2\nillustrates the differences between gate activation, S2 activation and separable S2 activation.\n3.4\nSEPARABLE LAYER NORMALIZATION\nEquivariant layer normalization used by Equiformer normalizes vectors of different degrees inde-\npendently. However, it potentially ignores the relative importance of different degrees since the\nrelative magnitudes between different degrees become the same after the normalization. Therefore,\ninstead of performing normalization to each degree independently, motivated by the separable S2\nactivation mentioned above, we propose separable layer normalization (SLN), which separates nor-\nmalization for vectors of degree 0 and those of degrees \u0105 0. Mathematically, let x P RpLmax`1q2\u02c6C\ndenote an input irreps feature of maximum degree Lmax and C channels, and xpLq\nm,i denote the L-th\ndegree, m-th order and i-th channel of x. SLN calculates the output y as follows. For L \u201c 0,\nyp0q \u201c \u03b3p0q \u02dd\n\u00b4\nxp0q\u00b4\u00b5p0q\n\u03c3p0q\n\u00af\n` \u03b2p0q, where \u00b5p0q \u201c 1\nC\n\u0159C\ni\u201c1 xp0q\n0,i and \u03c3p0q \u201c\nb\n1\nC\n\u0159C\ni\u201c1pxp0q\n0,i \u00b4 \u00b5p0qq2.\nFor L \u0105 0, ypLq \u201c \u03b3pLq \u02dd\n\u00b4\nxpLq\n\u03c3pL\u01050q\n\u00af\n, where \u03c3pL\u01050q \u201c\nb\n1\nLmax\n\u0159Lmax\nL\u201c1\n`\n\u03c3pLq\u02d82 and \u03c3pLq \u201c\nc\n1\nC\n\u0159C\ni\u201c1\n1\n2L`1\n\u0159L\nm\u201c\u00b4L\n\u00b4\nxpLq\nm,i\n\u00af2\n. \u03b3p0q, \u03b3pLq, \u03b2p0q P RC are learnable parameters, \u00b5p0q and \u03c3p0q\nare mean and standard deviation of vectors of degree 0, \u03c3pLq and \u03c3pL\u01050q are root mean square values\n(RMS), and \u02dd denotes element-wise product. Figure 3 compares how \u00b5p0q, \u03c3p0q, \u03c3pLq and \u03c3pL\u01050q are\ncalculated in equivariant layer normalization and SLN. Preserving the relative magnitudes between\ndegrees \u0105 0 improves performance as shown in Table 1a.\n3.5\nOVERALL ARCHITECTURE\nEquivariant Graph Attention.\nFigure 1b illustrates equivariant graph attention after the above\nmodifications. Given node embeddings xi and xj, we first concatenate them along the channel\ndimension and then rotate them with rotation matrices Dij based on their relative positions or edge\n5\nPublished as a conference paper at ICLR 2024\ndirections \u20d7rij. We replace depth-wise tensor products and linear layers between xi, xj and fij with\na single SOp2q linear layer. To consider the information of relative distances ||\u20d7rij||, we transform\n||\u20d7rij|| with a radial function to obtain edge distance embeddings and then multiply the edge distance\nembeddings with concatenated node embeddings before the first SOp2q linear layer. We split the\noutputs fij of the first SOp2q linear layer into two parts. The first part is scalar features f p0q\nij , which\nonly contains vectors of degree 0, and the second part is irreps features f pLq\nij\nand includes vectors\nof all degrees up to Lmax. As mentioned in Sec. 3.2, we first apply an additional LN to f p0q\nij\nand\nthen follow the design of Equiformer by applying one leaky ReLU layer, one linear layer and a final\nsoftmax layer to obtain attention weights aij. As for value vij, we replace the gate activation with\nseparable S2 activation with F being a single SiLU activation and then apply the second SOp2q\nlinear layer. While in Equiformer, the message mij sent from node j to node i is mij \u201c aij \u02c6 vij,\nhere we need to rotate aij \u02c6 vij back to original coordinate frames and the message mij becomes\nD\u00b41\nij paij \u02c6 vijq. Finally, we can perform h parallel equivariant graph attention functions given fij.\nThe h different outputs are concatenated and projected with a linear layer to become the final output\nyi. Parallelizing attention functions and concatenating can be implemented with \u201cReshape\u201d.\nFeed Forward Network.\nAs shown in Figure 1d, we replace the gate activation with separable S2\nactivation. The function F consists of a two-layer MLP, with each linear layer followed by SiLU, and\na final linear layer.\nEmbedding.\nThis module consists of atom embedding and edge-degree embedding. The former is\nthe same as that in Equiformer. For the latter, as depicted in the right branch in Figure 1c, we replace\noriginal linear layers and depth-wise tensor products with a single SOp2q linear layer followed by a\nrotation matrix D\u00b41\nij . Similar to equivariant graph attention, we consider the information of relative\ndistances by multiplying the outputs of the SOp2q linear layer with edge distance embeddings.\nRadial Basis and Radial Function.\nWe represent relative distances ||\u20d7rij|| with a finite radial basis\nlike Gaussian radial basis functions (Sch\u00fctt et al., 2017) to capture their subtle changes. We transform\nradial basis with a learnable radial function to generate edge distance embeddings. The function\nconsists of a two-layer MLP, with each linear layer followed by LN and SiLU, and a final linear layer.\nOutput Head.\nTo predict scalar quantities like energy, we use a feed forward network to transform\nirreps features on each node into a scalar and then sum over all nodes. For predicting atom-wise forces,\nwe use a block of equivariant graph attention and treat the output of degree 1 as our predictions.\n4\nEXPERIMENTS\n4.1\nOC20 DATASET\nOur main experiments focus on the large and diverse OC20 dataset (Chanussot* et al., 2021). Please\nrefer to Sec. D.1 for details on the dataset. We first conduct ablation studies on EquiformerV2 trained\non the 2M split of OC20 S2EF dataset (Sec. 4.1.1). Then, we report the results of training All and\nAll+MD splits (Sec. 4.1.2). Additionally, we investigate the performance of EquiformerV2 when\nused in the AdsorbML algorithm (Lan et al., 2022) (Sec. 4.1.3). Please refer to Sec. C and D for\ndetails of models and training.\n4.1.1\nABLATION STUDIES\nArchitectural Improvements.\nIn Table 1a, we start with incorporating eSCN convolutions into\nEquiformer for higher-degree representations (Index 1) and then ablate the three proposed architectural\nimprovements. First, with attention re-normalization, energy mean absolute errors (MAE) improve by\n2.4%, while force MAE are about the same (Index 1 and 2). Second, we replace the gate activation\nwith S2 activation, but that does not converge (Index 3). With the proposed S2 activation, we stabilize\ntraining and successfully leverage more expressive activation to improve force MAE (Index 4). Third,\nreplacing equivariant layer normalization with separable layer normalization further improves force\nMAE (Index 5). We note that simply incorporating eSCN convolutions into Equiformer and using\n6\nPublished as a conference paper at ICLR 2024\nAttention\nIndex re-normalization\nActivation\nNormalization Epochs forces\nenergy\n1\n\u2717\nGate\nLN\n12\n21.85\n286\n2\n\u2713\nGate\nLN\n12\n21.86\n279\n3\n\u2713\nS2\nLN\n12\ndidn\u2019t converge\n4\n\u2713\nSep. S2\nLN\n12\n20.77\n285\n5\n\u2713\nSep. S2\nSLN\n12\n20.46\n285\n6\n\u2713\nSep. S2\nLN\n20\n20.02\n276\n7\n\u2713\nSep. S2\nSLN\n20\n19.72\n278\n8\neSCN baseline\n12\n21.3\n294\n(a) Architectural improvements. Attention re-normalization\nimproves energies, and separable S2 activation (\u201cSep. S2\u201d)\nand separable layer normalization (\u201cSLN\u201d) improve forces.\neSCN\nEquiformerV2\nLmax\nEpochs forces\nenergy\nforces\nenergy\n6\n12\n21.3\n294\n20.46\n285\n6\n20\n20.6\n290\n19.78\n280\n6\n30\n20.1\n285\n19.42\n278\n8\n12\n21.3\n296\n20.46\n279\n8\n20\n-\n-\n19.95\n273\n(b) Training epochs. Training for more epochs\nconsistently leads to better results.\neSCN\nEquiformerV2\nLmax\nforces\nenergy\nforces\nenergy\n4\n22.2\n291\n21.37\n284\n6\n21.3\n294\n20.46\n285\n8\n21.3\n296\n20.46\n279\n(c) Degrees Lmax. Higher degrees\nare consistently helpful.\neSCN\nEquiformerV2\nMmax\nforces\nenergy\nforces\nenergy\n2\n21.3\n294\n20.46\n285\n3\n21.2\n295\n20.24\n284\n4\n21.2\n298\n20.24\n282\n6\n-\n-\n20.26\n278\n(d) Orders Mmax. Higher orders\nmainly improve energy predictions.\neSCN\nEquiformerV2\nLayers\nforces\nenergy\nforces\nenergy\n8\n22.4\n306\n21.18\n293\n12\n21.3\n294\n20.46\n285\n16\n20.5\n283\n20.11\n282\n(e) Number of Transformer blocks.\nAdding more blocks can help both\nforce and energy predictions.\nTable 1: Ablation studies of EquiformerV2. We report mean absolute errors (MAE) for forces in meV/\u00c5 and\nenergy in meV, and lower is better. All models are trained on the 2M split of the OC20 S2EF dataset, and errors\nare averaged over the four validation sub-splits. The base model setting is marked in gray .\n(a) Trade-offs between inference speed\nand validation force MAE.\n(b) Trade-offs between training cost and\nvalidation force MAE.\nFigure 4: EquiformerV2 achieves better accuracy trade-offs both in terms of inference speed as well as training\ncost. All models are trained on the S2EF-2M split and measured on V100 GPUs.\nhigher degrees (Index 1) do not result in better performance than the original eSCN baseline (Index 8),\nand that the proposed architectural changes are necessary. We additionally compare the performance\ngain of architectural improvements with that of training for longer. Attention re-normalization (Index\n1 and 2) improves energy MAE by the same amount as increasing training epochs from 12 to 20\n(Index 5 and 7). The improvement of SLN (Index 4 and 5) in force MAE is about 40% of that of\nincreasing training epochs from 12 to 20 (Index 5 and 7), and SLN is about 6% faster in training. We\nalso conduct the ablation studies on the QM9 dataset and summarize the results in Sec. F.3.\nScaling of Parameters.\nIn Tables 1c, 1d, 1e, we vary the maximum degree Lmax, the maximum\norder Mmax, and the number of Transformer blocks and compare with equivalent eSCN variants.\nAcross all experiments, EquiformerV2 performs better than its eSCN counterparts. Besides, while one\nmight intuitively expect higher resolutions and larger models to perform better, this is only true for\nEquiformerV2, not eSCN. For example, increasing Lmax from 6 to 8 or Mmax from 3 to 4 degrades\nthe performance of eSCN on energy predictions but helps that of EquiformerV2. In Table 1b, we\nshow that increasing the training epochs from 12 to 30 epochs can consistently improve performance.\nSpeed-Accuracy Trade-offs.\nWe compare trade-offs between inference speed or training time\nand force MAE among prior works and EquiformerV2 and summarize the results in Figure 4a\nand Figure 4b. EquifomerrV2 achieves the lowest force MAE across a wide range of inference speed\nand training cost.\n7\nPublished as a conference paper at ICLR 2024\nThroughput\nS2EF validation\nS2EF test\nIS2RS test\nIS2RE test\nTraining\nNumber of\nTraining time\nSamples /\nEnergy MAE\nForce MAE\nEnergy MAE\nForce MAE\nAFbT\nADwT\nEnergy MAE\nset\nModel\nparameters \u00d3\n(GPU-days) \u00d3\nGPU sec. \u00d2\n(meV) \u00d3\n(meV/\u00c5) \u00d3\n(meV) \u00d3\n(meV/\u00c5) \u00d3\n(%) \u00d2\n(%) \u00d2\n(meV) \u00d3\nOC20 S2EF-All\nSchNet (Sch\u00fctt et al., 2017)\n9.1M\n194\n-\n549\n56.8\n540\n54.7\n-\n14.4\n749\nDimeNet++-L-F+E (Gasteiger et al., 2020a)\n10.7M\n1600\n4.6\n515\n32.8\n480\n31.3\n21.7\n51.7\n559\nSpinConv (Shuaibi et al., 2021)\n8.5M\n275\n6.0\n371\n41.2\n336\n29.7\n16.7\n53.6\n437\nGemNet-dT (Klicpera et al., 2021)\n32M\n492\n25.8\n315\n27.2\n292\n24.2\n27.6\n58.7\n400\nGemNet-OC (Gasteiger et al., 2022)\n39M\n336\n18.3\n244\n21.7\n233\n20.7\n35.3\n60.3\n355\nSCN L=8 K=20 (Zitnick et al., 2022)\n271M\n645\n-\n-\n-\n244\n17.7\n40.3\n67.1\n330\neSCN L=6 K=20 (Passaro & Zitnick, 2023)\n200M\n600\n2.9\n-\n-\n242\n17.1\n48.5\n65.7\n341\nEquiformerV2 (\u03bbE \u201c 2, 153M)\n153M\n1368\n1.8\n236\n15.7\n229\n14.8\n53.0\n69.0\n316\nOC20 S2EF-All+MD\nGemNet-OC-L-E (Gasteiger et al., 2022)\n56M\n640\n7.5\n239\n22.1\n230\n21.0\n-\n-\n-\nGemNet-OC-L-F (Gasteiger et al., 2022)\n216M\n765\n3.2\n252\n20.0\n241\n19.0\n40.6\n60.4\n-\nGemNet-OC-L-F+E (Gasteiger et al., 2022)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n348\nSCN L=6 K=16 (4-tap 2-band) (Zitnick et al., 2022)\n168M\n414\n-\n-\n-\n228\n17.8\n43.3\n64.9\n328\nSCN L=8 K=20 (Zitnick et al., 2022)\n271M\n1280\n-\n-\n-\n237\n17.2\n43.6\n67.5\n321\neSCN L=6 K=20 (Passaro & Zitnick, 2023)\n200M\n568\n2.9\n243\n17.1\n228\n15.6\n50.3\n66.7\n323\nEquiformerV2 (\u03bbE \u201c 4, 31M)\n31M\n705\n7.1\n232\n16.3\n228\n15.5\n47.6\n68.3\n315\nEquiformerV2 (\u03bbE \u201c 2, 153M)\n153M\n1368\n1.8\n230\n14.6\n227\n13.8\n55.4\n69.8\n311\nEquiformerV2 (\u03bbE \u201c 4, 153M)\n153M\n1571\n1.8\n227\n15.0\n219\n14.2\n54.4\n69.4\n309\nTable 2: OC20 results on S2EF validation and test splits, and IS2RS and IS2RE test splits when trained on\nOC20 S2EF-All or S2EF-All+MD splits. Throughput is reported as the number of structures processed per\nGPU-second during training and measured on V100 GPUs. \u03bbE is the coefficient of the energy loss.\n4.1.2\nMAIN RESULTS\nTable 2 reports results on the test splits for all the three tasks of OC20 averaged over all four sub-splits.\nModels are trained on either OC20 S2EF-All or S2EF-All+MD splits. All test results are computed\nvia the EvalAI evaluation server1. We train EquiformerV2 of two sizes, one with 153M parameters\nand the other with 31M parameters. When trained on the S2EF-All+MD split, EquiformerV2\n(\u03bbE \u201c 4, 153M) improves previous state-of-the-art S2EF energy MAE by 4%, S2EF force MAE by\n9%, IS2RS Average Forces below Threshold (AFbT) by absolute 4% and IS2RE energy MAE by 4%.\nIn particular, the improvement in force predictions is significant. Going from SCN to eSCN, S2EF\ntest force MAE improves from 17.2 meV/\u00c5 to 15.6 meV/\u00c5 , largely due to replacing approximate\nequivariance in SCN with strict equivariance in eSCN during message passing. Similarly, by scaling\nup the degrees of representations in Equiformer, EquiformerV2 (\u03bbE \u201c 4, 153M) further improves\nforce MAE to 14.2 meV/\u00c5, which is similar to the gain of going from SCN to eSCN. Additionally, the\nsmaller EquiformerV2 (\u03bbE \u201c 4, 31M) improves upon previously best results for all metrics except\nIS2RS AFbT and achieves comparable training throughput to the fastest GemNet-OC-L-E. Although\nthe training time of EquiformerV2 is higher here, we note that this is because training EquiformerV2\nfor longer keeps improving performance and that we already demonstrate EquiformerV2 achieves\nbetter trade-offs between force MAE and speed.\n4.1.3\nADSORBML RESULTS\nLan et al. (2022) recently proposes the AdsorbML algorithm, wherein they show that recent state-of-\nthe-art GNNs can achieve more than 1000\u02c6 speedup over DFT relaxations at computing adsorption\nenergies within a 0.1eV margin of DFT results with an 87% success rate. This is done by using\nOC20-trained models to perform structure relaxations for an average 90 configurations of an adsorbate\nplaced on a catalyst surface, followed by DFT single-point calculations for the top-k structures with\nlowest predicted relaxed energies, as a proxy for calculating the global energy minimum or adsorption\nenergy. We refer readers to Sec. D.4 and the work (Lan et al., 2022) for more details. We benchmark\nAdsorbML with EquiformerV2, and Table 3 shows that EquiformerV2 (\u03bbE \u201c 4, 153M) improves\nover SCN by a significant margin, with 8% and 5% absolute improvements at k \u201c 1 and k \u201c 2,\nrespectively. Moreover, EquiformerV2 (\u03bbE \u201c 4, 153M) at k \u201c 2 is more accurate at adsorption\nenergy calculations than all the other models even at k \u201c 5, thus requiring at least 2\u02c6 fewer DFT\ncalculations. Since the speedup is with respect to using DFT for structure relaxations and that ML\nmodels are much faster than DFT, the speedup is dominated by the final DFT single-point calculations\nand ML models with the same value of k have roughly the same speedup. To better understand the\nspeed-accuracy trade-offs of different models, we also report average GPU-seconds of running one\nstructure relaxation in Table 3. Particularly, EquiformerV2 (\u03bbE \u201c 4, 31M) improves upon previous\nmethods while being 3.7\u02c6 to 9.8\u02c6 faster than GemNet-OC-MD-Large and SCN, respectively.\n1eval.ai/web/challenges/challenge-page/712\n8\nPublished as a conference paper at ICLR 2024\nGPU-seconds\nk \u201c 1\nk \u201c 2\nk \u201c 3\nk \u201c 4\nk \u201c 5\nModel\nper relaxation \u00d3\nSuccess\nSpeedup\nSuccess\nSpeedup\nSuccess\nSpeedup\nSuccess\nSpeedup\nSuccess\nSpeedup\nSchNet (Sch\u00fctt et al., 2017)\n-\n2.77%\n4266.13\n3.91%\n2155.36\n4.32%\n1458.77\n4.73%\n1104.88\n5.04%\n892.79\nDimeNet++ (Gasteiger et al., 2020a)\n-\n5.34%\n4271.23\n7.61%\n2149.78\n8.84%\n1435.21\n10.07%\n1081.96\n10.79%\n865.20\nPaiNN (Sch\u00fctt et al., 2021)\n-\n27.44%\n4089.77\n33.61%\n2077.65\n36.69%\n1395.55\n38.64%\n1048.63\n39.57%\n840.44\nGemNet-OC (Gasteiger et al., 2022)\n9.4\n68.76%\n4185.18\n77.29%\n2087.11\n80.78%\n1392.51\n81.50%\n1046.85\n82.94%\n840.25\nGemNet-OC-MD (Gasteiger et al., 2022)\n9.4\n68.76%\n4182.04\n78.21%\n2092.27\n81.81%\n1404.11\n83.25%\n1053.36\n84.38%\n841.64\nGemNet-OC-MD-Large (Gasteiger et al., 2022)\n45.0\n73.18%\n4078.76\n79.65%\n2065.15\n83.25%\n1381.39\n85.41%\n1041.50\n86.02%\n834.46\nSCN-MD-Large (Zitnick et al., 2022)\n120.0\n77.80%\n3974.21\n84.28%\n1989.32\n86.33%\n1331.43\n87.36%\n1004.40\n87.77%\n807.00\nEquiformerV2 (\u03bbE \u201c 4, 31M)\n12.2\n84.17%\n3983.41\n87.15%\n1992.64\n87.87%\n1331.35\n88.69%\n1000.62\n89.31%\n802.95\nEquiformerV2 (\u03bbE \u201c 4, 153M)\n45.0\n85.41%\n4001.71\n88.90%\n2012.47\n90.54%\n1352.08\n91.06%\n1016.31\n91.57%\n815.87\nTable 3: AdsorbML results with EquiformerV2 (\u03bbE \u201c 4, 31M) and (\u03bbE \u201c 4, 153M) trained on S2EF-All+MD\nfrom Table 2. We visualize the speed-accuracy trade-offs of different models in Figure 5.\nS2EF-Total validation\nS2EF-Total test\nIS2RE-Total test\nNumber of\nEnergy MAE (meV) \u00d3 Force MAE (meV/\u00c5) \u00d3\nEnergy MAE (meV) \u00d3 Force MAE (meV/\u00c5) \u00d3\nEnergy MAE (meV) \u00d3\nModel\nTraining Set\nLinear reference\nparameters\nID\nOOD\nID\nOOD\nID\nOOD\nID\nOOD\nID\nOOD\nGemNet-OC (Gasteiger et al., 2022)\nOC22\n39M\n545\n1011\n30\n40\n374\n829\n29.4\n39.6\n1329\n1584\nGemNet-OC (Gasteiger et al., 2022)\nOC22\n\u2713\n39M\n-\n-\n-\n-\n357\n1057\n30.0\n40.0\n-\n-\nGemNet-OC (Gasteiger et al., 2022)\nOC22 + OC20\n39M\n464\n859\n27\n34\n311\n689\n26.9\n34.2\n1200\n1534\neSCN (Passaro & Zitnick, 2023)\nOC22\n\u2713\n200M\n-\n-\n-\n-\n350\n789\n23.8\n35.7\n-\n-\nEquiformerV2 (\u03bbE \u201c 1, \u03bbF \u201c 1)\nOC22\n\u2713\n122M\n343\n580\n24.42\n34.31\n182.8\n677.4\n22.98\n35.57\n1084\n1444\nEquiformerV2 (\u03bbE \u201c 4, \u03bbF \u201c 100)\nOC22\n\u2713\n122M\n433\n629\n22.88\n30.70\n263.7\n659.8\n21.58\n32.65\n1119\n1440\nTable 4: OC22 results on S2EF-Total validation and test splits and IS2RE-Total test split.\n4.2\nOC22 DATASET\nDataset.\nThe OC22 dataset (Tran* et al., 2022) focuses on oxide electrocatalysis. One crucial\ndifference between OC22 and OC20 is that the energies in OC22 are DFT total energies. DFT total\nenergies are harder to predict but are the most general and closest to a DFT surrogate, offering\nthe flexibility to study property prediction beyond adsorption energies. Similar to OC20, the tasks\nin OC22 are S2EF-Total and IS2RE-Total. We train models on the OC22 S2EF-Total dataset and\nevaluate them on energy and force MAE on the S2EF-Total validation and test splits. We use\nthe trained models to perform structural relaxations and predict relaxed energy. Relaxed energy\npredictions are evaluated on the IS2RE-Total test split.\nTraining Details.\nPlease refer to Section E.1 for details on architectures, hyper-parameters and\ntraining time.\nResults.\nWe train two EquiformerV2 models with different energy coefficients \u03bbE and force\ncoefficients \u03bbF . We follow the practice of OC22 models and use linear reference (Tran* et al., 2022).\nThe results are summarized in Table 4. EquiformerV2 improves upon previous models on all the tasks.\nEquiformerV2 (\u03bbE \u201c 4, \u03bbF \u201c 100) trained on only OC22 achieves better results on all the tasks than\nGemNet-OC trained on both OC20 and OC22. We note that OC22 contains about 8.4M structures\nand OC20 contains about 130M structures, and therefore EquiformerV2 demonstrates significatly\nbetter data efficiency. Additionally, the performance gap between eSCN and EquiformerV2 is larger\nthan that on OC20, suggesting that more complicated structures can benefit more from the proposed\narchitecture. When trained on OC20 S2EF-All+MD, EquiformerV2 (\u03bbE \u201c 4, 153M) improves upon\neSCN by 4% on energy MAE and 9% on force MAE. For OC22, EquiformerV2 (\u03bbE \u201c 4, \u03bbF \u201c 100)\nimproves upon eSCN by 18.9% on average energy MAE and 8.9% on average force MAE.\n4.3\nCOMPARISON WITH EQUIFORMER\nQM9 Dataset.\nWe follow the setting of Equiformer and train EquiformerV2 on the QM9 dataset (Ra-\nmakrishnan et al., 2014; Ruddigkeit et al., 2012) and summarize the results in Table 5. The per-\nformance gain of using higher degrees and the improved architecture is not as significant as that\non OC20 and OC22 datasets. This, however, is not surprising. The training set of QM9 contains\nonly 110k examples, which is much less than OC20 S2EF-2M with 2M examples and OC22 with\n8.4M examples. Moreover, QM9 has much less numbers of atoms in each example and much less\ndiverse atom types, and each example has less angular variations. Nevertheless, EquiformerV2\nachieves better results than Equiformer on 9 out of the 12 tasks and is therefore the overall best\nperforming model. We additionally train EquiformerV2 with Noisy Nodes (Godwin et al., 2022) to\nbetter understand the gain of higher degrees in Sec. F.1.\n9\nPublished as a conference paper at ICLR 2024\nTask\n\u03b1\n\u2206\u03b5\n\u03b5HOMO\n\u03b5LUMO\n\u00b5\nC\u03bd\nG\nH\nR2\nU\nU0\nZPVE\nModel\nUnits\na3\n0\nmeV\nmeV\nmeV\nD\ncal/mol K\nmeV\nmeV\na2\n0\nmeV\nmeV\nmeV\nDimeNet++ (Gasteiger et al., 2020a)\n.044\n33\n25\n20\n.030\n.023\n8\n7\n.331\n6\n6\n1.21\nEGNN (Satorras et al., 2021):\n.071\n48\n29\n25\n.029\n.031\n12\n12\n.106\n12\n11\n1.55\nPaiNN (Sch\u00fctt et al., 2021)\n.045\n46\n28\n20\n.012\n.024\n7.35\n5.98\n.066\n5.83\n5.85\n1.28\nTorchMD-NET (Th\u00f6lke & Fabritiis, 2022)\n.059\n36\n20\n18\n.011\n.026\n7.62\n6.16\n.033\n6.38\n6.15\n1.84\nSphereNet (Liu et al., 2022)\n.046\n32\n23\n18\n.026\n.021\n8\n6\n.292\n7\n6\n1.12\nSEGNN (Brandstetter et al., 2022):\n.060\n42\n24\n21\n.023\n.031\n15\n16\n.660\n13\n15\n1.62\nEQGAT (Le et al., 2022)\n.053\n32\n20\n16\n.011\n.024\n23\n24\n.382\n25\n25\n2.00\nEquiformer (Liao & Smidt, 2023)\n.046\n30\n15\n14\n.011\n.023\n7.63\n6.63\n.251\n6.74\n6.59\n1.26\nEquiformerV2\n.050\n29\n14\n13\n.010\n.023\n7.57\n6.22\n.186\n6.49\n6.17\n1.47\nTable 5: Mean absolute error results on QM9 test set. : denotes using different data partitions.\nModel\nLmax\nEnergy MAE (meV)\u00d3\nForce MAE (meV/\u00c5)\u00d3\nTraining time (GPU-hours)\u00d3\nEquiformer (Liao & Smidt, 2023)\n2\n297\n27.57\n1365\nEquiformer (Liao & Smidt, 2023)\n3\nOOM\nOOM\nOOM\nEquiformerV2\n2\n298\n26.24\n600\nEquiformerV2\n4\n284\n21.37\n966\nEquiformerV2\n6\n285\n20.46\n1412\nTable 6: Comparison on OC20 S2EF-2M dataset. Errors are averaged over the four validation sub-splits. \u201cOOM\u201d\ndenotes out-of-memory error, and we cannot use Lmax \u201c 3 for Equiformer.\nOC20 S2EF-2M Dataset.\nWe use similar configurations (e.g., numbers of blocks and numbers of\nchannels) and train Equiformer on OC20 S2EF-2M dataset for the same number of epochs as training\nEquiformerV2. We vary Lmax for both Equiformer and EquiformerV2 and compare the results in\nTable 6. For Lmax \u201c 2, EquiformerV2 is 2.3\u02c6 faster than Equiformer since EquiformerV2 uses\neSCN convolutions for efficient SOp3q convolutions. Additionally, EquiformerV2 achieves better\nforce MAE and similar energy MAE, demonstrating the effectiveness of the proposed improved\narchitecture. For Lmax \u0105 2, we encounter out-of-memory errors when training Equiformer even\nafter we reduce the number of blocks and use the batch size \u201c 1. In contrast, We can easily train\nEquiformerV2 with Lmax up to 6. When increasing Lmax from 2 to 4, EquiformerV2 achieves lower\nenergy MAE and significantly lower force MAE than Equiformer and requires 1.4\u02c6 less training\ntime. The comparison suggests that complicated datasets have more performance to gain from using\nmore expressive models, enabling better performance and lower computational cost.\nDiscussion.\nOne limitation of EquiformerV2 is that the performance gains brought by scaling to\nhigher degrees and the proposed architectural improvements can depend on datasets and tasks. For\nsmall datasets like QM9, the performance gain is not significant. We additionally compare Equiformer\nand EquiformerV2 on OC20 IS2RE dataset in Sec. D.5. For different tasks, the improvements are\nalso different, and force predictions benefit more from better expressivity than energy predictions.\nWe note that the first issue can be mitigated by first pre-training on large datasets like OC20 and\nPCQM4Mv2 (Nakata & Shimazaki, 2017) optionally via denoising (Godwin et al., 2022; Zaidi et al.,\n2023) and then fine-tuning on smaller datasets. The second issue might be mitigated by combining\nDFT with ML models. For example, AdsorbML uses ML forces for structural relaxations and a\nsingle-point DFT for calculating the final relaxed energies.\n5\nCONCLUSION\nIn this work, we investigate how equivariant Transformers can be scaled up to higher degrees of\nequivariant representations. We start by replacing SOp3q convolutions in Equiformer with eSCN\nconvolutions, and then we propose three architectural improvements to better leverage the power of\nhigher degrees \u2013 attention re-normalization, separable S2 activation and separable layer normalization.\nWith these modifications, we propose EquiformerV2, which outperforms state-of-the-art methods\non all the tasks on the OC20 and OC22 datasets, improves speed-accuracy trade-offs, and achieves\nthe best success rate when used in AdsorbML. We also compare EquiformerV2 with Equiformer to\nbetter understand the performance gain brought by higher degrees and the improved architecture.\n6\nETHICS STATEMENT\nEquiformerV2 achieves more accurate approximation of quantum mechanical calculations and\ndemonstrates one further step toward being able to replace DFT compute force fields with machine\n10\nPublished as a conference paper at ICLR 2024\nlearned ones for practical applications in chemistry and material science. We hope these promising\nresults will encourage the community to make further progress in applications like material design\nand drug discovery, rather than use these methods for adversarial purposes. We note that these\nmethods only facilitate the identification of molecules or materials with specific properties; there\nremain substantial hurdles to synthesize and deploy such molecules or materials at scale. Finally,\nwe note that the proposed method is general and can be applied to different problems like protein\nstructure prediction (Lee et al., 2022) as long as inputs can be modeled as 3D graphs.\n7\nREPRODUCIBILITY STATEMENT\nWe include details on architectures, hyper-parameters and training time in Sec. D.2 (OC20), Sec. E.1\n(OC22) and Sec. F.2 (QM9).\nThe code for reproducing the results of EquiformerV2 trained on OC20 S2EF-2M and QM9 datasets\nis available at https://github.com/atomicarchitects/equiformer_v2.\nACKNOWLEDGEMENT\nWe thank Larry Zitnick and Saro Passaro for helpful discussions. We also thank Muhammed Shuaibi\nfor helping with the DFT evaluations for AdsorbML (Lan et al., 2022). We acknowledge the MIT\nSuperCloud and Lincoln Laboratory Supercomputing Center (Reuther et al., 2018) for providing\nhigh performance computing and consultation resources that have contributed to the research results\nreported within this paper.\nYi-Lun Liao and Tess Smidt were supported by DOE ICDI grant DE-SC0022215.\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arxiv preprint\narxiv:1607.06450, 2016.\nIlyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. MACE:\nHigher order equivariant message passing neural networks for fast and accurate force fields. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2022.\nSimon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth,\nNicola Molinari, Tess E. Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for\ndata-efficient and accurate interatomic potentials. Nature Communications, 13(1), May 2022. doi:\n10.1038/s41467-022-29939-5. URL https://doi.org/10.1038/s41467-022-29939-5.\nJohannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric\nand physical quantities improve e(3) equivariant message passing. In International Conference on\nLearning Representations (ICLR), 2022. URL https://openreview.net/forum?id=_xwr8gOBeV1.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer\nVision (ECCV), 2020.\nLowik Chanussot*, Abhishek Das*, Siddharth Goyal*, Thibaut Lavril*, Muhammed Shuaibi*,\nMorgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati,\nAnuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary\nUlissi. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 2021. doi:\n10.1021/acscatal.0c04525.\n11\nPublished as a conference paper at ICLR 2024\nTaco S. Cohen, Mario Geiger, Jonas K\u00f6hler, and Max Welling. Spherical CNNs. In International\nConference on Learning Representations (ICLR), 2018. URL https://openreview.net/forum?id=\nHkbd5xZRb.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton,\nLucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer,\nJoan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh\nMahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier,\nAlexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander\nKolesnikov, Filip Paveti\u00b4c, Dustin Tran, Thomas Kipf, Mario Lu\u02c7ci\u00b4c, Xiaohua Zhai, Daniel Keysers,\nJeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. arXiv\npreprint arXiv:2302.05442, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arxiv preprint arxiv:1810.04805, 2019.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representations (ICLR), 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nMildred S Dresselhaus, Gene Dresselhaus, and Ado Jorio. Group theory. Springer, Berlin, Germany,\n2008 edition, March 2007.\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.\narxiv preprint arxiv:2012.09699, 2020.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network\nfunction approximation in reinforcement learning. arXiv preprint arXiv:1702.03118, 2017.\nNathan Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor\nColey, and Vijay Gadepally. Neural scaling of deep chemical models. ChemRxiv, 2022. doi:\n10.26434/chemrxiv-2022-3s512.\nFabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-\ntranslation equivariant attention networks. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\nJohannes Gasteiger, Shankari Giri, Johannes T. Margraf, and Stephan G\u00fcnnemann.\nFast and\nuncertainty-aware directional message passing for non-equilibrium molecules. In Machine Learn-\ning for Molecules Workshop, NeurIPS, 2020a.\nJohannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular\ngraphs. In International Conference on Learning Representations (ICLR), 2020b.\nJohannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan G\u00fcnnemann, Zachary Ulissi,\nC Lawrence Zitnick, and Abhishek Das. GemNet-OC: Developing Graph Neural Networks for\nLarge and Diverse Molecular Simulation Datasets. Transactions on Machine Learning Research\n(TMLR), 2022.\nMario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller, Wouter Boomsma, Bradley Dice, Kos-\ntiantyn Lapchevskyi, Maurice Weiler, Micha\u0142 Tyszkiewicz, Simon Batzner, Dylan Madisetti,\nMartin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel R\u00f8d,\nand Michael Bailey. e3nn/e3nn: 2022-04-13, April 2022. URL https://doi.org/10.5281/zenodo.\n6459381.\nJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural\nmessage passing for quantum chemistry. In International Conference on Machine Learning (ICML),\n2017.\n12\nPublished as a conference paper at ICLR 2024\nJonathan Godwin, Michael Schaarschmidt, Alexander L Gaunt, Alvaro Sanchez-Gonzalez, Yulia\nRubanova, Petar Veli\u02c7ckovi\u00b4c, James Kirkpatrick, and Peter Battaglia. Simple GNN regularisation\nfor 3d molecular property prediction and beyond. In International Conference on Learning\nRepresentations (ICLR), 2022. URL https://openreview.net/forum?id=1wVvweK3oIb.\nB. Hammer, L. B. Hansen, and J. K. N\u00f8rskov. Improved adsorption energetics within density-\nfunctional theory using revised perdew-burke-ernzerhof functionals. Phys. Rev. B, 1999.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with\nstochastic depth. In European Conference on Computer Vision (ECCV), 2016.\nWeile Jia, Han Wang, Mohan Chen, Denghui Lu, Lin Lin, Roberto Car, Weinan E, and Linfeng\nZhang. Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms\nwith machine learning. In Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, SC \u201920. IEEE Press, 2020.\nBowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror.\nLearning from protein structure with geometric vector perceptrons. In International Conference on\nLearning Representations (ICLR), 2021. URL https://openreview.net/forum?id=1YLJDvSx6J4.\nJohannes Klicpera, Florian Becker, and Stephan G\u00fcnnemann. Gemnet: Universal directional graph\nneural networks for molecules. In Advances in Neural Information Processing Systems (NeurIPS),\n2021.\nRisi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch\u2013gordan nets: a fully fourier space spherical\nconvolutional neural network. In Advances in Neural Information Processing Systems 32, pp.\n10117\u201310126, 2018.\nDevin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent L\u00e9tourneau, and Prudencio Tossou.\nRethinking graph transformers with spectral attention. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2021. URL https://openreview.net/forum?id=huAdB-Tj4yG.\nJanice Lan, Aini Palizhati, Muhammed Shuaibi, Brandon M Wood, Brook Wander, Abhishek Das,\nMatt Uyttendaele, C Lawrence Zitnick, and Zachary W Ulissi. AdsorbML: Accelerating adsorption\nenergy calculations with machine learning. arXiv preprint arXiv:2211.16486, 2022.\nTuan Le, Frank No\u00e9, and Djork-Arn\u00e9 Clevert. Equivariant graph attention networks for molecular\nproperty prediction. arXiv preprint arXiv:2202.09891, 2022.\nJae Hyeon Lee, Payman Yadollahpour, Andrew Watkins, Nathan C. Frey, Andrew Leaver-Fay,\nStephen Ra, Kyunghyun Cho, Vladimir Gligorijevic, Aviv Regev, and Richard Bonneau. Equifold:\nProtein structure prediction with a novel coarse-grained structure representation. bioRxiv, 2022.\ndoi: 10.1101/2022.10.07.511322. URL https://www.biorxiv.org/content/early/2022/10/08/\n2022.10.07.511322.\nYi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic\ngraphs. In International Conference on Learning Representations (ICLR), 2023. URL https:\n//openreview.net/forum?id=KwmPfARgOTD.\nYi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical\nmessage passing for 3d molecular graphs. In International Conference on Learning Representations\n(ICLR), 2022. URL https://openreview.net/forum?id=givsRXsOt9r.\nDenghui Lu, Han Wang, Mohan Chen, Lin Lin, Roberto Car, Weinan E, Weile Jia, and Linfeng\nZhang. 86 pflops deep potential molecular dynamics simulation of 100 million atoms with\nab initio accuracy. Computer Physics Communications, 259:107624, 2021. ISSN 0010-4655.\ndoi: https://doi.org/10.1016/j.cpc.2020.107624. URL https://www.sciencedirect.com/science/\narticle/pii/S001046552030299X.\nBenjamin Kurt Miller, Mario Geiger, Tess E. Smidt, and Frank No\u00e9. Relevance of rotationally\nequivariant convolutions for predicting molecular properties. arxiv preprint arxiv:2008.08461,\n2020.\n13\nPublished as a conference paper at ICLR 2024\nAlbert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J. Owen, Mordechai\nKornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic\ndynamics. arxiv preprint arxiv:2204.05249, 2022.\nMaho Nakata and Tomomi Shimazaki. Pubchemqc project: A large-scale first-principles electronic\nstructure database for data-driven chemistry. Journal of chemical information and modeling, 57 6:\n1300\u20131308, 2017.\nSaro Passaro and C Lawrence Zitnick. Reducing SO(3) Convolutions to SO(2) for Efficient Equivari-\nant GNNs. In International Conference on Machine Learning (ICML), 2023.\nZhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R. Manby, and Thomas F.\nMiller. OrbNet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital\nfeatures. The Journal of Chemical Physics, 2020.\nJoshua A Rackers, Lucas Tecot, Mario Geiger, and Tess E Smidt. A recipe for cracking the quantum\nscaling limit with machine learned electron densities. Machine Learning: Science and Technology,\n4(1):015027, feb 2023. doi: 10.1088/2632-2153/acb314. URL https://dx.doi.org/10.1088/\n2632-2153/acb314.\nPrajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. arXiv preprint\narXiv:1710.05941, 2017.\nRaghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum\nchemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.\nAlbert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor,\nBill Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein,\nLauren Milechin, Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, and Peter Michaleas.\nInteractive supercomputing on 40,000 cores for machine learning and data analysis. In 2018 IEEE\nHigh Performance extreme Computing Conference (HPEC), pp. 1\u20136. IEEE, 2018.\nLars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration\nof 166 billion organic small molecules in the chemical universe database gdb-17. Journal of\nChemical Information and Modeling, 52(11):2864\u20132875, 2012. doi: 10.1021/ci300415d. URL\nhttps://doi.org/10.1021/ci300415d. PMID: 23088335.\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W.\nBattaglia. Learning to simulate complex physics with graph networks. In International Conference\non Machine Learning (ICML), 2020.\nV\u00edctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks.\nIn International Conference on Machine Learning (ICML), 2021.\nK. T. Sch\u00fctt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. M\u00fcller. Schnet:\nA continuous-filter convolutional neural network for modeling quantum interactions. In Advances\nin Neural Information Processing Systems (NeurIPS), 2017.\nKristof T. Sch\u00fctt, Oliver T. Unke, and Michael Gastegger. Equivariant message passing for the\nprediction of tensorial properties and molecular spectra. In International Conference on Machine\nLearning (ICML), 2021.\nYu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie Luo, Chang Liu,\nDi He, and Tie-Yan Liu. Benchmarking graphormer on large-scale molecular modeling datasets.\narxiv preprint arxiv:2203.04810, 2022.\nMuhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi,\nand C. Lawrence Zitnick. Rotation invariant graph neural networks using spin convolutions. arxiv\npreprint arxiv:2106.09575, 2021.\nAnuroop Sriram, Abhishek Das, Brandon M Wood, and C. Lawrence Zitnick. Towards training\nbillion parameter graph neural networks for atomic simulations. In International Conference on\nLearning Representations (ICLR), 2022.\n14\nPublished as a conference paper at ICLR 2024\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning\nResearch, 15(56):1929\u20131958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.\nPhilipp Th\u00f6lke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular\npotentials. In International Conference on Learning Representations (ICLR), 2022. URL https:\n//openreview.net/forum?id=zNHzqZ9wrRB.\nNathaniel Thomas, Tess E. Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick\nRiley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point\nclouds. arxiv preprint arXiv:1802.08219, 2018.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9\nJ\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nRaphael J. L. Townshend, Brent Townshend, Stephan Eismann, and Ron O. Dror. Geometric\nprediction: Moving beyond scalars. arXiv preprint arXiv:2006.14163, 2020.\nRichard Tran*, Janice Lan*, Muhammed Shuaibi*, Brandon Wood*, Siddharth Goyal*, Abhishek\nDas, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram,\nZachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges\nfor oxide electrocatalysis. arXiv preprint arXiv:2206.08917, 2022.\nOliver T. Unke and Markus Meuwly. PhysNet: A neural network for predicting energies, forces,\ndipole moments, and partial charges. Journal of Chemical Theory and Computation, 15(6):\n3678\u20133693, may 2019.\nOliver Thorsten Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger, Tess Smidt, and\nKlaus Robert Muller. SE(3)-equivariant prediction of molecular wavefunctions and electronic\ndensities. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in\nNeural Information Processing Systems (NeurIPS), 2021. URL https://openreview.net/forum?\nid=auGY2UQfhSu.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nPetar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua\nBengio. Graph attention networks. In International Conference on Learning Representations\n(ICLR), 2018. URL https://openreview.net/forum?id=rJXMpikCZ.\nMaurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D Steerable\nCNNs: Learning Rotationally Equivariant Features in Volumetric Data. In Advances in Neural\nInformation Processing Systems 32, pp. 10402\u201310413, 2018.\nDaniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic\nnetworks: Deep translation and rotation equivariance. arxiv preprint arxiv:1612.04642, 2016.\nTian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and\ninterpretable prediction of material properties. Physical Review Letters, 2018.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and\nTie-Yan Liu. Do transformers really perform badly for graph representation? In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. URL https://openreview.net/forum?\nid=OeWooOxFwDa.\nSheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro\nSanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denois-\ning for molecular property prediction. In International Conference on Learning Representations\n(ICLR), 2023.\nA. Zee. Group Theory in a Nutshell for Physicists. Princeton University Press, USA, 2016.\n15\nPublished as a conference paper at ICLR 2024\nLinfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep potential molecular\ndynamics: A scalable model with the accuracy of quantum mechanics. Phys. Rev. Lett., 120:\n143001, Apr 2018. doi: 10.1103/PhysRevLett.120.143001. URL https://link.aps.org/doi/10.\n1103/PhysRevLett.120.143001.\nLarry Zitnick, Abhishek Das, Adeesh Kolluru, Janice Lan, Muhammed Shuaibi, Anuroop Sriram,\nZachary Ulissi, and Brandon Wood. Spherical channels for modeling atomic interactions. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2022.\n16\nPublished as a conference paper at ICLR 2024\nAPPENDIX\nA\nAdditional background\nA.1\nGroup theory\nA.2\nEquivariance\nA.3\neSCN convolution\nA.4\nS2 activation\nB\nRelated works\nB.1\nSE(3)/E(3)-equivariant GNNs\nB.2\nInvariant GNNs\nC\nDetails of architecture\nD\nDetails of experiments on OC20\nD.1\nDetailed description of OC20 dataset\nD.2\nTraining details\nD.3\nDetails of running relaxations\nD.4\nDetails of AdsorbML\nD.5\nAdditional comparison with Equiformer on OC20 IS2RE\nE\nDetails of experiments on OC22\nE.1\nTraining details\nF\nDetails of experiments on QM9\nF.1\nAdditional results of training with Noisy Nodes\nF.2\nTraining details\nF.3\nAblation study on architectural improvements\nA\nADDITIONAL BACKGROUND\nWe first provide relevant mathematical background on group theory and equivariance. We note that\nmost of the content is adapted from Equiformer (Liao & Smidt, 2023) and that these works (Zee,\n2016; Dresselhaus et al., 2007) have more in-depth and pedagogical discussions. Then, we provide\nmathematical details of eSCN convolutions.\nA.1\nGROUP THEORY\nDefinition of Groups.\nA group is an algebraic structure that consists of a set G and a binary\noperator \u02dd : G \u02c6 G \u00d1 G. Typically denoted as G, groups satisfy the following four axioms:\n1. Closure: g \u02dd h P G for all g, h P G.\n2. Identity: There exists an identity element e P G such that g \u02dd e \u201c e \u02dd g \u201c g for all g P G.\n3. Inverse: For each g P G, there exists an inverse element g\u00b41 P G such that g \u02dd g\u00b41 \u201c\ng\u00b41 \u02dd g \u201c e.\n4. Associativity: g \u02dd h \u02dd i \u201c pg \u02dd hq \u02dd i \u201c g \u02dd ph \u02dd iq for all g, h, i P G.\nIn this work, we consider 3D Euclidean symmetry, and relevant groups are:\n1. The Euclidean group in three dimensions Ep3q: 3D rotation, translation and inversion.\n2. The special Euclidean group in three dimensions SEp3q: 3D rotation and translation.\n3. The orthogonal group in three dimensions Op3q: 3D rotation and inversion.\n4. The special orthogonal group in three dimensions SOp3q: 3D rotation.\nSince eSCN (Passaro & Zitnick, 2023) and this work only consider equivariance to 3D rotation and\ninvariance to 3D translation but not inversion, we mainly discuss SEp3q-equivariance in the main\ntext and in appendix and note that more details of Ep3q-equivariance can be found in the work of\nEquiformer (Liao & Smidt, 2023).\n17\nPublished as a conference paper at ICLR 2024\nGroup Representations.\nGiven a vector space X, the way a group G acts on X is given by\nthe group representation DX. DX is parameterized by g P G, with DXpgq : X \u00d1 X. Group\nrepresentations DX are invertible matrices, and group transformations, or group actions, take the\nform of matrix multiplications. This definition of group representations satisfies the requirements of\ngroups, including associativity, DpgqDphq \u201c Dpg \u02dd hq for all g, h P G. We say that the two group\nrepresentations Dpgq and D1pgq are equivalent if there exists a change-of-basis N \u02c6 N matrix P\nsuch that P \u00b41DpgqP \u201c D1pgq for all g P G. Dpgq is reducible if D1pgq is block diagonal for all\ng P G, meaning that D1pgq acts on multiple independent subspaces of the vector space. Otherwise,\nthe representation Dpgq is said to be irreducible. Irreducible representations, or irreps, are a class\nof representations that are convenient for composing different group representations. Specifically,\nfor the case of SOp3q, Wigner-D matrices are irreducible representations, and we can express any\ngroup representation of SOp3q as a direct sum (concatentation) of Wigner-D matrices (Zee, 2016;\nDresselhaus et al., 2007; Geiger et al., 2022):\nDpgq \u201c P \u00b41\n\u02dc\n\u00e0\ni\nDpLiqpgq\n\u00b8\nP \u201c P \u00b41\n\u00a8\n\u02da\n\u02dd\nDpL0qpgq\nDpL1qpgq\n...\n\u02db\n\u2039\u201aP\n(2)\nwhere DpLiqpgq are Wigner-D matrices of degree Li.\nA.2\nEQUIVARIANCE\nA function f mapping between vector spaces X and Y is equivariant to a group of transformations G if\nfor any input x P X, output y P Y and group element g P G, we have fpDXpgqxq \u201c DY pgqfpxq \u201c\nDY pgqy, where DXpgq and DY pgq are transformation matrices or group representations parametrized\nby g in X and Y . Additionally, f is invariant when DY pgq is an identity matrix for any g P G.\nAs neural networks comprise many composable operations, equivariant neural networks comprise\nmany equivariant operations to maintain the equivariance of input, intermediate, and output features.\nIncorporating equivariance as a strong prior knowledge can help improve data efficiency and general-\nization of neural networks (Batzner et al., 2022; Rackers et al., 2023; Frey et al., 2022). In this work,\nwe achieve equivariance to 3D rotation by operating on vector spaces of SOp3q irreps, incorporate\ninvariance to 3D translation by acting on relative positions, but do not consider inversion.\nA.3\nESCN CONVOLUTION\nMessage passing is used to update equivariant irreps features and is typically implemented as SOp3q\nconvolutions. A traditional SOp3q convolution interacts input irrep features xpLiq\nmi\nand spherical\nharmonic projections of relative positions Y pLf q\nmf\np\u20d7rtsq with an SOp3q tensor product with Clebsch-\nGordan coefficients CpLo,moq\npLi,miq,pLf ,mf q. Since tensor products are compute-intensive, eSCN convolu-\ntions (Passaro & Zitnick, 2023) are proposed to reduce the complexity of tensor products when they\nare used in SOp3q convolutions. Rotating the input irreps features xpLiq\nmi\nbased on the relative position\nvectors \u20d7rts simplifies the tensor products and enables reducing SOp3q convolutions to SOp2q linear\noperations. Below we provide the mathematical details of SOp3q convolutions built from tensor\nproducts and how rotation can reduce their computational complexity.\nTensor products interact type-Li vector xpLiq and type-Lf vector f pLf q to produce type-Lo\nvector ypLoq with Clebsch-Gordan coefficients CpLo,moq\npLi,miq,pLf ,mf q.\nClebsch-Gordan coefficients\nCpLo,moq\npLi,miq,pLf ,mf q are non-zero only when |Li \u00b4 Lo| \u010f Lf \u010f |Li ` Lo|. Each non-trivial com-\nbination of Li b Lf \u00d1 Lo is called a path, and each path is independently equivariant and can be\nassigned a learnable weight wLi,Lf ,Lo.\nWe consider the message mts sent from source node s to target node t in an SOp3q convolution. The\nLo-th degree of mts can be expressed as:\nmpLoq\nts\n\u201c\n\u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00b4\nxpLiq\ns\nbLo\nLi,Lf Y pLf qp\u02c6rtsq\n\u00af\n(3)\n18\nPublished as a conference paper at ICLR 2024\nwhere xs is the irreps feature at source node s, xpLiq\ns\ndenotes the Li-th degree of xs, and \u02c6rts \u201c\n\u20d7rts\n|\u20d7rts|.\nThe spherical harmonic projection of relative positions Y pLf qp\u02c6rtsq becomes sparse if we rotate \u02c6rts\nwith a rotation matrix Rts to align with the direction of L \u201c 0 and m \u201c 0, which corresponds to\nthe z axis traditionally but the y axis in the conventions of e3nn (Geiger et al., 2022). Concretely,\ngiven Rts\u02c6rts aligned with the y axis, Y pLf q\nmf\npRts\u02c6rtsq \u2030 0 only for mf \u201c 0. Without loss of\nequivariance, we re-scale Y pLf q\n0\npRts\u20d7rtsq to be one. Besides, we denote DpLiqpRtsq \u201c DpLiq and\nDpLoqpRtsq \u201c DpLoq as Wigner-D matrices of degrees Li and Lo based on rotation matrix Rts,\nrespectively, and we define DpLiqxpLiq\ns\n\u201c \u02dcxpLiq\ns\n. Therefore, by rotating xpLiq\ns\nand Y pLf q based on\n\u02c6rts, we can simplify Eq. 3 as follows:\nmpLoq\nts\n\u201c\n\u00b4\nDpLoqpRtsq\n\u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00b4\nDpLiqpRtsqxpLiq\ns\nbLo\nLi,Lf Y pLf qpRts\u02c6rtsq\n\u00af\n\u201c\n\u00b4\nDpLoq\u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00e0\nmo\n\u02dc \u00ff\nmi,mf\n\u00b4\nDpLiqxpLiq\ns\n\u00af\nmi CpLo,moq\npLi,miq,pLf ,mf q\n\u00b4\nY pLf qpRts \u02c6\nrtsq\n\u00af\nmf\n\u00b8\n\u201c\n\u00b4\nDpLoq\u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00e0\nmo\n\u02dc\u00ff\nmi\n\u00b4\nDpLiqxpLiq\ns\n\u00af\nmi CpLo,moq\npLi,miq,pLf ,0q\n\u00b8\n\u201c\n\u00b4\nDpLoq\u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00e0\nmo\n\u02dc\u00ff\nmi\n\u00b4\n\u02dcxpLiq\ns\n\u00af\nmi\nCpLo,moq\npLi,miq,pLf ,0q\n\u00b8\n(4)\nwhere \u00c0 denotes concatenation.\nAdditionally, given mf \u201c 0, Clebsch-Gordan coefficients\nCpLo,moq\npLi,miq,pLf ,0q are sparse and are non-zero only when mi \u201c \u02d8mo, which further simplifies Eq. 4:\nmpLoq\nts\n\u201c\n\u00b4\nDpLoq\u00af\u00b41 \u00ff\nLi,Lf\nwLi,Lf ,Lo\n\u00e0\nmo\n\u02c6\u00b4\n\u02dcxpLiq\ns\n\u00af\nmo CpLo,moq\npLi,moq,pLf ,0q `\n\u00b4\n\u02dcxpLiq\ns\n\u00af\n\u00b4mo CpLo,moq\npLi,\u00b4moq,pLf ,0q\n\u02d9\n(5)\nBy re-ordering the summations and concatenation in Eq. 5, we have:\n\u00b4\nDpLoq\u00af\u00b41 \u00ff\nLi\n\u00e0\nmo\n\u00a8\n\u02dd\n\u00b4\n\u02dcxpLiq\ns\n\u00af\nmo\n\u00ff\nLf\n\u00b4\nwLi,Lf ,LoCpLo,moq\npLi,moq,pLf ,0q\n\u00af\n`\n\u00b4\n\u02dcxpLiq\ns\n\u00af\n\u00b4mo\n\u00ff\nLf\n\u00b4\nwLi,Lf ,LoCpLo,moq\npLi,\u00b4moq,pLf ,0q\n\u00af\n\u02db\n\u201a\n(6)\nInstead of using learnable parameters for wLi,Lf ,Lo, eSCN proposes to parametrize \u02dcwpLi,Loq\nmo\nand\n\u02dcwpLi,Loq\n\u00b4mo\nas below:\n\u02dcwpLi,Loq\nmo\n\u201c\n\u00ff\nLf\nwLi,Lf ,LoCpLo,moq\npLi,moq,pLf ,0q \u201c\n\u00ff\nLf\nwLi,Lf ,LoCpLo,\u00b4moq\npLi,\u00b4moq,pLf ,0q\nfor m \u0105\u201c 0\n\u02dcwpLi,Loq\n\u00b4mo\n\u201c\n\u00ff\nLf\nwLi,Lf ,LoCpLo,\u00b4moq\npLi,moq,pLf ,0q \u201c \u00b4\n\u00ff\nLf\nwLi,Lf ,LoCpLo,moq\npLi,\u00b4moq,pLf ,0q\nfor m \u0105 0\n(7)\nThe parametrization of \u02dcwpLi,Loq\nmo\nand \u02dcwpLi,Loq\n\u00b4mo\nenables removing the summation over Lf and further\nsimplifies the computation. By combining Eq. 6 and Eq. 7, we have:\nmpLoq\nts\n\u201c\n\u00b4\nDpLoq\u00af\u00b41 \u00ff\nLi\n\u00e0\nmo\n\u00b4\nypLi,Loq\nts\n\u00af\nmo\n\u00b4\nypLi,Loq\nts\n\u00af\nmo \u201c \u02dcwpLi,Loq\nmo\n\u00b4\n\u02dcxpLiq\ns\n\u00af\nmo \u00b4 \u02dcwpLi,Loq\n\u00b4mo\n\u00b4\n\u02dcxpLiq\ns\n\u00af\n\u00b4mo\nfor mo \u0105 0\n\u00b4\nypLi,Loq\nts\n\u00af\n\u00b4mo \u201c \u02dcwpLi,Loq\n\u00b4mo\n\u00b4\n\u02dcxpLiq\ns\n\u00af\nmo ` \u02dcwpLi,Loq\nmo\n\u00b4\n\u02dcxpLiq\ns\n\u00af\n\u00b4mo\nfor mo \u0105 0\n\u00b4\nypLi,Loq\nts\n\u00af\nmo \u201c \u02dcwpLi,Loq\nmo\n\u00b4\n\u02dcxpLiq\ns\n\u00af\nmo\nfor mo \u201c 0\n(8)\n19\nPublished as a conference paper at ICLR 2024\nThe formulation of ypLi,Loq\nts\ncoincides with performing SOp2q linear operations (Worrall et al., 2016;\nPassaro & Zitnick, 2023). Additionally, eSCN convolutions can further simplify the computation by\nconsidering only a subset of mo components in Eq. 8, i.e., |mo| \u010f Mmax.\nIn summary, efficient SOp3q convolutions can be achieved by first rotating irreps features xpLiq\ns\nbased\non relative position vectors \u20d7rts and then performing SOp2q linear operations on rotated features.\nThe key idea is that rotation simplifies the computation as in Eq. 4, 5, 7, and 8. Please refer to their\nwork (Passaro & Zitnick, 2023) for more details. We note that eSCN convolutions consider only\nsimplifying the case of taking tensor products between input irreps features and spherical harmonic\nprojections of relative position vectors. eSCN convolutions do not simplify general cases such as\ntaking tensor products between input irreps features and themselves (Batatia et al., 2022) since the\nrelative position vectors used to rotate irreps features are not clearly defined.\nA.4\nS2 ACTIVATION\nS2 activation was first proposed in Spherical CNNs (Cohen et al., 2018). Our implementation\nof S2 activation is the same as that in e3nn (Geiger et al., 2022), SCN (Zitnick et al., 2022) and\neSCN (Passaro & Zitnick, 2023). Basically, we uniformly sample a fixed set of points on a unit\nsphere along the dimensions of longitude, parametrized by \u03b1 P r0, 2\u03c0q, and latitude, parametrized\nby \u03b2 P r0, \u03c0q. We set the resolutions R of \u03b1 and \u03b2 to be 18 when Lmax \u201c 6, meaning that we\nwill have 324 p\u201c 18 \u02c6 18q points. Once the points are sampled, they are kept the same during\ntraining and inference, and therefore there is no randomness. For each point on the unit sphere, we\ncompute the spherical harmonics projection of degrees up to Lmax. We consider an equivariant\nfeature of C channels and each channel contains vectors of all degrees from 0 to Lmax. When\nperforming S2 activation, for each channel and for each sampled point, we first compute the inner\nproduct between the vectors of all degrees contained in one channel of the equivariant feature and the\nspherical harmonics projections of a sampled point. This results in R \u02c6 R \u02c6 C values, where the\nfirst two dimensions, R \u02c6 R, correspond to grid resolutions and the last dimension corresponds to\nchannels. They can be viewed as 2D grid feature maps and treated as scalars, and we can apply any\nstandard or typical activation functions like SiLU or use standard linear layers performing feature\naggregation along the channel dimension. After applying these functions, we project back to vectors\nof all degrees by multiplying those values with their corresponding spherical harmonics projections of\nsampled points. The process is the same as performing a Fourier transform, applying some functions\nand then performing an inverse Fourier transform.\nMoreover, since the inner products between one channel of vectors of all degrees and the spherical\nharmonics projections of sampled points sum over all degrees, the conversion to 2D grid feature\nmaps implicitly considers the information of all degrees. Therefore, S2 activation, which converts\nequivariant features into 2D grid feature maps, uses the information of all degrees to determine the\nnon-linearity. In contrast, gate activation only uses vectors of degree 0 to determine the non-linearity\nof vectors of higher degrees. For tasks such as force predictions, where the information of degrees\nis critical, S2 activation can be better than gate activation since S2 activation uses all degrees to\ndetermine non-linearity.\nAlthough there is sampling on a sphere, the works (Cohen et al., 2018; Passaro & Zitnick, 2023)\nmention that as long as the number of samples, or resolution R, is high enough, the equivariance\nerror can be close to zero. Furthermore, eSCN (Passaro & Zitnick, 2023) empirically computes such\nerrors in Figure 9 in their latest manuscript and shows that the errors of using Lmax \u201c 6 and R \u201c 18\nare close to 0.2%, which is similar to the equivariance errors of tensor products in e3nn (Geiger et al.,\n2022). We note that the equivariance errors in e3nn are due to numerical precision.\nB\nRELATED WORKS\nB.1\nSE(3)/E(3)-EQUIVARIANT GNNS\nEquivariant neural networks (Thomas et al., 2018; Kondor et al., 2018; Weiler et al., 2018; Fuchs et al.,\n2020; Miller et al., 2020; Townshend et al., 2020; Batzner et al., 2022; Jing et al., 2021; Sch\u00fctt et al.,\n2021; Satorras et al., 2021; Unke et al., 2021; Brandstetter et al., 2022; Th\u00f6lke & Fabritiis, 2022; Le\net al., 2022; Musaelian et al., 2022; Batatia et al., 2022; Liao & Smidt, 2023; Passaro & Zitnick, 2023)\n20\nPublished as a conference paper at ICLR 2024\nuse equivariant irreps features built from vector spaces of irreducible representations (irreps) to achieve\nequivariance to 3D rotation (Thomas et al., 2018; Weiler et al., 2018; Kondor et al., 2018). They\noperate on irreps features with equivariant operations like tensor products. Previous works differ in\nequivariant operations used in their networks and how they combine those operations. TFN (Thomas\net al., 2018) and NequIP (Batzner et al., 2022) use equivariant graph convolution with linear messages\nbuilt from tensor products, with the latter utilizing extra equivariant gate activation (Weiler et al.,\n2018). SEGNN (Brandstetter et al., 2022) introduces non-linearity to messages passing (Gilmer\net al., 2017; Sanchez-Gonzalez et al., 2020) with equivariant gate activation, and the non-linear\nmessages improve upon linear messages. SE(3)-Transformer (Fuchs et al., 2020) adopts equivariant\ndot product attention (Vaswani et al., 2017) with linear messages. Equiformer (Liao & Smidt, 2023)\nimproves upon previously mentioned equivariant GNNs by combining MLP attention and non-linear\nmessages. Equiformer additionally introduces equivariant layer normalization and regularizations like\ndropout (Srivastava et al., 2014) and stochastic depth (Huang et al., 2016). However, the networks\nmentioned above rely on compute-intensive SOp3q tensor products to mix the information of vectors\nof different degrees during message passing, and therefore they are limited to small values for\nmaximum degrees Lmax of equivariant representations. SCN (Zitnick et al., 2022) proposes rotating\nirreps features based on relative position vectors and identifies a subset of spherical harmonics\ncoefficients, on which they can apply unconstrained functions. They further propose relaxing the\nrequirement for strict equivariance and apply typical functions to rotated features during message\npassing, which trades strict equivariance for computational efficiency and enables using higher values\nof Lmax. eSCN (Passaro & Zitnick, 2023) further improves upon SCN by replacing typical functions\nwith SOp2q linear layers for rotated features and imposing strict equivariance during message passing.\nHowever, except using more efficient operations for higher Lmax, SCN and eSCN mainly adopt\nthe same network design as SEGNN, which is less performant than Equiformer. In this work, we\npropose EquiformerV2, which includes all the benefits of the above networks by incorporating eSCN\nconvolutions into Equiformer and adopts three additional architectural improvements.\nB.2\nINVARIANT GNNS\nPrior works (Sch\u00fctt et al., 2017; Xie & Grossman, 2018; Unke & Meuwly, 2019; Gasteiger et al.,\n2020b;a; Qiao et al., 2020; Liu et al., 2022; Shuaibi et al., 2021; Klicpera et al., 2021; Sriram et al.,\n2022; Gasteiger et al., 2022) extract invariant information from 3D atomistic graphs and operate on\nthe resulting graphs augmented with invariant features. Their differences lie in leveraging different\ngeometric features such as distances, bond angles (3 atom features) or dihedral angles (4 atom\nfeatures). SchNet (Sch\u00fctt et al., 2017) models interaction between atoms with only relative distances.\nDimeNet series (Gasteiger et al., 2020b;a) use triplet representations of atoms to incorporate bond\nangles. SphereNet (Liu et al., 2022) and GemNet (Klicpera et al., 2021; Gasteiger et al., 2022) further\ninclude dihedral angles by considering quadruplet representations. However, the memory complexity\nof triplet and quadruplet representations of atoms do not scale well with the number of atoms, and\nthis requires additional modifications like interaction hierarchy used by GemNet-OC (Gasteiger et al.,\n2022) for large datasets like OC20 (Chanussot* et al., 2021). Additionally, for the task of predicting\nDFT calculations of energies and forces on the large-scale OC20 dataset, invariant GNNs have been\nsurpassed by equivariant GNNs recently.\nC\nDETAILS OF ARCHITECTURE\nIn this section, we define architectural hyper-parameters like maximum degrees and numbers of\nchannels in certain layers in EquiformerV2, which are used to specify the detailed architectures in\nSec. D.2, Sec. D.5, Sec. E.1 and Sec. F.2. Besides, we note that eSCN (Passaro & Zitnick, 2023) and\nthis work mainly consider SEp3q-equivariance.\nWe denote embedding dimensions as dembed, which defines the dimensions of most irreps features.\nSpecifically, the output irreps features of all modules except the output head in Figure 1a have\ndimension dembed. For separable S2 activation as illustrated in Figure 2c, we denote the resolution\nof point samples on a sphere as R, which can depend on maximum degree Lmax, and denote the\nunconstrained functions after projecting to point samples as F.\nFor equivariant graph attention in Figure 1b, the input irreps features xi and xj have dimension\ndembed. The dimension of the irreps feature f pLq\nij\nis denoted as dattn_hidden. Equivariant graph\n21\nPublished as a conference paper at ICLR 2024\nattention can have h parallel attention functions. For each attention function, we denote the dimension\nof the scalar feature f p0q\nij\nas dattn_alpha and denote the dimension of the value vector, which is in\nthe form of irreps features, as dattn_value. For the separable S2 activation used in equivariant graph\nattention, the resolution of point samples is R, and we use a single SiLU activation for F. We share\nthe layer normalization in attention re-normalization across all h attention functions but have different\nh linear layers after that. The last linear layer projects the dimension back to dembed. The two\nintermediate SOp2q linear layers operate with maximum degree Lmax and maximum order Mmax.\nFor feed forward networks (FFNs) in Figure 1d, we denote the dimension of the output irreps features\nof the first linear layer as dffn. For the separable S2 activation used in FFNs, the resolution of point\nsamples is R, and F consists of a two-layer MLP, with each linear layer followed by SiLU, and a\nfinal linear layer. The linear layers have the same number of channels as dffn.\nFor radial functions, we denote the dimension of hidden scalar features as dedge. For experiments on\nOC20, same as eSCN (Passaro & Zitnick, 2023), we use Gaussian radial basis to represent relative\ndistances and additionally embed the atomic numbers at source nodes and target nodes with two\nscalar features of dimension dedge. The radial basis and the two embeddings of atomic numbers are\nfed to the radial function to generate edge distance embeddings.\nThe maximum degree of irreps features is denoted as Lmax. All irreps features have degrees from 0\nto Lmax and have C channels for each degree. We denote the dimension as pLmax, Cq. For example,\nirreps feature xirreps of dimension p6, 128q has maximum degree 6 and 128 channels for each degree.\nThe dimension of scalar feature xscalar can be expressed as p0, Cscalarq.\nFollowing Equiformer (Liao & Smidt, 2023), we apply dropout (Srivastava et al., 2014) to attention\nweights and stochastic depth (Huang et al., 2016) to outputs of equivariant graph attention and feed\nforward networks. However, we do not apply dropout or stochastic depth to the output head.\nD\nDETAILS OF EXPERIMENTS ON OC20\nD.1\nDETAILED DESCRIPTION OF OC20 DATASET\nThe large and diverse OC20 dataset (Chanussot* et al., 2021) (Creative Commons Attribution 4.0\nLicense) consists of 1.2M DFT relaxations for training and evaluation, computed with the revised\nPerdew-Burke-Ernzerhof (RPBE) functional (Hammer et al., 1999). Each structure in OC20 has\nan adsorbate molecule placed on a catalyst surface, and the core task is Structure to Energy Forces\n(S2EF), which is to predict the energy of the structure and per-atom forces. Models trained for the\nS2EF task are evaluated on energy and force mean absolute error (MAE). These models can in turn\nbe used for performing structure relaxations by using the model\u2019s force predictions to iteratively\nupdate the atomic positions until a relaxed structure corresponding to a local energy minimum\nis found. These relaxed structure and energy predictions are evaluated on the Initial Structure to\nRelaxed Structure (IS2RS) and Initial Structure to Relaxed Energy (IS2RE) tasks. The \u201cAll\u201d split\nof OC20 contains 134M training structures spanning 56 elements, the \u201cMD\u201d split consists of 38M\nstructures, and the \u201c2M\u201d split has 2M structures. For validation and test splits, there are four sub-splits\ncontaining in-distribution adsorbates and catalysts (ID), out-of-distribution adsorbates (OOD Ads),\nout-of-distribution catalysts (OOD Cat), and out-of-distribution adsorbates and catalysts (OOD Both).\nD.2\nTRAINING DETAILS\nHyper-Parameters.\nWe summarize the hyper-parameters for the base model setting on OC20\nS2EF-2M dataset and the main results on OC20 S2EF-All and S2EF-All+MD datasets in Table 7.\nFor the ablation studies on OC20 S2EF-2M dataset, when trained for 20 or 30 epochs as in Table 1b,\nwe increase the learning rate from 2 \u02c6 10\u00b44 to 4 \u02c6 10\u00b44. When using Lmax \u201c 8 as in Table 1c,\nwe increase the resolution of point samples R from 18 to 20. We vary Lmax and the widths for\nspeed-accuracy trade-offs in Figure 4. Specifically, we first decrease Lmax from 6 to 4. Then, we\nmultiply h and the number of channels of pdembed, dattn_hidden, dffnq by 0.75 and 0.5. We train all\nmodels for 30 epochs. The same strategy to scale down eSCN models is adopted for fair comparisons.\n22\nPublished as a conference paper at ICLR 2024\nBase model setting on\nEquiformerV2 (31M) on\nEquiformerV2 (153M) on\nHyper-parameters\nS2EF-2M\nS2EF-All+MD\nS2EF-All/S2EF-All+MD\nOptimizer\nAdamW\nAdamW\nAdamW\nLearning rate scheduling\nCosine learning rate with\nCosine learning rate with\nCosine learning rate with\nlinear warmup\nlinear warmup\nlinear warmup\nWarmup epochs\n0.1\n0.01\n0.01\nMaximum learning rate\n2 \u02c6 10\u00b44\n4 \u02c6 10\u00b44\n4 \u02c6 10\u00b44\nBatch size\n64\n512\n256 for S2EF-All,\n512 for S2EF-All+MD\nNumber of epochs\n12\n3\n1\nWeight decay\n1 \u02c6 10\u00b43\n1 \u02c6 10\u00b43\n1 \u02c6 10\u00b43\nDropout rate\n0.1\n0.1\n0.1\nStochastic depth\n0.05\n0.1\n0.1\nEnergy coefficient \u03bbE\n2\n4\n2 for S2EF-All,\n2, 4 for S2EF-All+MD\nForce coefficient \u03bbF\n100\n100\n100\nGradient clipping norm threshold\n100\n100\n100\nModel EMA decay\n0.999\n0.999\n0.999\nCutoff radius (\u00c5)\n12\n12\n12\nMaximum number of neighbors\n20\n20\n20\nNumber of radial bases\n600\n600\n600\nDimension of hidden scalar features in radial functions dedge\np0, 128q\np0, 128q\np0, 128q\nMaximum degree Lmax\n6\n4\n6\nMaximum order Mmax\n2\n2\n3\nNumber of Transformer blocks\n12\n8\n20\nEmbedding dimension dembed\np6, 128q\np4, 128q\np6, 128q\nf pLq\nij\ndimension dattn_hidden\np6, 64q\np4, 64q\np6, 64q\nNumber of attention heads h\n8\n8\n8\nf p0q\nij\ndimension dattn_alpha\np0, 64q\np0, 64q\np0, 64q\nValue dimension dattn_value\np6, 16q\np4, 16q\np6, 16q\nHidden dimension in feed forward networks dffn\np6, 128q\np4, 128q\np6, 128q\nResolution of point samples R\n18\n18\n18\nTable 7: Hyper-parameters for the base model setting on OC20 S2EF-2M dataset and the main results on OC20\nS2EF-All and S2EF-All+MD datasets.\nAttention\nNumber of\nTraining time\nInference speed\nNumber of\nTraining set\nre-normalization\nActivation\nNormalization\nLmax\nMmax\nTransformer blocks\n(GPU-hours)\n(Samples / GPU sec.)\nparameters\nS2EF-2M\n\u2717\nGate\nLN\n6\n2\n12\n965\n19.06\n91.06M\n\u2713\nGate\nLN\n6\n2\n12\n998\n19.07\n91.06M\n\u2713\nS2\nLN\n6\n2\n12\n1476\n12.80\n81.46M\n\u2713\nSep. S2\nLN\n6\n2\n12\n1505\n12.51\n83.16M\n\u2713\nSep. S2\nSLN\n6\n2\n12\n1412\n13.22\n83.16M\n\u2713\nSep. S2\nSLN\n4\n2\n12\n965\n19.86\n44.83M\n\u2713\nSep. S2\nSLN\n8\n2\n12\n2709\n7.86\n134.28M\n\u2713\nSep. S2\nSLN\n6\n3\n12\n1623\n11.92\n95.11M\n\u2713\nSep. S2\nSLN\n6\n4\n12\n2706\n7.98\n102.14M\n\u2713\nSep. S2\nSLN\n6\n6\n12\n3052\n7.13\n106.63M\nS2EF-All\n\u2713\nSep. S2\nSLN\n6\n3\n20\n20499\n6.08\n153.60M\nS2EF-All+MD (\u03bbE \u201c 2)\n\u2713\nSep. S2\nSLN\n6\n3\n20\n32834\n6.08\n153.60M\nS2EF-All+MD (\u03bbE \u201c 4)\n\u2713\nSep. S2\nSLN\n4\n2\n8\n16931\n29.21\n31.06M\nS2EF-All+MD (\u03bbE \u201c 4)\n\u2713\nSep. S2\nSLN\n6\n3\n20\n37692\n6.08\n153.60M\nTable 8: Training time, inference speed and numbers of parameters of different models trained on OC20\nS2EF-2M, S2EF-All and S2EF-All+MD datasets. All numbers are measured on V100 GPUs with 32GB.\nTraining Time, Inference Speed and Numbers of Parameters.\nTable 8 summarizes the training\ntime, inference speed and numbers of parameters of models in Tables 1a (Index 1, 2, 3, 4, 5), 1c, 1d\nand 2. V100 GPUs with 32GB are used to train all models. We use 16 GPUs to train each individual\nmodel on S2EF-2M dataset, 64 GPUs for S2EF-All, 64 GPUs for EquiformerV2 (31M) on S2EF-\nAll+MD, and 128 GPUs for EquiformerV2 (153M) on S2EF-All+MD.\nD.3\nDETAILS OF RUNNING RELAXATIONS\nA structural relaxation is a local optimization where atom positions are iteratively updated based\non forces to minimize the energy of the structure. We perform ML relaxations using the LBFGS\noptimizer (quasi-Newton) implemented in the Open Catalyst Github repository (Chanussot* et al.,\n2021). The structural relaxations for OC20 IS2RE and IS2RS tasks are allowed to run for 200\nsteps or until the maximum predicted force per atom Fmax \u010f 0.02 eV/\u00c5 , and the relaxations for\nAdsorbML are allowed to run for 300 steps or until Fmax \u010f 0.02 eV/\u00c5 . These settings are chosen to\nbe consistent with prior works. We run relaxations on V100 GPUs with 32GB. The computational\ncost of running relaxations with EquiformerV2 (153M) for OC20 IS2RE and IS2RS tasks is 1011\nGPU-hours, and that of running ML relaxations for AdsorbML is 1075 GPU-hours. The time for\n23\nPublished as a conference paper at ICLR 2024\nFigure 5: Speed-accuracy trade-offs of different models when used in the AdsorbML algorithm.\nrunning relaxations with EquiformerV2 (31M) is 240 GPU-hours for OC20 IS2RE and IS2RS and\n298 GPU-hours for AdsorbML.\nD.4\nDETAILS OF ADSORBML\nWe run the AdsorbML algorithm on the OC20-Dense dataset in accordance with the procedure laid\nout in the paper (Lan et al., 2022), which is summarized here:\n1. Run ML relaxations on all initial structures in the OC20-Dense dataset. There are around\n1000 different adsorbate-surface combinations with about 90 adsorbate placements per\ncombination, and therefore we have roughly 90k structures in total.\n2. Remove invalid ML relaxed structures based on physical constraints and rank the other ML\nrelaxed structures in order of lowest to highest ML predicted energy.\n3. Take the top k ML relaxed structures with the lowest ML predicted energies for each\nadsorbate-surface combination and run DFT single-point calculations. The single-point\ncalculations are performed on the ML relaxed structures to improve the energy predictions\nwithout running a full DFT relaxation and are run with VASP using the same setting as the\noriginal AdsorbML experiments. As shown in Table 3, we vary k from 1 to 5.\n4. Compute success and speedup metrics based on our lowest DFT single-point energy per\nadsorbate-surface combination and the DFT labels provided in the OC20-Dense dataset.\nTo better understand the speed-accuracy trade-offs of different models, we compare the AdsorbML\nsuccess rate averaged over k from 1 to 5 and average GPU-seconds of running one structure relaxation\nin Figure 5. We visualize some examples of relaxed structures from eSCN (Passaro & Zitnick, 2023),\nEquiformerV2 and DFT in Figure 6.\nD.5\nADDITIONAL COMPARISON WITH EQUIFORMER ON OC20 IS2RE\nTraining Details.\nWe follow the same setting as Equiformer (Liao & Smidt, 2023) and train two\nEquiformerV2 models on OC20 IS2RE dataset without and with IS2RS auxiliary task. We use the\nsame radial basis function as Equiformer. When IS2RS auxiliary task is adopted, we use a linearly\ndecayed weight for loss associated with IS2RS, which starts at 15 and decays to 1 and adopt Noisy\nNodes data augmentation (Godwin et al., 2022). The hyper-parameters are summarized in Table 9.\nWe train EquiformerV2 on OC20 IS2RE with 16 V100 GPUs with 32GB and train on OC20 IS2RE\nwith IS2RS auxiliary task and Noisy Nodes data augmentation with 32 V100 GPUs. The training\ncosts are 574 and 2075 GPU-hours, and the numbers of parameters are 36.03M and 95.24M.\nResults.\nThe comparison is shown in Table 10. Without IS2RS auxiliary task, EquiformerV2\noverfits the training set due to higher degrees and achieves worse results than Equiformer. However,\nwith IS2RS auxiliary task and Noisy Nodes data augmentation, EquiformerV2 achieves better energy\nMAE. The different rankings of models under different settings is also found in Noisy Nodes (Godwin\n24\nPublished as a conference paper at ICLR 2024\nEquiformerV2 on OC20 IS2RE\nEquiformerV2 on OC20 IS2RE\nwith IS2RS auxiliary task and Noisy Nodes\nOptimizer\nAdamW\nAdamW\nLearning rate scheduling\nCosine learning rate with\nCosine learning rate with\nlinear warmup\nlinear warmup\nWarmup epochs\n2\n2\nMaximum learning rate\n4 \u02c6 10\u00b44\n5 \u02c6 10\u00b44\nBatch size\n32\n64\nNumber of epochs\n20\n40\nWeight decay\n1 \u02c6 10\u00b43\n1 \u02c6 10\u00b43\nDropout rate\n0.2\n0.2\nStochastic depth\n0.05\n0.1\nGradient clipping norm threshold\n100\n100\nModel EMA decay\n0.999\n0.999\nCutoff radius (\u00c5)\n5.0\n5.0\nMaximum number of neighbors\n50\n50\nNumber of radial basis\n128\n128\nDimension of hidden scalar features in radial functions dedge\np0, 64q\np0, 64q\nMaximum degree Lmax\n6\n6\nMaximum order Mmax\n2\n2\nNumber of Transformer blocks\n6\n15\nEmbedding dimension dembed\np6, 128q\np6, 128q\nf pLq\nij\ndimension dattn_hidden\np6, 64q\np6, 64q\nNumber of attention heads h\n8\n8\nf p0q\nij\ndimension dattn_alpha\np0, 64q\np0, 64q\nValue dimension dattn_value\np6, 16q\np6, 16q\nHidden dimension in feed forward networks dffn\np6, 128q\np6, 128q\nResolution of point samples R\n18\n18\nTable 9: Hyper-parameters for OC20 IS2RE dataset.\nIS2RE direct\nIS2RE direct with IS2RS auxiliary task and Noisy Nodes\nEnergy MAE (meV)\u00d3\nEnergy MAE (meV)\u00d3\nModel\nID\nOOD Ads\nOOD Cat\nOOD Both\nAverage\nID\nOOD Ads\nOOD Cat\nOOD Both\nAverage\nEquiformer (Liao & Smidt, 2023)\n508.8\n627.1\n505.1\n554.5\n548.9\n415.6\n497.6\n416.5\n434.4\n441.0\nEquiformerV2\n516.1\n704.1\n524.5\n636.5\n595.3\n400.4\n459.0\n406.2\n401.8\n416.9\nTable 10: OC20 IS2RE results on the validation split. With IS2RS auxiliary task and Noisy Nodes (Godwin\net al., 2022) data augmentation, EquiformerV2 achieves better energy MAE.\net al., 2022), where they mention a node-level auxiliary task can prevent overfitting and enable more\nexpressive models to perform better.\nE\nDETAILS OF EXPERIMENTS ON OC22\nE.1\nTRAINING DETAILS\nThe hyper-parameters for OC22 dataset is summarized in Table 11. We use 32 V100 GPUs with\n32GB and train two EquiformerV2 models with different energy coefficients \u03bbE and force coefficeints\n\u03bbF . The number of parameters is 121.53M, and the training cost of each model is 4552 GPU-hours.\nThe time for running relaxations for OC22 IS2RE is 38 GPU-hours.\nF\nDETAILS OF EXPERIMENTS ON QM9\nF.1\nADDITIONAL RESULTS OF TRAINING WITH NOISY NODES\nSimilar to Sec. D.5, we train EquiformV2 on QM9 with Noisy Nodes (Godwin et al., 2022) to\nshow that the performance gain brought by using higher degrees can be larger when trained with a\nnode-level auxiliary task and data augmentation. In Table 12, we summarize the results and compare\nwith previous works using Noisy Nodes and pre-training via denoising (Zaidi et al., 2023). When\ntrained with Noisy Nodes, EquiformerV2 performs better than Equiformer on more tasks. Specifically,\nwithout Noisy Nodes, EquiformerV2 is better than Equiformer on 9 out of the 12 tasks, similar on\n1 task, and worse on the other 2 tasks. With Noisy Nodes, EquiformerV2 achieves better MAE on\n10 tasks, similar on 1 task, and worse on 1 task. Additionally, we note that EquiformerV2 with\nNoisy Nodes is overall better than GNS-TAT with both pre-training and Noisy Nodes (Zaidi et al.,\n2023) on 9 out of the 12 tasks even though EquiformerV2 is not pre-trained on PCQM4Mv2 dataset,\n25\nPublished as a conference paper at ICLR 2024\nHyper-parameters\nValue or description\nOptimizer\nAdamW\nLearning rate scheduling\nCosine learning rate with linear warmup\nWarmup epochs\n0.1\nMaximum learning rate\n2 \u02c6 10\u00b44\nBatch size\n128\nNumber of epochs\n6\nWeight decay\n1 \u02c6 10\u00b43\nDropout rate\n0.1\nStochastic depth\n0.1\nEnergy coefficient \u03bbE\n1, 4\nForce coefficient \u03bbF\n1, 100\nGradient clipping norm threshold\n50\nModel EMA decay\n0.999\nCutoff radius (\u00c5)\n12\nMaximum number of neighbors\n20\nNumber of radial bases\n600\nDimension of hidden scalar features in radial functions dedge\np0, 128q\nMaximum degree Lmax\n6\nMaximum order Mmax\n2\nNumber of Transformer blocks\n18\nEmbedding dimension dembed\np6, 128q\nf pLq\nij\ndimension dattn_hidden\np6, 64q\nNumber of attention heads h\n8\nf p0q\nij\ndimension dattn_alpha\np0, 64q\nValue dimension dattn_value\np6, 16q\nHidden dimension in feed forward networks dffn\np6, 128q\nResolution of point samples R\n18\nTable 11: Hyper-parameters for OC22 dataset.\nTask\n\u03b1\n\u2206\u03b5\n\u03b5HOMO\n\u03b5LUMO\n\u00b5\nC\u03bd\nG\nH\nR2\nU\nU0\nZPVE\nModel\nUnits\na3\n0\nmeV\nmeV\nmeV\nD\ncal/mol K\nmeV\nmeV\na2\n0\nmeV\nmeV\nmeV\nEquiformer (Liao & Smidt, 2023)\n.046\n30\n15\n14\n.011\n.023\n7.63\n6.63\n.251\n6.74\n6.59\n1.26\nEquiformer (Liao & Smidt, 2023) + NN:\n.040\n26.4\n13.7\n13.0\n.011\n.020\n5.49\n4.61\n.235\n4.81\n4.61\n1.18\nGNS-TAT + NN (Zaidi et al., 2023);\n.047\n25.7\n17.3\n17.1\n.021\n.022\n7.41\n6.42\n0.65\n6.39\n6.39\n1.080\nGNS-TAT + NN + pretraining (Zaidi et al., 2023);\n.040\n22.0\n14.9\n14.7\n.016\n.020\n6.90\n5.79\n0.44\n5.76\n5.76\n1.018\nEquiformerV2\n.050\n29\n14\n13\n.010\n.023\n7.57\n6.22\n.186\n6.49\n6.17\n1.47\nEquiformerV2 + NN\n.039\n24.2\n12.2\n11.4\n.009\n.020\n5.34\n4.24\n.182\n4.28\n4.34\n1.21\nTable 12: Mean absolute error results on QM9 test set when trained with Noisy Nodes (Godwin et al., 2022).\n: denotes that the results are produced by this work. ; denotes using different data partitions. \u201cNN\u201d denotes\nNoisy Nodes, and \u201cpretraining\u201d denotes pretraining on PCQM4Mv2 dataset (Nakata & Shimazaki, 2017). The\nperformance gain from Equiformer to EquiformerV2 becomes larger when trained with Noisy Nodes.\nwhich is more than 30\u02c6 larger than QM9. This shows that a more expressive model can match the\nperformance with significantly less data.\nF.2\nTRAINING DETAILS\nWe follow the data partition of Equiformer. For the tasks of \u00b5, \u03b1, \u03b5HOMO, \u03b5LUMO, \u2206\u03b5, and C\u03bd, we\nuse batch size \u201c 64, the number of epochs \u201c 300, learning rate \u201c 5 \u02c6 10\u00b44, Gaussian radial basis\nfunctions with the number of bases \u201c 128, the number of Transformer blocks \u201c 6, weight decay\n\u201c 5 \u02c6 10\u00b43, and dropout rate \u201c 0.2 and use mixed precision for training. For the task of R2, we\nuse batch size \u201c 48, the number of epochs \u201c 300, learning rate \u201c 1.5 \u02c6 10\u00b44, Gaussian radial basis\nfunctions with the number of bases \u201c 128, the number of Transformer blocks \u201c 5, weight decay\n\u201c 5 \u02c6 10\u00b43, and dropout rate \u201c 0.1 and use single precision for training. For the task of ZPVE, we\nuse batch size \u201c 48, the number of epochs \u201c 300, learning rate \u201c 1.5 \u02c6 10\u00b44, Gaussian radial basis\nfunctions with the number of bases \u201c 128, the number of Transformer blocks \u201c 5, weight decay\n\u201c 5 \u02c6 10\u00b43, and dropout rate \u201c 0.2 and use single precision for training. For the task of G, H, U,\nand U0, we use batch size \u201c 48, the number of epochs \u201c 300, learning rate \u201c 1.5 \u02c6 10\u00b44, Gaussian\nradial basis functions with the number of bases \u201c 128, the number of Transformer blocks \u201c 5, weight\ndecay \u201c 0.0, and dropout rate \u201c 0.0 and use single precision for training. Other hyper-parameters are\nthe same across all the tasks, and we summarize them in Table 13. We use a single A6000 GPU and\ntrain different models for different tasks. The training costs are 72 GPU-hours for mixed precision\ntraining and 137 GPU-hours for single precision training. The number of parameters are 11.20M for\n6 blocks and 9.35M for 5 blocks.\nAs for training with Noisy Nodes as mentioned in Sec. F.1, we add noise to atomic coordinates\nand incorporate a node-level auxiliary task of denoising atomic coordinates. We thus introduce\n26\nPublished as a conference paper at ICLR 2024\nHyper-parameters\nValue or description\nOptimizer\nAdamW\nLearning rate scheduling\nCosine learning rate with linear warmup\nWarmup epochs\n5\nMaximum learning rate\n1.5 \u02c6 10\u00b44, 5 \u02c6 10\u00b44\nBatch size\n48, 64\nNumber of epochs\n300\nWeight decay\n0.0, 5 \u02c6 10\u00b43\nDropout rate\n0.0, 0.1, 0.2\nStochastic depth\n0.0, 0.05\nCutoff radius (\u00c5)\n5.0\nMaximum number of neighbors\n500\nNumber of radial bases\n128\nDimension of hidden scalar features in radial functions dedge\np0, 64q\nMaximum degree Lmax\n4\nMaximum order Mmax\n4\nNumber of Transformer blocks\n5, 6\nEmbedding dimension dembed\np4, 96q\nf pLq\nij\ndimension dattn_hidden\np4, 48q\nNumber of attention heads h\n4\nf p0q\nij\ndimension dattn_alpha\np0, 64q\nValue dimension dattn_value\np4, 24q\nHidden dimension in feed forward networks dffn\np4, 96q\nResolution of point samples R\n18\nNoise standard deviation \u03c3denoise\n0.02\nDenoising coefficient \u03bbdenoise\n0.1\nDenoising probability pdenoise\n0.5\nCorrupt ratio rdenoise\n0.125, 0.25\nTable 13: Hyper-parameters for QM9 dataset.\nfour additional hyper-parameters, which are noise standard deviation \u03c3denoise, denoising coefficient\n\u03bbdenoise, denoising probability pdenoise and corrupt ratio rdenoise. The noise standard deviation \u03c3denoise\ndenotes the standard deviation of Gaussian noise added to each xyz component of atomic coordinates.\nThe denoising coefficient \u03bbdenoise controls the relative importance of the auxiliary task compared\nto the original task. The denoising probability pdenoise denotes the probability of adding noise to\natomic coordinates and optimizing for both the auxiliary task and the original task. Using pdenoise \u0103 1\nenables taking original atomistic structures without any noise as inputs and optimizing for only the\noriginal task for some training iterations. The corrupt ratio rdenoise denotes the ratio of the number\nof atoms, which we add noise to and denoise, to the total number of atoms. Using rdenoise \u0103 1\nallows only adding noise to and denoising a subset of atoms within a structure. For the task of\nR2, we use \u03c3denoise \u201c 0.02, \u03bbdenoise \u201c 0.1, pdenoise \u201c 0.5 and rdenoise \u201c 0.125. For other tasks,\nwe use \u03c3denoise \u201c 0.02, \u03bbdenoise \u201c 0.1, pdenoise \u201c 0.5 and rdenoise \u201c 0.25. We share the above\nhyper-parameters for training EquiformerV2 and Equiformer, and we add one additional block of\nequivariant graph attention for the auxiliary task. We slightly tune other hyper-parameters when\ntrained with Noisy Nodes. For Equiformer, we additionally use stochastic depth \u201c 0.05 for the tasks\nof \u03b1, \u2206\u03b5, \u03b5HOMO, \u03b5LUMO, and C\u03bd. As for EquiformerV2, we additionally use stochastic depth \u201c 0.05\nfor the tasks of \u00b5, \u2206\u03b5, \u03b5HOMO, \u03b5LUMO, and C\u03bd. We increase the number of blocks from 5 to 6 and\nincrease the batch size from 48 to 64 for the tasks of G, H, U, and U0. We increase the learning rate\nfrom 1.5 \u02c6 10\u00b44 to 5 \u02c6 10\u00b44 and increase the number of blocks from 5 to 6 for the task of R2.\nF.3\nABLATION STUDY ON ARCHITECTURAL IMPROVEMENTS\nWe conduct ablation studies on the proposed architectural improvements using the task of \u2206\u03b5 on\nQM9 and compare with Equiformer baseline (Liao & Smidt, 2023). The reults are summarized in\nTable 14. The comparison between Index 0 and Index 1 shows that directly increasing Lmax from 2\nto 4 and using eSCN convolutions degrade the performance. This is due to overfitting since the QM9\ndataset is smaller, and each structure in QM9 has fewer atoms, less diverse atom types and much less\nangular variations than OC20 and OC22. Comparing Index 1 and Index 2, attention re-normalization\nclearly improves the MAE result. Although using S2 activation is stable here (Index 3) unlike OC20,\nit results in higher error than using gate activation (Index 2) and the Equiformer baseline (Index\n0). When using the proposed separable S2 activation (Index 4), we achieve lower error than using\ngate activation (Index 2). We can further reduce the error by using the proposed separable layer\nnormalization (Index 5). Comparing Index 0 and Index 5, we note that the proposed architectural\nimprovements are necessary to achieve better results than the baseline when using higher degrees on\nQM9. Overall, these ablation results follow the same trends as OC20.\n27\nPublished as a conference paper at ICLR 2024\nAttention\nIndex\nre-normalization\nActivation\nNormalization\nLmax\n\u2206\u03b5 MAE (meV)\n0\nEquiformer baseline\n2\n29.98\n1\n\u2717\nGate\nLN\n4\n30.46\n2\n\u2713\nGate\nLN\n4\n29.51\n3\n\u2713\nS2\nLN\n4\n30.23\n4\n\u2713\nSep. S2\nLN\n4\n29.31\n5\n\u2713\nSep. S2\nSLN\n4\n29.03\nTable 14: Ablation studies on the proposed architectural improvements using the task of \u2206\u03b5 of the QM9 dataset.\n28\nPublished as a conference paper at ICLR 2024\nFigure 6: Qualitative examples of the initial configuration of an adsorbate on a catalyst surface (column 1), and\ncorresponding relaxed configurations obtained from eSCN (Passaro & Zitnick, 2023) (column 2), EquiformerV2\n(column 3), and DFT (column 4). All examples are selected from the OC20-Dense dataset (Lan et al., 2022). We\nshow top-down views of each structure, with dashed lines showing the boundary of the unit cell repeating in the\nx and y directions.\n29\n"
  }
]