[
  {
    "title": "Large-Scale Automatic Audiobook Creation",
    "link": "https://arxiv.org/pdf/2309.03926.pdf",
    "upvote": "51",
    "text": "Large-Scale Automatic Audiobook Creation\nBrendan Walsh*1, Mark Hamilton*1,2, Greg Newby3, Xi Wang1, Serena Ruan1, Sheng Zhao1,\nLei He1, Shaofei Zhang1, Eric Dettinger1, William T. Freeman2,4, Markus Weimer1\n1Microsoft, 2MIT, 3Project Gutenberg, 4Google, *Equal Contribution\nAbstract\nAn audiobook can dramatically improve a work of litera-\nture\u2019s accessibility and improve reader engagement. However,\naudiobooks can take hundreds of hours of human effort to cre-\nate, edit, and publish. In this work, we present a system that\ncan automatically generate high-quality audiobooks from on-\nline e-books. In particular, we leverage recent advances in neu-\nral text-to-speech to create and release thousands of human-\nquality, open-license audiobooks from the Project Gutenberg e-\nbook collection. Our method can identify the proper subset of\ne-book content to read for a wide collection of diversely struc-\ntured books and can operate on hundreds of books in parallel.\nOur system allows users to customize an audiobook\u2019s speaking\nspeed and style, emotional intonation, and can even match a de-\nsired voice using a small amount of sample audio. This work\ncontributed over five thousand open-license audiobooks and an\ninteractive demo that allows users to quickly create their own\ncustomized audiobooks. To listen to the audiobook collection\nvisit https://aka.ms/audiobook.\n1. Introduction\nAudiobooks have become a popular way to consume literature,\nnews, and other publications. Audiobooks not only allow ex-\nisting readers to be able to enjoy content on the go, but can\nhelp make content accessible to communities such as children,\nthe visually impaired, and new language learners. Traditional\nmethods of audiobook production, such as professional human\nnarration or volunteer-driven projects like LibriVox, are time-\nconsuming, expensive, and can vary in recording quality. These\nfactors make it difficult to keep up with an ever-increasing rate\nof book publication.\nIn contrast, automatic audiobook cre-\nation is orders of magnitude faster, cheaper, and more consistent\nbut has historically suffered from the robotic nature of text-to-\nspeech systems and the challenge of deciding what text should\nnot be read aloud (e.g. tables of contents, page numbers, fig-\nures, and footnotes).\nWe present a system that overcomes both of the aforemen-\ntioned challenges by generating high-quality audiobooks from\nheterogeneous collections of online e-books. In particular, our\nsystem combines recent advances in neural text-to-speech, emo-\ntive reading, scalable computing, and automatic detection of\nrelevant text to create thousands of reasonable-sounding audio-\nbooks. We contribute over five thousand audiobooks totaling\napproximately thirty-five thousand hours of speech to the open\nsource. We also contribute a demonstration app that allows con-\nference attendees to create a custom audiobook, read aloud in\ntheir own voice, from any book from the collection using only\na few seconds of example sound.\n2. Related Work\nLibriVox is a well-known project that creates open-license au-\ndiobooks using human volunteers. Although it has made sig-\nnificant contributions to the accessibility of audiobooks, the\nquality of the produced audiobooks can be inconsistent due\nto the varying skills and recording environments of the volun-\nteers. Furthermore, the scalability of the project is limited by\nthe availability of volunteers and the time it takes to record\nand edit a single audiobook.\nPrivate platforms such as Au-\ndible create high-quality audiobooks but do not release their\nworks openly and charge users for their audiobooks. Project\nGutenberg hosts a broad collection of free e-books and a few\naudiobooks. Their existing audiobooks feature a robotic text-\nto-speech voice which limits listen-ability.\nText-to-speech is a well-studied problem and recent deep\nlearning methods such as WaveNet [1], Tacotron [2], and Fast-\nspeech [3] have shown considerable progress towards generat-\ning speech that rivals human quality and naturalness. In con-\ntrast, the problem of selecting which text to read from an e-\nbook has received considerably less attention. Nevertheless, re-\ncent work by [4] has explored whether it\u2019s possible to predict\nthe \u201cstart reading location\u201d using LSTM-based models but does\nnot tackle the cleaning of other irrelevant text throughout the\nbody of an e-book.\n3. Methods\nThis work introduces a scalable system capable of convert-\ning HTML-based e-books to high-quality audiobooks.\nOur\npipeline is built using SynapseML[5], a scalable machine learn-\ning framework that enables distributed orchestration of the en-\ntire audiobook creation process.\n3.1. Parsing e-Book HTML\nOur pipeline begins with thousands of free e-books provided\nby Project Gutenberg. These e-books are provided in several\ndifferent formats, and our work focuses on their HTML format\nwhich is most amenable to automated parsing. Parsing this ex-\ntremely heterogeneous and diverse collection of e-books was\nthe most significant challenge we encountered. Project Guten-\nberg does not standardize the contents of its HTML files and its\ne-books contain a significant amount of text that would not be\nrelevant for audio readers including pre-ambles, tables of con-\ntents, tables, illustrations, in-text page numbers, footnotes, tran-\nscriber notes, and other strange artifacts.\nTo create a high-quality subset of e-books we first featurize\neach e-book\u2019s HTML Document Object Model (DOM) tree us-\ning a combination of automated (the TF-IDF statistic on HTML\nComponents) and hand-crafted HTML features. This allowed\narXiv:2309.03926v1  [cs.SD]  7 Sep 2023\nFigure 1: t-SNE Representation of Clustered Ebooks. Colored\nareas represent uniformly formatted clusters of books.\nus to cluster and visualize the entire collection of Project Guten-\nberg HTML files and allowed us to find several large groups of\ncommonly structured files. We used these clusters of HTML\nfiles to build a rule-based HTML normalizer that converted the\nlargest classes of e-books into a standard representation that\ncould be automatically parsed. This analysis allowed us to cre-\nate a system that could quickly and deterministically parse a\nlarge number of books. Most importantly it also allowed us to\nrestrict attention to a subset of files that would generate high-\nquality recordings when read. Figure 1 shows the results of this\nclustering procedure, demonstrating that several clusters of sim-\nilarly structured e-books naturally emerge in the Project Guten-\nberg collection. Once parsed we can extract a stream of plain\ntext to feed to text-to-speech algorithms.\n3.2. Generating High Quality Speech\nDifferent audiobooks require different reading styles. Nonfic-\ntion works benefit from a clear and neutral voice while fictional\nworks with dialogue can benefit from an emotive reading and\nsome \u201cacting\u201d. For the majority of the books, we use a clear\nand neutral neural text-to-speech voice, However, in our live\ndemonstration we will present users with the ability to cus-\ntomize the voice, speed, pitch, and intonation of the text.\nTo clone a user\u2019s voice we utilize zero-shot text-to-speech\nmethods [6] to efficiently transfer the voice characteristics from\nlimited enrolled recordings. This allows a user to quickly cre-\nate an audiobook in their own voice using a small amount of\nrecorded audio.\nTo create an emotive reading of the text, we use an au-\ntomatic speaker and emotion inference system to dynamically\nchange the reading voice and tone based on context. This makes\npassages with multiple characters and emotional dialogue more\nlife-like and engaging. To this end, we first segment the text\ninto narration and dialogue and identify the speaker for each di-\nalogue section. We then predict the emotion of each dialogue\nusing [7] in a self-supervised manner. Finally, we assign sep-\narate voices and emotions to the narrator and the character di-\nalogues using the multi-style and contextual-based neural text-\nto-speech model proposed in [8].\n4. The Project Gutenberg Open Audiobook\nCollection\nWe introduce the Project Gutenberg Open Audiobook Collec-\ntion: over five thousand high-quality audiobooks generated\nfrom the Project Gutenberg collection and available for free\ndownload and open use. We host these files as a single zip file\nfor the research community as well as on the major podcast and\naudio file hosting platforms for use by the broader community.\nThis collection offers over thirty-five thousand hours of content\nincluding classic literature, non-fiction, plays, and biographical\nworks narrated in a clear and consistent voice. We hope this\ncontribution can provide value to both the research community,\nand the broader community of audiobook listeners.\n5. Demonstration\nWe plan to host a live demonstration application that allows\nconference attendees to create their own custom audiobooks us-\ning our system. Users will first start by selecting a book from\nthe 5,000 titles in our collection using a simple search inter-\nface. They can then select what voice they would like to use\nfor the recording from a large collection of existing neutral and\nemotion-aware voices or even their own voice. If a user wants\nto create a custom audiobook using their own voice, they will\nbe asked to speak a few sentences to quickly train a custom\nvoice profile. Users will be able to listen to a preview of their\naudiobook in real time and add an optional custom dedication\nbefore submitting a larger job that reads the entire book. Once\nthe pipeline finishes we will email the user a link to download\ntheir custom-made audiobook.\n6. Conclusions\nIn this work, we present a novel pipeline to automate the cre-\nation of high-quality audiobooks from heterogeneous e-books.\nOur system uses new advances in neural text-to-speech, emo-\ntion recognition, custom voice cloning, and distributed comput-\ning to create engaging and lifelike audiobooks. We apply this\nsystem to donate over five thousand audiobooks to the open-\nsource community and aim to demonstrate this system by al-\nlowing conference attendees to create custom audiobooks. We\nbelieve that this work has the potential to greatly improve the\naccessibility and availability of audiobooks.\n7. References\n[1] A. van den Oord et al., \u201cWavenet: A generative model for raw\naudio,\u201d 2016.\n[2] Y. Wang et al., \u201cTacotron: Towards end-to-end speech synthesis,\u201d\nin Proc. INTERSPEECH 2017 Annual Conference of the Interna-\ntional Speech Communication Association, 2017, pp. 4006\u2013\u20134010.\n[3] Y. Ren et al., \u201cFastspeech: Fast, robust and controllable text to\nspeech,\u201d in Advances in Neural Information Processing Systems,\n2019, pp. 3165\u2013\u20133174.\n[4] S. B. Bodapati et al., \u201cA machine learning approach to detecting\nstart reading location of ebooks,\u201d in 2018 IEEE International Con-\nference on Data Mining Workshops (ICDMW).\nIEEE, 2018, pp.\n1522\u20131529.\n[5] M. Hamilton et al., \u201cFlexible and scalable deep learning with\nmmlspark,\u201d in Proceedings of The 4th International Conference\non Predictive Applications and APIs, ser. Proceedings of Machine\nLearning Research, vol. 82.\nPMLR, 24\u201325 Oct 2018, pp. 11\u201322.\n[6] Y. Wu et al., \u201cAdaspeech 4: Adaptive text to speech in zero-shot\nscenarios,\u201d in Proc. INTERSPEECH 2022 Annual Conference of\nthe International Speech Communication Association, 2022, pp.\n2568\u2014-2572.\n[7] \u2014\u2014, \u201cSelf-supervised context-aware style representation for ex-\npressive speech synthesis,\u201d in Proc. INTERSPEECH 2022 Annual\nConference of the International Speech Communication Associa-\ntion, 2022, pp. 5503\u2014-5507.\n[8] H. Guo et al., \u201cConversational end-to-end tts for voice agent,\u201d in\nProc. SLT 2021 IEEE Spoken Language Technology Workshop,\n2021, pp. 403\u2013\u2013409.\n"
  },
  {
    "title": "From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting",
    "link": "https://arxiv.org/pdf/2309.04269.pdf",
    "upvote": "26",
    "text": "From Sparse to Dense:\nGPT-4 Summarization with Chain of Density Prompting\nGriffin Adams\u2660,\u2663\ngriffin.adams@columbia.edu\nAlexander R. Fabbri\u2662\nafabbri@salesforce.com\nFaisal Ladhak \u2660\nfaisal@cs.columbia.edu\nEric Lehman\u2661\nlehmer16@mit.edu\nNo\u00e9mie Elhadad\u2660,\u2663\nnoemie.elhadad@columbia.edu\nSalesforce AI\u2662\nMIT\u2661\nColumbia University: CS\u2660, Biomedical Informatics\u2663\nAbstract\nSelecting the \u201cright\u201d amount of information to\ninclude in a summary is a difficult task. A good\nsummary should be detailed and entity-centric\nwithout being overly dense and hard to follow.\nTo better understand this tradeoff, we solicit\nincreasingly dense GPT-4 summaries with what\nwe refer to as a \u201cChain of Density\u201d (CoD) prompt.\nSpecifically, GPT-4 generates an initial entity-\nsparse summary before iteratively incorporating\nmissing salient entities without increasing the\nlength. Summaries generated by CoD are more\nabstractive, exhibit more fusion, and have less of\na lead bias than GPT-4 summaries generated by a\nvanilla prompt. We conduct a human preference\nstudy on 100 CNN DailyMail articles and find\nthat that humans prefer GPT-4 summaries that\nare more dense than those generated by a vanilla\nprompt and almost as dense as human written\nsummaries.\nQualitative analysis supports the\nnotion that there exists a tradeoff between infor-\nmativeness and readability. 500 annotated CoD\nsummaries, as well as an extra 5,000 unannotated\nsummaries, are freely available on HuggingFace1.\n1\nIntroduction\nAutomatic summarization has come a long way in\nthe past few years, largely due to a paradigm shift\naway from supervised fine-tuning on labeled datasets\nto zero-shot prompting with Large Language Models\n(LLMs), such as GPT-4 (OpenAI, 2023). Without\nadditional training, careful prompting can enable\nfine-grained control over summary characteristics,\nsuch as length (Goyal et al., 2022), topics (Bhaskar\net al., 2023), and style (Pu and Demberg, 2023).\nAn overlooked aspect is the information density of\nan summary. In theory, as a compression of another\ntext, a summary should be denser\u2013containing a higher\nconcentration of information\u2013than the source docu-\nment. Given the high latency of LLM decoding (Kad-\ndour et al., 2023), covering more information in fewer\n1https://huggingface.co/datasets/\ngriffin/chain_of_density\nHuman Summary\nVanilla GPT-4\nHuman Preferred CoD\nFigure 1:\nChain of Density (CoD) summaries grow\nincreasingly entity dense, starting off closer to vanilla\nGPT-4 summaries and eventually surpassing that of human\nwritten summaries. Human annotations suggest that a\ndensity similar to that of human-written summaries is\npreferable\u2013striking the right balance between clarity (favors\nless dense) and informativeness (favors more dense).\nwords is a worthy goal, especially for real-time appli-\ncations. Yet, how dense is an open question. A sum-\nmary is uninformative if it contains insufficient detail.\nIf it contains too much information, however, it can be-\ncome difficult to follow without having to increase the\noverall length. Conveying more information subject to\na fixed token budget requires a combination of abstrac-\ntion, compression, and fusion. There is a limit to how\nmuch space can be made for additional information\nbefore becoming illegible or even factually incorrect.\nIn this paper, we seek to identify this limit by solic-\niting human preferences on a set of increasingly dense\nsummaries produced by GPT-4. Treating entities, and,\nin particular, the average number of entities per token,\nas a proxy for density, we generate an initial, entity-\nsparse summary. Then, we iteratively identify and fuse\n1-3 missing entities from the previous summary with-\nout increasing the overall length (5x overall). Each\nsummary has a higher ratio of entities to tokens than\nthe previous one. Based on human preference data, we\ndetermine that humans prefer summaries that are al-\nmost as dense as human-written summaries and more\narXiv:2309.04269v1  [cs.CL]  8 Sep 2023\nAr t i cl e:  { { ARTI CLE} }\nYou wi l l  gener at e i ncr easi ngl y conci se,  ent i t y- dense summar i es of  t he \nabove Ar t i cl e.\nRepeat  t he f ol l owi ng 2 st eps 5 t i mes.\nSt ep 1.  I dent i f y 1- 3 i nf or mat i ve Ent i t i es ( \" ; \"  del i mi t ed)  f r om t he \nAr t i cl e whi ch ar e mi ssi ng f r om t he pr evi ousl y gener at ed summar y.\nSt ep 2.  Wr i t e a new,  denser  summar y of  i dent i cal  l engt h whi ch cover s \never y ent i t y and det ai l  f r om t he pr evi ous summar y pl us t he Mi ssi ng \nEnt i t i es.\nA Mi ssi ng Ent i t y i s:\n-  Rel evant :  t o t he mai n st or y.\n-  Speci f i c:  descr i pt i ve yet  conci se ( 5 wor ds or  f ewer ) .\n-  Novel :  not  i n t he pr evi ous summar y.\n-  Fai t hf ul :  pr esent  i n t he Ar t i cl e.\n-  Anywher e:  l ocat ed anywher e i n t he Ar t i cl e.\nGui del i nes:\n-  The f i r st  summar y shoul d be l ong ( 4- 5 sent ences,  ~80 wor ds)  yet  \nhi ghl y non- speci f i c,  cont ai ni ng l i t t l e i nf or mat i on beyond t he \nent i t i es mar ked as mi ssi ng.  Use over l y ver bose l anguage and f i l l er s \n( e. g. ,  \" t hi s ar t i cl e di scusses\" )  t o r each ~80 wor ds.\n-  Make ever y wor d count :  r e- wr i t e t he pr evi ous summar y t o i mpr ove \nf l ow and make space f or  addi t i onal  ent i t i es.\n-  Make space wi t h f usi on,  compr essi on,  and r emoval  of  uni nf or mat i ve \nphr ases l i ke \" t he ar t i cl e di scusses\" .\n-  The summar i es shoul d become hi ghl y dense and conci se yet  \nsel f - cont ai ned,  e. g. ,  easi l y under st ood wi t hout  t he Ar t i cl e.\n-  Mi ssi ng ent i t i es can appear  anywher e i n t he new summar y.\n-  Never  dr op ent i t i es f r om t he pr evi ous summar y.  I f  space cannot  be \nmade,  add f ewer  new ent i t i es.\nRemember ,  use t he exact  same number  of  wor ds f or  each summar y.\nAnswer  i n JSON.  The JSON shoul d be a l i st  ( l engt h 5)  of  di ct i onar i es \nwhose keys ar e \" Mi ssi ng_Ent i t i es\"  and \" Denser _Summar y\" .\nThi s ar t i cl e di scusses an i nci dent  t hat  occur r ed dur i ng t he Chi nese Gr and Pr i x i nvol vi ng \nt wo r aci ng dr i ver s,  Jenson But t on and Past or  Mal donado.  The t wo wer e compet i ng f or  t he \n13t h pl ace when But t on col l i ded wi t h Mal donado' s vehi cl e,  causi ng damage t o bot h car s.  \nThe i nci dent  r esul t ed i n a penal t y f or  But t on,  who was demot ed t o 14t h pl ace.  Mal donado,  \non t he ot her  hand,  had t o r et i r e f r om t he r ace due t o t he damage hi s car  sust ai ned.\nDur i ng t he Chi nese Gr and Pr i x,  Jenson But t on,  dr i vi ng f or  McLar en,  col l i ded wi t h Past or  \nMal donado' s Lot us,  whi l e t hey wer e vyi ng f or  13t h pl ace.  The cr ash caused Mal donado t o \nr et i r e and But t on t o r ecei ve a penal t y,  dr oppi ng hi m t o 14t h.  The i nci dent  unf ol ded i n \nf r ont  of  Fer nando Al onso,  who managed t o avoi d t he col l i si on and move up t wo pl aces.\nI n t he Chi nese Gr and Pr i x,  Jenson But t on' s McLar en hi t  Past or  Mal donado' s Lot us,  causi ng \nMal donado t o r et i r e and But t on t o r ecei ve a f i ve- second penal t y,  demot i ng hi m t o 14t h.  \nBut t on al so r ecei ved t wo penal t y poi nt s on hi s super l i cence.  Fer nando Al onso,  who \nwi t nessed t he i nci dent ,  advanced t wo pl aces,  whi l e But t on was l apped by Ni co Rosber g' s \nMer cedes.\nJenson But t on' s McLar en col l i ded wi t h Past or  Mal donado' s Lot us dur i ng t he Chi nese Gr and \nPr i x,  causi ng f r ont  wi ng damage t o But t on' s car  and r ear - end damage t o Mal donado' s,  \nf or ci ng hi s r et i r ement .  But t on r ecei ved a f i ve- second penal t y and t wo super l i cence \npoi nt s,  dr oppi ng hi m t o 14t h.  Fer nando Al onso advanced t wo pl aces,  whi l e But t on was \nl apped by Ni co Rosber g and Al onso by Sebast i an Vet t el  and Ki mi  Rai kkonen.\nOn l ap 49 of  t he i nci dent - packed Chi nese Gr and Pr i x,  Jenson But t on' s McLar en hi t  Past or  \nMal donado' s Lot us,  causi ng damage and Mal donado' s r et i r ement .  But t on r ecei ved a \nf i ve- second penal t y and t wo super l i cence poi nt s,  f al l i ng t o 14t h.  Fer nando Al onso,  who \nwi t nessed t he cr ash,  advanced t wo pl aces,  whi l e But t on was l apped by Ni co Rosber g and \nAl onso by Fer r ar i ' s Sebast i an Vet t el  and Ki mi  Rai kkonen.\nChai n of  Densi t y ( CoD)  Pr ompt\nCoD Out put s ( Added Det ai l s f r om pr evi ous)\nFigure 2: Chain of Density (CoD) Prompt and example output. At each step, 1-3 additional details (entities) are added\nto the previous summary without increasing the length. To make room for new entities, existing content is re-written (e.g.,\ncompression, fusion). Half the annotators (2/4) prefer the second to last summary, with the others preferring the final one.\ndense than those generated by a vanilla GPT-4 prompt.\nOur primary contributions are to:\n\u2022 Develop a prompt-based iterative method (CoD)\nfor making summaries increasingly entity dense.\n\u2022 Conduct both human and automatic evaluation\nof increasingly dense summaries on CNN/Dai-\nlymail articles to better understand the tradeoff\nbetween informativeness (favoring more entities)\nand clarity (favoring fewer entities).\n\u2022 Open source GPT-4 summaries, annotations, and\na set of 5,000 unannotated CoD summaries to\nbe used for evaluation or distillation.\n2\nChain of Density Prompting\nPrompt.\nOur goal is to generate a set of summaries\nwith GPT-4 with varying levels of information density,\nwhile controlling for length, which has proven to be a\nstrong confounder when evaluating summaries (Fabbri\net al., 2021; Liu et al., 2023b). To do this, we formu-\nlate a single Chain of Density (CoD) prompt, whereby\nan initial summary is generated and made increasingly\nentity dense. Specifically, for a fixed number of turns,\na set of unique salient entities from the source text\nare identified and fused into the previous summary\nwithout increasing the length. The first summary is\nentity-sparse as it focuses on only 1-3 initial entities.\nTo maintain the same length while increasing the num-\nber of entities covered, abstraction, fusion, and com-\npression is explicitly encouraged, rather than dropping\nmeaningful content from previous summaries.\nFigure 2 displays the prompt along with an\nexample output. Rather than be prescriptive about the\ntypes of entities, we simply define a Missing Entity as:\n\u2022 Relevant: to the main story.\n\u2022 Specific: descriptive yet concise (5 words or\nfewer).\n\u2022 Novel: not in the previous summary.\n\u2022 Faithful: present in the Article.\n\u2022 Anywhere: located anywhere in the Article.\nData.\nWe randomly sample 100 articles from the\nCNN/DailyMail summarization (Nallapati et al.,\n2016) test set for which to generate CoD summaries.\nReference Points.\nFor frame of reference, we\ncompare CoD summary statistics to human-written\nbullet-point style reference summaries as well as\nsummaries generated by GPT-4 with a vanilla prompt:\n\u201cWrite a VERY short summary of the Article. Do not\nexceed 70 words.\u201d We set the desired token length to\nmatch that of CoD summaries (shown in Table 1).\n3\nStatistics\nDirect statistics (tokens, entities, entity density) are\nones directly controlled for by CoD, while Indirect\nVanilla GPT-4\nHuman Summary\nHuman Summary\nVanilla GPT-4\nVanilla GPT-4\nHuman Summary\nFigure 3: CoD-generated summaries grow increasingly abstractive while exhibiting more fusion and less of a lead bias.\nstatistics are expected byproducts of densification.\nCoD Step\nTokens\nEntities\nDensity (E/T)\n1\n72\n6.4\n0.089\n2\n67\n8.7\n0.129\n3\n67\n9.9\n0.148\n4\n69\n10.8\n0.158\n5\n72\n12.1\n0.167\nHuman\n60\n8.8\n0.151\nVanilla GPT-4\n70\n8.5\n0.122\nTable 1: Explicit statistics for GPT-4 CoD summaries.\nDirect Statistics.\nIn Table 1, we compute tokens\nwith NLTK (Loper and Bird, 2002), measure unique\nentities with Spacy2, and compute entity density as the\nratio. The CoD prompt largely adheres to a fixed to-\nken budget. In fact, the second step leads to an average\n5-token (72 to 67) reduction in length as unnecessary\nwords are removed from the initially verbose\nsummary. The entity density rises\u2013starting at 0.089,\ninitially below Human and Vanilla GPT-4 (0.151 and\n0.122)\u2013to 0.167 after 5 steps of densification.\nIndirect Statistics.\nAbstractiveness should increase\nwith each CoD step because summaries are itera-\ntively re-written to make space for each additional\nentity. We measure abstractiveness with extractive\ndensity: the average squared length of extractive frag-\nments (Grusky et al., 2018). Similarly, the level of\nconcept Fusion should increase monotonically as en-\ntities are added to a fixed-length summary. We proxy\nfusion as average number of source sentences aligned\nto each summary sentence. For alignment, we use\nthe relative ROUGE gain method (Zhou et al., 2018),\nwhich aligns source sentences to a target sentence un-\ntil the relative ROUGE gain of an additional sentence\nis no longer positive. We also expect the Content\nDistribution\u2013the position in the Article from which\nsummary content is sourced\u2013to shift. Specifically, we\nexpect that CoD summaries initially exhibit a strong\nLead Bias yet gradually start to pull in entities from the\n2https://spacy.io.\nmiddle and end of the article. To measure this, we use\nour alignments from fusion and measure the average\nsentence rank of all aligned source sentences. Figure 3\nconfirms these hypotheses: abstractiveness increases\nwith the number of re-writing steps (lower extractive\ndensity on the left), the rate of fusion rises (middle\nfigure), and the summaries start to incorporate content\nfrom the middle and end of the article (right figure).\nInterestingly, all CoD summaries are more abstrac-\ntive than both human written and baseline summaries.\n4\nResults\nTo better understand the tradeoffs present with CoD\nsummaries, we conduct a preference-based human\nstudy and a rating-based evaluation with GPT-4.\nCoD\n% Share of First Place Votes\nStep\nIndividual Annotators\nAggregate\n1\n3.0\n2.0\n13.0\n17.4\n8.3\n2\n25.0\n28.0\n43.0\n31.4\n30.8\n3\n22.0\n28.0\n21.0\n24.4\n23.0\n4\n29.0\n25.0\n13.0\n26.7\n22.5\n5\n21.0\n17.0\n10.0\n16.3\n15.5\nTable 2:\nBreakdown of first-place votes for CoD\nsummaries by step. Based on aggregate preferences, the\nmodal CoD step is 2, median is 3, and expected is 3.06.\nHuman\nPreferences.\nWe\nconduct\na\nhuman\nevaluation to assess the impact of densification on\nhuman assessments of overall quality. Specifically,\nthe first four authors of the paper were presented with\nrandomly shuffled CoD summaries, along with the\narticles, for the same 100 articles (5 steps * 100 =\n500 total summaries). Based on the definition of a\n\u201cgood summary\" from Stiennon et al. (2020) (Table\n6 from their paper), each annotator indicated their top\npreferred summary. Table 2 reports the breakdown of\nfirst place votes by CoD step across annotators\u2013as\nwell as aggregated across annotators. First, we report\na low Fleiss\u2019 kappa (Fleiss, 1971) of 0.112, which\npoints to the subtle differences between summaries\nand the subjective nature of the task. Recent work has\nCoD Step\nEntity Density\nInformative\nQuality\nCoherence\nAttributable\nOverall\nGPT-4 Eval Average\n1\n0.089\n4.34\n4.75\n4.96\n4.96\n4.41\n4.69\n2\n0.129\n4.62\n4.79\n4.92\n5.00\n4.58\n4.78\n3\n0.148\n4.67\n4.76\n4.84\n5.00\n4.57\n4.77\n4\n0.158\n4.74\n4.69\n4.75\n5.00\n4.61\n4.76\n5\n0.167\n4.73\n4.65\n4.61\n4.97\n4.58\n4.71\nTable 3: GPT-4 Likert-scale (1-5) assessments of Chain of Density (CoD) Summaries by step.\nFigure 4: An example of a human-preferred densification step (left) and one which is not preferred. For the left, the\nbottom summary is preferred because the addition of \u201cLiverpool\u201d and the goal-scorers is relevant. The second summary\nmakes room with sensible compressions, such as synthesizing \u201ca potential route back into the game\u201d into \u201ca comeback\u201d.\nFor the right, the addition of more details on \u201cTVMonde\u201d does not make up for the presence of an awkward fusion of\nentities (\u201ccyberattack\u201d, and \u201cYves Bigot\u201d), which was a direct result of having to tighten the previous summary.\nsimilarly noted low instance-level agreement when\njudging GPT-based summaries (Goyal et al., 2022).\nYet, at the system level, some trends start to\nemerge.\nFor 3 of the 4 annotators, CoD step 1\nreceived the largest share of first-place votes across\nthe 100 examples (28, 43, and 31.4%, respectively).\nYet, in aggregate, 61% of first placed summaries\n(23.0+22.5+15.5) involved \u22653 densification steps.\nThe median preferred CoD step is in the middle (3),\nand the expected step is 3.06.\nBased on the average density of Step 3 summaries,\nwe can roughly infer a preferred entity density of\n\u223c 0.15 across the CoD candidates. From Table 1,\nwe can see that this density aligns with human-written\nsummaries (0.151), yet is noticeable higher than sum-\nmaries produced with a vanilla GPT-4 prompt (0.122).\nAutomatic Metrics.\nAs an evaluator,\nGPT-4\nhas been shown to adequately correlate to human\njudgments (Fu et al., 2023; Liu et al., 2023a), even\npotentially outperforming crowd-sourced workers\non some annotation tasks (Gilardi et al., 2023). As\na complement to our human evaluation (below), we\nprompt GPT-4 to rate CoD summaries (1-5) along\n5 dimensions: Informative, Quality, Coherence, At-\ntributable, and Overall. The definitions of Informa-\ntive, Quality, and Attributable come from Aharoni\net al. (2023), while Coherence comes from Fabbri\net al. (2021)3. Overall aims to capture the qualities\njointly. Please see Appendix A for the prompts used\n3Quality and Coherence are article-independent metrics.\nto solicit scores for each dimension. Table 3 suggests\nthat densification is correlated with informativeness,\nyet there is a limit, with the score peaking at Step 4\n(4.74). Article-free dimensions: Quality and Coher-\nence, decline sooner (after 2 and 1 steps, respectively).\nAll summaries are deemed Attributable to the source\narticle. The Overall scores skew toward denser and\nmore informative summaries, with Step 4 having the\nhighest score. On average across dimensions, the first\nand last CoD steps are least favored, while the mid-\ndle three are close (4.78, 4.77, and 4.76, respectively).\nIn Appendix A, we report highest summary-\nlevel correlations of the Overall metric to human\njudgments (0.31 Pearson correlation), yet note low cor-\nrelations overall\u2013a phenomenon observed by Deutsch\net al. (2022) when summaries are of similar quality.\nQualitative Analysis.\nThere exists a clear trade-off\nbetween coherence / readability of summaries and in-\nformativeness. To illustrate, in Figure 4, we present\ntwo CoD steps: one for which the summary is im-\nproved with more detail, and one for which the sum-\nmary is harmed. On average, intermediate CoD sum-\nmaries best achieved this balance, yet we leave it to fu-\nture work to precisely define and quantify this tradeoff.\n5\nRelated Work\nGPT Summarization. Goyal et al. (2022) bench-\nmarked GPT-3 on news article summarization and\nfound that humans preferred GPT-3 summaries\nover previous supervised baselines, which was\nnot reflective of existing reference-based and\nreference-free metrics. Zhang et al. (2023) find that\nzeroshot GPT-3 summaries perform on par with\nhumans by soliciting high-quality summaries from\nfreelance writers. Entity-Based Summarization.\nNarayan et al. (2021) proposed generating entity\nchains as a planning step for supervised fine-tuning\nof summarization models, in contrast to keywords\n(Li et al., 2020; Dou et al., 2021) or purely extractive\nunits (Dou et al., 2021; Adams et al., 2023a). Entities\nhave also been incorporated for summarization as a\nform of control (Liu and Chen, 2021; He et al., 2022;\nMaddela et al., 2022), to improve faithfulness (Nan\net al., 2021; Adams et al., 2022), and as a unit for\nevaluation (Cao et al., 2022; Adams et al., 2023b).\n6\nConclusion\nWe study the impact of summary densification on\nhuman preferences of overall quality. We find that\na degree of densification is preferred, yet, when\nsummaries contain too many entities per token, it\nis very difficult maintain readability and coherence.\nWe open-source annotated test set as well as a larger\nun-annotated training set for further research into the\ntopic of fixed-length, variable density summarization.\n7\nLimitations\nWe only analyze CoD for a single domain, news\nsummarization.\nAnnotations did not show high\nsummary-level agreement yet did start to show\nsystem-level trends, which is in line with previous\nwork on LLM-based evaluation (Goyal et al., 2022).\nFinally, GPT-4 is a closed source model so we cannot\nshare model weights.\nWe do, however, publish\nall evaluation data, annotations, as well as 5, 000\nun-annotated CoD to be used for downstream uses\ncases, e.g., density distillation into an open-sourced\nmodel such as LLAMA-2 (Touvron et al., 2023).\nReferences\nGriffin Adams, Alex Fabbri, Faisal Ladhak, No\u00e9mie\nElhadad, and Kathleen McKeown. 2023a. Generating\nEDU extracts for plan-guided summary re-ranking.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2680\u20132697, Toronto, Canada.\nAssociation for Computational Linguistics.\nGriffin Adams, Han-Chin Shing, Qing Sun, Christopher\nWinestock, Kathleen McKeown, and No\u00e9mie Elhadad.\n2022.\nLearning to revise references for faithful\nsummarization.\nIn Findings of the Association for\nComputational Linguistics:\nEMNLP 2022, pages\n4009\u20134027,\nAbu Dhabi,\nUnited Arab Emirates.\nAssociation for Computational Linguistics.\nGriffin Adams, Jason Zucker, and No\u00e9mie Elhadad.\n2023b.\nA meta-evaluation of faithfulness metrics\nfor long-form hospital-course summarization. arXiv\npreprint arXiv:2303.03948.\nRoee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan\nHerzig, Elizabeth Clark, and Mirella Lapata. 2023. Mul-\ntilingual summarization with factual consistency evalu-\nation. In Findings of the Association for Computational\nLinguistics: ACL 2023, pages 3562\u20133591, Toronto,\nCanada. Association for Computational Linguistics.\nAdithya Bhaskar, Alex Fabbri, and Greg Durrett. 2023.\nPrompted opinion summarization with GPT-3.5.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 9282\u20139300, Toronto,\nCanada. Association for Computational Linguistics.\nMeng Cao, Yue Dong, and Jackie Cheung. 2022.\nHallucinated but factual!\ninspecting the factuality\nof hallucinations in abstractive summarization.\nIn\nProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3340\u20133354, Dublin, Ireland.\nAssociation for Computational Linguistics.\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2022.\nRe-examining system-level correlations of automatic\nsummarization evaluation metrics. In Proceedings of the\n2022 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Lan-\nguage Technologies, pages 6038\u20136052, Seattle, United\nStates. Association for Computational Linguistics.\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. 2021. GSum: A general\nframework for guided neural abstractive summarization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4830\u20134842, Online. Association for Computational\nLinguistics.\nAlexander R. Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan McCann,\nCaiming Xiong, Richard Socher, and Dragomir Radev.\n2021.\nSummEval:\nRe-evaluating summarization\nevaluation.\nTransactions of the Association for\nComputational Linguistics, 9:391\u2013409.\nJoseph L Fleiss. 1971. Measuring nominal scale agreement\namong many raters. Psychological bulletin, 76(5):378.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. 2023.\nChatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of gpt-3.\narXiv preprint arXiv:2209.12356.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies.\nIn Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages\n708\u2013719, New Orleans, Louisiana. Association for\nComputational Linguistics.\nJunxian He, Wojciech Kryscinski, Bryan McCann,\nNazneen Rajani, and Caiming Xiong. 2022. CTRLsum:\nTowards generic controllable text summarization. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages\n5879\u20135915,\nAbu Dhabi,\nUnited Arab Emirates.\nAssociation for Computational Linguistics.\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie\nBradley, Roberta Raileanu, and Robert McHardy. 2023.\nChallenges and applications of large language models.\narXiv preprint arXiv:2307.10169.\nHaoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong,\nand Xiaodong He. 2020. Keywords-guided abstractive\nsentence summarization. In Proceedings of the AAAI\nconference on artificial intelligence, volume 34, pages\n8196\u20138203.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023a. Gpteval:\nNlg evaluation using gpt-4 with better human alignment.\narXiv preprint arXiv:2303.16634.\nYixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao,\nLinyong Nan, Ruilin Han, Simeng Han, Shafiq Joty,\nChien-Sheng Wu, Caiming Xiong, and Dragomir\nRadev. 2023b. Revisiting the gold standard: Grounding\nsummarization evaluation with robust human evaluation.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4140\u20134170, Toronto, Canada.\nAssociation for Computational Linguistics.\nZhengyuan Liu and Nancy Chen. 2021.\nControllable\nneural dialogue summarization with personal named\nentity planning. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing,\npages 92\u2013106, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nEdward Loper and Steven Bird. 2002. Nltk: The natural\nlanguage toolkit. arXiv preprint cs/0205028.\nMounica Maddela,\nMayank Kulkarni,\nand Daniel\nPreotiuc-Pietro. 2022.\nEntSUM: A data set for\nentity-centric extractive summarization. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 3355\u20133366, Dublin, Ireland. Association for\nComputational Linguistics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, \u00c7a\u02d8glar\nGul\u00e7ehre, and Bing Xiang. 2016.\nAbstractive text\nsummarization using sequence-to-sequence RNNs\nand beyond.\nIn Proceedings of the 20th SIGNLL\nConference on Computational Natural Language\nLearning, pages 280\u2013290, Berlin, Germany. Association\nfor Computational Linguistics.\nFeng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero\nNogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kath-\nleen McKeown, and Bing Xiang. 2021. Entity-level\nfactual consistency of abstractive text summarization.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 2727\u20132733, Online.\nAssociation for Computational Linguistics.\nShashi Narayan, Yao Zhao, Joshua Maynez, Gon\u00e7alo\nSim\u00f5es, Vitaly Nikolaev, and Ryan McDonald. 2021.\nPlanning with learned entity prompts for abstractive\nsummarization. Transactions of the Association for\nComputational Linguistics, 9:1475\u20131492.\nOpenAI. 2023.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nDongqi Pu and Vera Demberg. 2023.\nChatGPT vs\nhuman-authored text: Insights into controllable text\nsummarization and sentence style transfer. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 4: Student Research\nWorkshop), pages 1\u201318, Toronto, Canada. Association\nfor Computational Linguistics.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul F Christiano. 2020. Learning to\nsummarize with human feedback. Advances in Neural\nInformation Processing Systems, 33:3008\u20133021.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n2023. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B Hashimoto.\n2023. Benchmarking large language models for news\nsummarization. arXiv preprint arXiv:2301.13848.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural document\nsummarization by jointly learning to score and select\nsentences. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 654\u2013663, Melbourne,\nAustralia. Association for Computational Linguistics.\nA\nGPT-4 Metrics\nFor the GPT-4 Likert-style evaluation, we use the\nfollowing prompt template.\nArticle: {{Article}}\nSummary: {{Summary}}\nPlease rate the summary\n(1=worst to 5=best) with\nrespect to {{Dimension}}.\n{{Definition}}\nBelow, we present the definitions provided for each\nquality metric.\n\u2022 Informative: An informative summary captures\nthe important information in the article and\npresents it accurately and concisely.\n\u2022 Quality: A high quality summary is comprehen-\nsible and understandable.\n\u2022 Coherence:\nA coherent summary is well-\nstructured and well-organized.\n\u2022 Attributable:\nIs all the information in the\nsummary fully attributable to the Article?\n\u2022 Overall Preference: A good summary should\nconvey the main ideas in the Article in a concise,\nlogical, and coherent fashion.\nThe Quality and Coherence prompts do not in-\nclude the Article in the prompt. These definitions were\nparaphrased from previous summarization annotation\nefforts: (Fabbri et al., 2021; Aharoni et al., 2023).\nDimension\nCorrelation\nInformative\n0.215\nQuality\n0.120\nCoherence\n0.178\nAttributable\n0.245\nOverall\n0.311\nTable 4: Summary-Level Pearson Correlation coefficient\nbetween human preferences and GPT-4 Likert ratings.\nMeta-Evaluation.\nTo compute the summary-level\ncorrelation, we first turned the preference data into a\nvector representing the number of times that summary\nreceived a first-placed vote. Table 4 demonstrates,\nunsurprisingly, that a prompt designed to capture\noverall summary rating has the highest summary-level\nPearson correlation to overall preferences (31), yet\noverall correlations are still low.\n"
  },
  {
    "title": "Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts",
    "link": "https://arxiv.org/pdf/2309.04354.pdf",
    "upvote": "13",
    "text": "Mobile V-MoEs: Scaling Down Vision Transformers\nvia Sparse Mixture-of-Experts\nErik Daxberger*\nFloris Weers\nBowen Zhang\nTom Gunter\nRuoming Pang\nMarcin Eichner\nMichael Emmersberger\nYinfei Yang\nAlexander Toshev\nXianzhi Du\nApple\nAbstract\nSparse Mixture-of-Experts models (MoEs) have recently\ngained popularity due to their ability to decouple model\nsize from inference efficiency by only activating a small sub-\nset of the model parameters for any given input token. As\nsuch, sparse MoEs have enabled unprecedented scalability,\nresulting in tremendous successes across domains such as\nnatural language processing and computer vision. In this\nwork, we instead explore the use of sparse MoEs to scale-\ndown Vision Transformers (ViTs) to make them more attrac-\ntive for resource-constrained vision applications. To this\nend, we propose a simplified and mobile-friendly MoE de-\nsign where entire images rather than individual patches are\nrouted to the experts. We also propose a stable MoE train-\ning procedure that uses super-class information to guide the\nrouter. We empirically show that our sparse Mobile Vision\nMoEs (V-MoEs) can achieve a better trade-off between per-\nformance and efficiency than the corresponding dense ViTs.\nFor example, for the ViT-Tiny model, our Mobile V-MoE\noutperforms its dense counterpart by 3.39% on ImageNet-\n1k. For an even smaller ViT variant with only 54M FLOPs\ninference cost, our MoE achieves an improvement of 4.66%.\n1. Introduction\nThe trade-off between performance and efficiency of\nneural networks (NNs) remains a challenge, especially in\nsettings where computational resources are limited.\nRe-\ncently, sparsely-gated Mixture-of-Experts models (sparse\nMoEs) have gained popularity as they provide a promis-\ning solution to this problem by enabling the decoupling of\nmodel size from inference efficiency [3]. MoEs are NNs\nthat are partitioned into \u201cexperts\u201d, which are trained jointly\nwith a router to specialize on subsets of the data. In MoEs,\neach input is processed by only a small subset of model\nparameters (aka conditional computation). In contrast, tra-\nditional dense models activate all parameters for each input.\n*Correspondence to: Erik Daxberger, edaxberger@apple.com.\n50M\n100M\n500M\n1B\nFLOPs\n35\n40\n45\n50\n55\n60\n65\n70\n75\nTop-1 Validation Accuracy on ImageNet\n12x384\n6x384\n12x192\n(ViT-Tiny)\n6x192\n6x96\n6x64\nMobile V-MoE (ours)\nDense ViT\nFigure 1. Accuracy vs. FLOPs for ViTs of different sizes. La-\nbels (e.g. 12\u00d7192, which is ViT-Tiny) refer to the number of ViT\nlayers (e.g. 12) and the hidden embedding dimension (e.g. 192).\nThe sparse MoEs outperform their corresponding dense baselines\nacross different model scales. Fig. 3a lists all numerical results.\nSparse MoEs were popularized in deep learning by [16],\nwhich introduced sparse MoE-layers as drop-in replace-\nments for standard NN layers. Most recent MoEs are based\non the Transformer [19], which processes individual input\ntokens; in accordance, recent MoEs also route individual in-\nput tokens to experts, i.e., image patches in the case of Vi-\nsion Transformers (ViTs) [2,13] (see Fig. 2b). Conditional\ncomputation as implemented by sparse MoEs has enabled\nthe training of Transformers of unprecedented size [4]. As\na result, MoEs have achieved impressive successes across\nvarious domains including language [4, 10], vision [13],\nspeech [20] and multi-modal learning [12], and currently\nhold state-of-the-art results on many benchmarks [21].\nThe ability to increase model capacity while keeping in-\nference cost low is also appealing for resource-constrained\nvision problems. While Transformers are getting increas-\ningly established as the de-facto standard architecture for\nlarge-scale visual modeling [2, 13], virtually all mobile-\n1\narXiv:2309.04354v1  [cs.CV]  8 Sep 2023\n(a) Dense ViT\n(b) Regular V-MoE\n(c) Mobile V-MoE\n(d) Layer types\nFigure 2. Model architectures. (a) The dense ViT baseline model uses dense ViT layers throughout. (b) Regular sparse V-MoE with\nlayer-wise per-patch routers. (c) Our proposed sparse Mobile V-MoE design with a single per-image router. In both (b) and (c), dense ViT\nlayers are followed by MoE-ViT layers (here, k = 1 out of E = 3 experts are activated per input). (d) In contrast to dense ViT layers [19],\nMoE-ViT layers have a separate MLP per expert (preceded by a router) while all other parts of the layer are shared across all experts [13].\nfriendly models still leverage convolutions due to their effi-\nciency [1,5,6,11,15,18]. As such, conditional computation\ncould potentially enable attention-based models to reduce\nthe gap to convolutional models in the small-scale regime.\nHowever, Transformer-based MoEs have not yet been ex-\nplored for resource-constrained settings; this might be due\nto two main weaknesses of recently-popularized MoEs [16].\nFirstly, while per-token routing increases the flexibility\nto learn an optimal computation path through the model, it\nmakes inference inefficient, as many (or even all) experts\nneed to be loaded for a single input image. Secondly, recent\nMoEs train the routers jointly with the rest or the model in\nan end-to-end fashion. To avoid collapse to just a few ex-\nperts while ignoring all others, one needs to use load balanc-\ning mechanisms [3] such as dedicated auxiliary losses [16].\nHowever, the resulting complex optimization objectives of-\nten lead to training instabilities / divergence [4,10,12,21].\nIn this work, we investigate the potential of sparse MoEs\nto scale-down ViTs for resource-constrained vision appli-\ncations via an MoE design and training procedure that ad-\ndresses the aforementioned issues. Our contributions are:\n1. We propose a simplified, mobile-friendly sparse MoE\ndesign in which a single router assigns entire images\n(rather than image patches) to the experts (see Fig. 2c).\n2. We develop a simple yet robust training procedure in\nwhich expert imbalance is avoided by leveraging se-\nmantic super-classes to guide the router training.\n3. We empirically show that our proposed sparse MoE\napproach allows us to scale-down ViT models by im-\nproving their performance vs. efficiency trade-off.\n2. Scaling down ViTs via sparse MoEs\n2.1. Conditional computation with sparse MoEs\nAn MoE implements conditional computation by activat-\ning different subsets of a NN (so-called experts) for differ-\nent inputs. We consider an MoE layer with E experts as\nMoE(x) =\nE\nX\ni=1\ng(x)iei(x),\n(1)\nwhere x \u2208 RD is the input to the layer, ei : RD \u2192 RD\nis the function computed by expert i, and g : RD \u2192 RE\nis the routing function which computes an input-dependent\nweight for each expert [16]. In a ViT-based MoE, each ex-\npert ei is parameterized by a separate multi-layer perceptron\n2\n(MLP) within the ViT layer, while the other parts are shared\nacross experts (see Fig. 2d). We use the routing function\ng(x) = TOPk(softmax(Wx)),\n(2)\nwhere the operation TOPk(x) sets all elements of x to zero\nexcept those with the k largest values [13]. In a sparse MoE,\nwe have k \u226a E, s.t. we only need to load and compute the\nk experts with the largest routing weights. This allows us\nto scale-up the overall model capacity (determined by E)\nwithout increasing the inference cost (determined by k).\n2.2. Efficient and robust MoEs for small-scale ViTs\nPer-image routing. Recent large-scale sparse MoEs use\nper-patch routing (i.e. the inputs x are individual image\npatches). This generally requires a larger number of experts\nto be activated for each image. For example, [13] show that\nin their MoE with per-patch routing, \u201cmost images use \u2013on\naggregate by pooling over all their patches\u2013 most of the ex-\nperts\u201d [13, Appendix E.3]. Thus, per-patch routing can in-\ncrease the computational and memory overhead of the rout-\ning mechanism and reduce the overall model efficiency. We\ninstead propose to use per-image routing (i.e., the inputs x\nare entire images) to reduce the number of activated experts\nper image, as also done in early works on MoEs [7,9].\nSuper-class-based routing. Previous works on sparse\nMoEs jointly train the router end-to-end together with the\nexperts and the dense ViT backbone, to allow the model to\nlearn the optimal assignment from inputs to experts based\non the data [13]. While learning the optimal routing mecha-\nnism from scratch can result in improved performance, it of-\nten leads to training instabilities and expert collapse, where\nmost inputs are routed to only a small subset of the experts,\nwhile all other experts get neglected during training [3].\nThus, an additional auxiliary loss is typically required to\nensure load-balancing between the different experts, which\ncan increase the complexity of the training process [3].\nIn contrast, we propose to group the classes of the dataset\ninto super-classes and explictly train the router to make each\nexpert specialize on one super-class. To this end, we add\nan additional cross-entropy loss Lg between the router out-\nput g(x) in Eq. (2) and the ground truth super-class labels\nto the regular classification loss LC to obtain the overall\nweighted loss L = LC +\u03bbLg (we use \u03bb = 0.3 in our exper-\niments, which we found to work well). Such a super-class\ndivision is often readily provided with the dataset (e.g. for\nCIFAR-10/100 or MS-COCO). If a dataset does not come\nwith a super-class division, we can easily obtain one as fol-\nlows: 1) we first train a dense baseline model on the dataset;\n2) we then compute the model\u2019s confusion matrix over a\nheld-out validation set; 3) we finally construct a confusion\ngraph from the confusion matrix and apply a graph cluster-\ning algorithm to obtain the super-class division [8]. This ap-\nproach encourages the super-classes to contain semantically\nID\nClasses\nSuper-class\n0\nboxer, pug, Rottweiler\ndogs\n1\norangutan, weasel, panda\nother mammals\n2\ntoucan, flamingo, ostrich\nbirds\n3\neel, scorpion, hammerhead\nother animals\n4\nminivan, ambulance, taxi\nland vehicles\n5\nsubmarine, canoe, pirate\nsea vehicles\n6\nguacamole, hotdog, banana\nfood\n7\nbackpack, pyjama, kimono\nclothes\n8\nmonitor, iPod, photocopier\ntech devices\n9\nxylophone, harp, trumpet\ninstruments\nTable 1. Super-class division for E = 10. For each super-class,\nwe list three randomly chosen class names (which turn out to be\nsemantically related) together with a possible super-class name.\nsimilar images that the model often confuses. Intuitively,\nby allowing the different MoE experts to specialize on the\ndifferent semantic data clusters, performance on the highly-\nconfused classes should be improved. We use this approach\nin our experiments on ImageNet-1k, computing the con-\nfusion matrix via a dense ViT-S/16 model. The resulting\nsuper-class division for E = 10 experts is shown in Tab. 1;\nthe super-classes contain semantically related classes.\n3. Experiments\nWe now present empirical results on the standard\nImageNet-1k classification benchmark [14]. We train all\nmodels from scratch on the ImageNet-1k training set of\n1.28M images, and then evaluate their top-1 accuracy on\nthe held-out validation set of 50K images. In Sec. 3.1, we\nfirst evaluate our proposed sparse Mobile V-MoE across a\nrange of model scales and show that they achieve better per-\nformance vs. efficiency trade-offs than the respective dense\nViT baselines. In Sec. 3.2, we then conduct several ablation\nstudies to get a better understanding of the properties of our\nproposed sparse MoE model design and training procedure.\n3.1. Accuracy vs. efficiency across ViT scales\nWe consider ViT models (both MoEs and corresponding\ndense baselines) of different sizes by scaling the total num-\nber of layers (we use 12, 9 or 6) and the hidden embedding\nsize (we use 384, 192, 96 or 64). The number of multi-head\nself-attention heads is (6, 3, 3, 2) for the different hidden\nembedding sizes. The embedding size of the MLP is 4\u00d7\nthe hidden embedding size, as is common practice. We use\nE = 10 experts in total for the MoE, out of which k = 1\nis activated per input image. Our MoEs comprise of L = 2\nMoE-ViT layers preceded by (10, 7 or 4) dense ViT layers\n(see Fig. 2c). We use a patch size of 32 \u00d7 32 for all mod-\nels. This is because the the patch size effectively controls\n3\nModel\nFLOPs\nTop-1 Accuracy\nDense\nMoE\n\u2206\n12\u00d7384\n2297M\n71.88\n74.23\n+2.35\n9\u00d7384\n1752M\n69.94\n72.47\n+2.53\n6\u00d7384\n1207M\n63.21\n66.91\n+3.70\n12\u00d71921\n618M\n59.51\n62.90\n+3.39\n9\u00d7192\n478M\n56.50\n59.52\n+3.02\n6\u00d7192\n338M\n51.18\n55.69\n+4.51\n12\u00d796\n176M\n53.79\n55.39\n+1.60\n9\u00d796\n140M\n51.27\n52.99\n+1.72\n6\u00d796\n103M\n46.54\n50.28\n+3.74\n12\u00d764\n88M\n42.90\n46.07\n+3.17\n9\u00d764\n71M\n40.46\n43.95\n+3.49\n6\u00d764\n54M\n36.64\n41.30\n+4.66\n(a) Accuracy vs. efficiency across ViT scales.\nE\nRouter\nMoE\n\u2206\n5\n86.43\n72.33\n+1.64\n7\n87.43\n73.13\n+2.44\n10\n87.12\n73.52\n+2.83\n15\n84.16\n73.10\n+2.41\n20\n84.08\n73.36\n+2.67\n(b) Total number of experts E.\nL\nRouter\nMoE\n\u2206\n1\n90.17\n72.14\n+1.45\n2\n87.12\n73.52\n+2.83\n4\n82.12\n71.67\n+0.98\n6\n77.70\n70.07\n-0.62\n8\n72.09\n64.47\n-6.22\n(c) Number of MoE layers L.\nk\nFLOPs\nDense\nMoE\n\u2206\n1\n2534M\n70.79\n73.44\n+2.65\n2\n2769M\n71.70\n74.42\n+2.72\n3\n3005M\n73.58\n74.44\n+0.86\n5\n3476M\n74.87\n74.32\n-0.55\n10\n4653M\n75.10\n74.37\n-0.73\n(d) Number of experts k per image.\nRouting\nInput\nAcc.\n\u2206\nDense\nN/A\n71.70\nSuper-class\nimage\n74.42\n+2.72\nRand. class\nimage\n69.22\n-2.48\nEnd-to-end\nimage\n73.57\n+1.87\nEnd-to-end\ntoken\n74.85\n+3.15\n(e) Routing strategies.\nFigure 3.\nEmpirical results.\n(a) Our Mobile V-MoEs outperform the respective dense ViTs across model scales.\nModel names\n(e.g. 12\u00d7192) refer to the number of layers (12) and the embedding size (192). (b-e) Ablation studies using DeiT-Ti/16 [17], with k = 1,\nE = 10, L = 2 by default. Best performance vs. efficiency trade-off is achieved with (b) E = 10 experts total, (c) L = 2 MoE layers (out\nof 12 layers total), (d) k = 1 or k = 2 experts activated per image, (e) our semantic super-class routing; the settings used in (a) are bolded.\nthe trade-off between FLOPs and number of model param-\neters: as we aim to optimize for FLOPs, a larger patch size\n(resulting in a fewer number of patches) is beneficial. We\nalso tried using a smaller patch size of 16 \u00d7 16, where the\nresult trends were basically the same (but where the num-\nber of FLOPs was higher relative to the model capacity and\nthus accuracy). For the ViTs with hidden sizes 384 and 192,\nwe use the DeiT training recipe [17], while for hidden sizes\n96 and 64, we use the standard ViT training recipe [2] to\navoid underfitting. Figs. 1 and 3a compare top-1 validation\naccuracy vs. FLOPs. Our Mobile V-MoEs outperform the\ncorresponding dense ViT baselines across all model sizes.\n3.2. Ablation studies\nWe train DeiT-Tiny [17] (12 layers total, 192 embedding\nsize, 16 \u00d7 16 patch size) with k = 1 out of E = 10 experts\nper input, and with L = 2 MoE layers (unless noted other-\nwise); the dense ViT baseline achieves 70.79% accuracy.\nTotal number of experts E.\nWe consider different\nwidths of the MoE, i.e., different numbers of experts E (and\nthus super-classes), ranging between E = 5 and E = 20.\nWe report both the accuracy of the entire MoE model (i.e.,\non the 1,000-way classification task), as well as the accu-\nracy of the router (i.e., on the E-way super-classification\ntask). Fig. 3b shows that overall performance improves until\nE = 10, from which point onwards it stagnates. The router\naccuracy also drops beyond E = 10 due to the increased\ndifficulty of the E-way super-classification problem.\nNumber of MoE layers L. We consider different depths\nof the MoE, i.e., different numbers of MoE layers L, rang-\n1This corresponds to the ViT-Tiny model [17] with patch size 32 \u00d7 32.\ning between L = 1 and L = 8 (out of 12 ViT layers in\ntotal). We again report both the full MoE and router ac-\ncuracies. Fig. 3c shows that overall performance peaks at\nL = 2, and rapidly decreases for larger L. This is due to\nthe router accuracy, which declines with increasing L as the\nrouter gets less information (from the 12 \u2212 L ViT layers).\nNumber of experts k per image. We vary the number\nof experts k activated per image. We compare against dense\nbaselines that use an MLP with hidden dimension scaled\nby k to match the MoE\u2019s inference FLOPs. Fig. 3d shows\nthat k = 1 and k = 2 perform best (relative to the dense\nbaseline), with decreasing performance delta for larger k.\nRouting strategies. We compare our proposed semantic\nsuper-class per-image routing vs. end-to-end-learned rout-\ning (both per-image and per-token) and a baseline with ran-\ndom super-classes (for k=2). Fig. 3e shows that our method\n(Fig. 2c) is better, except for learned per-token routing (as\nin the regular V-MoE [13], Fig. 2b), which however needs\nto activate many more experts and thus model parameters\nfor each input image (up to 11.05M, vs. 6.31M for ours).\n4. Conclusions and future work\nWe showed that sparse MoEs can improve the perfor-\nmance vs. efficiency trade-off compared to dense ViTs,\nin an attempt to make ViTs more amenable to resource-\nconstrained applications. In the future, we aim to apply our\nMoE design to models that are more mobile-friendly than\nViTs, e.g., light-weight CNNs such as MobileNets [5,6,15]\nor ViT-CNN hybrids [1, 11, 18]. We also aim to consider\nother vision tasks, e.g., object detection. Finally, we aim to\nget actual on-device latency measurements for all models.\n4\nReferences\n[1] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen\nLiu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu.\nMobile-\nformer: Bridging mobilenet and transformer. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5270\u20135279, 2022. 2, 4\n[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 1, 4\n[3] William Fedus, Jeff Dean, and Barret Zoph.\nA review\nof sparse expert models in deep learning.\narXiv preprint\narXiv:2209.01667, 2022. 1, 2, 3\n[4] William Fedus, Barret Zoph, and Noam Shazeer.\nSwitch\ntransformers: Scaling to trillion parameter models with sim-\nple and efficient sparsity.\nJ. Mach. Learn. Res, 23:1\u201340,\n2021. 1, 2\n[5] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3.\nIn Proceedings of the IEEE/CVF international\nconference on computer vision, pages 1314\u20131324, 2019. 2,\n4\n[6] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2, 4\n[7] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and\nGeoffrey E Hinton. Adaptive mixtures of local experts. Neu-\nral computation, 3(1):79\u201387, 1991. 3\n[8] Ruochun Jin, Yong Dou, Yueqing Wang, and Xin Niu. Con-\nfusion graph:\nDetecting confusion communities in large\nscale image classification. In IJCAI, pages 1980\u20131986, 2017.\n3\n[9] Michael I Jordan and Robert A Jacobs. Hierarchical mix-\ntures of experts and the em algorithm. Neural computation,\n6(2):181\u2013214, 1994. 3\n[10] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao\nChen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam\nShazeer, and Zhifeng Chen. Gshard: Scaling giant models\nwith conditional computation and automatic sharding. arXiv\npreprint arXiv:2006.16668, 2020. 1, 2\n[11] Sachin Mehta and Mohammad Rastegari. Mobilevit: light-\nweight, general-purpose, and mobile-friendly vision trans-\nformer. arXiv preprint arXiv:2110.02178, 2021. 2, 4\n[12] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe\nJenatton, and Neil Houlsby. Multimodal contrastive learning\nwith limoe: the language-image mixture of experts. arXiv\npreprint arXiv:2206.02770, 2022. 1, 2\n[13] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim\nNeumann, Rodolphe Jenatton, Andr\u00b4e Susano Pinto, Daniel\nKeysers, and Neil Houlsby. Scaling vision with sparse mix-\nture of experts. Advances in Neural Information Processing\nSystems, 34:8583\u20138595, 2021. 1, 2, 3, 4\n[14] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115:211\u2013252, 2015. 3\n[15] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510\u20134520, 2018. 2, 4\n[16] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy\nDavis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-\ngeously large neural networks: The sparsely-gated mixture-\nof-experts layer. arXiv preprint arXiv:1701.06538, 2017. 1,\n2\n[17] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International conference on machine learning,\npages 10347\u201310357. PMLR, 2021. 4\n[18] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, On-\ncel Tuzel, and Anurag Ranjan. An improved one millisecond\nmobile backbone. arXiv preprint arXiv:2206.04040, 2022. 2,\n4\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 1, 2\n[20] Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe:\nScaling to large acoustic models with dynamic routing mix-\nture of experts. arXiv preprint arXiv:2105.03036, 2021. 1\n[21] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-\nping Huang, Jeff Dean, Noam Shazeer, and William Fedus.\nDesigning effective sparse expert models.\narXiv preprint\narXiv:2202.08906, 2022. 1, 2\n5\n"
  },
  {
    "title": "Towards Practical Capture of High-Fidelity Relightable Avatars",
    "link": "https://arxiv.org/pdf/2309.04247.pdf",
    "upvote": "8",
    "text": "Towards Practical Capture of High-Fidelity Relightable Avatars\nHaotian Yang\nKuaishou Technology\nChina\nyanghaotian03@kuaishou.com\nMingwu Zheng\nKuaishou Technology\nChina\nzhengmingwu@kuaishou.com\nWanquan Feng\nKuaishou Technology\nChina\nfengwanquan@kuaishou.com\nHaibin Huang\nKuaishou Technology\nChina\nhuanghaibin03@kuaishou.com\nYu-Kun Lai\nCardiff University\nUnited Kingdom\nlaiy4@cardiff.ac.uk\nPengfei Wan\nKuaishou Technology\nChina\nwanpengfei@kuaishou.com\nZhongyuan Wang\nKuaishou Technology\nChina\nwangzhongyuan@kuaishou.com\nChongyang Ma*\nKuaishou Technology\nChina\nchongyangma@kuaishou.com\nVolumetric avatar\nEnvironment map relighting\nVideo-driven animation\nFigure 1: We present TRAvatar, a novel framework to capture and reconstruct high-fidelity volumetric avatars. Trained\nefficiently end-to-end on multi-view image sequences under varying illuminations, our virtual avatars can be relighted and\nanimated in real-time of high fidelity.\nABSTRACT\nIn this paper, we propose a novel framework, Tracking-free Re-\nlightable Avatar (TRAvatar), for capturing and reconstructing high-\nfidelity 3D avatars. Compared to previous methods, TRAvatar works\nin a more practical and efficient setting. Specifically, TRAvatar is\ntrained with dynamic image sequences captured in a Light Stage\nunder varying lighting conditions, enabling realistic relighting and\nreal-time animation for avatars in diverse scenes. Additionally, TRA-\nvatar allows for tracking-free avatar capture and obviates the need\n* Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0315-7/23/12...$15.00\nhttps://doi.org/10.1145/3610548.3618138\nfor accurate surface tracking under varying illumination condi-\ntions. Our contributions are two-fold: First, we propose a novel\nnetwork architecture that explicitly builds on and ensures the sat-\nisfaction of the linear nature of lighting. Trained on simple group\nlight captures, TRAvatar can predict the appearance in real-time\nwith a single forward pass, achieving high-quality relighting effects\nunder illuminations of arbitrary environment maps. Second, we\njointly optimize the facial geometry and relightable appearance\nfrom scratch based on image sequences, where the tracking is im-\nplicitly learned. This tracking-free approach brings robustness for\nestablishing temporal correspondences between frames under dif-\nferent lighting conditions. Extensive qualitative and quantitative\nexperiments demonstrate that our framework achieves superior\nperformance for photorealistic avatar animation and relighting.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Volumetric models; Motion\ncapture; Reflectance modeling.\narXiv:2309.04247v1  [cs.CV]  8 Sep 2023\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nH. Yang, M. Zheng, W. Feng, H. Huang, Y.-K. Lai, P. Wan, Z. Wang, and C. Ma\nKEYWORDS\nRelighting; Facial animation; Neural rendering; View synthesis;\nAppearance acquisition.\nACM Reference Format:\nHaotian Yang, Mingwu Zheng, Wanquan Feng, Haibin Huang, Yu-Kun\nLai, Pengfei Wan, Zhongyuan Wang, and Chongyang Ma. 2023. Towards\nPractical Capture of High-Fidelity Relightable Avatars. In SIGGRAPH Asia\n2023 Conference Papers (SA Conference Papers \u201923), December 12\u201315, 2023,\nSydney, NSW, Australia. ACM, New York, NY, USA, 12 pages. https://doi.\norg/10.1145/3610548.3618138\n1\nINTRODUCTION\nIn this work, we focus on the capture and reconstruction of high-\nfidelity avatars in a Light Stage environment. As virtual repre-\nsentations of humans, avatars are crucial components in various\ndownstream applications, such as video games, virtual reality, telep-\nresence, and more [Bi et al. 2021; Guo et al. 2019; Lombardi et al.\n2018; Moser et al. 2021; Schwartz et al. 2020].\nAvatar creation has been a popular and challenging research\ntopic in computer graphics and computer vision for decades. Despite\nconsiderable progress in this field, there are still many challenges to\novercome, including expensive and sophisticated setup for avatar\ncapture, lack of support for realistic relighting and animation, and\nhigh resource demands making training time-consuming and real-\ntime deployment difficult to achieve. Traditional frameworks based\non graphics pipeline, including geometry reconstruction [Beeler\net al. 2010, 2011; Collet et al. 2015; Guo et al. 2019; Riviere et al. 2020;\nWu et al. 2018] and physically-inspired reflectance capture [De-\nbevec et al. 2000; Ghosh et al. 2011; Ma et al. 2007; Moser et al. 2021;\nWeyrich et al. 2006], are often difficult to set up and lack robustness,\nespecially for dynamic subjects and non-facial parts. Recent deep\nlearning based methods [Bi et al. 2021; Cao et al. 2022; Lombardi\net al. 2018, 2021; Remelli et al. 2022] have demonstrated promis-\ning improvements for avatar representation by approximating the\ngeometry and appearance with neural networks. However, most\nlearning-based methods struggle to handle relighting effectively\nand have computationally expensive pre-processing and training\nsteps that cannot meet the aforementioned requirements.\nTo this end, we propose a novel framework, Tracking-free Re-\nlightable Avatar (TRAvatar), that can circumvent the above obsta-\ncles, supporting efficient capture, high-quality reconstruction, as\nwell as real-time animation and relighting (see Figure 1). Specifi-\ncally, we improve the entire pipeline at its two primary stages, i.e.,\nboth the data capture and avatar reconstruction.\nFor the data capture stage, we record a subject\u2019s performance\nunder various expressions and lighting conditions. To faithfully\nreproduce the identity and detailed expressions of a specific sub-\nject, both dynamic geometry and reflectance should be captured.\nConsidering the complexity of lighting conditions, it is non-trivial\nfor the avatar network to directly learn the mapping from envi-\nronment maps to the appearance. Furthermore, it is challenging to\nachieve satisfactory decoupling of lighting and other input condi-\ntions. To overcome this challenge, we take advantage of the prior\nknowledge of lighting, specifically its linear nature, to guide the net-\nwork design. We design a network structure that explicitly exploits\nand guarantees to satisfy the linear nature of lighting, making it\neasy to train and enabling excellent generalization ability. Trained\non dynamically captured image sequences in simple controllable\ngroup light illumination [Bi et al. 2021], our model can predict the\nappearance under arbitrary and complex lighting condition in a\nsingle forward pass, which facilitates real-time environment re-\nlighting. The learned disentangled representation also allows our\ndata-driven avatar to be animated, relighted, and rendered under\nnovel viewpoints.\nFor avatar reconstruction, we generate a 3D model from captured\ndata that can be manipulated in real time. It is a challenging task\nto estimate temporal correspondences between captured frames\nwith different lighting conditions. Previous learning-based methods\ntypically rely on a pre-processing step to compute explicit tracked\ngeometry (as a deformable base mesh), which is computationally\nexpensive and not robust to varying light conditions. Therefore, we\npropose to jointly optimize the relightable appearance and latent\ngeometry from scratch from image sequences, where the tracking\nis implicitly learned. Different from previous methods that separate\nmesh tracking and avatar creation in two stages, our tracking-free\napproach implicitly learns the dynamic deformation of the base\nmesh directly from the multi-view captured data, along with the\nrelightable appearance in a joint optimization process. In addition\nto being much more efficient, this joint optimization allows our\nmodel to be directly trained on images in varying illumination,\nwhich is challenging for traditional explicit surface tracking.\nOur experiments with TRAvatar show its effectiveness in creat-\ning high-quality and authentic avatars that can be animated and\nrelighted in real-time with superior visual quality and computa-\ntional efficiency compared to previous methods.\nIn summary, our contributions are:\n\u2022 We present TRAvatar, a practical and efficient capture solution for\ncreating high-fidelity avatars that can be animated and relighted\nin real time.\n\u2022 We propose a novel network architecture that explicitly exploits\nthe linear nature of lighting to improve generalizability, enabling\nreal-time relighting with high realism for given environment\nmaps.\n\u2022 We propose to jointly optimize the relightable appearance and\nlatent geometry of avatars from image sequences captured under\nvarying lighting conditions, allowing more efficient and effective\ncreation of relightable virtual avatars.\n\u2022 We demonstrate that TRAvatar outperforms previous methods\nin terms of both visual quality and computational efficiency.\n2\nRELATED WORK\nCreating a data-driven, relightable facial avatar of a specific sub-\nject typically involves capturing both dynamic geometry and re-\nflectance. This is followed by constructing a parametric model from\nthe captured data, or alternatively, employing image-based relight-\ning techniques to synthesize the output. Below, we provide a concise\noverview of most relevant methods.\nGeometry and reflectance acquisition. 3D face reconstruction and\nperformance capture have been active research topics for decades.\nAccordingly, sophisticated 3D scanning systems have been devel-\noped for both static geometry reconstruction [Beeler et al. 2010;\nGhosh et al. 2011] and dynamic performance capture [Beeler et al.\nTowards Practical Capture of High-Fidelity Relightable Avatars\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n2011; Bradley et al. 2010; Collet et al. 2015; Dou et al. 2017a; Guo\net al. 2019; Huang et al. 2011]. These methods utilize either multi-\nview stereo (MVS) or structured light for point cloud acquisition\nand then estimate the deforming geometry to achieve temporally\nconsistent mesh tracking. The tracking process often involves time-\nconsuming MVS reconstruction for thousands of frames and dense\noptical-flow optimization, while existing real-time face tracking\nalgorithms cannot achieve satisfactory accuracy.\nBesides, another crucial aspect of realistic relightable avatars\nis to estimate the way in which the light interacts with the sub-\nject, i.e., the reflectance property. Previous methods usually assume\nphysically-inspired reflectance functions modeled as bidirectional\nreflectance distribution function (BRDF) [Schlick 1994] and solve\nthe parameters by observing the appearance under active or pas-\nsive lighting. Active lighting methods typically require specialized\nsetups with controllable illuminations and synchronized cameras.\nDebevec et al. [2000] pioneer in using a Light Stage for facial re-\nflectance acquisition. One-light-at-a-time (OLAT) capture is per-\nformed to obtain the dense reflectance field. Later, polarized [Ghosh\net al. 2011; Ma et al. 2007; Zhang et al. 2022] and color gradient\nilluminations [Fyffe and Debevec 2015; Guo et al. 2019] are used\nfor rapid acquisition. Passive capture methods have significantly\nreduced the necessity for an expensive capture setup. For example,\nRiviere et al. [2020] and Zheng et al. [2023] propose to estimate\nphysically-based facial textures via inverse rendering.\n3D face modeling. Modeling of facial geometry and appearance\nhas been a fundamental component of human related tasks in com-\nputer graphics and computer vision. The seminal work on 3D mor-\nphable models (3DMMs) [Blanz and Vetter 1999; Cao et al. 2013;\nYang et al. 2020] employs Principal Component Analyze (PCA) to\nderive the shape basis from head scans. Despite its widespread\nuse in various applications such as single-view face reconstruc-\ntion and tracking [Dou et al. 2017b; Thies et al. 2016; Zhu et al.\n2017], the shape space of 3DMMs is limited by its low-dimensional\nlinear representation. Follow-up methods separate the parametric\nspace dimensions [Jiang et al. 2019; Li et al. 2017; Vlasic et al. 2005]\nor use local deformation models [Wu et al. 2016] to enhance the\nrepresentation power of the morphable model.\nIn recent years, deep learning based methods [Bagautdinov et al.\n2018; Tran and Liu 2018, 2019; Zhang et al. 2022; Zheng et al. 2022]\nhave been widely used to achieve impressive realism in face model-\ning. Lombardi et al. [2018] utilize a Variational Autoencoder (VAE)\n[Kingma and Welling 2013] to jointly model the mesh and dynamic\ntexture, which is used for monocular [Yoon et al. 2019] and binoc-\nular [Cao et al. 2021] facial performance capture. Bi et al. [2021]\npropose to extend the VAE-based deep appearance model by cap-\nturing the dynamic performance under controllable group light\nilluminations to enable relighting.\nWhile mesh-based methods typically require dense correspon-\ndence based on sophisticated surface tracking algorithms [Beeler\net al. 2011; Wu et al. 2018] for training and degrade in non-facial\nregions, recent progress in neural volumetric rendering further en-\nables photorealistic avatar creation. Lombardi et al. [2021] propose\nMVP (Mixture of Volumetric Primitives), a hybrid volumetric and\nprimitive-based representation that produces high-fidelity render-\ning results with efficient runtime performance. More recently, Li et\nFigure 2: Illustration of our capture setup. Top left: Our cus-\ntomized capturing apparatus. Top right: The layout of 24\ncameras. Bottom: Snapshots of captured frames from the\nfrontal camera in a recording. Both the expression and the\nlighting condition change across different frames.\nal. [2023] extend MVP with eyeglasses to be relightable follow-\ning [Bi et al. 2021]. But it requires additional efforts for real-time\nrelighting.\nSome other methods have been proposed to create a facial avatar\nfrom monocular videos [Gao et al. 2022; Zielonka et al. 2023] or\nRGB-D input [Cao et al. 2022] without a specialized capturing\napparatus. However, these approaches do not provide a relightable\nappearance, and their quality cannot match that of avatars built\nfrom industrial capture setups.\nImage-based relighting. In contrast to model-based reflectance\nacquisition approaches, image-based relighting addresses the prob-\nlem from an orthogonal perspective. By exploiting the linear nature\nof light transport, Debevec et al. [2000] propose to add up hundreds\nof images of densely sampled reflectance fields from OLAT capture\nto synthesize rendering results under novel lighting conditions.\nSubsequently, the number of sampled images is reduced by using\nspecifically designed illumination patterns [Peers et al. 2009; Reddy\net al. 2012] or employing sparse sampling [Fuchs et al. 2007; Wang\net al. 2009]. Xu et al. [2018] propose to train a network for relighting\na scene from only five input images. Meka et al. [2019] show that\nthe full 4D reflectance field of human faces can be regressed from\ntwo images under color gradient light illumination. Sun et al. [2020]\npropose a learning-based method to achieve higher lighting resolu-\ntion than the original Light Stage OLAT capture. Although these\napproaches achieve photorealistic rendering under novel lighting\nconditions, they only work from fixed viewpoints.\nMeka et al. [2020] achieve relightable free viewpoint rendering\nof dynamic facial performance by extending Meka et al. [2019]\nwith explicit 3D reconstruction and multi-view capture. However,\nthey extract pixel-aligned features from captured raw images under\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nH. Yang, M. Zheng, W. Feng, H. Huang, Y.-K. Lai, P. Wan, Z. Wang, and C. Ma\n\ud835\udc49\ud835\udc5f\ud835\udc54\ud835\udc4f\nGround Truth\n\u2112\ud835\udc56\ud835\udc5a\ud835\udc54\nConvT\nLReLU\nConvT\nConvT\nLReLU\nConvT\n.\n\u2026\n\u2026\nConvT\nLReLU\nConvT\nConvT\nRendered\n\ud835\udc1d\nMLP\n\ud835\udc45\u210e\ud835\udc52\ud835\udc4e\ud835\udc51, \ud835\udc61\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\n\ud835\udc67\ud835\udc52\n\u2130\ud835\udc5a\nMLP\n\ud835\udc49\ud835\udefc\n\ud835\udeff\ud835\udc2f\n\ud835\udc9f\ud835\udc5f\ud835\udc54\ud835\udc4f\nFC\nFC\n\ud835\udc45\ud835\udc5d, \ud835\udc61\ud835\udc5d, \ud835\udc60\ud835\udc5d\n\ud835\udc9f\ud835\udc47\n\ud835\udc59\nImages\nc\n.\n.\n\ud835\udc9f\ud835\udc5a\ud835\udc52\ud835\udc60\u210e\nBase mesh\nVolume\nImage synthesis\nReconstruct\n\ud835\udc9f\ud835\udefc\nFigure 3: The pipeline of our framework. TRAvatar is a relightable volumetric avatar representation learned from multiview\nimage sequences, including dynamic expressions and varying illuminations. For each frame, a motion encoder E\ud835\udc5a forecasts\nthe disentangled global rigid transformation {\ud835\udc45\u210e\ud835\udc52\ud835\udc4e\ud835\udc51,\ud835\udc61\u210e\ud835\udc52\ud835\udc4e\ud835\udc51} and expression code \ud835\udc67\ud835\udc52. With the given expression code, lighting\ncondition \ud835\udc59, and view direction d, a series of decoders subsequently predict the base mesh and the volumetric primitives mounted\non it. Notably, a physically-inspired appearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f (detailed in Section 4.2) is proposed to facilitate network training.\nUltimately, the avatar representation is computed and then rendered, adaptable to any viewpoint and any lighting condition.\ncolor gradient light illumination to build relightable textures, which\nlimits its usage scenarios to performance replay. In contrast, our\napproach enables the creation of virtual avatars that not only allows\nfor free viewpoint rendering with a relightable appearance but\nalso possesses the capability of being controlled by an animation\nsequence of a different subject.\n3\nCAPTURING APPARATUS\nTo create an animatable and relightable avatar with ultra-high real-\nism and specific identity, it is necessary to capture its performance\nunder various expressions and lighting conditions. To this end,\nwe have constructed an apparatus following the design principles\nof Light Stages [Debevec 2012; Guo et al. 2019]. Our customized\ncapturing apparatus is shown in Figure 2.\nOur Light Stage, installed on a spherical structure with a 3.6-\nmeter diameter, comprises 356 lighting units and 24 machine vision\ncameras. We strategically place the cameras to capture the subject\nfrom multiple angles, and arrange the lighting units for precise\ncontrol over illumination conditions. The Light Stage is placed in a\ndark room to prevent environment light interference.\nLighting units. The 356 lighting units are uniformly mounted on\nthe sphere and are oriented towards the center. Each customized\nlighting unit comprises 132 high-brightness Light-Emitting Diodes\n(LEDs) that are controlled by a programmable embedded system.\nThe LEDs are equipped with diffusers and lenses to ensure equal\ndensity illumination at the center.\nThere are five different types of LEDs on the lighting unit, namely\nred, green, blue, white 4500K, and white 6500K. The setup follows\nthe latitude-longitude polarization as proposed in [Ghosh et al.\n2011], and each type of LED is grouped into three categories with\ndifferent polarization arrangements. The brightness of each group\nof lights can be adjusted independently using Pulse Width Modula-\ntion up to 100KHz. All the lighting units are connected to a central\ncontrol unit and a computer via a CAN bus. The lighting pattern\ncan be shuffled within 2ms, allowing us to capture the subject\u2019s\nperformance under various lighting conditions quickly.\nCameras. Our apparatus includes 24 machine vision cameras in-\nstalled around the sphere, with a focus on the center. The cameras\nconsist of four 31M RGB cameras, 12 5M RGB cameras, and eight\n12M monochrome cameras. The trigger ports of these cameras are\nlinked to the central control unit, which synchronizes the cameras\nand lighting units to capture the subject\u2019s performance under vari-\nous lighting conditions. We have disabled postprocessing features\nsuch as automatic gain adjustments in the cameras to ensure a\nlinear response to the illuminance.\nDepending on the camera types, we transmit the captured images\nto seven PCs via 10G Ethernet or USB ports. We calibrate the camera\narray with a 250mm calibration sphere similar to [Beeler et al. 2010]\nand undistort the images to ensure high-quality reconstruction.\nThe mean reprojection error is less than 0.4 pixels, which facilitates\nhigh-quality creation of the target avatar.\n4\nMETHOD\nIn this section, we formally introduce our novel framework, namely\nTRAvatar, which learns a disentangled representation for the target\navatar to be animated, relighted, and rendered from novel view-\npoints. As shown in Figure 3, our approach is based on a variational\nautoencoder (VAE) [Kingma and Welling 2013] architecture, where\nTowards Practical Capture of High-Fidelity Relightable Avatars\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nthe latent space is designed to be disentangled with linear responses\nto varying lighting conditions , providing efficient and accurate\nmodeling of dynamic geometry and reflectance fields.\nWe will first describe the details of our TRAvatar, including the\ntraining framework and network architecture (Section 4.1). The\ndetails of our specifically designed appearance decoder will be\nexplained in Section 4.2. We will then describe how to use our Light\nStage for data capture under various illuminations (Section 4.3).\nFinally, we will introduce the loss functions and regularization\nterms used for end-to-end network training (Section 4.4).\n4.1\nTRAvatar\nOur volumetric avatar is built upon Mixed Volumetric Primitives\n(MVP) [Lombardi et al. 2021], which is a generalized hybrid repre-\nsentation using both a base mesh and volumetric primitives (see\nFigure 4). Each primitive is mounted to the base mesh and is repre-\nsented as a volumetric grid with a resolution of \ud835\udc403. We set \ud835\udc40 = 8\nin our implementation.\nInspired by the success of image based relighting methods, our\nlighting condition is modeled as a vector \ud835\udc59 \u2208 R+356 representing the\nincoming light field of 356 densely sampled directions correspond-\ning to the light positions of the Light Stage. We employ a VAE based\narchitecture to train our relightable avatar. Different from previ-\nous methods [Bi et al. 2021; Remelli et al. 2022], we do not require\ntracked geometry in training. Note that the motion of a human head\ncan be separated into global rigid motion and expression related\nmotion. We utilize a motion encoder E\ud835\udc5a, to predict the disentan-\ngled motion. During training, for each frame, the convolutional\nmotion encoder E\ud835\udc5a takes a subset of the camera views as input\nand outputs the global head rotation \ud835\udc45\u210e\ud835\udc52\ud835\udc4e\ud835\udc51 \u2208 \ud835\udc46\ud835\udc42(3) and translation\n\ud835\udc61\u210e\ud835\udc52\ud835\udc4e\ud835\udc51 \u2208 R3 as well as the mean \ud835\udf07 \u2208 R256 and the standard deviation\n\ud835\udf0e \u2208 R+256 of a Gaussian distribution N (\ud835\udf07, \ud835\udf0e2). The expression code\n\ud835\udc67\ud835\udc52 \u2208 R256 is sampled from this Gaussian distribution and represents\nexpression related motion.\nTaking the expression code \ud835\udc67\ud835\udc52, the lighting condition \ud835\udc59, and the\nview direction d as input, we use several decoders to predict the base\nmesh and volumetric primitives for output synthesis. Specifically,\na mesh decoder D\ud835\udc5a\ud835\udc52\ud835\udc60\u210e : R256 \u2192 R3\u00d7\ud835\udc41\ud835\udc5a\ud835\udc52\ud835\udc60\u210e, which is a multilayer\nperceptron, predicts the residual vertex positions \ud835\udeffv based on the\nvertex positions \u02c6v of a template mesh with a fixed topology, where\n\ud835\udc41\ud835\udc5a\ud835\udc52\ud835\udc60\u210e is the number of mesh vertices. Then the resulting vertex\nposition v of the base mesh is computed as v = \ud835\udc45\u210e\ud835\udc52\ud835\udc4e\ud835\udc51 (\u02c6v+\ud835\udeffv)+\ud835\udc61\u210e\ud835\udc52\ud835\udc4e\ud835\udc51.\nFollowing [Lombardi et al. 2021], three decoders D\ud835\udc47 , D\ud835\udefc, and\nD\ud835\udc5f\ud835\udc54\ud835\udc4f with 2D convolutional architectures predict the volumet-\nric primitives upon the base mesh. Specifically, the transforma-\ntion decoder D\ud835\udc47 : R256 \u2192 R9\u00d7\ud835\udc41\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5a computes the rotation \ud835\udc45\ud835\udc5d,\ntranslation \ud835\udc61\ud835\udc5d, and scale \ud835\udc60\ud835\udc5d of \ud835\udc41\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5a primitives relative to the tan-\ngent space of the base mesh, which compensate for the motion\nthat is not modeled by the mesh vertex v. The opacity decoder\nD\ud835\udefc : R256 \u2192 R\ud835\udc403\u00d7\ud835\udc41\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5a also takes the expression code \ud835\udc67\ud835\udc52 as\ninput and decodes the voxel opacity \ud835\udc49\ud835\udefc of the primitives. The ap-\npearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f : R256+356+3 \u2192 R3\u00d7\ud835\udc403\u00d7\ud835\udc41\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5a takes the\nexpression code \ud835\udc67\ud835\udc52, the lighting condition \ud835\udc59, and the view direction\nd as input and predicts the RGB colors \ud835\udc49\ud835\udc5f\ud835\udc54\ud835\udc4f of the primitives. The\narchitecture of our relightable appearance decoder is designed to\nleverage the linear nature of lighting (see Section 4.2).\nCaptured image\nBase mesh\nVolumetric primitives\nFigure 4: Illustration of our hybrid avatar representation.\nThe base mesh and the volumetric primitives have consistent\nstructures which provide flexible control such as video driven\nanimation.\nOutput synthesis. Given the volumetric primitives, we use a dif-\nferentiable accumulative ray marching algorithm [Karras and Aila\n2013; Lombardi et al. 2021] to render the output images. Specifically,\nfor a ray r\ud835\udc5d (\ud835\udc61) = o\ud835\udc5d + \ud835\udc61d\ud835\udc5d with a direction d\ud835\udc5d starting from a pixel\n\ud835\udc5d with a 3D position o\ud835\udc5d, we compute the pixel color \ud835\udc3c\ud835\udc5f\ud835\udc54\ud835\udc4f (\ud835\udc5d) as:\n\ud835\udc3c\ud835\udc5f\ud835\udc54\ud835\udc4f (\ud835\udc5d) =\n\u222b \ud835\udc61max\n\ud835\udc61min\n\ud835\udc49\ud835\udc5f\ud835\udc54\ud835\udc4f (r\ud835\udc5d (\ud835\udc61))\ud835\udc51\ud835\udc47 (\ud835\udc5d,\ud835\udc61)\n\ud835\udc51\ud835\udc61\n,\n(1)\n\ud835\udc47 (\ud835\udc5d,\ud835\udc61) = min\n\u0010\n1,\n\u222b \ud835\udc61\n\ud835\udc61min\n\ud835\udc49\ud835\udefc\n\u0000r\ud835\udc5d (\ud835\udc61)\u0001\u0011\n,\n(2)\nwhere \ud835\udc61min and \ud835\udc61max are the predefined near and far bounds of the\nrendering range. The opacity of a pixel \ud835\udc5d is set as \ud835\udc3c\ud835\udefc (\ud835\udc5d) = \ud835\udc47 (\ud835\udc5d,\ud835\udc61max).\n4.2\nRelightable Appearance\nIn this section, we detail our specially designed appearance decoder\nD\ud835\udc5f\ud835\udc54\ud835\udc4f that enables high-fidelity real-time relighting using environ-\nment maps. Although the appearance changes drastically when\nlighting condition changes, previous methods [Basri and Jacobs\n2003; Xu et al. 2018] have shown that the relighted images often lie\nin low-dimensional subspaces. For example, nearly all the lighting\neffects are linear [Chandrasekhar 2013; Debevec et al. 2000] and\nthe full reflectance field can be predicted from a few images of the\nobject in specific lighting conditions [Meka et al. 2019; Xu et al.\n2018]. However, directly predicting all OLAT images and adding\nthem up for environment map relighting is not feasible for real-time\nrendering. Our key observation is that we can design a network\narchitecture upon the disentangled representation for our appear-\nance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f to strictly satisfy the linear nature of lighting,\ni.e.:\nD\ud835\udc5f\ud835\udc54\ud835\udc4f (\ud835\udc67\ud835\udc52,\ud835\udc581\ud835\udc591 + \ud835\udc582\ud835\udc592, d) = \ud835\udc581D\ud835\udc5f\ud835\udc54\ud835\udc4f (\ud835\udc67\ud835\udc52,\ud835\udc591, d)\n+ \ud835\udc582D\ud835\udc5f\ud835\udc54\ud835\udc4f (\ud835\udc67\ud835\udc52,\ud835\udc592, d), \u2200\ud835\udc581 and \ud835\udc582 \u2208 R.\n(3)\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nH. Yang, M. Zheng, W. Feng, H. Huang, Y.-K. Lai, P. Wan, Z. Wang, and C. Ma\nWe show the architecture of D\ud835\udc5f\ud835\udc54\ud835\udc4f in Figure 3. Considering the\nspatially structured effect for each light, we use a convolutional ar-\nchitecture for D\ud835\udc5f\ud835\udc54\ud835\udc4f. The expression code \ud835\udc67\ud835\udc52 and the view direction\nd are fed into an ordinary non-linear branch. The lighting condi-\ntion \ud835\udc59 is injected in a separate linear branch, where the activation\nlayers and the bias in the fully connected layer and transposed\nconvolutional layers are removed. The feature maps of the linear\nbranch Flin is point-wise multiplied with the feature maps from\nthe non-linear branch Fnlin at each stage:\nF \ud835\udc56+1\nlin\n= \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc47 \u0000F \ud835\udc56\nlin \u2299 (F \ud835\udc56\nnlin + 1)\u0001,\n(4)\nwhere \ud835\udc56 is the index of the stage, \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc47 represents the transposed\nconvolution operation, and \u2299 is point-wise multiplication. The plus\none term acts as a residual connection that stabilizes training (this\nterm is omitted in Figure 3 to avoid clutter). In this way, the appear-\nance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f is strictly linear to the lighting condition \ud835\udc59 while\nbeing non-linear to the expression code \ud835\udc67\ud835\udc52 and the view direction\nd that does not limit the representation power. We empirically find\nthat our architecture significantly improves the generalization abil-\nity for novel lighting conditions (see Section 5.3 for some related\nevaluation results).\n4.3\nData Acquisition\nCapturing each transient facial expression under a variety of light-\ning conditions for relightable appearance poses a significant chal-\nlenge. Instead, for each subject, we record image sequences of dy-\nnamic expressions with different lighting conditions in each frame\nand rely on our self-supervised training framework for disentan-\nglement by using information across frames. Following [Bi et al.\n2021; Li et al. 2023], we use group light patterns for capture, i.e., for\neach frame seven randomly selected adjacent lights are turned to\nthe maximum. Differently, since we do not use interleaved full-on\nframes for tracking, we find a large part of the face is dark in group\nlight conditions that makes the implicit tracking in our network\nunstable. To provide basic illumination, we set all lights not in-\ncluded in the selected group to a known low brightness. Thanks to\nthe linear nature of light and our network architecture design, the\nfully disentangled relightable appearance can be learned from such\ncoalescent lighting conditions.\nDuring the capture process, a subject is asked to perform 41 pre-\ndefined expressions and read out two paragraphs. Then a freestyle\nperformance is captured to cover extreme and complex expres-\nsion combinations. We capture 10200 frames for each subject at\n20fps. We show a snapshot of our captured images in Figure 2. The\nbackground without the subject is also captured.\n4.4\nNetwork Training\nOur model is trained end-to-end on the multi-view image sequences\nunder varying illuminations. The training loss L\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 consists of\ntwo parts: L\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = L\ud835\udc56\ud835\udc5a\ud835\udc54 + L\ud835\udc5f\ud835\udc52\ud835\udc54, where L\ud835\udc56\ud835\udc5a\ud835\udc54 is the data term and\nL\ud835\udc5f\ud835\udc52\ud835\udc54 is the regularization term.\nThe data term L\ud835\udc56\ud835\udc5a\ud835\udc54 contains three components and measures\nthe similarity between the captured input and the rendered output:\nL\ud835\udc56\ud835\udc5a\ud835\udc54 = L1 + \ud835\udf06VGGLVGG + \ud835\udf06GANLGAN,\n(5)\nwhere L1 is the MAE loss, LVGG is the perceptual loss, and LGAN is\nthe adversarial loss that improves the visual quality. \ud835\udf06VGG and \ud835\udf06GAN\nare the balancing weights. We clip the pixel values of the rendered\nimages \ud835\udc3c\ud835\udc5f\ud835\udc54\ud835\udc4f before calculating loss to simulate the truncation of the\nimaging process.\nThe regularization loss L\ud835\udc5f\ud835\udc52\ud835\udc54 comprises four components:\nL\ud835\udc5f\ud835\udc52\ud835\udc54 = \ud835\udf06LapLLap + \ud835\udf06\ud835\udc5d\ud835\udc45L\ud835\udc5d\ud835\udc45 + \ud835\udf06\ud835\udc63\ud835\udc5c\ud835\udc59 L\ud835\udc63\ud835\udc5c\ud835\udc59 + \ud835\udf06KLDLKLD,\n(6)\nwhere LLap = ||L(v \u2212 v\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52)||2 is the expression-aware Lapla-\ncian loss to encourage a smooth base mesh. L is the sparse Lapla-\ncian matrix. v\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 = B(B\ud835\udc47 B)\u22121B\ud835\udc47 v is calculated in a least-squares\nmanner based on the 51 predefined expression blendshapes B \u2208\nR51\u00d73\ud835\udc41\ud835\udc5a\ud835\udc52\ud835\udc60\u210e from the FaceScape dataset [Yang et al. 2020]. L\ud835\udc5d\ud835\udc45 =\n1\n\ud835\udc41\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5a ||(D\ud835\udc47 )\ud835\udc45,\ud835\udc61 || regularizes the predicted rotation and transla-\ntion (D\ud835\udc47 )\ud835\udc45,\ud835\udc61 to be small. We apply a predefined mask on the base\nmesh to assign higher weights of LLap and L\ud835\udc5d\ud835\udc45 on facial regions\ncompared to non-facial parts. L\ud835\udc63\ud835\udc5c\ud835\udc59 and LKLD are the volume mini-\nmization prior and KL-divergence loss as in [Lombardi et al. 2021],\nrespectively. \ud835\udf06Lap, \ud835\udf06\ud835\udc5d\ud835\udc45, \ud835\udf06\ud835\udc63\ud835\udc5c\ud835\udc59, and \ud835\udf06KLD are balancing weights.\nSince our training images are captured under varying illumi-\nnations, the background changes across frames. To prevent the\nencoding of background flashes into the avatar, the final image \u02c6\ud835\udc3c\nin training is generated by blending the rendered foreground \ud835\udc3c\ud835\udc5f\ud835\udc54\ud835\udc4f\nwith the captured background \ud835\udc3c\ud835\udc35\ud835\udc3a based on the computed opacity\nvalue \ud835\udc3c\ud835\udefc:\n\u02c6\ud835\udc3c = \ud835\udc3c\ud835\udefc\ud835\udc3c\ud835\udc5f\ud835\udc54\ud835\udc4f + (1 \u2212 \ud835\udc3c\ud835\udefc)\ud835\udc3c\ud835\udc35\ud835\udc3a.\n(7)\nWe use the Adam optimizer [Kingma and Ba 2015] to train the\nnetwork with a learning rate of 10\u22124. We choose frontal, left, and\nright views as input of the encoder. The input images are normal-\nized and converted to grayscale to prevent the light from being\nencoded in the expression code \ud835\udc67\ud835\udc52. We use the per-camera color cal-\nibration similar to [Lombardi et al. 2021]. For monochrome cameras,\nthe rendered images are explicitly converted to grayscale before\ncalculating loss functions. We fit a base mesh on the first frame for\ninitialization.\nThe network training for each subject takes about two days on a\nsingle NVIDIA V100 graphics card. The decoding and rendering take\naround 22ms for a frame of a resolution 1280 \u00d7 960, enabling real-\ntime relighting and animation. Please refer to our supplementary\nmaterials for implementation details such as network architectures\nand hyperparameters.\n5\nEXPERIMENTS\n5.1\nQualitative Evaluation Results\nMesh-volume representation. Figure 4 shows two examples of our\navatars based on the hybrid mesh-volume representation. Although\nour avatars are trained without explicit tracking, the base mesh and\nthe volumetric primitives are roughly aligned. The inherently con-\nsistent structures enable explicit control and can be naturally used\nfor applications such as video-driven animations and relighting.\nDisentanglement of illumination and motion. Both illumination\nand motion are varied in our captured sequences. To evaluate the\ndisentanglement of illumination and motion in our model, for each\ninput frame, we keep the extracted expression code \ud835\udc67\ud835\udc52 fixed and\nchange the lighting condition \ud835\udc59 extracted from environment maps\nto generate the relighting results. We use the appearance decoder\nto predict the relighted appearance of RGB channels separately for\nTowards Practical Capture of High-Fidelity Relightable Avatars\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nGround-truth\nMVP\nMVP+\nOurs\nFigure 5: Comparison to MVP [Lombardi et al. 2021] on novel\nview synthesis. Our results are comparable to MVP and MVP+\n(an improved version of MVP trained by ourselves) even with-\nout explicit tracking of the base mesh.\nTable 1: Quantitative evaluation results of novel view synthe-\nsis in comparison with MVP [Lombardi et al. 2021]. The two\nsubjects are from the Multiface Dataset [Wuu et al. 2022].\nSubject #002421669\nSubject #5067077\nMethod\nMAE \u2193\nSSIM \u2191\nLPIPS \u2193\nMAE \u2193\nSSIM \u2191\nLPIPS \u2193\nMVP\n2.08\n0.910\n0.273\n2.21\n0.923\n0.232\nMVP+\n1.76\n0.930\n0.193\n2.11\n0.928\n0.211\nOurs\n1.73\n0.932\n0.186\n2.01\n0.934\n0.208\ncolorful environment map relighting. As shown in Figure 8, the\nlighting conditions are fully disentangled from the motion and are\nconsistent across different subjects.\n5.2\nComparisons to Prior Work\nComparison to MVP. Since existing explicit surface tracking meth-\nods [Beeler et al. 2011; Wu et al. 2018] do not generalize well under\nvarying lighting conditions, we compare to MVP [Lombardi et al.\n2021] on the publicly available Multiface Dataset [Wuu et al. 2022],\nwhich consists of high quality multi-view recordings of 13 differ-\nent identities under fixed illumination. We perform qualitative and\nquantitative evaluations on eight held out views of two subjects.\nThe vanilla MVP uses an L2 loss during training, which leads to\nblurry results. We train an improved version, namely MVP+, us-\ning the similar data term as ours for fair comparison. The other\ncomponents remain identical to the vanilla MVP.\nThe visual comparison on Subject #002421669 from the dataset is\nshown in Figure 5. The Mean Absolute Error (MAE), Structural Sim-\nilarity Index (SSIM), and Learned Perceptual Image Patch Similarity\n(LPIPS) measurements are reported in Table 1. Both our method\nand MVP+ generate clearer details compared to vanilla MVP. Even\nwithout a computationally intensive tracking process, the quan-\ntitative reconstruction error of our method is slightly lower than\nthat of MVP+. We attribute the improvement to the avoidance of\ninformation loss in the explicit surface tracking process.\nInput\nDPR\nOurs\nFigure 6: Comparison to DPR [Zhou et al. 2019] on single-\nview portrait relighting. The input illumination is shown as\ninset in each relighting result.\nTable 2: Quantitative evaluation results of ablation study. In\neach column, the best number is highlighted in bold. Some\ncorresponding visual results are shown in Figure 9.\nSubject A\nSubject B\nMethod\nMAE \u2193\nSSIM \u2191\nLPIPS \u2193\nMAE \u2193\nSSIM \u2191\nLPIPS \u2193\nNL\n10.47\n0.665\n0.417\n13.87\n0.601\n0.440\nNL + ENV\n7.10\n0.677\n0.418\n9.78\n0.604\n0.445\nNL + LCL\n9.74\n0.661\n0.428\n12.19\n0.601\n0.449\nNL + TS\n8.03\n0.672\n0.401\n9.77\n0.597\n0.423\nOurs\n6.32\n0.707\n0.334\n7.99\n0.635\n0.356\nComparison to single-view portrait relighting methods. We com-\npare our method to Deep Portrait Relighting (DPR) [Zhou et al.\n2019] to evaluate the relighting results. The illumination is repre-\nsented as the first three bands of Spherical Harmonics (SH) in DPR.\nWe use their default SH coefficients and calculate the corresponding\npoint light brightness for our model. We use a portrait in uniform\nillumination as the input of DPR.\nAs shown in Figure 6, DPR fails to predict correct relighting ef-\nfects such as specularities and shadows consistent with the identity-\nspecific geometry and skin material. As a result, the identity is\nshifted after relighting. In contrast, our method achieves more\nfaithful portrait relighting results.\n5.3\nAblation Study\nWe perform ablation studies to evaluate the effectiveness of our\nphysically-inspired appearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f. Specifically, we com-\npare our method to four alternative design options:\n(1) NL: We remove the linear lighting branch of D\ud835\udc5f\ud835\udc54\ud835\udc4f and di-\nrectly feed the concatenated lighting condition \ud835\udc59 and other\nlatent codes to an ordinary non-linear network with the\nsame layers as for appearance prediction.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nH. Yang, M. Zheng, W. Feng, H. Huang, Y.-K. Lai, P. Wan, Z. Wang, and C. Ma\n(2) NL + ENV: We use the same network architecture as in (1) but\nuse the Light Stage to simulate environment maps [Debevec\net al. 2002] instead of group lights for training.\n(3) NL + LCL: We adopt the same network architecture as in (1)\nand add a lighting consistency loss inspired by the recent\nsingle image portrait relighting method [Yeh et al. 2022] to\nenforce the linearity of lighting.\n(4) NL + TS: We adopt the same network architecture as in (1)\nand use a two-stage training framework [Bi et al. 2021] for\nrelighting. Specifically, we initially train an appearance de-\ncoder D\ud835\udc5f\ud835\udc54\ud835\udc4f for OLAT relighting, and subsequently use the\ntrained network to synthesize data for training the environ-\nment map relighting appearance decoder.\nWe capture 600 frames for each subject under various preset\nlighting conditions in a Light Stage as ground truth for quantitative\nevaluation. Quantitative results are summarized in Table 2 and\nqualitative comparisons are shown in Figure 9. Note that not all\nthe lighting conditions can be simulated in a Light Stage due to\nhardware limitations such as the maximum brightness of a lighting\nunit. The results demonstrate that our linear lighting branch of\nD\ud835\udc5f\ud835\udc54\ud835\udc4f significantly enhances the generalization performance for\nrelighting.\n5.4\nVideo-Driven Animation\nOur volumetric avatar can be animated by replacing the motion\nencoder E\ud835\udc5a with an application-specific module predicting the\nlow-dimensional expression code \ud835\udc67\ud835\udc52. Existing methods perform\ndomain adaptation on synthetic datasets [Lombardi et al. 2018] or\nuse triplet supervision [Zhang et al. 2022] to train the expression\ncode predictor. In our implementation, we simply use an off-the-\nshelf expression regressor similar to [Weise et al. 2011] to predict\nthe identity-independent blendshape weights of each frame from\nthe frontal view in our captured data. Then we train a three-layer\nMLP to predict the expression code \ud835\udc67\ud835\udc52 from the blendshape weights.\nOur volumetric avatar can be animated by the extracted blendshape\nweights from monocular videos.\nWe find that the rigid head rotation and translation are success-\nfully disentangled from the expression code even without explicit\nconstraint. Thanks to the consistent structures of the base mesh\nand volumetric primitives, we can explicitly constrain the motion\nbeyond face, achieving plausible animation results. Figure 7 shows\nsome performance-driven animation results. Please refer to our\naccompanying video for the corresponding animations results.\n6\nCONCLUSION AND FUTURE WORK\nIn this work, we propose a novel framework, named TRAvatar,\nfor capturing and reconstructing high-fidelity and relightable 3D\navatars in a practical and efficient setting. We train the framework\nwith dynamic image sequences captured in a Light Stage under\nvarying lighting conditions, enabling natural relighting and video-\ndriven animation.\nOur contributions are two-fold. First, we present a novel network\narchitecture that satisfies the linear nature of lighting, allowing\nfor real-time appearance prediction and high-quality relighting\neffects. Second, we propose to jointly optimize facial geometry\nand relightable appearance based on image sequences, with the\nInput\nSubject B\nSubject C\nFigure 7: Video-driven animation results. Our method can\nfaithfully generate identity-specific dynamic wrinkle details\nfor different expressions.\ndeformation of the base mesh implicitly learned. Our tracking-\nfree scheme provides robustness for establishing temporal corre-\nspondences between frames under different lighting conditions.\nBoth qualitative and quantitative experiments demonstrate that\nour framework achieves superior performance in photorealistic\navatar animation and relighting, facilitating further advancements\nin content creation of 3D avatars.\nDespite our promising results, there are some limitations to be ad-\ndressed in future work. First, the data capturing apparatus employed\nin our framework is expensive, which may limit its applicability and\nadoption. Second, due to the lack of sufficient surface constraints,\nit becomes challenging to perform precise manual control on the\nlearned implicit representation. Future work could explore methods\nto create relightable avatars with more affordable equipment and\ninvestigate representations that offer more flexible control. Finally,\nwe are interested in extending our method to handle near-field and\nhigh-frequency relighting [Bi et al. 2021; Sun et al. 2020] as well as\naccessories such as glasses [Li et al. 2023].\nACKNOWLEDGMENTS\nWe would like to thank Qianfang Zou and Xuesong Niu for being\nour capture subjects, the authors of [Lombardi et al. 2021] and [Zhou\net al. 2019] for releasing their source code, as well as anonymous\nreviewers for their valuable feedback.\nTowards Practical Capture of High-Fidelity Relightable Avatars\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nSubject D\nLighting 1\nLighting 2\nSubject E\nLighting 1\nLighting 2\nFigure 8: Evaluation results of lighting and motion disentanglement. For both subjects, we show the input frames of two\ndifferent expressions on the left and the corresponding relighting results in the middle and on the right. The two input\nenvironment maps for relighting are shown on the top. The relighting effects are consistent with the dynamic expressions.\nNL\nNL + ENV\nNL + LCL\nNL + TS\nOurs\nGround truth\nFigure 9: Ablation study results on Subjects A (top) and B (bottom) about our physically inspired linear light branch for the\nappearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f. From left to right: relighting results of four alternative baselines (see detailed explanations in\nSection 5.3), our results, and the ground truth. Note that here we use simulated environment map light which is similar to the\nlighting conditions that NL + ENV is trained on. Therefore, the results of NL + ENV are comparable to ours in this figure but\ndowngrades significantly when using real HDR environments for testing (see more results in our supplementary materials).\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nH. Yang, M. Zheng, W. Feng, H. Huang, Y.-K. Lai, P. Wan, Z. Wang, and C. Ma\nImage A\nConv(3, 64, 4, 2)\nLeakyReLU\nConv(64, 128, 4, 2)\nLeakyReLU\nConv(128, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(3, 64, 4, 2)\nLeakyReLU\nConv(64, 128, 4, 2)\nLeakyReLU\nConv(128, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(3, 64, 4, 2)\nLeakyReLU\nConv(64, 128, 4, 2)\nLeakyReLU\nConv(128, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nConv(256, 256, 4, 2)\nLeakyReLU\nImage B\nImage C\nc\nFC(512, 128)\nLeakyReLU\nFC(128, 6)\nFC(3840, 512)\nLeakyReLU\nFC(512, 256)\nFC(512, 256)\n\ud835\udf07\n\ud835\udf0e2\n\ud835\udc45\u210e\ud835\udc52\ud835\udc4e\ud835\udc51, \ud835\udc61\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\n\ud835\udc67\ud835\udc52\nFC(256, 4096)\nConvT(256, 256, 4, 2)\nLeakyReLU\nConvT(256, 128, 4, 2)\nLeakyReLU\nConvT(128, 128, 4, 2)\nLeakyReLU\nConvT(128, 64, 4, 2)\nLeakyReLU\nConvT(64, 64 , 2)\nLeakyReLU\nConvT(64, 32, 4, 2)\nLeakyReLU\nConvT(32, 32, 4, 2)\nLeakyReLU\nConvT(32, 8, 4, 2)\n\ud835\udc49\ud835\udefc\n\ud835\udc67\ud835\udc52, \ud835\udc1d\nFC(259, 4096)\n\ud835\udc59\nFC(356, 4096)\nLeakyReLU\nLeakyReLU\nConvT(256, 256, 4, 2)\nLeakyReLU\nConvT(256, 256, 4, 2)\n.\nConvT(256, 128, 4, 2)\nLeakyReLU\nConvT(256, 128, 4, 2)\n.\nConvT(128, 128, 4, 2)\nLeakyReLU\n.\nConvT(128, 128, 4, 2)\nConvT(128, 64, 4, 2)\nLeakyReLU\n.\nConvT(128, 64, 4, 2)\nConvT(64, 64, 4, 2)\nLeakyReLU\n.\nConvT(64, 64, 4, 2)\nConvT(64, 32, 4, 2)\nLeakyReLU\n.\nConvT(64, 32, 4, 2)\nConvT(32, 32, 4, 2)\nLeakyReLU\n.\nConvT(32, 32, 4, 2)\n.\nConvT(32, 24, 4, 2)\n\ud835\udc49\ud835\udc5f\ud835\udc54\ud835\udc4f\n\u2130\ud835\udc5a\n\ud835\udc9f\ud835\udefc\n\ud835\udc9f\ud835\udc5f\ud835\udc54\ud835\udc4f\n+1\n+1\n+1\n+1\n+1\n+1\n+1\n+1\nReLU\nFigure A1: Detailed architecture of our motion encoder E\ud835\udc5a, opacity decoder D\ud835\udefc, and appearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f. The convolutional\nlayer is represented as \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc63(\ud835\udc56\ud835\udc5b\ud835\udc50\u210e\ud835\udc60,\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc50\u210e\ud835\udc60,\ud835\udc58\ud835\udc52\ud835\udc5f\ud835\udc5b\ud835\udc52\ud835\udc59\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc60\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc51\ud835\udc52), where \ud835\udc56\ud835\udc5b\ud835\udc50\u210e\ud835\udc60 is the number of input channels and \ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc50\u210e\ud835\udc60 is the number\nof output channels. The representation of transposed convolutional layers are similar. The fully connected layer is represented\nas \ud835\udc39\ud835\udc36(\ud835\udc56\ud835\udc5b\ud835\udc50\u210e\ud835\udc60,\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc50\u210e\ud835\udc60). The architecture of the transformation decoder D\ud835\udc47 is similar to the opacity decoder, except that there is no\nactivation layer at the end. The mesh decoder D\ud835\udc5a\ud835\udc52\ud835\udc60\u210e is a three-layer MLP with LeakyReLU activation layers, which is omitted\nin this figure.\nTable A1: Values of our hyperparameters.\nParameter\nValue\nParameter\nValue\nParameter\nValue\n\ud835\udc41\ud835\udc5a\ud835\udc52\ud835\udc60\u210e\n7306\n\ud835\udc41\ud835\udc5d\ud835\udc5f\ud835\udc56\ud835\udc5a\n16384\n\ud835\udc40\n8\n\ud835\udf06VGG\n0.1\n\ud835\udf06GAN\n0.005\n\ud835\udf06Lap\n0.01\n\ud835\udf06\ud835\udc5d\ud835\udc45\n10\n\ud835\udf06\ud835\udc63\ud835\udc5c\ud835\udc59\n0.01\n\ud835\udf06KLD\n0.001\nA\nIMPLEMENTATION DETAILS\nNetwork architectures and hyperparameters. We provide detailed\narchitectures of our neural networks in Figure A1. The values of\nhyperparameters in our implementation are provided in Table A1,\nwhich are identical in all our experiments.\nEnvironment map relighting. Given a high dynamic range en-\nvironment map in the longitude-latitude format, we extract the\nlighting condition \ud835\udc59 for our appearance decoder. Specifically, we\nproject the position of each light unit of the Light Stage onto the\nenvironment map image and split the space using a Voronoi di-\nagram [Aurenhammer 1991]. The corresponding value of \ud835\udc59 is set\naccording to the weighted-average pixel values in the cell. We show\nthe projected light positions in Figure A2.\nB\nADDITIONAL RESULTS\nFigure A3 shows additional ablation study results using two envi-\nronment maps, which demonstrate that the linear lighting branch\nof our appearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f can significantly enhance the gen-\neralization performance for relighting.\nFigure A2: The projected light positions on an environment\nmap. Green circles: positions of 356 lighting units projected\non an environment map in the longitude-latitude format.\nBlue lines: edges of the Voronoi diagram.\nREFERENCES\nFranz Aurenhammer. 1991. Voronoi diagrams\u2014a survey of a fundamental geometric\ndata structure. ACM Computing Surveys (CSUR) 23, 3 (1991), 345\u2013405.\nTimur Bagautdinov, Chenglei Wu, Jason Saragih, Pascal Fua, and Yaser Sheikh. 2018.\nModeling facial geometry using compositional vaes. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 3877\u20133886.\nRonen Basri and David W Jacobs. 2003. Lambertian reflectance and linear subspaces.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 25, 2 (2003), 218\u2013233.\nThabo Beeler, Bernd Bickel, Paul Beardsley, Bob Sumner, and Markus Gross. 2010.\nHigh-quality single-shot capture of facial geometry. In ACM SIGGRAPH 2010 papers.\n1\u20139.\nThabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel, Paul Beardsley, Craig Gots-\nman, Robert W Sumner, and Markus Gross. 2011. High-quality passive facial\nperformance capture using anchor frames. In ACM SIGGRAPH 2011 papers. 1\u201310.\nTowards Practical Capture of High-Fidelity Relightable Avatars\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nLight\nNL\nNL + ENV\nNL + LCL\nNL + TS\nOurs\nFigure A3: Ablation study results about our physically inspired linear lighting branch for the appearance decoder D\ud835\udc5f\ud835\udc54\ud835\udc4f. The\ninput environment maps are shown on the left. The relighting results of four alternative baseline approaches (see detailed\nexplanations in Section 5.3) and ours are shown on the right.\nSai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail,\nRavi Ramamoorthi, Yaser Sheikh, and Jason Saragih. 2021. Deep relightable ap-\npearance models for animatable faces. ACM Transactions on Graphics (TOG) 40, 4\n(2021), 1\u201315.\nVolker Blanz and Thomas Vetter. 1999. A morphable model for the synthesis of 3D faces.\nIn Proceedings of the 26th annual conference on Computer graphics and interactive\ntechniques. 187\u2013194.\nDerek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla Sheffer. 2010. High resolution\npassive facial performance capture. In ACM SIGGRAPH 2010 papers. 1\u201310.\nChen Cao, Vasu Agrawal, Fernando De La Torre, Lele Chen, Jason Saragih, Tomas\nSimon, and Yaser Sheikh. 2021. Real-time 3D neural facial animation from binocular\nvideo. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1\u201317.\nChen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke\nSaito, Stephen Lombardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, et al. 2022.\nAuthentic volumetric avatars from a phone scan. ACM Transactions on Graphics\n(TOG) 41, 4 (2022), 1\u201319.\nChen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. 2013. Faceware-\nhouse: A 3d facial expression database for visual computing. IEEE Transactions on\nVisualization and Computer Graphics 20, 3 (2013), 413\u2013425.\nSubrahmanyan Chandrasekhar. 2013. Radiative transfer. Courier Corporation.\nAlvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese,\nHugues Hoppe, Adam Kirk, and Steve Sullivan. 2015. High-quality streamable\nfree-viewpoint video. ACM Transactions on Graphics (TOG) 34, 4 (2015), 1\u201313.\nPaul Debevec. 2012. The light stages and their applications to photoreal digital actors.\nTechnical Report. UNIVERSITY OF SOUTHERN CALIFORNIA LOS ANGELES.\nPaul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and\nMark Sagar. 2000. Acquiring the reflectance field of a human face. In Proceedings of\nthe 27th annual conference on Computer graphics and interactive techniques. 145\u2013156.\nPaul Debevec, Andreas Wenger, Chris Tchou, Andrew Gardner, Jamie Waese, and Tim\nHawkins. 2002. A lighting reproduction approach to live-action compositing. ACM\nTransactions on Graphics (TOG) 21, 3 (2002), 547\u2013556.\nMingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh Khamis, Adarsh Kowdle,\nChristoph Rhemann, Vladimir Tankovich, and Shahram Izadi. 2017a. Motion2fusion:\nReal-time volumetric performance capture. ACM Transactions on Graphics (TOG)\n36, 6 (2017), 1\u201316.\nPengfei Dou, Shishir K Shah, and Ioannis A Kakadiaris. 2017b. End-to-end 3D face\nreconstruction with deep neural networks. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 5908\u20135917.\nMartin Fuchs, Volker Blanz, Hendrik PA Lensch, and Hans-Peter Seidel. 2007. Adaptive\nsampling of reflectance fields. ACM Transactions on Graphics (TOG) 26, 2 (2007),\n10\u2013es.\nGraham Fyffe and Paul Debevec. 2015. Single-shot reflectance measurement from\npolarized color gradient illumination. In 2015 IEEE International Conference on\nComputational Photography (ICCP). IEEE, 1\u201310.\nXuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang.\n2022. Reconstructing personalized semantic facial nerf models from monocular\nvideo. ACM Transactions on Graphics (TOG) 41, 6 (2022), 1\u201312.\nAbhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay Busch, Xueming Yu, and\nPaul Debevec. 2011. Multiview face capture using polarized spherical gradient\nillumination. ACM Transactions on Graphics (TOG) 30, 6 (2011), 1\u201310.\nKaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen,\nGeoff Harvey, Sergio Orts-Escolano, Rohit Pandey, Jason Dourgarian, et al. 2019. The\nrelightables: Volumetric performance capture of humans with realistic relighting.\nACM Transactions on Graphics (TOG) 38, 6 (2019), 1\u201319.\nHaoda Huang, Jinxiang Chai, Xin Tong, and Hsiang-Tao Wu. 2011. Leveraging motion\ncapture and 3D scanning for high-fidelity facial performance acquisition. In ACM\nSIGGRAPH 2011 papers. 1\u201310.\nZi-Hang Jiang, Qianyi Wu, Keyu Chen, and Juyong Zhang. 2019. Disentangled rep-\nresentation learning for 3D face shape. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 11957\u201311966.\nTero Karras and Timo Aila. 2013. Fast parallel construction of high-quality bounding\nvolume hierarchies. In Proceedings of the 5th High-Performance Graphics Conference.\n89\u201399.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization.\nIn International Conference on Learning Representations, ICLR.\nDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114 (2013).\nJunxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lombardi, Hongdong Li, and Jason\nSaragih. 2023. MEGANE: Morphable Eyeglass and Avatar Network. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition. 12769\u201312779.\nTianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier Romero. 2017. Learning a\nmodel of facial shape and expression from 4D scans. ACM Transactions on Graphics\n(TOG) 36, 6 (2017), 194:1\u2013194:17.\nStephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. 2018. Deep appear-\nance models for face rendering. ACM Transactions on Graphics (TOG) 37, 4 (2018),\n1\u201313.\nStephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh,\nand Jason Saragih. 2021. Mixture of volumetric primitives for efficient neural\nrendering. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1\u201313.\nWan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-Felix Chabert, Malte Weiss, Paul E\nDebevec, et al. 2007. Rapid Acquisition of Specular and Diffuse Normal Maps from\nPolarized Spherical Gradient Illumination. Rendering Techniques 2007, 9 (2007), 10.\nAbhimitra Meka, Christian Haene, Rohit Pandey, Michael Zollh\u00f6fer, Sean Fanello,\nGraham Fyffe, Adarsh Kowdle, Xueming Yu, Jay Busch, Jason Dourgarian, et al.\n2019. Deep reflectance fields: high-quality facial reflectance field inference from\ncolor gradient illumination. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1\u201312.\nAbhimitra Meka, Rohit Pandey, Christian Haene, Sergio Orts-Escolano, Peter Barnum,\nPhilip David-Son, Daniel Erickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz,\net al. 2020. Deep relightable textures: volumetric performance capture with neural\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nH. Yang, M. Zheng, W. Feng, H. Huang, Y.-K. Lai, P. Wan, Z. Wang, and C. Ma\nrendering. ACM Transactions on Graphics (TOG) 39, 6 (2020), 1\u201321.\nLucio Moser, Chinyu Chien, Mark Williams, Jose Serra, Darren Hendler, and Doug\nRoble. 2021. Semi-supervised video-driven facial animation transfer for production.\nACM Transactions on Graphics (TOG) 40, 6 (2021), 1\u201318.\nPieter Peers, Dhruv K Mahajan, Bruce Lamond, Abhijeet Ghosh, Wojciech Matusik,\nRavi Ramamoorthi, and Paul Debevec. 2009. Compressive light transport sensing.\nACM Transactions on Graphics (TOG) 28, 1 (2009), 1\u201318.\nDikpal Reddy, Ravi Ramamoorthi, and Brian Curless. 2012. Frequency-space decompo-\nsition and acquisition of light transport under spatially varying illumination. In\nComputer Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence,\nItaly, October 7-13, 2012, Proceedings, Part VI 12. Springer, 596\u2013610.\nEdoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon,\nShih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, et al. 2022. Driv-\nable volumetric avatars using texel-aligned features. In ACM SIGGRAPH 2022 Con-\nference Proceedings. 1\u20139.\nJ\u00e9r\u00e9my Riviere, Paulo Gotardo, Derek Bradley, Abhijeet Ghosh, and Thabo Beeler.\n2020. Single-shot high-quality facial geometry and skin appearance capture. ACM\nTransactions on Graphics (TOG) 39, 4 (2020), 1\u201312.\nChristophe Schlick. 1994. An inexpensive BRDF model for physically-based rendering.\nIn Computer graphics forum. 233\u2013246.\nGabriel Schwartz, Shih-En Wei, Te-Li Wang, Stephen Lombardi, Tomas Simon, Jason\nSaragih, and Yaser Sheikh. 2020. The eyes have it: An integrated eye and face\nmodel for photorealistic facial animation. ACM Transactions on Graphics (TOG) 39,\n4 (2020), 91:1\u201391:15.\nTiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello, Christoph Rhemann, Paul\nDebevec, Yun-Ta Tsai, Jonathan T Barron, and Ravi Ramamoorthi. 2020. Light\nstage super-resolution: continuous high-frequency relighting. ACM Transactions\non Graphics (TOG) 39, 6 (2020), 1\u201312.\nJustus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias\nNie\u00dfner. 2016. Face2face: Real-time face capture and reenactment of RGB videos.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n2387\u20132395.\nLuan Tran and Xiaoming Liu. 2018. Nonlinear 3D face morphable model. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition. 7346\u20137355.\nLuan Tran and Xiaoming Liu. 2019. On learning 3D face morphable model from\nin-the-wild images. IEEE Transactions on Pattern Analysis and Machine Intelligence\n43, 1 (2019), 157\u2013171.\nDaniel Vlasic, Matthew Brand, Hanspeter Pfister, and Jovan Popovi\u0107. 2005. Face\nTransfer with Multilinear Models. ACM Transactions on Graphics (TOG) 24, 3 (2005),\n426\u2013433.\nJiaping Wang, Yue Dong, Xin Tong, Zhouchen Lin, and Baining Guo. 2009. Kernel\nNystr\u00f6m method for light transport. In ACM SIGGRAPH 2009 papers. 1\u201310.\nThibaut Weise, Sofien Bouaziz, Hao Li, and Mark Pauly. 2011. Realtime performance-\nbased facial animation. ACM Transactions on Graphics (TOG) 30, 4 (2011), 1\u201310.\nTim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien\nTu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, et al. 2006.\nAnalysis of human faces using a measurement-based skin reflectance model. ACM\nTransactions on Graphics (TOG) 25, 3 (2006), 1013\u20131024.\nChenglei Wu, Derek Bradley, Markus Gross, and Thabo Beeler. 2016. An anatomically-\nconstrained local deformation model for monocular face capture. ACM Transactions\non Graphics (TOG) 35, 4 (2016), 1\u201312.\nChenglei Wu, Takaaki Shiratori, and Yaser Sheikh. 2018. Deep incremental learning\nfor efficient high-fidelity face tracking. ACM Transactions on Graphics (TOG) 37, 6\n(2018), 234:1\u2013234:12.\nCheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan Bali, Danielle Belko, Eric\nBrockmeyer, Lucas Evans, Timothy Godisart, Hyowon Ha, Alexander Hypes, Tay-\nlor Koska, Steven Krenn, Stephen Lombardi, Xiaomin Luo, Kevyn McPhail, Laura\nMillerschoen, Michal Perdoch, Mark Pitts, Alexander Richard, Jason Saragih, Junko\nSaragih, Takaaki Shiratori, Tomas Simon, Matt Stewart, Autumn Trimble, Xin-\nshuo Weng, David Whitewolf, Chenglei Wu, Shoou-I Yu, and Yaser Sheikh. 2022.\nMultiface: A Dataset for Neural Face Rendering. arXiv preprint arXiv:2207.11243\n(2022).\nZexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. 2018. Deep image-\nbased relighting from optimal sparse samples. ACM Transactions on Graphics (TOG)\n37, 4 (2018), 1\u201313.\nHaotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and\nXun Cao. 2020. Facescape: a large-scale high quality 3D face dataset and detailed\nriggable 3d face prediction. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. 601\u2013610.\nYu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, and Ting-Chun\nWang. 2022. Learning to Relight Portrait Images via a Virtual Light Stage and\nSynthetic-to-Real Adaptation. ACM Transactions on Graphics (TOG) 41, 6 (2022),\n1\u201321.\nJae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, and Hyun Soo Park. 2019. Self-supervised\nadaptation of high-fidelity face models for monocular performance tracking. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n4601\u20134609.\nLongwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang Lin, Ruixiang Cao, Wei\nYang, Lan Xu, and Jingyi Yu. 2022. Video-driven neural physically-based facial\nasset for production. ACM Transactions on Graphics (TOG) 41, 6 (2022), 1\u201316.\nMingwu Zheng, Zhang Haiyu, Hongyu Yang, and Di Huang. 2023. NeuFace: Realistic\n3D Neural Face Rendering from Multi-view Images. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 16868\u201316877.\nMingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen. 2022. ImFace: A Nonlinear\n3D Morphable Face Model with Implicit Neural Representations. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition. 20343\u201320352.\nHao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Jacobs. 2019. Deep single-\nimage portrait relighting. In Proceedings of the IEEE International Conference on\nComputer Vision. 7194\u20137202.\nXiangyu Zhu, Xiaoming Liu, Zhen Lei, and Stan Z Li. 2017. Face alignment in full\npose range: A 3d total solution. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 41, 1 (2017), 78\u201392.\nWojciech Zielonka, Timo Bolkart, and Justus Thies. 2023. Instant volumetric head\navatars. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. 4574\u20134584.\n"
  },
  {
    "title": "DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs",
    "link": "https://arxiv.org/pdf/2309.03907.pdf",
    "upvote": "6",
    "text": "1\u201314\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on\nDrug Molecule Graphs\nYouwei Liang\u2217\nRuiyi Zhang\u2217\nLi Zhang\nPengtao Xie\nUniversity of California San Diego, *Equal contribution.\nAbstract\nA ChatGPT-like system for drug compound analysis could be a game-changer in pharma-\nceutical research, accelerating drug discovery, enhancing our understanding of structure-\nactivity relationships, guiding lead optimization, aiding drug repurposing, reducing the\nfailure rate, and streamlining clinical trials.\nIn this work, we make an initial attempt\ntowards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a pro-\ntotype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a\ncompound molecule graph and ask various questions about this compound.\nDrugChat\nwill answer these questions in a multi-turn, interactive manner.\nThe DrugChat sys-\ntem consists of a graph neural network (GNN), a large language model (LLM), and an\nadaptor. The GNN takes a compound molecule graph as input and learns a representa-\ntion for this graph. The adaptor transforms the graph representation produced by the\nGNN into another representation that is acceptable to the LLM. The LLM takes the\ncompound representation transformed by the adaptor and users\u2019 questions about this\ncompound as inputs and generates answers.\nAll these components are trained end-to-\nend. To train DrugChat, we collected instruction tuning datasets which contain 10,834\ndrug compounds and 143,517 question-answer pairs. The code and data is available at\nhttps://github.com/UCSD-AI4H/drugchat\n1. Introduction\nThe process of drug discovery and development is a time-intensive and costly endeavor, often\ntaking years and billions of dollars to bring a single drug to market (Avorn, 2015). This\nprocess involves the exploration and understanding of vast chemical spaces and the intricate\nrelationships between molecular structures and their biological activities, commonly known\nas structure-activity relationships (SAR) (Idakwo et al., 2020). Traditional methods (Rycker\net al., 2018) often involve laborious iterative testing, with a high rate of late-stage failures.\nRecent advancements in computational chemistry and chemoinformatics (Zeng et al., 2022)\nhave o\ufb00ered some respite, but there is still a pressing need for tools that can intuitively\nunderstand and generate meaningful insights from the complex data inherent in molecular\ngraphs of drug compounds.\n\u00a9 Y. Liang\u2217, R. Zhang\u2217, L. Zhang & P. Xie.\narXiv:2309.03907v1  [q-bio.BM]  18 May 2023\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nThis technical report introduces the concept of applying ChatGPT-like capabilities to\ndrug molecule graphs, aiming to revolutionize the way we interact with and understand\nthese complex entities. By transforming these molecular graphs into a form amenable to\nAI analysis, we can enable dynamic exploration of chemical spaces, e\ufb03cient prediction\nof compound properties, and intelligent suggestions for drug design and optimization. A\nChatGPT-like AI system capable of understanding drug compound molecule graphs and\nanswering various questions about these drugs could revolutionize pharmaceutical research\nin several ways:\n\u2022 Speeding up Drug Discovery. A ChatGPT-like AI system could drastically cut\ndown the time required for initial stages of drug discovery by providing immediate in-\nsights into a compound\u2019s potential therapeutic uses, side e\ufb00ects, and contraindications\nbased on its structure.\n\u2022 Predicting Drug Interactions. A ChatGPT-like AI system could predict potential\ninteractions between new drug candidates and existing drugs.\nBy comparing the\nmolecular structures of thousands of known substances, the system could identify\npossible con\ufb02icts or synergies, helping researchers to better anticipate how a new\ndrug might behave in the real world.\n\u2022 Understanding Structure-Activity Relationships (SAR). SAR (Idakwo et al.,\n2020) is a crucial aspect of drug design.\nA ChatGPT-like AI system could help\nresearchers understand the relationship between a drug\u2019s chemical structure and its\nbiological activity.\nIt could also help predict what modi\ufb01cations to the chemical\nstructure might enhance its e\ufb00ectiveness or reduce unwanted side e\ufb00ects.\n\u2022 Guiding Lead Optimization. During the drug discovery process, \u2018lead\u2019 compounds\n(those that show promise in initial screenings) (Hughes et al., 2011) are typically\noptimized for better e\ufb03cacy, reduced toxicity, and improved pharmacokinetics. A\nChatGPT-like AI system could o\ufb00er suggestions for structural modi\ufb01cations to en-\nhance these parameters, guiding researchers in the right direction and saving valuable\ntime.\n\u2022 Supporting Drug Repurposing.\nA ChatGPT-like AI system could also aid in\ndrug repurposing (Pushpakom et al., 2019) e\ufb00orts. By understanding the structural\nproperties of existing drugs, it could identify candidates that may be e\ufb00ective against\ndiseases they were not initially developed to treat. This could help breathe new life\ninto existing drugs and o\ufb00er more rapid routes to treatment for challenging diseases.\n\u2022 Reducing the Failure Rate. The failure rate in drug discovery is high (Hughes\net al., 2011), often due to unforeseen toxicity or e\ufb03cacy issues that emerge late in\ndevelopment. By providing more accurate predictions about a drug\u2019s properties and\ne\ufb00ects at the outset, a ChatGPT-like AI system could help reduce these costly late-\nstage failures.\n\u2022 Streamlining Clinical Trials. A ChatGPT-like AI could help design more e\ufb00ective\nclinical trials by predicting how a drug will interact with other medications or condi-\n2\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\ntions, enabling researchers to target their trials more e\ufb00ectively and recruit suitable\npatient cohorts.\nDeveloping a ChatGPT-like system for drug molecule graphs is highly challenging. First,\none of the primary challenges lies in the representation of molecular graphs. Unlike text\ndata, which is sequential and has a well-de\ufb01ned structure, molecular graphs are inherently\nnon-sequential and highly complex, with no clear start or end point. Developing a methodol-\nogy to translate these graphs into a format that a GPT model can process is crucial. Second,\nanother signi\ufb01cant challenge is capturing and understanding the intricate structure-activity\nrelationships (SAR) in drug compounds. These relationships are often not straightforward\nand can involve subtle interactions between di\ufb00erent parts of a molecule. Third, to train\nsuch a system, large datasets of molecular structures, along with their associated biological\nactivities, side e\ufb00ects, and other properties, would be required. The generation or compi-\nlation of such datasets is a non-trivial task, given the proprietary nature of much of this\ninformation and the sheer diversity of the chemical space.\nIn this technical report, we make an initial attempt towards enabling ChatGPT-like ca-\npabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat\nworks in a similar way as ChatGPT. Users upload a compound molecule graph and ask var-\nious questions about this compound. DrugChat will answer these questions in a multi-turn,\ninteractive manner.\nThe DrugChat system consists of a graph neural network (GNN) (Hu et al., 2020), a\nlarge language model (LLM) (Chiang et al., 2023), and an adaptor. The GNN takes a\ncompound molecule graph as input and learns a representation for this graph. The adaptor\ntransforms the graph representation produced by the GNN into another representation\nthat is acceptable to the LLM. The LLM takes the compound representation transformed\nby the adaptor and users\u2019 questions about this compound as inputs and generates answers.\nAll these components are trained end-to-end. To train DrugChat, we collected instruction\ntuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs.\nThe major contributions of this work are as follows:\n\u2022 We develop DrugChat, a prototype system aiming to achieve ChatGPT-like capabili-\nties on drug molecule graphs. DrugChat allows users to interactively ask open-ended\nquestions about drug compounds and provides informative answers.\n\u2022 We collect instruction tuning datasets which contain 10,834 drug compounds and\n143,517 question-answer pairs. These datasets enable the training of ChatGPT-like\nmodels for drug compounds and are publicly available.\n\u2022 To our best knowledge, DrugChat represents the \ufb01rst system that bridges graph\ndata with large language models (LLMs), which enables interactive conversations on\ngraphs. Our system seamlessly integrates graph neural networks with LLMs and can\nbe easily extended to analyze other graph data beyond compound molecule graphs.\n2. Related Works\nAI-based drug properties analysis (Paul et al., 2021) is a promising approach to signi\ufb01-\ncantly reduce costs and time associated with the traditional drug discovery and development\n3\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\npipeline. For example, ImageMol (Zeng et al., 2022) is an unsupervised pretraining method\nthat processes images of chemical structures and outputs molecular properties, such as drug\nmetabolism, brain penetration, toxicity, and molecular target pro\ufb01les like beta-secretase en-\nzyme and kinases. ImageMol was pretrained on 10 million unlabeled drug-like, bioactive\nmolecules through predicting molecular targets of candidate compounds. It was assessed on\n51 benchmark datasets and demonstrated high accuracy in identifying anti-SARS-CoV-2\nmolecules across 13 high-throughput experimental datasets.\nLarge language models (LLMs) (Brown et al., 2020) have demonstrated outstanding ca-\npabilities in generating inventive text, responding to reading comprehension queries, math-\nematical reasoning, etc. Nevertheless, the weight parameters of some of the most powerful\nLLMs are not publicly available, which considerably hinders academic research. Moreover,\nearly LLMs were limited to processing text information as input, restricting their ability to\nunderstand information in other modalities. Consequently, researchers have developed an\narray of publicly accessible LLMs, including multi-modal variants, to address these chal-\nlenges.\nLLaMA (Touvron et al., 2023), an LLM with 65 billion parameters, was developed by\nMeta AI 1. This model is trained on publicly accessible datasets including English Com-\nmonCrawl, C4, Github, Wikipedia, Gutenberg Project, ArXiv, and Stack Exchange. This\ntraining dataset encompasses 20 languages with the most speakers. Although it is smaller\nthan GPT-3, LLaMA demonstrates superior performance on many benchmarks, includ-\ning commonsense reasoning, question answering, reading comprehension, code generation,\netc. Vicuna (Chiang et al., 2023) is an open-source chatbot trained by \ufb01ne-tuning LLaMA\non around 70,000 user-shared conversations with ChatGPT 2. Vicuna was evaluated using\nGPT-4. MiniGPT-4 (Zhu et al., 2023) is a vision-language model capable of processing im-\nages and generating textual answers 3. For instance, it can create stories and poems from\ninput images and o\ufb00er solutions to problems depicted in images. MiniGPT-4 incorporates\na linear projection layer to align the visual information from a pretrained vision encoder\nwith a large language model. LLaVA (Liu et al., 2023) is another multi-modal model that\nintegrates a vision encoder with an LLM to facilitate general-purpose visual and language\nunderstanding 4. Di\ufb00erent from MiniGPT-4, LLaVA is trained on multi-turn conversation\ndata. The projection layer and the LLM are both updated in the training stage while only\nthe visual encoder parameters are frozen. The training data was created by converting raw\nimage-text pairs into a suitable instruction-following format using ChatGPT/GPT-4.\n3. Drug Instruction Tuning Data\nTo train DrugChat, we curate instruction tuning data for drug compounds, from two sources\n- ChEMBL and PubChem. Table 1 summarizes the statistics of the curated two datasets.\n1. https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\n2. https://lmsys.org/blog/2023-03-30-vicuna/\n3. https://minigpt-4.github.io/\n4. https://llava-vl.github.io/\n4\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nTable 1: Dataset statistics.\nDataset\nNumber of Drugs\nNumber of Question-Answer Pairs\nChEMBL\n3,892\n129,699\nPubChem\n6,942\n13,818\n3.1. ChEMBL Drug Instruction Tuning Dataset\nIn this section, we outline the process of constructing the ChEMBL Drug Instruction Tuning\ndataset. We gathered data from the ChEMBL website5, which houses information for a total\nof 2,354,965 chemical compounds. We downloaded the sqlite version of the data dump,\nwhich was last updated on February 28, 20236. Out of the entire dataset, we identi\ufb01ed\n14,816 compounds that contain drug information. After applying further \ufb01ltering to exclude\ndrugs with insu\ufb03cient descriptive information, we ended up with a dataset containing 3,892\ndrugs.\nFor each drug, we initially gathered its SMILES string, representing the molecular struc-\nture. Subsequently, we obtained various molecular features, including the complete molec-\nular formula and its classi\ufb01cation as an acid, base, or neutral compound. Additionally,\nwe collected drug-speci\ufb01c properties, such as the mechanism of action and therapeutic ap-\nplications. Based on these attributes, we manually crafted 129,699 question-answer pairs.\nQuestion-answer (QA) pairs for an exemplar drug in the ChEMBL Drug Instruction Tuning\nDataset is shown in Table 2.\nTable 2. Question-answer (QA) pairs for an exemplar drug in the ChEMBL Drug\nInstruction Tuning Dataset.\nSMILES of the drug\nCC(=O)/N=c1/sc(S(N)(=O)=O)nn1C\nQA Pair 1\nQ: Please provide a description of this drug\u2019s mechanism of action.\nA: Carbonic anhydrase II inhibitor.\nQA Pair 2\nQ: Is this compound an acid, a base, or neutral?\nA: Neutral.\nQA Pair 3\nQ: How many rotatable bonds does this compound have?\nA: 1.\nQA Pair 4\nQ: Determine if this drug is administered as a racemic mixture, a single\nstereoisomer, an achiral molecule, or has an unknown chirality.\nA: An achiral molecule.\nQA Pair 5\nQ: Does this compound satisfy the rule-of-three criteria?\nA: No.\nQA Pair 6\nQ: How many violations of Lipinski\u2019s Rule of Five are there for this\ncompound, using the HBA LIPINSKI and HBD LIPINSKI counts?\nA: 0.\n5. https://www.ebi.ac.uk/chembl/\n6. https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/\n5\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nQA Pair 7\nQ: Is it known whether this drug is administered parenterally?\nA: No.\nQA Pair 8\nQ: Is this compound a small molecule polymer,\nsuch as polystyrene sulfonate?\nA: No.\nQA Pair 9\nQ: What is the calculated ALogP value for this compound?\nA: -1.42.\nQA Pair 10\nQ: Is this molecule characterized by a small molecular\nstructure or a protein sequence?\nA: It has a small molecule structure.\nQA Pair 11\nQ: Determine if this molecule is inorganic, meaning it contains only metal\natoms and fewer than two carbon atoms.\nA: No.\nQA Pair 12\nQ: What is the highest development stage achieved for this compound\nacross all indications? Please respond with Approved, Phase 3 Clinical\nTrials, Phase 2 Clinical Trials, Phase 1 Clinical Trials,\nEarly Phase 1 Clinical Trials, or Clinical Phase Unknown.\nA: Approved.\nQA Pair 13\nQ: What is the \ufb01rst recorded year of approval for this drug?\nA: 1959.\nQA Pair 14\nQ: Is there a black box warning associated with this drug?\nA: No.\nQA Pair 15\nQ: Is it known if this drug is the \ufb01rst approved in its class, regardless of\nthe indication or route of administration, acting on a speci\ufb01c target?\nA: No.\nQA Pair 16\nQ: How many hydrogen bond acceptors does this compound have?\nA: 6.\nQA Pair 17\nQ: Is this drug used for therapeutic purposes, rather than for imaging,\nadditives, or other non-therapeutic applications?\nA: Yes.\nQA Pair 18\nQ: What is the polar surface area (PSA) value of this compound?\nA: 107.41.\nQA Pair 19\nQ: Has this approved drug been withdrawn due to toxicity\nreasons for all indications, populations, and\ndoses in at least one country (not necessarily the US)?\nA: No.\nQA Pair 20\nQ: How many violations of Lipinski\u2019s Rule of Five (using HBA and\nHBD de\ufb01nitions) are there for this compound?\nA: 0.\nQA Pair 21\nQ: How many heavy (non-hydrogen) atoms does this compound have?\nA: 14.\nQA Pair 22\nQ: How many aromatic rings does this compound have?\nA: 1.\n6\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nQA Pair 23\nQ: Determine the type of availability for this drug.\nA: Prescription only.\nQA Pair 24\nQ: Is it known whether this drug is taken orally?\nA: Yes.\nQA Pair 25\nQ: How many hydrogen bond acceptors are there in this compound,\ncalculated according to Lipinski\u2019s original rules\n(i.e., counting N and O atoms)?\nA: 7.\nQA Pair 26\nQ: Is the drug administered\nin this speci\ufb01c form, such as a particular salt?\nA: Yes.\nQA Pair 27\nQ: Determine if this compound is a prodrug.\nA: No.\nQA Pair 28\nQ: What is the molecular weight of this compound\u2019s parent molecule?\nA: 236.28.\nQA Pair 29\nQ: How many hydrogen bond donors are there in this compound,\ncalculated according to Lipinski\u2019s original\nrules (i.e., counting NH and OH groups)?\nA: 2.\nQA Pair 30\nQ: Which USAN substem can\nthis drug or clinical candidate name be matched with?\nA: Zolamide.\nQA Pair 31\nQ: What is the de\ufb01nition of this compound\u2019s USAN stem?\nA: Carbonic anhydrase inhibitors.\nQA Pair 32\nQ: What is the molecular formula\nof this compound, including any salt that it may have?\nA: C5H8N4O3S2.\nQA Pair 33\nQ: How many hydrogen bond donors does this compound have?\nA: 1.\nQA Pair 34\nQ: What is the classi\ufb01cation of this molecule? Please respond with\nSmall Molecule, Protein, Antibody, Oligosaccharide, Oligonucleotide,\nCell, Enzyme, Gene, or Unknown.\nA: Small molecule.\n3.2. PubChem Drug Instruction Tuning Dataset\nIn this section, we outline the process of constructing the PubChem Drug Instruction Tuning\nDataset. The data was collected from the PubChem website7, which houses information\nof 66,469,244 chemical compounds. We utilized the data version last updated on May 9,\n20238. Among these compounds, 19,319 possess drug information. After \ufb01ltering out drugs\nlacking detailed textual descriptions, we retained 6,942 drugs.\n7. https://pubchem.ncbi.nlm.nih.gov/\n8. https://pubchem.ncbi.nlm.nih.gov/classification/\n7\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nFor each drug, we gathered its SMILES string and descriptions from databases including\nChEBI, LOTUS, and YMDB. In total, we acquired 13,818 compound description summaries\nand curated 13,818 question-answer (QA) pairs. QA pairs for an exemplar drug in PubChem\nare provided in Table 3.\nTable 3: Question-answer (QA) pairs for an exemplar drug in the PubChem Drug Instruc-\ntion Tuning Dataset.\nCompound Name\nAcetylcarnitine\nCanonical SMILES\nCC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\nChEBI Database\nQ: Please describe this drug.\nA: O-acetylcarnitine is an O-acylcarnitine\nhaving acetyl as the acyl substituent.\nIt has a role as a human metabolite.\nIt is functionally related to an acetic acid.\nIt is a conjugate base of an O-acetylcarnitinium.\nLOTUS Database\nQ: Please describe this drug.\nA: Acetylcarnitine is a natural\nproduct found in Pseudo-nitzschia multistriata,\nEuglena gracilis, and other organisms with data available.\nYMDB Database\nQ: Please describe this drug.\nA: L-Acetylcarnitine is a metabolite\nfound in or produced by Saccharomyces cerevisiae.\n4. Method\nAn overview of DrugChat is provided in Figure 1. It takes a compound molecule graph\nas input and allows users to ask multi-turn questions about this compound.\nFor each\nquestion, DrugChat generates an answer. DrugChat consists of a graph neural network\n(GNN), a large language model (LLM), and an adaptor between GNN and LLM. The GNN\nlearns a representation for the compound molecule graph. The adaptor (which is a linear\ntransformation matrix) transforms the graph representation into an LLM-compatible soft\nprompt vector. The LLM takes a user-question and the graph prompt vector as inputs\nand generates an answer.\nWe employ a pretrained GNN from (Hu et al., 2020) and a\npretrained LLM - Vicuna13b (Chiang et al., 2023). When training DrugChat, we \ufb01x the\nweight parameters of the GNN and LLM, and only update the adaptor\u2019s weights. Given\na drug in the instruction tuning data and a question about this drug, the drug\u2019s graph is\n\ufb01rst fed into the GNN to produce a representation vector which is then fed into the adaptor\nto produce a prompt vector. The prompt vector and the question are fed into the LLM\nto generate an answer. A negative log likelihood loss between the generated answer and\ngroundtruth answer is calculated. The adaptor is trained by minimizing this loss. Next, we\nintroduce each component in DrugChat.\n8\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nFigure 1: An overview of the DrugChat framework.\n4.1. Graph Neural Networks (GNNs)\nA GNN (Hu et al., 2020) utilizes the graph structure and node features to learn multiple\nlayers of representation vectors for each node v and a representation vector hG for the\nentire graph G. The representation of each node is updated by aggregating features from\nits neighboring nodes. At layer k, the representation hk\nv of node v encompasses information\nfrom nodes within the k-hop network neighborhood of v.\nFormally, hk\nv is calculated as\nfollows:\nhk\nv = \u03c3(hk\u22121\nv\n, AGG({hk\u22121\nu\n, u \u2208 N(v)})),\n(1)\nwhere AGG represents a function that aggregates information from neighboring nodes.\n\u03c3 is a function that combines information from the node feature of the previous layer\nand neighboring nodes. N(v) indicates all neighboring nodes for node v. To acquire the\nrepresentation vector for the entire graph G, a pooling function f (which is permutation-\ninvariant, such as averaging) extracts information from all node features at the \ufb01nal layer\nK:\nhG = f({hK\nv , v \u2208 G}).\n(2)\n4.2. Large Language Models (LLMs)\nLLMs utilize the Transformer (Vaswani et al., 2017) decoder to model the conditional\nprobability p\u03b8(ni|n<i) for token ni in a language model. The Transformer decoder applies a\nmulti-head self-attention block (Vaswani et al., 2017) to the input context tokens and uses a\nposition-wise feedforward network to calculate the probabilities over output tokens. Given\nthe context vector of tokens, the token generation probability is computed as follows:\nh0 = Ni\u22121We + Wp\nhl = transformer block(hl\u22121)(1 \u2264 l \u2264 m)\np(ni|Ni\u22121) = Softmax(hmW T\ne )\n(3)\nwhere Ni\u22121 = (n1, n2, ..., ni\u22121) denotes the context vector of tokens, m refers to the num-\nber of layers, We denotes the token embedding matrix, and Wp represents the positional\nembedding matrix.\n9\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nFigure 2: A demonstration of the DrugChat framework.\n10\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nFigure 3: A demonstration of the DrugChat framework.\n11\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\n4.3. Aligned Graph-Text Generation\nAt this stage, we create a prompt for every training graph-text pair, which allows the LLM\nto generate descriptions from drug compound graphs. We utilize a prompt template that\nadheres to the conversational format of Vicuna-13b:\nQ: < Graph >< GraphFeature >< /Graph >< Instruction >\nA: < Desc >\n(4)\nIn this prompt, < GraphFeature > is a soft prompt that symbolizes the graph structure\nfeature encoded by the linear projection layer.\n< Instruction > serves as a directive\nsentence, prompting the LLM to generate descriptions for the drug, such as \u201cDescribe the\nmechanism of this drug\u201d. During the training stage, < Desc > is populated with descriptive\ntext from human experts to train the linear projection layer. In the testing stage, < Desc >\nremains empty, and the model is expected to generate descriptive text for the provided drug\nstructure.\n5. Results\nWe tested DrugChat on compound graphs that are not contained in the training data. Fig-\nure 2 and 3 show two examples. DrugChat can answer diverse multi-turn questions about\ncompounds, such as \u201cwhat makes this compound unique?\u201d, \u201cwhat diseases can this com-\npound potentially treat?\u201d. Note that these questions are not contained in the training data.\nWe will perform a systematic quantitative evaluation by collaborating with pharmaceutical\nscientists.\n6. Conclusions, Limitations and Future Work\nIn this report, we present the DrugChat framework, designed to answer questions and\ngenerate text descriptions for drugs from their molecular graphs. We develop the ChEMBL\nDrug Dataset and the PubChem Drug Dataset to train the DrugChat model. With further\ndevelopment and evaluation, DrugChat has the potential to enable conversational analysis\nof drug compounds.\nA potential limitation of DrugChat is language hallucination. Since DrugChat incor-\nporates an LLM module, it may occasionally produce untrustworthy answers and descrip-\ntions for drugs, which hampers its practical application in real drug discovery pipelines. If\nDrugChat generates seemingly convincing yet incorrect text descriptions, it could mislead\nhuman decision-makers and potentially result in adverse consequences.\nIn the future, this issue could be mitigated by utilizing higher-quality training data\nand implementing e\ufb00ective \ufb01ltering strategies. Additionally, more advanced GNN encoders\nand LLMs will play a crucial role in addressing this challenge. As the number of users\nincreases, human feedback can also be utilized to \ufb01ne-tune the DrugChat model through\nreinforcement learning techniques (Ouyang et al., 2022).\n12\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nReferences\nJerry Avorn. The $2.6 billion pill\u2013methodologic and policy considerations. N Engl J Med.,\n2015.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Je\ufb00rey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,\neditors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901.\nCurran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/\npaper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\nMarch 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and\nJure Leskovec. Strategies for pre-training graph neural networks. In International Con-\nference on Learning Representations, 2020. URL https://openreview.net/forum?id=\nHJlWWJSFDH.\nJP. Hughes, S. Rees, SB. Kalindjian, and KL. Philpott. Principles of early drug discovery.\nBr J Pharmacol., 2011.\nG. Idakwo, S. Thangapandian, Luttrell, and J. et al. Structure\u2013activity relationship-based\nchemical classi\ufb01cation of highly imbalanced tox21 datasets. J Cheminform, 2020.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\narXiv:2304.08485, 2023.\nLong Ouyang, Je\ufb00rey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,\nPeter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe.\nTraining lan-\nguage models to follow instructions with human feedback.\nIn S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 27730\u201327744. Curran Associates,\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nD. Paul, G. Sanap, S. Shenoy, D. Kalyane, K. Kalia, and RK. Tekade. Arti\ufb01cial intelligence\nin drug discovery and development. Drug Discov Today., 2021.\n13\nDrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs\nS. Pushpakom, F. Iorio, and P. et al. Eyers. Drug repurposing: progress, challenges and\nrecommendations. Nat Rev Drug Discov, 2019.\nDe Rycker, B. M., Baraga\u02dcna, and S.L. et al. Duce. Challenges and recent progress in drug\ndiscovery for tropical diseases. Nature, 2018.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\ne\ufb03cient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nXiangxiang Zeng, Hongxin Xiang, Linhui Yu, Jianmin Wang, Kenli Li, Ruth Nussinov, and\nFeixiong Cheng. Accurate prediction of molecular properties and drug targets using a\nself-supervised image representation learning framework. Nature Machine Intelligence,\n2022.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4:\nEnhancing vision-language understanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023.\n14\n"
  }
]