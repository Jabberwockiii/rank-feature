[
  {
    "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
    "link": "https://arxiv.org/pdf/2309.09400.pdf",
    "upvote": "74",
    "text": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\nLanguage Models in 167 Languages\nThuat Nguyen1, Chien Van Nguyen1, Viet Dac Lai1, Hieu Man1, Nghia Trung Ngo1\nFranck Dernoncourt2, Ryan A. Rossi2, Thien Huu Nguyen1\n1Dept. of Computer Science, University of Oregon, OR, USA\n2Adobe Research, USA\nnguyenhuuthuat09@gmail.com\n{chienn,vietl@cs,hieum,nghian,thien@cs}@uoregon.edu\n{franck.dernoncourt,ryrossi}@adobe.com\nAbstract\nThe driving factors behind the development\nof large language models (LLMs) with im-\npressive learning capabilities are their colos-\nsal model sizes and extensive training datasets.\nAlong with the progress in natural language\nprocessing, LLMs have been frequently made\naccessible to the public to foster deeper inves-\ntigation and applications. However, when it\ncomes to training datasets for these LLMs, es-\npecially the recent state-of-the-art models, they\nare often not fully disclosed. Creating training\ndata for high-performing LLMs involves exten-\nsive cleaning and deduplication to ensure the\nnecessary level of quality. The lack of trans-\nparency for training data has thus hampered\nresearch on attributing and addressing hallu-\ncination and bias issues in LLMs, hindering\nreplication efforts and further advancements\nin the community. These challenges become\neven more pronounced in multilingual learn-\ning scenarios, where the available multilingual\ntext datasets are often inadequately collected\nand cleaned. Consequently, there is a lack of\nopen-source and readily usable dataset to effec-\ntively train LLMs in multiple languages. To\novercome this issue, we present CulturaX, a\nsubstantial multilingual dataset with 6.3 tril-\nlion tokens in 167 languages, tailored for LLM\ndevelopment. Our dataset undergoes meticu-\nlous cleaning and deduplication through a rig-\norous pipeline of multiple stages to accom-\nplish the best quality for model training, in-\ncluding language identification, URL-based\nfiltering, metric-based cleaning, document re-\nfinement, and data deduplication. CulturaX is\nfully released to the public in HuggingFace to\nfacilitate research and advancements in mul-\ntilingual LLMs: https://huggingface.co/\ndatasets/uonlp/CulturaX.\n1\nIntroduction\nLarge language models (LLMs) have fundamen-\ntally transformed research and applications of nat-\nural language processing (NLP), significantly ad-\nvancing the state-of-the-art performance for nu-\nmerous tasks and revealing new emergent abilities\n(Brown et al., 2020; Wei et al., 2022). Based on\nthe transformer architecture (Vaswani et al., 2017),\nthree major variants of LLMs have been explored\nin the literature: the encoder-only models to en-\ncode input texts into representation vectors, e.g.,\nBERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019); the decoder-only models to generate\ntexts, e.g., GPT (Radford et al., 2019; Brown et al.,\n2020); and the encoder-decoder models to per-\nform sequence-to-sequence generation, e.g., BART\n(Lewis et al., 2020) and T5 (Raffel et al., 2020).\nThe remarkable capabilities of LLMs have pri-\nmarily been propelled by the ever-expanding scale\nof model sizes and training datasets, which have\nbeen deemed essential for achieving optimal perfor-\nmance by the scaling laws (Hernandez et al., 2022).\nFor instance, beginning with the BERT model,\nwhich had a mere few hundred million parame-\nters (Devlin et al., 2019), recent GPT-based models\nhave been expanded to encompass hundreds of bil-\nlions of parameters (Shoeybi et al., 2019; Scao\net al., 2022; Lieber et al., 2021; Chowdhery et al.,\n2022). Similarly, the training datasets for LLMs\nhave grown exponentially, evolving from a modest\n13GB of text data from Wikipedia and books used\nfor BERT (Devlin et al., 2019; Liu et al., 2019)\nto consume terabytes of data for the latest mod-\nels, such as Falcon (Penedo et al., 2023), MPT\n(MosaicML, 2023), LLaMa (Touvron et al., 2023),\nPolyLM (Wei et al., 2023) and ChatGPT1.\nAs the field keeps progressing rapidly, pre-\ntrained LLMs have typically been released to the\npublic to foster further research and advancements.\nThese models are obtainable either through com-\nmercial APIs, as illustrated by ChatGPT and GPT-\n4, or via open-source initiatives, exemplified by\nFalcon and LLaMa.\nNevertheless, in contrast\nto the public accessibility of LLMs, the training\n1https://openai.com/blog/chatgpt\narXiv:2309.09400v1  [cs.CL]  17 Sep 2023\ndatasets that underpin the state-of-the-art mod-\nels have mostly remained closely guarded secrets,\neven in the case of open-source LLMs such as\nBLOOM, LLaMa, MPT, and Falcon. For exam-\nple, Falcon (Penedo et al., 2023) and BLOOM\n(Scao et al., 2022) only provide a glimpse of their\ncomplete training data, whereas MPT\u2019s, LLaMa\u2019s\nand PolyLM\u2019s datasets (Touvron et al., 2023; Wei\net al., 2023) remain inaccessible to the public. On\none hand, the lack of transparency has impeded in-\ndepth analysis and comprehension of LLMs, hin-\ndering crucial research into attributing and address-\ning fundamental issues stemming from the training\ndata, such as hallucinations, biases, and toxic con-\ntent (Tamkin et al., 2021; Weidinger et al., 2021;\nKenton et al., 2021; Bommasani et al., 2021). On\nthe other hand, concealing the training data restricts\nthe development of LLMs to a select few stakehold-\ners with ample resources, thereby constraining the\ndemocratization and benefits of the technology and\nexacerbating its biases within broader society.\nTo attain transparency and democratization for\nLLMs, it is thus crucial to create large-scale and\nhigh-quality datasets for training high-performing\nLLMs while ensuring their public accessibility to\nfoster deeper research and advancements. In the\nrealm of LLMs, high-quality training datasets are\noften crafted through the application of extensive\ndata cleaning and deduplication processes, aimed at\neliminating noisy and redundant content from vast\ntext collections (Allamanis, 2018; Penedo et al.,\n2023). To this end, there have been recent efforts\nfrom the community to develop such open-source\ndatasets for LLMs, such as RedPajama with 1.21T\ntokens (Computer, 2023), SlimPajama2 with 627B\ntokens, and AI2 Dolma3 with 3T tokens. How-\never, most of the existing open-source datasets for\nLLMs are tailored for the English language, which\nhinders the utilization and performance of the re-\nsulting LLMs when applied to non-English lan-\nguages, particularly those with limited linguistic\nresources (Bang et al., 2023; Lai et al., 2023). This\nemphasis on English also restricts the capacity of\nopen-source datasets to comprehensively tackle the\nresearch challenges and democratization concerns\nof LLMs across the diverse spectrum of over 7,000\nlanguages spoken worldwide.\n2https://www.cerebras.net/blog/slimpajama-a-6\n27b-token-cleaned-and-deduplicated-version-of-r\nedpajama\n3https://blog.allenai.org/dolma-3-trillion-to\nkens-open-llm-corpus-9a0ff4b8da64\nSimultaneously, some multilingual datasets have\nbeen developed and made available, providing text\ndata for multiple languages. Nevertheless, their\nquality and scale fall short of meeting the require-\nments for training high-performing LLMs. Specif-\nically, the multilingual text dataset sourced from\nWikipedia, while of high quality, is regarded as\nrelatively small when it comes to training LLMs\n(Conneau et al., 2020). The OSCAR datasets (Or-\ntiz Su\u00e1rez et al., 2019; Ortiz Su\u00e1rez et al., 2020;\nAbadji et al., 2021, 2022)4 extract text data from\nCommonCrawl (CC) for more than 160 languages.\nHowever, these datasets lack document-level dedu-\nplication (i.e., removing similar documents in the\ndataset), leading to the inclusion of redundant in-\nformation and impairing the performance of gener-\native LLMs (Lee et al., 2022). Similarly, the mC4\n(Xue et al., 2021), CCAligned (Conneau et al.,\n2020), WikiMatrix (Schwenk et al., 2021), and\nParaCrawl (Ba\u00f1\u00f3n et al., 2020) datasets altogether\nsupport over 100 languages but suffers from less\naccurate language identification, introducing noise\ninto the data (Kreutzer et al., 2022). These datasets\nare also not deduplicated at fuzzy and document\nlevels, e.g., via MinHash (Broder, 1997). Addi-\ntionally, the CC100 dataset (Wenzek et al., 2020;\nConneau et al., 2020), employed in training the\nmultilingual XLM-RoBERTa model across 100 lan-\nguages, only considers the snapshots of CC in 2018,\nconstraining its size and the availability of up-to-\ndate information to train high-performing LLMs.\nTo address the aforementioned issues for open-\nsource datasets, our work introduces a novel multi-\nlingual dataset, called CulturaX, for training LLMs\nin 167 languages. CulturaX merges the latest it-\neration of mC4 (version 3.1.0) with all available\nOSCAR corpora up to the current year, encompass-\ning distributions 20.19, 21.09, 22.01, and 23.01.\nThis amalgamation results in a large multilingual\ndataset, comprising 27 TB of text data with 6.3\ntrillion tokens and offering the most up-to-date\ndata for LLM development. More than half of\nour dataset is dedicated to non-English languages\nto significantly boost the data size and enhance\nthe feasibility of training models in multilingual\nscenarios. Importantly, CulturaX is extensively\ncleaned and deduplicated at the document level\nto produce the highest quality to train LLMs for\nmultiple languages. In particular, our data clean-\ning process includes a comprehensive pipeline de-\n4https://oscar-project.org\nsigned to eliminate low-quality data. This involves\nremoving noisy text, non-linguistic content, toxic\ndata, incorrect language identification, and more.\nOur data cleaning pipeline employs a variant of\nthe Interquartile Range (IQR) method (Dekking\net al., 2007) to select appropriate thresholds for var-\nious dataset metrics (e.g., stopword ratios, data per-\nplexity, and language identification scores), which\ncan be used to filter noisy outliers for the dataset.\nAs such, we leverage the percentiles of the distri-\nbutions computed over large samples of data to\neffectively guide the threshold selection process\nfor each filtering metric and language. Finally,\nwe perform extensive deduplication for the data\nof the languages within our datasets based on the\nnear deduplication method MinHashLSH (Broder,\n1997; Leskovec et al., 2020) and URLs, leading\nto high-quality data to train multilingual LLMs.\nOur dataset will be fully available to the public to\npromote further research and development for mul-\ntilingual learning. To our knowledge, CulturaX is\nthe largest open-source multilingual dataset to date\nthat is deeply cleaned and deduplicated for LLM\nand NLP applications.\n2\nMultilingual Dataset Creation\nTo develop a multilingual public dataset for LLMs,\nour strategy is to combine mC4 (Xue et al., 2021)\nand OSCAR (Ortiz Su\u00e1rez et al., 2019; Abadji et al.,\n2021, 2022), two largest multilingual datasets at\nour disposal. We then process the data with an ex-\ntensive pipeline, involving two major steps of clean-\ning and deduplication, to produce an enormous and\nhigh-quality dataset for multilingual LLMs.\nmC4 is a multilingual document-level dataset,\noriginally created to train the multilingual encoder-\ndecoder model mT5 (Xue et al., 2021) for 101 lan-\nguages. This dataset is extracted from 71 monthly\nsnapshots from CC by removing pages with less\nthan three long lines (line length filter), pages\nwith bad words, and duplicated lines across doc-\numents. Language identification for the pages in\nmC4 is done by the cld3 tool (Botha et al., 2017)5,\nwhich is a small feed-forward network (Xue et al.,\n2021). Any pages with a language confidence be-\nlow 0.95% are excluded. mC4 is deduplicated with\nexact match at the document level; however, fuzzy\ndocument-level deduplication is not performed. We\nutilize the latest version of mC4 (version 3.1.0)6\n5https://github.com/google/cld3\n6https://huggingface.co/datasets/mc4\nmC4\n66%\nOSCAR 20.19\n7%\nOSCAR 21.09\n9%\nOSCAR 22.01\n7%\nOSCAR 23.01\n11%\nFigure 1: Distribution of document counts from mC4\nand OSCAR in our initial dataset.\nprepared by AllenAI in this work.\nA notable aspect of our dataset pertains to the\nweb-based origin of our selected datasets, mC4\nand OSCAR, extracted from CC. This differs\nfrom certain previous work (Radford et al., 2019;\nMosaicML, 2023; Touvron et al., 2023) that has\nalso relied on curated datasets like The Pile (Gao\net al., 2020) and BookCorpus (Zhu et al., 2015) to\ntrain LLMs, presuming their higher overall qual-\nity. However, in the context of multilingual set-\ntings, we argue that web-scraped datasets can be\na more suitable approach, as curated datasets of\nsuperior quality might not be available for various\nlanguages. Our strategy of using web-scraped data\nfacilitates efficient data collection across multiple\nlanguages, contributing to enhanced training data\nscales. Furthermore, recent studies have demon-\nstrated the effectiveness of cleaning web-scraped\ndata to yield state-of-the-art LLMs (Raffel et al.,\n2020; Almazrouei et al., 2023). In total, the com-\nbination of mC4 and OSCAR provides us 13.5B\ndocuments for further processing. Figure 1 illus-\ntrates the distribution of the document counts for\nmC4 and the four available versions of OSCAR in\nour initial dataset.\n2.1\nData Cleaning\nGiven the combination of the mC4 and OSCAR\ndatasets, we first perform a comprehensive data\ncleaning procedure to remove noisy and bad con-\ntent from the data, including language identifica-\ntion, ULR-based filtering, metric-based cleaning,\nand document refinement.\nLanguage Identification: A particular issue\nconcerns the use of two different language iden-\ntification tools, i.e., cld3 and FastText, for mC4\nand OSCAR (respectively). It has been shown in\nprevious studies that cld3 is significantly worse\nthan FastText, causing substantially more language\ndetection errors for mC4 (Kreutzer et al., 2022). In\nfact, compared to several other language detectors,\nFastText has demonstrated state-of-the-art perfor-\nmance over benchmark datasets7. To this end, our\nfirst data cleaning step involves applying FastText\nto re-predict the languages for the documents in\nmC4. Documents whose predicted languages are\ndifferent from the provided ones in mC4 will be\nremoved from the dataset. The rationale is to avoid\ndocuments that are confusing for the language de-\ntectors cld3 and FastText, thus potentially intro-\nducing noise for the data. Finally, to ensure the\nhighest quality, we remove data for any language\nfound in mC4 but not supported by FastText.\nURL-based Filtering: In the next step, we aim\nto eliminate pages from the known toxic and harm-\nful sources to reduce relevant risks from our data.\nIn particular, we leverage the latest UT1 blacklist of\nURLs and domains provided by the University of\nToulouse to support Internet use regulation for ad-\nministrators at schools. This list involves sites from\ndifferent topics, including pornography, grumbling,\nand hacking, that should be discarded for LLM\ntraining. Updated twice to thrice per week, the\nblacklist involves more than 3.7M records that are\ncontributed by both human and robots (e.g., search\nengines, known addresses and indexes) (Abadji\net al., 2022). As such, we remove any page from\nour dataset whose associated URL matches a site\nin the blacklist. This step is helpful for our dataset\nas the blacklist is not employed before for the mC4\ndataset. In addition, although OSCAR has already\nused this blacklist for data cleaning, our approach\nincorporates the most up-to-date information from\nthe list, which might not be available for the current\ndistributions of OSCAR.\nMetric-based Cleaning:\nTo enhance the\ndataset\u2019s quality, motivated by the data process-\ning pipeline from the BigScience\u2019s ROOTS corpus\nfor BLOOM (Lauren\u00e7on et al., 2022; Scao et al.,\n2022), we further utilize the distributions for var-\nious dataset metrics to identify and filter outlying\ndocuments. Each metric provides a singular value\n7https://modelpredict.com/\nlanguage-identification-survey\nfor every document within the dataset, quantify-\ning specific attributes such as number_words, stop-\nword_ratios, and perplexity_score for each doc-\nument. For each metric and its range of possi-\nble values within the dataset, a threshold will be\ndetermined to partition the range into two zones:\na normal range and an abnormal range. The ab-\nnormal range is designated for documents exhibit-\ning metric values significantly deviating from the\nnorm, classifying them as outliers/noises, and con-\nsequently, these outliers are removed from our\ndataset. As such, we employ a comprehensive ar-\nray of dataset metrics, which will be collectively\nemployed to refine our dataset, as outlined below:\n\u2022 Number of words\n\u2022 Character repetition ratio\n\u2022 Word repetition ratio\n\u2022 Special character ratio\n\u2022 Stop word ratio\n\u2022 Flagged word ratio\n\u2022 Language identification confidence\n\u2022 Perplexity score\n\u2022 Document length (number of characters)\n\u2022 Number of lines\n\u2022 Short line length ratio\n\u2022 Short line ratio\nThe last four metrics are suggested by the OS-\nCAR dataset while the others are inherited from the\nBigScience ROOTS corpus\u2019s pipeline to process\nOSCAR data. For the perplexity score, following\nthe BigScience ROOTS corpus, we train a Senten-\ncePiece tokenizer (Kudo, 2018) and 5-gram Kneser-\nNey language models as provided in the KenLM\nlibrary (Heafield, 2011) using the 20230501 dumps\nof Wikipedia. Documents displaying high perplex-\nity scores based on these KenLM models are con-\nsidered notably different from Wikipedia articles.\nThis indicates a level of noise that will be excluded\nfrom our dataset (Wenzek et al., 2020). The tok-\nenizer will also be used to obtain the number of\nwords/tokens in the documents for our metrics. We\npublicly release our KenLM models in Hugging-\nFace8 to faciliate future exploration.\nRepeated information (e.g., words, paragraphs)\ncan appear in the web-curated data due to crawling\nerrors and low-quality sources, causing detrimental\nconsequences for training LLMs (Holtzman et al.,\n2019). The character and word repetition ratios\nare thus designed to avoid documents with exces-\n8https://huggingface.co/uonlp/kenlm\nsively repeated information. High frequencies of\nspecial characters, stop words, or flagged words\ncan indicate noisy and low-quality documents. We\nthus utilize the stop word and flagged word lists for\ndifferent languages to compute their ratios for doc-\nument removal. In addition to the stop word and\nflagged word lists provided by BigScience ROOTS\nfor their 13 languages, we further collect dictionar-\nies for these types of words for other languages.\nWe prioritize the lists that have been shared on\npersonal GitHub accounts for various languages,\nas these are often crafted by native speakers and\nexhibit higher quality. Moreover, lower language\nidentification confidence might also suggest noisy\nlanguage structures for the data. For each document\nin the dataset, we thus obtain a language identifi-\ncation confidence via the probability that FastText\nassigns to its corresponding language to aid data\nfiltering. Finally, for the short line-based criteria,\nwe implement a threshold of 100 characters to clas-\nsify lines as short, as used by OSCAR. Documents\nwith excessive occurrence of short lines will not be\nretained in our dataset.\nThreshold Selection: Given the set of dataset\nmetrics, an important question concerns the selec-\ntion of appropriate thresholds for each metric and\nlanguage to generate high-quality multilingual data.\nIn the BigScience ROOTS project (Lauren\u00e7on et al.,\n2022), this selection process is carried out by native\nspeakers of 13 languages. The resulting thresholds\nare employed for the rest of their 46 languages.\nThe project offers a visualization interface that in-\ndexes a sample of a few thousand documents per\nlanguage, enabling users to monitor data statistics\nas they adjust thresholds for the metrics. However,\nthis process cannot be easily extended to different\nlanguages due to the requirement of experienced\nnative speakers, which incurs significant costs. Fur-\nthermore, the limited sample sizes hinder the repre-\nsentativeness of the chosen thresholds for the full\ndatasets. In our analysis, we observe that some\nselected thresholds for certain languages within\nBigScience ROOTS almost fall outside the value\nranges for the entire dataset, leading to the deacti-\nvation of the corresponding metrics.\nTo address these issues, we leverage a variant\nof the Interquartile Range (IQR) method (Dekking\net al., 2007) to select appropriate thresholds for\nthe filtering metrics for our dataset. For each met-\nric and language, we generate a distribution of its\npossible values across the entire dataset for the lan-\nguage. There is an exception for languages with\nsubstantial amounts of data, such as Spanish and\nRussian, where only 25% of the data is used to cal-\nculate these distributions. Afterward, we compute\nthe Q1-th and Q3-th percentiles of the distribution\n(Q1 < Q3) and use them for the thresholds for\nour filtering metrics. In particular, the lower Q1-\nth percentile will be chosen for the metrics that\nfavor high values (e.g., language identification con-\nfidence), while metrics favoring low values (e.g.,\nperplexity scores and document length) will uti-\nlize the upper Q3-th percentile. We investigate\ndifferent values for (Q1, Q3), considering (25, 75),\n(20, 80), (15, 85), (10, 90), and (5, 95). The selec-\ntion of Q1 = 10 and Q2 = 90 has achieved the\nbest data quality for a sample of languages in our\nexamination.\nIt is worth emphasizing that the utilization of\npercentiles for threshold selection enables our ap-\nproach to efficiently draw upon more extensive data\nsamples for each language compared to those em-\nployed in the BigScience ROOTS project. This re-\nsults in more reliable thresholds for the full datasets\nover different languages. Specifically, concerning\nthe large languages where only a 25% data sample\nis employed to compute the value distribution for a\nmetric, we observe that the proportion of discarded\ndata to the entire dataset closely aligns with that of\nthe data sample when applying the same selected\nfiltering threshold. This underscores the represen-\ntativeness of the thresholds selected through our\nmethodology. Finally, once the thresholds for the\nmetrics in a given language have been determined,\nwe will eliminate any document that surpasses a\nmetric\u2019s threshold and enters the unfavorable range\nof the data.\nDocument Refinement: The previous cleaning\nsteps are done at the dataset level, aiming to remove\nlow-quality documents from the dataset. In this\nstep, we further clean the retained documents to\nimprove the quality. It is important to note that our\nprior metric-based filtering step plays a vital role in\neliminating highly noisy documents, which, in turn,\nstreamlines the process of developing effective doc-\nument cleaning rules during this step. Notably,\nsince the documents from mC4 and OSCAR are\nextracted from HTML pages crawled from the Inter-\nnet, a significant portion of them may carry crawl-\ning and extraction errors, including long JavaScript\nlines and extraneous content. Consequently, filter-\ning out these documents greatly simplifies our task\nof designing rules to clean the documents within\nour dataset.\nAs such, for each document, we eliminate its\nnoisy or irrelevant portions via a series of oper-\nations. First, we remove any short lines located\nat the end of each document, as these lines typi-\ncally contain footer details or unhelpful informa-\ntion from the websites. Second, we eliminate the\nlines that contain words from our list of JavaScript\n(JS) keywords (e.g., \u201c<script\u201d) to avoid irrelevant\nand non-linguistic information. Here, we exclu-\nsively remove JS lines if the document contains\njust one line with JS keywords, and this particular\nline must also feature at least two different types\nof JS keywords. We adopt this approach as docu-\nments with more than two JS lines are likely coding\ntutorials in our data, which should be preserved to\nimprove diversity. In addition, certain JS keywords\nare used in natural language, e.g., \u201cvar\u201d. By re-\nquiring at least two different types of JS keywords,\nwe reduce the risk of inadvertently omitting helpful\ncontent and disrupting the document\u2019s structure.\n2.2\nData Deduplication\nDespite thorough data cleaning, the remaining\ndataset might still contain a substantial amount\nof repeated data due to various reasons, including\ninformation being reposted on the web, multiple\nreferences to the same articles, boilerplate content,\nand plagiarism. The duplicated data can thus cause\nmemorization and significantly hinder generaliza-\ntion for LLMs (Lee et al., 2022; Hernandez et al.,\n2022). Although expensive, data deduplication\nis thus considered as a crucial step to guarantee\nthe highest quality of data for training LLMs. To\nthis end, we undertake a comprehensive deduplica-\ntion procedure for our dataset, utilizing MinHash\n(Broder, 1997) and URLs. This deduplication pro-\ncess is carried out independently for each language.\nFurthermore, we restrict deduplication to languages\nthat retain over 100K documents following our data\ncleaning procedures (i.e., 51.5% of our languages),\naiming to promote smaller languages within our\ndataset.\nMinHash Deduplication: For each language\u2019s\ndataset, we first apply the MinHashLSH method\n(Leskovec et al., 2020) to filter similar documents\nin the dataset. MinHashLSH is a near deduplication\ntechnique based on MinHash (Broder, 1997) with\nmultiple hash functions for n-grams and the Jac-\ncard similarity. Locality-Sensitive Hashing (LSH)\nis incorporated to improve efficiency by focusing\non document pairs that are most likely similar. We\nleverage a variant of the Spark implementation of\nMinHashLSH in the text-dedup repo9, employing\n5-grams and a threshold of 0.8 to determine similar\ndocuments for the Jaccard similarity. Running Min-\nHashLSH for each language\u2019s dataset, especially\nfor languages with the largest data volumes like\nEnglish, Russian, Spanish, and Chinese, represents\nthe most computationally expensive operation in\nour dataset creation effort.\nURL-based Deduplication: Finally, we elimi-\nnate all documents that share identical URLs with\nother documents in the dataset. This step is neces-\nsary to address situations where various versions of\nthe same articles are linked to identical URLs but\nhave been updated or modified during the publica-\ntion process, effectively bypassing the near dedu-\nplication step. Some URLs for the articles in CC\nmight only display their general domains due to\ncrawling errors. To enhance accuracy, we refrain\nfrom removing URLs that only include their gen-\neral domains.\nWe utilize 600 AWS c5.24xlarge EC2 instances\nto preprocess and deduplicate our multilingual\ndataset. Each instance is equipped with 96 CPU\ncores, 192GB of memory, and 1TB of disk space.\nThe disk space can be used to replace memory\nwhen necessary (e.g., for data deduplication).\n3\nData Analysis and Experiments\nAfter completing all the cleaning and deduplication\nsteps, our ultimate dataset comprises 6.3 trillion to-\nkens spanning 167 languages. Table 1 provides an\noverview of the number of documents and tokens\nfor the top 42 languages in CulturaX following\neach processing stage. As can be seen, our data-\ncleaning pipeline can substantially reduce the num-\nber of documents in the original mC4 and OSCAR\ndatasets for each language. The total number of\nremoved documents accounts for 46.48% of our\ninitial documents, suggesting the the effectiveness\nof our approaches to filter noisy information for\nmultilingual datasets.\n4\nRelated Work\nCompared to other NLP tasks, language models\ncan be trained with unlabeled data, enabling effi-\ncient data collection to produce gigantic scales for\n9https://github.com/ChenghaoMou/text-dedup/\ntree/main\nCode\nLanguage\n#Documents (M)\n#Tokens\nInitial\nURL\nMetric\nMinHash\nURL\nFiltering\n(B)\n(%)\nFiltering\nFiltering\nDedup\nDedup\nRate (%)\nen\nEnglish\n5783.24\n5766.08\n3586.85\n3308.30\n3241.07\n43.96\n2846.97\n45.13\nru\nRussian\n1431.35\n1429.05\n922.34\n845.64\n799.31\n44.16\n737.20\n11.69\nes\nSpanish\n844.48\n842.75\n530.01\n479.65\n450.94\n46.60\n373.85\n5.93\nde\nGerman\n863.18\n861.46\n515.83\n447.06\n420.02\n51.34\n357.03\n5.66\nfr\nFrench\n711.64\n709.48\n439.69\n387.37\n363.75\n48.89\n319.33\n5.06\nzh\nChinese\n444.37\n444.03\n258.35\n222.37\n218.62\n50.80\n227.06\n3.60\nit\nItalian\n406.87\n406.04\n254.72\n226.42\n211.31\n48.06\n165.45\n2.62\npt\nPortuguese\n347.47\n346.76\n217.21\n200.11\n190.29\n45.24\n136.94\n2.17\npl\nPolish\n270.12\n269.73\n170.86\n151.71\n142.17\n47.37\n117.27\n1.86\nja\nJapanese\n247.67\n247.19\n137.88\n114.64\n111.19\n55.11\n107.87\n1.71\nvi\nVietnamese\n182.88\n182.72\n118.67\n108.77\n102.41\n44.00\n98.45\n1.56\nnl\nDutch\n238.92\n238.56\n148.19\n125.51\n117.39\n50.87\n80.03\n1.27\nar\nArabic\n132.88\n132.65\n84.84\n77.65\n74.03\n44.29\n69.35\n1.10\ntr\nTurkish\n183.65\n183.47\n109.94\n99.18\n94.21\n48.70\n64.29\n1.02\ncs\nCzech\n136.91\n136.44\n80.38\n69.01\n65.35\n52.27\n56.91\n0.90\nfa\nPersian\n118.55\n118.50\n70.26\n62.42\n59.53\n49.78\n45.95\n0.73\nhu\nHungarian\n88.59\n88.21\n53.29\n46.89\n44.13\n50.19\n43.42\n0.69\nel\nGreek\n100.77\n100.68\n61.43\n54.33\n51.43\n48.96\n43.15\n0.68\nro\nRomanian\n89.37\n89.25\n45.99\n42.8\n40.33\n54.87\n39.65\n0.63\nsv\nSwedish\n103.04\n102.76\n58.67\n52.09\n49.71\n51.76\n38.49\n0.61\nuk\nUkrainian\n81.50\n81.44\n50.95\n47.12\n44.74\n45.10\n38.23\n0.61\nfi\nFinnish\n59.85\n59.80\n36.69\n32.15\n30.47\n49.09\n28.93\n0.46\nko\nKorean\n46.09\n45.85\n25.19\n21.17\n20.56\n55.39\n24.77\n0.39\nda\nDanish\n53.16\n52.99\n28.67\n26.48\n25.43\n52.16\n22.92\n0.36\nbg\nBulgarian\n47.01\n46.90\n28.09\n25.45\n24.13\n48.67\n22.92\n0.36\nno\nNorwegian\n40.07\n40.01\n20.69\n19.49\n18.91\n52.81\n18.43\n0.29\nhi\nHindi\n35.59\n35.50\n22.01\n20.77\n19.67\n44.73\n16.79\n0.27\nsk\nSlovak\n40.13\n39.95\n22.20\n19.56\n18.58\n53.70\n16.44\n0.26\nth\nThai\n49.04\n48.96\n26.20\n21.93\n20.96\n57.26\n15.72\n0.25\nlt\nLithuanian\n27.08\n27.01\n15.87\n14.25\n13.34\n50.74\n14.25\n0.23\nca\nCatalan\n31.13\n31.12\n18.99\n16.46\n15.53\n50.11\n12.53\n0.20\nid\nIndonesian\n48.08\n48.05\n25.79\n23.74\n23.25\n51.64\n12.06\n0.19\nbn\nBangla\n20.90\n20.85\n13.82\n13.22\n12.44\n40.48\n9.57\n0.15\net\nEstonian\n16.20\n16.15\n9.69\n8.45\n8.00\n50.62\n8.81\n0.14\nsl\nSlovenian\n15.46\n15.39\n8.00\n7.60\n7.34\n52.52\n8.01\n0.13\nlv\nLatvian\n14.14\n14.09\n8.37\n7.48\n7.14\n49.50\n7.85\n0.12\nhe\nHebrew\n10.78\n10.77\n5.90\n4.77\n4.65\n56.86\n4.94\n0.08\nsr\nSerbian\n7.80\n7.75\n4.80\n4.25\n4.05\n48.08\n4.62\n0.07\nta\nTamil\n8.77\n8.75\n5.27\n4.94\n4.73\n46.07\n4.38\n0.07\nsq\nAlbanian\n9.40\n9.38\n5.96\n5.04\n5.21\n44.57\n3.65\n0.06\naz\nAzerbaijani\n9.66\n9.65\n5.73\n5.24\n5.08\n47.41\n3.51\n0.06\nTotal (42 languages)\n13397.79\n13366.17\n8254.28\n7471.48\n7181.40\n46.40\n6267.99\n99.37\nTotal (167 languages)\n13506.76\n13474.94\n8308.74\n7521.23\n7228.91\n46.48\n6308.42\n100.00\nTable 1: Data statistics for 42 languages with the percentages of tokens greater than 0.05% in our dataset. Columns\ngrouped with the \u201c#Documents (M)\u201d label indicate the number of documents for each language after the correspond-\ning cleaning and reduplication steps. The token counts are based on our final dataset (i.e., after all the cleaning and\ndeduplication steps).\nthe training data. There are two primary types of\ndata commonly used for training LLMs: curated\ndata and web crawl data. Curated data typically\nconsists of well-written and well-formatted text\nfrom targeted sources and domains, e.g., Wikipedia\narticles, books, newswire articles, and scientific\npapers, as used for the \u201cThe Pile\u201d (Gao et al., 2020)\nand \u201cBookCorpus\u201d (Zhu et al., 2015) datasets. In\ncontrast, web crawl data encompasses text gathered\nfrom a wide array of sources across the internet,\nvarying significantly in terms of format and writing\nstyles, e.g., blogs, social media posts, news arti-\ncles, and advertisements. CommonCrawl (CC) is a\nwidely-used web crawl repository that has collected\npetabytes of data over the Internet for 12 years. To\nthis end, curated data is frequently considered to\npossess higher quality, which has resulted in its\npreference for training early LLMs, e.g., BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al.,\n2019). However, as the demand for larger models\nhas grown, web crawl data has gained more atten-\ntion as it contributes a substantial portion to the\ntraining data of recent LLMs, e.g., RoBERTa (Liu\net al., 2019), BART (Lewis et al., 2020), T5 (Raf-\nfel et al., 2020), GPT-3 (Rae et al., 2021), LLaMa\n(Touvron et al., 2023), MPT (MosaicML, 2023),\nand Falcon (Almazrouei et al., 2023). As such,\ndifferent extractions of CC has been produced to\ntrain such LLMs, including C4 (Raffel et al., 2020),\nCC-News (Nagel), and STORIES (Trinh and Le,\n2018).\nRegarding the accessibility of training data,\ndatasets used to train early LLMs are often made\navailable to the public (Devlin et al., 2019; Raffel\net al., 2020). However, in the case of the most\nrecent state-of-the-art (SOTA) generative LLMs,\ntheir training datasets are not released fully, po-\ntentially due to commercial interests. This applies\nnot only to proprietary models like ChatGPT and\nGPT-4 but also to models that claim to be open-\nsource models such as LLaMa, MPT, Falcon, and\nBLOOM (Scao et al., 2022). To address the trans-\nparency issue with existing LLMs, recent efforts\nhave been made to replicate and release the train-\ning datasets for the state-of-the-art LLMs, i.e., Red-\nPajama (Computer, 2023), SlimPajama, and AI2\nDolma. The key distinctions for these datasets\nconcern their large-scale text data that has been\nmeticulously cleaned and document-level dedupli-\ncated to ensure high quality for training LLMs.\nNonetheless, a common drawback of these open-\nsource datasets is that they remain predominantly\nfocused on English data, offering limited data for\nother languages.\nTo obtain a multilingual large-scale dataset for\ntraining LLMs, it is more convenient to exploit\nweb-scrape datasets such as CC to enable efficient\ndata collection with up-to-date information in mul-\ntiple languages. In addition, to ensure high quality\nfor high-performing LLMs, it is necessary to ex-\ntensively clean and deduplicate the multilingual\ndata to avoid noisy and irrelevant content, e.g.,\nlow-quality machine-generated text and adult con-\ntent (Trinh and Le, 2018; Kreutzer et al., 2022;\nRaffel et al., 2020). As such, a typical data pro-\ncessing pipeline to generate high-quality datasets\ncan involve multiple steps, as demonstrated by\nFastText (Joulin et al., 2016), CC-Net (Wenzek\net al., 2020), the BigScience ROOTS corpus for\nthe BLOOM models (Lauren\u00e7on et al., 2022; Scao\net al., 2022), the RefinedWeb dataset for the Fal-\ncon model (Penedo et al., 2023; Almazrouei et al.,\n2023), and the dataset to train the LLaMa models\n(Touvron et al., 2023). The first step necessitates in\nsuch pipelines language identification to appropri-\nately assign data to their corresponding languages\n(Joulin et al., 2016). The next steps features various\ndataset-specific rules and heuristics to filter undesir-\nable content according to the ratios of special char-\nacters, short lines, bad words, among others (Grave\net al., 2018; Lauren\u00e7on et al., 2022). The data can\nalso be filtered via lightweight models, e.g., via\nthe KenLM language models (Heafield, 2011), to\navoid noisy documents (Wenzek et al., 2020). Fi-\nnally, data deduplication should be performed to\nremove similar or repeated information (Lauren\u00e7on\net al., 2022; Penedo et al., 2023). An important\nstep in this regard involves fuzzy deduplication at\ndocument level, e.g., via MinHash (Broder, 1997),\nto eliminate similar documents, thus mitigating\nmemorization and improving the generalization for\nresulting LLMs (Lee et al., 2022).\nTo this end, while there are multilingual open-\nsource datasets with text data in multiple languages,\nsuch as mC4 (Xue et al., 2021), OSCAR (Ortiz\nSu\u00e1rez et al., 2019), CC100 (Wenzek et al., 2020;\nConneau et al., 2020), and the BigScience ROOT\ncorpus (Lauren\u00e7on et al., 2022), their quality and\nscale do not meet the requirements for effectively\ntraining LLMs, particularly generative models such\nas GPT. For example, as highlighted in the intro-\nduction, both mC4 and OSCAR lack fuzzy dedu-\nplication for the data at the document level. mC4\nalso suffers from its poorer language identifica-\ntion due to the use of cld3. BigScience ROOTS\nonly provides a small sample data for 46 languages\nwhile CC100 does not have information beyond\n2018. Our dataset CulturaX thus comprehensively\naddresses the issues for the existing datasets, of-\nfering a multilingual, open-source, and large-scale\ndataset with readily usable and high-quality data to\ntrain LLMs.\n5\nConclusion\nWe present CulturaX, a novel multilingual dataset\nwith text data for 167 languages.\nOur dataset\nis cleaned and deduplicated via a comprehensive\npipeline, producing 6.3 trillion tokens. CulturaX is\nthus a large-scale and high-quality dataset, which\ncan be readily used to train high-performing LLMs\nfor multiple languages. Our data is openly acces-\nsible to the public to promote further research and\napplications of multilingual learning.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBeno\u00eet Sagot. 2022. Towards a cleaner document-\noriented multilingual crawled corpus. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4344\u20134355, Marseille, France.\nEuropean Language Resources Association.\nJulien Abadji, Pedro Javier Ortiz Su\u00e1rez, Laurent Ro-\nmary, and Beno\u00eet Sagot. 2021. Ungoliant: An op-\ntimized pipeline for the generation of a very large-\nscale multilingual web corpus. In Proceedings of\nthe Workshop on Challenges in the Management of\nLarge Corpora (CMLC-9) 2021. Limerick, 12 July\n2021 (Online-Event).\nMiltiadis Allamanis. 2018. The adverse effects of code\nduplication in machine learning models of code. Pro-\nceedings of the 2019 ACM SIGPLAN International\nSymposium on New Ideas, New Paradigms, and Re-\nflections on Programming and Software.\nEbtesam Almazrouei, Hamza Alobeidli, and Abdulaziz\nAlshamsi et al. 2023. Falcon-40B: an open large\nlanguage model with state-of-the-art performance.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. ArXiv, abs/2302.04023.\nMarta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth\nHeafield, Hieu Hoang, Miquel Espl\u00e0-Gomis, Mikel L.\nForcada, Amir Kamran, Faheem Kirefu, Philipp\nKoehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere,\nGema Ram\u00edrez-S\u00e1nchez, Elsa Sarr\u00edas, Marek Strelec,\nBrian Thompson, William Waites, Dion Wiggins, and\nJaume Zaragoza. 2020. ParaCrawl: Web-scale acqui-\nsition of parallel corpora. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4555\u20134567, Online. Association\nfor Computational Linguistics.\nRishi Bommasani, Drew A. Hudson, and Ehsan Adeli\net al. 2021. On the opportunities and risks of founda-\ntion models. ArXiv, abs/2108.07258.\nJan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex\nSalcianu, David Weiss, Ryan McDonald, and Slav\nPetrov. 2017. Natural language processing with small\nfeed-forward networks. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2879\u20132885, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nA. Broder. 1997. On the resemblance and containment\nof documents. In Proceedings of the Compression\nand Complexity of Sequences.\nTom Brown,\nBenjamin Mann,\nand et al. 2020.\nLanguage models are few-shot learners.\nArXiv,\nabs/2005.14165.\nAakanksha Chowdhery, Sharan Narang, and Jacob De-\nvlin et al. 2022. Palm: Scaling language modeling\nwith pathways. ArXiv, abs/2204.02311.\nTogether Computer. 2023. Redpajama: An open source\nrecipe to reproduce llama training dataset.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440\u2013\n8451, Online. Association for Computational Lin-\nguistics.\nMichel Dekking, Cornelis Kraaikamp, Hendrik Paul,\nand Ludolf Erwin Meester. 2007. A modern intro-\nduction to probability and statistics: Understanding\nwhy and how. In Springer Texts in Statistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLeo Gao, Stella Rose Biderman, Sid Black, Laurence\nGolding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020.\nThe pile: An\n800gb dataset of diverse text for language modeling.\nArXiv, abs/2101.00027.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nKenneth Heafield. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187\u2013197, Edinburgh, Scotland. Association for Com-\nputational Linguistics.\nDanny Hernandez, Tom B. Brown, Tom Conerly, Nova\nDasSarma, Dawn Drain, Sheer El-Showk, Nelson\nElhage, Zac Hatfield-Dodds, T. J. Henighan, Tristan\nHume, Scott Johnston, Benjamin Mann, Christopher\nOlah, Catherine Olsson, Dario Amodei, Nicholas\nJoseph, Jared Kaplan, and Sam McCandlish. 2022.\nScaling laws and interpretability of learning from\nrepeated data. ArXiv, abs/2205.10487.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. ArXiv, abs/1904.09751.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Herv\u00e9 J\u00e9gou, and Tomas Mikolov.\n2016. Fasttext.zip: Compressing text classification\nmodels. ArXiv, abs/1612.03651.\nZachary Kenton, Tom Everitt, Laura Weidinger, Ia-\nson Gabriel, Vladimir Mikulik, and Geoffrey Irv-\ning. 2021. Alignment of language agents. ArXiv,\nabs/2103.14659.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, An-\nnette Rios, Isabel Papadimitriou, Salomey Osei, Pe-\ndro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50\u201372.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66\u201375,\nMelbourne, Australia. Association for Computational\nLinguistics.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben\nVeyseh, Hieu Man, Franck Dernoncourt, Trung Bui,\nand Thien Huu Nguyen. 2023. Chatgpt beyond en-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. ArXiv,\nabs/2304.05613.\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao Mou,\nEduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, J\u00f6rg\nFrohberg, Mario \u0160a\u0161ko, Quentin Lhoest, Angelina\nMcMillan-Major, G\u00e9rard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Romero Mu\u00f1oz,\nJian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid\nAlmubarak, Vu Minh Chien, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ife-\noluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas\nPai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-\ngaret Mitchell, Sasha Luccioni, and Yacine Jernite.\n2022.\nThe bigscience ROOTS corpus: A 1.6TB\ncomposite multilingual dataset. In Thirty-sixth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424\u20138445, Dublin, Ireland. Association for\nComputational Linguistics.\nJure Leskovec, Anand Rajaraman, and Jeffrey David\nUllman. 2020. Mining of massive datasets. In Cam-\nbridge University Press.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871\u20137880, Online. Association for Computa-\ntional Linguistics.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\n2021. Jurassic-1: Technical details and evaluation.\nWhite Paper. AI21 Labs.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nMosaicML. 2023. Introducing mpt-7b: A new standard\nfor open-source, commercially usable llms. https:\n//www.mosaicml.com/blog/mpt-7b.\nSebastian\nNagel.\nCc-news.\nhttp:\n//web.archive.org/save/http:\n//commoncrawl.org/2016/10/news- dataset-available.\nPedro Javier Ortiz Su\u00e1rez, Laurent Romary, and Beno\u00eet\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 1703\u2013\n1714, Online. Association for Computational Linguis-\ntics.\nPedro Javier Ortiz Su\u00e1rez, Beno\u00eet Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low resource infras-\ntructures. In Proceedings of the Workshop on Chal-\nlenges in the Management of Large Corpora (CMLC-\n7) 2019. Cardiff, 22nd July 2019.\nGuilherme Penedo, Quentin Malartic, Daniel Hess-\nlow, Ruxandra-Aim\u00e9e Cojocaru, Alessandro Cap-\npelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam\nAlmazrouei, and Julien Launay. 2023. The refined-\nweb dataset for falcon llm: Outperforming curated\ncorpora with web data, and web data only. ArXiv,\nabs/2306.01116.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nJack Rae, Sebastian Borgeaud, and et al. 2021. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher. ArXiv, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. In Journal of Machine Learning Research.\nTeven Scao, Angela Fan, and et al. 2022. Bloom: A\n176b-parameter open-access multilingual language\nmodel. ArXiv, abs/2211.05100.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm\u00e1n. 2021. Wiki-\nMatrix: Mining 135M parallel sentences in 1620 lan-\nguage pairs from Wikipedia. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 1351\u20131361, Online. Association for Computa-\ntional Linguistics.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019.\nMegatron-lm: Training multi-billion\nparameter language models using model parallelism.\nArXiv, abs/1909.08053.\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep\nGanguli. 2021. Understanding the capabilities, limi-\ntations, and societal impact of large language models.\nArXiv, abs/2102.02503.\nHugo Touvron, Thibaut Lavril, and Gautier Izacard et al.\n2023. Llama: Open and efficient foundation lan-\nguage models. ArXiv, abs/2302.13971.\nTrieu H. Trinh and Quoc V. Le. 2018. A simple method\nfor commonsense reasoning. ArXiv, abs/1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nXiangpeng Wei, Hao-Ran Wei, Huan Lin, Tianhao Li,\nPei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhi-\nwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li,\nBinyuan Hui, Bowen Yu, Dayiheng Liu, Baosong\nYang, Fei Huang, and Jun Xie. 2023. Polylm: An\nopen source polyglot large language model. ArXiv,\nabs/2307.06018.\nLaura Weidinger, John F. J. Mellor, and Maribeth Rauh\net al. 2021. Ethical and social risks of harm from\nlanguage models. ArXiv, abs/2112.04359.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, On-\nline. Association for Computational Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. Proceedings of the\nIEEE International Conference on Computer Vision\n(ICCV).\n"
  },
  {
    "title": "Adapting Large Language Models via Reading Comprehension",
    "link": "https://arxiv.org/pdf/2309.09530.pdf",
    "upvote": "60",
    "text": "Published as a conference paper at ICLR 2024\nADAPTING LARGE LANGUAGE MODELS VIA\nREADING COMPREHENSION\nDaixuan Cheng, Shaohan Huang\u2217 & Furu Wei\nMicrosoft Research\nhttps://huggingface.co/AdaptLLM\nABSTRACT\nWe explore how continued pre-training on domain-specific corpora influences\nlarge language models, revealing that training on the raw corpora endows\nthe model with domain knowledge, but drastically hurts its prompting ability\nfor question answering.\nTaken inspiration from human learning via reading\ncomprehension\u2014practice after reading improves the ability to answer questions\nbased on the learned knowledge\u2014we propose a simple method for transforming\nraw corpora into reading comprehension texts. Each raw text is enriched with a\nseries of tasks related to its content. Our method, highly scalable and applica-\nble to any pre-training corpora, consistently enhances performance across various\ntasks in three different domains: biomedicine, finance, and law. Notably, our 7B\nlanguage model achieves competitive performance with domain-specific models\nof much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate\nthat domain-specific reading comprehension texts can improve the model\u2019s per-\nformance even on general benchmarks, showing the potential to develop a general\nmodel across even more domains. Our model, code, and data are available at\nhttps://github.com/microsoft/LMOps.\n25\n35\n45\n55\n65\nChemProt\nRCT\nMQP\nPubMedQA\n25\n40\n55\n70\n85\nConvFinQA\nFPB\nFiQA SA\nHeadline\n7\n14\n21\n28\n35\nSCOTUS-mac\nSCOTUS-mic\nCaseHOLD-mac\nCaseHOLD-mic\nBiomedicine\nFinance\nLaw\n4\n12\n20\n28\n36\nSCOTUS-mac\nSCOTUS-mic\nCaseHOLD-mac\nCaseHOLD-mic\nGeneral LLM\nDAPT\nAdaptLLM\nFigure 1: Domain-specific task performance in biomedicine, finance, and law. General LLM is\nthe general language model without continued training, DAPT (Gururangan et al., 2020) continues\nto train the general model on domain-specific raw corpora, and AdaptLLM continues to train the\ngeneral model on the reading comprehension texts constructed based on the raw corpora, mixed with\ngeneral instructions.\n\u2217Corresponding author: shaohanh@microsoft.com\n1\narXiv:2309.09530v2  [cs.CL]  21 Feb 2024\nPublished as a conference paper at ICLR 2024\nHere is the first part of an article about biomedicine: Recent reported\nevidence indicates that vocal cord carcinoma is evolving similarly to\noropharyngeal cancer with an increasing number of patients (...)\nAnswer questions based on the article:\nWhat is asummary? Glottic Carcinoma in Young Patients.\nGenerate a sentence\nthat includes these biomedicine\nkeywords [carcinoma, oropharyngeal, papillomavirus]: Recent\nreported evidence indicates that vocal cord carcinoma is evolving\u2026\nPremise:\u2026 Hypothesis:\u2026 Does the premiseentail the hypothesis? Yes\nWhat is the reason for \u201d\u2026\"? the morphology of the lesions and the\npatients' young age.\nCompose a sentence thatcontradicts the meaning of \"Historically,\nglottic carcinoma \u2026 \u201d.\nAnswer: Recent published evidence \u2026\nHow would you complete the article? This finding further\nsupports\u2026\nRaw Text\nReading Comprehension\nGlottic Carcinoma in Young Patients\nRecent reported evidence indicates that vocal cord\ncarcinoma\nis\nevolving\nsimilarly\nto\noropharyngeal\ncancer\nwith\nan\nincreasing\nnumber of patients without a smoking history\nhaving human papillomavirus (HPV) disease.\n(...) Therefore, an investigation was done to\nexamine the incidence of glottic carcinoma in\npatients 30 years old or younger. (...) due to\nthe morphology of the lesions and the patients'\nyoung\nage.\nHistorically,\nglottic\ncarcinoma\nis\nconsidered to be a tobacco-induced disease. In\ncontrast, recent published evidence shows\nthat glottic carcinoma can be an HPV-related\ndisease with increasing incidence in nonsmokers.\n(...) This finding further supports\u2026\nCause & effect\nText \nending\nTitle\nDomain \nkeywords\nEntailment \nrelation\nSemantic \nsimilarity\nFigure 2: A simplified example of a reading comprehension text, wherein the raw text is followed\nby a series of tasks constructed from it, including Summarization (purple), Word-to-Text (blue),\nNatural Language Inference (red), Commonsense Reasoning (teal), Paraphrase Detection (yellow),\nand Text Completion (green). The complete version is in Appendix G.\n1\nINTRODUCTION\nThe proliferation of general large language models (LLMs) has given rise to the emergence of\ndomain-specific large language models. Existing methods can be broadly classified into three ap-\nproaches. The first trains models from scratch on a mixture of domain-specific and general cor-\npora (Wu et al., 2023b). While this intuitively creates domain-specific LLMs, the substantial com-\nputational and data requirements raise significant concerns (Yang et al., 2023; Ling et al., 2023).\nThe second fine-tunes the language model using supervised datasets (Singhal et al., 2022; 2023; Li\net al., 2023b;a; Wang et al., 2023; Han et al., 2023; Xiong et al., 2023; Huang et al., 2023), offer-\ning a more cost-effective option. However, there are uncertainties about how well fine-tuned LLMs\ngrasp domain knowledge that can be universally applied to all domain-specific tasks, as discussed\nby Zhou et al. (2023) and Gudibande et al. (2023). The third prompts the general language model\nwith retrieved domain knowledge (Li et al., 2023b; Cui et al., 2023; Huang et al., 2023), which can\nbe considered as an application of LLM rather than a direct enhancement to the LLM itself.\nContinued pre-training on domain-specific corpora,\nalso known as domain-adaptive pre-\ntraining (Gururangan et al., 2020), has been proven effective in adapting various natural language\nunderstanding models (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) to specific do-\nmains (Yao et al., 2021; Gururangan et al., 2020; Cheng et al., 2022). This approach enables lan-\nguage models to leverage general ability while incorporating domain-specific knowledge, benefiting\ndownstream domain-specific tasks at reduced costs. This motivates our investigation into whether\ndomain-adaptive pre-training also benefits large-scale generative models. We conduct preliminary\nexperiments on three domains\u2014biomedicine, finance, and law\u2014revealing that continued training\non the raw corpora results in a drastic drop in prompting performance but still benefits fine-tuning\nand knowledge probing evaluations. This leads us to conclude that domain-adaptive pre-training\nusing raw corpora endows the LLM with domain knowledge while hurting its prompting ability.\nTo leverage domain-specific knowledge while enhancing prompting performance, we introduce a\nsimple method for transforming large-scale raw corpora into reading comprehension texts: each raw\ntext is enriched with a series of tasks relevant to its content, as illustrated in Figure 2. These tasks are\ndesigned to help the model maintain its ability to answer questions using natural language, based on\nthe context of the raw text. Furthermore, we augment the reading comprehension texts with diverse\ngeneral instructions, thereby further enhancing prompting ability (Wei et al., 2022; Zhou et al., 2023;\n2\nPublished as a conference paper at ICLR 2024\nXu et al., 2023; Mukherjee et al., 2023). Our experiments in domains such as biomedicine, finance,\nand law highlight the effectiveness of our approach in improving model performance on various\ndomain-specific tasks. We refer to this resulting model as AdaptLLM, for Adapted Large Language\nModel. Looking ahead, we envision extending this methodology to the development of a general\nlarge language model, contributing to the ever-expanding landscape of tasks across more domains.\nIn summary, our contributions include:\n\u2022 We investigate domain-adaptive pre-training for large language models, where we find contin-\nued training on domain-specific raw corpora can endow the model with domain knowledge, but\ndrastically hurts its prompting ability.\n\u2022 We propose a simple recipe which automatically converts large-scale raw corpora into reading\ncomprehension texts, to effectively learn the domain knowledge while concurrently preserving\nprompting performance.\n\u2022 Our experiments show the effectiveness of our method in consistently improving model perfor-\nmance in three different domains: biomedicine, finance and law.\n2\nPRELIMINARY EXPLORATION ON DOMAIN-ADAPTIVE PRE-TRAINING\nGiven the proven efficacy and efficiency of domain-adaptive pre-training in adapting natural lan-\nguage understanding models (Gururangan et al., 2020; Yao et al., 2021; Cheng et al., 2022), we\nembark on an exploration to ascertain whether this method remains effective for large-scale genera-\ntive models. We continue to train the general LLaMA (Touvron et al., 2023a) on the domain-specific\nraw corpora of biomedicine, finance, and law, respectively, and conduct prompting, fine-tuning, and\nknowledge probing evaluations to assess the model performance within each domain (detailed ex-\nperiment settings are in Section 4).\nTable 1: Domain-specific task performance of the general language model (General LLM) and the\nmodel that has undergone vanilla domain-adaptive pretraining (DAPT (Gururangan et al., 2020)).\nWe report the average of task scores within each domain under prompting, fine-tuning and knowl-\nedge probing settings.\nMethod\nPrompting\nFine-tuning\nKnowledge Prob\nBioMed.\nFinance\nLaw\nBioMed.\nFinance\nLaw\nBioMed.\nLaw\nGeneral LLM\n44.2\n58.6\n34.2\n64.2\n79.9\n42.0\n36.5\n45.0\nDAPT\n41.7\n57.6\n35.0\n66.5\n80.9\n45.4\n36.9\n45.6\nPrompting vs. Fine-tuning. In Table 1, when conducting fine-tuning evaluation on the domain-\nspecific tasks, the model that has undergone domain-adaptive pre-training consistently outperforms\nthe general model across all three domains. This aligns with findings about language understand-\ning models (Gururangan et al., 2020), indicating that continued pre-training enriches the language\nmodel with domain-specific knowledge. Paradoxically, a contradictory trend emerges in the prompt-\ning performance, where a noticeable drop is observed across most domains after domain-adaptive\npre-training. This contradiction leads us to hypothesize that while vanilla domain-adaptive pre-\ntraining enhances the LLM\u2019s domain knowledge, contributing to the fine-tuning improvements, it\nalso significantly impairs its ability to perform well in prompting, causing the observed drop.\nDomain Knowledge Probing. To confirm whether the language model gains domain knowledge\nfrom domain-adaptive pre-training, we employ a probing method similar to LAMA (Petroni et al.,\n2019). Using the supervised datasets available in each domain as the basis, we create domain-\nspecific knowledge-probing datasets. The dataset creation process is detailed in Appendix A. In\nTable 1, we present the results of domain knowledge probing for the biomedicine and law domains1.\nIn both domains, we observe improved results after domain-adaptive pre-training, indicating that the\nmodel indeed acquires domain knowledge.\n1We were unable to construct a knowledge probing test for finance due to the limited availability of super-\nvised datasets in this domain.\n3\nPublished as a conference paper at ICLR 2024\nThe analyses above suggest that the drop in domain-specific prompting performance can be at-\ntributed to the reduced prompting ability. This reduction may result from the limited diversity of\nthe pre-training corpora within a specific domain (Longpre et al., 2023b), which limits the diversity\nof input-output patterns derived from raw texts (Wei et al., 2022). Therefore, improving prompting\nability becomes essential for effectively harnessing the domain knowledge acquired from domain-\nadaptive pre-training.\n3\nADAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION\nInstead of continuing to train the large language model on domain-specific raw corpora, we convert\nthe raw corpora into reading comprehension texts and adapt the model using the converted data. In\nreading comprehension, each raw text is followed by a series of tasks related to its content. We\nregard the model training phase on the raw text as the \u201creading\u201d phase, and the subsequent training\non the relevant tasks as the \u201ccomprehension\u201d phase (Chen et al., 2023; Gu et al., 2022; 2023).\nThese comprehension tasks follow the question-answering format, aimed at enriching the model\u2019s\nprompting ability to respond to input questions (Wei et al., 2022). This design is inspired from\nhuman learning, where practice after reading enhances the ability to answer questions based on the\nacquired knowledge. Besides, we augment the training data with general instructions (Zhou et al.,\n2023; Xu et al., 2023; Mukherjee et al., 2023) to benefit from the diversity of input-output patterns,\nthereby further improving prompting ability.\n3.1\nCREATING READING COMPREHENSION TEXTS\nTo create reading comprehension texts, we start by mining intrinsic tasks from the raw corpora with\na handful of mining patterns. The idea of mining tasks from pre-training corpora was introduced\nby van de Kar et al. (2022). This method mines intrinsic tasks through a few regex-based patterns,\nand then fine-tunes the model on these tasks to enhance zero-shot performance. Our approach\nleverages the self-supervised nature of this mining strategy. This enables us to scale up the transfer\nof raw pre-training corpora, capitalizing on the domain-specific knowledge embedded in the raw\ntexts and the enhanced prompting ability provided by the comprehension tasks.\nTable 2 gives an overview of the techniques used to mine and create tasks from each raw text.\nPhrases like Answer questions based on the article: are employed to concatenate the\nraw text with the followed tasks, as illustrated in Figure 2. Additionally, we paraphrase each input-\noutput template to multiple variations and turn the task around to enhance task diversity (Wei et al.,\n2022; Chung et al., 2022; Longpre et al., 2023a).\nSummarization prompts the models to generate a concise summary of the provided text, encour-\naging them to extract its main idea. We use queries such as What is a summary? to prompt the\nmodel to summarize the raw text, with the text title serving as the groundtruth. We also reverse the\ntask, asking the model to craft an article based on the given title.\nAdditionally, we prompt the models to identify sentence topics. To unearth such intrinsic tasks,\nwe utilize regex-based patterns to identify sentences aligning with the patterns specified in Table 2.\nWe then employ the corresponding templates to construct the input-output pairs (van de Kar et al.,\n2022).\nWord-to-Text enhances the model\u2019s grasp of domain-specific vocabulary by prompting it to gen-\nerate sentences incorporating specific words. To identify domain-specific words, we use the Sen-\ntencePiece tool (Kudo & Richardson, 2018) to build a vocabulary from the target domain corpora.\nWe then compare this domain vocabulary to the general language model\u2019s vocabulary, considering\nwords present in the domain vocabulary but absent from the general vocabulary as domain-specific.\nNext, we filter out tokens with fewer than 10 characters, resulting in a set of domain-specific key-\nwords.\nFor each sentence in the raw text, we count the number of domain-specific keywords. Sentences\nhaving more than three domain-specific keywords are selected for making Word-to-Text tasks. We\ntake the domain-specific keywords in the sentence as the input, asking the model to generate a\nsentence with Generate a sentence that includes these {DOMAIN} keywords.\n4\nPublished as a conference paper at ICLR 2024\nTable 2: Mining patterns and input-output templates. For mining, {VERBAL} is replaced with\nthe verbalizers in Table 3, {WORD} captures a single word, and {SENT} captures a single sentence.\nEach input-output template is paraphrased into multiple variations. We also turn the task around\u2014\nexchanging the input and output\u2014to enhance task diversity.\nTask Type\nMining Pattern\nInput-output Template\nSummarization\nTitle\nTitle as summary\nWhat is a summary? {TITLE}\nTopic\n{SENT1} {VERBAL} {SENT2}\n{SENT1} is about: {SENT2}\nWord-to-Text\nWord-to-text\nDomain keywords as input;\nsentence as output\nGenerate a sentence about\nthese {DOMAIN} keywords\n[{WORD1}, {WORD2}, {WORD3}]: {SENT}\nDefinition\n{WORD} {VERBAL} {SENT}\nHow to define {WORD}? {SENT}\nNatural Language Inference\nEntail\n{SENT1} {VERBAL}, {SENT2}\nDoes \"{SENT1}\" entail \"{SENT2}\"?\n{Yes/Maybe/No}\nNeutral\nContradict\nCommonsense Reasoning\nCause-effect\n{SENT1} {VERBAL}, {SENT2}\nWhat is the {effect/cause}\nof {SENT1}? {SENT2}\nEffect-cause\n{SENT1} {VERBAL} {SENT2}\nParagraph Detection\nSimilar\n{SENT1} {VERBAL}, {SENT2}\nCompose a sentence to {support/\ncontradict} \"{SENT1}\". {SENT2}\nDifferent\nText Completion\nText completion\nText ending as completion\nHow would you complete the\narticle? {ENDING}\nWe also turn the task around by taking the sentence as input and asking the model to find the key-\nwords about the target domain, using What keywords about {DOMAIN} can be extracted\nfrom this sentence?. Here we point out the target domain by replacing {DOMAIN} with do-\nmain names such as biomedicine, finance, or law. Besides, we prompt the model to define\nconcepts using the mining patterns and input-output templates in Table 2.\nNatural Language Inference concerns how two sentences relate, typically asking, given a first\nsentence, whether a second sentence is true, false or possibly true. We use the regex-based patterns\nin Table 2 to search for \u201cpremise-hypothesis-relation\u201d triplets within the raw text. For example, we\ncategorize the relation between two sentences as \u201cEntailment\u201d if they are connected by the verbalizer\nTherefore, and as \u201cContradictory\u201d if connected by However.\nAdditionally, we enhance diversity by reformatting classification tasks into generation tasks. For in-\nstance, when the relation between two sentences is \u201cEntailment\u201d, we employ templates like {SENT1}\nThus? to prompt the model to generate an output, where the ground truth is the second sentence.\nCommonsense Reasoning evaluates the ability to perform physical or scientific reasoning while\nconsidering common sense. We identify cause-and-effect logic within sentences using the regex-\nbased patterns in Table 2. We then formulate the input-output pairs using templates such as What\nis the reason of {SENT1}? {SENT2}.\nParaphrase Detection asks a model to determine whether two sentences are semantically equiva-\nlent. To collect task data, we use regex-based patterns in Table 2 to search for \u201csentence1-sentence2-\nlabel\u201d data triplets. However, we empirically find that the mining patterns cannot consistently iden-\n5\nPublished as a conference paper at ICLR 2024\nTable 3: Verbalizers for mining patterns in Table 2.\nTask Type\nVerbalizer\nSummarization\nTopic\ntalks about, is about, \u2019s topic is\nWord-to-Text\nDefinition\nis defined as, \u2019s definition is\nNatural Language Inference\nEntail\nYes, Therefore, Thus, Accordingly, Hence, For this reason\nNeutral\nMaybe, Furthermore, Additionally, Moreover, In addition\nContradict\nNo, However, But, On the contrary, In contrast, Whereas\nCommonsense Reasoning\nCause-effect\nTherefore, Thus, Accordingly, Hence, For this reason\nEffect-cause\ndue to, on account of, owing to\nParagraph Detection\nSimilar\nSimilarly, Equally, In other words, Namely, That is to say\nDifferent\nNo, However, But, On the contrary, In contrast, Whereas\ntify two sentences with strictly equivalent meanings. For instance, sentences linked by the verbalizer\nSimilarly may not share similar meanings.\nTherefore, we reformat the classification task into a generation task to reduce dependence on label\naccuracy. Instead of inquiring whether two sentences are similar, we prompt the model to generate a\nsentence that either supports or contradicts the meaning of a given sentence, using input-output tem-\nplates like Can you create a sentence that contradicts the meaning of {SENT1}?\n{SENT2} when the mined label is \u201cDifferent\u201d.\nText Completion. In addition to the inherent casual language modeling task within generative\nlanguage models, we insert queries such as How would you complete the article? between\nsentences to prompt the language model to generate the subsequent section. An advantage of Text\nCompletion task is that it does not require any specific mining patterns, thus can be applied to any\nraw texts.\n3.2\nMIXING WITH GENERAL INSTRUCTIONS\nWhile we have designed diverse mining patterns, input-output templates and task reversals to en-\nhance prompting ability, they might not fully address the infinite task diversity in real-world scenar-\nios. In light of this, we propose to mix the reading comprehension texts with general instructions to\ncover a wider range of input-output patterns.\n4\nEXPERIMENT SETTINGS\nDomain-adaptive Pre-training. PubMed Abstracts and FreeLaw Opinions from the Pile (Gao et al.,\n2021) are utilized as the pre-training corpora for the biomedicine and law domains, respectively. For\nfinance, we collect financial news from May 2022 to May 20232 for over 7, 000 stocks, using the\nFinGPT codebase (Yang et al., 2023). General instructions are sourced from LIMA (Zhou et al.,\n2023), WizardLM (Xu et al., 2023), and Orca (Mukherjee et al., 2023; Lian et al., 2023). We con-\ntinue to train LLaMA-7B (Touvron et al., 2023a) on each domain, and explore different ratios for\nmixing reading comprehension texts with general instructions; the optimal ratios for biomedicine, fi-\nnance, and law are 1 : 1, 1 : 2, and 1 : 1, respectively. Implementation details, dataset specifications,\nand other pre-training hyper-parameters can be found in Appendix B.\n2Access to earlier news is limited.\n6\nPublished as a conference paper at ICLR 2024\nCreating Reading Comprehension Texts. Using the mining patterns in Table 2, we search for sub-\ncategories within each task type. To prevent task dominance, we limit the number of task examples\nper sub-category to two for each raw text. For each mined example, we randomly sample from\nvarious paraphrased or task-reversed templates to generate an input-output example. To structure\nthe reading comprehension text, we use \\n\\n to connect comprehension tasks and link them with\nthe raw text. On average, about two input-output examples are collected per reading comprehension\ntext. Please refer to Appendix C for mining pattern implementation details and Appendix G for\ncases of reading comprehension texts.\nDomain-specific Tasks.\nFor biomedicine, we evaluate on PubMedQA (Jin et al., 2019),\nChemProt (Kringelum et al., 2016), MQP (McCreery et al., 2020), RCT (Dernoncourt & Lee, 2017),\nand USMLE (Jin et al., 2020). For finance, we evaluate on the five publicly available tasks also eval-\nuated by BloombergGPT (Wu et al., 2023b): ConvFinQA (Chen et al., 2022), FPB (Malo et al.,\n2014), FiQA SA (Maia et al., 2018), Headline (Sinha & Khandait, 2020), and NER (Alvarado\net al., 2015), and adopt similar prompting settings with BloombergGPT. For law, we evaluate on\nSCOTUS (Spaeth et al., 2020), CaseHOLD (Zheng et al., 2021) and UNFAIR-ToS (Lippi et al.,\n2019) from the LexGLUE (Chalkidis et al., 2022) benchmark. Evaluation details are provided in\nAppendix D.\n5\nMAIN RESULTS\nIn Table 4, we compare our models (AdaptLLM) with the general language models, and the models\nthat have undergone vanilla domain-adaptive pre-training on raw corpora (DAPT). On various tasks\nin the three domains, the use of raw corpora in DAPT adversely affects the prompting performance.\nHowever, the transformation of raw corpora and the inclusion of general instructions in AdaptLLM\nmanage to counteract this effect, outperforming the general language model.\nBesides, we compare AdaptLLM with other publicly-available models/results in each domain as\nfollows.\nBiomedicine. We compare with MedAlpaca (Han et al., 2023), which fine-tunes LLaMA (Tou-\nvron et al., 2023a) on medical question-answering instructions. While supervised instructions help\nMedAlpaca outperform LLaMA in some domain-specific tasks, this advantage isn\u2019t consistent, pos-\nsibly because the instructions don\u2019t fully infuse domain knowledge, or the instructions specific to\none domain struggle with various input-output scenarios.\nFinance. We compare our results with those reported in BloombergGPT (Wu et al., 2023b), a model\ntrained from scratch on a mixture of financial and general corpora. While LLaMA-7B scores lower\nthan BloombergGPT-50B, AdaptLLM-7B achieves competitive performance with it. This highlights\nthe computational and data efficiency of our approach compared to training from scratch.\nLaw. We compare with LexGPT (Lee, 2023) which conducts vanilla domain-adaptive pre-training\non GPT-J (Wang & Komatsuzaki, 2021) using the raw corpora of Pile of Law (Henderson et al.,\n2022). In contrast to GPT-J, LexGPT shows negative prompting results. This trend aligns with\nour observation in section 2 that continued pre-training on domain-specific raw corpora leads to\nworse prompting performance. However, our method contributes to positive prompting results,\nemphasizing the effectiveness of comprehension tasks and general instructions.\n6\nABLATIONS ON TRAINING DATA\nTable 5 presents ablation results on different training data and data mixtures: (1) Raw Text refers\nto the raw corpora used in vanilla domain-adaptive pre-training. (2) Read. Compre. converts the\nraw corpora into reading comprehension texts, boosting the prompting ability to show better results\nin all of the adapted domains. (3) Gen. Ins. refers to general instructions. (4) Read. + Gen. Ins.\naugments reading comprehension texts with general instructions. Compared to using reading com-\nprehension texts only, the inclusion of general instructions further improves the prompting ability,\nleading to better task results. Moreover, compared to the use of general instructions alone, the uti-\nlization of reading comprehension texts provides domain knowledge that enhances performance in\ndomain-specific tasks. Furthermore, we provide ablations for each of the comprehension task types\n7\nPublished as a conference paper at ICLR 2024\nTable 4: Domain-specific task performance of the general language models, the models that\nhas undergone vanilla domain-adaptive pre-training (DAPT), and ours (AdaptLLM) in prompting\nevaluation. We also display prompting results of MedAlpaca (Han et al., 2023) in biomedicine,\nBloombergGPT (Wu et al., 2023b) in finance, and LexGPT (Lee, 2023) in law. AdaptLLM-13B in\nbiomedicine is trained from LLaMA-13B and AdaptLLM-6B in law is trained from GPT-J-6B.\nBiomedicine\nPubMedQA\nChemProt\nMQP\nRCT\nUMSLE\nAVERAGE\nLLaMA-7B\n59.6\n31.4\n50.7\n45.1\n34.5\n44.2\nDAPT-7B\n52.6\n26.6\n49.2\n46.6\n33.5\n41.7\nMedAlpaca-7B\n58.6\n39.0\n50.7\n40.8\n36.7\n45.1\nAdaptLLM-7B\n63.3\n35.2\n54.4\n50.4\n33.1\n47.3\nLLaMA-13B\n59.6\n42.8\n49.3\n56.7\n34.7\n48.6\nDAPT-13B\n51.1\n38.0\n49.0\n50.9\n34.6\n44.7\nMedAlpaca-13B\n60.7\n38.4\n57.4\n51.3\n41.2\n49.8\nAdaptLLM-13B\n66.0\n47.6\n73.0\n50.4\n34.0\n54.2\nFinance\nConvFinQA\nFPB\nFiQA SA\nHeadline\nNER\nAVERAGE\nBloombergGPT-50B\n43.4\n51.1\n75.1\n82.2\n60.8\n62.5\nLLaMA-7B\n29.2\n55.9\n69.2\n77.7\n61.1\n58.6\nDAPT-7B\n29.6\n55.3\n64.9\n77.5\n60.6\n57.6\nAdaptLLM-7B\n41.5\n62.5\n72.1\n81.4\n59.3\n63.4\nLaw\nSCOTUS\nCaseHOLD\nUNFAIR-ToS\nAVERAGE\nmic-F1\nmac-F1\nmic-F1\nmac-F1\nGPT-J-6B\n15.9\n13.6\n34.9\n34.9\n79.8\n35.9\nDAPT-6B\n10.1\n10.5\n34.6\n34.6\n84.9\n35.0\nLexGPT-6B\n16.9\n7.7\n27.0\n27.0\n81.9\n32.1\nAdaptLLM-6B\n18.8\n20.1\n34.7\n34.7\n80.0\n37.7\nLLaMA-7B\n28.3\n10.8\n32.9\n32.9\n65.8\n34.2\nDAPT-7B\n25.0\n9.8\n34.2\n34.2\n72.0\n35.0\nAdaptLLM-7B\n30.0\n17.8\n35.1\n35.1\n74.4\n38.5\nin Appendix E, where we find that Word-to-Text and Natural Language Inference exhibit the highest\neffectiveness on domain-specific tasks.\nTable 5: Ablation results on training data. Raw Text refers to raw corpora, Read. Compre. refers\nto reading comprehension texts, Gen. Ins. refers to general instructions, and Raw. + Gen. Ins. and\nRead. + Gen. Ins. correspond to different data mixtures. We report the average of task scores in\nprompting evaluation within each domain.\nData\nRaw Text\nRead. Compre.\nGen. Ins.\nRaw. + Gen. Ins.\nRead. + Gen. Ins.\nBioMed.\n41.7\n44.3\n43.3\n44.8\n47.3\nFinance\n57.6\n60.0\n62.2\n61.7\n63.4\nLaw\n35.0\n37.0\n37.8\n34.7\n38.5\n7\nANALYSIS OF DOMAIN KNOWLEDGE AND PROMPTING ABILITY\nOur design of reading comprehension is to learn the domain-specific knowledge from the raw texts\nand to enhance the prompting ability from the comprehension tasks. In this section, we conduct\nanalyses on the two aspects respectively.\n8\nPublished as a conference paper at ICLR 2024\nSummarize\nWord-to\n-Text\nN.L.I\nCommon.\nReason.\nParaphrase\nText\nComple.\nClose.\nQA\nRead.\nCompre.\n14\n35\n45\n55\n65\n75\n85\nBioMed.\nFinance\nLaw\nFine-tune Score\nDomain Knowledge\nPrompting Ability\nSummarization\nWord-to-Text\nLanguage\nInference\nCommonsense\nReasoning\nParaphrase\nDetection\nText\nCompletion\nNatural\nQuestion\nReading\nComprehension\nGeneral LLM\nRaw Text\nRead. Compre.\n14.1\n19.2\n37.6\n72.8\n39.5\n12.0\n29.6\n32.3\n11\n9\n19\n15\n12\n38\n31\n24\n24\n32\n40\n12\n10\n7\n18\n24\n30\n33\n27\n20\n44\n58\n73\nFigure 3: Fine-tuning evaluation on domain-specific tasks (left) and prompting evaluation on\ngeneral tasks (right). General LLM is the general language model, Raw Text trains the general\nmodel on the domain-specific raw corpora, and Read. Compre. trains the general model on the\nreading comprehension texts constructed based on the raw corpora. We report the average of task\nscores within each domain/type, detailed results are listed in Appendix F.\nDomain Knowledge. In addition to the prompting evaluation in Sections 5 and 6, we conduct\nfine-tuning and knowledge probing evaluations to assess whether the continued training on reading\ncomprehension texts endows the general model with domain knowledge. As shown in the fine-\ntuning results in Figure 3, after the training on reading comprehension texts, the model consistently\nexhibits improved results on the domain-specific tasks. The fine-tuning and knowledge probing\nimprovements (detailed in Appendix A) provide empirical evidence that the reading comprehension\ntexts indeed imbue the general model with domain knowledge.\nNotably, Read. Compre. outperforms Raw Text in all the adapted domains in the fine-tuning results.\nThis may be because the inclusion of diverse comprehension tasks naturally creates a \u201cmulti-task\ninstruction tuning\u201d setting, which benefits single-task fine-tuning (Longpre et al., 2023a).\nPrompting Ability. Our approach focuses on enhancing prompting ability through the comprehen-\nsion tasks. To assess their effectiveness, we employ general LLM benchmarks to evaluate zero-shot\nprompting performance. Specifically, we evaluate at least three general tasks for each comprehen-\nsion task type, following the task clustering setup in FLAN (Wei et al., 2022). Besides, we assess\nperformance on general Reading Comprehension and Closed-book QA tasks to evaluate the ability\nto answer questions with or without contexts.\nFigure 3 presents the average task scores within each task type, subsequently averaged across the\nthree adapted language models. Converting raw texts into reading comprehension texts consistently\nimproves prompting performance across all task types. Remarkably, when trained on our domain-\nspecific reading comprehension texts (without the inclusion of general instructions), we achieve even\nbetter results than the general language model on most task types. This highlights our approach\u2019s\npotential in developing a general language model across more domains. In Appendix E, we provide\nablations for each comprehension task type to analyze its impact on related downstream tasks.\n8\nRELATED WORK\nRecent works that apply large language models to specific domains (Singhal et al., 2022; 2023; Li\net al., 2023b; Wu et al., 2023a; Li et al., 2023a; Wang et al., 2023; Xiong et al., 2023; Wu et al.,\n2023b; Yang et al., 2023; Cui et al., 2023; Huang et al., 2023) can be categorized into three main\napproaches: training from scratch, instruction fine-tuning and retrieval-augmented prompting.\n9\nPublished as a conference paper at ICLR 2024\nTraining from Scratch. Training a domain-specific language models from scratch is an intuitive\napproach to realize domain specialization. BloombergGPT (Wu et al., 2023b) represents an early ex-\nample of large language models in the financial domain, trained on a mixture of financial and general\ncorpora. It demonstrates great performance on financial tasks without sacrificing the performance\non general LLM benchmarks. However, studies (Yang et al., 2023; Ling et al., 2023) have pointed\nout \u201ctraining from scratch\u201d comes with expensive computational and data requirements, which mo-\ntivates the need for low-cost adaptation methods such as fine-tuning or continued pre-training.\nInstruction Fine-tuning. Fine-tuning large language models on domain-specific tasks, particularly\nthose involving question-answering instructions, serves as a cost-effective adaptation method (Sing-\nhal et al., 2022; 2023; Li et al., 2023b;a; Wang et al., 2023; Han et al., 2023; Xiong et al., 2023;\nHuang et al., 2023). However, models fine-tuned with limited data might struggle to acquire suf-\nficient domain knowledge. Therefore, creating large-scale supervised data becomes significant ob-\njective. Previous methods employ high-performing LLMs (OpenAI, 2023) to generate question-\nanswering pairs (Li et al., 2023a), but the inference cost can be substantial. In such situations,\nharnessing large-scale domain-specific corpora is a promising for acquiring domain knowledge.\nRetrieval-augmented Prompting. Retrieval augmentation enhances LLMs by integrating external\ndomain-specific information without modifying the model parameters (Li et al., 2023b; Cui et al.,\n2023; Huang et al., 2023). This enables LLMs to better answer domain-specific questions and\naddress issues like hallucination. It\u2019s important to allow LLMs the option to accept or reject retrieved\ninformation due to potential incompleteness or conflicts (Ling et al., 2023). Training LLMs to\nincorporate domain knowledge can aid in making such informed acceptance or rejection decisions.\n9\nCONCLUSION\nThis paper focuses on adapting large language models via continued training on domain-specific\ncorpora. We propose a simple method to transform large-scale domain-specific raw corpora into\nreading comprehension texts, enabling the model to acquire domain knowledge from raw texts and\nto enhance prompting ability through comprehension tasks. Experiments in three different domains\nconfirm the effectiveness and generalizability of this method. Moreover, the reading comprehension\ntexts enhance model performance on general LLM benchmarks, suggesting potential for improving\ngeneral language models across more domains. We hope our method can inspire further exploration\ninto adapting large language models with the use of large-scale pre-training corpora, efficiently\nempowering language models for downstream tasks in specialized areas.\nREFERENCES\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. Domain adaption of named\nentity recognition to support credit risk assessment. In ALTA, pp. 84\u201390. ACL, 2015.\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. The\nfifth PASCAL recognizing textual entailment challenge. In TAC. NIST, 2009.\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richard-\nson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have\nsolved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge.\nCoRR, abs/2102.03315, 2021.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,\nAviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large\nlanguage models across training and scaling. In ICML, volume 202 of Proceedings of Machine\nLearning Research, pp. 2397\u20132430. PMLR, 2023.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about\nphysical commonsense in natural language. In AAAI, pp. 7432\u20137439. AAAI Press, 2020.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shiv-\n10\nPublished as a conference paper at ICLR 2024\nanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b:\nAn open-source autoregressive language model. CoRR, abs/2204.06745, 2022.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-\ntated corpus for learning natural language inference. In EMNLP, pp. 632\u2013642. The Association\nfor Computational Linguistics, 2015.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\nIlias Chalkidis. Chatgpt may pass the bar exam soon, but has a long way to go for the lexglue\nbenchmark.\nSSRN, 2023.\nURL https://papers.ssrn.com/sol3/papers.cfm?\nabstract_id=4385460.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael J. Bommarito II, Ion Androutsopoulos,\nDaniel Martin Katz, and Nikolaos Aletras. Lexglue: A benchmark dataset for legal language\nunderstanding in english. In ACL (1), pp. 4310\u20134330. Association for Computational Linguistics,\n2022.\nZeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, and Antoine Bosselut. RECKONING:\nreasoning through dynamic knowledge encoding. CoRR, abs/2305.06349, 2023.\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang.\nConvfinqa: Exploring the chain of numerical reasoning in conversational finance question an-\nswering. In EMNLP, pp. 6279\u20136292. Association for Computational Linguistics, 2022.\nDaixuan Cheng, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Furu Wei, Denvy Deng,\nand Qi Zhang. Snapshot-guided domain adaptation for ELECTRA. In EMNLP (Findings), pp.\n2226\u20132232. Association for Computational Linguistics, 2022.\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun,\nFuru Wei, Weiwei Deng, and Qi Zhang. UPRISE: universal prompt retrieval for improving zero-\nshot evaluation. In EMNLP, pp. 12318\u201312337. Association for Computational Linguistics, 2023.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In NIPS, pp. 4299\u20134307, 2017.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi,\nJeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\nScaling\ninstruction-finetuned language models. CoRR, abs/2210.11416, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL-HLT\n(1), pp. 2924\u20132936. Association for Computational Linguistics, 2019.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.\nELECTRA: pre-\ntraining text encoders as discriminators rather than generators. In ICLR. OpenReview.net, 2020.\nTogether Computer. Redpajama: an open dataset for training large language models, 2023. URL\nhttps://github.com/togethercomputer/RedPajama-Data.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large\nlanguage model with integrated external knowledge bases. CoRR, abs/2306.16092, 2023.\nFranck Dernoncourt and Ji Young Lee. Pubmed 200k RCT: a dataset for sequential sentence clas-\nsification in medical abstracts. In IJCNLP, pp. 308\u2013313. Asian Federation of Natural Language\nProcessing, 2017.\n11\nPublished as a conference paper at ICLR 2024\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT (1), pp. 4171\u20134186. As-\nsociation for Computational Linguistics, 2019.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn IWP@IJCNLP. Asian Federation of Natural Language Processing, 2005.\nOndrej Dusek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural\nlanguage generation. In INLG, pp. 421\u2013426. Association for Computational Linguistics, 2019.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\nAn 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021.\nYuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang. Learning instructions with unlabeled data for\nzero-shot cross-task generalization. In EMNLP, pp. 1617\u20131634. Association for Computational\nLinguistics, 2022.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. In ACL (1), pp.\n4849\u20134870. Association for Computational Linguistics, 2023.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey\nLevine, and Dawn Song. The false promise of imitating proprietary llms. CoRR, abs/2305.15717,\n2023.\nSuchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In\nACL, pp. 8342\u20138360. Association for Computational Linguistics, 2020.\nTianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser,\nAlexander L\u00a8oser, Daniel Truhn, and Keno K. Bressem. Medalpaca - an open-source collection of\nmedical conversational AI models and training data. CoRR, abs/2304.08247, 2023.\nPeter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky,\nand Daniel E. Ho. Pile of law: Learning responsible data filtering from the law and a 256gb\nopen-source legal dataset. In NeurIPS, 2022.\nQuzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu, and\nYansong Feng. Lawyer llama technical report. CoRR, abs/2305.15062, 2023.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-\nease does this patient have? A large-scale open domain question answering dataset from medical\nexams. CoRR, abs/2009.13081, 2020.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: A\ndataset for biomedical research question answering. In EMNLP/IJCNLP (1), pp. 2567\u20132577.\nAssociation for Computational Linguistics, 2019.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLook-\ning beyond the surface: A challenge set for reading comprehension over multiple sentences. In\nNAACL-HLT, pp. 252\u2013262. Association for Computational Linguistics, 2018.\nJens Kringelum, Sonny Kim Kj\u00e6rulff, S\u00f8ren Brunak, Ole Lund, Tudor I. Oprea, and Olivier\nTaboureau.\nChemprot-3.0: a global chemical biology diseases mapping.\nDatabase J. Biol.\nDatabases Curation, 2016, 2016.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In EMNLP (Demonstration), pp. 66\u201371.\nAssociation for Computational Linguistics, 2018.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput.\nLinguistics, 7:452\u2013466, 2019.\n12\nPublished as a conference paper at ICLR 2024\nJieh-Sheng Lee. Lexgpt 0.1: pre-trained GPT-J models with pile of law. CoRR, abs/2306.05431,\n2023.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assis-\ntant for biomedicine in one day. CoRR, abs/2306.00890, 2023a.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain knowledge. CoRR, abs/2303.14070, 2023b.\nWing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \u201dTeknium\u201d.\nOpenorca: An open dataset of gpt augmented flan reasoning traces.\nhttps://https://\nhuggingface.co/Open-Orca/OpenOrca, 2023.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and\nXiang Ren. Commongen: A constrained text generation challenge for generative commonsense\nreasoning. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp. 1823\u20131840.\nAssociation for Computational Linguistics, 2020.\nChen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy\nChowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu\nWang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Carl Yang, and\nLiang Zhao. Beyond one-model-fits-all: A survey of domain specialization for large language\nmodels. CoRR, abs/2305.18703, 2023.\nMarco Lippi, Przemyslaw Palka, Giuseppe Contissa, Francesca Lagioia, Hans-Wolfgang Micklitz,\nGiovanni Sartor, and Paolo Torroni. CLAUDETTE: an automated detector of potentially unfair\nclauses in online terms of service. Artif. Intell. Law, 27(2):117\u2013139, 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.\nLe, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and meth-\nods for effective instruction tuning. In ICML, volume 202 of Proceedings of Machine Learning\nResearch, pp. 22631\u201322648. PMLR, 2023a.\nShayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny\nZhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. A pretrainer\u2019s guide to\ntraining data: Measuring the effects of data age, domain coverage, quality, & toxicity. CoRR,\nabs/2305.13169, 2023b.\nShuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim,\nBarun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. Torchscale: Transformers at scale.\nCoRR, abs/2211.13184, 2022.\nMacedo Maia, Siegfried Handschuh, Andr\u00b4e Freitas, Brian Davis, Ross McDermott, Manel Zarrouk,\nand Alexandra Balahur. Www\u201918 open challenge: Financial opinion mining and question an-\nswering. In WWW (Companion Volume), pp. 1941\u20131942. ACM, 2018.\nPekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki Wallenius, and Pyry Takala. Good debt or\nbad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf. Sci. Technol., 65(4):\n782\u2013796, 2014.\nClara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Ef-\nfective transfer learning for identifying similar questions: Matching user questions to COVID-19\nfaqs. In KDD, pp. 3458\u20133465. ACM, 2020.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question answering. In EMNLP, pp. 2381\u20132391. Association\nfor Computational Linguistics, 2018.\n13\nPublished as a conference paper at ICLR 2024\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Hassan Awadallah. Orca: Progressive learning from complex explanation traces of GPT-\n4. CoRR, abs/2306.02707, 2023.\nLinyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto,\nJessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao\nYu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani.\nDART: open-domain structured data record to text generation. In NAACL-HLT, pp. 432\u2013447.\nAssociation for Computational Linguistics, 2021.\nCourtney Napoles, Matthew R. Gormley, and Benjamin Van Durme. Annotated gigaword. In AKBC-\nWEKEX@NAACL-HLT, pp. 95\u2013100. Association for Computational Linguistics, 2012.\nOpenAI. Gpt-4 technical report. Technical report, OpenAI, March 2023. URL https://cdn.\nopenai.com/papers/gpt-4.pdf.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In CHIL, volume 174\nof Proceedings of Machine Learning Research, pp. 248\u2013260. PMLR, 2022.\nFabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander H. Miller. Language models as knowledge bases? In EMNLP/IJCNLP (1),\npp. 2463\u20132473. Association for Computational Linguistics, 2019.\nAlec Radford and Karthik Narasimhan.\nImproving language understanding by generative pre-\ntraining. 2018.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP, pp. 2383\u20132392. The Association for Computational\nLinguistics, 2016.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions\nfor squad. In ACL (2), pp. 784\u2013789. Association for Computational Linguistics, 2018.\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical\nFormalizations of Commonsense Reasoning. AAAI, 2011.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Senevi-\nratne, Paul Gamble, Chris Kelly, Nathaneal Sch\u00a8arli, Aakanksha Chowdhery, Philip Andrew Mans-\nfield, Blaise Ag\u00a8uera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou,\nJuraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs,\nAlan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge.\nCoRR, abs/2212.13138, 2022.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin,\nSami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska,\nBlaise Ag\u00a8uera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara\nMahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh Azizi,\nAlan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering\nwith large language models. CoRR, abs/2305.09617, 2023.\nAnkur Sinha and Tanmay Khandait. Impact of news on the commodity market: Dataset and results.\nCoRR, abs/2009.04202, 2020.\nHarold J. Spaeth, Lee Epstein, Jeffrey A. Segal Andrew D. Martin, Theodore J. Ruger, and Sara C.\nBenesh. Supreme Court Database, Version 2020 Release 01. Washington University Law, 2020.\nURL http://Supremecourtdatabase.org.\n14\nPublished as a conference paper at ICLR 2024\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge. In NAACL-HLT (1), pp. 4149\u20134158.\nAssociation for Computational Linguistics, 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00b4elien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. CoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00b4elien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\nCoRR, abs/2307.09288, 2023b.\nDon Tuggener, Pius von D\u00a8aniken, Thomas Peetz, and Mark Cieliebak. LEDGAR: A large-scale\nmulti-label corpus for text classification of legal provisions in contracts. In LREC, pp. 1235\u2013\n1241. European Language Resources Association, 2020.\nMozes van de Kar, Mengzhou Xia, Danqi Chen, and Mikel Artetxe. Don\u2019t prompt, search! mining-\nbased zero-shot learning with language models.\nIn EMNLP, pp. 7508\u20137520. Association for\nComputational Linguistics, 2022.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nICLR (Poster). OpenReview.net, 2019.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:\nTuning llama model with chinese medical knowledge. CoRR, abs/2304.06975, 2023.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR.\nOpenReview.net, 2022.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In NAACL-HLT, pp. 1112\u20131122. Association for Com-\nputational Linguistics, 2018.\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further fine-\ntuning llama on medical papers. CoRR, abs/2304.14454, 2023a.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab-\nhanjan Kambadur, David S. Rosenberg, and Gideon Mann. Bloomberggpt: A large language\nmodel for finance. CoRR, abs/2303.17564, 2023b.\nHonglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and\nDinggang Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. CoRR,\nabs/2304.01097, 2023.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\nCoRR, abs/2304.12244, 2023.\n15\nPublished as a conference paper at ICLR 2024\nHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large\nlanguage models. CoRR, abs/2306.06031, 2023.\nYunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. Adapt-and-distill: Developing\nsmall, fast and effective pretrained language models for domains. In ACL/IJCNLP (Findings),\nvolume ACL/IJCNLP 2021 of Findings of ACL, pp. 460\u2013470. Association for Computational\nLinguistics, 2021.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence?\nIn ACL (1), pp. 4791\u20134800. Association for Computational\nLinguistics, 2019.\nRui Zhang and Joel R. Tetreault. This email could save your life: Introducing the task of email\nsubject line generation. In ACL (1), pp. 446\u2013456. Association for Computational Linguistics,\n2019.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassification. In NIPS, pp. 649\u2013657, 2015.\nYuan Zhang, Jason Baldridge, and Luheng He. PAWS: paraphrase adversaries from word scram-\nbling. In NAACL-HLT (1), pp. 1298\u20131308. Association for Computational Linguistics, 2019.\nLucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, and Daniel E. Ho. When does\npretraining help?: assessing self-supervised learning for law and the casehold dataset of 53, 000+\nlegal holdings. In ICAIL, pp. 159\u2013168. ACM, 2021.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\nLIMA: less is more for alignment. CoRR, abs/2305.11206, 2023.\n16\nPublished as a conference paper at ICLR 2024\nA\nDOMAIN KNOWLEDGE PROBING\nWe devise domain knowledge probing tests to determine whether continued training on the domain-\nspecific texts can enhance the model\u2019s domain-specific knowledge. Our probing test design is in-\nspired by LAMA (Petroni et al., 2019), where the task format closely resembles the pre-training\ntask. This allows us to analyze the model\u2019s inherent knowledge without altering its architecture\n(e.g., adding a model head) or parameters (e.g., fine-tuning). LAMA utilizes \u201cfill-in-the-blank\u201d\ncloze statements to match the masked language modeling task of BERT (Devlin et al., 2019). Sim-\nilarly, we create \u201cpredict-the-next-token/sentence\u201d tests to align with the casual language modeling\ntasks of generative language models (Radford & Narasimhan, 2018). Table 6 presents the knowl-\nedge probing results in the biomedicine and law domains. We observe that continued training on\ndomain-specific raw/reading comprehension texts indeed endows the large language model with\ndomain knowledge.\nTable 6: Domain knowledge probing results of general large language model (General LLM),\nthe model trained on domain-specific raw corpora (Raw Text) and the model trained on the reading\ncomprehension texts constructed based on the raw corpora (Read. Compre.).\nDomain\nGeneral LLM\nRaw Text\nRead. Compre.\nBioMed.\n36.5\n36.9\n36.8\nLaw\n45.0\n45.6\n46.4\nBiomedicine. To create a knowledge probing test for the biomedicine domain, we utilize the MedM-\nCQA (Pal et al., 2022) dataset. This dataset comprises numerous high-quality multiple-choice ques-\ntions, covering diverse healthcare topics and 21 medical subjects. To align the testing format with\ncasual language modeling, we remove data samples in the instruction-following format. These in-\nclude samples starting with question words like \u201cWhat\u201d, \u201cWho\u201d and \u201cWhen\u201d, or ending with \u201c:\u201d,\n\u201c?\u201d, and \u201c-\u201d. Additionally, samples having the fill-in-the-blank marker \u201c \u201d are also removed.\nThe evaluation is similar to zero-shot prompting: we feed into the model the raw data input, without\nany demonstrations, and then compare per-token-likelihood of each option to get the model predic-\ntion. This evaluation is conducted individually for each of the 21 medical subjects, and the average\nscore across all the subjects is reported.\nLaw. For the law domain knowledge probing, we employ the LEDGAR dataset (Tuggener et al.,\n2020). This dataset is designed for contract provision classification and encompasses a wide spec-\ntrum of 100 distinct law topics. Each label represents the principal topic of the given contract\nprovision. Originally structured as a 100-classification task, we adapt it for knowledge probing by\nsimplifying it into a four-choice option format. For each data sample, we preserve the label class\nand randomly select three additional classes to create the four candidate options.\nSimilar to biomedicine knowledge probing, we feed into the model the data input using the tem-\nplate \u201c{CONTRACT} The topic is\u201d, without any demonstrations, and then compare per-token-\nlikelihood of each option to get the model prediction. The evaluation is performed individually for\neach of the 100 law topics, and the average score across all the topics is reported.\nB\nDOMAIN-ADAPTIVE PRE-TRAINING\nTable 7 presents specifications of the pre-training corpora in each domain and Table 8 presents pre-\ntraining hyper-parameters. A <pad> token is added to the model vocabulary for sentence padding.\nOur pre-training code is based on TorchScale (Ma et al., 2022)3. In each domain, we explore dif-\nferent ratios for mixing domain-specific raw/reading comprehension texts with general instructions,\nspecifically considering ratios of 1 : 2, 1 : 1, and 2 : 1. The end-of-sentence token </s> is used to\nconcatenate between documents, where a document could be a raw text, a reading comprehension\ntext, or a general instruction.\n3https://github.com/microsoft/torchscale\n17\nPublished as a conference paper at ICLR 2024\nTable 7: Pre-training corpora.\nDomain\nData Source\nRaw Size\n# Tokens\n# Docs\nBioMed.\nPubMed Abstracts (Gao et al., 2021)\n19.3 GiB\n5.4 B\n15.5 M\nFinance\nStock News\n5.1 GiB\n1.2 B\n1.1 M\nLaw\nFreeLaw Opinions (Gao et al., 2021)\n51.2 GiB\n16.7 B\n3.6 M\nTable 8: Hyper-parameters of domain-adaptive pre-training.\nHyper-parameter\nAssignment\nComputing infrastructure\n32 V100-32GB GPUs\nRun-time\n24 Hours\nNumber of steps\n10,000\nBatch size\n32\nMaximum sequence length\n2,048\nMaximum learning rate\n1e-5\nOptimizer\nAdam\nAdam beta weights\n0.9, 0.95\nLearning rate scheduler\ncosine\nWeight decay\n0.1\nWarm-up steps\n1000\nGradient clipping\n1.0\nDropout ratio\n0.1\nC\nCREATING READING COMPREHENSION TEXTS\nTitle Collection for Summarization Tasks. In the biomedicine domain, the title for each raw text\nin PubMed Abstracts (Gao et al., 2021) is the first sentence within the text, separated from other\nsentences by a newline character \\n. In the finance domain, we specifically collect the titles when\ngathering news using the FinGPT codebase (Yang et al., 2023). In FreeLaw Opinions corpora (Gao\net al., 2021) of the law domain, there are no explicit titles for individual raw texts. Instead, titles are\navailable for some sections within the raw text. Hence, we initially divide each raw text into sections\nand gather as many titles as possible from these sections. Subsequently, we consider one section,\nrather than an entire raw text, as the basis for creating one reading comprehension text.\nRegex Pattern Implementation. Before mining task examples, we truncate each raw text to its ini-\ntial 1,800 tokens, enabling the insertion of comprehension tasks within a maximum sequence length\nof 2,048. For tasks which employ regex patterns to mine task examples, we fill in the patterns with\nthe corresponding verbalizer and identify sentences that match the patterns. This process of ex-\npanding patterns into regular expressions follows van de Kar et al. (2022): {VERBAL} is substituted\nwith a capturing group that incorporates all verbalizers, separated by the alternation operator |. For\ninstance, the verbalizer set Therefore, Thus, Hence is expanded to (Therefore|Thus|Hence).\nSubsequently, the keywords listed in Table 9 are replaced with the corresponding regular expres-\nsions. The result is a regular expression containing capturing groups for extracting sentences.\nD\nDOMAIN-SPECIFIC TASKS EVALUATION\nPrompting. In prompting evaluation, each task corresponds to multiple prompt templates and we\nrandomly sample one of them for each data example, to mitigate result variance caused by template\nsensitivity. Prompt template examples are presented in Table 11. Following Brown et al. (2020), we\nclassify tasks into two question types to get model predictions: 1) For multiple-choice questions,\nwe compare the per-token likelihood of each option to determine the model prediction; 2) For text\ncompletion questions, we employ greedy search to get the free-form answer.\n18\nPublished as a conference paper at ICLR 2024\nTable 9: Keywords that compile into regular expressions. These keywords are used in the mining\npatterns and verbalizers (van de Kar et al., 2022).\nKeyword\nRegex\n{VERBAL}\nReplaced with the verbalizer\n{WORD}\nregex: ([\u02c6.!?\\n,;\\\"\\s]{10,})\nMatches a single word having more than 9 characters\n{SENT}\nregex: ([\u02c6.!?\\n]{50,}[.!?]+)\nMatches a single sentence having more than 50 characters\nOur prompting settings in the finance domain follow BloombergGPT (Wu et al., 2023b), with the\nexception that we use multiple templates to address template sensitivity. The prompt templates for\nlaw domain are based on Chalkidis (2023). The UNFAIR-ToS (Lippi et al., 2019) task is a multi-\nlabel classification task. To get model predictions for this task, we categorize it as a multiple-choice\nquestion, and the accuracy of an individual data example is considered true if the model prediction\n(i.e., the option with the highest per-token likelihood) belongs to the label(s) set. In the biomedicine\ndomain, some classification tasks, including MQP (McCreery et al., 2020), RCT (Dernoncourt &\nLee, 2017), and ChemProt (Kringelum et al., 2016), are too challenging for the model, thus we\nconduct few-shot prompting and maintain the number of demonstrations the same in each class.\nFine-tuning. In fine-tuning, we utilize a fixed prompt template (the one displayed in Table 11) for\neach task to convert data input-output into question-answering pairs. The model is then trained on\nthese pairs for one epoch with the warm-up step as 0. All other training settings are the same with\ndomain-adaptive pre-training. Fine-tuning evaluation is similar to prompting evaluation, but with\ntwo differences to align with the fine-tuning training stage: no demonstration is presented before the\nprompt input, and the prompt template is the same with the one used in the training stage.\nTable 10: Specifications of the domain-specific task datasets. # Demos is the number of demon-\nstrations in prompting evaluation.\nTask\nType\nMetric\n# Demos\nBioMed.\nMQP (McCreery et al., 2020)\nBinary classification\nAccuracy\n4\nPubMedQA (Jin et al., 2019)\nMulti-class classification\nAccuracy\n0\nUSMLE (Jin et al., 2020)\nMulti-chioice QA\nAccuracy\n0\nRCT (Dernoncourt & Lee, 2017)\nMulti-class classification\nMicro F1\n10\nChemProt (Kringelum et al., 2016)\nMulti-class classification\nMicro F1\n13\nFinance\nFiQA SA (Maia et al., 2018)\nMulti-class classification\nWeighted F1\n5\nFPB (Malo et al., 2014)\nMulti-class classification\nWeighted F1\n5\nNER (Alvarado et al., 2015)\nNamed entity recongnition\nEntity-level F1\n20\nHeadline (Sinha & Khandait, 2020)\nBinary-class classification\nWeighted F1\n5\nConvFinQA (Chen et al., 2022)\nQuestion Answering\nExact Match\n0\nLaw\nSCOTUS (Spaeth et al., 2020)\nMulti-class classification\nMicro/Macro F1\n0\nCaseHOLD (Zheng et al., 2021)\nMulti-chioice QA\nMicro/Macro F1\n0\nUNFAIR-ToS (Lippi et al., 2019)\nMulti-label classification\nAccuracy\n4\n19\nPublished as a conference paper at ICLR 2024\nTable 11: Prompt templates. Each template example is paraphrased to multiple variations for\nprompting evaluation.\nTask\nTemplate\nBioMed.\nMQP\nQuestion 1: {QUESTION1}\nQuestion 2: {QUESTION2}\nAre questions 1 and 2 asking the same thing? {ANSWER}\nPubMedQA\nContext: {CONTEXT}\nQuestion: {QUESTION}\nAnswer: {ANSWER}\nUSMLE\nQuestion: {QUESTION}\nAnswer: {ANSWER}\nRCT\n{SENTENCE}\nQuestion: what is the role of this sentence in an abstract?\nAnswer: {ANSWER}\nChemProt\n{SENTENCE}\nQuestion: what is the relation?\nAnswer: {ANSWER}\nFinance\nFiQA SA\n{SENTENCE}\nQuestion: what is the sentiment on {TARGET}?\nAnswer: {ANSWER}\nFPB\n{SENTENCE}\nQuestion: what is the sentiment?\nAnswer: {ANSWER}\nNER\n{SENTENCE}\nExtract named entity: {ANSWER}\nHeadline\n{SENTENCE}\nQuestion: {QUESTION}\nAnswer: {ANSWER}\nConvFinQA\n{CONTEXT}\n{PREVIOUS QAS}\n{QUESTION} {ANSWER}\nLaw\nSCOTUS\nGiven the following opinion from the Supreme Court of USA (SCOTUS):\n\"{TEXT}\"\nThe relevant issue area is: {ANSWER}\nCaseHOLD\nComplete the following excerpt from a US court opinion:\n{CONTEXT}: {ANSWER}\nUNFAIR-ToS\nGiven the following sentence from an online Term of Services:\n\"{SENTENCE}\"\nThe sentence is unfair with respect to: {ANSWER}\n20\nPublished as a conference paper at ICLR 2024\nE\nFURTHER ABLATIONS ON COMPREHENSION TASKS\nFigure 4 presents the percentages of mined examples of each task type in all the comprehension\ntask examples, with Word-To-Text, Summarization, and Text Completion accounting for the highest\nratios.\nSummarize, \n26.3 \nNLI, 2.2 \nWord-to-\nText, 50.3 \nCommon \nReason, 0.3 Text \nComple., \n17.4 \nParaphrase, 3.5 \nSummarize, \n31.4 \nNLI, 3.8 \nWord-to-\nText, 35.8 \nCommon \nReason, \n0.5 \nText \nComple., \n20.7 \nParaphrase, 7.8 \nSummarize, \n7.2 NLI, 2.7 \nWord-to-\nText, 62.3 \nCommon \nReason, \n1.0 \nText \nComple., \n22.0 \nParaphrase, \n4.9 \nBiomedicine\nFinance\nLaw\nFigure 4: Percentages of mined examples of each task type in all the comprehension task ex-\namples.\nIn the biomedicine domain, we conduct ablations on each comprehension task type by systematically\nremoving each task type from the reading comprehension texts. We then use the resulting modified\nreading comprehension texts to train the general model. Subsequently, we evaluate these trained\nmodels on both domain-specific tasks and general LLM benchmarks to analyze the impacts of these\nablations.\nDomain Tasks\nGeneral Tasks\n40\n41\n42\n43\n44\n45\nAll\nw/o\nSumm.\nw/o\nWord.\nw/o\nNLI\nw/o\nCommon.\nw/o\nPara.\nw/o\nText\nCom.\nDomain Task Score\n0\n20\n40\n60\n80\nSummarize Word-to-Text\nNLI\nCommon\nReason\nParaphrase\nText\nCompletion\nGeneral  Task  Score\nAll\nw/o\nSumm.\nAll\nw/o\nWord.\nAll\nw/o\nNLI\nAll\nw/o\nComm.\nAll\nw/o\nPara.\nAll\nw/o\nText.\nFigure 5: Prompting scores of domain-specific tasks (left) and general LLM benchmarks\n(right) of models trained with different comprehension tasks. All denotes the model trained with all\nthe comprehension tasks, while w/o Summ. represents the model trained with the comprehension\ntasks excluding Summarization tasks, w/o Word. represents the model trained with the comprehen-\nsion tasks excluding Word-to-Text tasks, and so on. We report the average task scores within each\ndomain/type.\nDomain-specific Tasks. As shown in Figure 5, when evaluating on the domain-specific tasks, the\nremoval of any comprehension task type leads to a decrease in task performance, showing their con-\ntributions to these domain-specific tasks. Notably, removing Word-to-Text, Summarization, or Text\nCompletion tasks results in a noticeable drop in performance, aligning with the high percentages of\nthese tasks in the mined examples.\nInterestingly, even though the Natural Language Inference task type doesn\u2019t constitute a large por-\ntion of the comprehension tasks, its removal leads to a substantial decrease in performance. This\ncould be attributed to its unique role as the only classification task type within all the comprehension\n21\nPublished as a conference paper at ICLR 2024\ntasks. In contrast, the impact of removing Commonsense Reasoning and Paraphrase Detection tasks\nis less pronounced, reflecting their lower percentages in the mined task examples. However, this\nalso suggests the potential benefits of including more diverse task examples, which could further\nenhance domain-specific task performance.\nGeneral LLM Benchmarks. Additionally, we conduct experiments where we remove a specific\ntask type from all the comprehension tasks. We then evaluate the trained model\u2019s performance\nspecifically on the general tasks corresponding to the removed task type, aiming to demonstrate\nwhether the comprehension tasks have a positive impact on the respective downstream tasks.\nIn the results for general tasks in Figure 5, when we exclude a particular task type from the com-\nprehension tasks, we observe performance declines in the corresponding downstream tasks, specifi-\ncally for Summarization, Word-to-Text, Natural Language Inference, and Commonsense Reasoning\ntasks. This suggests a beneficial connection between the trained comprehension tasks and their\ncorresponding downstream tasks.\nHowever, when we remove Paraphrase Detection or Text Completion, it does not lead to a perfor-\nmance decline in the corresponding tasks. This discrepancy may be attributed to the reformatting\nof Paraphrase Detection from a classification task to a generation task in the comprehension tasks,\ncausing a mismatch between training and evaluation settings. Furthermore, the Text Completion\ncomprehension task type lacks obvious counterparts in the general LLM benchmarks, which may\ncontribute to the mismatch in the performance change trend.\nF\nANALYSIS OF DOMAIN KNOWLEDGE AND PROMPTING ABILITY\nTable 12: Fine-tuning performance on the domain-specific tasks of the general large language\nmodel (General LLM), the model trained on domain-specific raw corpora (Raw Text) and the\nmodel trained on the reading comprehension texts constructed based on the raw corpora (Read.\nCompre.).\nBioMed.\nPubMedQA\nChemProt\nMQP\nRCT\nUMSLE\nAVERAGE\nGeneral LLM\n75.4\n64.6\n55.4\n87.0\n38.5\n64.2\nRaw Text\n76.2\n64.8\n65.6\n87.0\n39.0\n66.5\nRead. Compre.\n76.0\n65.4\n87.9\n87.5\n41.0\n71.5\nFinance\nConvFinQA\nFPB\nFiQA SA\nHeadline\nNER\nAVERAGE\nGeneral LLM\n58.1\n81.9\n86.4\n95.7\n77.5\n79.9\nRaw Text\n56.2\n83.3\n87.9\n95.8\n81.3\n80.9\nRead. Compre.\n57.2\n88.6\n83.1\n96.1\n82.5\n81.5\nLaw\nSCOTUS\nCaseHOLD\nUNFAIR-ToS\nAVERAGE\nmic-F1\nmac-F1\nmic-F1\nmac-F1\nGeneral LLM\n31.7\n14.0\n35.3\n35.3\n93.8\n42.0\nRaw Text\n36.7\n26.0\n35.4\n35.4\n93.7\n45.4\nRead. Compre.\n40.0\n26.0\n35.5\n35.5\n94.2\n46.2\n22\nPublished as a conference paper at ICLR 2024\nTable 13: Prompting results on general LLM benchmarks. Raw trains on the raw corpora,\nand Read trains on the reading comprehension texts constructed based on the raw corpora. Text\nCompletion is more close to a question type than a task type in general benchmarks, so we report the\naverage of all the tasks following the free-form text completion question type. Each task corresponds\nto multiple prompt templates taken from FLAN (Wei et al., 2022), and we remove the option suffixes\nfrom the templates (Cheng et al., 2023) to fit for the prompting evaluation approach by Brown et al.\n(2020).\nTask\nMetric\nGeneral\nLLM\nBioMed.\nFinance\nLaw\nRaw\nRead\nRaw\nRead\nRaw\nRead\nSummarization\nAGNews (Zhang et al., 2015)\nAcc\n58.7\n51.7\n55.5\n56.1\n50.1\n57.8\n60.6\nAESLC (Zhang & Tetreault, 2019)\nR-1\n1.5\n3.6\n7.5\n1.9\n10.8\n3.4\n7.4\nR-2\n0.2\n0.9\n2.8\n0.3\n3.8\n0.8\n2.7\nR-L\n1.5\n3.6\n7.2\n1.8\n10.3\n3.3\n7.2\nGigaword (Napoles et al., 2012)\nR-1\n0.6\n3.8\n9.3\n3.1\n13.2\n3.4\n9.8\nR-2\n0.1\n0.7\n2.4\n0.6\n3.8\n0.6\n2.5\nR-L\n0.6\n3.5\n8.5\n2.9\n12.1\n3.1\n8.9\nWord-to-Text\nCommonGen (Lin et al., 2020)\nR-1\n14.2\n16.1\n24.9\n17.2\n24.2\n17.8\n27.9\nR-2\n0.0\n1.6\n5.9\n1.5\n6.2\n2.5\n7.2\nR-L\n14.2\n15.4\n22.2\n16.2\n21.1\n16.7\n24.3\nDART (Nan et al., 2021)\nR-1\n17.0\n21.3\n20.0\n21.9\n22.3\n23.6\n19.5\nR-2\n3.7\n6.0\n5.0\n6.7\n7.1\n7.5\n5.9\nR-L\n16.0\n19.6\n18.5\n20.1\n20.5\n21.4\n18.0\nE2ENLG (Dusek et al., 2019)\nR-1\n14.3\n18.4\n27.6\n22.6\n37.3\n18.4\n28.3\nR-2\n3.2\n5.2\n9.8\n7.5\n15.1\n5.8\n10.5\nR-L\n13.4\n17.1\n24.5\n20.4\n32.9\n17.0\n25.5\nNatural Language Inference\nMNLI-m (Williams et al., 2018)\nAcc\n35.6\n35.0\n36.7\n36.2\n37.4\n34.8\n34.9\nMNLI-mm (Williams et al., 2018)\nAcc\n34.9\n34.9\n36.1\n36.8\n37.2\n35.7\n34.4\nQNLI (Rajpurkar et al., 2018)\nAcc\n51.1\n52.4\n53.7\n52.1\n52.0\n50.7\n51.9\nRTE (Bentivogli et al., 2009)\nAcc\n30.7\n9.4\n30.0\n23.1\n26.0\n18.8\n32.9\nSNLI (Bowman et al., 2015)\nAcc\n34.0\n34.5\n33.8\n36.8\n38.1\n34.7\n33.9\nCommonsense Reasoning\nCOPA (Roemmele et al., 2011)\nAcc\n71.0\n68.0\n75.0\n74.0\n73.0\n72.0\n74.0\nPIQA (Bisk et al., 2020)\nAcc\n71.7\n70.9\n71.3\n71.6\n71.7\n71.9\n71.8\nHellaSwag (Zellers et al., 2019)\nAcc\n71.8\n71.8\n72.6\n72.5\n73.0\n72.0\n72.5\nParaphrase Detection\nMRPC (Dolan & Brockett, 2005)\nAcc\n35.5\n31.6\n34.8\n33.3\n37.3\n34.1\n36.0\nF1\n17.0\n0.0\n13.6\n9.9\n22.4\n8.8\n19.7\nQQP (Wang et al., 2019)\nAcc\n56.0\n62.7\n61.1\n52.5\n54.3\n60.6\n59.9\nF1\n34.0\n3.0\n10.7\n33.1\n44.5\n15.3\n14.4\nPaws Wiki (Zhang et al., 2019)\nAcc\n54.8\n55.6\n54.9\n53.7\n53.7\n55.4\n54.8\nClosed-book QA\nARC-c (Bhakthavatsalam et al., 2021)\nAcc\n37.3\n36.7\n39.0\n37.8\n38.8\n37.6\n39.3\nARC-e (Bhakthavatsalam et al., 2021)\nAcc\n58.5\n59.2\n63.1\n59.4\n62.3\n60.0\n62.9\nNQ (Kwiatkowski et al., 2019)\nEM\n2.2\n0.1\n0.5\n0.1\n0.0\n0.1\n0.1\nF1\n3.4\n0.8\n1.9\n1.2\n2.1\n1.3\n1.8\nCMSQA (Talmor et al., 2019)\nAcc\n39.6\n40.4\n43.5\n40.0\n42.6\n40.9\n44.6\nReading Comprehension\nBoolQ (Clark et al., 2019)\nAcc\n55.7\n42.6\n50.7\n53.4\n55.8\n51.0\n53.9\nOBQA (Mihaylov et al., 2018)\nAcc\n46.2\n45.8\n46.4\n46.0\n47.0\n45.4\n47.2\nSQuADv1 (Rajpurkar et al., 2016)\nEM\n0.1\n1.0\n3.5\n0.2\n0.0\n0.3\n2.0\nF1\n0.2\n4.6\n10.0\n0.6\n7.3\n6.3\n10.7\nMultiRC (Khashabi et al., 2018)\nAcc\n55.9\n47.5\n53.0\n49.9\n49.7\n52.5\n52.4\n23\nPublished as a conference paper at ICLR 2024\nG\nCASES OF READING COMPREHENSION TEXTS\nHere is the first part of an article about biomedicine: Recent reported\nevidence indicates that vocal cord carcinoma is evolving similarly to\noropharyngeal cancer with an increasing number of patients without a\nsmoking history having human papillomavirus (HPV) disease. Observations\nalso suggest that an increasing number of patients who present with glottic\ncarcinoma are younger than has been reported in the past. (\u2026TL; DR...)\nHowever, recent published evidence shows that glottic carcinoma can be an\nHPV-related disease with increasing incidence in nonsmokers. It isn't\nsurprising that alternate malignant pathways may have a different timeline.\nAnswer questions based on the article:\nWhat is a summary? Glottic Carcinoma in Young Patients.\nGenerate a sentence that includes these biomedicine words [carcinoma,\noropharyngeal, papillomavirus]: Recent reported evidence indicates that\nvocal cord carcinoma is evolving similarly to oropharyngeal cancer with an\nincreasing number of patients without a smoking history having human\npapillomavirus (HPV) disease.\nPremise: Observations also suggest that an increasing number of\npatients who present with glottic carcinoma are younger than has been\nreported in the past.\nHypothesis: An investigation was done to examine the incidence of\nglottic carcinoma in patients 30 years old (y/o) or younger.\nDoes the premise entail the hypothesis? Yes\nWhat is the reason for \"None of the 11 glottic cancer patients had been\npreviously treated for benign recurrent respiratory papillomatosis\nalthough it was initially suspected prior to biopsy\"? The morphology of\nthe lesions and the patients' young age.\nCompose a sentence that contradicts the meaning of \"Historically,\nglottic carcinoma is considered to be a tobacco-induced disease.\u201d.\nAnswer: Recent published evidence shows that glottic carcinoma can be an\nHPV-related disease with increasing incidence in nonsmokers.\nHow would you complete the article? This finding further supports the\nconcept that glottic carcinoma is an evolving disease, and it demonstrates\nthe increasing importance of discriminating potential glottic carcinomas in\nyoung\npatients\nfrom\nbenign\nlow-risk\nHPV\nrecurrent\nrespiratory\npapillomatosis.\nText Completion\nSummarization\nWord-to-Text\nNatural Language\nInference\nParaphrase\nDetection\nCommonsense\nReasoning\nRaw Text\nFigure 6: An example of a reading comprehension text constructed from a raw text. The\nunderlined sentence is added to guide the model to answer questions based the given context.\n24\nPublished as a conference paper at ICLR 2024\nTable 14: A case of a reading comprehension text in biomedicine domain. Certain portions are\nomitted for brevity and are represented as (...).\nPancreastatin (PST), a chromogranin A-derived peptide, has been found to modulate glucose,\nlipid, and protein metabolism in rat adipocytes. PST has an overall counterregulatory effect on\ninsulin action by activating a specific receptor-effector system (Galpha(q/11) protein-PLC-beta-\nPKC(classical)). However, PST stimulates both basal and insulin-mediated protein synthesis in rat\nadipocytes. In order to further investigate the mechanisms underlying the effect of PST stimulating\nprotein synthesis, we sought to study the regulation of different components of the core translational\nmachinery by the signaling triggered by PST. Thus, we studied ribosomal p70 S6 kinase, phosphory-\nlation of the cap-binding protein (initiation factor) eIF4E, and phosphorylation of the eIF4E-binding\nprotein 4E-BP1 (PHAS-I). We have found that PST stimulates the S6 kinase activity, as assessed by\nkinase assay using specific immunoprecipitates and substrate. This effect was checked by Western\nblot with specific antibodies against the phosphorylated S6 kinase. Thus, PST dose-dependently\nstimulates Thr421/Ser424 phosphorylation of S6 kinase. Moreover, PST promotes phosphorylation\nof regulatory sites in 4E-BP1 (PHAS-I) (Thr37, Thr46). The initiation factor eIF4E itself, whose\nactivity is also increased upon phosphorylation, is phosphorylated in Ser209 by PST stimulation.\n(...)\nUse evidence from the biomedicine article to answer these questions:\nAssess the relationship between Sentence 1: \u201cThis effect was checked by Western blot with specific\nantibodies against the phosphorylated S6 kinase.\u201d\nSentence 2: \u201cPST dose-dependently stimulates Thr421/Ser424 phosphorylation of S6 kinase.\u201d\nIs it characterized as Entailment, Neutral, or Contradiction? Entailment\nAssess the relationship between Sentence 1: \u201cPST has an overall counterregulatory effect on\ninsulin action by activating a specific receptor-effector system (Galpha(q/11) protein-PLC-beta-\nPKC(classical)).\u201d\nSentence 2: \u201cPST stimulates both basal and insulin-mediated protein synthesis in rat adipocytes.\u201d\nIs it characterized as Entailment, Neutral, or Contradiction? Contradiction\nNext question: What is the reason of the following sentence?\nWe studied ribosomal p70 S6 kinase, phosphorylation of the cap-binding protein (initiation factor)\neIF4E, and phosphorylation of the eIF4E-binding protein 4E-BP1 (PHAS-I). In order to further\ninvestigate the mechanisms underlying the effect of PST stimulating protein synthesis, we sought\nto study the regulation of different components of the core translational machinery by the signaling\ntriggered by PST.\nNext question: What is the reason of the following sentence?\nPST dose-dependently stimulates Thr421/Ser424 phosphorylation of S6 kinase. This effect was\nchecked by Western blot with specific antibodies against the phosphorylated S6 kinase.\n\u201cPST has an overall counterregulatory effect on insulin action by activating a specific receptor-\neffector system (Galpha(q/11) protein-PLC-beta-PKC(classical)).\u201d Generate a sentence that ex-\npresses a contrasting idea to the previous statement. PST stimulates both basal and insulin-mediated\nprotein synthesis in rat adipocytes.\nHere are some concepts about biomedicine: mechanisms, regulation, translational.\nGenerate a detailed description of them In order to further investigate the mechanisms underlying\nthe effect of PST stimulating protein synthesis, we sought to study the regulation of different com-\nponents of the core translational machinery by the signaling triggered by PST.\nHere are some concepts about biomedicine: phosphorylation, stimulation, phosphorylated.\nGenerate a detailed description of them The initiation factor eIF4E itself, whose activity is also\nincreased upon phosphorylation, is phosphorylated in Ser209 by PST stimulation.\nBriefly summarize this text. Pancreastatin, a chromogranin A-derived peptide, activates protein\nsynthesis signaling cascade in rat adipocytes.\n25\nPublished as a conference paper at ICLR 2024\nTable 15: A case of a reading comprehension text in finance domain. Certain portions are omitted\nfor brevity and are represented as (...).\nRead the beginning of an article on finance: In this article, we discuss the 12 biggest commer-\ncial janitorial companies in USA. If you want to skip our detailed analysis of these companies, go\ndirectly to the 5 Biggest Commercial Janitorial Companies In USA. According to Statista, the jani-\ntorial services sector\u2019s market size will increase by 6.6 percent in 2022. The annualized percentage\nof this market\u2019s growth was 6.7% between 2017 and 2022. Additionally, between 2020 and 2021,\nthe number of janitors and cleaners employed in the United States rose by nearly 50,000. By 2024,\nit is predicted that the U.S. janitorial services industry will generate around $50.1 billion in sales.\nThe market size, measured by revenue, of the janitorial services industry is $97.6 billion in 2022.\nBusinesses frequently grow when corporate profits increase, raising demand for janitorial services.\nHowever, dwindling corporate profit decreases demand for janitorial services as companies close fa-\ncilities and cut back on the frequency of contracted cleaning to cut expenses. The industry may have\nan opportunity in 2022 as corporate profits are anticipated to rise, according to ibisworld. According\nto Allied Market Research, the cleaning services market is to reach $111.49 billion globally by 2030\nat a 6.5% CAGR, as reported by Bloomberg. The global cleaning services industry is expanding\ndue to service providers expanding their online presence and rising commercial consumer demand.\nHowever, heightened rivalry and the introduction of new companies limit market expansion. On the\nother hand, the demand for construction and post-construction cleaning services will open up new\nprospects. The Covid-19 pandemic has had a favorable effect on the global cleaning services busi-\nness. Deep cleaning and disinfection have become more popular among residential and commercial\nusers to reduce the risk of cross-contamination. (...)\nHow would you extend the article? Moore adds that Casella Waste Systems, Inc. (NASDAQ:CWST)\nis also the only public waste company its size not to be acquired, which provides \u201ddownside support\nto the valuation on a takeout potential\u201d. 09. Stericycle, Inc. (NASDAQ: SRCL) Market Capitaliza-\ntion as of November 18, 2022: $4.73 billion Stericycle, Inc. (NASDAQ:SRCL) was incorporated in\n1989 and is located in Bannockburn, Illinois. (...)\nThen, answer the following questions based on the whole article:\nPlease write a short summary for the above article. 12 Biggest Commercial Janitorial Companies In\nUSA\nGiven the sentence: The global cleaning services industry is expanding due to service providers\nexpanding their online presence and rising commercial consumer demand.\nSelect the most suitable word to connect it with: \u201cHeightened rivalry and the introduction of new\ncompanies limit market expansion.\u201d\nOptions: Therefore, However, Maybe However\nAnswer the following question. The global cleaning services industry is expanding due to? Service\nproviders expanding their online presence and rising commercial consumer demand.\nGiven the sentence \u201cBusinesses frequently grow when corporate profits increase, raising demand\nfor janitorial services.\u201d, come up with a different sentence that contradicts its meaning. Dwindling\ncorporate profit decreases demand for janitorial services as companies close facilities and cut back\non the frequency of contracted cleaning to cut expenses.\nGiven the sentence \u201cThe global cleaning services industry is expanding due to service providers\nexpanding their online presence and rising commercial consumer demand.\u201d, come up with a different\nsentence that contradicts its meaning. Heightened rivalry and the introduction of new companies\nlimit market expansion.\nGenerate a sentence that includes the following finance keywords: Capitalization, Incorporated,\nIndustries ABM Industries Incorporated (NYSE: ABM) Market Capitalization as of November 18,\n2022: $2.98 billion ABM Industries Incorporated (NYSE:ABM) was incorporated in 1985 and is\nbased in New York, New York.\n26\nPublished as a conference paper at ICLR 2024\nTable 16: A case of a reading comprehension text in law domain. Certain portions are omitted\nfor brevity and are represented as (...).\nHere is the first part of an article about law: The district court ordered Arledge to pay restitution\nin the amount of $5,829,334.90, without interest, to the Settlement Fund pursuant to the Mandatory\nVictims Restitution Act of 1996 (\u201dMVRA\u201d), 18 U.S.C. \u00a73663A. Arledge disputes the calculation\nused to determine the amount of loss, claiming that the government did not sufficiently prove that\nthe Settlement Fund had paid false claims arising from Arledge\u2019s illegal conduct. Specifically, he\nobjects to the calculation of losses related to the Fen Phen II settlement.\nThe \u201cgeneral rule is that a district court can award restitution to victims of the offense, but the\nrestitution award can encompass only those losses that resulted directly from the offense for which\nthe defendant was convicted.\u201d United States v. Maturin, 488 F.3d 657, 660-61 (5th Cir. 2007)\n(citing Hughey v. United States, 495 U.S. 411, 413, 110 S.Ct. 1979, 109 L.Ed.2d 408 (1990)). The\npre-sentence report attributed forty-seven fraudulent claims to the offenses for which Arledge was\nconvicted. There were three categories of evidence used to substantiate the government\u2019s assertion\nthat these claims resulted from Arledge\u2019s illegal conduct: (1) the testimony of Wyatt, an employee\nof S&A, who created fraudulent documents; (...)\nPlease complete it: We have stated repeatedly that an order of restitution must be limited to losses\ncaused by the specific conduct underlying the offense of conviction. See United States v. Griffin,\n324 F.3d 330, 367 (5th Cir.2003) (holding that restitution is restricted to the limits of the offense);\nTencer, 107 F.3d at 1135-36. Accordingly, we decline the government\u2019s request to adopt a harmless\nerror analysis for the calculation of loss under the MVRA, and we remand for a re calculation of\nactual loss based upon the evidence in the record.\nThen, answer the following questions based on the whole article:\nBased on the statement Arledge makes no convincing argument regarding why the identification by\nAHP is not sufficient or how the district court abused its discretion in relying upon the represen-\ntations by AHP., provide a continuation using the word \u201cHence\u201d to express the following idea. It\nwas not an abuse of discretion for the court to award restitution that encompassed those losses re-\nsulting from the creation of fraudulent documents in furtherance of the scheme to defraud for which\nArledge was convicted.\n(...)\nWhat happens before \u201cThe government asks this court to find that the failure to provide proof of\nthese three claims was harmless error.\u201d? It also asserts that it would be able to provide, on remand,\nproof of an additional false claim that was not originally included in the district court\u2019s restitution\norder, which resulted in a loss of $488,000.\nIn opposition to the sentence \u201cThus, it was not an abuse of discretion for the court to award restitution\nthat encompassed those losses resulting from the creation of fraudulent documents in furtherance of\nthe scheme to defraud for which Arledge was convicted.\u201d, create a sentence with a contradictory\nmeaning. There are three claims for which the government concedes that there was no proof of\nfalsity: Florestine Baker ($18,000), Shirley E.\nProduce a long descriptive sentence about law that uses all these words: prescription, manufactured,\nfraudulent. There were three categories of evidence used to substantiate the government\u2019s assertion\nthat these claims resulted from Arledge\u2019s illegal conduct: (1) the testimony of Wyatt, an employee\nof S&A, who created fraudulent documents; (2) the testimony of two pharmacists who testified that\nspecific prescriptions allegedly from their pharmacies were, in fact, manufactured; and (3) represen-\ntations by AHP that the claims were fraudulent.\nProduce a long descriptive sentence about law that uses all these words: restitution, fraudulent, fur-\ntherance. Thus, it was not an abuse of discretion for the court to award restitution that encompassed\nthose losses resulting from the creation of fraudulent documents in furtherance of the scheme to\ndefraud for which Arledge was convicted.\nWhat was this article about? Amount of Loss\n27\nPublished as a conference paper at ICLR 2024\nH\nAPPLICABILITY TO OTHER MODELS\nTo assess the effectiveness of our method across different language models, such as smaller or\nlarger models, models of various types, and those that have undergone reinforcement learning from\nhuman feedback (RLHF) (Christiano et al., 2017), we apply our method to the following models:\nPythia-70M (Biderman et al., 2023), LLaMA-13B (Touvron et al., 2023a), and LLaMA-2-Chat-\n7B (Touvron et al., 2023b). The experimental results presented in Table 17 demonstrate consistent\nimprovements achieved by our methods across various models in the biomedicine domain.\nTable 17: Domain-specific task performance of different language models.\nPythia-70M\nLLaMA-13B\nLLaMA-2-Chat-7B\nGeneral LLM\nAdaptLLM\nGeneral LLM\nAdaptLLM\nGeneral LLM\nAdaptLLM\nPubMedQA\n34.4\n50.3\n59.6\n66.0\n55.2\n65.0\nChemProt\n7.6\n10.4\n42.8\n47.6\n33.8\n36.6\nMQP\n52.1\n50.5\n49.3\n73.0\n59.3\n60.3\nRCT\n29.6\n30.6\n56.7\n50.4\n53.4\n59.2\nUSMLE\n24.0\n24.4\n34.7\n34.0\n29.1\n33.4\nAVERAGE\n29.5\n33.2\n48.6\n54.2\n46.2\n50.9\nSmaller and Different: Pythia-70M. Pythia-70M (Biderman et al., 2023) is a recently released\nand representative language model with 70 million parameters. It shares the same architecture as\nGPT-NeoX (Black et al., 2022) and can also serve as a model with a different architecture than\nLLaMA.\nLarger: LLaMA-13B. We scale up our base model to the one with 13 billion parameters. Due\nto computational constraints, we reduce the maximum sequence length to 512 tokens (1/4 of\nAdaptLLM-7B) during continued training. To maintain the same number of trained tokens, we\nincrease the continued training steps to 40k (4 times that of AdaptLLM-7B).\nAfter RLHF: LLaMA-2-Chat.\nWe apply our method to LLaMA-2-Chat-7B (Touvron et al.,\n2023b), which has undergone supervised fine-tuning and reinforcement learning with human feed-\nback (Christiano et al., 2017) to align with human preferences. LLaMA-2-Chat requires specific\ndata formats; an example of the data format provided by Hugging Face 4 is shown in Figure 7.\n<s>[INST] <<SYS>>\n{SYSTEMPROMPT}\n<</SYS>>\n{USER_MSG_1} [/INST] {MODEL_ANSWER_1} </s><s>[INST] {USER_MSG_2}} [/INST] {MODEL_ANSWER_1}</s>\nFigure 7: Input data format for LLaMA-2-Chat.\nWe observe that our reading comprehension can seamlessly conform to the required data format by\ntransforming the reading comprehension into a multi-turn conversation. For example, we can convert\nthe reading comprehension shown in Figure 2 into the conversation format depicted in Figure 8,\nwhere we employ the system prompt from the Hugging Face LLaMA-2 chat demo.\nNote that during the training of LLaMA-2-Chat, each input in the training data comprises only\none piece of single/multi-turn conversation, because we empirically find that concatenating multiple\nconversations as one data input would lead to uncontrollable gradient overflow, which would quickly\nshut down the training. In the evaluation stage, we adhere to this data format and transform each\nfew-shot prompt into a multi-turn conversation.\n4https://huggingface.co/blog/llama2\n28\nPublished as a conference paper at ICLR 2024\n<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being\nsafe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal\ncontent. Please ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering\nsomething not correct. If you don't know the answer to a question, please don't share false\ninformation.\n<</SYS>>\nHere is the first part of an article about biomedicine: Recent reported evidence indicates that vocal\ncord carcinoma is evolving similarly to oropharyngeal cancer with an increasing number of patients (...)\nAnswer questions based on the article:\nWhat is a summary? [/INST] Glottic Carcinoma in Young Patients.\n</s><s>[INST]\nGenerate a sentence that includes these biomedicine keywords [carcinoma, oropharyngeal,\npapillomavirus]: [/INST] Recent reported evidence indicates that vocal cord carcinoma is evolving\u2026\n</s><s>[INST]\nPremise:\u2026 Hypothesis:\u2026 Does the premise entail the hypothesis? [/INST] Yes\n</s><s>[INST]\nWhat is the reason for \"\u2026\"? [/INST] the morphology of the lesions and the patients' young age.\n</s><s>[INST]\nCompose a sentence that contradicts the meaning of \"Historically, glottic carcinoma \u2026 \u201d.\nAnswer: [/INST] Recent published evidence \u2026\n</s><s>[INST]\nHow would you complete the article? [/INST] This finding further supports\u2026</s>\nFigure 8: An example of transforming reading comprehension into a multi-turn conversation\nfor LLaMA-2-Chat.\nI\nANALYSIS ON VERBALIZER EFFECT\nWe conduct an analysis on how the diversity of verbalizers influences performance. Intuitively, our\nhypothesis is that a greater diversity of verbalizers leads to a broader range of comprehension tasks,\nultimately enhancing performance. To test this hypothesis, we conduct an ablation by preserving\nonly one verbalizer for each task type. Specifically, we preserve only the first verbalizer in each row\nin Table 3, and remake the reading comprehension data to observe the effect.\nFirst, the most direct impact is that the number of mined task examples decreases. When using all\nthe verbalizers, we mine an average of 2.1 examples per text, while using one verbalizer results in\n1.4 examples per text. Next, we conduct an experiment to train the general model using the new data,\nand the task results shows that with fewer verbalizers, the downstream task performance declines\n(from 44.1 to 42.7), verifying that including more verbalizers can contribute to better performance.\nJ\nANALYSIS ON DOMAIN-SPECIFIC RAW CORPORA\nAs shown in the experiment results in Section 2, continued training on domain-specific raw cor-\npora leads to decreased prompting ability. To verify whether this reduction results from the limited\ndiversity of input-output patterns within the domain-specific corpora, we conduct a case study on\nthe question answering pattern. Specifically, we search over all three domain-specific corpora and\na representative corpora in the general domain\u2014RedPajama (Computer, 2023)\u2014to estimate how\nmany sentence pairs follow the question-answering pattern, utilizing this mining pattern:\n29\nPublished as a conference paper at ICLR 2024\n{What|Who|When|Where|Why|Which|How}{SENT1}? {SENT2}.\nTable 18 lists the number of tokens belonging to the mined question-answering pairs per million\ntokens. Compared with the general corpora, domain-specific corpora have much lower rates of data\nfollowing the question-answering pattern, which could lead to the observed decrease in question-\nanswering ability.\nTable 18: Number of tokens belonging to the mined question-answering pairs per million tokens\nof different pre-training corpora.\nPre-train Corpora\nBioMed.\nLaw\nFinance\nGeneral\n# QA tokens\n22.5\n0.8\n236.8\n490.26\n30\n"
  },
  {
    "title": "PDFTriage: Question Answering over Long, Structured Documents",
    "link": "https://arxiv.org/pdf/2309.08872.pdf",
    "upvote": "50",
    "text": "PDFTriage: Question Answering over Long, Structured Documents\nJon Saad-Falcon\nStanford University\njonsaadfalcon@stanford.edu\nJoe Barrow\nAdobe Research\njbarrow@adobe.com\nAlexa Siu\nAdobe Research\nasiu@adobe.com\nAni Nenkova\nAdobe Research\nnenkova@adobe.com\nDavid Seunghyun Yoon\nAdobe Research\nsyoon@adobe.com\nRyan A. Rossi\nAdobe Research\nryrossi@adobe.com\nFranck Dernoncourt\nAdobe Research\ndernonco@adobe.com\nAbstract\nLarge Language Models (LLMs) have issues\nwith document question answering (QA) in sit-\nuations where the document is unable to fit in\nthe small context length of an LLM. To over-\ncome this issue, most existing works focus on\nretrieving the relevant context from the docu-\nment, representing them as plain text. However,\ndocuments such as PDFs, web pages, and pre-\nsentations are naturally structured with differ-\nent pages, tables, sections, and so on. Repre-\nsenting such structured documents as plain text\nis incongruous with the user\u2019s mental model\nof these documents with rich structure. When\na system has to query the document for con-\ntext, this incongruity is brought to the fore,\nand seemingly trivial questions can trip up\nthe QA system. To bridge this fundamental\ngap in handling structured documents, we pro-\npose an approach called PDFTriage that en-\nables models to retrieve the context based on\neither structure or content. Our experiments\ndemonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several\nclasses of questions where existing retrieval-\naugmented LLMs fail.\nTo facilitate further\nresearch on this fundamental problem, we re-\nlease our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured\ndocuments from 10 different categories of ques-\ntion types for document QA. Our code and\ndatasets will be released soon on Github.\n1\nIntroduction\nWhen a document does not fit in the limited con-\ntext window of an LLM, different strategies can\nbe deployed to fetch relevant context. Current ap-\nproaches often rely on a pre-retrieval step to fetch\nthe relevant context from documents (Pereira et al.,\n2023; Gao et al., 2022). These pre-retrieval steps\ntend to represent the document as plain text chunks,\nsharing some similarity with the user query and\npotentially containing the answer. However, many\ndocument types have rich structure, such as web\npages, PDFs, presentations, and so on. For these\nstructured documents, representing the document\nas plain text is often incongruous with the user\u2019s\nmental model of a structured document. This can\nlead to questions that, to users, may be trivially an-\nswerable, but fail with common/current approaches\nto document QA using LLMs. For instance, con-\nsider the following two questions:\nQ1 \u201cCan you summarize the key takeaways from\npages 5-7?\u201d\nQ2 \u201cWhat year [in table 3] has the maximum rev-\nenue?\u201d\nIn the first question, document structure is ex-\nplicitly referenced (\u201cpages 5-7\u201d). In the second\nquestion, document structure is implicitly refer-\nenced (\u201cin table 3\u201d). In both cases, a representation\nof document structure is necessary to identify the\nsalient context and answer the question. Consider-\ning the document as plain text discards the relevant\nstructure needed to answer these questions.\nWe propose addressing this simplification of doc-\numents by allowing models to retrieve the context\nbased on either structure or content. Our approach,\nwhich we refer to as PDFTriage, gives models ac-\ncess to metadata about the structure of the docu-\nment. We leverage document structure by augment-\ning prompts with both document structure meta-\ndata and a set of model-callable retrieval functions\nover various types of structure. For example, we\nintroduce the fetch_pages(pages: list[int])\nfunction, which allows the model to fetch a list of\npages. We show that by providing the structure\nand the ability to issue queries over that structure,\nPDFTriage-augmented models can reliably answer\nseveral classes of questions that plain retrieval-\naugmented LLMs could not.\nIn order to evaluate our approach, we construct\na dataset of roughly 900 human-written questions\narXiv:2309.08872v2  [cs.CL]  8 Nov 2023\nover 90 documents, representing 10 different cat-\negories of questions that users might ask. Those\ncategories include \u201cdocument structure questions\u201d,\n\u201ctable reasoning questions\u201d, and \u201ctrick questions\u201d,\namong several others. We will release the dataset\nof questions, documents, model answers, and anno-\ntator preferences. In addition, we release the code\nand prompts used.\nThe key contributions of this paper are:\n\u2022 We identify a gap in question answering over\nstructured documents with current LLM ap-\nproaches, namely treating documents as plain\ntext rather than structured objects;\n\u2022 We release a dataset of tagged question types,\nalong with model responses, in order to facili-\ntate further research on this topic; and\n\u2022 We present a method of prompting the model,\ncalled PDFTriage, that improves the ability\nof an LLM to respond to questions over struc-\ntured documents.\nThe rest of the paper proceeds as follows: in\nSection 2, we identify the related works to this\none, and identify the distinguishing features of our\nwork; in Section 3 we outline the PDFTriage ap-\nproach, including the document representation, the\nnew retrieval functions, and the prompting tech-\nniques; in Section 4 we outline how we constructed\nthe evaluation dataset of human-written questions;\nin Section 5 we detail the experiments we run to\nsupport the above contributions; in Section 6 we\nlist the key takeaways of those experiments; and,\nlastly, in Section 7 we describe the limitations of\nour current work and future directions.\n2\nRelated Works\n2.1\nTool and Retrieval Augmented LLMs\nTool-augmented LLMs have become increasingly\npopular as a way to enhance existing LLMs to\nutilize tools for responding to human instruc-\ntions (Schick et al., 2023). ReAct (Yao et al., 2022)\nis a few-shot prompting approach that leverages the\nWikipedia API to generate a sequence of API calls\nto solve a specific task. Such task-solving trajecto-\nries are shown to be more interpretable compared\nto baselines. Self-ask (Press et al., 2022) prompt\nprovides the follow-up question explicitly before\nanswering it, and for ease of parsing uses a specific\nscaffold such as \u201cFollow-up question:\u201d or \u201cSo the\nfinal answer is:\u201d. Toolformer (Schick et al., 2023)\nuses self-supervision to teach itself to use tools by\nleveraging the few-shot capabilities of an LM to\nobtain a sample of potential tool uses, which is then\nfine-tuned on a sample of its own generations based\non those that improve the model\u2019s ability to predict\nfuture tokens. TALM (Parisi et al., 2022) augments\nLMs with non-differentiable tools using only text\nalong with an iterative technique to bootstrap per-\nformance using only a few examples. Recently,\nTaskmatrix (Liang et al., 2023) and Gorilla (Patil\net al., 2023) have focused on improving the ability\nof LLMs to handle millions of tools from a vari-\nety of applications. There have also been many\nworks focused on benchmarks for tool-augmented\nLLMs (Li et al., 2023; Zhuang et al., 2023). These\ninclude API-Bank (Li et al., 2023), focused on\nevaluating LLMs\u2019 ability to plan, retrieve, and cor-\nrectly execute step-by-step API calls for carrying\nout various tasks, and ToolQA (Zhuang et al., 2023)\nthat focused on question-answering using external\ntools.\nRetrieval-augmented language models aim to en-\nhance the reasoning capabilities of LLMs using\nexternal knowledge sources for retrieving related\ndocuments (Asai et al., 2022; Gao et al., 2022;\nLin et al., 2023; Yu et al., 2023; Zhao et al., 2023;\nFeng et al., 2023). In particular, HyDE (Gao et al.,\n2022) generates a hypothetical document (captur-\ning relevance patterns) by zero-shot instructing an\ninstruction-following LLM, then encodes the doc-\nument into an embedding vector via an unsuper-\nvised contrastively learned encoder, which is used\nto retrieve real documents that are similar to the\ngenerated document. More recently, Feng et al.\n(2023) proposed InteR that iteratively refines the\ninputs of search engines and LLMs for more ac-\ncurate retrieval. In particular, InteR uses search\nengines to enhance the knowledge in queries us-\ning LLM-generated knowledge collections whereas\nLLMs improve prompt formulation by leveraging\nthe retrieved documents from the search engine.\nFor further details on augmented language models,\nsee the recent survey (Mialon et al., 2023).\n2.2\nQuestion Answering\nMuch of the existing work in QA does not ground\nthe questions in structured documents, instead pri-\nmarily focusing on extractive QA tasks such as\nGLUE (Wang et al., 2018). For example, text-only\ndocuments in QA datasets, like SQuAD (Rajpurkar\net al., 2016) and NaturalQuestions (Kwiatkowski\nPages\n\u2026\nDocument Metadata Representation\nSection\nTitle: \"2 Related Works\" \nPages: [2, 3]\nSection\nTitle: \"2.1 Tool and Retrieval Augmented LLMs\"\nPages: [2]\nTable\nCaption: \"Table 1: GPTriage functions for Document QA\"\nPages: [4]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\u2026\nDocument\nSection\nSection\nSection\nSection\n\u2026\nH1\nP\nUL\nP\nLI\nLI\nQ1: \u201cCan you summarize the key \ntakeaways from pages 5-7?\u201d\nQ2: \u201cWhat year [in table 3] has \nthe maximum revenue?\u201d\nStep 1:  Generate a structured metadata representation of the document.\nStep 2:  LLM-based Triage \n     (frame selection/filling)\nStep 3:  Question answering \n     with selected context\nQuestion: \"Can you summarize the key \ntakeaways from pages 5-7?\"\nDocument Context:\nAvailable Functions: fetch_pages, fetch_section, search, \u2026\nfetch_pages(pages: [5, 6, 7])\nAnswer: The key takeaways of ...\nQuestion: \"Can you summarize the key \ntakeaways from pages 5-7?\"\nPage 5:\n...length less than 10 pages, to ensure that \nthere is sufficient but not excessive...\nPage 6:\n...the query embedding. We then feed each \npage\u2019s text as context for answering...\nPage 7:\n...1. The overall quality of the question, \nsuch as its difficulty, clarity,...\nFigure 1: Overview of the PDFTriage technique: PDFTriage leverages a PDF\u2019s structured metadata to implement\na more precise and accurate document question-answering approach. It starts by generating a structured metadata\nrepresentation of the document, extracting information surrounding section text, figure captions, headers, and tables.\nNext, given a query, a LLM-based Triage selects the document frame needed for answering the query and retrieves\nit directly from the selected page, section, figure, or table. Finally, the selected context and inputted query are\nprocessed by the LLM before the generated answer is outputted.\net al., 2019), don\u2019t contain tables or figures.\nDocument Question Answering\n.\nSeveral\ndatasets have been constructed to benchmark\ndifferent aspects of document-focused question-\nanswering.\nDocVQA (Mathew et al., 2021) is\na visual question-answering dataset focused that\nuses document scans.\nA recent work by Lan-\ndeghem et al. (2023) focused on a dataset for docu-\nment understanding and evaluation called DUDE,\nwhich uses both scans and born-digital PDFs. Both\nDUDE and DocVQA have questions that can\nbe answered short-form; DUDE answers average\nroughly 3.35 tokens and DocVQA tokens average\n2.11 tokens. QASPER (Dasigi et al., 2021) is a\ndataset focused on information-seeking questions\nand their answers from research papers, where the\ndocuments are parsed from raw LATEXsources and\nthe questions are primarily focused on document\ncontents. The PDFTriage evaluation dataset seeks\nto expand on the question types in these datasets,\ngetting questions that can reference the document\nstructure or content, can be extractive or abstractive,\nand can require long-form answers or rewrites.\n3\nPDFTriage: Structured Retrieval from\nDocument Metadata\nThe PDFTriage approach consists of three steps to\nanswer a user\u2019s question, shown in Figure 1:\n1. Generate document metadata (Sec. 3.1):\nExtract the structural elements of a document\nand convert them into readable metadata.\n2. LLM-based triage (Sec. 3.2): Query the\nLLM to select the precise content (pages, sec-\ntions, retrieved content) from the document.\n3. Answer using retrieved content (Sec. 3.3):\nBased on the question and retrieved content,\ngenerate an answer.\n# of Documents\n82\n# of Questions\n908\nEasy Questions\n393\nMedium Questions\n144\nHard Questions\n266\n\u201cUnsure\u201d Questions\n105\nTable 1: Dataset statistics for the PDFTriage evaluation\ndataset.\nFigure 2: PDFTriage Document Distribution by Word\nCount\n3.1\nDocument Representation\nWe consider born-digital PDF documents as the\nstructured documents that users will be interacting\nwith. Using the Adobe Extract API, we convert\nthe PDFs into an HTML-like tree, which allows us\nto extract sections, section titles, page information,\ntables, and figures.1 The Extract API generates\na hierarchical tree of elements in the PDF, which\nincludes section titles, tables, figures, paragraphs,\nand more. Each element contains metadata, such\nas its page and location. We can parse that tree\nto identify sections, section-levels, and headings,\ngather all the text on a certain page, or get the text\naround figures and tables. We map that structured\ninformation into a JSON type, that we use as the\ninitial prompt for the LLM. The content is con-\nverted to markdown. An overview of this process\nis shown at the top of Figure 1.\n3.2\nLLM Querying of Document\nPDFTriage utilizes five different functions in\nthe approach:\nfetch_pages, fetch_sections,\n1https://developer.adobe.com/\ndocument-services/apis/pdf-extract/\nfetch_table, fetch_figure, and retrieve. As\ndescribed in Table 2, each function allows the PDF-\nTriage system to gather precise information related\nto the given PDF document, centering around struc-\ntured textual data in headers, subheaders, figures,\ntables, and section paragraphs. The functions are\nused in separate queries by the PDFTriage system\nfor each question, synthesizing multiple pieces of\ninformation to arrive at the final answer. The func-\ntions are provided and called in separate chat turns\nvia the OpenAI function calling API,2 though it\nwould be possible to organize the prompting in a\nReAct (Yao et al., 2022) or Toolformer (Schick\net al., 2023) -like way.\n3.3\nQuestion Answering\nTo initialize PDFTriage for question-answering, we\nuse the system prompt format of GPT-3.5 to input\nthe following:\nYou are an expert document question answer-\ning system. You answer questions by finding\nrelevant content in the document and answer-\ning questions based on that content.\nDocument:\n<textual\nmetadata\nof\ndocument>\nUsing user prompting, we then input the query\nwith no additional formatting. Next, the PDFTriage\nsystem uses the functions established in Section 2\nto query the document for any necessary informa-\ntion to answer the question. In each turn, PDF-\nTriage uses a singular function to gather the needed\ninformation before processing the retrieved context.\nIn the final turn, the model outputs an answer to\nthe question. For all of our experiments, we use the\ngpt-35-turbo-0613 model.\n4\nDataset Construction\nTo test the efficacy of PDFTriage, we constructed a\ndocument-focused set of question-answering tasks.\nEach task seeks to evaluate different aspects of\ndocument question-answering, analyzing reason-\ning across text, tables, and figures within a docu-\nment. Additionally, we wanted to create questions\nranging from single-step answering on an individ-\nual document page to multi-step reasoning across\nthe whole document.\n2https://platform.openai.com/docs/\napi-reference\nFunction\nDescription\nfetch_pages\nGet the text contained in the pages listed.\nfetch_sections\nGet the text contained in the section listed.\nfetch_figure\nGet the text contained in the figure caption listed.\nfetch_table\nGet the text contained in the table caption listed.\nretrieve\nIssue a natural language query over the document, and fetch relevant chunks.\nTable 2: PDFTriage Functions for Document QA.\nWe collected questions using Mechanical Turk.3\nThe goal of our question collection task was to\ncollect real-world document-oriented questions for\nvarious professional settings. For our documents,\nwe sampled 1000 documents from the common\ncrawl to get visually-rich, professional documents\nfrom various domains, then subsampled 100 docu-\nments based on their reading level (Flesch, 1948). 4\nBy collecting a broad set of document-oriented\nquestions, we built a robust set of tasks across in-\ndustries for testing the PDFTriage technique.\nIn order to collect a diverse set of questions, we\ngenerated our taxonomy of question types and then\nproceeded to collect a stratified sample across the\ntypes in the taxonomy. Each category highlights a\ndifferent approach to document-oriented QA, cov-\nering multi-step reasoning that is not found in many\nother QA datasets. We asked annotators to read a\ndocument before writing a question. They were\nthen tasked with writing a salient question in the\nspecified category.\nFor our taxonomy, we consider ten different cat-\negories along with their associated descriptions:\n1. Figure Questions (6.5%): Ask a question\nabout a figure in the document.\n2. Text Questions (26.2%): Ask a question\nabout the document.\n3. Table Reasoning (7.4%): Ask a question\nabout a table in the document.\n4. Structure Questions (3.7%): Ask a question\nabout the structure of the document.\n5. Summarization (16.4%): Ask for a summary\nof parts of the document or the full document.\n6. Extraction (21.2%): Ask for specific content\nto be extracted from the document.\n7. Rewrite (5.2%): Ask for a rewrite of some\ntext in the document.\n3https://mturk.com\n4https://commoncrawl.org/\n8. Outside Questions (8.6%): Ask a question\nthat can\u2019t be answered with just the document.\n9. Cross-page Tasks (1.1%): Ask a question\nthat needs multiple parts of the document to\nanswer.\n10. Classification (3.7%): Ask about the type of\nthe document.\nIn total, our dataset consists of 908 questions\nacross 82 documents. On average a document con-\ntains 4,257 tokens of text, connected to headers,\nsubheaders, section paragraphs, captions, and more.\nIn Figure 2, we present the document distribution\nby word count. We provide detailed descriptions\nand examples of each of the classes in the appendix.\n5\nExperiments\nWe outline the models and strategies used in our\napproach along with our baselines for comparison.\nThe code and datasets for reproducing our results\nwill be released soon on Github.\n5.1\nPDFTriage\nFor our primary experiment, we use our PDFTriage\napproach to answer various questions in the se-\nlected PDF document dataset. This strategy lever-\nages the structure of PDFs and the interactive sys-\ntem functions capability of GPT-3.5 to extract an-\nswers more precisely and accurately than existing\nnaive approaches.\n5.2\nRetrieval Baselines\nPage Retrieval\n. For our first baseline, we in-\ndex the pages of each individual document using\ntext-embedding-ada-002 embeddings. Using co-\nsine similarity, we retrieve the pages most similar\nto the query embedding. We then feed each page\u2019s\ntext as context for answering the given question un-\ntil we reach the context window limit for a model.\n0\n20\n40\n60\n80\n100\nAnnotator Preferences (%)\nClassification\nExtraction\nText Questions\nSummarization\nCross-page Tasks\nFigure Questions\nOutside Questions\nRewrite\nTable Reasoning\nStructure Questions\n \nOverall\n45.0%\n25.0%\n30.0%\n45.6%\n24.6%\n29.8%\n46.8%\n19.2%\n34.0%\n47.7%\n34.1%\n18.2%\n50.0%\n33.3%\n16.7%\n51.4%\n22.9%\n25.7%\n52.4%\n33.3%\n14.3%\n57.1%\n14.3%\n28.6%\n62.5%\n12.5%\n25.0%\n75.0%\n20.0%\n50.8%\n27.1%\n22.1%\nPDFTriage\nPage Retrieval\nChunk Retrieval\nFigure 3: User Preferences between PDFTriage and Alternate Approaches: Overall, PDFTriage-generated\nanswers were favored the most by the users, claiming 50.8% of the top-ranked answers overall. Furthermore,\nPDFTriage answers ranked higher on certain multi-page tasks, such as structure questions and table reasoning,\nwhile ranking lower on generalized textual tasks, such as classification and text questions. However, across all the\nquestion categories, PDFTriage beat both the Page Retrieval and Chunk Retrieval approaches on a head-to-head\nranking.\nChunk Retrieval\n. In our second baseline, we\nconcatenate all the document\u2019s text before chunk-\ning it into 100-word pieces. We then index each\nchunk using text-embedding-ada-002 embeddings\nbefore using cosine similarity calculations to re-\ntrieve the chunks most similar to the query embed-\nding. Finally, we feed each chunk\u2019s textual contents\nas context for answering the given question until\nwe reach the context window limit for a model.\nPrompting\n. For both retrieval baselines, we use\nthe following prompt to get an answer from GPT-\n3.5:\nYou are an expert document question answer-\ning system. You answer questions by finding\nrelevant content in the document and answer-\ning questions based on that content.\nDocument: <retrieved pages/chunks>\nQuestion: <question>\n5.3\nHuman Evaluation\nTo measure any difference between PDFTriage and\nthe retrieval baselines, we established a human la-\nbeling study on Upwork. In the study, we hired 12\nexperienced English-speaking annotators to judge\nthe answers generated by each system. Please see\nAppendix A to see the full annotation questions for\neach question-document and its generated answers\n(for the overview, we use a sample question) as well\nas demographic information about the annotators.\nOur questions seek to understand several key\nattributes of each question-document pair as well\nas the associated general questions:\n1. The overall quality of the question, such as its\ndifficulty, clarity, and information needed for\nanswering it.\n2. The category of the question, using the taxon-\nomy in section 4.\n3. The ranking of each generated answer for the\ngiven question-document pair.\n4. The accuracy,\ninformativeness,\nreadabil-\nity/understandability, and clarity of each gen-\nerated answer.\n6\nResults and Analysis\nIn Table 1, we present the annotated question dif-\nficulty of each question in our sample. Overall,\nthe largest group of questions (43.3%) were cate-\ngorized as Easy while roughly a third of questions\nwere categorized as Hard for various reasons.\nIn addition to question difficulty, we asked an-\nnotators to categorize questions by type using the\nsame categories as Section 4. Our annotation frame-\nwork results in a dataset that\u2019s diverse across both\nquestion types and question difficulties, covering\ntextual sections, tables, figures, and headings as\nwell as single-page and multi-page querying. The\ndiversity of questions allows us to robustly evaluate\nmultiple styles of document-centered QA, testing\nthe efficacy of PDFTriage for different reasoning\ntechniques.\n6.1\nPDFTriage yields better answers than\nretrieval-based approaches.\nIn our annotation study, we asked the annotators\nto rank PDFTriage compared to our two baselines,\nPage Retrieval and Chunk Retrieval (Section 5).\nIn Figure 3, we found that annotators favored the\nPDFTriage answer over half of the time (50.7%)\nand favored the Chunk Retrieval approach over\nthe Page Retrieval approach. When comparing\ndifferent provided answers for the same question,\nPDFTriage performs substantially better than cur-\nrent alternatives, ranking higher than the alternate\napproaches across all the question types.\n6.2\nPDFTriage improves answer quality,\naccuracy, readability, and informativeness\nIn our annotation study, we also asked the an-\nnotators to score PDFTriage, Page Retrieval,\nand Chunk Retrieval answers across five ma-\njor qualities: accuracy, informativeness, readabil-\nity/understandability, and clarity. We hoped to bet-\nter understand the strengths of each answer for\nusers in document question-answering tasks. In\nTable 3, we show that PDFTriage answers score\nPDFTriage\nPage\nRetrieval\nChunk\nRetrieval\nReadability\n4.2\n4.1\n4.1\nInformativeness\n3.9\n3.7\n3.4\nClarity\n2.0\n2.1\n2.3\nAccuracy\n3.8\n3.6\n3.4\nOverall Quality\n3.9\n3.8\n3.6\nTable 3: Answer Quality Scoring\nhigher than Page Retrieval and Chunk Retrieval\nacross all answer qualities except for Clarity. Cru-\ncially, PDFTriage had the highest scores for Overall\nQuality and Answer Accuracy. For annotator agree-\nment, we calculated an average Cohen\u2019s kappa\nscore of 0.584.\nIn Appendix A, we provide a high-resolution\nbreakdown of annotations for \"Overall Quality\"\nand \"Accuracy\" by question category. We find that\nPDFTriage tends to be stronger for categories like\nsummarization, table reasoning, extraction, and fig-\nure questions which require multi-step reasoning\nacross different parts of a document. Additionally,\nPDFTriage performs similarly to Page Retrieval\nand Chunk Retrieval on other more generalized\nreasoning tasks, such as text questions and classifi-\ncation.\n6.3\nPDFTriage requires fewer retrieved\ntokens to produce better answers\nFor the PDF document sample, the average token\nlength of retrieved PDFTriage text is 1568 tokens\n(using the GPT-3.5 tokenizer). The average meta-\ndata length of textual inputs in document JSONs is\n4,257 tokens (using the GPT-3.5 tokenizer).\nWhile PDFTriage utilizes more tokens than Page\nRetrieval (3611 tokens on average) and Chunk Re-\ntrieval (3934 tokens on average), the tokens are\nretrieved from multiple sections of the document\nthat are non-consecutive. Furthermore, the sections\nused in Page Retrieval and Chunk Retrieval are\noften insufficient for answering the question, as in-\ndicated by lower answer quality scores on average\nfor \"Overall Quality\" and \"Accuracy\". However,\nsimply concatenating all the document\u2019s text to-\ngether would not ultimately replace PDFTriage due\nto both context window limits and the need to per-\nform multi-hop reasoning for document QA tasks.\nPDFTriage helps overcome this issue through the\nmulti-stage querying of the document, retrieving\nand adding context as needed for different docu-\nment QA tasks.\nQuestion\nPDFTriage\nChunk Retrieval\nWhich file formats are listed\nunder the \u201cCreate Using This\nFormat\u201d section?\nFunction used: fetch_section()\nThe file formats listed under the\n\"Create Using This Format\" section\nare: CSV, JPG, LPT, PDF, PNG,\nPS, RTF, XLS, and XML.\nThe document does not provide\ninformation about the specific\nfile formats listed under the\n\u201cCreate Using This Format\u201d\nsection.\nSummarize the second paragraph\nin one long sentence.\nFunction used: fetch_section()\nThe second paragraph explains the\nwriter\u2019s support for H.B. 5540,\nwhich aims to ban guns without\nserial numbers and regulate those\nsold in a form requiring assembly.\nThe writer also supports the...\nIn her testimony, Michele\nMudrick, Legislative Advocate\nfor the Connecticut Conference\nUnited Church of Christ,\nexpresses her support for\nH B 5540 and H B 5542, two\nacts concerning ghost guns, the\npermit application process, and...\nTable 4: A comparison of fetch_section() being called successfully and unsuccessfully. Answers highlighted\nin red were considered incorrect. In the second example, both approaches are incorrect; the PDFTriage approach\nfetches the incorrect section, rather than just the first page, the chunk retrieval approach has no knowledge of\ndocument structure and paragraph order.\nFigure 4: PDFTriage Performance compared to Docu-\nment Page Length (uses \"Overall Quality\" scores)\n6.4\nPDFTriage performs consistently across\ndocument lengths\nWe also wanted to calculate the correlation between\nPDFTriage performance and the length of the doc-\nument overall. Between the human-annotated PDF-\nTriage answer score for \"Overall Quality\" and doc-\nument length, we found a Pearson\u2019s correlation\ncoefficient of -0.015. This indicates that document\nlength has a negligible effect on the efficacy of\nPDFTriage, strengthening the generalizability of\nour technique to both short and long documents.\nThe length of different document types seems to\nultimately have no effect on overall performance.\nThe ability of PDFTriage to query specific textual\nsections within the document prevents the need to\ningest documents with excessively large contexts.\nIt allows PDFTriage to connect disparate parts of\na document for multi-page questions such as ta-\nble reasoning, cross-page tasks, figure questions,\nand structure questions, prioritizing relevant con-\ntext and minimizing irrelevant information. As a\nresult, GPT-3 and other LLMs are better capable\nof handling the reduced context size and ultimately\nutilize less computational and financial resources\nfor document QA tasks.\n7\nFuture Work & Conclusions\nIn this work, we present PDFTriage, a novel\nquestion-answering technique specialized for\ndocument-oriented tasks.\nWe compare our\napproach to existing techniques for question-\nanswering, such as page retrieval and chunk re-\ntrieval, to demonstrate the strengths of our ap-\nproach. We find that PDFTriage offers superior per-\nformance to existing approaches. PDFTriage also\nproves effective across various document lengths\nand contexts used for retrieval. We are considering\nthe following directions for future work:\n1. Developing multi-modal approaches that in-\ncorporate table and figure information into\nGPT-4 question-answering for documents.\n2. Incorporate question type in PDFTriage ap-\nproach to improve efficiency and efficacy of\nthe approach.\nReferences\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A Smith, and Matt Gardner. 2021. A dataset of\ninformation-seeking questions and answers anchored\nin research papers. arXiv preprint arXiv:2105.03011.\nJiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen,\nCan Xu, Guodong Long, Dongyan Zhao, and Daxin\nJiang. 2023. Knowledge refinement via interaction\nbetween search engines and large language models.\narXiv preprint arXiv:2305.07402.\nRudolph Flesch. 1948. A new readability yardstick.\nJournal of applied psychology, 32(3):221.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels. arXiv preprint arXiv:2212.10496.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srini-\nvasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen\nWang, Chenjie Gu, Wolfgang Macherey, Arnaud\nDoucet, Orhan Firat, and Nando de Freitas. 2023.\nReinforced self-training (rest) for language model-\ning.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453\u2013\n466.\nJordy Landeghem, Rub\u00e9n Tito, \u0141ukasz Borchmann,\nMicha\u0142 Pietruszka, Pawe\u0142 J\u00f3ziak, Rafa\u0142 Powalski,\nDawid Jurkiewicz, Micka\u00ebl Coustaty, Bertrand Ack-\naert, Ernest Valveny, et al. 2023.\nDocument un-\nderstanding dataset and evaluation (dude). arXiv\npreprint arXiv:2305.08455.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms. arXiv\npreprint arXiv:2304.08244.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. 2023. Taskmatrix. ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. arXiv preprint arXiv:2303.16434.\nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,\nJimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun\nChen. 2023.\nHow to train your dragon: Diverse\naugmentation towards generalizable dense retrieval.\narXiv preprint arXiv:2302.07452.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawa-\nhar. 2021. Docvqa: A dataset for vqa on document\nimages. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages\n2200\u20132209.\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models. arXiv preprint\narXiv:2205.12255.\nShishir G Patil, Tianjun Zhang, Xin Wang, and\nJoseph E Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\ndrigo Nogueira. 2023. Visconde: Multi-document\nqa with gpt-3 and neural reranking. In European\nConference on Information Retrieval, pages 534\u2013543.\nSpringer.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016.\nSquad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353\u2013355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nZichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.\n2023. Augmentation-adapted retriever improves gen-\neralization of language models as generic plug-in.\narXiv preprint arXiv:2305.17331.\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai\nJiao, Xuan Long Do, Chengwei Qin, Bosheng\nDing, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al.\n2023. Retrieving multimodal information for aug-\nmented generation:\nA survey.\narXiv preprint\narXiv:2303.10868.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and\nChao Zhang. 2023. Toolqa: A dataset for llm ques-\ntion answering with external tools. arXiv preprint\narXiv:2306.13304.\nA\nAppendix\nA.1\nQuestion Categories and Examples\nIn Table 6, and Table 7, we present descriptions\nas well as positive and negative examples for each\nquestion category. Each question category seeks to\ncapture a different document question-answering\ntask that is relevant across various professional\nfields.\nA.2\nAnnotator Demographic Information\nWe used UpWork to recruit 12 English-speaking\nannotators to judge the answers generated by PDF-\nTriage and the baseline approaches. We paid all\nthe annotators the same standard rate used for US\nannotators. Here is the demographic breakdown of\nannotators:\n\u2022 4 participants were located in India\n\u2022 2 participants were located in Pakistan\n\u2022 2 participants were located in South Africa\n\u2022 2 participants were located in Australia\n\u2022 2 participants were located in the Phillipines\n\u2022 2 participants were located in the United\nStates.\nCategory\nPositive Examples\nFigure Questions\nWhat is the main takeaway of Figure 4?\nWhat is the largest value in Figure 4?\nWhat kind of graph is used on page 5?\nText Questions\nIs 2pm on Wednesday free?\nWhat evidence is used to support the author\u2019s conclusion in section #5?\nTable Reasoning\nCan you convert the minutes column in Table 2 to hours?\nWhat row has the maximum value of the \u201cAccuracy\u201d column?\nStructure Questions\nWhat is the main takeaway from section 5?\nWhat counterexamples are provided in paragraph 3, section #1?\nSummarization\nCan you provide a concise summary of section 2?\nWrite a detailed summary about the main takeaways of the paper.\nExtraction\nFind all the council members mentioned in this document.\nWhat are the three central claims of the author?\nWhat are the main findings?\nRewrite\n- Can you rewrite this in more modern language:\n\u201cThe thousand injuries of Fortunato I had borne as best I could.\nBut when he ventured upon insult, I vowed revenge.\u201d\n- Can you simplify this: \u201cIn mice, immunoregulatory APCs express\nthe dendritic cell (DC) marker CD11c, and one or more distinctive\nmarkers (CD8, B220, DX5).\u201d\nOutside Questions\n(Closed-book QA)\nWhat other books were written by the novelist author?\nBesides the theory discussed in this document,\nwhat other scientific theories explain the given phenomena?\nCan you explain the term \u201cmitochondria\u201d?\nCross-page Tasks\nDo the results in the conclusions support the claims in the abstract?\nClassification\nIs this document a scientific article?\nIs this document about a residential lease or a commercial lease?\nTrick Question\nA good trick question might:\n(a) be related to the document\n(b) refer to non-existent tables, figures, or sections\n(c) not have enough information to answer it\n(d) not be related to the document at all\nTable 6: Positive Examples for Question Categories\nCategory\nNegative Examples\nFigure Questions\nWhat is the main takeaway of the second graph.\n(missing reference to page or figure number)\nText Questions\nWhat is the title of subsection #4?\n(too easy to answer)\nTable Reasoning\nWhat value is in the third column, fourth row?\n(too easy to answer)\nStructure Questions\nHow many sections are there in the document?\n(too easy to answer)\nWhat is the title of the document?\n(too easy to answer)\nSummarization\nWhat is a summary of the document?\n(does not specify summary length)\nWrite a short summary.\n(does not specify summary content)\nExtraction\n\u201cHow many times does the author\nmention the title character?\u201d\n(not relevant question)\nRewrite\nRemove all typos.\n(too broad, does not refer to specific text)\nOutside Questions\n(Closed-book QA)\nQuestions that are unrelated\nto the document\u2019s content\nCross-page Tasks\nAny task that is answerable in\none place in the document, or\nnot answerable at all.\nClassification\nCategories that are unrelated\nto the document.\nTrick Question\nTable 7: Negative Examples for Question Categories\nB\nEvaluation Details\nB.1\nHuman Evaluation Interface\nFigure 5: Annotation Question #1\nFigure 6: Annotation Question #2\nFigure 7: Annotation Question #3\nFigure 8: Annotation Question #4\nFigure 9: Annotation Question #5\nFigure 10: Annotation Question #6\nFigure 11: Annotation Question #7\nFigure 12: Annotation Question #8\nB.2\nGPT Evaluation and Discussion\nFor each question and document pair in our PDF-\nTriage document sample, we gather the correspond-\ning PDFTriage, Page Retrieval, and Chunks Re-\ntrieval answers for comparison. Next, for automatic\nevaluation, we use the gpt-3.5-turbo model since\nwe used the same model for our PDFTriage system\nand comparative baselines. We query the model\nusing the following system prompt:\nGive a score (1-5) for how well the question\nwas answered. Only provide the numerical\nrating. Do not give any explanation for your\nrating.\nQuestion: <question here>\nAnswer: <answer here>\nBetween our GPT-4 evaluation scores and the\n\"Overall Quality\" score of the human annotations,\nwe calculated a Cohen\u2019s kappa score of 0.067 and\na Pearson\u2019s correlation coefficient of 0.19 across\nthe entire dataset. Both these metrics indicate a\nnegligible alignment between the GPT-4 evaluation\nscores and the human annotations.\nTherefore, we believe the automated GPT-4 eval-\nuation requires further instructions or fine-tuning to\nbetter align with human preferences for document\nquestion-answering tasks. Recent work has taken\nsteps towards improving automated LLM evalu-\nation alignment with human preferences (Zheng\net al., 2023; Gulcehre et al., 2023). For future\nresearch, it would be worth considering how we\ncan leverage few-shot prompt-tuning to better align\ngenerative LLMs with human preferences in evalu-\nation tasks.\nB.3\nPerformance vs. Context Window\nTrade-off\nTo better understand the connection between PDF-\nTriage performance and the length of the context\nwindow of the text retrieved from the document,\nwe calculated the correlation between the human\nannotators\u2019 scores for PDFTriage answers and the\nlength of the context retrieved from the document\nmetadata. We found that the Pearson\u2019s correlation\ncoefficient is 0.062, indicating a negligible connec-\ntion between the retrieved context of PDFTriage\nand its overall efficacy.\nInterestingly, it seems like longer context length\ndoes not improve PDFTriage performance, accord-\ning to the human annotations. PDFTriage instead\nneeds to query the precise information needed for\nanswering different document QA questions, par-\nticularly those like cross-page tasks and structure\nquestions which require multiple stages of query-\ning. This suggests that full-concatenation of the\ndocument text wouldn\u2019t necessarily improve doc-\nument QA performance since additional text does\nnot correlate with improved accuracy or overall\nquality scores for the answers.\nB.4\nEvaluation Breakdown by Question\nCategory\nFigure 13: Accuracy Annotation Scores by Question\nCategory\nFigure 14: Overall Quality Annotation Scores by Ques-\ntion Category\nFigure 15: Informativeness Annotation Scores by Ques-\ntion Category\nFigure 16: Clarity Annotation Scores by Question Cate-\ngory\nFigure 17: Readability Annotation Scores by Question\nCategory\n"
  },
  {
    "title": "Contrastive Decoding Improves Reasoning in Large Language Models",
    "link": "https://arxiv.org/pdf/2309.09117.pdf",
    "upvote": "37",
    "text": "CONTRASTIVE DECODING IMPROVES REASONING IN\nLARGE LANGUAGE MODELS\nSean O\u2019Brien1,2\u2217\nMike Lewis2\n1University of California, San Diego\n2Meta AI\nseobrien@ucsd.edu, mikelewis@meta.com\nABSTRACT\nWe demonstrate that Contrastive Decoding \u2013 a simple, computationally light, and\ntraining-free text generation method proposed by Li et al 2022 \u2013 achieves large\nout-of-the-box improvements over greedy decoding on a variety of reasoning\ntasks. Originally shown to improve the perceived quality of long-form text gen-\neration, Contrastive Decoding searches for strings that maximize a weighted dif-\nference in likelihood between strong and weak models. We show that Contrastive\nDecoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on\nthe HellaSwag commonsense reasoning benchmark, and to outperform LLaMA\n2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark,\nin addition to improvements on a collection of other tasks. Analysis suggests\nthat Contrastive Decoding improves over existing methods by preventing some\nabstract reasoning errors, as well as by avoiding simpler modes such as copy-\ning sections of the input during chain-of-thought. Overall, Contrastive Decoding\noutperforms nucleus sampling for long-form generation and greedy decoding for\nreasoning tasks, making it a powerful general purpose method for generating text\nfrom language models.\nFigure 1: Contrastive decoding improves reason-\ning across model scales and reasoning tasks.\nFigure 2: Contrastive scoring significantly im-\nproves performance on HellaSwag, a standard\ncommonsense reasoning benchmark.\n1\nINTRODUCTION\nText is generated from large language models (LLMs) in different ways for different tasks. For open-\nended text generation tasks, truncated sampling is normally used, as the most likely strings under a\nmodel tend to be short and uninteresting (Holtzman et al., 2020). For reasoning problems, greedy\ndecoding is normally preferred, to avoid risking sampling errors. This bifurcation is undesirable; for\nexample it increases the likelihood of reasoning errors during open-ended generation.\n*Work done as an AI resident at Meta.\n1\narXiv:2309.09117v2  [cs.CL]  29 Sep 2023\nFigure 3: CD accentuates what the expert model has learned that the amateur model has not. Results\nare taken from greedy decoding with a 65B parameter expert, using \u03b1 = 0.1, \u03b2 = 0.5 for CD.\nWe explore the use of Contrastive Decoding (Li et al., 2022) for solving reasoning problems with\nLLMs. Contrastive Decoding (CD) searches for strings that maximize a weighted difference in\nlikelihood between a stronger expert and a weaker amateur model, and was shown to outperform\nexisting methods for open-ended text generation. It achieves this by avoiding undesirable modes of\nthe expert model\u2019s distribution, such as short or generic strings, which tend to be the most likely\nunder any model, including the amateur.\nWe show that Contrastive Decoding outperforms greedy decoding on reasoning problems.\nOn\nGSM8K, a widely used benchmark consisting of grade-school word math problems, contrastive de-\ncoding improves the performance of various LLaMA models by up to 8 absolute percentage points.\nThis result outperforms LLaMA 2, which has 5 billion more parameters and is trained on 40% more\ndata. On HellaSwag, using the CD objective to rank answers leads LLaMA to outperform all existing\nmodels except GPT-4. We find general improvement on arithmetic reasoning and multiple-choice\nranking tasks, including on models as large as LLaMA-65B, suggesting that Contrastive Decoding\ncould bring such widespread improvements to much larger models.\nWe also analyze the cause of the improvement from Constrastive Decoding. Empirically, we find\nthat Contrastive Decoding performs less surface-level copying from the prompt than greedy decod-\ning and misses fewer reasoning steps. This result suggests that, similarly to findings in Li et al.\n(2022), Contrastive Decoding works by reducing repetitive or other undesirable modes of the model\ndistribution. Our current method yields mixed results for commonsense reasoning tasks and slightly\ndegrades factual retrieval, both trends that encourage further refinement of the method.\nOverall, we show that Contrastive Decoding not only substantially improves LLM accuracies on\na range of benchmarks, but is also the first generation algorithm to achieve state-of-the-art results\nin both reasoning and text generation problems. These results allow a more unified method for\nimproving generation from language models across tasks.\n2\nCONTRASTIVE DECODING\n2.1\nSIMPLIFIED FORMULATION\nThe original Contrastive Decoding formulation from Li et al. (2022) explicitly chooses two pa-\nrameters: \u03b1 and the intermediate temperature of the amateur distribution \u03c4a, with the intermediate\ntemperature of the expert fixed at \u03c4e = 1. We slightly refactor the hyperparameter choice to be more\ninterpretable and simplify the algorithm by working directly in logit space.\n2\nLet s(i)\na\nand s(i)\ne\nbe the unnormalized scores (logits) assigned to token i by the amateur and ex-\npert models, respectively. \u03b1 is the same hyperparameter in the original paper: a proportion of the\nmaximum probability assigned by the expert model, with any tokens assigned a lower probability\nmasked out. \u03b2 is a hyperparameter corresponding to the strength of the amateur penalty. We include\na leading (1 + \u03b2) coefficient to the expert logits to decouple the strength of the contrastive penalty\nfrom the expected scale of the output logits, cleanly delineating between the contrastive tradeoff and\nthe final sampling temperature. This matches the formulation of DExperts (Liu et al., 2021), with\nthe expert model serving both as the base prior and steering expert.\n1. Determine \u03b1-mask.\nVvalid = {j \u2208 V, s(j)\ne\n\u2265 log \u03b1 + maxk\u2208V s(k)\ne }\n2. Subtract amateur logits.\ns(i)\nCD =\n(\n(1 + \u03b2)s(i)\ne\n\u2212 \u03b2s(i)\na\ni \u2208 Vvalid\n\u2212\u221e\ni \u0338\u2208 Vvalid\nA PyTorch implementation for this formulation, as well as the original, can be found in subsec-\ntion A.1 of the appendix. Our implementation takes three lines of readable code.\n2.2\nPROBABILISTIC INTERPRETATION\nOur implementation of \u03b1-masking has the same interpretation as in Li et al. (2022), given that the\nexpert temperature is fixed to \u03c4e = 1. We show the equivalence in Appendix A.2.\nFurther, we can consider the post-softmax probabilities produced by CD as a perturbation of the\nprobabilities predicted by the expert model. Not including \u03b1-masking, the probability assigned to\ntoken i by CD is a normalized adjustment of the probability assigned by the expert model:\np(i)\nCD \u221d p(i)\ne\n \np(i)\ne\np(i)\na\n!\u03b2\n(1)\nIt is therefore clear that as \u03b2 \u2192 0 the contrastive penalty disappears, and as \u03b2 \u2192 \u221e the distribution\ncollapses to the argmax of p(i)\ne /p(i)\na , which is the original formulation from Li et al. (2022).\n3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETUP\nModels.\nWe use untuned models from the LLaMA 1 family (Touvron et al., 2023) at all scales.\nUnless otherwise stated, we use an untuned LLaMA-65B as the expert and an untuned, LLaMA-\narchitecture model with 1.5B parameters trained on the same data as the other LLaMA 1 models as\nan amateur. For one ablation study, we use models from the FLAN-T5 family (Chung et al., 2022).\nDecoding Parameters.\nWe set \u03b2 = 0.5 and \u03b1 = 0.1 for all experiments unless otherwise stated.\nWe use greedy decoding, except for self-consistency experiments for which we sample at \u03c4 = 0.7\nfollowing Touvron et al. (2023).\nPrompting.\nFor generation tasks, we use 8-shot chain-of-thought prompting, in line with Tou-\nvron et al. (2023). The examples are the same as in LLaMA for tasks contained in that paper, and\ntaken from Wei et al. (2023) for other mathematical tasks.\nDatasets.\nFollowing prior works, we evaluate on a number of datasets. The following tasks\nmeasure performance on algebraic word problems: AQuA (Ling et al., 2017), ASDiv (Miao et al.,\n3\n2021), GSM8K (Cobbe et al., 2021), and SVAMP (Patel et al., 2021). We also evaluate on MATH\n(Hendrycks et al., 2021b), a larger and more challenging benchmark.\nFor commonsense reasoning, we measure open-ended performance on CommonsenseQA (Talmor\net al., 2019) and StrategyQA (Geva et al., 2021).\nWe also evaluate on a battery of multiple-\nchoice reasoning benchmarks: both the easy and challenge splits of the AI2 Reasoning Challenge\ndataset (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), MMLU\n(Hendrycks et al., 2021a), PIQA (Bisk et al., 2019), SIQA (Sap et al., 2019), and WinoGrande\n(Sakaguchi et al., 2019).\n3.2\nHYPERPARAMETER SELECTION\nContrastive decoding has three major hyperparameters: the masking ratio \u03b1, the contrastive strength\n\u03b2 and the size of the amateur model. We find that results are fairly insensitive to \u03b1 as long as \u03b2 is\nreasonably small (below 1); unless otherwise stated we use \u03b1 = 0.1 across experiments.\nNext we consider the size of the amateur model. In agreement with Li et al. (2022), we find that\nperformance benefits from smaller amateur models ( Figure 4); while a 1B-parameter amateur helps\nreasoning performance, a 7B-parameter amateur harms it. We also examine different types of am-\nateurs; ablation studies show that a partially-trained amateur performs better than a fully-trained\none, and that a poorly-prompted expert can be successfully used as an amateur as well (see subsec-\ntion 4.2).\nFinally, we examine the effect of \u03b2. The optimal value depends on the task, but for both generation\ntasks like GSM8K and multiple-choice ranking tasks like PIQA we find that \u03b2 = 0.5 performs well.\nSetting \u03b2 too high can place too much weight in the contrastive penalty and harm performance,\nespecially with a larger gap between amateur and expert models. \u03b2 = 0 corresponds to standard\ngreedy decoding with no contrastive penalty. Results of \u03b2 hyperparameter sweeps can be found in\nTable 1, Figure 4, Figure 5 and Appendix B.\nThe best result on GSM8K, with LLaMA-65B and \u03b2 = 0.25, is 57.7 (Table 1), outperforming\nPaLM-540B (56.5), LLaMA-2 (56.8) and GPT-3.5 (57.1).* (Anil et al., 2023; OpenAI, 2023)\nFigure 4: Results on GSM8K with LLaMA-\n65B as the expert. While a 7B amateur harms\nperformance, a 1.5B amateur helps.\nExpert\n\u03b2 = 0\n\u03b2 = 0.25\n\u03b2 = 0.5\n\u03b2 = 1\n7B\n10.7\n11.5\n13.6\n11.0\n13B\n17.0\n21.0\n22.9\n20.4\n30B\n35.2\n40.0\n43.4\n42.0\n65B\n51.0\n57.7\n56.8\n44.6\nTable 1: Results on GSM8K. \u03b2 = 0.5 tends to give\ngood results across expert sizes.\n3.3\nARITHMETIC REASONING\nWe find that contrastive decoding tends to help on arithmetic reasoning tasks with chain-of-thought\nprompting; see Table 2 for all results. One exception to this is the MATH dataset, which proves to\nbe challenging for both standard and contrastive decoding. We conjecture that because contrastive\ndecoding amplifies skills that the expert has learned better than the amateur, it cannot help on tasks\nthat are well beyond the expert\u2019s ability.\nWe also experiment with normalizing the \u03b1-masked CD scores via softmax, then temperature sam-\npling from the resulting distribution. This permits CD to generate multiple candidate reasoning\nchains to be used for self-consistency (taking the majority answer) (Wang et al., 2023b). We show\nacross both mathematical and commonsense reasoning, CD improves self-consistency performance.\n*OpenAI (2023) evaluates GPT-3.5 5-shot; all others are 8-shot.\n4\n(a)\n(b)\nFigure 5: Two examples of sweeping through \u03b2 values on multiple-choice reasoning tasks across\nmodel scales. Dashed horizontal lines mark performance without contrastive decoding.\nTable 2: Results on math generation tasks. Contrastive decoding generally improves performance.\nModel\nCD\nAQuA\nASDiv\nGSM8K\nMATH\nSVAMP\nAverage\n7B\n\u2717\n21.0\u2217\n40.2\n10.7\n3.0\n27.3\n20.4\n13B\n\u2717\n18.1\u2217\n49.0\n17.4\n4.2\n39.4\n25.6\n30B\n\u2717\n23.8\n60.1\n35.3\n6.9\n55.9\n36.4\n65B\n\u2717\n33.3\n67.2\n51.0\n10.6\n69.1\n46.2\n65B maj@20\n\u2717\n38.2\n73.6\n68.0\n\u2013\u2020\n77.3\n64.3\n7B\n\u2713\n19.0\u2217 (-2.0)\n39.7 (-0.5)\n14.3 (+3.6)\n2.9 (-0.1)\n31.5 (+4.2)\n21.5 (+1.1)\n13B\n\u2713\n16.0\u2217 (-2.1)\n52.0 (+3.0)\n22.7 (+5.5)\n3.8 (-0.4)\n43.1 (+3.7)\n27.5 (+1.9)\n30B\n\u2713\n29.8 (+6.0)\n62.5 (+2.4)\n43.1 (+8.1)\n8.1 (+1.2)\n59.3 (+3.4)\n40.6 (+4.2)\n65B\n\u2713\n36.9 (+3.6)\n71.9 (+4.7)\n56.8 (+5.8)\n10.3 (-0.3)\n67.8 (-1.3)\n48.7 (+2.5)\n65B maj@20\n\u2713\n39.4 (+1.2)\n77.4 (+3.8)\n74.0 (+6.0)\n\u2013\u2020\n79.0 (+1.7)\n67.5 (+3.2)\n3.4\nCOMMONSENSE REASONING\nResults are more mixed for CommonsenseQA and StrategyQA. For both of these tasks, we 8-shot\nprompt our model and compute the exact match score against the ground-truth answers. We find that\ncontrastive decoding harms performance for smaller models, but that this harm equalizes somewhat\nfor the 65B model and evens out when using self-consistency. See Table 3 for full results.\nTable 3: CD harms commonsense reasoning with a smaller expert,\nbut performance evens out with a larger expert-amateur gap.\nModel\nCD\nCSQA\nStrategyQA\nAverage\n7B\n\u2717\n40.0\n59.2\n49.6\n13B\n\u2717\n60.4\n64.5\n62.5\n30B\n\u2717\n66.4\n68.7\n67.6\n65B\n\u2717\n77.5\n69.5\n73.5\n65B maj@20\n\u2717\n77.0\n79.3\n78.2\n7B\n\u2713\n37.3 (-2.7)\n58.3 (-0.9)\n47.8 (-1.8)\n13B\n\u2713\n58.5 (-1.9)\n65.5 (+1.0)\n62.0 (-0.5)\n30B\n\u2713\n62.8 (-3.6)\n67.6 (-1.1)\n65.2 (-2.4)\n65B\n\u2713\n77.1 (-0.4)\n71.5 (+2.0)\n74.3 (+0.8)\n65B maj@20\n\u2713\n77.9 (+0.9)\n79.3 (+0.0)\n78.6 (+0.4)\n*In the AQuA task, the model selects one out of five given options. Thus the random baseline is 20%, and\nresults below that threshold are not meaningful.\n\u2020Given the size of the dataset and length of generations, we do not evaluate maj @ 20 on MATH.\n5\n3.5\nCONTRASTIVE RANKING\nWe further evaluate a contrastive objective as a scoring function to rank answers to multiple-choice\nquestions. These tasks are zero-shot, multiple-choice cloze tasks; instead of open-ended generation\nthe model scores each potential completion, length-normalizing following Touvron et al. (2023).\nWe find comparable performance across most tasks, with more substantive gains on HellaSwag and\nARC-Challenge. Notably, on HellaSwag CD leads LLaMA-65B to score 88.0, which outperforms\nLLaMA-2 (85.3), GPT-3.5 (85.5) (OpenAI, 2023) and PALM 2-Large (86.8) (Anil et al., 2023).\nTable 4: Results on multiple-choice reasoning tasks. CD generally provides a modest boost.\n\u03b2\nARC-E\nARC-C\nBoolQ\nHSwag\nPIQA\nSIQA\nWGrande\nMMLU\nAvg\n0.0\n79.1\n56.1\n84.2\n84.2\n82.6\n52.3\n77.3\n63.5\n72.4\n0.5\n79.0\n59.5\n84.3\n87.4\n83.1\n53.3\n77.8\n63.4\n74.9\n1.0\n76.9\n59.7\n84.1\n88.0\n82.9\n53.3\n76.5\n63.2\n74.5\n4\nADDITIONAL STUDIES\n4.1\nEFFECTS OF CONTRASTIVE DECODING\nCD is worse at arithmetic but better at logical reasoning.\nWe conduct a manual error analysis\nof 100 randomly selected examples from the GSM8K set between continuations from greedy decod-\ning and CD (\u03b2 = 0.5, \u03b1 = 0.1). We follow Wang et al. (2023a) and categorize wrong answers as\nprimarily being due to an arithmetic error, a missing step or a semantic misunderstanding. We add\none category of \u201cdegeneration,\u201d chosen when the model lapses into excessive repetition. Our small-\nscale analysis finds that CD makes more arithmetic errors, but that this is offset by better semantic\nreasoning and fewer missing steps (see Table 5).\nTable 5: Proportion of errors in of a set of 100 GSM8K questions. CD makes more\narithmetic errors, but omits fewer steps and avoids semantic misunderstandings.\nCD\nArithmetic\nMissing Step\nSemantic\nDegeneration\nTotal Errors\n\u2717\n4%\n22%\n24%\n4%\n54%\n\u2713\n8%\n20%\n21%\n3%\n52%\nTo further explore the claim that the benefit of CD does not stem from arithmetic evaluation, we\ngenerate a toy dataset of 1,0000 multiplication and subtraction equations with operands up to four\ndigits and then 8-shot prompt models to complete the expression, measuring exact match accuracy.\nWe find that CD does not improve performance on this task, and in fact may degrade it slightly.\nResults are shown in Table 8.\nStandard\nCD\nCorrect %\n44.6\n51.1\nParseable %\n95.2\n95.6\nAverage # chars\n215.2\n217.2\nTable 6: High-level generation statistics\nfrom sampled generations on GSM8K.\nResponses are similar lengths, despite\nthe performance improvement from CD.\nFigure 6: CD reduces copying from the question\nin the generated Chain of Thought, as measured\nby n-gram overlap on GSM8K generations.\nCD reduces copying from the prompt.\nWe analyze 26,000 sampled generations from CD-\nsampling on GSM8K against the corresponding set from temperature sampling; both of these sets\nof generations are used in our self-consistency study. We find that responses are roughly the same\nlength and follow the few-shot template roughly the same proportion of the time. This rules out the\n6\nhypothesis that contrastive decoding simply leads the model to follow the template better, prevents\ndegeneration or induces longer answers with more reasoning steps. Further, we run an automatic\nevaluation of greedy generations using ROSCOE (Golovneva et al., 2022) but do not find significant\ndifferences in any of these metrics. However, we measure the precision and recall of the tokens in\nthe prompt by the sampled generations and find that CD systematically reduces token-level copying\nfrom the prompt. This may be related to increased reasoning ability, as surface-level copying from\nthe prompt does not provide new information to the problem.\nCD can harm factual recall.\nOur primary claim is that contrastive decoding improves chain-\nof-thought reasoning. However, we also test CD on two pure factual-recall tests that do not utilize\nchain-of-thought: OpenBookQA (Mihaylov et al., 2018) and TriviaQA (Joshi et al., 2017). Open-\nBookQA (\u201cOBQA\u201d), is a multiple-choice completion task, while TriviaQA is a 5-shot generation\ntask. Reusing the same setup from reasoning leads to a slight degradation of performance, as seen\nin Table 7.\nTable 7: CD can harm perfor-\nmance on factual recall tasks.\nCD\nOBQA\nTriviaQA\u2217\n\u2717\n60.0\n72.2\n\u2713\n57.8 (-2.4)\n69.9 (-2.1)\nTable 8:\nCD slightly harms perfor-\nmance on a synthetic task of evaluat-\ning arithmetic expressions.\nCD\n7B\n13B\n30B\n65B\n\u2717\n31.0\n36.3\n52.3\n58.4\n\u2713\n30.9\n35.6\n52.2\n57.6\nCD outperforms other reasoning enhancements in FLOP efficiency.\nWe note that contrastive\ndecoding introduces relatively little overhead in comparison to other reasoning-enhancing methods.\nWe estimate that with a 1.5B amateur and 65.2B expert, contrastive decoding increases the total\nnumber of FLOPs by 3.25% (see section C of the appendix). This compares favorably to self-\nconsistency, which requires several extra full generation loops. We show in Figure 9 that CD is\nsignificantly more efficient than self-consistency.\n4.2\nABLATION STUDIES\n\u03b1-masking alone is not enough.\nWhen sampling and performing self-consistency, \u03b1-masking\nprevents the sampling of tokens the expert finds to be unlikely. It is natural to ask what portion of\nthe benefit comes purely from \u03b1-masking and not the contrastive objective itself.\nTo answer this, we set \u03b2 = 0 but \u03b1 = 0.1; that is, we mask out candidates based on the expert but do\nnot apply the contrastive objective. When sampling one path, we expect \u03b1-masking to improve over\ntemperature sampling alone as it eliminates unlikely results and thus provides a closer approximation\nto greedy sampling. This holds, but as we increase the number of paths we find no benefit from \u03b1-\nmasking alone. This suggests that the contrastive objective, and not \u03b1-masking, is the primary\nsource of improved self-consistency results. See Figure 7 for results of this ablation.\nCD requires chain-of-thought prompting to improve results.\nWe next study whether con-\ntrastive decoding provides an advantage in the absence of chain-of-thought prompting. We remove\nthe chains of thought from the GSM8K fewshot prompt, and find that as expected performance drops\nfor both standard and contrastive decoding (Figure 8); further, without chains of thought contrastive\ndecoding provides no consistent improvement. As with the MATH dataset, solving problems with-\nout explicit reasoning steps may be too challenging of a task for the expert model, and thus leave\ntoo small a gap between the expert and amateur to contrastively exploit.\nCD can benefit non-LLaMA models.\nWe conduct a short study to show that CD can benefit\nmodels outside of the LLaMA family. For this study, we choose the FLAN-T5 family as it is open-\nsource, has a wide range of model sizes that share a single tokenizer, and obtains good performance\non chain-of-thought reasoning tasks. We use FLAN-T5-XXL (11B) as the expert model and FLAN-\nT5-Small (80M) as amateur. We evaluate on GSM8K using the 8-shot random prompts from Fu\n*On manual examination, we find the set of correct answers provided by TriviaQA to be insufficient. Ran-\ndomly sampling 100 supposedly incorrect answers generated by CD and standard decoding, we find roughly\nhalf are in fact correct (46/100 with CD and 49/100 without). A rough linear extrapolation gives us estimates\nfor non-CD and CD scores of 85.8 and 83.7, respectively.\n7\nFigure 7: GSM8K scores via temperature sam-\npling and maj @ k with various values of k.\n\u03b1-masking alone does not yield significant im-\nprovement, while full CD does.\nFigure 8: Comparison of GSM8K scores with\nLLaMA-65B, both with and without chain-of-\nthought prompts.\nCD only helps when using\nCoT.\net al. (2023); note that GSM8K is within the set of tasks that FLAN-T5 is finetuned on. CD provides\na slight boost in performance, as seen in Table 9. We leave more extensive experiments on other\nfamilies of models to future work.\nFigure 9:\nFLOP increases, with increasing\ncompute from using more samples for self-\nconsistency. CD achieves similar or better per-\nformance with a smaller increase in FLOPs.\nCD\n\u03b2\nGSM8K\n\u2717\n0\n16.4\n\u2713\n0.5\n17.1\n\u2713\n1.0\n17.4\nTable 9: FLAN-T5 per-\nformance on GSM8K. CD\nprovides a boost to perfor-\nmance.\nSmall-scale amateurs beat \u201cnegative prompting.\u201d\nWe experiment to determine if there is a\nmore effective weak amateur model to use for contrastive decoding. We define a set of \u201cnegative\nprompts\u201d by sampling 7B model outputs on the fewshot prompts and collecting the incorrect re-\nsponses. We use these responses as fewshot prompts to mimic the failure modes of the family of\nmodels. These negative prompts should harm the performance of models they are prompted with,\nand specifically bias results towards the error distribution of the 65B model.\nWe find that contrasting with a negative prompt does not harm performance, but does not improve\nit as much as contrasting with a small amateur (see Table 10). In an ablation study, we find that\nnegative prompting does not harm performance that much; prompting a 65B model with incorrect\nfewshot examples on GSM8K gives a score of 41.3, which underperforms prompting with cor-\nrect examples (51.2) but significantly beats non-chain-of-thought prompting (13.5). This supports\nWang et al. (2023a), who find that even incorrect chain-of-thought rationales improve reasoning. A\nprompting strategy which better incapacitates the expert model might yield better results.\nMid-training checkpoints make for good amateurs.\nWe experiment with checkpoints of a mid-\ntraining 7B-parameter LLaMA model taken 10% and 23% of the way through the full training run.\nEven while a fully-trained 7B amateur harms performance on GSM8K, we find that a partially-\ntrained amateur improves performance. We do not perform extensive hyperparameter sweeps here,\ninstead reusing \u03b1 = 0.1, \u03b2 = 0.5 as before. We do not pursue partially-trained amateurs for our main\nresults as results may vary based on the order of training data, but this result allows us to interpret\ncontrastive decoding as a first-order optimization step over the output of a model, highlighting the\nhigh-level behaviors that it learns later on in the course of training. See Table 11 for full results.\n8\nTable 10: On GSM8K, negative prompting out-\nperforms greedy decoding but weakens CD.\nExpert\nGreedy\nNP\nCD\nCD + NP\n7B\n10.7\n11.4\n14.3\n12.7\n13B\n17.4\n17.5\n22.7\n20.7\n30B\n35.3\n36.9\n43.1\n42.9\n65B\n51.0\n52.0\n56.8\n54.7\nTable 11: Early-training checkpoints can be\ngood amateurs, even when late-stage check-\npoints harm performance.\nAmateur\nAmateur Tokens\nGSM8K\n7B\n130B\n57.0\n7B\n300B\n56.8\n7B\n1.3T\n49.9\n5\nRELATED WORK\nSteering methods for reasoning.\nOther works more explicitly model the error distribution of\nreasoning steps and use this to steer decoding. For example GRACE (Khalifa et al., 2023) uses a\ncontrastive loss to train an external step-level discriminator, which it then uses to select between\ncandidate steps sampled from a base model. Using the interpretation of contrastive decoding as\nmutual distinguishability between amateur and expert, we see that our method is close to FUDGE\n(Yang & Klein, 2021) where the binary predictor is an estimate of the probability that the generated\ntoken has come from the expert rather than the amateur.\nPrompting Methods for Reasoning.\nThere are many recent prompting methods to improve lan-\nguage model reasoning; see Qiao et al. (2023) for a survey. We perform our experiments with\nchain-of-thought prompting (Wei et al., 2023).\nSampling methods\nSeveral decoding methods exist to improve the quality of generations from\nlarge language models. For open-ended generation, truncated sampling schemes like top-k sampling\n(Fan et al., 2018), nucleus sampling (Holtzman et al., 2020) and typical sampling (Meister et al.,\n2023) have been shown to reduce repetition in comparison to greedy decoding and beam search\nwhile producing more coherent generations than standard temperature sampling. However, sampling\ncan still introduce errors into logical chains, and so greedy decoding is used to more effectively solve\nreasoning tasks. (Wei et al., 2023; Anil et al., 2023)\nContrastive Generation Methods.\nOur formulation\u2019s objective can be interpreted as a special\ncase of DExperts (Liu et al., 2021), using the larger model as both an expert and base LM prior.\nYona et al. (2023) identify model biases with Contrastive Input Decoding, a contrastive-decoding-\nstyle technique similar to negative prompting that operates on perturbed text inputs.\nConcurrently to our work, Chuang et al. (2023) propose DoLA, which improves factuality and rea-\nsoning through contrastive decoding between the predictions of later layers and earlier layers in a\nlanguage model. We study a wider array of reasoning tasks and demonstrate that a 7B amateur is\ntoo large, finding greater gains in reasoning just by scaling down the amateur to 1.5B parameters.\nOur paper differentiates itself from Li et al. (2022), which initially proposed Contrastive Decoding,\nin several ways: by testing on standard reasoning benchmarks, by our exploration of \u03b2 as a hyper-\nparameter, by ablations with various types of amateurs, and by a careful analysis of the combination\nof Contrastive Decoding with chain-of-thought prompting and self-consistency.\n6\nLIMITATIONS\nOur investigation is also limited mainly to the LLaMA family of models. While the method contin-\nues to provide benefit to larger LLaMA models, further work is required to definitively establish the\neffect of contrastive decoding on larger, tuned models.\n7\nCONCLUSION\nOur study shows that contrastive decoding can improve chain-of-thought reasoning in large lan-\nguage models. While challenges like factual recall remain, this strengthens the case for contrastive\ndecoding as a simple, general-purpose method to elicit more desirable behavior from large language\nmodels.\n9\nREPRODUCIBILITY STATEMENT\nThe training process and model architecture for the 1.5B-parameter LLaMA model used as the ama-\nteur in several results is publicly available, but the weights are not, which limits public reproducibil-\nity of results relying on that model. The results on FLAN-T5, as well as the negative-prompting\nstudy and examination of 7B-LLaMA as an amateur, are all built on entirely open-source models\nand data.\nREFERENCES\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, and Zhifeng Chen et al. Palm 2 technical report,\n2023.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language, 2019.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola:\nDecoding by contrasting layers improves factuality in large language models, 2023.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language\nmodels, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge,\n2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A\ncontinuous effort to measure large language models\u2019 reasoning performance, 2023.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies, 2021.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning,\n2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration, 2020.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension, 2017.\n10\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels, 2020.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang.\nDiscriminator-guided multi-step reasoning with language models, 2023.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization,\n2022.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen-\neration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 158\u2013167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1015.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. Dexperts: Decoding-time controlled text generation with experts and anti-experts,\n2021.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling, 2023.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\nenglish math word problem solvers, 2021.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering, 2018.\nOpenAI. Gpt-4 technical report, 2023.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\nword problems?, 2021.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei\nHuang, and Huajun Chen. Reasoning with language model prompting: A survey, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\nsarial winograd schema challenge at scale, 2019.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Common-\nsense reasoning about social interactions, 2019.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge, 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.\nTowards understanding chain-of-thought prompting: An empirical study of what matters, 2023a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n2023b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models,\n2023.\n11\nKevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies. Association for Computational Linguistics,\n2021. doi: 10.18653/v1/2021.naacl-main.276.\nGal Yona, Or Honovich, Itay Laish, and Roee Aharoni. Surfacing biases in large language models\nusing contrastive input decoding, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence?, 2019.\n12\nA\nAPPENDIX\nA.1\nCODE IMPLEMENTATION\nWe include PyTorch implementations of contrastive decoding in Algorithm 1 and Algorithm 2\nAlgorithm 1: Original formulation\n# expert logits - unnormalized scores from the expert model\n# amateur logits - unnormalized scores from the amateur model\n# amateur temp - temperature to normalize amateur distribution\n# alpha - masking threshold\nexpert probs = softmax(expert logits, dim=-1)\namateur probs = softmax(amateur logits / amateur temp, dim=-1)\ncutoff = alpha*expert probs.max(dim=-1, keepdim=True).values\ndiffs = log(expert probs) - log(amateur probs)\ncd logits = diffs.masked fill(expert probs < cutoff, -float(\u2019inf\u2019))\nAlgorithm 2: Our formulation\n# expert logits - unnormalized scores from the expert model\n# amateur logits - unnormalized scores from the amateur model\n# alpha - masking threshold\n# beta - expert-amateur tradeoff parameter\ncutoff = log(alpha) + expert logits.max(dim=-1, keepdim=True).values\ndiffs = (1 + beta)*expert logits - beta*amateur logits\ncd logits = diffs.masked fill(expert logits < cutoff, -float(\u2019inf\u2019))\n13\nA.2\nEQUIVALENCE - MASKING\nFor the sake of completeness, we show the equivalency between the two masking schemes. We\nrestrict our consideration to the generation of a single token. Let\n- V be the vocabulary size of each model\n- {ei}V\ni=1 be the logits produced by the expert model\n- {ai}V\ni=1 be the logits produced by the amateur model\n- Ne and Na be the normalization terms of the softmax function; for example, Ne = PV\nj=1 exp(ei)\nThe probability that the expert assigns to token i after the softmax is by definition pe(i) = exp(si)\nNe\nNow consider any token that is masked out. We have that pe(i) < \u03b1 \u2217 pe(imax), where imax is the\nthe token that maximizes pe.\nBecause ex and log x are both strictly increasing functions, we obtain:\nsi \u2212 log Ne < log \u03b1 + smax \u2212 log Ne\nsi < log \u03b1 + smax\nThese two conditions are equivalent, and so we can mask tokens by thresholding their logits against\nlog \u03b1 + smax.\nFurther, let us introduce an expert temperature parameter \u03c4 \u2208 (0, \u221e) that scales the logits arbitrarily.\nThen we obtain a new set of logits ci = si\n\u03c4\nBy then substituting \u03b1\u03c4 = \u03b11/\u03c4, we obtain the same mask. Thus the mask is a function that depends\nonly on the quantity \u03c4 log \u03b1, or equivalently \u03b1 exp(\u03c4). As we later show, we can fix \u03c4 = 1 by\nintroducing a new hyperparameter \u03b2. So if we fix \u03c4 = 1, then our mask depends solely on \u03b1.\nLetting p(i)\ne\ncorrespond to the post-softmax probability that the expert assigns to token i, we have\nshown that the valid set produced by our method is the same as in the original:\nVvalid =\n\u001a\nj \u2208 V, p(j)\ne\n\u2265 1\n\u03b1 max\nk\u2208V p(k)\ne\n\u001b\nA.3\nEQUIVALENCE - LOGIT COMBINATION\nTo be concise, first define q(i) = pe(i)\npa(i) be the ratio of the probability assigned by the expert model\nover the probability from the amateur model, both on token i.\nFurther, let si denote the value of the logit that CD assigns to token i. Then\nsi = (1 + \u03b2) log pe(i) \u2212 \u03b2 log pa(i)\nexp(si) = pe(i)q(i)\u03b2\npcd(i) \u221d pe(i)q(i)\u03b2\nExpanding this, we obtain:\npcd(i) \u221d exp ((1 + \u03b2)ei \u2212 \u03b2ai)\nwhich is equivalent to simply linearly combining the expert and amateur logits.\nWhen we introduce temperatures, the equation becomes\n14\npcd(i) \u221d exp\n\u00121 + \u03b2\n\u03c4e\nei \u2212 \u03b2\n\u03c4a\nai\n\u0013\nmask(i) = f(\u03c4e log \u03b1)\nWhen sampling, the temperature with which we sample from the CD logits is introduced as \u03c4out\npcd(i) \u221d exp\n\u0012 1\n\u03c4out\n\u00121 + \u03b2\n\u03c4e\nei \u2212 \u03b2\n\u03c4a\nai\n\u0013\u0013\nThese four parameters \u03c4out, \u03c4e, \u03c4a and \u03b2 combine into only two coefficients \u2013 one for ei and one for\nei.\npcd(i) \u221d exp (\u03baeei \u2212 \u03baaai)\nWe now fix \u03c4a and \u03c4e to 1. We can obtain almost the same range of values with the \u03c4out, \u03b2 formu-\nlation as with the \u03bae, \u03baa formulation. The only exception is the case for which \u03bae = \u03baa, which\nwe exclude after finding that weighing expert and amateur equally gives worse results than down-\nweighing the amateur. Despite this exception, we prefer the \u03b2, \u03c4out formulation because it decouples\nthe scale of the final logits from the \u03b2 parameter: in expectation \u03b2 does not scale the logits up or\ndown, and so when sampling \u03c4 will affect generation diversity and \u03b2 will affect the expert-amateur\ntradeoff.\n15\nB\nMULTIPLE-CHOICE BETA SWEEP RESULTS\nHere we include the plots for all beta sweeps through the multiple-choice tasks.\n16\nC\nFLOP ESTIMATES\nWe follow Kaplan et al. (2020) and estimate the number of flops for one forward pass of a Trans-\nformer model with N non-embedding parameters as roughly 2N. For the purposes of this analysis,\nwe drop the negligibly small 2nlayernctxdattn term. This allows us to consider the cost of generating\none token to be constant regardless of the context length.\nWith this approximation, we find that contrastive decoding adds\n1.5\n65.2 \u2248 2.30% to the number of\nFLOPs per forward pass for a 65B expert model. To ensure a fair comparison, we also include the\nsmall increase in generation lengths induced by CD (see Table 6), bringing its total percent increase\nup to 3.25%.\n17\n"
  },
  {
    "title": "Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)",
    "link": "https://arxiv.org/pdf/2309.08968.pdf",
    "upvote": "21",
    "text": "Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\nLanguage Models for Dynamic Inference\nParsa Kavehzadeh2, Mojtaba Valipour1, Marzieh Tahaei2,\nAli Ghodsi1, Boxing Chen2, and Mehdi Rezagholizadeh2\n1University of Waterloo\n2Huawei Noah\u2019s Ark Lab\n{mojtaba.valipour, ali.ghodsi}@uwaterloo.ca,\n{parsa.kavehzadeh, mehdi.rezagholizadeh, marzieh.tahaei, boxing.chen}@huawei.com\nAbstract\nLarge language models (LLMs) have revolu-\ntionized natural language processing (NLP)\nby excelling at understanding and generating\nhuman-like text. However, their widespread de-\nployment can be prohibitively expensive. Sort-\nedNet is a recent training technique for en-\nabling dynamic inference by leveraging the\nmodularity in networks and sorting sub-models\nbased on computation/accuracy in a nested\nmanner.\nWe extend SortedNet to genera-\ntive NLP tasks, making large language mod-\nels dynamic without any Pre-Training and by\nonly replacing Standard Fine-Tuning (SFT)\nwith Sorted Fine-Tuning (SoFT). Our approach\nboosts model efficiency, eliminating the need\nfor multiple models for various scenarios dur-\ning inference.\nWe show that this approach\ncan unlock the power of intermediate layers\nof transformers in generating the target out-\nput. Our sub-models remain integral compo-\nnents of the original model, minimizing stor-\nage requirements and transition costs between\ndifferent computational/latency budgets. The\nefficacy of our proposed method was demon-\nstrated by applying it to tune LLaMA 2 13B on\nthe Stanford Alpaca dataset for instruction fol-\nlowing and TriviaQA for closed-book question\nanswering. Our results show the superior per-\nformance of sub-models in comparison to Stan-\ndard Fine-Tuning and SFT+ICT (Early-Exit),\nall achieved with efficient tuning and without\nadditional memory usage during inference.\n1\nIntroduction\nLarge language models are revolutionizing the way\nwe interact with information in today\u2019s world (Hoff-\nmann et al., 2022; Brown et al., 2020; Penedo et al.,\n2023; Scao et al., 2022). New models are continu-\nally emerging, demonstrating their capabilities in\nunderstanding and, more importantly, in generating\nhuman-like text. Notably, models such as ChatGPT,\nLLaMA 2 70B (Touvron et al., 2023b), and Falcon\n180B (Almazrouei et al., 2023) have had a profound\nimpact on the applicability of large language mod-\nels (LLMs). However, deploying these expansive\nlanguage models can become prohibitively expen-\nsive.\nWhat distinguishes this new era of ChatGPT-like\nmodels is their ability to perform an extraordinar-\nily wide array of tasks in natural language pro-\ncessing (NLP), reasoning, and more, all through\nbehavior cloning (Wei et al., 2021; Wang et al.,\n2022). In fact, a single model can leverage the\nstrong contextual learning ability offered by Stan-\ndard Fine-Tuning to address numerous tasks, span-\nning from language comprehension to complex rea-\nsoning. While this unified usage simplifies the\ndeployment of these models as general assistants,\nit remains highly inefficient. Enabling dynamic\ninference, where the computational resources allo-\ncated to a given query vary at inference time, can\nsignificantly enhance the practicality of employing\nsuch models in real-time scenarios. This enables\nthe use of smaller models when the budget is lim-\nited or latency is critical. It is important to note\nthat dynamic inference strategies for large models\nwith a substantial number of parameters should not\nrequire loading different models during inference.\nPrevious research has explored methods for train-\ning dynamic models capable of adapting to evolv-\ning resource constraints (Cai et al., 2019; Hou et al.,\n2020; Xin et al., 2020; Fan et al., 2019). However,\nexisting approaches often rely on complex training\nprocedures or necessitate modifications to the orig-\ninal model architecture. SortedNet (Valipour et al.,\n2023) introduces a novel approach to training deep\nneural networks that leverages the inherent mod-\nularity of these networks to construct sub-models\nwith varying computational loads. This method\nsorts sub-models hierarchically based on their com-\nputation/accuracy characteristics, facilitating effi-\ncient deployment during inference. Furthermore, it\nemploys an efficient updating scheme combining\nrandom sub-model sampling with gradient accumu-\narXiv:2309.08968v2  [cs.CL]  8 Feb 2024\nlation to minimize the training cost. Consequently,\nwith a single round of training, numerous models\ncan be obtained within a single model.\nWhile the SortedNet approach has primarily\nbeen applied to vision and language understand-\ning tasks, given the significant impact of generative\nlanguage models in today\u2019s AI landscape, the effi-\ncacy of this method for generative tasks in NLP is\nof considerable interest. In fact, being able to make\na large language model dynamic without the need\nfor Pre-Training and only at the cost of a round of\nStandard Fine-Tuning can open doors to efficient\ninference of these models without incurring addi-\ntional expenses associated with common model\ncompression methods like knowledge distillation\nand pruning, among others. Moreover, since all\nthe resultant models are components of the original\nmodel, the storage requirements and the cost as-\nsociated with transitioning between different com-\nputation demands become minimal. Otherwise,\nmanaging multiple models for various scenarios\nduring inference becomes impractical.\nIn this study, we challenge the conventional ap-\nproach of relying solely on the last layer\u2019s con-\ntextual embeddings and use Sorted Fine-Tuning\n(SoFT) in place of Standard Fine-Tuning to en-\nhance the performance of these models across mul-\ntiple layers. By doing so, we aim to provide new\ninsights into the efficiency and effectiveness of mid-\ndle layers in producing high-quality results for spe-\ncific downstream tasks. Our proposed approach\ncan potentially optimize these sub-models in addi-\ntion to the main model, ultimately enhancing their\noverall performance. In this paper, we seek to an-\nswer the following questions through systematic\nevaluation:\ni) Do the intermediate layers resulting from Stan-\ndard Fine-Tuning of a large language model gen-\nerate accurate and meaningful outputs? ii) Does\nStandard Fine-Tuning exhibit a sorted behavior,\nmeaning that later layers produce more accurate\nand meaningful results than earlier layers? If so, to\nwhat extent? iii) How can we enhance this sorted\nbehavior with minimal cost?\nTo answer these questions, we employ LLaMA 2\n13B and perform both Standard Fine-Tuning (SFT)\nand Sorted Fine-Tuning (SoFT) on the Stanford Al-\npaca (Taori et al., 2023) and TriviaQA (Joshi et al.,\n2017) datasets. For Sorted Fine-Tuning, we tar-\nget 8 sub-models and share the LLM head among\nthem to ensure cost parity. We utilize the PandaLM\nbenchmark (Wang et al., 2023) to assess the perfor-\nmance of the sub-models on Alpaca dataset. Our\nfindings demonstrate the superior performance of\nSoFT in comparison to SFT and even to memory-\ndemanding methods like Early Exit (Xin et al.,\n2020). The contributions of this paper can be sum-\nmarized as follows:\n\u2022 Extending the SortedNet method for tuning\nauto-regressive language models for genera-\ntive tasks by sharing a single LLM head layer\namong sub-models.\n\u2022 Generating 8 nested sub-models, ranging from\n12 to 40 layers, from LLaMA2 13B by apply-\ning Sorted Fine-Tuning on the Stanford Al-\npaca dataset and TriviaQA benchmarks and at\na cost equivalent to Standard Fine-Tuning.\n\u2022 Evaluating the performance of the sub-models\nof a LLaMA 2 and demonstrating the effec-\ntiveness of SoFT in enhancing the ability of\nintermediate layers for text generation and\nquestion answering through extensive evalua-\ntion.\n2\nRelated Work\nThis section briefly introduces the most relevant\npapers to our work.\nMany-in-One Models\nDeep neural networks\n(DNNs) are often overparameterized, motivating\nresearchers to explore ways to use the parameters\nof the models more efficiently. More number of\nparameters lead to higher costs of deployment for\nneural networks. Moreover, in practice, these over-\nparametrized DNNs are expected to accommodate\ncustomers with varying requirements and computa-\ntional resources. To address these diverse demands,\none can think of training models of different sizes,\nwhich can be prohibitively costly (in terms of train-\ning and memory), or another alternative is to train\nmany-in-one networks (Cai et al., 2019). Many-\nin-one solutions aim to train a network along with\nsome of its sub-networks simultaneously for spe-\ncific tasks. For example, we can consider the Early-\nExit method (Xin et al., 2020), wherein a prediction\nhead is fine-tuned on top of specific intermediate\nlayers within a network. Another approach is Layer\nDrop (Fan et al., 2019), which trains a network in\nany depth by randomly dropping the layers during\ntraining. While both Early-Exit and Layer Drop\nare simple solutions, they are not state-of-the-art in\nterms of performance. In Early-Exit, we only train\nthe output prediction layer on top of each interme-\ndiate layer, and this layer might not have enough\nQuery: Summarize a meeting from the given list of bullet points. Be sure to convert shorthand into a first-hand account.\nRose: Analyze data and presents findings\nJohn: propose new idea\nJane: appointed to head project\nTom: need more time to fix software bug\nLayer\nLLaMA Response (SFT+ICT)\nSorted LLaMA Response (SoFT)\n12\nsummatronics21220f0f ...\nThe meeting was held in order to discuss the project and its plans for\nthe next few months. The presenters were present in the room, and they\nwere able to provide a detailed explanation of the project and its goals.\nThe feedback from the meeting was positive and encouraged the team to\ncontinue working towards the goal of delivering the project.\n16\nThe first thing we hear about these characters individually separately\napart aparticularly divisidedly mannered mannerednesses ...\nThe meeting held at the company\u2019s headquarters, where the three named\nindividuals were present. The first person named in the report is Rousness,\nwho has been assigned to task task task. The second person named in the\nreport is John, who has proposed a new idea. The third person named\nin the report is Jane, who has been assigned to task task task. The third\nperson named in the report is Jane, who has been assigned to task task\ntask. The second person named in the report is Jane, who has been\nassigned to task task task.\n20\nmeeting held meeting held meeting held participants discussion discus-\nsion discussion notable mentionsnone none None notable mentionsnone\n...\nThe meeting held in the room with Rose, John, Jane, and Tom discusses\nthe need for analyzing data and presenting findings , as well as the\nproposal of new ideas by John. Jane is appointed to head the project, and\nTom needs more time to fix the software bug.\n24\nmeeting summary: attended attended attended attended ...\nThe meeting was attended by Rose, John, Jane, and Tom. Rose was\ntasked with analyzing data and presenting findings. John proposed a new\nidea. Jane was appointed to head the project. Tom was granted additional\ntime to fix the software bug.\n28\nsummary: During meeting participants discuss data analysis findings\npresented by Rose followed proposal new idea by John appoint Jane head\nproject Tom request additional time fix software bug\nIn the meeting, Rose analyzed data and presented findings, John proposed\na new idea, and Jane was appointed to head the project. Tom also stated\nthat he needed more time to fix the software bug.\n32\nsummary: During meeting attended by Rose Analyst who analyzed\ndata presented findings proposed new idea by John appointed Jane head\nproject Tom Software engineer request more time fix bug\nIn the meeting, Rose analyzed data and presented her findings, while\nJohn proposed a new idea. Jane was appointed to head the project, and\nTom requested more time to fix a software bug.\n36\nDuring the meeting, Rose presented her analysis of data collected from\nvarious sources. John proposed a new idea that could potentially improve\nefficiency and increase productivity. Jane was appointed head of a new\nproject that will be launched soon. Tom requested more time to fix a\nsoftware bug that has been affecting the company\u2019vity since last month.\nIn the meeting, Rose analyzed data and presented her findings, while\nJohn proposed a new idea. Jane was appointed to head the project, and\nTom requested more time to fix the software bug.\n40\nAt the meeting, Rose presented the analysis of the data and shared her\nfindings with the team. John proposed a new idea that he believed could\nbenefit the company. Jane was appointed to head a new project that the\ncompany had recently taken on. Tom, however, requested more time\nto fix a software bug that had been discovered. The team discussed the\nimportance of the bug fix and agreed to give Tom the additional time he\nneeded.\nIn the meeting, Rose analyzed data and presented her findings. John\nproposed a new idea, and Jane was appointed to head the project. Tom\nrequested more time to fix the software bug.\nTable 1: Generated responses in different layers of LLaMA (Early Exit) and Sorted LLaMA for two samples from PandaLM\nvalidation set. Black phrases mean the answer is following the instruction and Red ones demonstrate errors and hallucinations.\ncapacity to retain a good performance. Layer Drop,\nconversely, suffers from the abundant number of\npossible sub-models in training, which makes the\ntraining process exhaustive and sub-optimal. Fur-\nthermore, this approach requires tuning the extent\nof dropping layers during training. This additional\nhyper-parameter, layer drop rate during training\ndetermines the best size and setting of the model\nat the inference time. Deviating from the train-\ning drop rate at the inference time can result in a\nsignificant drop in performance.\nCai et al. (2019) in Once for All (OFA) pro-\nposed an alternative solution to neural architecture\nsearch (NAS). OFA requires training the model\nand all possible sub-models in an arbitrary pro-\ngressive way followed by a separate search phase.\nDyna-BERT (Hou et al., 2020) is another work\nthat targets training Dynamic pre-trained many-\nin-one BERT models in two stages: first, distill-\ning from the main network to the width adaptive\nnetworks and then distilling from the width adap-\ntive networks to depth adaptive networks. Both\nwidth adaptive and depth adaptive networks have\na limited pre-defined set of width and depth for\nthe sub-models. While both OFA and DynaBERT\nhave shown successful results, their solutions are\nhardly applicable to multi-billion-parameter LLMs\nbecause of their complicated multi-stage training\nprocess and their search and knowledge distillation\nrequirements. SortedNet (Valipour et al., 2023) is a\nrecent method that forms and trains sub-models of\na network in a sorted manner while not requiring\nany search during training or inference. SortedNet\nhas shown superior performance compared to other\n12 (4.1B)\n-0.118\n0.276\n0.512\n0.441\n0.371\n0.071\n-0.553\n-0.797\n16 (5.4B)\n0.024\n0.329\n0.506\n0.441\n0.394\n0.132\n-0.547\n-0.753\n20 (6.6B)\n0.318\n0.612\n0.703\n0.706\n0.647\n0.494\n-0.203\n-0.479\n24 (7.9B)\n0.494\n0.694\n0.762\n0.797\n0.715\n0.621\n0.024\n-0.268\n28 (9.2B)\n0.535\n0.729\n0.812\n0.788\n0.735\n0.6\n0.076\n-0.259\n32 (10.4B)\n0.671\n0.829\n0.9\n0.874\n0.824\n0.756\n0.235\n-0.115\n36 (11.7B)\n0.691\n0.844\n0.891\n0.874\n0.788\n0.741\n0.271\n-0.076\n40 (13B)\n0.724\n12\n0.847\n16\n0.9\n20\n0.874\n24\n0.794\n28\n0.75\n32\n0.318\n36\n-0.059\n40\nSoFT\nSFT + ICT (Early-Exit)\n12 (4.1B)\n-0.165\n0.147\n0.518\n0.541\n0.429\n0.253\n-0.471\n-0.797\n16 (5.4B)\n-0.047\n0.194\n0.518\n0.55\n0.468\n0.353\n-0.365\n-0.753\n20 (6.6B)\n0.312\n0.553\n0.712\n0.747\n0.691\n0.6\n-0.071\n-0.479\n24 (7.9B)\n0.465\n0.606\n0.776\n0.829\n0.762\n0.738\n0.212\n-0.268\n28 (9.2B)\n0.476\n0.706\n0.812\n0.818\n0.774\n0.724\n0.218\n-0.259\n32 (10.4B)\n0.665\n0.788\n0.882\n0.894\n0.821\n0.806\n0.374\n-0.115\n36 (11.7B)\n0.662\n0.797\n0.885\n0.912\n0.797\n0.782\n0.409\n-0.076\n40 (13B)\n0.688\n12\n0.835\n16\n0.9\n20\n0.906\n24\n0.8\n28\n0.803\n32\n0.45\n36\n-0.059\n40\nSoFT\nSFT\nFigure 1: SoFT vs. SFT + ICT (Early-Exit) (Left) and SoFT vs. SFT (Right). Note that for our SoFT method, the output\nprediction layer is shared between all sub-models whereas, for Early-Exit, a separate prediction head is learned per sub-model,\nmaking inference inefficient. Both SoFT and SFT had equivalent training time (2 Epochs) in this experiment. The number in each\ncell is calculated by considering wins as the times SoFT sub-models (rows) were preferred, losses as the times SFT sub-models\n(columns) were preferred and ties when non of them were preferred (Equation 2). Algorithm performance is correlated to cell\nwhiteness: white is better, zero is on-par, dark is worse.\npreviously mentioned methods in terms of simplic-\nity, performance, scalability, and generalization.\nConsidering these benefits, we target deploying the\nSortedNet training algorithm for developing many-\nin-one LLMs.\nMany-in-One Large Language Models (LLMs)\nLarge language models have recently gained sig-\nnificant attention in the literature (Touvron et al.,\n2023a; Brown et al., 2020; OpenAI, 2023; Chowd-\nhery et al., 2022; Ouyang et al., 2022).\nIn\npractice, these LLMs serve users with different\ntasks, expectations, and computational budget re-\nquirements (Sun et al., 2022).\nThere are two\ntypes of adaptation approaches to make LLMs\nsuitable for customer requirements: first is the\nso-called parameter efficient tuning (PEFT), and\nsecond is model compression.\nIn PEFT, the\ncore backbone model remains the same, and we\njust update much smaller adapter parameters (e.g.\nLoRA (Hu et al., 2021), KRONA (Edalati et al.,\n2022), Adapter (Houlsby et al., 2019; Pfeiffer\net al., 2020), DyLoRA (Valipour et al., 2022), Lad-\nder Side-Tuning (Sung et al., 2022)) and Com-\npacter (Karimi Mahabadi et al., 2021). In model\ncompression, the larger model is compressed us-\ning any model compression solutions such as\nknowledge distillation (Hinton et al., 2015; Hsieh\net al., 2023; Wu et al., 2023), pruning (Bansal\net al., 2023), and quantization (Prato et al., 2019;\nDettmers et al., 2023), a good related survey can\nbe found in (Zhu et al., 2023). Even though PEFT\nsolutions are pretty popular with LLMs, they do\nnot provide dynamic-size LLMs. Model compres-\nsion solutions can provide models with different\nsizes, but they need to train each compressed model\nseparately, and they are not many-in-one models.\nTo the best of our knowledge, this work is\nthe first attempt to obtain many-in-one generative\nLLMs by applying the Sorted Fine-Tuning to the\nLLaMA 13B model. Considering the benefits of\nmany-in-one networks and the growing applica-\ntion of LLMs, we are hoping that this will help\nthe community to build more efficient large lan-\nguage models that can be deployed adaptive during\ninference using methods such as SoFT.\n3\nMethodology\nThis paper focuses on making generative LLMs\nmany-in-one by unlocking the potential of in-\ntermediate layers through the SortedNet ap-\nproach (Valipour et al., 2023).\nLet\u2019s consider a language model f(x; \u03b8) with the\nparameters \u03b8 and the input x. The following is the\nsorted training procedure:\nForming Sub-Networks\nFirst, we need to form\nthe sub-networks of the LLM. For the sake of sim-\nplicity and without loss of generality, we focus on\nthe depth-wise sub-networks. Supposed that the\nsub-network fn(x; \u03b8n) refers to the first n layers\nof f(x; \u03b8). In this paper, the language model is\nconsidered to be LLaMA2 13B. Since LLaMA2\ncomprises 40 layers, we define the sub-networks as\nn \u2208 B = {12, 16, 20, 24, 28, 32, 36, 40}.\nCalculating the Output of Sub-Networks\nEach\nsub-model\u2019s output will be predicted using the\nshared output prediction head from the last layer\n(original network). Remember that in the LLaMA\nmodel, there is an RMSNorm layer (Zhang and Sen-\nnrich, 2019) before the output prediction head. This\nRMSNorm is added before the shared prediction\nhead of every sub-model. This normalization may\nbe an important factor that helps Sorted LLaMA to\ngeneralize better for all sub-models.\nObjective Function\nLet Ln(x; \u03b8n) be the loss\nfor the nth sub-model for input batch x. To train\nthe network, we define the loss as the summation\nof the losses of all these sub-models:\nL =\nP\nn\u2208B Ln(x; \u03b8n)\n|B|\n(1)\nFor the experiments conducted in the paper,\n|B| = 8. Note that these sub-models have shared\nparameters through a nested style i.e. \u03b81 \u2282 \u03b82... \u2282\n\u03b8n.\nTraining Dataset\nWe utilized the Stanford Al-\npaca dataset (Taori et al., 2023), which includes\ndemonstrations of 52K instruction-following exam-\nples. We also used TriviaQA open-domain QA\nbenchmark (Joshi et al., 2017) including 110K\nclosed-book question-answer pairs.\nEvaluation\nIn this paper, in addition to embed-\nding the last layer, we evaluate the quality of the\nembeddings of intermediate outputs spanning from\nblock 1 to n. PandaLM benchmark (Wang et al.,\n2023) compares the output of different sub-models.\nPandaLM deploys a large language model (Fine-\nTuned LLaMA 7b) to judge the quality of generated\ntext from two sources. PandaLM provides a valida-\ntion set consisting of 170 instructions1, to evaluate\ntarget models for instruction-following tasks. To\nensure that the order of the models\u2019 responses does\nnot influence the judgment of the PandaLM evalu-\nator, we reported an average score under both the\nModel 1 first and the Model 2 first scenarios. The\noutput of the PandaLM evaluation is the number of\nwins, denoted as W, the number of losses, denoted\nas L, and the number of ties, denoted as T, in the\nvalidation set. The final reported score has been\ncalculated using the following formula:\nScore = (W \u2212 L)\nT\n(2)\n1github.com/WeOpenML/PandaLM/blob/main/data/testset-\ninference-v1.json\nThe final score is a number between -1 and 1, in\nwhich 1 represents a strong win rate and -1 means\na poor performance of the model.\nWe used accuracy (exact match) as the evalua-\ntion metric for the TriviaQA benchmark.\n12 (4.1B)\n-0.05\n-0.556\n-0.668\n-0.756\n16 (5.4B)\n0.068\n-0.468\n-0.609\n-0.721\n20 (6.6B)\n0.385\n-0.168\n-0.385\n-0.503\n24 (7.9B)\n0.506\n0.053\n-0.156\n-0.259\n28 (9.2B)\n0.582\n0.071\n-0.085\n-0.212\n32 (10.4B)\n0.721\n0.321\n0.112\n-0.068\n36 (11.7B)\n0.697\n0.341\n0.159\n-0.056\n40 (13B)\n0.668\n12\n0.382\n20\n0.194\n28\n-0.041\n36\nSoFT\nExtracted Fine-Tuning\n12 (4.1B)\n0.138\n-0.453\n-0.55\n-0.659\n16 (5.4B)\n0.265\n-0.276\n-0.35\n-0.524\n20 (6.6B)\n0.565\n0.032\n-0.156\n-0.291\n24 (7.9B)\n0.597\n0.226\n0.044\n-0.171\n28 (9.2B)\n0.685\n0.226\n0.038\n-0.171\n32 (10.4B)\n0.741\n0.403\n0.15\n-0.038\n36 (11.7B)\n0.756\n0.418\n0.235\n0.044\n40 (13B)\n0.788\n12\n0.397\n20\n0.271\n28\n0.053\n36\nSoFT\nExtracted Fine-Tuning\nFigure 2: SoFT vs. Extracted Fine-Tuning. The left figure\nshows an equal training time setup (2 epochs), and the figure\non the right considers two extra training epochs for SoFT.\nBaseline\nThe primary objective of the LLM in\nthis paper is to follow the provided instructions\nby a query. Therefore, following the setup of Al-\npaca (Taori et al., 2023), we fine-tuned LLaMA2\n13B on the Stanford Alpaca Dataset with two se-\ntups: (1) Regular Standard Fine-Tuning (SFT) as\nthe baseline, focusing only on the training of the\nlast layer of the network as the common practice\nin the literature; (2) Sorted Fine-Tuning (SoFT),\ncalculating loss for multiple outputs from layer 12\nto layer 40 (last layer) with four intervals, and train-\ning multiple models simultaneously as explained\nin the previous section.\n4\nExperiments\nThis section delves into the experiments\u2019 specifics\nand the analysis provided to understand better the\neffect of Sorted Fine-Tuning over the performance\nof a large language model like LLaMA2 (Touvron\net al., 2023b). The details of the experimental setup\nused for these experiments are available in the ap-\npendix A.1. Before diving into results, we are\ngoing to define certain notations that we used for\ndifferent setups in our experiments:\n\u2022 SoFT/SFT: We first train the model with\nSoFT or SFT paradigms and use the sub-\nmodels after training without any further train-\ning of the language model head for intermedi-\nate layers.\nEpoch 2\nEpoch 4\nEpoch 6\n0\n5\n10\n15\n20\n25\n30\n35\n40\nAccuracy\nAccuracy of SFT and SoFT over Different Training Epochs on TriviaQA Validation Set\nExtracted\nSFT+ICT\nLayer12\nLayer16\nLayer20\nLayer24\nLayer28\nLayer32\nLayer36\nLayer40\nSoFT\nSFT\nFigure 3: The results of TriviaQA. We reported case-sensitive\nexact match accuracy as the main metric. SFT+ICT and Ex-\ntracted Fine-Tuned results can be found in Epochs 2, as we\nfound Epoch 2 checkpoint saturated for the original SFT ex-\nperiment (main LLaMA2 13b model with 40 layers).\n\u2022 SFT+Intermediate Classifier Tuning (ICT):\nWe first train the model with SFT paradigm\nand then further fine-tune the language model\nhead exclusively for each sub-model while\nkeeping their weights frozen. The SFT+ICT\nis also known as Early-Exit (Xin et al., 2020)\nin the literature.\n\u2022 Extracted Fine-Tuning: We extract the sub-\nmodels from the learned weights of the pre-\ntrained original model and train each sub-\nmodel separately.\n4.1\nWhat is the effect of sorting information\nacross layers of a generative model?\nAs mentioned before, we generated responses for\nall the layers n \u2208 B for both SFT and SoFT-based\ntrained models. Then, we conducted a pair-wise\ncomparison between all the sub-models in the two\ntrained models using the PandaLM evaluator. As\nthe results suggest in Figure 1, sorted training sig-\nnificantly unlocks the potential of intermediate lay-\ners in generating the desired output. Some gener-\nated examples can be found in Table 1.\nSorted LLaMA (aka SoFT) is outperforming reg-\nular fine-tuning (SFT) in nearly all layer compar-\nisons by a meaningful margin, as shown through\nautomated evaluation in Figure 1.\nIt might be noted that the Layer 12 performance\nof SFT is slightly better compared to Layer 12 of\nSorted LLaMA. We argue this is happening be-\ncause the outputs of early layers in SFT are mostly\ngibberish (see Table 1 as an example), and the Pan-\ndaLM evaluator has not been trained on such data.\nHence the automatic evaluation results for this layer\nare not meaningful. To further investigate the rea-\nson behind the results for early sub-models, we\nconducted human evaluation on 6 cells of two ta-\nbles in Figure 1 (Layer 12 of SFT and SFT+ICT\nvs Layers 12,16, and 20 SoFT) to verify our claim.\nWe observed that SoFT early sub-models could sig-\nnificantly outperform sub-model layer 12 of both\nSFT and SFT+ICT models, proving the negative\nimpact of gibberish text on PandaLM evaluator per-\nformance. As we go to higher layers in SFT, the\ngenerated text becomes meaningful, which makes\nthe comparison with the Sorted LLaMA layer coun-\nterpart more reasonable.\nMoreover, to improve SFT results, inspired by\nEarly-Exit (Xin et al., 2020), we also tried the sce-\nnario in which a separate classifier head is dedi-\ncated to all sub-models of SFT. This method has\nbeen introduced in the notation section as SFT+ICT.\nThese classification heads have been trained an ad-\nditional epoch after SFT tuning while keeping the\nbase model frozen. Note that this setting suffers\nfrom significant memory overhead during tuning\nand inference compared to our SoFT method. In\nfact, the extra number of parameters for SFT+ICT\n(Early Exit) is |B| \u2212 1 \u00d7 D \u00d7 V , where |B| is the\nnumber of sub-models, D is the hidden size of the\nmodel, and V is the vocabulary size. For LLaMA\n2 13B, this is equivalent to 1B extra parameters.\nThe results of comparing sorted with the early\nexit are shown in figure 1 (Left). Despite hav-\ning far more parameters, SFT+ICT (Early-Exit)\nunderperforms our sorted tuning for most sub-\nmodels. According to the results, the sub-model in\nSorted LLaMA with 36 layers performs almost as\nwell as regular fine-tuning of the full-size model.\nThis showcases the impressive ability of our pro-\nposed paradigm to generate powerful, small sub-\nmodels that perform similarly to the original model.\nAnother experiment that has been conducted in\nappendix A.3, further investigated the impact of\nlonger training time for SoFT. The results show that\nour model was still under-trained, and we could ob-\nserve a significant improvement in Sorted LLaMA\nperformance with longer training time.\nMoreover, we compared the performance of\nSorted LLaMA sub-models with the actual capac-\nity of these models by fine-tuning the sub-models\nseparately and reporting the results in both equal\ntraining time and more training time for SoFT. We\nextracted 4 sub-models (Layer 12, Layer 20, Layer\n28, and Layer 36) and each time fully fine-tuned\nthe extracted sub-model separately for two epochs\non the Alpaca dataset. Figure 2 and Table 9 shows\nthe comparison between Extracted Fine-Tuned and\nPandaLM\nTriviaQA\nAuto-regressive Decoding\nModel\nTime per Token (ms)\nScore\nRejection Ratio\nTime per Token (ms)\nAccuracy\nRejection Ratio\nLayer 40 (full)\n94.07\n-\n-\n91.27\n37.95\n-\nSpeculative Decoding\nDraft Model\nTime per Token (ms)\nScore\nRejection Ratio\nTime per Token (ms)\nAccuracy\nRejection Ratio\nLayer 12\n80.86 (1.16\u00d7)\n-0.144\n0.37\n110.50 (0.82\u00d7)\n34.36\n0.72\nLayer 16\n84.10 (1.11\u00d7)\n-0.211\n0.31\n118.92 (0.76\u00d7)\n34.16\n0.70\nLayer 20\n84.50 (1.11\u00d7)\n-0.144\n0.26\n139.78 (0.65\u00d7)\n34.19\n0.66\nInstance-Aware Dynamic Inference\nModel\nTime per Token (ms)\nScore\nRejection Ratio\nTime per Token (ms)\nAccuracy\nRejection Ratio\nLayer 12:40\n69.91 (1.34\u00d7)\n-0.050\n-\n81.01 (1.12\u00d7)\n36.53\n-\nTable 2: Speed-up in inference time on three PandaLM and TriviaQA benchmarks by utilizing Speculative Decoding and\nInstance-Aware Dynamic Inference techniques. Score column in PandaLM section means the score of the model versus the\nAuto-regressive generated results based on Equation 2.\nSoFT sub-models. The first part in Table 9 shows\nthe equal training budget setup (2 Epochs) com-\nparison in which SFT demonstrates slightly better\nperformance compared to the similar SoFT sub-\nmodels. Further training SoFT will lead to better\nsorted sub-models in which SoFT outperforms the\nfully fine-tuned sub-models, proving the positive\nimpact of SoFT on the performance of lower sub-\nmodels.\nThe analysis of the computational overhead of\nSoFT can be found in A.2.\n4.2\nHow does SoFT work for other domains?\nWe further evaluated Sorted LLaMA in a different\ndomain from the instruction following, selecting\nthe TriviaQA (Joshi et al., 2017) benchmark to\nassess the sub-models performance in open-domain\nclosed-book questions answering.\nFigure 3 shows the performance of SoFT and\nthree baselines (SFT, Extracted Fine-Tuning and\nSFT+ICT) in different checkpoints through the\ntraining procedure on the TriviaQA benchmark.\nSoFT sub-models show significant superior per-\nformance compared to SFT and SFT+ICT coun-\nterparts in all sub-models. Similar to PandaLM,\nthe gap between SoFT and SFT full-model per-\nformance is small in TriviaQA, which can un-\nderscore the SoFT capability in maintaining full-\nmodel performance compared to SFT. We also did\nExtracted Fine-Tuning on intermediate sub-models\nfor 2 Epochs and results demonstrate close perfor-\nmance of SoFT intermediate layers to Extracted\nFine-Tuning counterparts.\n4.3\nHow can SoFT accelerate text generation?\nImproving Speculative Sampling\nSpeculative\nDecoding (SD) is a technique introduced by (Chen\net al., 2023) to increase the speed of text decoding\nin large models. The method utilizes a large target\nand smaller draft models to generate tokens faster.\nWe can verify the generated tokens by the large\nmodel in parallel. We used the same paradigm for\nSorted LLaMA as we used earlier sub-models as\ndraft and the full-size model as the target model.\nAs the parameters have been shared between the\nlarge and draft models in this setup, we can avoid\nany extra memory overhead, unlike the standard\nSpeculative Sampling. Table 2 reports the inference\nresults of using speculative decoding on Alpaca and\nTriviaQA benchmarks in SoFT by using three dif-\nferent sub-models as drafts (Layer 12, 16, and 20).\nAs shown combining Speculative decoding and\nSorted LLaMA can speed up the token generation\nup to 1.16\u00d7 compared to normal auto-regressive\ndecoding in PandaLM with negligible performance\ndrop. Due to the short average length of answers in\nTriviaQA, speculative decoding does not result in\nspeed up as the draft generation process does not\nfind any opportunity to accelerate inference.\nInstance-Aware Dynamic Inference\nWe also dy-\nnamically utilize SoFT sub-models to increase the\nspeed of text generation during inference. Based on\nthe confidence of the sub-model\u2019s predicted tokens,\nwe decide which sub-model should generate which\ntoken. Given each token, the sub-models would\nprocess the token in size order (first smallest sub-\nmodel 12, then 16, and so on). Wherever in this\nprocedure, the confidence of the predicted token by\na sub-model reach a pre-defined threshold, the pre-\ndicted token would be chosen as the next token and\nexit the model. We also implemented an adaptive\ncaching mechanism in order to utilize KV caching\nin this non-trivial scenario where each token can\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n40\n36\n32\n28\n24\n20\n16\n12\nLayers of SFT Model\n0.38\n2.1\n2.7\n3\n3.6\n4\n4\n4.3\n4.1\n4.6\n5.2\n4.7\n5.4\n5.3\n5.3\n5.3\n5.4\n5.3\n5.5\n5.2\n3\n5.4\n6.3\n6.2\n6.7\n6.7\n6.7\n6.7\n6.8\n7\n7\n6.8\n7.2\n7.1\n7\n7.2\n7\n7.1\n7.1\n6.9\n4.1\n6.5\n6.8\n6.4\n6.9\n7.1\n7.2\n7.2\n7.3\n7.4\n7.5\n7.1\n7.5\n7.4\n7.4\n7.5\n7.6\n7.5\n7.7\n7.7\n4.5\n6.9\n7.3\n6.7\n7.3\n7.4\n7.5\n7.4\n7.7\n7.7\n7.8\n7.6\n7.8\n7.7\n7.8\n7.9\n7.9\n7.8\n7.8\n7.9\n4.7\n7\n7.2\n6.9\n7.1\n7.2\n7.4\n7.5\n7.5\n7.6\n7.5\n7.5\n7.4\n7.4\n7.5\n7.5\n7.5\n7.5\n7.6\n7.4\n4.6\n6.7\n6.8\n6.7\n6.8\n7\n7\n7\n7.1\n7\n7.1\n7\n6.8\n7\n7.1\n7.1\n7.1\n7\n7\n7\n5.5\n7.1\n7.5\n7\n7.3\n7.5\n7.5\n7.6\n7.6\n7.7\n7.6\n7.4\n7.4\n7.5\n7.6\n7.4\n7.6\n7.5\n7.5\n7.4\n6.6\n7.6\n7.9\n7.1\n7.4\n7.3\n7.4\n7.4\n7.3\n7.2\n7.3\n7.2\n7.1\n7.2\n7.2\n7.1\n7\n7.2\n7.2\n7\nKL-Divergence Comparison of Logits Distributions with Sorted Last Layer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n40\n36\n32\n28\n24\n20\n16\n12\nLayers of SFT Model\n0.8\n0.7\n0.6\n0.6\n0.6\n0.5\n0.5\n0.5\n0.5\n0.5\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.6\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.5\n0.2\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.5\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.09\n0.08\n0.08\n0.09\n0.08\n0.08\n0.4\n0.2\n0.1\n0.1\n0.1\n0.1\n0.09\n0.08\n0.08\n0.08\n0.07\n0.07\n0.07\n0.06\n0.06\n0.06\n0.06\n0.05\n0.05\n0.05\n0.3\n0.1\n0.1\n0.08\n0.07\n0.06\n0.06\n0.05\n0.05\n0.05\n0.04\n0.04\n0.04\n0.04\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.2\n0.06\n0.04\n0.03\n0.03\n0.02\n0.02\n0.02\n0.02\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01 0.008 0.008 0.01 0.006\n0.1\n0.02\n0.02\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01 0.008 0.009 0.008 0.008 0.009 0.005 0.008 0.006\nCosine Similarity Comparison of Hidden States Representations with Sorted Last Layer\n1\n2\n3\n4\n5\n6\n7\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a) SortedLLaMA Last Layer - LLaMA sub-models\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n40\n36\n32\n28\n24\n20\n16\n12\nLayers of Sorted Model\n0.38\n2.1\n2.7\n3\n3.6\n4\n4\n4.3\n4.1\n4.6\n5.2\n4.7\n5.4\n5.3\n5.3\n5.3\n5.4\n5.3\n5.5\n5.2\n1\n3\n3.5\n3.6\n4.2\n4.6\n5.3\n5\n5\n5.6\n6.1\n5.9\n6\n6.1\n5.9\n6.5\n6.4\n6.5\n6.4\n6.4\n1.1\n3.3\n3.9\n4.1\n4.7\n5.1\n5.5\n5.3\n5\n5.8\n6.3\n5.5\n5.9\n6.1\n5.9\n5.9\n6\n5.6\n6.1\n6.5\n1.1\n3.2\n3.9\n4.1\n4.9\n5\n5.6\n5.4\n5.3\n5.5\n5.9\n6\n6.1\n6.2\n5.8\n6.2\n5.9\n6.2\n6.2\n5.9\n1.1\n3.7\n4.2\n4.8\n5.2\n5.4\n5.9\n5.7\n5.8\n5.4\n6\n5.8\n6.1\n6.3\n6.6\n6.5\n6.8\n6.4\n6.2\n5.9\n1.3\n4.1\n4.4\n4.8\n5.8\n5.9\n6.2\n6.1\n6.2\n6\n6.3\n6.5\n6.1\n6.3\n6.7\n6.4\n6.2\n6.4\n6.4\n6.3\n1.5\n4.5\n4.9\n5.7\n6.1\n5.9\n6.2\n6.3\n6.5\n6.6\n6.8\n6.3\n6.8\n6.7\n6.8\n6.8\n6.5\n6.5\n7\n6.5\n1.7\n4.8\n5.6\n6.3\n6.4\n6.6\n6.5\n6.7\n6.9\n6.7\n6.7\n6.3\n6.8\n6.7\n6.6\n7\n6.6\n6.7\n7\n6.8\nKL-Divergence Comparison of Logits Distributions with SFT Last Layer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n40\n36\n32\n28\n24\n20\n16\n12\nLayers of Sorted Model\n0.8\n0.7\n0.6\n0.6\n0.6\n0.5\n0.5\n0.5\n0.5\n0.5\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.7\n0.5\n0.5\n0.5\n0.4\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.6\n0.5\n0.4\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.6\n0.4\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.5\n0.4\n0.3\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.4\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.3\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.09\n0.09\n0.09\n0.1\n0.09\n0.09\n0.3\n0.1\n0.1\n0.1\n0.09\n0.08\n0.08\n0.08\n0.07\n0.07\n0.07\n0.07\n0.07\n0.06\n0.06\n0.06\n0.06\n0.06\n0.05\n0.06\nCosine Similarity Comparison of Hidden States Representations with SFT Last Layer\n1\n2\n3\n4\n5\n6\n7\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(b) SortedLLaMA sub-models - LLaMA Last Layer\nFigure 4: An inter-model comparison of sub-models based on output logits and hidden state cosine similarity. The numbers are\naverage of all 170 samples in the PandaLM validation set. The similarity is stronger if the cell is darker.\nexit from a different layer. Table 2 shows that\nInstance-Aware Dynamic Inference can speed up\nthe normal auto-regressive approach in all bench-\nmarks up to 1.34\u00d7 in PandaLM and 1.12\u00d7 in Triv-\niaQA. Furthermore dynamic inference can result\nin better performance in PandaLM and TriviaQA\ncompared to speculative decoding.\n4.4\nAnalysis\n4.4.1\nA comparison between the learned\nprobability distribution of SoFT versus\nSFT\nSorted tuning aims to make sub-models perfor-\nmance similar to the full model. To explore the\nefficacy of the SoFT in closing the gap between sub-\nmodels and the full model in instruction following\ntask, we measure the similarity between probabil-\nity distributions of each token in each sub-model\nversus the full model using the Kullback\u2013Leibler\n(KL) divergence. Figure 4 (Left) compares the\nprobability distribution of Sorted LLaMA and SFT\nsub-models at different output positions.\nFigure 4a (Left) compares different SFT layers\nand the last Sorted LLaMA layer. The figure shows\nthat only SFT\u2019s full-size output distribution is close\nto the sorted full-size model, while the other lay-\ners\u2019 distribution diverges faster in the initial steps\ncompared to the SoFT. This is expected as the lan-\nguage model head is unfamiliar with the learned\nrepresentation of the middle layers in SFT. In the\nnext section, we compared the learned representa-\ntions of different sub-models to understand SoFT\u2019s\nimpact better.\nFigure 4b (Left) compares the output distribution\nof all sorted layers to the last SFT layer. Compared\nto Figure 4a (Left), Figure 4b (Left) Sorted LLaMA\ncan preserve the output distribution close to the\nSFT full-size model even in lower layers for initial\noutput tokens.\nThe comparison between the last layer and the\nlayers 12 to 36 in the SFT model is shown in Figure\n5a (Left). It is clear from this figure that the output\ndistribution diverges quickly compared to the last\nlayer after generating a few initial tokens, even in\nhigher layers like 36 and 32. It is important to note\nthat this evaluation was generated without adjusting\nthe classifier head.\nFinally, Figure 5b (Left) demonstrates that in\nSorted LLaMA, the likelihood distribution of the\nproduced outcome becomes increasingly more sim-\nilar to the full-size model as we get closer to the\nlast layer.\n4.4.2\nA comparison between the learned\nrepresentation of SoFT versus SFT\nDuring regular fine-tuning, no connection between\nthe language model head and sub-models can in-\ntensify the divergence of probability distributions\nin Figure 4 (Left). To overcome this, we conducted\nanother experiment to compare the hidden state rep-\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n36\n32\n28\n24\n20\n16\n12\nLayers of SFT Model\n2.6\n5.3\n6\n7\n7\n7.5\n7.5\n7.8\n7.9\n7.9\n7.8\n7.9\n8.2\n7.9\n8.1\n8.3\n8.2\n7.8\n8.1\n7.9\n3.6\n6.5\n6.8\n7.5\n7.7\n8.1\n8.6\n8.5\n8.4\n8.6\n8.6\n8.6\n9.1\n8.8\n8.8\n9\n9.1\n8.9\n9.5\n9.2\n4.1\n6.6\n7.3\n7.6\n8.2\n8.5\n8.6\n8.6\n8.9\n9.1\n9.5\n9.1\n9.3\n9.5\n9.3\n9.4\n9.4\n9.4\n9.6\n9.7\n4.6\n6.8\n7.5\n8.1\n8.3\n8.9\n9.1\n9.3\n9.3\n9.4\n9.6\n9.5\n9.5\n9.6\n9.5\n9.9\n9.8\n10\n9.9\n10\n5.7\n7.1\n7.4\n8.1\n8\n8.3\n8.4\n8.3\n8.5\n8.4\n8.5\n8.6\n8.4\n8.6\n8.7\n8.8\n8.8\n8.8\n8.9\n8.9\n7.4\n7.5\n7.8\n8.5\n8.2\n8.4\n8.4\n8.3\n8.2\n8.5\n8.4\n8.3\n8.6\n8.4\n8.3\n8.3\n8.3\n8.3\n8.5\n8.4\n8.4\n7.7\n8.1\n8.3\n7.9\n8.2\n8.1\n8\n8\n7.9\n8\n7.9\n8\n8\n7.9\n7.8\n7.9\n7.9\n8\n7.9\nKL-Divergence Comparison of Logits Distributions with SFT Last Layer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n36\n32\n28\n24\n20\n16\n12\nLayers of SFT Model\n0.7\n0.4\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.7\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.6\n0.3\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.5\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.09\n0.08\n0.09\n0.08\n0.08\n0.08\n0.07\n0.07\n0.07\n0.06\n0.06\n0.06\n0.06\n0.4\n0.2\n0.1\n0.09\n0.07\n0.07\n0.07\n0.06\n0.05\n0.05\n0.04\n0.04\n0.04\n0.04\n0.03\n0.04\n0.03\n0.03\n0.03\n0.03\n0.2\n0.07\n0.05\n0.03\n0.03\n0.02\n0.02\n0.02\n0.02\n0.02\n0.01\n0.01\n0.01\n0.01\n0.01 0.009 0.009 0.007 0.006 0.007\n0.1\n0.03\n0.02\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01 0.008 0.009 0.01 0.008 0.009 0.009\nCosine Similarity Comparison of Hidden States Representations with SFT Last Layer\n3\n4\n5\n6\n7\n8\n9\n10\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(a) LLaMA sub-models vs LLaMA Last Layer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n36\n32\n28\n24\n20\n16\n12\nLayers of Sorted Model\n0.81\n1.9\n2.4\n2.5\n3\n3.1\n3.7\n3.7\n3.9\n3.9\n4.5\n4.6\n4.4\n4.2\n4.7\n4.9\n5.2\n5.5\n5.4\n5.5\n0.88\n2.4\n3\n3.2\n4.1\n4\n4.3\n4.3\n4.8\n5\n5\n4.7\n5.1\n5\n5.6\n5.4\n5.5\n5.4\n5.5\n6.2\n0.95\n2.5\n3.2\n3.3\n4\n4.2\n4.6\n4.9\n5\n5.1\n5.2\n5.5\n5.5\n5.3\n5.6\n5.6\n6.2\n6\n6\n5.9\n0.97\n3.1\n3.5\n4.1\n4.7\n4.7\n5.1\n5.3\n5.2\n5.6\n5.4\n5.5\n6.2\n5.9\n6\n5.8\n6.6\n6.2\n6\n5.9\n1.2\n3.6\n4\n4.6\n5.1\n5.3\n5.5\n5.5\n5.9\n6.3\n6\n6.4\n6.2\n5.8\n6.3\n5.6\n6.2\n5.9\n6.1\n6.2\n1.4\n3.9\n4.5\n5.1\n5.7\n5.8\n5.7\n6\n6.3\n6.5\n6.7\n6.4\n6.5\n6.5\n6.3\n6.4\n6.1\n6.5\n6.5\n6.6\n1.6\n4.6\n5.3\n6\n6.4\n6.2\n6.3\n6.4\n6.7\n6.5\n6.4\n6.1\n6.6\n6.6\n6.8\n6.6\n6.3\n6.9\n6.8\n6.8\nKL-Divergence Comparison of Logits Distributions with Sorted Last Layer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nPositions of Generated Response\n36\n32\n28\n24\n20\n16\n12\nLayers of Sorted Model\n0.8\n0.7\n0.7\n0.7\n0.6\n0.6\n0.6\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.4\n0.4\n0.4\n0.4\n0.4\n0.3\n0.8\n0.6\n0.5\n0.6\n0.5\n0.5\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.7\n0.5\n0.5\n0.5\n0.4\n0.4\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.6\n0.5\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.5\n0.3\n0.3\n0.3\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.4\n0.2\n0.2\n0.2\n0.2\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n0.3\n0.2\n0.1\n0.1\n0.1\n0.09\n0.09\n0.08\n0.07\n0.07\n0.07\n0.08\n0.07\n0.07\n0.06\n0.06\n0.07\n0.06\n0.06\n0.06\nCosine Similarity Comparison of Hidden States Representations with Sorted Last Layer\n1\n2\n3\n4\n5\n6\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(b) SortedLLaMA sub-models - SortedLLaMA Last Layer\nFigure 5: An intra-model comparison of sub-models based on output logits and hidden state cosine similarity. The similarity is\nstronger if the cell is darker.\nresentation in the last and middle layers just before\npassing the hidden states to the language model\nhead. Figure 4 (Right) compares the learned hid-\nden state representation of SFT and Sorted LLaMA\nsub-models at various positions in the output. This\nwill make the analysis independent of the language\nmodel head. We used cosine similarity to measure\nthe difference between the two representations. As\nshown using heatmaps, the cosine similarities are\nhighly correlated to the KL-Divergence compari-\nson explained in the previous section.\nFigure 4a (Right) compares all SFT sub-models\nwith the Sorted last layer regarding hidden represen-\ntation similarity. Again, similar to probability dis-\ntribution analysis, the similarity between the SFT\nsub-model and Sorted last layer tends to fade imme-\ndiately after generating the first few tokens, while\nFigure 4b demonstrates the capability of Sorted\nLLaMA sub-models in preserving the learned rep-\nresentations closely similar to the SFT last layer\nhidden states.\nFigure 5a (Right) depicts the heatmap of hidden\nstates cosine similarity among different SFT sub-\nmodels compared to the SFT last layer. Similar to\nits left plot, the similarity quickly diminishes after\na few tokens, and this fade is more considerable in\nearlier layers.\nOn the other hand, Figure 5b (Right) shows that\nthe representations of Sorted sub-models stay sim-\nilar to the Sorted last layer even after generating\nmultiple initial tokens.\n5\nConclusion\nThis work presents sorted LLaMA, a many-in-one\nlanguage model for dynamic inference obtained us-\ning Sorted Fine-Tuning (SoFT) instead of Standard\nFine-tuning. Sorted LLaMA unlocks the potential\ncapability of intermediate layers, offering dynamic\nadaptation without pre-training or additional costs\nrelated to model compression. It presents a promis-\ning avenue for optimizing generative language mod-\nels in NLP. Our approach makes the deployment\nof these models more efficient. As all sub-models\nremain integral components of the original model,\nthe burden of storage requirements and transition\ncosts between different computational demands is\nminimized, making the management of multiple\nmodels during inference a practical reality.\nOur systematic evaluation of instruction follow-\ning and questions answering benchmarks chal-\nlenged conventional wisdom by empowering mid-\ndle layers to produce high-quality results. This, in\nturn, enables dynamic inference of LLMs with a\nhighly efficient tuning method (SoFT), ultimately\noptimizing the usage of LLMs. Our encouraging\nresults show the promising capability of SortedNet\n(Valipour et al., 2023) to train multiple language\nmodels with different sizes at once.\n6\nLimitations\nDespite showing the effectiveness of the Sorted-\nNet approach for large language models, further\nresearch is necessary to better understand the scope\nof its applicability in LLMs. For example, apply-\ning this method during pre-training, sorting other\nmodel dimensions such as attention heads and hid-\nden dimensions, and investigating the impact of\nchoosing a specific architecture could offer poten-\ntial avenues for future research. Our study might be\nslightly biased to automated evaluation, requiring\nfurther investigation through human evaluation.\nAcknowledgements\nWe thank Mindspore, which is a new deep learning\ncomputing framework, for partial support of this\nwork.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMaitha Alhammadi, Mazzotta Daniele, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. The falcon series of language models: To-\nwards open frontier models.\nHritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal,\nSravan Bodapati, Katrin Kirchhoff, and Dan Roth.\n2023. Rethinking the role of scale for in-context\nlearning: An interpretability-based case study at 66\nbillion scale. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 11833\u201311856,\nToronto, Canada. Association for Computational Lin-\nguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nHan Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang,\nand Song Han. 2019. Once-for-all: Train one net-\nwork and specialize it for efficient deployment. arXiv\npreprint arXiv:1908.09791.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nAli Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Par-\ntovi Nia, James J Clark, and Mehdi Rezagholizadeh.\n2022. Krona: Parameter efficient tuning with kro-\nnecker adapter. arXiv preprint arXiv:2212.10650.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. Advances in Neural\nInformation Processing Systems, 33:9782\u20139793.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790\u20132799. PMLR.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n2023. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller\nmodel sizes. arXiv preprint arXiv:2305.02301.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021. Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural\nInformation Processing Systems, 34:1022\u20131035.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9,\nKyunghyun Cho,\nand Iryna Gurevych. 2020.\nAdapterfusion: Non-destructive task composition for\ntransfer learning. arXiv preprint arXiv:2005.00247.\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh.\n2019.\nFully\nquantized\ntrans-\nformer for machine translation.\narXiv preprint\narXiv:1910.10485.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning for\nlanguage-model-as-a-service. In International Con-\nference on Machine Learning, pages 20841\u201320855.\nPMLR.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nLst: Ladder side-tuning for parameter and memory\nefficient transfer learning. Advances in Neural Infor-\nmation Processing Systems, 35:12991\u201313005.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nMojtaba\nValipour,\nMehdi\nRezagholizadeh,\nIvan\nKobyzev, and Ali Ghodsi. 2022. Dylora: Parameter\nefficient tuning of pre-trained models using dynamic\nsearch-free low-rank adaptation.\narXiv preprint\narXiv:2210.07558.\nMojtaba Valipour, Mehdi Rezagholizadeh, Hossein Ra-\njabzadeh, Marzieh Tahaei, Boxing Chen, and Ali\nGhodsi. 2023. Sortednet, a place for every network\nand every network in its place: Towards a generalized\nsolution for training many-in-one neural networks.\narXiv preprint arXiv:2309.00255.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi\nYang, Cunxiang Wang, Hao Chen, Chaoya Jiang,\nRui Xie, Jindong Wang, Xing Xie, et al. 2023.\nPandalm: An automatic evaluation benchmark for\nllm instruction tuning optimization. arXiv preprint\narXiv:2306.05087.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nMinghao Wu, Abdul Waheed, Chiyu Zhang, Muham-\nmad Abdul-Mageed, and Alham Fikri Aji. 2023.\nLamini-lm:\nA diverse herd of distilled mod-\nels from large-scale instructions.\narXiv preprint\narXiv:2304.14402.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020.\nDeebert: Dynamic early exit-\ning for accelerating bert inference. arXiv preprint\narXiv:2004.12993.\nBiao Zhang and Rico Sennrich. 2019. Root mean square\nlayer normalization. Advances in Neural Information\nProcessing Systems, 32.\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weip-\ning Wang. 2023.\nA survey on model compres-\nsion for large language models.\narXiv preprint\narXiv:2308.07633.\nA\nAppendix\nA.1\nExperimental Setup\nWe used the pre-trained LLaMA2 13b weights, pub-\nlicly available on Hugging Face, as our starting\npoint. For SFT+ICT (Early-Exit) setup, we froze\nthe parameters of the transformer blocks and only\nfurther trained the weights of the language model\nhead classifier for one additional epoch. We used\na batch size of 32 and gradient accumulation of\n8. The learning scheduler was cosine annealing.\nThe learning rate was set to 2e-5 and seed to 42.\nWe trained the models on 8 V100 32GB GPUs.\nThe same GPUs were used during inference time.\nThe training maximum input sequence length was\n2024, with a maximum of 50 (TriviaQA) and 256\n(PandaLM) generated tokens during inference. Ad-\nditionally, we used greedy search as the decoding\nstrategy in all of our experiments. We also extended\nthe huggingface assisted decoding code to imple-\nment Speculative Decoding and Instance-Aware\nAdaptive Inference. In Speculative Decoding, we\nused adaptive K window-size (the same as hug-\ngingface) starting with K=4. In Instance-Aware\nDynamic Inference, we set the confidence thresh-\nolds of intermediate layers as follow: Layer 12 =\n0.95, Layer 16 = 0.95, Layer 20 = 0.9, Layer 24 =\n0.9, Layer 28 = 0.8, Layer 32 = 0.8 and Layer 36 =\n0.7.\nA.2\nComputational Overhead of SoFT\nMethod\nAvg Time per Epoch (s)\nAvg Memory Usage per Epoch (MB)\nSFT\n25,765.95\n99,168\nSoFT\n25,269.87 (0.98\u00d7)\n125,682\nTable 3: Training Time and Memory Usage comparison of\nSoFT and SFT on Alpaca dataset.\nGiven the nested pattern of sub-models and the\nfact that we share the language model head across\nsub-models, we do not expect to see any compu-\ntation overhead for SoFT versus SFT. To validate\nthis claim, we compared SoFT and SFT regarding\ntraining time and memory usage in our experiment\non the Alpaca dataset. Table 3 shows the result for\ntwo main experiments of SoFT and SFT. As ex-\npected, training with SoFT leads to equal training\ntime compared to SFT. During training, SoFT has\nabout 25% memory overhead in PyTorch compared\nto SFT, which only provides a single full model at\nthe end.\nA.2.1\nCase Specific Analysis\nTable 1 shows a sample of instructions from the\nPandaLM benchmark and the generated responses\nby SFT+ICT (Early-Exit) and Sorted LLaMA sub-\nmodels. Sorted LLaMA performs better in preserv-\ning and transferring the last layer performance to\nearlier sub-models based on the information made\nvisible by black (related to the query) and red (hal-\nlucinations, irrelevant, etc.) colors.\nSorted sub-models generate almost correct an-\nswers from the 20 layers sub-model, while the first\nmeaningful result from SFT+ICT sub-models ap-\npears in layer 28. Other samples generated by SoFT\nand Early-Exit can be found in A.4.\nA.3\nAdditional Experiments\nTable 4 shows the detailed results of the Sorted\nLLaMA and SFT performance on the PandaLM\nbenchmark in different setup in equal training time\n(2 Epochs for both SFT and SoFT). As we can\nsee, sorted sub-models outperform their SFT coun-\nterparts (and even higher sub-models) , while in\nSFT+ICT (Early-Exit), as we go higher in sub-\nmodels (e.g. layer 36), we can see a noticeable\nimprovement in the performance compared to the\nSFT. This can demonstrate the importance of tun-\ning the language model classifier in improving text\ngeneration capability in the latest layers in the stan-\ndard fine-tuning format.\nTable 5 shows the SoFT and SFT comparison in\na different training time setup in which SoFT has\naccess to doubled training time (4 Epochs). Results\nshow that Sorted LLaMA can outperform standard\nfine-tuned LLaMA further by continuing the SoFT\nprocess. The improvement in Sorted LLaMA sub-\nmodels performance can be observed specifically\nin intermediate layers.\nA.4\nAnalysis\nTable 6 and 7 show some samples generated by sub-\nmodels of LLaMA (SFT+ICT) and SoFT on Pan-\ndaLM evaluation set. In the first query of Table 6,\nLLaMA sub-models until layer 36 struggle to gen-\nerate relevant responses about books in the Crime\nand Mystery genre. Sorted LLaMA sub-models,\nhowever, start to address the related novels from\nlayer 24. The second query in the table is a simpler\ninstruction, which is a multi-label classification\nproblem. Again Sorted LLaMA sub-models start\nto generate the correct label in much earlier lay-\ners (layer 20) compared to the LLaMA sub-models\n(layer 24). Table 7 first example shows the per-\nformance gap of the LLaMA and Sorted LLaMA\nintermediate sub-models even in a more severe case.\nTo write a review about a restaurant with certain\naspects, LLaMA sub-models before layer 32 hallu-\ncinate or generate gibberish, while Sorted LLaMA\nstarts to generate a complete review addressing key\npoints mentioned in the instruction even in the first\nsub-model (layer 16). In the second example, the\nsame pattern occurs where SoFT sub-models can\ngenerate meaningful response starting from layer\n16 while LLaMA first reasonable text happens at\nlayer 36.\nTable 8 shows an example of SFT and SoFT per-\nformance on TriviaQA benchmark. While LLaMA\nstruggles to generate single answer token even in\nthe sub-models close to the last layer, SoFT could\ntransfer the question answering capability of LLM\nuntil sub-layer 20 and still generate the correct final\nanswer.\nAfter all, Sorted LLaMA sub-models demon-\nstrate the ability to generate more comprehensive\n(Example 1 of Table 6 and Table 7) and informa-\ntive (Table 6 example 2) answers in earlier layers\ncompared to LLaMA. Based on our observation,\nLLaMA sub-models mostly tend to generate irrele-\nvant or even gibberish in earlier blocks (layers 12\nto 24), while the generated texts by Sorted LLaMA\nexhibit sufficient learned information to answer the\ninput instruction despite having much fewer param-\neters.\nSorted LLaMA/LLaMA\n12 (4.1B)\n16 (5.4B)\n20 (6.6B)\n24 (7.9B)\n28 (9.2B)\n32 (10.4B)\n36 (11.7B)\n40 (13B)\nSoFT vs. SFT\n12 (4.1B)\n71.0/99.0/0.0\n97.5/72.5/0.0\n129.0/41.0/0.0\n131.0/39.0/0.0\n121.5/48.5/0.0\n106.5/63.5/0.0\n45.0/125.0/0.0\n17.0/152.5/0.5\n16 (5.4B)\n81.0/89.0/0.0\n101.5/68.5/0.0\n128.5/40.5/1.0\n131.5/38.0/0.5\n124.0/44.5/1.5\n114.0/54.0/2.0\n52.0/114.0/4.0\n18.0/146.0/6.0\n20 (6.6B)\n111.5/58.5/0.0\n132.0/38.0/0.0\n144.5/23.5/2.0\n147.5/20.5/2.0\n141.5/24.0/4.5\n132.5/30.5/7.0\n73.5/85.5/11.0\n32.5/114.0/23.5\n24 (7.9B)\n124.5/45.5/0.0\n136.5/33.5/0.0\n150.0/18.0/2.0\n154.5/13.5/2.0\n148.0/18.5/3.5\n144.5/19.0/6.5\n98.0/62.0/10.0\n44.5/90.0/35.5\n28 (9.2B)\n125.5/44.5/0.0\n145.0/25.0/0.0\n153.0/15.0/2.0\n153.5/14.5/2.0\n148.0/16.5/5.5\n143.5/20.5/6.0\n96.5/59.5/14.0\n45.0/89.0/36.0\n32 (10.4B)\n141.5/28.5/0.0\n152.0/18.0/0.0\n159.0/9.0/2.0\n160.0/8.0/2.0\n152.0/12.5/5.5\n150.5/13.5/6.0\n108.5/45.0/16.5\n55.5/75.0/39.5\n36 (11.7B)\n141.0/28.5/0.5\n152.5/17.0/0.5\n159.0/8.5/2.5\n161.5/6.5/2.0\n150.0/14.5/5.5\n148.5/15.5/6.0\n112.0/42.5/15.5\n53.0/66.0/51.0\n40 (13B)\n143.5/26.5/0.0\n156.0/14.0/0.0\n160.5/7.5/2.0\n161.0/7.0/2.0\n150.0/14.0/6.0\n150.0/13.5/6.5\n115.5/39.0/15.5\n52.5/62.5/55.0\nSoFT vs. SFT+ICT(Early-Exit)\n12 (4.1B)\n75.0/95.0/0.0\n108.5/61.5/0.0\n128.5/41.5/0.0\n122.5/47.5/0.0\n116.5/53.5/0.0\n91.0/79.0/0.0\n37.5/131.5/1.0\n17.0/152.5/0.5\n16 (5.4B)\n86.5/82.5/1.0\n113.0/57.0/0.0\n127.0/41.0/2.0\n122.0/47.0/1.0\n117.5/50.5/2.0\n94.5/72.0/3.5\n36.0/129.0/5.0\n18.0/146.0/6.0\n20 (6.6B)\n111.5/57.5/1.0\n137.0/33.0/0.0\n143.5/24.0/2.5\n143.0/23.0/4.0\n137.0/27.0/6.0\n122.0/38.0/10.0\n60.0/94.5/15.5\n32.5/114.0/23.5\n24 (7.9B)\n126.5/42.5/1.0\n144.0/26.0/0.0\n149.0/19.5/1.5\n151.0/15.5/3.5\n143.0/21.5/5.5\n133.5/28.0/8.5\n76.5/72.5/21.0\n44.5/90.0/35.5\n28 (9.2B)\n130.0/39.0/1.0\n147.0/23.0/0.0\n153.5/15.5/1.0\n150.0/16.0/4.0\n143.5/18.5/8.0\n131.0/29.0/10.0\n79.0/66.0/25.0\n45.0/89.0/36.0\n32 (10.4B)\n141.5/27.5/1.0\n155.5/14.5/0.0\n161.0/8.0/1.0\n157.0/8.5/4.5\n151.0/11.0/8.0\n143.5/15.0/11.5\n89.5/49.5/31.0\n55.5/75.0/39.5\n36 (11.7B)\n143.0/25.5/1.5\n156.5/13.0/0.5\n160.0/8.5/1.5\n157.0/8.5/4.5\n148.0/14.0/8.0\n142.5/16.5/11.0\n92.5/46.5/31.0\n53.0/66.0/51.0\n40 (13B)\n146.0/23.0/1.0\n157.0/13.0/0.0\n160.5/7.5/2.0\n157.5/9.0/3.5\n149.0/14.0/7.0\n143.5/16.0/10.5\n97.5/43.5/29.0\n52.5/62.5/55.0\nTable 4: Pair-wise comparison for different layers (sub-models) in Standard Fine-Tuning and SoFT at equal training cost (2\nEpochs). Each cell consists of three values: Wins, Losses, Ties. Wins demonstrate the number of times that the generated text of\nthe sub-model in row (sorted) is preferred to the sub-model in column (Fine-Tuned) and Losses is the opposite. Numbers are\naverage of two separate experiments with different order of inputs to evaluator in order to neutralize the order bias.\nSorted LLaMA/LLaMA\n12 (4.1B)\n16 (5.4B)\n20 (6.6B)\n24 (7.9B)\n28 (9.2B)\n32 (10.4B)\n36 (11.7B)\n40 (13B)\nSoFT vs. SFT\n12 (4.1B)\n88.5/81.5/0.0\n108.0/62.0/0.0\n134.5/35.5/0.0\n135.0/35.0/0.0\n129.0/41.0/0.0\n120.0/49.0/1.0\n57.0/109.5/3.5\n23.5/144.0/2.5\n16 (5.4B)\n106.5/63.0/0.5\n120.0/50.0/0.0\n140.0/29.0/1.0\n144.5/24.5/1.0\n142.0/26.5/1.5\n136.0/32.0/2.0\n70.0/95.0/5.0\n34.5/124.5/11.0\n20 (6.6B)\n127.0/43.0/0.0\n138.5/31.5/0.0\n151.5/16.5/2.0\n152.0/17.0/1.0\n143.5/23.5/3.0\n144.0/21.5/4.5\n94.5/67.5/8.0\n47.0/99.5/23.5\n24 (7.9B)\n138.5/31.5/0.0\n149.5/20.5/0.0\n159.0/9.0/2.0\n158.0/10.5/1.5\n151.5/13.5/5.0\n149.0/15.5/5.5\n107.0/49.5/13.5\n53.0/81.0/36.0\n28 (9.2B)\n137.0/33.0/0.0\n149.0/21.0/0.0\n158.0/10.0/2.0\n159.5/8.5/2.0\n150.0/15.0/5.0\n149.5/15.0/5.5\n107.0/47.5/15.5\n50.5/78.0/41.5\n32 (10.4B)\n146.0/24.0/0.0\n157.0/13.0/0.0\n163.0/5.0/2.0\n163.0/5.0/2.0\n154.5/10.5/5.0\n151.5/12.5/6.0\n117.5/37.5/15.0\n63.5/62.0/44.5\n36 (11.7B)\n149.5/20.5/0.0\n160.0/10.0/0.0\n164.0/4.0/2.0\n162.5/5.5/2.0\n157.5/7.5/5.0\n154.0/10.0/6.0\n119.5/34.5/16.0\n62.5/60.0/47.5\n40 (13B)\n153.5/16.5/0.0\n163.0/7.0/0.0\n165.5/3.0/1.5\n163.5/4.5/2.0\n157.0/8.0/5.0\n156.0/8.5/5.5\n121.0/33.5/15.5\n67.5/52.0/50.5\nSoFT vs. SFT+ICT(Early-Exit)\n12 (4.1B)\n91.5/77.5/1.0\n123.5/46.5/0.0\n138.5/31.5/0.0\n134.0/36.0/0.0\n130.5/39.0/0.5\n107.5/59.0/3.5\n46.0/120.5/3.5\n23.5/144.0/2.5\n16 (5.4B)\n106.5/63.5/0.0\n128.5/41.0/0.5\n145.0/24.0/1.0\n144.5/25.0/0.5\n139.0/29.5/1.5\n122.5/43.0/4.5\n55.5/106.5/8.0\n34.5/124.5/11.0\n20 (6.6B)\n128.0/40.5/1.5\n142.0/27.5/0.5\n152.5/16.0/1.5\n148.0/19.0/3.0\n142.0/22.5/5.5\n131.0/31.0/8.0\n74.0/79.0/17.0\n47.0/99.5/23.5\n24 (7.9B)\n140.5/28.5/1.0\n155.0/15.0/0.0\n159.0/10.0/1.0\n156.0/10.5/3.5\n149.5/12.5/8.0\n141.5/19.0/9.5\n90.0/54.5/25.5\n53.0/81.0/36.0\n28 (9.2B)\n142.0/27.0/1.0\n155.5/14.5/0.0\n161.5/7.5/1.0\n155.5/10.5/4.0\n148.5/13.0/8.5\n141.0/19.0/10.0\n91.5/53.5/25.0\n50.5/78.0/41.5\n32 (10.4B)\n150.0/19.0/1.0\n161.0/9.0/0.0\n161.5/7.5/1.0\n160.0/6.0/4.0\n153.0/8.5/8.5\n145.0/14.0/11.0\n99.0/41.0/30.0\n63.5/62.0/44.5\n36 (11.7B)\n151.0/18.0/1.0\n163.5/6.5/0.0\n165.0/3.5/1.5\n161.0/5.0/4.0\n155.0/7.5/7.5\n147.5/12.0/10.5\n105.5/36.0/28.5\n62.5/60.0/47.5\n40 (13B)\n156.0/13.0/1.0\n165.5/4.5/0.0\n165.0/3.5/1.5\n161.0/5.0/4.0\n155.0/7.5/7.5\n150.5/10.0/9.5\n106.0/35.0/29.0\n67.5/52.0/50.5\nTable 5: Pair-wise comparison for different layers (sub-models) in Standard Fine-Tuning and SoFT (4 Epochs) after tuning\nLanguage Model classifier for each SFT sub-model.\nQuery: Choose the best books from the given genre. Crime & Mystery\nLayer\nLLaMA Response (SFT + ICT)\nSorted LLaMA Response (SoFT)\n12\nTherewiguptaranapped byoremandoesleven years ago agomairiensum...\nHere are some of the best books from the genre: 1. The Fear of the unknown: The\nFist of the Darkness2. The Fist of the Deaths of the Deaths of the Deaths of the\nDeaths of the Deaths of the Deaths ...\n16\nHere list list list!!!!!!!!!!!!!!!\u02d800e4ndersonally preferences personal ...\nHere is a list of top 10 books from the genre Crime & Mystery that you can read for\nyour next reading book list: 1. The Great mystery of the Mystery of the Mystery of\nthe Mystery of the Mystery of the ...\n20\nHere listicle lists some best books genreed intoxicatedly intoxicatedly ...\nHere are some popular and well-known crime and mystery books that you can choose\nfrom: 1. The Secret of the Mystery solved by the detective by Jameson Jameson is\na classic crime and mystery novel that is widely regarded as one of the best crime\nand mystery books ever written. 2. The Secret of the Mystery solved by the detective\nby Jameson Jameson is a classic crime and mystery novel that is widely regarded as\none of the best crime and mystery books ever written. 3. The Secret of the Mystery\nsolved ...\n24\nHere list contains some best books belonging belongs genre crime & mystery genre\nordered alphabetically alphabetically ...\nHere are some of the best crime and mystery books: 1. The Hatchery by Stephen\nKing - This book is a classic horror/ crime thriller that follows the story of a group of\nfriends who discover a hidden hatchery of horrors. 2. The Last Thistle by Murphy\nMurphy - This book is a thriller that follows the investigation of a murder in a small\ntown. The story is filled with suspense and twists that will keep you hooked. 3. The\nLast Thistle by Murphy Murphy - This book is a thriller that follows the investigation\nof a murder in a small town. The story is filled with suspense and twists that will keep\nyou hooked. 4. The Last Thistle by Murphy ...\n28\ncrime mystery books best crime mystery books crime mystery books best crime\nmystery ...\nHere are some of the best crime and mystery books that you might enjoy: 1. The\nGreat Gatsby by F. Scott Fitzgerald - This classic novel is a classic tale of love,\nbetrayal, and the dark side of the American Dream. 2. The Great Gatsby by F. Scott\nFitzgerald - This classic novel is a classic tale of love, betrayal, and the dark side of\nthe American Dream. 3. The Great Gatsby ...\n32\nCrime mystery genre has many great books to choose from. Here are some recom-\nmendations: crime mystery books recommended by readers: 1) crime mystery books\nrecommended by readers: Crime mystery genre has many great books to choose from.\nHere are some recommendations: crime mystery books ...\nCrime & Mystery is a popular genre, and there are many great books that fall into\nthis category. Here are some of the best crime and mystery books that have stood\nthe test of time and are widely acclaimed by readers and critics alike: 1. The\nHound of the Baskervilles by Sir Arthur Conan Doyle: This classic detective novel\nfeatures the iconic detective Sherlock Holmes and his sidekick, Dr. John Watson.\nThe story follows Holmes as he investigates the mysterious death of a wealthy man\nand the disappearance of his family. 2. The Silent Patient by Sarah C. Dessen:\nThis psychological thriller follows the story of a woman who is institutionized after\nshooting her husband. The novel explores themes of trauma, mental illness, and the\npower of love to heal. 3. The Silent majority by Agatha Christie: This classic mystery\nnovel features the beloved detective Hercule Poirot as he investigates the murder of a\nwealthy man. The novel is full of twists and turns, and the identity of the murderer is\nnot revealed until the end.\n36\nCrime fiction is a genre that has captivated readers for centuries with intriguvity and\nsuspenseful plots. Here are some of the best crime books that you should definitely\nread: 1. Sherlock Holmes series by Sir Arthur Conan Doyle: regarded as one of the\ngreatest fictional detectives of all time, Sherlock Holmes is known for his deductive\nreasoning and ability to solve complex cases. 2. Agatha Christie novels: regarded as\nthe queen of crime fiction, Agatha Christie wrote numerous novels and short stories\nthat are still widely read today. Some of her most famous works include Hercules\nPoirot series and Miss Marple series. Raymond Chandler novels: regarded as one\nof the greatest crime writers of all time, Raymond Chandler wrote novels set in Los\nAngeles featuring detective Philip Marlowe. Some of his most famous works include\n\u00a8The Big Sleep\u00e4nd \u00a8The Long Goodbye\u00a8. 4. crime novels by crime novelist Raymond\nChandler: regarded as one of the greatest crime writers of all time, Raymond Chandler\nwrote novels set in Los Angeles featuring detective Philip Marlowe. Some of his\nmost famous\nBased on your preference for crime and mystery books, here are some highly recom-\nmended titles: 1. To Kill a Mockingbird by Harper Lee: This classic novel explores\nthemes of racism, prejudice, and injustice in the Deep South during the 1930s. 2.\nThe Silent Patient by Alex Michaelides: A psychological thriller about a woman\nwho shoots her husband and then refuses to speak. 3. The Girl with the Dragon\nTattoo by Stieg Larsson: This novel kicks off the Millennium Trilogy, following the\ninvestigation of a missing journalist and the mysterious disappearance of a young\nwoman. 4. The Da Vinci Code by Dan Brown: This fast-paced thriller follows the\ntrail of a centuries-old conspiracy involving the Catholic Church, the Knights Templar,\nand the secrets of the Da Vinci Code. 5. The Girl with the Dragon Tattoo by Stieg\nLarsson: This novel kicks off the Millennium Trilogy, following the investigation of\na missing journalist and the mysterious\n40\nHere are some of the best books in the Crime and Mystery genre: 1. The Hound of the\nBaskervilles by Sir Arthur Conan Doyle 2. The Maltese Falcon by Dashiell Hammett\n3. The Big Sleep by Raymond Chandler 4. The Adventures of Sherlock Holmes by\nSir Arthur Conan Doyle 5. The Thin Man by Dashiell Hammett 6. The Murder of\nRoger Ackroyd by Agatha Christie 7. The Silence of the Lambs by Thomas Harris 8.\nThe Red House Mystery by A.A. Milne 9. The Secret of the Old Clock by Carolyn\nKeene 10. The Adventures of the Black Stallion by Walter Farley.\nHere are some highly recommended crime and mystery books: 1. To Kill a Mock-\ningbird by Harper Lee 2. The Hound of the Baskervilles by Sir Arthur Conan Doyle\n3. The Silence of the Lambs by Thomas Harris 4. The Great Gatsby by F. Scott\nFitzgerald 5. The Catcher in the Rye by J.D. Salinger 6. The Da Vinci Code by Dan\nBrown 7. The Girl with the Dragon Tattoo by Stieg Larsson 8. The Adventures of\nSherlock Holmes by Sir Arthur Conan Doyle 9. The Secret of the Old Clock by\nCarolyn Keene 10. The Help by Kathryn Stockett.\nQuery: Classify the questions in the FAQ into Finding policy details, Change account settings, Filing a claim and viewing status, or Cancelling coverage.\nCould you deposit money into my account rather than mailing me a physical cheque?\nLayer\nLLaMA Response (SFT + ICT)\nSorted LLaMA Response (SoFT)\n12\nYes sir/ sorry sorryonymouslyovoyedance Festivalandeiamot...\nThe question is not classified as a request or request for information. It is a question\nthat is addressed to the customer\u2019s request.\n16\nbelongs belongsfrontieralandersonallyi...\nThe given question is a question of financial accounting and it belongs to the Can-\ncelling coverage category.\n20\nbelongs category \u00a8changeable via email or phone call or online portal or phone con-\ntactless payment etceterially...\nChange account settings\n24\nChange account settings\nChange account settings\n28\nThis question falls category of Change account settings.\nChange account settings\n32\nChange account settings\nChange account settings\n36\nChange account settings\nChange account settings\n40\nChange account settings\nChange account settings\nTable 6: Generated responses in different layers of SFT (Early Exit) and Sorted LLaMA for two samples from PandaLM\nvalidation set.\nQuery: Based on the given keywords, write a review of a restaurant. Name: Pizzeria\ngood prices, quiet place, tasty, nice staff\nLayer\nLLaMA Response (SFT + ICT)\nSorted LLaMA Response (SoFT)\n12\nHinweis B\u02d800e9jar Brothersqueeries...\nThe restaurant is located in a small town, with an impressive list of options for meals\nand drinks. The atmosphere is comfortable and comfortable, with a small, large-scale\nroom, with a large enough room for comfortable comfortable rooms. The menu is\na great way to choose the best option for meals and drinks. The menu is easy to\nunderstand and can be easily used to find the best option for meals and drinks. The\nmenu is also available online and can be easily accessed by anyone who is interested\nin making a purchase. The menu is also made up of a variety of cuisine, with a variety\nof cuisine. The menu is also made up of a variety of cuisine...\n16\nReview: description description Cassandra Smithsoniusque ...\nThe Pizzeria is a great place to eat and enjoy a relaxing meal. It is a quiet place, so\nit is perfect for a quiet meal and a relaxing meal. The food is incredibly delicious,\nand the staff is friendly and friendly. The prices are great, and the atmosphere is\ncomfortable and relaxing. It is a great place to eat and enjoy a relaxing meal.\n20\nReview: visited recently visited recently visited recently visited recently...\nI recently visited Pizzeria, a restaurant that is both affordable and a quiet place to\nenjoy a meal. The prices are definitely good, and the food is definitely tasteful. The\nstaff is nice and friendly, making the experience even more enjoyable. Overall, I\nhighly recommend Pizzeria for anyone looking for a good meal at a reasonable price.\n24\nReview: visited recently visited recently visited recently visited recently ...\nI recently visited Pizzeria, and I must say, it was a pleasant experience. The prices\nwere reasonable, and the atmosphere was quiet and relaxing. The staff was friendly\nand helpful, making me feel welcome and comfortable. The pizza was tasty and made\nme feel satisfied after a long day. Overall, I would recommend Pizzeria to anyone\nlooking for a good pizza, good prices, and a nice atmosphere\n28\nRevieweddit rating: stars star star star ...\nPizzeria is a hidden gem in the heart of the city. The prices are reasonable and the\natmosphere is a refreshing change from the crowded restaurants. The staff is friendly\nand welcoming, making the experience even more enjoyable. The pizza is tasty and\nthe crust is crisp and crunchy. The sauce is rich and the cheese is melted perfectly.\nOverall, Pizzeria is a must-visit for anyone looking for a good pizza and a quiet place\nto enjoy it.\n32\nReview: visited this place with friends recently after hearing good reviews about their\npizza pie. Upon arrival we noticed how quiet it was inside which made it perfect place\nto relax after hectic weekdays. prices were reasonable considering quality of food\nserved. staff members were friendly and helpful throughout entire duration of visit.\noverall experience exceed expectations thanks to delicious food served hot freshly\nbaked pizza pie tasteful ingunctions of Italian cu cu cu cu cu cu cu cu cu cu ...\nI recently visited Pizzeria and I must say, it was a pleasant experience. The prices\nwere reasonable, and the atmosphere was quiet and serene. The staff was friendly and\nhelpful, making me feel welcome. The pizza was delicious, with a crisp crust and a\ndelicious, creamy sauce. Overall, I would definitely recommend Pizzeria to anyone\nlooking for a good meal in a quiet, relaxing setting.\n36\nReview of Pizzeria: upon walking into pizzeria, you are greetvity welcomed by nice\nstaff members who are always ready to assist you with your needs. Their prices are\nreasonable considering the quality of food they offer. Their pizza is mouthwateringly\ndelicious and always freshly made to order. Additionally, pizzeria is a quiet place\nperfect for dates or family gatherings. Overall, pizzeria is a great place to visit for\ndelicious food and great service. highly recommended!\nI recently visited Pizzeria and had a wonderful experience. The prices were reasonable\nand the atmosphere was quiet and relaxing. The food was delicious and the staff was\nfriendly and attentive. Overall, I would highly recommend Pizzeria to anyone looking\nfor a tasty meal in a nice, quiet setting.\n40\nPizzeria is a great place to go for a quiet meal with tasty food and friendly staff. The\nprices are good, making it a great value for your money. The atmosphere is cozy and\nthe service is prompt. The pizza is cooked to perfection and the toppings are fresh and\ndelicious. Overall, it\u2019s a great place to enjoy a meal with friends or family. Highly\nrecommended!\nPizzeria is a great place to enjoy a delicious pizza in a quiet and cozy atmosphere.\nThe prices are reasonable and the staff is friendly and accommodating. The pizzas\nare cooked to perfection and the toppings are fresh and tasty. The restaurant is perfect\nfor a casual meal with friends or family, and the ambiance is relaxing and inviting.\nOverall, Pizzeria is a great option for a satisfying pizza experience.\nQuery: Define what the underlined word means for kids. _keep a promise\nLayer\nLLaMA Response (SFT + ICT)\nSorted LLaMA Response (SoFT)\n12\nThereuh Scientificallyally compromiseable ...\nThe word machery is a word that is used to describe a person or a person\u2019s behavior,\nor a person\u2019s behavior in a particular manner. It means that someone has been able to\nsuccessfully successfully deal with the situation or situation situationally.\n16\nmeans something importantlyrics song songwriter\nThe word Kick A promise is a common term in the context of a commitment to\ndeliver something or to take action. It means that someone has taken a promise to\nmake a promise to make sure that they are doing something or fulfilling their promise.\nIt is a kind of commitment that is often used in personal and professional settings.\n20\nWhen someone promise means promise means promise means promise means promise\nmeans ...\nKeeping a promise means keeping a promise made to someone else, and not making a\npromise to someone else or even yourself that you will not do something, but actually\ndoing it. It means being kind, honesty, and kindness towards others. It means being\nkind to your parents, siblings, friends, and even your peers. It means being kind to\nyourself too. It means being kind to yourself ...\n24\nWhen someone promise promise means promise promise means promise ...\nKeep a promise means to make sure that you do what you promised to do. It means\nthat you are reliable and reliable people are people who are trusted and respected.\nWhen you promise something to someone, it means that you are committed to it and\nyou will do it.\n28\nkeep promise means promise promise means promise promise means promise promise\n...\nKeeping a promise means to do what you promised to do, no matter what. It means\nkeeping your word and keeping your word is important. It\u2019s important to keep\npromises because it helps people trust you and trust that you will do what you say\nyou will do.\n32\nWhen you promise something means prom promprompr...\nKeeping a promise means that you promise something to someone and you do it. It\nmeans that you do not break your word and you do what you said you would do. It is\nimportant to keep promises because it shows that you are trustworthy and reliable.\n36\nKeeping a promise means promising something to someone else or yourself, and then\ndoing your best to make sure that promise comes true. It means being honest and\nkeeping your word even when it\u2019vity hard or takes effort. It means being responsible\nand reliable so people can trust you to do what you say you will do.\nKeeping a promise means doing what you said you would do. It\u2019s important to follow\nthrough on your promises to others and to be reliable.\n40\nKeep a promise means to do what you said you would do. It means to be honest and\nto keep your word. When you keep a promise, you show that you are trustworthy and\nthat people can count on you.\nKeeping a promise means doing what you said you would do, even if it\u2019s hard or\ntakes a long time. It\u2019s important to be honest and reliable, and to follow through on\nyour words.\nTable 7: Generated responses in different layers of SFT (Early Exit) and Sorted LLaMA for a sample from PandaLM validation\nset.\nQuery: Bandar Seri Begawan International airport is in which country?\nLayer\nLLaMA Response (SFT)\nSorted LLaMA Response (SoFT)\n12\niernohotter than than ...\nMalta\n16\nNederlige territ territorially ...\nBurma (Myanmar)\n20\nAustralia AustraliaAustral Australia Australia ...\nBrunei\n24\nMalays Malays Malays Malays ...\nBrunei\n28\nMalays Malays Malays Malays ...\nBrunei\n32\nBrunei\nBrunei\n36\nBrunei\nBrunei\n40\nBrunei\nBrunei\nTable 8: Generated responses in different layers of SFT and SoFT for a sample from TriviaQA benchmark.\nSorted/Fully Fine-tuned\n12 (4.1B)\n20 (6.6B)\n28 (9.2B)\n36 (11.7B)\n2 SFT Epochs/2 SoFT Epochs\n12 (4.1B)\n80.0/88.5/1.5\n37.5/132.0/0.5\n28.0/141.5/0.5\n20.0/148.5/1.5\n16 (5.4B)\n88.5/77.0/4.5\n42.0/121.5/6.5\n31.5/135.0/3.5\n20.0/142.5/7.5\n20 (6.6B)\n114.0/48.5/7.5\n56.0/84.5/29.5\n42.5/108.0/19.5\n32.0/117.5/20.5\n24 (7.9B)\n123.0/37.0/10.0\n70.5/61.5/38.0\n53.5/80.0/36.5\n45.5/89.5/35.0\n28 (9.2B)\n131.0/32.0/7.0\n75.0/63.0/32.0\n56.0/70.5/43.5\n46.5/82.5/41.0\n32 (10.4B)\n143.5/21.0/5.5\n98.0/43.5/28.5\n73.0/54.0/43.0\n54.0/65.5/50.5\n36 (11.7B)\n140.5/22.0/7.5\n98.5/40.5/31.0\n76.0/49.0/45.0\n53.0/62.5/54.5\n40 (13B)\n137.5/24.0/8.5\n102.0/37.0/31.0\n78.5/45.5/46.0\n55.0/62.0/53.0\n2 SFT Epochs/4 SoFT Epochs\n12 (4.1B)\n94.5/71.0/4.5\n44.0/121.0/5.0\n37.0/130.5/2.5\n26.5/138.5/5.0\n16 (5.4B)\n105.0/60.0/5.0\n55.0/102.0/13.0\n51.0/110.5/8.5\n34.0/123.0/13.0\n20 (6.6B)\n129.5/33.5/7.0\n73.0/67.5/29.5\n58.5/85.0/26.5\n47.0/96.5/26.5\n24 (7.9B)\n132.0/30.5/7.5\n89.5/51.0/29.5\n70.0/62.5/37.5\n51.0/80.0/39.0\n28 (9.2B)\n140.0/23.5/6.5\n89.5/51.0/29.5\n66.5/60.0/43.5\n48.5/77.5/44.0\n32 (10.4B)\n144.5/18.5/7.0\n103.5/35.0/31.5\n77.5/52.0/40.5\n55.5/62.0/52.5\n36 (11.7B)\n146.0/17.5/6.5\n105.5/34.5/30.0\n84.5/44.5/41.0\n60.0/52.5/57.5\n40 (13B)\n149.0/15.0/6.0\n105.0/37.5/27.5\n87.5/41.5/41.0\n62.5/53.5/54.0\nTable 9: Pair-wise comparison between Extracted fine-tuned and SoFT sub-models.\n"
  },
  {
    "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models",
    "link": "https://arxiv.org/pdf/2309.09958.pdf",
    "upvote": "17",
    "text": "arXiv:2309.09958v1  [cs.CV]  18 Sep 2023\nAn Empirical Study of Scaling Instruction-Tuned\nLarge Multimodal Models\nYadong Lu\u22171, Chunyuan Li\u22172, Haotian Liu3, Jianwei Yang2, Jianfeng Gao2, Yelong Shen1\n1Microsoft Azure AI\n2Microsoft Research\n3University of Wisconsin\u2013Madison\nAbstract\nVisual instruction tuning has recently shown encouraging progress with open-\nsource large multimodal models (LMM) such as LLaVA and MiniGPT-4. How-\never, most existing studies of open-source LMM are performed using models with\n13B parameters or smaller. In this paper we present an empirical study of scal-\ning LLaVA up to 33B and 65B/70B, and share our \ufb01ndings from our explorations\nin image resolution, data mixing and parameter-ef\ufb01cient training methods such\nas LoRA/QLoRA. These are evaluated by their impact on the multi-modal and\nlanguage capabilities when completing real-world tasks in the wild. We \ufb01nd that\nscaling LMM consistently enhances model performance and improves language\ncapabilities, and performance of LoRA/QLoRA tuning of LMM are comparable\nto the performance of full-model \ufb01ne-tuning. Additionally, the study highlights\nthe importance of higher image resolutions and mixing multimodal-language data\nto improve LMM performance, and visual instruction tuning can sometimes im-\nprove LMM\u2019s pure language capability. We hope this study makes state-of-the-art\nLMM research at a larger scale more accessible, thus helping establish stronger\nbaselines for future research. Code and checkpoints will be made public.\n1\nIntroduction\nRecent studies on large multimodal models (LMM) [9, 10] have been focused on the methods of\nvisual instruction tuning [12]. The results are promising: e.g., the open-source project Large Lan-\nguage and Vision Assistant (LLaVA) shows that training a 7B large language model (LLM) with\nmultimodal instruction-following data for 3 hours on 8 A-100 GPUs leads to a LMM with strong\nvisual understanding and reasoning capabilities in the wild: reproducing some of the most appealing\nexamples of the proprietary OpenAI multimodal GPT-4 model [14]. A similar idea is explored in\nits co-current work MiniGPT-4 [20]. It has rapidly become a prominent research topic, spurring the\ndevelopment of numerous new models, benchmarks, and applications [10]. However, the high com-\npute cost has led most existing studies to utilize 7B and 13B LLMs. Thus, the impact of signi\ufb01cantly\nscaling up the model size to e.g., 33B and 65B remains unexplored.\nThis study aims to \ufb01ll this gap by empirically investigating language models of larger sizes for LMM,\nsharing insights of our scaling experiments and establishing stronger baselines using larger-scale\nLLaVA for future research. Speci\ufb01cally, we explore the impact of larger model sizes, model tuning\nand data mixing methods on model performance, and present our \ufb01ndings and recommendations.\nThe scaling recipe leads to new state-of-the-art (SoTA) performance on LLaVA-Bench [12] and\nMM-VET [19]. We hope that our \ufb01ndings and larger LLaVA checkpoints would provide a reference\nfor future research on visual instruction tuning.\n*These authors contributed equally to this work\nPreprint. Work in progress\n2\nExperiment Setup\nModel Checkpoints.\nTo study the impact of scaling up LLM on multimmodal capabilities, we\nincrease the language model size to 33B and 65B [15], in addition to the 7B and 13B models used\nfor existing LMM.\n\u2022 LLaVA-33B\nWe employ the open source Vicuna-33B checkpoint 1 [16] to preform the two-\nstage training. The training data is around 125K conversations collected from ShareGPT.com.\n\u2022 LLaVA-65B Due to a lack of public 65B Vicuna checkpoint, we conduct our own training of\nthe Vicuna-65B model, utilizing ShareGPT data that we have independently processed. This\ndata contains 159M tokens used during training. As a comparison, the reported number of\ntokens used in training Vicuna 33B is 370M 2.\nOnce the instruction-tuned LLM is given, we follow [12] to perform the two-stage LLaVA lightning\ntraining: (i) Stage 1: Pre-training for Feature Alignment. The linear projection layer is trained,\nwhich maps the visual feature (the features before the last layer of the pre-trained image encoder)\nto word embedding space of LLM. More specifcally, the projection dimension is 1024\u21926656 for\nthe 33B model and 1024\u21928192 for the 65B model, respectively. In this stage, we use the concept-\nbalanced subset of LAION-CC-SBU data with 558K samples. (ii) Stage 2: Visual Instruction\nTuning. We use the LLaVA-80K multimodal instruct dataset for the \ufb01ne-tuning stage. Various\ntraining schedules are explored to enable the model to follow the diverse instructions to complete\ntasks in the wild, as to be detailed below.\nTuning Methods.\nWe explore both the trainable modules and training data mixing for ef\ufb01cient\nand effective visual instruct tuning of large models.\n\u2022 Trainable modules. In addition to tuning the linear projection layer, two schemes are consid-\nered to tune the LLM: (i) Full-model \ufb01ne-tuning of LLM and (ii) Parameter-ef\ufb01cient training\nmethods. For the latter, LoRA [7] and QLoRA [4] are employed to allow us to tune large mod-\nels with limited compute resource. This aims to gain an in-depth understanding of the trade-off\nbetween the training cost and model performance.\n\u2022 Data mixing.\nTypically only the multimodal instruction data is used in Stage-2. We further\nconsider mixing the language-only instruct data ShareGPT with the LLaVA-80K multimodal\ninstruction data to gain an in-depth understanding of the trade-off between models\u2019 language\nand multimodal capabilities.\nHyper-parameters.\nIn the training process of both stages, we utilize the DeepSpeed library 3 and\nemploy the ZeRO3 optimizer, except for QLoRA runs we use ZeRO2. We use a maximum sequence\nlength of 2048. For Stage 1, we train both the 33B and 65B models with a learning rate of 1\u00d710\u22124\nwith no weight decay, and a learning rate with linear decay and linear warmup for 3% of training\nsteps in total. For Stage 2, we use a learning rate of 2\u00d710\u22125 in full \ufb01ne-tuning to train 1 epoch\nfor all the models in full \ufb01netuning, and a learning rate of 1\u00d710\u22124 for the LoRA/QLoRA runs.\nWe conducted a set of hyperparameter search and for LoRA runs, and found larger LoRA alpha or\nequivalently larger learning rate was crucial to get the best performance. Speci\ufb01cally, we use LoRA\nalpha equals 2 times the LoRA rank, and a learning rate of 1\u00d710\u22124, which works the best for all the\nmodels. For full \ufb01ne-tuning, we use a total batch size of 512 on 4 A100 nodes, where each of these\nnodes is equipped with 8 A100-80G GPUs. For LoRA/QLoRA runs, we use a total batchsize of 64\non 1 A100 node for 33B model and 2 nodes for 65B model.\n3\nResults\nWe \ufb01rst compare our large checkpoints on two recent benchmarks which are speci\ufb01cally designed\nfor LMM, then report our \ufb01ndings in the course of scaling up LLaVA models.\n1https://huggingface.co/lmsys/vicuna-33b-v1.3\n2https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md\n3https://github.com/microsoft/DeepSpeed\n2\nModels\nReasoning\nConversation\nDetail\nOverall\nBard-0718\n78.7\n83.7\n69.7\n77.8\nBing-Chat-0629\n90.1\n59.6\n52.2\n71.5\nLLaVA-13B (beam=1)\n81.7\n64.3\n55.9\n70.1\nLLaVA-13B (beam=5)\n84.3\n68.4\n59.9\n73.5\nLLaVA-33B (beam=1)\n82.9\n70.2\n62.6\n73.9\nLLaVA-33B (beam=5)\n83.5\n72.6\n61.9\n74.8\nLLaVA-65B (beam=1)\n87.3\n63.8\n62.3\n74.2\nLLaVA-65B (beam=5)\n88.7\n59.4\n65.7\n74.4\nTable 1: The performance comparison on LLaVA-Bench. Beam search sizes at 1 and 5 are reported.\nModel\nRec\nOCR\nKnowledge\nGeneration\nSpatial\nMath\nTotal\nResults of various open-source LMM on reported in the MM-VET paper [19]\nLLaMA-Adapter v2-7B [5]\n16.8\n7.8\n2.5\n3.0\n16.6\n4.4\n13.6\u00b10.2\nOpenFlamingo-9B [1, 2]\n24.6\n14.4\n13.0\n12.3\n18.0\n15.0\n21.8\u00b10.1\nMiniGPT-4-8B [20]\n27.4\n15.0\n12.8\n13.9\n20.3\n7.7\n22.1\u00b10.1\nBLIP-2-12B [11]\n27.5\n11.1\n11.8\n7.0\n16.2\n5.8\n22.4\u00b10.2\nLLaVA-7B [12]\n28.0\n17.1\n16.3\n18.9\n21.2\n11.5\n23.8\u00b10.6\nMiniGPT-4-14B [20]\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4\u00b10.4\nOtter-9B [8]\n28.4\n16.4\n19.4\n20.7\n19.3\n15.0\n24.6\u00b10.2\nInstructBLIP-14B [3]\n30.8\n16.0\n9.8\n9.0\n21.1\n10.5\n25.6\u00b10.3\nInstructBLIP-8B [3]\n32.4\n14.6\n16.5\n18.2\n18.6\n7.7\n26.2\u00b10.2\nLLaVA-13B [12]\n30.9\n20.1\n23.5\n26.4\n24.3\n7.7\n26.4\u00b10.1\nMM-ReAct-GPT-3.5 [18]\n24.2\n31.5\n21.5\n20.7\n32.3\n26.2\n27.9\u00b10.1\nLLaVA-7B (LLaMA-2) [12]\n32.9\n20.1\n19.0\n20.1\n25.7\n5.2\n28.1\u00b10.4\nLLaVA-13B (V1.3, 336px) [12]\n38.1\n22.3\n25.2\n25.8\n31.3\n11.2\n32.5\u00b10.1\nLLaVA-13B (LLaMA-2) [12]\n39.2\n22.7\n26.5\n29.3\n29.6\n7.7\n32.9\u00b10.1\nMM-ReAct-GPT-4 [18]\n33.1\n65.7\n29.0\n35.0\n56.8\n69.2\n44.6\u00b10.2\nResults with our own experiment runs\nLLaVA-13B (LLaMA-2)\n38.4\n21.0\n26.3\n28.8\n28.0\n7.7\n32.6\u00b10.1\nLLaVA-33B\n38.5\n25.0\n26.2\n28.2\n29.2\n7.7\n32.9\u00b10.3\nLLaVA-33B (Data Mixing)\n37.7\n27.1\n26.2\n28.6\n28.1\n11.5\n34.1\u00b10.3\nLLaVA-65B\n39.2\n28.2\n26.2\n28.3\n33.0\n15.0\n35.5\u00b10.3\nLLaVA-65B (Data Mixing)\n41.8\n27.9\n30.4\n32.3\n30.5\n7.3\n36.4\u00b10.2\nTable 2: Performance of various open-source LMM on MM-VET. Note that MM-ReAct is not an\nsingle multimodal model, it is a system built on chaining visual tools via GPT-3.5 or GPT-4, which\nwe append as a reference. Our experiment run on LLaVA-13B (LLaMA-2) yields very similar score\nwith the same checkpoint reported in MM-VET paper, indicating that our evaluation pipelines are\nconsistent.\n3.1\nComparisons on Benchmarks\nLLaVA-Bench.\nLLaVA-Bench (In-the-Wild)4 [12] is a diverse evaluation dataset consisting of 24\nimages with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches.\nEach image is paired with a manually-curated, detailed description and a set of properly-selected\nquestions related to open-ended visual chat scenarios. Each questions belongs to one of three types\nof tasks: conversations that contain simple visual recognition & QA questions, detailed descriptions\nthat characterize the image with a long paragraph, and a complex reasoning task that focuses on de-\nducing implications from an image. Language GPT-4 (gpt4-0314) is used to score to the generated\nanswers. The relative scores between the model output and gold response are reported. We com-\npare LLaVA against the commercial visual chat systems including Microsoft BingChat5 and Google\nBard6 on LLaVA-Bench [12].\n4https://github.com/haotian-liu/LLaVA/blob/main/docs/LLaVA_Bench.md\n5https://www.bing.com/chat\n6https://bard.google.com/\n3\nThe results are presented in Table 1. The 33B and 65B checkpoints outperform the 13B LLaVA\nmodel and Bing Chat. Despite the fact that LLaVA-Bench is small (thus the comparison might not be\nstatistically signi\ufb01cant), the results are encouraging: compared to large LMM, small open-sourced\nLMM are far more cost-effective to be deployed in real-world applications. With negligible increase\nof inference latency, we can signi\ufb01cantly improve the performance for all model sizes by increasing\nthe beam search size from 1 to 5. Our results show that larger LLaVA models generally exhibit\nbetter performancein tasks involving complex reasoning and generating detailed descriptions, which\nrequires strong language competencies from larger LLM. In addition, larger LLaVA models obtain\ncomparable results to BingChat in multi-turn, multi-modal conversation tasks that require strong\nimage understanding capability.\nMM-VET.\nMM-VET [19] is designed based on the assumption that the intriguing capability of\nsolving complicated tasks is often achieved by a generalist LMM which is able to integrate a varity of\nvision-language (VL) capabilities. MM-Vet contains 200 images and 218 questions (samples), aim-\ning to evaluate6 core VL capabilities (recognition, OCR, knowledge, language generation, spatial\nawareness, and math) and their combinations. For evaluation, an LLM-based evaluator (gpt4-0613)\nis used to score open-ended outputs of different forms. In Table 2, we report the results on MM-\nVET. The performance is consistently improved from 13B to 33B and 65B. The largest LLaVA\nmodel improves SoTA performance among the end-to-end open-source LMM. The most signi\ufb01cant\nimprovements are observed when evaluating the capabilities of knowledge and generation, followed\nby recognition and OCR. The performance on spatial and math remains comparable. The result\nreveals that the improved LLM capability is instrumental in storing more knowledge in the weights\nand leading to a stronger language responding capability.\n3.2\nScaling up LLaVA\nThe experiments are conducted to answer three research questions.\n1\n\u20dd Which scaling factor matters?\nWe study the relative contribution of three scaling-up factors\nto the performance improvement of LLaVA. The results are summarized in Table 3 (a).\n\u2022 Model size.\nIncreasing the model size consistently improves the overall performance. We\nconjecture that larger data size is essential to train a larger model. For example, if we only train\non LLaVA-80K data, we see smaller gain when model size becomes larger.\n\u2022 Image resolution.\nBy \ufb01xing the CLIP ViT image encoder, we compare the variants that are\npre-trained to take image resolution 224\u00d7224 and 336\u00d7336, and \ufb01nd that the higher resolution\nconsistently yields 2-3 points improvement across all four LLM sizes.\n\u2022 Data mixing.\nLarger models tend to have higher capability of \ufb01tting the instruction data.\nBy mixing the language-only instruction data (ShareGPT) with LLaVA-80K, we can improve\nmodel performance by 2 points, compared to training on multimodal instruction data only.\nIn Table 3 (b), we present our result on MM-Bench [13], which contains a set of 2,974 questions,\nwhich evaluate models\u2019 reasoning skills of six categories. The combination of the three factors\nimprove the baseline LLaVA 7B model, reported in [13].\n2\n\u20dd When should the parameter-ef\ufb01cient training method be considered?\nAs model size in-\ncreases, it becomes necessary to consider using tuning methods that are more ef\ufb01cient than full-\nmodel \ufb01ne-tuning. LoRA and QLoRA are well-known parameter-ef\ufb01cient tuning methods. As\nshown in Table 4, we report compute cost using GPU hours per node, because the unit can be equiv-\nalent to the price $13.63/hour (ND A100 v4 series) on Azure 7. The total cost can be estimated by\nmultiplying the #hours and #epochs.\nIn Table 4(a), we train both the 33B and 65B model with LoRA rank 8 and 64 for 1 epoch on the\nLLaVA-80K instruction-tuning dataset. For models with 33B parameters and above, as we increase\nthe LoRA rank values, we notice an increase in both performance and cost until full-model tuning\nreaches its maximum performance for a speci\ufb01c model size. In the case of the 13B model, we\n\ufb01nd that a rank of 64 can deliver comparable performance to full-model tuning. The cost is more\nrelated to the total number of parameters than the number of trainable parameters. The cost increase\n7https://azure.microsoft.com/en-us/pricing/details/machine-learning/\n4\nImage Size\nData Mixing\n7B\n13B\n33B\n65B\n224\u00d7224\n\u2717\n63.6\n67.1\n69.3\n70.3\n336\u00d7336\n\u2717\n65.9\n70.1\n72.0\n72.3\n336\u00d7336\n\u2713\n\u2013\n\u2013\n73.9\n74.2\n(a) Performance scores on LLaVA-Bench.\nCheckpoint\nImage Size\nData Mixing\nOverall\nLR\nAR\nRR\nFP-S\nFP-C\nCP\nLLaVA-7B\n224\u00d7224\n\u2717\n36.2\n15.9\n53.6\n28.6\n41.8\n20.0\n40.4\nLLaVA-33B\n336\u00d7336\n\u2713\n55.7\n23.3\n74.0\n46.0\n51.5\n50.4\n67.2\nLLaVA-65B\n336\u00d7336\n\u2713\n56.0\n24.4\n72.3\n49.3\n50.5\n51.2\n68.1\n(b) Performance scores on MM-Bench. The skills to evaluate include logic reasoning (LR), attribute reason-\ning (AR), relation reasoning (RR), \ufb01ne-grained single-instance perception (FP-S), \ufb01ne-grained cross-instance\nperception (FP-C), and coarse perception (CP).\nTable 3: The performance to scale up model size, image resolution and data mixing.\n7B\n13B\n33B\n65B\nLoRA Rank\nFull\n64\nFull\n8\n64-QLoRA\n64\nFull\n64\nFull\nPerformance \u2191\n65.9\n70.1\n70.1\n70.3\n71.6\n71.8\n72.0\n72.2\n72.3\nTime (GPU Hours per node) \u2193\n1.3\n2.1\n2.3\n4.62\n4.68\n4.79\n5.80\n9.17\n13.50\n# Trainable Parameters (B) \u2193\n7\n0.26\n13\n0.06\n0.49\n0.49\n33\n0.81\n65\nTable 4: The trade-off between performance and compute cost among different model sizes and\ntraing methods on LLaVA-80K data. \u201cFull\u201d indicates the full-model \ufb01ne-tuning. \u201cTime\u201d is reported\nas the total GPU hours to \ufb01nish 1 epoch training (running time \u00d7 #GPUs) divided by 8 (#GPUs\nper node). All models are trained on LLaVA-80K data, results are obtained through averaging 3\nrepeated evaluation runs with same set up on LLaVA-Bench.\ndue to raising the LoRA rank for a given model size is signi\ufb01cantly smaller than the cost increase\nby enlarging model sizes. For example, increasing the LoRA rank from 8 to 64 nearly matches\nthe performance as LoRA \ufb01ne-tuning a 65B model with same rank, but only requires 50% of 65B\nmodel\u2019s training cost. In practice we \ufb01nd that tuning 33B model provide a good trade-off between\ncost and performance.\nDifferent LoRA variations have similar performance, and QLoRA requires lower GPU memory\ncost and running-time cost than LoRA. When large models (e.g., 65B) are trained with DeepSpeed\nZeRO2 mode, they can \ufb01t into GPU with QLoRA, while yield the OOM issue with LoRA. In the\nexperiments, we \ufb01nd that the hyperparameters of LoRA have a large impact of performance:(i)\nLarge learning rate and alpha value of LoRA improves the results signi\ufb01cantly. For example, With\nthe same rank=64, we reduce the learning rate=2 \u00d7 10\u22125 and alpha=16, the performance decrease\nfrom 71.8 to 65.5 on LLaVA-Bench. (ii) Under the same setting, large ranks leads to little improve-\nment. e.g., we increase the rank from 64 to 128 and 512, it improves from 65.5 to 66.1 and 68.1,\nrespectively.\n3\n\u20dd A LMM with strong capabilities in both language and multimodal?\nWe expand our eval-\nuation in two aspects: (i) MM-VET is added to measure the integrated multimodal capabilities of\nLMM; (ii) The pure language ability of LMM is measured using Vicuna-80 [16] and MMLU [6],\nwhere the former evaluates the instruct-following ability in real-world language tasks, the latter eval-\nuates the multilingual multi-task language ability. The results are shown in Table 5, where all models\nare full-model \ufb01ne-tuned.\nCompared to Vicuna which initializes the LLM weights of LLaVA, it is surprising to observe that\nLLaVA, after being trained solely on multimodal instruction data, exhibits a comparable language\ncapability. Mixing language instruction data can boost LLaVA\u2019s multimodal ability, but not the lan-\nguage ability. This is partially attributed to the inclusion of complex reasoning questions, and long-\nform answers in LLaVA-Instruct-158K, which helps maintain the language capabilities of LLaVA.\n5\nModel\nData Mix\nMultimodal\nLanguage\nLLaVA-Bench\nMM-VET\nVicuna-80\nMMLU\nVicuna-13B\n-\n-\n-\n79.9\n55.8\nLLaVA-13B\n\u2717\n70.1\n32.5\n79.6\n55.0\nVicuna-33B\n-\n-\n-\n85.6\n59.0\nLLaVA-33B\n\u2717\n72.0\n32.9\n85.3\n56.1\nLLaVA-33B\n\u2713\n73.9\n34.1\n80.3\n58.6\nVicuna-65B\n-\n-\n-\n83.2\n62.5\nLLaVA-65B\n\u2717\n72.3\n35.5\n84.5\n62.6\nLLaVA-65B\n\u2713\n74.2\n36.4\n82.6\n62.2\nLLaMA-2-70B-Chat\n-\n-\n-\n84.7\n63.1\nLLaVA-70B\n\u2713\n69.8\n35.4\n81.3\n65.1\nTable 5: Performance on both multimodal and language capabilities.\nWe also train LLaVA-70B based on the LLaMA-2-70B-Chat checkpoint [15], and \ufb01nd that mixed\nresults on multimodal and language abilities. Interestingly, we improve LLaMA-2-70B-Chat by 2.4\npoints on MMLU, yielding an overall MMLU score of 65.1, which is the best performance for the\n70B model size, according to [17] and the Chatbot Arena Leaderboard 8. To the best of our knowl-\nedge, this is the \ufb01rst reported result which shows visual instructing tuning improve language ability\nof large-scale LMM.\n4\nConclusions and Limitations\nWe present an empirical study of scaling the language model size for LMM. Our main \ufb01ndings are:\n(i) Scaling LMM consistently enhances model performance, resulting in signi\ufb01cant improvements\nin language capabilities, primarily due to the increased LLM model size. We leave it to future work\nhow to scale the vision encoder to enhance the visual capabilities and improve model performance on\nvision recognition and understanding tasks. (ii) Parameter-ef\ufb01cient methods such as LoRA/QLoRA\nare viable solutions to \ufb01netune large-scale LLMs for a good performance-cost trade-off in some\nreal-world settings with limited GPU memory. We observe that LoRA/QLoRA\u2019s performance are\ncomparable to that of \ufb01ne-tuning the full model, establishing their effectiveness through signi\ufb01cant\ncost reduction in both model training and serving. (iii) Our study of training data curation reveals\nthat properly selecting image resolutions and mixing multimodal-language data for model training\ncan signi\ufb01cantly improve the performance of the resultant LMM. We also show for the \ufb01rst time that\nvisual instruction tuning can improve LMM\u2019s language capability. Note that the training datasets\nused in this study is small. So, our \ufb01ndings are still preliminary. In future work, we will experiment\nusing much larger datasets to investigate in detail whether and how different methods of training\ndata selection and mixing can improve the quality of much larger LMM.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022. 3\n[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpen\ufb02amingo: An open-\nsource framework for training large autoregressive vision-language models. arXiv preprint\narXiv:2308.01390, 2023. 3\n8https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n6\n[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purposevision-\nlanguage models with instruction tuning, 2023. 3\n[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Ef\ufb01cient \ufb01ne-\ntuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. 2\n[5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan\nLu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-ef\ufb01cient visual instruction\nmodel. arXiv preprint arXiv:2304.15010, 2023. 3\n[6] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020. 5\n[7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2\n[8] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\n3\n[9] Chunyuan Li.\nLarge multimodal models: Notes on CVPR 2023 tutorial.\narXiv preprint\narXiv:2306.14895, 2023. 1\n[10] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv\npreprint, 2023. 1\n[11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 3\n[12] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n1, 2, 3\n[13] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,\nJiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around\nplayer? arXiv preprint arXiv:2307.06281, 2023. 4\n[14] OpenAI. Gpt-4 technical report, 2023. 1\n[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and \ufb01ne-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 6\n[16] Vicuna.\nVicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\nhttps://vicuna.lmsys.org/, 2023. 2, 5\n[17] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\nChandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al.\nHow far\ncan camels go? exploring the state of instruction tuning on open resources. arXiv preprint\narXiv:2306.04751, 2023. 6\n[18] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action, 2023. 3\n[19] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao\nWang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabil-\nities. arXiv preprint arXiv:2308.02490, 2023. 1, 3, 4\n[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023. 1, 3\n7\nThis figure \"lora_loss.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2309.09958v1\n"
  },
  {
    "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models",
    "link": "https://arxiv.org/pdf/2309.09506.pdf",
    "upvote": "14",
    "text": "*Preprint\nLAYOUTNUWA: REVEALING THE HIDDEN LAYOUT\nEXPERTISE OF LARGE LANGUAGE MODELS\nZecheng Tang1,2\u2217\nChenfei Wu2\u2217\nJuntao Li1\nNan Duan2\u2020\n1Soochow University\n2Microsoft Research Asia\n{zctang@stu., ljt}@suda.edu.cn, {chewu,nanduan}@microsoft.com\n<svg width=\"100\" height=\"150\">\n<rect data\u00adcategory=\"title\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data\u00adcategory=\"table\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data\u00adcategory=\"text\",\u00a0x=<M>,y=<M>,width=<M>,height=<M>/>\n</svg>\nCODE\nText\nTable\nTitle\nOutput\u00a0Layout\nTitle\nTable\nText\nwidth=100\nheight=150\nInputs\nCode\u00a0\nInitialization\nCode\nRendering\nInstruction:\u00a0I\u00a0want\u00a0to\u00a0generate\u00a0layout\u00a0in\u00a0document style.\nPlease\u00a0generate\u00a0according\u00a0to\u00a0the\u00a0categories I\u00a0provide:\nLarge Language Model\n(CR)\n(CI)\nCode\nCompletion\n(CC)\n<svg width=\"100\" height=\"150\">\n<rect data\u00adcategory=\"title\",x=\"15\",y=\"5\",width=\"70\",height=\"20\"/>\n<rect data\u00adcategory=\"table\",x=\"10\",y=\"30\",width=\"80\",height=\"30\"/>\n<rect data\u00adcategory=\"text\", x=\"10\",y=\"65\",width=\"80\",height=\"80\"/>\n</svg>\nCODE\nFigure 1: Overview of LayoutNUWA, in which we view layout generation as a code generation\ntask to enhance the semantic information in layouts as well as naturally harness the hidden layout\nexpertise of large language models. In detail, we propose a Code Instruct Tuning (CIT) approach that\nconsists of three modules: 1) the Code Initialization (CI) module quantifies the numerical conditions\nand initializes them as an HTML code with masks; 2) the Code Completion (CC) module utilizes\nthe knowledge of large language models to complete the masked portions within the HTML code; 3)\nthe Code Rendering (CR) module directly renders the completed code into the final graphic layout.\nABSTRACT\nGraphic layout generation, a growing research field, plays a significant role in\nuser engagement and information perception. Existing methods primarily treat\nlayout generation as a numerical optimization task, focusing on quantitative as-\npects while overlooking the semantic information of layout, such as the relation-\nship between each layout element. In this paper, we propose LayoutNUWA, the\nfirst model that treats layout generation as a code generation task to enhance se-\nmantic information and harnesses the hidden layout expertise of large language\nmodels (LLMs). More concretely, we develop a Code Instruct Tuning (CIT) ap-\nproach comprising three interconnected modules: 1) the Code Initialization (CI)\nmodule quantifies the numerical conditions and initializes them as HTML code\nwith strategically placed masks; 2) the Code Completion (CC) module employs\nthe formatting knowledge of LLMs to fill in the masked portions within the HTML\ncode; 3) the Code Rendering (CR) module transforms the completed code into the\nfinal layout output, ensuring a highly interpretable and transparent layout gen-\neration procedure that directly maps code to a visualized layout. We attain sig-\nnificant state-of-the-art performance (even over 50% improvements) on multiple\ndatasets, showcasing the strong capabilities of LayoutNUWA. Our code is avail-\nable at https://github.com/ProjectNUWA/LayoutNUWA.\n\u2217Both authors contributed equally to this research. During Zecheng\u2019s internship under the mentorship of Chenfei at MSRA.\n\u2020Corresponding author.\n1\narXiv:2309.09506v2  [cs.CV]  19 Sep 2023\n*Preprint\n1\nINTRODUCTION\nGraphic layout, which refers to the organization and positioning of design elements, significantly\ninfluences the way users engage with and perceive the presented information (Lee et al., 2020). As\na growing research field, layout generation (Li et al., 2019; Yang et al., 2020) aims to create diverse\nand realistic layouts that streamline the design process and cater to various applications, such as user\ninterfaces (Deka et al., 2017; Jiang et al., 2022), indoor scenes (Di & Yu, 2021; Feng et al., 2023),\ndocument layouts (Zheng et al., 2019; Yamaguchi, 2021), presentation slides (Fu et al., 2022), etc.\nCurrent approaches (Jyothi et al., 2019; Li et al., 2019; Arroyo et al., 2021; Zhang et al., 2023a)\nregard each element in the layout as numerical tuples (c, x, y, w, h), in which c indicates the element\ncategory, x and y represent coordinates, w and h correspond to width and height. For example,\nautoregressive-based methods (Yang et al., 2020; Jiang et al., 2022) view the tuple as a sequence\nand predict their values sequentially, while diffusion-based methods (Chai et al., 2023; Inoue et al.,\n2023) consider the tuple as a whole and predict their values through a denoising approach. Despite\nadopting different generative models, all of these methods fundamentally consider layout generation\nas a numerical tuple optimization task. However, representing layouts as numerical tuples have\nits limitations, as it primarily focuses on capturing the quantitative aspects of the layout, such as\npositions and sizes, while lacking semantic information, e.g., the attribute of each numerical value,\nwhich may limit the model\u2019s ability to capture more complex and rich layout information.\nAn insightful question emerges from the limitations of existing methods in layout generation: can\nwe integrate semantic information into the layout generation process to enrich the overall represen-\ntation and enhance the quality of the generated layouts? Addressing this question brings forth two\nmajor benefits: firstly, it bolsters the understanding of relationships among various layout elements,\nand secondly, it enables us to tap into the semantic capabilities of LLMs (Tang et al., 2023), result-\ning in more intricate and contextually relevant layouts for a wide range of applications (Jiang et al.,\n2022). Considering the inherent logical nature of layouts, which involve dependency relationships\namong layout elements, and the fact that each graphic layout can be represented with a fixed struc-\nture sequence, code languages emerge as a promising alternative. Code languages can encompass\nnumerical and semantic information while possessing a strong logical foundation (Chen et al., 2022),\nwhich can thus bridge the gap between existing methods and the desired enriched representation.\nBased on the above observations, we propose LayoutNUWA, a groundbreaking model that revolu-\ntionizes the layout generation task by treating it as a code generation task. Our innovative approach\nis designed to not only enhance the semantic information within layouts but also seamlessly leverage\nthe expertise of LLMs in the layout generation process. To achieve this, we design a Code Instruct\nTuning (CIT) approach comprising three interconnected modules: 1) firstly, the Code Initialization\n(CI) module quantifies the numerical conditions and initializes them as HTML code with strate-\ngically placed masks, paving the way for more meaningful and coherent layouts; 2) secondly, the\nCode Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked\nportions within the HTML code, thereby harnessing the power of LLMs to improve the accuracy\nand consistency of the generated layouts; 3) lastly, the Code Rendering (CR) module transforms the\ncompleted code into the final layout output, ensuring a highly interpretable and transparent layout\ngeneration procedure that directly maps code to a visualized layout.\nExperiments across a variety of conditional layout generation tasks on three datasets, i.e., Rico (Deka\net al., 2017), PubLayNet (Zhong et al., 2019) and Magazine (Zheng et al., 2019), highlight the\nsuperiority of our method, in which LayoutNUWA can significantly outperform all the baselines and\nshows comparable results with the task-specific models. Furthermore, LayoutNUWA can achieve at\nleast a 50% improvement in performance compared to the best baseline on the low-resource datasets,\ne.g., the Magazine dataset. In a nutshell, our contributions can be outlined as follows:\n\u2022 We introduce LayoutNUWA, the first model that treats the layout generation task as a code\ngeneration task, effectively harnessing the hidden layout expertise of LLMs.\n\u2022 We propose Code Instruct Tuning, which empowers the model to adhere to instructions and\nenriches the semantic information of layout, resulting in precise and standardized code.\n\u2022 We attain significant state-of-the-art performance on multiple datasets, showcasing the ro-\nbust capabilities of LayoutNUWA.\n2\n*Preprint\n2\nRELATED WORK\n2.1\nLAYOUT GENERATION\nAutomatic layout generation, an important task for automatic graphical design for various scenarios\nsuch as document layouts (Zheng et al., 2019; Zhong et al., 2019; Yamaguchi, 2021; Fu et al., 2022),\nposters (Yang et al., 2016; Guo et al., 2021; Li et al., 2023) and user interface (Deka et al., 2017), has\nbeen recently extensively researched. Early approaches for layout generation involve embedding de-\nsign rules into manually-defined energy functions (O\u2019Donovan et al., 2014; O\u2019Donovan et al., 2015),\nwhile other methods have explored generative models such as GANs and VAEs for generating nu-\nmerical graphic and scene layouts, including LayoutGAN (Li et al., 2019), LayoutVAE (Jyothi et al.,\n2019), LayoutGAN++ (Kikuchi et al., 2021), NDN (Lee et al., 2020) and READ (Patil et al., 2020).\nApart from them, transformer-based approaches utilize self-attention mechanisms to learn numeri-\ncal contextual relationships between elements and achieve layout completion based on partial layout\ninputs (Yang et al., 2020; Kong et al., 2022; Feng et al., 2023). Recently, with the prevalence of\ndiffusion models, several works also adopted diffusion models to tackle a broader range of condi-\ntional layout generation (Chai et al., 2023; Inoue et al., 2023; Zhang et al., 2023a; Hui et al., 2023;\nCheng et al., 2023). However, existing methods primarily treat layout generation as a numerical\noptimization task, focusing on quantitative aspects while overlooking the semantic information of\nlayout, such as the relationship between each layout element. Different from previous works, we\nconvert the layout generation task into the code generation task to directly generate the layout in\ncode language and thus utilize the rich knowledge from LLMs, which can significantly improve the\nFID by 50% in the Magazine dataset in \u00a7 4.2.\n2.2\nINSTRUCTION TUNING\nInstruction tuning represents the process of fine-tuning LLMs on the instruction dataset in a super-\nvised fashion, which narrows the gap between the next-word prediction manner of LLMs and the\nusers\u2019 objective of having LLMs adhere to human instructions (Zhang et al., 2023c). Early attempts\non instruction tuning involve multi-task training with manually-written descriptions about differ-\nent tasks (Mishra et al., 2021; Wei et al., 2021; Sanh et al., 2021; Xu et al., 2022; Muennighoff\net al., 2022; Iyer et al., 2022) or automatically generated instructions (Wang et al., 2022; Gu et al.,\n2022; Zhang et al., 2023b; Honovich et al., 2022a;b). Apart from controlling the LLMs through\ninput instruction, Nye et al. (2021) show that LLM can handle more complex tasks by generating\nthe intermediate steps and Wei et al. (2022) propose chain-of-thought technique by enriching the\ninstruction with intermediate reasoning step descriptions, which endows LLMs with better perfor-\nmance (Wang et al., 2022; Zelikman et al., 2022; Wu et al., 2023; Xu et al., 2023). However, the\ninstruction tuning methods mentioned above are primarily intended for text generation tasks and not\nideal for layout generation tasks, which involve numerical optimization. Thus, we propose a code\ninstruction tuning method that is specially designed for layout generation task. Experiments in \u00a7 5.1\nindicate that the performance significantly drops if the code instruction tuning is not adopted.\n3\nMETHODOLOGY\n3.1\nPROBLEM FORMULATION\nThe layout generation task aims to generate a well-organized layout S = {si}N\ni=1, with N repre-\nsenting the number of elements in the layout. Each element, si = (ci, xi, yi, wi, hi), consists of the\nfollowing components: ci is the category, xi, yi indicate the center location, and wi, hi represent\nthe width and height, respectively. In this study, we focus on the conditional layout generation task,\nwherein partial components in si are masked with M, and the complete layout S should be predicted\nby model f\u03b8 conditioned on the remaining components S\\M:\nS = f\u03b8(S\\M)\n(1)\nPrevious works (Jyothi et al., 2019; Yang et al., 2020; Inoue et al., 2023) regard each element si\nas a sequence of numerical values, e.g., (0, 10, 20, 25, 30), and train a model to directly generate\nthese values. However, this approach overlooks the semantic information of the components, thus\nlimiting the model\u2019s understanding of the layout semantics. Based on this observation, we propose\n3\n*Preprint\n<html>\n<body>\n<svg width=\"100\" height=\"150\">\n<rect data-category=\"title\",x=15,y=10,width=70,height=20/>\n<rect data-category=\"table\",x=10,y=40,width=80,height=40/>\n<rect data-category=\"text\", x=10,y=90,width=90,height=80/>\n</svg>\n</body>\n</html>\nGolden Code\nLarge Language Model\nCode Instruct Tuning (CIT)\n/* Layout Style Description; Format & Task Definition;\n{HTML Structure}\nSuffix: ###bbox html: */\nTask Instruction\n<rect data-category={\ud835\udc50},x={\ud835\udc65},y={\ud835\udc66},width={\ud835\udc64},height={\u210e}/>\nElement Code\n<html>\n<body>\n<svg width={\ud835\udc4a} height={\ud835\udc3b}>\n. . . \n</svg>\n</body>\n</html>\nHTML Structure Code\nContent Tag\nClosing Tag\nOpening Tag\nSpecify Background \n& Layout Style\nElement \u00d7\ud835\udc8f\nTask Instruction + Code (Category \u00e0 Size + Position)\nI want to generate layout in document style.\nPlease generate according to the categories I provide:\n```<html>\n<body>\n<svg width=\"100\" height=\"150\">\n<rect data-category=\"title\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data-category=\"table\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data-category=\"text\", x=<M>,y=<M>,width=<M>,height=<M>/>\n</svg>\n</body>\n</html>```\nTask Instruction + Code (Category + Size \u00e0 Position)\nI want to generate layout in document style.\nPlease generate according to the categories I provide:\n```<html>\n<body>\n<svg width=\"100\" height=\"150\">\n<rect data-category=\"title\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data-category=\"table\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data-category=\"text\", x=<M>,y=<M>,width=<M>,height=<M>/>\n</svg>\n</body>\n</html>```\nTask Instruction + Code (Completion)\nI want to generate layout in document style.\nPlease generate according to the remaining values I provide:\n```<html>\n<body>\n<svg width=\"100\" height=\"150\">\n<rect data-category=\"title\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data-category=\"table\",x=<M>,y=<M>,width=<M>,height=<M>/>\n<rect data-category=\"text\", x=<M>,y=<M>,width=<M>,height=<M>/>\n</svg>\n</body>\n</html>```\nCode Initialization (CI)\nCategory + Size \u00e0 Position\n\u00d7\ud835\udfcf\n\u00d7\ud835\udfcf\n\u00d7\ud835\udfcf\nText\nTitle\nTable\n\ud835\udfce, \ud835\udc8e, \ud835\udc8e, \ud835\udfd5\ud835\udfce, \ud835\udfd0\ud835\udfce\n\ud835\udfd0, \ud835\udc8e, \ud835\udc8e, \ud835\udfd6\ud835\udfce, \ud835\udfd2\ud835\udfce\n\ud835\udfcf, \ud835\udc8e, \ud835\udc8e, \ud835\udfd6\ud835\udfce, \ud835\udfd6\ud835\udfce\nNumerical Input\nCondition\n(\ud835\udc84 , \ud835\udc99 , \ud835\udc9a , \ud835\udc98 , \ud835\udc89)\nTitle\nTable\nText\n\u00d7\ud835\udfcf\n\u00d7\ud835\udfcf\n\u00d7\ud835\udfcf\nCategory \u00e0 Size + Position\n\ud835\udfce, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e\n\ud835\udfd0, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e\n\ud835\udfcf, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e\nNumerical Input\nCondition\n(\ud835\udc84 , \ud835\udc99 , \ud835\udc9a , \ud835\udc98 , \ud835\udc89)\nCompletion\n\ud835\udfce, \ud835\udc8e, \ud835\udfcf\ud835\udfce, \ud835\udc8e, \ud835\udc8e \n\ud835\udfd0, \ud835\udfcf\ud835\udfce, \ud835\udc8e, \ud835\udfd6\ud835\udfce, \ud835\udc8e\n\ud835\udfcf, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e, \ud835\udc8e  \nNumerical Input\nCondition\nText\n\u00d7\ud835\udfcf\nTitle\n\u00d7\ud835\udfcf\nTable\n\u00d7\ud835\udfcf\n\ud835\udc9a=10\n\ud835\udc99=10, \ud835\udc98=80\n(\ud835\udc84 , \ud835\udc99 , \ud835\udc9a , \ud835\udc98 , \ud835\udc89)\nCategory 0:Title 1:Text 2:Table\nText\nTitle\nTable\nGolden Layout\n\ud835\udfce, \ud835\udfcf\ud835\udfd3, \ud835\udfcf\ud835\udfce, \ud835\udfd5\ud835\udfce, \ud835\udfd0\ud835\udfce\n\ud835\udfd0, \ud835\udfcf\ud835\udfce, \ud835\udfd2\ud835\udfce, \ud835\udfd6\ud835\udfce, \ud835\udfd2\ud835\udfce\n\ud835\udfcf, \ud835\udfcf\ud835\udfce, \ud835\udfd7\ud835\udfce, \ud835\udfd6\ud835\udfce, \ud835\udfd6\ud835\udfce\nBackground\nW=100  H=150\nJoint Loss\nCode Completion (CC)\nFigure 2: The training process of LayoutNUWA, which converts layout generation task to code\ngeneration task and utilizes a code instruct tuning to leverage LLM\u2019s capability for layout generation.\na new problem definition, where we convert the input S\\M and output S into a code language and\nview the layout generation task as a code generation task:\nCODE(S) = f\u03b8(CODE(S\\M))\n(2)\nEq. 2 has the following 3 advantages compared with Eq. 1:\n\u2022 Semantic Insights: By converting the numerical values into code language, the model can\nbetter capture the semantic relationships between different components of the layout.\n\u2022 LLM Utilization: By using code language, the model can further leverage the knowledge\nof Large Language Models (LLMs) and thus enhance the quality of the generated layouts.\n\u2022 Model Scalability: The code language has a stronger expressive capability compared to\nnumerical values, which allows the addition of more attributes for layout elements.\n3.2\nCODE INSTRUCT TUNING\nAs shown in Fig. 1, we propose Code Instruct Tuning (CIT) with three modules: (1) Code Ini-\ntialization module converts layout into masked code language with dynamic templates; (2) Code\nCompletion module inputs the masked code to LLMs to generate complete code; (3) Code Render-\ning module directly renders code to the final graphic layout. We illustrate these modules below.\n3.2.1\nCODE INITIALIZATION\nElement Quantization\nWe quantify the numerical values of i-th element position {xi, yi} and\nsize {wi, hi} in the layout with Adaptive Quantization method (Inoue et al., 2023) that applies\nk-Means algorithm (MacQueen et al., 1967) to cluster the position and size information of each\nelement, addressing the highly imbalanced distribution of these values, e.g., elements may overlap\nor cluster together. Different from the previous works (Chai et al., 2023; Zhang et al., 2023a; Inoue\net al., 2023), we use absolute position to represent the coordinates rather than relative positions. This\naligns with code language and allows direct rendering of layouts without necessitating coordinate\nconversion, thereby preventing potential information loss. We maintain precision up to one decimal\nplace and directly convert the clustered results into strings.\n4\n*Preprint\nTemplate Construction\nThe overview of template construction is shown in Fig. 2. We construct\nthe templates based on the most common web page layout code, HTML, which contains a wealth of\ninformation and is easily accessed by LLMs during the pre-training process (Touvron et al., 2023;\nRozi`ere et al., 2023). Specifically, in HTML code, each element is described with a tag that provides\ninformation about the content or the element structure. Since the elements in the layout are regular\nsquares, we chose the <rect> tag as the content tag to describe each element:\n<rect data-category={ci} x={xi} y={yi} width={wi} height={hi}>\nwhere ci is the element category in textual format and {xi, yi, wi, hi} are the quantified position\nand size of the i-th element. Then, to combine all the elements into a unified structure, we used an\nopening tag and a closing tag to define the boundaries of each layout, which can be written as:\n<html><body><svg width={W} height={H}> ... </svg></body></html>\nwhere W and H are the background width and height of the layout.\nIn order to facilitate better learning of layout in various domains and tasks and leverage the\ninstruction-following capabilities of LLMs, we design the following prompts:\nI want to generate layout in {Domain} style. Please generate the\nlayout according to the {Task Condition} I provide:\nwhere the {domain} and the {Task Condition} will vary according to different domains and\ntasks. For instance, for the RICO dataset, we set Domain as \u201cmobile UI\u201d, and for the layout\ncompletion task, we set Task Condition as \u201cremaining values\u201d. Afterwards, we prepend the\ntask instruction before the layout code.\n3.2.2\nCODE COMPLETION\nTo construct the conditional input of the layout generation task, we utilize the mask tokens of LLMs\nto represent the masked values M and let the model predict the masked values within the HTML\ncode. Different from previous works (Chai et al., 2023; Zhang et al., 2023a; Inoue et al., 2023)\nthat applied the customized numerical vocabulary, we employ the LLM\u2019s token vocabulary directly.\nBy doing so, we can leverage the knowledge of the numerical tokens inherit in the LLMs. Consid-\nering that almost all the LLMs follow auto-regressive generation manner and it brings significant\nlimitation to the layout generation task since the model should predict the same layout under dif-\nferent element orders, even if the layout doesn\u2019t have a naturally defined order (Yang et al., 2020).\nThus, we design a self-consistency strategy that randomly permutes the order of the input elements\nin the layout within a mini-batch. Meanwhile, in order to adapt LLMs to different conditional lay-\nout generation tasks, we have performed multi-task modeling on the same layout, utilizing various\nconditions and implementing a joint loss for these tasks. Given the permutation times K and task\nnumbers T, the joint loss for each layout S can be written as:\nL(S | \u03b8) =\nT\nX\nt=1\nN\nX\nj=1\nK\nX\nk=1\nL(s(k)\nj \\M (t)\nj\n| \u03b8),\n(3)\nwhere \u03b8 is the model parameters and sj denote the j-th element in the layout S.\n3.2.3\nCODE RENDERING\nMost existing works require the extra conversion step to render the graphic layouts (Yang et al.,\n2020; Chai et al., 2023; Zhang et al., 2023a), e.g., converting the relative position to the absolute\nposition, causing the information loss. Different from previous work, LayoutNUWA allows for the\nimmediate rendering as it generate the absolute position directly. Besides, considering the potential\noutput issues such as boundary overflow (Inoue et al., 2023) and format errors, we employ regular\nexpressions to remove mismatched formats and implement clipping operations for elements that\nexceed the background size.\n5\n*Preprint\n4\nEXPERIMENT\n4.1\nEXPERIMENTAL SETTINGS\nDataset\nWe evaluate the model performance on three widely used public datasets. RICO (Deka\net al., 2017) is a user interface design dataset for mobile applications containing 25 element cat-\negories and 66K+ UI layouts.\nPubLayNet (Zhong et al., 2019) consists of 360K+ layouts for\ndocuments with 5 element categories. Magazine (Zheng et al., 2019) is a low-resource magazine\nlayout dataset containing around 4K annotated layouts and 6 element categories. We follow Lay-\noutDM (Inoue et al., 2023) to view the original validation data as the testing set and pre-process all\nthree datasets by discarding the layouts containing more than 25 elements as well as splitting the\nfiltered data into the training and new validation sets by 95% and 5%.\nEvaluation Metrics\nWe employ four metrics to evaluate the generation results comprehensively,\nincluding Frechet Inception Distance (FID), Maximum Interaction over Union (mIoU), Alignment\n(Align.), and Overlap. Among them, FID compares the distribution of generated and real layouts.\nSimilar to the previous work (Inoue et al., 2023), we utilize an enhanced feature extraction model\nfor layouts (Kikuchi et al., 2021) to compute the FID score. We measure the conditional similarity\nbetween generated and real layouts using mIoU, which is done by calculating the maximum IoU\nbetween bounding boxes of generated and real layouts with the same type set. Alignment and Over-\nlap scores are calculated following the previous work (Li et al., 2019) to evaluate proper element\nalignment and overlapping in a generated layout, and it is worth noting that we ignore normal over-\nlaps, e.g., elements on top of the background, and discard the layouts that failed to generate. For\nreference, we show the evaluation results between the validation set and test set as Real data.\nTasks and Baselines\nWe evaluate LayoutNUWA on three conditional layout generation tasks.\nThese include the Category to Size and Position (C \u2192 S+P) task, the Category and Size to Position\n(C+S \u2192 P) task, and the Completion task. More concretely, the C \u2192 S+P task requires the model to\npredict the position and size of the element based on its category. For the C+S \u2192 P task, the model\npredicts the position of the element based on both its size and category. Finally, in the completion\ntask, the element\u2019s size and position values are randomly masked up to 80%, and the model predicts\nthe entire layout using the remaining values. We compare LayoutNUWA with six strong baselines,\nincluding LayoutTrans (Yang et al., 2020), BLT (Kong et al., 2022), LayoutGAN++ (Li et al., 2019),\nMaskGIT (Chang et al., 2022), DiffusionLM (Li et al., 2022) and LayoutDM (Inoue et al., 2023).\nImplementation Details\nWe implement LayoutNUWA with two 7B LLMs: LLaMA21 (L2) (Tou-\nvron et al., 2023) and CodeLLaMA2 (CL) (Rozi`ere et al., 2023). We train LayoutNUWA with two\nsettings: (1) Domain-Specific (DS) setting, where the model is trained on distinct datasets, and (2)\nDomain-Agnostic (DA) setting, where the model is trained on all three datasets, including RICO,\nPubLayNet, and Magazine. The default configuration for LayoutNUWA utilizes CodeLLaMA (CL)\nand Domain-Agnostic (DA), i.e., LayoutNUWA-L2-DS. We set permutation times K = 10 and task\nnumbers T = 3. For model training, we use DeepSpeed Library (Rajbhandari et al., 2020) to run\nall experiments on 64 NVIDIA V100 GPUs. We apply Top-p sampling (Holtzman et al., 2019) for\ninference, where p = 0.9 and the temperature is 0.6, and set the maximum generation length as 512.\n4.2\nQUANTITATIVE EVALUATION\nWe report the model performance on three datasets: the Magazine dataset in Tab. 1, RICO, and\nPubLayNet datasets in Tab. 2. For the Magazine dataset, LayoutNUWA demonstrates a remark-\nable performance by significantly surpassing all baseline measures across all tasks. Moreover, it\noutperforms the strong baseline LayoutDM by more than 50% when assessed with the FID metric.\nThe significant improvements in Tab. 1 are due to three aspects: 1) previous approaches generated\nnumerical values, while LayoutNUWA generates code with labels, which greatly benefits the model\nby utilizing the semantic information of layout attributes such as width, height, position, and cate-\ngory; 2) none of the previous methods used LLMs. However, we have introduced LLMs for the first\n1https://huggingface.co/meta-llama/Llama-2-7b\n2https://huggingface.co/codellama/CodeLlama-7b-hf\n6\n*Preprint\nModel\nLayout\nFormat\nLLM\nDomain\nC \u2192 S + P\nC + S \u2192 P\nCompletion\nmIOU (\u2191)\nFID (\u2193)\nmIOU (\u2191)\nFID (\u2193)\nmIOU (\u2191)\nFID (\u2193)\nLayoutTrans\nNumerical\n-\nSpecific\n0.116\n36.207\n0.153\n33.931\n0.228\n25.804\nBLT\nNumerical\n-\nSpecific\n0.087\n65.372\n0.126\n41.089\n0.103\n97.142\nLayoutGAN++\nNumerical\n-\nSpecific\n0.259\n16.952\n0.293\n11.569\n-\n-\nMaskGIT\nNumerical\n-\nSpecific\n0.059\n140.94\n0.100\n78.226\n0.024\n152.591\nDiffusionLM\nNumerical\n-\nSpecific\n0.151\n32.114\n0.144\n24.370\n0.138\n33.172\nLayoutDM\nNumerical\n-\nSpecific\n0.234\n19.206\n0.308\n14.265\n0.328\n15.804\nLayoutNUWA-L2-DS (ours)\nCode\nLLaMA2\nSpecific\n0.260\n9.741\n0.358\n6.682\n0.418\n8.257\nLayoutNUWA-L2-DA (ours)\nCode\nLLaMA2\nAgnostic\n0.293\n9.632\n0.394\n7.238\n0.413\n8.734\nLayoutNUWA-CL-DS (ours)\nCode\nCodeLLaMA\nSpecific\n0.293\n8.985\n0.348\n5.355\n0.410\n7.341\nLayoutNUWA (ours)\nCode\nCodeLLaMA\nAgnostic\n0.312\n8.791\n0.418\n6.755\n0.495\n7.572\nReal Data\n-\n-\n-\n0.348\n6.695\n0.348\n6.695\n0.348\n6.695\nTable 1: Quantitative comparison on Magazine dataset, where the bold font denotes the best result\nand underline represents the second-best performance.\nTasks\nModels\nRICO\nPubLayNet\nmIoU (\u2191)\nAlign. (\u2192)\nOverlap (\u2192)\nFID (\u2193)\nmIoU (\u2191)\nAlign. (\u2192)\nOverlap (\u2192)\nFID (\u2193)\nCondition\nC \u2192 S + P\nLayoutTrans\n0.219\n0.014\n13.012\n11.237\n0.271\n0.016\n3.229\n38.910\nBLT\n0.203\n0.013\n11.743\n14.260\n0.232\n0.009\n16.742\n76.499\nLayoutGAN++\n0.263\n0.016\n3.544\n6.842\n0.354\n0.011\n1.713\n10.219\nMaskGIT\n0.267\n0.001\n26.865\n27.470\n0.320\n0.004\n1.857\n16.898\nDiffusionLM\n0.299\n0.018\n17.655\n31.644\n0.262\n0.027\n3.532\n20.021\nLayoutDM\n0.275\n0.010\n11.938\n3.576\n0.310\n0.010\n0.024\n7.915\nLayoutNUWA-L2-DS (ours)\n0.351\n0.009\n10.190\n3.728\n0.337\n0.009\n0.058\n6.986\nLayoutNUWA-L2-DA (ours)\n0.386\n0.011\n10.214\n3.101\n0.324\n0.011\n0.077\n6.890\nLayoutNUWA-CL-DS (ours)\n0.377\n0.009\n10.263\n3.706\n0.376\n0.008\n0.053\n6.715\nLayoutNUWA (ours)\n0.445\n0.004\n7.943\n2.524\n0.385\n0.001\n0.086\n6.579\nCondition\nC + S \u2192 P\nLayoutTrans\n0.311\n0.011\n11.902\n9.368\n0.315\n0.013\n2.531\n31.627\nBLT\n0.341\n0.008\n13.470\n4.487\n0.356\n0.006\n5.469\n8.831\nLayoutGAN++\n0.349\n0.011\n9.628\n6.219\n0.346\n0.008\n2.746\n9.936\nMaskGIT\n0.331\n0.003\n26.390\n12.898\n0.384\n0.005\n1.950\n5.453\nDiffusionLM\n0.278\n0.020\n11.884\n15.931\n0.324\n0.014\n3.990\n16.407\nLayoutDM\n0.391\n0.009\n12.072\n2.288\n0.381\n0.010\n2.041\n4.175\nLayoutNUWA-L2-DS (ours)\n0.462\n0.008\n10.436\n3.035\n0.426\n0.010\n1.752\n4.105\nLayoutNUWA-L2-DA (ours)\n0.464\n0.007\n10.117\n2.973\n0.464\n0.009\n1.984\n3.993\nLayoutNUWA-CL-DS (ours)\n0.469\n0.007\n9.856\n2.984\n0.466\n0.009\n1.610\n4.012\nLayoutNUWA (ours)\n0.564\n0.007\n7.968\n2.870\n0.483\n0.002\n0.108\n3.697\nCompletion\nLayoutTrans\n0.561\n0.008\n10.080\n3.733\n0.439\n0.012\n2.053\n8.689\nBLT\u2020\n0.471\n0.007\n53.658\n121.110\n0.157\n0.002\n109.483\n155.157\nMaskGIT\n0.537\n0.024\n9.242\n33.463\n0.349\n0.011\n4.768\n12.013\nDiffusionLM\n0.218\n0.021\n8.681\n22.220\n0.332\n0.012\n4.436\n16.576\nLayoutDM\n0.580\n0.009\n15.676\n9.224\n0.377\n0.011\n1.891\n7.570\nLayoutNUWA-L2-DS (ours)\n0.610\n0.009\n7.239\n8.875\n0.407\n0.010\n1.337\n7.337\nLayoutNUWA-L2-DA (ours)\n0.624\n0.007\n10.457\n8.724\n0.477\n0.012\n1.383\n7.169\nLayoutNUWA-CL-DS (ours)\n0.641\n0.007\n7.529\n8.734\n0.473\n0.012\n1.311\n7.233\nLayoutNUWA (ours)\n0.616\n0.007\n8.123\n7.542\n0.481\n0.009\n1.292\n6.929\nReal Data\n-\n0.438\n0.004\n8.706\n6.25\n0.691\n0.001\n0.039\n1.85\nTable 2: Quantitative comparison on the RICO and PubLayNet Datasets. For Align. and Overlap\nmetrics, the closer to the real data, the better performance is (indicated by \u2192).\ntime, which has resulted in significant performance enhancements, i.e., performance has improved\nfrom 19.206 to 9.741. Furthermore, when we use CodeLLaMA, which is tuned on code language,\nthe performance improves even further to 8.985; 3) since different domains require distinct layout\nformats, early numerical-based methods could only be trained in a domain-specific manner. How-\never, LayoutNUWA is based on code structure, which can be trained in a domain-agnostic manner,\nallowing for complementary among data from various domains, thus further improving FID to 8.791.\nWe have also conducted extensive experiments on two other datasets: RICO and PubLayNet, as\nshown in Tab. 2. The LayoutNUWA notably surpasses all baseline methods in the majority of\ntasks. Although it does not achieve the best performance in two specific tasks, it still secures at\nleast the second-highest performance in those instances. This shows the strong generalization of the\nLayoutNUWA. It is worth mentioning that our model also achieves closer Align. and Overlap scores\nto the Real Data compared to the baselines. Although previous work has suggested that refinement\nand discriminator processes can contribute to improving the Align. and Overlap (Inoue et al., 2023;\nLi et al., 2019) scores, our method attains better results without employing these steps.\n7\n*Preprint\nText\nTitle\nText\nTitle\nText\nTitle\nTable\nText\nTitle\nFigure\nText\nTitle\nList\nFigure\nText\nTitle\nList\nFigure\nText\nTitle\nList\nFigure\nText\nList\n(Ours)\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nLayoutNUWA\nLayoutDM\n(Ours)\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nText\nTitle\nText\nTitle\nList\nTable\nText\nTitle\nList\nTable\nText\nTitle\nFigure\nC\u00a0\u2192 S\u00a0+\u00a0P\nC\u00a0+\u00a0S\u00a0\u2192 P\nCompletion\nFigure 3: Samples generated by LayoutNUWA on the PubLayNet dataset.\n4.3\nQUALITATIVE EVALUATION\nWe render the generated layout code with the Code Rendering (CR) method, and Fig. 3 shows the\nsampled rendering results of the PubLayNet dataset. By comparing with other baselines, we can\nobserve that the layouts generated by LayoutNUWA exhibit excellent element alignment, and the\nproportion of overlap between elements is minimal. Additionally, our results are the most consis-\ntent with the Real Design data, i.e., the size and position of the generated element are essentially\nconsistent with the real design, indicating that by treating the layout generation task as a code gener-\nation task, LayoutNUWA has successfully learned the distribution of document layouts, thus result\nin more precise and realistic layouts. More sampled cases can be referred to Fig. 5.\n5\nABLATION STUDY\nWe investigate the effectiveness of the CIT tuning method in Sec. 5.1 and compare the impact of\ndifferent output formats and fine-tuning in Sec. 5.2. More concretely, we set the LayoutNUWA-L2-\nDS model as the basic setting and conduct the ablation studies on the Magazine dataset.\n5.1\nEFFECT OF TUNING METHODS\nWe progressively reduce the modules in CIT and fine-tune the model using the corresponding con-\nstructed data. Specifically, we first exclude the code template and directly convert the element\ninformation into an ordered sequence S with a task instruction before it, i.e., the instruction tuning\nmethod. Then, we further remove the task instruction and directly fine-tune the model using data\nfrom different tasks separately, i.e., the numerical tuning method. As shown in Tab. 3, we can ob-\nserve that the model performance has declined significantly without the code template, and it can\nonly work in the DS setting since the model can simply generate repetitive and out-of-order results\nthat are inconsistent with the element sequence in the DA setting. Furthermore, the numerical tuning\nmethod can only support the DS setting as there is no task instruction for the model to distinguish\nbetween different tasks, and the model performance is far inferior compared to those of the CIT as\nsuch an approach overlooks the rich semantic information among the elements and can not calibrate\nthe prior code knowledge of LLMs.\n8\n*Preprint\nTask\nModels\nTuning Method\nmIoU (\u2191)\nAlign. (\u2192)\nOverlap (\u2192)\nFID (\u2193)\nFail (\u2193)\nCondition\nC \u2192 S + P\nLayoutNUWA-L2-DS\nCTT\n0.260\n0.021\n2.898\n9.741\n0.000 %\nw/o template\nInstruct Tuning (DS)\n0.124\n0.049\n3.221\n16.324\n1.020 %\nw/o template\nInstruct Tuning (DA)\n-\n-\n-\n-\n0.000 %\nw/o template&instruct\nNumerical Tuning\n0.126\n0.053\n3.581\n17.982\n3.571 %\nCondition\nC + S \u2192 P\nLayoutNUWA-L2-DS\nCIT\n0.358\n0.020\n2.483\n4.682\n0.000 %\nw/o template\nInstruct Tuning (DS)\n0.182\n0.021\n2.673\n12.432\n0.000 %\nw/o template\nInstruct Tuning (DA)\n-\n-\n-\n-\n0.000 %\nw/o template&instruct\nNumerical Tuning\n0.189\n0.024\n2.892\n14.326\n0.000 %\nCompletion\nLayoutNUWA-L2-DS\nCIT\n0.418\n0.020\n2.309\n7.257\n0.253 %\nw/o template\nInstruct Tuning (DS)\n0.206\n0.017\n2.882\n15.732\n5.102 %\nw/o template\nInstruct Tuning (DA)\n-\n-\n-\n-\n6.633 %\nw/o template&instruct\nNumerical Tuning\n0.214\n0.020\n3.003\n16.243\n6.122 %\nReal Data\n-\n-\n0.348\n0.016\n1.521\n6.695\n-\nTable 3: Comparison among different tuning methods, where \u201cFail\u201d is the failure ratio of generation.\nTask\nModel\nLayout\nFormat\nmIoU (\u2191)\nAlign. (\u2192)\nOverlap (\u2192)\nFID (\u2193)\nFail (\u2193)\nCondition\nC \u2192 S + P\nLayoutNUWA-N\nNumerical\n0.000\n0.000\n0.867\n-\n78.030 %\nLayoutNUWA-L2-DS\nCode\n0.260\n0.021\n2.898\n9.741\n0.000 %\nCondition\nC + S \u2192 P\nLayoutNUWA-N\nNumerical\n0.000\n0.000\n24.959\n349.231\n21.717 %\nLayoutNUWA-L2-DS\nCode\n0.358\n0.020\n2.483\n4.682\n0.000 %\nCompletion\nLayoutNUWA-N\nNumerical\n0.000\n0.000\n16.602\n-\n29.293 %\nLayoutNUWA-L2-DS\nCode\n0.418\n0.020\n2.309\n7.257\n0.253 %\nReal Data\n-\n-\n0.348\n0.016\n1.521\n6.695\n-\nTable 4: Comparison among different output formats.\n5.2\nEFFECT OF OUTPUT FORMAT AND FINETUNING\nModel\nC \u2192 S + P\nC + S \u2192 P\nCompletion\nFail (\u2193)\nFail (\u2193)\nFail (\u2193)\nLLaMA2 (Zero-Shot)\n100.0 %\n100.0 %\n100.0 %\nCodeLLaMA (Zero-shot)\n100.0 %\n100.0 %\n100.0 %\nGPT-4 (Zero-Shot)\n34.2 %\n28.8 %\n28.5 %\nLayoutNUWA\n0.0 %\n0.0 %\n0.3 %\nTable 5: Comparison with LLMs.\nWe compared the effects of the model out-\nput in code format and numerical format.\nFor the numerical output format, we de-\nsigned a Code Infilling task, which in-\nvolves making the LLM predict only the\nmasked values rather than predicting the\nentire code sequence. As shown in Tab. 4,\nwe can find that generating in numerical\nformat will increase the failure ratio of\nmodel generations, e.g., the model will\ngenerate repetitive results, and significantly decrease the model performance. This is because the\nlayout generated by the conditional layout generation task should be logical, while only predicting\nthe masked parts can lead to discrete values that lack logic. Besides, Due to the influence of the\nautoregressive manner, where the content generated in the next step depends on the previous history,\nthis phenomenon may result in a higher failure probability of model generation when predicting\nlayouts with more masked values. We also conduct a comparison between LayoutNUWA and GPT-\n4 (Bubeck et al., 2023). Specifically, we allow GPT-4 to perform inference by constructing the input\nusing the CIT method. Tab. 5 shows code instruct tuning for LLM is necessary, as using LLM in a\nzero-shot manner leads to a high fail rate (100% fail rate of LLaMA2 and around 30% for GPT-4).\n6\nCONCLUSION\nIn this paper, we propose LayoutNUWA, a groundbreaking approach that treats layout generation\nas a code generation task, effectively enriching the semantic information of layouts and leveraging\nthe hidden expertise of LLMs. Extensive experiments on multiple datasets have demonstrated the\nsuperiority of our method. This research has the potential to revolutionize the field of layout genera-\ntion and pave the way for further exploration and development of semantic-aware layout generation\napproaches in various applications.\n9\n*Preprint\nREFERENCES\nDiego Martin Arroyo, Janis Postels, and Federico Tombari. Variational transformer networks for\nlayout generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 13642\u201313652, 2021.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\nSparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nShang Chai, Liansheng Zhuang, and Fengying Yan. Layoutdm: Transformer-based diffusion model\nfor layout generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 18349\u201318358, 2023.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 11315\u201311325, 2022.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nChin-Yi Cheng, Forrest Huang, Gang Li, and Yang Li. Play: Parametrically conditioned layout\ngeneration using latent diffusion. arXiv preprint arXiv:2301.11529, 2023.\nBiplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey\nNichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design ap-\nplications. In Proceedings of the 30th annual ACM symposium on user interface software and\ntechnology, pp. 845\u2013854, 2017.\nXinhan Di and Pengqian Yu. Multi-agent reinforcement learning of 3d furniture layout simulation\nin indoor graphics scenes. arXiv preprint arXiv:2102.09137, 2021.\nWeixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and genera-\ntion with large language models. arXiv preprint arXiv:2305.15393, 2023.\nTsu-Jui Fu, William Yang Wang, Daniel McDuff, and Yale Song. Doc2ppt: automatic presentation\nslides generation from scientific documents. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pp. 634\u2013642, 2022.\nYuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang. Learning instructions with unlabeled data for\nzero-shot cross-task generalization. arXiv preprint arXiv:2210.09175, 2022.\nShunan Guo, Zhuochen Jin, Fuling Sun, Jingwen Li, Zhaorui Li, Yang Shi, and Nan Cao. Vinci: an\nintelligent graphic design system for generating advertising posters. In Proceedings of the 2021\nCHI conference on human factors in computing systems, pp. 1\u201317, 2021.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022a.\nOr Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few\nexamples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022b.\nMude Hui, Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yuwang Wang, and Yan Lu. Unifying\nlayout generation with a decoupled diffusion model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 1942\u20131951, 2023.\nNaoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Layoutdm:\nDiscrete diffusion model for controllable layout generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 10167\u201310176, 2023.\n10\n*Preprint\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00b4aniel Simig, Ping Yu,\nKurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model\ninstruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017,\n2022.\nZhaoyun Jiang, Shizhao Sun, Jihua Zhu, Jian-Guang Lou, and Dongmei Zhang. Coarse-to-fine\ngenerative modeling for graphic layouts. In Proceedings of the AAAI conference on artificial\nintelligence, volume 36, pp. 1096\u20131103, 2022.\nAkash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. Layoutvae: Stochas-\ntic scene layout generation from a label set. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pp. 9895\u20139904, 2019.\nKotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Constrained graphic layout\ngeneration via latent optimization. In Proceedings of the 29th ACM International Conference on\nMultimedia, pp. 88\u201396, 2021.\nXiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, and Irfan Essa. Blt:\nbidirectional layout transformer for controllable layout generation. In European Conference on\nComputer Vision, pp. 474\u2013490. Springer, 2022.\nHsin-Ying Lee, Lu Jiang, Irfan Essa, Phuong B Le, Haifeng Gong, Ming-Hsuan Yang, and Weilong\nYang. Neural design network: Graphic layout generation with constraints. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nIII 16, pp. 491\u2013506. Springer, 2020.\nFengheng Li, An Liu, Wei Feng, Honghe Zhu, Yaoyu Li, Zheng Zhang, Jingjing Lv, Xin Zhu,\nJunjie Shen, Zhangang Lin, et al. Relation-aware diffusion model for controllable poster layout\ngeneration. arXiv preprint arXiv:2306.09086, 2023.\nJianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, and Tingfa Xu. Layoutgan: Generating\ngraphic layouts with wireframe discriminators. arXiv preprint arXiv:1901.06767, 2019.\nXiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-\nlm improves controllable text generation. Advances in Neural Information Processing Systems,\n35:4328\u20134343, 2022.\nJames MacQueen et al. Some methods for classification and analysis of multivariate observations. In\nProceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1,\npp. 281\u2013297. Oakland, CA, USA, 1967.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual gen-\neralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al.\nShow\nyour work: Scratchpads for intermediate computation with language models.\narXiv preprint\narXiv:2112.00114, 2021.\nPeter O\u2019Donovan, Aseem Agarwala, and Aaron Hertzmann. Designscape: Design with interactive\nlayout suggestions. In Proceedings of the 33rd annual ACM conference on human factors in\ncomputing systems, pp. 1221\u20131224, 2015.\nPeter O\u2019Donovan, Aseem Agarwala, and Aaron Hertzmann. Learning layouts for single-pagegraphic\ndesigns. IEEE transactions on visualization and computer graphics, 20(8):1200\u20131213, 2014.\nAkshay Gadi Patil, Omri Ben-Eliezer, Or Perel, and Hadar Averbuch-Elor. Read: Recursive au-\ntoencoders for document layout generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pp. 544\u2013545, 2020.\n11\n*Preprint\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20: International Conference for High Perfor-\nmance Computing, Networking, Storage and Analysis, pp. 1\u201316. IEEE, 2020.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\nenables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan\nZhang. Large language models are in-context semantic reasoners rather than symbolic reasoners.\narXiv preprint arXiv:2305.14825, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nLijun Wu, Xu Tan, Di He, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu.\nBeyond error\npropagation in neural machine translation: Characteristics of language also matter. arXiv preprint\narXiv:1809.00120, 2018.\nYang Wu, Yanyan Zhao, Zhongyang Li, Bing Qin, and Kai Xiong. Improving cross-task generaliza-\ntion with step-by-step instructions. arXiv preprint arXiv:2305.04429, 2023.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\narXiv preprint arXiv:2304.12244, 2023.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Ze-\nroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization.\narXiv preprint arXiv:2201.06910, 2022.\nKota Yamaguchi. Canvasvae: Learning to generate vector graphic documents. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 5481\u20135489, 2021.\nCheng-Fu Yang, Wan-Cyuan Fan, Fu-En Yang, and Yu-Chiang Frank Wang. Layouttransformer:\nRelation-aware scene layout generation. 2020.\nXuyong Yang, Tao Mei, Ying-Qing Xu, Yong Rui, and Shipeng Li. Automatic generation of visual-\ntextual presentation layout. ACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM), 12(2):1\u201322, 2016.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.\nJunyi Zhang, Jiaqi Guo, Shizhao Sun, Jian-Guang Lou, and Dongmei Zhang. Layoutdiffusion:\nImproving graphic layout generation by discrete diffusion probabilistic models. arXiv preprint\narXiv:2303.11589, 2023a.\n12\n*Preprint\nRuohong Zhang, Yau-Shian Wang, and Yiming Yang. Generation-driven contrastive self-training for\nzero-shot text classification with instruction-tuned gpt. arXiv preprint arXiv:2304.11872, 2023b.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi\nHu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv\npreprint arXiv:2308.10792, 2023c.\nXinru Zheng, Xiaotian Qiao, Ying Cao, and Rynson WH Lau. Content-aware generative modeling\nof graphic design layouts. ACM Transactions on Graphics (TOG), 38(4):1\u201315, 2019.\nXu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for docu-\nment layout analysis. In 2019 International Conference on Document Analysis and Recognition\n(ICDAR), pp. 1015\u20131022. IEEE, 2019.\n13\n*Preprint\nA\nLIMITATIONS\nSince LayoutNUWA employs the autoregressive (AR) LLMs as the backbone, our method naturally\ninherits the shortcomings of the AR models:\n\u2022 The generation speed is slower than the non-autoregressive models (Chang et al., 2022).\n\u2022 It suffers from the error propagation problem (Wu et al., 2018) especially when training is\ninsufficient, where the content generated later in the sequence may be negatively affected\nby the errors in the content generated earlier.\nIn our future work, we will address these challenges and make improvements to generate better\ngraphic layouts.\nB\nCOMPARISON WITH GPT-4\nWe utilize the GPT-4 model with the commercial API and strictly follow the usage policy 3. We\nreport the detailed performance of the GPT-4 model in Tab. 6 and show several rendered graphic\nlayouts in Fig. 4. We can observe that the content generated by GPT-4 in the zero-shot setting\nprimarily follows the layout design rule, which further confirms the potential capability of LLMs in\ngenerating layouts when guided by the CIT approach. However, when compared to LayoutNUWA,\nthere are several issues with the results generated by GPT-4: 1) the distribution of elements is uneven,\nwith elements tending to be concentrated in certain areas, such as the left side of the canvas; 2) the\nelement sizes are inconsistent, for instance, in some graphic layouts, there might be one or two large\nelements, which results in the high scores of the mIOU and Overlap metrics for some tasks; 3) there\nis a significant discrepancy between the data distribution of generated content and the real data.\nTask\nModel\nmIOU (\u2193)\nAlign. (\u2192)\nOverlap (\u2192)\nFID (\u2193)\nFail (\u2193)\nCondition\nC \u2192 S + P\nGPT-4 (Zero-Shot)\n0.264\n0.006\n0.165\n-\n34.184 %\nLayoutNUWA-L2-DS\n0.260\n0.021\n2.898\n9.741\n0.000 %\nCondition\nC + S \u2192 P\nGPT-4 (Zero-Shot)\n0.330\n0.011\n1.149\n-\n28.788 %\nLayoutNUWA-L2-DS\n0.358\n0.020\n2.483\n4.682\n0.000 %\nCompletion\nGPT-4 (Zero-Shot)\n0.362\n0.044\n0.728\n-\n28.535 %\nLayoutNUWA-L2-DS\n0.418\n0.020\n2.309\n7.257\n0.253 %\nReal Data\n-\n0.348\n0.016\n1.521\n6.695\n-\nTable 6: Detailed performance of GPT4 on the Magazine dataset. It is worth noting that due to the\nsignificant difference between the results generated by GPT-4 and the real data, the FID score cannot\nbe calculated.\nC\nHUMAN EVALUATION\nWe conduct the human evaluation for the model performance on the RICO and PubLayNet datasets.\nSpecifically, We compare LayoutNUWA with two other strong baselines, including LayoutDM (In-\noue et al., 2023) and LayoutTransformer (Yang et al., 2020), and randomly sample 25 graphic lay-\nouts generated from each model. We invite the annotators to choose which model performs better\naccording to two evaluation settings: 1) quality evaluation based on the detail depiction, overlapping\ndegree, and layout rationality in each layout; 2) diversity evaluation based on the diversity of the\nelement arrangement in each layout. We hire 10 annotators to give their preferences, and the results\nare shown in Fig. 4(a) and Fig. 4(b). We can observe that layoutNUWA significantly outperforms\nthe other two strong baselines, i.e., LayoutDM and LayoutTransformer, in terms of both generation\nquality and generation diversity. More generated cases can be referred to Fig. 4 (Magazine dataset)\nand Fig. 5 (RICO and PubLayNet datasets).\n3https://openai.com/policies/terms-of-use\n14\n*Preprint\nQuality\nDiversity\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.60\n0.56\n0.32\n0.32\n0.08\n0.12\nLayoutNUWA\nLayoutDM\nLayoutTrans\n(a) Human evaluation on the RICO dataset.\nQuality\nDiversity\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.48\n0.44\n0.36\n0.32\n0.16\n0.24\nLayoutNUWA\nLayoutDM\nLayoutTrans\n(b) Human evaluation on the PubLayNet dataset.\n15\n*Preprint\nLayoutNUWA\nGPT-4\nGolden\nC + S \u00e0 P\nLayoutNUWA\nGPT-4\nGolden\nC \u00e0 S + P\nLayoutNUWA\nGPT-4\nGolden\nCompletion\nFigure 4: Comparison of rendered graphic layouts between GPT4 and LayoutNUWA on the Maga-\nzine dataset.\n16\n*Preprint\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\n(Ours)\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nText\nImage\nIcon\nInput\nWeb\nAdver\nText\nImage\nIcon\nButton\nIndic\nSwitch\nToolbar\nText\nImage\nIcon\nButton\nIndic\nText\nButton\nIcon\nList\nToolbar\nText\nImage\nIcon\nButton\nToolbar\nMulti\nText\nImage\nIcon\nButton\nList\nWeb\u00a0View\nDrawer\nAdver\nText\nImage\nIcon\nText\nImage\nIcon\nButton\nInput\nCard\nAdver\nToolbar\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nText\nImage\nButton\nText\nImage\nButton\nDrawer\nToolbar\nText\nImage\nIcon\nButton\nInput\nToolbar\nText\nImage\nIcon\nButton\nWeb\u00a0View\nAdver\nToolbar\nC\u00a0\u2192 S\u00a0+\u00a0P\nC\u00a0+\u00a0S\u00a0\u2192 P\nCompletion\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nText\nTitle\nText\nTitle\nText\nTitle\nTable\nText\nTitle\nFigure\nText\nTitle\nList\nFigure\nText\nTitle\nList\nFigure\nText\nTitle\nList\nFigure\nText\nList\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nText\nTitle\nText\nTitle\nList\nTable\nText\nTitle\nList\nTable\nText\nTitle\nFigure\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nC\u00a0\u2192 S\u00a0+\u00a0P\nC\u00a0+\u00a0S\u00a0\u2192 P\nCompletion\nR\u00a0I\u00a0C\u00a0O\nP\u00a0u\u00a0b\u00a0L\u00a0a\u00a0y\u00a0N\u00a0e\u00a0t\n(Ours)\nLayoutNUWA\nLayoutDM\nLayoutTrans\nMaskGIT\nReal\u00a0Design\nCategory\nFigure 5: Samples generated by LayoutNUWA on the RICO and PubLayNet dataset.\n17\n"
  },
  {
    "title": "Cure the headache of Transformers via Collinear Constrained Attention",
    "link": "https://arxiv.org/pdf/2309.08646.pdf",
    "upvote": "12",
    "text": "CoCA: Fusing Position Embedding with Collinear Constrained Attention\nin Transformers for Long Context Window Extending\nShiyi Zhu, Jing Ye, Wei Jiang, Siqiao Xue, Qi Zhang, Yifan Wu, Jianguo Li\nAnt Group\n{zhushiyi.zsy, qianye.yj, shouzhi.jw, lijg.zero}@antgroup.com\nAbstract\nSelf-attention and position embedding are two\nkey modules in transformer-based Large Lan-\nguage Models (LLMs). However, the poten-\ntial relationship between them is far from well\nstudied, especially for long context window ex-\ntending. In fact, anomalous behaviors harming\nlong context extrapolation exist between Ro-\ntary Position Embedding (RoPE) and vanilla\nself-attention unveiled by our work. To address\nthis issue, we propose a novel attention mecha-\nnism, CoCA (Collinear Constrained Attention).\nSpecifically, we enforce a collinear constraint\nbetween Q and K to seamlessly integrate RoPE\nand self-attention. While only adding minimal\ncomputational and spatial complexity, this in-\ntegration significantly enhances long context\nwindow extrapolation ability. We provide an\noptimized implementation, making it a drop-\nin replacement for any existing transformer-\nbased models. Extensive experiments show\nthat CoCA performs extraordinarily well in\nextending context windows. A CoCA-based\nGPT model, trained with a context length of\n512, can seamlessly extend the context win-\ndow up to 32K (60\u00d7), without any fine-tuning.\nAdditionally, by dropping CoCA in LLaMA-\n7B, we achieve extrapolation up to 32K within\nonly 2K training length.\nOur code is pub-\nlicly available at: https://github.com/codefuse-\nai/Collinear-Constrained-Attention\n1\nIntroduction\nIn the seminal work of Transformer (Vaswani et al.,\n2017), it claims the ability of \"extrapolating to se-\nquence length longer than the ones encountered\nduring training\". This is an ideal hypothesis, but ac-\ntually not work in practice for vanilla Transformer.\nSeveral subsequent works, collectively known as\nlong context extrapolation, have delved into ex-\nploring the capabilities of large language models\n(LLMs) trained within the range of [1, N \u2212 1] to\neffectively extend the testing sequence \u2265 N.\nFigure 1: Perplexity evaluation on 100 PG-19 documents with\na sliding window strategy (Stride = 512). The perplexity of\nRoFormer (Su et al., 2024) sharply exceeds 1000 beyond its\ntraining length, while CoCA maintains a low plateau even at\n60 \u00d7 its training length. ALibi (Press et al., 2022) encounters\nOut of Memory (OOM) issues for input Nmax > 8000 due to\nflash-attention (Dao et al., 2022) incompatibility, we suppose\nit maintains perplexity for Nmax > 8000.\nExisting studies primarily focus on attention ker-\nnel (Beltagy et al., 2020; Ding et al., 2023; Han\net al., 2023) or position embedding (Huang et al.,\n2023), often neglecting the intrinsic relationship\nbetween the two key modules. Attention bias is an\nalternative to the explicit encoding of positional in-\nformation. ALibi (Press et al., 2022) and KERPLE\n(Chi et al., 2022), incorporate heuristic and com-\npositional triangle kernel-based negative causal at-\ntention bias, respectively. While these approaches\neffectively manage to maintain low perplexity, they\nfall short in capturing long-range dependencies due\nto introducing local hypotheses to context tokens.\nAnother branch of methods involve simply scal-\ning Rotary Position Embedding (RoPE) (Su et al.,\n2024) to extrapolate the inference context length\nwith minimal or no fine-tuning. For instance, Posi-\ntion Interpolation (PI) (Chen et al., 2023) employs\nlinear scaling on each position number from n to\nn/k, where k is the extrapolation ratio. NTK-aware\nScaled RoPE (bloc97, 2023) and Dynamic-NTK\narXiv:2309.08646v3  [cs.LG]  28 Feb 2024\n(Emozilla, 2023) combine high-frequency extrapo-\nlation and low-frequency interpolation. They scale\nthe basis in RoPE upon the sequence length to\nadapt to the unseen position indices. However,\nthese methods primarily alleviate the problem of\nmodeling the rotation angles in out-of-distribution\npositions, without recognizing the intrinsic correla-\ntion between attention matrices and rotation angles.\nTherefore, these methods still suffer from a limited\ncontext window extending ratio.\nHere, we present a new perspective on the rela-\ntionship between position embedding (with a fo-\ncus on RoPE) and the self-attention mechanism.\nIn a nutshell, RoPE utilizes a rotation matrix to\nencode absolute positions while simultaneously\nincorporating explicit relative position dependen-\ncies within the self-attention formulation (Su et al.,\n2024). It is designed based on the relative angular\ndifference between the queries (Q) and keys (K).\nHowever, latent relationships exist between Q and\nK, as these two matrices are directly multiplied.\nWe demonstrate that incorrect initialization of the\nangle between Q and K in RoPE leads to undesir-\nable behavior around the context window boundary,\nharming its performance for context extrapolation.\nTo address this undesirable behavior , we pro-\npose an innovative architecture called Collinear\nConstrained Attention (CoCA). Specifically, we\nenforce a collinear constraint between Q and K\nby initializing the angle between every two hid-\nden dimensions in the Q and K vectors to 0. This\nallows for a seamless integration of RoPE and self-\nattention. The model architecture and comparison\nwith RoFomer (Su et al., 2024) is illustrated in\nFigure 2.\nExtensive experiments show that a CoCA-based\nGPT model, trained within 512 context length,\nseamlessly extends the context window up to 32K\n(60x) without perplexity divergence. A compre-\nhensive comparison between our method and ex-\nisting methods is presented in Figure 1. Further-\nmore, it enhances long-context retrieval ability,\nachieving a passkey retrieval accuracy of 50%+\neven when extrapolating to 16x longer than its\ntraining context length by applying Dynamic-NTK\n(Emozilla, 2023). Additionally, by dropping CoCA\nin LLaMA-7B, we achieve extrapolation up to 32K\nwithin only 2K training length.\nOur main contributions can be summarized as\nfollows:\n\u2022 We unveil undesirable context boundary be-\nhavior resulting from the absence of modeling\nthe relationship between position embeddings\nand self-attention.\n\u2022 To tackle the undesirable context boundary be-\nhavior, we propose Collinear Constrained At-\ntention (CoCA) to seamlessly integrate the po-\nsition embeddings and self-attention, achiev-\ning excellent long context window extrapola-\ntion performance.\n\u2022 CoCA extends its context window from 512\nto 32K without fine-tuning, achieving over\n50% accuracy even when 16 \u00d7 longer than its\ntraining length. Using CoCA in LLaMA-7B,\nwe achieve extrapolation up to 32K within just\n2K training length.\n\u2022 CoCA introduces minimal computational and\nspatial complexity compared to vanilla self-\nattention. We provide an optimized imple-\nmentation of CoCA, making it able to be\na seamless drop-in replacement for existing\ntransformer-based models.\n2\nMethod\nIn this section, we describe our proposed Collinear\nConstrained Attention (CoCA). We begin with in-\ntroducing the background theory of RoPE (Su et al.,\n2024) in Section 2.1, and then analyze the anoma-\nlous behaviors between the attention matrices and\nRoPE in Section 2.2. Finally, we introduce the\nproposed method CoCA in section 2.3 and derive\na slack constraint version of CoCA in Section 2.4,\nrespectively.\n2.1\nRotary Position Embedding\nPosition embedding is a crucial component in\ntransformer-based models. Here we focus on Ro-\ntary Position Embedding (RoPE) (Su et al., 2024),\nwhich is widely used by LLMs including LLaMA\n(Touvron et al., 2023a), LLaMA-2 (Touvron et al.,\n2023b), GPT-NeoX (Black et al., 2022) and Qwen\n(Bai et al., 2023). Suppose the positional index is\nan integer n \u2208 [1, N], and the corresponding in-\nput vector x = [x0, x1, ..., xd\u22121]T, where N is the\nsequence length, d is the dimension of the atten-\ntion head. RoPE defines a vector-valued complex\nfunction f(x, n):\nf(x, n) = [(x0 + ix1)ein\u03b80, (x2 + ix3)ein\u03b81,\n. . . , (xd\u22121 + ixd)ein\u03b8d/2\u22121]T,\nwhere \u03b8j = B\u22122j/d,\n(1)\nFigure 2: Architecture comparison between RoFormer and CoCA. (a) RoFormer; (b) CoCA; (c) The implementation detial\nof K in CoCA. Q, T, and V are produced using projection matrices identical to those employed in the vanilla self-attention. T\nundergoes a halving operation, with the other half being duplicated. K is then computed as the element-wise product of Q and T,\nadhering to a collinear constraint with Q. Note that kn \u2208 RN\u00d7d, where n \u2208 [1, N] is the positional index of key, d is the head\ndimension, N is the sequence length.\nin this paper, the base B = 10, 000.\nAfter the application of RoPE, the transformed\nvectors for query (q) and key (k) become f(q, m)\nand f(k, n), respectively. Here, m, n \u2208 [0, N]\nrepresent the positional indices of q and k. The\nattention operation is computed as the dot product\nbetween f(q, m) and f(k, n), defined as follows:\na(m, n) = Re(\u27e8f(q, m), f(k, n)\u27e9)\n= Re\n\uf8ee\n\uf8f0\nd/2\u22121\nX\nj=0\n(q2j + iq2j+1)(k2j \u2212 ik2j+1)ei(m\u2212n)\u03b8j\n\uf8f9\n\uf8fb\n=\nd/2\u22121\nX\nj=0\n[(q2jk2j + q2j+1k2j+1) cos((m \u2212 n)\u03b8j)\n+ (q2jk2j+1 \u2212 q2j+1k2j) sin((m \u2212 n)\u03b8j)]\n(2)\nThe attention score a(m \u2212 n) depends on the rela-\ntive position (m \u2212 n).\n2.2\nAnomalous Behavior between RoPE and\nAttention Matrices\nAfter applying RoPE, the attention score a(m \u2212 n)\ncan be interpreted as the sum of d/2 inner prod-\nucts of complex numbers, as illustrated in Equa-\ntion (2). For any pair of qj = (q2j, q2j+1) and\nkj = (k2j, k2j+1), which is the 2-dimensional\nslicing of q (or qm) and k (or kn), we introduce\nthe initial angle \u0398j between them, measured coun-\nterclockwise from kj to qj in the complex plane.\nThroughout our analysis, we keep the position of\nkj fixed, systematically rotating qj to compre-\nhensively examine their relative positions. The\nfinal angle between qj and kj is represented as\n\u03b8(qj, kj) = \u0398j + (m \u2212 n)\u03b8j, where m and n are\npositional indices of qj and kj.\nIn this concept, the attention score can be for-\nmulized as:\na(m, n) =\nd/2\u22121\nX\nj=0\n|qj||kj| cos(\u03b8(qj, kj))\n(3)\nRefer to Figure 3 for a visual representation of\nthis concept for any individual j \u2208 [0, d/2] in the\n2-D subspace. There are four distinct scenarios\nbetween qj and kj after rotation.\n(1) Scenario (b) and (c): When m > n and\n\u0398j \u2264 \u03c0, or m < n and \u0398j > \u03c0, the value of\ncos(\u03b8(qj, kj)) between qj and kj decreases with\nthe expanding distance between m and n. In these\n2 scenarios, no anomalous behavior is observed,\nas the attention score naturally decreases with the\npositional distance. This trend persists until the rel-\native angle \u03b8(qj, kj) rotates beyond the boundary\nof \u03c0.\nFigure 3: Anomalous behavior of RoPE in 2-D plane. The inner product of vectors qj and kj is contingent upon the relative angle\n\u03b8(qj, kj), defined as \u0398j + (m \u2212 n)\u03b8j. Here, \u0398j represents the initial angle, and (m \u2212 n)\u03b8j signifies the position-dependent\nrotation angle. (a) m < n and \u0398j \u2264 \u03c0. (b) m > n and \u0398j \u2264 \u03c0. (c) m < n and \u0398j > \u03c0. (d) m > n and \u0398j > \u03c0.\n(2) Scenario (a) and (d): When m < n and\n\u0398j \u2264 \u03c0, or m > n and \u0398j > \u03c0, intriguing phe-\nnomena emerge. As the distance between m and\nn grows, the value of cos(\u03b8(qj, kj)) between qj\nand kj paradoxically increases. This anomaly has\na notable impact on attention scores, particularly\naffecting the \u03c4 closest tokens. In this context, \u03c4 is\ndefined as \u0398j/\u03b8j for scenario (a) and (2\u03c0\u2212\u0398j)/\u03b8j\nfor scenario (d). Consequently, attention scores for\nthese tokens are abnormally diminished.\nFor bidirectional language models, all four cases\nmay occur. For causal models, only scenario (b)\nand (d) manifest, as m consistently exceeds n.\nThe attention score a(m \u2212 n) is the sum of d/2\ninner-products, one of them turns out anomalous\nmay be insignificant, however, experiments con-\nfirmed this significance. Further analysis of this\nrotary borders anomalous behaviour is discussed in\nAppendix D.2.\n2.3\nCollinear Constrained Attention\nTo tackle the anomalous behavior between RoPE\nand attention matrices, we propose a novel ap-\nproach called Collinear Constrained Attention\n(CoCA). Specifically, by applying a collinear con-\nstraint to any pair of qj = (q2j, q2j+1) and kj =\n(k2j, k2j+1), we seamlessly integrate RoPE into\nself-attention mechanism, achieving long context\nextrapolation.\nTo formalize this, considering a sequence of N\ninput tokens SN = {wn}N\nn=1, with corresponding\nword embeddings EN = {xn}N\nn=1, where xn \u2208\nRd is the d-dimensional word embedding vector of\ntoken wn without position information. First, the\nqueries qm are obtained:\nqm = WQxm, \u2200m \u2208 [1, N]\n(4)\nNext, we derive the keys kn with collinear con-\nstraints. This begins with the introducing of the\nconstraint coefficient tn for each token position n,\nas depicted in Equation (5).\ntn = WT xn, \u2200n \u2208 [1, N]\n(5)\nNext, Equation (6) imposes the collinearity con-\ndition on the coefficients t2j and t2j+1, where\ntn = [t0, t1, ..., td\u22121]T, ensuring that each pair is\nidentical. This step effectively duplicates each 2-\ndimensional segment of the tensor.\nt2j = t2j+1, \u2200j \u2208 [0, d/2 \u2212 1]\ntn = Relu(tn)\n(6)\nSubsequently, the keys are calculated as shown\nin Equation (7), where kn are represented by the\nelement-wise multiplication of Q = (q1, ..., qN)\nand tn. This results in an expansion of dimen-\nsionality, as kn \u2208 RN\u00d7d now includes an addi-\ntional sequence length dimension.\nWe address\npotential memory pressure by optimizing tensor\ncontractions, ensuring no net increase in memory\nconsumption. For an in-depth analysis, please refer\nto Appendix C.\nkn = Q \u2299 tn = (q1 \u25e6 tn, ..., qN \u25e6 tn)\n(7)\nAfter that, we apply RoPE on Q and K, with the\nfunction f detailed in Equation (1).\nf(qm) = f(qm, m)\nf(kn) = f(Q \u2299 tn, n) = f(Q, n) \u2299 tn\n(8)\nFinally, the attention score of CoCA would be:\na(m, n) = Re(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9)\n(9)\nEquation (9) illustrates the additional dimension\nof the keys in our CoCA mechanism. Specifically,\nit maps the index of each query to the additional\ndimension, establishing a collinear relationship be-\ntween the n-th key and the m-th query. This is a\ncritical aspect of our method.\n2.4\nSlacking the Constraint on Query\nIn Section 2.3, we present a theoretically precise\nsolution for CoCA. However, practical implemen-\ntation faces challenges due to the complexity of\nO(N2d) when storing f(Q, n). To address this is-\nsue, we provide a dual implementation with O(Nd)\ncomplexity in this section and prove their equiva-\nlence.\nTheorem 1. (Dual implementation of CoCA) For\nany attention score defined in Equation (9), there\nexists an equivalent form as follows:\na(m, n) = Re(\u27e8f(qm, m), qm \u25e6 f(tn, n)\u27e9)\n(10)\nwith constraint:\nq2j = q2j+1, \u2200j \u2208 [0, d/2 \u2212 1]\n(11)\nProof: The proof consists of two steps.\nStep 1.\nWe prove that, by imposing the con-\nstraint q2j\n=\nq2j+1, \u2200j\n\u2208\n[0, d/2 \u2212 1],\nRe(\u27e8f(qm, m), qm \u25e6 f(tn, n)\u27e9) is equivalent to\nRe(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9).\nTo see this, we calculate the difference between\nf(qm, n) \u25e6 tn and qm \u25e6 f(tn, n):\nf(qm, n) \u25e6 tn \u2212 qm \u25e6 f(tn, n)\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nt0(q0 cos n\u03b80 \u2212 q1 sin n\u03b80)\nt1(q0 sin n\u03b80 + q1 cos n\u03b80)\n. . .\ntd\u22122(qd\u22122 cos n\u03b8d/2\u22121 \u2212 qd\u22121 sin n\u03b8d/2\u22121)\ntd\u22121(qd\u22122 sin n\u03b8d/2\u22121 + qd\u22121 cos n\u03b8d/2\u22121)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\u2212\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nq0(t0 cos n\u03b80 \u2212 t1 sin n\u03b80)\nq1(t0 sin n\u03b80 + t1 cos n\u03b80)\n. . .\nqd\u22122(td\u22122 cos n\u03b8d/2\u22121 \u2212 td\u22121 sin n\u03b8d/2\u22121)\nqd\u22121(td\u22122 sin n\u03b8d/2\u22121 + td\u22121 cos n\u03b8d/2\u22121)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(12)\nRecall that t2j = t2j+1, \u2200j \u2208 [0, d/2 \u2212 1] (see\nEquation (6)), Equation (12) is equivalent to:\nf(qm, n) \u25e6 tn \u2212 qm \u25e6 f(tn, n)\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nt0(q0 \u2212 q1) sin n\u03b80\nt1(q0 \u2212 q1) sin n\u03b80\n. . .\ntd\u22122(qd\u22122 \u2212 qd\u22121) sin n\u03b8d/2\u22121\ntd\u22121(qd\u22122 \u2212 qd\u22121) sin n\u03b8d/2\u22121\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(13)\nClearly, if we impose the constraint q2j\n=\nq2j+1, \u2200j \u2208 [0, d/2 \u2212 1], the vector in Equation\n(13) becomes null and we deduce that:\nf(qm, n) \u25e6 tn \u2212 qm \u25e6 f(tn, n) = 0\n(14)\nConsequently,\nwith the constraint q2j\n=\nq2j+1, \u2200j \u2208 [0, d/2 \u2212 1], we have:\nRe(\u27e8f(qm, m), qm \u25e6 f(tn, n)\u27e9)\n= Re(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9)\n(15)\nStep\n2.\nWe\nfurther\ndemonstrate\nthat,\nq2j\n=\nq2j+1, \u2200j\n\u2208\n[0, d/2 \u2212 1] is in fact\na\nredundant\nconstraint\nwhen\ncalculating\nRe(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9).\nTo verify\nthis, we expand the inner product:\nRe(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9)\n=\nd/2\u22121\nX\nj=0\n[(q2\n2jt2j + q2\n2j+1t2j+1) cos((m \u2212 n)\u03b8j)\n+ (q2jq2j+1t2j \u2212 q2j+1q2jt2j+1) sin((m \u2212 n)\u03b8j)]\n(16)\nRecall again t2j = t2j+1, \u2200j \u2208 [0, d/2 \u2212 1], we\nhave\nRe(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9)\n=\nd/2\u22121\nX\nj=0\nt2j[(q2\n2j + q2\n2j+1) cos((m \u2212 n)\u03b8j)]\n=\nd/2\u22121\nX\nj=0\nt2j|qj|2 cos((m \u2212 n)\u03b8j)\n(17)\nThis implies that Re(\u27e8f(qm, m), f(qm, n) \u25e6\ntn\u27e9) depends solely on the magnitude of qj =\n(q2j, q2j+1) in 2-D subspace, demonstrating the in-\ndependence of the relationship between q2j and\nq2j+1. Refer to Appendix D.3 for the rigorous\nproof.\nNow\nwe\nconclude\nthat,\nwith\nthe\ncon-\nstraint q2j\n=\nq2j+1, \u2200j\n\u2208\n[0, d/2 \u2212 1],\nRe(\u27e8f(qm, m), qm \u25e6 f(tn, n)\u27e9) is equivalent\nto\nRe(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9)\nwith\nno\nconstraint on query.\nBy removing q2j = q2j+1 constraint, we desig-\nnate this modified version as CoCA-Slack. The\nmathematical definition is provided in Appendix\nD.4.\n3\nExperimental Setting\nThis section provides an overview of the experimen-\ntal setup, including details regarding the training\ndata utilized and the baseline models employed to\nevaluate the effectiveness of the proposed method.\n3.1\nTraining Data\nOur model undergoes training on a combination of\ndatasets, including the Pile training dataset (Gao\net al., 2020), BookCorpus (Zhu et al., 2015), and\nthe Wikipedia Corpus (Foundation, 2021). Ad-\nditionally, we integrate manually collected open-\nsource code from GitHub repositories with at least\n1 star. From these datasets, we derive a sample of\napproximately 50B tokens, maintaining a composi-\ntion of 75% text and 25% code.\n3.2\nModel Variants\nTo evaluate the effectiveness of our proposed ap-\nproach, we train 3 models from scratch under iden-\ntical experimental settings, including ALibi (Press\net al., 2022), RoFomer (Su et al., 2024), and Ro-\nFormer+CoCA. All models share common specifi-\ncations, featuring a size of 350M, 24 layers, a hid-\nden dimension of 1024, 16 attention heads, and a\nmaximum sequence length of 512. The key distinc-\ntions among them lie in variations in self-attention\nmechanisms and position embeddings. The imple-\nmentation is optimized based on EleutherAI GPT-\nNeoX1. Training a model from scratch demands\nsubstantial computational resources. Therefore, we\nalso conduct experiments involving fine-tuning ex-\nisting LLMs with a drop-in CoCA module. For\nthis purpose, we utilize the LLaMA-7B model\n(Touvron et al., 2023a), which was trained with\na context length of 2,048. Additionally, we employ\ndynamic-NTK for all the above models.\nIn summary, our comparison models are cat-\negorized as follows:\nALibi, RoFormer, Ro-\nFormer+CoCA, RoFormer+dynamic NTK, and Ro-\nFormer+dynamic NTK & CoCA, all falling un-\nder the training from scratch category.\nMean-\nwhile, LLaMA-7B, LLaMA-7B+CoCA, LLaMA-\n7B+dynamic NTK, and LLaMA-7B+dynamic\nNTK & CoCA belong to the fine-tuning LLM with\ndrop-in CoCA category.\n3.3\nImplementation Detials\nPre-training Procedure We train all models us-\ning the next token prediction objective. We use\nAdamW (Loshchilov and Hutter, 2017) with \u03b21\n= 0.9 and \u03b22 = 0.95. The learning rate follows a\nlinear warm-up of 1% of total steps, starting from\n1e-7. Subsequently, the learning rate is adjusted\nto 1e-4 with linear decay, eventually reaching 1e-5.\nThe training utilizes 8 A100 GPUs, with a global\nbatch size of 256 and 2 gradient steps accumulation,\ntaking approximately 96 hours for 2 epochs.\nFine-tuning Procedure To integrate CoCA in\nLLaMA, we employ a three-stage fine-tuning strat-\negy: (1) only updating the K projection (7% of\nparameters). This stage aims to reconstruct the\nK projection in CoCA. By freezing the other pa-\nrameters, we maintain attention scores as closely as\npossible to those of vanilla self-attention. (2) updat-\ning the QKV projection (21% of parameters). This\nstage aims to address intrinsic over-fitting in vanilla\n1https://github.com/EleutherAI/gpt-neox/tree/v2.0\nself-attention caused by undesired behaviors be-\ntween RoPE and attention matrices. (3) fine-tuning\nall parameters. Each stage involves 15K steps, to-\ntaling 7.5B tokens (22B tokens overall), using the\nnext token prediction objective. The training length\nof LLaMA-7B + CoCA remains at 2,048 as in the\noriginal model. All experiments are conducted with\n32 A100 GPUs, setting a per-device batch size to 8\nwithout gradient accumulation.\n4\nExperiment Results\nWe conducted experiments to shed light on the\nfollowing reasonable doubts:\n\u2022 Can our new attention mechanism CoCA im-\nprove the long context extrapolation perfor-\nmance of existing models?\n\u2022 Can combining CoCA with other extending\nmethods for RoPE effectively solve the three\ntypes of rotational boundary problems dis-\ncussed in Appendix D.2?\n4.1\nLong Sequence Language Modeling\nWe evaluate the long sequence language model-\ning performance of both our model and baseline\nmodels on the test splits of the PG-19 dataset (Rae\net al., 2020). For this evaluation, we randomly se-\nlect a subsample comprising 100 documents, each\ncontaining at least 32,768 SentencePiece (Kudo\nand Richardson, 2018) tokens. We then truncate\neach test document to its initial 32,768 tokens. The\nevaluation involves calculating perplexity across\ndifferent context window sizes using a sliding win-\ndow approach, as described by (Press et al., 2022),\nwith a stride of 512. The perplexity results for both\nour models and baselines are presented in Table 1\nand Figure 1.\nBased on our experiments, the evaluation re-\nsults indicate that models combined with CoCA ex-\nhibit significantly improved perplexity with longer\ninference sequence length. For pre-trained mod-\nels, by increasing the context window size from\n512 (training context window size) to 32k, the\nperplexity of CoCA only increases from 20.11 to\n171.63, whereas the perplexity of RoFormer be-\ncomes inf. Additionally, by increasing the context\nwindow size from 2K to 32K, the perplexity of fine-\ntuned LLaMA-7B+CoCA only increases 21.68,\nwhile LLaMA-7B with other extending methods\nincreases more than 100. In general, we observe a\nconsistent trend of CoCA achieving better perplex-\nity with longer context windows. This suggests\nMethod\nEvaluation Context Window Size (Perplexity \u2193)\n512\n1024\n2048\n4096\n8192\n16k\n32k\nTraining model from scratch\nALibi\n18.69\n21.27\n28.20\n35.66\n37.03\nOOM\nOOM\nRoFomer\n19.66\n411.50\n3276.00\n3026.00\n3028.00\ninf\ninf\n+ dynamic NTK\n19.66\n22.30\n38.00\n75.75\n138.13\n370.75\n380.75\n+ CoCA\n20.11\n33.47\n69.06\n113.19\n157.38\n141.00\n171.63\n+ dynamic NTK & CoCA\n20.11\n20.81\n25.88\n34.16\n55.75\n89.31\n101.13\nFine-tuning LLM with drop-in CoCA\nLLaMA-7B\n9.25\n7.56\n7.30\n9673.14\ninf\ninf\ninf\n+ dynamic NTK\n9.25\n7.56\n7.30\n9.40\n14.40\n63.62\n133.87\n+ CoCA\n9.91\n8.49\n8.27\n24.23\n42.00\n23.83\n29.95\n+ dynamic NTK & CoCA\n9.91\n8.49\n8.27\n8.61\n9.56\n11.10\n13.98\nTable 1: Evaluation perplexity on 100 PG-19 documents using sliding window (S = 512) strategy. Dynamic-NTK is employed\nwithout fine-tuning. The best result is highlighted in bold.\nthat CoCA has a more robust position embedding,\nenabling it to handle long context more effectively.\nIn contrast, we observe that models extended\nthrough the direct application of dynamic NTK-\naware Scaled RoPE exhibit a larger increase in\nperplexity at longer sequences.\nThe perplexity\nof both RoFormer+dynamic NTK and LLaMA-\n7B+dynamic NTK remains significantly higher\nthan that combining CoCA. This difference be-\ncomes more pronounced as the sequence length\nincreases. When the inference sequence length\nreaches 32k, the perplexity of RoFormer+dynamic\nNTK increases to 380.75, while the result for\nRoFormer+CoCA is only 171.63. Similarly, the\nperplexity of LLaMA-7B+dynamic NTK reaches\n133.87, whereas LLaMA-7B+CoCA is only 29.95.\nIt is worth noting that the model achieves the best\nperformance when both dynamic NTK and CoCA\nare combined. Particularly, LLaMA-7B+dynamic\nNTK & CoCA consistently maintains a very low\nperplexity.\nEven when the inference sequence\nlength has reached 32k (16 \u00d7 longer than the train-\ning length), the perplexity is only 13.89. This indi-\ncates that combining CoCA with other extending\nmethods for RoPE can effectively address the three\ntypes of rotational boundary problems, achieving\nrobust long-text extrapolation modeling capabili-\nties.\n4.2\nLong Context Retrieval\nPerplexity evaluates the performance of language\nmodel in predicting the next token. However, it is\ninsufficient for a comprehensive assessment of the\neffective context window size. To address this, we\nconducted experiments using a passkey retrieval\ntask (Mohtashami and Jaggi, 2023) to evaluate our\nmethod and baselines. The task involves identi-\nfying and retrieving a randomly hidden passkey\nwithin a lengthy document. More details of task\ndefinition and test sample generation settings can\nbe found in Appendix B.1. Table 2 illustrates the\naccuracy of all tested models and their variants.\nIt is evident that ALibi exhibited failures when\ntested on sequences that were 1\u00d7 longer than its\ntraining length, attributed to its local hypothesis.\nIn contrast, our model consistently demonstrated\nsuperior accuracy. RoFormer+dynamic NTK &\nCoCA maintained a 50% accuracy, even with the\ntest sequence length expanded to 16\u00d7 its training\nlength. Similarly, LLaMA-7B+dynamic NTK &\nCoCA still maintained a 30% accuracy when the\ntest length was up to 32K.\n4.3\nImpact of Strict and Slack Constraint on\nQ\nAs mentioned in Section 2.4, we implement a\nslack version of CoCA, referred to as CoCA-Slack.\nIn this section, under the same experimental set-\ntings, we implement two versions of CoCA based\non RoFormer-350M, labeled as CoCA-Slack and\nCoCA-Strict.\nThe comparison results between\nthem are shown in Table 3.\nWe observe that the CoCA-Strict and CoCA-\nSlack models exhibit similar performance in long\nsequence language modeling, as evidenced by\ncomparable perplexity results. However, in the\npasskey retrieval task, contrary to our initial ex-\npectations, the CoCA-Strict model produces sig-\nnificantly lower results. This unexpected outcome\nsuggests that models with a slack constraint may\nMethod\nEvaluation Context Window Size (Accuracy\u2191)\n512\n1024\n2048\n4096\n8192\n16k\n32k\nTraning model from scratch\nALibi\n0.82\n0.65\n0.28\n0.18\n0.12\nOOM\nOOM\nRoFomer\n0.99\n0.53\n0.30\n0.18\n0.04\n0.02\n0.04\n+ dynamic NTK\n0.99\n1.00\n0.95\n0.70\n0.41\n0.16\n0.06\n+ CoCA\n1.00\n0.64\n0.33\n0.19\n0.06\n0.02\n0.04\n+ dynamic NTK & CoCA\n1.00\n1.00\n0.96\n0.89\n0.50\n0.23\n0.08\nFine-tuning LLM with drop-in CoCA\nLLaMA-7B\n1.00\n1.00\n1.00\n0.61\n0.21\n0.07\n0.09\n+ dynamic NTK\n1.00\n1.00\n1.00\n0.81\n0.26\n0.06\n0.03\n+ CoCA\n1.00\n1.00\n1.00\n0.71\n0.28\n0.11\n0.10\n+ dynamic NTK & CoCA\n1.00\n1.00\n1.00\n1.00\n0.85\n0.51\n0.30\nTable 2: Long context retrieval performance on passkey retrieval task. The best result is highlighted in bold.\nMethod\n512\n1024\n2048\n4096\n8192\n16384\n32768\nPerformance on Long Sequence Modeling (Perplexity)\nCoCA-Slack\n20.11\n19.02\n24.92\n40.53\n68.38\n92.75\n103.44\nntk-2\nCoCA-Strict\n+0.07\n+0.61\n-1.58\n-4.03\n+15.37\n+12.38\n+1.94\nCoCA-Slack\n20.11\n20.81\n25.88\n34.16\n55.75\n89.31\n101.13\nntk-4\nCoCA-Strict\n+0.07\n-0.49\n-0.66\n-0.88\n+3.16\n-18.25\n-2.57\nCoCA-Slack\n20.11\n23.66\n29.05\n37.47\n55.5\n88.88\n111.38\nntk-8\nCoCA-Strict\n+0.07\n-1.74\n-0.64\n+1.16\n+0.03\n+0.5\n+0.31\nPerformance on Long Context Retrieval (Passkey Accuracy)\nCoCA-Slack\n1.0\n0.99\n0.94\n0.77\n0.47\n0.27\n0.15\nntk-2\nCoCA-Strict\n+0.0\n-0.12\n-0.3\n-0.42\n-0.34\n-0.22\n-0.07\nCoCA-Slack\n1.0\n1.0\n0.96\n0.89\n0.5\n0.23\n0.08\nntk-4\nCoCA-Strict\n+0.0\n-0.11\n-0.38\n-0.46\n-0.38\n-0.19\n-0.02\nCoCA-Slack\n1.0\n0.98\n0.99\n0.85\n0.5\n0.11\n0.02\nntk-8\nCoCA-Strict\n+0.0\n-0.05\n-0.34\n-0.51\n-0.4\n-0.07\n-0.01\nTable 3: Comparison results for the Strict and Slack Constraints of Q in our proposed CoCA module. Superior performance to\nCoCA-Slack is indicated by the green color, while inferior performance is signified by the red color. The perplexity of the strict\nand slack models is comparable, whereas the strict model achieved lower accuracy in the passkey retrieval task.\noffer additional performance advantages, such as a\nlarger effective context window size.\nUnderstanding the reasons behind the superiority\nof slack constraints will be a key focus of our future\nwork. In this regard, we provide some theoretical\ninsights in Appendices D.3 and D.4. These insights\naim to shed light on the underlying mechanisms\nthat contribute to the observed differences and lay\nthe groundwork for a more comprehensive analysis\nin subsequent research.\n5\nConclusion\nIn this paper, we introduce Collinear Constrained\nAttention (CoCA), a novel approach that integrates\nposition embedding with the self-attention mecha-\nnism. This innovation addresses undesired behav-\niors occurring around the context window bound-\nary, which stem from discrepancies between RoPE\nand attention matrices. To the best of our knowl-\nedge, we are the first to analyze the initial angles be-\ntween queries and keys in the self-attention mecha-\nnism, which gives rise to anomalous phenomena in\nRoPE. Furthermore, we deduce a slack constraint\nfor our implementation of CoCA. Extensive ex-\nperiments demonstrate that incorporating CoCA\ninto existing models significantly enhances perfor-\nmance in both long sequence language modeling\nand long context retrieval tasks. Additionally, the\nsimultaneous integration of CoCA with other ex-\ntended RoPE methods (e.g., dynamic-NTK) effec-\ntively mitigates three types of rotation boundary\nissues, resulting in remarkably improved capabili-\nties for long context extrapolation.\nLimitations\nOur current approach, CoCA, has thus far under-\ngone exclusive validation on RoPE. Experimen-\ntal results demonstrate that CoCA enhances the\nlong-context extrapolation performance of LLMs\nand further augments other extension methods by\naddressing rotational boundary issues. However,\nquestions arise regarding its applicability to more\ngeneral methods. While the effectiveness of slack\nposition embedding (SPE) is evident, a deeper un-\nderstanding of the underlying reasons for its supe-\nrior performance necessitates further investigation.\nReferences\nDaniel G. a. Smith and Johnnie Gray. 2018. opt_einsum\n- a python package for optimizing contraction order\nfor einsum-like expressions. Journal of Open Source\nSoftware, 3(26):753.\nJinze Bai, Shuai Bai, Yunfei Chu, et al. 2023. Qwen\ntechnical report. arXiv preprint arXiv:2309.16609.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nSidney Black, Stella Biderman, Eric Hallahan, et al.\n2022. GPT-NeoX-20B: An open-source autoregres-\nsive language model. In Proceedings of BigScience\nEpisode #5 \u2013 Workshop on Challenges & Perspec-\ntives in Creating Large Language Models, pages 95\u2013\n136, virtual+Dublin. Association for Computational\nLinguistics.\nbloc97. 2023. Ntk-aware scaled rope allows llama mod-\nels to have extended (8k+) context size without any\nfine-tuning and minimal perplexity degradation.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\nArXiv, abs/2306.15595.\nTa-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and\nAlexander Rudnicky. 2022. KERPLE: kernelized\nrelative positional embedding for length extrapola-\ntion. In Advances in Neural Information Processing\nSystems 35: Annual Conference on Neural Informa-\ntion Processing Systems 2022, NeurIPS 2022, New\nOrleans, LA, USA, November 28 - December 9, 2022.\nOpenCompass Contributors. 2023.\nOpencompass:\nA universal evaluation platform for foundation\nmodels.\nhttps://github.com/open-compass/\nopencompass.\nTri Dao, Daniel Y. Fu, Stefano Ermon, et al. 2022.\nFlashAttention: Fast and memory-efficient exact at-\ntention with IO-awareness. In Advances in Neural\nInformation Processing Systems.\nJiayu Ding, Shuming Ma, Li Dong, et al. 2023. Longnet:\nScaling transformers to 1,000,000,000 tokens. arXiv\npreprint arXiv:2307.02486.\nEmozilla. 2023. Dynamically scaled rope further in-\ncreases performance of long context llama with zero\nfine-tuning.\nWikimedia Foundation. 2021. Wikimedia downloads.\nLeo Gao, Stella Rose Biderman, Sid Black, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. ArXiv, abs/2101.00027.\nChi Han, Qifan Wang, Wenhan Xiong, et al. 2023.\nLm-infinite: Simple on-the-fly length generaliza-\ntion for large language models.\narXiv preprint\narXiv:2308.16137.\nYunpeng Huang, Jingwei Xu, Zixu Jiang, et al. 2023.\nAdvancing transformer architecture in long-context\nlarge language models: A comprehensive survey.\narXiv preprint arXiv:2311.12351.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66\u201371. Asso-\nciation for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2017.\nFixing\nweight decay regularization in adam.\nArXiv,\nabs/1711.05101.\nAmirkeivan Mohtashami and Martin Jaggi. 2023. Land-\nmark attention:\nRandom-access infinite context\nlength for transformers. CoRR, abs/2305.16300.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context win-\ndow extension of large language models.\nCoRR,\nabs/2309.00071.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nSebastian\nRuder,\nMatthew\nE.\nPeters,\nSwabha\nSwayamdipta, and Thomas Wolf. 2019.\nTrans-\nfer learning in natural language processing.\nIn\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2,\n2019, Tutorial Abstracts, pages 15\u201318. Association\nfor Computational Linguistics.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng\nPan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-\nhanced transformer with rotary position embedding.\nNeurocomputing, 568:127063.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\nSong, and Furu Wei. 2023. A length-extrapolatable\ntransformer. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 14590\u201314604. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, et al.\n2023a. Llama: Open and efficient foundation lan-\nguage models. ArXiv, abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin R. Stone, et al.\n2023b. Llama 2: Open foundation and fine-tuned\nchat models. ArXiv, abs/2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998\u20136008.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, et al.\n2023. Efficient streaming language models with at-\ntention sinks. arXiv preprint arXiv:2309.17453.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19\u201327.\nIEEE Computer Society.\nA\nRelated Work\nExisting researches are mainly focused on the sub-\nmodule of attention kernel or position embedding\n(Huang et al., 2023). In the following sections, we\nwill separately introduce works on these two as-\npects: Section A.1 primarily addresses the former,\nwhile Section A.2 delves into the latter.\nA.1\nEfficient Attention Mechanisms\nSeveral works aim to implement efficient atten-\ntion mechanisms with reduced computational de-\nmands, even achieving linear complexity. This en-\nables extending the effective context length bound-\nary of LLMs during inference by directly increas-\ning Lmax in the pre-training stage (Ding et al.,\n2023; Mohtashami and Jaggi, 2023). Notewor-\nthy approaches include Longformer (Beltagy et al.,\n2020), utilizing slide window attention, and mod-\nels such as StreamingLLM (Xiao et al., 2023) and\nLM-Infinite (Han et al., 2023), which leverage a\nglobal-local attention mechanism. These variants\nhave achieved success to a certain extent, but still\nface issues we unveiled in this work when using\nRoPE as their positional encoding method.\nA.2\nExtrapolative Position Embedding\nMethods\nExtrapolative position embedding methods aim\nto enhance the length generalization capability of\nLLMs.\nA.2.1\nAttention Bias\nIn seeking alternatives to the explicit encoding of\npositional information, researchers have explored\nthe integration of attention bias to capture the se-\nquential and temporal nuances inherent in natural\nlanguage. Early approaches, such as T5 (Ruder\net al., 2019), incorporate learnable attention bias.\nHowever, these methods do not explicitly address\nthe challenge of length extrapolation. ALibi (Press\net al., 2022) introduces a negative causal atten-\ntion bias in a heuristic manner.\nExtending the\nALiBi-style attention bias, KERPLE (Chi et al.,\n2022) treats it as a composition triangle kernel\nfor self-attention and modifies style Xpos (Sun\net al., 2023) by integrating it with RoPE. While\nthese approaches effectively manage to maintain\nlow perplexity levels, they fall short in capturing\nlong-range dependencies due to introducing local\nhypotheses to context tokens.\nA.2.2\nExtend RoPE\nBesides, various strategies have been explored\nto extend RoPE (Su et al., 2024), a commonly\nemployed positional encoding method in popu-\nlar LLMs.\nRecent approaches involve simply\nscaling it to extrapolate the inference context\nlength with minimal or no fine-tuning. For in-\nstance, Position Interpolation (PI) (Chen et al.,\n2023) applies linear scaling on each position num-\nber from n to n/k, densifying the representation\nspace to extend the farthest length boundary by\nk times. Other approaches, such as NTK-aware\nScaled RoPE (bloc97, 2023) and Dynamic-NTK\n(Emozilla, 2023), combine high-frequency extrap-\nolation and low-frequency interpolation. These\ntraining-free methods require limited code changes\nduring inference (Peng et al., 2023). However,\nthese methods aim solely at alleviating the prob-\nlem of modeling the rotation angles in out-of-\ndistribution (OOD) positions without recognizing\nthe intrinsic correlation between attention matrices\nand rotation angles. Therefore, these methods still\nsuffer from a limited context window extending\nratio.\nPrevious methods independently investigate self-\nattention and position embedding without consid-\nering their intrinsic relationship, especially for the\nwidely used RoPE method.\nB\nAdditional Experiment\nB.1\nPasskey Retrieval Task Definition\nThere is an important info hidden inside a\nlot of irrelevant text. Find it and\nmemorize them. I will quiz you about the\nimportant information there.\nThe grass is green. The sky is blue. The\nsun is yellow. Here we go. There and back\nagain.\n...\n// Repeat x times.\n// Passkey is 5 randomly generated numbers.\nThe passkey is 12345. Remeber it. 12345 is\nthe passkey.\nThe grass is green. The sky is blue. The\nsun is yellow. Here we go. There and back\nagain.\n...\n// Repeat y times.\nWhat is the passkey?\nListing 1: Prompt format for passkey retrieval (Mohtashami\nand Jaggi, 2023). The passkey is randomly generated from\n10,000 to 99,999.\nThe passkey retrieval task, as proposed by Mo-\nhtashami and Jaggi (2023), involves the model re-\ncovering a randomly generated passkey hidden in\na long document (see Listing 1 for the task prompt\nformat). Given a language model, we can deter-\nmine the effective context window by assessing\nthe upper and lower bounds. We assume a random\npasskey is k tokens away from the end of the in-\nput. If a model consistently fails to recover the\npasskey in multiple attempts, it suggests a context\nwindow size smaller than k. Conversely, successful\nretrievals indicate an effective context window size\nof at least k tokens (Chen et al., 2023).\nIn our experiments, we generate test samples\nbased on the prompt template in Listing 1, with\nlengths ranging from 512 to 32k. There are 100\ntest cases for each length. Given a language model,\nwe input the passkey task prompt, examine the\nmodel\u2019s output for the new 64 tokens, and calculate\nthe accuracy.\nB.2\nAnalysis I : Consistency of Optimization\nin Position Embedding\nThe passkey retrieval results are presented in Sec-\ntion 4.2. Our model demonstrates superior passkey\nretrieval accuracy compared to baseline models\nunder various conditions. However, we remain in-\ntrigued about its optimization, specifically whether\nit occurs within or beyond the confines of the train-\ning context window. To probe this further, we cat-\negorize the experimental data into two segments:\npasskey distance shorter and farther than the train-\ning context window length.\nFigure 4 (a) illustrates the comparison results\nwhen the passkey is inserted less than 512 tokens\naway from the end token, while Figure 4 (b) illus-\ntrates that outside this range. When the passkey is\ninserted outside the 512 window, RoFormer+NTK\n& CoCA consistently outperforms Roformer+NTK\nacross various lengths of inference sequences. This\nsuperiority persists when the passkey is inserted\ninside the 512 window. Notably, with an increase\nin the length of the inference sequence, RoFormer\n+ NTK & CoCA demonstrates increasingly supe-\nrior performance compared to RoFormer + NTK.\nThese experiments suggest that our model can con-\nsistently optimize the position embedding and ex-\ntend the effective context window.\nB.3\nAnalysis II : Impact of Dynamic-NTK in\nCoCA\nWe utilize the dynamic NTK method (Emozilla,\n2023) during the inference process, applying it sep-\narately to both our model and the baseline model.\nTo comprehensively assess the robustness of these\nmodels, we conduct a thorough validation by vary-\ning scaling factors (2, 4, and 8).\nThe results in Figures 1 and 5 demonstrate that,\nwith the integration of the dynamic NTK method,\nour model achieves higher passkey accuracy and\nlower perplexity. Additionally, when the scaling\nfactor varies between 2, 4, and 8, the vanilla Ro-\nFormer model fails to maintain stable performance.\nIn contrast, CoCA consistently outperforms Ro-\nFormer at different scaling rates. This consistent\ntrend indicates that our model is more robust, show-\ning minimal performance fluctuations with changes\nin the scaling factor.\n(a) Inserting passkey inside 512 tokens away from end tokens (b) Inserting passkey outside 512 tokens away from end tokens\nFigure 4: Comparison of effective context window between RoFormer + NTK and RoFormer + NTK & CoCA.\nFigure 5: Passkey accuracy distribution on 4 range of distances.\nCoCA outperforms RoFormer for all distances and scaling\nfactors of NTK.\nFurthermore, it suggests that by implement-\ning collinear constraints, we can cleverly address\nanomalous behavior in RoPE, allowing RoPE to\nbetter leverage other extrapolation techniques.\nB.4\nAnalysis III : Compatibility of CoCA with\nPI\nB.4.1\nExperiment Setup\nWe conduct experiments utilizing the pre-trained\nLLaMA-7B model (Touvron et al., 2023a) and\nLLaMA-7B + CoCA from Section 3.2. To apply\nPI , we follow the settings of Chen et al. (2023):\nWe set the fine-tuning sequence length to 32,768.\nThe learning rate is adjusted to 2e \u2212 5 with no de-\ncay to match. All other settings are maintained as\nthe LLaMA-7B configuration. All experiments are\nconducted with 32 A100 GPUs, setting a per-device\nbatch size to 1 without gradient accumulation. The\nexperiments take 6,000 steps to accomplish.\nB.4.2\nLong Context Validation\nThe results of fine-tuning with PI are presented\nin Table 4. In terms of long sequence modeling,\nboth LLaMA-7B+PI and LLaMA-7B+CoCA & PI\ndemonstrate competitive performance across se-\nquence lengths ranging from 512 to 8192. How-\never, at longer sequence lengths (16384 and 32768),\nLLaMA-7B+CoCA & PI exhibits a slight perfor-\nmance advantage over LLaMA-7B+PI. For long\ncontext retrieval, both methods achieve exception-\nally high accuracy, with scores approaching the\nideal value of 1.0 across all sequence lengths.\nOverall, these findings suggest that the integra-\ntion of PI and the CoCA module with the LLaMA-\n7B model yields robust performance in both long\nsequence modeling and long context retrieval tasks.\nAdditionally, the CoCA module demonstrates the\nability to maintain performance levels compara-\nble to PI, particularly evident at longer sequence\nlengths.\nB.4.3\nShort Context Validation\nIn addition to enhancing long-context extrapola-\ntion, it is imperative to consider the practicality and\nscalability of CoCA in short contexts. Hence, we\nevaluate our model on OpenCompass (Contribu-\ntors, 2023), which comprises various dimensions,\nincluding reasoning, understanding, language, and\nexamination. The results are presented in Table 5.\nThe table demonstrates that LLaMA-7B models\nintegrated with CoCA achieve performance com-\nparable to the baseline LLaMA-7B across all eval-\nuated dimensions. Specifically, the integration of\nMethod\n512\n1024\n2048\n4096\n8192\n16384\n32768\nPerformance on Long Sequence Modeling (Perplexity)\nLLaMA-7B+PI\n9.06\n7.55\n7.74\n7.16\n7.04\n6.93\n7.11\n+ CoCA & PI\n9.65\n8.19\n8.37\n7.87\n7.84\n7.83\n7.96\nPerformance on Long Context Retrieval (Passkey Accuracy)\nLLaMA-7B+PI\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99\n+ CoCA & PI\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99\n0.99\nTable 4: Comparison results for LLaMA-7B+PI and LLaMA-7B+CoCA & PI after fine-tuning with sequence length of 32,768.\nCoCA succeeds in maintaining the performance of PI within fine-tuning window size.\nMethod\nReasoning\nUnderstanding\nLanguage\nExamination\nAverage\nLLaMA-7B\n48.25\n47.57\n46.41\n29.63\n42.97\n+ CoCA\n45.55\n51.14\n55.27\n25.14\n44.28\n+ PI\n44.98\n51.54\n54.79\n27.03\n44.59\n+ CoCA & PI\n46.88\n51.82\n55.56\n25.31\n44.89\nTable 5: OpenCompass results of LLaMA-7B and its vari-\nants. Models integrated with CoCA achieved comparable\nperformance to LLaMA-7B, leading no harm to the expres-\nsion ability of the model.\nCoCA yields no significant degradation in the ex-\npression ability of the model. This suggests that\nCoCA is effective not only in long-context scenar-\nios but also in short-context tasks, demonstrating its\nversatility and suitability for practical applications.\nC\nComputational and Spatial Complexity\nAnalysis\nModule\nvanilla self-attention\nCoCA\nComputational\nSpatial\nComputational\nSpatial\nWQK(T )V\n3Nd2h\nNd\n3Nd2h\nNd\nT half\n\u2014\n\u2014\nNdh\nNd\nT Relu\n\u2014\n\u2014\nNdh\nNd\nQK(T) rotation\n2Ndh\nNd\n2Ndh\nNd\nKrot = Q \u25e6 Trot\n\u2014\n\u2014\nN 2dh\nN 2d\nQrotKT\nrot\nN 2dh\nN 2\nN 2dh\nN 2\nMask\nN 2\nN 2\nN 2\nN 2\nSoftmax\nN 2\nN 2\nN 2\nN 2\nTable 6: The comparison of computational and spatial com-\nplexity between vanilla self-attention block and CoCA. Here,\nN represents the sequence length, h denotes the number of\nheads, and d signifies the dimension of each head.\nIn this section, we analyze the computational\nand spatial complexities of CoCA. Table 6 pro-\nvides a detailed comparison between the vanilla\nself-attention mechanism and CoCA.\nWhen using the operation Krot = Q \u25e6 Trot, the\ncomputational complexity of CoCA does not ex-\nceed twice that of the vanilla self-attention. In\npractice, the training and inference speed of CoCA\nare comparable to the vanilla self-attention mech-\nanism, with only a slight increase of about 5% to\n10% , as depicted in Figure 6. However, there is\nFigure 6: Inference speed comparison between CoCA and\nvanilla self-attention.\na significant increase in spatial complexity when\nexpanding Krot = Q \u25e6 Trot, becoming d times that\nof the vanilla self-attention. This level of spatial\ncomplexity is not practical for applications.\nTo address this problem, we can draw inspiration\nfrom the computation of QrotKT\nrot, which involves\ntwo steps: element-wise multiplication between\nQrot and Krot followed by summation along the\nhidden dimension. Optimization is attainable by\ncondensing the hidden dimension before fully ex-\npanding the sequence length dimension. Conse-\nquently, the spatial complexity is effectively re-\nduced from N2d to N2. This optimization strategy\nis equally applicable to Krot = Q \u25e6 Trot. These\ntwo components can be unified as articulated in\nEquation (18):\nQrotKT\nrot = Qrot(Q \u25e6 Trot)T\n(18)\nThe commendable work accomplished by\nopt_einsum (a. Smith and Gray, 2018) facilitates\nthe optimization of Equation (18). Experimen-\ntal results indicate that Roformer+CoCA only de-\nmands approximately 60GB of GPU memory dur-\ning inference with a sequence length of 32k, align-\ning closely with the memory consumption of the\nvanilla self-attention mechanism.\nD\nTheoretical Proof\nD.1\nStrong Form of Long-term Decay with\nCoCA\nWe have introduced the basic theory of Rotary Po-\nsition Embedding in Section 2.1. In fact, (Su et al.,\n2024) shows that RoPE has the characteristic of\nlong-term decay:\n|a(s)| =\n\f\f\f\f\fRe\n\uf8ee\n\uf8f0\nd/2\u22121\nX\nj=0\nhjeis\u03b8j\n\uf8f9\n\uf8fb\n\f\f\f\f\f\n\u2264 (max\ni\n|hi+1 \u2212 hi|)\nd/2\u22121\nX\nj=0\n|Sj+1|\n(19)\nwhere hj := (q2j + iq2j+1)(k2j \u2212 ik2j+1) and\nSj := Pj\u22121\nk=0 eis\u03b8k, s = (m \u2212 n), m for the index\nof query, n for the index of key. Since the value of\nPd/2\u22121\nj=0\n|Sj+1| decays with the relative distance s,\nthe attention score decays either.\nThis characteristic ensures the stability of RoPE\nduring extrapolation to some extent by preventing\noutliers. For CoCA, a stronger deduction can be\nformulated as follows:\n|a(s)| \u2264 (max\ni\n|li+1 \u2212 li|)\nd/2\u22121\nX\nj=0\n|Cj+1|\n(20)\nwhere lj := |q2j+iq2j+1||k2j+ik2j+1|, and Cj :=\nPj\u22121\nk=0 cos(s\u03b8k). Furthermore, it holds that:\n|li+1 \u2212 li| \u2264 |hi+1 \u2212 hi|\n(21)\nProof: Notice that when the initial angle \u0398j be-\ntween qj and kj is 0, from Equation (17), the at-\ntention score can be simplified as:\na(s) = Re\n\uf8ee\n\uf8f0\nd/2\u22121\nX\nj=0\nhjeis\u03b8j\n\uf8f9\n\uf8fb\n=\nd/2\u22121\nX\nj=0\nlj cos(s\u03b8j)\n(22)\nBy following the study of (Su et al., 2024), we\ncan easily derive the estimation in Equation (20).\nFor Equation (21), applying the triangle inequal-\nity, we get:\n|hi+1 \u2212 hi| \u2265 ||hi+1| \u2212 |hi||\n(23)\nReviewing the definition of hi\n=\n(q2j +\niq2j+1)(k2j \u2212 ik2j+1), we will find:\n|hi+1 \u2212 hi| \u2265 ||hi+1| \u2212 |hi||\n= ||qi+1k\u2217\ni+1| \u2212 |qik\u2217\ni ||\n= ||qi+1ki+1| \u2212 |qiki||\n= |li+1 \u2212 li|\n(24)\nFigure 7: Rotary Borders Analysis. Regarding qj as x-axis, 3\ndistinct boundaries correspond to kj, \u2212qj, and qj\nD.2\nRotary Borders Analysis\nIn Section 2.2, we analyzed the anomalous phe-\nnomena of RoPE. To illustrate the rotation anoma-\nlies, let\u2019s focus on a specific instance (case (d) of\nSection 2.2). As shown in Figure 7, three distinct\nboundaries emerge during the rotation. By adopt-\ning a relative coordinate system with qj serving\nas the x-axis, these boundaries correspond to kj,\n\u2212qj, and qj.\nEverytime the relative angle of qj and kj crosses\nthese boundaries, the monotonicity of their inner-\nproduct < qj, kj > undergoes a reversal. Thus,\nfor the vanilla self-attention, it learnt a piecewise\nmonotonic function of < qj, kj >:\n< qj, kj >=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u2191 (m \u2212 n), \u2200 \u2212 (2\u03c0 \u2212 \u0398j) \u2264 \u03b8(qj, kj) < 0\n\u2193 (m \u2212 n), \u22000 \u2264 \u03b8(qj, kj) < \u03c0\n\u2191 (m \u2212 n), \u2200\u03c0 \u2264 \u03b8(qj, kj) < 2\u03c0\n...\n\u2191 (m \u2212 n), \u2200(2k \u2212 1)\u03c0 \u2264 \u03b8(qj, kj) < (2k)\u03c0\n\u2193 (m \u2212 n), \u2200(2k)\u03c0 \u2264 \u03b8(qj, kj) < (2k + 1)\u03c0\n(25)\nwhere \u03b8(qj, kj) = \u0398j + (m \u2212 n)\u03b8j defined in\nSection 2.2.\nThis introduces confusion into the model dur-\ning direct context extrapolation. Therefore, meth-\nods like PI and NTK tried to introduce interpola-\ntion or extrapolation techniques to eliminate out-\nof-distribution (OOD) positions.\nExcept the first equation in Equation (25), the\ntwo boundaries caused by \u2212qj, and qj are regular\nwith periodicity of 2\u03c0, it is easy to handle when\napplying methods like PI or NTK. However, the\nboundaries caused by kj are hard to handle. There\nare d/2\u2217h\u2217L (d for head dimension, h for number\nof heads, L for number of layers) different bound-\naries during context extrapolation, which break the\nperiodicity of 2\u03c0.\nFurthermore, after applying interpolation or ex-\ntrapolation techniques, more positions will fall into\nthis abnormal area. It increased k times (k for in-\nterpolation factor) for PI and \u03bb2j/d times (\u03bb for\nscaling factor) for NTK.\nFrom this perspective, positional concentration\nof PI resulted in more trouble than NTK, i.e. ad-\nditionally more positions in abnormal area during\ncontext extrapolation. This may explain in some ex-\ntent why NTK could be used without fine-tuning for\nvanilla self-attention, but PI requires fine-tuning.\nBy enforcing \u0398j to 0, our proposed CoCA, con-\nstraining kj to be collinear with qj, effectively re-\nsolves the border-related challenge associated with\nkj.\nFrom experiments in Secton 4, with the inte-\ngrating of CoCA, now NTK can be leveraged well\nthrough direct use, while PI achieved improvement\nfor direct use but still limited, which requires fur-\nther studies.\nD.3\nHomeomorphism of Representation\nSpace\nTheorem 2. (Homeomorphism of representation\nspace) For any attention score defined as follows:\na(m, n) = Re(\u27e8f(qm, m), f(qm, n) \u25e6 tn\u27e9)\n(26)\nwhere qm is the query, m is the index number of\nquery, tn is the collinear coefficient of CoCA, n is\nthe index number of key, f is the rotation operator.\nDenote its representation space with respect to\nqm as:\nF(Q) = {a(m, n)|\u2200qm \u2208 Q \u2282 Rd}\n(27)\nwhere qm = WQxm, xm \u2208 EN, m \u2208 [1, N]\nand EN is the word embedding space, WQ is the\nprojection matrix.\nThen we have the following homeomorphism:\nF(Q) \u223c= F(Qhalf)\n(28)\nwhere Qhalf = Q|q2j=q2j+1,\u2200j\u2208[0,d/2\u22121].\nProof: We prove it by demonstrating the homeo-\nmorphism mapping G:\nG : F(Q) \u2192 F(Qhalf)\nF((q0, ..., qd\u22121) 7\u2192 F((\nr\nq2\n0 + q2\n1\n2\n, ...,\ns\nq2\nd\u22122 + q2\nd\u22121\n2\n)\n(29)\nIt consists of three parts:\nPart I (G is a bijection): recall Equation (17), we\nhave:\nG(X) = X, \u2200X \u2208 F(Q)\n(30)\nwhich implies that G is an identity mapping, natu-\nrally injective.\nNext, we prove that G is also surjective: for\nany Y = F((q0, ..., qd\u22121)|q2j=q2j+1) \u2208 F(Qhalf),\nthere exists eY \u2208 F(Q) such that G(eY ) = Y . Let\neY = F((q0, ..., qd\u22121)|q2j=q2j+1) \u2208 F(Q)\n(31)\nobviously we have G(eY ) = Y .\nPart II (G is continuous): For any X0 \u2208 F(Q),\n\u03f5 > 0, there exists \u03b4, such that if |X \u2212 X0| < \u03b4,\nthen |G(X) \u2212 G(X0)| < \u03f5.\nFrom Part I, G is an identity mapping, let \u03b4 = \u03f5,\nthen the continuity of G holds.\nPart III (G\u22121 is continuous): G is an identity map-\nping, so is G\u22121. Following Part II, we immediately\ndeduce that G\u22121 is continuous.\nD.4\nSlack Position Embedding\nLet H be a Hilbert space, and {T (n)|n \u2265 0} \u2282\nL(H) is a family of bounded linear operator on H.\nA is the inner-product defined on H.\nIf it satisfies the following property, then we\ncall {T (n)|n \u2265 0} is a relative (bounded linear)\noperator on H:\n\u2203 {S(m)|m \u2208 Z} : H \u00d7 H \u2192 C\n(X, Y ) 7\u2192 S(m)(X, Y )\nis a family of semi-bilinear operator on H\ns.t. S(p \u2212 q)(X, Y ) = A(T (p)(X), T (q)(Y ))\n\u2200 p, q \u2208 [0, N], X, Y \u2208 H,\n(32)\nAdditionally, if it satisfies the following property,\nthen we call {T (n)|n \u2265 0} is a slack relative\n(bounded linear) operator on H:\n\u2203 {S(m)|m \u2208 Z} : H \u00d7 H \u2192 C\n(X, Y ) 7\u2192 S(m)(X, Y )\nis a family of semi-bilinear operator on H\nand H\u2032 \u2282 H, H\u2032 \u0338= \u2205\ns.t. S(p \u2212 q)(X, Y ) = A(T (p)(X), T (q)(Y ))\n\u2200 p, q \u2208 [0, N], X, Y \u2208 H\u2032,\n(33)\nSpecifically, when H represents our projection\nspace in self-attention, and {T (n)|n \u2265 0} is a po-\nsition embedding on it, such as the Rotary Position\nEmbedding (RoPE), we refer to it as a Slack Po-\nsition Embedding (SPE) if it satisfies the property\ndescribed in Equation (33).\n"
  },
  {
    "title": "MindAgent: Emergent Gaming Interaction",
    "link": "https://arxiv.org/pdf/2309.09971.pdf",
    "upvote": "11",
    "text": "MINDAGENT: EMERGENT GAMING INTERACTION\nRan Gong1\u2020\u2217, Qiuyuan Huang2\u2021\u2217, Xiaojian Ma1\u2217, Hoi Vo3, Zane Durante4\u2020, Yusuke Noda3,\nZilong Zheng5, Song-Chun Zhu1567, Demetri Terzopoulos1, Li Fei-Fei4, Jianfeng Gao2\n1UCLA; 2Microsoft Research, Redmond; 3Xbox Team, Microsoft; 4Stanford;5BIGAI; 6PKU; 7THU\nFigure 1: The MINDAGENT system for gaming interactions. MINDAGENT enables complex task planning in a\nmulti-agent system and human-AI collaborated infrastructure across different domains.\nABSTRACT\nLarge Language Models (LLMs) have the capacity of performing complex\nscheduling in a multi-agent system and can coordinate these agents into com-\npleting sophisticated tasks that require extensive collaboration. However, despite\nthe introduction of numerous gaming frameworks, the community has insuffi-\ncient benchmarks towards building general multi-agents collaboration infrastruc-\nture that encompass both LLM and human-NPCs collaborations. In this work, we\npropose a novel infrastructure - MindAgent - to evaluate planning and coordina-\ntion emergent capabilities for gaming interaction. In particular, our infrastructure\nleverages existing gaming framework, to i) require understanding of the coordina-\ntor for a multi-agent system, ii) collaborate with human players via un-finetuned\nproper instructions, and iii) establish an in-context learning on few-shot prompt\nwith feedback. Furthermore, we introduce CUISINEWORLD, a new gaming sce-\nnario and related benchmark that dispatch a multi-agent collaboration efficiency\nand supervise multiple agents playing the game simultaneously. We conduct com-\nprehensive evaluations with new auto-metric collaboration score CoS for calcu-\nlating the collaboration efficiency. Finally, our infrastructure can be deployed into\nreal-world gaming scenarios in a customized VR version of CUISINEWORLD and\nadapted in existing broader \u201cMinecraft\u201d gaming domain. We hope our findings on\nLLMs and the new infrastructure for general-purpose scheduling and coordina-\ntion can help shed light on how such skills can be obtained by learning from large\nlanguage corpora. Project webpage: https://mindagent.github.io.\n\u2217 Equal Contribution. \u2021 Project Leader.\n\u2020 Work done while Ran and Zane interning at Microsoft Research, Redmond.\n1\narXiv:2309.09971v2  [cs.AI]  19 Sep 2023\n1\nINTRODUCTION\nLarge language Models (LLMs) have been piloting the effort of developing general intelligent ma-\nchines(Bubeck et al., 2023; Mirchandani et al., 2023) . Although they are trained in large text\ncorpora, their superior problem-solving capacity is not limited to canonical language processing\ndomains. LLMs already demonstrate the potential to tackle complex tasks that were previously\npresumed exclusive to domain-specific algorithms or human experts, ranging from mathematical\nreasoning (Imani et al., 2023; Wei et al., 2022; Zhu et al., 2022) to answering questions of pro-\nfessional law (Blair-Stanek et al., 2023; Choi et al., 2023; Nay, 2022) and medicine (Nov et al.,\n2023; Yang et al., 2023; Jeblick et al., 2022). More recently, some research has shown the possi-\nbility of using LLMs to generate complex plans for robots and game AI (Liang et al., 2022; Wang\net al., 2023b;a; Yao et al., 2023; Huang et al., 2023), marking an important milestone for LLMs as\ngeneralist intelligent agents.\nIn this work, we would like to further investigate the planning capacity of LLMs. Specifically, we\nare interested in planning in a multi-agent system (Stone & Veloso, 2000), i.e.multi-agent plan-\nning. Compared to planning for a single agent, which has been extensively studied by previous\nresearch (Wang et al., 2023b;a), multi-agent planning imposes much higher problem-solving com-\nplexity due to the exponentially growing action space (w.r.t. number of agents). The planner has\nto simultaneously control multiple agents, avoid possible conflicts, and coordinate them into com-\npleting a shared goal that requires sophisticated collaborations. To understand to which extent can\nLLMs obtain multi-agent planning skills, we first establish a new benchmark, CUISINEWORLD as\nillustrated in Figure 1.\nTo incorporate agent AI into video games, we main design an infrastructure - MINDAGENT - in-\nspired by multi-agent task allocation optimization theories to facilitate LLM multi-agent planning\ncapabilities. Our infrastructure enables LLMs to perform complex coordination and scheduling\nwith multiple different agents. We conduct comprehensive evaluations with recently introduced\nLLMs playing our game with our infrastructure, including GPT-4, Claude, and LLaMA. Through\nthe proposed MINDAGENT interactive multi-agent planning framework for LLMs, we make the fol-\nlowing key observations: 1) zero shot multi-agent planning: Without bells and whistles, powerful\npretrained LLMs like GPT-4 are capable of scheduling multiple agents (ranging from 2 to 4) into\ncompleting dishes, and even collaborate with human players, by merely reading simple game in-\nstructions and recipes; 2) planning with advanced prompting: We are able to significantly boost\ntheir multi-agent planning performances by leveraging the emergent in-context learning capabil-\nity (Brown et al., 2020; Wei et al., 2021): adding very few expert demonstrations even from dif-\nferent game levels to the prompt, explaining the rationale of certain actions as in Chain-of-Thought\nprompting (Wei et al., 2022), and providing on-the-fly feedback to the LLMs during planning; 3)\ngeneralist potentials: LLMs exhibits great potentials of being generalist multi-agent planner as it\nhas strong generalization to coordinate more agents with examples of fewer agents, and adaptation\nto new game domains like Minecraft.\nWhile compared to canonical domain-specific automated planning systems, multi-agent planning\nwith LLMs can still be bottlenecked by challenging computation cost, context length limitation,\nnon-optimal plans, etc., it has the potential of improving from data without fine-tuning (via in-\ncontext learning), seamlessly adapting to planning problems from different domains and offering\nmore flexible interfaces. We hope our findings on LLMs for general-purpose scheduling and coor-\ndination can help shed some light on how such skills can be obtained by learning from large text\ncorpora, and facilitate the emergence of better LLM planners.\nTo summarize, our key contributions are as follows:\n\u2022 We establish a new gaming scenario and related benchmark based on a multi-agent virtual kitchen\nenvironment, CUISINEWORLD. It adopts a minimal text-based game format and supports various\nplanning task structures and difficulties, making it an ideal test bed for the emergent multi-agent\nplanning (scheduling and coordination) capacity of LLMs.\n\u2022 We introduce MINDAGENT, an infrastructure for interactive multi-agent planning with LLMs,\nwhich demonstrates the in-context learning multi-agent planning capacity of LLMs and brings\nseveral prompting techniques that help facilitate their planning ability, including providing few-\nshot demonstrations, planning rationals, and environmental feedback.\n2\n\u2022 We conduct extensive evaluations with multiple LLMs and prompting settings on our benchmark.\nExperimental results confirm their potential on being generalist multi-agent planners in terms of\ngeneralizing to more agents.\n\u2022 We deploy our system into real-world gaming scenarios and demonstrate its capabilities in human-\nAI interactions.\n2\nRELATED WORK\nMulti-Agent Coordination. The field of multi-agent collaborations boasts a comprehensive body\nof literature. Traditionally, such collaborations have been modeled using MDP/POMDP (Lowe et al.,\n2017; Rashid et al., 2020; Jain et al., 2019) frameworks.\nHowever, there has been a recent shift towards utilizing Large Language Models (LLMs) for these\ncollaborations. For instance, Zhang et al. (2023b) delved into how large language models might\ncommunicate and cooperate in a watch-and-help (WAH) task. Meanwhile, Zhang et al. (2023a)\ninvestigated a two-agent collaboration game inspired by the simpler dynamics of the two-agent\nOvercooked-style game. Notably, their research chiefly concentrated on the task success rate, with\nmost studies typically anchored to a singular task objective. In contrast, we emphasize the impor-\ntance of collaboration efficiency in scenarios encompassing multiple task objectives. Further, our\nresearch uniquely focuses on evaluating the collaborative efficiency of more than two agents. Ad-\nditionally, while other works like Park et al. (2023) simulate each agent individually, we employ a\ncentralized system. This approach not only significantly reduces the number of API calls but also\nreduces context length, making it more appropriate for gaming applications.\nPlanning with LLMs. There exists a number of works that leverage LLMs to perform task planning\n(Huang et al., 2022a; Wang et al., 2023a; Yao et al., 2023). They leverage the LLMs\u2019 internet-scale\ndomain knowledge and emergent zero-shot planning abilities to perform complex task planning and\nreasoning. Recent works in robotics also leverage LLMs to perform task planning, they decompose\na natural language instruction into a sequence of subtasks, either in natural language form or in\npython code (Ahn et al., 2022; Huang et al., 2022b; Liang et al., 2022). Then they use a low-level\ncontroller to execute these subtasks. Additionally, (Huang et al., 2022b; Liang et al., 2022; Wang\net al., 2023b) also incorporate environment feedback to improve task performance.\nBenchmarks using Games.\nNumerous games have been developed to study task planning Baker\net al. (2022); Carroll et al. (2019), yet only a handful delve into multi-agent collaborations. Even\nwithin this limited subset, the focus predominantly remains on two-agent interactions where re-\nsponsibilities are not evenly distributed. As evidenced by (Wan et al., 2022; Puig et al., 2020), it\u2019s\ncommon for one player to assume a dominant role while the other provides support. In contrast, our\npaper assumes equal responsibilities across agents, and we expand our investigation to encompass\ncollaborations involving more than just two agents, even with human players. While some previous\nstudies have ventured into multi-task settings, none have delved into scenarios where agents must\ncomplete multiple distinct tasks using competing resources within a single episode. Furthermore,\nour game presents tasks with varied levels of difficulty.\nAdditionally, our work distinguishes itself from Carroll et al. (2019). Contrary to their settings, our\ngame settings feature a diverse array of tools and task objectives, thereby generating an exponentially\nlarger task space. A comparison between our work and other related games is shown in Table 1.\n3\nTHE NEW GAMING CUISINEWORLD DESIGN AND BENCHMARK\nWe introduce CUISINEWORLD as a novel and flexible game for multi-agent scheduling and coor-\ndination in a virtual kitchen environment. In this game, a multi-agent system needs to overlook\nmultiple agents and coordinate them, with the goal of completing as many dish orders as possible.\nIt is equipped with a textual interface since our focus is evaluating LLM-based planning agents.\nOur modularized design separates tasks and game engines, allowing more tasks (type of dishes) and\ndomains (how to implement the \u201ckitchen\u201d: text-based engine, Unity, Minecraft, etc.) to be included.\n3\nBenchmark\nMulti-task\nObject\nInteraction\nTool\nUse\nMaximum\nAgents\nCollabo-\nration\nHuman\nin-the-loop\nProcedural\nLevel Generation\nALFWorld (Shridhar et al., 2020)\n\u2713\n\u2713\n\u2713\n1\n\u2717\n\u2717\n\u2717\nWAH (Puig et al., 2020)\n\u2713\n\u2713\n\u2717\n2\n\u2713\n\u2713\n\u2717\nTextWorld (C\u02c6ot\u00b4e et al., 2019)\n\u2713\n\u2713\n\u2713\n1\n\u2717\n\u2717\n\u2713\nGenerative Agents (Park et al., 2023)\n\u2713\n\u2713\n\u2713\n25\n\u2717\n\u2717\n\u2713\nEMATP (Liu et al., 2022)\n\u2713\n\u2713\n\u2713\n2\n\u2713\n\u2717\n\u2717\nOvercooked-AI (Carroll et al., 2019)\n\u2717\n\u2713\n\u2713\n2\n\u2713\n\u2713\n\u2717\nHandMeThat (Wan et al., 2022)\n\u2713\n\u2713\n\u2713\n2\n\u2713\n\u2717\n\u2717\nDialFRED (Gao et al., 2022)\n\u2713\n\u2713\n\u2713\n2\n\u2713\u2217\n\u2717\n\u2717\nTEACH (Padmakumar et al., 2022)\n\u2713\n\u2713\n\u2713\n2\n\u2713\u2217\n\u2717\n\u2717\nCerealBar (Suhr et al., 2019)\n\u2717\n\u2717\n\u2717\n2\n\u2713\n\u2717\n\u2717\nLIGHT (Urbanek et al., 2019)\n\u2713\n\u2717\n\u2717\n1369\n\u2717\n\u2713\n\u2713\nDiplomacy (Bakhtin et al., 2022)\n\u2717\n\u2717\n\u2717\n7\n\u2713\n\u2713\n\u2717\nCUISINEWORLD (Ours)\n\u2713\n\u2713\n\u2713\n4+\n\u2713\n\u2713\n\u2713\nTable 1: Comparsion between CUISINEWORLD and other related benchmarks. Multi-task: The benchmark\ncontains multiple different tasks. Object Interaction: Agents have to manipulate or engage with different\nitems or environmental elements to achieve certain goals with irreversible actions. Tool Use: Completing tasks\nnecessitates the use of specific tools by the agents. Maximum Agents: This denotes the upper limit of agents\nthat can be present in a single experiment. Collaboration: Many tasks mandate teamwork and collaboration\nbetween different agents. Human in-the-loop: The framework allows humans to join the game and collaborate\nactively with the agents. Procedural Level Generation: There\u2019s flexibility in adding new tasks, making the\ngame dynamic and adaptable.\n\u2217: Notably, even though multiple agents can be present, the second agent is\nlimited to communicating with the first agent. The second agent cannot interact with the environment in an\nactive gaming capacity.\nType\nArguments\nDescription\ngoto\nagent\nlocation\nMove agent to\nlocation\nget\nagent\nlocation\n(item)\nagent obtain item\nfrom location\nput\nagent\nlocation\nagent put everything\nit holds to location\nactivate\nagent\nlocation\nagent turn on\nlocation\nnoop\nagent\nnot dispatching agent\nTable 2: Action space in CUISINEWORLD.\nNum. of\ntools\nNum. of\nings.\nNum. of\nsteps\nMax. mix\nsize\n8\n6\n8\n6\n14\n15\n11\n15\n8\n7\n10\n7\n3\n5\n4\n5\n1\n2\n3\n4\nFigure 2:\nDish distribution over the number of\ntools and ingredients (ings.)\ninvolved, cooking\nsteps, and maximum mixture size as in the recipe.\n3.1\nTASK DEFINITION\nWe follow prior works (Yao et al., 2023; Liu et al., 2023; Deng et al., 2023) to interactively evaluate\nLLMs as planning agents. Overall, the interactive evaluation can be formulated as a Markov\nDecision Process (S, A, T , R, G), with state space S, action space A, (effectively indicating all the\npossible schedules that can be made at a single time step), transition dynamics T , reward function R\nand task instruction space G. Note that, although there are multiple agents inside CUISINEWORLD\nthat can be coordinated, as we mentioned above, we adopt a centralized planning scheme and thereby\nformulate our game as a single-agent and fully-observable decision-making problem. An illustration\nof the state & action space and the possible tasks of our game can be found in Figure 1.\nState Space S. In CUISINEWORLD virtual kitchen, there are two types of entity: location and\nagent. For each entity, the game will provide a set of descriptions, the aggregated descriptions\nof all entities will be the state returned by our game. A location can be storage, where you\ncould obtain ingredients and dispense waste, a serving table, where you should put the completed\ndish on, or a cooking tool, e.g. pan, blender. We offer up to two descriptions for each location:\ninside(location, items), indicating what items (some ingredients, completed dishes, etc.)\nare now inside the location; and occupy(location), suggesting location is now being used\n4\nand cannot be touched, e.g. an activated blender. A agent is an entity that can be dispatched\nto complete the task, and we provide up to three descriptions for each agent: at(location,\nagent), indicating now agent is at location; hold(agent, items), suggesting what\nitems agent is holding; and finally occupy(agent), implying agent is now operating a tool,\ne.g. chopping some fruits, and will not respond to any dispatching command.\nAction Space A. An action in CUISINEWORLD is a list of dispatching commands. Given N\nagent entities, a total of N commands need to be generated. The agent provides the follow-\ning commands (also illustrated in Table 2): 1) goto(agent, location), to let agent move\nto location; 2) get(agent, location, item), to let agent get a specific item from\nlocation; 3) put(agent, location), to put whatever agent is holding into location;\n4) activate(agent, location), to let agent turn on location if it is a cooking tool,\ne.g. blender; 5) noop(agent), to have agent perform no actions in this round of dispatching.\nWe will provide more detailed illustrations and rules about the action space in appendix. Note\nthat, to avoid the possible confusion of multiple agents being dispatched to operate with the same\nlocation, the dispatcher also needs to properly order the dispatching commands as they will be\nexecuted sequentially.\nTasks and Reward.\nA task in CUISINEWORLD is a dish order, ranging from the most basic\ntunaSashimi, which can be made by simply chopping some tuna meat, to sophisticated dishes\nlike porkPasta that requires various cooking tools. In a game episode with maximum steps of T,\nevery \u03c4int steps (we name this task interval), a new task or dish order will be added to the active task\nlist. A task will be viewed as completed and removed from the active task list when a matched dish\nhas been put on the serving table. On the contrary, a task will be deemed to have failed and removed\nfrom the list when it reaches its lifetime \u03c4lft. Lifetime depends on the complexity of the dish and\ndetails can be found in appendix. Along with the tasks, the game provides rewards & penalties or\nfeedback on certain occasions, e.g. when a task is just completed, some infeasible commands are\ndispatched, etc. Due to the space limit, we defer details on tasks to Appendix B..\n3.2\nIMPLEMENTING CUISINEWORLD\nThe implementation of CUISINEWORLD mostly follows the spirit of Overcooked!, a renowned video\ngame. Therefore we refer to many of its game mechanisms while simplifying some of them, e.g. we\nskip low-level control and assume all agent have access to all location at any time (detailed\ncomparisons between CUISINEWORLD and the original video game can be found in appendix).\nSpecifically, we crawled the rules and recipes from the community-contributed wiki1, streamlined\nthem and made necessary modifications, ending up with the basic version of CUISINEWORLD com-\nprising 10 types of location (serving table, storage, and 8 different cooking tools), 27 types of\ningredients, and 33 unique dishes. We group the dishes based on their difficulty to make (primarily\nthe number of cooking tools involved) and design 12 game levels, which are further categorized\ninto 4 classes: entry, simple, intermediate and advanced, with 3 levels each. Note that the recipes,\ndishes, and levels can be easily extended to allow more challenging tasks.\n3.3\nEVALUATION METRIC\nCollaboration Score (CoS). We would like to evaluate to which extent the dispatcher (played by an\nLLM) can coordinate multiple agents into completing dish orders, across different scenarios. Similar\nto the original Overcooked! game, we are particularly interested in this question: Can the dispatcher\nstill coordinate the agents into efficient collaborations with smaller \u03c4int, i.e. more dish orders are\nflooding in? Our hypothesis is, an ideal dispatcher should be capable of coordinating agents until\nthere are way more tasks than the system can handle. Therefore, we introduce collaboration score\nCoS, defined as below:\nCoS = 1\nM\nM\nX\ni=1\n#completed task\n\u0002\n\u03c4int,(i)\n\u0003\n#completed task\n\u0002\n\u03c4int,(i)\n\u0003\n+ #failed task\n\u0002\n\u03c4int,(i)\n\u0003,\n(1)\nwhere M is the total amount of \u03c4int we evaluate. Effectively, CoS is the average task completion rate\nacross different \u03c4int conditions. In our default setting, we use M = 5. While the actual values of \u03c4int\n1https://steamcommunity.com/sharedfiles/filedetails/?id=1769729191\n5\nFigure 3: Our overview of our MINDAGENT architecture. Planning Skill & Tool Use: The game environment\nrequires diverse planning skills and tool use to complete tasks. It emits related game information. This module\nalso converts relevant game data into a structured text format so the LLMs can process it. LLM: The main\nworkhorse of our infrastructure makes decisions, which is a dispatcher for the multi-agent system. Memory\nHistory: A storage utility that stores relevant information. Action Module, extract actions from text inputs and\nconvert them into domain-specific language. Validate DSLs so they don\u2019t cause errors when executing.\ndepend on the game level, we ensure they elicit a wide range of difficulty including both extremely\nrelaxed and intense scenarios.\nIn a word, CuisineWorld is a game that emulates a virtual kitchen, where several robots are com-\nmanded to use various cooking tools and ingredients to prepare as many dish orders as possible in a\nlimited period of time. To facilitate collaboration, new orders will keep flooding in while the exist-\ning ones should be completed before expiration. Therefore, LLMs need to properly coordinate these\nrobots to maximize overall productivity. CUISINEWORLD also offers game levels with a wide range\nof planning difficulty: dishes with different complexity (number of ingredients and tools involved),\nnumber of agents, order frequency and lifetime, etc, making it an ideal test bed for LLM-based\nmulti-agent planning.\n4\nMINDAGENT: INFRASTRUCTURE FOR GAMING AI\n4.1\nINFRASTRUCTURE\nOur first foray into the challenging CUISINEWORLD benchmark is an interactive multi-agent plan-\nning framework for LLMs: MINDAGENT It adopts a minimalist design for the purpose of demon-\nstrating the emergent capacity of LLMs in scheduling and coordination, while also bringing in ex-\nploratory prompting techniques that facilitate better planning and shed some light on future ap-\nproaches. Our infrastructure follows in-context learning. We will outline the key techniques below:\nTo facilitate in-context learning, our MINDAGENT infrastructure is composed of three primary com-\nponents: the prompt, current state, and memory.\nWithin the prompt component, there are four distinct sub-components: recipes, general instructions,\ninference knowledge, and a one-shot demo.\nRecipes. outline the hierarchical procedure for preparing various dishes at the given level. They\nspecify the necessary ingredients for each intermediate or final product, the appropriate tools re-\nquired, and the expected outcome post-cooking.\n6\nInstructions. detail the foundational rules of CUISINEWORLD. These instructions delineate the\narray of actions agents can undertake within the game and enumerate the characteristics of every tool\navailable in the current kitchen scenario. Moreover, they inform agents about the base ingredients\nretrievable from storage, as well as all potential intermediate products they can procure. Agents are\nalso explicitly advised to remain cautious about feedback from the environment.\nInference Knowledge. houses insights and helpful hints for the agent. When utilized appropriately,\nthese hints can guide agents to sidestep potential errors and enhance their collaborative efficiency.\nOne-shot Demo. presents a step-by-step demonstration of the preparation of a distinct dish, differ-\nent from other dishes at the current level. This demonstration spans several time steps, each of which\nis incorporated as part of the prompt. The demonstration illustrates the major procedures for cook-\ning one dish in CUISINEWORLD, including obtaining ingredients, putting ingredients into different\ntools, transporting intermediate ingredients, and delivering the final dish to the serving table.\nCurrent State. provides a snapshot of the prevailing observations from the environment. It en-\ncompasses information such as the agents\u2019 locations, the objects currently in the agents\u2019 possession,\nthe tools that are accessible within the environment, the ingredients present within each tool, and\nthe tools that are actively in use. Moreover, it includes optional feedback from the environment,\ntriggered when the agents\u2019 actions contravene the environment rules\u2014 for instance, when assigning\ntwo distinct actions to the same agent.\nMemory History. archives the interaction history with the environment. Specifically, it chronicles\nthe state of the environment and the state of the agents at every time step.\nIn addition to the prompt modules, additional modules are implemented to help interface between\nLLMs and CUISINEWORLD.\nAction Extraction. employs a regular expression matching procedure to distill agent actions from\nthe LLM\u2019s textual output. This module is indispensable because, on occasion, the LLM\u2019s output is\nnot clean. The output contains information reflecting its internal thought processes. At times, the\nLLM might even issue apologies for prior missteps in reaction to environment feedback.\nAction Validation. utilizes a look-ahead checking mechanism. This module parses the proposed\nactions, assessing their feasibility. Should an action be deemed inexecutable, an error message is\npromptly returned.\n4.2\nINFRASTRUCTURE MECHANISM\nAssuming a multi-agent system with a total of N agents, the system must complete a sequence of P\ndifferent tasks. Each task has Mp different sub-tasks. Furthermore, the number and types of tasks\nare unknown at the beginning of the episode. The environment will sample a task for the agents to\nfinish for a given interval. Then the agents need to complete the designated task along with other\ntasks in the task queue. In addition, each task has an expiration time. After the expiration time, the\ntask will be marked as a failure. The objective of the multi-agent system is to finish as many tasks\nas possible and fail as fewer tasks as possible within a given time frame.\nWe aim to find valid and optimal task planning, scheduling, and allocations. We define qpim and\ncpim as quality and cost, respectively, for allocating agent i to work on the sub-task m for the p th\ntask in the episode. Then the combined utility for the sub-task is:\nupim =\n\u001aqpim \u2212 cpim,\nif agent i can execute sub-task m for the p th task in the episode\n\u2212\u221e.\notherwise\nWe define the assignment of sub-task m to agent i as\nvpim =\n\u001a1,\nagent i is assigned to sub-task m for the p th task in the episode\n0.\notherwise\nThe goal is to maximize the utility of the episode under a time constraint. Define the execution\ntime for task m by agent i for the p th task in the episode as \u03c4pim, and the maximum time allowed\nto execute the task as Tmax, we can express the task decomposition and assignment problem as\nfollows:\n7\narg max\nv\nP\nX\np=1\nN\nX\ni=1\nMp\nX\nm=1\nupimvpim\n(2)\nSubject to:\nP\np\nP\ni\nP\nm \u03c4pimvpim\n\u2264 Tmax\nP\ni vpim\n\u2264 1\n\u2200m \u2208 M, \u2200p \u2208 P\nvpim\n\u2208 {0, 1}\n\u2200i \u2208 N, \u2200m \u2208 M, \u2200p \u2208 P\nAs pointed out by (Korsah et al., 2013), this problem cannot be solved in polynomial time. In this\nwork, we tackle this problem by using large-language models.\nOur prompt design choices try to help LLM system solve Equation 2. In practice, we reformu-\nlate Equation 2 with qualities or rewards expressed in natural languages as environment feedback.\nFor example, when the agent successfully collects an item, the environment emits a signal \u201ccollect\nfinish.\u201d When the dispatcher assigns a different task to the same agent, the environment will emit a\nsignal \u201cagent ids cannot be the same.\u201d As rewards are not immediately observable, we borrow sprites\nfrom temporal difference learning. We accumulate state-action history into memory history. Due\nto context length limits, it\u2019s infeasible to fit the entire history into the context window. We select a\nfixed horizon history as a part of the prompt to guide the model performance. We further express\nthe constraints of the system in natural language formats and repeat important constraints multiple\ntimes if necessary.\n5\nEXPERIMENTS AND RESULTS\nOverview. We conduct extensive experiments in CUISINEWORLD. We first introduce the exper-\niment settings and present an analysis of empirical results in CUISINEWORLD. Our experiments\nfocus on addressing the following research questions:\nQ1: How efficiently can the model dispatch multiple agents?\nQ2: Can the model dispatch agents for dynamic, on-the-fly goals across different tasks?\nQ3: How do various components of the input prompt influence the model\u2019s performance?\nQ4: How do other LLMs perform compared to GPT-4?\nQ5: To what extent can the existing methods collaborate with human users?\nQ6: What\u2019s the human perception of collaborating with numerous intelligent agents?\n5.1\nLLM SETTINGS\nWe perform experiments on CUISINEWORLD through OpenAI APIs and anthropic APIs. All GPT-\n4 experiments are using gpt-4-0613 model, and all chat-GPT experiments are using gpt-3.5-turbo-\n0613. For Llama 2 experiments, we use hugging face inference endpoints Llama-2-70b-chat-hf. We\nset the temperature for all experiments to 0.1 following (Wang et al., 2023a). We report the average\nresults over three episodes.\n5.2\nEXPERIMENT SETTING I: LLMS DISPATCH MULTI-AGENTS (NPC)\nCollaboration Efficiency (Q1, Q2). Figure 4 and Table 3, Table 4 and Table 5 reports the system\nperformance under different settings. In particular, Table 3 reports the multi-agent collaboration\nresults among two agents. Table 4 reports the multi-agent collaboration results among three agents,\nand Table 5 reports the multi-agent collaboration results among four agents. Figure 4 displays the\ncollaboration efficiency curve.\nAs shown in Figure 4, across different task levels, more agents generally lead to better collaboration\nefficiencies. As the collaboration efficiency curve is generally higher with more agents.\nComputing CoS by levels also reveals that more agents will lead to better collaboration efficiencies.\nAs shown in the tables, the CoS score is the highest when there are two agents in two cases. The\n8\n3\n4\n5\n6\n7\n8\n9\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_0\n2-agent\n3-agent\n4-agent\n3\n4\n5\n6\n7\n8\n9\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_1\n6\n8\n10\n12\n14\ntask interval\n0.2\n0.4\n0.6\n0.8\nsuccess rate\nlevel_2\n6\n8\n10\n12\n14\n16\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_3\n6\n8\n10\n12\n14\ntask interval\n0.4\n0.6\n0.8\nsuccess rate\nlevel_4\n8\n10\n12\n14\n16\n18\n20\ntask interval\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_5\n6\n8\n10\n12\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_7\n6\n8\n10\n12\n14\ntask interval\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_8\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_9\n8\n10\n12\n14\n16\n18\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_10\n8\n10\n12\n14\n16\n18\ntask interval\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nlevel_11\n6\n8\n10\n12\n14\ntask interval\n0.2\n0.4\n0.6\n0.8\nsuccess rate\nlevel_12\nFigure 4: Collaboration Results on Different Tasks\nCoS score is the highest when there are three agents in seven cases. The CoS score is the highest\nwhen there are four agents in three cases. The results also confirm that more agents will lead to\nhigher collaboration efficiencies.\nFindings. First, we observe that the system performance is generally better when there are more\nagents, indicating that LLM dispatcher can coordinate more agents to execute tasks more efficiently.\nSecond, we observe that the system performance degrades with more agents in less demanding\nconditions, indicating that LLM dispatcher struggles when there are fewer tasks.\n5.3\nEXPERIMENT SETTING II: HUMAN AND MULTI-NPCS WITH LLMS\n5.3.1\nHUMAN DATA COLLECTION\nHuman Testing of Study Protocol. Before starting the experiment, a webpage introduction to\nthe game is handed to the players. It contains rules and the basic controls of the game. Then we\nrandomly assign the playing order. Participants can drop out of the testing at any time as they wise;\nin that case, their data will be discarded. The human evaluation interface is shown in Appendix D.\nMeasurement. In the background, we collect the number of failed and successful tasks during the\nparticipant\u2019s interaction with the game system. In addition, we record the entire action history of\nplayers and intelligent agents. Therefore, we can replay action histories for further analysis. After\neach episode, the participants must complete a survey about their engagement with the system on a\n5-point likert chart.\nOur objective measure is intended to evaluate the human AI teaming performance, and the subjective\nmeasure is designed to evaluate users\u2019 perceptions of the system.\n5.3.2\nEXPERIMENT II SETTING\nWe conducted a user study in our gaming environment that tries to answer Q5, Q6.\n9\n2-agent\nvery simple\nsimple\nintermediate\nadvanced\nAvg.\nlevel 0\nlevel 1\nlevel 7\nlevel 2\nlevel 4\nlevel 8\nlevel 3\nlevel 9\nlevel 10\nlevel 5\nlevel 11\nlevel 12\nGPT4 \u03c4int,(1)\n18/54\n18/56\n12/31\n14/34\n12/30\n3/30\n10/26\n7/20\n7/23\n6/23\n6/21\n10/36\n0.318\nGPT4 \u03c4int,(2)\n18/31\n17/34\n10/23\n13/26\n12/22\n9/22\n10/17\n8/11\n6/12\n5/13\n4/14\n8/21\n0.486\nGPT4 \u03c4int,(3)\n18/25\n19/25\n10/17\n16/18\n11/18\n6/16\n11/13\n6/8\n7/10\n8/10\n9/9\n8/17\n0.709\nGPT4 \u03c4int,(4)\n18/18\n18/19\n12/12\n11/14\n11/12\n7/11\n12/12\n8/8\n9/9\n6/7\n8/9\n11/12\n0.912\nGPT4 \u03c4int,(5)\n18/18\n17/17\n12/12\n11/13\n11/13\n9/9\n11/11\n4/5\n7/7\n8/8\n8/8\n9/12\n0.937\nCoS\n0.727\n0.706\n0.682\n0.687\n0.664\n0.504\n0.764\n0.725\n0.701\n0.661\n0.692\n0.559\n0.673\nTable 3: 2 agents performance on different tasks\n3-agent\nvery simple\nsimple\nintermediate\nadvanced\nAverage\nlevel 0\nlevel 1\nlevel 7\nlevel 2\nlevel 4\nlevel 8\nlevel 3\nlevel 9\nlevel 10\nlevel 5\nlevel 11\nlevel 12\nGPT4 \u03c4int,(1)\n21/55\n24/55\n16/33\n17/33\n9/28\n6/32\n12/25\n5/20\n8/21\n7/22\n7/22\n9/26\n0.368\nGPT4 \u03c4int,(2)\n20/31\n25/33\n11/22\n4/24\n13/24\n7/21\n14/20\n9/12\n9/13\n7/14\n8/14\n10/23\n0.549\nGPT4 \u03c4int,(3)\n22/25\n21/26\n17/17\n11/20\n9/17\n4/15\n13/14\n8/8\n12/12\n7/7\n9/10\n10/16\n0.791\nGPT4 \u03c4int,(4)\n22/22\n20/21\n14/14\n9/13\n7/10\n6/10\n10/10\n6/7\n10/10\n5/8\n7/8\n11/13\n0.846\nGPT4 \u03c4int,(5)\n20/20\n15/16\n11/12\n10/14\n10/11\n8/9\n12/12\n6/6\n8/8\n5/5\n8/8\n6/10\n0.914\nCoS\n0.781\n0.778\n0.780\n0.528\n0.600\n0.455\n0.822\n0.771\n0.815\n0.689\n0.733\n0.570\n0.694\nTable 4: 3 agents performance on different tasks\n4-agent\nvery simple\nsimple\nintermediate\nadvanced\nAverage\nlevel 0\nlevel 1\nlevel 7\nlevel 2\nlevel 4\nlevel 8\nlevel 3\nlevel 9\nlevel 10\nlevel 5\nlevel 11\nlevel 12\nGPT4 \u03c4int,(1)\n22/54\n18/55\n17/34\n13/34\n8/28\n9/33\n16/27\n5/20\n8/23\n5/22\n8/22\n8/35\n0.349\nGPT4 \u03c4int,(2)\n24/32\n21/33\n14/24\n14/25\n12/24\n11/22\n16/19\n7/12\n9/15\n7/14\n6/12\n12/23\n0.590\nGPT4 \u03c4int,(3)\n23/25\n23/26\n13/18\n11/19\n10/17\n11/17\n15/17\n8/9\n11/11\n7/8\n10/11\n9/17\n0.785\nGPT4 \u03c4int,(4)\n22/22\n21/22\n14/14\n7/15\n10/13\n10/12\n12/13\n9/9\n10/10\n6/7\n8/8\n9/13\n0.875\nGPT4 \u03c4int,(5)\n14/18\n20/20\n14/14\n7/13\n9/11\n7/8\n12/12\n5/5\n7/7\n6/6\n3/5\n7/10\n0.859\nCoS\n0.771\n0.761\n0.761\n0.505\n0.592\n0.626\n0.848\n0.744\n0.790\n0.692\n0.675\n0.534\n0.692\nTable 5: 4 agents performance on different tasks\nThe user study evaluates the LLM dispatcher\u2019s capabilities of collaborating with humans, where\nparticipants are collaborating with 1,2,3 agents or working alone on the virtual cooking tasks. We\nconsider the most general setting, where the LLM works on the unseen task, level 3.\n5.3.3\nEXPERIMENT II DESIGN\nHypotheses. The user study tests the following hypotheses:\n\u2022 H1: Task productivity. Participants have higher productivity if collaborating with AI agents.\n\u2022 H2: Task productivity with more agents. Participants have higher productivity if collaborating\nwith more AI agents.\n\u2022 H3: Perception of the robot. Participants would have higher perceived task efficiency and have\nmore fun playing the game due to collaboration.\nManipulated Variables. We use a within-subject design for our experiment. In particular, every\nuser tries to finish the task by himself or collaborates with different numbers of robots with varying\ndegrees of competency. We randomize the order of the treatment to mitigate practice effects, fatigue\neffects, and carryover effects.\n\u2022 Single agent: Participants work on the task by themselves.\n\u2022 LLM powered multi-agent system: Participants collaborate with the multi-agent system pow-\nered by LLM.\n\u2022 Random agent: Random agents execute random actions from a pool of valid actions. Participants\ncollaborate with random agents.\nMain Results. We recruited 12 subjects for our study. Among them, there are two females and 10\nmales.\nWe use ANOVA to test the effects of different experimental conditions on collaboration performance\nand subjective perception of the AI agents. Tukey HSD tests are conducted on all possible pairs of\nexperimental conditions.\n10\n(a) Collaboration score We can\ntell that the collaboration score is\nhigher if more agents are collab-\norating with human players, even\nthough the difference is not signif-\nicant.\n(b) Perceived Enjoyment Humans\nenjoy the game more if they col-\nlaborate with the right number of\nagents\n(c) Perceived more fun due to col-\nlaboration. Players enjoy the game\nmore because of collaborating with\ncompetent agents.\n(d) Perceived Assisting. There is\nno significant difference in terms\nof human perceptions of helpful-\nness when collaborating with more\nagents, even though the task suc-\ncess rate is higher.\n(e)\nPerceived\ndependability.\nWhen collaborating with more\nagents,\nplayers depend on the\nagents more.\n(f) Perceived Predictability. There\nis no difference in terms of the\npredictability\nof\nagents\u2019\nbehav-\niors when collaborating with more\nagents.\n(g) Perceived productivity. Play-\ners think collaborating with AI\nagents will improve productivity.\n(h) Perceived Trust. There is no\ndifference in terms of trust when\ncollaborating with more agents.\nFigure 5: Human Evaluations\nFindings. We find significant effects on team collaboration success rate F(4, 55) = 28.11, p <\n0.001. Post-hoc comparisons using the Tukey HSD tests revealed that the team of the player with\nLLM agents achieves a higher success rate than a human working alone, p < 0.001 across different\nnumbers of agents, confirming H1. Even though the success rate is generally higher when collab-\norating with more agents, there is no significant effect compared with collaborating with one agent,\ncollaborating with two agents p = 0.774, or collaborating with three agents p = 0.231. We observe\nthat human players have more fun playing the game when collaborating with LLM-powered intel-\nligent agents than playing alone, p = 0.0126. Players feel that collaboration with intelligent agents\nleads to higher productivity, p = 0.0104, thus confirming H3.\nIn addition, when playing with intelligent agents, human players will take their actions based on\nother players\u2019 actions p = 0.00266. Human players also found that intelligent agents are more\npredictable compared with random agents p < 0.001.\nFurther insights from player feedback highlighted an intriguing trade-off: while more agents im-\nproved overall task success rates, it reduced the game\u2019s enjoyment. Often, players felt sidelined and\nless involved. Thus, game developers should adjust AI performance to maintain player engagement\n11\nand fun. As indicated by Yuan et al. (2022), aligning human values with AIs might be a promising\nway to solve this problem.\n5.4\nVISUALING \u201dCUISINEWORLD\u201d\nTo implement CUISINEWORLD into a real game system, we built on top of Gao et al. (2020). In our\ngame, as visually depicted in Figure 6, players are given the opportunity to engage in collaborative\ninteractions with NPCs. In this game, human players\u2019 actions can be obtained from an inverse\ndynamic model by checking preconditions and post-effects. This introduces a unique dynamic to the\ngameplay, enabling users to experience a more immersive cooperative environment. Additionally,\nthe game\u2019s interface is versatile, allowing players multiple ways to interact within the game world.\nThey can either use a standard keyboard setup, which is more conventional and likely familiar to\nmost PC gamers, or they can immerse themselves even further using a Virtual Reality (VR) device.\nThis VR functionality ensures a more tactile and realistic interaction, as players can physically move,\ngesture, and engage with the NPCs and other in-game elements in a 3D environment.\nMulti-agent\nHuman-agent\nVR Interaction\nFigure 6: The top two images show a multi-agent collaboration example in CuisineWorld, the three agents are\npreparing a mixed juice together. The middle two images show a human player as the head chef instructing\nthe agents to cook mixed juice. The bottom two images show a human player collaborating with collaborative\nagents in VR.\n6\nANALYSIS AND EMERGENT GAMING ABILITIES\n6.1\nABLATION STUDY FOR MULTI-AGENTS\nStudy on the Prompt Components Q3. In Table 7, we elucidate the performance of LLM dis-\npatchers with certain components of the prompt omitted. Details about prompt can be found in\nAppendix Figure 9 and Figure 8. Specifically, for these tests, we excluded individual components\nlike inference knowledge, reduced the prompt example to a mere two steps instead of the complete\ndemonstration, and evaluated the model without environment feedback. For context, our principal\nexperiments, varying in the number of agents, incorporate a one-shot example for the correspond-\n12\ning number of agents. Our ablation studies further probe how varying the number of agents can\ninfluence model performance, with details in Table 8.\nFindings: From Table 7, a significant drop in performance is observed when environment feedback\nis excluded, underscoring its pivotal role in the efficacy of the LLM dispatcher. Replaying action\nsequences reveals that, without feedback, the LLM dispatcher tends to repeat mistakes and gets\nstuck in specific states for prolonged durations. Another key takeaway is that a succinct two-step\ndemonstration of input and output format can still achieve commendable performance for unseen\ntasks with dynamic objectives. Notably, in these two-step instances, there\u2019s no explicit guide to finish\nany tasks. Yet, the model doesn\u2019t merely complete the task but continually performs additional tasks\nwithin the same episode. Furthermore, we also observe that integrating human-crafted inference\nknowledge bolsters the LLM dispatcher\u2019s performance. Lastly, even with few-shot demonstrations\ninvolving fewer agents, the LLM dispatcher retains satisfactory performance as shown in Table 8.\nStudy on Other LLMs\u2019 Performance Q4. To study how other LLMs perform on our tasks, we\ntested the collaboration performance of GPT-3.5, Claude-2 and LLaMA in Table 6. For a fair com-\nparison, all tests employed identical prompt inputs.\nFindings: We observe that while other LLMs tend to underperform, models such as Claude-2 still\nmanage to complete the task to a considerable extent.\n6.2\nEMERGING CAPABILITIES\nAcross our experiments, we observe the following emergent properties under our MINDAGENT\nframework.\nEmergent Collaboration Tasks Understanding. As shown in Table 7, especially in the few-step\nablation entries, GPT-4 exhibits its proficiency even when not provided with a full demonstration for\nspecific tasks. To clarify, a \u201dfull few-shot demo\u201d typically refers to a comprehensive demonstration\nof a task, detailing each step and procedure involved. In contrast, we use provide GPT-4 with only a\npartial demonstration or a glimpse of the task only executing two steps.\nYet, despite this limited input, GPT-4\u2019s performance is remarkable. This underscores GPT-4\u2019s im-\npressive emergent zero-shot multi-agent planning capabilities. Beyond simply completing unseen\ntasks, GPT-4 also demonstrates adaptability by dynamically prioritizing multiple different tasks as\nthey arise, emphasizing its emergent multi-task, on-the-fly planning skills.\nEmergent Multi-agent Reasoning Capabilities. Referencing Table 8, GPT-4 has the capability to\ndeploy more agents based on demonstrations of fewer agents. For instance, GPT-4 can effectively\ndispatch four agents having only seen demonstrations involving two agents. Moreover, the efficiency\nof collaboration is higher as the number of agents increases, spotlighting its emergent collaboration\nprowess.\n2 agent\n3 agent\n4 agent\nGPT-4\nClaude-2\nLLaMA\nChatGPT\nGPT-4\nClaude-2\nLLaMA\nChatGPT\nGPT-4\nClaude-2\nLLaMA\nChatGPT\n\u03c4int,(1)\n10/26\n3/24\n0\n0/24\n12/25\n5/26\n0\n0/24\n16/27\n9/25\n0\n0/24\n\u03c4int,(2)\n10/17\n3/16\n0\n0/15\n14/20\n4/16\n0\n0/15\n16/19\n4/15\n0\n0/15\n\u03c4int,(3)\n11/18\n3/12\n0\n0/12\n13/14\n3/12\n0\n0/12\n15/17\n4/12\n0\n0/12\n\u03c4int,(4)\n11/13\n3/9\n0\n0/9\n10/10\n5/11\n0\n0/9\n12/13\n6/11\n0\n0/9\n\u03c4int,(5)\n11/11\n4/6\n0\n0/6\n12/12\n5/7\n0\n0/6\n12/12\n6/7\n0\n0/6\nCoS\n0.686\n0.3125\n0\n0\n0.822\n0.372\n0\n0\n0.848\n0.473\n0\n0\nTable 6: Performance of Other LLMs on Level 3\n2 agent\nGPT-4\nGPT-4 w/ few-step\nGPT-4 w/o inference knowledge\nGPT-4 w/o feedback\n\u03c4int,(1)\n10/26\n8/26\n8/25\n4/25\n\u03c4int,(2)\n10/17\n11/19\n9/17\n4/17\n\u03c4int,(3)\n11/13\n11/13\n10/12\n4/12\n\u03c4int,(4)\n12/12\n9/11\n8/9\n1/9\n\u03c4int,(5)\n11/11\n10/10\n9/9\n5/7\nCoS\n0.764\n0.710\n0.714\n0.311\nTable 7: Additional Ablation\n13\nlevel 3\n4agent using 4agent module\n4agent using 2agent module\n3agent using 3agent module\n3agent using 2agent module\nGPT4 \u03c4int,(1)\n16/27\n14/27\n12/25\n11/25\nGPT4 \u03c4int,(2)\n16/19\n16/20\n14/20\n11/19\nGPT4 \u03c4int,(3)\n15/17\n15/16\n13/14\n12/14\nGPT4 \u03c4int,(4)\n12/13\n13/13\n10/10\n12/12\nGPT4 \u03c4int,(5)\n12/12\n12/12\n12/12\n11/11\nCoS\n0.848\n0.851\n0.822\n0.775\nTable 8: Using different numbers of agent demos\n7\nNOVEL GAME ADAPTATION\nIn line with our ongoing efforts to create collaborative, in-game, multi-agent systems, we ventured\nbeyond CuisineWorld and made strides in integrating our infrastructure into the widely popular\nsandbox game, Minecraft. In this new adaptation, we designed several unique cooking tasks where\ntwo in-game agents, Alex and Steve, are assigned the responsibility of cooking various types of meat\nas shown in Figure 7. After cooking, agents need to deposit the items into a chest. More details can\nbe found in Appendix C. The experiment results are presented in Table 9.\nWe define the following actions for the multi-agent system in our Minecraft game:\n1)\ngoto(agent, location); 2) killMob(agent, mobType); 3) mineBlock(agent,\nblockType); 4) putFuelFurnace(agent, fuelType), to put the item from agent\u2019s in-\nventory to the furnace\u2019s bottom slot. 5) putItemFurnace(agent, itemType), to put the\nitem from agent\u2019s inventory to the furnace\u2019s top slot; 6) takeOutFurnace(agent), take out the\ncooked item from the furnace 7)\nputInChest(agent, itemType) ;\nThe state space in Minecraft contains the following: 1) nearby blocks for each agent 2) nearby\nentities for each agent. 3) each agent\u2019s inventory 4) items inside the furnace 5) items inside the\nchest. 6) human player\u2019s inventory if a human player is involved.\nTo ensure reproducibility, we modify the game mechanism. A killed mob will respawn nearby, and\na mined block will also respawn nearby.\nThe empirical data we collected from these game sessions provided us with compelling evidence that\nthe multi-agent collaboration infrastructure we\u2019ve developed has the robustness to be extrapolated\nand adapted across multiple distinct games, paving the way for broader applications in the gaming\nindustry.\nGoing a step further, we bridged the gap between human players and in-game (NPC) agents by inte-\ngrating Microsoft\u2019s Azure speech-to-text API into the Minecraft environment. This addition allows\nhuman players to communicate and collaborate with in-game NPC agents using voice chat. Human\nplayers can express their intents and desired goals to NPCs in real-time through voice chat. This\nreal-time vocal interaction enriches the gameplay experience, fostering a deeper level of immersion\nand synergy between human players and AI agents. Moreover, this integration opens the door for\nresearch into the efficacy of voice-assisted AI learning and how real-world human interactions can\nshape AI behavior in virtual domains.\nIn the case of the human player chatting with the multi-agent system, the prompt contains additional\nhuman instructions and human dialog history components.\nIn addition, by integrating Minecraft VR mode with our infrastructure, we can bring the player\ninteractive experiences to the next level.\nGPT-4 minecraft\n\u03c4int,(1)\n\u03c4int,(2)\n\u03c4int,(3)\n\u03c4int,(4)\n\u03c4int,(5)\nCoS\nPerformance\n0.195\n0.381\n0.704\n0.792\n0.833\n0.581\nTable 9: Performance of our framework in Minecraft\n14\nMulti-agent\nHuman-agent\nVR Interaction\nFigure 7: The top two images show a multi-agent collaboration example in Minecraft. In the left image, Alex\nand Steve are killing different animals, and in the right image, Alex and Steve are cooking meat in a furnace\ntogether. The middle two images show a human player instructing the agents to perform certain actions. The\nbottom two images show a human player collaborating with agents in VR.\n8\nCONCLUSION\nIn this paper, we presented MINDAGENT, an infrastructure for multi-agent collaboration through\nLLMs across multiple gaming domains. We investigated the multi-agent planning capabilities of\nMINDAGENT, and we deployed our infrastructure into real-world video games to demonstrate its\neffectiveness for multi-agent collaboration and human-AI collaboration. Beyond its practical appli-\ncations, we hope that our endeavor serves as a beacon, guiding the development of future gaming\nsystems where human-AI collaboration is seamless and intuitive. Furthermore, we are optimistic\nthat our insights and findings might catalyze innovations in crafting games that are not only techno-\nlogically advanced but also significantly more engaging and enjoyable for players.\nACKNOWLEDGMENTS\nWe are especially grateful to Johannes Gehrke, Ryen White, Haiyan Zhang, Kareem Choudhry for\ntheir enormous advice, support and encouragement of the work. We appreciate Katja Hofmann,\nAndrzej Banburski-Fahey, Jianwei Yang, Michel Galley, Nebojsa Jojic, Bill Dolan for the early in-\nsightful discussions, suggestions and comments. The authors gratefully acknowledge Adrian Brown\nfrom X-Box team for his discussion, feedback and pointers to the modeling generation and litera-\nture. We thank Rohan Taori, Janardhan Kulkarni, Ziheng Zhou, Yu Wang, Eloi Moliner Juanpere,\nXiaofeng Gao, Collin Huang, Xiaodong Yu, and Shuwen Qiu for their help on the human experiment\nsetup.\n15\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey,\nSally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy\nZeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint\narXiv:2204.01691, 2022. 3\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. Advances in Neural Information Processing Systems, 35:24639\u201324654,\n2022. 3\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew\nGoff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by com-\nbining language models with strategic reasoning. Science, 378(6624):1067\u20131074, 2022. 4\nAndrew Blair-Stanek, Nils Holzenberger, and Benjamin Van Durme. Can gpt-3 perform statutory\nreasoning? arXiv preprint arXiv:2302.06100, 2023. 2\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 2\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\nSparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 2\nMicah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca\nDragan. On the utility of learning about humans for human-ai coordination. Advances in neural\ninformation processing systems, 32, 2019. 3, 4\nJonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. Chatgpt goes to law\nschool. Available at SSRN, 2023. 2\nMarc-Alexandre C\u02c6ot\u00b4e, Akos K\u00b4ad\u00b4ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning\nenvironment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Con-\njunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm,\nSweden, July 13, 2018, Revised Selected Papers 7, pp. 41\u201375. Springer, 2019. 4\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070,\n2023. 4\nXiaofeng Gao, Ran Gong, Yizhou Zhao, Shu Wang, Tianmin Shu, and Song-Chun Zhu. Joint mind\nmodeling for explanation generation in complex human-robot collaborative tasks. In 2020 29th\nIEEE international conference on robot and human interactive communication (RO-MAN), pp.\n1119\u20131126. IEEE, 2020. 12\nXiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, and Gaurav S Sukhatme.\nDialfred: Dialogue-enabled agents for embodied instruction following. IEEE Robotics and Au-\ntomation Letters, 7(4):10049\u201310056, 2022. 4\nQiuyuan Huang, Jae Sung Park, Abhinav Gupta, Paul Bennett, Ran Gong, Subhojit Som, Baolin\nPeng, Owais Khan Mohammed, Chris Pal, Yejin Choi, et al. Ark: Augmented reality with knowl-\nedge interactive emergent ability. arXiv preprint arXiv:2305.00970, 2023. 2\n16\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of\nthe 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine\nLearning Research, pp. 9118\u20139147. PMLR, 17\u201323 Jul 2022a. URL https://proceedings.\nmlr.press/v162/huang22a.html. 3\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models. In arXiv preprint arXiv:2207.05608, 2022b. 3\nShima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large\nlanguage models. arXiv preprint arXiv:2303.05398, 2023. 2\nUnnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexan-\nder G Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task comple-\ntion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 6689\u20136699, 2019. 3\nKatharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa St\u00a8uber,\nJohanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, et al. Chatgpt makes\nmedicine easy to swallow: An exploratory case study on simplified radiology reports. arXiv\npreprint arXiv:2212.14882, 2022. 2\nG Ayorkor Korsah, Anthony Stentz, and M Bernardine Dias. A comprehensive taxonomy for multi-\nrobot task allocation. The International Journal of Robotics Research, 32(12):1495\u20131512, 2013.\n8\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In arXiv preprint\narXiv:2209.07753, 2022. 2, 3\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, et al.\nAgentbench: Evaluating llms as agents.\narXiv preprint\narXiv:2308.03688, 2023. 4\nXinzhu Liu, Xinghang Li, Di Guo, Sinan Tan, Huaping Liu, and Fuchun Sun. Embodied multi-agent\ntask planning from ambiguous instruction. Proceedings of robotics: science and systems, New\nYork City, NY, USA, pp. 1\u201314, 2022. 4\nRyan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-\nagent actor-critic for mixed cooperative-competitive environments. Advances in neural informa-\ntion processing systems, 30, 2017. 3\nSuvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Are-\nnas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern\nmachines. arXiv preprint arXiv:2307.04721, 2023. 2\nJohn J Nay. Law informs code: A legal informatics approach to aligning artificial intelligence with\nhumans. Nw. J. Tech. & Intell. Prop., 20:309, 2022. 2\nOded Nov, Nina Singh, and Devin M Mann. Putting chatgpt\u2019s medical advice to the (turing) test.\nmedRxiv, pp. 2023\u201301, 2023. 2\nAishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen,\nSpandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven\nembodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 2017\u20132025, 2022. 4\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023. 3, 4\n17\nXavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja\nFidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai\ncollaboration. arXiv preprint arXiv:2010.09890, 2020. 3, 4\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,\nand Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement\nlearning. The Journal of Machine Learning Research, 21(1):7234\u20137284, 2020. 3\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u02c6ot\u00b4e, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768, 2020. 4\nPeter Stone and Manuela Veloso. Multiagent systems: A survey from a machine learning perspec-\ntive. Autonomous Robots, 8:345\u2013383, 2000. 2\nAlane Suhr, Claudia Yan, Charlotte Schluger, Stanley Yu, Hadi Khader, Marwa Mouallem, Iris\nZhang, and Yoav Artzi.\nExecuting instructions in situated collaborative interactions.\narXiv\npreprint arXiv:1910.03655, 2019. 4\nJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim\nRockt\u00a8aschel, Douwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a\nfantasy text adventure game. arXiv preprint arXiv:1903.03094, 2019. 4\nYanming Wan, Jiayuan Mao, and Josh Tenenbaum. Handmethat: Human-robot communication\nin physical and social environments. Advances in Neural Information Processing Systems, 35:\n12014\u201312026, 2022. 3, 4\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a. 2, 3, 8\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023b. 2, 3\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021. 2\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022. 2\nKailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, and Sophia Ananiadou. On the evalu-\nations of chatgpt and emotion-enhanced prompting for mental health analysis. arXiv preprint\narXiv:2304.03347, 2023. 2\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations (ICLR), 2023. 2, 3, 4\nLuyao Yuan, Xiaofeng Gao, Zilong Zheng, Mark Edmonds, Ying Nian Wu, Federico Rossano,\nHongjing Lu, Yixin Zhu, and Song-Chun Zhu. In situ bidirectional human-robot value alignment.\nScience robotics, 7(68):eabm4183, 2022. 12\nCeyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei\nZhang, Anji Liu, Song-Chun Zhu, et al. Proagent: Building proactive cooperative ai with large\nlanguage models. arXiv preprint arXiv:2308.11339, 2023a. 3\nHongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tian-\nmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language\nmodels. arXiv preprint arXiv:2307.02485, 2023b. 3\n18\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.\nSolving math word problem via cooperative reasoning induced language models. arXiv preprint\narXiv:2210.16257, 2022. 2\n19\nAPPENDIX\nA\nPROMPT EXAMPLES\nWe provide some prompt examples for CuisineWorld. Figure 8 shows an example of the system\nprompt info. Figure 9 shows an example of a partial demonstration.\nFigure 8: The MINDAGENT system prompt example.\nFigure 9: The MINDAGENT system partial one-shot demo example.\n20\nB\nTASK DETAILS IN CUISINEWORLD\nHere we visualize different task graphs in CUISINEWORLD. In CUISINEWORLD, we provide tasks\nof different complexities to holistically evaluate the multi-agent system\u2019s performance. In addition,\nthe environment is highly customizable and extendable. Users only need to modify the JSON files\nto add more tasks or modify existing tasks.\nB.1\nLEVEL 0\nFigure 10: Salmon Meatcake\nB.2\nLEVEL 1\n(a) Salmon Meatcake\n(b) Lamb Meatcake\n(c) Lobster Meatcake\n21\nB.3\nLEVEL 2\n(a) Salmon Sashimi\n(b) Tuna Sashimi\n(c) MixedSashimi\nB.4\nLEVEL 3\n(a) Salmon Sushi\n(b) Tuna Sushi\n22\nB.5\nLEVEL 4\n(a) Tomato Salad\n(b) Lettuce Salad\n(c) Tomato Lettuce Salad\n(d)\nTomato\nCucumber\nSalad\nB.6\nLEVEL 5\n(a) Tomato Pasta\n(b) Beef Pasta\n(c) Pork Pasta\n23\nB.7\nLEVEL 6\n(a) pepperoniPizza\n(b) hawaiianPizza\n(c) chickenPizza\nB.8\nLEVEL 7\n(a) onionPotatoCarrotSoup\n(b) onionPotatoLeekSoup\n(c) onionBroccoliCheeseSoup\nB.9\nLEVEL 8\n(a) Beef Dumpling\n(b) Pork Dumpling\n(c) Salmon Dumpling\n24\nB.10\nLEVEL 9\n(a) Cheese Burger\n(b) MaxJr\n(c) Hopper\nB.11\nLEVEL 10\n(a) BurritodePastor\n(b) BurritodePollo\n(c) BurritodeAsada\n25\nB.12\nLEVEL 11\n(a) BurritodePastor\n(b) BurritodePollo\n(c) BurritodeAsada\n(d) SalmonSushi\n(e) TunaSushi\nB.13\nLEVEL 12\n(a) Potato Salad\n(b) French Fries\n(c) Smashed Potato\n26\nC\nMINECRAFT\nHere we visualize the task graphs for different tasks in Minecraft.\n(a) Cooking chicken in Minecraft\n(b) Cooking mutton in Minecraft\n(c) Cooking steak in Minecraft\n(d) Cooking porkchop in Minecraft\n27\nD\nHUMAN EVALUATION INTERFACE\nWe use the human evaluation interface to test the human\u2019s perception of collaborative agents. This\ngives us a more controlled environment so users\u2019 perception of collaborative agents does not depend\non their ability to control the keyboard and mouse, and their perception of collaborative agents does\nnot depend on the latency and rate limits of GPT-4.\n(a) Welcom Screen for human evaluation\n(b) Human Evaluation Example\n(c) Human Evaluation Example\n(d) Human Instructions\n28\n"
  },
  {
    "title": "Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?",
    "link": "https://arxiv.org/pdf/2309.08963.pdf",
    "upvote": "9",
    "text": "STRUC-BENCH: Are Large Language Models Really Good at Generating\nComplex Structured Data?\nXiangru Tang\u2660 Yiming Zong\u2661 Jason Phang\u2662 Yilun Zhao\u2660 Wangchunshu Zhou\u2663\nArman Cohan\u2660\u2217\nMark Gerstein\u2660\u2217\n\u2660 Yale University\n\u2661 Zhejiang University\n\u2662 New York University\n\u2663 ETH Zurich\n{xiangru.tang, arman.cohan, mark.gerstein}@yale.edu\nAbstract\nDespite the power of Large Language Models\n(LLMs) like GPT-4, they still struggle with tasks\nthat require generating complex, structured outputs.\nIn this study, we assess the capability of Current\nLLMs in generating complex structured data and\npropose a structure-aware fine-tuning approach as a\nsolution to improve this ability. To perform a com-\nprehensive evaluation, we propose STRUC-BENCH,\ninclude five representative LLMs (i.e., GPT-NeoX-\n20B, GPT-3.5, GPT-4, and Vicuna) and evaluate\nthem on our carefully constructed datasets span-\nning raw text, HTML, and LaTeX tables. Based\non our analysis of current model performance, we\nidentify specific common formatting errors and ar-\neas of potential improvement. To address complex\nformatting requirements, we utilize a FORMATCOT\n(Chain-of-Thought) to generate format instructions\nfrom target outputs. Our experiments show that\nour structure-aware fine-tuning method, when ap-\nplied to LLaMA-7B, significantly improves ad-\nherence to natural language constraints, outper-\nforming other evaluated LLMs. Based on these\nresults, we present an ability map of model capa-\nbilities from six dimensions (i.e., coverage, for-\nmatting, reasoning, comprehension, pragmatics,\nand hallucination). This map highlights the weak-\nnesses of LLMs in handling complex structured out-\nputs and suggests promising directions for future\nwork. Our code and models can be found at https:\n//github.com/gersteinlab/Struc-Bench.\n1\nIntroduction\nSignificant advancements have been made in var-\nious natural language processing tasks by Large\nLanguage Models (LLMs) (Brown et al., 2020;\nScao et al., 2022; Ouyang et al., 2022; Muennighoff\n\u2217 Contributed equally\nDataset\nCuration\nFormatCoT self-instruct with\nin-context examples\nTrain LLaMA-7B\nGuiding Questions\nfor Prompting\nInput:\n###Task: Generate a LaTex table from given text\n###Text\nInput:\n###Task: Generate a LaTex table from given text\nand format\u00a0description\n###Text\n###Format Instruction\n###Data\nDemo/examples:...\n###Describe the detailed format of a given latex table according\nto the commands and tags with more than 500 words\nWhether there are table borderlines?\nHow is text alignment?\u00a0\nWhat are table attributes?\u00a0\nWhether to bold?\u00a0\nWhether to add \\ref?\u00a0\nWhether there are horizontal and vertical lines bordering each row\nand column?\u00a0\nSay anything about special \\\" \\ \\\" format token in latex.\u00a0\nBenchmark and\nmetrics\nFigure 1: A system for describing complex structured\nformats and learning to follow this format in human\nlanguage. We use zero-shot for inference.\net al., 2022; OpenAI, 2023; Zhao et al., 2023a), es-\npecially in text generation tasks (Qin et al., 2023).\nThe ability to output structured data, one of the key\naspects of generative capability, has also attracted\ngreat interest in previous studies (Wu et al., 2022;\nZhao et al., 2023c,b).\nHowever, LLMs still underperform in generat-\ning complex structured outputs\u2013a critical ability\nfor various applications ranging from coding as-\nsistance to automated report writing. Furthermore,\nmost evaluation of LLMs has been on natural text\nor code generation, and relatively less research has\nbeen conducted to evaluate LLMs on their abil-\nity to generate structured output. This leaves it\nunclear whether LLMs can generate complex struc-\ntured data effectively. We aim to address these\nunanswered questions and deliver an in-depth ex-\namination in our research.\nFirst, there is a lack of systematic analysis of the\nability of LLMs to output complex structured data.\nPrevious efforts on evaluating LLMs (Qin et al.,\n2023; Ma et al., 2023) on structured data primar-\nily centered around simple Information Extraction\n(IE) tasks: recogniting named entities, extracting\nrelations, and detecting events. Here the goal of IE\ntasks is to gathered the extracted data in a highly\nstructured form (Zhong and Chen, 2020). Much\nearlier work was considerably more task-centric as\nopposed to LLM-centric. The focus was predomi-\narXiv:2309.08963v2  [cs.CL]  19 Sep 2023\nnantly on generating structured data from text (text-\nto-data) tasks with pre-trained models (He et al.,\n2023; Rossiello et al., 2022; Whitehouse et al.,\n2023; Pietruszka et al., 2022) like BART (Lewis\net al., 2019) and T5 (Raffel et al., 2020).\nSecond, there is a lack of fine-grained evaluation\nand comprehensive benchmarks of LLMs perfor-\nmance. Existing benchmarks often rely on rudi-\nmentary objective metrics such as word overlap\nto measure the accuracy of the content generated\nby the model (Li et al., 2023; Wu et al., 2022;\nPietruszka et al., 2022). This may be insufficient\nfor evaluating whether LLMs can generate struc-\ntured output, as an ideal evaluation metric ought to\nalso consider the format of generated content.\nThird, is there potential for enhancing the perfor-\nmance of current LLMs to better follow human nat-\nural language inputs, thereby generating outputs\nwith the accurate format and error-free content?\nThis work aims to fill in these gaps in the litera-\nture and expand on both the evaluation metrics and\ntraining datasets for LLMs generating structured\noutput. Our contributions are summarized as:\n(1) We develop a benchmark, called STRUC-\nBENCH focusing on generating structured texts in\nraw text, HTML, and LaTeX formats, and thor-\noughly examine the capabilities of popular LLMs,\nuncovering key issues in content accuracy, format-\nting, numerical reasoning, and handling long tables.\n(2) Incorporating prominent datasets and expand-\ning to diverse domains, we conduct empirical eval-\nuations of popular LLMs on our structured text\ngeneration benchmark, providing a deeper under-\nstanding of the prevalent error types and dimen-\nsions of shortcomings. Our findings suggest that\nboth GPT-3.5 and GPT-4 struggle to produce out-\nputs that are exactly correct, with issues primarily\nstemming from erroneous content, inaccurate for-\nmatting, inadequate numerical reasoning abilities,\nand their inability to handle long tables. (3) To ad-\ndress these issues, we introduce structure-aware in-\nstruction tuning, using ChatGPT to generate format\ninstructions and then training the LLaMA model\nto follow these formats. The promising results on\nboth seen and unseen data indicate that it could\ngreatly enhance the ability of LLMs to generate\nstructured outputs.\n2\nProblem Analysis and Benchmark\n2.1\nPreliminary\nThe task of generating complex structured data\npresents a notable challenge that tests the capabili-\nties of LLMs in producing intricate, format-specific\noutputs. This task moves beyond conventional text\ngeneration. The complexity lies not only in the\nneed to generate accurate and coherent content but\nalso in maintaining a strict and specific data struc-\nture or format. For example, text-to-table is a task\nthat aims to convert unstructured textual data into\nstructured tabular data, by extracting necessary con-\ntents from text and following the required structure\nor format.\n2.2\nProblem Analysis\nIn our study, we have identified a significant limi-\ntation of GPT-3.5 and GPT-4 in handling complex\nstructured output. Despite being state-of-the-art\nLLMs developed by OpenAI, these models both\nhave demonstrated certain limitations in generating\noutput in more intricate formats, examples could\nbe found in Appendix A.\nThis shortcoming becomes evident when the\nmodel is tasked with producing data that adhere\nto specific structural formats or templates, such\nas tables. We find that only 3% of the output of\nGPT-3.5 1 is completely correct, while GPT-4 is\nonly 9%. This could be attributed to the inherent\ndesign of the GPT family, which, while excelling\nat capturing the statistical patterns of human lan-\nguage, does not specifically account for structured\noutputs that require maintaining a state across a\nlonger span of tokens. Here, we select Rotowire as\nan investigation, as shown in Appendix B. We uti-\nlized the crowdsourcing approach on MTurk (See\nAppendix C) to examine the error types in 100 ex-\nample instances. Figure 2 presents the proportions\nof errors and each error type: ELEMENT ERRORS,\nELEMENT FORMAT ERRORS, STRUCTURE ER-\nROR, STRUCTURE NAMING ERRORS.\n9%\n3%\n91%\n97%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nGPT4\nGPT-3.5\nCorrect\nError\n82%\n88%\n81%\n89%\n73%\n75%\n79%\n81%\nElement Errors\nElement Format Errors\nStructure Naming Error\nStructure Errors\nFigure 2: Error analysis by human annotation. Some\nerror types are explained in Appendix A.\n2.3\nBenchmark\nIn our investigation, we incorporate four prominent\ndata-to-text datasets: Rotowire (Wiseman et al.,\n1In all our scenarios we are using Azure OpenAI Service\nmodels. GPT-3.5 means gpt-35-turbo. We noticed that the re-\nsults of the Azure deployed gpt-35-turbo-v0301 model diverge\nsubstantially from OpenAI gpt-3.5-turbo-0301.\n2017), E2E (Novikova et al., 2017), WikiTableText\n(Bao et al., 2018), and WikiBio (Lebret et al., 2016),\nwe specifically selected tables with dimensions\ngreater than 3x3 to ensure a sufficient level of com-\nplexity. Concurrently, we construct more diverse\ndatasets drawn from broader domains, encompass-\ning tables from LATEX and HTML data sourced from\nGitHub. Each of these table types comes with its\nunique nuances, complexities, and levels of struc-\nturation, providing extensive coverage for our ex-\nperiments. Table 1 gives statistics for the Rotowire\ndataset and our constructed datasets. Through em-\npirical testing, we evaluate the capacity of popu-\nlar LLMs, including GPT-NeoX-20B (Black et al.,\n2022), GPT-3.5 (Ouyang et al., 2022), GPT-4 (Ope-\nnAI, 2023) and Vicuna-13B (Chiang et al., 2023),\non our STRUC-BENCH, see Section 4.2. For LaTex\nand HTML without paired text, we use GPT-3.5\nto construct synthetic descriptions as input for our\nbenchmark.\nDataset\n# Train\n# Test\nFormat\nRows & Columns\nRotowire (Wiseman et al., 2017)\n3.4k\n728\nRaw tex\n7.26 & 8.75\nStruc-Bench LATEX\n5.3k\n500\nLATEX\n2.75 & 4.47\nStruc-Bench HTML\n5.4k\n499\nHTML\n5.50 & 3.54\nTable 1: Struc-Bench data statistics. The number of\nRows & Columns has been averaged.\nRaw text tables are more informal, unstandard-\nized, and often need manual interpretation. In con-\ntrast, LaTeX tables are used for scientific docu-\nments and demand high precision in their structure\nand syntax. HTML tables, widely used on the web,\ncarry their own tags and structure, aligning with\nthe rules of HTML language.\n3\nMethodology\n3.1\nData Generation\nAs shown in Figure 1, we propose FORMATCOT\nand self-instruct with GPT-3.5 to generate data, in-\nstruction pairs. Inspired by Gorilla (Patil et al.,\n2023), We provide three demos with in-context\nlearning and task the model with generating instruc-\ntions that describe the format of the given structure.\nWe specifically instruct the model to use natural\nlanguage. We have structured 6 demos for each of\nthe three data formats, all of which are hand-written\nor modified data.\n3.2\nFinetuning LLaMA-7B\nHere we propose a structure-aware instruction tun-\ning method to bolster the capability of LLMs in\ngenerating structured text. We employ the standard\ninstruction tuning method to fine-tune LLaMA-7B\n(Touvron et al., 2023). Our ultimate goal is to en-\nable LLaMA to comprehend the task at hand and\ndeliver the output in a conversational mode. This is\nakin to engaging in a dialogue with the user, culmi-\nnating in the successful completion of our defined\ntask. The entire pipeline can be found in Figure 1.\n3.3\nEvaluation Metrics\nEvaluating the similarity of generated tables to the\nground-truth tables is non-trivial: for instance, the\nsame table can be formatted in many different ways\nin HTML or LATEX. Hence, our evaluation metric\nshould ideally capture meaningful differences in\nthe data presented, while being invariant to insignif-\nicant differences in formatting.\nWe propose to break down the similarity of two\ntables into two coarse components: content and\nstructure. In scoring content similarity, we attempt\nto parse content out the data within the table cells,\nand compute the similarity. This similarity is com-\nputed between the generated and ground-truth table\ncells by commonly used similarity metrics. In scor-\ning structure similarity, we place higher emphasis\non components such as the number of columns and\nrows, cell alignment, and the table caption. Both\nsimilarity scores do overlap (e.g. a table with the\nwrong number of rows/columns would likely score\npoorly on content), but we find that these two scor-\ning categories allow us to perform more involved\nanalysis on where predicted and ground-truth tables\ndiffer.\n3.3.1\nGPTscore\nWe further take two approaches to score each met-\nric.\nFirst, we perform model-based evaluation,\nquerying GPT-3.5 with both tables and having it\nscore the similarity of content and structure sepa-\nrately. Following Wang et al. (2023), we prompt\nthe model to perform Chain-of-Thought Wei et al.\n(2023) reasoning before outputting its scores, and\nwe query the model with the predicted and ground-\ntruth tables in both orders and average the scores.\nWe report these as the GPTscore. The prompt of\nGPTscore can be found in Appendix D.\n3.3.2\nH-Score\nIn addition to model-based evaluation, we also im-\nplement hand-crafted scoring functions to score the\nsimilarity of the tables. Because of the many ways,\nthe tables can be presented in the different data for-\nmats, we implement several heuristics to normalize\nthe tables and to compute their similarity. The spe-\nModel\nSacreBLEU\nROUGE-L\nBERTScore\nBARTScore\nBLEURT\nContent GPTscore\nFormat GPTscore\nContent H-Score\nFormat H-Score\nTables from Raw Text\nGPT-NeoX-20B\n35.24\n55.78\n68.91\n-2.34\n33.51\n3.86\n6.10\n0.50\n-1.32\nGPT-3.5\n56.92\n70.97\n91.35\n-1.68\n36.85\n6.19\n8.16\n0.52\n-1.27\nGPT-4\n68.13\n75.44\n94.89\n-0.99\n55.24\n6.88\n8.30\n0.85\n0.53\nVicuna-13B\n40.12\n50.77\n75.21\n-2.05\n40.02\n4.07\n6.33\n0.55\n-1.38\nOurs-7B\n90.6\n88.98\n98.54\n-0.69\n66.07\n7.69\n8.60\n1.65\n3.61\nw.o.finetune\n9.9\n36.56\n81.63\n-2.50\n70.24\n4.58\n6.00\n0.51\n-1.01\nLaTeX\nGPT-NeoX-20B\n45.92\n65.10\n76.09\n-2.05\n40.87\n7.23\n7.02\n0.56\n0.72\nGPT-3.5\n56.94\n75.99\n86.25\n-1.30\n42.89\n8.22\n8.41\n0.99\n1.27\nGPT-4\n78.15\n85.34\n88.07\n-1.09\n67.11\n8.78\n8.81\n1.10\n1.35\nVicuna-13B\n50.80\n69.48\n80.44\n-1.07\n36.74\n7.70\n8.10\n0.78\n1.06\nOurs-7B\n89.13\n88.99\n98.55\n-0.69\n66.07\n8.94\n9.05\n1.14\n1.52\nw.o.finetune\n47.24\n70.89\n73.27\n-2.13\n38.13\n7.10\n6.98\n0.51\n0.69\nHTML\nGPT-NeoX-20B\n60.36\n72.13\n86.88\n-1.59\n30.06\n8.42\n8.94\n0.81\n0.92\nGPT-3.5\n73.80\n85.19\n96.76\n-1.46\n34.81\n9.11\n9.35\n1.10\n2.15\nGPT-4\n79.25\n85.95\n97.22\n-1.31\n41.59\n9.17\n9.62\n1.15\n2.29\nVicuna-13B\n58.75\n70.37\n88.65\n-1.58\n31.11\n8.55\n8.88\n0.79\n0.93\nOurs-7B\n77.50\n86.08\n96.25\n-1.30\n42.89\n9.20\n9.70\n1.18\n2.49\nw.o.finetune\n65.30\n78.24\n88.12\n-1.57\n32.78\n8.22\n8.81\n0.92\n0.96\nTable 2: Automated evaluation results on the test set, involving five types of previous metrics and four proposed\nones. w.o.finetune means that we also compared the performance of our model without structure-aware finetuning\nas an ablation study.\ncific implementation of scoring functions for dif-\nferent formats can be found in Appendix D. Where\nsimilarities between strings or data structures are\ncomputed, we use an average of Levenshtein dis-\ntance and the Ratcliff/Obershelp similarity metric.\nWe report these heuristically normalized metrics as\nthe H-Score.\n4\nExperiments\n4.1\nBasic Settings\nFor metrics, we use SacreBLEU, ROUGE-L,\nBERTScore, BARTScore and BLEURT metrics as\nthey are all classical metrics to evaluate text similar-\nity, which is also useful in this task. Besides, we use\nour two proposed metrics: GPT score and H-score.\nWe evaluate the following models: GPT-NeoX-\n20B, GPT-3.5, GPT-4, Vicuna-13B, our structure-\naware finetuning LLaMa-7B and original LLaMa-\n7B. GPT-NeoX-20B, GPT-3.5 and GPT-4 represent\nthe state-of-art performance of current LLMs and\nVicuna-13B is another version finetuned by LLaMa,\nwhich can reach 90% of the capacity of ChatGPT.\nWe think these models are strong enough to be per-\nsuasive. For the first 4 models, we simply call their\nAPIs from OpenAI or HuggingFace to generate\nresults without further finetuning. In our dataset,\neach item consists of three parts: instruction, input,\nand output. When generating results, we put each\nitem\u2019s instruction and input together as the final\ninput to models.\nDuring the inference process, we will provide the\nmodel with a natural language prompt to describe\nthe form and content of our task, as well as the\nexpected response (e.g., \u201cplease generate a table\ngiven by the following information and format\u201d).\n4.2\nResults\nTable 2 provides a comparative analysis of dif-\nferent language models based on several perfor-\nmance metrics. For \u2018Tables from Raw Text\u2019, the\nOurs-7B outperforms the other models in every\nmetric. Interestingly, without fine-tuning, the per-\nformance drops significantly, particularly in Sacre-\nBLEU, ROUGE-L, and BERTScore. The results\nfor \u2018LaTeX\u2019 reveal a similar trend where we again\nachieve the best results across all metrics, except\nfor the BLEURT metric, where GPT-4 takes the\nlead. In the \u2018HTML\u2019 category, GPT-4 scores the\nhighest in SacreBLEU and BERTScore. However,\nours comes out on top for the rest of the metrics.\nConsidering the inconsistency observed by dif-\nferent metrics, we also conducted a human evalu-\nation. We also carried out a human evaluation on\n100 examples using MTurk. Evaluators rated each\nexample on a scale from 0 to 10, assessing both for-\nmat consistency and content consistency. Although\nwe cannot enumerate the details due to space con-\nstraints, we discovered that the Content GPTscore\nand Content H-Score are more closely aligned with\nexisting metrics. However, our proposed Format\nGPTscore and Format H-Score significantly sur-\npass other metrics, particularly in terms of instance-\nlevel Spearman correlation for format accuracy.\nThese human evaluations underscore the efficacy\nof our proposed metrics. However, larger-scale hu-\nman evaluations are needed to further explore and\nsubstantiate these findings.\nMoreover, we delve into an in-depth analysis,\nattributing observed shortcomings to several error\ntypes, spanning two key dimensions: Content Se-\nlection and Format Planning, as well as the Reason-\ning Process, see details in Appendix G. Based on\nthese, we present an ability map of model capabili-\nties from six dimensions.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nCoverage\nFormatting\nReasoning\nComprehension\nPragmatics\nHallucination\nControl\nVicuna\nChatGPT\nGPT-4\nOurs\nFigure 3: Visualization of LLM capability with human\nevaluation over STRUC-BENCH.\n5\nConclusion\nIn conclusion, this research offers a comprehensive\nexploration of the structured text generation limita-\ntions inherent in Large Language Models (LLMs)\nlike ChatGPT and GPT-4. Through developing a\nbenchmark specifically designed for structured text\ngeneration and integrating a wide range of datasets,\nwe have been able to thoroughly assess the capabil-\nities of prevalent LLMs. Our analysis has identified\nseveral areas of concern, particularly in regard to\ncontent accuracy, formatting, numerical reasoning,\nand the handling of long tables.\n6\nLimitations\nAlthough we present an in-depth and comprehen-\nsive analysis, the exploration of LLMs in structured\ntext generation presented in this paper has several\nlimitations:\nDomain-Specific\nBenchmark\nDevelopment\nWhile we\u2019ve made strides in constructing bench-\nmarks for structured text generation, it may be\nbeneficial to develop benchmarks that cater to\nspecific domains.\nDifferent fields might have\nunique structural requirements and understanding\nthese nuances can significantly improve the\nmodels\u2019 applicability across diverse contexts.\nExpand the Range of Datasets\nThere are end-\nless data types and sources that can be explored.\nIncorporating a broader variety of datasets could\nexpose the models to an even wider range of struc-\ntural formats, ultimately enhancing their overall\nperformance.\nEnhancing Numerical Reasoning Capabilities\nOur study identified inadequate numerical reason-\ning as one of the challenges faced by LLMs. Inves-\ntigating techniques to bolster numerical reasoning\nin these models could lead to significant improve-\nments in their performance.\nDeveloping Advanced Methods\nWhile our\nstructure-aware instruction tuning method showed\npromising results, more sophisticated techniques\ncould be developed. For instance, future work\ncould explore ways of incorporating more explicit\nstructural information into the model or develop-\ning methods that allow the model to learn structural\npatterns more effectively.\nExploring Multimodal LLMs\nAs LLMs con-\ntinue to evolve, there are opportunities to explore\nmultimodal models that can process and generate\nboth text and other forms of data, such as sound\nor images (Kamigaito et al., 2023), in a structured\nmanner.\nReferences\nJunwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua\nLv, Ming Zhou, and Tiejun Zhao. 2018. Table-to-\ntext: Describing table region with natural language.\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume 32.\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nYuxin He, Jingyue Hu, and Buzhou Tang. 2023. Revisit-\ning event argument extraction: Can eae models learn\nbetter when being aware of event co-occurrences?\narXiv preprint arXiv:2306.00502.\nHidetaka Kamigaito, Katsuhiko Hayashi, and Taro\nWatanabe. 2023.\nTable and image generation\nfor investigating knowledge of entities in pre-\ntrained vision and language models. arXiv preprint\narXiv:2306.02115.\nR\u00e9mi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. arXiv preprint\narXiv:1603.07771.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nTong Li, Zhihao Wang, Liangying Shao, Xuling Zheng,\nXiaoli Wang, and Jinsong Su. 2023. A sequence-\nto-sequence&set model for text-to-table generation.\narXiv preprint arXiv:2306.00137.\nYubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.\n2023. Large language model is not a good few-shot\ninformation extractor, but a good reranker for hard\nsamples! arXiv preprint arXiv:2303.08559.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nJekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser.\n2017. The e2e dataset: New challenges for end-to-\nend generation. arXiv preprint arXiv:1706.09254.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nMicha\u0142 Pietruszka, Micha\u0142 Turski, \u0141ukasz Borchmann,\nTomasz Dwojak, Gabriela Pa\u0142ka, Karolina Szyndler,\nDawid Jurkiewicz, and \u0141ukasz Garncarek. 2022. Sta-\nble: Table generation framework for encoder-decoder\nmodels. arXiv preprint arXiv:2206.04045.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nGaetano Rossiello, Faisal Chowdhury, Nandana Mi-\nhindukulasooriya, Owen Cornec, and Alfio Gliozzo.\n2022. Knowgl: Knowledge generation and linking\nfrom text. arXiv preprint arXiv:2210.13952.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nChenxi Whitehouse, Clara Vania, Alham Fikri Aji,\nChristos Christodoulopoulos, and Andrea Pierleoni.\n2023. Webie: Faithful and robust information extrac-\ntion on the web. arXiv preprint arXiv:2305.14293.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in data-to-document genera-\ntion. arXiv preprint arXiv:1707.08052.\nXueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Text-\nto-table: A new way of information extraction. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2518\u20132533, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023a. A\nsurvey of large language models.\narXiv preprint\narXiv:2303.18223.\nYilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan,\nXiangru Tang, and Arman Cohan. 2023b. Large lan-\nguage models are effective table-to-text generators,\nevaluators, and feedback providers. arXiv preprint\narXiv:2305.14987.\nYilun Zhao, Chen Zhao, Linyong Nan, Zhenting\nQi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and\nDragomir Radev. 2023c.\nRobut:\nA system-\natic study of table qa robustness against human-\nannotated adversarial perturbations. arXiv preprint\narXiv:2306.14321.\nZexuan Zhong and Danqi Chen. 2020.\nA frustrat-\ningly easy approach for entity and relation extraction.\narXiv preprint arXiv:2010.12812.\nA\nAnalysis with Examples\nA.1\nExample Table A\nThe main difference between the reference tables\nand the tables generated by GPT-3.5 and GPT4\nis in the completeness and precision of the data\nprovided.\nIn the reference tables, all relevant data is fully\nrepresented: For the teams (Table 1), each team has\na precise number or percentage for every statistic.\nSimilarly, for the players (Table 2), each player\nhas a definite number for every statistic, including\nminutes played in the format \u201cmm:ss\u201d.\nPlayer\nAssists\n3-pointers \nattempted\n3-pointers \nmade\nField goals \nattempted\nField goals \nmade\nMinutes \nplayed\nPoints\nTotal \nrebounds\nMarc Gasol\n6\n-\n-\n-\n-\n-\n18\n5\nCourtney \nLee\n-\n5\n4\n14\n9\n-\n22\n-\nMike Conley\n11\n4\n3\n14\n9\n-\n24\n-\nMarkieff \nMorris\n-\n-\n-\n-\n-\n-\n20\n5\nGoran \nDragic\n-\n-\n-\n-\n-\n26\n6\n-\nEric Bledsoe\n4\n-\n-\n12\n9\n-\n23\n5\nIsaiah \nThomas\n2\n-\n-\n-\n-\n-\n15\n-\nTeam\nNumber of \nteam assists\nPercentage \nof field goals\nLosses\nTotal points\nPoints in 3rd \nquarter\nPoints in 4th \nquarter\nRebounds\nWins\nSuns\n13\n-\n2\n91\n19\n20\n35\n3\nGrizzlies\n25\n50\n-\n102\n30\n26\n37\n-\nTeam\nNumber of team \nassists\nPercentage of \nfield goals\nLosses\nTotal points\nPoints in 3rd \nquarter\nPoints in 4th \nquarter\nRebounds\nWins\nSuns\n13\n47.7%\n2\n91\n19\n20\n35\n0\nGrizzlies\n25\n50.0%\n0\n102\n30\n26\n37\n1\nPlayer\nAssists\n3-pointers \nattempted\n3-pointers \nmade\nField goals \nattempted\nField goals \nmade\nMinutes \nplayed\nPoints\nTotal \nrebounds\nMarc Gasol\n6\n0\n0\n12\n8\n35:00\n18\n5\nCourtney Lee\n1\n5\n4\n14\n9\n34:00\n22\n2\nMike Conley\n11\n4\n3\n14\n9\n36:00\n24\n0\nMarkieff \nMorris\n1\n5\n2\n14\n8\n34:00\n20\n5\nGoran Dragic\n4\n2\n1\n12\n9\n26:00\n6\n5\nEric Bledsoe\n4\n3\n1\n12\n9\n34:00\n23\n5\nIsaiah Thomas\n2\n5\n2\n11\n5\n26:00\n15\n0\nTeam\nHalf-Time \nScore\nFinal Score\nField Goals \n(%)\n3Pt (%)\nRebounds\nAssists\nPoints in \nthe Paint\nGrizzlies\n46\n102\n50\nN/A\n37\n25\n46\nSuns\n52\n91\nN/A\nN/A\n35\n13\n32\nPlayer\nTeam\nPoints\nAssists\nRebounds\nField Goals \n(FG)\nThree-Points \n(3Pt)\nCourtney Lee\nGrizzlies\n22\nN/A\nN/A\n9 - 14\n4 - 5\nMike Conley\nGrizzlies\n24\n11\nN/A\n9 - 14\n3 - 4\nMarc Gasol\nGrizzlies\n18\n6\n5\nN/A\nN/A\nEric Bledsoe\nSuns\n23\n4\n5\n9 - 12\nN/A\nGoran Dragic\nSuns\n6\nN/A\nN/A\nN/A\nN/A\nIsaiah \nThomas\nSuns\n15\n2\nN/A\nN/A\nN/A\nMarkieff \nMorris\nSuns\n20\nN/A\n5\nN/A\nN/A\nReference\nReference\nChatGPT\nGPT4\nChatGPT\nGPT4\nTable 1: Team \nSummary\nTable 2: Player \nStatistics\nThe Grizzlies (50) used a strong second half to outlast the Suns (3 - 2) 102 - 91 in Phoenix on Wednesday night. Memphis found itself behind six at halftime but outscored Phoenix 30 - 19 in\nthe third quarter and 26 - 20 in the final period. The Grizzlies shot 50 percent from the field, led by strong performances from Courtney Lee and Mike Conley. Lee scored 22 points (9 - 14\nFG, 4 - 5 3Pt), while Conley led all scorers with 24 (9 - 14 FG, 3 - 4 3Pt) and 11 assists. Marc Gasol added 18 points, six assists, and five rebounds. The Suns, who beat the Lakers 112 - 106 on\nTuesday, were paced by 23 points (9 - 12 FG), five rebounds and four assists from Eric Bledsoe. It was a quiet night for Goran Dragic, who scored just six points in 26 minutes. The third\nmember of the backcourt trio, Isaiah Thomas, had 15 points and two assists off the bench, while Markieff Morris added 20 points and five rebounds. The Grizzlies out - rebounded Phoenix\n37 - 35 and outscored the Suns in the paint 46 - 32. Memphis also registered 25 assists compared to only 13 - on 32 field goals - for the Suns. Memphis now heads to Oklahoma City to take\non the Thunder on Friday. Phoenix, meanwhile, hosts the Kings on Friday.\nFigure 4: Using GPT-3.5 and GPT-4 to generate a table\nbased on the input text, the generated results contain\na large number of errors, including format errors and\ncontent errors.\nIn contrast, the generated tables show data that\nis incomplete and imprecise. For GPT-3.5 gener-\nated one, the team statistics table has some statis-\ntics missing, as represented by empty cells, and\nsome are not presented as percentages. The player\nstatistics table also has missing data in a similar\nfashion, and it lacks the \"minutes played\" statis-\ntics entirely. For instance, in the \u2019team\u2019 table, the\n\"Percentage of field goals\" column for the Suns\nis missing. Similarly, in the \u2018player\u2019 table, many\nkey statistics such as \"3-pointers attempted\", \"3-\npointers made\", \"Field goals attempted\", \"Field\ngoals made\", and \"Minutes played\" are missing for\nvarious players. Regarding the format, we observe\na lot of format errors. For example, the \u2018Percentage\nof field goals\u2019 column for Grizzlies is represented\nas \"50\" instead of \"50.0%\". Moreover, the \u2018Wins\u2019\ncolumn for the Suns is represented as \"3\" instead of\n\"0\". This misrepresentation can lead to significant\nmisunderstanding of the data. The \u2018Player\u2019 table\nalso has format errors. For instance, the \u2018Minutes\nplayed\u2019 column is missing the time format (i.e.,\n\u201c00:00\u201d). On the other hand, the reference tables\nadhere to a standard format. Percentage data is rep-\nresented with a \u2018%\u2019 sign, time data uses the \u201800:00\u2019\nformat, and numeric data correctly represents each\nstatistic.\nA.2\nError Type\nStructure Errors:\nThese errors pertain to the\nstructural integrity of the generated tables. Specifi-\ncally, they include instances where there are excess\nor missing rows or columns in comparison to the\ncorrect table structure.\nStructure Naming Errors:\nThis category cap-\ntures errors related to the naming conventions used\nfor rows or columns. Any discrepancies in a row or\ncolumn names between the generated and correct\ntable are flagged as structure naming errors.\nElement Errors:\nThese are inaccuracies ob-\nserved at the element level within the generated\ntable. Element errors encompass incorrect num-\nbers, values, or inappropriately empty cells, reflect-\ning discrepancies in individual table entries relative\nto the correct table.\nB\nRationale for Selecting the RotoWire\nDataset\nTraditional\ndata-to-text\ndatasets\ninclude\nRo-\ntowire (Wiseman et al., 2017), E2E (Novikova\net al., 2017), WikiTableText (Bao et al., 2018),\nand WikiBio (Lebret et al., 2016). Given that only\nthe RotoWire dataset contains tables with more\nthan 2 columns, we specifically opted to utilize this\ndataset. Furthermore, to maintain a certain level of\ncomplexity in our study, we filtered out tables with\ndimensions smaller than 3x3 in Rotowire.\nDataset\nTrain\nValid\nTest\n# of tokens\n# of rows\n# of columns\nE2E\n42.1k\n4.7k\n4.7k\n24.90\n4.58\n2.00\nWikiTableText\n10.0k\n1.3k\n2.0k\n19.59\n4.26\n2.00\nWikiBio\n582.7k\n72.8k\n72.7k\n122.30\n4.20\n2.00\nTable 3: Statistics of E2E, WikiTableText, and WikiBio\ndatasets, including the number of instances in training,\nvalidation, and test sets, number of BPE tokens per\ninstance, and number of rows per instance.\nC\nMTurk\nAbout the qualifications of Amazon Mechanical\nTurk (MTurk) workers, we use the following qual-\nifications to recruit in total of 10 MTurk workers\nwith good track records: HIT approval rate greater\nthan or equal to 98%, number of HITs approved\ngreater than or equal to 500, and located in one\nof the following English native-speaking countries:\nAustralia, Canada, New Zealand, United Kingdom,\nUnited States. Each annotator is limited to anno-\ntating 10 examples, including both the output of\nGPT-3.5 and GPT-4.\nAnnotators workers were compensated $7, cali-\nbrated to equal a $42/hour pay rate. We first anno-\ntated examples in-house to determine the required\nannotation speed. A summary block usually takes\naround 10 minutes.\nTo demonstrate our annotation template and fa-\ncilitate future research, we show the interface for\nannotations.\nFigure 5: Interface of Mturk.\nFigure 6: Interface of Mturk.\nD\nScoring\nD.1\nGPTscore\nHere we display our prompt for GPTscore. First,\ntell GPT how to score the \"content similarity\" and\n\"structural similarity\" and then tell GPT how to\noutput the answer:\n\u201cBased on the above, we wanted to determine\nif the above tables are similar. Ideally, they should\nhave identical content and structure. Score the\n\"content similarity\" and \"structural similarity\" be-\ntween 0 and 10.\n- Content similarity: 10 if the contents of the\ntable cells are identical, 0 if they are entirely differ-\nent. If about 50% of the cells have the same data,\nthe score should be 5.\n- Structural similarity: 10 if the tables have\nthe same structure (e.g. same column and rows\nwith identical ordering, same alignment, etc.) al-\nthough text formatting differences can be ignored\n(e.g. colors, font).\nOutput a JSON object such as the following:\n\"\"\"json\n{{\n\"content_similarity\": ...\n\"structural_similarity\": ...\n}}\n\"\"\"\nThink carefully, and then output the scores.\u201d\nD.2\nH-Score\nLATEX\nWe use the pylatexenc library to parse a\ngiven LATEX table, and walk through the parse-tree\nstructure in the tabular environment to identify\nthe table \u201ccells\u201d. We score the content similarity\nbased on strings within the cells, and score struc-\ntural similarity based on having the matching num-\nber of rows and columns, the same caption, and the\nsame cell alignment.\nHTML\nWe use the beautifulsoup4 library to\nparse a given LATEX HTML snippet and walk\nthrough the parse-tree structure in <table>, <ul>\nor <ol> tags to identify data cells. We separately\nbuild a tree of white-listed HTML tags to score\nthe structural similarity, traversing an HTML doc-\nument tree structure, disregarding the actual con-\ntent within the tags and simplifying it by focusing\nonly on specific HTML tags (defined in RECOG-\nNIZED_HTML_TAGS). We score the content sim-\nilarity based on strings within the cells and score\nstructural similarity based on the similarity of the\nstructure tree and the total number of cells match-\ning.\nWhite-listed HTML tags:\nRECOGNIZED_HTML_TAGS = [\n\"table\", \"tr\", \"th\", \"td\",\n\"ul\", \"ol\", \"li\",\n\"div\", \"span\", \"p\",\n\"a\", \"img\", \"embed\", \"pre\",\n\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\",\n\"input\", \"button\",\n]\nRaw Text Tables\nIn our evaluated dataset, each\nexample consists of two tables (Team and Player).\nWe do a string search for \"Team\" and \"Player\"\nheaders to identify the two tables. We then parse\nthe tables according to Markdown formatting, with\nnewlines and pipes as row and column dividers\nrespectively, to identify the table cells. We score\nthe content similarity based on strings within the\ncells, and score structural similarity based on the\nsimilarity of column names and the number of rows\nand columns matching.\nString Similarity Measurement:\nOur script in-\ncludes methods to calculate the similarity between\ntwo strings. These methods can be used to com-\npare the structure or content of HTML, latex docu-\nments or any other pair of strings. The similarity\nis evaluated using well-established algorithms in\ntext analysis: the Levenshtein distance and the Se-\nquenceMatcher from Python\u2019s difflib module.\nE\nPrompt for Description Generation and\nInference\nRaw Text Table Description Prompt\nTradi-\ntional data-to-text datasets only have raw text for\neach table. However, it is not enough for Chatgpt\nor other LLMs to generate correct tables. As a\nresult, we added some format descriptions to help\nthem generate the correct tables. We use GPT-3.5\nto achieve this. We want to get detailed format in-\nformation without concrete contents in cells, so we\nexplicitly include these requirements in the prompt.\nHere is our prompt: Describe details about the\ngiven text. First, give the number of tables, and\nthen for each table, describe its format such as the\nnumber of columns and rows, column names, and\nrow names.\nHTML Table Description Prompt\nUnlike data-\nto-text datasets, HTML datasets only have final\noutputs, so we are required to generate a detailed\ndescription of their format and content. For con-\ntent descriptions, we can simply ask GPT-3.5 to\noutput raw text without HTML tags. For format\ndescriptions, however, we need to ask GPT-3.5 to\ndescribe each tag, otherwise, it will leave out some\ntags and describe the table in general rather than\ndetailed information. Moreover, it is necessary to\nask it to use specific numbers instead of \u2019several\u2019 or\n\u2019multiple\u2019. Here is our prompt for HTML format\ndescriptions: Describe the format of this HTML in\ndetail according to each HTML tag of the follow-\ning HTML code. Be careful and make sure don\u2019t\nmiss any HTML tags. Please use more than 300\nwords to explain the format. Use specific numbers\nrather than being vague about several.\nLatex Table Description Prompt\nSimilar to\nHTML prompt generation, it is necessary to ask\nGPT-3.5 to generate both format descriptions and\ncontent descriptions as latex datasets only have\nfinal outputs. For content descriptions, we can sim-\nply ask GPT-3.5 to describe the given latex table\nas detailed as it can and include all cells. For for-\nmat description, since the latex format is too com-\nplex, we need to give it a small example to learn.\nThen we ask GPT-3.5 to describe the detailed for-\nmat of a given latex table, including specific ques-\ntions to help it generate format descriptions. Here\nis our prompt for latex format descriptions: De-\nscribe the detailed format of a given latex table\naccording to the commands and tags with more\nthan 500 words. Include: Whether there is table\nborder lines? How is text alignment? What are\ntable attributes? Whether to bold? Whether to\nadd \\ref? Please clearly explain whether there are\nhorizontal and vertical lines bordering each row\nand column. Say anything about a special \"\\\" for-\nmat token in latex if there is. Don\u2019t display latex\ncode directly. Use natural language. And provide\nenough format information for me to recreate this\ntable based on your output description.\nPrompt for Inference\nWhen inferencing raw\ntext tables, LLMs tend to output tabular results\nrather than raw text tables. As a result, we need to\ngive it an example output first, then tell the model\nthat the input consists of two parts, text and format\ndescriptions, and ask the model to generate the out-\nput based on them. For HTML and Latex inference,\nwe can simply ask models to infer from the input\nand specify the format and content sections in the\ninput, since models can generate correct syntax.\nF\nPrompt for GPT scores\nWe prompt the model to perform Chain-of-Thought\nreasoning before outputting its scores, and we\nquery the model with the predicted and ground\ntruth tables in both orders and average the scores.\nHere we use the GPT scores prompt for raw text\ntables as an example:\nWe want to evaluate how similar the following\ntables/data structures are.\nTable 1:\n\u201c\u2018 input1 \u201c\u2018\nTable 2:\n\u201c\u2018 input2 \u201c\u2018\nBased on the above, we wanted to determine if\nthe above tables are similar. Ideally, they should\nhave identical content and structure. Score the \"con-\ntent similarity\" and \"structural similarity\" between\n0 and 10.\n- Content similarity: 10 if the contents of the\ntable cells are identical, 0 if they are entirely differ-\nent. If about 50% of the cells have the same data,\nthe score should be 5.\n- Structural similarity: 10 if the tables have the\nsame structure (e.g. same column and rows with\nidentical ordering, same alignment, etc) although\ntext formatting differences can be ignored (e.g. col-\nors, font).\nOutput a JSON object such as the following:\n\u201c\u2018json\n\"content_similarity\":\n...\n\"struc-\ntural_similarity\": ... \u201c\u2018\nThink carefully, and then output the scores.\nG\nAbility Map\nBased on our automated evaluation, we selected Vi-\ncuna, ChatGPT, GPT-4, and Ours as representative\nmodels and conducted an in-depth analysis of the\ncauses of model errors.\nWe identified content accuracy, formatting, nu-\nmerical reasoning, and handling of long tables as\nthe main sources of these errors.\nAt the fundamental level, we decompose the pro-\ncess of model-generated complex structured out-\nputs into two parts: Content Selection and Format\nPlanning. Initially, the model needs to identify key\ninformation from a given vast amount of unstruc-\ntured input, extract this information, understand\nit, and organize it. Subsequently, it needs to plan\nhow to summarize these extracted details, devise\nthe format of the table to be generated, and then fill\nin the information.\nAccordingly, we can break down the model\u2019s ca-\npabilities into Coverage, Formatting Reasoning,\nComprehension, Pragmatics, and Hallucination\nControl.\nCoverage entails the model\u2019s ability to accurately\ncover the content in the input. Formatting Reason-\ning pertains to judgment about the output format,\nassessing if the model can find the most appropriate\nand reasonable structured format.\nComprehension reflects whether the model can\nunderstand the content of the input, as there are\ntimes when it is necessary to infer from a large\namount of data (including performing addition or\nsubtraction or comparing multiple elements).\nPragmatics involves the ability to utilize special\nformats, such as HTML tags and specific syntax in\nLaTeX.\nFinally, Hallucination Control signifies the\nmodel\u2019s ability to refrain from generating content\nnot present in the input.\nWe carried out manual annotations and obtained\nvisualized results to demonstrate these aspects.\n"
  },
  {
    "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following",
    "link": "https://arxiv.org/pdf/2309.08637.pdf",
    "upvote": "7",
    "text": "Preprint\nTEXTBIND: MULTI-TURN INTERLEAVED MULTIMODAL\nINSTRUCTION-FOLLOWING IN THE WILD\nHuayang Li\u2661\u2660\u2217\nSiheng Li\u2661\u2663\u2217\nDeng Cai\u2661\u2217\nLongyue Wang\u2661\nLemao Liu\u2661\nTaro Watanabe\u2660\nYujiu Yang\u2663\nShuming Shi\u2661\n\u2661Tencent AI Lab\n\u2660Nara Institute of Science and Technology\n\u2663Tsinghua University\nhttps://textbind.github.io\nABSTRACT\nLarge language models with instruction-following abilities have revolutionized\nthe field of artificial intelligence. These models show exceptional generalizabil-\nity to tackle various real-world tasks through their natural language interfaces.\nHowever, their performance heavily relies on high-quality exemplar data, which\nis often difficult to obtain. This challenge is further exacerbated when it comes to\nmultimodal instruction following. We introduce TEXTBIND, an almost annotation-\nfree framework for empowering LLMs with multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption pairs\nand generates multi-turn multimodal instruction-response conversations from a\nlanguage model. To accommodate interleaved image-text inputs and outputs, we de-\nvise MIM, a language model-centric architecture that seamlessly integrates image\nencoder and decoder models. Extensive quantitative and qualitative experiments\ndemonstrate that MIM trained on TEXTBIND achieves remarkable generation\ncapability in multi-modal conversations compared to recent baselines.\n1\nINTRODUCTION\nArtificial intelligence (AI) has experienced a significant paradigm shift with the rise of large language\nmodels (LLMs). These models are capable of processing a wide range of natural language processing\n(NLP) applications through natural language interactions with users (OpenAI, 2022; 2023). Despite\ntheir remarkable performance, these models cannot process and generate visual content.\nRecently, a number of efforts have been made to augment LLMs with visual perception and un-\nderstanding abilities. Prior work uses template-based instruction-following datasets for training\n(Xu et al., 2023b; Dai et al., 2023; Li et al., 2023c). These datasets comprise a variety of classic\ncomputer vision (CV) tasks, e.g., object detection, with each task being converted into an instructional\nformat using a handful of human-written natural language instructions. However, classic CV tasks\noften represent manageable and focused abstractions or simplifications of real-world tasks (Marr,\n2010), they generally fall short in representing the true variety and complexity of real-world tasks\nand capturing the lexical diversity of human language. For example, most of them are single-turn\ninquiries about a single input image, whereas a small fraction supports multi-turn textual interactions\nor multiple image inputs. Consequently, the instruction-following capabilities of models trained on\nthese datasets remain limited in open-world scenarios (Xu et al., 2023a). This is reminiscent of the\nearly development of instruction tuning in NLP, where public NLP tasks were eventually superseded\nby high-quality, diverse open-world instruction data (Ouyang et al., 2022). Nevertheless, collecting\nsuch data for multimodal models can be extremely costly.\nIn this paper, we address the above challenge by introducing TEXTBIND, an almost annotation-\nfree framework for augmenting LLMs with multi-turn interleaved multimodal instruction-following\n\u2217Equal Contribution. Work done during HL and SL\u2019s internships at Tencent AI Lab. Correspondence to DC\n(jcykcai@tencent.com).\n1\narXiv:2309.08637v4  [cs.CL]  8 Jan 2024\nPreprint\ncapabilities. The main idea is to represent images through their textual descriptions, e.g., captions,\nand utilize an LLM to generate multi-turn instructions and responses. To ensure the coherence\nand meaningfulness of the constructed multi-turn conversations, we propose a series of strategies\nsuch as topic-aware image sampling and human-in-the-loop refinement of in-context demonstrations.\nTEXTBIND can harvest large-scale datasets given the abundance of public image-caption pairs.\nTEXTBIND provides examples of processing and generating arbitrarily interleaved image-and-text\ncontent. To accommodate interleaved image-text inputs and outputs, we devise MIM, a multimodal\nmodel that emphasizes the reasoning abilities of LLMs and seamlessly integrates image encoder and\ndecoder models. The comparison of TEXTBIND and previous representative datasets is shown in Tab.\n8 (Appx. D), accompanied by an illustration of the models trained on different datasets in Fig. 10\n(Appx. D).\nTo assess the generative capabilities of MIM trained on TEXTBIND, we perform comprehensive\nanalyses in the context of multi-modal conversations (\u00a76). In particular, thorough reference-based\nautomatic evaluation metrics reveal that the MIM model substantially surpasses MiniGPT-4 Zhu\net al. (2023) and LLaVA Liu et al. (2023b) in textual response generation, and outperforms GILL\nKoh et al. (2023a) and Stable Diffusion Podell et al. (2023) in image generation by a considerable\nmargin. Furthermore, our holistic evaluation demonstrates that MIM consistently outperforms\nthe representative baselines. In addition, our qualitative experiments show that MIM trained on\nTEXTBIND can perform a wide range of tasks, including composing engaging stories inspired by a\nset of images (Fig. 10), comparing the common and different parts in multiple images (Fig. 6b (Appx.\nA)), explaining concepts with vivid images (Fig. 5a (Appx. A)), generating long coherent stories with\nillustrations (Fig. 4 (Appx. A)), etc. More demonstrations are shown in Appx. A. Most interestingly,\nthe core innovation of our model is its capability to interact with users naturally. For instance, rather\nthan requiring users to supply the model with explicit descriptions of the desired image, our model\ncan spontaneously generate images in proper conversation contexts. We hope TEXTBIND serves as\nan initial step towards building AGI that can interact with humans flexibly in different modalities and\nbroad real-world scenarios.\n2\nRELATED WORK\nMultimodal Datasets\nExisting multimodal datasets can be broadly classified into two categories:\n(1) Conventional datasets for specific vision-language tasks such as image captioning (Chen et al.,\n2015; Agrawal et al., 2019; Young et al., 2014) and visually-grounded question answering (Hudson\n& Manning, 2019; Marino et al., 2019; Singh et al., 2019; Lu et al., 2022; Zhou et al., 2018; Goyal\net al., 2017; Gurari et al., 2018). (2) Recent dataset for general instruction following. For instance,\nMultiInstruct (Xu et al., 2023b), InstructBLIP (Dai et al., 2023), and M3IT (Li et al., 2023c) convert\nexisting vision-language datasets into a unified instructional format with handcrafted templates. This\napproach is reminiscent of the early explorations on instruction tuning in NLP (Wei et al., 2022; Sanh\net al., 2022), where existing NLP tasks were phrased as instructions. However, it has been reported\nthat such instruction-tuned multimodal models still generalize poorly to open-world scenarios (Xu\net al., 2023a). This finding also aligns with the observations in NLP (Ouyang et al., 2022), where\ntemplate-based instruction tuning is less effective than instruction tuning data collected from real-\nworld scenarios due to its restricted diversity. There are also some attempts to convert the output of\nexisting vision-language models into natural language answers for constructing instruction-tuning\ndata (Liu et al., 2023b; Zhu et al., 2023; Chen et al., 2023a).\nCompared to existing instruction-tuning data, the examples in TEXTBIND (1) generally exhibit greater\ntask and lexicon diversity; (2) typically involve multiple images scattered throughout a multi-urn\nconversation; (3) support multimodal output (image generation).\nMultimodal Models\nTo augment existing LLMs with visual abilities, one straightforward approach\nis to employ off-the-shelf vision models as external tools. That is, the LLM calls expert vision models\nthrough their language interfaces for completing specific visual tasks when needed (Wu et al., 2023a;\nShen et al., 2023; Chen et al., 2023b; Zou et al., 2022; Yang et al., 2023; Sur\u00b4\u0131s et al., 2023).However,\nthese approaches may suffer from cross-modal information loss and lack of generality.\nRecently, end-to-end multimodal language models have garnered significant interest. Flamingo\n(Alayrac et al., 2022) and OpenFlamingo (Alayrac et al., 2022) are among the pioneering work\n2\nPreprint\nto extend LLMs to vision-language pretraining. Different from training from scratch, subsequent\nresearch efforts have focused on integrating pretrained vision and language models. BLIP-2 (Li et al.,\n2023b) proposes Qformer to align the feature spaces of vision models and language models. To date,\nvarious network architectures and training strategies have been proposed (Zhu et al., 2023; Liu et al.,\n2023b; Ye et al., 2023; Li et al., 2023a; Zhang et al., 2023; Du et al., 2022; Chen et al., 2023a; Dai\net al., 2023; Liu et al., 2023a). However, these models are limited to the use of visual content as input.\nOur work is inspired by recent work on LLM-empowered image retrieval or generation (Koh et al.,\n2023b;a) and the pioneer work of (Sun et al., 2022) for chitchat in the context of single photo sharing.\nContrary to prior work, we aim to present the first instruction-following model capable of processing\nand generating arbitrarily interleaved image-text inputs and outputs.\nNumerous contemporary studies also exist in this field (Team, 2023; Yao et al., 2023; Dong et al.,\n2023; Zheng et al., 2023; Ge et al., 2023). A unique characteristic of our work lies in our emphasis\non the aspect of data creation, whereas other studies primarily concentrate on architecture design or\ntraining algorithms. Pan et al. (2023) focuses on image editing, while Wu et al. (2023b); Moon et al.\n(2023) place emphasis on incorporating additional modalities, such as audio and video.\nEvaluation\nConventional vision datasets designed for specific tasks and scenarios may suffer\nfrom data contamination issues for evaluating LLMs. Recently, efforts have been made to provide\nsystematic evaluations with a broader coverage of diverse visual abilities. MME (Fu et al., 2023) is\nan evaluation dataset containing visually-grounded Yes/No questions. OwlEval (Ye et al., 2023) is a\nbenchmark comprising 82 questions based on 50 images and relies on human feedback evaluation.\nThe test size is limited, and the results may suffer from subjective bias. In response to these challenges,\nMMbench (Liu et al., 2023c) and MM-Vet (Yu et al., 2023) are two recent benchmarks aiming to offer\nmore comprehensive evaluations by incorporating the use of ChatGPT/GPT4 for answer verification.\nLVLM Arena (Xu et al., 2023a), an online evaluation framework that ranks different models using\nhuman judgment, is also introduced. However, the above benchmarks primarily focus on question\nanswering based on a single image at the beginning of a conversation.\n3\nTEXTBIND\nIn this work, we seek to enhance the multi-turn instruction-following capabilities of a language model\nin the context of arbitrarily interleaved images and text. Constructing such datasets poses significant\nchallenges: 1) it demands inventive thinking for devising high-quality visually-grounded instructions\nand their responses; 2) it requires specialized expertise to craft appropriate images. To tackle these\nissues, we introduce TEXTBIND, a method that predominantly resorts to existing text-only language\nmodels1 to produce the desired data.\n3.1\nDEFINITION OF DATA\nThe goal of TEXTBIND is to construct a collection of multi-turn conversation such as\n[x1\nu, x1\na, . . . , xT\nu , xT\na ], where T is the number of turns, xi\nu denotes the i-th instruction from the\nuser, and xi\na represents the i-th response from the assistant. The conversation is also accompanied\nby an image set {m1, . . . , mn}, where n is the number of unique images in this conversation. Each\ninstruction xi\nu or response xi\na is a sequence of tokens in Vlang \u222a Vimg, where Vlang is the ordinary\nvocabulary of a language model and Vimg contains n distinct pointers to the images m1, . . . , mn\nrespectively. It is worth noting that every image can appear at any point within the conversation.\n3.2\nAUTOMATIC DATA GENERATION\nTEXTBIND consists of a three-step pipeline: 1) topic-aware image sampling for ensuring the coher-\nence of each conversation and the diversity across conversations; 2) LLM-empowered multi-turn\ninstruction-response generation to create natural and practical conversations; 3) post-processing and\nfiltering to eliminate low-quality data. An overview of the TEXTBIND pipeline is shown in Fig. 1.\nTopic-Aware Image Sampling\nThe initial step of TEXTBIND entails assembling groups of images\nthat will serve as the foundation for generating multi-turn conversations. In order to facilitate coherent,\n1Although OpenAI claims that GPT4 supports visual input, this feature is yet to be made public.\n3\nPreprint\nHi! I'm trying to write a cartoon story for my kids featuring \na grumpy clown and a lovable cupcake. Can you help me \ncraft this?\nOf course! That sounds like a wonderful idea. We can start \nby establishing our characters. The grumpy clown could be \nsomeone who struggles finding joy in his own \nperformances.\nThat\u2019s a wonderful idea! How about the cute cupcake?\nThe cupcake, on the other hand, is always upbeat and tries \nto help the clown find his happiness. Despite being a tiny \ncupcake, it has a big heart and a contagious joy. \nInteresting! What should be their adventure? \nThey could go on a journey to find the ultimate joke that \nguarantees laughter. The lovable cupcake, with his \nrelentless positivity and cheer, helps the grumpy clown see \nthe lighter side of life and finally find his joy.\nCartoon\nKids\nBasketball Players\n\u00b7\u00b7\u00b7\nTopic-awared Image Sampling\nConversation Based on Sampled Images\nLLM\nIn-context \nExamples\nExample Set\nHuman Rating\n1\nPost-processing\nDataset\n3\n2\nFigure 1: Illustration of the TEXTBIND method. In the top-left corner, we display five representative\nimages from each of the three example clusters obtained via unsupervised clustering. On the right-\nhand side, a conversation is showcased and constructed using two randomly sampled images from\nthe cartoon cluster. In the bottom-left corner, we outline the additional TEXTBIND pipeline, which\nincludes human-in-the-loop refinement and post-processing stages.\nmeaningful, and practical conversations, the images within each group should exhibit meaningful\ninterconnections. Furthermore, to guarantee a comprehensive representation of real-world scenarios,\nthe topics of images across different conversations should demonstrate a wide range of diversity.\nFollowing the above inspirations, we employ unsupervised clustering algorithms to group the images\nin our dataset into clusters and execute a two-step image sampling process for each conversation.\nConcretely, we use the image encoder of the CLIP model (Radford et al., 2021) to obtain vector\nrepresentations of images. Then, we execute the k-means algorithm to classify all images into K\nclusters (topics). Examples of such clusters are given in Fig. 1. For each conversation, we randomly\nsample a cluster from the available K clusters, then sample n \u2208 {2, 3, 4} images from the chosen\ncluster. We want to higlight that the clustered images are semantically relevant, rather than visually\nsimilar.\nGeneration of Multi-turn Conversations\nAfter selecting a list of images, we proceed to leverage a\ntext-only LLM, such as GPT-4, to simulate a conversation between a user and an assistant based on the\nchosen images. The core idea is to let LLMs receive and process the textual descriptions of the images\nas if they see the actual images. Given the abundance of publicly available image-caption pairs, we\npropose representing an image with an XML-like string <imgX> DESCRIPTION </imgX>, where\nDESCRIPTION serves as a placeholder for the image caption, <imgX> and </imgX> mark the caption\nboundaries, and X denotes the image index in the input image list. After generating the conversation,\nwe replace the XML-like strings in the conversation with the original images. Importantly, to ensure\nthat a caption faithfully describes its corresponding image, we employ the CLIP model (Radford\net al., 2021) to filter out image-caption pairs with matching scores below a high threshold.\nThe detailed prompt can be found in Appx. B, and examples of generated conversations before\nmapping the textual descriptions back to visual images are shown in Appx. C. In the prompt, we also\nprovide in-context examples to improve the generation quality. We collect the in-context examples\nthrough a human-in-the-loop refinement process, which is elaborated in \u00a73.3.\nPost-processing and Low-quality Filtering\nTo ensure data quality, we filter out conversations\nwhere there is a pair of input and output image descriptions with an edit distance higher than 0.1. We\n4\nPreprint\n9%\n57%\n34%\nExcellent\nSatisfactory\nPoor\nImg Create\ng Compare\nIntrinsic  \nUnderstand.\nExtrinsic \nUnderstand.\nPercentage\n0% 20% 40% 60% 80%\nAVG. Image Num\n0.0\n0.2\n0.4\n0.5\n0.7\nTurn Num\n0\n1\n2\n3\n4\n5\nUser\nAssistant\n(a)\n57%\n34%\nExcellent\nSatisfactory\nPoor\nImg Create\nImg Compare\nIntrinsic  \nUnderstand.\nExtrinsic \nUnderstand.\nPercentage\n0% 20% 40% 60% 80%\nAVG. Image Num\n0.0\n0.2\n0.4\n0.5\n0.7\nTurn Num\n0\n1\n2\n3\n4\n5\nUser\nAssistant\n(b)\n57%\n34%\nExcellent\nSatisfactory\nPoor\nImg Create\nImg Compare\nIntrinsic  \nUnderstand.\nExtrinsic \nUnderstand.\nPercentage\n0% 20% 40% 60% 80%\nAVG. Image Num\n0.0\n0.2\n0.4\n0.5\n0.7\nTurn Num\n0\n1\n2\n3\n4\n5\nUser\nAssistant\n(c)\nFigure 2: Statistics of data quality and diversity. The results in Fig. 2a and 2b are based on the human\nannotations on 100 randomly sampled conversations.\nalso exclude conversations containing image descriptions not present in the provided image list and\nconversations containing formatting errors such as co-reference errors and invalid image tags.\n3.3\nHUMAN-IN-THE-LOOP REFINEMENT\nIn-context learning has been demonstrated to be crucial for enhancing the generation quality of LLMs\n(Brown et al., 2020; Wang et al., 2023). Therefore, we also construct a seed set of high-quality\nin-context examples S. The seed set S begins as an empty set and is iteratively updated with human\nfeedback. In each iteration, we follow the steps detailed below:\n1. We employ the latest S and the template in Appx. B, and generate 100 new conversations\nusing TEXTBIND (\u00a73).\n2. We manually analyze the generated conversations. Each conversation is assigned a quality\nlabel (\u201cExcellent\u201d, \u201cSatisfactory\u201d, or \u201cPoor\u201d). Besides, we label the visual abilities required\nfor each conversation. The detailed annotation guideline for quality labels and visual abilities\nis outlined in Tab. 9 (Appx. E).\n3. We add the generated conversations with \u201cExcellent\u201d or \u201cSatisfactory\u201d labels to S.\nStatistics\n# of conversations\n25, 629\nAvg. # turns in conversations\n3.36\nAvg. # images\nin conversations\n2.46\nin instructions\n0.94\nin responses\n1.52\nAvg. # words\nin conversations\n285.90\nin instructions\n78.66\nin responses\n207.24\nTable 1: Statistics of the dataset by ap-\nplying TEXTBIND to GPT-4.\nTo ensure diversity among different conversations, we\nrandomly sample three in-context examples from the seed\nset for each generation. We further require that at least\none in-context example is labeled \u201cExcellent\u201d and the\nthree sampled examples encompass all four visual abilities.\nAfter three iterations, we fix the seed set and employ it\nto generate the remaining data. The percentage of \u201cPoor\u201d\ndata annotated by humans declines from 30% to 9%.\n4\nTEXTBIND DATA FROM GPT4\nWe apply TEXTBIND to GPT4 and the CC3M dataset\n(Sharma et al., 2018; Changpinyo et al., 2021) as a case\nstudy. The details of the construction process can be found in Appx. F. In this section, we present\ncomprehensive analyses of the constructed dataset.\nStatistics\nAs depicted in Tab. 1, our constructed dataset comprises 25, 629 conversations. The\naverage number of turns per conversation is 3.36 (each turn is defined as a pair of instruction and\nresponse). The mean number of images in each conversation is 2.46.\nDiversity\nTo understand the lexical and task diversity of our constructed data, we identify four types\nof required visual abilities and show their distribution in Fig. 2b. We observe that a significant portion\nof conversations in our dataset focuses on more insightful and informative tasks, such as extrinsic\n5\nPreprint\nunderstanding and image comparison. For topic diversity, we display three randomly sampled clusters\nin Fig. 1. The distribution of images across different turns is depicted in Fig. 2c. We also compare\nthe lexical diversity of our dataset and existing datasets in Tab. 2.\nDataset\nInstruct\nResponse\nOverall\nLLAVA\n1.56\n1.84\n1.70\nMINIGPT-4\n0.00\n1.11\n0.89\nMULTIINSTRUCT\n0.51\n1.69\n0.51\nPLATYPUS\n0.98\n0.75\n0.78\nSHIKRA\n0.89\n1.08\n0.87\nTEXTBIND\n1.76\n1.92\n1.84\nTable 2: Averaged diversity scores of\nroles in various datasets. Details of this\nanalysis are in Appx. D.\nQuality\nTo check the quality of the generated data, we\nrandomly sample 100 conversations and perform an in-\ndepth error analysis. As shown in Fig. 2a, only 9% conver-\nsations in the dataset are labeled as \u201cPoor\u201d. Note that we\nlabel the whole conversation as \u201cPoor\u201d if any of its turns\nhas a problem. We analyze the error types (image-caption\nmismatch, incoherence, and hallucination) in Appx. G.\n5\nAUGMENTING LLMS WITH VISUAL I/O\n5.1\nMODEL\nTo support interleaved multimodal inputs and outputs, we\nsupplement LLMs with visual input and output modules. Specifically, LLama2-Chat2 (Touvron\net al., 2023) is employed as the backbone LM. For visual input, we use the vision encoder from\nBLIP2 (Li et al., 2023b)3, followed by a pretrained Q-Former model (Li et al., 2023b) that maps the\nfeatures from the vision model into the embedding space of the LM. Inspired by GILL (Koh et al.,\n2023a), we attempt to learn a mapping from the output space of the LM to the input space of a stable\ndiffusion (SD) model (Rombach et al., 2022) (in this work, the embeddings produced by the text\nencoder of Stable Diffusion XL (Podell et al., 2023)). To this end, we explore three model variants\nin our preliminary experiments. The training examples of the MIM model follow the standard of\nLlama-Chat, as shown in Appx. J. The content in different conversation turns is concatenated. The\nmodel is trained to minimize the cross-entropy loss on the assistant\u2019s turns, conditioned on the entire\npreceding conversation history.\nQ-Former as Medium. We add a special token <IMG> to the vocabulary of the LM, indicating that\nan image should be generated when it is emitted. We then use a Q-Former (Li et al., 2023b) that takes\nall previous hidden states of the LM as input and outputs the SD embeddings.\nQ-Former with Prompt Tokens as Medium. To further leverage the reasoning abilities of the LM,\nwe incorporate a series of special tokens (<img1>, ..., <IMG{r}>), instead of a single token (<IMG>),\nto the LM. When <img1> is emitted, the generation of the special token sequence is enforced, serving\nas additional reasoning steps for predicting the forthcoming image. Subsequently, the Q-Former only\naccepts the hidden states of special tokens as input.\nLanguage Description as Medium. The previous two variants try to align the continuous hidden\nspaces of different models. An alternative is to use discrete language descriptions for information\nexchange, as depicted in Fig. 3. Specifically, we add two special tokens, <start> and <end>, and\nencode the generated text between these two tokens using the text encoder in the SD model.\nSimilar to GILL (Koh et al., 2023a), we optimize the first two variants by minimizing the mean\nsquared error (MSE) loss between the output embeddings and the SD embeddings. For the third\nvariant, we employ the standard cross-entropy loss. We empirically find that only the last method\ndemonstrates satisfactory performance on multi-turn interleaved multimodal instruction-following,\nfor which we name it MIM.\n5.2\nTRAINING\nOur training process consists of two stages, namely, the multimodal alignment stage and the multi-\nmodal instruction tuning stage.\nMultimodal Alignment\nThe first stage aims to align the feature spaces of the vision model and the\nlanguage model. We utilize massive image-caption pairs for training, drawing from datasets such as\n2https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n3https://huggingface.co/Salesforce/blip2-flan-t5-xxl\n6\nPreprint\nLarge Language Model\nHi, Do you know \nthe breed of this cat?\nYes, the cat in the\nimage appears to\nbe a Ragdoll, a\nbreed known for\nits\naffectionate\nnature.  Here  is \nanother one:\nVision Encoder\nQ-Former\nLinear\nWow, I see, they\nare so adorable. Do\nyou\nknow\nany\nother breeds of cats?\nStable Diffusion\n<IMG>\n\u2026\n<Assis>\n\u2026\n<start>\nRagdoll\n<user>\n\u2026\n<start>\n\u2026\n<start>\n\u2026\nPersian\n<Assis>\nWow\n\u2026\n<user>\n\u2026\nStable Diffusion\nSure!\nThe\nMaine Coon\nis\na\nlarge\ndomestic cat\nbreed\nfrom\nthe U.S.\nand the Persian\ncat\nis\na\nlong-\nhaired breed with\na distinctive flat\nface\nand\nlarge\neyes, like this:\nFigure 3: The architecture of MIM. It integrates a vision model, a language model, and a stable\ndiffusion model. MIM is able to process multi-turn interleaved multimodal inputs and outputs.\nConceptual Captions (Changpinyo et al., 2021; Sharma et al., 2018) and SBU (Ordonez et al., 2011).\nDuring training, only the Q-Former connecting the vision and language models is optimized while\nother model components remain frozen.\nMultimodal Instruction Following\nThe second stage further trains the joint model on multimodal\ninstruction tuning data to improve its instruction-following capabilities. The Q-Former model and\nLLM are optimized in this stage. In addition to TEXTBIND data, we also explore existing multimodal\ninstruction data including MultiInstruct (Xu et al., 2023b), MiniGPT-4 (Zhu et al., 2023), LLaVA\n(Liu et al., 2023b), and Shikra (Chen et al., 2023a).\n6\nEXPERIMENTS\nTo verify the effectiveness of the proposed methods, we carry out quantitative evaluations against\na set of recent baselines. Our quantitative evaluations are divided into three parts: textual response\ngeneration, image generation, and a holistic evaluation of multimodal instruction-following.\n6.1\nTEXTBINDEVAL\nTo facilitate comprehensive and dedicated evaluation for instruction-following in realistic scenarios,\nwe construct a new dataset named TEXTBINDEVAL. TEXTBINDEVAL is initially generated through\nthe automatic pipeline of TEXTBIND (\u00a73) and subsequently refined by human annotators. These\nannotators are tasked with discarding low-quality examples or rectifying amendable issues such\nas revising incoherent or hallucinated content. After a rigorous review, we establish an evaluation\ndataset comprising 278 conversations in total.\n6.2\nTEXTUAL RESPONSE GENERATION\nSetup\nWe consider each assistant turn of each conversation in TEXTBINDEVAL as a test point. All\nits preceding context is treated as input (which may contain interleaved images and text), and the\ngoal is to generate a coherent and helpful response. We measure the response quality using a set of\nreference-based evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and\nBERTScore (Zhang et al., 2020). We also report the Diversity (Su et al., 2022) scores of the generated\nresponses. For simplicity, we replace any image in the responses with a special token <image>.\nFor a fair comparison, we compare different MIM models trained on different datasets (Xu et al.,\n2023b; Zhu et al., 2023; Liu et al., 2023b; Chen et al., 2023a)4 and GILL (Koh et al., 2023a)5. The\nimplementation details are shown in Appx. H.\nResults\nAs shown in Tab. 3, the MIM model trained on TEXTBIND outperforms all other baselines\nby wide margins across all evaluation metrics. The results suggest that more realistic and diverse\ntraining data such as TEXTBIND is necessary for tackling open-world tasks, which cannot be well-\nsupported by existing template-based and VQA-like datasets. Nevertheless, we also find that the\n4The original papers of these datasets used distinct model architectures such as different pretrained language\nmodels. One common feature is that all of them do not support image generation.\n5For a fair comparison, we replicate GILL using the same image-captioning data to train by our models.\n7\nPreprint\nMethods\nBLEU-2\nBLEU-4\nROUGE-2\nROUGE-L\nBERTScore\nDiversity\nGILL (Koh et al., 2023a)\n3.97\n1.44\n4.61\n13.97\n0.847\n0.902\nMultiInstruct (Xu et al., 2023b)6\n7.16\n2.27\n3.16\n10.60\n0.830\n0.654\nMiniGPT-4 (Zhu et al., 2023)\n9.24\n3.29\n6.77\n17.56\n0.858\n0.658\nLLaVA (Liu et al., 2023b)\n12.16\n4.41\n8.66\n19.79\n0.872\n0.852\nShikra (Chen et al., 2023a)\n10.37\n3.83\n7.79\n18.63\n0.864\n0.722\nTEXTBIND\n24.45\n11.83\n15.45\n28.69\n0.891\n0.927\nMix\n27.64\n14.49\n17.90\n31.22\n0.896\n0.912\nTable 3: Evaluation of textual response generation. Mix represents the mixture of MultiInstruct,\nMiniGPT-4, LLaVA, Shikra, and TEXTBIND.\nperformance can be further improved when combining different datasets, indicating that there is a\ncomplementary relationship between TEXTBIND and existing datasets.\n6.3\nIMAGE GENERATION\nSetup\nThe models trained on existing datasets, i.e., the baselines in \u00a76.2 except for GILL, are\nincapable of generating images. To showcase the image generation capabilities of our model, we\ncompare it with Stable Diffusion XL (SD-XL) (Podell et al., 2023) and GILL (Koh et al., 2023a). In\naddition, we present the results of the two model variants described in \u00a75.1, namely, Q-former as\nMedium and Q-former with Prompt Tokens as Medium.\nWe take each image from the assistant in TEXTBINDEVAL as a test point. All its preceding context\nis taken as input, and the models are enforced to output an image. We take the original images in\nTEXTBINDEVAL as references. Following Koh et al. (2023a), we evaluate image generation with\ntwo reference-based metrics: (1) CLIP Similarity. We use the CLIP vision encoder to produce\nimage representations and compute the cosine similarity between generated images and reference\nimages. A higher score means better semantic similarity. (2) Learned Perceptual Image Path\nSimilarity (LPIPS). LPIPS (Zhang et al., 2018) measures the distance between generated images\nand reference images. A lower score means that images are more similar in perceptual space. (3)\nFrechet Inception Distance (FID). FID measures the distributional difference between the generated\nimages and reference images. A lower score indicates better resemblance to reference images.\nResults\nTo gain further insights into the multi-turn instruction-following abilities, we group different\ntest points by the number of previous conversation turns. The results are shown in Tab. 6. As seen,\nMIM generally achieves better performance than SD-XL and GILL across different turns and\nevaluation metrics. Importantly, the performance gaps are enlarged as the number of turns increases.\nThis indicates that our model exhibits a better understanding ability of multi-turn conversations.\nCompared to the two model variants, MIM is substantially better. Our case study reveals that the\ndisparity stems from the one-to-many nature of image generation in real-world conversations. Unlike\ngenerating images for explicit descriptions, there can exist numerous distinct images for a given\nconversation context. Operating in the hidden space may inadvertently average all possibilities,\nresulting in ambiguous or noisy images. However, MIM mitigates the one-to-many issue by taking\nfull advantage of the autoregressive generation of language models for decision-making.\n6.4\nHOLISTIC EVALUATION\nIn addition to the above automatic evaluation, we also conduct a holistic evaluation of instruction-\nfollowing abilities through human annotation. To further show where the derived dataset and training\nhelps, we ask human annotators to evaluate the quality of the generated responses in terms of three\nfine-grained dimensions: instruction-following (fulfill the intent of users), multi-modal context\nunderstanding (correctly understand the information in text and images), and the informativeness of\nthe generated responses. For each dimension, a human annotator will assign a score in {1, 2, 3, 4}.\nThe four scores ranging from 1 to 4 indicate \u201cmajor error\u201d, \u201cminor error\u201d, \u201cacceptable\u201d, and \u201cperfect\u201d,\nrespectively. We compare TEXTBIND with LLaVA (the second best model in our holistic evaluation\nin 6) on 100 randomly sampled data. As shown in the Table 5, the model trained on TEXTBIND\ncan better follow the instructions of humans and leverage the multi-modal context. Notably, the\ninformativeness of model trained on TEXTBIND is comparable with that trained on LLaVA.\n8\nPreprint\nModel\nInstruction-following\nMultimodal Context Understanding\nInformativeness\nLLaVA (Liu et al., 2023b)\n3.59\n3.56\n3.78\nTEXTBIND\n3.99\n3.82\n3.72\nTable 5: Fine-grained analysis using human evaluation.\nCLIP Similarity (\u2191)\nLPIPS (\u2193)\nFID (\u2193)\nModel\nTurn-1\nTurn-2\nTurn-3\nTurn-1\nTurn-2\nTurn-3\nAll\nSD-XL (Podell et al., 2023)\n0.612\n0.599\n0.608\n0.712\n0.735\n0.735\n144.76\nGILL (Koh et al., 2023a)\n0.569\n0.550\n0.530\n0.712\n0.734\n0.742\n158.64\nQ-Former as Medium\n0.558\n0.568\n0.592\n0.717\n0.728\n0.729\n155.01\nQ-Former with Prompt Tokens as Medium\n0.566\n0.571\n0.606\n0.718\n0.727\n0.732\n152.23\nMIM\n0.640\n0.645\n0.673\n0.712\n0.720\n0.726\n139.46\nTable 6: Evaluation of image generation.\nMethods\nAVG. Score\nPercent. (\u2265 3)\nGILL\n1.71\n0.19\nLLaVA\n2.93\n0.89\nMIM\n3.39\n0.70\nTable 4: Averaged human scores and the\npercentage of averaged scores \u2265 3. Krip-\npendorff\u2019s \u03b1 = 0.75.\nSetup\nWe\nrandomly\nsample\n100\ncontexts\nfrom\nTEXTBINDEVAL and evaluate the responses generated\nby MIM and two representative baselines, LLaVA (Liu\net al., 2023b) and GILL (Koh et al., 2023a). We instruct\nthree human annotators to score the quality of each gen-\nerated response on a Likert scale from 1 to 4 (The details\nof evaluation guideline are in Appx. I).\nResults\nAs shown in Table 4, MIM achieves higher hu-\nman scores than GILL and LLaVA, indicating its remark-\nable generation capability in open-world multi-modal conversations. In addition, the Krippendorff\u2019s\n\u03b1 = 0.75 indicates a high inter-annotation agreement between annotators.\n6.5\nRESULTS ON EXISTING BENCHMARK\nFinally, we report the results on two popular multimodal benchmarks, MME (Fu et al., 2023),\nMMBench (Liu et al., 2023c), and MM-Vet (Yu et al., 2023). As shown in Tab. 7, TEXTBIND gets a\nrelatively lower score than other datasets. The reason stems from the intrinsic difference between\nTEXTBIND and the two benchmarks. TEXTBIND focuses more on realistic instructions (e.g., create a\nstory based on the images, give some suggestions for having fun in the winter). In contrast, MME,\nMMBench and MM-Vet focus more on VQA questions, e.g., who is this person, what is the color\nof the object, which are more similar to the data in MultiInstruct, LLaVA, and Shikra. For example,\nthe model trained on MultiInstruct achieves the best performance on MME, though it displays the\nworst performance in open-world scenarios in Tab. 3. Another interesting observation is that the mix\nof all datasets attains the best overall performance on MMBench, indicating that different datasets\nare complementary. In other words,the capabilities that TextBind can bring are almost orthogonal to\nexisting multimodal instruction-following datasets.\nMME\nMMBench\nMM-Vet\nTraining Dataset\nPerception\nCognition\nLR\nAR\nRR\nFP-S\nFP-C\nCP\nOverall\n-\nMultiInstruct (2023b)\n1099.16\n302.50\n11.93\n39.79\n28.64\n28.75\n23.20\n41.91\n31.54\n17.2\nMiniGPT-4 (2023)\n0.00\n0.00\n14.20\n50.52\n17.37\n32.75\n15.20\n41.70\n31.87\n9.8\nLLaVA (2023b)\n683.28\n267.86\n7.95\n55.71\n31.46\n42.50\n31.60\n56.60\n42.10\n23.4\nShikra (2023a)\n166.87\n2.86\n18.18\n64.01\n22.54\n39.75\n31.20\n50.43\n41.10\n19.9\nTEXTBIND\n549.00\n226.43\n11.93\n36.33\n6.57\n23.25\n6.00\n33.83\n22.64\n19.4\nMix\n1023.33\n255.00\n13.64\n56.75\n37.09\n43.50\n42.80\n55.32\n44.94\n23.9\nTable 7: Results on MME (Fu et al., 2023), MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023).\n9\nPreprint\n7\nCONCLUSION\nIn conclusion, the introduction of the TEXTBIND framework has opened new doors for enhancing\nlarge language models with multi-turn interleaved multimodal instruction-following capabilities. By\nrequiring only image-caption pairs, our approach significantly reduces the need for high-quality\nexemplar data, making it a more accessible and scalable solution for various real-world tasks. The\nMIM architecture seamlessly integrates image encoder and decoder models, enabling the model\nto effectively handle interleaved image-text inputs and outputs. Comprehensive quantitative and\nqualitative experiments demonstrate the remarkable performance of MIM, trained on TEXTBIND,\nwhen compared to recent baselines in open-world multimodal conversations.\nREFERENCES\nHarsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson,\nDhruv Batra, Devi Parikh, and Stefan Lee. nocaps: novel object captioning at scale. In 2019\nIEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),\nOctober 27 - November 2, 2019, 2019.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35, 2022.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle,\nMarc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances\nin Neural Information Processing Systems 33: Annual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, 2021.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\nmultimodal llm\u2019s referential dialogue magic. ArXiv preprint, abs/2306.15195, 2023a.\nLiangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell,\nand Ziwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foundation Models, 2023b.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. ArXiv\npreprint, abs/1504.00325, 2015.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning. ArXiv preprint, abs/2305.06500, 2023.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian\nSun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.\nDreamllm: Synergistic multimodal comprehension and creation. ArXiv preprint, abs/2309.11499,\n2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM:\nGeneral language model pretraining with autoregressive blank infilling. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022.\n10\nPreprint\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei\nLin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive\nevaluation benchmark for multimodal large language models. ArXiv preprint, abs/2306.13394,\n2023.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making\nllama see and draw with seed tokenizer. ArXiv preprint, abs/2310.01218, 2023.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in\nVQA matter: Elevating the role of image understanding in visual question answering. In 2017\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017, 2017.\nDanna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and\nJeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In\n2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018, 2018.\nDrew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual reasoning\nand compositional question answering. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, 2019.\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. Billion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3), 2019.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. ArXiv preprint, abs/2305.17216, 2023a.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for\nmultimodal inputs and outputs. ICML, 2023b.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. ArXiv preprint, abs/2305.03726, 2023a.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image\npre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesv\u00b4ari, Gang Niu, and Sivan Sabato (eds.), International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning Research, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. ArXiv preprint, abs/2301.12597,\n2023b.\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction\ntuning. ArXiv preprint, abs/2306.04387, 2023c.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, 2004.\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating halluci-\nnation in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565,\n1, 2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv\npreprint, abs/2304.08485, 2023b.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\nArXiv preprint, abs/2307.06281, 2023c.\n11\nPreprint\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. Advances in Neural Information Processing Systems, 35, 2022.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual\nquestion answering benchmark requiring external knowledge. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, 2019.\nDavid Marr. Vision: A computational investigation into the human representation and processing of\nvisual information. 2010.\nSeungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain,\nChun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and\nscalable any-modality augmented language model. ArXiv preprint, abs/2309.16058, 2023.\nOpenAI. Introducing chatgpt. 2022.\nOpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million\ncaptioned photographs. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N.\nPereira, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems\n24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a\nmeeting held 12-14 December 2011, Granada, Spain, 2011.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35, 2022.\nXichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Gen-\nerating images in context with multimodal large language models. ArXiv preprint, abs/2310.02992,\n2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association\nfor Computational Linguistics, 2002.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. ArXiv preprint, abs/2307.01952, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, 2021.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza-\ntions enable training deep learning models with over 100 billion parameters. In Rajesh Gupta, Yan\nLiu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD \u201920: The 26th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, 2020.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 2022.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V.\nNayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,\nSheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,\nJos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00b4evry, Jason Alan Fries, Ryan Teehan,\nTeven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask\nprompted training enables zero-shot task generalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n12\nPreprint\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. ArXiv preprint, abs/2303.17580, 2023.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. Towards VQA models that can read. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, 2019.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive\nframework for neural text generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang,\nXiubo Geng, and Daxin Jiang. Multimodal dialogue response generation. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n2022.\nD\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. ArXiv preprint, abs/2303.08128, 2023.\nInternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities.\nhttps://github.com/InternLM/InternLM-techreport, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. ArXiv preprint, abs/2302.13971, 2023.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), 2023.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,\n2022, 2022.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, 2020.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-\nsual chatgpt: Talking, drawing and editing with visual foundation models.\nArXiv preprint,\nabs/2303.04671, 2023a.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal\nllm, 2023b.\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models. ArXiv preprint, abs/2306.09265, 2023a.\nZhiyang Xu, Ying Shen, and Lifu Huang. MultiInstruct: Improving multi-modal zero-shot learning via\ninstruction tuning. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2023b.\n13\nPreprint\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning\nand action. ArXiv preprint, abs/2303.11381, 2023.\nZhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad\nAwan, Samyam Rajbhandari, and Yuxiong He. DeepSpeed-VisualChat: Multi-Round Multi-Image\nInterleave Chat via Multi-Modal Causal Attention. ArXiv preprint, abs/2309.14327, 2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. ArXiv preprint, abs/2304.14178, 2023.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions\nof the Association for Computational Linguistics, 2, 2014.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\nand Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. ArXiv\npreprint, abs/2308.02490, 2023.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\nArXiv preprint, abs/2303.16199, 2023.\nRichard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, 2018.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating\ntext generation with BERT. In 8th International Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\nKaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation\nvia generative vokens, 2023.\nLuowei Zhou, Chenliang Xu, and Jason J. Corso. Towards automatic learning of procedures from\nweb instructional videos. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings\nof the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018,\n2018.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. ArXiv preprint,\nabs/2304.10592, 2023.\nXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. ArXiv\npreprint, abs/2212.11270, 2022.\n14\nPreprint\nA\nDEMONSTRATIONS\nThe four high-level characteristics of the TEXTBIND dataset equips MIM with a variety of capabilities.\nWe demonstrate those capabilities with concrete user cases.\nImage Creation\nOne core innovation of TEXTBIND is that it enables the model to create images\nbased on the conversation context without explicit dictations from the users. This characteristic is\nextremely useful for open-world scenarios, because in many cases people may just have an implicit\nintention and have no clear thoughts about what the images should be. We observe that our model\ncan explain concepts and ideas for users with vivid images (Figure 5a), creating images with correct\nemotions (Figure 5b), and editing images based on the whole context (Figure 5c and 5d). Furthermore,\nas shown in Figure 4, we discover that our model is proficient in generating long stories featuring\ninterleaved text and images while maintaining exceptional coherence.\nImage Comparison\nAnother interesting feature of TEXTBIND is that it can compare or relate\nthe information in multiple images. For example, our model can correctly explain the different and\ncommon parts between images in Figure 6.\nIntrinsic & Extrinsic Image Understanding\nThe model trained on TEXTBIND can understand\nthe content in images precisely in a multi-turn conversation. In all the three sub-figures of Figure 7,\nthe model precisely follows the human instructions and explains the details of the images to users.\nMoreover, TEXTBIND also enables the model to explore the meaning of an image beyond the symbols\nin it. For example, the model also explains the the influence of Bob Dylan\u2019s album in Figure 7b and\nthe impact of iPhone in Figure 7c.\n1\n2\n3\nFigure 4: Generation of a long story with interleaved text and images.\n15\nPreprint\n(a) Explaining concepts with multiple images.\n(b) Creating images with correct emotions.\n(c) Editing images based on context.\n(d) Creating images based on context.\nFigure 5: User cases of creating images.\n16\nPreprint\n(a) Comparing music styles.\n(b) Relating images.\n(c) Comparing movies.\n(d) Comparing different concepts.\nFigure 6: User cases of comparing images.\n17\nPreprint\n(a)\n(b)\n(c)\n(d)\nFigure 7: User cases of understanding both intrinsic & extrinsic information in the images.\n18\nPreprint\nB\nPROMPT OF TEXTBIND\nGPT-4 Prompt\nPlease construct a dialogue between a human and a helpful, honest and harmless assistant. The dialogue\ncontains interleaved text and images. Each image is represented by <imgX> DESCRIPTION </imgX>,\nwhere DESCRIPTION is a textual description of the image and X is an index of the image. Please do not\nassume any further visual information beyond the description.\nThe constructed dialogues must and can only contain the following input images:\n<img0> museum - the 1st nuclear submarine </img0>\n<img1> response to the production of heavy </img1>\nCharacteristics about the assistant:\n1. The assistant is trained to understand text, images, and their combinations.\n2. The assistant can reply the human with images and/or text.\n3. The assistant has exceptional world knowledge and commonsense reasoning capabilities.\n4. The assistant does not have access to the Internet or any other external tools.\n5. If the assistant is asked to create an image, it can only show the image in the provided image list.\n6. Please do not copy the images appearing in the dialogue. The assistant should refer to the previously\nmentioned image by natural language.\nCharacteristics about the human:\n1. The human may send images and/or text to the assistant.\n2. The human may ask questions requiring visual reasoning and/or understanding the relations between\nmultiple images.\n3. The human may ask the assistant to show images based on his/her intention.\n4. The human may ask the assitant to do interesting things, rather than simply describing the content of the\nimage.\nProperties of a bad dialogue:\n1. Simply describing or analyzing the content in the image.\n2. Dialogue without a good logic.\nProperties of a good dialogue:\n1. Introducing extrinsic and attractive information of stuff shown in the image.\n2. Discovering the connection between multiple images.\n3. The dialogue happens in a reasonable context.\nExamples:\n========\nCase 1:\nInput Images: ...\nOutput Dialogue: .. .\n\u2014\u2014\u2014\nCase 2:\nInput Images: ...\nOutput Dialogue: .. .\n\u2014\u2014\u2014\nCase 3:\nInput Images: ...\nOutput Dialogue: .. .\n======\nPlease directly give the dialogue if you understand. The number of turns of the dialogue should be less\nthan 6. The dialogue should be self-contained. Do NOT assume any previous dialogue between the human\nand the assistant. Please use the same format <imgX> DESCRIPTION </imgX> to denote images in\nthe dialogue and do not modify the description of the image. Note that the dialogue can stop at any time\nwithout an explicit ending.\nFigure 8: Illustration of the prompt used for TEXTBIND.\n19\nPreprint\nC\nEXAMPLES OF CONSTRUCTED CONVERSATIONS\nGPT-4 Ouput\nExample #1\nHuman: Hi! I\u2019m trying to write a cartoon story for my kids featuring a grumpy clown and a lovable\ncupcake. Can you help me craft this?\nAssistant: Of course! That sounds like a wonderful idea. We can start by establishing our characters. The\ngrumpy clown could be someone who struggles finding joy in his own performances. <img0> a cartoon\nillustration of a clown looking angry </img0>\nHuman: That\u2019s a wonderful idea! How about the cute cupcake?\nAssistant: The cupcake, on the other hand, is always upbeat and tries to help the clown find his happiness.\nDespite being a tiny cupcake, it has a big heart and a contagious joy. <img1> cartoon illustration of a\ncupcake with a happy expression </img1>\nHuman: Interesting! What should be their adventure?\nAssistant: They could go on a journey to find the ultimate joke that guarantees laughter. The lovable\ncupcake, with his relentless positivity and cheer, helps the grumpy clown see the lighter side of life and\nfinally find his joy.\nExample #2\nHuman: Hi, I am researching small business operations. Can you show me an example of informal small\nbusiness?\nAssistant: Of course. Here\u2019s an example of informal small business: <img0> a mother and daughter\nselling gum and cigarettes in person </img0>. This concept of business is most commonly found in rural\nor underdeveloped areas where formal establishments might not be as prevalent.\nHuman: Interesting! On a somewhat related note, what sells well in such arrangements?\nAssistant: It depends on the location and the demand. Mostly, such businesses sell products of daily use\nsuch as food items, candy, toiletries etc. For example an ice cream cart is a common sight in many areas,\nas it\u2019s a popular treat among people of all ages.\nHuman: Can you show me an image of a kid indulging in something like that?\nAssistant: Absolutely! Here\u2019s a delightful image of <img1> a child eating ice cream </img1>. Ice\ncream and similar snacks are often popular products for small informal businesses.\nExample #3\nHuman: Hi there! I\u2019m preparing a speech on unity and cooperation in global issues. I have a few images\nbut I\u2019m not sure how to describe them in a way that fits my theme.\nAssistant: I\u2019d be happy to help with that! Could you show me the images?\nHuman: Sure, here they are. <img0> large group of people in the shape of flag </img0> and <img1>\nrear view of a male boxer holding globe with flag painted on his back </img1>\nAssistant: The first image can be seen as a symbol of unity. Here, numerous individuals come together\nto form the shape of a flag, showing that collective action can lead to a powerful representation, like a\nflag - often a symbol of identity and unity. The second image illustrates the idea of global responsibility\nand commitment. The male boxer, suggesting strength and determination, has a globe \u2013 representing the\nworld \u2013 in his hands, which could indicate a keen sense of global responsibility. The flag painted on his\nback can symbolize allegiance or commitment to his country, and by extension to global harmony if we\u2019re\nexpanding the theme to encompass international cooperation.\nHuman: That sounds great! I also have this image. Not sure if it fits. <img2> diplomatic handshake\nbetween countries : flags overprinted the hands stock photo </img2>\nAssistant: This image very much fits your theme. It can represent a formal agreement or cohesion between\ncountries. The flags overprinted on the shaking hands signify the countries involved in the meet. This\nimage can be used to illustrate diplomatic dialogue or mutual understanding which is an integral component\nto solving global issues through cooperation and unity.\nFigure 9: Illustration of in-context examples of TEXTBIND.\nD\nCOMPARISON WITH PREVIOUS DATASETS.\nWe further investigate the lexical diversity of generated data, including both instructions and responses.\nThe diversity of text in a dataset is defined as P4\nn=2( #Unique n-grams\n#Total n-grams ), in line with previous works (Su\n20\nPreprint\nDataset\nMulti-turn\nArbitrarily\nInterleaved\nMultimodal\nInstruction\nFollowing\nImage\nGeneration\nPLATYPUS\n\u2717\n\u2717\n\u2717\n\u2713\n\u2717\nFLAMINGO\n\u2717\n\u2713\n\u2713\n\u2717\n\u2713\nMULTIINSTRUCT\n\u2717\n\u2717\n\u2713\n\u2713\n\u2717\nMINIGPT-4\n\u2717\n\u2717\n\u2713\n\u2713\n\u2717\nLLAVA\n\u2713\n\u2717\n\u2713\n\u2713\n\u2717\nTEXTBIND\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 8: Comparison with previous datasets.\nIt's so beautiful outside! Look at the scenery:\nI want to go outside and have some fun. Do you have\nany suggestions?\nYes\nThe image shows a snowy landscape with a pond in\nthe foreground and a pagoda in the background. The\npond is surrounded by snow-covered trees and rocks,\nand there is a path leading to the pagoda. The sky is\nclear and blue, with some fluffy clouds in the\ndistance. The overall mood of the image is peaceful\nand serene.\nSure! You could try playing a game of snowball fight\nwith your friends. It's a fun and exciting way to\nenjoy the snow. Here's an image of a snowball fight:\nMiniGPT-4\nMultiInstruct\nTextBind\n(ours)\nRecently, I've been really interested in them:\nCan You create a fun story based on these two characters?\nYes\nThe image shows a man in a red and black costume standing on a\nrooftop with a city in the background. The man is wearing a mask and\nhas a serious expression on his face. The city is lit up by the sun, with\nbuildings and streets visible in the background. The man is holding a\nsword in his hand and appears to be ready to fight.\nSure! Once upon a time, in a world of superheroes, there was a\nlegendary hero named Spiderman. The Spiderman was known for his\nincredible strength, agility, and his ability to climb walls and ceilings.\nHe was a true hero, always ready to save the day. One day, while\nSpiderman was patrolling the city, he stumbled upon a mysterious\nfigure. The figure was a powerful sorcerer, who had the ability to\ncontrol the elements. The sorcerer was known as the Monkey King. The\nMonkey King was a mischievous character, always causing trouble and\nchaos wherever he went. He had a pet monkey, who was also his loyal\ncompanion. The Monkey King and Spiderman had a long history of\nbattles. They were always at odds, with the Monkey King trying to\noutsmart Spiderman and cause chaos in the city. One day \u2026\nGiven the beautiful snowy scenery, there are plenty\nof\nfun\nactivities\nyou\ncan\nenjoy\noutside.\nSome\nsuggestions include:\n1. Going for a walk or a jog along the river, taking in\nthe serene winter landscape.\n2. Building a snowman or snow fort in the snow-\ncovered park.\n3. Having a snowball fight with friends or family.\n4. Taking a leisurely skate on the frozen river or\npond, enjoying the crisp winter air.\n5. Exploring the snow-covered garden, admiring the\nwinter-adapted plants and trees\u2026\nLLaVA\nOnce upon a time, in a bustling city filled with skyscrapers, a young\nman dressed in a red and black costume, resembling a monkey, was\nperched on top of a building. He was a superhero, known as the Monkey\nMan, and he had just saved the city from an evil villain.\nAs he rested on the rooftop, he noticed a group of people gathered\nbelow, admiring his heroic feat. The Monkey Man felt a sense of pride\nand satisfaction, knowing that his actions had made a difference in the\nlives of the people he protected. \nSuddenly, a new challenge emerged as a group of thieves snuck into the\ncity, attempting to steal valuable items from the residents. The\nMonkey Man sprang into action, using his incredible strength and\nagility to chase the thieves and bring them to justice \u2026\nUser\nFigure 10: Comparison among models trained on different datasets.\net al., 2022). As shown in Tab. 2, our dataset achieves better diversity scores than most existing\nvisual-language datasets, indicating that the language used in our dataset is more informative.\nE\nHUMAN ANNOTATION GUIDELINE.\nThe comprehensive guideline for human evaluation is shown in Table 9.\nF\nIMPLEMENTATION DETAILS (DATA)\nWe construct our TEXTBIND dataset based on the CONCEPTUAL CAPTIONS 3M (CC3M) (Sharma\net al., 2018; Changpinyo et al., 2021) dataset, which only provides image-caption pairs. In our\nexperiments, we employ the clip-vit-base-patch16 model released by OpenAI7 (Radford et al.,\n2021) to filter out image-caption pairs with matching scores lower than 30. We use the k-means clus-\n7https://huggingface.co/openai/clip-vit-base-patch16\n21\nPreprint\nAnnotation\nLabels\nDescription\nOverall Quality\nExcellent\nThis conversation is very interesting, practical, or intricate.\nSatisfactory\nThis conversation is coherent and reasonable without any factual errors.\nPoor\nAt least one turn in the conversation is unreasonable in some aspects,\ne.g., unrealistic content, illegal formats, etc.\nRequired Abilities\nImage Creation\nTo create new images in appropriate contexts.\nImage Comparison\nTo combine, relate, or compare the information in different images.\nIntrinsic Image Understanding\nTo identify and recognize the objects, colors, shapes, and patterns in\nimages.\nExtrinsic Image Understanding\nTo interpret the underlying meaning of images, e.g., the context, emo-\ntions, symbolism, or narrative conveyed by the images. It goes beyond\nmerely recognizing the elements in the images and often requires ex-\nternal knowledge and/or deep analysis.\nTable 9: Human annotation guideline.\nTraining Stage\nEpoch\nLearning Rate\nBatch Size\nMax Sequence Length\nTraining Modules\nMultimodel Alignment\n2\n1e-4\n256\n256\nQ-Former, Linear\nMultimodel Instruction Following\n3\n1e-5\n64\n768\nQ-Former, Linear, LLM\nTable 11: Training Configures of our Experiments\ntering algorithm implemented by FAISS (Johnson et al., 2019) toolkit to classify the cleaned CC3M\ndataset into 4096 clusters. The features used for k-means clustering are the hidden representations of\nimages encoded by clip-vit-base-patch16 model. In addition, clusters with less than 32 images\nare regarded as outliers and will not be considered. The number of images in each conversation is\nsampled from {2, 3, 4}. We access the GPT-4 model through the OpenAI API8, and set top p and\ntemperature hyper-parameters to 1.0.\nG\nCONSTRUCTED CONVERSATIONS WITH \u201cPOOR\u201d LABEL\nIn Table 10, we identify three typical errors present in the constructed dataset. Despite setting a high\nthreshold to filter out mismatched image-caption pairs, some mismatched cases cannot be detected\nby the CLIP model (Radford et al., 2021). A few conversations suffering from incoherence and\nhallucinations may be attributed to the GPT-4 model. Overall, while a small number of conversations\nare affected by errors that are difficult to detect using rules, most generated conversations exhibit high\nquality. We present several cases labeled with \u201cPoor\u201d. We can find that most of those \u201cPoor\u201d cases\nonly have minor and non-obvious problems.\nH\nIMPLEMENTATION DETAILS (MODEL)\nError Type\nPercentage\nImg-Cap Mismatch\n0.03\nIncoherence\n0.03\nHallucination\n0.03\nTable 10: Error types and percentage in dataset\nconstructed by TEXTBIND.\nOur experiments are based on Huggingface\nTransformers9 (Wolf et al., 2020) and Deep-\nSpeed10 (Rasley et al., 2020). We use the fil-\ntered synthetic captions given by BLIP11 (Li\net al., 2022), including Conceptual Captions\n(Changpinyo et al., 2021; Sharma et al., 2018)\nand SBU (Ordonez et al., 2011), totally 12M\nimage-caption pairs. We employ the same vi-\nsual encoder and Q-Former as used in BLIP-2\n(Li et al., 2023b) and use their weights for initial-\nization. LLama2-Chat12 (Touvron et al., 2023)\nis utilized as the backbone language model. For the image generation model, we use Stable Diffusion\n8https://openai.com/blog/openai-api\n9https://huggingface.co/docs/transformers/index\n10https://github.com/microsoft/DeepSpeed\n11https://github.com/salesforce/BLIP\n12https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n22\nPreprint\nXL13 (Podell et al., 2023). The training configures are shown in Table 11. We use 8 NVIDIA A100\n(40G) GPUs for all experiments.\nI\nHUMAN EVALUATION GUIDELINE\n\u2022 Score 4: The response is excellent.\n\u2022 Score 3: The response is acceptable but may not be very informative and interesting.\n\u2022 Score 2: The response has minor issues, such as slight hallucinations when describing the\nimages in the context.\n\u2022 Score 1: The response is invalid and has significant drawbacks, e.g., irrelevant to the context.\nJ\nDATA FORMAT OF MODEL INPUT\n1\n{\n2\n\"conversation\" {\n3\n{\n4\n\"role\": \"user\",\n5\n\"content\": \"<image > Do you...\",\n6\n\"image_list\": [\"1235. png\"]\n7\n\"caption_list\": [\"a runing husky ...\"]\n8\n},\n9\n{\n10\n\"role\": \"assistant\",\n11\n\"content\": \"Yes , I do! ...\",\n12\n\"image_list\": [],\n13\n\"caption_list\": []\n14\n},\n15\n...\n16\n}\n17\n}\n13https://github.com/Stability-AI/generative-models\n23\nPreprint\n(a) Hallucination Case: The topic is about rain\nboot. However, not all the boots shown in the\nsecond image are rain boots.\n(b) Incoherence Case: The first turn discusses\nabout relaxing while the second turn is about\noutdoor activities. There are no connections.\n(c) Image-caption Mismatch Case: The first im-\nage only shows a bathroom, but the caption is\n\u201cdouble room with private bathroom in a cot-\ntage\u201d.\nFigure 11: Constructed conversations with \u201cPoor\u201d Label. The caption is shown below the image with\ngray color.\n24\n"
  },
  {
    "title": "A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale",
    "link": "https://arxiv.org/pdf/2309.06497.pdf",
    "upvote": "4",
    "text": "A Distributed Data-Parallel PyTorch Implementation of the\nDistributed Shampoo Optimizer for Training Neural\nNetworks At-Scale\nHAO-JUN MICHAEL SHI\u2217, Meta Platforms, Inc., USA\nTSUNG-HSIEN LEE\u2217, Independent Researcher, USA\nSHINTARO IWASAKI\u2020, Meta Platforms, Inc., USA\nJOSE GALLEGO-POSADA\u2021, Mila & University of Montreal, Canada\nZHIJING LI, Meta Platforms, Inc., USA\nKAUSHIK RANGADURAI, Meta Platforms, Inc., USA\nDHEEVATSA MUDIGERE\u00a7, NVIDIA Corporation, USA\nMICHAEL RABBAT, Meta Platforms, Inc., USA\nShampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods\nfor training neural networks. It constructs a block-diagonal preconditioner where each block consists of a\ncoarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network.\nIn this work, we provide a complete description of the algorithm as well as the performance optimizations\nthat our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables\nfast multi-GPU distributed data-parallel training by distributing the memory and computation associated\nwith blocks of each parameter via PyTorch\u2019s DTensor data structure and performing an AllGather primitive\non the computed search directions at each iteration. This major performance enhancement enables us to\nachieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-\nscaling-based adaptive gradient methods. We validate our implementation by performing an ablation study on\ntraining ImageNet ResNet50, demonstrating Shampoo\u2019s superiority against standard training recipes with\nminimal hyperparameter tuning.\nOur code is available at github.com/facebookresearch/optimizers/tree/main/distributed_shampoo.\nAdditional Key Words and Phrases: stochastic optimization, online convex optimization, training algorithms,\ndeep learning, neural networks, PyTorch\n1\nINTRODUCTION\nAdaptive gradient methods (Adam(W), AdaGrad, RMSProp) have been widely adopted as the de-\nfacto methods for training neural networks across a range of applications, including computer\nvision, natural language processing, and ranking and recommendation [Dosovitskiy et al. 2021;\nNaumov et al. 2019; Zhang et al. 2022]. Originally motivated by the need for per-feature, sparsity-\naware learning rates [Duchi et al. 2011], these methods have proven to be especially useful because\ntheir hyperparameters are easier to tune with faster convergence in some cases.\n\u2217Both authors contributed to the original implementation of this work.\n\u2020Contributed to the primary distributed performance optimization implementation in this work.\n\u2021Work was performed while a visiting researcher at Meta Platforms, Inc. Performed experimental ablations.\n\u00a7Work was performed while at Meta Platforms, Inc.\nAuthors\u2019 addresses: Hao-Jun Michael Shi, hjmshi@meta.com, Meta Platforms, Inc., 1 Hacker Way, Menlo Park, California,\nUSA; Tsung-Hsien Lee, Independent Researcher, USA, tsung.hsien.lee@gmail.com; Shintaro Iwasaki, siwasaki@meta.com,\nMeta Platforms, Inc., 1 Hacker Way, Menlo Park, California, USA; Jose Gallego-Posada, Mila & University of Montreal,\n6666 Rue Saint-Urbain, Montreal, Quebec, Canada, josegp@meta.com; Zhijing Li, zhijing@meta.com, Meta Platforms, Inc.,\n1 Hacker Way, Menlo Park, California, USA; Kaushik Rangadurai, krangadu@meta.com, Meta Platforms, Inc., 1 Hacker\nWay, Menlo Park, California, USA; Dheevatsa Mudigere, dheevatsa@nvidia.com, NVIDIA Corporation, 2788 San Tomas\nExpressway, Santa Clara, California, USA; Michael Rabbat, mikerabbat@meta.com, Meta Platforms, Inc., 1 Hacker Way,\nMenlo Park, California, USA.\narXiv:2309.06497v1  [cs.LG]  12 Sep 2023\n2\nShi et al.\nThe most widely-used versions of adaptive gradient methods involve per-coordinate scaling,\nwhich is equivalent to applying a diagonal preconditioner to the stochastic gradient. When training\nlarge models typical of deep learning applications, which can have millions to billions of variables,\nit is tractable to store and apply optimizer states of this order. For example, the optimizers (diagonal)\nAdaGrad, RMSProp, and Adam(W) all make use of auxiliary states that combined are 2\u20133 times the\nsize of the model. The auxiliary state tracks either the sum or an exponentially-weighted moving\naverage of functions of each component of the gradient (e.g., the square of the component, or the\ncomponent\u2019s value itself).\nOn the other hand, it is known that there exists a version of AdaGrad where the preconditioner is\na dense full matrix, and this full-matrix version offers stronger theoretical convergence guarantees\nthan diagonal AdaGrad [Duchi et al. 2011]. Its state tracks the sum of the outer product of the\nstochastic gradient with itself. Consequently, the size of the full-matrix AdaGrad state is quadratic\nin the model size. Furthermore, the method requires inverting the preconditioner matrix, and so\nthe computional cost is cubic in the model size. Its high memory and computational costs renders\nfull-matrix AdaGrad impractical for deep learning.\nThe Shampoo algorithm [Anil et al. 2020; Gupta et al. 2018] is an adaptive gradient method for\ntraining deep neural networks that fills the gap between diagonal and full-matrix preconditioning\nby applying two approximations. First, it restricts to block-diagonal preconditioners, where each\nblock preconditions a single layer. Second, Shampoo leverages the special structure of neural\nnetwork gradients to form a Kronecker product approximation of each preconditioner block, further\nreducing the memory footprint. Combined, these approximations reduce the cost of Shampoo to\napproximately 4\u20137 times the model size, which makes Shampoo feasible for training networks at\nscale. Whereas diagonal adaptive gradient methods fail to capture any cross-parameter correlations,\nShampoo captures some of the correlations within each block. This has led to demonstrable\nimprovements in convergence over previous methods, and has enabled Shampoo\u2019s productionization\nfor real-world use-cases, such as in Google\u2019s ads recommendations systems [Anil et al. 2022].\nIt is worth noting that, although Shampoo involves preconditioning the (stochastic) gradient, the\nmotivation behind Shampoo and full-matrix AdaGrad is distinct from second-order Newton-type\nmethods. Newton-based methods approximate a smooth function locally using Taylor expansions\nto achieve fast local convergence near a minimizer. On the other hand, adaptive gradient methods\nlike AdaGrad are motivated by the design of preconditioners to maximally decrease the distance\nto the solution after a fixed number of iterations, specifically for convex non-smooth functions.\nFurthermore, in machine learning applications, there is a greater emphasis on the initial behavior\nof the training dynamics, as opposed to other applications of nonlinear programming, which place\ngreater importance on obtaining high-accuracy solutions and fast local convergence [Bottou et al.\n2018].\nThe contribution of this paper is the description and design of a PyTorch implementation of the\nDistributed Shampoo algorithm. It is designed specifically for distributed data-parallel training using\nPyTorch\u2019s DistributedDataParallel module, where each worker only computes a local subset of\ngradients (called the local batch), and the global mini-batch gradient is aggregated across workers.\nUnlike the JAX implementation, which is optimized for heterogeneous TPU/CPU architectures [Anil\nand Gupta 2021], the PyTorch implementation is optimized for homogeneous GPU architectures.\nUnder standard data parallelism, the cost of the optimizer step is assumed to be marginal relative\nto the forward and backward passes on the network, and therefore the computation is replicated\nacross all workers. Indeed, these optimizers\u2019 inherent simplicity (implemented through element-\nwise operations) have enabled highly performant (arguably, ideal) implementations via horizontal\nand vertical fusion; see NVIDIA\u2019s APEX optimizers as an example [NVIDIA 2019].\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n3\nFig. 1. Outline of each distributed data-parallel iteration with the Distributed Shampoo optimizer.\nInstead, because Shampoo significantly increases the total amount of FLOPs-per-iteration by\nreplacing element-wise operations with matrix operations, Shampoo requires a different set of\nperformance optimizations in order to remain competitive with standard diagonal adaptive gradient\nmethods in terms of wall-clock time. Rather than replicating the optimizer state and computation\nacross all workers, as with standard diagonal scaling methods, our implementation distributes the\noverall memory and compute of each Shampoo update, only requiring each worker to compute\na subset of the search directions (with respect to each parameter) based on a pre-determined\ngreedy assignment, similar to the ZeRO-1 optimizer in [Rajbhandari et al. 2020]. After each worker\ncompletes their portion of the work, the search directions for each parameter are collected across\nall workers; see Figure 1. This enables a performant implementation of Shampoo that is practically\napplicable for large-scale deep learning training by incurring at most a 10% increase in wall-clock\ntime per-iteration relative to diagonal adaptive gradient methods.\nFor machine learning engineers and scientists, this performant implementation offers two\npotential measurable benefits: (1) faster convergence (in terms of number of iterations and wall-\nclock time) to a model of the same quality; and/or (2) a nontrivial improvement of the model quality\nafter a fixed number of iterations, with additional training costs but no increase in inference and\nserving costs.\n1.1\nMain Contributions\nThe main contributions of this paper are three-fold:\n(1) We provide a complete characterization of the Distributed Shampoo algorithm, including\nlearning rate grafting as well as important deep learning heuristics (exponential moving\naverages, momentum, weight decay, etc.) necessary to make Shampoo work well in practice.\nThese are incorporated into our PyTorch Shampoo implementation. Where possible, we\nprovide interpretations of those heuristics; see Sections 2 and 3.\n(2) We describe the main performance optimizations that enable the PyTorch Distributed\nShampoo implementation to be competitive with standard diagonal adaptive gradient\nmethods in terms of wall-clock time. This will enable Distributed Shampoo to converge\nfaster than diagonal adaptive gradient methods in terms of wall-clock time (by taking fewer\nsteps than diagonal methods) or achieve better model quality with marginal increases in\ntraining time (after the same number of steps); see Section 4.\n(3) We provide corroborating evidence for Distributed Shampoo\u2019s improvement in convergence\nand model quality by providing ablations and numerical results on ImageNet ResNet50 with\nstandard benchmark training recipes; see Section 5. Specifically, Shampoo over 60 epochs is\nable to achieve the same validation accuracy as SGD with Nesterov over 90 epochs with\nminimal hyperparameter tuning. This yields a 1.35x improvement in overall wall-clock time\nwhen training.\n4\nShi et al.\nOur implementation is available online, and the open-source repository includes a README and user\nguide which complement the discussion in this paper. For details, see:\nhttps://github.com/facebookresearch/optimizers/tree/main/distributed_shampoo.\n1.2\nTerminology and Notation\nFor a vectors or matrices \ud835\udc34, \ud835\udc35 \u2208 R\ud835\udc5a\u00d7\ud835\udc5b, we define the element-wise square operator \ud835\udc34\u22992 \u2208 R\ud835\udc5a\u00d7\ud835\udc5b,\ndivision operator \ud835\udc34/\ud835\udc35 \u2208 R\ud835\udc5a\u00d7\ud835\udc5b, and square-root operator\n\u221a\n\ud835\udc34 \u2208 R\ud835\udc5a\u00d7\ud835\udc5b element-wise, i.e., [\ud835\udc34\u22992]\ud835\udc56\ud835\udc57 =\n\ud835\udc342\n\ud835\udc56\ud835\udc57, [\ud835\udc34/\ud835\udc35]\ud835\udc56\ud835\udc57 = \ud835\udc34\ud835\udc56\ud835\udc57/\ud835\udc35\ud835\udc56\ud835\udc57, and\nh\u221a\n\ud835\udc34\ni\n\ud835\udc56\ud835\udc57 = \u221a\ufe01\ud835\udc34\ud835\udc56\ud835\udc57. This is in contrast to \ud835\udc34\ud835\udc5d, which denotes the matrix\n\ud835\udc5d-th power of \ud835\udc34. We will use square brackets to denote [\ud835\udc5b] = {1, ...,\ud835\udc5b} for \ud835\udc5b \u2208 N. We let \ud835\udc3c\ud835\udc5a denote\nthe \ud835\udc5a-dimensional identity matrix, 1\ud835\udc5a = (1, 1, ..., 1)\ud835\udc47 \u2208 R\ud835\udc5a denote the ones vector of length \ud835\udc5a, and\n0\ud835\udc5a\u00d7\ud835\udc5b denote a \ud835\udc5a \u00d7 \ud835\udc5b-dimensional zeros matrix.\nWe define the diag : R\ud835\udc5b \u2192 R\ud835\udc5b\u00d7\ud835\udc5b operator as the function that forms a diagonal matrix with the\ninput vector\u2019s entries on the diagonal, i.e., if \ud835\udc4e = (\ud835\udc4e1, ...,\ud835\udc4e\ud835\udc5b)\ud835\udc47 \u2208 R\ud835\udc5b, then\ndiag(\ud835\udc4e) =\n\u00a9\u00ad\u00ad\u00ad\u00ad\n\u00ab\n\ud835\udc4e11\n0\n...\n0\n0\n\ud835\udc4e22\n...\n0\n...\n...\n...\n...\n0\n0\n...\n\ud835\udc4e\ud835\udc5b\ud835\udc5b\n\u00aa\u00ae\u00ae\u00ae\u00ae\n\u00ac\n\u2208 R\ud835\udc5b\u00d7\ud835\udc5b.\n(1)\nFor \ud835\udc5b1,\ud835\udc5b2, ...,\ud835\udc5b\ud835\udc59 \u2208 N, we define blkdiag : R\ud835\udc5b1\u00d7\ud835\udc5b1 \u00d7...\u00d7R\ud835\udc5b\ud835\udc59 \u00d7\ud835\udc5b\ud835\udc59 \u2192 R(\ud835\udc5b1+...+\ud835\udc5b\ud835\udc59 )\u00d7(\ud835\udc5b1+...+\ud835\udc5b\ud835\udc59 ) as the operator\nthat forms a block diagonal matrix from square matrices, i.e., if \ud835\udc34\ud835\udc56 \u2208 R\ud835\udc5b\ud835\udc56 \u00d7\ud835\udc5b\ud835\udc56 for \ud835\udc56 = 1, ...,\ud835\udc59, then:\nblkdiag(\ud835\udc341, ...,\ud835\udc34\ud835\udc59) =\n\u00a9\u00ad\u00ad\u00ad\u00ad\n\u00ab\n\ud835\udc341\n0\n...\n0\n0\n\ud835\udc342\n...\n0\n...\n...\n...\n...\n0\n0\n...\n\ud835\udc34\ud835\udc59\n\u00aa\u00ae\u00ae\u00ae\u00ae\n\u00ac\n\u2208 R(\ud835\udc5b1+...+\ud835\udc5b\ud835\udc59 )\u00d7(\ud835\udc5b1+...+\ud835\udc5b\ud835\udc59 ).\n(2)\nWe define a matrix diagonal operator matdiag : R\ud835\udc5b\u00d7\ud835\udc5b \u2192 R\ud835\udc5b\u00d7\ud835\udc5b as the operator that returns a matrix\nof the same shape but only with its diagonal entries and zero elsewhere, i.e., given \ud835\udc34 \u2208 R\ud835\udc5b\u00d7\ud835\udc5b, then:\nmatdiag(\ud835\udc34) = \ud835\udc34 \u2299 \ud835\udc3c\ud835\udc5b =\n\u00a9\u00ad\u00ad\u00ad\u00ad\n\u00ab\n\ud835\udc4e11\n0\n...\n0\n0\n\ud835\udc4e22\n...\n0\n...\n...\n...\n...\n0\n0\n...\n\ud835\udc4e\ud835\udc5b\ud835\udc5b\n\u00aa\u00ae\u00ae\u00ae\u00ae\n\u00ac\n\u2208 R\ud835\udc5b\u00d7\ud835\udc5b,\n(3)\nwhere \u2299 corresponds to element-wise multiplication of two matrices of the same shape. Vectoriza-\ntion of matrices is performed in a row-wise fashion, i.e., if\n\ud835\udc34 =\n\u00a9\u00ad\u00ad\u00ad\u00ad\n\u00ab\n\ud835\udc4e\ud835\udc47\n1\n\ud835\udc4e\ud835\udc47\n2...\n\ud835\udc4e\ud835\udc47\n\ud835\udc5a\n\u00aa\u00ae\u00ae\u00ae\u00ae\n\u00ac\n(4)\nthen vec(\ud835\udc34) = (\n\ud835\udc4e\ud835\udc47\n1\n,\n\ud835\udc4e\ud835\udc47\n2\n, ...,\n\ud835\udc4e\ud835\udc47\n\ud835\udc5a\n)\ud835\udc47 .\nFor matrices \ud835\udc34 \u2208 R\ud835\udc5a\u00d7\ud835\udc5b and \ud835\udc35 \u2208 R\ud835\udc5e\u00d7\ud835\udc5f, their Kronecker product is defined as\n\ud835\udc34 \u2297 \ud835\udc35 =\n\u00a9\u00ad\u00ad\u00ad\u00ad\n\u00ab\n\ud835\udc4e11\ud835\udc35\n\ud835\udc4e12\ud835\udc35\n...\n\ud835\udc4e1\ud835\udc5b\ud835\udc35\n\ud835\udc4e21\ud835\udc35\n\ud835\udc4e22\ud835\udc35\n...\n\ud835\udc4e2\ud835\udc5b\ud835\udc35\n...\n...\n...\n...\n\ud835\udc4e\ud835\udc5a1\ud835\udc35\n\ud835\udc4e\ud835\udc5a2\ud835\udc35\n...\n\ud835\udc4e\ud835\udc5a\ud835\udc5b\ud835\udc35\n\u00aa\u00ae\u00ae\u00ae\u00ae\n\u00ac\n\u2208 R\ud835\udc5a\ud835\udc5e\u00d7\ud835\udc5b\ud835\udc5f .\n(5)\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n5\nThere are a few nice properties of Kronecker products and their relationship with row vectorization\nthat we exploit, namely,\n\u2022 If both\ud835\udc34 and \ud835\udc35 are square symmetric positive semi-definite matrices, then (\ud835\udc34\u2297\ud835\udc35)\ud835\udc5d = \ud835\udc34\ud835\udc5d \u2297\ud835\udc35\ud835\udc5d\nfor \ud835\udc5d \u2265 0. If \ud835\udc34 and \ud835\udc35 are symmetric positive definite, then this holds for all \ud835\udc5d \u2208 R.\n\u2022 If \ud835\udc34 and \ud835\udc35 are square matrices and \ud835\udc3a \u2208 R\ud835\udc5a\u00d7\ud835\udc5e is an \ud835\udc5a \u00d7 \ud835\udc5e matrix, then vec(\ud835\udc34\ud835\udc3a\ud835\udc35) =\n(\ud835\udc34 \u2297 \ud835\udc35\ud835\udc47 ) vec(\ud835\udc3a).\nWe will call \ud835\udc34 and \ud835\udc35 the Kronecker factor matrices.\n2\nPROBLEM STATEMENT AND SHAMPOO ALGORITHM\n2.1\nNeural Network Training\nThe neural network training problem can be posed as a stochastic optimization problem of the\nform:\nmin\n\ud835\udc64\u2208R\ud835\udc51\n\b\n\ud835\udc53 (\ud835\udc64) = E(\ud835\udc65,\ud835\udc66)\u223cD [\u2113(\ud835\udc5a(\ud835\udc64;\ud835\udc65);\ud835\udc66)]\n\t\n(6)\nwhere (\ud835\udc65,\ud835\udc66) \u2208 R\ud835\udc510 \u00d7R\ud835\udc51\ud835\udc5b correspond to a feature vector-label pair, D corresponds to the underlying\ndata distribution, \ud835\udc5a : R\ud835\udc51 \u00d7 R\ud835\udc510 \u2192 R\ud835\udc51\ud835\udc5b represents a neural network model that takes as input\nthe model parameters \ud835\udc64 and feature vector \ud835\udc65 and outputs a prediction in R\ud835\udc51\ud835\udc5b. The loss function\n\u2113 : R\ud835\udc51\ud835\udc5b \u00d7R\ud835\udc51\ud835\udc5b \u2192 R measures how well the model\u2019s prediction matches the target label\ud835\udc66. The model is\nparameterized by a list of tensors\ud835\udc4a (1), ...,\ud835\udc4a (\ud835\udc5b) with vec(\ud835\udc4a (1)) \u2208 R\ud835\udc51 (1), ..., vec(\ud835\udc4a (\ud835\udc5b)) \u2208 R\ud835\udc51 (\ud835\udc5b) . Each\ntensor \ud835\udc4a (\ud835\udc56) will be called a parameter, consistent with PyTorch\u2019s terminology for the enumerable\nrepresentation of the tensor list passed into torch.optim.Optimizer. The full list of tensors will\nbe called the model\u2019s parameters.\nWe will denote the concatenated vectorized parameters as \ud835\udc64 = (vec(\ud835\udc4a (1))\ud835\udc47, ..., vec(\ud835\udc4a (\ud835\udc5b))\ud835\udc47 )\ud835\udc47 \u2208\nR\ud835\udc51 with\ud835\udc51 = \u00cd\ud835\udc5b\n\ud835\udc56=1 \ud835\udc51 (\ud835\udc56). Using this language, we say that our network has\ud835\udc5b parameters, but\ud835\udc51 variables\nor weights. We will refer to the learning rate, momentum parameter, etc. as hyperparameters to\navoid overloading the term parameter.\nA simple example of a neural network model is a multi-layer perceptron consisting of linear\nlayers (ignoring the bias terms) of the form:\n\ud835\udc5a(\ud835\udc64;\ud835\udc65) = \ud835\udc4a (\ud835\udc5b)\ud835\udf0e(\ud835\udc4a (\ud835\udc5b\u22121)\ud835\udf0e(...\ud835\udf0e(\ud835\udc4a (1)\ud835\udc65)...)),\n(7)\nwhere \ud835\udc4a (\ud835\udc56) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 is a parameter, \ud835\udc64 = (vec(\ud835\udc4a (1))\ud835\udc47, ..., vec(\ud835\udc4a (\ud835\udc5b))\ud835\udc47 )\ud835\udc47 \u2208 R\ud835\udc51 with \ud835\udc51 (\ud835\udc56) =\n\ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc56\u22121 and \ud835\udc51 = \u00cd\ud835\udc5b\n\ud835\udc56=1 \ud835\udc51 (\ud835\udc56) = \u00cd\ud835\udc5b\n\ud835\udc56=1 \ud835\udc51\ud835\udc56\ud835\udc51\ud835\udc56\u22121 is the vector of all parameters of dimension \ud835\udc51, and \ud835\udf0e\nis a componentwise activation function, i.e., [\ud835\udf0e(\ud835\udc65)] \ud835\udc57 = \ud835\udf0e(\ud835\udc65\ud835\udc57). For example, a ReLU activation\nfunction is defined as \ud835\udf0e(\ud835\udc65) = max(\ud835\udc65, 0). Consistent with the parameter shapes, we will denote\n\ud835\udc3a (\ud835\udc56) = \u2207\ud835\udc4a (\ud835\udc56) \u2113(\ud835\udc5a(\ud835\udc64;\ud835\udc65),\ud835\udc66) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 as the (mini-batch) stochastic gradient of parameter \ud835\udc56 and\n\ud835\udc54 = (vec(\ud835\udc3a (1))\ud835\udc47, ..., vec(\ud835\udc3a (\ud835\udc5b))\ud835\udc47 )\ud835\udc47 \u2208 R\ud835\udc51 as the (mini-batch) stochastic gradient vector.\u2217 Here, \ud835\udc51 (\ud835\udc56)\ncorresponds to the number of variables within parameter \ud835\udc56, \ud835\udc51\ud835\udc56 corresponds to the dimension of\nthe activation after layer or parameter \ud835\udc56, and \ud835\udc51 corresponds to the total number of variables in the\noptimization problem.\nClosely related to the stochastic optimization formulation is the online convex optimization\nproblem. These formulations have been shown to be equivalent under certain scenarios [Cesa-\nBianchi et al. 2001]. The online optimization problem has relevance to settings where online training\non streaming data is used in practice, often to fine-tune models. This problem may be formulated\nas a game where at round \ud835\udc61, a player makes a prediction \ud835\udc64\ud835\udc61 \u2208 R\ud835\udc51, receives a loss evaluated at the\n\u2217If we use the mini-batch stochastic gradient, then given a global mini-batch size \ud835\udc35, we would sample a mini-batch of\nsamples {(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56 )}\ud835\udc35\n\ud835\udc56=1 and the mini-batch stochastic gradient would be defined as \ud835\udc3a (\ud835\udc56) = 1\n\ud835\udc35\n\u00cd\ud835\udc35\n\ud835\udc56=1 \u2207\ud835\udc4a (\ud835\udc56) \u2113 (\ud835\udc5a(\ud835\udc64;\ud835\udc65\ud835\udc56 ), \ud835\udc66\ud835\udc56 ) \u2208\nR\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121.\n6\nShi et al.\npredicted point \ud835\udc53\ud835\udc61 (\ud835\udc64\ud835\udc61) (and its gradient \u2207\ud835\udc53\ud835\udc61 (\ud835\udc64\ud835\udc61)), and updates their prediction for the next round\n\ud835\udc64\ud835\udc61+1. The functions must belong to a predetermined bounded function class \ud835\udc53\ud835\udc61 \u2208 F , but, unlike\nin the stochastic optimization setting, are not assumed to arise from some underlying probability\ndistribution. This setting can therefore model settings where the underlying data distribution may\nshift during training, as in ranking and recommendation [Naumov et al. 2019].\n2.2\nDiagonal Adaptive Gradient Methods\nThe family of adaptive gradient methods [Dozat 2016; Duchi et al. 2011; Kingma and Ba 2015; Reddi\net al. 2018] is designed for both the stochastic optimization and online convex optimization. The\nAdaGrad method preconditions the (sub)gradient by the pseudo-inverse square-root of the sum of\nsquared gradients or gradient outer products, i.e.,\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc34\u2020/2\n\ud835\udc61\n\ud835\udc54\ud835\udc61\n(8)\nwhere \ud835\udc54\ud835\udc61 \u2208 R\ud835\udc51 is the vectorized (mini-batch) stochastic gradient, \ud835\udefc\ud835\udc61 > 0 is the learning rate or\nsteplength, and\n\ud835\udc34\ud835\udc61 =\n(\u00cd\ud835\udc61\n\ud835\udc60=0 \ud835\udc54\ud835\udc60\ud835\udc54\ud835\udc47\n\ud835\udc60\n(Full-Matrix AdaGrad)\n\u00cd\ud835\udc61\n\ud835\udc60=0 diag(\ud835\udc54\u22992\n\ud835\udc60 ) = \u00cd\ud835\udc61\n\ud835\udc60=0 matdiag(\ud835\udc54\ud835\udc60\ud835\udc54\ud835\udc47\n\ud835\udc60 )\n(Diagonal AdaGrad)\n(9)\nfor \ud835\udf16 > 0. In this case, \ud835\udc5d\ud835\udc61 = \ud835\udc34\u2020/2\n\ud835\udc61\n\ud835\udc54\ud835\udc61 is the adaptive gradient search direction. Note that full-matrix\nAdaGrad requires \ud835\udc42(\ud835\udc512) memory and \ud835\udc42(\ud835\udc513) computation, while diagonal AdaGrad requires \ud835\udc42(\ud835\udc51)\nmemory and \ud835\udc42(\ud835\udc51) computation. Related methods like RMSProp and Adam use exponential moving\naverages in place of the summation, i.e.,\n\ud835\udc34\ud835\udc61 = \ud835\udefd2\ud835\udc34\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2) diag(\ud835\udc54\u22992\n\ud835\udc61 ),\n(10)\nwith \ud835\udc34\u22121 = 0 and may incorporate a bias correction term. Since \ud835\udc51 is commonly on the or-\nder of billions or even trillions of parameters, full-matrix AdaGrad is not practically feasible,\nand its diagonal approximation is commonly applied instead. In the diagonal case, AdaGrad\u2019s\noptimizer state is instantiated with the same shapes as the neural network parameters \ud835\udc34\ud835\udc61 =\ndiag((vec(\ud835\udc34(1)\n\ud835\udc61\n)\ud835\udc47, ..., vec(\ud835\udc34(\ud835\udc5b)\n\ud835\udc61\n)\ud835\udc47 )\ud835\udc47 ) with dim(\ud835\udc34(\ud835\udc56)\n\ud835\udc61 ) = dim(\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ) for \ud835\udc56 = 1, ...,\ud835\udc5b, and the algorithm\nupdate is implemented in a per-parameter fashion, i.e.,\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 /\n\u221a\ufe03\n\ud835\udc34(\ud835\udc56)\n\ud835\udc61 , \u2200\ud835\udc56 = 1, ...,\ud835\udc5b,\n(11)\nwhere division \u00b7/\u00b7 and square-root operators \u221a\u00b7 are applied componentwise.\nObserve that \ud835\udc34\ud835\udc61 is symmetric positive semi-definite for both full-matrix and diagonal AdaGrad.\nSince these methods can only guarantee symmetric positive semi-definiteness, a small regularization\nterm \ud835\udf16\ud835\udc3c is inserted into the preconditioner to ensure positive-definiteness, either by computing:\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udc34\ud835\udc61 + \ud835\udf16\ud835\udc3c)\u22121/2\ud835\udc54\ud835\udc61\n(12)\nor\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udc341/2\n\ud835\udc61\n+ \ud835\udf16\ud835\udc3c)\u22121\ud835\udc54\ud835\udc61.\n(13)\nAlthough the latter is more common for (diagonal) AdaGrad, RMSProp, and Adam, we will use the\nformer for Shampoo.\nSince \ud835\udc34\ud835\udc61 is real symmetric positive semi-definite, the pseudo-inverse square root is defined\nin terms of its real eigendecomposition \ud835\udc34\ud835\udc61 = \ud835\udc44\ud835\udc61\n\u0014\n\u039b\ud835\udc61\n0\ud835\udc51\u00d7(\ud835\udc51\u2212\ud835\udc5f )\n0(\ud835\udc51\u2212\ud835\udc5f )\u00d7\ud835\udc51\n0(\ud835\udc51\u2212\ud835\udc5f )\u00d7(\ud835\udc51\u2212\ud835\udc5f )\n\u0015\n\ud835\udc44\ud835\udc47\n\ud835\udc61 where \u039b\ud835\udc61 \u2208 R\ud835\udc5a\u00d7\ud835\udc5a\nfor \ud835\udc5f \u2264 \ud835\udc51 is a diagonal matrix consisting of the positive eigenvalues of \ud835\udc34\ud835\udc61 and \ud835\udc44\ud835\udc61 \u2208 R\ud835\udc51\u00d7\ud835\udc51 is an\northogonal matrix. Note that \ud835\udc5f is the rank of \ud835\udc34\ud835\udc61. The matrix pseudo-inverse square root is therefore\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n7\nFig. 2. Picture of block-diagonal and Kronecker product approximations used in Shampoo.\ndefined as \ud835\udc34\u2020/2\n\ud835\udc61\n= \ud835\udc44\ud835\udc61\n\u0014\n\u039b\u22121/2\n\ud835\udc61\n0\ud835\udc51\u00d7(\ud835\udc51\u2212\ud835\udc5f )\n0(\ud835\udc51\u2212\ud835\udc5f )\u00d7\ud835\udc51\n0(\ud835\udc51\u2212\ud835\udc5f )\u00d7(\ud835\udc51\u2212\ud835\udc5f )\n\u0015\n\ud835\udc44\ud835\udc47\n\ud835\udc61 where \u039b\u22121/2\n\ud835\udc61\nis the inverse square root of the\ndiagonal entries in the matrix [Golub and Van Loan 2013; Higham 2008].\nNote that this is not equal to the element-wise root inverse, which we denote as 1/\u221a\u00b7. However,\nwhen applied to diagonal AdaGrad (with regularization), it is sufficient to take the inverse square\nroot of each diagonal component since \ud835\udc34\ud835\udc61 is already diagonalized.\n2.3\nThe Shampoo Algorithm\nAlthough diagonal AdaGrad is efficient for training, it ignores (uncentered) correlations, and\nyields a worse constant in its regret bound and convergence rate [Duchi et al. 2011]. Full-matrix\nAdaGrad incorporates these correlations to obtain a better search direction at each step. On the\nother hand, full-matrix AdaGrad is not tractable due to its quadratic memory and cubic computation\nrequirements. Shampoo provides a scalable solution in between these two regimes by applying two\napproximations:\n(1) Block-Diagonal Approximation: Rather than constructing a single matrix preconditioner\nfor all parameters simultaneously, Shampoo exploits the parameter list representation\nof the neural network and constructs a block-diagonal preconditioner where each block\npreconditions each individual parameter independently. Note that this implies that cross-\nparameter correlations are ignored.\n(2) Kronecker Product Approximation: In order to exploit the underlying tensor structure of\neach parameter, full-matrix AdaGrad is replaced with a Kronecker product approximation\nto capture (uncentered) correlations.\nFor simplicity, let us focus on the multi-layer perceptron case where each parameter consists\nof a matrix \ud835\udc4a (\ud835\udc56) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 and focus solely on a single parameter \ud835\udc4a (\ud835\udc56). Note that the gradient\n\ud835\udc3a (\ud835\udc56) = \u2207\ud835\udc4a (\ud835\udc56) \ud835\udc53 (\ud835\udc64) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 shares the same shape as the parameter.\nThe gradient of a fully-connected layer for a single data point can be written as the outer\nproduct of the pre-activation gradient and the activation before layer \ud835\udc56. More precisely, we can\nisolate a single fully-connected layer as the only parameter in the objective function with all\nother parameters fixed, i.e., \ud835\udc53 (\ud835\udc56) (\ud835\udc4a (\ud835\udc56)) = \ud835\udf19 (\ud835\udc56) (\ud835\udc4a (\ud835\udc56)\ud835\udc4e(\ud835\udc56\u22121)), where \ud835\udf19 (\ud835\udc56) : R\ud835\udc51\ud835\udc56 \u2192 R is composed\nof the loss function and the rest of the model and \ud835\udc4e(\ud835\udc56\u22121) is the activation before layer \ud835\udc56; see\nAppendix A for their precise definition for multi-layer perceptrons. Then the gradient can be\nwritten as \ud835\udc3a (\ud835\udc56) = \u2207\ud835\udc4a (\ud835\udc56) \ud835\udc53 (\ud835\udc56) (\ud835\udc4a (\ud835\udc56)) = \u2207\ud835\udf19 (\ud835\udc56) (\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\ud835\udc4e(\ud835\udc56\u22121) (\ud835\udc4e(\ud835\udc56\u22121))\ud835\udc47 , and its row vectorization is\n\ud835\udc54 = vec(\ud835\udc3a\ud835\udc56) = \u2207\ud835\udf19 (\ud835\udc56) (\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\ud835\udc4e(\ud835\udc56\u22121) \u2297 \ud835\udc4e(\ud835\udc56\u22121).\n8\nShi et al.\nLet the subscript \ud835\udc60 denote the gradient, function, or activation at iteration \ud835\udc60. Then full-matrix\nAdaGrad for layer \ud835\udc56 accumulates a summation of Kronecker products:\n\ud835\udc34(\ud835\udc56)\n\ud835\udc61\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n\ud835\udc54\ud835\udc60\ud835\udc54\ud835\udc47\n\ud835\udc60\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n\u2297 \ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n\u2297 \ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)\ud835\udc47\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60 (\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)\ud835\udc47 ) \u2297 (\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n(\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)\ud835\udc47 ).\nWe aim to approximate \ud835\udc34(\ud835\udc56)\n\ud835\udc61\nby a single Kronecker product of two factor matrices \ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56 and\n\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2208 R\ud835\udc51\ud835\udc56\u22121\u00d7\ud835\udc51\ud835\udc56\u22121 such that \ud835\udc34(\ud835\udc56)\n\ud835\udc61\n\u2248 \ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2297 \ud835\udc45(\ud835\udc56)\n\ud835\udc61 . Rather than vectorizing the gradient, these matrices\nwill operate directly on the tensor (matrix in the fully-connected case) \ud835\udc3a (\ud835\udc56)\n\ud835\udc61 . More specifically,\n\ud835\udc3f(\ud835\udc56)\n\ud835\udc61 , \ud835\udc45(\ud835\udc56)\n\ud835\udc61\nare defined as:\n\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n\ud835\udc3a (\ud835\udc56)\n\ud835\udc60\n[\ud835\udc3a (\ud835\udc56)\n\ud835\udc60 ]\ud835\udc47 + \ud835\udf16\ud835\udc3c\ud835\udc51\ud835\udc56,\n(14)\n\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n[\ud835\udc3a (\ud835\udc56)\n\ud835\udc60 ]\ud835\udc47\ud835\udc3a (\ud835\udc56)\n\ud835\udc60\n+ \ud835\udf16\ud835\udc3c\ud835\udc51\ud835\udc56\u22121,\n(15)\nand its Kronecker product is defined as\n\u00af\ud835\udc34(\ud835\udc56)\n\ud835\udc61\n= [\ud835\udc3f(\ud835\udc56)\n\ud835\udc61 ]1/2 \u2297 [\ud835\udc45(\ud835\udc56)\n\ud835\udc61 ]1/2\n(16)\nfor all \ud835\udc56 = 1, ...,\ud835\udc5b. Since both \ud835\udc3f(\ud835\udc56)\n\ud835\udc61\nand \ud835\udc45(\ud835\udc56)\n\ud835\udc61\nare symmetric by definition, the transpose can be\nignored. Therefore, using the fact that vec(\ud835\udc3f\ud835\udc3a\ud835\udc45\ud835\udc47 ) = (\ud835\udc3f \u2297 \ud835\udc45) vec(\ud835\udc3a) for arbitrary matrices \ud835\udc3f,\ud835\udc3a, \ud835\udc45\nof appropriate shape with equations (12) and (16), the Shampoo update can be written as:\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61 [\ud835\udc3f(\ud835\udc56)\n\ud835\udc61 ]\u22121/4\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n[\ud835\udc45(\ud835\udc56)\n\ud835\udc61 ]\u22121/4 for \ud835\udc56 = 1, ...,\ud835\udc5b.\n(17)\nNotice that full-matrix AdaGrad for parameter\ud835\udc4a (\ud835\udc56) costs \ud835\udc42(\ud835\udc512\n\ud835\udc56 \ud835\udc512\n\ud835\udc56\u22121) memory and \ud835\udc42(\ud835\udc513\n\ud835\udc56 \ud835\udc513\n\ud835\udc56\u22121) FLOPs-\nper-iteration. By utilizing this approximation, the memory footprint can be reduced to \ud835\udc42(\ud835\udc512\n\ud835\udc56 +\ud835\udc512\n\ud835\udc56\u22121)\nand the amount of computation to \ud835\udc42(\ud835\udc513\n\ud835\udc56 + \ud835\udc513\n\ud835\udc56\u22121) FLOPs-per-iteration.\nIf the update is expanded across all vectorized parameter weights \ud835\udc64, the full Shampoo update\ncan be rewritten as:\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 \u00af\ud835\udc34\u22121/2\n\ud835\udc61\n\ud835\udc54\ud835\udc61\n(18)\nwhere \u00af\ud835\udc34\ud835\udc61 is a block diagonal matrix of the form\n\u00af\ud835\udc34\ud835\udc61 = blkdiag( \u00af\ud835\udc34(1)\n\ud835\udc61\n, ..., \u00af\ud835\udc34(\ud835\udc5b)\n\ud835\udc61\n)\n= blkdiag([\ud835\udc3f(1)\n\ud835\udc61\n]1/2 \u2297 [\ud835\udc45(1)\n\ud835\udc61\n]1/2, ..., [\ud835\udc3f(\ud835\udc5b)\n\ud835\udc61\n]1/2 \u2297 [\ud835\udc45(\ud835\udc5b)\n\ud835\udc61\n]1/2)\n=\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n[\ud835\udc3f(1)\n\ud835\udc61\n]1/2 \u2297 [\ud835\udc45(1)\n\ud835\udc61\n]1/2\n0\n...\n0\n0\n[\ud835\udc3f(2)\n\ud835\udc61\n]1/2 \u2297 [\ud835\udc45(2)\n\ud835\udc61\n]1/2\n...\n0\n0\n0\n...\n0\n0\n0\n...\n[\ud835\udc3f(\ud835\udc5b)\n\ud835\udc61\n]1/2 \u2297 [\ud835\udc45(\ud835\udc5b)\n\ud835\udc61\n]1/2\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n(19)\nShampoo generalizes these ideas to models containing parameters of arbitrary tensor order and\ndimension; see Section 4 in [Gupta et al. 2018].\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n9\n2.4\nLayer-wise Learning Rate Grafting\nOne major requirement to make Shampoo work in practice is the inclusion of layer-wise learning\nrate grafting. Learning rate grafting was introduced in [Agarwal et al. 2020] in order to transfer\na pre-existing learning rate schedule from a previous method. The idea is to maintain the search\ndirection from another method (called the grafted method) and re-scale each layer\u2019s Shampoo\nsearch direction to the norm of the search direction of the grafted method. Preconditioners for both\nShampoo and the grafting method are updated based on the same sequence of iterates.\nFrom a global perspective, grafting can be re-interpreted as a heuristic block re-scaling. Let\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo denote the Shampoo search direction for block \ud835\udc56 and iteration \ud835\udc61.\u2020 Given a separate grafted\nmethod \ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft, learning rate grafting modifies the Shampoo step to:\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft\n\r\r\r\n\ud835\udc39\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo\n\r\r\r\n\ud835\udc39\n, \u2200\ud835\udc56 = 1, ...,\ud835\udc5b.\n(20)\nNote that \ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft is defined based on the iterate sequence from Shampoo \ud835\udc64\ud835\udc61, not a separate sequence\nof iterates. We can therefore re-write the full update as\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc37\ud835\udc61 \u00af\ud835\udc34\u22121/2\n\ud835\udc61\n\ud835\udc54\ud835\udc61\n(21)\nwhere\n\ud835\udc37\ud835\udc61 = blkdiag \u00a9\u00ad\n\u00ab\n\u2225\ud835\udc43 (1)\n\ud835\udc61,graft\u2225\ud835\udc39\n\u2225\ud835\udc43 (1)\n\ud835\udc61,Shampoo\u2225\ud835\udc39\n\ud835\udc3c\ud835\udc51 (1), ...,\n\u2225\ud835\udc43 (\ud835\udc5b)\n\ud835\udc61,graft\u2225\ud835\udc39\n\u2225\ud835\udc43 (\ud835\udc5b)\n\ud835\udc61,Shampoo\u2225\ud835\udc39\n\ud835\udc3c\ud835\udc51 (\ud835\udc5b) \u00aa\u00ae\n\u00ac\n(22)\n=\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2225\ud835\udc43 (1)\n\ud835\udc61,graft\u2225\ud835\udc39\n\u2225\ud835\udc43 (1)\n\ud835\udc61,Shampoo\u2225\ud835\udc39 \ud835\udc3c\ud835\udc51 (1)\n0\n...\n0\n0\n\u2225\ud835\udc43 (2)\n\ud835\udc61,graft\u2225\ud835\udc39\n\u2225\ud835\udc43 (2)\n\ud835\udc61,Shampoo\u2225\ud835\udc39 \ud835\udc3c\ud835\udc51 (2)\n...\n0\n...\n...\n...\n...\n0\n0\n...\n\u2225\ud835\udc43 (\ud835\udc5b)\n\ud835\udc61,graft\u2225\ud835\udc39\n\u2225\ud835\udc43 (\ud835\udc5b)\n\ud835\udc61,Shampoo\u2225\ud835\udc39 \ud835\udc3c\ud835\udc51 (\ud835\udc5b)\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n(23)\nOur implementation supports all standard diagonal-scaling-based optimizers in PyTorch, includ-\ning AdaGrad, RMSProp, Adam(W), and SGD. In the case of AdaGrad, RMSProp, and Adam, we\nimplement layerwise learning rate grafting by maintaining the diagonal preconditioner for our\ngrafted method \u02dc\ud835\udc34\ud835\udc61 = blkdiag(diag(vec( \u02dc\ud835\udc34(1)\n\ud835\udc61\n)), ..., diag(vec( \u02dc\ud835\udc34(\ud835\udc5b)\n\ud835\udc61\n))) where dim( \u02dc\ud835\udc34(\ud835\udc56)\n\ud835\udc61 ) = dim(\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 )\nfor \ud835\udc56 = 1, ...,\ud835\udc5b. For example, if we are grafting from AdaGrad, the grafted preconditioner is defined\nas \u02dc\ud835\udc34(\ud835\udc56)\n\ud835\udc61\n= \u00cd\ud835\udc61\n\ud835\udc60=0[\ud835\udc3a (\ud835\udc56)\n\ud835\udc60 ]\u22992 = \u00cd\ud835\udc61\n\ud835\udc60=0[\u2207\ud835\udc4a (\ud835\udc56) \ud835\udc53\ud835\udc61 (\ud835\udc64\ud835\udc60)]\u22992. Note that the preconditioner is updated using the\nstochastic gradients evaluated at the same sequence of iterates generated and used by Shampoo; we\nuse \u02dc\ud835\udc34 to distinguish this key difference between standard diagonal AdaGrad and the grafted method.\nIn the case of AdaGrad, RMSProp, and Adam grafting, the grafted search direction is defined as\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft = \ud835\udc3a (\ud835\udc56)\n\ud835\udc61 /([ \u02dc\ud835\udc34(\ud835\udc56)\n\ud835\udc61 ]1/2 +\ud835\udf161\ud835\udc51\ud835\udc561\ud835\udc47\n\ud835\udc51\ud835\udc56\u22121) for parameter \ud835\udc56, where \u02dc\ud835\udc34(\ud835\udc56)\n\ud835\udc61\nis the AdaGrad, RMSProp, or Adam\nsecond-moment estimate.\nThis heuristic makes Shampoo significantly easier to tune given a pre-existing learning rate\nscheduler. By grafting from the previous optimizer\u2019s learning rate schedule, one generally sees\nimmediate improvements in convergence with little additional hyperparameter tuning. This can be\n\u2020If we are operating on a matrix, then \ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo := [\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n]\u22121/4\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n[\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n]\u22121/4, as seen in Section 2.\n10\nShi et al.\nused as an easy baseline for further fine-tuning of the optimizer. For more details, please refer to\n[Agarwal et al. 2020; Anil et al. 2020].\nA high-level pseudocode for the Shampoo algorithm (with standard accumulation of the factor\nmatrices and AdaGrad learning rate grafting) is provided in Algorithm 1.\nAlgorithm 1 Shampoo Pseudocode (with AdaGrad Grafting) for Training MLPs\nRequire: Parameters {\ud835\udc4a (\ud835\udc56)}\ud835\udc5b\n\ud835\udc56=1 with \ud835\udc4a (\ud835\udc56)\n0\n\u2261 \ud835\udc4a (\ud835\udc56) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121, learning rate schedule {\ud835\udefc\ud835\udc61}\ud835\udc47\n\ud835\udc61=1 with\n\ud835\udefc\ud835\udc61 > 0, epsilon for Shampoo \ud835\udf16 > 0, epsilon for AdaGrad \ud835\udf16graft > 0, maximum number of iterations\n\ud835\udc47\nfor \ud835\udc56 = 1, ...,\ud835\udc5b do\nSet \ud835\udc3f(\ud835\udc56)\n\u22121 = \ud835\udf16\ud835\udc3c\ud835\udc51\ud835\udc56 \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56, \ud835\udc45(\ud835\udc56)\n\u22121 = \ud835\udf16\ud835\udc3c\ud835\udc51\ud835\udc56\u22121 \u2208 R\ud835\udc51\ud835\udc56\u22121\u00d7\ud835\udc51\ud835\udc56\u22121.\n\u22b2 Initialize Shampoo states.\nSet \ud835\udc34(\ud835\udc56)\n\u22121 = 0 \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121.\n\u22b2 Initialize AdaGrad state.\nend for\nfor \ud835\udc61 = 0, 1, 2, ...,\ud835\udc47 \u2212 1 do\nfor \ud835\udc56 = 1, ...,\ud835\udc5b do\nCompute (mini-batch) stochastic gradient \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n= \u2207\ud835\udc4a (\ud835\udc56) \ud835\udc53\ud835\udc61 (\ud835\udc64) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 for parameter \ud835\udc56.\nUpdate Shampoo factor matrices:\n\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc3f(\ud835\udc56)\n\ud835\udc61\u22121 + \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n[\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ]\ud835\udc47,\n\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc45(\ud835\udc56)\n\ud835\udc61\u22121 + [\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ]\ud835\udc47\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\nUpdate AdaGrad state:\n\ud835\udc34(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc34(\ud835\udc56)\n\ud835\udc61\u22121 + [\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ]\u22992\nCompute matrix root inverses:\n\u00af\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2190 [\ud835\udc3f(\ud835\udc56)\n\ud835\udc61 ]\u22121/4,\n\u00af\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2190 [\ud835\udc45(\ud835\udc56)\n\ud835\udc61 ]\u22121/4\nCompute Shampoo search direction:\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo \u2190 \u00af\ud835\udc3f(\ud835\udc56)\n\ud835\udc61 \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n\u00af\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft \u2190 \ud835\udc3a (\ud835\udc56)\n\ud835\udc61 /([\ud835\udc34(\ud835\udc56)\n\ud835\udc61 ]1/2 + \ud835\udf16graft1\ud835\udc51\ud835\udc561\ud835\udc47\n\ud835\udc51\ud835\udc56\u22121)\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n\u2190 \u2212\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft\n\r\r\r\n\ud835\udc39\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo\n\r\r\r\n\ud835\udc39\nUpdate parameter:\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 \u2190 \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udefc\ud835\udc61\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\nend for\nend for\n3\nIMPLEMENTATION DETAILS\nMany additional improvements and heuristics are incorporated into the Shampoo optimizer imple-\nmentations. Several of these heuristics have been employed in the JAX and OPTAX implementations\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n11\nof Shampoo and have also been incorporated into our PyTorch implementation here [Anil et al.\n2020; Bradbury et al. 2018; Paszke et al. 2019]. We provide a high-level description of different\nheuristics, including using exponentially-weighted moving average estimates of the first- and\nsecond-moments, weight decay, momentum and Nesterov acceleration, and the exponent multiplier\nand override options. A complete description of the algorithm including learning rate grafting,\nthe main heuristics, and the main distributed memory/computation performance optimization is\nprovided in Algorithm 2. (We ignore merging and blocking here.)\n3.1\nTraining Heuristics\nIn this subsection, we describe some of the additional heuristics that are commonly used with\ndeep learning optimizers and that have been enabled with Shampoo and layer-wise learning rate\ngrafting. When possible, we provide intuition for each heuristic.\n3.1.1\nFirst and Second Moment Estimation. It is common to use gradient filtering or exponential\nmoving averages of the \u201cfirst moment\u201d of the gradient. This has been widely interpreted as the\nnatural extension of momentum to adaptive gradient methods, and has been demonstrated to\nbe useful for deterministic nonsmooth optimization as well as deep learning training; see [Boyd\net al. 2003; Kingma and Ba 2015]. More specifically, we can filter the gradient estimator \u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\nvia\nexponential moving averaging and use this in place of \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\nwhere\n\u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n= \ud835\udefd1 \u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd1)\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ,\n(24)\nwith \u02dc\ud835\udc3a (\ud835\udc56)\n\u22121 = 0. When grafting, the grafted method\u2019s state is updated using the original stochastic\ngradient \ud835\udc3a (\ud835\udc56)\n\ud835\udc61 , but the search direction is computed based on the filtered gradient \u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\ninstead.\nOne can similarly apply exponential moving averages of the Shampoo approximation for matrices\nas follows:\n\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n= \ud835\udefd2\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2)\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n[\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ]\ud835\udc47,\n(25)\n\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n= \ud835\udefd2\ud835\udc45(\ud835\udc56)\n\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2)[\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ]\ud835\udc47\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 ,\n(26)\nwith \ud835\udc3f(\ud835\udc56)\n\u22121 = 0 and \ud835\udc45(\ud835\udc56)\n\u22121 = 0. A similar modification can be made for Shampoo for general tensors.\nA bias correction term can be employed by setting \u02c6\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n= \u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61 /(1 \u2212 \ud835\udefd\ud835\udc61+1\n1\n), \u02c6\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n= \ud835\udc3f(\ud835\udc56)\n\ud835\udc61 /(1 \u2212 \ud835\udefd\ud835\udc61+1\n2\n),\n\u02c6\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n= \ud835\udc45(\ud835\udc56)\n\ud835\udc61 /(1 \u2212 \ud835\udefd\ud835\udc61+1\n2\n), etc. Bias correction can be interpreted either as an implicit modification\nto the learning rate schedule or an approach to ensure that the statistical estimate is unbiased,\nparticularly when only a few updates of the exponential moving average have been performed; see\n[Kingma and Ba 2015].\nUsage: To use exponential moving averaging of these quantities, one should set betas = (beta1,\nbeta2) to the desired values. If beta2 = 1, then the implementation will use the standard summa-\ntion. To enable bias correction, simply set the flag use_bias_correction = True. (This is enabled\nby default.)\n3.1.2\n\u21132-Regularization and (Decoupled) Weight Decay. There are two variants of regularization\nthat we have enabled: (1) standard \u21132 regularization and (2) decoupled weight decay. Weight decay\nis sometimes used to refer to appending an L2-regularization term to the training loss function, i.e.,\nmin\n\ud835\udc64\u2208R\ud835\udc51 E(\ud835\udc65,\ud835\udc66)\u223cD\n\u0014\n\u2113(\ud835\udc5a(\ud835\udc64;\ud835\udc65);\ud835\udc66) + \ud835\udf06\n2 \u2225\ud835\udc64\u22252\n\u0015\n.\n(27)\n12\nShi et al.\nAlgorithm 2 Complete Distributed Shampoo Pseudocode (on Worker \ud835\udc57)\nRequire: Parameters {\ud835\udc4a (\ud835\udc56) }\ud835\udc5b\n\ud835\udc56=1 with \ud835\udc4a (\ud835\udc56)\n0\n\u2261 \ud835\udc4a (\ud835\udc56) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121, learning rate schedule {\ud835\udefc\ud835\udc61 }\ud835\udc47\n\ud835\udc61=1 with \ud835\udefc\ud835\udc61 > 0, exponential\nmoving average weights \ud835\udefd1 \u2208 [0, 1), \ud835\udefd2 \u2208 (0, 1], momentum \ud835\udf07 > 0, weight decay \ud835\udf06 \u2265 0, period for computing root\ninverse precondition_frequency, initial iteration for using Shampoo preconditioning start_preconditioning_step,\ngrafting method, maximum number of iterations \ud835\udc47, flag for bias correction use_bias_correction, flag for decoupled\nweight decay use_decoupled_weight_decay, number of workers \ud835\udc3d , number of workers per group \ud835\udc3d\ud835\udc3a\nAssign preconditioners to different workers using a greedy method \ud835\udc3c1, \ud835\udc3c2, ..., \ud835\udc3c\ud835\udc3d \u2282 [\ud835\udc5b] based on \ud835\udc510,\ud835\udc511, ...,\ud835\udc51\ud835\udc5b.\nfor \ud835\udc56 \u2208 \ud835\udc3c\ud835\udc57 do\nSet \ud835\udc3f(\ud835\udc56)\n\u22121 = 0 \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56 , \ud835\udc45(\ud835\udc56)\n\u22121 = 0 \u2208 R\ud835\udc51\ud835\udc56\u22121\u00d7\ud835\udc51\ud835\udc56\u22121.\n\u22b2 Initialize Shampoo states.\nSet \u02dc\ud835\udc3a (\ud835\udc56)\n\u22121 = 0 \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 if \ud835\udefd1 > 0, \ud835\udc40 (\ud835\udc56)\n\u22121 = 0 \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 if \ud835\udf07 > 0.\n\u22b2 Initialize additional states.\nSet \ud835\udc34(\ud835\udc56)\n\u22121 = 0 \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 (if necessary).\n\u22b2 Initialize grafting state (if necessary).\nend for\nfor \ud835\udc61 = 0, 1, 2, ...,\ud835\udc47 \u2212 1 do\nCompute (mini-batch) stochastic gradient \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n= \u2207\ud835\udc4a (\ud835\udc56) \ud835\udc53\ud835\udc61 (\ud835\udc64) \u2208 R\ud835\udc51\ud835\udc56 \u00d7\ud835\udc51\ud835\udc56\u22121 for all parameters \ud835\udc56 in DDP fashion.\nfor \ud835\udc56 \u2208 \ud835\udc3c\ud835\udc57 do\nif \ud835\udf06 > 0 and not use_decoupled_weight_decay then\n\u22b2 Incorporate \u21132-regularization.\n\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udf06\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\nend if\nif \ud835\udefd2 < 1 then\n\u22b2 Update Shampoo factor matrices.\n\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udefd1\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd1)\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n[\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n]\ud835\udc47\n\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udefd1\ud835\udc45(\ud835\udc56)\n\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd1)[\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n]\ud835\udc47\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\nelse\n\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc3f(\ud835\udc56)\n\ud835\udc61\u22121 + \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n[\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n]\ud835\udc47\n\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc45(\ud835\udc56)\n\ud835\udc61\u22121 + [\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n]\ud835\udc47\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\nend if\n\ud835\udc34(\ud835\udc56)\n\ud835\udc61\n\u2190 UpdateGraftingState(\ud835\udc34(\ud835\udc56)\n\ud835\udc61\u22121,\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n)\n\u22b2 Update grafting method\u2019s state (if necessary).\nif \ud835\udc61 \u2265 start_preconditioning_step and \ud835\udc61 % precondition_frequency = 0 then\n\u00af\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u2190 ComputeMatrixRootInverse(\ud835\udc3f(\ud835\udc56)\n\ud835\udc61 ,\ud835\udf16,\ud835\udc61, use_bias_correction)\n\u00af\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2190 ComputeMatrixRootInverse(\ud835\udc45(\ud835\udc56)\n\ud835\udc61 ,\ud835\udf16,\ud835\udc61, use_bias_correction)\nend if\nif \ud835\udefd1 > 0 then\n\u22b2 Compute filtered/exponential moving averaged gradient.\n\u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udefd1 \u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd1)\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\nend if\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft \u2190 ComputeGraftingDirection( \u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n,\ud835\udc61, use_bias_correction)\n\u22b2 Compute grafting direction.\nif \ud835\udc61 \u2265 start_preconditioning_step then\n\u22b2 Compute scaled Shampoo direction.\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n\u2190 \u2212\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft\n\r\r\r\n\ud835\udc39\n\u00af\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n\u00af\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2225 \u00af\ud835\udc3f(\ud835\udc56)\n\ud835\udc61\n\u02dc\ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n\u00af\ud835\udc45(\ud835\udc56)\n\ud835\udc61\n\u2225\ud835\udc39\nelse\n\u22b2 Use grafting search direction.\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft\nend if\nif \ud835\udf06 > 0 and use_decoupled_weight_decay then\n\u22b2 Incorporate decoupled weight decay.\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udf06\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\nend if\nif \ud835\udf07 > 0 then\n\u22b2 Incorporate momentum.\n\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udf07\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\nif use_nesterov then\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udf07\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\nelse\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc40 (\ud835\udc56)\n\ud835\udc61\nend if\nend if\nend for\n{\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n}\ud835\udc5b\n\ud835\udc56=1 \u2190 AllGather({\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n}\ud835\udc56\u2208\ud835\udc3c\ud835\udc57 )\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 \u2190 \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udefc\ud835\udc61\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\nfor all \ud835\udc56 = 1, ...,\ud835\udc5b.\n\u22b2 Update parameters.\nend for\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n13\nFrom an implementation perspective, this modifies the gradient by adding an additional regular-\nization term: \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n\u2190 \ud835\udc3a (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udf06\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\nfor all \ud835\udc56. Notice that this impacts all aspects of the optimizer,\nincluding the gradients used in the updates of the Shampoo preconditioners and grafting method.\nOn the other hand, weight decay as originally introduced in Hanson and Pratt [1988] \u2014 now\ncommonly referred to as decoupled weight decay following Loshchilov and Hutter [2019] \u2014 is not\nequivalent to \u21132 regularization in general.\u2021 Decoupled weight decay involves a modification of the\ntraining algorithm outside of the preconditioner. More precisely, it is defined as:\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = (1 \u2212 \ud835\udefc\ud835\udc61\ud835\udf06)\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(28)\n= \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61 (\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udf06\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n)\n(29)\nfor \ud835\udc56 = 1, ...,\ud835\udc5b. This method has been interpreted as a first-order approximation to a proximal\nmethod for enforcing \u21132-regularization that is scale-invariant, i.e., the method remains the same\neven when the objective function is multiplied by some positive constant and eases hyperparameter\ntuning [Zhuang et al. 2022].\nDecoupled weight decay is often implemented as a separate transformation of the parameters\nunless combined with momentum, as we see below. In our experiments, we found that decoupled\nweight decay is more effective in obtaining solutions with better generalization. Decoupled weight\ndecay is also implemented independent of learning rate grafting, that is,\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = (1 \u2212 \ud835\udefc\ud835\udc61\ud835\udf06)\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,graft\n\r\r\r\n\ud835\udc39\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo\n\r\r\r\ud835\udc43 (\ud835\udc56)\n\ud835\udc61,Shampoo\n\r\r\r\n\ud835\udc39\n.\n(30)\nUsage: To use weight decay with parameter \ud835\udf06, one should set the argument weight_decay.\nTo toggle between decoupled weight decay and \u21132 regularization, one can use the\nuse_decoupled_weight_decay flag, which is True by default.\n3.1.3\nMomentum and Nesterov Acceleration. For some applications, momentum and Nesterov accel-\neration are imperative for achieving good generalization performance and have been successfully\nemployed with the Shampoo optimizer [Anil and Gupta 2021]. This differs from first-moment\nestimation or gradient filtering in its functional form through its aggregation, not of the gradients,\nbut of the search direction of the algorithm. In particular, given the Shampoo search direction\n\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61) for layer \ud835\udc56 at weights \ud835\udc64\ud835\udc61, the momentum update is defined as:\n\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\n= \ud835\udf07\ud835\udc61\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\u22121 + \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61)\n(31)\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61\ud835\udc40 (\ud835\udc56)\n\ud835\udc61 ,\n(32)\nwith (potentially iterate-dependent) momentum parameter \ud835\udf07\ud835\udc61 > 0. Normally in practice, \ud835\udf07\ud835\udc61 = 0.9 is\nfixed over all iterations.\nSimilarly, what is known as Nesterov momentum or Nesterov acceleration applies a momentum-like\nterm a second time within the update:\n\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\n= \ud835\udf07\ud835\udc61\u22121\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\u22121 + \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61)\n(33)\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udefc\ud835\udc61 (\ud835\udf07\ud835\udc61\ud835\udc40 (\ud835\udc56)\n\ud835\udc61\n+ \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61)).\n(34)\nWhile momentum and Nesterov acceleration are widely used in conjunction with SGD, momentum\nand Nesterov acceleration methods are misnomers given that they arise from methods for mini-\nmizing deterministic quadratic functions and strongly convex functions with Lipschitz continuous\n\u2021It is equivalent to \u21132-regularization when using SGD through a reparameterization [Loshchilov and Hutter 2019].\n14\nShi et al.\ngradients, with a specialized choice of \ud835\udf07\ud835\udc61. These intuitions and approximations do not necessarily\nhold in the stochastic regime. We provide an alternative interpretation here, building on [Defazio\n2020] that re-interprets the methods as stochastic primal iterate averaging [Tao et al. 2018].\nIn particular, one can show that the momentum method (31)-(32) is equivalent to the iteration:\n\ud835\udc4d (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4d (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udf02\ud835\udc61\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61)\n(35)\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc50\ud835\udc61\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n+ (1 \u2212 \ud835\udc50\ud835\udc61)\ud835\udc4d (\ud835\udc56)\n\ud835\udc61+1\n(36)\nfor \ud835\udc50\ud835\udc61 \u2208 (0, 1) and \ud835\udf02\ud835\udc61 > 0. This is similar to exponential moving averaging applied on the weights,\na close variant of Polyak-Ruppert averaging [Polyak and Juditsky 1992] and stochastic weight\naveraging [Izmailov et al. 2018]. Rather than generating a sequence of averaged weights independent\nof the original sequence, this algorithm uses the intermediate averaged weights to determine the\nsearch direction at each step. Similarly, one can show that the Nesterov accelerated method (33)-(34)\nis equivalent to the iteration:\n\ud835\udc4d (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc4d (\ud835\udc56)\n\ud835\udc61\n\u2212 \ud835\udf02\ud835\udc61 (\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc43 (\ud835\udc56)\n\ud835\udc61\n(\ud835\udc64\ud835\udc61) \u2212 \ud835\udc43 (\ud835\udc56)\n\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))\n(37)\n\ud835\udc4a (\ud835\udc56)\n\ud835\udc61+1 = \ud835\udc50\ud835\udc61\ud835\udc4a (\ud835\udc56)\n\ud835\udc61\n+ (1 \u2212 \ud835\udc50\ud835\udc61)\ud835\udc4d (\ud835\udc56)\n\ud835\udc61+1.\n(38)\nA formal proof for both of these equivalences is provided in Appendix C.\nThis interpretation provides a principled approach for incorporating weight decay and gradient\nfiltering into momentum and Nesterov acceleration appropriately - momentum should be applied\non top of all changes to the update to the parameters, including the filtered gradient and weight\ndecay terms. Because of this interpretation, while gradient filtering and momentum may appear\nsimilar on the surface, they should be viewed as orthogonal changes to the algorithm, and therefore\nwe have included both options in our implementation. In addition, this technique can be used\neven when changing between different search directions, as it is primarily incorporating a form of\niterate averaging; this motivates our design choice of using a consistent momentum term for both\nthe grafting method and Shampoo when incorporating an initial grafting warmup phase.\nUsage: To enable momentum, simply set momentum to a positive number; 0.9 or 0.5 is a common\nsetting. To toggle Nesterov acceleration, set the Boolean variable use_nesterov.\n3.1.4\nExponent Override and Exponent Multiplier. Consistent with [Anil et al. 2020], we allow the\nuser to modify the exponent used for Shampoo through two options: exponent_override and\nexponent_multiplier. These two options correspond to the following change:\n\ud835\udc4a\ud835\udc61+1 = \ud835\udc4a\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc3f\u2212\ud835\udf02/\ud835\udc5d\n\ud835\udc61\n\ud835\udc3a\ud835\udc61\ud835\udc45\u2212\ud835\udf02/\ud835\udc5d\n\ud835\udc61\n(39)\nwhere \ud835\udf02 > 0 is the exponent multiplier and \ud835\udc5d \u2208 N corresponds to the exponent override. Note that\n\ud835\udc5d will override the standard root of 2\ud835\udf14, where \ud835\udf14 is the order of the tensor parameter. We have\nfound that using either an exponent override of \ud835\udc5d = 2 or exponent multiplier of \ud835\udf02 = 1.82 is often\neffective in practice for training networks dominated by fully-connected linear layers; for more\ndetails as to why this may be the case, see Shampoo\u2019s relationship with AdaFactor in Appendix B.\nUsage: To enable, one can set exponent_override as any integer and exponent_multiplier as\na positive number.\n3.2\nNumerical Considerations\nWhen implementing Shampoo, one must consider how to efficiently and accurately compute the\nroot inverse of the factor matrices. If the root inverse is computed too inaccurately, the resulting\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n15\nsearch directions may not even be guaranteed to be descent directions in expectation! However,\ncomputing the root inverse with unnecessarily high accuracy may significantly slow down the\niteration. In this subsection, we consider how to compute the root inverse of the preconditioner as\nwell as describe the empirical impact of numerical precision on the matrix root inverse computation.\n3.2.1\nMatrix Root Inverse Solvers. We describe different approaches that have been implemented\nfor computing the root inverse of each factor matrix. As noted above, all factor matrices \ud835\udc3f, \ud835\udc45 are\nsymmetric positive semi-definite by definition, and we want to compute the \ud835\udc5d-th inverse root of\n\ud835\udc3f\u22121/\ud835\udc5d, \ud835\udc45\u22121/\ud835\udc5d. By default, our implementation uses the symmetric eigendecomposition approach.\n(1) Symmetric Eigendecomposition: Since the factor matrices for each block preconditioner are\nsymmetric, we can apply the symmetric eigendecomposition solver torch.linalg.eigh\nto compute the eigendecomposition for each preconditioner. In particular, ignoring the\niteration number, let \ud835\udc3f = \ud835\udc44\ud835\udc3f\u039b\ud835\udc3f\ud835\udc44\ud835\udc47\n\ud835\udc3f and \ud835\udc45 = \ud835\udc44\ud835\udc45\u039b\ud835\udc45\ud835\udc44\ud835\udc47\n\ud835\udc45 be the eigendecompositions for \ud835\udc3f and\n\ud835\udc45, respectively, where \u039b\ud835\udc3f, \u039b\ud835\udc45 are diagonal matrices consisting of their eigenvalues and\n\ud835\udc44\ud835\udc3f,\ud835\udc44\ud835\udc45 are orthogonal matrices consisting of their eigenvectors. The standard approach for\ncomputing the root inverse is to compute the root inverse of the eigenvalues and reconstruct\nthe matrix by taking the root inverse of their eigenvalues, i.e., \ud835\udc3f\u22121/\ud835\udc5d = \ud835\udc44\ud835\udc3f\u039b\u22121/\ud835\udc5d\n\ud835\udc3f\n\ud835\udc44\ud835\udc47\n\ud835\udc3f and\n\ud835\udc45\u22121/\ud835\udc5d = \ud835\udc44\ud835\udc45\u039b\u22121/\ud835\udc5d\n\ud835\udc45\n\ud835\udc44\ud835\udc47\n\ud835\udc45. As expected, the computational cost of computing the matrix root\ninverse using a symmetric eigendecomposition is \ud835\udc42(\ud835\udc5b3).\nIn the presence of small positive or zero eigenvalues, numerical errors may cause some of the\neigenvalues returned by the eigendecomposition solver to be negative. This is problematic\nsince we cannot take the root inverse of a negative eigenvalue. Although one can add a\nmultiple \ud835\udf16\ud835\udc3c of the identity, it is not clear how large to choose \ud835\udf16 to avoid this problem. For\nthis reason, we incorporate a heuristic to ensure that all eigenvalues are sufficiently positive.\nThe heuristic is detailed as follows:\nSymmetric Eigendecomposition Approach for Computing Root Inverse\nGiven \ud835\udc3f \u2208 R\ud835\udc5b\u00d7\ud835\udc5b (or \ud835\udc45), perturbation \ud835\udf16 > 0, and desired exponent \ud835\udc5f.\n(a) Compute symmetric eigendecomposition \ud835\udf06,\ud835\udc44 \u2190 eigh(\ud835\udc3f) where \ud835\udf06 \u2208 R\ud835\udc5b and\n\ud835\udc44 \u2208 R\ud835\udc5b\u00d7\ud835\udc5b.\n(b) Compute \ud835\udf06min \u2190 min\ud835\udc56 \ud835\udf06\ud835\udc56.\n(c) Compute \ud835\udf06\ud835\udc5b\ud835\udc52\ud835\udc64 \u2190 \ud835\udf06 \u2212 min(\ud835\udf06min, 0)1 + \ud835\udf161.\n(d) Form and return matrix root inverse \ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc63 \u2190 \ud835\udc44 diag(\ud835\udf06\u2212\ud835\udc5f\n\ud835\udc5b\ud835\udc52\ud835\udc64)\ud835\udc44\ud835\udc47 .\nWe found this approach to be stable for even small choices of epsilon, such as \ud835\udf16 = 10\u221212, as\nsuggested in previous implementations.\n(2) Coupled Inverse Newton Iteration: Rather than employing a direct approach that decomposes\nthe factor matrices and constructs the root inverse, we can instead consider iterative methods\nto compute the root inverse. The coupled inverse Newton iteration is one such stable variant\nof Newton\u2019s method that requires an appropriate initialization of the matrix in order to\nguarantee convergence [Higham 2008]. If we are interested in computing the matrix root\n16\nShi et al.\ninverse of \ud835\udc3f \u2208 R\ud835\udc5b\u00d7\ud835\udc5b, the coupled inverse Newton iteration is defined as follows:\n\ud835\udc4b\ud835\udc58+1 = \ud835\udc4b\ud835\udc58\n\u0012 (\ud835\udc5d + 1)\ud835\udc3c \u2212 \ud835\udc40\ud835\udc58\n\ud835\udc5d\n\u0013\n,\n\ud835\udc4b0 = 1\n\ud835\udc50 \ud835\udc3c,\n(40)\n\ud835\udc40\ud835\udc58+1 =\n\u0012 (\ud835\udc5d + 1)\ud835\udc3c \u2212 \ud835\udc40\ud835\udc58\n\ud835\udc5d\n\u0013\ud835\udc5d\n\ud835\udc40\ud835\udc58,\n\ud835\udc400 = 1\n\ud835\udc50\ud835\udc5d \ud835\udc3f,\n(41)\nwhere \ud835\udc50 \u2208 R determines the initialization. Assuming proper initialization, one expects\n\ud835\udc4b\ud835\udc58 \u2192 \ud835\udc3f\u22121/\ud835\udc5d and \ud835\udc40\ud835\udc58 \u2192 \ud835\udc3c\ud835\udc5b in \ud835\udc42(\ud835\udc5b3) FLOPs.\nIn order to guarantee convergence of the algorithm (see Theorem 7.12 in [Higham 2008]),\nwe must establish that all the eigenvalues are contained in the interval [0, (\ud835\udc5d + 1)\ud835\udc50\ud835\udc5d). Since\n\ud835\udf06(\ud835\udc3f) \u2208 (0, \u2225\ud835\udc3f\u22252], it is sufficient to choose \ud835\udc50 such that \u2225\ud835\udc3f\u22252 < (\ud835\udc5d + 1)\ud835\udc50\ud835\udc5d. Note that \u2225\ud835\udc3f\u22252 is\nexpensive to compute, so we can instead bound \u2225\ud835\udc3f\u22252 \u2264 \u2225\ud835\udc3f\u2225\ud835\udc39 and require \u2225\ud835\udc3f\u2225\ud835\udc39 < (\ud835\udc5d + 1)\ud835\udc50\ud835\udc5d.\nTherefore, we must have \ud835\udc50 >\n\u0010\n\u2225\ud835\udc3f\u2225\ud835\udc39\n\ud835\udc5d+1\n\u00111/\ud835\udc5d\n. One practical choice of \ud835\udc50 is \ud835\udc50 =\n\u0010\n2\u2225\ud835\udc3f\u2225\ud835\udc39\n\ud835\udc5d+1\n\u00111/\ud835\udc5d\n.\nTo terminate the algorithm, we use the termination criterion based on \ud835\udc40\ud835\udc58 as suggested by\n[Higham 2008]:\n\u2225\ud835\udc40\ud835\udc58 \u2212 \ud835\udc3c \u2225\u221e < TOL\n(42)\nfor some tolerance TOL > 0. By default, we set TOL = 10\u22126. Note that this does not support\nthe exponent multiplier option.\nAlternative solvers for efficiently computing matrix root inverses is an active area of research (see\n[Fasi et al. 2023; Shumeli et al. 2022; Song et al. 2022]), and is left for future investigation.\n3.2.2\nPrecision for the Accumulation and Root Inverse Computation. It is common to use low preci-\nsion (FP16, BFLOAT16, FP8) in the forward and backward passes to compute the gradients. However,\nin order to ensure that we have sufficient accuracy in the matrix root inverse computation, we\naccumulate the factor matrices in FP32 or FP64 precision. With the symmetric eigendecomposition\napproach, we have found that using FP32 is sufficient, although the expected accuracy of the\ncomputation depends on the condition number as well as the gaps between consecutive eigenvalues\n[Golub and Van Loan 2013]. Therefore, the choice of precision may be model-dependent based on\nthe eigenvalue spectrum of each factor matrix for each parameter.\n3.2.3\nGuarding Against Eigendecomposition Failures. In order to protect against catastrophic failure\nof the torch.linalg.eigh kernel when applied to certain edge cases, we have enabled a retry\nmechanism with different precisions. The logic works as follows:\n(1) Attempt to compute eigh(L) in chosen precision (typically, FP32). If successful, continue.\n(2) Attempt to compute eigh(L.double()) in double precision. If successful, continue.\n(3) Otherwise, skip computation and proceed with previously computed matrix root inverse.\nUsage: The guarding mechanism is enabled by default through the flag use_protected_eigh.\n4\nMEMORY AND PERFORMANCE OPTIMIZATIONS\nIn this section, we describe some of the memory and performance optimizations to improve both\nthe memory footprint and speed (or wall-clock-time-per-iteration) of the algorithm. We focus\nprimarily on optimizing for GPU architectures, although CPU architectures are also supported by\nour implementation.\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n17\nFig. 3. A single optimizer step of Distributed Shampoo.\n4.1\nDistributed Memory and Preconditioner Computation\nWhile the Shampoo algorithm has been demonstrated to be more efficient than diagonal adaptive\ngradient methods at minimizing the objective function per-iteration, the additional FLOPs intro-\nduced by matrix multiplications (in lieu of element-wise multiplication) and passes to memory for\nintermediate buffer reads slow down the per-iteration wall-clock time. If we operate under the\nstandard distributed data-parallel regime, where the optimizer step is replicated across all workers,\neach step of Shampoo will be slower.\u00a7 An ideal practical implementation of Shampoo should have\nthe cost of each iteration be as efficient as diagonal-scaling-based adaptive gradient methods.\nIn order to reduce the memory footprint and improve the computational efficiency and systems\nutilization of our implementation, we propose to distribute the preconditioners and their associated\ncompute across all workers, similar to [Rajbhandari et al. 2020]. In particular, we will assign each\npreconditioner (including its factor matrices \ud835\udc3f, \ud835\udc45 and its grafting state \ud835\udc34) to only one or a small\nsubset of workers. Each worker will only be responsible for computing the matrix multiplications\nrequired for maintaining its assigned preconditioners\u2019 optimizer states, as well as the corresponding\npart of the global search direction. After each preconditioned search direction is computed, we\nperform an AllGather so that all workers have the search directions for all parameters, and\nthen they apply the parameter updates. An additional sufficiently sized buffer is required for this\ncommunication.\nThe pseudocode for this optimization is detailed in Algorithm 2. Figure 3 shows how a sin-\ngle Shampoo step is distributed and communicated with this optimization. We detail how we\nimplemented this optimization further below.\n4.1.1\nPreconditioner Assignment and Load-Balancing via Greedy Algorithm. In order to distribute\nthe preconditioner memory and computation, the preconditioners need to be partitioned across all\nworkers. Since the AllGather is performed on the search directions, we choose to load-balance\nbased on its memory cost and assign preconditioners to ensure that the maximum buffer size for\neach worker is minimized. To do this, we employ a sorted greedy approximation algorithm as\ndescribed in Algorithm 3. The key idea is to sort the parameters based on number of variables, and\nassign each parameter in descending order to the worker with the fewest variables. The assignments\nare made prior to the instantiation of the preconditioners; see Algorithm 2.\nThe distributed AllGather buffer will have length \ud835\udc44\ud835\udc3a max\ud835\udc57 \u2208[\ud835\udc5b] \ud835\udc36\ud835\udc57. In our implementation, we\nchoose to use the int8 data type for the distributed buffer, regardless of the precision being used.\n\u00a7In the case of some large-scale models, each step could be potentially even 50-75% slower than standard diagonal\nadaptive gradient methods!\n18\nShi et al.\nAlgorithm 3 Greedy Load-Balancing Assignment for Homogeneous Architectures\nRequire: Number of variables per parameter \ud835\udc51 (1), ...,\ud835\udc51 (\ud835\udc5b), total number of workers (world size) \ud835\udc3d,\nnumber of workers per process group \ud835\udc3d\ud835\udc3a\nSort the parameters such that \ud835\udc51 (\ud835\udc581) \u2265 \ud835\udc51 (\ud835\udc582) \u2265 ... \u2265 \ud835\udc51 (\ud835\udc58\ud835\udc5b) for \ud835\udc58\ud835\udc56 \u2208 [\ud835\udc5b].\nInitialize assignment sets \ud835\udc3c1 = {}, ..., \ud835\udc3c\ud835\udc3d = {}, where \ud835\udc3c\ud835\udc57 assigns the indexed parameters in the set\nto worker \ud835\udc57.\nInitialize variable counters \ud835\udc361 = 0, ...,\ud835\udc36\ud835\udc3d\ud835\udc3a = 0.\nfor \ud835\udc56 = 1, ...,\ud835\udc5b do\nFind the workers with the least variables: \u02dc\ud835\udc58\ud835\udc56 \u2208 arg min\ud835\udc58\u2208[\ud835\udc5b] \ud835\udc36\ud835\udc58.\nAssign \ud835\udc3c(\ud835\udc57\u22121)\ud835\udc3d\ud835\udc3a+ \u02dc\ud835\udc58\ud835\udc56 \u2190 \ud835\udc3c(\ud835\udc57\u22121)\ud835\udc3d\ud835\udc3a+ \u02dc\ud835\udc58\ud835\udc56 \u222a {\ud835\udc56} for all \ud835\udc57 \u2208 [\ud835\udc3d/\ud835\udc3d\ud835\udc3a].\nend for\nReturn assignments {\ud835\udc3c\ud835\udc57}\ud835\udc3d\n\ud835\udc57=1.\nFig. 4. Maximum buffer size allocation for AllGather primitive. In practice, the preconditioned tensors is a\ncollection of view() of the 1D communication buffer tensor to avoid extra copy at (\u2217) in the figure.\nFigure 4 shows how using the maximum buffer size allocation may result in additional memory\nconsumption.\n4.1.2\nBalancing Computation and Communication Through Multiple Process Groups. As opposed to\ndistributing the preconditioners across all workers, which may lead to high communication costs\nrelative to the amount of compute per-worker, one can instead create multiple distinct process\ngroups that partition the global world into smaller process groups. By distributing the computation\nwithin each process group while replicating the computation across different process groups, we\ncan achieve more balanced compute and communication costs and observe higher performance\ngains. This form of hierarchical parallelism maps efficiently to the underlying systems architecture\nand topology as well.\nWe therefore provide a user parameter num_trainers_per_group (corresponding to \ud835\udc44\ud835\udc3a in Algo-\nrithm 3), which specifies the number of workers each process group should contain. Here, we assume\nthat the user is running the algorithm on a homogeneous system architecture, where each node\ncontains the same number of workers. In particular, we require that the num_trainers_per_group\ndivides the total world size with no remainder. By default, num_trainers_per_group is equal to\nthe number of workers per node, although this is usually not ideal for training large-scale models.\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n19\nMatrix (\ud835\udc511 \u00d7 \ud835\udc512)\nOrder-\ud835\udf14 Tensor (\ud835\udc511 \u00d7 ... \u00d7 \ud835\udc51\ud835\udf14)\nLargeDimMethod\nMemory Cost\nComputational Cost\nMemory Cost\nComputational Cost\nBLOCKING\n4\ud835\udc511\ud835\udc512\n\ud835\udc42(\ud835\udc4f3)\n2\ud835\udf14\n\ud835\udc4f\ud835\udf14\u22122\n\u00ce\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56\n\ud835\udc42(\ud835\udc4f3)\nADAGRAD\n\ud835\udc511\ud835\udc512\n\ud835\udc42(\ud835\udc511\ud835\udc512)\n\u00ce\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56\n\ud835\udc42(\u00ce\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56)\nDIAGONAL\n\ud835\udc511 + \ud835\udc512\n\ud835\udc42(\ud835\udc511\ud835\udc512)\n\u00cd\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56\n\ud835\udc42(\u00ce\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56)\nTable 1. Summary of memory and computational requirements for different large-dimensional methods for\nmatrices and general tensors. Assumes that \ud835\udc4f is the block size.\n4.1.3\nDTensor State Allocation. In order to distribute the memory used by the factor matrices,\ngrafting, momentum, and filtered gradient states, we use a new PyTorch data structure called\nDTensor (for \u201cDistributed Tensor\u201d), which enhances the standard Tensor data structure with mesh\ninformation that describes how the tensor should be sharded or replicated across multiple workers.\nThis enables DTensor to support multi-level parallelism, including various combinations of data\nparallelism, tensor parallelism, and pipeline parallelism. By using DTensor, we can specify the\ntensor to be replicated across only a small subset of workers, while recognizing the existence of the\ndistributed tensor on every rank, which is necessary for creating efficient distributed checkpointing\nsolutions.\nThis solution enables us to approximately reduce the overall memory cost per-worker by a\nfactor of \ud835\udc44\ud835\udc3a. Note that this will depend on the quality of load-balancing, which depends on the\ndistribution of the parameter shapes. To enable DTensor, one can use the use_dtensor flag (this is\nenabled by default).\n4.2\nHandling Large-Dimensional Tensors\nShampoo significantly reduces the amount of memory required to produce a block-diagonal ap-\nproximation compared to full-matrix AdaGrad. However, for tensors with large dimensions, i.e.,\n\ud835\udc51\ud835\udc56 \u226b 0 for some \ud835\udc56, it is still possible for Shampoo to remain infeasible in terms of its computational\nand memory cost. In order to reduce memory consumption, we have enabled multiple approaches\nfor handling large tensors consistent with those suggested in [Anil et al. 2020; Gupta et al. 2018].\nWe present these approaches for completeness in the order from most-to-least memory-consuming.\nAll approaches rely on the same hyperparameter max_preconditioner_dim. The memory and\ncomputational cost of each of these approaches is summarized in Table 1.\n4.2.1\nMerging and Blocking. Instead of applying Shampoo to the full tensor, we can instead\nreshape the tensor by merging small dimensions and blocking the tensor into multiple smaller\nsub-tensors. On one extreme, blocking enables us to use a coarser approximation at lower memory\nand computational cost. It is an ideal approximation since it preserves the original tensor structure\nof the parameters that Shampoo relies upon for its Kronecker product approximation. On the other\nextreme, merging dimensions enables us to remove unnecessary dimensions and move towards\nusing full-matrix AdaGrad for that particular parameter.\nMerging small dimensions involves setting a maximum dimension and merging consecutive\ndimensions until its product exceeds the maximum dimension. For example, with maximum\ndimension 8, a 10 \u00d7 2 \u00d7 2 \u00d7 4 dimensional tensor would be reshaped to 10 \u00d7 4 \u00d7 4 after merging.\nThis is particularly useful for getting rid of redundant (or unit) dimensions. We merge consecutive\ndimensions in order to ensure that no data movement is required and only torch.view is necessary\nto reshape the tensor. If all dimensions are merged, then Shampoo is applied to a vector, which is\nequivalent to applying full-matrix AdaGrad.\n20\nShi et al.\nFig. 5. Picture of merging (left) and blocking (right).\nBlocking takes a tensor and creates multiple sub-tensors with a given block size \ud835\udc4f. For example,\nfor a second-order tensor (or matrix) \ud835\udc4a \u2208 R\ud835\udc5a\u00d7\ud835\udc5b, we may block the matrix as:\n\ud835\udc4a =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\ud835\udc4a1,1\n\ud835\udc4a1,2\n...\n\ud835\udc4a1,\ud835\udc58\ud835\udc5b\n\ud835\udc4a2,1\n\ud835\udc4a2,2\n...\n\ud835\udc4a2,\ud835\udc58\ud835\udc5b\n...\n...\n...\n...\n\ud835\udc4a\ud835\udc58\ud835\udc5a,1\n\ud835\udc4a\ud835\udc58\ud835\udc5a,2\n...\n\ud835\udc4a\ud835\udc58\ud835\udc5a,\ud835\udc58\ud835\udc5b\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwhere \ud835\udc58\ud835\udc5a = \u2308\ud835\udc5a/\ud835\udc4f\u2309 and \ud835\udc58\ud835\udc5b = \u2308\ud835\udc5b/\ud835\udc4f\u2309. Note that one can block such that \ud835\udc4a\ud835\udc56,\ud835\udc57 are all similar in size\n(which is not necessarily \ud835\udc4f \u00d7 \ud835\udc4f) or such that \ud835\udc4a\ud835\udc56,\ud835\udc57 \u2208 R\ud835\udc4f\u00d7\ud835\udc4f for \ud835\udc56 = 1, ...,\ud835\udc58\ud835\udc5a \u2212 1 and \ud835\udc57 = 1, ...,\ud835\udc58\ud835\udc5b \u2212 1. In\nour implementation, we opt for the latter in order to best exploit the GPU\u2019s capabilities.\nShampoo is then applied to each block \ud835\udc4a\ud835\udc56,\ud835\udc57. Note that this also corresponds to partitioning the\nfactors for \ud835\udc4a into smaller blocks, i.e., if \ud835\udc3f and \ud835\udc45 correspond to the left and right preconditioner\nfactors for \ud835\udc4a , then:\n\ud835\udc3f1/2 \u2297 \ud835\udc451/2 \u21a6\u2192 \ud835\udc43\ud835\udc47\n\ud835\udf0b\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\ud835\udc3f1/2\n1,1 \u2297 \ud835\udc451/2\n1,1\n0\n...\n0\n0\n\ud835\udc3f1/2\n1,2 \u2297 \ud835\udc451/2\n1,2\n...\n0\n0\n0\n...\n0\n0\n0\n...\n\ud835\udc3f1/2\n\ud835\udc59,\ud835\udc58 \u2297 \ud835\udc3f1/2\n\ud835\udc59,\ud835\udc58\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\ud835\udc43\ud835\udf0b.\nwhere \ud835\udc43\ud835\udf0b is a permutation matrix that maps \ud835\udc64 = vec(\ud835\udc4a )\ud835\udc47 to\n\ud835\udc64\ud835\udf0b = (vec(\ud835\udc4a1,1)\ud835\udc47, vec(\ud835\udc4a1,2)\ud835\udc47, ..., vec(\ud835\udc4a\ud835\udc59,\ud835\udc58)\ud835\udc47 )\ud835\udc47 = \ud835\udc43\ud835\udf0b\ud835\udc64.\nWe use the same block size hyperparameter, called max_preconditioner_dim in our implementa-\ntion, for both merging and blocking. Merging and blocking therefore has a multi-faceted impact on\nmodel quality, memory, and performance. We summarize the impact of modifying the block size\non each of these aspects below:\n(1) Model Quality: As the block size increases, we expect the model quality to improve because\nour approximation will remove dimensions and eventually use full-matrix AdaGrad for\nthat parameter. This incentivizes using large block sizes as long as the factor matrices fit in\nmemory and the algorithm\u2019s performance is not too slow.\n(2) Memory: For a general order-\ud835\udf14 tensor\ud835\udc4a \u2208 R\ud835\udc511\u00d7...\u00d7\ud835\udc51\ud835\udf14 and block size\ud835\udc4f that divides\ud835\udc511, ...,\ud835\udc51\ud835\udf14,\nthe total memory cost of blocked Shampoo is\n2\ud835\udf14\n\ud835\udc4f\ud835\udf14\u22122\n\u00ce\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56. The factor 2 arises because we\nhave to store both the factor matrices and their root inverses. Note that if \ud835\udf14 < 2, then as \ud835\udc4f\nincreases, the memory cost increases. However, if \ud835\udf14 > 2, then as \ud835\udc4f increases, the memory\ncost decreases. In the matrix case (\ud835\udf14 = 2), blocked Shampoo has constant memory cost\n4\ud835\udc511\ud835\udc512.\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n21\n(3) Performance: Using too small of a block size can lead to high latency from increased\nGPU/CUDA kernel launch overheads and reduced compute efficiency. On the other hand,\nusing too large of a block size results in large factor matrices that are costly to invert.\nTherefore, performance is optimized by a set of block sizes that trade off these two extremes.\nIn our experience, using a block size \ud835\udc4f \u2208 {1024, 2048, 4096, 8192} is ideal for performance.\n4.2.2\nDiagonal AdaGrad Preconditioner. Alternatively, we provide the option to use the standard\ndiagonal AdaGrad, RMSProp, or Adam preconditioner in place of Shampoo if any of the dimensions\nexceeds max_preconditioner_dim. This reduces the memory cost to \ud835\udc511\ud835\udc512 for the matrix case and\n\u00ce\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56 for the general order-\ud835\udf14 tensor case, and offers the same performance as diagonal adaptive\ngradient methods. In general, we expect this approach to yield model accuracies between blocked\nShampoo and diagonal Shampoo.\n4.2.3\nDiagonal Shampoo Preconditioner. Lastly, we can also diagonalize each factor matrix for\ndimensions larger than max_preconditioner_dim. In the two-dimensional case, this reduces to\nusing \u02dc\ud835\udc3f\ud835\udc61 = matdiag(\ud835\udc3f\ud835\udc61) and \u02dc\ud835\udc45\ud835\udc61 = matdiag(\ud835\udc45\ud835\udc61). Note that this reduces the memory cost to \ud835\udc511 + \ud835\udc512\nfor the matrix case and \u00cd\ud835\udf14\n\ud835\udc56=1 \ud835\udc51\ud835\udc56 for the general tensor case. Since the matrix is diagonal, it is not\nnecessary to store the root inverse matrices. This approximation may be useful for very large\ntensors, such as embedding tables, but yields a worse approximation than diagonal AdaGrad if\nall dimensions are diagonalized. Diagonal Shampoo shares a close relationship with AdaFactor\n[Shazeer and Stern 2018] and row-wise AdaGrad [Gupta et al. 2014; Mudigere et al. 2022]; see\nAppendix B for more details.\n4.3\nPeriodic Root Inverse Computation\nSince the most expensive computation is the root inverse computation, one natural way of reducing\nthe overall wall-clock time of each iteration is to only periodically compute the matrix root inverse of\nthe factor matrices, similar to [Anil et al. 2020]. This is controlled by the precondition_frequency\nhyperparameter. This speedup comes at the cost of using stale root inverse matrices, which can\nslow convergence and impact the final model quality achieved by the optimizer.\nStaleness can particularly have a detrimental impact on convergence at the beginning of training,\nwhen the preconditioners are less stable. For this reason, we also incorporate a hyperparam-\neter start_preconditioning_step for delaying Shampoo preconditioning. Prior to iteration\nstart_preconditioning_step, Distributed Shampoo will take steps using the grafted method\nbefore switching to Shampoo preconditioning (with grafting).\nBoth of these optimizations are consistent with [Anil et al. 2020]. However, because we are\nprimarily focused on supporting hardware architectures that support higher precision, we do not\noffload the matrix root inverse computation to CPU.\n4.4\nComparison with JAX Implementation for TPU/CPU Architectures\nWhile the core algorithm and some of the performance optimizations such as merging, blocking, and\nthe periodic computation of the matrix root inverses are shared across our PyTorch implementation\nand the JAX/OPTAX implementation [Anil et al. 2020], key framework (PyTorch vs JAX/OPTAX)\nand hardware architecture (homogeneous GPU and CPU architectures vs heterogeneous TPU/CPU\narchitectures) differences lead to some critical differences between these two implementations. We\ndiscuss these differences below.\n4.4.1\nCPU Offloading. Since both GPU and CPU natively support FP32 and FP64 computation,\nour implementation does not offload the root inverse computation onto CPU to avoid unnecessary\n22\nShi et al.\ndata movement. This contrasts with the JAX implementation for TPU/CPU architectures, which do\nnot offer FP32 or FP64 support, and therefore makes offloading a necessity [Anil et al. 2020].\nThis specifically impacts the staleness of the root inverse matrices. While the matrix root\ninverses in the PyTorch implementation will be stale for up to precondition_frequency iterations\n(before all root inverse matrices are re-computed based on the updated factor matrices), the JAX\nimplementation will be stale for 2 \u00d7 precondition_frequency, as its offloading onto CPU and\noverlapping of the matrix root inverse computation on CPU with Shampoo\u2019s (stale) preconditioned\nupdates on TPU creates two periods of staleness.\n4.4.2\nCompiler vs Hand-Optimized Kernels and Communications. Prior to PyTorch 2.0, PyTorch\ndid not offer a compiler that can automatically fuse operators using torch.compile. JAX, on the\nother hand, relies on XLA to compile and run NumPy programs on GPU and TPU. As a result, our\nPyTorch implementation requires the use of hand-optimized kernels in order to run efficiently.\nOne example is the use of PyTorch\u2019s _for_each operators, which fuse the element-wise operators\nfor each parameter together. Our communications optimizations and distributed buffer instantiation\nare also explicitly defined, unlike in the JAX implementation. Incorporation of PyTorch 2.0 is left\nfor future work.\n4.4.3\nFP32 vs FP64 Default Precision. Unlike Anil et al. [2020], we have found that using single\nprecision is often sufficient for our workloads, although we provide the option for the user to\nspecify the factor matrix precision through preconditioner_dtype. To further avoid failure of\nthe eigendecomposition, we have enabled a guarding mechanism as described in Section 3.2.3.\n4.4.4\nEigendecomposition vs Coupled Inverse Newton. By default, our implementation uses Py-\nTorch\u2019s Hermitian/symmetric eigendecomposition operator torch.linalg.eigh, which internally\ncalls CUSOLVER\u2019s symmetric eigendecomposition solvers. The JAX implementation instead relies\non a warm-started coupled inverse Newton, as described in [Anil et al. 2020].\n5\nNUMERICAL RESULTS\nIn this section, we provide experimental results for training a ResNet50 model, which contains 25.5M\ntrainable parameters, on the ImageNet-1k dataset [Deng et al. 2009; He et al. 2016]. We compare our\nimplementation of Distributed Shampoo against the standard baseline on this workload, SGD with\nNesterov acceleration. The results demonstrate that Shampoo can provide significant reductions in\noverall wall-clock time and number of steps compared to a well-tuned Nesterov baseline. These\nreductions are observed despite the additional FLOPs incurred by Shampoo\u2019s update rule.\nWe concentrate on three sets of experiments:\n(1) Comparing the performance of both methods with a fixed training budget of 90 epochs,\nwhich is the standard budget for optimal validation accuracy when training with SGD with\nNesterov momentum.\n(2) Comparing the performance of both methods to achieve a given target validation accuracy,\nwithout constraining the budget of training epochs. This enables us to observe substantial\nsavings in overall wall-clock time as well as number of epochs to achieve a fixed validation\naccuracy by Shampoo.\u00b6\n(3) Evaluating the sensitivity of Shampoo and Nesterov to the choice of base learning rate.\nWe use the notation average\n\u0002 max\nmin\n\u0003\n, when referring to aggregate results across multiple seeds.\n\u00b6Note that an experiment with more training epochs is not equivalent to merely continuing an experiment with fewer\nepochs since the learning rate schedule depends directly on the total number of steps in each experiment.\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n23\n5.1\nExperimental Setup\nWe use SGD with Nesterov momentum of \ud835\udf07 = 0.9 with a linear warmup-then-cosine learning rate\nscheduler as the baseline for our experiments. This choice of the learning rate schedule is standard\npractice in the computer vision community; see Section 5.1 of [He et al. 2019].\nTo highlight the versatility of Shampoo as an enhancement to existing training pipelines, our\nexperiments use SGD as the grafted method, matching the optimizer choice in the baseline. A\ncomprehensive description of the choice of hyperparameters for both methods are included in\nAppendix D.1.\nMuch of the additional overhead incurred by Shampoo depends on two factors: (i) the com-\nputation of the inverse preconditioners, and (ii) the preconditioning of the observed gradient to\nobtain the Shampoo search direction. We can control (i) by amortizing the cost of the precon-\nditioner computation across multiple iterations, using stale preconditioners in-between updates.\nMoreover, the cost of the preconditioner inversion will also be governed by the maximum allowed\npreconditioner dimension. Notably, in the ImageNet task, Shampoo can operate effectively with a\npreconditioner update frequency of 50 steps, and a maximum preconditioner dimension of 2048\n(beyond which blocking is applied, see \u00a74.2) with minimal overhead. All experiments below use\nthese settings. Appendix D.2 contains ablations on the choices of the max_preconditioner_dim\nand precondition_frequency hyperparameters.\n5.2\nResults and Analysis\n5.2.1\nFixed Epoch Budget. Figure 6 shows top-1 accuracy and cross-entropy loss metrics under\na fixed training budget of 90 epochs. Shampoo consistently achieves better validation accuracy\nthan Nesterov, at 77.44%\n\u0002 77.58\n77.36\n\u0003\nvs 76.85%\n\u0002 76.93\n76.78\n\u0003\n. The improvements in the validation metrics by\nShampoo can be observed throughout training with respect to both steps and wall-clock time.\nNotably, the accuracy and loss measurements for Shampoo in the validation set are significantly less\nvolatile than those of the Nesterov runs. This reduction in variability is desirable since it indicates\nmore robust behavior of the optimizer, and makes individual runs more informative of the method\u2019s\ngeneral performance. Despite the increased complexity of the update rule, using the amortization\nscheme above, Shampoo only incurs an 8% wall-clock time overhead to complete 90 epochs.\nWe want to emphasize that, in these experiments, Shampoo is run using exactly the same hy-\nperparameter values as in the Nesterov training recipe with grafting from SGD (including the\nnumber of epochs, base learning rate, learning rate scheduler, and weight decay), and that these\nhyperparameters were specifically tuned for Nesterov. The only hyperparameter tuning we per-\nformed for Shampoo were the ablations on the hyperparameters max_preconditioner_dim and\nprecondition_frequency (see App. D.2) to determine an acceptable trade-off between precondi-\ntioner staleness and computational overhead.\nThere is also a qualitative difference in the generalization gap induced by the different optimizers\nthroughout training. Interestingly, Shampoo produces models whose accuracy and loss track\neach other more closely between training and validation compared to Nesterov. This disparity is\nmost evident at the beginning of training and is reduced in later stages. It may be precisely this\ncloser tracking of the validation metrics that underlies Shampoo\u2019s improvements over SGD. An\nunderstanding of Shampoo\u2019s \u201cimplicit regularization\u201d is left for future research.\n5.2.2\nEpoch Budget Ablation. Figure 7 displays the results of experiments with a changing training\nbudget, between 40 and 90 epochs. Across all epoch budgets, Shampoo displays a similar reduction in\nthe volatility of the validation metrics as discussed above, and reliably achieves better performance\nthan Nesterov.\n24\nShi et al.\nShampoo\nNesterov\nTrain\nValidation\n0.00\n0.25\n0.50\n0.75\n1.00\nStep\n1e5\n20\n40\n60\n80\nTop-1 Accuracy\nShampoo\nNesterov\n0\n2\n4\n6\n8\n10\n12\n14\nWall-clock hours\n20\n40\n60\n80\nTop-1 Accuracy\nShampoo\nNesterov\n0.00\n0.25\n0.50\n0.75\n1.00\nStep\n1e5\n2\n4\n6\n8\nCross-Entropy Loss\nShampoo\nNesterov\n0\n2\n4\n6\n8\n10\n12\n14\nWall-clock hours\n2\n4\n6\n8\nCross-Entropy Loss\nShampoo\nNesterov\nFig. 6. Top-1 accuracy and cross-entropy loss on the ImageNet dataset. Shaded regions show min-max bounds\nacross 5 seeds. Bounds on the training metrics are omitted for readability. All throughout training, the\niterates visited by Shampoo achieve better validation accuracy and less variability than those of\nthe Nesterov baseline.\nFigures 7 (c) and (d) show the speed-ups in terms of number of steps and wall-clock time required\nfor Shampoo to achieve the same validation accuracy as when training with Nesterov for 90 epochs.\nShampoo required 1.5x fewer steps and 1.35x less time than the Nesterov baseline. Similarly, Figures\n7 (g) and (h) demonstrate that Shampoo yields speed-ups of 1.8x in terms of steps and 1.69x in\nterms of wall-clock time, to achieve the same validation loss as when training with Nesterov for 90\nepochs.\n5.2.3\nSensitivity to the Learning Rate. Figure 8 displays the validation loss and accuracy for both\nmethods trained using different values for the base learning rate. These experiments use a fixed\nbudget of 90 epochs. Ideally, adaptive methods should provide better robustness to choices of\ncertain hyperparameters like the learning rate. As seen in Figure 8, the performance of Shampoo is\nreliably superior to that of Nesterov over the tested range of learning rates. Nevertheless, different\nvalues of the learning rate lead to significant performance changes for both methods. These results\nindicate that, while Shampoo is a promising preconditioned gradient technique, further research is\nrequired to improve the method\u2019s robustness to hyperparameter choices.\n6\nRELATED WORK\nThere has been extensive research on the design of preconditioned stochastic gradient algorithms\nfor training deep neural networks. The stochastic gradient method initially enabled the training of\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n25\nShampoo\nNesterov\n0\n10 20 30 40 50 60 70 80 90\nEpoch\n0\n20\n40\n60\n80\nTop-1 Accuracy\nShampoo\nNesterov\n0\n2\n4\n6\n8\n10\n12\n14\nWall-clock hours\n0\n20\n40\n60\n80\nTop-1 Accuracy\nShampoo\nNesterov\n0\n10 20 30 40 50 60 70 80 90\nEpoch\n50\n55\n60\n65\n70\n75\nTop-1 Accuracy\nSpeed-up\n - 1.5x -\nShampoo\nNesterov\n0\n2\n4\n6\n8\n10\n12\n14\nWall-clock hours\n50\n55\n60\n65\n70\n75\nTop-1 Accuracy\nSpeed-up\n - 1.35x -\nShampoo\nNesterov\n0\n10 20 30 40 50 60 70 80 90\nEpoch\n2\n4\n6\n8\nCross-Entropy Loss\nShampoo\nNesterov\n0\n2\n4\n6\n8\n10\n12\n14\nWall-clock hours\n2\n4\n6\n8\nCross-Entropy Loss\nShampoo\nNesterov\n0\n10 20 30 40 50 60 70 80 90\nEpoch\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nCross-Entropy Loss\nSpeed-up\n - 1.8x -\nShampoo\nNesterov\n0\n2\n4\n6\n8\n10\n12\n14\nWall-clock hours\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nCross-Entropy Loss\nSpeed-up\n - 1.69x -\nShampoo\nNesterov\nFig. 7. Ablation on achieved validation performance with a changing budget of training epochs. Second and\nfourth rows correspond to a detail view of the first and third rows, respectively. Shaded regions show min-max\nbounds across 5 seeds. Training metrics are omitted for readability. 60-epoch Shampoo delivers a 1.35x\nreduction in terms of the wall-clock time required to achieve the validation accuracy of 90-epoch\nSGD. This corresponds to a 1.5x step-wise reduction.\n26\nShi et al.\n10\n2\n10\n1\nBase Learning Rate\n0.90\n0.95\n1.00\n1.05\nLoss\nShampoo\nNesterov\n10\n2\n10\n1\nBase Learning Rate\n74\n75\n76\n77\nTop-1 Accuracy\nShampoo\nNesterov\nFig. 8. Sensitivity of Nesterov and Shampoo to changes in the base learning rate. Plot shows metrics on\nthe validation set, with markers indicating different seeds. Shampoo achieves consistently better loss\nand accuracy than Nesterov across a wide range of choices of the base learning rate. However, the\nperformance of both methods is still heavily influenced by this hyperparameter.\nmachine learning models on large-scale datasets via stochastic approximation [Bottou 2010; Bottou\net al. 1991; LeCun et al. 1998; Robbins and Monro 1951]. Subsequent work focused on improving\nthe stochastic gradient method by incorporating momentum [Rumelhart et al. 1986] and iterate\naveraging techniques [Polyak and Juditsky 1992]. Multiple directions were subsequently pursued\nto improve upon the stochastic gradient method by incorporating preconditioning, both for general\nstochastic optimization and online convex optimization. Bottou et al. [2018] provides a good review\nof such methods. We elaborate on the three main directions next.\nThe first group of methods extend deterministic smooth optimization methods that utilize\ncurvature information, such as Newton and quasi-Newton methods, for stochastic optimization. This\nclass of methods has typically relied on diagonal approximations [Bordes et al. 2009], sub-sampling\nor sketching the Hessian [Berahas et al. 2020; Pilanci and Wainwright 2017; Xu et al. 2020a,b,\n2016], ensuring consistency when evaluating gradient differences [Berahas et al. 2016; Schraudolph\net al. 2007], re-sampling correction pairs [Berahas et al. 2022], and using adaptive sampling or\nprogressive batching, i.e., increasing the batch size via adaptive mechanisms [Bollapragada et al.\n2018a,b; Devarakonda et al. 2017]. These methods were investigated in the context of deep learning\nby [Martens et al. 2010; Martens and Sutskever 2011, 2012]. Most recently, Kronecker product\napproximations have also been applied to quasi-Newton methods through the development of the\nK-BFGS algorithm [Goldfarb et al. 2020].\nThe second group of methods extend the natural gradient method [Amari 1998] for training\nneural networks. The natural gradient method has been shown to be equivalent to the generalized\nGauss-Newton method [Kunstner et al. 2019; Martens 2020] and has been further analyzed in\nZhang et al. [2019]. K-FAC was the first method to propose using block-diagonal preconditioners\nwith Kronecker product approximations for training neural networks [Martens and Grosse 2015].\nThis method, which was built on top of the natural gradient method, was extended to different\nlayer types and distributed setups; see [Ba et al. 2017; George et al. 2018; Grosse and Martens 2016;\nMartens et al. 2018]. Alternatives that extend the natural gradient method such as TNT have also\nbeen proposed in Ren and Goldfarb [2021].\nLastly, a class of preconditioned gradient methods, known as adaptive gradient methods, precon-\nditioned the (stochastic) gradient by the accumulation of outer products of the observed gradients;\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n27\nsee [Duchi et al. 2011]. Originally designed for online (possibly nonsmooth) convex optimization,\nthe diagonally approximated forms of these methods have gained wide traction in the deep learning\ncommunity. Subsequent works extended these methods by incorporating other heuristics, such\nas gradient filtering, decoupled weight decay, and block re-scalings; see [Kingma and Ba 2015;\nLoshchilov and Hutter 2019; You et al. 2020, 2018]. The work on (Distributed) Shampoo exploited\nKronecker product approximations akin to K-FAC to design specialized adaptive gradient method\nfor training deep networks [Anil et al. 2020; Gupta et al. 2018].\nIn terms of performance optimizations, our implementation shares the most similarities with\nDeepSpeed\u2019s ZeRO-1 optimizer, which shards the optimizer states to optimize memory for large-\nscale models [Rajbhandari et al. 2020, 2021]. Our performance optimizations can also be interpreted\nas using solely the optimizer portion of PyTorch\u2019s Fully Sharded Data Parallel (FSDP) and Hybrid\nSharded Data Parallel (HSDP) [Zhao et al. 2023].\nACKNOWLEDGMENTS\nWe thank Rohan Anil and Vineet Gupta for the original development of the Distributed Shampoo\nalgorithm, its implementation in JAX, and their suggestions. We also thank Simon Lacoste-Julien\nfor his support of this work.\nWe thank Adnan Aziz, Malay Bag, Pavan Balaji, Xiao Cai, Shuo Chang, Nikan Chavoshi, Wenlin\nChen, Xi Chen, Ching-Hsiang Chu, Weiwei Chu, Aaron Defazio, Alban Desmaison, Quentin Duval,\nAssaf Eisenman, Zhuobo Feng, Leon Gao, Andrew Gu, Yizi Gu, Yuchen Hao, Tao Hu, Yusuo\nHu, Yuxi Hu, Jianyu Huang, Minhui Huang, Shakti Kumar, Ming Liang, Mark Kim-Mulgrew,\nGuna Lakshminarayanan, Ming Liang, Wanchao Liang, Xing Liu, Ying Liu, Liang Luo, Yinbin Ma,\nWenguang Mao, Maxim Naumov, Jongsoo Park, Yi Ren, Ke Sang, Xinyue Shen, Min Si, Dennis van\nder Staay, Ping Tak Peter Tang, Fei Tian, James Tian, Andrew Tulloch, Sanjay Vishwakarma, Ellie\nWen, Lin Xiao, Shawn Xu, Ye Wang, Chunzhi Yang, Jiyan Yang, Lin Yang, Chunxing Yin, Christina\nYou, Jiaqi Zhai, Susan Zhang, Zhang Zhang, Gedi Zhou, and Wang Zhou for their excellent internal\ncontributions, support, feedback, and backing of this work.\nREFERENCES\nNaman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang. 2020. Disentangling Adaptive Gradient Methods\nfrom Learning Rates. arXiv:2002.11803 (2020).\nShun-Ichi Amari. 1998. Natural Gradient Works Efficiently in Learning. Neural Computation (1998).\nRohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips, Cristina Pop, Kevin Regan,\nGil I Shamir, et al. 2022. On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models.\narXiv:2209.05310 (2022).\nRohan Anil and Vineet Gupta. 2021. Distributed Shampoo Implementation. https://github.com/google-research/google-\nresearch/tree/master/scalable_shampoo.\nRohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. 2020. Scalable Second Order Optimization for\nDeep Learning. arXiv:2002.09018 (2020).\nJimmy Ba, Roger Grosse, and James Martens. 2017. Distributed Second-Order Optimization using Kronecker-Factored\nApproximations. In ICLR.\nAlbert S Berahas, Raghu Bollapragada, and Jorge Nocedal. 2020. An investigation of Newton-Sketch and subsampled\nNewton methods. Optimization Methods and Software 35, 4 (2020), 661\u2013680.\nAlbert S Berahas, Majid Jahani, Peter Richt\u00e1rik, and Martin Tak\u00e1\u010d. 2022. Quasi-Newton methods for machine learning:\nforget the past, just sample. Optimization Methods and Software 37, 5 (2022), 1668\u20131704.\nAlbert S Berahas, Jorge Nocedal, and Martin Tak\u00e1c. 2016. A Multi-Batch L-BFGS Method for Machine Learning. In NeurIPS.\nRaghu Bollapragada, Richard Byrd, and Jorge Nocedal. 2018a. Adaptive Sampling Strategies for Stochastic Optimization.\nSIAM Journal on Optimization 28, 4 (2018), 3312\u20133343.\nRaghu Bollapragada, Jorge Nocedal, Dheevatsa Mudigere, Hao-Jun Shi, and Ping Tak Peter Tang. 2018b. A Progressive\nBatching L-BFGS Method for Machine Learning. In ICML.\n28\nShi et al.\nAntoine Bordes, L\u00e9on Bottou, and Patrick Gallinari. 2009. SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent.\nJournal of Machine Learning Research 10 (2009), 1737\u20131754.\nL\u00e9on Bottou. 2010. Large-Scale Machine Learning with Stochastic Gradient Descent. In COMPSTAT.\nL\u00e9on Bottou et al. 1991. Stochastic Gradient Learning in Neural Networks. Proceedings of Neuro-N\u00eemes 91, 8 (1991), 12.\nL\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. 2018. Optimization Methods for Large-Scale Machine Learning. SIAM Rev.\n60, 2 (2018), 223\u2013311.\nStephen Boyd, Lin Xiao, and Almir Mutapcic. 2003. Subgradient Methods. Lecture notes of EE392o, Stanford University,\nAutumn (2003).\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula,\nAdam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of\nPython+NumPy programs. http://github.com/google/jax\nNicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. 2001. On the Generalization Ability of On-Line Learning Algorithms.\nNeurIPS.\nWenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. 2015. Compressing Neural Networks with\nthe Hashing Trick. In ICML.\nAaron Defazio. 2020. Momentum via Primal Averaging: Theoretical Insights and Learning Rate Schedules for Non-Convex\nOptimization. arXiv:2010.00406 (2020).\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR.\nAditya Devarakonda, Maxim Naumov, and Michael Garland. 2017. AdaBatch: Adaptive Batch Sizes for Training Deep\nNeural Networks. arXiv:1712.02029 (2017).\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa\nDehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth\n16x16 Words: Transformers for Image Recognition at Scale. In ICLR.\nTimothy Dozat. 2016. Incorporating Nesterov Momentum into Adam. (2016).\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic\nOptimization. Journal of Machine Learning Research 12, 7 (2011).\nMassimiliano Fasi, Nicholas J Higham, and Xiaobo Liu. 2023. Computing the Square Root of a Low-Rank Perturbation of\nthe Scaled Identity Matrix. SIAM J. Matrix Anal. Appl. 44, 1 (2023), 156\u2013174.\nThomas George, C\u00e9sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. 2018. Fast Approximate Natural\nGradient Descent in a Kronecker Factored Eigenbasis. In NeurIPS.\nDonald Goldfarb, Yi Ren, and Achraf Bahamou. 2020. Practical Quasi-Newton Methods for Training Deep Neural Networks.\nNeurIPS (2020).\nGene H Golub and Charles F Van Loan. 2013. Matrix Computations. JHU press.\nRoger Grosse and James Martens. 2016. A Kronecker-factored approximate Fisher matrix for convolution layers. In ICML.\nMaya R Gupta, Samy Bengio, and Jason Weston. 2014. Training Highly Multiclass Classifiers. Journal of Machine Learning\nResearch 15, 1 (2014), 1461\u20131492.\nVineet Gupta, Tomer Koren, and Yoram Singer. 2018. Shampoo: Preconditioned Stochastic Tensor Optimization. In ICML.\nStephen J. Hanson and Lorien Y. Pratt. 1988. Comparing Biases for Minimal Network Construction with Back-Propagation.\nIn NeurIPS.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR.\nTong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2019. Bag of Tricks for Image Classification\nwith Convolutional Neural Networks. In CVPR.\nNicholas J Higham. 2008. Functions of Matrices: Theory and Computation. SIAM.\nDmytro Ivchenko, Dennis Van Der Staay, Colin Taylor, Xing Liu, Will Feng, Rahul Kindi, Anirudh Sudarshan, and Shahin\nSefati. 2022. TorchRec: a PyTorch Domain Library for Recommendation Systems. In RecSys.\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. Averaging Weights\nLeads to Wider Optima and Better Generalization. UAI.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.\nFrederik Kunstner, Philipp Hennig, and Lukas Balles. 2019. Limitations of the empirical Fisher approximation for natural\ngradient descent. NeurIPS (2019).\nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document\nrecognition. Proc. IEEE 86, 11 (1998), 2278\u20132324.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In ICLR.\nJames Martens. 2020. New Insights and Perspectives on the Natural Gradient Method. Journal of Machine Learning Research\n21, 1 (2020), 5776\u20135851.\nJames Martens et al. 2010. Deep learning via Hessian-free optimization. In ICML.\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n29\nJames Martens, Jimmy Ba, and Matt Johnson. 2018. Kronecker-factored Curvature Approximations for Recurrent Neural\nNetworks. In ICLR.\nJames Martens and Roger Grosse. 2015. Optimizing Neural Networks with Kronecker-factored Approximate Curvature. In\nICML.\nJames Martens and Ilya Sutskever. 2011. Learning Recurrent Neural Networks with Hessian-Free Optimization. In ICML.\nJames Martens and Ilya Sutskever. 2012. Training Deep and Recurrent Networks with Hessian-Free Optimization. Neural\nNetworks: Tricks of the Trade: Second Edition (2012), 479\u2013535.\nDheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa\nOzdal, Jade Nie, Jongsoo Park, et al. 2022. Software-hardware co-design for fast and scalable training of deep learning\nrecommendation models. In ISCA.\nMaxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xi-\naodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. 2019. Deep Learning Recommendation Model for\nPersonalization and Recommendation Systems. arXiv:1906.00091 (2019).\nNVIDIA. 2019. Apex. https://github.com/NVIDIA/apex.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga, et al. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library.\nNeurIPS (2019).\nMert Pilanci and Martin J Wainwright. 2017. Newton Sketch: A Near Linear-Time Optimization Algorithm with Linear-\nQuadratic Convergence. SIAM Journal on Optimization 27, 1 (2017), 205\u2013245.\nBoris T Polyak and Anatoli B Juditsky. 1992. cceleration of Stochastic Approximation by Averaging. SIAM Journal on\nControl and Optimization 30, 4 (1992), 838\u2013855.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: memory optimizations toward training\ntrillion parameter models. In SC.\nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. ZeRO-infinity: breaking the GPU\nmemory wall for extreme scale deep learning. In SC.\nSashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the Convergence of Adam and Beyond. In ICLR.\nYi Ren and Donald Goldfarb. 2021. Tensor Normal Training for Deep Learning Models. NeurIPS.\nHerbert Robbins and Sutton Monro. 1951. A Stochastic Approximation Method. Annals of Mathematical Statistics (1951),\n400\u2013407.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by back-propagating errors.\nNature 323, 6088 (1986), 533\u2013536.\nNicol N Schraudolph, Jin Yu, and Simon G\u00fcnter. 2007. A Stochastic Quasi-Newton Method for Online Convex Optimization.\nIn AISTATS.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In ICML.\nHao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020. Compositional Embeddings Using\nComplementary Partitions for Memory-Efficient Recommendation Systems. In SIGKDD.\nShany Shumeli, Petros Drineas, and Haim Avron. 2022. Low-Rank Updates of Matrix Square Roots. Numerical Linear\nAlgebra with Applications (2022).\nYue Song, Nicu Sebe, and Wei Wang. 2022. Fast Differentiable Matrix Square Root. ICLR.\nWei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. 2018. Primal Averaging: A New Gradient Evaluation Step to Attain the\nOptimal Individual Convergence. IEEE Transactions on Cybernetics 50, 2 (2018), 835\u2013845.\nPeng Xu, Fred Roosta, and Michael W Mahoney. 2020a. Newton-type methods for non-convex optimization under inexact\nHessian information. Mathematical Programming 184, 1-2 (2020), 35\u201370.\nPeng Xu, Fred Roosta, and Michael W Mahoney. 2020b. Second-order Optimization for Non-convex Machine Learning: an\nEmpirical Study. In SDM. 199\u2013207.\nPeng Xu, Jiyan Yang, Fred Roosta, Christopher R\u00e9, and Michael W Mahoney. 2016. Sub-sampled Newton Methods with\nNon-uniform Sampling. NeurIPS.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt\nKeutzer, and Cho-Jui Hsieh. 2020. Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. In ICLR.\nYang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. 2018. Imagenet training in minutes. In ICPP.\nGuodong Zhang, James Martens, and Roger B Grosse. 2019. Fast Convergence of Natural Gradient Descent for Over-\nParameterized Neural Networks. NeurIPS.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. 2022. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068 (2022).\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott,\nSam Shleifer, et al. 2023. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. arXiv:2304.11277 (2023).\n30\nShi et al.\nZhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, and Francesco Orabona. 2022. Understanding AdamW through Proximal\nMethods and Scale-Freeness. arXiv:2202.00089 (2022).\nA\nMOTIVATION FOR KRONECKER PRODUCT APPROXIMATIONS\nHere, we provide a complete description of the motivation for using a Kronecker product approxi-\nmation for a matrix parameter arising from a fully-connected layer when training a multi-layer\nperceptron. Recall that the problem of neural network training is posed as\nmin\n\ud835\udc64\u2208R\ud835\udc51\n\b\n\ud835\udc53 (\ud835\udc64) = E(\ud835\udc65,\ud835\udc66)\u223cD [\u2113(\ud835\udc5a(\ud835\udc64;\ud835\udc65);\ud835\udc66)]\n\t\n(43)\nwhere the multi-layer perceptron model is defined as\n\ud835\udc5a(\ud835\udc64;\ud835\udc65) = \ud835\udc4a (\ud835\udc5b)\ud835\udf0e(\ud835\udc4a (\ud835\udc5b\u22121)\ud835\udf0e(...\ud835\udf0e(\ud835\udc4a (1)\ud835\udc65)...)),\n(44)\nwith \ud835\udc64 = (vec(\ud835\udc4a (1))\ud835\udc47, ..., vec(\ud835\udc4a (\ud835\udc5b))\ud835\udc47 )\ud835\udc47 . In order to examine its structure, we would like to derive\nfull-matrix AdaGrad for a single parameter \ud835\udc4a (\ud835\udc56).\nFor a single datapoint (\ud835\udc65,\ud835\udc66) \u223c D, we can isolate the problem for parameter\ud835\udc4a (\ud835\udc56) by defining the\nfunction\n\ud835\udc53 (\ud835\udc56) (\ud835\udc4a ) = \ud835\udf19 (\ud835\udc56) (\ud835\udc4a\ud835\udc4e(\ud835\udc56\u22121)).\n(45)\nHere, the activation \ud835\udc4e(\ud835\udc56\u22121) before layer \ud835\udc56 and the function \ud835\udf19 (\ud835\udc56) : R\ud835\udc51\ud835\udc56 \u2192 R are defined as:\n\ud835\udc4e(\ud835\udc56\u22121) = \ud835\udf0e(\ud835\udc4a (\ud835\udc56\u22121)...\ud835\udf0e(\ud835\udc4a (2)\ud835\udf0e(\ud835\udc4a (1)\ud835\udc65))...)\n(46)\n\ud835\udf19 (\ud835\udc56) (\ud835\udc67) = \u2113(\ud835\udc4a (\ud835\udc5b)\ud835\udf0e(\ud835\udc4a (\ud835\udc5b\u22121)\ud835\udf0e(...\ud835\udf0e(\ud835\udc67)...)),\ud835\udc66).\n(47)\nNote that \ud835\udc4e(\ud835\udc56\u22121) has an implicit dependence on \ud835\udc65 and \ud835\udf19 (\ud835\udc56) has an implicit dependence on \ud835\udc66. This\nstructure also holds for simpler machine learning models, such as multi-class logistic regression.\nThe gradient in both matrix and vector form for a single sample may therefore be derived as:\n\u2207\ud835\udc53 (\ud835\udc56) (\ud835\udc4a (\ud835\udc56)) = \u2207\ud835\udf19(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\ud835\udc4e(\ud835\udc56\u22121) (\ud835\udc4e(\ud835\udc56\u22121))\ud835\udc47\n(48)\nvec(\u2207\ud835\udc53 (\ud835\udc56) (\ud835\udc4a (\ud835\udc56))) = \u2207\ud835\udf19(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\ud835\udc4e(\ud835\udc56\u22121) \u2297 \ud835\udc4e(\ud835\udc56\u22121).\n(49)\nLet the subscript \ud835\udc60 denote the gradient, function, or activation at iteration \ud835\udc60. We can therefore\nexpand the definition of full-matrix AdaGrad as\n\ud835\udc34(\ud835\udc56)\n\ud835\udc61\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\nvec(\u2207\ud835\udc53 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n)) vec(\u2207\ud835\udc53 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n))\ud835\udc47\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n\u2297 \ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n\u2297 \ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)\ud835\udc47\n=\n\ud835\udc61\u2211\ufe01\n\ud835\udc60=0\n(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60 (\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n(\u2207\ud835\udf19 (\ud835\udc56)\n\ud835\udc60\n(\ud835\udc67)|\ud835\udc67=\ud835\udc4a (\ud835\udc56)\n\ud835\udc60\n\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)\ud835\udc47 ) \u2297 (\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n(\ud835\udc4e(\ud835\udc56\u22121)\n\ud835\udc60\n)\ud835\udc47 ).\nwhere (\ud835\udc65\ud835\udc60,\ud835\udc66\ud835\udc60) \u223c D is sampled at iteration \ud835\udc60. So \ud835\udc34\ud835\udc61 is in fact a sum of Kronecker products.\nB\nPER-PARAMETER RELATIONSHIP WITH ROW-WISE ADAGRAD AND ADAFACTOR\nRow-wise AdaGrad and AdaFactor are two optimizers with sublinear memory cost designed for\noptimizing embedding tables and large language models, respectively [Gupta et al. 2014; Mudigere\net al. 2022; Shazeer and Stern 2018]. We will show that two separate versions of diagonal Shampoo\nare, in fact, equivalent to both AdaFactor and row-wise AdaGrad when applied to a single matrix\nparameter \ud835\udc4a \u2208 R\ud835\udc5a\u00d7\ud835\udc5b. (These equivalences will not hold, however, for the general multi-parameter\ncase.)\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n31\nWe will rely on the following generalization of the Shampoo update equation (17)\n\ud835\udc4a\ud835\udc61+1 = \ud835\udc4a\ud835\udc61 \u2212 \u00af\ud835\udefc\ud835\udc61\ud835\udc3f\u22121/2\ud835\udc5d\n\ud835\udc61\n\ud835\udc3a\ud835\udc61\ud835\udc45\u22121/2\ud835\udc5e\n\ud835\udc61\n(50)\nfor 1\n\ud835\udc5d + 1\n\ud835\udc5e = 1 with \ud835\udc5d,\ud835\udc5e > 0; see [Anil et al. 2020].\nB.1\nEquivalence to Row-Wise AdaGrad\nRow-wise AdaGrad is an approximation to diagonal AdaGrad designed specifically for training\nlarge-scale embedding tables that arise in ranking and recommendation models [Gupta et al. 2014;\nMudigere et al. 2022]. Categorical features are often encoded using a one-hot or multi-hot encoding\n\ud835\udc65 \u2208 R\ud835\udc5a where each component of the encoding takes values in {0, 1}:\n\ud835\udc65\ud835\udc56 =\n(\n1\nif category \ud835\udc56 is active,\n0\notherwise.\n(51)\nNote that \ud835\udc5a corresponds to the total number of categories. In the case of ranking and recommen-\ndation models, the categories are normally hashed via a modulo function, and therefore \ud835\udc5a will\ncorrespond to the number of hash buckets [Chen et al. 2015; Naumov et al. 2019; Shi et al. 2020].\nEmbedding tables map individual or small subsets of categories to dense embedding vector\nrepresentations in some lower-dimensional space. In the simplest case, an embedding table \ud835\udc38 \u2208\nR\ud835\udc5a\u00d7\ud835\udc5b maps a categorical feature (represented by a one-hot or multi-hot encoding) \ud835\udc65 \u2208 R\ud835\udc5a to an\nembedding vector \ud835\udc52 \u2208 R\ud835\udc5b of dimension \ud835\udc5b by computing\n\ud835\udc52 = \ud835\udc38\ud835\udc47\ud835\udc65.\n(52)\nIn the case of one-hot encodings, this corresponds to a single-row lookup. In the multi-hot encoding\nsetting, this corresponds to performing sum pooling over all active rows. Our corresponding\noptimization problem is therefore:\nmin\n\ud835\udc38\u2208R\ud835\udc5a\u00d7\ud835\udc5b E\ud835\udc65\u223cX[\ud835\udc53 (\ud835\udc38;\ud835\udc65)] = E\ud835\udc65\u223cX[\ud835\udf19(\ud835\udc38\ud835\udc47\ud835\udc65)]\n(53)\nwhere \ud835\udf19 : R\ud835\udc5b \u2192 R. Note that the gradient for a single sample can be derived as:\n\u2207\ud835\udc53 (\ud835\udc38;\ud835\udc65) = \ud835\udc65(\u2207\ud835\udf19(\ud835\udc66)|\ud835\udc66=\ud835\udc38\ud835\udc47 \ud835\udc65)\ud835\udc47 .\n(54)\nRow-wise AdaGrad approximates diagonal AdaGrad by only storing a single optimizer state for\neach row by averaging over the squared gradients of each row of the embedding table. In particular,\nlet the row-wise AdaGrad optimizer state be denoted by \ud835\udc63 \u2208 R\ud835\udc5a with \ud835\udc63\u22121 = \ud835\udf161\ud835\udc5a for \ud835\udf16 > 0. Then\nrow-wise AdaGrad performs the update:\n\ud835\udc63\ud835\udc61 = \ud835\udc63\ud835\udc61\u22121 +\n\u00cd\ud835\udc5b\n\ud835\udc57=1[\ud835\udc3a \u22992\n\ud835\udc61 ]:,\ud835\udc57\n\ud835\udc5b\n(55)\n\ud835\udc38\ud835\udc61+1 = \ud835\udc38\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc3a\ud835\udc61/\n\u221a\ufe03\n\ud835\udc63\ud835\udc611\ud835\udc47\ud835\udc5a.\n(56)\nNote that this can be re-written as:\n\ud835\udc49\ud835\udc61 = \ud835\udc49\ud835\udc61\u22121 + matdiag(\ud835\udc3a\ud835\udc61\ud835\udc3a\ud835\udc47\n\ud835\udc61 )\n\ud835\udc5b\n(57)\n\ud835\udc38\ud835\udc61+1 = \ud835\udc38\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc49 \u22121/2\n\ud835\udc61\n\ud835\udc3a\ud835\udc61\n(58)\nwith \ud835\udc49\u22121 = \ud835\udf16\ud835\udc3c\ud835\udc5a. With some slight re-writing, we obtain:\n\u02dc\ud835\udc3f\ud835\udc61 = \u02dc\ud835\udc3f\ud835\udc61\u22121 + matdiag(\ud835\udc3a\ud835\udc61\ud835\udc3a\ud835\udc47\n\ud835\udc61 )\n(59)\n\ud835\udc38\ud835\udc61+1 = \ud835\udc38\ud835\udc61 \u2212 \u00af\ud835\udefc\ud835\udc61 \u02dc\ud835\udc3f\u22121/2\n\ud835\udc61\n\ud835\udc3a\ud835\udc61,\n(60)\n32\nShi et al.\nwhere \ud835\udc3f\u22121 = \u00af\ud835\udf16\ud835\udc3c\ud835\udc5a with \u00af\ud835\udf16 = \ud835\udf16/\ud835\udc5b and \u00af\ud835\udefc\ud835\udc61 = \ud835\udefc\ud835\udc61/\ud835\udc5b. Note that this is precisely diagonal Shampoo (with a\nmodified learning rate and epsilon) with update formula (50) with \ud835\udc5d = 1 and \ud835\udc5e = \u221e!\nB.2\nRelationship with AdaFactor\nAdaFactor is a sublinear memory adaptive gradient method designed for training large language\nmodels [Shazeer and Stern 2018]. It approximates diagonal AdaGrad for matrices by using a rank-\none approximation. By observing that all entries of AdaGrad are non-negative, one can minimize\nthe I-divergence between diagonal AdaGrad and its rank-one approximation and obtain a closed\nform solution expressed in terms of row sums and column sums, which are linear functions of the\nsecond moment.\nIn particular, suppose we are interested in minimizing the matrix function\nmin\n\ud835\udc4a \u2208R\ud835\udc5a\u00d7\ud835\udc5b E\ud835\udc65\u223cX[\ud835\udc53 (\ud835\udc4a ;\ud835\udc65)] = E\ud835\udc65\u223cX[\ud835\udf19(\ud835\udc4a\ud835\udc65)],\nwhere \ud835\udc4a \u2208 R\ud835\udc5a\u00d7\ud835\udc5b, \ud835\udc65 \u2208 R\ud835\udc5b is sampled from some underlying probability distribution X, and\n\ud835\udf19 : R\ud835\udc5a \u2192 R. Then AdaFactor (ignoring bias correction) will store two vectors \ud835\udc5f \u2208 R\ud835\udc5a and \ud835\udc50 \u2208 R\ud835\udc5b\nfor the row and column sums, respectively, and update the parameters as:\n\ud835\udc5f\ud835\udc61 = \ud835\udefd2\ud835\udc5f\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2)[\ud835\udc3a \u22992\n\ud835\udc61 ]1\ud835\udc5b\n(61)\n\ud835\udc50\ud835\udc61 = \ud835\udefd2\ud835\udc50\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2)[\ud835\udc3a \u22992\n\ud835\udc61 ]\ud835\udc47 1\ud835\udc5a\n(62)\n\u02c6\ud835\udc34\ud835\udc61 = \ud835\udc5f\ud835\udc61\ud835\udc50\ud835\udc47\n\ud835\udc61 /1\ud835\udc47\n\ud835\udc5a\ud835\udc5f\ud835\udc61\n(63)\n\ud835\udc4a\ud835\udc61+1 = \ud835\udc4a\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc3a\ud835\udc61/(\n\u221a\ufe03\n\u02c6\ud835\udc34\ud835\udc61 + \ud835\udf161\ud835\udc5a1\ud835\udc47\n\ud835\udc5b).\n(64)\nNote that this can be re-written as\n\u02dc\ud835\udc3f\ud835\udc61 = \ud835\udefd2 \u02dc\ud835\udc3f\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2) matdiag(\ud835\udc3a\ud835\udc61\ud835\udc3a\ud835\udc47\n\ud835\udc61 )\n(65)\n\u02dc\ud835\udc45\ud835\udc61 = \ud835\udefd2 \u02dc\ud835\udc45\ud835\udc61\u22121 + (1 \u2212 \ud835\udefd2) matdiag(\ud835\udc3a\ud835\udc47\n\ud835\udc61 \ud835\udc3a\ud835\udc61)\n(66)\n\ud835\udc4a\ud835\udc61+1 = \ud835\udc4a\ud835\udc61 \u2212 \u00af\ud835\udefc\ud835\udc61 \u02dc\ud835\udc3f\u22121/2\n\ud835\udc61\n\ud835\udc3a\ud835\udc61 \u02dc\ud835\udc45\u22121/2\n\ud835\udc61\n(67)\nwith \u00af\ud835\udefc\ud835\udc61 =\n\u221a\ufe01\n1\ud835\udc47\ud835\udc5a\ud835\udc3f\ud835\udc611\ud835\udc5a\ud835\udefc\ud835\udc61. This shares the same functional form as diagonal Shampoo (with exponential\nmoving averaging of the factor matrices), except for a key difference with the exponent, where\nAdaFactor uses \u22121/2 as opposed to \u22121/4 with Shampoo.\nC\nPROOFS\nFor completeness, we present the algorithms and theorems with proof below. We will focus on the\niterate-dependent case for generality. Recall that the momentum method (31)-(32) can be vectorized\nand written as the iteration:\n\ud835\udc5a\ud835\udc61 = \ud835\udf07\ud835\udc61\ud835\udc5a\ud835\udc61\u22121 + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)\n(68)\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc5a\ud835\udc61\n(69)\nfor momentum hyperparameter \ud835\udf07\ud835\udc61 > 0 and learning rate \ud835\udefc\ud835\udc61 > 0. Similarly, Nesterov acceleration\ncan be written as\n\ud835\udc5a\ud835\udc61 = \ud835\udf07\ud835\udc61\u22121\ud835\udc5a\ud835\udc61\u22121 + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)\n(70)\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udf07\ud835\udc61\ud835\udc5a\ud835\udc61 + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)).\n(71)\nThis generalizes the equivalence for SGD with momentum observed in Defazio [2020].\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n33\nTheorem C.1. The momentum method defined in (68)-(69) is equivalent to the heavy ball iteration\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udeff\ud835\udc61 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\n(72)\nand the stochastic primal iterate averaging iteration\n\ud835\udc67\ud835\udc61+1 = \ud835\udc67\ud835\udc61 \u2212 \ud835\udf02\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)\n(73)\n\ud835\udc64\ud835\udc61+1 = (1 \u2212 \ud835\udc50\ud835\udc61)\ud835\udc64\ud835\udc61 + \ud835\udc50\ud835\udc61\ud835\udc67\ud835\udc61+1\n(74)\nwith \ud835\udc50\ud835\udc61 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 \u2208 [0, 1) for some \ud835\udefe\ud835\udc61 > 0, \ud835\udeff\ud835\udc61 = \ud835\udefc\ud835\udc61\u22121\n\ud835\udefc\ud835\udc61 \ud835\udf07\ud835\udc61 = \ud835\udefe\ud835\udc61\u22121\n\ud835\udefe\ud835\udc61 +1, and \ud835\udefc\ud835\udc61 =\n\ud835\udf02\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 = (1 \u2212 \ud835\udc50\ud835\udc61)\ud835\udf02\ud835\udc61. In particular,\nif the hyperparameters are fixed, i.e., \ud835\udefc\ud835\udc61 \u2261 \ud835\udefc, \ud835\udf07\ud835\udc61 \u2261 \ud835\udf07, \ud835\udc50\ud835\udc61 \u2261 \ud835\udc50, \ud835\udf02\ud835\udc61 \u2261 \ud835\udf02, and \ud835\udeff\ud835\udc61 \u2261 \ud835\udeff, then \ud835\udefc = (1 \u2212\ud835\udc50)\ud835\udf02 and\n\ud835\udeff = \ud835\udf07 = \ud835\udc50.\nProof. We will show that both methods may be written in the form of (72). For the momentum\nequations (68)-(69), note that if we plug in (68) into (69), we obtain:\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udf07\ud835\udc61\ud835\udefc\ud835\udc61\ud835\udc5a\ud835\udc61\u22121\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udeff\ud835\udc61 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121).\nwhere the last line follows since \ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121 = \u2212\ud835\udefc\ud835\udc61\u22121\ud835\udc5a\ud835\udc61\u22121 and if we choose \ud835\udeff\ud835\udc61 = \ud835\udefc\ud835\udc61\u22121\n\ud835\udefc\ud835\udc61 \ud835\udf07\ud835\udc61.\nNow, if we plug in \ud835\udc50\ud835\udc61 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 into (73)-(74), we obtain\n\ud835\udc67\ud835\udc61+1 = \ud835\udc67\ud835\udc61 \u2212 \ud835\udf02\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)\n\ud835\udc64\ud835\udc61+1 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc64\ud835\udc61 +\n1\n\ud835\udefe\ud835\udc61 + 1\ud835\udc67\ud835\udc61+1.\nEquivalently,\n\ud835\udc67\ud835\udc61+1 = (\ud835\udefe\ud835\udc61 + 1)\ud835\udc64\ud835\udc61+1 \u2212 \ud835\udefe\ud835\udc61\ud835\udc64\ud835\udc61.\nTherefore,\n\ud835\udc64\ud835\udc61+1 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc64\ud835\udc61 +\n1\n\ud835\udefe\ud835\udc61 + 1 (\ud835\udc67\ud835\udc61 \u2212 \ud835\udf02\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61))\n=\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc64\ud835\udc61 +\n1\n\ud835\udefe\ud835\udc61 + 1 ((\ud835\udefe\ud835\udc61\u22121 + 1)\ud835\udc64\ud835\udc61 \u2212 \ud835\udefe\ud835\udc61\u22121\ud835\udc64\ud835\udc61\u22121 \u2212 \ud835\udf02\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61))\n= \ud835\udc64\ud835\udc61 \u2212\n\ud835\udf02\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udefe\ud835\udc61\u22121\n\ud835\udefe\ud835\udc61 + 1 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udeff\ud835\udc61 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\nsince \ud835\udefc\ud835\udc61 =\n\ud835\udf02\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 and \ud835\udeff\ud835\udc61 = \ud835\udefe\ud835\udc61\u22121\n\ud835\udefe\ud835\udc61 +1.\n\u25a1\nObserve that this equivalence provides some guidelines for choosing the hyperparameters for\nmomentum methods. In particular, if the momentum hyperparameter is fixed to \ud835\udf07\ud835\udc61 \u2261 \ud835\udf07 = 0.9 (as is\ntypically set in practice), then we should multiply the maximum initial learning rate \ud835\udf020 \u2264 \u00af\ud835\udf020 by 0.1\nwhen using the momentum method to prevent divergence, i.e., \ud835\udefc0 \u2264 \u00af\ud835\udefc0 = 0.1\u00af\ud835\udf020.\nTypically, we apply learning rate warmup and/or different learning rate annealing techniques,\nfor example by cutting the learning rate periodically or applying cosine annealing. In order to\nensure that we are applying a consistent averaging hyperparameter \ud835\udc50\ud835\udc61 \u2261 \ud835\udc50, we need to adjust the\nmomentum hyperparameter when the learning rate changes. For example, if \ud835\udefc\ud835\udc61 = \ud835\udf01\ud835\udefc\ud835\udc61\u22121, then\nwe should set \ud835\udf07\ud835\udc61 = \ud835\udf01 \ud835\udf07\ud835\udc61\u22121. Otherwise, fixing the momentum hyperparameter while decaying the\nlearning rate corresponds to a larger averaging hyperparameter \ud835\udc50\ud835\udc61.\nWe can show a similar theorem for the Nesterov acceleration case.\n34\nShi et al.\nTheorem C.2. The Nesterov accelerated method defined in (70)-(71) is equivalent to the modified\nheavy ball iteration\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))) + \ud835\udeff\ud835\udc61 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\n(75)\nand the modified stochastic primal iterate averaging iteration\n\ud835\udc67\ud835\udc61+1 = \ud835\udc67\ud835\udc61 \u2212 \ud835\udf02\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))\n(76)\n\ud835\udc64\ud835\udc61+1 = (1 \u2212 \ud835\udc50\ud835\udc61)\ud835\udc64\ud835\udc61 + \ud835\udc50\ud835\udc61\ud835\udc67\ud835\udc61+1\n(77)\nwith \ud835\udc50\ud835\udc61 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 \u2208 [0, 1) for some \ud835\udefe\ud835\udc61 > 0, \ud835\udeff\ud835\udc61 = \ud835\udefc\ud835\udc61\u22121\n\ud835\udefc\ud835\udc61 \ud835\udf07\ud835\udc61 = \ud835\udefe\ud835\udc61\u22121\n\ud835\udefe\ud835\udc61 +1, and \ud835\udefc\ud835\udc61 =\n\ud835\udf02\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 = (1 \u2212 \ud835\udc50\ud835\udc61)\ud835\udf02\ud835\udc61. In particular,\nif the hyperparameters are fixed, i.e., \ud835\udefc\ud835\udc61 \u2261 \ud835\udefc, \ud835\udf07\ud835\udc61 \u2261 \ud835\udf07, \ud835\udc50\ud835\udc61 \u2261 \ud835\udc50, \ud835\udf02\ud835\udc61 \u2261 \ud835\udf02, and \ud835\udeff\ud835\udc61 \u2261 \ud835\udeff, then \ud835\udefc = (1 \u2212\ud835\udc50)\ud835\udf02 and\n\ud835\udeff = \ud835\udf07 = \ud835\udc50.\nProof. We will show that both methods may be written in the form (75). For the Nesterov\naccelerated equations (70)-(71), note that if we plug in (70) into (71), we obtain:\n\ud835\udc64\ud835\udc61+1 = \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udf07\ud835\udc61\ud835\udc5a\ud835\udc61 + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61))\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udf07\ud835\udc61 (\ud835\udf07\ud835\udc61\u22121\ud835\udc5a\ud835\udc61\u22121 + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)) + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61))\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udf07\ud835\udc61\ud835\udf07\ud835\udc61\u22121\ud835\udc5a\ud835\udc61\u22121 \u2212 \ud835\udefc\ud835\udc61 (1 + \ud835\udf07\ud835\udc61)\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61)\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udf07\ud835\udc61\ud835\udefc\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121)) \u2212 \ud835\udf07\ud835\udc61\ud835\udefc\ud835\udc61 (\ud835\udf07\ud835\udc61\u22121\ud835\udc5a\ud835\udc61\u22121 + \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))) + \ud835\udeff\ud835\udc61 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\nwhere the last line follows since \ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121 = \u2212\ud835\udefc\ud835\udc61\u22121(\ud835\udf07\ud835\udc61\u22121\ud835\udc5a\ud835\udc61\u22121 + \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121)) and if we choose\n\ud835\udeff\ud835\udc61 = \ud835\udefc\ud835\udc61\u22121\n\ud835\udefc\ud835\udc61 \ud835\udf07\ud835\udc61.\nNow, if we plug in \ud835\udc50\ud835\udc61 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 into (76)-(77), we obtain\n\ud835\udc67\ud835\udc61+1 = \ud835\udc67\ud835\udc61 \u2212 \ud835\udf02\ud835\udc61 (\ud835\udf07\ud835\udc61\ud835\udc5a\ud835\udc61 + \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61))\n\ud835\udc64\ud835\udc61+1 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc64\ud835\udc61 +\n1\n\ud835\udefe\ud835\udc61 + 1\ud835\udc67\ud835\udc61+1.\nEquivalently,\n\ud835\udc67\ud835\udc61+1 = (\ud835\udefe\ud835\udc61 + 1)\ud835\udc64\ud835\udc61+1 \u2212 \ud835\udefe\ud835\udc61\ud835\udc64\ud835\udc61.\nTherefore,\n\ud835\udc64\ud835\udc61+1 =\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc64\ud835\udc61 +\n1\n\ud835\udefe\ud835\udc61 + 1 (\ud835\udc67\ud835\udc61 \u2212 \ud835\udf02\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))))\n=\n\ud835\udefe\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1\ud835\udc64\ud835\udc61 +\n1\n\ud835\udefe\ud835\udc61 + 1 ((\ud835\udefe\ud835\udc61\u22121 + 1)\ud835\udc64\ud835\udc61 \u2212 \ud835\udefe\ud835\udc61\u22121\ud835\udc64\ud835\udc61\u22121 \u2212 \ud835\udf02\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))))\n= \ud835\udc64\ud835\udc61 \u2212\n\ud835\udf02\ud835\udc61\n\ud835\udefe\ud835\udc61 + 1 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121))) + \ud835\udefe\ud835\udc61\u22121\n\ud835\udefe\ud835\udc61 + 1 (\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\n= \ud835\udc64\ud835\udc61 \u2212 \ud835\udefc\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) + \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121)) + \ud835\udeff(\ud835\udc64\ud835\udc61 \u2212 \ud835\udc64\ud835\udc61\u22121)\nsince \ud835\udefc\ud835\udc61 =\n\ud835\udf02\ud835\udc61\n\ud835\udefe\ud835\udc61 +1 and \ud835\udeff\ud835\udc61 = \ud835\udefe\ud835\udc61\u22121\n\ud835\udefe\ud835\udc61 +1.\n\u25a1\nObserve that the primary difference between using momentum and Nesterov acceleration has\nto do with the inclusion of an additional correction term \ud835\udf07\ud835\udc61 (\ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2212 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121)) to the search\ndirection based on the previous search direction in the iterative averaging scheme. Notice that\nif \ud835\udc5d\ud835\udc61 (\ud835\udc64\ud835\udc61) \u2248 \ud835\udc5d\ud835\udc61\u22121(\ud835\udc64\ud835\udc61\u22121), then no correction is performed. The practical effectiveness of such a\ncorrection term is still not well understood and left for future research.\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n35\nD\nEXPERIMENTAL DETAILS\nD.1\nMethod Hyperparameters\nTables 2 and 3 contain the common hyperparameters used for our experiments with Shampoo\nand SGD with Nesterov, respectively. These hyperparameters are used in all experiments, unless\nexplicitly mentioned otherwise.\nParameter\nValue\nBasic Shampoo parameters\nlr\nSee LR scheduler section\nbetas\n[0., 0.999]\nmomentum\n0.9\nuse_nesterov\nTrue\nuse_bias_correction\nTrue\nweight_decay\n1e-4\nuse_decoupled_weight_decay\nTrue\nuse_weight_decay_on_bias_and_norm_params\nTrue\nAccelerators\nNumber of GPUs\n8 Tesla V100\nbatch_size_per_gpu\n128\nnum_trainers_per_group\n-1\nPreconditioning\nmax_preconditioner_dim\n2048\nprecondition_frequency\n50\nstart_preconditioning_step\n0\npreconditioner_dtype\ntorch.float\nlarge_dim_method\nBlocking\nuse_merge_dims\nTrue\nPreconditioner exponent heuristics\nexponent_override\n2\ud835\udf14 (Default)\nexponent_multiplier\n1.0 (Default)\nGrafting\ngrafting_type\nSGD\ngrafting_epsilon\n1e-8\ngrafting_beta2\n0.999\nLR scheduler\nName\nCosine decay with warmup\ninitial_lr\n0.1\nwarmup_epochs\n5\nRoot inverse\nroot_inv_method\nEigendecomposition\nepsilon\n1e-12\nuse_protected_eigh\nTrue\nTable 2. Generic Shampoo settings for ImageNet task\nD.2\nPerformance-Related Ablations\nTwo of the key hyperparameters controlling the (computational) performance of Shampoo are:\n(i) the frequency with which the preconditioners are updated, and (ii) the maximum allowed\npreconditioner size. In this section, we perform ablations of these hyperparameters to demonstrate\nrobustness to their choices in terms of accuracy, allowing us to select settings for these two\nhyperparameters focused on reducing wall-clock time.\nThe tables presented below contain information about the accuracy of Shampoo using different\nhyperparameter configurations compared to Nesterov. We compare both under a fixed epoch budget\n36\nShi et al.\nParameter\nValue\nBasic Nesterov parameters\nlr\nSee LR scheduler section\nbetas\n[0., 0.999]\nmomentum\n0.9\nnesterov\nTrue\ndampening\n0\nuse_weight_decay_on_bias_and_norm_params\nTrue\nAccelerators\nNumber of GPUs\n8 Tesla V100\nbatch_size_per_gpu\n128\nLR scheduler\nName\nCosine decay with warmup\ninitial_lr\n0.1\nwarmup_epochs\n5\nTable 3. Generic Nesterov settings for ImageNet task\nof 90 epochs as well as a time budget based on the total wall-clock time required by Nesterov to\ncomplete 90 epochs.\nD.2.1\nPreconditioner (Update) Frequency. We begin by studying the effect of the frequency\nof preconditioner updates. Recall that all stochastic gradients are preconditioned. However\nsome of the used preconditioners may be stale. The degree of staleness is controlled by the\nprecondition_frequency hyperparameter. In this ablation, we fix all configurations of Shampoo\nas in Table 2, except for the max_preconditioner_dim which is set to 1024. Results are presented\nin Table 4.\nPreconditioner\nVal. accuracy\nVal. accuracy\nSteps\nTime overhead\nupdate frequency\nat 90 epochs\nat eq. time\nper second\nwrt Nesterov\nNesterov\n76.937\n76.937\n2.373\n\u2014\n1\n77.476\n62.031\n1.12\n53.1%\n20\n77.495\n75.444\n2.07\n12.80%\n50\n77.454\n77.135\n2.25\n5.15%\n100\n77.442\n77.473\n2.32\n1.98%\nTable 4. Effect of the preconditioner update frequency in Shampoo.\nShampoo exhibits marginal differences in validation accuracy after 90 epochs, with significant\ncomputational performance changes depending on the choice of update frequency. As expected,\ncarrying out very frequent preconditioner updates can induce a prohibitively large overhead in the\nexecution of Shampoo. Fortunately, results demonstrate that it is possible use stale preconditioners\nsuccessfully, corroborating [Anil et al. 2020]. It is possible to recover all of the accuracy gains\nobtained as when preconditioning at every step by using preconditioners updated every 50 or 100\nsteps, with only a 2-5% computational overhead.\nBased on these results, we decided to set the precondition_frequency hyperparameter to 50\nfor all other experiments.\nD.2.2\nMaximum Preconditioner Dimension. This section studies the effect of the maximum allowed\npreconditioner size. This hyperparameter trades off between a closer approximation to block\nfull-matrix Adagrad (which corresponds to a preconditioner containing all variables within each\nA Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks\nAt-Scale\n37\nparameter) and the cost of linear algebra operations such as the inverse root computation (which\nmay have a cubic dependency on the preconditioner size). While theoretically decreasing the\nblock size should yield a smaller FLOP count, this is not necessarily the case due to CPU-GPU\nlatency and additional kernel launches. As per the previous section, for this ablation we fix the\nprecondition_frequency to 50 and all other hyperparameters as in Table 2.\nTable 5 shows that the effect of the maximum preconditioner size is much less significant than\nthat of the preconditioner update frequency. As expected, there is a monotonic cost increase when\nallowing larger preconditioner sizes. However, the small increases in overhead ( 1% per factor of 2\nin preconditioner size) demonstrate that the preconditioner inversion is not the main bottleneck in\nthe execution of a weight update for Shampoo.\nWorking under the assumption that a closer approximation to the full-matrix preconditioner is\ndesirable, and given that the additional time overhead between values 2048 vs 1024 was about 1.5%,\nwe decided to set max_preconditioner_dim of 2048 for our experiments.\nMax preconditioner\nVal. accuracy\nVal. accuracy\nSteps\nTime overhead\ndimension\nat 90 epochs\nat eq. time\nper second\nwrt Nesterov\nNesterov\n76.937\n76.937\n2.373\n\u2014\n8192\n77.419\n76.547\n2.173\n8.40%\n4096\n77.266\n76.582\n2.193\n7.54%\n2048\n77.507\n76.817\n2.213\n6.70%\n1024\n77.454\n77.081\n2.250\n5.15%\nTable 5. Effect of the maximum preconditioner dimension in Shampoo.\nE\nIMPLEMENTATION STRUCTURE\nThe implementation currently has the following class structure:\n\u2022 DistributedShampoo(torch.optim.Optimizer)\n[distributed_shampoo.py]:\nMain\noptimizer class. Depending on the selected large-dimensional method, uses different\nPreconditioner classes, including:\n\u2013 BlockShampooPreconditioner(DistributedPreconditioner)\n[shampoo_utils.py]: Block Shampoo preconditioner class that blocks the pa-\nrameters and applies Shampoo to each block. Contains a ShampooPreconditioner\nobject for each layer\u2019s block.\n\u2013 ShampooPreconditioner(DistributedPreconditioner)\n[shampoo_utils.py]:\nShampoo preconditioner class that provides functionality for applying Shampoo to\na particular parameter. Constructs multiple Kronecker factors corresponding to the\nnumber of dimensions. Implemented using ShampooKroneckerFactor and Grafting\nclasses.\n\u2217 ShampooKroneckerFactor(OptimizerModule)\n[shampoo_utils.py]:\nData\nclass containing the factor matrix, the root inverse factor matrix, and additional\nmetadata for distributed computation.\n\u2217 SGDGrafting(Grafting) [shampoo_utils.py]: SGD grafting class.\n38\nShi et al.\n\u2217 AdaGradGrafting(Grafting) [shampoo_utils.py]: AdaGrad grafting class.\nContains AdaGradPreconditioner class.\n\u2217 RMSPropGrafting(AdaGradGrafting) [shampoo_utils.py]: RMSProp graft-\ning class built on AdaGradGrafting.\n\u2217 AdamGrafting(AdaGradGrafting) [shampoo_utils.py]: Adam grafting class\nbuilt on AdaGradGrafting.\n\u2217 AdaGradNormalizedGrafting(AdaGradGrafting)\n[shampoo_utils.py]:\nNormalized AdaGrad grafting class built on AdaGradGrafting. Normalizes the\ngradient before using it to update the AdaGrad preconditioner.\n\u2217 RMSPropNormalizedGrafting(AdaGradGrafting)\n[shampoo_utils.py]:\nNormalized RMSProp grafting class built on AdaGradGrafting. Normalizes the\ngradient before using it to update the RMSProp preconditioner.\n\u2217 AdamNormalizedGrafting(AdaGradGrafting) [shampoo_utils.py]: Normal-\nized Adam grafting class built on AdaGradGrafting. Normalizes the gradient\nbefore using it to update the Adam preconditioner.\n\u2013 AdaGradPreconditioner(DistributedPreconditioner)\n[shampoo_utils.py]:\nAdaGrad preconditioner class that implements AdaGrad and RMSProp. Uses standard\nsummation if beta2 = 1, otherwise uses exponential moving averaging.\nWe build on top of the following abstract data structures:\n\u2022 OptimizerModule [optimizer_modules.py]: Generic optimizer module that enables re-\ncursive generation of a nested state dictionary for optimizers. Supported by TorchRec\u2019s\nKeyedOptimizer\u2019s state_dict and load_state_dict functionality, which is necessary\nfor checkpointing [Ivchenko et al. 2022].\n\u2022 Preconditioner(OptimizerModule) [shampoo_utils.py]: Generic preconditioner class\ncontaining functions for updating the preconditioner and preconditioning a vector.\n\u2022 DistributedPreconditioner(Preconditioner) [shampoo_utils.py]: Contains all dis-\ntributed buffer functionality necessary for distributing the computation.\n"
  },
  {
    "title": "Recovering from Privacy-Preserving Masking with Large Language Models",
    "link": "https://arxiv.org/pdf/2309.08628.pdf",
    "upvote": "4",
    "text": "RECOVERING FROM PRIVACY-PRESERVING MASKING WITH LARGE LANGUAGE\nMODELS\nArpita Vats1, Zhe Liu2, Peng Su2, Debjyoti Paul2, Yingyi Ma2, Yutong Pang2, Zeeshan Ahmed2, Ozlem Kalinli2\n1Santa Clara University, Santa Clara, CA,\n2Meta, Menlo Park, CA\nABSTRACT\nModel adaptation is crucial to handle the discrepancy between proxy\ntraining data and actual users\u2019 data received. To effectively perform\nadaptation, textual data of users is typically stored on servers or their\nlocal devices, where downstream natural language processing (NLP)\nmodels can be directly trained using such in-domain data. However,\nthis might raise privacy and security concerns due to the extra risks\nof exposing user information to adversaries. Replacing identifying\ninformation in textual data with a generic marker has been recently\nexplored. In this work, we leverage large language models (LLMs)\nto suggest substitutes of masked tokens and have their effectiveness\nevaluated on downstream language modeling tasks. Specifically, we\npropose multiple pre-trained and fine-tuned LLM-based approaches\nand perform empirical studies on various datasets for the comparison\nof these methods. Experimental results show that models trained on\nthe obfuscation corpora are able to achieve comparable performance\nwith the ones trained on the original data without privacy-preserving\ntoken masking.\nIndex Terms\u2014 Privacy-preserving machine learning, language\nmodeling, large language models, automatic speech recognition\n1. INTRODUCTION\nA common issue arising after deploying a machine learning model\non central servers or user devices is the discrepancy between training\ndata and actual user data received. Specifically, in the applications\nof natural language processing (NLP), semantic characteristics and\ntopics of real users\u2019 textual data could be very different from those\nof server-side proxy corpora, in which scenarios model adaptation is\nindispensable [1, 2].\nTo effectively perform model adaptation, textual data of users is\ntypically stored on servers or their devices, where any downstream\nNLP models will be trained using such in-domain data. However,\nusers\u2019 personal data might contain sensitive user information, such\nas people\u2019s names, addresses, and credit card numbers. Therefore,\nthis conventional practice of users\u2019 data storage might raise privacy\nand security concerns due to the risks of exposing user information\nto adversaries. In addition, recent research has shown that sensitive\ninformation in training datasets can be detected and then extracted in\nunexpected ways [3, 4, 5, 6, 7]. Particularly, language models (LMs)\nare prone to unintentionally memorize rare or unique sequences of\ndata, and when being prompted appropriately, they will be able to\nemit the memorized text verbatim [8]. Thus, having NLP models di-\nrectly trained on private user data might have extra risks of exposing\nsensitive information.\n* Work done during an internship at Meta.\nTo overcome these challenges, replacing identifying information\nin textual data with a generic marker has been explored [9, 10, 11].\nTo be more specific, tokens considered as sensitive or private are\nmasked out using some special symbol, such as \u201c[MASK]\u201d. In the\nexample where the raw textual sequence is \u201cTom lives in Chicago\u201d,\none might mark the words of \u201cTom\u201d and \u201cChicago\u201d as personal and\nthus replace them with the mask symbol. The resulting sequence\nis \u201c[MASK] lives in [MASK]\u201d, which will be stored into servers or\nlocal devices for model adaptation purposes later on.\nWhile this strategy is capable to provide privacy protections on\nuser data, it also introduces significant complexities to the training of\nany NLP models for downstream adaptation tasks. The existence of\nmarkers might break the semantic structures, disrupt the coherence\nof languages, or fail to preserve the meaning of the original textual\nsequences. As a result, models directly trained on the masked corpus\ncould yield much worse performance compared with the ones trained\non the raw corpus without privacy-preserving token masking. There-\nfore, it calls for advanced approaches on effectively substituting the\nmasked tokens in the corpus and bridge the accuracy gaps in NLP\nmodels for adaptation tasks.\nIn this work, we propose to use large language models (LLMs)\nto provide appropriate candidate tokens to fill in the generic markers\nin any masked corpus. Note that predicting the masked tokens based\non the surrounding context can be considered as a task of masked\nLM (MLM), thus bi-directional Transformer [12] based pre-trained\nLLMs, such as BERT [13] and RoBERTa [14], would be suitable for\nthis endeavor. Upon observing the remarkable capabilities demon-\nstrated by decoder-only LLMs, models such as ChatGPT [15] and\nLLaMA2 [16] can also be utilized here for providing substitutes of\nmasked tokens. Our goal is not to restore any markers to the origi-\nnal tokens without masking, instead, we aim to replace any masked\ntoken with some substitute of the same type. More specifically, the\nefficiency of any recovering method from privacy-preserving mask-\ning shall be evaluated on the downstream adaptation tasks, through\nthe NLP models trained on the obfuscation corpus. In this paper, we\nuse language modeling and LM-fused automatic speech recognition\n(ASR) [17, 18, 19, 20, 21] as the downstream tasks.\nWe make the following contributions:\n\u2022 To the best of our knowledge, our work is the first to leverage\nLLMs to suggest substitutes of masked tokens and have their\neffectiveness evaluated on downstream LM and ASR tasks;\n\u2022 We propose multiple pre-trained and fine-tuned LLM-based\nmethods and conduct empirical experiments on various NLP\ndatasets for the comparison of adapted models accordingly.\nThe results of our experiments indicate that models trained\non the obfuscation corpora have comparable performance\nwith the ones trained on the original data without privacy-\npreserving token masking;\narXiv:2309.08628v3  [cs.CL]  14 Dec 2023\n\u2022 We also present three token masking techniques and measure\nthe performance of our proposed methods on each of them in\ndownstream tasks as well.\nThe rest of the paper is organized as follows. We review related\nworks in Section 2. Section 3 describes the details of our proposed\nframework on privacy-preserving token masking and the substitutes\nof masked tokens using LLMs. Next, Section 4 shows the exper-\niments and results for downstream tasks of LM and ASR. Finally,\nWe conclude in Section 5.\n2. RELATED WORKS\nPrivacy protection has been becoming crucial in NLP research [10].\nOne important direction in this area is through anonymization, which\ninvolves the removal of identifying information from textual corpus\n[9, 22, 23]. More recently, obfuscation, replacing any sensitive in-\nformation with a different substitute of the same type has been in-\nvestigated. In particular, a survey of profanity obfuscation in NLP is\nconducted in [24]. Authors in [25] employs a neural model that aims\nto preserve the syntactic relationships of the original sentence so that\nthe obfuscated sentence can be parsed instead of the original one; it\noutperforms random substitution baselines across syntactic parsers.\nThe work of [11] studies named entity obfuscation in speech, which\nfocuses on identifying, replacing, and inserting replacement named\nentities synthesized using voice cloning into original audio. The pa-\nper of [26] improves the speech recognition of personal identifiers by\nincluding fake textual substitutes in the training data of ASR. None\nof these existing works explore the use and comparison of different\nLLMs for suggesting token substitutes in obfuscation.\n3. METHODOLOGY\nWe describe our proposed approaches on privacy-preserving token\nmasking and the substitutes of masked tokens using LLMs. Specifi-\ncally, we introduce several token masking techniques in Section 3.1;\nLLM-based methods on replacing the masked tokens are presented\nin Section 3.2; Section 3.3 discusses the use of obfuscation corpus\nfor performing language modeling task.\nThe overall framework is depicted in Figure 1.\nFig. 1.\nThe framework of token masking and obfuscation using\nLLMs.\n3.1. Token Masking Techniques\nMasking sensitive tokens from users\u2019 data helps reduce the privacy\nrisks and prevent any personal information being leaked or extracted\nfrom adversaries. Such token masking task shall be performed with-\nout human-in-the-loop since practitioners are not allowed to have the\naccess to annotate or label private data of users.\nTo automatically conceal sensitive information in some private\ncorpus, we propose the following token masking techniques:\n\u2022 allowList: This is a pre-defined list of tokens that are con-\nsidered non-sensitive and safe to keep. Typically, such list is\nhandcrafted by linguistic specialists. Then during the process\nof masking, any token not present in this allow list will be\nmasked out;\n\u2022 vocabThres: This involves the selection of N most frequent\ntokens from a vocabulary as the list of non-sensitive tokens.\nThat is, any token with its frequency less than some threshold\nwill be masked out. Here, the vocabulary set can be built from\nsome generic large corpora;\n\u2022 entityTagger: In this approach, named entity recognition\n(NER) models are utilized to identify potential entities in any\nprivate corpus, which will be treated as personal tokens and\nmasked out. These entities include but are not limit to indi-\nviduals\u2019 names, locations, and organizations.\nThroughout these masking techniques, we will more likely mask the\nnon-common tokens in any corpus, assuming privacy information is\nmore related to rare or unique tokens. After applying the masking,\nwe obtain a masked corpus where the masked tokens were replaced\nwith the symbol of \u201c[MASK]\u201d.\n3.2. Recovery Methods from Masking\nToken masking provides privacy protections, however, the resulting\nmasked corpus might not be suitable to be directly used for training\nNLP models for downstream tasks.\nGiven any masked corpus, we propose to use LLMs to fill in\neach mask symbol with appropriate token that matches the semantic\ncontexts. It is important to note that we are not aiming to predict\nexactly the same token with the original one in the raw corpus. We\nexpect to substitute it with some token that makes the whole sentence\nlinguistically correct and complete.\nThe following illustrates different strategies on leveraging LLMs\nfor substituting masked tokens:\n\u2022 Top-1: In this method, we directly use the 1-best predicted\ntoken from an LLM to replace the masked token. Here, token\nfilling is considered as a masked LM task. If there are multi-\nple markers in the sentence, they are replaced in a sequential\norder from the left to the right, one at a time;\n\u2022 Top-K: This approach extends the token filling candidates\nfrom the 1-best to the K-best from the predictions of an LLM.\nSpecifically, we randomly choose a token from the top-K\npredictions. Then this selected token is used to fill in the\nmarker in the sentence. For substituting any masked tokens\nfrom allowList or vocabThres based masking techniques,\nwe prefer the predicted tokens not being included in the cor-\nresponding token list, thus we repeat the random sampling\nprocess until this condition is met or there is no available can-\ndidates of predicted tokens among the top-K;\n\u2022 Fine-Tuning(FT): In the previous two approaches, we\nutilize the token predictions from a pre-trained LLM. Fine-\ntuning a pre-trained LLM using in-domain corpus helps the\nmodel gain domain-specific knowledge, and hence enhance\nthe performance in the masked token prediction. To accom-\nplish this, samples without any masked tokens can be used\nfor fine-tuning. However, in many scenarios, it is possible\nthat majority of samples contain at least one mask symbol so\nthat fine-tuning is less effective especially when the size of\ncorpus is small. Alternatively, the top-1 or top-K predictions\nfrom the same pre-trained LLM can be firstly used to substi-\ntute the masked tokens in any samples, and then the entire ob-\nfuscation corpus can be used for fine-tuning the LLM. Once\nwe have a fine-tuned LLM, either Top-1 or Top-K can be\napplied for the substitution of masked tokens. Note that the\nprocess above can be utilized for multiple times.\nAfter applying any of these methods, we obtain an obfuscation cor-\npus that does not contain any masks.\n3.3. Performing Downstream Tasks\nOnce we have substituted masked tokens, the resulting corpus can be\nused for training machine learning models for any downstream tasks.\nNotice that the effectiveness of any token filling approach should be\nmeasured by the performance of these machine learning models on\nthese downstream tasks.\nIn this work, we consider the language modeling adaptation task\nwhere a generic pre-trained LM is fine-tuned on the obfuscation cor-\npus. This adapted LM will be evaluated on a (unmasked) test set\nwhich has the same domain with the raw corpus. The performance\nof LM is measured in term of perplexity.\nWhen integrating an adapted LM with an ASR model via shal-\nlow fusion, word error rate (WER) can also be evaluated on a test set\nof utterances.\n4. EXPERIMENTS\n4.1. Datasets\nTo compare the performance of multiple baselines and our proposed\napproaches on the downstream language modeling task, we explore\nthree datasets in the experiments: Fisher [27], Pushshift.io Red-\ndit1 [28], and Wall Street Journal (WSJ) [29]. The statistics of these\ndatasets are summarized in Table 1. The test set of WSJ data also\nconsists of voice utterances and is thus used for evaluating the ASR\nmodels with fused LMs.\nTable 1. Data information.\nTrain Set (#sent)\nTest Set (#sent)\nFisher\n1,158,496\n50,000\nReddit\n763,683\n49,570\nWSJ\n6,000\n800\n4.2. Setups\n4.2.1. Downstream Tasks\nThe downstream LM is a Transformer with 6 layers, 12 attention\nheads, and 768 hidden units. The set of word vocabulary is around\n1Pushshift.io Reddit dataset is a previously existing dataset extracted and\nobtained by a third party that contains preprocessed comments posted on the\nsocial network Reddit and hosted by pushshift.io. We will refer this dataset\nas \u201cReddit\u201d in the rest of the paper.\n85K. The LM is pre-trained on WikiText-103 corpus [30].\nFor each of the masking techniques considered in this study,\nLMs are fine-tuned on the obfuscation train sets of Fisher, Reddit,\nand WSJ data. Their perplexities are evaluated on the corresponding\ntest sets.\nOn the WSJ test set, we also evaluate the ASR performance. The\nASR model is an RNN-T model with the Emformer encoder [31],\nLSTM predictor, and a joiner. It has around 80 million parameters\nand is trained from scratch using the train split of LibriSpeech ASR\ncorpus [32].\n4.2.2. Masking Techniques\nIn our experiments, allowList contains a set of 5K curated com-\nmon words, and vocabThres consists of 10K most frequent words\namong the same 85K word vocabulary mentioned above. For the\nentityTagger masking technique, we utilize the BERT-NER model\n[13, 33] for tagging named entities in the train sets.\nFor each of these masking techniques, Table 2 shows the per-\ncentage of masked tokens per dataset. We can see that allowList\nmasks many more tokens than the other two techniques.\nTable 2. Percentages of masked tokens.\nallowList\nvocabThres\nentityTagger\nFisher\n12.5%\n1.3%\n1.7%\nReddit\n22.7%\n11.9%\n4.2%\nWSJ\n30.4%\n11.2%\n9.1%\n4.2.3. Baselines\nWe consider the following methods as the baselines:\n\u2022 Oracle: an LM is trained on the ground-truth sentences\nwithout any masking, which provides the upper bound for the\nmodel performance on each dataset;\n\u2022 Baseline0: an LM is directly trained on the masked cor-\npus, where the mask symbol \u201c[MASK]\u201d is treated as a special\ntoken during model training;\n\u2022 Baseline1: zero weight is assigned to any mask symbol\n\u201c[MASK]\u201d in the LM loss function during model training.\nNote that for each of these methods, the LM is still pre-trained on\nthe WikiText-103 corpus.\n4.2.4. LLM-Based Methods\nIn our experiments, we consider the following LLMs for substituting\nmasked tokens in any training sequences: BERT (base, uncased),\nRoBERTa (base), and LLaMA2 (7B model parameters).\nFor the fine-tuning of BERT and RoBERTa, we use MLM as\nthe training task. During the inference time of using pre-trained or\nfine-tuned BERT and RoBERTa to substitute masked tokens, any\nconsecutive markers of \u201c[MASK]\u201d are merged into one marker. We\nset K = 10 in the Top-K method.\nFor LLaMA2, we adopt a different approach for the fine-tuning\nprocess since it is an auto-regressive model. Specifically, for each\ntraining sample, we generate prompts by combining some instruc-\ntion, input, and output text: instruction contains the text of \u201cPredict\nthe [MASK] tokens in the given sentence\u201d; input is the same training\nsample but having a few tokens randomly replaced with the symbol\nof \u201c[MASK]\u201d; and output is the original training sample (without\nmasking). We leverage the low-rank adaptation (LoRA) method [34]\nfor fine-tuning LLaMA2 on the set of prompts. During the inference\ntime, the instruction and input are provided to the fine-tuned model,\nwhich allows the model for continued text generation.\n4.3. Results\nTable 3 shows the perplexity results of the baselines and proposed\nmethods on Fisher dataset. We have the following observations:\n\u2022 All proposed methods give lower perplexity results than the\ntwo baseline methods;\n\u2022 In all scenarios, Top-K outperforms Top-1 based methods;\nfine-tuned BERT and RoBERTa obtain better results than the\nones without fine-tuning;\n\u2022 Since more tokens are masked out with allowList, the gap\nbetween Oracle and any other method is much larger than\nthat of vocabThres or entityTagger masking technique;\n\u2022 RoBERTa yields the best perplexity performance across all\nthe masking techniques. In particular, for vocabThres and\nentityTagger, perplexity results from fine-tuned RoBERTa\nare very close to those of Oracle, which indicates that most\nof the missing information can be recovered in the obfusca-\ntion dataset;\n\u2022 LLaMA2(Top-1,FT) is a competitive method but is not as\ngood as fine-tuned BERT or RoBERTa for this task.\nTable 3. Perplexity results on Fisher dataset.\nallowList\nvocabThres\nentityTagger\nOracle\n37.3\n37.3\n37.3\nBaseline0\n120.1\n42.3\n41.7\nBaseline1\n109.4\n41.6\n41.6\nBERT(Top-1)\n93.0\n41.3\n41.5\nRoBERTa(Top-1)\n71.6\n40.5\n39.5\nBERT(Top-K)\n75.2\n40.8\n40.5\nRoBERTa(Top-K)\n70.2\n38.9\n38.7\nBERT(Top-K,FT)\n73.6\n39.8\n39.7\nRoBERTa(Top-K,FT)\n65.3\n38.9\n38.5\nLLaMA2(Top-1,FT)\n89.3\n40.8\n40.7\nTable 4 shows the experimental results on Reddit dataset. The\nobservations are similar to the ones in Fisher dataset. In particular,\nRoBERTa(Top-K,FT) again achieves the best perplexity results\nacross all the masking techniques.\nTable 4. Perplexity results on Reddit dataset.\nallowList\nvocabThres\nentityTagger\nOracle\n76.0\n76.0\n76.0\nBaseline0\n339.6\n168.2\n82.3\nBaseline1\n221.9\n134.9\n79.8\nBERT(Top-1)\n196.2\n121.2\n78.9\nRoBERTa(Top-1)\n117.3\n94.2\n78.4\nBERT(Top-K)\n127.4\n106.3\n78.7\nRoBERTa(Top-K)\n123.4\n92.6\n77.4\nBERT(Top-K,FT)\n117.4\n102.5\n77.6\nRoBERTa(Top-K,FT)\n98.5\n82.1\n76.8\nLLaMA2(Top-1,FT)\n123.3\n107.7\n78.7\nTable 5 and Table 6 show the perplexity and WER results on\nWSJ dataset, respectively. We have the following findings:\n\u2022 The use of fused LM for conducting domain adaptation in\nASR models is effective: comparing the WERs between ASR\nmodels with the pre-trained LM and the Oracle LM, there\nis a more than 15% WER improvement achieved by the latter;\n\u2022 The best WERs obtained by proposed methods have rela-\ntively small gaps compared with those of the Oracle LM.\nFor vocabThres and entityTagger masking techniques, the\nWERs from Oracle are lifted by only 1% (10.7 versus 10.6)\nand 5% (11.1 versus 10.6), respectively. That is, the proposed\nmethods are able to achieve significant improvements over\nthe pre-trained LM (without adaptation), while they also pro-\nvide better privacy protection than the Oracle LM.\nTable 5. Perplexity results on WSJ dataset.\nallowList\nvocabThres\nentityTagger\nOracle\n86.5\n86.5\n86.5\nBaseline0\n309.0\n144.3\n204.0\nBaseline1\n210.0\n122.9\n198.2\nBERT(Top-1)\n205.9\n119.4\n149.3\nRoBERTa(Top-1)\n181.1\n102.5\n118.2\nBERT(Top-K)\n174.1\n103.3\n108.3\nRoBERTa(Top-K)\n114.5\n93.4\n98.7\nBERT(Top-K,FT)\n186.7\n113.4\n162.3\nRoBERTa(Top-K,FT)\n120.7\n110.4\n157.8\nLLaMA2(Top-1,FT)\n135.6\n106.8\n145.6\nTable 6. WER results on WSJ dataset.\nallowList\nvocabThres\nentityTagger\nASR-without-LM\n14.4\n14.4\n14.4\nPre-Trained-LM\n12.6\n12.6\n12.6\nOracle\n10.6\n10.6\n10.6\nBaseline0\n13.0\n12.6\n11.3\nBaseline1\n12.5\n11.2\n11.2\nBERT(Top-1)\n12.4\n11.1\n11.2\nRoBERTa(Top-1)\n12.4\n10.9\n11.1\nBERT(Top-K)\n12.1\n11.1\n11.4\nRoBERTa(Top-K)\n11.9\n10.9\n11.1\nBERT(Top-K,FT)\n12.7\n11.5\n11.7\nRoBERTa(Top-K,FT)\n11.8\n11.4\n11.1\nLLaMA2(Top-1,FT)\n12.0\n10.7\n11.2\n5. CONCLUSION\nIn this paper, we propose multiple pre-trained and fine-tuned LLM-\nbased methods to recover from privacy-preserving token masking\non textual corpus and perform empirical studies on various datasets\nfor the comparison of these approaches. Our experimental results\ndemonstrate that LMs trained on the obfuscation corpora can obtain\ncomparable accuracy with the ones trained on the raw data without\nprivacy-preserving token masking.\nFuture research might include fine-tuning LLMs with the object\nfunction designed to be more directly related to the downstream NLP\ntasks. Also, we would consider a combination of these three masking\ntechniques and adopt class-specific markers such as \u201c[PERSON]\u201d,\n\u201c[NUMBER]\u201d, etc.\n6. REFERENCES\n[1] Ke Li, Zhe Liu, Tianxing He, Hongzhao Huang, Fuchun Peng, Daniel\nPovey, and Sanjeev Khudanpur, \u201cAn empirical study of transformer-\nbased neural language model adaptation,\u201d in Proc. ICASSP, 2020.\n[2] Zhe Liu, Ke Li, Shreyan Bakshi, and Fuchun Peng,\n\u201cPrivate lan-\nguage model adaptation for speech recognition,\u201d\narXiv preprint\narXiv:2110.10026, 2021.\n[3] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart, \u201cModel inver-\nsion attacks that exploit confidence information and basic countermea-\nsures,\u201d in Proc. ACM SIGSAC, 2015.\n[4] Congzheng Song and Vitaly Shmatikov, \u201cAuditing data provenance in\ntext-generation models,\u201d in Proc. ACM SIGKDD, 2019.\n[5] Nicholas Carlini, Chang Liu, \u00b4Ulfar Erlingsson, Jernej Kos, and Dawn\nSong,\n\u201cThe secret sharer: Evaluating and testing unintended mem-\norization in neural networks,\u201d in 28th USENIX Security Symposium,\n2019.\n[6] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski,\nAriel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn\nSong, Ulfar Erlingsson, et al., \u201cExtracting training data from large lan-\nguage models,\u201d in 30th USENIX Security Symposium, 2021.\n[7] W Ronny Huang, Steve Chien, Om Thakkar, and Rajiv Mathews, \u201cDe-\ntecting unintended memorization in language-model-fused ASR,\u201d in\nProc. Interspeech, 2022.\n[8] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee,\nFlorian Tramer, and Chiyuan Zhang,\n\u201cQuantifying memorization\nacross neural language models,\u201d\narXiv preprint arXiv:2202.07646,\n2022.\n[9] Sergio Mart\u00b4\u0131nez, David S\u00b4anchez, Aida Valls, and Montserrat Batet,\n\u201cPrivacy protection of textual attributes through a semantic-based\nmasking method,\u201d Information Fusion, vol. 13, no. 4, pp. 304\u2013314,\n2012.\n[10] Samuel Sousa and Roman Kern, \u201cHow to keep text private? a sys-\ntematic review of deep learning methods for privacy-preserving natural\nlanguage processing,\u201d Artificial Intelligence Review, vol. 56, no. 2, pp.\n1427\u20131492, 2023.\n[11] Judita Preiss, \u201cAutomatic named entity obfuscation in speech,\u201d in Find-\nings of ACL, 2023.\n[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, \u201cAttention\nis all you need,\u201d Advances in NeurIPS, 2017.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,\n\u201cBERT: Pre-training of deep bidirectional transformers for language\nunderstanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy-\nanov, \u201cRoBERTa: A robustly optimized BERT pretraining approach,\u201d\narXiv preprint arXiv:1907.11692, 2019.\n[15] OpenAI, \u201cChatGPT: Optimizing language models for dialogue,\u201d Feb\n2022.\n[16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, et al., \u201cLlama 2: Open foundation and fine-\ntuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n[17] Tom\u00b4a\u02c7s Mikolov, Martin Karafi\u00b4at, Luk\u00b4a\u02c7s Burget, Jan \u02c7Cernock`y, and\nSanjeev Khudanpur,\n\u201cRecurrent neural network based language\nmodel,\u201d in Proc. Interspeech, 2010.\n[18] Xie Chen, Xunying Liu, Mark JF Gales, and Philip C Woodland, \u201cIm-\nproving the training and evaluation efficiency of recurrent neural net-\nwork language models,\u201d in Proc. ICASSP, 2015.\n[19] Xunying Liu, Yongqiang Wang, Xie Chen, Mark JF Gales, and Philip C\nWoodland, \u201cEfficient lattice rescoring using recurrent neural network\nlanguage models,\u201d in Proc. ICASSP, 2014.\n[20] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng\nChen, and Rohit Prabhavalkar, \u201cAn analysis of incorporating an ex-\nternal language model into a sequence-to-sequence model,\u201d in Proc.\nICASSP, 2018.\n[21] Kazuki Irie, Albert Zeyer, Ralf Schl\u00a8uter, and Hermann Ney, \u201cLanguage\nmodeling with deep transformers,\u201d in Proc. Interspeech, 2019.\n[22] Pierre Lison, Ildik\u00b4o Pil\u00b4an, David S\u00b4anchez, Montserrat Batet, and Lilja\n\u00d8vrelid, \u201cAnonymisation models for text data: State of the art, chal-\nlenges and future directions,\u201d in Proc. ACL, 2021.\n[23] Tzvika Hartman, Michael D Howell, Jeff Dean, Shlomo Hoory, Ronit\nSlyper, Itay Laish, Oren Gilon, Danny Vainstein, Greg Corrado,\nKatherine Chou, et al., \u201cCustomization scenarios for de-identification\nof clinical notes,\u201d BMC medical informatics and decision making, vol.\n20, no. 1, pp. 1\u20139, 2020.\n[24] Debora Nozza and Dirk Hovy, \u201cThe state of profanity obfuscation in\nnatural language processing,\u201d arXiv preprint arXiv:2210.07595, 2022.\n[25] Zhifeng Hu, Serhii Havrylov, Ivan Titov, and Shay B. Cohen, \u201cObfus-\ncation for privacy-preserving syntactic parsing,\u201d 2020.\n[26] Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew\nRosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa\nHaghani, and Bhuvana Ramabhadran,\n\u201cUsing text injection to im-\nprove recognition of personal identifiers in speech,\u201d\narXiv preprint\narXiv:2308.07393, 2023.\n[27] Christopher Cieri, David Miller, and Kevin Walker, \u201cThe fisher corpus:\na resource for the next generations of speech-to-text,\u201d in International\nConference on Language Resources and Evaluation, 2004.\n[28] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire,\nand Jeremy Blackburn, \u201cThe Pushshift reddit dataset,\u201d in International\nConference on Web and Social Media, 2020.\n[29] Lukas Drude, Jens Heitkaemper, Christoph Boeddeker, and Reinhold\nHaeb-Umbach,\n\u201cSMS-WSJ: Database, performance measures, and\nbaseline recipe for multi-channel source separation and recognition,\u201d\narXiv preprint arXiv:1910.13934, 2019.\n[30] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher,\n\u201cPointer sentinel mixture models,\u201d arXiv preprint arXiv:1609.07843,\n2016.\n[31] Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Ju-\nlian Chan, Frank Zhang, Duc Le, and Mike Seltzer, \u201cEmformer: Effi-\ncient memory transformer based acoustic model for low latency stream-\ning speech recognition,\u201d in Proc. ICASSP, 2021.\n[32] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudan-\npur,\n\u201cLibriSpeech: an ASR corpus based on public domain audio\nbooks,\u201d in Proc. ICASSP, 2015.\n[33] Erik F. Tjong Kim Sang and Fien De Meulder, \u201cIntroduction to the\nCoNLL-2003 shared task: Language-independent named entity recog-\nnition,\u201d\nin Proc. of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, 2003.\n[34] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen, \u201cLoRA: Low-rank adap-\ntation of large language models,\u201d arXiv preprint arXiv:2106.09685,\n2021.\n"
  },
  {
    "title": "Stack-and-Delay: a new codebook pattern for music generation",
    "link": "https://arxiv.org/pdf/2309.08804.pdf",
    "upvote": "3",
    "text": "STACK-AND-DELAY: A NEW CODEBOOK PATTERN FOR MUSIC GENERATION\nGael Le Lan\nVarun Nagaraja\nErnie Chang\nDavid Kant\nZhaoheng Ni\nYangyang Shi\nForrest Iandola\nVikas Chandra\nMeta AI\nABSTRACT\nIn language modeling based music generation, a gener-\nated waveform is represented by a sequence of hierarchical\ntoken stacks that can be decoded either in an auto-regressive\nmanner or in parallel, depending on the codebook patterns.\nIn particular, flattening the codebooks represents the highest\nquality decoding strategy, while being notoriously slow. To\nthis end, we propose a novel stack-and-delay style of decod-\ning strategy to improve upon the flat pattern decoding where\ngeneration speed is four times faster as opposed to vanilla flat\ndecoding. This brings the inference time close to that of the\ndelay decoding strategy, and allows for faster inference on\nGPU for small batch sizes. For the same inference efficiency\nbudget as the delay pattern, we show that the proposed ap-\nproach performs better in objective evaluations, almost clos-\ning the gap with the flat pattern in terms of quality. The re-\nsults are corroborated by subjective evaluations which show\nthat samples generated by the new model are slightly more\noften preferred to samples generated by the competing model\ngiven the same text prompts.\nIndex Terms\u2014 music generation, audio generation, effi-\ncient decoding, transformer decoder\n1. INTRODUCTION\nThe task of text-to-music generation has seen an increasing\ninterest from the research community in the past year [1, 2, 3,\n4, 5, 6]. This was enabled by the emergence of two competing\narchitectures originating from the computer vision and natu-\nral language processing spaces, respectively: diffusion [7, 8]\nand Transformer-based language models (LMs) [9, 10]. The\nformer method can be referred to as parallel decoding while\nthe latter is usually auto-regressive.\nThe level of quality is getting closer to that of original\nsongs, paving the road towards new commercial use cases\nsuch as personalized on-device music generation, where the\nbatch size is typically small. However those models often\ncome with a quality trade off: the higher the quality, the\nslower the generation and vice versa [3, 6]. During inference,\nthe decoding strategy, hardware and model size influence the\nspeed of the generation. [4] recently proposed a single-stage\nauto-regressive Transformer decoder that models sequences\nof compressed discrete music representations (i.e.\ntokens\ncompute by an audio compression model [11]). The authors\nexplored several codebook patterns for the discrete tokens\nsequence modeling. In particular, they showed that the best\nperforming pattern relies on flattening the token stack (which\nwill be referred to as the flat pattern in the rest of the paper).\nIndeed each piece of generated waveform is actually repre-\nsented by not only one token but several, corresponding to\nthe number C of residual projections in the Residual Vector\nQuantizer (RVQ) [12] module of the compression model.\nFlattening the token stack comes with the cost of gen-\nerating (and training) for a C times longer sequence, which\nimplies a significantly higher real-time-factor (RTF), making\nthe model unusable in practice for interactive user experience.\nTo overcome that issue, the proposed delay pattern [4] was\nshown to be a good trade off between speed and quality.\nIn this paper we hypothesize that despite its efficiency,\nthe delay pattern could affect the model ability to generate\nhigh quality samples by design. Starting from the stronger\nbut slower flat pattern, we propose a new strategy called stack-\ndelay that is able to generate music as fast as the original delay\nstrategy, with significantly higher quality. The contributions\nof this paper are:\n\u2022 a new stack codebook pattern that inherits the quality of\nflat while being faster and memory efficient during in-\nference by reducing the past key/value streaming cache\nfootprint.\n\u2022 a new stack-delay pattern that:\n\u2013 benefits from the stack pattern strengths while be-\ning as fast as the delay pattern for generation.\n\u2013 produces higher quality music than delay, shown\nby objective and subjective evaluations.\n\u2022 an new decoding schedule that involves interleaving de-\ncoded positions that prevents the model from decoding\nadjacent positions until they have enough context.\n2. STACK-DELAY CODEBOOK PATTERN\n2.1. Music generation\nGiven a text description, a sequence of text embeddings com-\nputed by the T5 encoder [13] serves as the conditioning signal\narXiv:2309.08804v1  [eess.AS]  15 Sep 2023\nt1\nt2\nt1\nt3\nt2\nt1\nt3\nt2\nt1\nt5\nt3\nt2\nt6\nt5\nt3\nt7\nt6\nt5\nt0\nt0\nt0\nt0\nt4\nt4\nt4\nt4\nt0\nt0\nt0\nt0\nt0\nt0\nt1\nt1\nt1\nt1\nt1\nt1\nt0\nt0\nt0\nt0\nt1\nt1\nt1\nt1\nt1\nt2\nt5\nt0\nt4\nc0\nc1\nc2\nc3\nc0\nc1\nc2\nc3\nc0\nc1\nc2\nc3\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\nDELAY\nSTACK-DELAY\nSTACK\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\ns10\ns11\ns12\ns13\ns14\ns15\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nt1\nt2\nt5\nt0\nt4\nt3\nt6\nt9\nt7\nt10\nt13\nt8\nt12\nt16\nc0\nc1\nc2\nc3\ns16\ns17\ns18\ns19\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\nFig. 1. Comparison of the proposed stack-delay pattern (right) with the delay (top left) and stack (bottom left). Under the stack-\ndelay pattern the tokens are generated in a multi-stream fashion, in parallel. Time steps are decoded in a permuted manner. Only\nkey/value embeddings from the top-level stream are stored in long-term streaming cache, which makes inference as efficient as\ndelay while retaining better quality from stack pattern.\nfor a Transformer decoder model (using cross attention). The\nmodel generates a sequence of EnCodec [11] token stacks\n{cit}C\u22121\ni=0 that are CNN-decoded into an audio waveform. i\nrepresents the token level while t represents the time step in\nthe generated sequence.\nIn this paper we only consider the auto-regressive Trans-\nformer decoder architecture [9] that emits a probability dis-\ntribution over the token space that is conditioned on the pre-\nviously generated tokens (causal self attention in the Trans-\nformer decoder).\nDuring inference, the past self attention\nkeys and values are stored in a streaming cache to optimize\nthe generation time. Depending on the tokenizer framerate f\n(e.g. f = 50Hz), the duration of audio to generate d and\nthe size of the token stack C (e.g. C = 4), the model has\nto generate f \u00d7 C \u00d7 d tokens in a given amount of decoding\nsteps that depend on the token codebook pattern and decod-\ning schedule. The decoding schedule can be formalized as a\nfunction G(i, t) defining the decoding step for each cit.\n2.2. Codebook patterns\nContrary to the text domain, a segment of audio is not repre-\nsented by a single token but by a stack of hierarchical tokens\ncomputed by quantizing [12] the latent embedding of a CNN\nauto-encoder [11]. This usually means the lower the token in\nthe stack, the more information it carries. To address the issue\nof predicting tokens in a hierarchical manner, several code-\nbook interleaving patterns have been explored [14, 4, 15],\nwith the common idea to decode the lowest level token first\nthen handle the higher levels in further decoding steps, which\nis the case for both auto-regressive (AR) [4] and non auto-\nregressive (NAR) [6] decoding architectures. Namely the de-\ncoding schedule is constrained such that:\nG(0, t) < G(i, t), \u2200i \u2208 [1, C[\n(1)\n2.2.1. Delay\nRegarding music generation, the delay interleaving pattern\n(presented on the top left part of Figure 1) was shown to be\na good compromise between quality and AR decoding step\ncount. Under the delay pattern, the C codebook levels are\npredicted in parallel but with a shift of in the decoded time\nstep. Namely G(i, t) = t + i. This means that each subse-\nquent time step in the sequence starts to be decoded with only\npartial knowledge of the previous adjacent time step. For ex-\nample, the prediction of c0t1 in decoding step s1 in the Figure\nis only conditioned on c0t0, previously decoded in s0, but not\non higher levels {ci}C\u22121\ni=0 of time step t0.\n2.2.2. Stack\n[4] showed that to obtain the highest music quality, flattening\nthe codebooks performed the best, at the expense of C times\nmore decoding steps.\nG(i, t) = C \u00d7 t + i < C \u00d7 T\n(2)\nThis can be easily explained by the fact that subsequent de-\ncoded time steps benefit from the full context of the preceding\nones. In such case the prediction of c0t+1 is effectively con-\nditioned on c[0,C\u22121][0,t]. The context length is C times bigger\npattern\ndecoding steps\ncontext length\ndelay\nT\nT\nflat\nT \u00d7 C\nT \u00d7 C\nstack\nT \u00d7 C\nT + C\nstack-delay\nT\nT\nTable 1. Required decoding step count and maximum context\nlength of the streaming cache during inference, as a function\nof the sequence length to generate T = d \u00d7 f and the number\nof codebook levels C.\nthan delay since the at most C \u00d7 T past Transformer self at-\ntention key/value representations are stored in the streaming\ncache during inference. To reduce the cache size we adapt the\nflat pattern by retaining and stacking the lower level tokens\nthroughout the decoding process, as shown in Figure 1. Once\na full stack has been decoded for a given time step, partial\nstacks can be erased from the streaming cache as the full stack\ncontains all the required information. This way the maximum\ncache length is only of C +T instead of C \u00d7T. The stack pat-\ntern requires a customized attention mask during training that\nsimulates the inference dynamic caching behavior. However\nit still requires C times more decoding steps than delay.\n2.2.3. Stack-delay\nTo compensate for the increased decoding step count (i.e. in-\nference time) of the stack pattern, we propose to introduce C\nparallel decoding streams in what we call the stack-delay pat-\ntern, illustrated in the right part of Figure 1. Having C parallel\nstreams decoding a C times longer sequence means that over-\nall the total number of decoding steps is the same as for the\ndelay pattern (i.e. T). The main difference with delay is that\nwe no longer stack tokens from different time steps but always\nfrom the same time step. This also allows positional encoding\nto encode not only the decoded time step but also the decoded\ntoken level, hence hinting the model about which time step\nand level is about to be decoded. We hope this will improve\nthe overall model performance for the same inference effi-\nciency budget as delay, due to the use of parallel-optimized\ncompute hardware. We report the decoding step count and\nmaximum context length in Table 1 for each pattern.\n2.2.4. Timesteps interleaving\nFinally, we introduce time steps permutation in the decod-\ning schedule: the decoding remains auto-regressive but the\nmodel is trained to predict the token sequence in a time step-\npermuted order. This aims to offer more context for adjacent\ntime steps decoding. An example of such interleaving pattern\nis shown on the right part of Figure 1, which corresponds to\nthe decoding schedule defined in equation 3 with k = 3. Ac-\ncording to the equation, the delay pattern decoding schedule\ncorresponds to the case where k = 1.\nG(i, t) = t + (t\nmod (k + 1)) \u00d7 (k \u2212 1) + i\n(3)\n3. EXPERIMENTAL SETUP\nMost of the experimental setup follows that of MusicGen [4],\nwe refer the readers to it for more details.\n3.1. Model\nThe tokenizer is an EnCodec model [11], made of CNN au-\ntoencoder and Residual Vector Quantization module applied\nto the latent representation of waveforms. The RVQ module\nis made of C = 4 quantizers, each with a codebook size of\n2048. It encodes 32 kHz monophonic audio into a stack of 4\ntokens every 20ms (50 Hz framerate).\nThe Transformer decoder is made of 300M parameters,\nimplemented with a customized version of audiocraft1. It\nuses Pytorch 2.02 flash attention for faster training and gener-\nation with optimized memory footprint. The model is trained\non 30-seconds random crops of the full track. The models\nare trained for 200 epochs (400k steps) with the AdamW op-\ntimizer, a batch size of 192, \u03b21 = 0.9, \u03b22 = 0.95, a decoupled\nweight decay of 0.1 and no gradient clipping. A cosine learn-\ning rate schedule with a warmup of 4000 steps is used at the\nbeginning of training. Models are trained with an exponential\nmoving average with 0.99 decay. Training uses fp16 mixed\nprecision and distributed data parallelism on 24 A100 GPUs.\n3.2. Generation\nAt each decoding step the Transformer decoder emits a prob-\nability distribution over the token space for time steps and lev-\nels to decode according to the decoding schedule. Tokens are\nsampled from the distribution with top-k nucleus sampling\nwith k = 250 tokens and a temperature of 1.0. We apply\nclassifier-free guidance [16] when sampling from the model\u2019s\nlogits, with a guidance scale of 3.0.\nThe baseline model uses the delay codebook pattern from\n[4]. This translates 30 seconds of audio into T = 500 auto-\nregressive steps. For text conditioning, we use the T5 [13]\ntext encoder. During training we drop the text condition with\na probability of 0.1. We experiment with flat, stack and stack-\ndelay codebook patterns.\n3.3. Data\nWe train our models on 20K hours of licensed music: an in-\nternal dataset of 10K high-quality music tracks and the Shut-\nterStock and Pond5 music data collections3 with respectively\n25K and 365K instrument-only recordings. All recordings are\nsampled at 32 kHz and come with a textual description. The\nmodels are evaluated on an in-domain split different from that\nof [4] and on the MusicCaps dataset [17].\n1https://github.com/facebookresearch/audiocraft\n2https://pytorch.org/\n3www.shutterstock.com/music and www.pond5.com\npattern\nin-domain\nMusicCaps\nRTF\nFAD\nKLD\nCLAP\nFAD\n(A100)\ndelay\n0.69\n0.48\n0.36\n4.91\n1.07\nflat\n0.42\n0.47\n0.37\n5.25\n4.69\nstack\n0.38\n0.48\n0.37\n5.16\n4.77\nstack-delay\n0.48\n0.48\n0.37\n4.88\n1.13\nTable 2. Quality/efficiency trade off of the proposed token\nsequence patterns for 30 seconds generated tracks.\ndecoding schedule G(i, t)\nFAD\nKLD\nCLAP\nt + i (delay)\n0.45\n0.50\n0.38\nt + i (stack-delay)\n0.43\n0.51\n0.37\nt + (t mod 3) \u00d7 1 + i\n0.42\n0.50\n0.37\nt + (t mod 4) \u00d7 2 + i\n0.36\n0.51\n0.38\nt + (t mod 5) \u00d7 3 + i\n0.34\n0.52\n0.38\nTable 3. Ablation study on the effect of permuting timesteps\nin the decoding schedule of the stack-delay pattern, for 10s\nsamples on the in-domain evaluation dataset.\n3.4. Evaluation\nThe different models are evaluated through a set of generated\nsamples from a list of evaluation text-prompts. For objective\nevaluation we compute Frechet Audio Distance (FAD) using\nVGG classifier [18], Kullback\u2013Leibler divergence (KLD) us-\ning PaSST model [19], and CLAP similarity score [20]. For\nsubjective evaluation we run a blind pairwise comparison test\nwhere we present the evaluator two samples generated by two\nmodels but using the same text prompt, for a list of 20 text\nprompts. The human evaluators are asked to select the pre-\nferred sample from each pair based on perceived quality. Fi-\nnally we report the RTF computed on A100 GPU when gen-\nerating one sample (effective batch size of 2 from the model\nperspective due to classifier free guidance).\n4. RESULTS\n4.1. Baselines - flat and delay patterns\nWe consider two baselines: flat, which is known to produce\nthe highest quality audio although requiring much more com-\npute than delay, and delay, a good compromise between speed\nand performance, achieving a RTF close to 1, potentially un-\nlocking streaming scenarios. flat achieves an in-domain FAD\nof 0.42, 39% lower than delay, while KLD and CLAP remain\nclose. Despite its higher quality the RTF is above 4.\n4.2. Stack pattern\nWe first investigate the stack pattern as a replacement for the\n(so far) state-of-the-art flat. Our results indicate that it is com-\npetitive with flat, even outperforming its FAD score with 0.38,\nwith a similar RTF. The better FAD score indicates that the\nshorter required context length for generation might have a\npositive effect on music quality for long samples generations.\n4.3. Stack-delay pattern\nWhen considering the stack-delay pattern, our results indicate\nthat it outperforms delay with a FAD of 0.48, although not as\nlow at stack, but much more efficient with almost the same\nRTF as delay, unlocking potential real time streaming scenar-\nios with better quality than the baseline. For subjective eval-\nuation we only compare the stack-delay and delay versions.\nOur results indicate that samples generated by the stack-delay\nare preferred 51.3% of the time compared with delay. Such a\nsmall difference is to be expected given the small scale of our\nsubjective evaluation.\n4.4. Ablation - permuting decoded time steps\nFinally, we look into the interleaved time steps decoding\nschedules defined in section 2.2.4. The ablation results are\npresented in Table 3 that compares four different schedules\napplied with the stack-delay pattern, and also including the\ndelay baseline.\nThe table shows that the higher the decoding step count\nseparating adjacent positions, the lower the FAD, with KLD\nand CLAP scores in a similar range. This shows the ben-\nefit of permuting the time steps in the stack-delay pattern.\nWithout permutation (i.e. following the same ascending time\nsteps schedule as delay), the stack-delay pattern only achieves\nmarginal improvement. We also tried applying the delay pat-\ntern with the same permuted schedules and the performance\nwas only on par with the baseline, which means that the\ncombination of the proposed pattern and permuted decoding\nschedule is essential for better performance.\n5. CONCLUSION\nWe introduce a new codebook pattern that relies on stack-\ning the discrete music tokens, delaying/shifting the decoding\nof subsequent levels, and permuting the order of time steps\nto decode in the decoding schedule. The combination of the\nthree outperforms the delay baseline quality-wise with a in-\ndomain FAD reduction of 45% for the same inference effi-\nciency budget, due to parallel decoding that compensates for\nan increased sequence length. We also show that stacking\nthe tokens should be preferred to flattening them best when\nthe highest quality is a priority. Finally the ablation study\nshows that time step permutation is key to achieve optimal\nperformance, indicating that decoding of adjacent positions\nwith only partial knowledge of previous time steps probably\naffects the performance of the delay pattern. Overall we hope\nour findings can help design better non-autoregressive decod-\ning strategies in the future.\n6. REFERENCES\n[1] Flavio Schneider, Zhijing Jin, and Bernhard Sch\u00a8olkopf,\n\u201cMo\\\u02c6 usai: Text-to-music generation with long-context\nlatent diffusion,\u201d\narXiv preprint arXiv:2301.11757,\n2023.\n[2] Qingqing Huang, Daniel S Park, Tao Wang, Timo I\nDenk, Andy Ly, Nanxin Chen, Zhengdong Zhang,\nZhishuai Zhang, Jiahui Yu, Christian Frank, et al.,\n\u201cNoise2music: Text-conditioned music generation with\ndiffusion models,\u201d\narXiv preprint arXiv:2302.03917,\n2023.\n[3] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan\nFeng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma,\nXuchen Song, et al.,\n\u201cEfficient neural music genera-\ntion,\u201d arXiv preprint arXiv:2305.15719, 2023.\n[4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David\nKant, Gabriel Synnaeve, Yossi Adi, and Alexandre\nD\u00b4efossez, \u201cSimple and controllable music generation,\u201d\narXiv preprint arXiv:2306.05284, 2023.\n[5] Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen\nWang, and Alex Wang, \u201cJen-1: Text-guided universal\nmusic generation with omnidirectional diffusion mod-\nels,\u201d arXiv preprint arXiv:2308.04729, 2023.\n[6] Hugo Flores Garcia, Prem Seetharaman, Rithesh Ku-\nmar, and Bryan Pardo,\n\u201cVampnet: Music generation\nvia masked acoustic token modeling,\u201d arXiv preprint\narXiv:2307.04686, 2023.\n[7] Prafulla Dhariwal and Alexander Nichol,\n\u201cDiffusion\nmodels beat gans on image synthesis,\u201d Advances in neu-\nral information processing systems, vol. 34, pp. 8780\u2013\n8794, 2021.\n[8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and\nWilliam T Freeman, \u201cMaskgit: Masked generative im-\nage transformer,\u201d in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\n2022, pp. 11315\u201311325.\n[9] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al., \u201cLanguage models\nare unsupervised multitask learners,\u201d OpenAI blog, vol.\n1, no. 8, pp. 9, 2019.\n[10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al., \u201cLlama: Open and efficient foundation lan-\nguage models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n[11] Alexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and\nYossi Adi,\n\u201cHigh fidelity neural audio compression,\u201d\narXiv preprint arXiv:2210.13438, 2022.\n[12] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan\nSkoglund, and Marco Tagliasacchi, \u201cSoundstream: An\nend-to-end neural audio codec,\u201d IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing, vol.\n30, pp. 495\u2013507, 2021.\n[13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu, \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d\nThe\nJournal of Machine Learning Research, vol. 21, no. 1,\npp. 5485\u20135551, 2020.\n[14] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, et al.,\n\u201cNeural codec lan-\nguage models are zero-shot text to speech synthesizers,\u201d\narXiv preprint arXiv:2301.02111, 2023.\n[15] Zal\u00b4an Borsos, Matt Sharifi, Damien Vincent, Eugene\nKharitonov, Neil Zeghidour, and Marco Tagliasac-\nchi, \u201cSoundstorm: Efficient parallel audio generation,\u201d\narXiv preprint arXiv:2305.09636, 2023.\n[16] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel\nSinger, Alexandre D\u00b4efossez, Jade Copet, Devi Parikh,\nYaniv Taigman, and Yossi Adi, \u201cAudiogen: Textually\nguided audio generation,\u201d in The Eleventh International\nConference on Learning Representations, 2022.\n[17] Andrea Agostinelli, Timo I Denk, Zal\u00b4an Borsos, Jesse\nEngel, Mauro Verzetti, Antoine Caillon, Qingqing\nHuang, Aren Jansen, Adam Roberts, Marco Tagliasac-\nchi, et al.,\n\u201cMusiclm: Generating music from text,\u201d\narXiv preprint arXiv:2301.11325, 2023.\n[18] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis,\nJort F Gemmeke, Aren Jansen, R Channing Moore,\nManoj Plakal, Devin Platt, Rif A Saurous, Bryan Sey-\nbold, et al.,\n\u201cCnn architectures for large-scale audio\nclassification,\u201d in 2017 ieee international conference on\nacoustics, speech and signal processing (icassp). IEEE,\n2017, pp. 131\u2013135.\n[19] Khaled Koutini, Jan Schl\u00a8uter, Hamid Eghbal-Zadeh, and\nGerhard Widmer, \u201cEfficient training of audio transform-\ners with patchout,\u201d arXiv preprint arXiv:2110.05069,\n2021.\n[20] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-\nmail, and Huaming Wang, \u201cClap learning audio con-\ncepts from natural language supervision,\u201d in ICASSP\n2023-2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE,\n2023, pp. 1\u20135.\n"
  },
  {
    "title": "S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs",
    "link": "https://arxiv.org/pdf/2309.08827.pdf",
    "upvote": "3",
    "text": "S3-DST: Structured Open-Domain Dialogue Segmentation\nand State Tracking in the Era of LLMs\nSarkar Snigdha Sarathi Das1,\u2020,\u2021, Chirag Shah2,\u2021, Mengting Wan3, Jennifer Neville3,\nLongqi Yang3, Reid Andersen3, Georg Buscher3, Tara Safavi3,\u2020\n1Pennsylvania State University, 2University of Washington, 3Microsoft\n\u2020Corresponding authors: sfd5525@psu.edu, tarasafavi@microsoft.com\n\u2021Work done at Microsoft, USA\nAbstract\nThe traditional Dialogue State Tracking (DST)\nproblem aims to track user preferences and in-\ntents in user-agent conversations. While suffi-\ncient for task-oriented dialogue systems sup-\nporting narrow domain applications, the ad-\nvent of Large Language Model (LLM)-based\nchat systems has introduced many real-world\nintricacies in open-domain dialogues. These\nintricacies manifest in the form of increased\ncomplexity in contextual interactions, extended\ndialogue sessions encompassing a diverse ar-\nray of topics, and more frequent contextual\nshifts. To handle these intricacies arising from\nevolving LLM-based chat systems, we propose\njoint dialogue segmentation and state tracking\nper segment in open-domain dialogue systems.\nAssuming a zero-shot setting appropriate to\na true open-domain dialogue system, we pro-\npose S3-DST, a structured prompting technique\nthat harnesses Pre-Analytical Recollection, a\nnovel grounding mechanism we designed for\nimproving long context tracking. To demon-\nstrate the efficacy of our proposed approach\nin joint segmentation and state tracking, we\nevaluate S3-DST on a proprietary anonymized\nopen-domain dialogue dataset, as well as pub-\nlicly available DST and segmentation datasets.\nAcross all datasets and settings, S3-DST consis-\ntently outperforms the state-of-the-art, demon-\nstrating its potency and robustness the next gen-\neration of LLM-based chat systems.\n1\nIntroduction\nThe advent of open-domain Large Language Model\n(LLM)-based chat systems like ChatGPT and Bing\nChat has ushered in a new age of dialogue systems.\nPreviously, dialogue systems were relatively con-\nstrained in their scope and abilities, typically con-\nfined to either narrow task-oriented conversations\nor social chitchat (Gao et al., 2018). By contrast,\nLLM-based chat systems are remarkable because\nthey can converse fluidly with users over a seem-\ningly infinite range of topics, and can accomplish\nFigure 1: A single intent may span several turns in open-\ndomain conversation, and a single conversation may con-\ntain multiple intents: A synthetic dialogue inspired by\nanonymized Bing Chat logs. Different user intents (cre-\nating an annotated bibliography, social chitchat, check-\ning the weather) are highlighted by different colors.\nmany user tasks out-of-the-box that previously re-\nquired specialized systems, like code generation,\nquestion answering, and more.\nIn this paper, we argue that because LLM-based\nchat systems have significantly changed the land-\nscape of human-AI dialogue, understanding user\nintent in such dialogues calls for new analysis and\ntagging frameworks. We focus in particular on\nthe task of dialogue state tracking (DST). Tradi-\ntional DST consists of extracting and matching\nusers\u2019 intents in task-oriented dialogue systems to\na structured backend schema (Williams et al., 2016;\nBudzianowski et al., 2018). However, DST in open-\ndomain conversation is yet undefined; as such, in\nthis paper we make a first attempt at identifying the\nstate values of interest in LLM-based chat systems.\nAs exemplified by Figure 1, we make the key\nobservation that real open-domain dialogue often\nexhibits extensive back-and-forth between parties\n(e.g., clarification, negotiation, etc) in order to pur-\nsue a single intent or topic, and contexts may shift\nmultiple times within a single dialogue among un-\narXiv:2309.08827v1  [cs.CL]  16 Sep 2023\nrelated intents and/or topics. Based on this obser-\nvation, we propose to track both segments and\nstates in open-domain dialogue: Segmentation\nhelps us identify boundaries that mark the start and\nend of contextually cohesive conversation \u201cunits,\u201d\nwhereas states are the intent variables of interest\nwe wish to track, applied per segment.\nBeyond bringing DST into the era of open-\ndomain conversation and LLMs, we introduce\nLLM-based solutions for open-domain DST. As-\nsuming a zero-shot setting for dialogue tagging,\nwhich is realistic due to the cost of labeling, we in-\ntroduce S3-DST, a structured prompting approach\nfor open-domain DST. Within S3-DST we propose\na novel Pre-Analytical Recollection (PAR) prompt-\ning strategy that grounds each output state predic-\ntion on the content of the corresponding dialogue\nturn, thereby helping the LLM track long dialogue\ncontext without forgetting or hallucination.\nWe evaluate S3-DST on a fully anonymized\nopen-domain dialogue dataset collected from Mi-\ncrosoft\u2019s Bing Chat system, alongside public DST\nand segmentation benchmarks.1 S3-DST achieves\nlarge gains over comparable baselines across all\nbenchmarks, suggesting its suitability as a starting\npoint for further research in open-domain dialogue\nmodeling. In summary, our contributions are:\n\u2022 Open-domain DST problem definition: We\nbring dialogue state tracking into the era of\nopen-domain LLM chat. We cast the prob-\nlem as a joint segmentation and state tracking\ntask, motivated by our observations of how\nreal open-domain human-AI conversation is\nconducted on anonymized Bing Chat log data.\n\u2022 Zero-shot S3-DST approach:\nWe pro-\npose S3-DST, a structured zero-shot joint\nsegmentation and state tracking approach for\nopen-domain, multi-intent dialogue. S3-DST\ncontributes new approaches for structured\nprompt templating and dialogue tag genera-\ntion, as well as Pre-Analytical Recollection\n(PAR), a grounding technique that improves\nlong context tracking.\n\u2022 Extensive experiments and analysis: We\nconduct extensive experiments on both pro-\nprietary and public datasets, achieving large\ngains over comparable zero-shot prompts. S3-\nDST achieves state-of-the-art zero-shot per-\n1The use of Bing Chat logs is in compliance with the terms\nformance on the MWOZ 2.1 and 2.4 DST\nbenchmarks, alongside the DialSeg711 dia-\nlogue topic segmentation benchmark.\n2\nProblem Definition\nInformally, the goal of traditional DST is to predict\nthe dialogue state yt given a sequence of user and\nagent utterance turns Ct = [U1, A1, . . . , Ut, At].2\nThe state yt consists of a set of slot-value pairs,\nwhere slots correspond to intent attributes in a\nparticular application domain (e.g., \u201crestaurant-\nname\u201d, \u201chotel-address\u201d) and values correspond to\npredefined categorical options or unconstrained\ntext (Budzianowski et al., 2018).\nHowever, as we have previously discussed, a sin-\ngle open-domain conversation will often consist of\nmultiple potentially unrelated intents across a va-\nriety of topics. Indeed, according to a preliminary\nanalysis on 10K anonymized Bing Chat conversa-\ntions, we estimate that over 50% of conversations\ndisplay multiple user intents and over 90% of con-\nversations contain discussion of multiple topics.\nTherefore, we propose to merge dialogue segmen-\ntation, which aims to find contextually cohesive\n\u201cunits\u201d of dialogue within a larger conversation,\nwith dialogue state tracking. In particular, we per-\nform state tracking at the segment level, where\nthe goal is to label each segment with the slots\nand values of interest, such that multiple segments\nwithin a conversation may have diverging or con-\nflicting state values, reflecting the true variety of\nopen-domain chat.\nIn the rest of this section, we define segmentation\nand state, and finally formalize the joint task.\n2.1\nSegment\nFollowing previous work in dialogue topic segmen-\ntation (Xing and Carenini, 2021; Xia et al., 2022;\nGao et al., 2023), we define dialogue segments as\ncontiguous subsequences of Ct in which all user\nand agent utterances are topically related. Formally,\nlet Bt = [b1, . . . , bt\u22121] indicate the boundary in-\ndices between adjacent user-agent utterance pairs\nin Ct. The output of segmentation is a set of bound-\nary indices Bk \u2286 Bt, where k represents the num-\nber of boundaries determined by the segmentation\nalgorithm and the span [Um, Am, . . . Un, An] repre-\nof use of Bing Chat.\n2Note that in current LLM-based chat systems, users may\nissue multiple utterances before a single agent response is is-\nsued. In these (infrequent) cases, we group all user utterances\nprior to the agent response into a single utterance.\nsents the contiguous segment between boundaries\nbm and bn, where m \u2208 [1, t\u22121] and n \u2208 [m, t\u22121].\n2.2\nSegment state\nTypically, dialogue state tracking methods extract\nnew elements of state at each turn (Hu et al., 2022).\nHowever, this is because DST evaluation bench-\nmarks make the relatively narrow assumption that\nusers provide new and relevant elements of intent\nat each turn, and that intents build upon or comple-\nment each other but do not fundamentally change\nor conflict throughout the conversation. As we\nhave previously discussed, open-domain dialogue\nexhibits far more varied characteristics, and multi-\nintent and/or multi-domain conversations are rela-\ntively common.\nWe therefore propose to extract state at the\nsegment rather than turn level.\nWe define the\nsegment-level state as {Sm:n = (s(i)\nm:n, v(i)\nm:n), i =\n1 . . . Nm:n}, where s(i)\nm:n refers to the i-th slot ap-\nplied to the segment from boundaries bm to bn,\nv(i)\nm:n refers to the slot\u2019s corresponding value, and\nNm:n refers to the total number of slots to applied\nto this segment. Any schema of slot-value pairs is\nvalid here; we describe our particular state schema\nfor Bing Chat in \u00a7 4.1 and Appendix B.\n2.3\nProblem statement\nHaving defined segments and per-segment state,\nwe are equipped to state our full definition of open-\ndomain DST. Given a sequence of user-agent ut-\nterance pairs Ct = [U1, A1, . . . , Ut, At], we define\nthe goal of open-domain dialogue state tracking as\njointly predicting\nyt = Bk \u222a {Sm:n ; \u2200(bm, bn) \u2208 Bk},\n(1)\nwhere Bk \u2286 Bt refers to the segment boundary\nindices described earlier and Sm:n refers to the\nsegment state between boundaries bm and bn, con-\nsisting of N arbitrary slot-value pairs:\nSm:n = {(s(i)\nm:n, v(i)\nm:n), i = 1 . . . Nm:n}.\n(2)\n3\nPrompting Strategies\nAs discussed previously, real-world dialogues of-\nten exhibit extensive discourse that extends over\nmultiple conversational turns in order to discuss\ndiverse topics. This prolonged conversational na-\nture makes it highly challenging to track contex-\ntual coherence. Previous studies (Hu et al., 2022)\naimed at disassociating individual dialogue turns\nand processing them one by one for tracking dia-\nlogue state changes, which worked reasonably well\nin task-oriented dialogues confined within prede-\nfined narrow domains.\nHowever, real-world dialogues commonly re-\nquire multiple turns to adequately comprehend the\ncontextual nuances, which is a challenge because\nTransformers still struggle when processing lengthy\ninput contexts, particularly in the middle (Liu et al.,\n2023). To address these difficulties, we propose a\nnovel turn-by-turn prompting technique that gives\nstructure to inputs and outputs while accurately pre-\nserving the context in the process. We discuss these\ndesign aspects of our prompts below:\n3.1\nStructured Outputs and Inputs\nStructured Output\nOur goal is a set of labels per\ndialogue turn representing the segment boundaries\n(binary labels) and state values (categorical labels\nor open text). To provide a flexible yet structured\nformat to the LLM\u2019s output, we propose to instruct\nit to generate outputs in a hierarchical XML format.\nWe see XML as advantageous because it provides\ncode-like structure to the DST task, which has been\nshown to greatly improve performance compared\nto plain-text outputs, while still being extensible\nand flexible compared to more rigid output formats\nlike SQL (Hu et al., 2022).\nOur\napproach\nuses\nan\nXML\nformat\nin\nwhich each turn from 1 to t comprises an\nXML tree <T{id}>...</T{id}> and several\nnested XML tags within it.\nThe labels of\nthese nested tags (e.g.\n<preceding_topi-\ncal_relation>...</preceding_topical_-\nrelation>,\n<intent>...</intent>,\nand\n<domain>...</domain> in Figure 2(iii)) represent\nthe segment boundaries and slots of interest, and\neach value between opening and closing tags\nrepresent the model\u2019s inferred value.\nThis strategy is beneficial from two fronts: (i)\nDue to bounded well-defined structured formatting,\ngenerated outputs are more likely to be aligned with\nlabeling instructions than free-form texts, and (ii)\nWell-formed structured output formats are easier to\nparse, thus reducing postprocessing requirements.\nStructured Input\nFor prompting LLMs, al-\nthough it is trivial to channel plain conversation\nhistory in a flat format for analysis and inference,\nthe unstructured nature inherent to this linear con-\nfiguration makes it difficult to refer back and lever-\nFigure 2: Prompt flow of S3-DST. Given a raw conversation, (i) we convert it into a hierarchical XML-structured\nrepresentation and insert it into a similarly structured prompt template. We pass the prompt through the LLM and\n(ii) obtain a hierarchical XML-structured output, where each turn contains (iii) a PAR grounding reference to the\nconversation alongside the desired segmentation and state label predictions.\nage different information across multiple conversa-\ntional turns. To handle this challenge, consistent\nwith the output format, we propose a structured in-\nputting format, where each conversational history\nis formed into a hierarchical XML format where\nconversational turns are marked with turn id num-\nber <T{id}>...</T{id}> numbered from 1 to t\nand each conversational turn consists of nested user\nand agent turns marked with appropriate XML tags\n(<user>...</user> and <agent>...</agent>).\nSince we propose instructing the LLM to infer\nper-turn labels during our output, this input scheme\nhelps us accurately refer back to the input turn and\nthus maintain coherence even for long dialogue\ncontexts. Consistent with this XML-tagged input\nformat, we also format all the valid segment and\nstate categories in an XML-formatted list using\nthe following structure: <valid_category_name>\n<item>{label name}</item> <description> {de-\nscription of label, if available} </description>\n<valid_category_name> Empirically, this struc-\ntured input and prompt formatting help constrain\nthe LLM generation to follow the labeling instruc-\ntions. Figure 2(i) shows this format where each\nvalid segment boundary and state category are first\nstaged in an XML-formatted list and subsequently\ninput dialogue is shown in a hierarchical configura-\ntion.\n3.2\nPre-Analytical Recollection (PAR)\nAs previously discussed, open-domain dialogues\nmay be long and highly variable in conversation\nflow. Therefore, it is crucial to ensure that the LLM\ncan accurately monitor the evolving dialogue con-\ntext without forgetting or hallucination. To this end,\nwe propose Pre-Analytical Recollection (PAR), a\ngrounding strategy for turn-by-turn prompting that\ninstructs the LLM to first summarize the turn using\n<summary>...</summary> tags in 3 sentences or\nfewer before providing the segment and state val-\nues. PAR is inspired by chain-of-thought prompt-\ning (Wei et al., 2022), as it is a technique for\ngenerating relevant intermediary outputs in order\nto improve reasoning accuracy. However, unlike\nchain-of-thought, PAR is also a grounding tech-\nnique that provides references from the model\u2019s\noutput directly to the conversation. Figure 2(ii)\ndemonstrates how PAR refers back to the content\nof each conversational turn before analyzing it to\ninfer the conversational states.\n3.3\nFinal Prompt Configuration\nThe final prompt flow of S3-DST is provided in\nFigure 2. Given a raw conversation and a prede-\nfined set of segment and state labels, we insert the\nlabels into a structured prompt template and format\nthe conversation in a hierarchical XML-structured\nrepresentation. We pass the prompt through the\nLLM, instructing it to follow PAR before jointly\nTable 1: Evaluation test set statistics.\n# Convs\n# Turns\n# segments/conv\n(avg.)\nBing Chat\n334\n2308\n1.51\nMWOZ 2.1\n1,000\n7368\n-\nMWOZ 2.4\n1,000\n7368\n-\nDialSeg711\n711\n19350\n3.87\ngenerating the hierarchical turn-by-turn segmenta-\ntion and state labels applied per segment. The full\ntext of our prompt is provided in Appendix A.1.\n4\nExperiments\nWe conduct comprehensive evaluations across mul-\ntiple datasets. We primarily evaluate our approach\non fully anonymized Bing Chat logs annotated by\ndomain experts. Additionally, we evaluate S3-DST\non the standard task-oriented DST and segmenta-\ntion tasks using public benchmark datasets Multi-\nWOZ (Budzianowski et al., 2018) and DialSeg711\n(Xu et al., 2021) respectively. A detailed descrip-\ntion of these datasets is provided below, alongside\ndataset statistics in Table 1:\n4.1\nInternal Human-LLM Dialogue Dataset\nIn order to evaluate the efficacy of our approach\non real-world open-domain human-LLM conversa-\ntions, we collected anonymized chat log data from\nMicrosoft\u2019s Bing Chat system, an LLM chat inter-\nface backed by the Bing search engine.\nBenchmark construction\nWe sample 484 En-\nglish conversations conducted on Bing Chat be-\ntween April 5, 2023 to April 30, 2023 via two\napproaches: (i) Random and (ii) \u201cLong\u201d conver-\nsations of 5 or more turns only. We balance these\ntwo approaches 50/50. Since we operate under a\nzero-shot assumption, we do not need any training\ndata. Therefore, we hold out 150 conversations for\ndevelopment and the remaining 334 for testing.\nAnnotation\nTo obtain ground-truth labels for\nevaluation, we gathered human annotations for seg-\nment and state. We recruited three in-house anno-\ntators with a high degree of technical expertise and\nfamiliarity with the Bing Chat system.\nFor each turn, we instructed annotators to pro-\nvide binary IsSegmentBoundary labels, categor-\nical SegmentIntent labels, and categorical Seg-\nmentDomain labels. We instructed annotators to\nmark a segment boundary when no topical relation\nbetween a turn and its preceding context could be\nidentified. For intent and domain, we used tax-\nonomies developed in-house for the Bing Chat sys-\ntem consisting of 4 intents (Information Seeking,\nAnalysis, Creation, and Open-Ended Discovery)\nand 49 domains (see Appendix B.1 for the full\nlist). Because of the large number of domains,\nper turn we provided annotators four candidate do-\nmain values and an \u201cOther\u201d option. Appendix B\nprovides further details on the annotation scheme\nand domain sampling procedure. To ensure inter-\nannotator agreement before labeling the full dataset,\nwe first gathered annotations on a set of 10 ran-\ndomly selected conversations (68 turns total) and\ncomputed Fleiss\u2019 kappa (Fleiss, 1971) per label\ntype. We observed a Fleiss kappa of \u03ba = 0.83 for\nIsSegmentBoundary, \u03ba = 0.74 for SegmentIn-\ntent, and \u03ba = 0.88 for SegmentDomain, all of\nwhich are considered high agreement on the Fleiss\nkappa scale.\n4.2\nPublic Benchmarks\nWe are not aware of any existing public dialogue\nbenchmarks reflective of the broadly open-domain\nBing Chat data. Therefore, we resort to separate\nDST and segmentation evaluations on public bench-\nmarks using three datasets.\nMultiWOZ\nThe MultiWOZ (MWOZ) multi-\ndomain dialogue dataset (Budzianowski et al.,\n2018) is currently the most common DST bench-\nmark. MWOZ is a task-oriented dataset consisting\nof 1K test dialogues. We use two updated versions\nof the original: MWOZ 2.1 (Eric et al., 2019) and\n2.4 (Ye et al., 2021). The latter is considered the\n\u201ccleanest\u201d version of MWOZ, while the former has\nbeen used more frequently in the literature.\nDialSeg711\nThe DialSeg711 benchmark was in-\ntroduced by (Xu et al., 2021) and has been used fre-\nquently in recent dialogue segmentation research.\nIt is an English dataset in which 711 multi-segment\ndialogues are constructed by joining dialogues\nfrom existing task-oriented dialogue corpora.\n4.3\nBaselines\nAs baselines we consider zero-shot LLM prompts\nonly, for a fair comparison to S3-DST. We discuss\nthe baselines and their considerations below for dif-\nferent datasets. All original prompts are provided\nin Appendix A. We set a maximum of 1500 output\ntokens per LLM call with a temperature of zero.\nTable 2: S3-DST achieves state-of-the-art performance on state tracking over our internal Bing Chat benchmark.\nAll prompts are run with GPT4.\nIndividual accuracy\nJGA\nSegment\nIntent\nDomain\nI/D\nS/I/D\nTBT-DST\n-\n0.6707\n0.6221\n0.4169\n-\nIC-DST\n0.8567\n0.7123\n0.6049\n0.4610\n0.4387\nS3-DST (No PAR)\n0.8859\n0.7173\n0.6251\n0.4377\n0.4078\nS3-DST (Unstructured input)\n0.8810\n0.7163\n0.6307\n0.4640\n0.4331\nS3-DST\n0.8992\n0.7366\n0.6429\n0.4752\n0.4504\nBing Chat\nIn this dataset, we consider IC-DST\nas our primary baseline, which is a zero-shot ver-\nsion of the prompting strategy introduced by (Hu\net al., 2022), heavily adapted for open-domain dia-\nlogue setting to jointly track segment and dialogue\nstates. The TBT-DST baseline is a version of S3-\nDST that does not include segmentation instruc-\ntions and obtains intent and domain labels on a\nturn-by-turn basis using our S3-DST prompt con-\nfiguration. Moreover, to analyze the importance\nof two key aspects of our prompt, PAR and XML-\nstructured formatting, we also consider two ab-\nlations of S3-DST: No PAR refers to a S3-DST\nprompt without the PAR instructions, and Unstruc-\ntured input refers to a S3-DST prompt that formats\nall instructions and dialogue using plain text rather\nthan XML. We use GPT4 as the backbone LLM\nfor all prompts.\nMWOZ\nFor MWOZ task-oriented dialogue state\ntracking dataset, we compare against IC-DST us-\ning Codex-175B as reported by Hu et al. (2022).\nWe also reevaluate zero-shot IC-DST with GPT-4\nto account for the backbone model improvement in\nbaseline performance. Finally, we compare against\nthe zero-shot ChatGPT performance on MWOZ\n2.1 as reported by (Heck et al., 2023).\nDialSeg711\nWe consider the unsupervised Text-\nTiling (Hearst, 1997), CSM (Xing and Carenini,\n2021), and DialStart (Gao et al., 2023) methods.\nWe reprint all numbers from (Gao et al., 2023). Fi-\nnally, we use our IC-DST baseline prompted to\nelicit segmentation labels in the same SQL output\nformat as the original IC-DST (Hu et al., 2022).\n4.4\nMetrics\nFor state tracking, we consider Joint Goal Accu-\nracy (JGA), which measures the proportion of\nturns for which all state values are correctly in-\nferred. For Bing Chat, we report JGA with just\nFigure 3: S3-DST outperforms baselines for dialogues\nof all lengths by emphasizing context tracking. We bin\nBing Chat dialogues by length and plot JGA per bin.\nThe large performance degradation of both baselines as\nthe dialogue length increases confirms the importance\nof our PAR grounding strategy.\nTable 3: S3-DST achieves state-of-the-art JGA com-\npared to zero-shot LLM baselines on the public dialogue\nstate tracking benchmarks MWoZ 2.1 + 2.4.\nJGA\nMWOZ 2.1\nMWOZ 2.4\nIC-DST (Codex)\n0.3534\n0.3530\nIC-DST (GPT4)\n0.4045\n0.4625\nChatGPT\n0.3150\n-\nS3-DST\n0.4513\n0.5327\nintent and domain (I/D) as these are the true state\nvalues of interest, as well as JGA with segment, in-\ntent, and domain accuracy (S/I/D) for completeness.\nWe also report segmentation, intent, and domain ac-\ncuracy separately on Bing Chat to provide a sense\nof the current capabilities and limitations of LLMs\non open-domain conversational data. For segmen-\ntation, we consider PK and WindowDiff (Pevzner\nand Hearst, 2002), which are both error metrics\n(i.e., lower is better) that quantify the difference be-\ntween predicted and ground-truth segment bound-\naries using an adjustable sliding window.\nTable 4: Zero-shot per-domain comparison (JGA) on\nMWOZ 2.1.\nPer-domain JGA\nattr.\nhotel\nrest.\ntaxi\ntrain\nIC-DST (Codex)\n0.5997\n0.4669\n0.5728\n0.7135\n0.4937\nIC-DST (GPT4)\n0.7177\n0.4872\n0.6526\n0.7781\n0.5710\nChatGPT\n0.5270\n0.4200\n0.5580\n0.7090\n0.6080\nS3-DST\n0.6781\n0.5215\n0.6713\n0.8258\n0.7027\n4.5\nResults\nBing Chat\nAs shown in Table 2, our S3-DST\nprompt achieves the highest performance across\nintent, domain, and JGA across turns. We make\nthe following observations: First, TBT-DST, which\ndoes not explicitly perform segmentation, is by far\nour weakest baseline. We find that this is because\nwithout instructing the LLM to use the same intent\nand domain within a segment, the LLM tends to\noverindex on the content of the turn without con-\nsidering the fuller preceding context. This leads to\nconflicting intent and domain labels between turns\nwithin a coherent single-topic dialogue.\nSecond, our adapted version of IC-DST is a very\nstrong baseline. However, while IC-DST makes\nuse of structured outputs, it does not have a corre-\nsponding structured input representation. We find\nthat this hurts its performance in some cases, as hal-\nlucination of nonexistent turns is relatively more\ncommon compared to S3-DST.\nFinally, the two ablations of S3-DST both un-\nderperform compared to S3-DST, confirming the\nimportance of PAR and structured inputs that the\nLLM can refer back to during generation. Indeed,\nFigure 3, which plots the relationship between dia-\nlogue length and performance, shows that S3-DST\navoids the steep degradation in performance of the\nno-PAR ablation as the dialogues get longer. For\nexample, the no-PAR ablation performs compara-\nbly to S3-DST on conversations of 3 turns or fewer,\nbut drops over 10 points JGA for conversations of 4\nturns or more. These results in particular highlight\nthe necessity of PAR for long dialogues.\nMWOZ\nTables 3 and 4 provide MWOZ numbers\nin total and per-domain. S3-DST achieves state-of-\nthe-art zero-shot JGA compared to strong LLMs by\na large margin. Even our strongest zero-shot base-\nline, IC-DST (GPT4), has an absolute performance\ngap of nearly 5 points JGA on MWOZ 2.1 and 7\npoints on MWOZ 2.4. In nearly all individual do-\nmains, S3-DST outperforms IC-DST (GPT4), and\nTable 5: S3-DST achieves state-of-the-art performance\non the public segmentation benchmark DialSeg711.\nPk(\u2193)\nWindowDiff (\u2193)\nTextTiling\n0.4044\n0.4463\nCSM\n0.2430\n0.2635\nDialSTART\n0.1786\n0.1980\nIC-DST\n0.2889\n0.2419\nS3-DST\n0.0091\n0.0081\nsome by a large margin, for example over 13 points\nJGA improvement on the train domain.\nDialSeg711\nFinally, Table 5 shows performance\non DialSeg711. S3-DST achieves nearly zero error\non this dataset, which we find unsurprising given\nthat the dataset\u2019s construction. Specifically, Di-\nalSeg711 is constructed by joining dialogues about\nvery different topics, which leads to very artificial\nand abrupt context shifts between segments. How-\never, we find that our IC-DST prompting baseline\nleads to much higher error than S3-DST. On fur-\nther inspection, we find that the LLM fails to track\nthe dialogue context for several conversations in\nthe dataset, leading to forgetting of the original\nconversation context. These results highlight the\nimportance of PAR and dialogue context tracking\nfor successful segmentation. S3-DST\u2019s strong per-\nformance also suggests that DialSeg711 may not\nbe a difficult enough task in future for LLMs, and\nfurther motivates the need for joint segmentation\nand state tracking, as the goal of segmentation is\nultimately to improve state tracking performance.\n5\nRelated Work\n5.1\nDialogue State Tracking\nTo accurately track the passage of Human-AI con-\nversation, robust state tracking is crucial toward\ninferring user intentions and goals. Since the in-\ntroduction of the MultiWOZ (Budzianowski et al.,\n2018) dataset to the community, a plethora of tech-\nniques have been proposed to improve DST per-\nformance. Earlier attempts including copy mech-\nanism (Lei et al., 2018), transfer learning (Wu\net al., 2019), data augmentation (Zhang et al.,\n2020), contrastive pretraining (Wu et al., 2020),\netc. have yielded improvements in supervised fine-\ntuning scenarios; meanwhile, MultiWOZ also went\nthrough several annotation revisions (Eric et al.,\n2019; Ye et al., 2021; Zang et al., 2020; Han et al.,\n2020). While other techniques (Peng et al., 2021;\nLin et al., 2020; Zhao et al., 2022; Yu et al., 2020;\nPlatanios et al., 2021) have also been proposed,\nthe resource-intensive and laborious nature of data\nlabeling has gradually redirected attention toward\nthe exploration of few- and zero-shot dialogue state\ntracking (Shin et al., 2022; Hu et al., 2022; Heck\net al., 2023). While the state-of-the-art approach\nin this discipline (Hu et al., 2022) can leverage\nLLMs for tracking states, it notably lacks proper\ngrounding mechanisms which can potentially hurt\nperformance in real-world extended dialogue ses-\nsions. Furthermore, none of the aforementioned\nprevious work accounts for topic coherence and\ncontext switches prevalent in flexible open-domain\nLLM-based chat systems.\n5.2\nDialogue Topic Segmentation\nSegmenting a dialogue into topically coherent units\nis foundational to successful downstream dialogue\nmodeling. While the paucity of annotated data\nhas been a challenge in dialogue topic segmenta-\ntion, recent unsupervised attempts have exhibited\nsome promising outcomes in topic segmentation.\nMore specifically, extensions based on the classi-\ncal text segmentation algorithm TextTiling (Hearst,\n1997) have primarily led the benchmark in this\naspect (Song et al., 2016). More recently, text-\npair coherence scoring (Xing and Carenini, 2021)\nand topic-aware representation learning (Gao et al.,\n2023) have advanced the state of the art. Neverthe-\nless, all of these techniques fall short in accounting\nfor the complete contextual essence of a conver-\nsation (i.e., explicitly modeling intent and other\nimportant state variables), which can lead to sub-\noptimal results.\n5.3\nIntent Classification\nRelated to dialogue state tracking, another funda-\nmental problem in task-oriented dialogue systems\nis intent classification (IC). Often paired with an-\nother complementary problem slot-filling (SF), re-\nsearchers have proposed a wide range of techniques\nover the years (Liu and Lane, 2016; Zhang and\nWang, 2016; Goo et al., 2018; Qin et al., 2019,\n2021), achieving impressive performance in popu-\nlar public datasets. Few-shot techniques have also\nbeen investigated in data-constrained scenarios for\njoint IC/SF task (Krone et al., 2020; Bhathiya and\nThayasivam, 2020; Liu et al., 2021). While re-\nlated to DST, IC/SF primarily deals with individ-\nual utterances in isolation, which makes it less apt\nfor real-world human-AI dialogue which often re-\nquires modeling intricate contextual connections\nspanning multiple utterances within a conversa-\ntional session.\n6\nDiscussion and Conclusion\nLLM-based chat systems have broadened the hori-\nzons of human-AI conversation, warranting new\nmethods for tracking user intentions. Therefore,\nwe bring dialogue state tracking in the realm of\nopen-domain dialogue systems by jointly track-\ning topically coherent segments and state intent\nvariables per segment. Since this requires the as-\nsumption of a zero-shot setting due to the impracti-\ncality of annotation across all disciplines, we pro-\npose S3-DST, a structured segmentation and state\ntracking approach using zero-shot prompting for\nopen-domain state tracking. S3-DST structures\nthe prompt in an XML format and leverages our\nproposed grounding mechanism (PAR) for long\ncontext tracking. Across extensive experiments\non proprietary and public datasets, S3-DST shows\nlarge performance gains over state-of-the-art zero-\nshot techniques in dialogue state tracking and seg-\nmentation approaches. In the future, as LLM-based\nchat systems become more prevalent, we expect di-\nalogue systems research to shift further toward un-\nderstanding and modeling open-domain dialogue.\nIn this respect, we aim to further study and develop\ntechniques for extended context preservation, to im-\nprove grounding in DST alongside other important\ndialogue modeling tasks.\nReferences\nHemanthage S Bhathiya and Uthayasanker Thayasivam.\n2020. Meta learning for few-shot joint intent de-\ntection and slot-filling. In Proceedings of the 2020\n5th International Conference on Machine Learning\nTechnologies, pages 86\u201392.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016\u20135026.\nMihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar,\nAbhishek Sethi, Peter Ku, Anuj Kumar Goyal, San-\nchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur.\n2019. Multiwoz 2.1: A consolidated multi-domain\ndialogue dataset with state corrections and state track-\ning baselines. arXiv preprint arXiv:1907.01669.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nHaoyu Gao, Rui Wang, Ting-En Lin, Yuchuan Wu, Min\nYang, Fei Huang, and Yongbin Li. 2023. Unsuper-\nvised dialogue topic segmentation with topic-aware\nutterance representation. In Proceedings of the 46th\nAnnual International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval.\nJianfeng Gao, Michel Galley, and Lihong Li. 2018. Neu-\nral approaches to conversational ai. In The 41st in-\nternational ACM SIGIR conference on research &\ndevelopment in information retrieval, pages 1371\u2013\n1374.\nChih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo,\nTsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung\nChen. 2018. Slot-gated modeling for joint slot filling\nand intent prediction. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 753\u2013757.\nTing Han, Ximing Liu, Ryuichi Takanobu, Yixin\nLian, Chongxuan Huang, Wei Peng, and Minlie\nHuang. 2020. Multiwoz 2.3: A multi-domain task-\noriented dataset enhanced with annotation correc-\ntions and co-reference annotation. arXiv preprint\narXiv:2010.05594.\nMarti A Hearst. 1997. Text tiling: Segmenting text into\nmulti-paragraph subtopic passages. Computational\nlinguistics, 23(1):33\u201364.\nMichael Heck, Nurul Lubis, Benjamin Ruppik, Renato\nVukovic, Shutong Feng, Christian Geishauser, Hsien-\nchin Lin, Carel van Niekerk, and Milica Gasic. 2023.\nChatGPT for zero-shot dialogue state tracking: A\nsolution or an opportunity? In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n936\u2013950, Toronto, Canada. Association for Compu-\ntational Linguistics.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022.\nIn-\ncontext learning for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2627\u20132643, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJason Krone, Yi Zhang, and Mona Diab. 2020. Learning\nto classify intents and slot labels given a handful of\nexamples. arXiv preprint arXiv:2004.10793.\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren,\nXiangnan He, and Dawei Yin. 2018. Sequicity: Sim-\nplifying task-oriented dialogue systems with single\nsequence-to-sequence architectures. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1437\u20131447.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020. Mintl: Minimalist transfer\nlearning for task-oriented dialogue systems. arXiv\npreprint arXiv:2009.12005.\nBing Liu and Ian Lane. 2016. Attention-based recurrent\nneural network models for joint intent detection and\nslot filling. arXiv preprint arXiv:1609.01454.\nHan Liu, Feng Zhang, Xiaotong Zhang, Siyang Zhao,\nand Xianchao Zhang. 2021. An explicit-joint and\nsupervised-contrastive learning framework for few-\nshot intent classification and slot filling.\narXiv\npreprint arXiv:2110.13691.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023.\nLost in the middle:\nHow lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2021. Soloist:\nBuilding task bots at scale with transfer learning and\nmachine teaching. Transactions of the Association\nfor Computational Linguistics, 9:807\u2013824.\nLev Pevzner and Marti A Hearst. 2002.\nA critique\nand improvement of an evaluation metric for text\nsegmentation. Computational Linguistics, 28(1):19\u2013\n36.\nEmmanouil Antonios Platanios, Adam Pauls, Subhro\nRoy, Yuchen Zhang, Alexander Kyte, Alan Guo, Sam\nThomson, Jayant Krishnamurthy, Jason Wolfe, Jacob\nAndreas, and Dan Klein. 2021. Value-agnostic con-\nversational semantic parsing. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3666\u20133681, Online. As-\nsociation for Computational Linguistics.\nLibo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,\nand Ting Liu. 2019. A stack-propagation framework\nwith token-level intent detection for spoken language\nunderstanding. arXiv preprint arXiv:1909.02188.\nLibo Qin, Tailu Liu, Wanxiang Che, Bingbing Kang,\nSendong Zhao, and Ting Liu. 2021. A co-interactive\ntransformer for joint slot filling and intent detection.\nIn ICASSP 2021-2021 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8193\u20138197. IEEE.\nJamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea\nMadotto, and Juneyoung Park. 2022. Dialogue sum-\nmaries as dialogue states (DS2), template-guided\nsummarization for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 3824\u20133846, Dublin,\nIreland. Association for Computational Linguistics.\nYiping Song, Lili Mou, Rui Yan, Li Yi, Zinan Zhu,\nXiaohua Hu, and Ming Zhang. 2016. Dialogue ses-\nsion segmentation by embedding-enhanced texttiling.\narXiv preprint arXiv:1610.03955.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nJason D Williams, Antoine Raux, and Matthew Hender-\nson. 2016. The dialog state tracking challenge series:\nA review. Dialogue & Discourse, 7(3):4\u201333.\nChien-Sheng Wu, Steven C.H. Hoi, Richard Socher,\nand Caiming Xiong. 2020. TOD-BERT: Pre-trained\nnatural language understanding for task-oriented di-\nalogue. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 917\u2013929, Online. Association for\nComputational Linguistics.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl,\nCaiming Xiong, Richard Socher, and Pascale Fung.\n2019. Transferable multi-domain state generator for\ntask-oriented dialogue systems. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 808\u2013819, Florence, Italy.\nAssociation for Computational Linguistics.\nJinxiong Xia, Cao Liu, Jiansong Chen, Yuchen Li, Fan\nYang, Xunliang Cai, Guanglu Wan, and Houfeng\nWang. 2022. Dialogue topic segmentation via paral-\nlel extraction network with neighbor smoothing. In\nProceedings of the 45th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 2126\u20132131.\nLinzi Xing and Giuseppe Carenini. 2021.\nImprov-\ning unsupervised dialogue topic segmentation with\nutterance-pair coherence scoring. In Proceedings\nof the 22nd Annual Meeting of the Special Inter-\nest Group on Discourse and Dialogue, pages 167\u2013\n177, Singapore and Online. Association for Compu-\ntational Linguistics.\nYi Xu, Hai Zhao, and Zhuosheng Zhang. 2021. Topic-\naware multi-turn dialogue modeling. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 14176\u201314184.\nFanghua Ye, Jarana Manotumruksa, and Emine Yilmaz.\n2021. Multiwoz 2.4: A multi-domain task-oriented\ndialogue dataset with essential annotation corrections\nto improve state tracking evaluation. arXiv preprint\narXiv:2104.00773.\nTao Yu, Rui Zhang, Alex Polozov, Christopher Meek,\nand Ahmed Hassan Awadallah. 2020. Score: Pre-\ntraining for context representation in conversational\nsemantic parsing. In International Conference on\nLearning Representations.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. Multiwoz 2.2: A dialogue dataset with addi-\ntional annotation corrections and state tracking base-\nlines. arXiv preprint arXiv:2007.12720.\nXiaodong Zhang and Houfeng Wang. 2016. A joint\nmodel of intent determination and slot filling for spo-\nken language understanding. In IJCAI, volume 16,\npages 2993\u20132999.\nYichi Zhang, Zhijian Ou, and Zhou Yu. 2020. Task-\noriented dialog systems that consider multiple appro-\npriate responses under the same context. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 9604\u20139611.\nJeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu,\nMingqiu Wang, Harrison Lee, Abhinav Rastogi,\nIzhak Shafran, and Yonghui Wu. 2022. Description-\ndriven task-oriented dialog modeling. arXiv preprint\narXiv:2201.08904.\nA\nPrompts\nA.1\nS3-DST prompts\nBing Chat\nBelow is the full prompt for S3-DST,\nwith templated values to be replaced by e.g., in-\ntent label names or descriptions in curly braces.\nAppendix B provides the full list of state values.\n<valid_domains>\n<item>{valid domain label name}</item>\n...\n</valid_domains>\n<valid_preceding_topical_relation>\n<item>\n<name>YES</name>\n<desc>The\ncurrent\nturn\nhas\n**some\nor\nany**\ntopical/subtopical\nrelation\nto\nthe\npreceding\nconversation context.</desc> </item>\n<item>\n<name>NO</name>\n<desc>The\ncurrent\nturn\nhas\n**absolutely\nno**\ntopical/subtopical\nrelation\nto\nthe\npreceding\nconversation\ncontext\nOR\nis\nthe\nfirst\nturn\nin\nthe conversation, marking the beginning of a new\ndialogue segment. </desc>\n</item>\n</valid_preceding_topical_relation>\n<valid_intents>\n<item>\n<name>{valid intent label name}</name>\n<desc>{intent description}</desc>\n</item>\n...\n</valid_intents>\n## TASK ##\nYou are given a dialogue between a user and an\nagent comprised of turns starting with T. For each\nturn you have to answer the following questions.\n- Summarize the turn in <=3 sentences\n-\nOutput\nthe\npreceding_topical_relation\nlabel\nusing\nthe\n<valid_preceding_topical_-\nrelation>...</valid_preceding_topical_relation>\nlist\n-\nOutput\nthe\nintent\nlabel\nfrom\nthe\n<valid_-\nintents>...</valid_intents> list\n-\nOutput\nthe\ndomain\nlabel\nfrom\nthe\n<valid_-\ndomains>...</valid_domains> list\n- When preceding_topical_relation is YES, you must\nuse the exact same intent and domain label for\nall turns in the segment.\n## OUTPUT FORMAT ##\n<T{turn number}>\n<summary>{turn summary in <=3 sentences}</summary>\n<preceding_topical_relation>{valid\npreceding\ntopical\nrelation\nlabel}</preceding_topical_-\nrelation>\n<intent>{valid intent label}</intent>\n<domain>{valid domain label}</domain>\n</T{turn number}>\n## INPUT ##\n{XML-structured dialogue}\n## OUTPUT ##\nFor the \u201cNo PAR\u201d baseline, we remove the turn\nsummarization instruction and summary tag from\nthe prompt. For the \u201cUnstructured input\u201d baseline,\nwe input the conversation as a list of plain-text\nturns numbered from T1 to T{t}. For the TBT-DST\nbaseline, we remove all segmentation instructions\nand labels from the prompt, and simply have the\nmodel output a valid intent and domain per turn.\nFor the DialSeg711 dataset, we remove all in-\nstructions and values related to intent and domain,\nand have the model output turn-level summaries\nand segment labels only.\nMWOZ\nBelow is the S3-DST prompt for the\nMWOZ dataset. Note that all descriptions for slots\nwere generated by GPT4.\n<slots>\n<item>\n<name>taxi-leave at</name>\n<description>the time when the user wants to get\nthe taxi</description>\n</item>\n<item>\n<name>{domain}-{intent}</name>\n<description{description of slot}</description>\n<valid_values>{valid categorical values for slot if applica-\nble, otherwise this tag does not appear}</valid_values>\n</item>\n...\n</slots>\n## TASK ##\nYou are given a dialogue between a user and an\nagent comprised of turns starting with T. For each\nturn you have to answer the following questions.\n- Output the user utterance verbatim.\n- Based on that utterance, extract the relevant\ninformation about user preferences for relevant\nslots\nfrom\n<slots>...</slots>\nand\nrepresent\nthem as a list of tags that follow the format\n[\u2019{SLOT}-{value}\u2019], where value is the specific\ninformation for that SLOT.\n- Remove any duplicates or conflicting pairs from\nthe list.\nIf the same SLOT appears more than\nonce in the list, keep only the most recent or\nrelevant value originated from a user utterance.\n- If the values for the same SLOT contradict each\nother, resolve the conflict by keeping the **most\nrecent** user provided value.\nOutput the final\nlist as the task result.\n-\nExample\noutput\nfor\n[\u2019{SLOT}-{value}\u2019].\nFor\nexample,\nthe\noutput\nmay\nlook\nlike\n[\u2019hotel-book\nday-monday\u2019,\n\u2019hotel-book\nnumber_-\nof_people-3\u2019,\n\u2019hotel-book\nnumber_of_days-4\u2019,\n\u2019hotel-name-wartworth\u2019,\n\u2019hotel-area-east\u2019,\n\u2019hotel-parking-yes\u2019,\n\u2019hotel-stars-4\u2019,\n\u2019hotel-internet-yes\u2019,\n\u2019train-book\nnumber_of_-\npeople-1\u2019, \u2019train-destination-bishops stortford\u2019,\n\u2019train-day-friday\u2019, \u2019train-arrive_by_time-19:45\u2019,\n\u2019train-departure-cambridge\u2019]\n-\nMake\nsure\nselected\nslots\nare\nonly\nfrom\npredefined <slots>...</slots> list.\nIf <valid_-\nvalues>...</valid_values> are mentioned for the\nslot, you must use one of the valid values for\nthat slot.\n-\nUse\ndontcare\nvalues\nonly\nif\nuser\nexplicitly\nmentions it.\nNow\nfor\n**every\nturn**,\nanswer\nthe\nfollowing\nquestions:\n<T{turn number}>\n<agent_context> {verbatim last agent utterance}\n</agent_context>\n<user_utterance> {verbatim user utterance of the\nturn} </user_utterance>\n<updated_slot_value>\nupdated\nlist\nof\n[\u2019{SLOT}-{value}\u2019]\ntaking\nslots\nfrom\n<slots>...</slots>\nand\nusing\n<valid_-\nvalues>...</valid_values>\nfor\nappropriate\nslots </updated_slot_value> </T{turn number}>\n##INPUT##\n{XML-structured dialogue}\n##OUTPUT##\nA.2\nIC-DST prompt\nBelow is the IC-DST prompt adapted to the Bing\nChat dataset. Note that for the DialSeg711 dataset,\nwe simply remove the domain and intent columns\nand instructions.\nCREATE TABLE states(\ndomain text CHECK (domain IN ({valid domain names)),\npreceding_topical_relation text CHECK (preceding_-\ntopical_relation IN (YES, NO)),\nintent text CHECK (intent IN ({valid intent names)),\n)\n/*\n## DESCRIPTION OF SELECTED COLUMN-VALUE PAIRS:\n- preceding_topical_relation-NO: The current turn\nhas **absolutely no** topical/subtopical relation\nto the preceding conversation context OR is the\nfirst\nturn\nin\nthe\nconversation,\nmarking\nthe\nbeginning of a new dialogue segment.\n- preceding_topical_relation-YES: The current turn\nhas **some or any** topical/subtopical relation\nto the preceding conversation context.\n- intent-INFORMATION SEEKING: The user wants to\nfind factual information or answers to specific\nquestions.\n{remaining intents and descriptions here}\n*/\n## TASK ##\nUsing\nvalid\nSQLite,\nanswer\nthe\nfollowing\nmulti-turn conversational questions for the table\nprovided above. Use the following steps:\n- For each user-agent turn starting with T, output\nthe answer SQL query.\n- When preceding_topical_relation is YES, you must\nuse the exact same intent and domain label for\nall turns in the segment.\n- Output your answer as a list, with one SQL query\nper turn starting with T.\n## OUTPUT FORMAT ##\nT{turn\nnumber}.\nSELECT\n*\nfrom\nstates\nWHERE\npreceding_topical_relation\n=\n{your\nanswer}\nAND\nintent = {your_answer} AND domain = {your answer};\n## INPUT ##\n{input dialogue}\n## OUTPUT ##\nB\nAnnotation Details\nB.1\nLabels provided to annotators\nBelow, we provide the labels and descriptions, if\navailable, that were given to the Bing Chat dataset\nannotators. For intent and domain, we developed\nthe label names and intent descriptions using an it-\nerative, semi-automated process in which we asked\nGPT4 to summarize a sample of conversation logs,\nextract the key themes, and compare these themes\nto identify the main differences among different\ntypes of intents and domains.\nIsSegmentBoundary\n\u2022 NO: The current turn has no syntactic, seman-\ntic, or topical relation to the preceding con-\nversation context OR is the first turn in the\nconversation.\n\u2022 YES: The current turn has any syntactic, se-\nmantic, or topical relation to the preceding\nconversation context.\nSegmentIntent\n\u2022 INFORMATION SEEKING: The user wants\nto find factual information or answers to spe-\ncific questions.\n\u2022 ANALYSIS: The user asks analytical or con-\nceptual questions about a complex topic or\nproblem. The user\u2019s questions require some\ndegree of reasoning, interpretation, argumen-\ntation, comparison, and/or data processing.\n\u2022 CREATION: The user asks the agent to either\ngenerate original content or translate existing\ncontent into new content based on specified\ncriteria or constraints.\n\u2022 OPEN-ENDED DISCOVERY: The user\nwants to casually chat or play with the agent\nout of curiosity, boredom, or humor, OR the\nuser\u2019s intent is so unclear/underspecified that\nit\u2019s impossible to categorize in any of the other\nintent classes. The user mainly treats the agent\nas a conversation or chitchat partner, and none\nof the other intent categories can be assigned.\nSegmentDomain\n\u2022 AI MACHINE LEARNING AND DATA SCI-\nENCE\n\u2022 ASTROLOGY\n\u2022 BIOLOGY AND LIFE SCIENCE\n\u2022 BUSINESS AND MARKETING\n\u2022 CAREER AND JOB APPLICATION\n\u2022 CLOTHING AND FASHION\n\u2022 COOKING FOOD AND DRINKS\n\u2022 CRAFTS\n\u2022 CULTURE AND HISTORY\n\u2022 CYBERSECURITY\n\u2022 DATING FRIENDSHIPS AND RELATION-\nSHIPS\n\u2022 DESIGN\n\u2022 EDUCATION\n\u2022 ENTERTAINMENT\n\u2022 ENVIRONMENT AGRICULTURE AND\nENERGY\n\u2022 FAMILY PARENTING AND WEDDINGS\n\u2022 FINANCE AND ECONOMICS\n\u2022 GAMES\n\u2022 GEOGRAPHY AND GEOLOGY\n\u2022 HEALTH AND MEDICINE\n\u2022 HOUSING AND HOMES\n\u2022 HUMOR AND SARCASM\n\u2022 LANGUAGE\n\u2022 LAW AND POLITICS\n\u2022 LITERATURE AND POETRY\n\u2022 MANUFACTURING AND MATERIALS\n\u2022 MATH LOGIC AND STATISTICS\n\u2022 MUSIC AND AUDIO\n\u2022 NEWS\n\u2022 PETS AND ANIMALS\n\u2022 PHILOSOPHY\n\u2022 PHYSICS CHEMISTRY AND ASTRON-\nOMY\n\u2022 PRODUCTIVITY\n\u2022 PSYCHOLOGY AND EMOTIONS\n\u2022 RELIGION AND MYTHOLOGY\n\u2022 SHIPPING AND DELIVERY\n\u2022 SHOPPING AND GIFTS\n\u2022 SMALL TALK\n\u2022 SOCIAL MEDIA\n\u2022 SOFTWARE AND WEB DEVELOPMENT\n\u2022 SPORTS AND FITNESS\n\u2022 TAXATION\n\u2022 TECHNOLOGY\n\u2022 TIME AND DATES\n\u2022 TRANSPORTATION AUTOMOTIVE AND\nAEROSPACE\n\u2022 TRAVEL\n\u2022 VISUAL ARTS AND PHOTOGRAPHY\n\u2022 WEATHER\n\u2022 WRITING JOURNALISM AND PUBLISH-\nING\nB.2\nDomain labeling procedure\nDue to the large number of domain values and the\npotential for high disagreement and cognitive over-\nload, we did not ask annotators to choose from the\nfull list of domains per turn. Rather, we provided a\ndropdown list of five options per turn. One option\nwas manually selected by the authors as being cor-\nrect or near-correct. Two options were chosen at\nrandom using Python. One option was \u201cOTHER,\u201d\nin which case the annotator was required to choose\nthe correct domain from the full list of 49 domains\nand explain their choice.\nFinally, the last option was a \u201chard negative\u201d cho-\nsen using the following procedure. First, we man-\nually grouped our domains into eight high-level\nclusters: STEM, arts, social sciences, health, com-\nmerce, professional, personal, and leisure. Then,\ngiven the aforementioned \u201cground-truth\u201d domain\nchosen by the authors, we randomly sampled an-\nother domain from the same high-level cluster as\nthe ground-truth label. For example, if the ground-\ntruth domain was chosen to be \u201cBIOLOGY AND\nLIFE SCIENCE\u201d, we sampled another domain\nfrom the STEM cluster as our final domain can-\ndidate.\n"
  },
  {
    "title": "Augmenting text for spoken language understanding with Large Language Models",
    "link": "https://arxiv.org/pdf/2309.09390.pdf",
    "upvote": "2",
    "text": "AUGMENTING TEXT FOR SPOKEN LANGUAGE UNDERSTANDING WITH LARGE\nLANGUAGE MODELS\nRoshan Sharma1\u2217, Suyoun Kim2, Daniel Lazar2, Trang Le2, Akshat Shrivastava2,\nKwanghoon An2, Piyush Kansal2, Leda Sari2, Ozlem Kalinli2, Michael Seltzer2\n1Carnegie Mellon University, Pittsburgh, USA and 2Meta, Seattle, USA\nABSTRACT\nSpoken semantic parsing (SSP) involves generating machine-\ncomprehensible parses from input speech.\nTraining robust mod-\nels for existing application domains represented in training data or\nextending to new domains requires corresponding triplets of speech-\ntranscript-semantic parse data, which is expensive to obtain. In this\npaper, we address this challenge by examining methods that can use\ntranscript-semantic parse data (unpaired text) without corresponding\nspeech. First, when unpaired text is drawn from existing textual\ncorpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are com-\npared as ways to generate speech representations for unpaired text.\nExperiments on the STOP dataset show that unpaired text from\nexisting and new domains improves performance by 2% and 30%\nin absolute Exact Match (EM) respectively. Second, we consider\nthe setting when unpaired text is not available in existing textual\ncorpora. We propose to prompt Large Language Models (LLMs) to\ngenerate unpaired text for existing and new domains. Experiments\nshow that examples and words that co-occur with intents can be\nused to generate unpaired text with Llama 2.0. Using the generated\ntext with JAT and TTS for spoken semantic parsing improves EM\non STOP by 1.4% and 2.6% absolute for existing and new domains\nrespectively.\nIndex Terms\u2014 spoken language understanding, on-device, un-\npaired data,large language models, prompting\n1. INTRODUCTION\nSpoken Language Understanding (SLU) is essential for many real-\nworld applications today including conversational agents and virtual\nassistants. Spoken Semantic Parsing (SSP) is the SLU task that in-\nvolves transforming a recording to a machine-comprehensible parse\ntree [1]. End-to-end models [2] operate directly on speech while\ncascade models [3] generate a semantic parse based on the transcript.\nTwo-pass deliberation models [4] combine the best of both worlds,\nby using first-pass transcripts and speech embeddings to improve\nspoken semantic parsing. However, training such models with super-\nvision requires matched triplets of speech, transcript, and semantic\nparse. Annotating these triplets is expensive, which limits the size of\ntraining data, and consequently model performance.\nThe need for matched data can be alleviated by developing meth-\nods that can use only text data. Text data (transcript-semantic parse)\nis more easily obtained than speech \u2013 either from existing textual\ncorpora or by prompting Large Language Models (LLMs), and train-\ning models with a small amount of paired speech-text data and a\nlarge amount of unpaired text is useful. It is non-trivial to incor-\nporate text-only data into end-to-end models because model outputs\n\u2217The first author worked on this while at Meta\nFig. 1. This paper: We describe ways to unpaired text to train delib-\neration models, where unpaired data can be obtained from LLMs or\nexisting textual corpora. We use JAT or TTS to obtain speech repre-\nsentations of unpaired data\ncannot be obtained without speech inputs. Prior work has explored\nthe use of text data for speech recognition [5\u20137]. External language\nmodels trained on text can be used to interpolate token prediction\nprobabilities [8], but require additional memory, making them un-\nsuitable for on-device applications. Coordinated learning methods\n[9, 10] project speech and text to a shared embedding space for\nspeech recognition, but such models require significant amounts of\npaired speech-text data to learn robust mappings. The final class of\nwork generates speech representations for unpaired speech - Joint\nAudio Text (JAT) [11] uses mean speech embeddings from paired\ndata to represent unpaired text. This is computationally inexpensive,\nbut the speech embeddings do not contain information embedded in\nreal speech. In contrast, synthetic speech from Text-to-speech (TTS)\nmodels [5] produce informative speech representations, but they can\nbe expensive to compute. There are two cases where additional tex-\ntual data may be acquired for semantic parsing \u2013 (a) to improve mod-\nels on existing domains (ED) and (b) to support new domains (ND).\nIn this paper, we compare JAT and TTS for SSP when unpaired text\ndata is drawn from existing and new domains.\nWhen unpaired text is not available from existing corpora, we\npropose prompting Large Language Models (LLMs)\n[12\u201314] to\ngenerate text data for SSP. LLMs are exceptional at generating re-\nalistic text based on input prompts, and, in this paper, we use LLama\n2.0 [14] to generate text data. For the ED setup, it is sufficient to\ngenerate transcripts since semantic parses can be obtained from tran-\narXiv:2309.09390v1  [cs.CL]  17 Sep 2023\nscripts using pre-trained semantic parsers. We describe two prompt-\ning methods: (a) intent-word-based prompting (IWP), where the\nLLM produces transcripts corresponding to a particular intent class\nand containing words that co-occur with the intent, and (b) exemplar-\nbased prompting (EP), where it generates transcripts that are similar\nto provided examples. We generate pseudo-labels for the generated\nutterances using a pre-trained RoBERTa [15] model and train SSP\nmodels using JAT. We find that EP is simpler but IWP generates the\ndesired intent more often. Using data from both methods improves\nthe Exact Match (EM) on STOP data by 1.4 points absolute.\nFor the ND setup, pre-trained models for pseudo-labeling are\nunavailable for the new domain(s), and hence LLMs are used to gen-\nerate semantic parses directly. The transcript is then inferred from\nthe semantic parse. Exemplar-based prompting (EP) is used with 3\nreal examples for every possible intent-slot combination to generate\nlarge-scale data. We find that the generated data improves EM by\n2.3 points absolute over a baseline that uses only 3 examples per\ncombination.\nIn summary, this paper makes the following contributions:\n1. Extends JAT, previously used for ASR, to end-to-end spoken\nsemantic parsing, and compares JAT with TTS for textual data\nfrom existing domains and new domains.\n2. Develops prompting strategies to generate textual transcripts\nand semantic parses in existing and new domains using\nLLMs.\n3. Demonstrates that LLM-generated textual data can be used in\nconjunction with JAT and TTS to improve spoken semantic\nparsing.\n2. DELIBERATION MODEL FOR SLU\nDeliberation-based SLU models [4, 16] are two-pass models that\npredict an ASR transcript in the first pass. Using the first pass tran-\nscript and audio, it then generates the semantic parse in the second\npass. In contrast to cascade models that utilize separately trained Au-\ntomatic Speech Recognition (ASR) and SLU components, a deliber-\nation model optimizes both ASR and SLU components jointly. To\nachieve on-device streaming functionality, the first pass ASR com-\nponent is implemented using the Recurrent Neural Network Trans-\nducer (RNNT)\n[17\u201319]. To maintain transcription accuracy, the\nASR component of our deliberation model is trained independently\nand kept frozen. Our deliberation-based SLU model comprises two\nprimary modules: (1) Fusion, and (2) Decoder. The fusion mod-\nule combines intermediate audio and text embeddings from the first\npass RNNT encoder and predictor respectively. Using Multi-Head\nAttention [20], the fusion module generates a combined representa-\ntion that is used by the transformer-based decoder module to predict\nthe target semantic parse sequence.\n3. SPEECH REPRESENTATIONS FOR UNPAIRED TEXT\n3.1. Joint Audio-Text Training (JAT)\nJoint Audio-Text training (JAT) [11] is a recent approach for leverag-\ning unpaired text-only data to improve ASR [10, 11, 21, 22]. Unlike\nshallow fusion that considers token distributions from an external\nneural network language model (NNLM), JAT does not require addi-\ntional model parameters or latency, making it suitable for on-device\nstreaming ASR. The core idea behind JAT is that speech represen-\ntations for unpaired text can be generated by simply using average\nspeech embeddings computed over available paired speech/text data.\nIn this paper, we use the JAT approach to train our Spoken Language\nUnderstanding (SLU) models to enable training with both \u201dspeech-\ntext-semantic parse\u201d and \u201dtext-semantic parse\u201d datasets.\n3.2. Speech Synthesis with Voicebox\nVoicebox[23] is a state-of-the-art non-autoregressive speech gener-\nation model based on Flow Matching [24]. We generate represen-\ntations for unpaired text by extracting speech features from synthe-\nsized speech. Synthetic speech can be obtained by using Voicebox\nin TTS mode, i.e. where audio is generated by conditioning on input\ntext. Different from [23], the Voicebox model we use represents\ninput text as graphemes rather than phonemes. To generate audio,\nwe first sample unit durations for each grapheme in the input text\nusing a flow-matching-based duration model and then upsample the\ngrapheme sequence using the unit duration information. This infor-\nmation is used as conditioning to generate the spectrogram using the\naudio model. Finally, we used a HiFi-GAN [25] vocoder to convert\nthe spectrograms into time-domain signals.\n4. GENERATING TEXTUAL DATA WITH LLAMA 2.0\nLLama 2.0 [14] is a public open-source large language model trained\non large volumes of publicly available data and code with context as\nlarge as 4096. In this paper, we use the 13B parameter chat model.\n4.1. Generating Textual Data for Existing Domains\nIn the ED setup, we propose to use LLMs to generate transcripts.\nCorresponding semantic parses are obtained using a pseudo-labeling\ntextual semantic parse model trained on existing paired data. The\nsemantic parse model here takes transcripts as inputs and produces\npseudo-label semantic parses as output. Transcripts can be gener-\nated using one of two prompting strategies, i.e., intent-word-based\nor exemplar-based.\nIntent Word-based prompting (IWP): The goal of IWP is to gener-\nate transcripts that may be classified under a certain intent, optionally\ncontaining \u201dintent words\u201d. Intent words are the words from semantic\nparses that occur most frequently with given intents after removing\nstop-words. An example is shown in Figure 2. The 40 words that co-\noccur most frequently with every intent in the STOP data are used as\nintent words. 40 examples are generated for every intent and intent-\nword combination. Though IWP produces good synthetic data, it is\nlimited by the fact that words that co-occur less frequently with the\nintent are less related to the intent. Such examples produced with\nless relevant intent words may not be classified under the desired\nintent class. This also limits the amount of synthetic data that can\nbe generated since the LLM cannot generate many unique examples\nusing a small number of intent-intent word combinations.\nYou are working in an intent-and-slot framework where every utterance\ncan be classified under an intent. Here are some examples of intents and\na description of their function:\n1. IN:ADD TIME TIMER - Creates a new timer\n2. IN:GET ESTIMATED DEPARTURE - gets estimated departure\nNow, we want to classify intents for the weather application. Given the in-\ntents IN:GET WEATHER, generate 40 utterances that are classified un-\nder this intent. You may use the word \u201dweather\u201d along with names of\npeople and places to generate 40 utterances. Your response should have\nnumbered utterances, with one utterance on each line. Make sure not to\nrepeat any responses. Start with 1.\nFig. 2. Prompt for IWP-based utterance generation\nGenerate 60 more sentences that are similar in intent to the following\nsentences:\n1. Is it going to be around 95 in degree Fahrenheit san francisco tomor-\nrow\n2. Is it around 72 in degree celsius karachi tonight\nWrite one sentence per line. Generate statements and questions with\ndifferent sentence structure.\nFig. 3. Prompt for EP-based utterance generation\nEach sentence should be enclosed in square brackets [ ]. The first square\nbracket [ should be followed by an intent that is in uppercase letters and\nbegins with IN:, for example, IN:GET WEATHER. Inside the sentence,\nyou should label some nouns with slots, which are also enclosed in brack-\nets [ ]. Slots are in all uppercase letters and begin with SL:, for example,\nSL:LOCATION. In each sentence, there can only be 1 intent, but there\ncan be many slots. Here are some examples:\n1. [IN:GET WEATHER what kind of weather is in [SL:LOCATION paris\n] ]\n2. [IN:GET WEATHER what is the temperature at the [SL:LOCATION\nnorth pole ] ]\n3. [IN:GET WEATHER tell me what the weather in [SL:LOCATION cen-\ntral park ] is like ]\nPlease generate more examples with the intent IN:GET WEATHER and\nany of the slots SL:LOCATION. The sentences should have an intent/slot\nformat like [IN:GET WEATHER [SL:LOCATION] ], but with some other\ntext, like the examples above. Write 30 similar sentences and then stop.\nUse names of people and places in your examples.\nFig. 4. Prompt for EP-based generation of seqlogical parses\nExemplar-based Prompting (EP): Since LLMs are strong in-\ncontext learners [26], an alternative approach is to prompt LLMs to\ngenerate transcripts based on examples. For every intent-slot com-\nbination, we provide up to 4 random example transcripts and ask\nthe model to generate 60 more transcripts that are similar but have\ndiverse sentence structures. An example prompt is shown in Fig\n3. Though the resulting transcripts may not always correspond to\nthe intent classes from which the examples are drawn, this method\nenables us to generate larger volumes of data without duplication.\nSemantic Parse generation and Quality Assessment: Transcripts\ngenerated by LLMs are first normalized \u2013 written text is converted to\nspoken form, punctuation except apostrophes are removed and text\nis transformed into lower case. Semantic parse pseudo-labels are\nobtained from these normalized transcripts using a strong RoBERTa-\nbased semantic parser trained on STOP (EM=86.8). To assess data\nquality, we compare the intent in the obtained pseudo-labels to the\nintent in the prompt for IWP or the intent of the provided examples\nfor EP. Intent Match Accuracy (IMA) is defined as the percentage of\ntimes the intent of the pseudo-label matches the desired intent of the\nprompt.\n4.2. Generating Transcript-Semantic Parse for New Domains\nFor new domains, paired data and pre-trained models are not avail-\nable, and therefore, we would need to directly generate pairs of tran-\nscript and semantic parse. One way to do this is to generate pairs\nof semantic parse and corresponding transcript using LLMs directly,\nhowever, maintaining consistency across generated parses and tran-\nscripts is challenging for current LLMs. Another alternative is to\ngenerate only the seqlogical form of the semantic parse from the\nLLM and infer the transcript from the parse. The seqlogical form of\nthe parse, unlike the decoupled form, comprises all the words in the\ntranscript along with slot and intent tags. Therefore, the transcript\ncan be obtained from the seqlogical parse merely by removing slot\nand intent tags.\nExemplar-based Prompting: We assume that (a) the intents and\nslots that must be recognized for the new domain are known, (b)\nthe slots that may occur with every intent, i.e., the intent-slot com-\nbinations are known, and (c) some manually annotated examples\nfor every intent-slot combination are known. Using this informa-\ntion, LLMs can be prompted as shown in Figure 4 to produce new\nseqlogical parses for a given intent-slot combinations. The prompt\nfirst describes the steps to generate a valid seqlogical parse and\nthen presents up to 3 examples of seqlogical parses with the desired\nintent-slot combinations.\nPost-processing: The generated seqlogical parses are checked for\ninvalid placement of brackets, and Out of Vocabulary (OOV) intents\nand slots. OOV intents were fixed by re-prompting the model to re-\nplace OOV intents with correct intents and replace any intents other\nthan the first. Any OOV slots are removed while retaining corre-\nsponding slot words.\n5. EXPERIMENTAL SETUP\n5.1. STOP Data, Model and Metrics\nData: STOP [27] is a public dataset with real speech for spoken\nsemantic parsing. STOP has data for 8 domains - alarm, event, mes-\nsaging, music, navigation, reminder, timer, and weather. The data\ncontains 28 unique intents and 82 slot types.\nMetrics: Exact Match (EM) is used to evaluate all our models. We\nreport EM (No Err) and EM w/ Err, which are the Exact Match ac-\ncuracies averaged over utterances with no ASR error and averaged\nover utterances with any ASR error respectively.\nModel Configuration: For the ASR module, we use RNNT with 3\nlayers of conformer in the encoder, 1 layer of LSTM in the predic-\ntor, and 1 linear layer in the joiner. For the deliberation model, we\nuse attention in the Fusion module, 2 transformer encoder layers in\nthe Pooling module, and a transformer decoder layer with a pointer-\ngenerator in the Decoder module [16]. Models are optimized with\nAdam [28], having a peak learning rate of 8e-3.\n5.2. Setup: Textual Data from Text Corpora\nFor experiments where we assume textual data is available, we split\nthe STOP datasets into two parts. We perform two experiments \u2013 one\nusing the first and second splits as paired and unpaired data respec-\ntively and the other using the second and first splits as paired and\nunpaired data respectively. The average performance across these 2\nexperiments is reported in each case. In the ED setup, equal amounts\nof data from every domain are present in the two splits. For the ND\nsetup, STOP is split by domain, where one split contains all train-\ning data from 4 domains(messaging, reminder, time, and weather),\nwhile the other split contains training data from the other 4 domains\n(alarm, event, music, and navigation). Both splits are designed to\nensure that they have a nearly equal number of utterances.\n5.3. Setup: Textual Data from LLMs\nWhen unpaired data is not available, we use Llama 2.0 to generate\nexamples for the ED and ND setups. For the ED setup, LLama 2.0\nis used to generate utterances. We then use a pre-trained 12-layer\nRoBERTa model trained on STOP to generate pseudo-labels for the\ngenerated utterances. We augment STOP with the generated LLama\n2.0 transcript-semantic parse. JAT is used to represent LLama 2 text.\nFor the ND setup, LLama 2.0 generated data is not suitable as a\nreal test set since it does not have matching real speech. Therefore,\nwe choose to partition the existing STOP data into 7 seen domains\nand 1 new domain - weather. We use exemplar-based prompting to\ngenerate transcript-semantic parse pairs for weather. For this, real\nexamples of transcript-semantic parse from STOP are used. We use\nTTS to generate equivalent speech representations for the generated\ndata. We compare the performance on the weather domain for mod-\nels trained on (a) 7 domains of STOP, (b) 7 domains of STOP with\nexamples for the weather (with TTS for examples and real speech\nfor 7 domains), (c) 7 domains of STOP with examples and Llama\n2.0 generated data, and (d) the topline that uses 7 domains of STOP\nwith real data and TTS.\n6. EXPERIMENTS\n6.1. When textual data is available\nTable 1 compares the performance of different models for the ED\nand ND settings where unpaired text is drawn from existing domains\nand new domains respectively. Across both ED and ND setups, we\nfind that the use of unpaired text improves EM scores.\nFor the ED setup, we find that JAT and TTS achieve similar Ex-\nact Match scores. Since JAT is comparable in performance to TTS\nand relatively inexpensive compared to complex TTS models like\nVoicebox, JAT is optimal for the ED setup. Further, the difference\nbetween JAT and TTS appears to be primarily on utterances with\nASR errors, since synthetic speech representations can be used to\nreduce the impact of ASR errors on semantic parsing. For the ND\nsetup, we find that though JAT outperforms the baseline, TTS out-\nperforms JAT. This is because new domains may have different enti-\nties and domain-specific terms that may be harder to recognize, and\nTTS provides valid speech representations that can be used to im-\nprove predictions based on the first-pass ASR. Figure 5 shows that\nthe amount of unpaired textual data is increased with constant paired\ndata, relative gains increase to a point and saturate.\nTable 1. Comparing JAT and TTS as speech representations for un-\npaired text from ED and ND. Number of paired and unpaired utter-\nances, and Exact Match (EM) is reported\nModel\n#Pair/#Unpair\nEM\nEM(No Err)\nEM w/ Err\nED\nBaseline\n60.4k / 0\n64.25\n80.51\n24.37\nw/ JAT\n60.4k / 60.4k\n66.92\n83.90\n25.25\nw/ TTS\n60.4 / 60.4k\n67.05\n83.88\n25.80\nND\nBaseline\n60.7k / 0\n33.28\n41.32\n13.54\nw/ JAT\n60.7k / 60.1k\n57.74\n73.34\n19.50\nw/ TTS\n60.7k / 60.1k\n63.95\n80.70\n22.88\nTopline\n120.9k / 0\n67.67\n84.52\n26.34\n6.2. LLama 2.0 Generated Data: ED Setup\nTable 2 compares various prompting strategies for generating utter-\nances in the same domain using Llama 2.0. We find that combining\nLLama-generated data with existing STOP data can improve per-\nformance across test examples with and without ASR errors. On\nfurther analysis, we find that significant improvements are observed\nacross domains with relatively poor performance in the STOP base-\nline. Between IWP and EP, we find that EP is slightly better. Since\nEP is not constrained to generate utterances that may be classified\nunder a given intent, the Intent Match Accuracy (IMA) is lower than\nFig. 5. Impact of increasing unpaired text on EM\nTable 2. Assessing the impact of augmenting the training data with\nLLama 2.0 generated utterances and RoBERTa pseudo-labels.EM is\nExact Match Accuracy\nModel\n#Utts\nIMA\nEM\nEM(No Err)\nEM w/ Err\nSTOP Baseline\n160k\n-\n67.37\n84.52\n26.34\n+ IWP-JAT\n230k\n68.87\n68.12\n84.96\n26.82\n+ EP-JAT\n218k\n64.24\n68.21\n85.01\n27.04\n+ (IWP+EP)-JAT\n298k\n67.87\n68.75\n85.82\n26.86\nthat of IWP. Combining the data generated from both these strategies\nfurther improves performance over the STOP baseline.\n6.3. LLama 2.0 Generated Data: ND Setup\nTable 3. Using TTS to generate speech for LLama 2.0 text when\nunpaired text is in an unseen new domain\nModel\n#Utts(Weather)\nWeather EM\nOverall EM\nSTOP 7 domain\n0\n0\n54.61\n+ 3 Examples-TTS\n360\n48.18\n61.80\n+ Exemplar LLama2-TTS\n2,910\n50.82\n62.29\nTopline: STOP Weather-TTS\n2,910\n63.80\n66.33\nTable 3 compares the performance of baseline models that have\nno data for weather or 360 examples for weather with models that\nuse LLama 2.0 generated data. Llama 2 generated text can improve\nperformance by over 2 points absolute EM but lags behind the per-\nformance of a topline that uses data from STOP.\n7. CONCLUSION\nWe address the high cost of manually labeling speech-transcript-\nsemantic parse data for spoken semantic parsing by enabling models\nto use text-only data. JAT is preferred for unpaired text in existing\ndomains for its efficiency and gain of 2.5 % EM over a paired data\nbaseline while remaining within 0.1 % EM of the more computation-\nally expensive TTS. For unpaired text in new domains, TTS outper-\nforms JAT by 6 % absolute EM overall, with a gain of 30.6 % EM\nover a paired baseline. When text data cannot be obtained from exist-\ning text corpora, we propose to prompt LLMs to generate transcript-\nsemantic parse pairs. We show that using different prompting strate-\ngies, we can generate unpaired text data in relatively large volumes.\nUsing JAT and TTS, we can leverage this LLM-generated data to fur-\nther improve SSP by 1.4 % EM and 2.6 % EM absolute for existing\nand new domains.\n8. REFERENCES\n[1]\nS. Wang, A. Shrivastava, and S. Livshits, Treepiece: Faster semantic\nparsing via tree tokenization, 2023.\n[2]\nS. Arora, H. Futami, S.-L. Wu, J. Huynh, Y. Peng, Y. Kashiwagi,\nE. Tsunoo, B. Yan, and S. Watanabe, \u201cA study on the integration of\npipeline and e2e slu systems for spoken semantic parsing toward stop\nquality challenge,\u201d in Proc. ICASSP, 2023, pp. 1\u20132.\n[3]\nH. Futami, J. Huynh, S. Arora, S.-L. Wu, Y. Kashiwagi, Y. Peng, B.\nYan, E. Tsunoo, and S. Watanabe, \u201cThe pipeline system of asr and nlu\nwith mlm-based data augmentation toward stop low-resource chal-\nlenge,\u201d in Proc. ICASSP, 2023, pp. 1\u20132.\n[4]\nD. Le, A. Shrivastava, P. Tomasello, S. Kim, A. Livshits, O. Kalinli,\nand M. L. Seltzer, \u201cDeliberation model for on-device spoken language\nunderstanding,\u201d Interspeech, 2022.\n[5]\nG. Wang, A. Rosenberg, Z. Chen, Y. Zhang, B. Ramabhadran, Y. Wu,\nand P. Moreno, \u201cImproving speech recognition using consistent pre-\ndictions on synthesized speech,\u201d in Proc. ICASSP, 2020, pp. 7029\u2013\n7033.\n[6]\nS. Toshniwal, A. Kannan, C.-C. Chiu, Y. Wu, T. N. Sainath, and K.\nLivescu, \u201cA comparison of techniques for language model integra-\ntion in encoder-decoder speech recognition,\u201d in 2018 IEEE spoken\nlanguage technology workshop (SLT), 2018, pp. 369\u2013375.\n[7]\nT. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and J.\nLe Roux, \u201cCycle-consistency training for end-to-end speech recogni-\ntion,\u201d in Proc. ICASSP, 2019, pp. 6271\u20136275.\n[8]\nZ. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong,\n\u201cInternal Language Model Adaptation with Text-Only Data for End-\nto-End Speech Recognition,\u201d in Proc. Interspeech, 2022, pp. 2608\u2013\n2612.\n[9]\nZ. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Repre-\nsentations through Modality Matching,\u201d in Proc. Interspeech, 2022,\npp. 4093\u20134097.\n[10]\nT. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen,\nB. Li, W. Wang, and T. Strohman, \u201cJoist: A joint speech and text\nstreaming model for asr,\u201d in Proc. SLT, 2023, pp. 52\u201359.\n[11]\nS. Kim, K. Li, L. Kabela, R. Huang, J. Zhu, O. Kalinli, and D. Le,\n\u201cJoint audio/text training for transformer rescorer of streaming speech\nrecognition,\u201d EMNLP, 2022.\n[12]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., \u201cTraining language\nmodels to follow instructions with human feedback,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 27 730\u201327 744,\n2022.\n[13]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T.\nLacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al., \u201cLlama:\nOpen and efficient foundation language models,\u201d arXiv preprint\narXiv:2302.13971, 2023.\n[14]\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., \u201cLlama\n2: Open foundation and fine-tuned chat models,\u201d arXiv preprint\narXiv:2307.09288, 2023.\n[15]\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, Ro{bert}a: A robustly optimized\n{bert} pretraining approach, 2020.\n[16]\nS. Kim, A. Shrivastava, D. Le, J. Lin, O. Kalinli, and M. L. Seltzer,\n\u201cModality confidence aware training for robust end-to-end spoken\nlanguage understanding,\u201d Interspeech, 2023.\n[17]\nA. Graves, \u201cSequence transduction with recurrent neural networks,\u201d\narXiv preprint arXiv:1211.3711, 2012.\n[18]\nS. Kim, Y. Shangguan, J. Mahadeokar, A. Bruguier, C. Fuegen, M. L.\nSeltzer, and D. Le, \u201cImproved neural language model fusion for\nstreaming recurrent neural network transducer,\u201d in Proc. ICASSP,\n2021, pp. 7333\u20137337.\n[19]\nC. Liu, F. Zhang, D. Le, S. Kim, Y. Saraf, and G. Zweig, \u201cImproving\nRNN Transducer Based ASR with Auxiliary Tasks,\u201d in Proc. SLT,\n2021.\n[20]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\n2017.\n[21]\nT. N. Sainath, R. Pang, R. J. Weiss, Y. He, C.-c. Chiu, and T.\nStrohman, \u201cAn attention-based joint acoustic and text on-device\nend-to-end model,\u201d in Proc. ICASSP, 2020, pp. 7039\u20137043.\n[22]\nP. Wang, T. N. Sainath, and R. J. Weiss, \u201cMultitask training with text\ndata for end-to-end speech recognition,\u201d arXiv preprint arXiv:2010.14318,\n2020.\n[23]\nM. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson,\nV. Manohar, Y. Adi, J. Mahadeokar, et al., \u201cVoicebox: Text-guided\nmultilingual universal speech generation at scale,\u201d arXiv preprint\narXiv:2306.15687, 2023.\n[24]\nY. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, \u201cFlow\nmatching for generative modeling,\u201d arXiv preprint arXiv:2210.02747,\n2022.\n[25]\nJ. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial net-\nworks for efficient and high fidelity speech synthesis,\u201d Advances in\nNeural Information Processing Systems, vol. 33, pp. 17 022\u201317 033,\n2020.\n[26]\nJ. Wei et al., \u201cEmergent abilities of large language models,\u201d Transac-\ntions on Machine Learning Research, 2022, Survey Certification.\n[27]\nP. Tomasello, A. Shrivastava, D. Lazar, P.-C. Hsu, D. Le, A. Sagar,\nA. Elkahky, J. Copet, W.-N. Hsu, Y. Adi, et al., \u201cStop: A dataset for\nspoken task oriented semantic parsing,\u201d in Proc. SLT, 2023, pp. 991\u2013\n998.\n[28]\nD. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimiza-\ntion,\u201d in Proc. ICLR, Y. Bengio and Y. LeCun, Eds., 2015.\n"
  },
  {
    "title": "Enhance audio generation controllability through representation similarity regularization",
    "link": "https://arxiv.org/pdf/2309.08773.pdf",
    "upvote": "2",
    "text": "ENHANCE AUDIO GENERATION CONTROLLABILITY THROUGH REPRESENTATION\nSIMILARITY REGULARIZATION\nYangyang Shi\nGael Le Lan\nVarun Nagaraja\nZhaoheng Ni\nXinhao Mei\nErnie Chang\nForrest Iandola\nYang Liu\nVikas Chandra\nMeta AI\nABSTRACT\nThis paper presents an innovative approach to enhance con-\ntrol over audio generation by emphasizing the alignment be-\ntween audio and text representations during model training.\nIn the context of language model-based audio generation, the\nmodel leverages input from both textual and audio token rep-\nresentations to predict subsequent audio tokens. However,\nthe current configuration lacks explicit regularization to en-\nsure the alignment between the chosen text representation and\nthe language model\u2019s predictions. Our proposal involves the\nincorporation of audio and text representation regularization,\nparticularly during the classifier-free guidance (CFG) phase,\nwhere the text condition is excluded from cross attention dur-\ning language model training. The aim of this proposed repre-\nsentation regularization is to minimize discrepancies in audio\nand text similarity compared to other samples within the same\ntraining batch. Experimental results on both music and au-\ndio generation tasks demonstrate that our proposed methods\nlead to improvements in objective metrics for both audio and\nmusic generation, as well as an enhancement in the human\nperception for audio generation.\nIndex Terms\u2014 Audio Generation, Music Generation,\nRepresentation regularization\n1. INTRODUCTION\nGenerating sound effects, music, and speech to meet specific\nrequirements holds immense importance as a pivotal tool in\ncontent creation spanning various domains, including aug-\nmented, virtual and mixed reality, video game development,\nand movie production. The advent of recent neural genera-\ntive models have brought about a transformative shift in the\nlandscape of digital content generation. Drawing inspiration\nfrom the remarkable progress in image generation [1, 2], the\nrealm of audio generation has undergone a paradigm shift \u2013\ntransitioning from conventional signal processing approaches\nto neural generative models [3, 4, 5, 6, 7, 8, 9, 10].\nJust as in the case of text-to-image generation models [1,\n11], harnessing the potential of diffusion probability mod-\nels [12, 13], the studies [9, 14, 15, 16, 4, 5, 17, 18] have show-\ncased impressive capacity in the realms of speech synthesis,\nsound effects creation, and music generation. Alongside the\ndiffusion-based approach, a parallel avenue has been pursued\nusing transformer-based language models [19], which have\nalso exhibited exceptional performance in audio generation\ntasks [20, 21, 22, 8, 6, 7].\nIn language model driven approach like MusicGen [8] and\nAudioGen [6], it first encodes raw audio into discrete tokens\nvia a neural audio compression model (e.g., [23, 24]). This\nmodel is end-to-end trained to compress and reconstruct in-\nput audio from discrete tokens with high quality and mini-\nmum perceptual loss. The generation model then employs\nan auto regressive transformer-decoder language model. The\nlanguage model operates on discrete audio tokens from the\nfirst phase and is conditioned on text inputs. Text is processed\nas text embedding representation using an text encoder pre-\ntrained on a large text corpus, such as T5 [25]. The text rep-\nresentation is used as cross attentions in the language model\ntraining. The language model is trained by cross-entropy loss\nto minimize the entropy to predict next discrete audio token\nbased on the previous audio tokens and the text representa-\ntion. However, in the whole training process, there is not any\nregularization to enforce the next audio token prediction to\nfully leverage representations from both audio token and con-\nditioning text. As a consequence, the generated audio often\nisn\u2019t fully aligned with the provided text prompt. It is of-\nten that the music generated based on the description \u201dHighly\nrhythmic orchestral piece illustrating wonder and awe. Fea-\ntures staccato violins, cellos, basses, trombone and grand pi-\nano\u201d, misses one or more instruments from the description.\nThe sound effects generated from the condition \u201dthe sound of\na ping pong ball bounce back once from the hard wood floor\u201d\nhas multiple ping pong ball bouncing sounds.\nThis paper introduces a method aiming at improving the\ntraining of the generation model to effectively capture rep-\nresentations from text conditions. This is achieved by mini-\nmizing the similarity between text and audio representations\nthrough regularization. Language model training comprises\ntwo modes: text-conditioned training and classifier-free guid-\nance (CFG) training [26, 6]. In CFG, the text condition is\nomitted during language model training. We enhance the au-\ndio and text representation similarity by reducing discrepan-\narXiv:2309.08773v1  [cs.SD]  15 Sep 2023\ncies in audio and text similarity compared to other samples\nwithin the same training batch. Experimental results in mu-\nsic and sound effects generation demonstrate the effective-\nness of the proposed approach, showcasing improvements in\nFrechet audio distance (FAD) using VGG classifier [27], kull-\nback\u2013leibler (KL) divergence using PaSST model [28], text\nand audio alignment score based on the contrastive language\naudio pretrained models (CLAP) [29], and human subjective\nevaluation for audio generation.\n2. RELATED WORK\nThis study applies the language model approach presented in\nworks such as [20, 21, 22, 8, 6, 7], in which the compression\nmodel discretizes audio into tokens for training and then de-\ncodes these tokens to audio. The language model learns to\ngenerate audio tokens. However, our emphasis lies in aug-\nmenting the semantic correlation between provided text de-\nscriptions and the generated audio. This enhancement is built\nupon the foundation of the MusicGen [8] and AudioGen [6]\nfor language model-driven audio generation.\nTo model the representation similarity between text and\naudio, one related work is CLAP [29] which uses contrastive\nloss. However, we found that using the contrastive loss in\nCLAP for generation model training did not improve the per-\nformance. Instead, we propose a new approach that first com-\nputes the representation similarities of audios and texts be-\ntween different samples. We then minimize the discrepan-\ncies between the audios\u2019 similarities and the texts\u2019 similari-\nties. Additionally, we found that max pooling is better than\naverage pooling for obtaining the sequence level representa-\ntion from individual time step output.\n3. REPRESENTATION REGULARIZATION\nFig. 1. Illustration of the language model training with cross\nentropy loss and representation regularization.\n3.1. Language model based audio generation\nThe language model based audio generation model is com-\nposed of several pivotal elements as shown in Fig 1. Firstly,\nit employs a compression model, such as the EnCodec\nmodel [30, 23] to encode the raw audio data into a discrete\nmulti-stream sequence of tokens ak,i. Here i \u2208 [1, Ta] and Ta\nis the length of the audio token sequence, while k \u2208 [1, K],\nindicating the particular codebook indexed as the k-th. Ad-\nditionally, the model incorporates a pre-trained text encoder,\nwhich transforms the text input into a sequence of embed-\nding representations identified as vj, where j \u2208 [1, Tv],\nTv corresponds to the length of the sequence containing\ntext embedding representations. Lastly, there is a language\nmodel component that is a stack of Transformer layers. The\nlanguage model leverages both the text embedding repre-\nsentation and the preceding audio tokens to generate the\nprobability distribution for the subsequent audio token as\np\u03b8(ak,i+1|ak,1, ..., ak,i, v1, ..., vTv). To render audio genera-\ntion more manageable, the generation of multi-stream audio\ntokens is trained in parallel, resulting in a substantial reduc-\ntion in the effective sequence length during model training.\nThe loss for the language model is the sum of the cross\nentropy loss for each stream k.\nLcond = \u2212\nK\nX\nk=1\nTa\nX\ni=1\nlog(p\u03b8(ak,i+1|ak,1, ..., ak,i, v1, ..., vTv)) (1)\n3.2. Representation regularization\nHowever, the cross entropy loss in language model lacks ex-\nplicit mechanism to enforce the audio token prediction align\nwith the provided text conditions. Furthermore, the correla-\ntion between text and audio gets even loosen as the classifier-\nfree guidance (CFG) method [26, 6, 8] is used in the training\nto regulate the balance between sample quality and diversity.\nEmploying CFG involves training the language model both\nconditionally and unconditionally. Similar to AudioGen [6],\n10% of the training samples have their accompanying text\nomitted during language model training. In unconditional sit-\nuation, the loss is simply\nLuncond = \u2212\nK\nX\nk=1\nTa\nX\ni=1\nlog(p\u03b8(ak,i+1|ak,1, ..., ak,i))\n(2)\nIn this work, the proposed representation regularization\nstrengthens the correlation between audio representation and\ntext representation while still maintains the effects of CFG\nmethod to train the language model unconditionally on text.\nGiven a batch of training samples, a pooling method F is used\nto get the text sequence representation as T b = F(vb\n1, ..., vb\nTv)\nand audio sequence representation as Ab = F(ub\n1, ..., ub\nTa) for\nthe particular sample b in the batch. In our experiments, the\nmax pooling achieved the best results.\nRather than directly mapping the text and audio represen-\ntations to the same space and maximizing the similarity be-\ntween audio and text as CLAP [29], we propose to minimize\ndiscrepancies in audio and text similarity compared to other\nsamples within the same training batch as follows:\nT b,\u02c6b =\nT b \u2217 T\n\u02c6b\n||T b||||T \u02c6b||\n(3)\nAb,\u02c6b =\nAb \u2217 A\n\u02c6b\n||Ab||||A\u02c6b||\n(4)\nLrr =\nP\nb!=\u02c6b(T b,\u02c6b \u2212 Ab,\u02c6b)2\nB \u2217 (B \u2212 1)\n(5)\nHere T b,\u02c6b denotes the representation similarity between\ntext inputs in sample b and \u02c6b. And Ab,\u02c6b denotes the represen-\ntation similarity between audio in sample b and \u02c6b. B is the\nbatch size. The Lrr enforces the text and audio in one sample\nhave the same differences regarding to the other samples.\nIn this study, the proposed representation regularization\nis exclusively applied during the CFG phase. The complete\nmodel training loss is defined as follows:\nL =\n(\nLuncond + \u03bbLrr\nif CFG is utilized\nLcond\nif CFG is not used\n(6)\nHere, \u03bb represents the weighting factor for the representa-\ntion regularization. Note that representation regularization is\nonly employed during regular training steps when CFG is in\nuse. We also conducted experiments involving representation\nregularization in non-CFG scenarios; however, these experi-\nments did not yield improvements in objective metrics. We\nbelieve the degradation may be attributed to the fact that rep-\nresentation regularization has the potential to hinder language\nmodel learning by copying the text representation from cross-\nattention as the audio representation in non-CFG.\n4. EXPERIMENTS\nIn this work, we use two sets of experiments including the\nsound effects generation and the music generation to verify\nthe effectiveness of proposed methods.\n4.1. Datasets\nIn music generation, we utilize a total of 20K hours of li-\ncensed music which comprises an internal compilation of 10K\nmusic tracks of high quality, and 390k instrument-only mu-\nsic tracks from the ShutterStock1 and Pond52. All datasets\nare full-length music with 32 kHz sampling rate, accompa-\nnied by comprehensive metadata such as textual descriptions,\ngenre categorizations, BPM, and tags. Our evaluation uses the\nMusicCaps benchmark [7]. The MusicCaps benchmark com-\nprises 5.5K samples including a subset of 1K samples bal-\nanced across various genres. We report objective metrics on\nthe unbalanced subset as [8].\nFor sound effect model training, a dataset encompass-\ning 4k hours of training data is employed.\nThis dataset\n1www.shutterstock.com/music\n2www.pond5.com\nincorporates resources like AudioSet [31], BBC sound ef-\nfects3, AudioCaps[32], Clotho v2 [33], VGG-Sound [34],\nFSD50K [35] and Free To Use Sounds4.\nAll audio files\nare sampled at a rate of 16kHz. We adopt a preprocessing\nmethodology akin to [6] for textual descriptions.\nTo be-\ngin, we utilize multi-label annotations from datasets such\nas AudioSet, VGG-Sound, FSD50K. Pseudo-sentences are\nconstructed by concatenating lists of tags linked with au-\ndio samples.\nSubsequently, we eliminate stop words and\nnumbers, and lemmatize natural language captions available\nin datasets including AudioCaps, Clotho v2, Free To Use\nSounds, and BBC Sound Effects. Lastly, samples containing\nthe term \u201dspeech\u201d in their tag or caption are filtered out, given\nthat speech predominates in the data.\n4.2. Setup\nOur approach involves a non-causal five-layer EnCodec\nmodel tailored for music generation, operating at 32 kHz\nfor monophonic music, and 16 kHz for sound effects genera-\ntion. These EnCodec models maintain a frame rate of 50 Hz,\ncommencing with an initial hidden size of 64, which doubles\nacross the model\u2019s five layers. Embeddings are subjected to\nquantization using an RVQ comprising four quantizers, each\nfeaturing a codebook size of 2048. These EnCodec models\nare trained using the same audio data as those in the language\nmodel training.\nThe transformer models used in this work have 300M pa-\nrameters. To enhance efficiency with long sequences, we em-\nploy memory-efficient Flash attention [36] from the xFormers\npackage [37], improving both speed and memory utilization.\nFor ablations, we consistently employ the sound effects gen-\neration model setup. For music generation model training,\n30-second audio segments are used, randomly sampled from\nthe complete track. In sound effects generation training, 10-\nsecond audio clips are used. Model training spans 100K steps,\nutilizing the AdamW optimizer [38], a batch size of 192 ex-\namples, \u03b21 = 0.9, \u03b22 = 0.95, a decoupled weight decay\nof 0.1, and gradient clipping of 1.0. A cosine learning rate\nschedule is employed, with a warmup of 4k steps. Further-\nmore, an exponential moving average is applied, character-\nized by a decay factor of 0.99. The model training employs\nthe mixed precision with Fully Sharded Data Parallel (FSDP)\nbfloat16. We used 16 GPUs and 32 GPUs for sound effects\ngeneration and music generation training, respectively. In the\nsampling process for inference, we adopt top-k sampling [39],\nretaining the top 250 tokens and applying a temperature of\n1.0.\n4.3. Ablation Study\nTable 1 presents the results of the ablation study conducted\non the sound effects generation model using the AudioCaps\ndataset. The optimal model was trained with representation\n3https://sound-effects.bbcrewind.co.uk/\n4https://www.freetousesounds.com/all-in-one-bundle/\nregularization based on max pooling, employing a weight pa-\nrameter of \u03bb = 3.0 and allocating 10% of the training data\nfor CFG training. In contrast, the use of average pooling-\nbased sequence representation regularization did not demon-\nstrate any improvement over the baseline. Furthermore, Ta-\nble 1 reaffirms the significant role of CFG training in reducing\nboth FAD and KL scores.\npool\nCFG\n\u03bb\nFAD(\u2193)\nKL(\u2193)\nCLAP(\u2191)\nmax\n0.1\n3\n1.43\n1.57\n0.31\nmax\n0.1\n4\n1.44\n1.58\n0.30\nmax\n0.1\n2\n1.56\n1.57\n0.31\nmax\n0.1\n1\n1.58\n1.61\n0.30\n-\n0.2\n0\n1.56\n1.60\n0.30\n-\n0.1\n0\n1.52\n1.60\n0.30\n-\n0.0\n0\n1.69\n1.58\n0.30\nmax\n0.2\n3\n1.59\n1.64\n0.30\naverage\n0.1\n3\n1.54\n1.59\n0.30\nTable 1.\nAblation study using sound effects generation\nbased on AudioCaps. The column \u2018pool\u2019 denotes the pool-\ning method to get the sequence level representation for both\naudio and text representation. \u2018CFG\u2019 column gives the ratio\nof using CFG in training. \u2018\u03bb\u2019 represents the weight used in\nrepresentation regularization.\n4.4. Music Generation\nTable 2 gives the objective metrics on the MusicCaps data.\nWe report the original metrics for MuiscLM, Noise2Music\nand MusicGen 1.5B model without melody. Notably, the in-\ntroduction of the proposed representation regularization re-\nsults in enhancements across all metrics. Our 300M param-\neter model, which incorporates representation regularization,\nsurpasses the performance of the MusicGen 1.5B parameter\nmodel in terms of FAD and CLAP.\nMethods\nFAD(\u2193)\nKL(\u2193)\nCLAP(\u2191)\nMusicLM [7]\n4.0\n-\n-\nNoise2Music[40]\n2.1\n-\n-\nMusicGen 1.5B[8]\n5.0\n1.31\n0.28\nours 300M w/o rr\n5.28\n1.36\n0.30\nours 300M w/ rr\n4.83\n1.32\n0.31\nTable 2. Music generation using MusicCaps. \u2019w/ rr\u2019 and \u2019w/o\nrr\u2019 mean with and without represenation regularization, re-\nspectively.\n4.5. Sound Effects Generation\nThe sound effects generation results on AudioCaps are shown\nin Table 3. The trend is the same as the music generation\nexperiments. The representation regularization improves the\nmodel performance on FAD, KL and CLAP. The results of\nAudioGen is referring to the github5.\nMethods\nFAD(\u2193)\nKL(\u2193)\nCLAP(\u2191)\nAudioGen [6]\n1.77\n1.58\n0.30\nours w/o rr\n1.52\n1.60\n0.30\nours w/ rr\n1.43\n1.57\n0.31\nTable 3. Sound effects generation using AudioCaps. \u2019w/ rr\u2019\nand \u2019w/o rr\u2019 mean with and without represenation regulariza-\ntion, respectively.\n4.6. Human preference evaluation\nTable 4 gives the subjective metrics for the sound and mu-\nsic generation models. Our subjective evaluation employed a\nblind pairwise comparison test, where evaluators were pre-\nsented with two samples generated by distinct models, all\nbased on the same text prompt. This comparison was con-\nducted across a set of 20 text prompts, and eight human eval-\nuators were tasked with determining their preference for the\nsample they believed exhibited better quality and better align-\nment with the provided prompt in each pair.\nNotably, both music and sound effects generation, when\nincorporating representation regularization, garnered higher\nuser preference ratings. A possible explanation for the more\nsignificant trend in the sound effects generation is that music\ntends to be more abstract than sound effects. Consequently,\nany discrepancies in alignment with the provided text may not\nbe as readily apparent to human evaluators.\nMethods\nmusic\nsound effects\nours w/o rr\n48%\n33%\nours w/ rr\n52%\n67%\nTable 4. Human preference evaluation\n5. CONCLUSION\nThis paper has introduced representation regularization to\nimprove controllability over audio generation by prioritiz-\ning alignment between audio and text representations during\nmodel training.\nThe proposed method integrated the au-\ndio and text similarity regularization, particularly during the\nclassifier-free guidance (CFG) phase, wherein the text condi-\ntion is excluded from cross attention during language model\ntraining. The experimental results, conducted across various\naudio and music generation tasks, demonstrate that the pro-\nposed representation regularization has led to improvements\nin objective metrics for both audio and music generation.\nMoreover, these improvements have translated into a no-\nticeable enhancement in human perception regarding audio\ngeneration quality and alignment.\n5https://github.com/facebookresearch/audiocraft/blob/main/model cards\n6. REFERENCES\n[1] Robin Rombach, Andreas Blattmann, et al., \u201cHigh-resolution\nimage synthesis with latent diffusion models,\u201d in CVPR, 2022.\n[2] Aditya Ramesh, Prafulla Dhariwal, et al., \u201cHierarchical Text-\nConditional image generation with CLIP latents,\u201d arXiv, 2022.\n[3] Yang Song, Jascha Sohl-Dickstein, et al., \u201cScore-Based gen-\nerative modeling through stochastic differential equations,\u201d\narXiv, 2020.\n[4] Haohe Liu, Qiao Tian, et al., \u201cAudioLDM 2: Learning holistic\naudio generation with self-supervised pretraining,\u201d arXiv, Aug.\n2023.\n[5] Haohe Liu, Zehua Chen, et al., \u201cAudioLDM: Text-to-Audio\ngeneration with latent diffusion models,\u201d arXiv, 2023.\n[6] Felix Kreuk, Gabriel Synnaeve, et al., \u201cAudioGen: Textually\nguided audio generation,\u201d arXiv, 2022.\n[7] Andrea Agostinelli, Timo I Denk, et al., \u201cMusicLM: Generat-\ning music from text,\u201d arXiv, 2023.\n[8] Jade Copet, Felix Kreuk, et al., \u201cSimple and controllable music\ngeneration,\u201d arXiv, 2023.\n[9] Matthew Le, Apoorv Vyas, et al.,\n\u201cVoicebox: Text-Guided\nmultilingual universal speech generation at scale,\u201d arXiv, 2023.\n[10] Max W Y Lam, Qiao Tian, et al., \u201cEfficient neural music gen-\neration,\u201d arXiv, 2023.\n[11] Prafulla Dhariwal and Alexander Nichol, \u201cDiffusion models\nbeat gans on image synthesis,\u201d Adv. Neural Inf. Process. Syst.,\n2021.\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel, \u201cDenoising dif-\nfusion probabilistic models,\u201d Adv. Neural Inf. Process. Syst.,\n2020.\n[13] Diederik Kingma, Tim Salimans, et al., \u201cVariational diffusion\nmodels,\u201d Adv. Neural Inf. Process. Syst., 2021.\n[14] Rongjie Huang, Max W Y Lam, et al., \u201cFastDiff: A fast con-\nditional diffusion model for High-Quality speech synthesis,\u201d\narXiv, 2022.\n[15] Sungwon Kim, Heeseung Kim, and Sungroh Yoon, \u201cGuided-\nTTS 2: A diffusion model for high-quality adaptive Text-to-\nSpeech with untranscribed data,\u201d arXiv, 2022.\n[16] Kai Shen, Zeqian Ju, et al., \u201cNaturalSpeech 2: Latent diffusion\nmodels are natural and Zero-Shot speech and singing synthe-\nsizers,\u201d arXiv, 2023.\n[17] Rongjie Huang, Jiawei Huang, et al., \u201cMake-An-Audio: Text-\nTo-Audio generation with Prompt-Enhanced diffusion mod-\nels,\u201d arXiv, 2023.\n[18] Flavio Schneider, Zhijing Jin, and Bernhard Sch\u00a8olkopf,\n\u201cMo\u02c6usai: Text-to-Music generation with Long-Context latent\ndiffusion,\u201d arXiv, 2023.\n[19] Ashish Vaswani, Noam Shazeer, et al., \u201cAttention is all you\nneed,\u201d Adv. Neural Inf. Process. Syst., 2017.\n[20] Zal\u00b4an Borsos, Rapha\u00a8el Marinier, et al., \u201cAudioLM: A language\nmodeling approach to audio generation,\u201d IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing, 2023.\n[21] Ewan Dunbar, Mathieu Bernard, et al.,\n\u201cThe zero resource\nspeech challenge 2021: Spoken language modelling,\u201d arXiv,\n2021.\n[22] Kushal Lakhotia, Eugene Kharitonov, et al., \u201cOn generative\nspoken language modeling from raw audio,\u201d Transactions of\nthe Association for Computational Linguistics, 2021.\n[23] Neil Zeghidour, Alejandro Luebs, et al., \u201cSoundStream: An\nEnd-to-End neural audio codec,\u201d IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 2022.\n[24] Alexandre D\u00b4efossez, Jade Copet, et al., \u201cHigh fidelity neural\naudio compression,\u201d arXiv, 2022.\n[25] Colin Raffel, Noam Shazeer, et al.,\n\u201cExploring the limits\nof transfer learning with a unified Text-to-Text transformer,\u201d\narXiv, 2019.\n[26] Jonathan Ho and Tim Salimans,\n\u201cClassifier-Free diffusion\nguidance,\u201d arXiv, 2022.\n[27] Shawn Hershey, Sourish Chaudhuri, et al., \u201cCNN architectures\nfor large-scale audio classification,\u201d in ICASSP, 2017.\n[28] Khaled Koutini, Jan Schl\u00a8uter, et al., \u201cEfficient training of audio\ntransformers with patchout,\u201d arXiv, 2021.\n[29] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,\nand Huaming Wang, \u201cCLAP: Learning audio concepts from\nnatural language supervision,\u201d arXiv, 2022.\n[30] Alexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and Yossi\nAdi, \u201cHigh fidelity neural audio compression,\u201d arXiv, 2022.\n[31] Jort F Gemmeke, Daniel P W Ellis, et al., \u201cAudio set: An ontol-\nogy and human-labeled dataset for audio events,\u201d in ICASSP,\n2017.\n[32] Chris Dongjoo Kim, Byeongchang Kim, et al., \u201cAudioCaps:\nGenerating captions for audios in the wild,\u201d in NAACL, 2019.\n[33] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen,\n\u201cClotho: an audio captioning dataset,\u201d in ICASSP, 2020.\n[34] Honglie Chen, Weidi Xie, et al., \u201cVggsound: A Large-Scale\nAudio-Visual dataset,\u201d in ICASSP, 2020.\n[35] Eduardo Fonseca, Xavier Favory, et al., \u201cFSD50K: An open\ndataset of Human-Labeled sound events,\u201d IEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, 2022.\n[36] Tri Dao, Daniel Y Fu, et al.,\n\u201cFlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness,\u201d arXiv,\n2022.\n[37] Benjamin Lefaudeux, Francisco Massa, et al., \u201cxformers: A\nmodular and hackable transformer modelling library,\u201d 2021.\n[38] Ilya Loshchilov and Frank Hutter, \u201cDecoupled weight decay\nregularization,\u201d arXiv, 2017.\n[39] Angela Fan, Mike Lewis, and Yann Dauphin, \u201cHierarchical\nneural story generation,\u201d arXiv, 2018.\n[40] Qingqing Huang, Daniel S Park, et al., \u201cNoise2Music: Text-\nconditioned music generation with diffusion models,\u201d arXiv,\n2023.\n"
  }
]