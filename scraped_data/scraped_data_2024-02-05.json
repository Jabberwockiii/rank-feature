[
  {
    "title": "Specialized Language Models with Cheap Inference from Limited Domain Data",
    "link": "https://arxiv.org/pdf/2402.01093.pdf",
    "upvote": "44",
    "text": "Specialized Language Models with Cheap Inference from Limited Domain Data\nDavid Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun\nApple Inc.\nAbstract\nLarge language models have emerged as a versatile tool but\nare challenging to apply to tasks lacking large inference\nbudgets and large in-domain training sets.\nThis work\nformalizes these constraints and distinguishes four important\nvariables: the pretraining budget (for training before the\ntarget domain is known), the specialization budget (for\ntraining after the target domain is known), the inference\nbudget, and the in-domain training set size. Across these\nsettings, we compare different approaches from the machine\nlearning literature.\nLimited by inference cost, we find\nbetter alternatives to the standard practice of training very\nlarge vanilla transformer models. In particular, we show\nthat hyper-networks and mixture of experts have better\nperplexity for large pretraining budgets, while small models\ntrained on importance sampled datasets are attractive for\nlarge specialization budgets.\n1\nIntroduction\nTraining large language models enables versatile models,\nbut their high inference cost limits them to high-value\napplications (Brown et al., 2020; Bommasani et al., 2022).\nDespite progress in approximated inference (Aminabadi\net al., 2022; Sheng et al., 2023; Dettmers & Zettlemoyer,\n2023), large models remain costly, or even impractical for\nmobile hardware. Under tight inference constraints, one\nmight consider a small model specialized to the domain\nat hand.\nThis paper studies training small specialized\nmodels, even with limited domain data. To achieve low\nperplexity, we use three key elements: generic training\ncorpora, importance sampling, and asymmetric models\nwith fewer parameters at inference than during training,\nsuch as mixtures of experts or hyper-networks.\nWith inference cost and in-domain training data limits,\nwe study alternative strategies with varying training cost.\nWe also take into account how training cost can be shared\nacross domains. For our study, we consider 4 important\nmetrics:\nGeneric training cost: the cost of the training phase\nInference budget?\nLarge Model\nSpecialization budget?\nImportance\nSampling\nHyper-networks\n& Mixtures\nLarge\nSmall\nLarge\nSmall\nFigure 1: Practical recommendations for training lan-\nguage models that fit a predefined computational budget.\nthat can be performed before the specialization data are\navailable, on a generic training set. This cost is shared across\nmultiple specialized models and is often called pretraining.\nAlthough not mandatory, generic training data are essential\nwhen specialization data are limited.\nSpecialization training cost: the cost of the training\nperformed once the specialization data are available. This\ncost is not shared across different specialized models.\nInference cost: the cost for running inference on a\nspecialized model. As part of the inference cost, one might\nalso be interested in a model which involves a small number\nof parameters per specialized task, e.g. considering memory\nand network constraints. Low inference cost allows wider\nmodel deployment.\nSize of the specialization training set: varies across\napplications and influences pretraining and specialization\nchoices.\nWe take the inference cost and the specialization data size\nas hard constraints and study the operating curves resulting\nfrom varying the generic and specialization training costs.\nWe compare different training approaches and highlight at\nwhich operating point they are interesting.\n2\nMethods\nWe consider different architectures to satisfy our inference\nconstraint while leveraging a large generic pretraining set.\n1\narXiv:2402.01093v1  [cs.LG]  2 Feb 2024\nWe assess the drop in perplexity from our inference con-\nstraint compared to a larger model trained on the same data.\nOur recommendations are summarized in Figure 1.\n2.1\nLarge Model\nLarge Model (LLM) One trains a large language model\n(LLM) on the generic data and uses this model as-is at in-\nference (Brown et al., 2020; Kaplan et al., 2020; Hoffmann\net al., 2022). This approach requires a high pretraining cost\nbut does not require specialization data and has no special-\nization cost. Inference cost for this method is high. Having\nnever seen specialized data, it might be inaccurate when\nthe specialization distribution is far from the pretraining\ndistribution.\nAfter generic pretraining, fine-tuning over specialization\ndata can adapt the model (Howard & Ruder, 2018; Agha-\njanyan et al., 2021). This step usually improves perplexity\nbut adds a specialization cost. This cost is limited when the\namount of specialization data is small: early stopping limits\nfine-tuning to a few updates to avoid overfitting. Fine-tuning\nleaves the inference cost unchanged.\nParameter Efficient Tuning One fine-tunes only a sub-\nset of the parameters once the specialization data are avail-\nable (Hu et al., 2021; Lester et al., 2021; Houlsby et al.,\n2019). This strategy is advantageous if the specialization\ndata are scarce because it mitigates overfitting. However,\ntuning fewer parameters may require more fine-tuning steps\nand hence increase the specialization cost. This method is\npractical when one needs to communicate only small model\ndeltas for each new specialization. See Section F in the\nAppendix.\n2.2\nSmall Model\nSmall Model (SLM) One trains a single small language\nmodel (SLM) before the specialization data are available\nand uses this model as-is at inference. This method does\nnot require specialization data or incur any specialization\ncost. The inference cost for this method is low and so is the\npretraining cost. However, a small model cannot use a large\namount of generic data as well as a large model resulting in\nworse downstream performance. Similarly to larger models,\nfine-tuning can improve performance with an additional\nspecialization cost.\nNo Pretraining (SLM-nopt) This method only trains\nthe model on the specialization data. This is advantageous\nwhen the specialization budget and amount of specialized\ndata are large or when the generic training distribution is\nvery far from the specialization domain.\nImportance Sampling (SLM-is) This method does not\npretrain the model before the specialization data are avail-\nable. Once the specialization set is given, SLM-is samples\na tailored training set from the generic pretraining data\nto match the specialization distribution (Xie et al., 2023).\nThis method is a case of data selection (Moore & Lewis,\n2010; Grangier & Iter, 2022) and is advantageous when the\nspecialization data are scarce. This method incurs a high\nspecialization cost since pretraining on a (possibly large)\ntailored training set is necessary for each specialization do-\nmain. After pretraining, the model can be further fine-tuned\nover the specialization data.\nDistillation (SLM-d) This method (Hinton et al., 2015;\nHsieh et al., 2023b; Zhu et al., 2023) uses a fine-tuned\nlarge model as a teacher and distills it in a small student\nmodel. Compared to the large teacher model, inference\ncost is lower, but accuracy is also reduced. Compared to\na small model without distillation, the accuracy can be\nbetter. The teacher model provides rich targets from a\nmodel with better generalization, often reducing overfitting\nto the specialization set. Compared to SLM fine-tuning, this\nmethod has a higher generic pretraining cost: it requires\ngeneric training of a large model and a small model. Its\nspecialization cost is also greater since it requires tuning\nthe teacher model and then collecting the teacher outputs\nduring distillation.\n2.3\nHard Mixture of Experts (SLM-mix)\nThis method (Eigen et al., 2014; Gross et al., 2017) divides\nthe large pretraining set in smaller subsets, e.g. via clus-\ntering, and pretrains a small model (expert) on each part.\nIts pretraining cost and its overall number of parameters\nare high as both scale linearly with the number of clusters.\nInference with a hard mixture is typically performed by\nforwarding each example to the expert corresponding to the\ncluster the example belongs to.\nOnce the specialization data are available, we specialize\nthe mixture by selecting a single expert for each special-\nization task.\nOne option is to select the expert whose\npretraining cluster is the most frequent cluster in the spe-\ncialization data. Alternatively, if the specialization budget\nis sufficient, we can select the expert with the smallest loss\non average on the specialization data. Selecting a single\nexpert per domain is advantageous as it communicates and\nloads only a small part of the mixture weights for inference\non the target domain, but it might be detrimental when the\nspecialization data are spread across multiple clusters.\nSLM-mix can be fine-tuned. With a large specialization\nbudget, one can fine-tune each expert and select the best-\nperforming one. With a smaller budget, one can instead\nfine-tune just the best-performing expert at pretraining or\nthe expert whose pretraining cluster is the most frequent\ncluster in the specialization data. This second option makes\n2\nthe cost of specialization identical to SLM fine-tuning.\n2.4\nHyper-Networks (SLM-hn)\nHyper-networks (Ha et al., 2017) are neural networks that\ndecompose into two parts: the hyper-sub-network and the\ninstantiated sub-network. The hyper-sub-network creates\nweights for the instantiated sub-network. We rely on hyper-\nnetworks to create a mixture of experts: the hyper-sub-\nnetwork takes the cluster membership of an input to produce\nthe sub-network weights. These cluster-specific weights\ninstantiate a small sub-network or expert. Compared to a\nhard mixture of experts, SLM-hn shares parameters across\nexperts via the hyper-sub-network and provides a flexible\nway to independently select the capacity of the mixture and\nthe number of clusters. An instantiated sub-network can be\nfine-tuned on specialization data.\n3\nExperimental Setup\nWe present the datasets for our experiments, the experimen-\ntal setting for each method and the evaluation metrics.\n3.1\nDatasets\nOur generic pretraining set is c4, a large filtered dataset of\nEnglish text derived from commoncrawl (Raffel et al., 2020).\nWe tokenize the data with a sentence piece model trained on\nc4 with a vocabulary size of 32k. We consider specializing\nto nine diverse domains, extracted from the Pile (Gao et al.,\n2021): arxiv (science articles), europarl (parliamentary\nproceedings), freelaw (legal text), gutenberg (old books\npusblished before 1919), opensubtitles (theatrical subtitles),\nopenwebtext2 (forum discussions), pubmed-abstracts (med-\nical article abstracts), stackexchange (Q&A mostly about\ntechnical topics), wikipedia (encyclopedia articles). We\nvary the amount of specialization training data available\nand consider sets of size 1, 8 and 64 million tokens for each\ndomain.\n3.2\nClustering\nHard mixture-of-experts, hyper-networks and importance\nsampling rely on document clustering. We use sentence\nBERT (Reimers & Gurevych, 2019) to embed the c4 docu-\nments into 768-dimensional vectors and cluster them with\nthe kmeans algorithm. We explore different numbers of\nclusters ranging for 4 to 1,024 clusters.\n3.3\nLanguage Models\nWe perform our experiments with transformer mod-\nels (Vaswani et al., 2017). We consider two model sizes,\nsmall and large. The small model has 126M parameters and\nconsists of 7 layers with a dimension of 1,024 and a latent\nfeed-forward dimension of 4,096. Our large model has\n770M parameters with 7 layers with a dimension of 2,816\nand a latent feed-forward dimension of 11,264. Models are\ntrained and evaluated with a context of at most 1,024 tokens,\nsplitting longer documents into non-overlapping windows.\n3.4\nDistillation\nFor distillation, we use a fine-tuned LLM as the teacher and\nan SLM pretrained on the generic set as the student. Distil-\nlation training operates on the specialization data and trains\nthe student to minimize the KL divergence between its pre-\ndiction and the teaching distribution, a mixture between the\ndata distribution and the teacher model prediction (Hinton\net al., 2015). The teaching mixture weight is a hyperparam-\neter (0.95 in our experiments). In this method, the generic\ntraining cost is dominated by the training of the teacher\nmodel while the specialization cost is also dominated by\nthe cost of fine-tuning the teacher model. This method has\nan additional smaller cost of pretraining the SLM on the\ngeneric dataset and teaching the SLM on the specialization\ndataset.\n3.5\nMixture of Experts\nWe train hard mixtures (Gross et al., 2017) of transformers\nfor pretraining. The pretraining set is divided into clusters\nand an independent SLM is trained on each cluster. For\nspecialization, we consider a simple strategy: we cluster the\nspecialization set with the pretraining centroids to determine\nthe most frequent cluster in the specialization set. We fine-\ntune only the model pretrained on this cluster. Hard mixtures\nare interesting here since they allow training a model with\na large total number of parameters while fine-tuning and\nrunning inference only with a small model.\nIf the pretraining budget is low, one can forgo pretraining\nmodels on all clusters and, instead, increase the special-\nization budget to train a model only on the cluster of the\ngeneric dataset corresponding to the most frequent cluster\nin the specialization set, once this set is available.\n3.6\nHyper-Networks\nHyper-network (Ha et al., 2017; Karimi Mahabadi et al.,\n2021) defines the general idea of a neural network whose\nweights are themselves generated from a secondary network,\nthe hyper-network, based on a conditioning input variable.\n3\nIn our case we associate each example with its cluster\nmembership variable, using the clustering mentioned in\nSection 3.2. This variable is the input of the hyper-network\nwhich produces the feed-forward (i.e. multi-layer percep-\ntron, MLP) matrices of a transformer language model for\nall layers except the first two. The other parameters of the\ntransformer do not depend on the cluster and are the same\nfor all examples.\nOur hyper-network instantiates two MLP matrices\n\ud835\udc4a (1,\ud835\udc59,\ud835\udc56), \ud835\udc4a (2,\ud835\udc59,\ud835\udc56) for each layer \ud835\udc59 and each cluster \ud835\udc56.\nIt\nrelies on two hyper-parameters: the latent dimension \u210e\nand the number of experts \ud835\udc5a. Each cluster \ud835\udc56 is associated\nwith the h-dimensional embedding \ud835\udc50(\ud835\udc56). Each layer l is\nassociated with the \u210e \u00d7 \ud835\udc5a-matrix \ud835\udc40 (\ud835\udc59). We compute the\nmatrices\n\ud835\udc4a (1,\ud835\udc59,\ud835\udc56) = \ud835\udc50(\ud835\udc56) \ud835\udc40 (\ud835\udc59) \u00b7 \ud835\udc47 (1,\ud835\udc59) and \ud835\udc4a (2,\ud835\udc59,\ud835\udc56) = \ud835\udc50(\ud835\udc56) \ud835\udc40 (\ud835\udc59) \u00b7 \ud835\udc47 (2,\ud835\udc59)\nas the weighted sum between the vector \ud835\udc50(\ud835\udc56) \ud835\udc40 (\ud835\udc59) and\nthe three dimensional tensors \ud835\udc47 (1,\ud835\udc59),\ud835\udc47 (2,\ud835\udc59) respectively of\nshape \ud835\udc5a \u00d7 \ud835\udc51latent \u00d7 \ud835\udc51in and \ud835\udc5a \u00d7 \ud835\udc51in \u00d7 \ud835\udc51latent. These tensors\nhold most of the model parameters, i.e. \ud835\udc5a times as many\nparameters as the corresponding MLP matrices.\nThis\nstrategy enables increasing \ud835\udc5a to increase the overall model\ncapacity while keeping the size of the model instantiated\nfor each cluster constant.\nOf course, we illustrate one\nchoice of hyper-network architecture, but many alternatives\nare possible (Muqeeth et al., 2023; Abnar et al., 2023).\nCompared to hard mixtures of experts, the hyper-networks\nhave stronger capacity limitations since the training problem\ncannot be split into independent, low-memory training tasks.\nOn the other hand, the weights of each of the \ud835\udc5a experts \u2014\nboth the attention parameters, which are the same for all\nexperts, and the MLP tensors \u2014 are trained jointly, hence\nthe hyper-network model can be more parameter efficient.\nFor specialization, we follow a strategy similar to the hard\nmixture case: we instantiate the model at the most frequent\ncluster on the specialization set and fine-tune it. Fine-tuning\ntherefore does not operate on the large hyper-network but\nonly on the small instantiated model.\n3.7\nImportance Sampling\nOur importance sampling method relies on the k-means\nclustering from Section 3.2.\nIt is a streaming method\nthat requires only the histogram of cluster frequencies in\nthe targeted distribution \u210e\ud835\udc61. It relies on a large buffer of\npretraining documents, e.g. \ud835\udc41 \u2243 100k. We compute the\ncluster histogram \u210e\ud835\udc4f in the buffer and take \ud835\udc41\ud835\udc56 documents in\neach cluster \ud835\udc56. \ud835\udc41\ud835\udc56 = \ud835\udc41 \u00d7\u210e\ud835\udc61\n\ud835\udc56 \u00d7min\ud835\udc57 (\u210e\ud835\udc4f\n\ud835\udc57 /\u210e\ud835\udc61\n\ud835\udc57). is the maximum\nnumber of documents we can take for each cluster while\nenforcing that the histogram of the \ud835\udc41\ud835\udc56 matches the target\nhistogram \u210e\ud835\udc61. The selected data are then used for training.\nTable 1: Number of parameters (in millions) for pretrain-\ning and inference.\nModel\nNum. parameters (M)\nGeneric Pretrain\nInference\nSmall LM (SLM)\n126\n126\nMixt. of experts (SLM-mix)\n2,016\n126\nHyper network (SLM-hn)\n1,422\n126\nLarge LM (LLM)\n771\n771\n3.8\nMetrics\nWe rely on perplexity, the standard language modeling\nmetric, for our evaluation.\nWe measure perplexity on\nheld out data using 20k documents per dataset. We focus\nsolely on language modeling and evaluating the models\non downstream tasks (e.g. question answering, sentiment\nanalysis, translation, etc) is beyond the scope of the paper.\nWe measure training cost (pretraining and specialization)\nin hours of graphic processor compute time (GPUh) on the\nsame hardware (Nvidia-A100). We consider pretraining\ncosts ranging from 10 to 650 GPUh and specialization cost\nranging from 0.3 to 120 GPUh.\n4\nEmpirical Results\nWe first report our main results before diving into a detailed\ndiscussion for each method.\nTable 1 reports the number of parameters for the pre-\ntrained and specialized models. Table 1 illustrates that\nSLM-hn and SLM-mix are as small as SLM for inference\nafter specialization while their overall number of pretrained\nparameters is larger than LLM. Table 2 reports the through-\nput of the models. All SLM models have the same special-\nization throughput while SLM-hn has a lower throughput\nthan SLM, SLM-mix for pretraining. LLM is more expen-\nsive in all cases. Table 3 presents the upper limit in training\nbudgets for pretraining and specialization over all settings.\nWe consider varying pretraining budget and report per-\nplexity on the generic pretraining set (c4) for each method\nin Figure 2. When we consider SLM-hn and SLM-mix, we\nobserve that even if the number of pretrained parameters\nis larger than LLM, they do not enjoy as good perplexity.\nHowever, their perplexity is better than SLM while they are\nas efficient when tested or fine-tuned on a single cluster.\nPerplexity on c4 is not our main goal and we report\nthe perplexity on the specialization domains from Pile,\nsee Section 3.1. We report held-out perplexity by macro-\naveraging on the nine sets. We compute the mean negative\n4\nTable 2: Model throughput (GPU hours per 1B training\ntokens).\nModel\nTraining\nInference\nGeneric Pre.\nSpecialization\nSLM\n2.2\n2.2\n0.61\nSLM-mix\n2.2\n2.2\n0.61\nSLM-hn\n3.6\n2.2\n0.61\nSLM-is\nN/A\n2.2\n0.61\nLLM\n7.7\n7.7\n2.54\nTable 3: Train cost limits for pretraining and specializa-\ntion (GPUh)\nModel\nPretraining\nSpecialization\n1M\n8M\n64M\nLLM\n\u2264 650\n\u2264 0.12\n\u2264 0.5\n\u2264 3.5\nSLM\n\u2264 530\n\u2264 0.02\n\u22640.07\n\u2264 0.5\nSLM-is\n0\n\u2264 130\n\u2264 130\n\u2264 130\nSLM-d\n\u2264 1,850\n\u2264 0.7\n\u2264 2.8\n\u2264 21\nSLM-mix\n\u2264 650\n\u2264 0.02\n\u22640.07\n\u2264 0.5\nSLM-hn\n\u2264 650\n\u2264 0.02\n\u22640.07\n\u2264 0.5\nlog likelihood per token for each set, average the nine\nnumbers and compute their exponential.\nAll domains\ntherefore get the same weight, regardless of the size of the\nheld-out set.\nFigure 3 (a) reports the results before fine-tuning. The\nreported perplexities are much higher than the c4 perplexi-\nties, and indicate that specialization is necessary. Figure 3\n(b) reports the results after fine-tuning several pretrained\ncheckpoints for each method on the 1M token dataset of\neach domain. Each domain-specific model is evaluated\nbefore macro-averaging. Since 1M tokens is a small set,\nfine-tuning relies on a small learning rate and early stop-\nping (base learning rate divided by 3, always stopping after\nless than 2k fine-tuning steps on one GPU). Fine-tuning is\nhighly beneficial for all methods and results in significantly\nimproved perplexity. We also remark that pre-fine-tuning\nperplexity on the Pile is not necessarily a good indicator\nof post-fine-tuning perplexity: e.g. the SLM checkpoints\nordering is very different on the two curves, the order-\ning between SLM-mix and SLM-hn also changes during\nfine-tuning.\nWe also consider fine-tuning on 8 and 64 million tokens\nfor each domain, see Figure 3 (c) and (d). More data allows\nus to train slightly longer and keep the base learning rate\nwithout overfitting. We stop at most after 4k steps and 30k\nsteps for the 8M and 64M cases respectively. We observe\nthat the benefit of a good starting point provided by SLM-hn\nand SLM-mix (compared to SLM) erodes as the domain\ntraining set size increases.\nThese figures report the perplexity of SLM-is as a constant\nline. This method has no pretraining as we can only start\ntraining once the domain data are available; bearing all the\ntraining cost in the specialization phase. SML-is is the\nbest method with a small inference model in terms of post-\nspecialization perplexity. Interestingly, it even outperforms\nthe much larger model when specific in-domain data are\nscarce (ie the 1M tokens case).\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n12\n14\n16\n18\n20\n22\nGeneric perplexity\nSLM     \nSLM-mix     \nSLM-hn     \nLLM     \nFigure 2: Generic pretraining perplexity on c4.\n4.1\nSmall and Large Models\nTable 4 compares the perplexity on the Pile subsets for\nthe baseline transformer models.\nPretraining and fine-\ntuning are both necessary to achieve good perplexity on our\nspecialization sets. Without pretraining (SLM-nopt), a lot\nof specialization data (64M tokens per domain) are required\nin order to get acceptable performance. We also observe\nthat for both large and small models there is a large gap\nin perplexity before and after finetuning; making it clear\nthat finetuning even on 1M in-domain tokens can result\nin significant boost in performance. Finally, as expected,\nthe LLM results also illustrate that, for large inference and\npretraining budgets, it is beneficial to train large models on\nthe pretraining set (c4).\n4.2\nDistillation\nOur distillation process takes a pretrained teacher (LLM)\nand a pretrained student (SLM). We fine-tune the teacher\non the specialization set and we use the fine-tuned teacher\nto supervise the student on the same set. In this process,\nthe generic pretraining cost sums two terms: teacher and\nstudent pretraining. It is a question how to best spread the\ncost between these two terms.\n5\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n20\n25\n30\n35\n40\nSpecific perplexity\nSLM     \nSLM-mix     \nSLM-hn     \nSLM-is     \nLLM     \n(a) Before fine-tuning\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n14\n16\n18\n20\n22\nSpecific perplexity\n(b) 1M tokens\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n12\n13\n14\n15\n16\n17\nSpecific perplexity\n(c) 8M tokens\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n10\n11\n12\n13\nSpecific perplexity\n(d) 64M tokens\nFigure 3: Specific perplexity on the Pile subsets (average) before and after fine-tuning with different amounts of\nspecialization data.\n0\n100\n200\n300\n400\n500\nStudent pretraining cost (GPUh)\n17\n18\n19\n20\n21\n22\nSpecific perplexity\n0\n200\n400\n600\n800\n1000 1200\nOverall pretraining cost (GPUh)\n17\n18\n19\n20\n21\n22\nSpecific perplexity\nSLM     \nteach 92 GPUh     \nteach 461 GPUh     \nteach 645 GPUh     \nSLM-mix     \nSLM-hn     \nFigure 4: Distillation results (dashed lines) on the 1M token specialization set for various teacher pretraining\nbudgets. On the left we show perplexity with respect to the student pretraining cost only and on the right with\nrespect to the overall pretraining cost.\nFigure 4 (left) reports SLM-d perplexities with each curve\ncorresponding to a different amount of teacher pretraining\nand has the student pretraining as the x-axis. It shows that\nfor settings over 276 GPUh of teacher pretraining (300k\nsteps), the student model SLM-d is significantly better than\nvanilla SLM at the same level of student pretraining. This\nplot demonstrates the benefit of a good teacher over an SLM\ntrained only over the specialization set targets.\nFigure 4 (right) shows the same data changing the x-axis\nto report the overall generic pretraining cost, summing the\nteacher and student pretraining cost. When the teacher\npretraining cost is accounted for, SLM-d is not competitive\nwith the best methods like SLM-hn and SLM-mix.\n6\nTable 4: Perplexity on the Pile (average) for small and\nlarge LMs (for a limit of 650 GPUh of generic pretraining)\nModel\nPretrained\nSpecialized\n1M\n8M\n64M\nSLM\n33.0\n18.2\n14.8\n12.0\nSLM-nopt\nN/A\n227.1\n45.6\n17.6\nLLM\n28.1\n14.4\n12.5\n10.0\n4.3\nMixture of Experts\nOur hard mixture of experts relies on the generic dataset\nsplit in clusters, see Section 3.2, and its number of experts\ncorresponds to the number of clusters. For fine-tuning, we\nfine-tune only the expert corresponding to the most frequent\ncluster in the targeted domain dataset. In this section, we\nvary the number of clusters and discuss whether selecting\nthe most frequent cluster is a good strategy.\nThe overall capacity of the mixture and its training cost\nis proportional to the number of clusters. Our main results\n(Fig. 2, Fig. 3, etc) use 16 experts. We compare results with\n4 to 256 experts. Intuitively, if the number of experts is too\nlarge, the model would cost more to train and each cluster\nwould not contain enough data to train a model of the size\nof SLM. Conversely, if the number of experts is too small,\nthe training cost is low but each SLM-sized expert would be\ntrained from a large cluster and would underfit its training\nset. Also, the large clusters might be too generic and far\nfrom the distribution of the targeted set. Figure 5 shows the\nmacro-averaged perplexity on the Pile as a function of the\ngeneric pretraining time for the different mixture sizes in\nthe case of the 1M token specialization set.\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n17\n18\n19\n20\n21\n22\n23\nSpecific perplexity\nmix-4     \nmix-16     \nmix-64     \nmix-256     \nFigure 5: Specific perplexity of mixture models with\n4-256 experts on Pile subsets (average) after fine-tuning\non 1M tokens.\nAs mentioned above, specialization fine-tunes a single\nexpert. We select the expert corresponding to the most\nfrequent cluster in the specialization data. Alternatively, we\nTable 5: Selecting the best expert. Average specific per-\nplexity for 1M tokens, fine-tuning different experts from\nthe same mixture of 64 experts after 700k pretraining\nsteps (\u223c 600 GPUh).\nMethod\nPerplexity\nSpecialization cost\nMost frequent cluster\n17.32\n1x\nBest pre-trained\n17.05\n1x\nBest fine-tuned\n16.98\n64x\nalso consider selecting the expert which has the lowest loss\non the specialization set before fine-tuning, which involves\nevaluating each expert. As a third costlier option, we fine-\ntune all experts and pick the best one a posteriori. Table 5\nreports this result when fine-tuning on 1M tokens for the\n64 expert model. The results of the different strategies are\nwithin 0.3 PPL of each other. The most costly option of\nfine-tuning all experts performs best.\nAs a final observation on SLM-mix, the strategy of fine-\ntuning only the expert corresponding to the most frequent\ncluster enables the transfer of training cost from pretraining\nto specialization. Namely, one can wait until the targeted\ndomain is known and then pretrain only one model on\nthe single cluster of interest. This is interesting when one\ntargets only a few domains. However, this strategy does\nnot perform as well as importance sampling as shown in\nFigure 6.\n0\n20\n40\n60\n80\nSpecialization training cost (GPUh)\n14\n16\n18\n20\n22\nSpecific perplexity\nSLM-is     \nSLM-mix     \nFigure 6: Specific perplexity after fine-tuning on 1M\ntokens, when one is only training SLM-mix on the most\nfrequent domain cluster.\n4.4\nHyper-networks\nThe number of experts in our hyper-networks allows tuning\nthe overall number of parameters while keeping the size of\nthe inference model constant. Figure 7 shows perplexity\non the Pile subsets after fine-tuning on 1M tokens. More\n7\nexperts always perform better per iteration, however, 32\nexperts is more compute-time efficient in our setup.\n0\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n17\n18\n19\n20\nSpecific perplexity\nhn-16     \nhn-32     \nhn-64     \nFigure 7: Specific perplexity for hyper-networks with\ndifferent number of experts after fine-tuning on 1M\ntokens.\n4.5\nImportance Sampling\nOur importance sampling strategy resamples c4 such that\nits cluster histogram matches the cluster histogram from the\ntargeted Pile subset. The number of clusters is an important\nparameter. A small number of clusters will change the\nc4 distribution only in a coarse manner and will provide\na low fidelity match with the targeted set. Conversely, a\nlarge number of clusters has two drawbacks. Firstly, when\nthe specialization set is small, cluster frequencies might be\npoorly estimated for a large number of clusters. Secondly,\nwith a large number of clusters, the targeted histogram\nmight concentrate a big fraction of the mass on a few small\nclusters, meaning that the resampled c4 dataset will contain\nmany repeated points from these clusters. This can degrade\nperformance as the effective size of the resampled c4 dataset\nwill be smaller with these repetitions.\nOur main results report the importance sampling results\nwith 1,024 clusters. Figure 8 reports the results with 16, 64,\n256 and 1,024 clusters.\nThe importance sampling method does not start train-\ning before the specialization set is given and a model is\npretrained from scratch on a different resampled dataset\nfor each specialization task. This means that importance\nsampling has a much larger specialization cost when com-\npared to fine-tuning and this discrepancy only becomes\nmore important when addressing many tasks. For a model,\nthe total cost of specialization over \ud835\udc41 tasks is\n\ud835\udc36total(\ud835\udc41) = \ud835\udc36generic pretrain + \ud835\udc36specialization \u00d7 \ud835\udc41.\n(1)\nFor methods like hyper-networks, most of the cost is\n\ud835\udc36generic pretrain and the main parameter to vary the total\ncost is the number of generic pretraining steps. For the\n0\n20\n40\n60\n80\n100\nSpecialization training cost (GPUh)\n14\n16\n18\n20\n22\nSpecific perplexity\nSLM-is-16     \nSLM-is-64     \nSLM-is-256     \nSLM-is-1024     \nFigure 8: Specific perplexity for importance sampling\nwith different number of clusters after fine-tuning on 1M\ntokens.\nimportance sampling method, \ud835\udc36generic pretrain = 0 and the\nmain parameter to vary the total cost is the number of\nsteps performed when training on the importance sampled\npretraining set, which is part of \ud835\udc36specialization.\nWe vary the total cost for SLM-hn and SLM-is when\nhypothetically addressing 1, 7 and 50 tasks by scaling the\nx-axis following Equation 1. Figure 9 shows that SLM-is\nbecomes less interesting when the number of tasks increases.\nThe specialization cost of fine-tuning for SLM-hn, which\nincreases linearly with the number of tasks, can be ignored\nas it takes \u223c 1GPU minute.\n0\n100\n200\n300\n400\n500\n600\nTotal training cost (GPUh)\n14\n16\n18\n20\n22\nSpecific perplexity\nSLM-is 1 tasks     \nSLM-is 7 tasks     \nSLM-is 50 tasks     \nSLM-hn     \nFigure 9: Specific perplexity for the hyper-network vs\nimportance sampling for the 1M token specialization\ndatasets. Varying the number of tasks increase the cost\nof importance sampling linearly.\n5\nRelated Work\nDomain adaptation for language modeling has a long his-\ntory, predating neural network language models (Rosenfeld,\n2000). This research stemmed from the observation that\nmodels trained on large amount of data, even far from the\ntargeted domain were impactful on end applications (Brants\n8\net al., 2007). After neural language models were intro-\nduced (Bengio et al., 2000), they were also scaled up to\nbenefit from increasing amount of training data (Raffel\net al., 2020; Brown et al., 2020; Chowdhery et al., 2022;\nTouvron et al., 2023). This growth involves a trade-off\nbetween training a model from a large dataset (i.e. reduc-\ning estimation errors) or a dataset representative of the\nend application domain (i.e. having a training distribution\nrepresentative of test condition), both essential to good\ngeneralization (Vapnik, 1995).\nModel fine-tuning and multi-task learning have become\nessential tools in order to both benefit from large generic\ntraining data and limited in-domain data (Caruana, 1993;\nCollobert et al., 2011; Gururangan et al., 2020).\nData\ncuration and selection methods have also been proposed\nin order to resample generic data with a given application\ndomain in mind (Moore & Lewis, 2010; Wang et al., 2018;\nXie et al., 2023). Most of these methods can be tied to\nimportance sampling, an established statistical tool (Kahn\n& Harris, 1951; Grangier & Iter, 2022).\nSimultaneously with the growth in large language model\nsize, concerns about model inference cost gave rise to re-\nsearch on efficient inference. Several routes are investigated\nwith this goal, including model distillation (Hsieh et al.,\n2023a; FitzGerald et al., 2022), weight quantization (Xiao\net al., 2023; Dettmers & Zettlemoyer, 2023) and prun-\ning (Ma et al., 2023; Xia et al., 2023). Alternatively to\nthese methods, mixtures of experts have been investigated\nas a way to decouple overall model capacity and inference\nefficiency (Shazeer et al., 2017; Du et al., 2022; Clark et al.,\n2022).\n6\nConclusions\nOur work on language modeling considers a common double\npractical constraint: the lack of in-domain training data\nand a limited inference budget.\nWe consider different\nalternative strategies to leverage a large, generic, out-of-\ndomain corpus under different training cost trade-offs. In\nparticular, we distinguish the generic training cost (shared\nacross different domains) and the specialization training cost\n(specific to each domain). For a large specialization budget,\nwe recommend small models pretrained with importance\nsampling, i.e. pretraining over the generic corpus resampled\nvia importance sampling.\nFor a smaller specialization\nbudget, it is better to invest in the generic pretraining of\nhyper-networks and mixtures of experts. These asymmetric\nmodels have a large parameter count during pretraining but\ncan be instantiated as a smaller model for specialization.\nSurprisingly, distillation is not competitive at the different\ncost trade-offs we consider.\nFigure 1 summarizes our\nrecommendations.\nAs future work, we want to expand our evaluation to other\ndomains and larger model sizes and consider downstream\ntasks evaluated with different metrics. Exploring hyper-\nnetwork architectures and their conditioning variable beyond\ndocument clustering could also further improve their results.\nAcknowledgments\nWe thanks Maartje ter Hoeve, Justin Deschenaux, Skyler\nSeto, Yizhe Zhang, Navdeep Jailty and Ronan Collobert for\ntheir advice and comments.\nReferences\nAbnar, S., Saremi, O., Dinh, L., Wilson, S., Bautista, M. A.,\nHuang, C., Thilak, V., Littwin, E., Gu, J., Susskind, J.,\nand Bengio, S. Adaptivity and modularity for efficient\ngeneralization over task complexity, 2023.\nAghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrinsic di-\nmensionality explains the effectiveness of language model\nfine-tuning. In Zong, C., Xia, F., Li, W., and Navigli, R.\n(eds.), Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pp. 7319\u20137328, Online,\nAugust 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.acl-long.568. URL https:\n//aclanthology.org/2021.acl-long.568.\nAminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A.,\nLi, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase,\nO., and He, Y. Deepspeed inference: Enabling efficient\ninference of transformer models at unprecedented scale,\n2022.\nBengio, Y., Ducharme, R., and Vincent, P.\nA neural\nprobabilistic language model. In Leen, T., Dietterich,\nT., and Tresp, V. (eds.), Advances in Neural Infor-\nmation Processing Systems, volume 13. MIT Press,\n2000.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2000/file/\n728f206c2a01bf572b5940d7d9a8fa4c-Paper.\npdf.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora,\nS., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut,\nA., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D.,\nCastellon, R., Chatterji, N., Chen, A., Creel, K., Davis,\nJ. Q., Demszky, D., Donahue, C., Doumbouya, M.,\nDurmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K.,\n9\nFei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K.,\nGoodman, N., Grossman, S., Guha, N., Hashimoto, T.,\nHenderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu,\nK., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri,\nP., Karamcheti, S., Keeling, G., Khani, F., Khattab, O.,\nKoh, P. W., Krass, M., Krishna, R., Kuditipudi, R.,\nKumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J.,\nLevent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning,\nC. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair,\nS., Narayan, A., Narayanan, D., Newman, B., Nie, A.,\nNiebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr,\nL., Papadimitriou, I., Park, J. S., Piech, C., Portelance,\nE., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong,\nF., Roohani, Y., Ruiz, C., Ryan, J., R\u00e9, C., Sadigh, D.,\nSagawa, S., Santhanam, K., Shih, A., Srinivasan, K.,\nTamkin, A., Taori, R., Thomas, A. W., Tram\u00e8r, F., Wang,\nR. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M.,\nYasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang,\nT., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang,\nP. On the opportunities and risks of foundation models,\n2022.\nBrants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.\nLarge language models in machine translation. In Eisner,\nJ. (ed.), Proceedings of the 2007 Joint Conference on\nEmpirical Methods in Natural Language Processing and\nComputational Natural Language Learning (EMNLP-\nCoNLL), pp. 858\u2013867, Prague, Czech Republic, June\n2007. Association for Computational Linguistics. URL\nhttps://aclanthology.org/D07-1090.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\nWinter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners.\nIn Larochelle, H.,\nRanzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.),\nAdvances in Neural Information Processing Systems,\nvolume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nCaruana, R. Multitask learning: A knowledge-based source\nof inductive bias. In Proceedings of the Tenth Inter-\nnational Conference on Machine Learning, pp. 41\u201348.\nCiteseer, 1993.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,\nG., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,\nS., Michalewski, H., Garcia, X., Misra, V., Robinson,\nK., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,\nH., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,\nAgrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat,\nM., Lewkowycz, A., Moreira, E., Child, R., Polozov, O.,\nLee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,\nJ., Petrov, S., and Fiedel, N. Palm: Scaling language\nmodeling with pathways, 2022.\nClark, A., Casas, D. d. l., Guy, A., Mensch, A., Paganini,\nM., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T.,\nBorgeaud, S., Driessche, G. v. d., Rutherford, E., Henni-\ngan, T., Johnson, M., Millican, K., Cassirer, A., Jones,\nC., Buchatskaya, E., Budden, D., Sifre, L., Osindero,\nS., Vinyals, O., Rae, J., Elsen, E., Kavukcuoglu, K.,\nand Simonyan, K. Unified scaling laws for routed lan-\nguage models. In Proceedings of the 39th International\nConference on Machine Learning. PMLR, 2022.\nCollobert, R., Weston, J., Bottou, L., Karlen, M.,\nKavukcuoglu, K., and Kuksa, P. Natural language process-\ning (almost) from scratch. Journal of machine learning\nresearch, 12(ARTICLE):2493\u20132537, 2011.\nDettmers, T. and Zettlemoyer, L. The case for 4-bit precision:\nk-bit inference scaling laws. In Krause, A., Brunskill,\nE., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J.\n(eds.), Proceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings of\nMachine Learning Research, pp. 7750\u20137774. PMLR, 23\u2013\n29 Jul 2023. URL https://proceedings.mlr.\npress/v202/dettmers23a.html.\nDu, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu,\nY., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B.,\nFedus, L., Bosma, M. P., Zhou, Z., Wang, T., Wang, E.,\nWebster, K., Pellat, M., Robinson, K., Meier-Hellstern,\nK., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y., Chen,\nZ., and Cui, C. GLaM: Efficient scaling of language mod-\nels with mixture-of-experts. In Chaudhuri, K., Jegelka,\nS., Song, L., Szepesvari, C., Niu, G., and Sabato, S.\n(eds.), Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pp. 5547\u20135569. PMLR, 17\u2013\n23 Jul 2022. URL https://proceedings.mlr.\npress/v162/du22c.html.\n10\nEigen, D., Ranzato, M., and Sutskever, I. Learning factored\nrepresentations in a deep mixture of experts. In Bengio,\nY. and LeCun, Y. (eds.), 2nd International Conference\non Learning Representations, ICLR 2014, Banff, AB,\nCanada, April 14-16, 2014, Workshop Track Proceed-\nings, 2014. URL http://arxiv.org/abs/1312.\n4314.\nFitzGerald, J., Ananthakrishnan, S., Arkoudas, K., Bernardi,\nD., Bhagia, A., Bovi, C. D., Cao, J., Chada, R., Chauhan,\nA., Chen, L., Dwarakanath, A., Dwivedi, S., Gojayev,\nT., Gopalakrishnan, K., Gueudr\u00e9, T., Hakkani-Tur, D.,\nHamza, W., H\u00fcser, J. J., Jose, K. M., Khan, H., Liu, B.,\nLu, J., Manzotti, A., Natarajan, P., Owczarzak, K., Oz, G.,\nPalumbo, E., Peris, C., Prakash, C. S., Rawls, S., Rosen-\nbaum, A., Shenoy, A., Soltan, S., Sridhar, M. H., Tan, L.,\nTriefenbach, F., Wei, P., Yu, H., Zheng, S., T\u00fcr, G., and\nNatarajan, P. Alexa teacher model: Pretraining and distill-\ning multi-billion-parameter encoders for natural language\nunderstanding systems. In Zhang, A. and Rangwala, H.\n(eds.), KDD \u201922: The 28th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, Washington,\nDC, USA, August 14 - 18, 2022, pp. 2893\u20132902. ACM,\n2022. doi: 10.1145/3534678.3539173. URL https:\n//doi.org/10.1145/3534678.3539173.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe,\nT., Foster, C., Phang, J., He, H., Thite, A., Nabeshima,\nN., Presser, S., and Leahy, C. The pile: An 800gb\ndataset of diverse text for language modeling. CoRR,\nabs/2101.00027, 2021.\nGrangier, D. and Iter, D. The trade-offs of domain adaptation\nfor neural language models. In Muresan, S., Nakov, P.,\nand Villavicencio, A. (eds.), Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pp. 3802\u20133813. Association\nfor Computational Linguistics, 2022. doi: 10.18653/V1/\n2022.ACL-LONG.264. URL https://doi.org/\n10.18653/v1/2022.acl-long.264.\nGross, S., Ranzato, M., and Szlam, A. Hard mixtures\nof experts for large scale weakly supervised vision. In\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-26,\n2017, pp. 5085\u20135093. IEEE Computer Society, 2017. doi:\n10.1109/CVPR.2017.540.\nGururangan, S., Marasovi\u0107, A., Swayamdipta, S., Lo, K.,\nBeltagy, I., Downey, D., and Smith, N. A. Don\u2019t stop pre-\ntraining: Adapt language models to domains and tasks. In\nJurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.),\nProceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pp. 8342\u20138360, On-\nline, July 2020. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2020.acl-main.740. URL https:\n//aclanthology.org/2020.acl-main.740.\nHa, D., Dai, A. M., and Le, Q. V. Hypernetworks. In\n5th International Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net, 2017.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowl-\nedge in a neural network. CoRR, abs/1503.02531, 2015.\nURL http://arxiv.org/abs/1503.02531.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., de Las Casas, D., Hendricks,\nL. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,\nMillican, K., van den Driessche, G., Damoc, B., Guy,\nA., Osindero, S., Simonyan, K., Elsen, E., Vinyals,\nO., Rae, J. W., and Sifre, L. An empirical analysis\nof compute-optimal large language model training.\nIn Koyejo, S., Mohamed, S., Agarwal, A., Belgrave,\nD., Cho, K., and Oh, A. (eds.), Advances in Neural\nInformation Processing Systems 35: Annual Conference\non Neural Information Processing Systems 2022,\nNeurIPS 2022, New Orleans, LA, USA, November 28\n- December 9, 2022, 2022. URL http://papers.\nnips.cc/paper_files/paper/2022/hash/\nc1e2faff6f588870935f114ebe04a3e5-Abstract-Conferen\nhtml.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M.,\nand Gelly, S. Parameter-efficient transfer learning for\nNLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.),\nProceedings of the 36th International Conference on\nMachine Learning, volume 97 of Proceedings of Ma-\nchine Learning Research, pp. 2790\u20132799. PMLR, 09\u2013\n15 Jun 2019. URL https://proceedings.mlr.\npress/v97/houlsby19a.html.\nHoward, J. and Ruder, S.\nUniversal language model\nfine-tuning for text classification.\nIn Gurevych, I.\nand Miyao, Y. (eds.), Proceedings of the 56th An-\nnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 328\u2013339, Mel-\nbourne, Australia, July 2018. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/P18-1031. URL\nhttps://aclanthology.org/P18-1031.\nHsieh, C., Li, C., Yeh, C., Nakhost, H., Fujii, Y., Rat-\nner, A., Krishna, R., Lee, C., and Pfister, T. Distilling\nstep-by-step!\noutperforming larger language models\n11\nwith less training data and smaller model sizes.\nIn\nRogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.),\nFindings of the Association for Computational Linguis-\ntics: ACL 2023, Toronto, Canada, July 9-14, 2023,\npp. 8003\u20138017. Association for Computational Linguis-\ntics, 2023a. doi: 10.18653/V1/2023.FINDINGS-ACL.\n507.\nURL https://doi.org/10.18653/v1/\n2023.findings-acl.507.\nHsieh, C., Li, C., Yeh, C., Nakhost, H., Fujii, Y., Rat-\nner, A., Krishna, R., Lee, C., and Pfister, T. Distilling\nstep-by-step!\noutperforming larger language models\nwith less training data and smaller model sizes.\nIn\nRogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.),\nFindings of the Association for Computational Linguis-\ntics: ACL 2023, Toronto, Canada, July 9-14, 2023,\npp. 8003\u20138017. Association for Computational Linguis-\ntics, 2023b. doi: 10.18653/V1/2023.FINDINGS-ACL.\n507.\nURL https://doi.org/10.18653/v1/\n2023.findings-acl.507.\nHu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S.,\nWang, L., Chen, W., et al. Lora: Low-rank adaptation of\nlarge language models. In International Conference on\nLearning Representations, 2021.\nKahn, H. and Harris, T. E. Estimation of particle transmis-\nsion by random sampling. National Bureau of Standards\napplied mathematics series, 12:27\u201330, 1951.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.,\n2020.\nKarimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen-\nderson, J. Parameter-efficient multi-task fine-tuning for\ntransformers via shared hypernetworks. In Annual Meet-\ning of the Association for Computational Linguistics,\n2021.\nLester, B., Al-Rfou, R., and Constant, N. The power of\nscale for parameter-efficient prompt tuning. In Moens,\nM.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.),\nProceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 3045\u20133059,\nOnline and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.emnlp-main.243. URL https://\naclanthology.org/2021.emnlp-main.243.\nMa, X., Fang, G., and Wang, X. Llm-pruner: On the\nstructural pruning of large language models, 2023.\nMoore, R. C. and Lewis, W. Intelligent selection of language\nmodel training data. In Haji\u010d, J., Carberry, S., Clark,\nS., and Nivre, J. (eds.), Proceedings of the ACL 2010\nConference Short Papers, pp. 220\u2013224, Uppsala, Sweden,\nJuly 2010. Association for Computational Linguistics.\nURL https://aclanthology.org/P10-2041.\nMuqeeth, M., Liu, H., and Raffel, C. Soft merging of\nexperts with adaptive routing, 2023.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research, 21\n(140):1\u201367, 2020.\nReimers, N. and Gurevych, I. Sentence-BERT: Sentence\nembeddings using Siamese BERT-networks. In Inui, K.,\nJiang, J., Ng, V., and Wan, X. (eds.), Proceedings of\nthe 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\n\u0132CNLP), pp. 3982\u20133992, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi:\n10.18653/v1/D19-1410.\nRosenfeld, R. Two decades of statistical language modeling:\nwhere do we go from here? Proc. IEEE, 88(8):1270\u2013\n1278, 2000. doi: 10.1109/5.880083. URL https:\n//doi.org/10.1109/5.880083.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\nQ. V., Hinton, G. E., and Dean, J. Outrageously large\nneural networks: The sparsely-gated mixture-of-experts\nlayer.\nIn 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings. OpenReview.net,\n2017. URL https://openreview.net/forum?\nid=B1ckMDqlg.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen,\nB., Liang, P., Re, C., Stoica, I., and Zhang, C. FlexGen:\nHigh-throughput generative inference of large language\nmodels with a single GPU. In Krause, A., Brunskill,\nE., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J.\n(eds.), Proceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings of Ma-\nchine Learning Research, pp. 31094\u201331116. PMLR, 23\u2013\n29 Jul 2023. URL https://proceedings.mlr.\npress/v202/sheng23a.html.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\n12\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian,\nR., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan,\nJ. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A.,\nKambadur, M., Narang, S., Rodriguez, A., Stojnic, R.,\nEdunov, S., and Scialom, T. Llama 2: Open foundation\nand fine-tuned chat models, 2023.\nVapnik, V. N. The nature of statistical learning theory.\nSpringer-Verlag New York, Inc., 1995. ISBN 0-387-\n94559-8.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.\nAttention is all you need. In Guyon, I., Luxburg, U. V.,\nBengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc.,\n2017.\nWang, W., Watanabe, T., Hughes, M., Nakagawa, T., and\nChelba, C. Denoising neural machine translation train-\ning with trusted data and online data selection.\nIn\nBojar, O., Chatterjee, R., Federmann, C., Fishel, M.,\nGraham, Y., Haddow, B., Huck, M., Yepes, A. J.,\nKoehn, P., Monz, C., Negri, M., N\u00e9v\u00e9ol, A., Neves,\nM., Post, M., Specia, L., Turchi, M., and Verspoor,\nK. (eds.), Proceedings of the Third Conference on Ma-\nchine Translation: Research Papers, pp. 133\u2013143, Brus-\nsels, Belgium, October 2018. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/W18-6314. URL\nhttps://aclanthology.org/W18-6314.\nXia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama:\nAccelerating language model pre-training via structured\npruning, 2023.\nXiao, G., Lin, J., Seznec, M., Wu, H., Demouth,\nJ., and Han, S.\nSmoothQuant:\nAccurate and ef-\nficient post-training quantization for large language\nmodels.\nIn Krause, A., Brunskill, E., Cho, K., En-\ngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\nceedings of the 40th International Conference on Ma-\nchine Learning, volume 202 of Proceedings of Ma-\nchine Learning Research, pp. 38087\u201338099. PMLR, 23\u2013\n29 Jul 2023. URL https://proceedings.mlr.\npress/v202/xiao23c.html.\nXie, S. M., Santurkar, S., Ma, T., and Liang, P. Data\nselection for language models via importance resampling.\nCoRR, abs/2302.03169, 2023. doi: 10.48550/ARXIV.\n2302.03169. URL https://doi.org/10.48550/\narXiv.2302.03169.\nZhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on\nmodel compression for large language models, 2023.\n13\nAppendix\nA\nHyper-parameters\nAll our language model are either instances of SLM or LLM.\nWe rely on the parameters from Table 6. Table 7 extends\nTable 1 to include the parameter count for the models from\nall the sections.\nB\nInterpolated Perplexities\nWe report the data from the Figures 2 \u2013 3 in Table 8. Since\nthe methods were evaluated at a fixed frequency in steps,\nwe linearly interpolate perplexities and step counts to report\nresults at the same pretraining costs for all methods.\nC\nNumber of Fine-tuning Steps\nFigure 10 reports the fine tuning cost each model. This\ncost corresponds to the number of steps to reach the best\nvalidation perplexity. It is an optimistic cost estimates as\none usually needs a few more steps to assess that further\nimprovement is not expected. The fine-tuning cost seems\nto grow \u223c 10X when the fine-tuning set size grows 8X.\nThe LLM usually requires less steps than the SLMs but its\nsteps are more expensive. The vanilla SLM overfits earlier\nthan the other SLMs (SLM-mix, SLM-hn) for the small 1M\nspecialization set but not for the larger sets.\nD\nClustering\nThe clustering of c4 is used by the mixture model to define\neach expert scope. Similarly it is used as the condition-\ning variable by the hyper-network. Finally it is used by\nTable 6: Transformer parameters\nSLM\nLLM\nArchitecture\nMum. layers\n7\n7\nModel dimension\n1024\n2816\nInner MLP dimension\n4096\n11264\nNum. attention heads\n8\n22\nOptimizer\nOptimizer\nAdam\nAdam\nLearning rate\n1e-4\n1e-4\nClipping norm\n5.0\n5.0\nLinear warmum steps\n1,000\n1,000\nTable 7: Number of parameters (in millions) for pretrain-\ning and inference.\nModel\nNum. parameters (m).\nOverall\nInference\nSLM\n126\n126\nSLM-hn\n16\nexperts\n756\n126\n32\n1,422\n126\n64\n2,770\n126\nSLM-mix\n4\nexperts\n504\n126\n16\n2,016\n126\n64\n8,064\n126\n256\n32,256\n126\nLLM\n771\n771\nimportance sampling to resample c4. Table 9 report the\nconcentration of each specialization domain from Pile on\ntheir most frequent cluster. A high concentration could\nbe positive since it means that, when fine-tuning SLM-hn\nor SLM-mix conditioned on this cluster, one starts starts\nfrom pretrained weights containing most of the pretrain-\ning data relevant to the domain at hand. The table also\nreports the most frequent cluster on c4 to highlight that\nthe specialization domain distributions differ from the c4\ndistribution.\nE\nIndividual Subset Results\nFigure 11 decomposes the results in Figure 3 (b) per domain.\nThe subset results are mostly consistent with the average\nbut we observe few differences. SLM-hn and SLM-mix\nhave a close average and the best method among them\nvaries per subset. Also we notice that both methods do\nnot outperform SLM on wikipedia and openwebtext2. The\ndisadvantage of SLM-hn and SLM-mix over SLM can be\nobserved before fine-tuning, as shown on Figure 12. We\nreport the entropy of the cluster histograms in Table 10 and\nobserve that wikipedia and openwebtext2 are the domains\nwith the highest entropy.\nThis means that the c4 data\nsimilar to these datasets is more spread across clusters\nthan for the other domains. Conditioning SLM-hn and\nSLM-mix on a single cluster variable might not model well\nthese domains. Of course, this correlation between entropy\nand fine-tuned perplexity of SLM-mix, SLM-hn could be\nfortuitous. This motivates us to investigate the impact of\nthe different clustering methods and their metrics in future\nresearch.\n14\nTable 8: Interpolated perplexities at fixed pretraining costs (GPUh)\nModel\nPretrain\nNum.\nNum.\nGeneric\nSpecific PPL\ncost\nsteps\nGPU\nPPL\nNo ft\n1M\n8M\n64M\nSLM\n100\n798k\n8\n20.51\n33.74\n19.31\n15.61\n12.37\nSLM-mix\n100\n464k\n16\n17.13\n34.35\n19.82\n15.82\n12.62\nSLM-hn\n100\n195k\n8\n18.90\n33.44\n18.57\n15.58\n12.53\nLLM\n100\n108k\n8\n17.00\n29.22\n17.11\n15.49\n11.55\nSLM\n200\n1597k\n8\n19.71\n34.43\n18.58\n15.12\n12.09\nSLM-mix\n200\n928k\n16\n15.92\n31.94\n18.48\n14.98\n12.15\nSLM-hn\n200\n390k\n8\n17.74\n32.30\n17.76\n14.95\n12.13\nLLM\n200\n217k\n8\n15.58\n28.18\n15.62\n14.03\n10.81\nSLM\n400\n3195k\n8\n19.17\n36.61\n18.22\n14.80\n12.00\nSLM-mix\n400\n1000k\n16\n15.82\n31.04\n17.56\n14.42\n11.84\nSLM-hn\n400\n780k\n8\n16.90\n32.54\n17.17\n14.48\n11.86\nLLM\n400\n434k\n8\n14.54\n28.98\n15.03\n13.05\n10.28\nSLM-mix\n600\n1000k\n16\n15.82\n31.03\n17.18\n14.21\n11.73\nSLM-hn\n600\n1170k\n8\n16.53\n32.53\n16.95\n14.29\n11.74\nLLM\n600\n651k\n8\n14.09\n28.62\n14.50\n12.64\n10.07\nTable 9: Fraction of data in the most frequent cluster, per\ndomain.\nDomain\nNum. clusters\n4\n16\n64\n256\n1024\narxiv\n0.95\n0.92\n0.55\n0.52\n0.29\neuroparl\n0.52\n0.53\n0.45\n0.44\n0.27\nfreelaw\n0.48\n0.73\n0.87\n0.72\n0.35\ngutenberg\n0.75\n0.54\n0.35\n0.27\n0.29\nopensubtitles\n0.97\n0.68\n0.26\n0.28\n0.32\nopenwebtext2\n0.53\n0.35\n0.12\n0.04\n0.02\npubmed abs.\n0.94\n0.54\n0.41\n0.20\n0.06\nstackexchange\n0.95\n0.94\n0.78\n0.61\n0.31\nwikipedia\n0.71\n0.58\n0.21\n0.07\n0.03\nc4\n0.32\n0.12\n0.04\n0.02\n0.00\nTable 10: Entropy of the cluster histogram for each\ndomain.\nDomain\nNum. clusters\n16\n64\n256\n1024\narxiv\n0.41\n1.02\n1.80\n2.58\neuroparl\n1.48\n1.83\n2.31\n3.14\nfreelaw\n1.01\n0.70\n1.44\n2.49\ngutenberg\n1.57\n2.42\n3.21\n3.85\nopensubtitles\n1.16\n2.61\n2.95\n3.44\nopenwebtext2\n2.19\n3.60\n4.89\n6.12\npubmed abs.\n1.07\n2.14\n3.22\n4.43\nstackexchange\n0.39\n0.97\n1.78\n3.24\nwikipedia\n1.73\n3.20\n4.54\n5.64\nc4\n2.73\n4.07\n5.46\n6.85\n15\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\n0.0225\nSpecialization cost (GPUh)\nSLM     \nSLM-mix     \nSLM-hn     \nLLM     \n(a) 1M tokens\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nSpecialization cost (GPUh)\n(b) 8M tokens\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nSpecialization cost (GPUh)\n(c) 64M tokens\nFigure 10: Fine tuning cost as a function of the pretraining cost.\nF\nParameter Efficient Fine-tuning\nWe also evaluate Low Rank Adaptation (LoRA) (Hu et al.,\n2021) as a fine-tuning method for the LLM. LoRA can help\nregularize the fine-tuning process when little specialization\nis available. It also reduces the storage and communication\ncosts of managing many specialized models when address-\ning many domains since only few parameters are learned\nfor each domain. LoRA does not reduce the pretraining\ncost, and even increases the fine-tuning cost as it requires\nmore fine-tuning steps, with a similar cost per step. In our\nLoRA experiments we use low-rank matrices of rank 64\nwhich results in 5M trainable parameters and fine-tune for\nup to 5\u00d7 more steps than for the LLM. We observe that\nLLM-lora required from 25% more steps than the LLM for\nthe 1M token dataset and 3\u00d7 more steps for the 64M token\ndataset. However, since the specialization cost is negligible\nin comparison to the pretraining cost these extra steps do\nnot really impact the overall training cost. Figure 13 reports\nthe results. LoRA performs very similarly to the LLM (dif-\nferences of less than 0.5 perplexity) and with the exception\nof the \"large\" domain-specific regime of 64M tokens we\ncan observe some ovefitting mitigation. Finally, LoRA still\nresults in a large model which is not suitable for the cases\nwhere the computational budget for inference is small.\n16\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n12\n13\n14\n15\n16\nSpecific perplexity\nSLM     \nSLM-mix     \nSLM-hn     \nSLM-is     \n(a) Arxiv\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n15\n20\n25\n30\n35\n40\n45\nSpecific perplexity\n(b) Europarl\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n10\n11\n12\n13\n14\n15\n16\n17\nSpecific perplexity\n(c) Freelaw\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\nSpecific perplexity\n(d) Gutenberg\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n16\n17\n18\n19\n20\n21\nSpecific perplexity\n(e) Opensubtitles\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\nSpecific perplexity\n(f) Openwebtext2\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n9\n10\n11\n12\n13\n14\n15\nSpecific perplexity\n(g) Pubmed abstracts\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n12\n14\n16\n18\n20\nSpecific perplexity\n(h) Stackexchange\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n14\n16\n18\n20\n22\n24\n26\nSpecific perplexity\n(i) Wikipedia\nFigure 11: Specific perplexity on individual subsets after fine-tuning on 1M tokens.\n17\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n100\n150\n200\n250\nSpecific perplexity\nSLM     \nSLM-mix     \nSLM-hn     \nSLM-is     \n(a) Arxiv\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n20\n30\n40\n50\n60\nSpecific perplexity\n(b) Europarl\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n16\n18\n20\n22\n24\n26\n28\nSpecific perplexity\n(c) Freelaw\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n60\n65\n70\n75\n80\nSpecific perplexity\n(d) Gutenberg\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n18\n20\n22\n24\n26\nSpecific perplexity\n(e) Opensubtitles\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n25\n30\n35\n40\n45\nSpecific perplexity\n(f) Openwebtext2\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n10\n12\n14\n16\nSpecific perplexity\n(g) Pubmed abstracts\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nSpecific perplexity\n(h) Stackexchange\n0\n200\n400\n600\nGeneric pretraining cost (GPUh)\n25\n30\n35\n40\nSpecific perplexity\n(i) Wikipedia\nFigure 12: Specific perplexity on individual subsets after fine-tuning on 1M tokens.\n18\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n14.5\n15.0\n15.5\n16.0\n16.5\n17.0\nSpecific perplexity\nLLM-lora\nLLM\n(a) 1M tokens\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n12.0\n12.5\n13.0\n13.5\n14.0\n14.5\n15.0\n15.5\nSpecific perplexity\nLLM-lora\nLLM\n(b) 8M tokens\n100\n200\n300\n400\n500\n600\nGeneric pretraining cost (GPUh)\n10.00\n10.25\n10.50\n10.75\n11.00\n11.25\n11.50\n11.75\nSpecific perplexity\nLLM-lora\nLLM\n(c) 64M tokens\nFigure 13: Specific perplexity of LoRA fine-tuning on the Pile subsets with respect to the pretraining cost. We\nobserve that LoRA fine-tuning performs very similarly to traditional fine-tuning with less than 0.5 perplexity\ndifferences.\n19\n"
  },
  {
    "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
    "link": "https://arxiv.org/pdf/2402.01391.pdf",
    "upvote": "41",
    "text": "StepCoder: Improve Code Generation\nwith Reinforcement Learning from Compiler Feedback\nShihan Dou1*\u2020, Yan Liu1\u2217, Haoxiang Jia2, Limao Xiong1, Enyu Zhou1, Wei Shen1,\nJunjie Shan3, Caishuang Huang1, Xiao Wang1, Xiaoran Fan1, Zhiheng Xi1,\nYuhao Zhou1, Tao Ji1, Rui Zheng1\u2020, Qi Zhang1\u2020, Xuanjing Huang1, Tao Gui1\u2020\n1 Fudan NLP Lab, Fudan University, China\n2 Huazhong University of Science and Technology, China\n3 KTH Royal Institute of Technology, Sweden\nAbstract\nThe advancement of large language models\n(LLMs) has significantly propelled the field of\ncode generation. Previous work integrated re-\ninforcement learning (RL) with compiler feed-\nback for exploring the output space of LLMs\nto enhance code generation quality. However,\nthe lengthy code generated by LLMs in re-\nsponse to complex human requirements makes\nRL exploration a challenge. Also, since the\nunit tests may not cover the complicated code,\noptimizing LLMs by using these unexecuted\ncode snippets is ineffective. To tackle these\nchallenges, we introduce StepCoder, a novel\nRL framework for code generation, consisting\nof two main components: CCCS addresses the\nexploration challenge by breaking the long se-\nquences code generation task into a Curriculum\nof Code Completion Subtasks, while FGO only\noptimizes the model by masking the unexe-\ncuted code segments to provide Fine-Grained\nOptimization. In addition, we furthermore con-\nstruct the APPS+ dataset for RL training, which\nis manually verified to ensure the correctness\nof unit tests. Experimental results show that\nour method improves the ability to explore the\noutput space and outperforms state-of-the-art\napproaches in corresponding benchmarks. Our\ndataset APPS+ and StepCoder are available on-\nline 1.\n1\nIntroduction\nCode generation or program synthesis aims to au-\ntomatically generate source code that adheres to a\nspecified programming requirement, which is typ-\nically described in natural language [36; 7]. Re-\ncently, with the development of large language\nmodels (LLMs), techniques based on LLM [15; 37;\n23] have demonstrated impressive ability in code\ngeneration. However, challenges persist in aligning\n* Equal contributions.\n\u2020 Correspondence to: shdou21@m.fudan.edu.cn, {rzhen\ng20, qz, tqui}@fudan.edu.cn\n1https://github.com/Ablustrund/APPS_Plus\nthese models with complex human requirements\n[2; 10; 27], indicating a gap that still exists in fully\nmeeting user expectations.\nIn this context, learning from compiler feedback\nexhibits impressive potential to improve the com-\nprehension of complicated human requirements\nand the quality of generated codes [14]. This feed-\nback from compilation and execution results is in-\nstrumental in directly ascertaining the functional\ncorrectness of programs [17; 39].\nResearchers\n[20; 33] introduce reinforcement learning (RL) and\nleverage compiler feedback from unit tests as a re-\nward metric to guide the exploration of the output\nspace of LLMs. The intention is for the policy\nmodel to favor actions that yield higher rewards in-\ncreasingly. Nevertheless, the optimization of LLMs\nfor code generation via RL presents several hur-\ndles. First, the increasing complexity of human re-\nquirements often results in the generation of longer\ncode sequences, which makes exploration strug-\ngle [9; 13]. Second, in cases where a single unit\ntest fails to cover the complex code, unexecuted\ncode snippets may emerge that are not relevant to\nthe reward. Rendering optimization based on the\nentire code sequence is potentially imprecise. Ad-\nditionally, our analysis reveals quality limitations\nin existing datasets like APPS [10] for RL training,\nwhich impedes accurate learning from compiler\nfeedback through RL.\nTo tackle these challenges, we first introduce\nStepCoder, an innovative framework developed for\nenhancing code generation through reinforcement\nlearning. StepCoder integrates two key compo-\nnents: Curriculum of Code Completion Subtasks\n(CCCS) and Fine-Grained Optimization (FGO).\nCCCS is designed to alleviate the complexities as-\nsociated with exploration in code generation, while\nFGO is designed to provide more precise and ef-\nfective optimization strategies. Specifically, CCCS\nemploys a step-by-step strategy to break down com-\nplex exploration problems (i.e., code generation)\narXiv:2402.01391v2  [cs.SE]  5 Feb 2024\ninto a curriculum of easier sub-tasks (i.e., code\ncompletion). As the training progresses, the diffi-\nculty of code completion tasks rises by increasing\nthe portion of code that needs to be completed.\nEventually, the aim is for the model to evolve to a\nstage where it can effectively generate code solely\nfrom human requirements, thus fulfilling the origi-\nnal training goal of code generation. On the other\nhand, the key insight of FGO is that code snippets\nthat are not executed in a unit test do not contribute\nto the final reward calculation. Therefore, FGO\nuses a dynamic masking technique to mask unexe-\ncuted snippets from unit test evaluations, ensuring\nthat the model is optimized utilizing only the rele-\nvant code segments.\nSubsequently, our endeavor involves the devel-\nopment of APPS+, a dataset of superior quality\nspecifically curated for code generation. APPS+ is\nmeticulously designed to exclude code segments\nthat exhibit syntax errors, are irrelevant to the stip-\nulated problem, or fail to produce any output. Ad-\nditionally, we have taken measures to standardize\nthe format of inputs and outputs in unit tests to\nguarantee deterministic output comparisons.\nWe evaluate the effectiveness of popular LLMs\non APPS+. The results reveal that although LLMs\nshow progressive improvements, they face difficul-\nties with complex human requirements. We fur-\nther evaluate our method on several extensively\nused benchmarks including MBPP [2] and Hu-\nmanEval [3]. The experimental results show that\nStepCoder effectively eases the exploration diffi-\nculty in code generation, outperforming other rein-\nforcement learning-based methods in effectiveness.\nThe main contributions of our paper are as follows:\n\u2022 We introduce StepCoder, a novelty training\nmethod via RL, including CCCS and FGO.\nCCCS makes exploration easier by break-\ning down the complicated goals into sub-\nobjectives curriculum. FGO provides fine-\ngrained optimization by only utilizing the ex-\necuted code in unit tests.\n\u2022 We constructed APPS+, a high-quality dataset\ndesigned for code generation. APPS+ pro-\nvides a more rigorous evaluation of LLMs\u2019\ncapabilities and a foundation to introduce re-\ninforcement learning in the training phase.\n\u2022 Experiments show that StepCoder can im-\nprove the exploration efficiency and effective-\nness and outperform other methods.\nimport random\ndef test():\n...\nfor _ in range(int(input())):\n\u2026\nrows[0] = p[::2]\nrows[1] = p[1::2]\nif sign(rows[0][0]) != sign(rows[1][0]):\nprint(0)\ncontinue\nfor r in range(2, max_rows):\nfor n in range(max_col - 1):\nrows[r][n] = rows[r - 1][0] * rows[r - 2][n + \n1] - rows[r - 2][0] * rows[r - 1][n + 1]\nlast = sign(rows[0][0])\nflag = 1\nfor i in range(1, len(rows)):\ncurr = sign(rows[i][0])\nif rows[r] == [0 for _ in range(max_col)]:\nfor n in range(max_col):\nrows[r][n] = rows[r - 1][n] * (max_pow + \n4 - (r + 1) - 2 * (n + 1))\nelif rows[i][0] == 0:\nif any([x != 0 for x in rows[i]]):\nflag = 0\nbreak\nelse:\ncurr = last\nif curr != last:\nflag = 0\nbreak\nlast = curr\n: conditional statement\n: executed code\n: unexecuted code\nFigure 1: The canonical solution of an instance in the\nAPPS dataset. We collect the conditional statements\nby analyzing their abstract syntax tree, and some con-\nditional statements are highlighted with a grey dashed\nbox. When inputting s = [1\\n10 12 1 5 3\\n], only 75%\nof the code fragment is executed, highlighted with a\ngreen background.\n2\nMotivation\nIn this section, we clearly illustrate the challenges\nfaced by reinforcement learning in code generation\nusing a simplified example from APPS [10], which\nwas widely used for RL training in code generation.\nExploration problems of RL in code genera-\ntion. Exploration methods play a crucial role in\ntackling complicated sequence but sparse reward\nproblems [43; 13]. When a policy model explores a\ntrajectory with high returns, it undergoes optimiza-\ntion, making it inclined to take similar actions in\nthe future [41; 28].\nConsider the code shown in Figure 1, aimed\nat fulfilling a given human requirement. We first\ncollect the conditional statements (CS) that are in-\ndicated by the dashed box by analyzing its abstract\nsyntax tree. Conditional statement introduces new\nindependent paths, increasing the complexity of\nthe program [32]. Suppose P\u03b8(CSi) denotes the\nprobability that the policy model with parameter \u03b8\ncompletes the i-th conditional statement. The prob-\nability that the policy model correctly generates\nthis code according to human requirements can be\nexpressed as follows:\nP \u221d Po\n3\nY\ni=1\nP\u03b8(CSi),\n(1)\nwhere Po is the probability of other code snippets\nexcept the code labeled in the figure. Typically, we\ninitialize the policy model with the SFT model in\nsequence generation tasks to facilitate easier explo-\nration [26; 45]. However, the limited performance\nof the SFT model in code generation still leads to\nthe probability P\u03b8(CSi) at low values [33; 27]. The\nincreasing complexity of human requirements in\ncode generation tasks often leads to a correspond-\ning rise in the number of conditional statements.\nThis escalation can result in a substantial decrease\nin the probability P\u03b8(CSi), potentially leading P to\nan exponential reduction. Such a scenario exacer-\nbates the challenges associated with exploration in\nlarge language models. An alternative approach to\nfacilitate exploration is through reward shaping, a\ntechnique where designers artificially introduce re-\nwards more frequently [13]. However, this method\nencounters a significant limitation in the context\nof our application. Specifically, in code generation\ntasks utilizing unit test feedback, rewards can only\nbe obtained after the execution of the completely\ngenerated code. Consequently, the exploration of\nhigh-return trajectories in tasks with complex se-\nquences and sparse rewards poses a significant chal-\nlenge in optimizing the policy model.\nOptimization problems of RL in code genera-\ntion. We first introduce the RL fine-tuning process\nin code generation. Formally, for a learned policy\nmodel \u03c0\u03b8 with parameter \u03b8, we treat the prediction\nof each token as an action a taken by \u03c0\u03b8 according\nto the history token sequences. The history token\nsequences can be viewed as the state s. Given a\nhuman requirement x, we denote the solution code\ny generated by \u03c0\u03b8 as an episode, and r(x, y) is the\nreward function from the compiler based on com-\npilation and execution. Updating the parameters of\n\u03c0\u03b8 by using gradient policy algorithm [35] can be\nrepresented as follows:\nmax\n\u03b8\nE(x,y)\u223cD\u03c0\u03b8 [\nX\nt\nAt\n\u03c0 log(yt|y1:t\u22121, x; \u03b8)] (2)\nwhere A\u03c0 is the advantage computed by the Gen-\neralized Advantage Estimator (GAE) [29] from\nreward r, to reduce the variability of predictions.\nIn code generation, rewards are contingent upon\nthe correctness of the unit test sample, which is only\nrelevant to the code snippet being executed. For\ninstance, as shown in Figure 1, when the input to\nthe function is [1\\n10 12 1 5 3\\n], 75% of the code\nfragment is executed, which is highlighted with a\ngreen dashed box. It indicates that some actions in\nthe code are irrelevant to the reward, which leads\nto inaccurate advantage. Therefore, optimizing the\npolicy model \u03c0\u03b8 with all actions is ineffective by\nusing Equation 2.\n3\nMethod\nIn this section, we elaborate on the methodological\ndetails of StepCoder, which provide an easier ex-\nploration and fine-grained optimization for RL in\ncode generation, respectively, as shown in Figure 2.\n3.1\nPriliminaries\nSuppose D = {(xi, yi, ui, ei)}N\ni=0 is the training\ndataset for code generation, which x, y, u denotes\nthe human requirement (i.e., the task description),\nthe canonical solution and the unit test samples,\nrespectively. ei = {stj, enj}Ei\nj=0 is a list of condi-\ntional statements by automatically analyzing the\nabstract syntax tree of the canonical solution yi,\nwhich st and en represent the start position and the\nend position of the statements, respectively. e is\nsorted in ascending order based on the start posi-\ntion st. For a human requirement x, its canonical\nsolution y can be represented as {at}T\nt=0. In code\ngeneration, given a human requirement x, the final\nstates are the set of codes passing the unit tests u.\n3.2\nStepCoder\nStepCoder integrates two key components: CCCS\nand FGO. CCCS is designed to break the code\ngeneration tasks into a curriculum of the code com-\npletion subtasks. It can alleviate the exploration\nchallenge in RL. FGO is specifically designed for\ncode generation tasks to provide fine-grained opti-\nmization by computing only the loss of executed\ncode snippets.\nCCCS. In code generation, the solution to a com-\nplicated human requirement usually involves a long\naction sequence taken by the policy model. Mean-\nwhile, the feedback from the compiler is delayed\nand sparse, i.e., the policy model only receives the\nreward after generating the entire code. In this sce-\nnario, exploring is difficult. The core of our method\nis to break down such a long sequence of explo-\nration problems into a curriculum of short, easily\nexplorable sub-tasks. We simplify code genera-\ntion to code completion sub-tasks. These sub-tasks\nVanilla Reinforcement Learning \nfrom Compiler Feedback\nStepCoder\nStep 1\nFinal\n.\n.\n.\n.\n.\n.\nHuman requirement\nLLM\nCompiler\nReward\nFGO\nCCCS\nGenerated code\nMask\nMask\n! log(\ud835\udc66!|\ud835\udc66\":!$\", \ud835\udc65\n!\n)\n! log(\ud835\udc66!|\ud835\udc66\":!$\", \ud835\udc65\n!\n)\nCode Completion\nCode Generation\nCode Completion\nCode Generation\nPart of canonical solution\nFigure 2: The overview of our method. In code generation, the environment with sparse and delayed rewards and\nthe complicated human requirement that involves a long sequence make exploration challenging for the Vanilla RL.\nIn CCCS, we break down a complicated exploration problem into a curriculum of sub-tasks. Utilizing a portion of\nthe canonical solution as the prompt enables the LLM to explore starting from simple sequences. The computation\nof rewards is only relevant for the executed code snippets, and it is imprecise to optimize the LLM with the entire\ncode (i.e.,\n). In FGO, we mask unexecuted tokens (i.e.,\n) in unit tests and only compute the loss function using\nexecuted tokens (i.e.,\n) to provide a fine-grained optimization.\nare automatically constructed from the canonical\nsolution in the training dataset.\nConsider a human requirement x, early in the\ntraining phase of CCCS, the starting point s\u2217 of\nexploration is the states near the final states. Specif-\nically, we provide the human requirement x and the\nfront part of the canonical solution xp = {ai}s\u2217\ni=0,\nand the policy model is trained to complete the\ncode based on x\n\u2032 = (x, xp). Let \u02c6y be the com-\nbined sequence of xp and the output trajectory \u03c4,\ni.e. \u02c6y = (xp, \u03c4). The reward model provides the\nreward r according to the correctness of the code\nsnippet \u03c4 with \u02c6y as input, where we use the same\nsetting as previous approaches [14; 33] as follows:\nr(x\n\u2032, \u02c6y) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n+ 1, if \u02c6y passed all unit tests\n\u22120.3, if \u02c6y failed any unit test\n\u22120.6, if \u02c6y happened runtime error\n\u2212 1, if \u02c6y happened compile error.\n(3)\nWe use the Proximal Policy Optimization (PPO)\nalgorithm [30] to optimize the policy model \u03c0\u03b8 by\nutilizing the reward r and the trajectory \u03c4. In the\noptimization phase, the canonical solution\u2019s code\nsegment xp used for providing prompts is masked,\nsuch that it does not contribute to the gradient for\nthe policy model \u03c0\u03b8 update. CCCS optimizes the\npolicy model \u03c0\u03b8 by maximizing the objection func-\ntion as follows:\nObjective(\u03b8) = E(x\u2032,\u02c6y)\u223cD\u03c0\u03b8 [r(x\n\u2032, \u02c6y)\n\u2212 \u03b2 log(\u03c0\u03b8(\u02c6y|x\n\u2032))/\u03c0ref(\u02c6y|x\n\u2032)]\n(4)\nwhere \u03c0ref is the reference model in PPO, which is\ninitialized by the SFT model.\nAs the training progresses, the starting point s\u2217\nof exploration gradually moves towards the begin-\nning of the canonical solution. Specifically, we set\na threshold \u03c1 for each training sample. Each time\nthe cumulative correct proportion of code segments\ngenerated by \u03c0\u03b8 is greater than \u03c1, we move the\nstarting point toward the beginning. In the later\nstages of training, the exploration of our method\nis equivalent to the exploration process of origi-\nnal reinforcement learning, i.e., s\u2217 = 0, where\nthe policy model generates code using only human\nrequirements as input.\nThe starting point s\u2217 is sampled at the beginning\nposition of the conditional statements to complete\nthe remaining unwritten code segments. Specifi-\ncally, a program with a greater number of condi-\ntional statements results in increased independent\npaths, leading to a higher logical complexity [32].\nThis complexity necessitates more frequent sam-\npling to improve the quality of training, while pro-\ngrams with fewer conditional statements need less\nfrequent sampling. This sampling method allows\nfor a balanced and representative sampling of code\nstructures, catering to both complex and simple\nsemantic constructs in the training dataset. To ac-\ncelerate the training phase, we set the i-th sample\u2019s\nnumber of curricula equal to \u2308\u221aEi\u2309, where Ei is\nits number of conditional statements. The i-th sam-\nple\u2019s stride of the training curriculum is \u2308\nEi\n\u2308\u221aEi\u2309\u2309\ninstead of one.\nThe key insight of CCCS can be summarized as\nfollows: 1) It is easy to explore from the states near\nthe goal (i.e., final states). 2) Exploring starting\nfrom the states distant from the goal is challenging,\nbut it becomes easier when can leverage states that\nhave already learned how to reach the goal.\nFGO. The relationship between reward and ac-\ntion in code generation differs from other reinforce-\nment learning tasks such as Atari [25; 19]. In code\ngeneration, we can exclude a set of actions irrele-\nvant to computing the rewards in generated code.\nSpecifically, as mentioned in Section 2, for a unit\ntest, the feedback from the compiler relates only\nto the code snippets being executed. However, in\nvanilla RL optimization objectives, as shown in\nEquation 4, all actions of the trajectory are engaged\nin the computation of the gradient used in the policy\nupdate, which is imprecise.\nTo improve the precision of optimization, we\nmask actions (i.e., tokens) that are not executed\nin unit tests when computing the loss for updating\nthe policy model. The full algorithm of CCCS and\nFGO is detailed in Algorithm 1.\n4\nExperiments\nIn this section, we first introduce APPS+, a high-\nquality dataset for code generation by manually\nverifying based on the APPS dataset. Then, we\nelaborate on the experiment details and the experi-\nmental results.\n4.1\nDataset Preprocessing\nReinforcement learning requires an amount of high-\nquality training data. During our investigation,\nwe found that among the currently available open-\nsource datasets, only APPS meets this requirement.\nHowever, we found there are incorrect instances,\nsuch as missing input, output, or canonical solu-\ntion, canonical solutions that were uncompileable\nor unexecutable, and discrepancies in execution\noutput.\nTo refine the APPS dataset, we excluded in-\nstances lacking input, output, or canonical solu-\ntions. Then, we standardized the formats of input\nand output to facilitate the execution and compari-\nson of unit tests. We conducted unit tests and man-\nual analysis for each instance, eliminating those\nwith incomplete or irrelevant code, syntax errors,\nAPI misuse, or missing library dependencies. For\ndiscrepancies in output, we manually reviewed the\nproblem description, correcting the expected output\nor eliminating the instance.\nFinally, we construct the APPS+ dataset, con-\ntaining 7,456 instances. Each instance includes\na programming problem description, a canonical\nsolution, a function name, unit tests (i.e., inputs\nand outputs), and starter code (i.e., the beginning\npart of the canonical solution). Appendix A illus-\ntrates an example from APPS+. The top section of\nthe figure shows the problem description, and the\nright section presents the canonical solution, unit\ntests, and metadata. Further details of APPS+ are\ndiscussed in Appendix B.1.\n4.2\nExperiment Details\nBenchmarks. In our study, we initially evaluated\nour method and baselines on our pre-processed\nAPPS+ dataset. Moreover, we also evaluate these\nmethods on several widely-used benchmarks in\ncode generation, i.e., MBPP (Mostly Basic Pro-\ngramming Problems) [2] and HumanEval [3]. We\nevaluate the MBPP and HumanEval benchmark in\na zero-shot learning setting which is the same as\nprevious approaches [14; 33]. In this setting, we\nfine-tune the models only on the APPS+ dataset\nand evaluate the code generation performance on\nMBPP and HumanEval. The detailed description\nof benchmarks can be found in the Appendix B.1.\nBaselines. To verify the effectiveness of Step-\nCoder and evaluate the performance of LLMs on\nour APPS+ dataset, we consider a wide range of\nbaselines, including StarCoder [15], WizardCoder\nModels\nSize\nAPPS+\nIntroductory\nInterview\nCompetition\nOverall\nBase Models\nCodeLlama [27]\n13B\n18.7\n11.0\n0.0\n13.0\nCodeLlama-Python [27]\n13B\n29.0\n12.3\n2.9\n17.9\nDeepSeek-Coder-Base [8]\n6.7B\n13.0\n10.3\n5.0\n10.9\nSupervised Fine-tuned Models\nStarCoder [15]\n15.6B\n6.3\n4.1\n0.7\n4.7\nCodeLlama-Instruct [27]\n13B\n33.3\n11.0\n1.4\n18.7\nWizardCoder-Python-V1.0 [23]\n13B\n39.7\n15.1\n4.3\n23.6\nDeepSeek-Coder-Instruct [8]\n6.7B\n49.4\n18.7\n3.6\n29.2\nSFT on APPS+\n6.7B\n50.1\n19.0\n6.4\n29.8\nReinforcement Learning-based Models (Using DeepSeek-Coder-Instruct-6.7B as the backbone)\nVanilla PPO\n6.7B\n53.7\n20.1\n5.0\n31.7\nPPOCoder [33]\n6.7B\n54.4\n20.3\n6.4\n32.1\nRLTF [20]\n6.7B\n55.1\n20.8\n6.4\n32.7\nStepCoder (Ours)\n6.7B\n59.7\n23.5\n8.6\n36.1\nw/o CCCS\n6.7B\n58.7\n21.7\n7.1\n34.6\nw/o FGO\n6.7B\n58.4\n23.3\n8.6\n35.5\nTable 1: Results of pass@1 on our proposed APPS+. We compare popular and widely used state-of-the-art methods\nwith our method. To ensure a fair comparison, we apply these RL-based methods using the same base model (i.e.,\nDeepSeek-Coder-Instruct-6.7B [8]) as a backbone on the APPS+ dataset. In addition, We conduct supervised\nfine-tuning using our APPS+ dataset based on DeepSeek-Coder-Instruct-6.7B to further validate the effectiveness\nand necessity of our approach.\n[23], DeepSeek-Coder [8], and three versions of\nCodeLlama (Base, Python, Instruct) [27]. More-\nover, we also consider vanilla PPO and two state-of-\nthe-art RL-based approaches, including PPOCoder\n[33] and RLTF [20]. We carried out experiments\napplying these methods utilizing the same back-\nbone (i.e., DeepSeek-Coder-Instruct [8]) on the\nAPPS+ dataset to ensure a fair comparison. In\naddition to demonstrating the necessity and effec-\ntiveness of our method, we also supervised fine-\ntuning DeepSeek-Coder-Instruct [8] on the APPS+\ndataset to exclude the effect of training data. The\ndetailed description of these baselines is discussed\nin Appendix B.2.\nImplementation Details. During the SFT phase,\nwe adopt a learning rate set at 2e\u22125, conduct train-\ning for three epochs, and employ a warm-up period\nof 0.3 epochs, with a linear decay to zero. The fine-\ntuning process was conducted on a device with\neight NVIDIA A100 80G GPUs, with the global\nbatch size set to 64. In the PPO training phase,\nwe employ a learning rate of 5e\u22127 for the policy\nmodel and 1.5e\u22126 for the critic model. For each ex-\nample, we collect a 16 roll-out code using nucleus\nsampling. The sampling temperature is set to 0.8,\ntop-p is set to 0.9, and the maximum output token\nlength is set to 1024. The token-level KL penalty\ncoefficient \u03b2 is set to 0.05, with a clip value of 0.8.\nIn the decoding phase, the temperature and top_p\nare set to 0.2 and 0.95, respectively.\nEvaluation & Metric.\nWe conduct the ex-\nperiments based on Python3.x.\nNote that we\nalso use Python3.x during the reward collection\nin RL-based methods.\nFollowing prior studies\n[27; 23; 14], we use Pass@k [3] metric to evaluate\nall the models. Pass@k quantifies the proportion of\ninstances in which at least one of the k-generated\ncode solutions per human requirement successfully\npasses all unit tests. The prompts used for code\ngeneration are listed in Appendix D.\n4.3\nExperimental Results on APPS+\nTo assess the performance of widely used LLMs\nand our StepCoder on code generation, we conduct\nexperiments on the APPS+ dataset that we con-\nstructed. The experimental results are illustrated in\nTable 1. The results indicate that RL-based models\noutperform other language models, including both\nbase models and SFT models. It is reasonable to in-\nfer that reinforcement learning can further enhance\nthe quality of code generation by more effectively\nnavigating the model\u2019s output space, guided by\ncompiler feedback.\nFurthermore, our StepCoder surpasses all base-\nline models including other RL-based approaches,\nachieving the highest score. Specifically, our ap-\nproach obtains 59.7%, 23.5% and 8.6% in the \u2018In-\ntroductory\u2019, \u2018Interview\u2019 and \u2018Competition\u2019, respec-\ntively. Our approach excels in exploring the out-\nput space compared to other RL-based methods,\nachieved by simplifying complex code generation\ntasks to code completion sub-tasks. Additionally,\nthe FGO process plays a pivotal role in precisely\noptimizing the policy model. We also found that\nthe performance of StepCoder is better than LLM\nwhich supervised fine-tuning on the APPS+ dataset\nbased on the same backbone. The latter did lit-\ntle to improve the pass rate of the generated code\ncompared with the backbone. This also directly\ndemonstrates that the method of using compiler\nfeedback to optimize the model improves the qual-\nity of the generated code better than next-token\nprediction in code generation.\nModels (6.7B)\nHumanEval MBPP\nDeepSeek-Coder-Instruct\n78.0\n64.2\nSFT on APPS+\n55.5\n54.8\nVanilla PPO\n78.0\n65.0\nPPOCoder\n76.8\n63.8\nRLTF\n76.8\n65.2\nStepCoder (Ours)\n78.7\n67.0\nTable 2: Results of pass@1 on MBPP and HumanEval.\nWe evaluate the LLMs\u2019 performance on code generation\nin a zero-shot learning setting. In this setting, the models\nare fine-tuned on our proposed APPS+ dataset and tested\nfor their ability on MBPP and HumanEval.\n4.4\nAblation Studies\nTo investigate the impact of individual components\nin StepCoder, we conducted ablation experiments\nwith two variations of our approach, including Step-\nCoder only with CCCS and only with FGO. The ex-\nperimental results are presented in Table 1. Experi-\nmental results demonstrate that both components of\nour approach improve the quality of the generated\ncode compared to vanilla PPO. CCCS can enhance\nits performance in addressing Competition-level\nproblems. This improvement is logical, consider-\ning that CCCS effectively simplifies the exploration\nof more complex human requirements. Simultane-\nously, FGO boosts the pass rate of unit tests by\nintegrating compiler feedback with the relevant ex-\necuted code snippet.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFraction of lines duplicated\n0\n100\n200\n300\n400\n500\n600\n700\n800\nNumber of tasks\nwith MBPP\nwith HumanEval\nFigure 3: Analysis of duplicated lines between APPS+\nand the two benchmarks. The overlap of data between\nAPPS+ and them is very small. Only 0.2% and 7.1%\nhad more than half of their lines matched somewhere in\nMBPP and HumanEval, respectively.\n4.5\nResults on MBPP and HumanEval\nTo further demonstrate the effectiveness of our\nmethod, we conducted comparative analyses\nof StepCoder against various approaches using\nthe well-recognized benchmarks MBPP and Hu-\nmanEval. These models are trained on APPS+ and\nthen evaluated on MBPP and HumanEval. The ex-\nperimental results are illustrated in Table 2 which\nshows that StepCoder is superior over all other\nmodels on both benchmarks.\nHowever, there are concerns regarding poten-\ntial overlaps in the training data between APPS+\nand the two benchmarks, which might contribute\nto an improvement in performance. To address\nthese concerns, we analyze the difference between\nAPPS+ and the benchmarks by calculating the code\nline overlap ratio of two corresponding canonical\nsolutions following previous work [2; 14]. The\nfindings are presented in Figure 3. This evidence\nunderscores our approach\u2019s effectiveness in enhanc-\ning the quality of generated code and its capability\nacross a broad spectrum of code generation tasks,\nprimarily by improving the exploration problem in\nreinforcement learning.\nMeanwhile, our findings revealed a significant\ndegradation in the performance of the SFT model\non both MBPP and HumanEval benchmarks. Fur-\nther analysis of the error cases showed that a minor-\nity were related to function name errors, while the\nmajority were associated with program correctness\nerrors. This also indicated that SFT on a single\ndataset may impair the ability to follow instruc-\ntions and the ability to generalize, thus affecting\nthe performance of code generation on other tasks.\nIn contrast, RL-based methods can improve the\nperformance for unseen tasks of code generation.\n4.6\nAnalysis by Unit Test Results\nWe further analyzed the results of cases that did\nnot pass all unit tests, as shown in Figure 4. The\nresults show that our proposed method can effec-\ntively reduce the likelihood of compilation errors,\nwhich is particularly evident in Interview-level and\nCompetition-level programming problems. How-\never, it was also observed that all LLMs are more\nprone to runtime errors and failures as compared to\ncompilation errors, albeit StepCoder shows a com-\nparatively lower rate of runtime errors and failures.\nThese results demonstrate that StepCoder is less\nprone to compilation errors, but still suffers from\nruntime errors and failure. Consequently, these\nfindings suggest that future research should further\nconcentrate on significantly reducing runtime er-\nrors, which could greatly enhance both the quality\nand the pass rate of the code generated by such\nmodels.\nIntro Inter Comp \nAll\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPercentage (%)\nCompile Error\nDeepseek-instruct\nVanilla PPO\nStepCoder (Ours)\nIntro Inter Comp \nAll\n0\n10\n20\n30\n40\n50\n60\n70\nRuntime Error & Failure\nFigure 4: Analysis by unit test results on APPS+. The\nresults are categorized into CompileError (Reward = -1)\nand Runtimeerror & Failure (Reward = -0.6 or -0.3).\n5\nRelated Work\n5.1\nLarge Language Models for Code\nGeneration\nRecently, LLMs have shown remarkable ability in\nunderstanding natural language and code genera-\ntion by training on large text corpora containing\ncode data. Several pre-trained language models\n(PLMs) demonstrate significant potential for code\ngeneration including CodeGPT [21], PanGu-Coder\n[4], SantaCoder [1], CodeGeex [44] and Phi-1.5\n[16]. In addition, SFT models achieve more com-\npetitive performance such as CodeX [3], StarCoder\n[15], WizardCoder [23], Code Llama Instruct [27],\nand DeepSeek-Coder [8].\nReinforcement Learning is a method of learning\nthe optimal policy by exploring the environment\nand obtaining rewards [41; 34]. Recently, some\nresearchers have introduced RL to LLMs and im-\nproved the quality of the generated code by uti-\nlizing the unit test feedback to explore the output\nspace of the policy model [33; 20; 14]. For instance,\nCodeRL [14] leverages signal from unit tests as re-\nwards and utilizes the actor-critic approach [12; 35]\nto enhance models on code generation. PPOCoder\n[33] refines CodeRL by employing the PPO algo-\nrithm [30] and RLTF [20] provides fine-grained\nrewards through the error locations, but the reward\nspace is still sparse. However, the exploration of\ncomplex tasks in an environment characterized by\na sparse reward is challenging. These methods still\nfall short of effectively using RL to enhance the\nmodel\u2019s performance in code generation.\n5.2\nExploration in Reinforcement Learning\nExploration is crucial in addressing long sequences\nand sparse reward problems [9; 13]. In the se-\nquence generation task, researchers improved ex-\nploration by initializing the policy model using the\nSFT model [26; 31]. Our proposed approach in-\ncorporates similar methods, but additional methods\nare necessary to ensure effective exploration. This\nis particularly evident when facing complex human\nrequirements, where the limited quality of code\ngenerated by SFT models makes exploration still\nchallenging [33].\nOther notable methods introduce the Process-\nSupervised Reward Model to provide step-by-step\nrewards for complex sequence generation tasks\nsuch as mathematical reasoning and code gener-\nation [38; 18; 22; 24]. However, these methods\nrequire labelling a large preference dataset to train\nthe reward model. Similar to our approach, some\nmethods construct a learning curriculum by initiat-\ning each episode from a sequence of progressively\nmore challenging starting states [28; 11; 6]. In\ncontrast to our approach, these methods are de-\nsigned to address the problem of exploration in\nother fields, such as gaming and robotic manipula-\ntion. Meanwhile, our approach combines software\nengineering features to dynamically determine the\nstarting states through conditional statements. We\nalso introduce FGO to provide a fine-grained op-\ntimization for the policy model by leveraging the\ncoverage information.\n6\nConclusion\nIn this paper, we introduce StepCoder, a novelty\ntraining framework via RL. StepCoder breaks down\ncomplicated exploration problems to reduce the\ndifficulty of exploring environments with sparse\nrewards while providing fine-grained optimization.\nIn addition, we also construct a high-quality dataset\nAPPS+, specifically for code generation. Experi-\nments indicate that our method can effectively im-\nprove the quality of generated code via reinforce-\nment learning compared to other approaches.\nReferences\n[1] Loubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don\u2019t\nreach for the stars! arXiv preprint arXiv:2301.03988.\n[2] Jacob Austin, Augustus Odena, Maxwell Nye,\nMaarten Bosma, Henryk Michalewski, David Do-\nhan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,\net al. 2021. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732.\n[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\n[4] Fenia Christopoulou, Gerasimos Lampouras, Mi-\nlan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi\nLi, Qi Zhang, Meng Xiao, Bo Shen, Lin Li,\net al. 2022. Pangu-coder: Program synthesis with\nfunction-level language modeling. arXiv preprint\narXiv:2207.11280.\n[5] OpenCompass Contributors. 2023. Opencompass:\nA universal evaluation platform for foundation\nmodels.\nhttps://github.com/open-compass/\nopencompass.\n[6] Carlos Florensa, David Held, Markus Wulfmeier,\nMichael Zhang, and Pieter Abbeel. 2017. Reverse\ncurriculum generation for reinforcement learning.\nIn Conference on robot learning, pages 482\u2013495.\nPMLR.\n[7] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh,\net al. 2017. Program synthesis. Foundations and\nTrends\u00ae in Programming Languages, 4(1-2):1\u2013119.\n[8] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie,\nKai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-\nfeng Liang. 2024. Deepseek-coder: When the large\nlanguage model meets programming \u2013 the rise of\ncode intelligence.\n[9] Jianye Hao, Tianpei Yang, Hongyao Tang, Chen-\njia Bai, Jinyi Liu, Zhaopeng Meng, Peng Liu, and\nZhen Wang. 2023. Exploration in deep reinforcement\nlearning: From single-agent to multiagent domain.\nIEEE Transactions on Neural Networks and Learning\nSystems.\n[10] Dan Hendrycks, Steven Basart, Saurav Kadavath,\nMantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, and\nJacob Steinhardt. 2021.\nMeasuring coding chal-\nlenge competence with APPS. In Proceedings of\nthe Neural Information Processing Systems Track on\nDatasets and Benchmarks 1, NeurIPS Datasets and\nBenchmarks 2021, December 2021, virtual.\n[11] Ionel-Alexandru Hosu and Traian Rebedea. 2016.\nPlaying atari games with deep reinforcement learn-\ning and human checkpoint replay. arXiv preprint\narXiv:1607.05077.\n[12] Vijay Konda and John Tsitsiklis. 1999. Actor-critic\nalgorithms. Advances in neural information process-\ning systems, 12.\n[13] Pawel Ladosz, Lilian Weng, Minwoo Kim, and\nHyondong Oh. 2022. Exploration in deep reinforce-\nment learning: A survey. Information Fusion, 85:1\u2013\n22.\n[14] Hung Le, Yue Wang, Akhilesh Deepak Gotmare,\nSilvio Savarese, and Steven Chu Hong Hoi. 2022.\nCoderl: Mastering code generation through pre-\ntrained models and deep reinforcement learning. Ad-\nvances in Neural Information Processing Systems,\n35:21314\u201321328.\n[15] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\n[16] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie\nDel Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.\nTextbooks are all you need ii: phi-1.5 technical report.\narXiv preprint arXiv:2309.05463.\n[17] Yujia Li, David Choi, Junyoung Chung, Nate\nKushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin\nDal Lago, et al. 2022. Competition-level code genera-\ntion with alphacode. Science, 378(6624):1092\u20131097.\n[18] Hunter Lightman, Vineet Kosaraju, Yura Burda,\nHarri Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023.\nLet\u2019s verify step by step.\narXiv preprint\narXiv:2305.20050.\n[19] Timothy P Lillicrap, Jonathan J Hunt, Alexander\nPritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra. 2015. Continuous control\nwith deep reinforcement learning. arXiv preprint\narXiv:1509.02971.\n[20] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao\nHan, Wei Yang, and Deheng Ye. 2023. Rltf: Rein-\nforcement learning from unit test feedback. arXiv\npreprint arXiv:2307.04349.\n[21] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang,\nAlexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, et al.\n2021. Codexglue: A machine learning benchmark\ndataset for code understanding and generation. arXiv\npreprint arXiv:2102.04664.\n[22] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\narXiv preprint arXiv:2308.09583.\n[23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct. arXiv preprint arXiv:2306.08568.\n[24] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo\nYuan, Pengfei Liu, Yang You, and Hongxia Yang.\n2023. Let\u2019s reward step by step: Step-level reward\nmodel as the navigators for reasoning. arXiv preprint\narXiv:2310.10080.\n[25] Volodymyr Mnih, Koray Kavukcuoglu, David Sil-\nver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidje-\nland, Georg Ostrovski, et al. 2015. Human-level\ncontrol through deep reinforcement learning. nature,\n518(7540):529\u2013533.\n[26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo\nAlmeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama,\nAlex Ray, et al. 2022. Training language models to\nfollow instructions with human feedback. Advances\nin Neural Information Processing Systems, 35:27730\u2013\n27744.\n[27] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\n[28] Tim Salimans and Richard Chen. 2018. Learning\nmontezuma\u2019s revenge from a single demonstration.\narXiv preprint arXiv:1812.03381.\n[29] John Schulman, Philipp Moritz, Sergey Levine,\nMichael Jordan, and Pieter Abbeel. 2015.\nHigh-\ndimensional continuous control using general-\nized\nadvantage\nestimation.\narXiv\npreprint\narXiv:1506.02438.\n[30] John Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\n[31] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shi-\nhan Dou, Tao Gui, Qi Zhang, and Xuan-Jing Huang.\n2023. Loose lips sink ships: Mitigating length bias\nin reinforcement learning from human feedback. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 2859\u20132873.\n[32] Martin Shepperd. 1988. A critique of cyclomatic\ncomplexity as a software metric. Software Engineer-\ning Journal, 3(2):30\u201336.\n[33] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni,\nand Chandan K Reddy. 2023. Execution-based code\ngeneration using deep reinforcement learning. arXiv\npreprint arXiv:2301.13816.\n[34] Richard S Sutton, Andrew G Barto, et al. 1998.\nIntroduction to reinforcement learning, volume 135.\nMIT press Cambridge.\n[35] Richard S Sutton, David McAllester, Satinder\nSingh, and Yishay Mansour. 1999. Policy gradient\nmethods for reinforcement learning with function\napproximation. Advances in neural information pro-\ncessing systems, 12.\n[36] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,\nand Neel Sundaresan. 2020. Intellicode compose:\nCode generation using transformer. In Proceedings\nof the 28th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foun-\ndations of Software Engineering, pages 1433\u20131443.\n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, et al. 2023. Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\n[38] Jonathan Uesato, Nate Kushman, Ramana Ku-\nmar, Francis Song, Noah Siegel, Lisa Wang, An-\ntonia Creswell, Geoffrey Irving, and Irina Hig-\ngins. 2022.\nSolving math word problems with\nprocess-and outcome-based feedback. arXiv preprint\narXiv:2211.14275.\n[39] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yi-\ntong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang,\nand Qun Liu. 2022. Compilable neural code gen-\neration with compiler feedback.\narXiv preprint\narXiv:2203.05132.\n[40] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,\nAlisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\n[41] Ronald J Williams. 1992.\nSimple statistical\ngradient-following algorithms for connectionist rein-\nforcement learning. Machine learning, 8:229\u2013256.\n[42] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023.\nWizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\n[43] Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi\nLiu, Jianye Hao, Zhaopeng Meng, Peng Liu, and\nZhen Wang. 2021. Exploration in deep reinforcement\nlearning: a comprehensive survey. arXiv preprint\narXiv:2109.06668.\n[44] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong,\nShan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, et al. 2023.\nCodegeex: A\npre-trained model for code generation with multi-\nlingual evaluations on humaneval-x. arXiv preprint\narXiv:2303.17568.\n[45] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua,\nWei Shen, Binghai Wang, Yan Liu, Senjie Jin, Yuhao\nZhou, Limao Xiong, et al. 2023. Delve into ppo:\nImplementation matters for stable rlhf. In NeurIPS\n2023 Workshop on Instruction Tuning and Instruction\nFollowing.\nA\nInstance of the APPS+ Dataset\nWe present an example from our APPS+ dataset,\nas shown in Figure 5.\nB\nExperiments Setup in Detail\nIn this section, we elaborate in detail on the base-\nlines we compare and the implementation details\nof our method.\nB.1\nBenchmarks\nAPPS+. We construct the new benchmark APPS+\nby refining the popular benchmark APPS [10].\nAPPS+ was categorized into three difficulty levels:\nIntroductory (2,850), Interview (4,020), and Com-\npetition (586). The mean length of each problem\nis 255.3 words, and that of the code is 21.9 lines.\nOn average, each instance is accompanied by three\nunit tests and includes a \u2018conditional statement\u2019 at-\ntribute representing the start and end position of the\nstatement in the canonical solution. We randomly\nselected about 25% instances (700 Introductory,\n1,000 Interview, and 140 Competition) for the val-\nidation dataset and another 25% instances for the\ntest dataset.\nMBPP. MBPP [2] is a smaller but common\nPython code generation benchmark. It contains\n974 instances created by crowd-sourcing to an in-\nternal pool of crowd workers with basic Python\nknowledge. The difficulty level of the problems\nin this dataset is introductory. Most problems are\noften conveyed in a single sentence of natural lan-\nguage, and each problem consists of a task descrip-\ntion, code solution, and three automated test cases.\nWe evaluate LLMs in a zero-shot learning setting\nwhich is the same as previous studies [14; 33]. In\nthis setting, we fine-tune models only based on the\nAPPS+ dataset and evaluate them on MBPP.\nHumanEval. HumanEval [3] is another exten-\nsively used benchmark for evaluating the ability\nof code generation. It comprises 164 hand-written\nPython problems that test language comprehension,\nalgorithmic thinking, and basic mathematics. The\ncomplexity of these problems is akin to that of sim-\nple software interview questions. We also evaluate\nmodels on the HumanEval benchmark in a zero-\nshot learning setting.\nB.2\nBaselines\nStarCoder. StarCoder [15] is a 15.5B parame-\nter model trained on 80+ programming languages\nsourced from GitHub, encompassing one trillion\ntokens. It undergoes fine-tuning specifically for\n35 billion Python tokens, enabling its proficiency\nacross a diverse set of coding tasks. With an ex-\ntended context length of 8K, StarCoder excels par-\nticularly in infilling capabilities.\nCodeLlama. CodeLlama [27] is a collection\nof pre-trained and fine-tuned generative text mod-\nels ranging in scale from 7B to 34B parameters.\nCodeLlama comes in three variants: CodeLlama:\nbase models designed for general code synthesis\nand understanding; CodeLlama-Python: designed\nspecifically to handle the Python programming lan-\nguage; CodeLlama-Instruct: for instruction fol-\nlowing and safer deployment.\nWizardCoder. WizardCoder [23] is fine-tuned\nby using a complicated dataset which is constructed\nby adapting the Evol-Instruct [42] on code-related\ntasks, which is a further improvement of self-\ninstruct method [40]. It has proven to be highly\neffective in code generation by fine-tuning more\ncomplex instruction data.\nDeepSeek-Coder. DeepSeek-Coder [8] demon-\nstrates state-of-the-art performance among open-\nsource code models across various programming\nlanguages. It encompasses a collection of code lan-\nguage models from 1B to 33B trained from scratch.\nThe training corpus for these models comprises an\nimpressive 2 trillion tokens which is the combina-\ntion of code and natural languages. Each model is\ntrained to utilize a window size of 16K, and a fill-\nin-the-blank task is incorporated into the training\nprocess, which enhances the models\u2019 capacity to\nfacilitate code completion and infilling tasks.\nPPOCoder. PPOCoder [33] initially employs\nthe Proximal Policy Optimization algorithm [30]\nfor code generations. In addition, it integrates dis-\n-----Task description-----\ndef numDistinct(self, s: str, t: str) -> int:\\n \n\"\"\"Given a string S and a string T, count the number of distinct subsequences of S which equals T. A subsequence of a string is \na new string which is formed from the original string by deleting some (can be none) of the characters without disturbing the\nrelative positions of the remaining characters. (ie, \"ACE\" is a subsequence of \"ABCDE\" while \"AEC\" is not). \"\"\"\n-----Example1-----\nInput: S = \"rabbbit\", T = \"rabbit\"\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate \"rabbit\" from S.\n-----Example2-----\nInput: S = \"babgbag\", T = \"bag\"\nOutput: 5\n\"inputs\": [\n[\n\"\\\"rabbbit\\\"\",\n\"\\\"rabbit\\\"\"\n]\n],\n\"outputs\": [\n3\n],\n\"fn_name\": \"numDistinct\",\n\"starter_code\": \"\\nclass Solution:\\n    def numDistinct(self, s: str, \nt: str) -> int:\\n\"\ndef numDistinct(self, s, t):\nsetOft=set(t)\nnews=\"\"\nfor ch in s:\nif ch in setOft:\nnews+=ch\ndp=[[1 for i in range(len(news)+1)] for j in range(len(t)+1)]\nfor j in range(1,len(t)+1):\ndp[j][0]=0\nfor i in range(len(t)):\nfor j in range(len(news)):\nif t[i]==news[j]:\ndp[i+1][j+1]=dp[i][j]+dp[i+1][j]\nelse:\ndp[i+1][j+1]=dp[i+1][j]\nreturn dp[len(t)][len(news)]\nUnit Test & Meta Data\nCanonical Solution\nProgramming Problem Description\nFigure 5: An instance from our APPS+ dataset includes a human requirement (top), corresponding canonical code\n(bottom left), metadata, and example cases for unit testing to evaluate the generated code (bottom right). We clean\nthe APPS dataset [10] to provide a more rigorous evaluation and a foundation for training by RL in code generation.\ncrete compiler feedback with syntax and semantics\nmatching scores between generated code and exe-\ncutable objectives which reduces the sparsity of the\nreward function, thereby providing better guidance\nfor generating code that aligns more closely with\nthe correct objectives.\nRLTF. RLTF [20] features real-time data gen-\neration during the training process and multi-\ngranularity unit test feedback. Except for the dis-\ncrete compiler feedback, it penalizes specific sec-\ntions in the code where errors occur through the\nerror locations from the feedback of unit tests.\nC\nThe algorithm of CCCS and FGO\nThe full algorithm of StepCoder is detailed in Al-\ngorithm 1.\nD\nThe prompts used for code generation\nFor DeepSeek-Coder-Instruct [8], we use the same\nprompt as the previous paper. Moreover, DeepSeek-\nCoder-Instruct serves as the backbone model for\nPPOCoder [33], RLTF [20], and our proposed Step-\nCoder. Consequently, we align the prompts for\nthese RL-based approaches with the prompt of\nDeepSeek-Coder-Instruct to maintain consistency.\nThe prompt used for other models such as CodeL-\nlama, WizardCoder and StarCoder is the same as\nin previous studies [5; 23; 15; 27].\nThe prompt used for DeepSeek-Coder-Instruct\nand LLMs based on it is as follows:\nYou are an AI programming assistant, utilizing the\nDeepseek Coder model, developed by Deepseek\nCompany, and you only answer questions related\nto computer science.\nFor politically sensitive questions, security and pri-\nvacy issues, and other non-computer science ques-\ntions, you will refuse to answer.\n### Instruction:\nwrite an algorithm in python:\n{Task description}\n### Response:\nAlgorithm 1 StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feed-\nback\nRequire: the train dataset D = {(xi, yi, ui, ei), 1 \u2264 i \u2264 n}, the threshold value \u03c1t for curriculum\ntraining.\nRequire: the policy model \u03c0\u03b8\n1: Initialize the stride of curriculum s = \u2308\nEi\n\u2308\u221aEi\u2309\u2309 for each sample\n2: Initialize the current curriculum c = \u2308\u221aEi\u2309 \u2212 1 for each training sample\n3: Initialize the pass rate \u03c1 = 0 for each training sample\n4: while TRUE do\n5:\nInitialize mini-batch Ds = {}\n6:\nGet latest policy model \u03c0\u03b8\n7:\nSample a mini-batch of size M from D\n8:\nfor i in 0, \u00b7 \u00b7 \u00b7 , M \u2212 1 do\n\u25b7 Begin to sample the trajectories\n9:\nCalculate the start position pos = si \u2217 ci\n\u25b7 CCCS\n10:\nReorganize the given context x\n\u2032\ni = xi + yi [: pos]\n11:\nSample trajectory \u02c6yi \u2190 \u03c0\u03b8(.|x\n\u2032\ni)\n12:\nCompute reward ri using Equation 3\n13:\nCalculate unexecuted snippets\u2019 mask matrix mij = [1 if \u02c6yj\ni is executed else 0]\n\u25b7 FGO\n14:\nAdd {x\n\u2032\ni, \u02c6yi, ui, ri, si, ci, mi} to mini-batch Ds\n15:\nend for\n16:\n\u03b8 \u2190 A(\u03b8, Ds)\n\u25b7 Update the policy model by PPO algorithm\n17:\nfor i in 0, \u00b7 \u00b7 \u00b7 , M \u2212 1 do\n18:\nif ri = 1 then\n\u25b7 Update pass rate using moving average\n19:\n\u03c1i = \u03b1 + (1 \u2212 \u03b1) \u2217 \u03c1i\n20:\nelse\n21:\n\u03c1i = (1 \u2212 \u03b1) \u2217 \u03c1i\n22:\nend if\n23:\nif \u03c1i > \u03c1t then\n\u25b7 Meet the update conditions, proceed to the next stage\n24:\n\u03c1i = 0\n25:\nci = min(ci \u2212 1, 0)\n26:\nend if\n27:\nend for\n28: end while\n"
  },
  {
    "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
    "link": "https://arxiv.org/pdf/2402.01622.pdf",
    "upvote": "28",
    "text": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents\nJian Xie\u2660 * Kai Zhang\u2663 * Jiangjie Chen\u2660 Tinghui Zhu\u2660 Renze Lou\u2661\nYuandong Tian\u2662 Yanghua Xiao\u2660 Yu Su\u2663\n\u2660Fudan University \u2663The Ohio State University\n\u2661The Pennsylvania State University \u2662Meta AI\njianxie22@m.fudan.edu.cn, shawyh@fudan.edu.cn, {zhang.13253, su.809}@osu.edu\nhttps://osu-nlp-group.github.io/TravelPlanner\nAbstract\nPlanning has been part of the core pursuit for arti-\nficial intelligence since its conception, but earlier\nAI agents mostly focused on constrained settings\nbecause many of the cognitive substrates neces-\nsary for human-level planning have been lack-\ning. Recently, language agents powered by large\nlanguage models (LLMs) have shown interest-\ning capabilities such as tool use and reasoning.\nAre these language agents capable of planning in\nmore complex settings that are out of the reach\nof prior AI agents? To advance this investigation,\nwe propose TravelPlanner, a new planning bench-\nmark that focuses on travel planning, a common\nreal-world planning scenario. It provides a rich\nsandbox environment, various tools for access-\ning nearly four million data records, and 1,225\nmeticulously curated planning intents and refer-\nence plans. Comprehensive evaluations show that\nthe current language agents are not yet capable\nof handling such complex planning tasks\u2014even\nGPT-4 only achieves a success rate of 0.6%. Lan-\nguage agents struggle to stay on task, use the right\ntools to collect information, or keep track of mul-\ntiple constraints. However, we note that the mere\npossibility for language agents to tackle such a\ncomplex problem is in itself non-trivial progress.\nTravelPlanner provides a challenging yet mean-\ningful testbed for future language agents.\n1. Introduction\nPlanning is a hallmark of human intelligence. It is an evo-\nlutionary feat built upon numerous other capacities: using\nvarious tools to iteratively collect information and make de-\n\u2217The first two authors contributed equally. Work done during\nJian\u2019s internship at OSU NLP Group.\ncisions, recording intermediate plans (in working memory\nor on a physical device) for deliberation, and exploring alter-\nnative plans by running simulations, which in turn depends\non a world model (Mattar & Lengyel, 2022; Ho et al., 2022).\nFor decades, researchers have been attempting to develop\nAI agents to mimic humans\u2019 planning capability (Russell\n& Norvig, 2010; Georgievski & Aiello, 2015; Karpas &\nMagazzeni, 2020), but often in constrained settings (Camp-\nbell et al., 2002; Silver et al., 2016; 2017) because many\nof the cognitive substrates necessary for human-level plan-\nning have been lacking. AI agents that can work robustly in\nthe largely unconstrained settings in which humans operate\nremain a distant goal.\nThe advent of large language models (LLMs; OpenAI (2022;\n2023); Touvron et al. (2023a;b); Jiang et al. (2023)) brings\nnew light to this classic problem. A new generation of\nlanguage agents (Su, 2023; Sumers et al., 2023; Xie et al.,\n2023) powered by LLMs has emerged, characterized by\ntheir capability of using language as a vehicle for thought\nand communication. These agents have shown interest-\ning capabilities, such as tool use (Schick et al., 2023; Patil\net al., 2023; Qin et al., 2024) and various forms of reason-\ning (Wei et al., 2022; Yao et al., 2022; Lewkowycz et al.,\n2022), potentially fulfilling the role of some of the cogni-\ntive substrates that were lacking in earlier AI agents. Re-\nsearchers have therefore investigated their potential in an ar-\nray of planning tasks ranging from classic planning settings\nlike Blocksworld (Valmeekam et al., 2023) to embodied\nagents (Huang et al., 2022; Ichter et al., 2022; Song et al.,\n2023; Wang et al., 2023) and web agents (Deng et al., 2023;\nZhou et al., 2024). However, the planning settings in exist-\ning work still largely follow the conventional setting that\nfocuses on single-objective optimization with fixed ground\ntruths. An agent is tasked with predicting from a pre-defined\nset of actions, just now made by an LLM-powered agent.\nAre language agents capable of planning in more complex\nyet realistic settings, closer to those in which humans oper-\nate? To advance this investigation, we propose TravelPlan-\nner, a new planning benchmark that focuses on a common\n1\narXiv:2402.01622v2  [cs.CL]  5 Feb 2024\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nInteraction with Environment\nI\u2019m going from Seattle \nto California from \nNovember 6 to 10, \n2023. I have a budget of \n$6,000.  For lodging, I \nprefer an entire room \nand the accommodations \nmust be pet-friendly.\nLet\u2019 me help! To solve \nthis problem, I need to \n(1) analyze certain \nconstraints,  \n(2) collect information \nthrough reasonable use \nof necessary tools.\n1. CitySearch(\u00b7) \n2. AttractionSearch(\u00b7) \n3. FlightSearch(\u00b7) \n4. DistanceMatrix(\u00b7) \n5. RestaurantSearch(\u00b7) \n6. AccommodationSearch(\u00b7)\nToolbox\nThe plan must adhere to \ncertain constraints, e.g., user \nneeds and commonsense. It's \nalso vital to \u2026\nUser Needs (Hard Constraints)\n1. Budget: $6000 \n2. Room Type: Entire Room \n3. Room Rule: Pet-friendly\nCommonsense Constraints\n1. Reasonable City Route \n2. Diverse Restaurants \n3. Diverse Attractions \n4. Non-conflicting Transportation \n5. Accommodation meets Minimum Night\nPlanning\nSeattle -> Los Angeles \n\u2022 Flight: F123, (13:40-16:12), Cost: $120 \n\u2022 Accommodation: Luxury building studio \n\u2022 Dinner: The Attraction        \n2023-11-06 \n[Tool] CitySearch[California] \n[Result]  San Francisco, Los \nAngeles,\u2026, San Diego\nInformation Collection\n[Tool] FlightSearch[Seattle, San Francisco, \n2023-11-06] \n[Result]  No Flights. \n[Tool] FlightSearch[Seattle, Los Angeles, \n2023-11-06]  \n[Result] Flight Number: F123, \n13:40-16:12, Cost: $120 \n[Tool] DistanceMatrix[Los Angeles, \nSan Diego, taxi] \n[Result]  Duration: 1 hour 57 mins, \nDistance: 193 km, Cost: $200 \n[Tool] TransportationSearch[San \nDiego,Seattle, 2023-11-10] \n[Result]  Flight Number: F789, \n(7:59-10:56), Cost: $300\n[Tool] AccommodationSearch[Los Angeles] \n[Result] 'Cozy Room for U', $130/\nnight, Minimum night: 8, Entire \nRoom, Pets allowed  \n'Luxury building studio', $150/\nnight, Minimum night: 1, Entire \nRoom, Pets allowed \n[Tool] RestaurantSearch[Los Angeles] \n[Result] The Attraction,Cuisine:French,\u2026\nUser\nAgent\nDelivery Plan\n\ud83d\udd0d\n2023-11-06\nSeattle -> Los Angeles\n\u25cf\nFlight: F123 \n(13:40-16:12),  \nCost: $120 \n\u25cf\nAccommodation: \nLuxury building \nstudio \n\u25cf\nDinner: \nThe Attraction\n\u25cf\nBreakfast: \nChicken Minar \n\u25cf\nLunch: \nRajdhani Restaurant \n\u25cf\nDinner: \nDomino\u2019s Pizza \n\u25cf\nAttractions: \nSanta Monica Pier;  \nGriffith Park \n\u25cf\nAccommodation: \nLuxury building studio \n\u25cf\nTake taxi to San Diego \n\u25cf\nBreakfast: \nOpen Yard \n\u25cf\nLunch: \nThe Lost Mughal \n\u25cf\nDinner: \nBurger King \n\u25cf\nAttractions: \nCabrillo Monument \n\u25cf\nAccommodation: \nEast Side Apartment \n\u25cf\nBreakfast: \nBaskin Robbins \n\u25cf\nLunch: \nHarry's Bar \n\u25cf\nDinner: \nDragon Way \n\u25cf\nAttractions: \nLa Jolla Shores Park;  \nCalifornia Tower \n\u25cf\nAccommodation: \nEast Side Apartment\nLos Angeles\nLos Angeles -> San Diego\nSan Diego\nSan Diego -> Seattle\n\u25cf\nFlight: F789 \n(7:59-10:56), \nCost: $300\n2023-11-07\n2023-11-08\n2023-11-09\n2023-11-10\n2023-11-09\nFigure 1. Overview of TravelPlanner. Given a query, language agents are tasked with employing various search tools to gather information.\nBased on the collected information, language agents are expected to deliver a plan that not only satisfies the user\u2019s needs specified in the\nquery but also adheres to commonsense constraints.\nreal-world planning scenario\u2014travel planning. This is a\nchallenging, time-consuming task even for humans (but\nmost people can do it successfully, with the right tools and\nenough time): 1) Planning a multi-day itinerary is inher-\nently long-horizon, involving a large number of interdepen-\ndent decisions on places, lodging, transportation, dining,\netc. 2) Travel planning involves many constraints, rang-\ning from explicit constraints such as budget and various\nuser needs to implicit commonsense constraints, e.g., peo-\nple cannot teletransport to another city without using some\nmeans of transportation. 3) Travel planning requires strong\nagency to proactively acquire necessary information using\nvarious tools (e.g., to search flights and restaurants) from\nthe partially observable environment and deliberate over the\ncollected information to further the planning while being\nmindful of all the explicit and implicit constraints. Planning\ntasks of such complexity are out of the reach of prior AI\nagents (Russell & Norvig, 2010).\nTravelPlanner provides a rich sandbox environment with\naround four million data entries crawled from the Internet\nthat can be accessed via six tools. We also meticulously\ncurate 1,225 diverse user queries (along with their reference\nplans), each imposing a different combination of constraints.\nA representative example is illustrated in Figure 1.\nWe comprehensively evaluate five LLMs, such as GPT-\n4 (OpenAI, 2023), Gemini (G Team et al., 2023), and Mix-\ntral (Jiang et al., 2024), and four planning strategies, such\nas ReAct (Yao et al., 2022) and Reflexion (Shinn et al.,\n2023), on their capability of delivering complete plans and\nfollowing constraints. The main findings are as follows:\n\u2022 State-of-the-art LLMs cannot handle complex planning\ntasks like those in TravelPlanner. GPT-4 successfully pro-\nduces a plan that meets all the constraints for a few tasks\n(0.6%), while all other LLMs fail to complete any tasks.\n\u2022 Existing planning strategies such as ReAct and Reflex-\nion, which may be effective for simpler planning settings,\nare insufficient for the multi-constraint tasks in TravelPlan-\nner. They often fail to convert their reasoning into the right\nactions correctly and keep track of global or multiple con-\nstraints. Language agents need more sophisticated planning\nstrategies to approach human-level planning.\n\u2022 Further analyses reveal many common failure modes of\nexisting language agents, such as argument errors in tool\nuse, being trapped in dead loops, and hallucinations.\nAlthough most of our findings lean negatively toward the\ncurrent language agents, we should note that the mere pos-\nsibility for an artificial agent to tackle such a complex task\nis non-trivial progress in itself. TravelPlanner provides a\nchallenging yet meaningful testbed for future agents to hill-\nclimb toward human-level planning in complex settings.\nFinally, a silver lining: while our well-trained human an-\nnotators averagely take 12 minutes to manually annotate\na plan, a language agent can produce a plan in just 1\u20132\nminutes automatically. Perhaps one day, language agents\nwill become capable enough to help automate away many\nof such tedious tasks for us.\n2\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n2. Related Work\n2.1. Large Language Model based Agents\nEmpowered by large language models (LLMs), language\nagents have the capability to decompose complex tasks and\narrive at solutions through a series of reasoned actions.\nNotable examples such as AutoGPT (AutoGPT, 2023),\nBabyAGI (Nakajima, 2023), and HuggingGPT (Shen et al.,\n2023) have illuminated the community with their impressive\nabilities. Current LLM-powered language agents, equipped\nwith Memory, Tool-use, and Planning modules, have seen\na substantial improvement in their general abilities (Weng,\n2023). Memory in language agents refers to their ability\nto acquire and process information. It is divided into two\ntypes: long-term memory, which is the parametric memory\ninherent in LLMs, and short-term memory, also known as\nin-context learning (Brown et al., 2020) or working memory.\nTechniques like memory summarization (Chen et al., 2023;\nZhou et al., 2023; Liang et al., 2023) and retrieval (An-\ndreas, 2022; Park et al., 2023; Zhong et al., 2023) are widely\nemployed to enhance the memory capabilities of language\nagents. Moreover, by interacting with external tools, lan-\nguage agents expand their potential capabilities significantly.\nThis tool-augmentation paradigm has been validated as ef-\nfective in previous work (Nakano et al., 2021; Lu et al.,\n2023; Ge et al., 2023; Xie et al., 2023). We further discuss\nthe planning module in Section 2.2.\n2.2. Planning\nPlanning, a hallmark of human intelligence, entails a se-\nquence of actions that involve decomposing tasks, searching\nfor solutions, and making final decisions (Hayes-Roth &\nHayes-Roth, 1979; Grafman et al., 2004; Su, 2023). This\nskill is crucial for achieving human-level intelligence and\nhas been widely studied in areas such as robotics (Mc-\nDermott, 1992; Alterovitz et al., 2016) and transportation\nscheduling (Cross & Estrada, 1994; Pinedo, 2005). The\nemergence of language agents powered by LLMs has fur-\nther intensified discussions around their planning capabili-\nties (Liu et al., 2023a; Valmeekam et al., 2023). Previous re-\nsearch has demonstrated that language agents can effectively\ndecompose tasks and engage in step-by-step reasoning, lead-\ning to significant improvements (Wei et al., 2022; Yuan et al.,\n2023; Zheng et al., 2024). Furthermore, to optimize solution\nsearches in fewer steps, classical data structures like trees\nand graphs have been employed in prior studies (Yao et al.,\n2023; Besta et al., 2023), enhancing the planning capabil-\nities of language agents. In addition, methods involving\nfeedback from the environment (Yao et al., 2022; Shinn\net al., 2023) have also been shown to be beneficial. How-\never, while these planning abilities have shown promise in\nspecific tasks, the effectiveness of these planning strategies\nin scenarios with multiple constraints remains uncertain.\n2.3. Evaluation of Language Agents\nPrevious studies typically assess LLM-powered language\nagents in focused domains: arithmetic reasoning targeting\ncorrect solutions (Roy & Roth, 2015; Cobbe et al., 2021;\nPatel et al., 2021); tool-use evaluating agents\u2019 proficiency\nin employing tools and reporting results (Li et al., 2023; Xu\net al., 2023; Zhuang et al., 2023); and web navigation, test-\ning agents\u2019 ability to locate specific websites (Deng et al.,\n2023; Zhou et al., 2024; Liu et al., 2024). However, the\ncomplexity of the real-world implies that previous evalu-\nation methods, which focus on single objective and fixed\nground truths, may fall short of capturing the full scope of\nagents\u2019 capabilities. To address this, we introduce Trav-\nelPlanner for comprehensive evaluations, assessing whether\nlanguage agents can generate feasible solutions facing vari-\nous objectives, referred to as constraints in this paper.\n3. TravelPlanner\n3.1. Overview\nWe introduce TravelPlanner, a benchmark crafted for eval-\nuating language agents in tool-use and complex planning\nwithin multiple constraints. Grounding to travel planning,\na real-world use-case that naturally includes diverse con-\nstraints such as user needs and commonsense constraints,\nTravelPlanner evaluates whether agents can develop flexible\ntravel plans by collecting information via diverse tools and\nmaking decisions while satisfying the constraints.\nTravelPlanner comprises 1,225 queries in total. The queries\nin TravelPlanner are divided into nine groups. This classifi-\ncation is based on two criteria: the duration of travel and the\nnumber of hard constraints. The dataset is divided into the\ntraining, validation, and test set. The training set includes 5\nqueries per group with human-annotated plans (45 pairs in\ntotal), the validation set includes 20 queries per group (180\nin total), and the test set includes 1,000 queries. Detailed\ndistributions are shown in Table A.1.\n3.2. Constraint Introduction\nIn order to assess whether agents can perceive, understand,\nand satisfy various constraints to formulate a feasible plan,\nas outlined in Table 1, we include three types of constraints:\n\u2022 Environment Constraints: The real-world is dynamic,\nnecessitating agents to be adaptable. For instance, flights to\na particular destination may be unavailable at certain times\n(e.g., no flights from Seattle to San Francisco in Figure\n1), possibly because tickets are sold out. In such cases, the\nagent must dynamically seek an alternative, like changing\nthe destination of the flight or the way of transportation. To\nsimulate this, we introduce environment constraints within\nTravelPlanner to test the adaptability of agents in planning.\n3\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nTable 1. Constraint description. The environment constraints are manifested through the feedback received from the environment,\nassessing whether the language agent can adjust its plan appropriately. The commonsense constraints and hard constraints are evaluated\nbased on how well the language agent\u2019s plan aligns with these specific criteria.\nConstraint\nDescription\nEnvironment Constraint\nUnavailable Transportation\nThere is no available flight or driving information between the two cities.\nUnavailable Attractions\nThere is no available attraction information in the queried city.\nCommonsense Constraint\nWithin Sandbox\nAll information in the plan must be within the closed sandbox; otherwise, it will be considered a hallucination.\nComplete Information\nNo key information should be left out of the plan, such as the lack of accommodation during travel.\nWithin Current City\nAll scheduled activities for the day must be located within that day\u2019s city(s).\nReasonable City Route\nChanges in cities during the trip must be reasonable.\nDiverse Restaurants\nRestaurant choices should not be repeated throughout the trip.\nDiverse Attractions\nAttraction choices should not be repeated throughout the trip.\nNon-conf. Transportation\nTransportation choices within the trip must be reasonable. For example, having both \u201cself-driving\u201d and \u201cflight\u201d would\nbe considered a conflict.\nMinimum Nights Stay\nThe number of consecutive days spent in a specific accommodation during the trip must meet the corresponding\nrequired minimum number of nights\u2019 stay.\nHard Constraint\nBudget\nThe total budget of the trip.\nRoom Rule\nRoom rules include \u201cNo parties\u201d, \u201cNo smoking\u201d, \u201cNo children under 10\u201d, \u201cNo pets\u201d, and \u201cNo visitors\u201d.\nRoom Type\nRoom types include \u201cEntire Room\u201d, \u201cPrivate Room\u201d, \u201cShared Room\u201d, and \u201cNo Shared Room\u201d.\nCuisine\nCuisines include \u201cChinese\u201d, \u201cAmerican\u201d, \u201cItalian\u201d, \u201cMexican\u201d, \u201cIndian\u201d, \u201cMediterranean\u201d, and \u201cFrench\u201d.\nTransportation\nTransportation options include \u201cNo flight\u201d and \u201cNo self-driving\u201d.\nTable 2. The number of data entries in the database.\nTool\nData Entries (#)\nCitySearch\n312\nFlightSearch\n3,827,361\nDistanceMatrix\n17,603\nRestaurantSearch\n9,552\nAttractionSearch\n5,303\nAccommodationSearch\n5,064\n\u2022 Commonsense Constraints: Agents, functioning in real-\nworld and serving humans, should consider commonsense\nwhen designing plans. For instance, repeatedly visiting the\nsame attraction is not typical. To evaluate agents\u2019 under-\nstanding and utilization of commonsense during planning,\nwe include the commonsense constraint in TravelPlanner.\n\u2022 Hard Constraints: A crucial ability for agents is to effec-\ntively satisfy personalized user needs. To evaluate this, Trav-\nelPlanner incorporates various user needs, such as budget\nconstraints. These user needs are termed hard constraints.\nThe hard constraint measures the agent\u2019s generalization abil-\nity with regard to different user needs.\n3.3. Benchmark Construction Pipeline\nThis section outlines the construction pipeline of TravelPlan-\nner, which involves the following steps: 1) Environment and\nevaluation setup. 2) Diverse travel query design. 3) Refer-\nence plan annotation. 4) Quality check.\nEnvironment Setting.\nIn TravelPlanner, we create a static\nand closed sandbox environment for consistent and unbiased\nevaluations. This setup ensures that all agents access the\nsame unchanging information from our static databases,\navoiding the variability and potential biases introduced by\ndynamic data. To offer various travel options that align\nwith the real-world, we ensure the database for each tool\nin TravelPlanner contains rich information. The database\nsize of each tool is listed in Table 2. For more tool details,\nplease refer to Appendix A.2 and A.3. Additionally, agents\nare instructed to use the \u201cNotebookWrite\u201d tool to record\nnecessary information for planning. This tool is integrated to\nevaluate agents\u2019 working memory management and prevents\nmaximum token limit caused by context accumulation.\nQuery Construction.\nTo create diverse queries for Trav-\nelPlanner, we begin with several fundamental elements, in-\ncluding departure city, destination, and specific date range,\nrandomly chosen to form the skeleton of each query. Subse-\nquently, we adjust the duration of travel and the number of\nhard constraints to create different levels of complexity.\nThe duration of the travel\u20143, 5, or 7 days\u2014determines the\nnumber of cities included in the plan. Specifically, 3-day\nplans focus on one city, while 5 days and 7 days involve vis-\niting one randomly chosen state, with trips to 2 cities for the\n5-day plans and 3 cities for the 7-day plans, respectively. A\ngreater number of days requires more frequent tool usage by\nlanguage agents, thus increasing the difficulty of managing\nthe long-horizon aspect of planning. The uncertain destina-\ntion challenges agents to decide on multiple cities, where\nthey must consider factors such as inter-city connectivity.\n4\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nFurthermore, we introduce diverse user needs as hard con-\nstraints to add further complexity and realism. The difficulty\nlevels are categorized as follows:\n\u2022 Easy:\nQueries at this level are primarily budget-\nconstrained for a single person. The initial budget for each\nquery is determined using a set of crafted heuristic rules.\n\u2022 Medium: In addition to budget constraints, medium\nqueries introduce an additional hard constraint, randomly\nselected from a constraint pool including cuisine type, room\ntype, and room rule. Furthermore, the number of people\nvaries between 2 and 8, which influences the calculation of\ncosts for both transportation and accommodation.\n\u2022 Hard: Hard queries include additional transportation pref-\nerence into the constraint pool, along with all the constraints\nin medium queries. Each hard query contains three hard\nconstraints randomly selected from the constraint pool.\nThis method ensures the diversity of queries. Minor changes\nin these elements can lead to significantly different plans. Fi-\nnally, based on elements, we utilize GPT-4 (OpenAI, 2023)\nto generate natural language queries.\nHuman Annotation.\nTo ensure every query has at least\none feasible plan, we invite 20 graduate students to metic-\nulously annotate plans for synthesized queries. One plan\nis deemed eligible only if it meets all the constraints out-\nlined in our evaluation script, which is detailed in Section\n3.4. This rigorous process resulted in the creation of 1,225\nvalidated query-plan pairs. We pay annotators an average of\n$0.80 for each plan they annotate.\nQuality Control.\nTo ensure the quality of each natural\nlanguage query and its corresponding annotated plan, the\nauthors performed a detailed review of every query and\nplan, rectifying any errors found. Additionally, to ensure\nthe challenges, we re-calibrate each query\u2019s budget using\nthe costs from corresponding human-annotated plans. This\napproach replaces the initial heuristic-generated budgets,\nwhich might be too high, thus reducing the number of feasi-\nble plans. Through multiple stages of human verification,\nwe ensure the high quality of each query in TravelPlan-\nner and the presence of at least one feasible solution.\n3.4. Evaluation\nTo ensure a comprehensive evaluation of the plans offered by\nagents, we assess them from multiple dimensions. Specifi-\ncally, we first extract key components1, including transporta-\ntion, restaurants, attractions, and accommodations, which\nare initially presented as natural language. These compo-\nnents are then organized into a formally structured plan,\n1In our experiments, we use GPT-4-Turbo for this extraction\nprocess. Please refer to Appendix B.3.5 for more details.\nwhich will be evaluated automatically through pre-defined\nscripts. The evaluation criteria include the following:\n\u2022 Delivery Rate: This metric assesses whether agents can\nsuccessfully deliver a final plan within a limited number\nof steps. Falling into dead loops, experiencing numerous\nfailed attempts, or reaching the maximum number of steps\n(30 steps in our experimental setting) will result in failure.\n\u2022 Commonsense Constraint Pass Rate: Comprising eight\ncommonsense dimensions, this metric evaluates whether\na language agent can incorporate commonsense into their\nplan without explicit instructions.\n\u2022 Hard Constraint Pass Rate: This metric measures\nwhether a plan satisfies all explicitly given hard constraints\nin the query, which aims to test the agents\u2019 ability to adapt\ntheir plans to diverse user needs.\n\u2022 Final Pass Rate: This metric represents the proportion\nof feasible plans that meet all aforementioned constraints\namong all tested plans. It serves as an indicator of agents\u2019\nproficiency in producing plans that meet a practical standard.\nWe do not separately assess environment constraints since\ntheir impact is inherently reflected in the \u201cWithin Sand-\nbox\u201d and \u201cComplete Information\u201d metrics. For instance,\nwhen cities lack transportation or attractions, agents typi-\ncally resort to hallucination or opt not to provide an answer,\nreflecting the impact of environment constraints.\nFor the Commonsense Constraint Pass Rate and Hard Con-\nstraint Pass Rate, we utilize two evaluation strategies: micro\nand macro. The micro strategy calculates the ratio of passed\nconstraints to the total number of constraints. The Micro\nPass Rate is defined as:\nMicro Pass Rate =\nP\np\u2208P\nP\nc\u2208Cp 1passed(c,p)\nP\np\u2208P |Cp|\n,\n(1)\nwhere P represents the set of all plans being evaluated, Cp\ndenotes the set of constraints applicable to a specific plan p\nin P, and passed(X, Y ) is a function determining whether\nY meets constraints X.\nThe macro strategy calculates the ratio of plans that pass\nall commonsense or hard constraints among all tested plans.\nWe define the Macro Pass Rate as:\nMacro Pass Rate =\nP\np\u2208P 1passed(Cp,p)\n|P|\n.\n(2)\nThese two metrics evaluate an agent\u2019s capability of follow-\ning individual constraints vs. all the constraints holistically.\n3.5. Sole-Planning Setting\nWhile TravelPlanner is designed to assess the overall abil-\nities of agents in tool-use and planning (two-stage mode),\n5\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nTable 3. Main results of different LLMs and planning strategies on the TravelPlanner validation and test set. The best results are marked\nin bold. When the collected information is insufficient, Gemini Pro tends to directly refuse to provide the plan. Interviews with annotators\nreveal that manually annotating a plan averagely takes around 12 minutes. However, language agents, such as GPT-3.5-Turbo, can\naccomplish this task in just 1 to 2 minutes, showcasing their efficiency.\nValidation (#180)\nTest (#1,000)\nDelivery\nRate\nCommonsense\nPass Rate\nHard Constraint\nPass Rate\nFinal\nPass Rate\nDelivery\nRate\nCommonsense\nPass Rate\nHard Constraint\nPass Rate\nFinal\nPass Rate\nMicro Macro Micro\nMacro\nMicro Macro Micro\nMacro\nGreedy Search\n100\n74.4\n0\n60.8\n37.8\n0\n100\n72.0\n0\n52.4\n31.8\n0\nTwo-stage\nMistral-7B-32K (Jiang et al., 2023)\n8.9\n5.9\n0\n0\n0\n0\n7.0\n4.8\n0\n0\n0\n0\nMixtral-8\u00d77B-MoE (Jiang et al., 2024)\n49.4\n30.0\n0\n1.2\n0.6\n0\n51.2\n32.2\n0.2\n0.7\n0.4\n0\nGemini Pro (G Team et al., 2023)\n28.9\n18.9\n0\n0.5\n0.6\n0\n39.1\n24.9\n0\n0.6\n0.1\n0\nGPT-3.5-Turbo (OpenAI, 2022)\n86.7\n54.0\n0\n0\n0\n0\n91.8\n57.9\n0\n0.5\n0.6\n0\nGPT-4-Turbo (OpenAI, 2023)\n89.4\n61.1\n2.8\n15.2\n10.6\n0.6\n93.1\n63.3\n2.0\n10.5\n5.5\n0.6\nSole-planning\nDirectGPT\u22123.5\u2212Turbo\n100\n60.2\n4.4\n11.0\n2.8\n0\n100\n59.5\n2.7\n9.5\n4.4\n0.6\nCoTGPT\u22123.5\u2212Turbo\n100\n66.3\n3.3\n11.9\n5.0\n0\n100\n64.4\n2.3\n9.8\n3.8\n0.4\nReActGPT\u22123.5\u2212Turbo\n82.2\n47.6\n3.9\n11.4\n6.7\n0.6\n81.6\n45.9\n2.5\n10.7\n3.1\n0.7\nReflexionGPT\u22123.5\u2212Turbo\n93.9\n53.8\n2.8\n11.0\n2.8\n0\n92.1\n52.1\n2.2\n9.9\n3.8\n0.6\nDirectMixtral\u22128x7B\u2212MoE\n100\n68.1\n5.0\n3.3\n1.1\n0\n99.3\n67.0\n3.7\n3.9\n1.6\n0.7\nDirectGemini Pro\n93.9\n65.0\n8.3\n9.3\n4.4\n0.6\n93.7\n64.7\n7.9\n10.6\n4.7\n2.1\nDirectGPT\u22124\u2212Turbo\n100\n80.4\n17.2\n47.1\n22.2\n4.4\n100\n80.6\n15.2\n44.3\n23.1\n4.4\nwe also setup a simplified mode solely evaluating agents\u2019\nplanning skills (sole-planning mode). In this setting, we\nutilize human-annotated plans to pre-determine the destina-\ntion cities, and provide detailed and necessary information\ndirectly to agents, such as restaurants in the provided cities.\nThis eliminates the need for tool calling as agents don\u2019t need\nto collect information from scratch via tools anymore.\n4. Experiments\nWe evaluate the performance of various LLMs and planning\nstrategies on TravelPlanner. In the two-stage mode, we use\nthe ReAct (Yao et al., 2022) framework for information col-\nlection, which is recognized for its effective iteration with\ntools (Zhuang et al., 2023) while varying the foundation\nLLMs. This approach allows us to assess how different\nLLMs perform under a uniform tool-use framework. The\nagents are required to give the plan directly based on the\ninformation collected by themselves, without employing\nany other planning strategies. In the sole-planning mode,\nour evaluation goes beyond varying LLMs to include differ-\nent planning strategies. This aims to assess if the strategies\nproven effective in other planning benchmarks maintain\ntheir efficacy in TravelPlanner. All experiments are con-\nducted in a zero-shot setting.\n4.1. Baselines\nGreedy Search.\nTo evaluate the effectiveness of tradi-\ntional rule-based strategies within TravelPlanner, we include\ngreedy search as a baseline and set cost as the optimization\nobjective. Please refer to Appendix B.1 for more details.\nLLMs.\nDue to the long context window requirement of\nReAct and the massive information as text, we limit our con-\nsideration to LLMs capable of handling inputs exceeding 8K\nin length. As a result, our selection includes three closed-\nsource LLMs: GPT-3.5-Turbo (OpenAI, 2022), GPT-4-\nTurbo (OpenAI, 2023), and Gemini Pro (G Team et al.,\n2023), as well as two open-source LLMs: Mistral-7B-\n32K (Jiang et al., 2023) and Mixtral-8x7B-MoE (Jiang\net al., 2024). For all these models, we adopt the official\ninstruction formats whenever available.\nPlanning Strategies.\nTo explore the effectiveness of cur-\nrent planning strategies, we evaluate four representative\nones: Direct, ZS-CoT (Wei et al., 2022), ReAct (Yao et al.,\n2022), and Reflexion (Shinn et al., 2023). For the imple-\nmentation details, please refer to Appendix B.1. We do not\ninclude ToT (Yao et al., 2023) and GoT (Besta et al., 2023)\nbecause they require extensive exploration of the search\nspace, prohibitively costly for problems as complex as Trav-\nelPlanner. Also, given their performance close to ReAct in\ncomplex tasks (Zhuang et al., 2024), the potential benefits\nof these methods may be limited.\n4.2. Main Results\nIn this section, we discuss the performance of various LLMs\nand planning strategies on TravelPlanner (Table 3). We have\nthe following observations:\nTravelPlanner poses a significant challenge. In the two-\nstage mode, GPT-4-Turbo with ReAct achieves only 0.6% in\nthe final pass rate, and none of the other LLMs can pass any\n6\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nMax Step Limit\nArgument Error\nInvalid Action Dead Loop\nSame Action Dead Loop\nMixtral-7\u00d78B-MoE\nGemini Pro\nGPT-3.5-Turbo\nGPT-4-Turbo\n39.7%\n12.8%\n30.4%\n17.1%\n66.7%\n8.3%\n20.1%\n4.9%\n56.8%\n20.3%\n17.6%\n5.4%\n56.7%\n37.3%\n6.0%\nFigure 2. Tool-use error distribution on the test set. An early stop\nwill be triggered if the agent either makes three consecutive failed\nattempts or repetitive actions, indicating a dead loop.\nof the tasks. Even given all the necessary information in the\nsole-planning mode, existing planning strategies like ReAct\nand Reflexion still struggle with planning in TravelPlanner,\neven though they have shown their effectiveness in more\nconventional planning tasks. It is noteworthy that the best-\nperforming agent still falls short on hard constraints even\nwhen compared to greedy search. This poor performance\nunderlines the difficulty of TravelPlanner and shows that\ncurrent agents still struggle with complex planning.\nAgents show a substantial gap between the two modes.\nThe comparison of the two modes reveals the agents\u2019 strug-\ngles in fiddling with both information collection and plan-\nning. Across all metrics, the scores of any model in the\ntwo-stage mode are lower than those in the sole-planning\nmode, with the largest gap reaching over 30%. Similar to\nhumans, language agents also seem to have a limited \u201ccog-\nnitive capacity\u201d and their performance deteriorates when\nmultitasking. We provide a further analysis in Section 5.2.\nAgents struggle in obtaining a high macro pass rate.\nWhile some agents achieve high micro scores, their macro\nscores remain low. This pattern shows that although agents\nmanage to satisfy some constraints, they often overlook\nsome other constraints in the meantime. Consequently, this\nindicates the current agents fail to consider multiple con-\nstraints holistically, a critical requirement for navigating the\ncomplex tasks in TravelPlanner.\nIn summary, TravelPlanner poses a great challenge to cur-\nrent agents. The SoTA LLMs and planning strategies, which\noften show equal or superior to human-level performance\non many traditional tasks, are still far from sufficient for\ncomplex planning tasks that humans are capable of. Trav-\nelPlanner provides a challenging yet meaningful benchmark\nfor the development of more capable language agents.\n5. In-Depth Analysis\n5.1. Tool-Use Error Analysis\nAs shown in Table 3, even based on GPT-4-Turbo, agents\nstill make mistakes in the process of information collection\nand thus fail to deliver a plan. This problem is more severe\nin Gemini Pro and Mixtral. To delve into the underlying\ncauses, we categorize all error types in Figure 2. We find:\nTable 4. Constraint pass rate of GPT-4-Turbo on test set. The\nresults of the sole-planning mode are based on the Direct strategy.\nConstraint Type\nTwo-stage\nSole-planning\nEasy Medium Hard Easy Medium Hard\nCommonsense Constraint\nWithin Sandbox\n37.4\n31.2\n33.9 76.4\n71.5\n79.3\nComplete Information\n53.4\n52.9\n58.0 94.5\n96.4\n96.2\nWithin Current City\n69.3\n67.3\n68.3 89.1\n80.8\n82.4\nReasonable City Route\n44.5\n45.6\n54.9 99.4\n99.7\n99.1\nDiverse Restaurants\n85.1\n81.4\n86.8 91.1\n89.8\n87.8\nDiverse Attractions\n94.3\n90.4\n94.0 100.0\n100.0\n100.0\nNon-conf. Transportation 70.1\n73.3\n83.1 60.1\n56.5\n87.5\nMinimum Nights Stay\n46.8\n46.2\n51.1 37.4\n28.8\n30.1\nHard Constraint\nBudget\n10.1\n8.4\n4.4\n37.4\n35.1\n25.1\nRoom Rule\n-\n5.6\n11.3\n-\n31.5\n43.6\nCuisine\n-\n10.8\n11.4\n-\n57.5\n46.7\nRoom Type\n-\n12.4\n13.8\n-\n45.7\n56.7\nTransportation\n-\n-\n18.6\n-\n-\n77.5\nFinal\nFinal Pass Rate\n1.1\n0.3\n0.3\n8.0\n2.7\n2.2\nTable 5. Comparison of the numbers of different tool uses between\nagent (GPT-4-Turbo) and reference. The results of agent are based\non the number of entries written into the \u201cNotebook\u201d.\nAverage\nAgent\nReference\n3-day 5-day 7-day 3-day 5-day 7-day\nFlightSearch\n0.7\n1.2\n0.8\n2.0\n3.0\n4.0\nDistanceMatrix\n0.3\n0.6\n1.2\n2.0\n3.0\n4.0\nRestaurantSearch\n0.9\n1.5\n2.4\n1.0\n2.0\n3.0\nAttractionSearch\n0.9\n1.7\n2.4\n1.0\n2.0\n3.0\nAccommodationSearch 0.9\n1.6\n2.5\n1.0\n2.0\n3.0\n1) Agents incorrectly use tools. Except for GPT-4-Turbo,\nother LLMs-based agents all have argument error problems\nto varying degrees. It sheds light that the use of simple tools\nstill poses a significant challenge for agents. 2) Agents trap\nin dead loops. Even with GPT-4-Turbo, invalid actions\nand repetitive action loops contribute to 37.3% and 6.0% of\nerrors, respectively. Despite receiving feedback that actions\nare invalid or yield null results, agents persistently repeat\nthese actions. This suggests that agents fail to dynamically\nadjust their plans based on environment feedback.\n5.2. Planning Error Analysis\nWe detail each constraint pass rate in Table 4, from which\nwe have the following observations:\nThe number of hard constraints affects the performance\nof agents. Agents consistently exhibit pass rates below 10%\nacross all levels of difficulty, and this performance deterio-\nrates further as more constraints are introduced. This trend\nhighlights that current agents struggle with multi-constraint\ntasks, a key aspect of TravelPlanner.\nComprehensive information collection is essential for\nagents to plan effectively. In the sole-planning mode, agent\nperformance improves over the two-stage mode. To investi-\n7\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nQuery: Please plan a travel itinerary for me. \nI'm departing from Cincinnati and heading to \nNorfolk for three days. The dates of travel \nare from March 10th to March 12th, 2022. \nI have a budget of $1,400 for this trip.\nTool-use\nTrajectory:\nAction 1: FlightSearch[Cincinnati, Norfolk, \n2023-03-10] \nObservation 1: There is no flight from \nCincinnati to Norfolk on 2023-03-10. \nAction 2: FlightSearch[Cincinnati, Norfolk, \n2023-03-09] \nAction 3: FlightSearch[Cincinnati, Norfolk, \n2023-03-11] \nAction 4: FlightSearch[Cincinnati, Richmond, \n2023-03-10] \n\u2026 \nAction 8: FlightSearch[Cincinnati, Charlotte, \n2023-03-10] \nAction 9: I'm sorry, but as an AI \nlanguage model, I am unable to execute \nactions or observe their outcomes.\nDirect\nQuery: Please create a travel itinerary for a \nsolo traveler departing from Jacksonville and \nheading to Los Angeles for a period of 3 \ndays, from March 25th to March 27th, 2022. \nThe budget for this trip is now set at $2,400.\nPlan:\nDay 1: \ntransportation: Flight Number: F3765485, \nfrom Jacksonville to Los Angeles \n\u2026 \naccommodation: Midtown East 2 Bed/2 Bath \nElevator Building Apt (7-day minimum night) \nDay 2: \n\u2026 \naccommodation: Midtown East 2 Bed/2 Bath \nElevator Building Apt (7-day minimum night) \nDay 3: \ntransportation: Flight Number: F3765485, \nfrom Los Angeles to Jacksonville \n\u2026\nReflexion\nQuery: Can you help construct a travel plan \nthat begins in Philadelphia and includes visits \nto 3 cities in Virginia? The trip duration is for \n7 days, from March 15th to March 21st, \n2022, with a total budget of $1,800.\nTrajectory:\n\u2026 \nObservation 3: The cost of your current \nplan is 316.0 dollars. \nThought 4: I need to select a more \nbudget-friendly accommodation to keep \nthe overall trip within the $1,800 budget.  \nAction 4: CostEnquiry[{\"transportation\": \"-\", \n\"breakfast\": \"-\", \"attraction\": \"Virginia \nMuseum\", \"lunch\": \"Guang Zhou Chinese \nRestaurant, Richmond\", \"dinner\": \"Sandpiper \nRestaurant & Lounge, Richmond\", \n\"accommodation\": \"2 bd 2 bathroom \nApartment in Upper East Side, Richmond\u201d}] \nObservation 4: The cost of your plan is \n389.0 dollars.\nFigure 3. Case studies of failures. Agents fail to finalize plans due to repeated errors like incorrect dates, confusion with information\ndetails leading to hallucinatory answers, and disconnects between reasoning and actions. All cases are gained from GPT-4-Turbo based\nagents. For details of GPT-4-Turbo with the Reflexion strategy, please refer to Appendix B.2. We provide more cases in Appendix C.2.\ngate this, Table 5 shows agents in two-stage mode use tools\nless effectively compared to the reference plans. This com-\nparison indicates that agents often fail to finish completed\ninformation collection. Consequently, they either generate\nmade-up information or omit specific details. This leads\nto low pass rates for the \u201cWithin Sandbox\u201d and \u201cComplete\nInformation\u201d constraints. Additionally, this discrepancy be-\ncomes more pronounced with an increase in the duration of\ntravel. This emphasizes the need for agents to improve their\ncapabilities in long-horizon tasks.\nAgents struggle with global planning scenarios. Global\nconstraints \u201cMinimum Nights Stay\u201d and \u201cBudget\u201d demand\na holistic approach to planning, necessitating that agents\nnot only assess their current decisions but also anticipate\nfuture implications. Current LLMs\u2019 auto-regressive nature\nlimits them to independently obtain outcomes from multiple\nfuture branches. This highlights the necessity and urgent\nneed of new strategies, such as backtracking for adjusting or\nemploying heuristic methods for forward-looking planning.\n5.3. Case Studies\nTo investigate the drawbacks of current agents in-depth, we\nprovide several failure cases in Figure 3. We conclude with\nthe following features:\nAgents fail to complete a plan due to the inability to rec-\ntify persistent errors. In tool-use scenarios, agents often\nfail to deliver a plan even when all preceding steps are exe-\ncuted correctly. Further investigation reveals that this issue\noften stems from incorrect date inputs. As shown in the left\npart of Figure 3, despite correct execution, agents repeatedly\nuse incorrect dates. This leads to null results, as the data\nin the TravelPlanner sandbox is based on 2022. Such re-\npeated failures eventually cause the agents to stop planning.\nThis indicates a significant limitation: current agents cannot\nself-correct their initial and incorrect assumptions.\nAgents produce hallucinatory answers due to informa-\ntion confusion. To understand why agents provide hallu-\ncinatory answers even when supplied with sufficient infor-\nmation in the sole-planning mode, we conduct a detailed\nanalysis. We observe a tendency for agents to confuse one\npiece of information with another. As shown in the middle\npart of Figure 3, agents mistakenly use the same flight num-\nber for both departure and return flights. Such errors result\nin hallucinations, as the information provided in the plan\ndoes not align with the data in the sandbox. This suggests\nthat agents might be lost when faced with mass information,\nknown as \u201cLost in the Middle\u201d (Liu et al., 2023b).\nAgents struggle to align their actions with their reason-\ning. To understand the reasons behind the lower delivery\nrate of Reflexion (Shinn et al., 2023), we examine specific\nexamples. As illustrated in the right part of Figure 3, we\nobserve a discrepancy between what agents think and what\nthey do. Despite recognizing the necessity to minimize\ncosts, they tend to randomly select items, some of which\nmay be more expensive. This discrepancy demonstrates\nthat agents struggle to synchronize their actions with their\nanalytical reasoning, severely impeding their delivery rate.\n6. Conclusion\nWe introduce TravelPlanner, a benchmark grounded in real-\nworld scenarios, designed to assess the multi-constraint\nplanning and tool-use abilities of current language agents.\nOur benchmark presents a significant challenge: even the\nmost advanced language agent frameworks only achieve\n8\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\na mere 0.6% score in the final pass rate. Further analysis\nshows that these agents are unable to take all constraints\ninto consideration to deliver feasible plans.\nTravelPlanner\u2019s intricate logic and general applicability\nstand as vital components in the progressive development\nof language agents, thus contributing to the broader quest\nfor AI abilities. We envision TravelPlanner as a catalyst\nfor future research, aiming to enhance agents\u2019 performance\nin increasingly complex scenarios, hill-climbing towards\nhuman-level cognitive capabilities.\n7. Impact Statements\nTravelPlanner aims to provide an effective benchmark for\ncomplex planning in future research. Some of the data in the\nTravelPlanner environment is derived from publicly avail-\nable data on the Internet, and the content involved does not\nrepresent the authors\u2019 viewpoints. We realize that every-\none\u2019s definition of commonsense may be different. Our\ncurrent evaluation criteria are based on the authors\u2019 con-\nsensus, and we encourage additional discussions to enrich\nour commonsense dimension, aiming for a more thorough\nevaluation. We will release our evaluation scripts to foster\ninnovation and aid the development of new methods. We\nencourage the use of evaluation feedback in training set,\nsuch as implementing reinforcement learning techniques, to\nenhance learning. However, we strictly prohibit any form of\ncheating in the validation and test sets to uphold the fairness\nand reliability of the benchmark\u2019s evaluation process.\nReferences\nAlterovitz, R., Koenig, S., and Likhachev, M. Robot plan-\nning in the real world: Research challenges and opportu-\nnities. Ai Magazine, 2016.\nAndreas, J. Language models as agent models. In Findings\nof EMNLP, 2022.\nAutoGPT. Autogpt, 2023. URL https://github.com/\nSignificant-Gravitas/AutoGPT.\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\nSolving elaborate problems with large language models.\narXiv preprint arXiv:2308.09687, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nIn Proceedings of NeurIPS, 2020.\nCampbell, M., Hoane Jr, A. J., and Hsu, F.-h. Deep blue.\nArtificial intelligence, 2002.\nChen, H., Pasunuru, R., Weston, J., and Celikyilmaz,\nA.\nWalking down the memory maze: Beyond con-\ntext limit through interactive reading. arXiv preprint\narXiv:2310.05029, 2023.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nCross, S. and Estrada, R. Dart: an example of accelerated\nevolutionary development. In Proceedings of Workshop\non RSP, 1994.\nDeng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang,\nB., Sun, H., and Su, Y. Mind2web: Towards a generalist\nagent for the web. In Proceedings of NeurIPS, 2023.\nG Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,\nYu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A.,\net al. Gemini: a family of highly capable multimodal\nmodels. arXiv preprint arXiv:2312.11805, 2023.\nGe, Y., Hua, W., Ji, J., Tan, J., Xu, S., and Zhang, Y. Ope-\nnagi: When llm meets domain experts. In Proceedings of\nNeurIPS, 2023.\nGeorgievski, I. and Aiello, M. Htn planning: Overview,\ncomparison, and beyond. Artificial Intelligence, 2015.\nGrafman, J., Spector, L., and Rattermann, M. J. Planning\nand the brain. In The cognitive psychology of planning.\n2004.\nHayes-Roth, B. and Hayes-Roth, F. A cognitive model of\nplanning. Cognitive science, 1979.\nHo, M. K., Saxe, R., and Cushman, F. Planning with theory\nof mind. Trends in Cognitive Sciences, 2022.\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Lan-\nguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In Proceedings of\nICML, 2022.\nIchter, B., Brohan, A., Chebotar, Y., Finn, C., Hausman,\nK., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E.,\nJulian, R., Kalashnikov, D., Levine, S., Lu, Y., Parada, C.,\nRao, K., Sermanet, P., Toshev, A., Vanhoucke, V., Xia, F.,\nXiao, T., Xu, P., Yan, M., Brown, N., Ahn, M., Cortes,\nO., Sievers, N., Tan, C., Xu, S., Reyes, D., Rettinghouse,\nJ., Quiambao, J., Pastor, P., Luu, L., Lee, K., Kuang, Y.,\nJesmonth, S., Joshi, N. J., Jeffrey, K., Ruano, R. J., Hsu,\nJ., Gopalakrishnan, K., David, B., Zeng, A., and Fu, C. K.\nDo as I can, not as I say: Grounding language in robotic\naffordances. In Proceedings of CoRL, 2022.\n9\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A.,\nSavary, B., Bamford, C., Chaplot, D. S., de las Casas,\nD., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G.,\nLample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-\nA., Stock, P., Subramanian, S., Yang, S., Antoniak, S.,\nScao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T.,\nand Sayed, W. E. Mixtral of experts. arXiv preprint\narXiv:2401.04088, 2024.\nKarpas, E. and Magazzeni, D. Automated planning for\nrobotics. Annual Review of Control, Robotics, and Au-\ntonomous Systems, 2020.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,\nMichalewski, H., Ramasesh, V. V., Slone, A., Anil, C.,\nSchlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-\nAri, G., and Misra, V. Solving quantitative reasoning prob-\nlems with language models. In Proceedings of NeurIPS,\n2022.\nLi, M., Zhao, Y., Yu, B., Song, F., Li, H., Yu, H., Li, Z.,\nHuang, F., and Li, Y. Api-bank: A comprehensive bench-\nmark for tool-augmented llms. In Proceedings of EMNLP,\n2023.\nLiang, X., Wang, B., Huang, H., Wu, S., Wu, P., Lu, L., Ma,\nZ., and Li, Z. Unleashing infinite-length input capacity for\nlarge-scale language models with self-controlled memory\nsystem. arXiv preprint arXiv:2304.13343, 2023.\nLiu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas,\nJ., and Stone, P. Llm+ p: Empowering large language\nmodels with optimal planning proficiency. arXiv preprint\narXiv:2304.11477, 2023a.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilac-\nqua, M., Petroni, F., and Liang, P. Lost in the middle:\nHow language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023b.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu,\nY., Ding, H., Men, K., Yang, K., et al. Agentbench:\nEvaluating llms as agents. In Proceedings of ICLR, 2024.\nLu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu,\nY. N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-play\ncompositional reasoning with large language models. In\nProceedings of NeurIPS, 2023.\nMattar, M. G. and Lengyel, M. Planning in the brain. Neu-\nron, 2022.\nMcDermott, D. Robot planning. AI magazine, 1992.\nNakajima, Y. Task-driven autonomous agent utilizing gpt-4,\npinecone, and langchain for diverse applications, 2023.\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,\nC., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.\nWebgpt: Browser-assisted question-answering with hu-\nman feedback. arXiv preprint arXiv:2112.09332, 2021.\nOpenAI.\nChatgpt, 2022.\nURL https://openai.com/\nblog/chatgpt.\nOpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023.\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of UIST,\n2023.\nPatel, A., Bhattamishra, S., and Goyal, N. Are NLP models\nreally able to solve simple math word problems?\nIn\nProceedings of NAACL, 2021.\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla:\nLarge language model connected with massive apis. arXiv\npreprint arXiv:2305.15334, 2023.\nPinedo, M. Planning and scheduling in manufacturing and\nservices. Springer, 2005.\nQin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y.,\nCong, X., Tang, X., Qian, B., et al. Toolllm: Facilitating\nlarge language models to master 16000+ real-world apis.\nIn Proceedings of ICLR, 2024.\nRoy, S. and Roth, D.\nSolving general arithmetic word\nproblems. In Proceedings of EMNLP, 2015.\nRussell, S. J. and Norvig, P. Artificial intelligence a modern\napproach. 2010.\nSchick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli,\nM., Hambro, E., Zettlemoyer, L., Cancedda, N., and\nScialom, T. Toolformer: Language models can teach\nthemselves to use tools.\nIn Proceedings of NeurIPS,\n2023.\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang,\nY. HuggingGPT: Solving AI tasks with chatGPT and\nits friends in hugging face. In Proceedings of NeurIPS,\n2023.\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R.,\nand Yao, S. Reflexion: Language agents with verbal\nreinforcement learning. In Proceedings of NeurIPS, 2023.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. Mastering the\n10\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\ngame of go with deep neural networks and tree search.\nnature, 2016.\nSilver, D., Hasselt, H., Hessel, M., Schaul, T., Guez, A.,\nHarley, T., Dulac-Arnold, G., Reichert, D., Rabinowitz,\nN., Barreto, A., et al. The predictron: End-to-end learning\nand planning. In Proceedings of ICML, 2017.\nSong, C. H., Wu, J., Washington, C., Sadler, B. M., Chao,\nW.-L., and Su, Y. Llm-planner: Few-shot grounded plan-\nning for embodied agents with large language models. In\nProceedings of ICCV, 2023.\nSu, Y. Language agents: a critical evolutionary step of artifi-\ncial intelligence. 2023. URL https://yusu.substack.\ncom/p/language-agents.\nSumers, T. R., Yao, S., Narasimhan, K., and Griffiths,\nT. L. Cognitive architectures for language agents. arXiv\npreprint arXiv:2309.02427, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nValmeekam, K., Olmo, A., Sreedharan, S., and Kambham-\npati, S. Planbench: An extensible benchmark for evalu-\nating large language models on planning and reasoning\nabout change. In Proceedings of NeurIPS, 2023.\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,\nY., Fan, L., and Anandkumar, A. Voyager: An open-\nended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models. In\nProceedings of NeurIPS, 2022.\nWeng, L.\nLlm-powered autonomous agents.\nlilian-\nweng.github.io, Jun 2023. URL https://lilianweng.\ngithub.io/posts/2023-06-23-agent/.\nXie, T., Zhou, F., Cheng, Z., Shi, P., Weng, L., Liu, Y., Hua,\nT. J., Zhao, J., Liu, Q., Liu, C., Liu, L. Z., Xu, Y., Su, H.,\nShin, D., Xiong, C., and Yu, T. Openagents: An open\nplatform for language agents in the wild. arXiv preprint\narXiv:2310.10634, 2023.\nXu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang,\nJ. On the tool manipulation capability of open-source\nlarge language models. arXiv preprint arXiv:2305.16504,\n2023.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK. R., and Cao, Y. React: Synergizing reasoning and\nacting in language models. In Proceedings of ICLR, 2022.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y.,\nand Narasimhan, K. R. Tree of thoughts: Deliberate prob-\nlem solving with large language models. In Proceedings\nof NeurIPS, 2023.\nYuan, S., Chen, J., Fu, Z., Ge, X., Shah, S., Jankowski, C.,\nXiao, Y., and Yang, D. Distilling script knowledge from\nlarge language models for constrained language planning.\nIn Proceedings of ACL, 2023.\nZheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v(ision)\nis a generalist web agent, if grounded. arXiv preprint\narXiv:2401.01614, 2024.\nZhong, W., Guo, L., Gao, Q., and Wang, Y. Memorybank:\nEnhancing large language models with long-term mem-\nory. arXiv preprint arXiv:2305.10250, 2023.\nZhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A.,\nCheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena:\nA realistic web environment for building autonomous\nagents. In Proceedings of ICLR, 2024.\nZhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou,\nY., Cotterell, R., and Sachan, M. Recurrentgpt: Interac-\ntive generation of (arbitrarily) long text. arXiv preprint\narXiv:2305.13304, 2023.\nZhuang, Y., Yu, Y., Wang, K., Sun, H., and Zhang, C.\nToolQA: A dataset for LLM question answering with\nexternal tools. In Proceedings of NeurIPS, 2023.\nZhuang, Y., Chen, X., Yu, T., Mitra, S., Bursztyn, V., Rossi,\nR. A., Sarkhel, S., and Zhang, C. Toolchain*: Efficient\naction space navigation in large language models with a*\nsearch. In Proceedings of ICLR, 2024.\n11\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nAppendices\nWithin this supplementary material, we elaborate on the following aspects:\n\u2022 Appendix A: Benchmark Details\n\u2022 Appendix B: Experiment Details\n\u2022 Appendix C: Case Presentation\nA. Benchmark Details\nA.1. Dataset Distribution\nIn Table A.1, we list the detailed group distribution on training, validation and test set.\nTable A.1. Dataset distribution.\nDays\nEasy\nMedium\nHard\nTraining (#45)\n3-day\n5\n5\n5\n5-day\n5\n5\n5\n7-day\n5\n5\n5\nValidation (#180)\n3-day\n20\n20\n20\n5-day\n20\n20\n20\n7-day\n20\n20\n20\nTest (#1,000)\n3-day\n122\n104\n82\n5-day\n116\n114\n121\n7-day\n110\n115\n116\nA.2. Tool Description\nIn Table A.2, we list the detailed tool description. The original data for each tool is sourced from publicly available Internet\ndata. We then modify this data, which includes adding, deleting, and altering certain keys and values to suit our requirements.\nIn this way, we effectively avoid the problem of data contamination. For more details, please refer to Appendix A.3.\nTable A.2. Tool description and the number of data entries in the database.\nTool\nData Entries(#)\nDescription\nCitySearch\n312\nSearch cities in the given state.\nFlightSearch\n3,827,361\nSearch flight information for a specific date between two cities.\nDistanceMatrix\n17,603\nSearch the driving distance, time, and possible cost between two cities.\nRestaurantSearch\n9,552\nSearch restaurants in the given city.\nAttractionSearch\n5,303\nSearch attractions in the given city.\nAccommodationSearch\n5,064\nSearch accommodations in the given city.\nNotebookWrite\n-\nWrite the selected data entry into the Notebook tool with a short description.\nA.3. Environment Database Construction\nFlightSearch\nFor FlightSearch, we source original data from the Kaggle Flight Status Prediction dataset2. From this\ndataset, we extract data spanning from 2022-03-01 to 2022-04-01. We specifically included fields like \u201cFlightDate\u201d,\n\u201cDepTime\u201d, \u201cArrTime\u201d, \u201cActualElapsedTime\u201d, \u201cDistance\u201d, \u201cOriginCityName\u201d, and \u201cDestCityName\u201d while discarding other\nvalues. To incorporate \u201cPrice\u201d into our dataset, we generate this value by multiplying the \u201cDistance\u201d by a random factor\n2https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022?select=Combined_Flights_2022.\ncsv\n12\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nranging from 0.2 to 0.5.\nDistanceMatrix\nWe utilize the Google Distance Matrix API3 to calculate the driving distance and estimated travel time\nbetween two cities. For the \u201cself-driving\u201d and \u201ctaxi\u201d modes of transportation, we calculate the \u2019Price\u2019 by multiplying the\n\u2019Distance\u2019 by factors of 1 and 0.15, respectively. To ensure consistency and reliability of data, we store the search results in\nour database, thereby creating a fixed dataset for our evaluations.\nRestaurantSearch\nOur restaurant data is sourced from the Kaggle Zomato Restaurants Dataset4. From this dataset,\nwe extract the \u201cRestaurant Name\u201d and \u201cAverage Cost\u201d for each establishment. Subsequently, we randomly assign these\nrestaurants to various cities. To align with the constraint requirements of TravelPlanner, we also randomly categorize each\nrestaurant under the following cuisines: \u201cChinese\u201d, \u201cAmerican\u201d, \u201cItalian\u201d, \u201cMexican\u201d, \u201cIndian\u201d,\u201cMediterranean\u201d, \u201cMiddle\nEastern\u201d, \u201cKorean\u201d, \u201cAsian\u201d, \u201cFrench\u201d.\nAttractionSearch\nFor AttractionSearch, we employ the Google Places API5 to gather information about attractions in\neach city. In TravelPlanner, we retain essential details such as \u201cName\u201d, \u201cAddress\u201d, \u201cPhone\u201d, \u201cWebsite\u201d, \u201cLatitude\u201d, and\n\u201cLongtitue\u201d for each attraction. To maintain data consistency and reliability, we store these search results in our database,\ncreating a standardized dataset for our analyses.\nAccommodationSearch\nOur accommodation data is obtained from the Kaggle Airbnb Open Data Dataset6. From this\ndataset, we extract key details \u201cNAME\u201d, \u201croom type\u201d, \u201cprice\u201d, \u201cminimum nights\u201d, \u201creview rate number\u201d, and \u201cmaximum\noccupancy\u201d. Items are then randomly assigned to various cities. To meet the specific constraint requirements of TravelPlanner,\nwe also assign each item random room rules, including \u201cNo parties\u201d, \u201cNo smoking\u201d, \u201cNo children under 10\u201d, \u201cNo pets\u201d,\nand \u201cNo visitors\u201d.\nB. Experiment Details\nB.1. Baselines\nGreedy Search\nTo assess the effectiveness of traditional search algorithms in TravelPlanner, we integrate a greedy search\napproach, focusing on minimizing costs. For 5 or 7-day travel plans, the first one or two cities in the returned city search\nresult are selected as destinations. The transportation choice is based on the lowest cost option among flights, taxis, and\nself-driving. The diet component involves selecting the restaurant with the lowest average cost. The cheapest accommodation\nis chosen for lodging. For attractions, we opt for a random selection for each day of the itinerary.\nPlanning Strategy\nCurrent planning strategies have shown effectiveness in traditional tasks like mathematical problem-\nsolving, but their capability to handle the more complex and constrained scenarios like TravelPlanner remains to be seen.\nTo explore this, we evaluate four distinct planning strategies on TravelPlanner: 1) Direct: In this method, the query is\ninput directly into the model along with instructions detailing the task and relevant information gathered. 2) ZS-CoT (Wei\net al., 2022): This strategy enhances the reasoning process by requiring intermediate steps. Building on the Direct method,\nwe add the prompt \u201cLet\u2019s think step by step\u201d to elicit reasoning. 3) ReAct (Yao et al., 2022): This strategy incorporates\nenvironmental feedback into the reasoning process. Thus, it enhances the language agent\u2019s reasoning ability by offering\nadditional information. In TravelPlanner, we provide the cost associated with each entire day\u2019s plan as environmental\nfeedback. 4) Reflexion (Shinn et al., 2023): This approach utilizes a reflection model to provide high-level insights on\nprevious erroneous attempts. Such reflective guidance aids language agents in identifying and correcting flawed reasoning.\nIn order to control the cost, we conduct tests on Direct using four different models, while the other strategies are evaluated\nusing GPT-3.5-Turbo. Detailed instructions for each strategy are available in Appendix B.3.\nB.2. GPT-4-Turbo with Reflexion strategy in sole-planning mode.\nWe provide the results of GPT-4-Turbo with Reflexion strategy on validation set in Table B.3.\n3https://developers.google.com/maps/documentation/distance-matrix/overview?hl=en\n4https://www.kaggle.com/datasets/shrutimehta/zomato-restaurants-data\n5https://developers.google.com/maps/documentation/places/web-service/overview?hl=en\n6https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata\n13\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nTable B.3. GPT-4-Turbo with Reflexion strategy on validation set.\nDelivery\nRate\nCommonsense\nPass Rate\nHard Constraint\nPass Rate\nFinal\nPass Rate\nMicro\nMacro\nMicro\nMacro\nReflexionGPT\u22124\u2212Turbo\n80.6\n62.9\n6.1\n52.4\n40.0\n3.3\nB.3. Prompt List\nB.3.1. TOOL-USE PROMPT\nWe tailor the ReAct (Yao et al., 2022) framework to suit our specific requirements in TravelPlanner. An example of the\ninstruction prompt for our needs is as follows:\n\u0007\n\u0004\nCollect information for a query plan using interleaving 'Thought ', 'Action ', and\n'\nObservation ' steps. Ensure you gather valid information related to transportation , dining ,\nattractions , and\naccommodation. All information should be written in Notebook , which\nwill then be input into the Planner\ntool. Note that the nested use of tools is prohibited\n. 'Thought ' can reason about the current situation , and 'Action ' can have 8 different\ntypes:\n(1) FlightSearch[Departure City , Destination City , Date]:\nDescription:\nA flight information retrieval tool.\nParameters:\nDeparture City: The city you 'll be flying out from.\nDestination City: The city you aim to reach.\nDate: The date of your travel in YYYY -MM -DD format.\nExample: FlightSearch[New York , London , 2022 -10 -01] would fetch flights from New York to\nLondon on October 1, 2022.\n(2) DistanceMatrix[Origin , Destination , Mode]:\nDescription: Estimate the distance , time and cost between two cities.\nParameters:\nOrigin: The departure city of your journey.\nDestination: The destination city of your journey.\nMode: The method of transportation. Choices include 'self -driving ' and 'taxi '.\nExample: DistanceMatrix[Paris , Lyon , self -driving] would provide driving distance , time\nand cost between Paris and Lyon.\n(3) AccommodationSearch[City]:\nDescription: Discover accommodations in your desired city.\nParameter: City - The name of the city where you 're seeking accommodation.\nExample: AccommodationSearch[Rome] would present a list of hotel rooms in Rome.\n(4) RestaurantSearch[City]:\nDescription: Explore dining options in a city of your choice.\nParameter: City - The name of the city where you 're seeking restaurant.\nExample: RestaurantSearch[Tokyo] would show a curated list of restaurants in Tokyo.\n(5) AttractionSearch[City]:\nDescription: Find attractions in a city of your choice.\nParameter: City - The name of the city where you 're seeking attractions.\nExample: AttractionSearch[London] would return attractions in London.\n(6) CitySearch[State]\nDescription: Find cities in a state of your choice.\nParameter: State - The name of the city where you 're seeking cities.\nExample: CitySearch[California] would return cities in California.\n(7) NotebookWrite[Short Description]\nDescription: Writes a new data entry into the Notebook tool with a short description. This\ntool should be used immediately after FlightSearch , AccommodationSearch , AttractionSearch\n14\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n, RestaurantSearch or DistanceMatrix. Only the data stored in Notebook can be seen by\nPlanner. So you should write all the information you need into Notebook.\nParameters: Short Description - A brief description or label for the stored data.\nYou don 't need to write all the information in the description.\nThe data you 've searched for will be automatically stored in the Notebook.\nExample: NotebookWrite[Flights from Rome to Paris in 2022 -02 -01] would store the\ninformatrion of flights from Rome to Paris in 2022 -02 -01 in the Notebook.\n(8) Planner[Query]\nDescription: A smart planning tool that crafts detailed plans based on user input and the\ninformation stroed in Notebook.\nParameters:\nQuery: The query from user.\nExample: Planner[Give me a 3-day trip plan from Seattle to New York] would return a\ndetailed 3-day trip plan.\nYou should use as many as possible steps to collect engough information to input to the\nPlanner tool.\nEach action only calls one function once. Do not add any description in the action.\nQuery: {query}\n\u0006\n\u0005\nB.3.2. DIRECT PLANNING PROMPT\nWe provide the instruction prompt of Direct strategy as follows:\n\u0007\n\u0004\nYou are a proficient planner. Based on the provided information and query , please give me\na detailed plan , including specifics such as flight numbers (e.g., F0123456), restaurant\nnames , and accommodation names. Note that all the information in your plan should be\nderived from the provided data. You must adhere to the format given in the example.\nAdditionally , all details should align with commonsense. The symbol '-' indicates that\ninformation is unnecessary. For example , in the provided sample , you do not need to plan\nafter returning to the departure city. When you travel to two cities in one day , you\nshould note it in the 'Current City ' section as in the example (i.e., from A to B).\n***** Example *****\nQuery: Could you create a challenging travel plan for 7 people from Roanoke to Illinois\nspanning a week , from March 8th to March 14th , 2022, with a budget of $30 ,200? The\npreference is for an entire room , and we would not be taking any flights. In terms of\ncuisine , we are interested in sampling some Italian and Chinese food.\nTravel Plan:\nDay 1:\nCurrent City: from Ithaca to Charlotte\nTransportation: Flight Number: F3633413 , from Ithaca to Charlotte , Departure Time: 05:38 ,\nArrival Time: 07:46\nBreakfast: Nagaland 's Kitchen , Charlotte\nAttraction: The Charlotte Museum of History , Charlotte\nLunch: Cafe Maple Street , Charlotte\nDinner: Bombay Vada Pav , Charlotte\nAccommodation: Affordable Spacious Refurbished Room in Bushwick!, Charlotte\nDay 2:\nCurrent City: Charlotte\nTransportation: -\nBreakfast: Olive Tree Cafe , Charlotte\nAttraction: The Mint Museum , Charlotte;Romare Bearden Park , Charlotte.\nLunch: Birbal Ji Dhaba , Charlotte\nDinner: Pind Balluchi , Charlotte\nAccommodation: Affordable Spacious Refurbished Room in Bushwick!, Charlotte\nDay 3:\nCurrent City: Charlotte\nTransportation: Flight Number: F3786167 , from Charlotte to Ithaca ,\nDeparture Time: 21:42, Arrival Time: 23:26\n15\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nBreakfast: Subway , Charlotte\nAttraction: Books Monument , Charlotte.\nLunch: Olive Tree Cafe , Charlotte\nDinner: Kylin Skybar , Charlotte\nAccommodation: -\n***** Example Ends *****\nGiven information: {text}\nQuery: {query}\nTravel Plan:\n\u0006\n\u0005\nB.3.3. REACT & REFLEXION PLANNING PROMPT\nThe instruction prompts for the React and Reflexion planning strategies are provided as follows:\n\u0007\n\u0004\nYou are a proficient planner. Based on the provided information and query , please give me\na detailed plan , including specifics such as flight numbers (e.g., F0123456), restaurant\nnames , and hotel names. Note that all the information in your plan should be derived from\nthe provided data. You must adhere to the format given in the example. Additionally , all\ndetails should align with common sense. Attraction visits and meals are expected to be\ndiverse. The symbol '-' indicates that information is unnecessary. For example , in the\nprovided sample , you do not need to plan after returning to the departure city. When you\ntravel to two cities in one day , you should note it in the 'Current City ' section as in\nthe example (i.e., from A to B). Solve this task by alternating between Thought , Action ,\nand Observation steps. The 'Thought ' phase involves reasoning about the current situation.\nThe 'Action ' phase can be of two types:\n(1) CostEnquiry[Sub Plan]: This function calculates the cost of a detailed sub plan , which\nyou need to input the people number and plan in JSON format. The sub plan should\nencompass a complete one -day plan. An example will be provided for reference.\n(2) Finish[Final Plan]: Use this function to indicate the completion of the task.\nYou must submit a final , complete plan as an argument.\n***** Example *****\nQuery: Could you create a challenging travel plan for 7 people from Roanoke to Illinois\nspanning a week , from March 8th to March 14th , 2022, with a budget of $30 ,200? The\npreference is for an entire room , and we would not be taking any flights. In terms of\ncuisine , we are interested in sampling some Italian and Chinese food.You can call\nCostEuquiry like CostEuquiry [{{\" people_number \": 7,\"day\": 1,\" current_city \": \"from Ithaca to\nCharlotte \",\" transportation \": \"Flight Number: F3633413 , from Ithaca to Charlotte ,\nDeparture Time: 05:38, Arrival Time: 07:46\" ,\" breakfast \": \"Nagaland 's Kitchen , Charlotte \",\"\nattraction \": \"The Charlotte Museum of History , Charlotte \",\"lunch \": \"Cafe Maple Street ,\nCharlotte \",\"dinner \": \"Bombay Vada Pav , Charlotte \",\" accommodation \": \"Affordable Spacious\nRefurbished Room in Bushwick!, Charlotte \"}}]\nYou can call Finish like Finish[\nDay: 1\nCurrent City: from Ithaca to Charlotte\nTransportation: Flight Number: F3633413 , from Ithaca to Charlotte , Departure Time: 05:38 ,\nArrival Time: 07:46\nBreakfast: Nagaland 's Kitchen , Charlotte\nAttraction: The Charlotte Museum of History , Charlotte\nLunch: Cafe Maple Street , Charlotte\nDinner: Bombay Vada Pav , Charlotte\nAccommodation: Affordable Spacious Refurbished Room in Bushwick!, Charlotte\nDay 2:\nCurrent City: Charlotte\nTransportation: -\nBreakfast: Olive Tree Cafe , Charlotte\nAttraction: The Mint Museum , Charlotte;Romare Bearden Park , Charlotte.\nLunch: Birbal Ji Dhaba , Charlotte\nDinner: Pind Balluchi , Charlotte\nAccommodation: Affordable Spacious Refurbished Room in Bushwick!, Charlotte\n16\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nDay 3:\nCurrent City: Charlotte\nTransportation: Flight Number: F3786167 , from Charlotte to Ithaca , Departure Time: 21:42 ,\nArrival Time: 23:26\nBreakfast: Subway , Charlotte\nAttraction: Books Monument , Charlotte.\nLunch: Olive Tree Cafe , Charlotte\nDinner: Kylin Skybar , Charlotte\nAccommodation: -]\n***** Example Ends *****\nYou must use Finish to indict you have finished the task. And each action only calls one\nfunction once.\nGiven information: {text}\nQuery: {query}\n\u0006\n\u0005\nB.3.4. QUERY GENERATION PROMPT\nThe instruction prompt for query generation is provided as follows:\n\u0007\n\u0004\nGiven a JSON , please help me generate a natural language query. In the JSON , 'org ' denotes\nthe departure city. When 'days ' exceeds 3, 'visiting_city_number ' specifies the number of\ncities to be covered in the destination state. Here are three examples.\n-----EXAMPLE 1-----\nJSON:\n{\"org\": \"Gulfport\", \"dest\": \"Charlotte\", \"days\": 3, \"visiting_city_number \": 1, \"date\":\n[\"2022 -03 -05\" , \"2022 -03 -06\" , \"2022 -03 -07\"] , \"people_number \": 1, \"constraint \": {\"room rule\n\": null , \"cuisine \": null , \"room type\": null}, \"budget \": 1800}\nQUERY:\nPlease design a travel plan departing Gulfport and heading to Charlotte for 3 days ,\nspanning March 5th to March 7th, 2022, with a budget of $1800.\n-----EXAMPLE 2-----\nJSON:\n{\"org\": \"Omaha\", \"dest\": \"Colorado\", \"days\": 5, \"visiting_city_number \": 2, \"date\":\n[\"2022 -03 -14\" , \"2022 -03 -15\" , \"2022 -03 -16\" , \"2022 -03 -17\" , \"2022 -03 -18\"] , \"people_number \":\n7, \"constraint \": {\"room rule\": \"pets\", \"cuisine \": null , \"room type\": null}, \"budget \":\n35300}\nQUERY:\nCould you provide a\n5-day travel itinerary for a group of 7, starting in Omaha and\nexploring 2 cities in Colorado between March 14th and March 18th , 2022? Our budget is set\nat $35 ,300, and it 's essential that our accommodations be pet -friendly since we 're\nbringing our pets.\n-----EXAMPLE 3-----\nJSON:\n{\"org\": \"Indianapolis\", \"dest\": \"Georgia\", \"days\": 7, \"visiting_city_number \": 3, \"date\":\n[\"2022 -03 -01\" , \"2022 -03 -02\" , \"2022 -03 -03\" , \"2022 -03 -04\" , \"2022 -03 -05\" , \"2022 -03 -06\" ,\n\"2022 -03 -07\"] , \"people_number \": 2, \"constraint \": {\"room rule\": null , \"cuisine \": [\" Bakery\",\n\"Indian\"], \"room type\": \"entire room\", \"transportation \": \"self driving\"}, \"budget \": 6200}\nQUERY:\nI'm looking for a week -long travel itinerary for 2 individuals. Our journey starts in\nIndianapolis , and we intend to explore 3 distinct cities in Georgia from March 1st to\nMarch 7th , 2022. Our budget is capped at $6 ,200. For our accommodations , we'd prefer an\nentire room. We plan to navigate our journey via self -driving. In terms of food , we're\nenthusiasts of bakery items , and we'd also appreciate indulging in genuine Indian cuisine.\n-----EXAMPLES END -----\nJSON: {json}\nQUERY:\n\u0006\n\u0005\nB.3.5. KEY COMPONENTS EXTRACTION PROMPT\nThe instruction prompt for plan key components extraction is provided as follows:\n17\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\u0007\n\u0004\nPlease assist me in extracting valid information from a given natural language text and\nreconstructing it in JSON format , as demonstrated in the following example. Use a ';' to\nseparate different attractions , with each attraction formatted as 'Name , City '. If there 's\ninformation about transportation , ensure that the 'current_city ' aligns with the\ndestination mentioned in the transportation details (i.e., the current city should follow\nthe format 'from A to B'). Also , ensure that all flight numbers and costs are followed by\na colon (i.e., 'Flight Number:' and 'Cost:'), consistent with the provided example. Each\nitem should include ['day ', 'current_city ', 'transportation ', 'breakfast ', 'attraction ', '\nlunch ', 'dinner ', 'accommodation ']. Replace non -specific information like 'eat at home/on\nthe road ' with '-'. Additionally , delete any '$' symbols.\n-----EXAMPLE -----\n[{{\n\"days\": 1,\n\"current_city \": \"from Dallas to Peoria\",\n\"transportation \": \"Flight Number: 4044830 , from Dallas to Peoria , Departure Time:\n13:10 , Arrival Time: 15:01\" ,\n\"breakfast \": \"-\",\n\"attraction \": \"Peoria Historical Society , Peoria;Peoria Holocaust Memorial , Peoria\n;\",\n\"lunch\": \"-\",\n\"dinner \": \"Tandoor Ka Zaika , Peoria\",\n\"accommodation \": \"Bushwick Music Mansion , Peoria\"\n}},\n{{\n\"days\": 2,\n\"current_city \": \"Peoria\",\n\"transportation \": \"-\",\n\"breakfast \": \"Tandoor Ka Zaika , Peoria\",\n\"attraction \": \"Peoria Riverfront Park , Peoria;The Peoria PlayHouse , Peoria;Glen\nOak Park , Peoria;\",\n\"lunch\": \"Cafe Hashtag LoL , Peoria\",\n\"dinner \": \"The Curzon Room - Maidens Hotel , Peoria\",\n\"accommodation \": \"Bushwick Music Mansion , Peoria\"\n}},\n{{\n\"days\": 3,\n\"current_city \": \"from Peoria to Dallas\",\n\"transportation \": \"Flight Number: 4045904 , from Peoria to Dallas , Departure Time:\n07:09 , Arrival Time: 09:20\" ,\n\"breakfast \": \"-\",\n\"attraction \": \"-\",\n\"lunch\": \"-\",\n\"dinner \": \"-\",\n\"accommodation \": \"-\"\n}}]\n-----EXAMPLE ENDS -----\nText: {text}\nJSON:\n\u0006\n\u0005\nC. Case Presentation\nC.1. Example of Query and Reference Plan\nwe present an example of a query and its corresponding reference plan in our train set as follows:\n\u0007\n\u0004\n{\n\"org\": \"Indianapolis\",\n\"dest\": \"Colorado\",\n\"days\": 7,\n\"visiting_city_number \": 3,\n\"date\": [\n\"2022 -03 -11\" ,\n\"2022 -03 -12\" ,\n18\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\"2022 -03 -13\" ,\n\"2022 -03 -14\" ,\n\"2022 -03 -15\" ,\n\"2022 -03 -16\" ,\n\"2022 -03 -17\"\n],\n\"people_number \": 5,\n\"room rule\": \"pets\",\n\"cuisine \": [\n\"Mexican\",\n\"Italian\",\n\"Mediterranean\",\n\"Indian\"\n],\n\"room type\": \"entire room\",\n\"transportation \": null ,\n\"budget \": 15100,\n\"query \": \"Can you help with generating a 7-day travel plan for a party of 5? We 're\nsetting off from Indianapolis and planning to explore 3 cities in Colorado from March\n11th to March 17th, 2022. We have a budget of $15 ,100 for this trip. We'll be bringing\nour pets , so pet -friendly accommodations are a must. We 're also hoping to find places\nthat offer Mexican , Italian , Mediterranean , and Indian cuisines. Entire rooms for\naccommodations would be ideal.\",\n\"level \": \"hard\",\n\"annotated plan\": [\n{\n\"days\": 1,\n\"current_city \": \"from Indianapolis to Grand Junction(Colorado)\",\n\"transportation \": \"Self -driving , from Indianapolis to Grand Junction(Colorado)\n, duration: 19 hours 21 mins , distance: 2,132 km , cost: 106\",\n\"breakfast \": \"-\",\n\"attraction \": \"-\",\n\"lunch\": \"-\",\n\"dinner \": \"Nukkadwala , Grand Junction(Colorado)\",\n\"accommodation \": \"Lovely 1 BD on the Upper West Side , Grand Junction(Colorado)\n\"\n},\n{\n\"days\": 2,\n\"current_city \": \"Grand Junction(Colorado)\",\n\"transportation \": \"-\",\n\"breakfast \": \"Om Ji Bhature Wale , Grand Junction(Colorado)\",\n\"attraction \": \"Museum of the West , Museums of Western Colorado , Grand Junction\n(Colorado);Eureka! McConnell Science Museum , Grand Junction(Colorado);\",\n\"lunch\": \"Penta Cafe , Grand Junction(Colorado)\",\n\"dinner \": \"Kings Kulfi , Grand Junction(Colorado)\",\n\"accommodation \": \"Lovely 1 BD on the Upper West Side , Grand Junction(Colorado)\n\"\n},\n{\n\"days\": 3,\n\"current_city \": \"from Grand Junction(Colorado) to Alamosa(Colorado)\",\n\"transportation \": \"Self -driving , from Grand Junction(Colorado) to Alamosa(\nColorado), duration: 4 hours 37 mins , distance: 397 km , cost: 19\",\n\"breakfast \": \"Punjab Da Pind , Grand Junction(Colorado)\",\n\"attraction \": \"Alamosa Colorado Welcome Center , Alamosa(Colorado);Toivo Malm\nTrail System , Alamosa(Colorado);\",\n\"lunch\": \"Emperor 's Lounge - The Taj Mahal Hotel , Alamosa(Colorado)\",\n\"dinner \": \"Cafe Dalal Street , Alamosa(Colorado)\",\n\"accommodation \": \"Sunny Chelsea Studio , Alamosa(Colorado)\"\n},\n{\n\"days\": 4,\n\"current_city \": \"Alamosa(Colorado)\",\n\"transportation \": \"-\",\n19\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\n\"breakfast \": \"Good Luck Cafe , Alamosa(Colorado)\",\n\"attraction \": \"Alamosa Archery Range , Alamosa(Colorado);Alamosa Riparian Park ,\nAlamosa(Colorado);Alamosa Sub , Alamosa(Colorado);\",\n\"lunch\": \"Shri Durga Dosa Corner , Alamosa(Colorado)\",\n\"dinner \": \"Lahore , Alamosa(Colorado)\",\n\"accommodation \": \"Sunny Chelsea Studio , Alamosa(Colorado)\"\n},\n{\n\"days\": 5,\n\"current_city \": \"from Alamosa(Colorado) to Denver(Colorado)\",\n\"transportation \": \"Self -driving , from Alamosa(Colorado) to Denver(Colorado),\nduration: 3 hours 38 mins , distance: 377 km , cost: 18\",\n\"breakfast \": \"Hamburg To Hyderabad , Alamosa(Colorado)\",\n\"attraction \": \"Denver Zoo , Denver(Colorado);\",\n\"lunch\": \"The Fatty Bao - Asian Gastro Bar , Denver(Colorado)\",\n\"dinner \": \"Woods Spice , Denver(Colorado)\",\n\"accommodation \": \"Peaceful , beautiful home away , Denver(Colorado)\"\n},\n{\n\"days\": 6,\n\"current_city \": \"Denver(Colorado)\",\n\"transportation \": \"-\",\n\"breakfast \": \"The Urban Socialite , Denver(Colorado)\",\n\"attraction \": \"Denver Art Museum , Denver(Colorado);Denver Museum of Nature &\nScience , Denver(Colorado);\",\n\"lunch\": \"Breaktym , Denver(Colorado)\",\n\"dinner \": \"Chawla 's\\u5b8a , Denver(Colorado)\",\n\"accommodation \": \"Peaceful , beautiful home away , Denver(Colorado)\"\n},\n{\n\"days\": 7,\n\"current_city \": \"from Denver(Colorado) to Indianapolis\",\n\"transportation \": \"Self -driving , from Denver(Colorado) to Indianapolis ,\nduration: 15 hours 37 mins , distance: 1,747 km , cost: 87\",\n\"breakfast \": \"Starve Stalkers , Denver(Colorado)\",\n\"attraction \": \"-\",\n\"lunch\": \"-\",\n\"dinner \": \"-\",\n\"accommodation \": \"-\"\n}\n]\n}\n\u0006\n\u0005\nC.2. Additional Case Study\n20\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nQuery: Please draw up a 3-day travel itinerary for \none person, beginning in Oakland and heading to \nTucson from March 15th to March 17th, 2022, with a \nbudget of $1,400.\nPlan:\n { \n\u201dday\u201d: 3, \n\"current_city\": \u201cTucson\", \n\"transportation\": \u201c-\", \n\"breakfast\": \"Bakers Oven, Tucson\u201d, \n\"attraction\": \"San Xavier del Bac Mission, Tucson; \nTucson Museum Of Art, Tucson\u201d, \n\"lunch\": \"Pizza Street, Tucson\", \n\"dinner\": \"-\", \n\"accommodation\": \u201c-\" \n}\nAnalysis:\nIn this instance, the planned trip did not form a \nclosed-loop trip, ending in Tucson on the third day. \nAdditionally, despite the trip including a stay in \nTucson, the language agent failed to schedule dinner \nor accommodation for that day.\nFigure C.1. Case 4 (Two-stage Mode).\nQuery: Can you create a 5-day travel plan for 2 \npeople departing from Syracuse to visit 2 cities in \nGeorgia? We are planning to travel from March 16th \nto March 20th, 2022. Our budget is approximately \n$2,000. We are interested in trying both American \nand Mediterranean cuisines during our trip.\nTrajectory:\n\u2026 \nAction 13: FlightSearch[Syracuse, Atlanta, \n2023-03-20] \nThought 14:  Since I have exhausted the options \nfor finding flights from Syracuse to any city in \nGeorgia on the desired dates, and I am unable to \nsearch for alternative transportation methods due to \nthe nested use of tools restriction, the only \nremaining step is to use the Planner tool with the \ninformation that has been gathered so far. \nAction 14: Planner[Create a 5-day travel plan from \nSyracuse to Georgia] \nAnalysis:\nIn this example, the language agent's persistent \ndate errors and fixation on airplane travel lead it to \nbypass valid information search. Instead, the agent \nchoose to directly plan, resulting in the fabrication of \ndetails like the fictitious flight number \"F1234567.\" \nThis approach highlights the agent's tendency to \ncreate erroneous information when unable to access \naccurate data.\nPlan:\nDay 1: \ntransportation: Flight Number: F1234567, \nfrom Syracuse toAtlanta \n\u2026\nFigure C.2. Case 5 (Two-stage mode).\n21\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nQuery: Please curate a 3-day travel plan for a solo \ntraveler from Tulsa to Houston from March 23rd to \nMarch 25th, 2022, with a total travel budget of \n$1,000.\nPlan:\n{ \n\"day\": 3, \n\"current_city\": \"from Houston to Tulsa\", \n\"transportation\": \"Flight Number: F4013298, from \nHouston to Tulsa, Departure Time: 08:20, Arrival \nTime: 09:43\", \n\"breakfast\": \"Earthen Spices, Houston\", \n\"attraction\": \"The Museum of Fine Arts, \nHouston; Hermann Park, Houston;\", \n\"lunch\": \"Chawla, Houston\", \n\"dinner\": \"-\", \n\"accommodation\": \"-\" \n} \nAnalysis:\nIn this example, the language agent scheduled an \n8:20 AM flight departure from Houston but still \nplanned a subsequent trip and lunch in Houston. \nThis plan is illogical, as it overlooks the fact that the \nagent would no longer be in the city post-flight.\nFigure C.3. Case 6 (Two-stage mode).\nQuery: Could you help develop a week-long travel \nitinerary suitable for a group of 6 people, departing \nfrom Baton Rouge and planning to visit 3 different \ncities in Texas? The travel dates are set from March \n17th to March 23rd, 2022. Our travel budget has \nbeen adjusted to $14,600. Bearing in mind that we \nh a ve c h i l d r e n u n d e r t e n ye a r s o l d , o u r \naccommodations need to allow young children and \nwe prefer to occupy entire rooms. We also prefer not \nto self-drive during this trip.\nPlan:\n{ \n\"days\": 1,\u201d \ncurrent_city\": \"from Baton Rouge to Texarkana\", \n\"breakfast\": \"-\", \n\"lunch\": \"Big City Bread Cafe, Texarkana\", \n\"dinner\": \"Poets Cafe, Texarkana\", \n}, \n{ \ndays\": 2, \n\"current_city\": \"Texarkana\", \n\"transportation\": \"-\", \n\"breakfast\": \"Big City Bread Cafe, Texarkana\u201d, \n\u2026}\nAnalysis:\nIn this scenario, the language agents select the \nsame restaurant for both lunch on the first day and \nbreakfast on the second day, a choice that seems to \ndefy commonsense.\nFigure C.4. Case 7 (Direct strategy in sole-planning mode).\n22\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nQuery: Can you help put together a 3-day travel \nplan for a group of 3, leaving from Daytona Beach \nand heading to Atlanta from March 2nd to March \n4th, 2022? We have a budget of $2,100. We require \naccommodations that allow children under 10 \nyears of age, and we prefer having entire rooms to \nourselves. Please note, we cannot utilize flights for \ntransportation on this trip.\nPlan:\n{ \n\"day\": 1, \n\"current_city\": \"from Daytona Beach to Atlanta\u201d, \n\u2026 \n\"accommodation\": \"Bright, Modern, Clean, \nSpacious, Brooklyn Home, Atlanta\" \n}, \n{ \n\"day\": 2, \n\u2026 \n\"accommodation\": \"Bright, Modern, Clean, \nSpacious, Brooklyn Home, Atlanta\" \n}\nAnalysis:\nIn this example, the language agent makes an error \nin their accommodation planning. Agent selects a \nproperty that prohibits children under 10 and \nrequires a minimum stay of three nights. However, \nthe agents only plan a two-day stay, leading to a \nviolation of the property's minimum stay \nrequirement.\nFigure C.5. Case 8 (Direct strategy in sole-planning mode).\n[ \n            { \n                \"day\": 1, \n                \"current_city\": \"from Missoula to Dallas\", \n\"transportation\": \"Flight Number: F3604254, from Missoula to Dallas,  \nDeparture Time: 14:27, Arrival Time: 18:26\", \n                \"breakfast\": \"-\", \n                \"attraction\": \"-\", \n                \"lunch\": \"-\", \n                \"dinner\": \"Coconuts Fish Cafe, Dallas\", \n                \u201caccommodation\": \"1BR, elevator, kitchen, doorman!, Dallas\" \n            }, \n            { \n                \"day\": 2, \n                \"current_city\": \"Dallas\", \n                \"transportation\": \"-\", \n                \"breakfast\": \"Cafe Gatherings, Dallas\", \n                \"attraction\": \"The Dallas World Aquarium, Dallas;  \nThe Sixth Floor Museum at Dealey Plaza, Dallas;\", \n                \"lunch\": \"1918 Bistro & Grill, Dallas\", \n                \"dinner\": \"Yanki Sizzlers, Dallas\", \n                \"accommodation\": \"1BR, elevator, kitchen, doorman!, Dallas\" \n            }, \n            { \n                \"day\": 3, \n                \"current_city\": \"from Dallas to Missoula\", \n                \"transportation\": \"Flight Number: F3604227,  \nfrom Dallas to Missoula, Departure Time: 11:28, Arrival Time: 13:48\", \n                \"breakfast\": \"MONKS, Dallas\", \n                \"attraction\": \"Reunion Tower, Dallas\", \n                \"lunch\": \"-\", \n                \"dinner\": \"-\", \n                \"accommodation\": \"-\" \n            } \n        ]\nPlan:\nQuery: Please create a travel plan for a 3-day trip from Missoula to \nDallas scheduled from March 23rd to March 25th, 2022. The budget for \nthis trip is set at $1,900.\nAnalysis:\nA feasible plan that meets all constraints.\nFigure C.6. Case 9 (Direct strategy in sole-planning mode).\n23\nTravelPlanner: A Benchmark for Real-World Planning with Language Agents\nAction 1: CostEnquiry[{\"people_number\": 1, \"day\": 1,\u2026\u201d}] \nObservation 1: Cost: The cost of your plan is 1412.0 dollars. \nThought 2: The cost for the first day is $1412, which includes the flight \nand activities in San Diego. The remaining budget is $1588\u2026 \nAction 2: CostEnquiry[{\"people_number\": 1, \"day\": 2,\u2026}] \nObservation 2: Cost: The cost of your plan is 721.0 dollars. \nThought 3: The cost for the second day is $721, which includes meals, \nattractions, and the same accommodation as the first day. The total spent \nso far is $2133, leaving a remaining budget of $867. \nAction 3: CostEnquiry[{\"people_number\": 1, \"day\": 3,\u2026}] \nObservation 3: Cost: The cost of your plan is 1114 dollars. \nThought 4: The cost for the third day is $1114, which includes the return \nflight and activities in San Diego before departure. The total cost for all \nthree days is now $3247, which exceeds the budget of $3000. To \nstay within budget, adjustments need to be made to the third day's plan, \npossibly by choosing a less expensive lunch option and ensuring no \nadditional costs are incurred after the return flight since the traveler will \nnot need accommodation or meals in Detroit after the evening flight. \nAction 4: CostEnquiry[{\"people_number\": 1, \"day\": 3,\u2026}] \nObservation 4: Cost: The cost of your plan is 1105 dollars. \nThought 5: The revised cost for the third day is $1105, which includes a \nmore affordable lunch option. The total cost for all three days is now \n$3238, which is still slightly over the budget of $3000. To further \nreduce costs, I can look for a less expensive breakfast option on the third \nday\u2026 \nAction 5: CostEnquiry[{\"people_number\": 1, \"day\": 3,..}] \nObservation 5: Cost: The cost of your plan is 1083 dollars. \nThought 6: The revised cost for the third day is now $1083 after choosing \na less expensive breakfast option. The total cost for all three days is \n$3216, which is closer to the budget but still slightly over. Since \nthe flight and accommodation costs are fixed, the only remaining option \nto adjust would be the attractions or lunch on the second day. However, \ngiven the small amount by which the budget is exceeded, it may be \nacceptable to slightly exceed the budget due to the variability of \ndaily expenses and the possibility of finding deals on-site. The \ntraveler should be prepared for a small buffer in their budget. \nAction 6: Finish\nTrajectory:\nQuery:Please assist in crafting a travel plan for a solo traveller, \njourneying from Detroit to San Diego for 3 days, from March 5th to \nMarch 7th, 2022. The travel plan should accommodate a total budget of \n$3,000. \nAnalysis:\nIn this example, the language agent acknowledges that the total cost \nexceeds 3,000 but, after several unsuccessful modifications, settles for \nthe existing solution. It primarily attempts to adjust dining plans, which \nare less expensive, rather than addressing the more costly transportation \nand accommodation. This behavior indicates that although the agent \nrecognizes plan flaws, it lacks effectiveness in making substantial \nadjustments. This highlights a limitation in the agent's ability to \nstrategically optimize cost-intensive aspects of planning.\nFigure C.7. Case 10 (Reflexion strategy in sole-planning mode).\n24\n"
  },
  {
    "title": "Pok\u00e9LLMon: A Human-Parity Agent for Pok\u00e9mon Battles with Large Language Models",
    "link": "https://arxiv.org/pdf/2402.01118.pdf",
    "upvote": "28",
    "text": "POK\u00b4ELLMON: A Human-Parity Agent for Pok\u00b4emon Battles\nwith Large Language Models\nSihao Hu, Tiansheng Huang, Ling Liu\nGeorgia Institute of Technology\nAtlanta, GA 30332, United States\n{sihaohu, thuang, ling.liu}@gatech.edu\nhttps://poke-llm-on.github.io/\nAbstract\nWe introduce POK\u00b4ELLMON, the first LLM-\nembodied agent that achieves human-parity per-\nformance in tactical battle games, as demonstrated\nin Pok\u00b4emon battles. The design of POK\u00b4ELLMON\nincorporates three key strategies: (i) In-context re-\ninforcement learning that instantly consumes text-\nbased feedback derived from battles to iteratively\nrefine the policy; (ii) Knowledge-augmented gen-\neration that retrieves external knowledge to coun-\nteract hallucination and enables the agent to act\ntimely and properly; (iii) Consistent action gener-\nation to mitigate the panic switching phenomenon\nwhen the agent faces a powerful opponent and\nwants to elude the battle. We show that online bat-\ntles against human demonstrates POK\u00b4ELLMON\u2019s\nhuman-like battle strategies and just-in-time de-\ncision making, achieving 49% of win rate in the\nLadder competitions and 56% of win rate in the\ninvited battles. Our implementation and playable\nbattle logs are available at: https://github.\ncom/git-disl/PokeLLMon.\n1. Introduction\nGenerative AI and Large Language Models (LLMs) have\nshown unprecedented success on NLP tasks (Ouyang et al.,\n2022; Brown et al., 2020; Xi et al., 2023; Wang et al., 2023b).\nOne of the forthcoming advancements will be to explore\nhow LLMs can autonomously act in the physical world with\nextended generation space from text to action, represent-\ning a pivotal paradigm in the pursuit of Artificial General\nIntelligence (Goertzel & Pennachin, 2007; Goertzel, 2014).\nGames are suitable test-beds to develop LLM-embodied\nagents (Duan et al., 2022; Batra et al., 2020) to interact with\nthe virtual environment in a way resembling human behavior.\nFor example, Generative Agents (Park et al., 2023) conducts\na social experiments with LLMs assuming various roles in a\nFigure 1. At each turn, the player is requested to decide which\naction to perform, i.e., whether to let Dragonite to take a move or\nswitch to another Pok\u00b4emon off the field.\n\u201cThe Sims\u201d-like sandbox, where agents exhbit behavior and\nsocial interactions mirroring humans. In Mincraft, decision-\nmaking agents (Wang et al., 2023a;c; Singh et al., 2023) are\ndesigned to explore the world and develop new skills for\nsolving tasks and making tools.\nCompared to existing games, tactical battle games (Ma et al.,\n2023) are better suited for benchmarking the game-playing\nability of LLMs as the win rate can be directly measured and\nconsistent opponents like AI or human players are always\navailable. Pok\u00b4emon battles, serving as a mechanism that\nevaluates the battle abilities of trainers in the well-known\nPok\u00b4emon games, offer several unique advantages as the first\nattempt for LLMs to play tactical battle games:\n(1) The state and action spaces are discrete and can be trans-\nlated into text losslessly. Figure 1 is an illustrative example\nfor a Pok\u00b4emon battle: At each turn, the player is requested\nto generate an action to perform given the current state of\nPok\u00b4emon from each side. The action space consists of four\nmoves and five possible Pok\u00b4emon to switch; (2) The turn-\nbased format eliminates the demands of intensive gameplay,\nalleviating the stress on the inference time cost for LLMs,\nmaking performance hinges solely on the reasoning abili-\nties of LLMs; (3) Despite its seemingly simple mechanism,\nPok\u00b4emon battle is strategic and complex: an experienced\nplayer takes various factors into consideration, including\nspecies/type/ability/stats/item/moves of all the Pok\u00b4emon\non and off the field. In a random battle, each Pok\u00b4emon is\n1\narXiv:2402.01118v2  [cs.AI]  25 Feb 2024\nrandomly selected from a large candidate pool (more than\n1,000) with distinct characteristics, demanding the players\nboth the Pok\u00b4emon knowledge and reasoning ability.\nScope and Contributions: The scope of this paper is to\ndevelop an LLM-embodied agent that mimics the way a hu-\nman player engages in Pok\u00b4emon battles. The objective is to\nexplore the key factors that make the LLM-embodied agent\na good player and to examine its strengths and weaknesses\nin battles against human players. To enable LLMs play\ngame autonomously, we implement an environment that\ncan parse and translate battle state into text description, and\ndeliver generated action back to the server. By evaluating\nexisting LLMs, we identify the presence of hallucination,\nand the panic switching phenomenon.\nHallucination: The agent can mistakenly send out Pok\u00b4emon\nat a type disadvantage or persist in using ineffective moves\nagainst the opponent. As a result, the most advanced LLM,\nGPT-4, achieves a win rate of 26% when playing against a\nheuristic bot, compared to 60% win rate of human players.\nTo combat hallucination, we introduce two strategies: (1)\nIn-context reinforcement learning: We provide the agent\nwith text-based feedback instantly derived from the battle,\nserving as a new form of \u201creward\u201d to iteratively refine the\naction generation policy without training; (2) Knowledge-\naugmented generation: We equip the agent with Pok\u00b4edex,\nan encyclopaedia in Pok\u00b4emon games that provides external\nknowledge like type advantage relationship or move/ability\ndescriptions, simulating a human player searching for the\ninformation of unfamiliar Pok\u00b4emon.\nPanic switching: We discover that when the agent encoun-\nters a powerful Pok\u00b4emon, it tends to panic and generates\ninconsistent actions like switching different Pok\u00b4emon in\nconsecutive turns to elude the battle, a phenomenon that is\nespecially pronounced with Chain-of-Thought (Wei et al.,\n2022) reasoning. Consistent action generation alleviates\nthe issue by voting out the most consistent action without\noverthinking. This observation mirrors human behavior,\nwhere in stressful situations, overthinking and exaggerating\ndifficulties can lead to panic and impede acting.\nOnline\nbattles\ndemonstrate\nPOK\u00b4ELLMON\u2019s\nhuman-\ncompetitive battle abilities: it achieves a 49% win rate in\nthe Ladder competitions and a 56% win rate in the invited\nbattles. Furthermore, we reveal its vulnerabilities to human\nplayers\u2019 attrition strategies and deceptive tricks.\nIn summary, this paper makes four original contributions:\n\u2022 We implement and release an environment that enables\nLLMs to autonomously play Pok\u00b4emon battles.\n\u2022 We propose in-context reinforcement learning to in-\nstantly and iteratively refine the policy, and knowledge-\naugmented generation to combat hallucination.\n\u2022 We discover that the agent with chain-of-thought ex-\nperiences panic when facing powerful opponents, and\nconsistent action generation can mitigate this issue.\n\u2022 POK\u00b4ELLMON, to the best of our knowledge, is the first\nLLM-embodied agent with human-parity performance\nin tactical battle games.\n2. LLMs as Game Players\nCommunicative games: Communicative games revolve\naround communication, deduction and sometimes deception\nbetween players. Existing studies show that LLMs demon-\nstrate strategic behaviors in board games like Werewolf (Xu\net al., 2023), Avalane (Light et al., 2023), WorldWar II (Hua\net al., 2023) and Diplomacy (Bakhtin et al., 2022).\nOpen-ended games: Open-ended games allow players\nto freely explore the game world and interact with oth-\ners. Generative Agent (Park et al., 2023) showcases that\nLLM-embodied agents exhibit behavior and social inter-\nactions mirroring human-like patterns. In MineCraft, Voy-\nager (Wang et al., 2023a) employs curriculum mechanism to\nexplore the world and generates and executes code for solv-\ning tasks. DEPS (Wang et al., 2023c) proposes an approach\nof \u201cDescribe, Explain, Plan and Select\u201d to accomplish 70+\ntasks. Planing-based frameworks like AutoGPT (Significant\nGravitas) and MetaGPT (Hong et al., 2023) can be adopted\nfor the exploration task as well.\nTactic battle games: Among various game types, tactical\nbattle games (Akata et al., 2023; Ma et al., 2023) are particu-\nlarly suitable for benchmarking LLMs\u2019 game-playing ability,\nas the win rate can be directly measured, and consistent op-\nponents are always available. Recently, LLMs are employed\nto play StarCraft II (Ma et al., 2023) against the built-in AI\nwith a text-based interface and a chain-of-summarization\napproach. In comparison, POK\u00b4ELLMON has several advan-\ntages: (1) Translating Pok\u00b4emon battle state into text is loss-\nless; (2) Turn-based format eliminates real-time stress given\nthe inference time cost of LLMs; (3) Battling against disci-\nplined human players elevates the difficulty to a new height.\n3. Background\n3.1. Pok\u00b4emon\nSpecies: There are more than 1,000 Pok\u00b4emon species (bul,\n2024c), each with its unique ability, type(s), statistics (stats)\nand battle moves.\nFigure 2 shows two representative\nPok\u00b4emon: Charizard and Venusaur.\nType: Each Pok\u00b4emon species has up to two elemental\ntypes, which determine its advantages and weaknesses. Fig-\nure 3 shows the advantage/weakness relationship between\n18 types of attack moves and attacked Pok\u00b4emon. For ex-\n2\nSpecies: Charizard \nAbility: Blaze\n Spe: 328\nHP: 319\nAtk: 293\nDef: 280\nLv.87\nDragon Dance\nFire Blast\nOutrage\nAir Slash\nMove:\nSpecies: Venusaur \nAbility: Overgrow\n Spe: 284\nHP: 364\nAtk: 289\nDef: 328\nLv.89\nMove:\nSynthesis\nSolar Beam\nSludge Bomb\nGiga Drain\n[120]\n[120]\n[75]\n[120]\n[90]\n[75]\nFigure 2. Two representative Pok\u00b4emon: Charizard and Venusaur.\nEach Pok\u00b4emon has type(s), ability, stats and four battle moves.\nample, fire-type moves like \u201cFire Blast\u201d of Charizard can\ncause double damage to grass-type Pok\u00b4emon like Venusaur,\nwhile Charizard is vulnerable to water-type moves.\nStats: Stats determine how well a Pok\u00b4emon performs in\nbattles. There are four stats: (1) Hit Points (HP): determines\nthe damage a Pok\u00b4emon can take before fainting; (2) Attack\n(Atk): affects the strength of attack moves; (3) Defense\n(Def): dictates resistance against attacks; (4) Speed (Spe):\ndetermines the order of moves in battle.\nAbility: Abilities are passive effects that can affect battles.\nFor example, Charizard\u2019s ability is \u201cBlaze\u201d, which enhances\nthe power of its fire-type moves when its HP is low.\nMove: A Pok\u00b4emon can learn four battle moves, categorized\nas attack moves or status moves. An attack move deals\ninstant damage with a power value and accuracy, and as-\nsociated with a specific type, which often correlates with\nthe Pok\u00b4emon\u2019s type but does not necessarily align; A status\nmove does not cause instant damage but affects the battle\nin various ways, such as altering stats, healing or protect\nPok\u00b4emon, or battle conditions, etc. There are 919 moves in\ntotal with distinctive effect (bul, 2024b).\n3.2. Battle Rule\nIn one-to-one random battles (Wikipedia, 2023), two battlers\nface off, each with six randomly selected Pok\u00b4emon. Initially,\neach battler sends out one Pok\u00b4emon onto the field, keeping\nthe others in reserve for future switches. The objective is to\nmake all the opponent\u2019s Pok\u00b4emon faint (by reducing their\nHP to zero) while ensuring that at least one of own Pok\u00b4emon\nremains unfainted. The battle is turn-based: at the start of\neach turn, both players choose an action to perform. Actions\nfall into two categories: (1) taking a move, or (2) switching\nto another Pok\u00b4emon. The battle engine executes actions and\nupdates the battle state for the next step. If a Pok\u00b4emon faints\nafter a turn and the battler has other Pok\u00b4emon unfainted, the\nbattle engine forces a switch, which does not count as the\nplayer\u2019s action for the next step. After a forced switch, the\nplayer can still choose a move or make another switch.\nTYPE\nFigure 3. Type advantage/weakness relationship.\n\u201c+\u201d denotes\nsuper-effective (2x damage); \u201c\u2212\u201d denotes ineffective (0.5x dam-\nage); \u201c\u00d7\u201d denotes no effect (0x damage). Unmarked is standard\n(1x) damage.\n4. Battle Environment\nBattle Engine: The environment interacts with a battle en-\ngine server called Pok\u00b4emon showdown (pok, 2024), which\nprovides a web-based GUI for human players, as well as\nweb APIs for interacting with message in defined formats.\nBattle Environment: We implement a battle environment\nbased on (Sahovic, 2023a) to support LLMs autonomously\nplay Pok\u00b4emon battles. Figure 4 illustrates how the entire\nframework works. At the beginning of a turn, the environ-\nment get an action-request message from the server, includ-\ning the execution result from the last turn. The environment\nfirst parses the message and update local state variables,\nand then translates the state variables into text. The text\ndescription primarily consists of four parts: (1) Own team\ninformation, including the attributes of Pok\u00b4emon both on-\nthe-field and off-the-field; (2) Opponent team information\nincluding the attributes of opposing Pok\u00b4emon on-the-field\nand off-the-field (some are unknown); (3) Battle field in-\nformation like the weather, entry hazard and terrain; (4)\nHistorical turn log information, including previous actions\nof both side Pok\u00b4emon, which is stored in a log queue. LLMs\ntake the translated state as input and output an action for\nthe next step. The action is sent to the server and executed\nalongside the action chosen by the human player.\n5. Preliminary Evaluation\nTo gain insights into the challenges associated with\nPok\u00b4emon battles, we evaluate the abilities of existing LLMs,\nincluding GPT-3.5 (Ouyang et al., 2022), GPT-4 (Achiam\net al., 2023), and LLaMA-2 (Touvron et al., 2023),\n5.1. Pok\u00b4emon Battles\nPlacing LLMs in direct competitions against human players\nis time-consuming as human needs time to think (4 minutes\nfor 1 battle in average). To save time, we adopt a heuris-\n3\nFigure 4. The framework that enables LLMs to battle with human players: It parses the messages received from the battle server and\ntranslates state logs into text. LLMs take these state descriptions and historical turn logs as input and generates an action for the next step.\nThe action is then sent to the battle server and executed alongside the action chosen by the opponent player.\ntic bot (Sahovic, 2023b) to initially battle against human\nplayers in the Ladder competitions, and then use the bot to\nbenchmark existing LLMs. The bot is programmed to use\nstatus boosting moves, set entry hazards, selecting the most\neffective actions by considering the stats of Pok\u00b4emon, the\npower of moves, and type advantages/weaknesses.\nTable 1. Performance of LLMs in battles against the bot.\nPlayer\nWin rate \u2191\nScore \u2191\nTurn #\nBattle #\nHuman\n59.84%\n6.75\n18.74\n254\nRandom\n1.2%\n2.34\n22.37\n200\nMaxPower\n10.40%\n3.79\n18.11\n200\nLLaMA-2\n8.00%\n3.47\n20.98\n200\nGPT-3.5\n4.00%\n2.61\n20.09\n100\nGPT-4\n26.00%\n4.65\n19.46\n100\nThe statistic results are presented in Table 1, where the battle\nscore is defined as the sum of the numbers of the opponent\u2019s\nfainted Pok\u00b4emon and the player\u2019s unfainted Pok\u00b4emon at the\nend of a battle. Consequently, the opponent player\u2019s battle\nscore is equal to 12 minus the player\u2019s battle score. Random\nis a simple strategy that randomly generates an action every\ntime, and MaxPower chooses the move with the highest\npower value. Obviously, GPT-3.5 and LLaMA-2 are just\nslightly better than Random and even GPT-4 cannot beat\nthe bot, let along well-disciplined human players from the\nLadder competitions.\nBy observing LLMs play battles and analyzing the ex-\nplanations generated with their actions, we identify the\noccurrence of hallucination (Rawte et al., 2023; Cabello\net al., 2023): LLMs can mistakenly claim non-existent type-\nadvantage relationships or, even worse, reverse the advan-\ntage relationships between types like sending a grass-type\nPok\u00b4emon to face with a fire-type Pok\u00b4emon. A clear under-\nstanding of type advantage/weakness is crucial in Pok\u00b4emon\nbattles, as choosing a Pok\u00b4emon with a type advantage can\nresult in dealing more damage and sustaining less.\n5.2. Test of Hallucination\nTo assess hallucination in the outputs of LLMs, we con-\nstruct the task of type advantage/weakness prediction. The\ntask involves asking LLMs to determine if an attack of a\ncertain type is A. super-effective (2x damage), B. standard\n(1x damage), C. ineffective (0.5x damage) or D. no effect\n(0x damage) against a certain type of Pok\u00b4emon. The 324\n(18x18) testing pairs are constructed based on Figure 3.\nTable 2. Confusion matrices for type advantage prediction.\nModel\nLLaMA-2\nGPT-3.5\nGPT-4\nClass\nA\nB\nC\nD\nA\nB\nC\nD\nA\nB\nC\nD\nA\n5\n46\n0\n0\n0\n0\n49\n2\n37\n8\n5\n1\nB\n25\n179\n0\n0\n2\n6\n185\n11\n0\n185\n17\n2\nC\n15\n46\n0\n0\n0\n2\n57\n2\n3\n24\n32\n2\nD\n1\n7\n0\n0\n0\n0\n7\n1\n0\n0\n0\n8\nTable 2 shows the three confusion matrices of LLMs, where\ntheir performance is highly related to their win rates in Ta-\nble 1. LLaMA-2 and GPT-3.5 suffer from severe hallucina-\ntion problems, while GPT-4 achieves the best performance\nwith an accuracy of 84.0%, we still observe it frequently\nmaking ineffective actions, which is because in a single bat-\ntle, LLMs need to compare the types of all the opponent\u2019s\nPok\u00b4emon with types of all their Pok\u00b4emon, as well as types\nof moves.\n4\nPok\u00e9dex\nfeedback\nState\nHistorical\nlogs\nTurn T\nTurn T+1\naction\naction\naction\naction\n(1) In-Context RL\n(2) Knowledge-Augmented\nGeneration\n(3) Consistent Action \nGeneration\nBattle\nBattle\nLLM\nFigure 5. POK\u00b4ELLMON is equipped with three strategies: (1)\nICRL that leverages instant feedbacks from the battle to itera-\ntively refine generation; (2) KAG that retrieves external knowledge\nto combat hallucination and to act timely and properly; (3) Consis-\ntent Action Generation to prevent the panic switching problem.\n6. POK\u00b4ELLMON\nOverview: The overall framework of POK\u00b4ELLMON is illus-\ntrated in Figure 5. In each turn, POK\u00b4ELLMON uses previous\nactions and corresponding text-based feedback to iteratively\nrefine the policy, and also augments the current state in-\nformation with external knowledge, such as type advan-\ntage/weakness relationships and move/ability effects. Given\nabove information as input, it independently generates mul-\ntiple actions and selects the most consistent ones as the final\noutput for execution.\n6.1. In-Context Reinforcement Learning (ICRL)\nHuman players make decisions based not only on the cur-\nrent state but also on the (implicit) feedback from previous\nactions, such as the change in a Pok\u00b4emon\u2019s HP over two\nconsecutive turns following an attack by a move. With-\nout feedback provided, the agent can continuously stick to\nthe same erroneous action. As illustrated in Figure 6, the\nagent uses \u201cCrabhammer\u201d, a water-type attack move against\nthe opposing Toxicroak, a Pok\u00b4emon with the ability \u201cDry\nSkin\u201d, which can nullify damage from water-type moves.\nThe \u201cImmune\u201d message displayed in the battle animation\ncan prompt a human player to change actions even without\nknowledge of \u201cDry Skin\u201d, however, is not included in the\nstate description. As a result, the agent repeats the same\naction, inadvertently giving the opponent two free turns to\ntriple Toxicroak\u2019s attack stats, leading to defeat.\nReinforcement Learning (Schulman et al., 2017; Mnih et al.,\n2016; Hafner et al., 2023) requires numeric rewards to eval-\nuate actions for refining policy. As LLMs can understand\nlanguages and distinguish what is good and bad, text-based\nfeedback description provides a new form of \u201creward\u201d. By\nincorporating text-based feedback from the previous turns\ninto the context, the agent is able to refine its \u201cpolicy\u201d it-\neratively and instantly during serving, namely In-Context\nReinforcement Learning (ICRL).\nIn practice, we generate four types of feedback: (1) The\nFigure 6. The agent repeatedly uses the same attack move but has\nzero effect to the opposing Pok\u00b4emon due to its ability \u201cDry Skin.\u201d\nFigure 7. In turn 3, the agent uses \u201cPsyshock\u201d, which cause zero\ndamage to the opposing Pok\u00b4emon. With ICRL, the agent switch to\nanother Pok\u00b4emon.\nchange in HP over two consecutive turns, which reflects\nthe actual damage caused by an attack move; (2) The effec-\ntiveness of attack moves, indicating whether they are super-\neffective, ineffective, or have no effect (immunity) due to\ntype advantages or ability/move effects; (3) The priority\nof move execution, providing a rough estimate of speed,\nas precise stats for the opposing Pok\u00b4emon are unavailable;\n(4) The actual effects of executed moves: both status and\ncertain attack moves can cause outcomes like stat boosts\nor debuffs, recover HP, inflict conditions such as poison,\nburns, or freezing, etc. Figure 4 presents several instances\nof generated text-based feedback for ICLR.\nTable 3. Performance of ICRL in battles against the bot.\nPlayer\nWin rate \u2191\nScore \u2191\nTurn #\nBattle #\nHuman\n59.84%\n6.75\n18.74\n254\nOrigin\n26.00%\n4.65\n19.46\n100\nICRL\n36.00%\n5.25\n20.64\n100\nTable 3 shows the improvement brought by ICRL. Com-\npared to the original performance of GPT-4, the win rate is\nboosted by 10%, and the battle score increases by 12.9%.\nDuring the battles, we observe that the agent begins to\nchange its action if the moves in previous turns do not meet\nthe expectation, as shown in Figure 7: After observing that\nthe opposing Pok\u00b4emon is immune to the attack, it switches\nto another Pok\u00b4emon.\n6.2. Knowledge-Augmented Generation (KAG)\nAlthough ICRL can mitigate the impact of hallucination,\nit can still cause fatal consequences before the feedback is\nreceived. For example, if the agent sends out a grass-type\nPok\u00b4emon against a fire-type Pok\u00b4emon, the former is likely\nbe defeated in a single turn before the agent realize it is\na bad decision. To further reduce hallucination, Retrieval-\nAugmented Generation (Lewis et al., 2020; Guu et al., 2020;\nPatil et al., 2023) employ external knowledge to augment\n5\nTable 4. Performance of KAG in battles against the bot.\nPlayer\nWin rate \u2191\nScore \u2191\nTurn #\nBattle #\nHuman\n59.84%\n6.75\n18.74\n254\nOrigin\n36.00%\n5.25\n20.64\n100\nKAG[Type]\n55.00%\n6.09\n19.28\n100\nKAG[Effect]\n40.00%\n5.64\n20.73\n100\nKAG\n58.00%\n6.53\n18.84\n100\nFigure 8. The agent understands the move effect and uses it prop-\nerly: Klefki is vulnerable to the ground-type attack of Rhydon.\nInstead of switching, the agent uses \u201cMagnet Rise\u201d, a move that\nprotects itself from the ground-type attack for five turns, invalidat-\ning the ground-type attack \u201cEarthquake\u201d of the opposing Rhydon.\ngeneration. In this section, we introduce two types of exter-\nnal knowledge to fundamentally mitigate hallucination.\nType advantage/weakness relationship: In the original\nstate description in Figure 4, we annotate all the type infor-\nmation of Pok\u00b4emon and moves to let the agent infer the type\nadvantage relationship by itself. To reduce the hallucination\ncontained in the reasoning, we explicitly annotate the type\nadvantage and weakness of the opposing Pok\u00b4emon and our\nPok\u00b4emon with descriptions like \u201cCharizard is strong against\ngrass-type Pok\u00b4emon yet weak to the fire-type moves\u201d.\nMove/ability effect: Given the numerous moves and abili-\nties with distinct effects, it is challenging to memorize all of\nthem even for experienced human players. For instance, it\u2019s\ndifficult to infer the effect of a status move based solely on\nits name: \u201cDragon Dance\u201d can boost the user\u2019s attack and\nspeed by one stage, whereas \u201cHaze\u201d can reset the boosted\nstats of both Pok\u00b4emon and remove abnormal statuses like\nbeing burnt. Even attack moves can have additional effects\nbesides dealing damage.\nWe collect all the effect descriptions of moves, abilities from\nBulbapedia (bul, 2024b;a) and store them into a Pok\u00b4edex, an\nencyclopaedia in Pok\u00b4emon games. For each Pok\u00b4emon on the\nbattlefield, its ability effect and move effects are retrieved\nfrom the Pok\u00b4edex and added to the state description.\nTable 4 shows the results of generations augmented with\ntwo types of knowledge, where type advantage relationship\n(KAG[Type]) significantly boosts the win rate from 36%\nto 55%, whereas, Move/ability effect descriptions also en-\nhance the win rate by 4 AP. By combining two of them,\nKAG achieves a win rate of 58% against the heuristic bot,\napproaching a level competitive with human.\nWith external knowledge, we observe that the agent starts\nto use very special moves at proper time. As an example\nshown in Figure 8, a steel-type Klefki is vulnerable to the\nground-type attack of the opposing Rhydon, a ground-type\nPok\u00b4emon. Usually in such a disadvantage, the agent will\nchoose to switch to another Pok\u00b4emon, however, it chooses\nto use the move \u201cMagnet Rise\u201d, which levitates the user to\nmake it immune to ground-type moves for five turns. As a\nresult, the ground-type attack \u201cEarthquake\u201d of the opposing\nRhydon becomes invalid.\n6.3. Consistent Action Generation\nExisting studies (Wei et al., 2022; Yao et al., 2022; Shinn\net al., 2023; Bommasani et al., 2021; Hu et al., 2023)\nshow that reasoning and prompting can improve the abil-\nity of LLMs on solving complex tasks. Instead of gen-\nerating a one-shot action, we evaluate existing prompting\napproaches including Chain-of-Thought (Wei et al., 2022)\n(CoT), Self-Consistency (Wang et al., 2022) (SC) and Tree-\nof-Thought (Yao et al., 2023) (ToT). For CoT, the agent\ninitially generates a thought that analyzes the current battle\nsituation and outputs an action conditioned on the thought.\nFor SC (k=3), the agent generates three times of actions and\nselect the most voted answer as the output. For ToT (k=3),\nthe agent generates three action options and picks out the\nbest one evaluated by itself.\nTable 5. Performance of prompting approaches in battles against\nthe bot.\nPlayer\nWin rate \u2191\nScore \u2191\nTurn #\nBattle #\nHuman\n59.84%\n6.75\n18.74\n254\nOrigin\n58.00%\n6.53\n18.84\n100\nCoT\n54.00%\n5.78\n19.60\n100\nSC (k=3)\n64.00%\n6.63\n18.86\n100\nToT (k=3)\n60.00%\n6.42\n20.24\n100\nTable 5 presents the comparison results of the original IO\nprompt generation and three algorithms. Notably, CoT re-\nsults in a performance degradation by a 6 AP drop in the\nwin rate. In comparison, SC brings a performance improve-\nment, with the win rate surpassing human players. Beyond\nthe results, our greater interest lies in understanding the\nunderlying reasons for these observations.\nAs introduced in Section 3.2, for each turn there is sin-\ngle action can be taken, which means if the agent chooses\nto switch yet the opponent choose to attack, the switch-in\nPok\u00b4emon will sustain the damage. Usually switching hap-\npens when the agent decides to leverage the type advantage\nof an off-the-battle Pok\u00b4emon, and thus the damage taken is\nsustainable since the switch-in Pok\u00b4emon is typically type-\nresistant to the opposing Pok\u00b4emon\u2019s moves. However, when\nthe agent with CoT reasoning faces a powerful opposing\nPok\u00b4emon, its actions become inconsistent by switching to\ndifferent Pok\u00b4emon in consecutive turns, which we call panic\nswitching. Panic switching wastes chances of taking moves\nand leading to the defeat. An illustrative example is shown\n6\nFigure 9. When facing a powerful Pok\u00b4emon, the agent with CoT\nswitches different Pok\u00b4emon in three consecutive to elude the battle.\nThis gives the opponent three free turns to quadruple its attack\nstats and quickly defeat the agent\u2019s entire team.\nin Figure 9: starting from turn 8, the agent chooses to con-\ntinuously switch to different Pok\u00b4emon in three consecutive\nturns, giving the opposing Pok\u00b4emon three free turns to boost\nits attack stats to four times and take down the agent\u2019s entire\nteam quickly.\nTable 6. Statistic analysis of panic switching\nPlayer\nWin rate \u2191\nSwitch rate\nCS1 rate\nCS2 rate\nOrigin\n58.00%\n17.05%\n6.21%\n22.98%\nCoT\n54.00%\n26.15%\n10.77%\n34.23%\nSC (k=3)\n64.00%\n16.00%\n1.99%\n19.86%\nToT (k=3)\n60.00%\n19.70%\n5.88%\n23.08%\nTable 6 provides statistical evidence, where CS1 represents\nthe ratio of active switches where the last-turn action is a\nswitch and CS2 rates represent the ratio of active switches\nhere at least one action from the last two turns is a switch,\namong all active switches, respectively. The higher the CS1\nrate, the greater the inconsistency of generation. Obviously,\nCoT largely increases the continuous switch rate, whereas,\nSC decreases the continuous switch rate.\nUpon examining the thoughts generated by CoT, we observe\nthat the thoughts contain panic feelings: the agent describes\nhow powerful the opposing Pok\u00b4emon is and the weaknesses\nof the current Pok\u00b4emon, and ultimately decides to switch to\nanother Pok\u00b4emon, as in \u201cDrapion has boosted its attack to\ntwo times, posing a significant threat that could potentially\nknock out Doublade with a single hit. Since Doublade is\nslower and likely to be knocked out, I need to switch to\nEntei because...\u201d. Action generation conditioned on panic\nthoughts leads the agent to continuously switch Pok\u00b4emon\ninstead of attacking. In comparison, consistent action gen-\neration with SC decreases the continuous switch ratio by\nindependently generating actions multiple times and voting\nout the most consistent action as shown in Figure 5, leading\nto a higher win rate. The observation is reflecting: when\nhumans face stressful situations, overthinking and exagger-\nating difficulties lead to panic feelings and paralyze their\nability to take actions, leading to even worse situations.\nFigure 10. POK\u00b4ELLMON selects effective moves in every turn,\ncausing the opponent\u2019s entire team to faint using one Pok\u00b4emon.\n7. Online Battle\nTo test the battle ability of POK\u00b4ELLMON against human,\nwe set up the eighth-gen battles on Pok\u00b4emon Showdown,\nwhere the agent battled against random human players for\nthe Ladder competitions from Jan. 25 to Jan. 26, 2024. Be-\nsides, we invited an human player who has over 15 years of\nexperience with Pok\u00b4emon games, representing the average\nability of human players to play against POK\u00b4ELLMON.\n7.1. Battle Against Human Players\nTable 7. Performance of POK\u00b4ELLMON against human players.\nv.s. Player\nWin rate \u2191\nScore \u2191\nTurn #\nBattle #\nLadder Player\n48.57%\n5.76\n18.68\n105\nInvited Player\n56.00%\n6.52\n22.42\n50\nTable 7 presents the performance of the agent against hu-\nman players. POK\u00b4ELLMON demonstrates comparable per-\nformance to disciplined Ladder players who have extensive\nbattle experience, and achieves a higher win rate than the\ninvited player. The average number of turns in the Ladder\ncompetitions is lower because human players sometimes\nforfeit when they believe they will lose to save time.\n7.2. Battle Skill Analysis\nStrength: POK\u00b4ELLMON seldom make mistakes at choos-\ning the effective move and switching to another suitable\nPok\u00b4emon due to the KAG strategy. As shown in Figure 10,\nin one battle, the agent uses only one Pok\u00b4emon to cause the\nentire opponent team fainted by choosing different attack\nmoves toward different Pok\u00b4emon.\nMoreover, POK\u00b4ELLMON exhibits human-like attrition strat-\negy: With some Pok\u00b4emon have the \u201cToxic\u201d move that can\ninflict additional damage every turn and the \u201cRecover\u201d move\nthat can recover its HP, the agent starts to first poisoned\nthe opposing Pok\u00b4emon and frequently uses the \u201cRecover\u201d\nto prevent itself from fainting. By prolonging the battle,\nthe opposing Pok\u00b4emon\u2019s HP is gradually depleted by the\npoisoning damage. Using attrition strategy requires an un-\n7\nFigure 11. POK\u00b4ELLMON suffers from attrition strategies: the op-\nponent players frequently recover high-defense Pok\u00b4emons. Break-\ning the dilemma requires joint effects across many turns.\nderstanding of moves like \u201cToxic\u201d, \u201cRecover\u201d and \u201cPro-\ntect\u201d, as well as the right timing for their use (such as\nwhen there\u2019s no type-weakness or when having high de-\nfense). An example with battle animation can be found at:\nhttps://poke-llm-on.github.io.\nWeakness: POK\u00b4ELLMON tends to take actions that can\nachieve short-term benefits, therefore, making it vulnerable\nto human players\u2019 attrition strategy that requires long-term\neffort to break. As shown in the two battles in Figure 11,\nafter many turns, the agent\u2019s entire team is defeated by the\nhuman players\u2019 Pok\u00b4emon, which have significantly boosted\ndefense and engage in frequent recovery. Table 8 reports\nthe performance of POK\u00b4ELLMON in battles where human\nplayers either use the attrition strategy or not. Obviously, in\nbattles without the attrition strategy, it outperforms Ladder\nplayers, while losing the majority of battles when human\nplay the attrition strategy.\nTable 8. Battle performance impacted by the attrition strategy\nLadder\nWin rate \u2191\nScore \u2191\nTurn #\nBattle #\nw. Attrition\n18.75%\n4.29\n33.88\n16\nw/o Attrition\n53.93%\n6.02\n15.95\n89\nThe \u201cRecover\u201d move recovers 50% HP in one turn, which\nmeans if an attack cannot cause the opposing Pok\u00b4emon more\nthan 50% HP damage in one turn, it will never faint. The\nkey to breaking the dilemma is to firstly boost a Pok\u00b4emon\u2019s\nattack to a very high stage and then attack to cause unre-\ncoverable damage, which is a long-term goal that requires\njoint efforts across many turns. POK\u00b4ELLMON is weak to the\nlong-term planing because current design does not keep a\nlong-term plan in mind across many timesteps, which will\nbe included in the future work.\nFinally, we observe that experienced human players can\nmisdirect the agent to bad actions. As shown in Figure 12,\nour Zygarde has one chance to use an enhanced attack move.\nAt the end of turn 2, the opposing Mawile is fainted, leading\nto a forced switch and the opponent choose to switch in\nFigure 12. An experienced human player misdirects the agent to\nuse a dragon-type attack by firstly sending out a dragon-type\nPok\u00b4emon and immediately switch to another Pok\u00b4emon immune to\nthe dragon-type attack.\nKyurem. This switch is a trick that lures the agent uses a\ndragon-type move in turn 3 because Kyurem is vulnerable\nto dragon-type attacks. In turn 3, the opponent switches in\nTapu Bulu at the beginning, a Pok\u00b4emon immune to dragon-\ntype attacks, making our enhanced attack chance wasted.\nThe agent is fooled because it makes decision only based\non the current state information, while experienced players\ncondition on not only the state information, but also the\nopponent\u2019s next action prediction.\nSeeing through tricks and predicting the opponent\u2019s next\naction require the agent being disciplined in the real battle\nenvironment, which is the future step in our work.\n8. Conclusion\nIn this paper, we enable LLMs to autonomously play the\nwell-known Pok\u00b4emon battles against human. We introduce\nPOK\u00b4ELLMON, the first LLM-embodied agent that achieves\nhuman-competent performance in tactical battle games. We\nintroduce three key strategies in the design of POK\u00b4ELLMON:\n(i) In-Context Reinforcement Learning, which consumes\nthe text-based feedback as \u201creward\u201d to iteratively refine the\naction generation policy without training; (ii) Knowledge-\nAugmented Generation that retrieves external knowledge to\ncombat hallucination and ensures the agent act timely and\nproperly; (iii) Consistent Action Generation that prevents\nthe panic switching issue when encountering powerful op-\nponents. The architecture of POK\u00b4ELLMON is general and\ncan be adapted for the design of LLM-embodied agents in\nmany other games, addressing the problems of hallucination\nand action inconsistency.\nOnline battles show that POK\u00b4ELLMON demonstrates human-\nlike battle ability and strategies, achieving 49% of win rate\nin the Ladder competitions and 56% of win rate in the\ninvited battles. Furthermore, we uncover its vulnerabilities\nto human players\u2019 attrition strategies and deception tricks,\nwhich are considered as our future work.\n8\nReferences\nList of abilities, 2024a. URL https://bulbapedia.\nbulbagarden.net/wiki/Ability#List_of_\nAbilities.\nList of moves, 2024b. URL https://bulbapedia.\nbulbagarden.net/wiki/List_of_moves.\nList of pok\u00b4emon by national pok\u00b4edex number, 2024c.\nURL\nhttps://bulbapedia.bulbagarden.\nnet/wiki/List_of_Pokmon_by_National_\nPokdex_number.\nPok\u00b4emon showdown, 2024.\nURL https://play.\npokemonshowdown.com.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAkata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M.,\nand Schulz, E. Playing repeated games with large lan-\nguage models. arXiv preprint arXiv:2305.16867, 2023.\nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C.,\nFried, D., Goff, A., Gray, J., Hu, H., et al. Human-level\nplay in the game of diplomacy by combining language\nmodels with strategic reasoning. Science, 378(6624):\n1067\u20131074, 2022.\nBatra, D., Chang, A. X., Chernova, S., Davison, A. J., Deng,\nJ., Koltun, V., Levine, S., Malik, J., Mordatch, I., Mot-\ntaghi, R., et al. Rearrangement: A challenge for embodied\nai. arXiv preprint arXiv:2011.01975, 2020.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\nlut, A., Brunskill, E., et al. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258,\n2021.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nCabello, L., Li, J., and Chalkidis, I. Pokemonchat: Audit-\ning chatgpt for pok\\\u2019emon universe knowledge. arXiv\npreprint arXiv:2306.03024, 2023.\nDuan, J., Yu, S., Tan, H. L., Zhu, H., and Tan, C. A sur-\nvey of embodied ai: From simulators to research tasks.\nIEEE Transactions on Emerging Topics in Computational\nIntelligence, 6(2):230\u2013244, 2022.\nGoertzel, B. Artificial general intelligence: concept, state of\nthe art, and future prospects. Journal of Artificial General\nIntelligence, 5(1):1, 2014.\nGoertzel, B. and Pennachin, C. Artificial general intelli-\ngence, volume 2. Springer, 2007.\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.\nRetrieval augmented language model pre-training. In\nInternational conference on machine learning, pp. 3929\u2013\n3938. PMLR, 2020.\nHafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering\ndiverse domains through world models. arXiv preprint\narXiv:2301.04104, 2023.\nHong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang,\nC., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al.\nMetagpt: Meta programming for multi-agent collabora-\ntive framework. arXiv preprint arXiv:2308.00352, 2023.\nHu, S., Huang, T., \u02d9Ilhan, F., Tekin, S. F., and Liu, L.\nLarge language model-powered smart contract vulner-\nability detection: New perspectives.\narXiv preprint\narXiv:2310.01152, 2023.\nHua, W., Fan, L., Li, L., Mei, K., Ji, J., Ge, Y., Hemphill, L.,\nand Zhang, Y. War and peace (waragent): Large language\nmodel-based multi-agent simulation of world wars. arXiv\npreprint arXiv:2311.17227, 2023.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,\nGoyal, N., K\u00a8uttler, H., Lewis, M., Yih, W.-t., Rockt\u00a8aschel,\nT., et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Advances in Neural Information Pro-\ncessing Systems, 33:9459\u20139474, 2020.\nLight, J., Cai, M., Shen, S., and Hu, Z. From text to tac-\ntic: Evaluating llms playing the game of avalon. arXiv\npreprint arXiv:2310.05036, 2023.\nMa, W., Mi, Q., Yan, X., Wu, Y., Lin, R., Zhang, H., and\nWang, J. Large language models play starcraft ii: Bench-\nmarks and a chain of summarization approach. arXiv\npreprint arXiv:2312.11865, 2023.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,\nT., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-\nchronous methods for deep reinforcement learning. In\nInternational conference on machine learning, pp. 1928\u2013\n1937. PMLR, 2016.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\n9\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nAnnual ACM Symposium on User Interface Software and\nTechnology, pp. 1\u201322, 2023.\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla:\nLarge language model connected with massive apis. arXiv\npreprint arXiv:2305.15334, 2023.\nRawte, V., Sheth, A., and Das, A.\nA survey of hallu-\ncination in large foundation models.\narXiv preprint\narXiv:2309.05922, 2023.\nSahovic, H. Poke-env: pokemon ai in python, 2023a. URL\nhttps://github.com/hsahovic/poke-env.\nSahovic, H.\npoke-env:\nHeuristicbot, 2023b.\nURL\nhttps://github.com/hsahovic/poke-env/\nblob/master/src/poke_env/player/\nbaselines.py.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nShinn, N., Labash, B., and Gopinath, A. Reflexion: an au-\ntonomous agent with dynamic memory and self-reflection.\narXiv preprint arXiv:2303.11366, 2023.\nSignificant Gravitas. AutoGPT. URL https://github.\ncom/Significant-Gravitas/AutoGPT.\nSingh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D.,\nTremblay, J., Fox, D., Thomason, J., and Garg, A. Prog-\nprompt: Generating situated robot task plans using large\nlanguage models. In 2023 IEEE International Conference\non Robotics and Automation (ICRA), pp. 11523\u201311530.\nIEEE, 2023.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,\nY., Fan, L., and Anandkumar, A. Voyager: An open-\nended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023a.\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\nChen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on\nlarge language model based autonomous agents. arXiv\npreprint arXiv:2308.11432, 2023b.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,\nS., Chowdhery, A., and Zhou, D. Self-consistency im-\nproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nWang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe,\nexplain, plan and select: Interactive planning with large\nlanguage models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023c.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824\u201324837, 2022.\nWikipedia.\nGameplay of pok\u00b4emon, 2023.\nURL\nhttps://en.wikipedia.org/wiki/\nGameplay_of_Pok%C3%A9mon.\nXi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B.,\nZhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and\npotential of large language model based agents: A survey.\narXiv preprint arXiv:2309.07864, 2023.\nXu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu,\nY. Exploring large language models for communication\ngames: An empirical study on werewolf. arXiv preprint\narXiv:2309.04658, 2023.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models. arXiv preprint arXiv:2210.03629,\n2022.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y.,\nand Narasimhan, K. Tree of thoughts: Deliberate prob-\nlem solving with large language models. arXiv preprint\narXiv:2305.10601, 2023.\n10\n"
  },
  {
    "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
    "link": "https://arxiv.org/pdf/2402.01566.pdf",
    "upvote": "24",
    "text": "Boximator: Generating Rich and Controllable Motions for Video Synthesis\nJiawei Wang\u2217\nYuchen Zhang\u2217\nJiaxin Zou\nYan Zeng\nGuoqiang Wei\nLiping Yuan\nHang Li\nByteDance Research\n\u2217 Equal Contribution\n{wangjiawei.424, zhangyuchen.zyc, zoujiaxin.zjx,\nzengyan.yanne, weiguoqiang.9, yuanliping.0o0 lihang.lh}@bytedance.com\nhttps://boximator.github.io\nAbstract\nGenerating rich and controllable motion is a pivotal\nchallenge in video synthesis. We propose Boximator, a new\napproach for fine-grained motion control. Boximator intro-\nduces two constraint types: hard box and soft box. Users\nselect objects in the conditional frame using hard boxes\nand then use either type of boxes to roughly or rigorously\ndefine the object\u2019s position, shape, or motion path in fu-\nture frames. Boximator functions as a plug-in for existing\nvideo diffusion models. Its training process preserves the\nbase model\u2019s knowledge by freezing the original weights\nand training only the control module.\nTo address train-\ning challenges, we introduce a novel self-tracking tech-\nnique that greatly simplifies the learning of box-object cor-\nrelations. Empirically, Boximator achieves state-of-the-art\nvideo quality (FVD) scores, improving on two base models,\nand further enhanced after incorporating box constraints.\nIts robust motion controllability is validated by drastic in-\ncreases in the bounding box alignment metric. Human eval-\nuation also shows that users favor Boximator generation\nresults over the base model.\n1. Introduction\nVideo synthesis has recently experienced remarkable ad-\nvancements [8, 9, 13, 14, 16, 26].\nThese models typi-\ncally utilize either a text prompt or a key frame to gener-\nate videos. Recent research focuses on enhancing the con-\ntrollability by incorporating frame-level constraints, such as\nsketches, depth maps [11, 34], human poses [7, 32, 38], tra-\njectories [35, 40], and conditional images [4, 23, 41].\nIn this work, we introduce a novel approach utilizing\nbox-shaped constraints as a universal mechanism for fine-\ngrained motion control. Our method introduces two types\nof constraints: hard box, which precisely delineates an ob-\nject\u2019s bounding box, and soft box, defining a broader re-\ngion within which the object must reside. The soft box can\nbe as tight as the object\u2019s exact bounding box, or as loose\nas the frame boundary. We control multiple objects across\nframes by associating unique object IDs with these boxes.\nOur proposed method, named Boximator (combining \u201cbox\u201d\nand \u201canimator\u201d), offers several benefits:\n1. Boximator serves as a flexible motion control tool. It\nmanages the motion of both foreground and background\nobjects, as well as modifies the pose of larger objects\n(e.g., human) by adjusting smaller components. Refer to\nFigure 1 for illustrations.\n2. In scenarios where generation is conditioned on an im-\nage, as seen in image-to-video and many state-of-the-art\ntext-to-video methods [9, 41], users can easily select ob-\njects by drawing hard boxes around them. This visually-\ngrounded approach is more straightforward compared to\nthe language-grounded controls [15, 21], which require\nverbal descriptions for all objects.\n3. For frames lacking user-defined boxes, Boximator al-\nlows approximate motion path control via algorithm-\ngenerated soft boxes.\nThese soft boxes can be con-\nstructed based on a pair of user-specified boxes, or based\non a hard box combined with a user-specified motion\npath. See Figure 1(c)-(e) for examples of user-specified\nmotion paths.\nBoximator functions as a plug-in for existing video diffu-\nsion models. We encode every box constraint by four coor-\ndinates, an object ID, and a hard/soft flag. During training,\nwe freeze the base model\u2019s text encoder and U-Net, feeding\nthe box encoding through a new type of self-attention layer.\nThis design is inspired by GLIGEN [17], where bounding\nboxes are combined with object description texts to achieve\nregion control for image synthesis. However, Boximator\narXiv:2402.01566v1  [cs.CV]  2 Feb 2024\nFigure 1. Motion control with Boximator: (a) use hard boxes to control the ending shape and position of a jumping cat; (b) force camera\nrotating to the left by pushing bed and window to the right; (c) control how a person raises a cup of coffee; (d) control the motion path of\na dog and a ball; (e) Use motion path and hard boxes to control the trajectory and proximity of two balloons. In all figures, dotted boxes\nrepresent first frame constraints; solid-line boxes represent last frame constraints; Arrowed lines represent motion paths. Example videos\nare initially generated as 256x256x16, then enhanced to 768x768x16 via a pretrained super-resolution model.\naims to control object motions without relying on textual\ngrounding, thus requires the learning of box-object correla-\ntion purely from visual inputs.\nEmpirically, we find that it is hard for the model to learn\nthis visual correlation through standard optimization. To\nmitigate this challenge, we introduce an novel training tech-\nnique termed self-tracking. This technique trains the model\nto generate colored bounding boxes as a part of the video.\nIt simplifies the challenge into two easier tasks: (1) produc-\ning a bounding box for each object with the right color, and\n(2) aligning these boxes with the Boximator constraints in\nevery frame. We observe that video diffusion models can\nquickly master these tasks. After that, we train the model\nto stop generating visible bounding boxes. Although these\nboxes are no longer visually present, their internal represen-\ntation persists, enabling the model to continue aligning with\nBoximator constraints.\nWe developed an automatic data annotation pipeline to\ngenerate 1.1M highly dynamic video clips with 2.2M an-\nnotated objects from the WebVid-10M dataset [1]. We uti-\nlized this dataset to train our Boximator model on two base\nmodels: the PixelDance model [41] and the open sourced\nModelScope model [31]. Extensive experiments show that\nBoximator retains the original video quality of these models\nwhile offering robust motion control in diverse real-world\nscenarios. On the MSR-VTT dataset, Boximator improves\nupon the base models in FVD score. With box constraints\nadded, video quality significantly improved further (Pixel-\nDance: 237 \u2192 174, ModelScope: 239 \u2192 216), and the\nobject detector\u2019s average precision (AP) score, measuring\nbox-object alignment, saw a remarkable increase (1.9-3.7x\nhigher on MSR-VTT and 4.4-8.9x higher on ActivityNet),\nhighlighting effective motion control. User study also fa-\nvored our model\u2019s video quality and motion control over\nthe base model by large margins (+18% for video quality,\n+74% for motion control). Furthermore, ablation studies\nconfirm the necessity of introducing soft boxes and training\nwith self-tracking for achieving these results.\n2. Related Work\nVideo diffusion models are natural extensions of image dif-\nfusion models. They extend the U-Net architecture from\nimage models by adding temporal layers [13, 26]. A widely\nadopted method for improving computational efficiency is\nto denoie in the latent space [12, 43]. Text-to-video (T2V)\ndiffusion models are often the foundation for various forms\nof conditional generation [2, 8, 31]. Recent advancements\nsuggest a two-step approach to T2V: initially creating an\nimage based on text, followed by producing a video that\nconsiders both the text and the pre-generated image. This\napproach allows the video model to concentrate on dynamic\naspects by using a static image as a reference, leading to\nimproved video quality [9, 33, 41]. The reference image\nprovides a natural grounding source for motion control.\nThere is a surge in research focused on enhancing\nthe controllability of T2V and I2V models.\nVideoCom-\nposer [34] enables conditions such as sketches, depth maps,\nand motion vectors.\nIn producing dance videos, hu-\nman poses extracted from reference videos are commonly\nused [7, 32, 38]. For more precise motion control, users\ncan plot object or camera trajectories [35, 40]. However,\nthese methods did not provide a precise way to define ob-\njects, making it challenging to select and control a larger\nor composite object from image. Moreover, trajectory does\nnot capture the object\u2019s shape and size, crucial for depicting\npose or proximity changes like arm spreading or approach-\ning movements.\nThere are two concurrent research studying the use of\nbounding boxes for motion control, but it should be noted\nthat their work differs from ours in key aspects.\nTrail-\nBlazer [21] is a training-free method that leverages attention\nmap edits to direct the model in generating a specific ob-\nject within a designated area. The object must be described\nin the text prompt. FACTOR [15] modified a transformer-\nbased generation model, Phenaki [30], by adding a box con-\ntrol module. Like TrailBlazer, FACTOR requires a text de-\nscription for each box, thus does not support visual ground-\ning. Neither of the above methods supports soft box con-\nstraints, nor do they study the associated challenges in train-\ning.\nMLP\nSpatial Self-Attention\nSpatial Cross-Attention\nBox Coordinates\nObject ID\nHard/Soft Flag\nCaption Tokens\nFrame-level Visual Tokens\nFrame-level Visual Tokens\nVisual Token\nControl Token (per object)\nBoximator Self-Attention\nFigure 2. Overview of the control module: adding a new self-\nattention layer to every spatial attention block, between the spatial\nself-attention and the spatial cross attention. During training, all\nthe original model parameters are frozen.\n3. Background: Video Diffusion Model\nBoximator is built on top of video diffusion models [14]\nusing the 3D U-Net architecture [24]. These models itera-\ntively predict the noise vector in noisy video inputs, grad-\nually transforming pure Gaussian noise into high-quality\nvideo frames. The U-Net, denoted by \u03f5\u03b8, processes a noisy\ninput z (either in pixel space or latent space), along with a\ntimestamp t and various conditions c, and predicts the noise\nin z. Optimization is achieved through a noise prediction\nloss:\nL\u03b8 = Ez0,c,\u03f5,t[\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, c)\u22252\n2],\nwhere z0 represents the ground truth video, \u03f5 is a Gaussian\nnoise vector, and zt is a noisily transformed version of z0:\nzt = \u221a\u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5.\nHere, \u00af\u03b1t denotes a predefined constant sequence.\nThe 3D U-Net is structured with alternating convolu-\ntion blocks and attention blocks.\nEach block comprises\ntwo components: a spatial component, handling individual\nvideo frames as separate images, and a temporal compo-\nnent, enabling information exchange across frames. Within\nevery attention block, the spatial component typically in-\ncludes a self-attention layer, followed by a cross-attention\nlayer, the latter used for conditioning the generation on a\ntext prompt.\n4. Boximator: Box-guided Motion Control\n4.1. Model Architecture\nOur objective is to endow existing video diffusion mod-\nels with motion control capabilities. Given that foundation\nmodels are pre-trained on extensive collections of web-scale\nimages and videos, it\u2019s crucial to preserve their acquired\nknowledge. To achieve this, we freeze the original model\nparameters and solely focus on training the newly incorpo-\nrated motion control module.\nThe architecture of our model is illustrated in Figure 2.\nIn every spatial attention block of video diffusion mod-\nels, there are two stacked attention layers: a spatial self-\nattention layer and a spatial cross-attention layer. We aug-\nment this stack by adding a new self-attention layer. Specif-\nically, if v denotes the visual tokens of a frame, and htext\nand hbox represent the embeddings of the text prompt and\nthe box constraints, respectively, then the modified spatial\nattention block is described as follows:\nv = v + SelfAttn(v)\nv = v + TS(SelfAttn([v, hbox]))\nv = v + CrossAttn(v, htext)\nwhere TS(\u00b7) is a token selection operation that exclusively\nconsiders visual tokens. The box embeddings hbox is a se-\nquence of control tokens. Each token represents a box and\nis defined by:\ntb = MLP(Fourier([bloc, bid, bflag])).\nHere, bloc is a 4-dimensional vector encapsulating the top-\nleft and bottom-right coordinates of the box, normalized to\nthe range [0,1]. The object ID, used to link boxes across\nframes, is represented by bid, which in our experiments is\nexpressed in RGB space. Each object thus corresponds to\na unique \u201ccolor\u201d for its boxes, making bid a 3-dimensional\nvector normalized to [0,1]. The bflag is a boolean indicator:\nit is set to 1 for hard boxes and 0 otherwise. These three\ninputs are concatenated and processed via a Fourier embed-\nding [22] followed by a multi-layer perceptron (MLP). Note\nhbox contains a fixed number of control tokens (indicated\nby N). When the actual number of boxes is smaller than N,\nwe use a learnable tnull to pad the empty slots.\n4.2. Data Pipeline\nIn the absence of a publicly available video dataset with ob-\nject tracking annotations, we curated our training set from\nthe WebVid-10M dataset [1]. Through empirical analysis,\nwe find that a vast majority of WebVid videos do not exhibit\nsubstantial object or camera movements.\nConsequently,\nsampling from this collection would be inefficient for train-\ning our motion control module. To address this issue, we cu-\nrated a more dynamic subset from WebVid. This involved\nevaluating every clip in the dataset, comparing their start\nand end frames, and retaining only those clips where the\ntwo frames are sufficiently different. This filtration yielded\na total of 1.1M video clips.\nFor every clip in our refined dataset, we took the first\nframe to generate an image description using a visual lan-\nguage model [19].\nThen we extract noun chunks from\nthese descriptions. These chunks, encompassing terms like\nFigure 3. Training data: all bounding boxes are projected to the\ncropped region (white dashed box).\nFigure 4. Self-tracking: train the model to track every constrained\nobject. This figure shows 3 frames where the black horse and the\nyellow box surrounding it are generated together.\n\u201dyoung man\u201d or \u201dwhite shirt,\u201d served as object prompts.\nWe then feed these prompts to a pre-trained grounding\nmodel [20] and object tracker [5] to generate bounding\nboxes and populate them across all frames of the video.\nThis approach successfully yielded bounding boxes for a\ntotal of 2.4M objects.\nDuring training, we take a random crop of the video,\nconforming to the specified target aspect ratio, and subse-\nquently project all bounding boxes onto this cropped re-\ngion (Figure 3).\nIf a bounding box entirely fall outside\nthe cropped area, then we project it as line segments along\nthe border of the crop. This allows users to control object\nmovements both into and out of the frame by drawing line\nsegments on the frame\u2019s border (See Figure 6(d) for an ex-\nample).\n4.3. Self-Tracking\nA significant challenge in video motion control lies in\nassociating box coordinates with objects and maintaining\ntemporal consistency across frames, namely making sure\nthat the same group of boxes always control the same ob-\nject. In practice, this proves to be challenging, as diffusion\nmodels often struggle to effectively link discrete control sig-\nnals, like coordinates and IDs, with visual elements. This\ndifficulty is exacerbated when the video contains multiple\noverlapping boxes. As Section 5.4 shows, with traditional\nloss optimization, the model failed to align to most box con-\nstraints after 110K steps of training.\nWe propose self-tracking as a simple technique to miti-\ngate this challenge. We train our model to generate colored\nbounding boxes for each constrained object in every frame,\nwith colors specified in the object\u2019s control token (Figure 4).\nIn other words, we train the model to perform generation\nand object tracking at the same time. This approach sim-\nplifies the problem into two easier tasks: (1) generating a\nbounding box for each object with the right color and (2)\naligning these boxes with the Boximator constraints in ev-\nery frame. Previous research in image synthesis [25] shows\nthat diffusion models can generate bounding boxes. We fur-\nther discover that diffusion models can maintain temporal\nconsistency, ensuring that boxes of the same color consis-\ntently track the same object across frames. With this ca-\npability, task (2) becomes straightforward. For hard box\nconstraints, the model only needs to put boxes at the spec-\nified coordinates, while for soft box constraints, it needs to\nput them within a specified region. Intuitively, self-tracked\nbounding boxes act as an intermediary representation. The\nmodel follows Boximator constraints to guide the genera-\ntion of these boxes, which in turn guide the generation of\nobjects.\nUpon completing the self-tracking training phase, we\nproceed to further train the model using the same dataset,\nbut excluding bounding boxes from the target frames. Re-\nmarkably, the model quickly learn to cease generating vis-\nible bounding boxes, but its box alignment ability persists.\nThis indicates that the self-tracking phase assists the model\nto develop an appropriate internal representation.\n4.4. Multi-Stage Training Procedure\nWe employ a multi-stage training procedure. Initially, in\nStage 1, the model is trained using all the provided ground\ntruth bounding boxes as hard box constraints. Since hard\nbox controls are easier to learn than the soft ones, this stage\nserves as a preliminary phase, establishing the model\u2019s ini-\ntial understanding of coordinates and IDs. Subsequently, in\nStage 2, we substitute 80% of these hard boxes with soft\nboxes. The soft boxes are generated by randomly and inde-\npendently expanding the hard ones in four directions: left,\nright, up, and down. The expansion margin for each di-\nrection is determined by a Beta(1, 8) distribution, so that\nthe average expansion is 1/9 of the frame\u2019s width or height,\nwhile the maximum expansion can extend up to the frame\u2019s\nboundary. Both Stage 1 and Stage 2 use the self-tracking\ntechnique outlined in Section 4.3. Finally, in Stage 3, we\ncontinue the Stage 2 training but without self-tracking.\n4.5. Inference\nDuring the inference stage, only a select few frames (such\nas the first and last) contain user-defined boxes. To achieve\nrobust control, we insert soft boxes to the other frames. This\nis done by first applying linear interpolation of user-defined\nboxes to those empty frames, and then relaxing the inter-\nFigure 5. Soft boxes in inference. We interpolate soft boxes and\nrelax them based on a pair of user-specified boxes (upper row), or\na user-specified box combined with a motion path (lower row).\npolated boxes by expanding the box regions (as described\nin Section 4.4) and marking them as \u201csoft box\u201d. This ap-\nproach ensures that the object roughly follows the intended\ntrajectory, while simultaneously offering the model suffi-\ncient flexibility to introduce variations. In cases where a\nuser draws a hard box in a frame and defines a motion path\nfor it, we let the box to slide along the path to construct in-\nterpolated boxes for each subsequent frame, then relax them\nto form soft box constraints. Figure 5 presents a visual il-\nlustration for the construction of soft boxes in both cases.\n5. Experiments\n5.1. Experiment Settings\nBase models\nWe train Boximator on two base models:\nPixelDance [41] and ModelScope [31]. Our experiments\nuse text prompts, box constraints, and optionally the first\nvideo frame as input conditions. PixelDance can directly\nuse the first frame as an input condition.\nModelScope\ndoesn\u2019t support direct image input, but we can still condi-\ntion it on an image by replacing the first frame of the noisy\nlatents z with the ground-truth frame\u2019s latents at each de-\nnoising step. In both cases, we freeze the original model\nweights and only train the control module. See Appendix A\nfor more training and inference details.\nDatasets\nWe test our models using the MSR-VTT [37],\nActivityNet [3] and UCF-1011 [28] datasets. MSR-VTT\ntest set consists of 2,990 samples with 20 prompts per ex-\nample. For text constraint, following [15, 41], we randomly\nselect one prompt per sample to generate one video. For\nbox constraint, MSR-VTT does not include bounding box\nannotations, so we automatically create reference (ground-\ntruth) bounding boxes. First, we identify noun chunks from\nthe text prompt. Then, we use Grounding DINO [20] to get\nbounding boxes on the first frame and DEVA [5] to extend\nthese boxes to subsequent frames.\n1The UCF-101 dataset details and results are discussed in Appendix B\nConsidering that the automatic annotations on MSR-\nVTT may be noisy, to increase the credibility of our results,\nwe manually annotated a portion of the ActivityNet valida-\ntion set. Specifically, we chose 796 video clips that include\nnoticeable object motion. The bounding boxes in the first\nframe have already been annotated by the ActivityNet En-\ntities dataset [44], and we manually extended the bounding\nbox annotations to all 16 frames.\nEvaluation metrics\nWe measure video quality using\nFr\u00b4echet Video Distance (FVD) [29] and measure text align-\nment using CLIP similarity score (CLIPSIM) [36].\nWe\ncompute the FVD metrics using the randomly selected 16\nframes of each ground truth video with an FPS of 4. For\nevaluating motion control, we use the average precision\n(AP) metric. We generate videos with ground-truth boxes\non the first and last frames as constraints. After creating a\nvideo, we detect bounding boxes with the aforementioned\nDINO+DEVA detection system. If an object is consistently\ntracked across all frames, we compare its detected bounding\nbox with the ground truth boxes on the first/last frame. AP\nis calculated following the MS COCO protocol [18]. When\nthe first frame is a given condition, we only compare boxes\non the last frame. We also report mean average precision\n(mAP), calculated as the average AP over 10 Intersection\nover Union (IoU) thresholds, from 0.5 to 0.95.\n5.2. Quantitative Evaluation\nVideo Quality\nTable 1 compares our models with recent\nvideo synthesis models on the MSR-VTT dataset. In text-\nto-video synthesis, our Boximator model outperforms the\nbase models, achieving competitive FVD scores of 237 and\n239 with PixelDance and ModelScope, respectively. This\nimprovement, despite using frozen base model weights, is\nprobably due to the control module\u2019s training on motion\ndata, enhancing dynamic scene handling.\nThe results in Table 1 indicates that the FVD score im-\nproves when extra conditions are added to the input. Specif-\nically, the introduction of box constraints (Box) enhances\nvideo quality (PixelDance: 237 \u2192 174; ModelScope: 239\n\u2192 216). We hypothesize this improvement is due to box\nconstraints providing a more realistic layout for video gen-\neration. However, when the generation is based on the first\nframe (F0), the impact of box constraints on FVD is reduced\n(PixelDance: 113 \u2192 102; ModelScope: 142 \u2192 132). This\nmight be because the layout is already set by F0.\nOur models achieve CLIPSIM scores that are on par with\nstate-of-the-art systems. We noticed a slight drop in CLIP-\nSIM scores when additional conditions (like F0 or Box) are\nintroduced. This occurs because the base model is opti-\nmized for aligning video with the text alone, whereas our\nmodel handles multiple types of alignment at the same time.\nA similar observation was reported in the FACTOR pa-\nper [15].\nMotion Control Precision\nTable 1 also presents the re-\nsults for motion control precision. In every case, adding box\nconstraints (Box) significantly improves the average preci-\nsion (AP) scores. This indicates that the model effectively\nunderstands and applies the box constraints. The FACTOR\npaper [15] reported the mAP score on MSR-VTT too. Al-\nthough our results aren\u2019t directly comparable to theirs due\nto differences in object annotations, we\u2019ve included their\nnumber (marked with \u2217) in Table 1 for reference.\nTable 2 presents the results on ActivityNet. We inten-\ntionally chose test videos from ActivityNet that feature sig-\nnificant object movements. As a result, the disparity in AP\nscores before and after adding box constraints is wider com-\npared to MSR-VTT. The mAP scores with box constraints\nare 4.4-8.9x higher than that without box on ActivityNet, in\ncontrast to 1.9-3.7x higher on MSR-VTT.\nIt\u2019s important to note that the AP scores in our experi-\nments are not equal to success rate in motion control. To\ncalculate AP, we compare the reference object boxes with\nthose generated by the video synthesis model and detected\nby the DINO+DEVA system. This detector isn\u2019t flawless;\nit might miss objects, detect irrelevant ones, or fail to track\nan object consistently across all frames. These potential er-\nrors in detection can impact the final AP score. Therefore,\nit\u2019s more insightful to focus on the difference in AP scores\nbetween methods, rather than the absolute values.\n5.3. Human Evaluation\nWe conducted a user preference study with four human\nraters on 100 samples. In each session, they were shown\ntwo videos in a random order: one generated by the base\nmodel (PixelDance), which uses a text prompt and the first\nframe as input, and the other by the Boximator model,\nwhich additionally uses box constraints. The raters were\nasked to evaluate their preference based on video quality\nand motion control. Detailed criteria for evaluation are pre-\nsented in Appendix C. As indicated in Table 3, the Boxima-\ntor model was preferred by a significant margin. It excelled\nin motion controls in 76% of the cases, outperformed by\nthe base model in only 2.2% of the cases. The Boxima-\ntor model\u2019s video quality was also favored (+18.4%), likely\ndue to the dynamic and vivid content resulting from box\nconstraints. See Appendix C for some sample videos.\n5.4. Ablation Study\nWe carry out ablation studies to understand the effect of our\ndesign choices. Initially, we exclude self-tracking from our\ntraining process. This means we train the model to pre-\ndict the original video without any visible bounding boxes.\nWe observe that omitting self-tracking greatly challenges\nModels\nExtra Input\nFVD(\u2193)\nCLIPSIM(\u2191)\nmAP/AP50/AP75(\u2191)\nMagicVideo [43]\n-\n1290\n-\n-\nLVDM [12]\n-\n742\n0.2381\n-\nModelScope [31]\n-\n550\n0.2930\n-\nShow-1 [42]\n-\n538\n0.3072\n-\nPixelDance [41]\n-\n381\n0.3125\n-\nPhenaki [30]\n-\n384\n0.2870\n-\nFACTOR-traj [15]\nBox\n317\n0.2787\n0.290\u2217/-/-\nPixelDance + Boximator\n-\n237\n0.3039\n0.094/0.193/0.076\nBox\n174\n0.2947\n0.349/0.479/0.359\nF0\n113\n0.2890\n0.194/0.330/0.177\nF0 + Box\n102\n0.2874\n0.365/0.521/0.384\nModelScope + Boximator\n-\n239\n0.3013\n0.096/0.195/0.084\nBox\n216\n0.2948\n0.312/0.470/0.309\nF0\n142\n0.2865\n0.141/0.260/0.126\nF0 + Box\n132\n0.2852\n0.300/0.456/0.299\nTable 1. Zero-shot results on MSR-VTT. F0 means given the first frame as condition. Box means box constraints. The results show\nthat Boximator retains or improves the video quality (FVD) of the base models. In all cases, adding box constraints (Box) significantly\nimproves the average precision (AP) score of bounding box alignment.\nBase Models\nExtra Input\nmAP/AP50/AP75(\u2191)\nPixelDance\n-\n0.050/0.103/0.041\nBox\n0.445/0.638/0.459\nF0\n0.079/0.165/0.072\nF0 + Box\n0.394/0.607/0.409\nModelScope\n-\n0.054/0.118/0.040\nBox\n0.361/0.563/0.372\nF0\n0.069/0.128/0.068\nF0 + Box\n0.304/0.522/0.291\nTable 2. Box alignment results on ActivityNet. In all cases, adding\nbox constraints significantly improves the AP score.\nCriteria\nBoximator wins\nDraw\nBase model wins\nVideo Quality\n35.2%\n48.0%\n16.8%\nMotion Control\n76.0%\n21.8%\n2.2%\nTable 3. Human side-by-side blind comparison on 100 samples.\nthe model\u2019s ability to associate control tokens with the cor-\nresponding objects. As shown in Table 4, the average pre-\ncision (AP) under box constraints falls drastically, reaching\na level that is only slightly better than the AP without box\nconstraints.\nNext, we examine the role of using soft boxes during\ninference. According to the standard inference method de-\nscribed in Section 4.5, we insert relaxed soft boxes in frames\n2-15, where the user does not specify any box constraints.\nTable 4 indicates that removing these relaxed soft boxes (by\nreplacing their control tokens with tnull) leads to a signif-\nicant decrease in average precision scores. We hypothe-\nsize that the inserted soft boxes act as a rough guide for\nMethods\nmAP (Box)\nmAP (F0+Box)\nMSR-VTT\nPixelDance + Boximator\n0.349\n0.365\nw/o self-tracking\n0.118\n0.187\nw/o soft boxes\n0.235\n0.274\nw/o freezing weights\n0.354\n0.343\nActivityNet\nPixelDance + Boximator\n0.445\n0.394\nw/o self-tracking\n0.083\n0.085\nw/o soft boxes\n0.248\n0.220\nw/o freezing weights\n0.404\n0.331\nTable 4. Ablation study: removing self-tracking and soft boxes\nboth result in significant drop in the box alignment metric. Train-\ning all model weights doesn\u2019t give extra benefits.\nmovement directions. Without this guide, the model tends\nto make more mistakes.\nFinally, we examine the impact of freezing the base\nmodel weights. For comparison, we trained a new model\nin which all parameters of the U-Net were optimized. We\nfind that the new model generates videos of roughly the\nsame quality, resulting in similar FVD scores as the stan-\ndard model in Table 1. When it comes to motion control\nprecision, as shown in Table 4, this new model scored sim-\nilarly as the default one on MSR-VTT, and lower on Activ-\nityNet. In summary, our results suggest that it\u2019s not neces-\nsary to train all the U-Net parameters.\n5.5. Case Study\nIn this section, we highlight the model\u2019s capability of han-\ndling complex scenarios. Figure 6(a) demonstrates a gen-\nFigure 6. Case study: (a) Generation and motion control based on four boxes; (b) A motion that affects significant portion of the frame; (c)\nBox defined on a combination of objects (e.g., \u201ca man on a horse\u201d); (d) Adding new objects to the scene.\neration task based on four boxes. Boximator successfully\npopulates each box with the target object (a pig) as spec-\nified in the text prompt.\nThis contrasts with the second\nrow, where the model without box constraint only produces\ntwo pigs. Indeed, previous research has found that text-\nconditioned diffusion models struggle with precise object\ncount control without box constraints [39].\nFigure 6(b) illustrates a dynamic scene where a baby is\nmoved across the entire frame.\nThe box has guided the\nmodel to generate the motion, which appeared to be chal-\nlenging to generate without box constraints (see the next\nrow). Figure 6(c) highlights the generalizability of box-\nbased visual grounding. Here, a user wants to control an\nobject combination: a man on a horse. The model inter-\nprets this constraint, moving the composite object towards\nthe frame\u2019s left edge. Finally, Figure 6(d) showcases the\nmodel\u2019s capability to introduce a new object into a scene.\nThe user indicates the entry point of a man by drawing a\nsegment line along the left border. The model successfully\ndirects a man entering from the left edge, stopping at the\ncenter.\n6. Conclusion\nWe proposed Boximator, a genral approach to controlling\nobject motion in video synthesis. Boximator utilizes two\ntypes of boxes to allow users to select arbitrary objects and\ndefine their motions without entering extra text. It can be\nbuilt on any video diffusion model without modifying the\noriginal model weights, thus its performance can improve\nwith evolving base models. Additionally, we proposed a\nself-tracking method that significantly simplifies the train-\ning for the control module.\nWe believe that our design\nchoices and training techniques can be adapted to enable\nother forms of control, such as conditioning with human\nposes and key points.\nEthical and Social Risks\nVideo\ngeneration\ntechnologies,\nespecially\nadvanced\nvideo diffusion models, carry potential ethical and so-\ncial risks.\nThese include the creation of deepfakes,\nwhich can lead to misinformation and privacy viola-\ntions; biases in AI-generated content, potentially leading\nto unfair or discriminatory outcomes; and impacts on\nintellectual property and creative industries,\npossibly\nundermining the value of human creativity.\nIt\u2019s cru-\ncial for developers and users of these technologies to\nbe aware of these risks and ensure their responsible use.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew\nZisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion, pages 1728\u20131738, 2021. 2, 4\n[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal,\nDaniel Mendelevitch, Maciej Kilian, Dominik Lorenz,\nYam Levi, Zion English, Vikram Voleti, Adam Letts,\net al.\nStable video diffusion: Scaling latent video\ndiffusion models to large datasets.\narXiv preprint\narXiv:2311.15127, 2023. 3, 12\n[3] Fabian Caba Heilbron, Victor Escorcia, Bernard\nGhanem, and Juan Carlos Niebles.\nActivitynet: A\nlarge-scale video benchmark for human activity un-\nderstanding. In Proceedings of the ieee conference on\ncomputer vision and pattern recognition, pages 961\u2013\n970, 2015. 5\n[4] Xinyuan\nChen,\nYaohui\nWang,\nLingjun\nZhang,\nShaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang,\nDahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-\nto-long video diffusion model for generative transi-\ntion and prediction. arXiv preprint arXiv:2310.20700,\n2023. 1\n[5] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexan-\nder Schwing, and Joon-Young Lee. Tracking anything\nwith decoupled video segmentation. In Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pages 1316\u20131326, 2023. 4, 5\n[6] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang,\nand Tat-Seng Chua.\nEmpowering dynamics-aware\ntext-to-video diffusion with large language models.\narXiv preprint arXiv:2308.13812, 2023. 12\n[7] Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng\nHui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi,\nXiaowen Li, et al. Dreamoving: A human video gen-\neration framework based on diffusion models. arXiv\ne-prints, pages arXiv\u20132312, 2023. 1, 3\n[8] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon,\nAndrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin\nHuang, Ming-Yu Liu, and Yogesh Balaji.\nPreserve\nyour own correlation: A noise prior for video diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 22930\u2013\n22941, 2023. 1, 3, 12\n[9] Rohit Girdhar,\nMannat Singh,\nAndrew Brown,\nQuentin Duval, Samaneh Azadi, Sai Saketh Ramb-\nhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan\nMisra. Emu video: Factorizing text-to-video gener-\nation by explicit image conditioning. arXiv preprint\narXiv:2311.10709, 2023. 1, 3\n[10] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu,\nXing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang,\nYu-Gang Jiang, and Hang Xu. Reuse and diffuse: It-\nerative denoising for text-to-video generation. arXiv\npreprint arXiv:2309.03549, 2023. 12\n[11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh\nAgrawala, Dahua Lin, and Bo Dai.\nSparsectrl:\nAdding sparse controls to text-to-video diffusion mod-\nels. arXiv preprint arXiv:2311.16933, 2023. 1\n[12] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,\nand Qifeng Chen. Latent video diffusion models for\nhigh-fidelity video generation with arbitrary lengths.\narXiv preprint arXiv:2211.13221, 2022. 3, 7, 12\n[13] Jonathan Ho, William Chan, Chitwan Saharia, Jay\nWhang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J\nFleet, et al.\nImagen video: High definition video\ngeneration with diffusion models.\narXiv preprint\narXiv:2210.02303, 2022. 1, 3\n[14] Jonathan Ho,\nTim Salimans,\nAlexey Gritsenko,\nWilliam Chan, Mohammad Norouzi, and David J\nFleet. Video diffusion models. In Advances in Neural\nInformation Processing Systems, pages 8633\u20138646.\nCurran Associates, Inc., 2022. 1, 3\n[15] Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu\nJiang, Xuhui Jia, Yukun Zhu, and Ming-Hsuan\nYang.\nFine-grained controllable video generation\nvia object appearance and context.\narXiv preprint\narXiv:2312.02919, 2023. 1, 3, 5, 6, 7\n[16] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00b4e Lezama,\nJonathan Huang, Rachel Hornung, Hartwig Adam,\nHassan Akbari, Yair Alon, Vighnesh Birodkar, et al.\nVideopoet:\nA large language model for zero-shot\nvideo generation. arXiv preprint arXiv:2312.14125,\n2023. 1\n[17] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou\nMu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee.\nGligen: Open-set grounded text-to-\nimage generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, pages 22511\u201322521, 2023. 1\n[18] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar,\nand C Lawrence Zitnick.\nMicrosoft coco:\nCom-\nmon objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part V 13,\npages 740\u2013755. Springer, 2014. 6\n[19] Haotian Liu,\nChunyuan Li,\nQingyang Wu,\nand\nYong Jae Lee.\nVisual instruction tuning.\narXiv\npreprint arXiv:2304.08485, 2023. 4\n[20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li,\nHao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying\ndino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023. 4,\n5\n[21] Wan-Duo Kurt Ma, J. P. Lewis, and W. Bastiaan\nKleijn. Trailblazer: Trajectory control for diffusion-\nbased video generation. 2023. 1, 3\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\nNerf: Representing scenes as neural radiance fields for\nview synthesis. Communications of the ACM, 65(1):\n99\u2013106, 2021. 4, 12\n[23] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang\nWang, Yujie Wei, Yingya Zhang, Changxin Gao,\nand Nong Sang.\nHierarchical spatio-temporal de-\ncoupling for text-to-video generation. arXiv preprint\narXiv:2312.04483, 2023. 1\n[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks for biomedical im-\nage segmentation. In Medical Image Computing and\nComputer-Assisted Intervention\u2013MICCAI 2015: 18th\nInternational Conference, Munich, Germany, October\n5-9, 2015, Proceedings, Part III 18, pages 234\u2013241.\nSpringer, 2015. 3\n[25] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval\nKirstain, Amit Zohar, Oron Ashual, Devi Parikh, and\nYaniv Taigman.\nEmu edit: Precise image editing\nvia recognition and generation tasks. arXiv preprint\narXiv:2311.10089, 2023. 5\n[26] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin,\nJie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-\nto-video generation without text-video data.\narXiv\npreprint arXiv:2209.14792, 2022. 1, 3, 12\n[27] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 12\n[28] Khurram Soomro, Amir Roshan Zamir, and Mubarak\nShah.\nUcf101:\nA dataset of 101 human actions\nclasses from videos in the wild.\narXiv preprint\narXiv:1212.0402, 2012. 5\n[29] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol\nKurach, Raphael Marinier, Marcin Michalski, and\nSylvain Gelly.\nTowards accurate generative models\nof video: A new metric & challenges. arXiv preprint\narXiv:1812.01717, 2018. 6\n[30] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan\nKindermans, Hernan Moraldo, Han Zhang, Moham-\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and\nDumitru Erhan. Phenaki: Variable length video gen-\neration from open domain textual description. arXiv\npreprint arXiv:2210.02399, 2022. 3, 7\n[31] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya\nZhang, Xiang Wang, and Shiwei Zhang.\nMod-\nelscope text-to-video technical report. arXiv preprint\narXiv:2308.06571, 2023. 2, 3, 5, 7, 12\n[32] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,\nZhengyuan Yang, Hanwang Zhang, Zicheng Liu, and\nLijuan Wang.\nDisco: Disentangled control for re-\nferring human dance generation in real world. arXiv\npreprint arXiv:2307.00040, 2023. 1, 3\n[33] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan,\nShuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu,\nJun Hao Liew, Hanshu Yan, et al.\nMagicvideo-v2:\nMulti-stage high-aesthetic video generation.\narXiv\npreprint arXiv:2401.04468, 2024. 3\n[34] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou\nChen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli\nZhao, and Jingren Zhou.\nVideocomposer: Compo-\nsitional video synthesis with motion controllability.\narXiv preprint arXiv:2306.02018, 2023. 1, 3\n[35] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui\nChen, Menghan Xia, Ping Luo, and Ying Shan. Mo-\ntionctrl: A unified and flexible motion controller for\nvideo generation. arXiv preprint arXiv:2312.03641,\n2023. 1, 3\n[36] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li,\nLei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.\nGodiva: Generating open-domain videos from natural\ndescriptions. arXiv preprint arXiv:2104.14806, 2021.\n6\n[37] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A\nlarge video description dataset for bridging video and\nlanguage. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5288\u2013\n5296, 2016. 5\n[38] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Han-\nshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and\nMike Zheng Shou. Magicanimate: Temporally con-\nsistent human image animation using diffusion model.\narXiv preprint arXiv:2311.16498, 2023. 1, 3\n[39] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Lin-\njie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng\nLiu, Ce Liu, Michael Zeng, et al.\nReco: Region-\ncontrolled text-to-image generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14246\u201314255, 2023. 8\n[40] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi,\nHouqiang Li, Gong Ming, and Nan Duan.\nDrag-\nnuwa: Fine-grained control in video generation by in-\ntegrating text, image, and trajectory. arXiv preprint\narXiv:2308.08089, 2023. 1, 3\n[41] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou,\nYang Wei, Yuchen Zhang, and Hang Li. Make pix-\nels dance: High-dynamic video generation.\narXiv\npreprint arXiv:2311.10982, 2023. 1, 2, 3, 5, 7, 12\n[42] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent\ndiffusion models for text-to-video generation. arXiv\npreprint arXiv:2309.15818, 2023. 7\n[43] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei\nLv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient\nvideo generation with latent diffusion models. arXiv\npreprint arXiv:2211.11018, 2022. 3, 7, 12\n[44] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J\nCorso, and Marcus Rohrbach.\nGrounded video de-\nscription. In CVPR, 2019. 6\nA. More Implementation Details\nControl Module\nWe follow NeRF [22] to use Fourier embeddings to encode box coordinates, object ID and the hard/soft\nflag. We make sure that all input dimensions are scaled between 0 and 1. For any given input x within this range, its Fourier\nembedding is defined as:\nFourier(x) = [cos(x \u00b7 1000/8), . . . , cos(x \u00b7 1007/8), sin(x \u00b7 1000/8), . . . , sin(x \u00b7 1007/8)].\nWe combine these Fourier embeddings of each input to form the overall embedding, which has a dimension of 128. As\nmentioned in Section 4.1, these embeddings are then processed through a multi-layer perceptron (MLP). This MLP has three\nhidden layers, each with a dimension of 512. Finally, the output control token is adjusted to match the dimension of the visual\ntoken, which is 1024.\nTraining & Inference Details\nOur models train on 16-frame sequences with a resolution of 256x256 pixels, running at 4\nframes per second. We limit the maximum number of objects to N = 8. The training uses the Adam optimizer, with a batch\nsize of 128 across 16 NVIDIA Tesla A100 GPUs. As outlined in Section 4.4, training occurs in three stages: 50k iterations\nfor stage 1, 50k iterations for stage 2, and 10k iterations for stage 3. We use 2 \u00d7 10\u22124 learning rate for the first stage, and\n3 \u00d7 10\u22125 for later stages. All stages use linear learning rate scheduler with 7,500 warm-up steps. Since box conditioning\nis optional, we use 25% of our training data from videos without any box annotation. Since first frame conditioning is also\noptional, we let half of the training samples include the video\u2019s first frame as a condition.\nFor all experiments, we use the DDIM inference algorithm [27] with 50 inference steps. To enable classifier-free guidance,\nwe construct negative conditions by substituting every control token with tnull. We set the classifier-free guidance scale to be\n9.\nB. Results on UCF-101\nModels\nExtra Input\nFVD(\u2193)\nmAP/AP50/AP75(\u2191)\nMagicVideo [43]\n-\n699\n-\nLVDM [12]\n-\n641\n-\nModelScope [31]\n-\n410\n-\nMake-A-Video [26]\n-\n367\n-\nVidRD [10]\n-\n363\n-\nPYOCO [8]\n-\n355\n-\nDysen-VDM [6]\n-\n325\n-\nPixelDance [41]\n-\n242\n-\nStable Video Diffusion [2]\n-\n242\n-\nPixelDance + Boximator\n-\n270\n0.060/0.127/0.044\nBox\n263\n0.228/0.354/0.229\nF0\n132\n0.171/0.272/0.163\nF0 + Box\n142\n0.284/0.419/0.279\nModelScope + Boximator\n-\n310\n0.063/0.131/0.047\nBox\n311\n0.192/0.308/0.184\nF0\n196\n0.132/0.223/0.119\nF0 + Box\n194\n0.212/0.343/0.205\nTable 5. Zero-shot results on UCF-101.\nWe follow the experiment settings of PixelDance [41] to evaluate on UCF-101. Specifically, we sampled 2,048 videos\nfrom the UCF-101 test set, generating descriptive text prompts for each of them, and then generated 10,240 16-frame videos.\nWe compute the FVD real features from the original 2,048 videos by sampling 16 frames from each video. Reference\nbounding boxes were automatically annotated using the same method as for MSR-VTT. Given generated videos, we employed\nDINO+DEVA for bounding box detection and computed average precision (AP) scores. It\u2019s noteworthy that UCF-101\u2019s\nprompts are more detailed than those for MSR-VTT and ActivityNet. Since the automatic annotation uses the text prompt to\nextract object names, the longer prompts lead to more, albeit noisier, boxes per video.\nTable B presents our UCF-101 results, showing trends consistent with MSR-VTT. The Boximator model roughly main-\ntained or improves the FVD scores compared to the base model. While using the first frame (F0) as a condition notably\nboosted FVD scores, box constraints had minimal impact to FVD, likely due to the noisier nature of UCF-101\u2019s boxes.\nIn all scenarios, using box constraints significantly increased AP scores, echoing results from MSR-VTT and ActivityNet.\nHowever, the absolute AP values on UCF-101 were lower than on the other datasets, probably due to the lower quality of box\nannotations.\nC. Human Evaluation Details\nWe selected 100 high-quality videos featuring prominent camera or object movements from WebVid (excluded from training\nset) and manually annotated their bounding boxes. Then we generate new videos using both the standard PixelDance model\nand PixelDance+Boximator, with the video caption and the first frame taken as inputs. The Boximator model additionally\nused bounding boxes from the first and last frames. Four human raters assessed the regenerated videos, marked as \u201cVideo 1\u201d\nand \u201cVideo 2,\u201d presented in a randomized order to obscure the generating model. Raters evaluated the videos for quality and\nmotion control, choosing between \u201cVideo 1 is better,\u201d \u201cVideo 2 is better,\u201d or \u201cno preference.\u201d\nVideo Quality\nRaters evaluated each video for visual distortions, blurs, or other quality defects, and for temporal inconsis-\ntencies, such as inconsistent object appearances across frames. In cases where both videos were free from these issues, raters\nfavored the video with richer content. For instance, when comparing two videos where one exhibits interesting motion and\nthe other remains mostly stationary, raters are expected to favor the more dynamic one.\nMotion Control\nThe evaluation focused on whether each video satisfied motion constraints set by the bounding boxes in\nthe initial and final frames. Preference was given to the video meeting these constraints. If both or neither video met the\nconstraints, raters are expected to select \u201cno preference.\u201d\nSome sample videos and their evaluations results are displayed in Figures 7 to 9.\nFigure 7. Sample videos from human evaluation (Part 1). Each group displays two rows: the first generated by the Boximator model\nand the second by the base model. Vote results are denoted as X/Y/Z, indicating raters\u2019 preferences: X for Boximator model, Y for no\npreference, and Z for base model.\nFigure 8. Sample videos from human evaluation (Part 2).\nFigure 9. Sample videos from human evaluation (Part 3).\n"
  },
  {
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "link": "https://arxiv.org/pdf/2402.01032.pdf",
    "upvote": "22",
    "text": "Repeat After Me:\nTransformers are Better than State Space Models at Copying\nTransformers are Better than State Space Models at Copying\nSamy Jelassi 1 David Brandfonbrener 2 Sham M. Kakade 2 3 Eran Malach 2\nAbstract\nTransformers are the dominant architecture for se-\nquence modeling, but there is growing interest in\nmodels that use a fixed-size latent state that does\nnot depend on the sequence length, which we refer\nto as \u201cgeneralized state space models\u201d (GSSMs).\nIn this paper we show that while GSSMs are\npromising in terms of inference-time efficiency,\nthey are limited compared to transformer models\non tasks that require copying from the input con-\ntext. We start with a theoretical analysis of the\nsimple task of string copying and prove that a two\nlayer transformer can copy strings of exponential\nlength while GSSMs are fundamentally limited by\ntheir fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of\nefficiency and generalization on synthetic tasks\nthat require copying the context. Finally, we eval-\nuate pretrained large language models and find\nthat transformer models dramatically outperform\nstate space models at copying and retrieving infor-\nmation from context. Taken together, these results\nsuggest a fundamental gap between transformers\nand GSSMs on tasks of practical interest.\n1. Introduction\nTransformers (Vaswani et al., 2017) are the workhorse of\nmodern sequence modeling, achieving remarkable perfor-\nmance on a variety of tasks, but they have unavoidable\ninefficiencies. Specifically, they require \u2126(L) memory1 and\n1Harvard University, Center of Mathematical Sciences and Ap-\nplications 2Harvard University, Kempner Institute for the Study\nof Natural and Artificial Intelligence 3Harvard University, Depart-\nments of Computer Science and Statistics. Correspondence to:\nSamy Jelassi <sjelassi@fas.harvard.edu>.\n1In some naive implementations of transformers, it is common\nto allocate a L \u00d7 L matrix to compute the attention. However,\nmemory efficient implementations, such as FlashAttention (Dao\net al., 2022), compute the attention with O(L) memory.\ncompute to predict the next token of a sequence of length L.\nThis has spurred a boom in attempts to create architectures\nthat can achieve similar performance as transformers, but\nwith O(1) memory to predict each token. This class of\nmodels includes state space models like S4 (Gu et al., 2021)\nor Mamba (Gu & Dao, 2023), as well as traditional RNN\nmodels (Hochreiter & Schmidhuber, 1997) and models that\ncan be trained in parallel like linear attention (Katharopoulos\net al., 2020; Choromanski et al., 2020) and parallel RNNs\n(Bradbury et al., 2016; Peng et al., 2023; Sun et al., 2023).\nIn this paper, we will refer to this entire class of models that\nuse a fixed-size memory as \u201cgeneralized state space models\u201d\nor GSSMs (see a formal definition in Section 2).\nRecent work has demonstrated impressive performance of\nGSSMs, but it is not yet clear what these models sacrifice\nfor their improved efficiency, if anything. In this paper, we\nfind that one particular capability that is sacrificed is the\nability to retrieve and repeat parts of the input context. As a\nresult, transformers are better than GSSMs at a variety of\ntasks that require accessing arbitrary parts of the context.\nTo understand this gap in capabilities, we begin by present-\ning a theoretical analysis of the copying task2. First, we\nshow via construction that a simple transformer model can\ncopy strings of length that is exponential in the number of\nheads of the transformer. This construction relies on the\nability of the transformer to implement a mechanism of\n\u201cstorage\u201d and retrieval of sequences of n tokens (n-grams),\nwhere the n-grams are used to track where to copy from. In\ncontrast, we show that, trivially, GSSMs cannot accurately\ncopy strings with more bits than the size of the latent state.\nOur theory studies representation expressivity, but not\nwhether these representations will be learned. Moreover, in\npractice a large GSSM may have enough capacity to repre-\nsent the entire input in the latent state, at least in theory. To\nresolve these concerns, we conduct a variety of synthetic\nexperiments with models of \u223c160M parameters. We find\nthat transformers are both much more efficient at learning to\n2Note that we study copying of the input and not copying of\ntraining data (McCoy et al., 2023; Carlini et al., 2022)\n1\narXiv:2402.01032v1  [cs.LG]  1 Feb 2024\nRepeat After Me: Transformers are Better than State Space Models at Copying\n104\n105\n106\nNumber of training examples\n0\n25\n50\n75\n100\nAccuracy (%)\nTransformer:\nGSSM:\n(a) Copying: training efficiency.\n37 50\n100\n200\n500\n1000\nNumber of characters in string\n0\n25\n50\n75\n100\nAccuracy (%)\nRoPE\nNoPE\nAlibi\nHAlibi\nLSTM\nMamba\n(b) Copying: length generalization\n20\n100\n200\nNumber of entries in phone-book\n0\n25\n50\n75\n100\nAccuracy (%)\nPythia:\n410M\n1.4B\n2.8B\nMamba:\n360M\n1.4B\n2.8B\n(c) Lookup with pretrained models\nFigure 1. (a) Copying: training efficiency. Here we train models to copy strings of length \u2264 300 and evaluate string-level accuracy on\nstrings of length 300. Transformers train much faster than GSSMs. An LSTM cannot even learn the task within this number of samples.\n(b) Copying: length generalization. Here we train models to copy on strings of length \u2264 50 until all models are perfect in-distribution\nand evaluate string-level accuracy. Purple dotted line indicates maximum training string length and green dotted line indicates context\nwindow during training. Evaluating on longer inputs, the transformer models dramatically outperform the GSSMs. Using our Hard-Alibi\npositional encoding, we can even generalize well beyond the training context size. (c) Lookup with pretrained models. Here the task\nrequires looking up and retrieving a number from a \u201cphone book\u201d of varying length that is entirely in context. We evaluate pretrained\nmodels 1-shot without any finetuning. Pythia (a transformer model) substantially outperforms Mamba (a GSSM) across model sizes.\ncopy (Figure 1a) and also generalize better to longer inputs\n(Figure 1b). Additionally, we verify experimentally that the\ncopy \u201calgorithm\u201d learned by transformers indeed relies on n-\ngrams to perform a lookup of where to copy from (Figure 3),\nsimilarly to our theoretical construction.\nFinally, we present a variety of experiments on pre-trained\nmodels to test their ability to remember and access the input\ncontext. In particular, we show that Pythia transformers\n(Biderman et al., 2023) outperform Mamba GSSMs (Gu &\nDao, 2023) of similar size at a variety of memory-intensive\ntasks including copying and retrieving information from the\ncontext (Figure 1c). This is especially notable since the\nMamba models achieve lower perplexity than the Pythia\nmodels at language modeling on the Pile (Gao et al., 2020).\nThese experiments illustrate the practical relevance of the\nmemory issues that we raise, and hint at one way that archi-\ntectual choices can impact the downstream performance of\nLLMs above and beyond training perplexity.\n2. Theory: Representational Capacity\nIn this section we use the copy task for a theoretical com-\nparison between state space models and transformers. We\nprove two main results. First, we construct a small trans-\nformer that solves the copy task for sequences lengths that\nare exponential in the transformer size. Second, we show\nthat any state space model fails to solve the copy task, unless\nits latent state grows linearly with the sequence length.\n2.1. Setting\nLet D be a dictionary, which contains D \u201calphabet\u201d tokens.\nA sequence-to-sequence model is a function H : D\u2217 \u2192\nD\u2217, which maps an input sequence of tokens to an output\nsequence. We think of the input x1, . . . , xi as the \u201cprompt\u201d\nto the model, and of the output sequence H(x1, . . . , xi) as\nthe generated \u201canswer\u201d.\nA sequence-to-token mapping is a function h : D\u2217 \u2192\nD. Any sequence-to-token model h naturally defines a\nsequence-to-sequence model H by auto-regressive infer-\nence. Namely, for every input sequence x1, . . . , xi \u2208 D\nwe define recursively xi+j = h(x1, . . . , xi+j\u22121) and let\nH(x1, . . . , xi) = (xi+1, xi+2, . . . ).\nGeneralized state space models.\nA state space S is some\nfinite set. We denote by mem(S) the number of bits required\nto encode the states of S, namely mem(S) = log(|S|).\nA generalized state space model (GSSM) is a sequence\nmodel defined by an update rule u : S \u00d7 D \u2192 S and\nsome output function r : S \u2192 D. Let s0 \u2208 S be some\ninitial state. Given some sequence x1, . . . , xL, the state\nof the model at iteration i is denoted by Si(x1, . . . , xi)\nand the output token is denoted by Ri(x1, . . . , xi). The\nstate and output are defined recursively:\n1) S0(\u2205) =\ns0, 2) Si(x1, . . . , xi) = u(Si\u22121(x1, . . . , xi\u22121), xi), 3)\nRi(x1, . . . , xi) = r(Si(x1, . . . , xi)).\nRemark 2.1. It is important to note that for any sequence\nmodel, there are two types of memory considerations:\n1) input-independent memory (parameters) and 2) input-\ndependent memory (activations). The GSSM definition con-\nstraints the input-dependent memory (activations), which\n2\nRepeat After Me: Transformers are Better than State Space Models at Copying\nFigure 2. An illustration of the copy task.\ncorresponds to mem(S), and does not restrict in any way\nthe amount of input-independent memory (parameters) or\nthe run-time of state updates. Since our main goal is to show\na lower bound on the state space memory, leaving all other\nconsiderations unconstrained only strengthens our results.\nTransformers.\nGiven some input of length L and di-\nmension d, denoted x1, . . . , xL \u2208 Rd, an attention head\nis parameterized by Wk, Wq, Wv \u2208 Rd\u00d7d.\nWe denote\nki = Wkxi, qi = Wqxi, vi = Wvxi and denote Ki =\n[k1, . . . , ki] \u2208 Rd\u00d7i and Vi = [v1, . . . , vi] \u2208 Rd\u00d7i. We\ndenote the output of the head at token i by oi \u2208 Rd, where\noi = Vi \u00b7 softmax(Ki \u00b7 qi).\nWe consider a transformer with l attention heads, each one\nof dimension d so that the full dimension of the Transformer\nis dl. An embedding is some mapping \u03a8 : D \u2192 Rd. An\nMLP is a function f : Rdl \u2192 Rdl s.t. f(x) = U1\u03c3(U2x),\nfor some activation function \u03c3. Both the embedding and the\nMLP layer are assumed to be applied on the token level. An\nattention-block is a set of l heads applied in parallel, and a\ntransformer-block is an attention-block followed by an MLP\nwhich operates on the concatenated output of the l heads.\nThe output of the model is sampled based on the output\nof the final layer. For simplicity, we study the arg max\n\u201csampling\u201d (i.e., predicting the most probable token).\nThe copy task.\nTo define the copy task, we add two\nspecial tokens to D: (1) beginning-of-sequence token, de-\nnoted \u27e8BOS\u27e9, and (2) copy token, denoted \u27e8COPY\u27e9. So now\n|D| = D + 2. A length-L copy distribution DL over DL+2\ngenerates strings of the form: \u201c\u27e8BOS\u27e9 , x1, . . . , xL, \u27e8COPY\u27e9\u201d,\nwhere x \u2208 (D \\ {\u27e8BOS\u27e9 , \u27e8COPY\u27e9})L.\nFor some sequence-to-sequence model H : D\u2217 \u2192 D\u2217, we\ndenote the error of H on a copy distribution DL by\nerrDL(H) = Pr\nDL [H1:L(\u27e8BOS\u27e9 , x, \u27e8COPY\u27e9) \u0338= x]\nwhere H1:L(\u00b7) denotes the first L tokens generated by H.\nThat is, we expect the model to output an exact copy of x.\n2.2. Transformers can copy inputs of exponential length\nIn this section we show that transformers can implement the\ncopy operation for input sequences with length exponential\nFigure 3. An illustration of the n-gram based copy algorithm. In\norder to predict the next token, we match the current n-gram to the\ncorresponding n-gram in the input, then output the next token.\nin the number of heads. Namely, we construct a transformer\nwith two blocks that gets small error on the copy task.\nConstruction: hash-based copying.\nThe key idea in the\nconstruction is to first \u201chash\u201d sequences of n tokens (n-\ngrams), then at each iteration of the auto-regression attend\nto the previous occurrence of the most recent n-gram, and\noutput the succeeding token. That is, we show that a trans-\nformer can implement the copying algorithm illustrated in\nFigure 3 (and see also Algorithm 1 in the Appendix).\nPositional embedding: Hard-ALiBi.\nTo perform the\nhashing described in the algorithm, we need to be able\nto leverage local positional information to define a hash,\nand also to apply this hash function globally on the entire\ninput. To do this, we use a hard version of ALiBi (Press\net al., 2021), which we call Hard-ALiBi. Just as in AL-\niBi, we add a bias bi to the i-th attention head as follows:\noi = Vi \u00b7 softmax(Ki \u00b7 qi + bi). Specifically, we set bi s.t.\nbi,j = \u2212\u221e for j \u2264 i \u2212 m and bi,j = 0 for j > i \u2212 m. We\nallow different heads with different choices of m and also\nallow for m = \u221e which corresponds to softmax attention\nwith no positional embedding. This is illustrated in Figure\n8c (Appendix). While the Hard-ALiBi is introduced for our\ntheoretical construction, we observe it also offers significant\nbenefits empirically, as discussed in Section 3.\nGuarantees.\nThe copy algorithm given in Algorithm 1\n(and similarly, our transformer construction) can perfectly\ncopy the input sequence, as long as there are no repeated\nn-gram patterns in the input. Therefore, the error of the\nalgorithm depends on the probability of repeated n-grams:\nDefinition 2.2. Let DL be some copy distribution. For some\nn \u2208 N, let pn\u2212gram(DL) be the probability that x1, . . . , xL\ncontains two repeated sequences of n tokens. Namely:\npn\u2212gram(DL) = Pr\nDL [\u2203i\u0338=j s.t. xi, . . . xi+n = xj, . . . , xj+n]\n3\nRepeat After Me: Transformers are Better than State Space Models at Copying\nBelow we state the main theoretical result on copying with\ntransformers, showing that transformers can copy their input,\nwith error bounded by the probability of repeated n-grams:\nTheorem 2.3. For all n, there exists a depth-2 transformer\nT of dimension O(n log(D)) s.t. for all 2n \u2264 L \u2264 Dn, and\nfor any copy distribution DL, errDL(T ) < pn\u2212gram(DL).\nIntuitively, the probability of repeated n-grams decays\nquickly when increasing the value of n. Indeed, we show\nthat for the uniform distribution over sequences, this proba-\nbility decays exponentially with n:\nLemma 2.4. Let DL be the copy distribution generated by\nsampling x from the uniform distribution over the \u201calphabet\u201d\n(non-special) tokens. Then, pn\u2212gram(DL) < L2D\u2212n.\nCombining the above results, we get that transformers can\ncopy sequences of tokens drawn from the uniform distri-\nbution, using a number of parameters that depends only\nlogarithmically on the input sequence length.\nCorollary 2.5. Fix some \u03f5 \u2208 (0, 1/2) and some L \u2265\n\u2126(log(1/\u03f5)). There exists a depth-2 transformer T of di-\nmension O(log(L/\u03f5) log(D)) s.t. for the uniform copy dis-\ntribution DL, errDL(T ) < \u03f5.\nRemark 2.6. For simplicity we do not limit the precision of\nthe parameters or activations, but note that our results hold\nfor finite-precision transormers, using O(log(log(L))) bits.\n2.3. State Space Models cannot copy inputs beyond\nmemory size\nWe saw that transformers are able to copy uniform se-\nquences of tokens, with parameter count logarithmic in the\nsequence length. We now show that GSSMs cannot copy\nuniform input sequences, unless the capacity of their state\nspace grows linearly with the size of the sequence length.\nThis is intuitive: to be able to copy the entire input sequence,\nthe model needs to store it in its state space, which requires\nthe memory to grow linearly with the sequence length.\nTheorem 2.7. Fix some GSSM H over state space S. Then,\nfor all L, for the uniform copy distribution DL, the model\nH has error errDL(H) > 1 \u2212 |S|\nDL .\nGiven Theorem 2.7, the following Corollary is immediate:\nCorollary 2.8. Fix some L \u2208 N. Then, every GSSM H\nwith state space S s.t. mem(S) < L log(D) \u2212 1 has error\nerrDL(H) > 1/2 for the uniform copy distribution DL.\nRemark 2.9. As mentioned previously, the input-dependent\nmemory of transformers grows linearly with the sequence\nlength, which is less memory-efficient compared to GSSMs.\nHowever, it is interesting to note that from the above result,\nat least for the copy task, transformers are almost optimal\nin terms of their input-dependent memory. More specifi-\ncally, an implication of Theorem 2.3 is that there exists a\ntransformer which can copy inputs of length L using \u02dcO(L)\ninput-dependent memory3, and due to Corollary 2.8 this is\nindeed optimal (up to logarithmic factors).\n3. Learning to Copy\nIn the previous section, we proved that transformers can rep-\nresent the copy operation for exponentially long sequences,\nwhile GSSMs fail to copy long sequences due to their lim-\nited memory. While these results show that in theory, trans-\nformers can outperform GSSMs, our theoretical results do\nnot establish that such a gap will be observed in practice\nfor two reasons. First, it is not clear that transformers can\nindeed learn to copy from examples. Second, GSSMs in\npractice may use a large latent state memory, so that our\nbounds only hold for very long sequences of tokens. For\nexample, a latent state of 1000 32-bit floating point numbers\nhas enough bits to store at least 2000 tokens from a 50K\ntoken vocabulary. However, even though a GSSM could fit\nthe context into memory, it may not learn to do so.\nOur goal in this section is to verify that our theoretical anal-\nysis bears out experimentally when training models from\nscratch on synthetic data, before moving on to study pre-\ntrained models in the next section. Specifically, we train\ntransformers and GSSMs (LSTM (Hochreiter & Schmidhu-\nber, 1997) and Mamba (Gu & Dao, 2023)) on variants of\nthe copy task shown in Figure 2.\n3.1. Experimental setup\nWe now provide a brief overview of our experimental setup.\nFurther details may be found in Appendix A.\nArchitecture.\nIn all our experiments, we set the model\nhyperparameters so that the Mamba and transformers have\na similar number of parameters (\u2248 160 million parameters).\nSince we find that large LSTMs are hard to train (as con-\nfirmed in Pascanu et al. (2013)), we use the largest LSTM\nwe managed to train which has \u2248 40 million parameters.\nDataset.\nDuring training, we generate in an online man-\nner a batch of 64 examples at each epoch. At test time,\nwe evaluate our models on 10 batches of 128 examples.\nWe report the mean and standard-deviation over these 10\nbatches.\nIf not specified otherwise, our token space V\nis of size 30 and made of the alphabet letters i.e. V =\n{a, . . . , z, \u27e8BOS\u27e9 , \u27e8EOS\u27e9 , \u27e8COPY\u27e9} where \u27e8BOS\u27e9 is the begin-\nning of sentence token, \u27e8EOS\u27e9 the end of sentence token\nand \u27e8COPY\u27e9 the separator token. All the strings are sampled\nuniformly i.e. we first sample the length of the sequence\nand then independently sample each position of the string\nfrom V. Finally, we \u201cpack the context\u201d with i.i.d. sequences\n3We use \u02dcO to hide logarithmic factors.\n4\nRepeat After Me: Transformers are Better than State Space Models at Copying\nduring training similarly to (Zhou et al., 2023): we fill the\ncontext with multiple independent samples of the task.\nPositional information.\nPositional information also plays\nan important role in the length generalization capacity of\nTransformers (Jelassi et al., 2023; Kazemnejad et al., 2023;\nShen et al., 2023). Previously popular methods of input-\nlayer positional embeddings (e.g. sinusoidal (Vaswani et al.,\n2017) or learned (Radford et al., 2019)) have been replaced\nby relative positional encodings at each attention layer (e.g.\nRoPE (Su et al., 2023), Alibi (Press et al., 2021), or NoPE\n(Kazemnejad et al., 2023)). Below, we experiment these\npositional encodings along with the Hard-Alibi encoding\nintroduced in Section 2.\n3.2. Data efficiency on the copy task\nWe begin by training our models on the simple task of copy-\ning a sequence of input tokens described in Figure 2. The\nmodel gets an input of \u2264 L tokens followed by a Separa-\ntor (\u27e8COPY\u27e9) token, and needs to output the same sequence\nagain from the beginning. In this section, we focus on in-\ndistribution learning: we train on strings of random length\n\u2264 L = 300 and record the string-level accuracy on evalua-\ntion strings sampled from the training distribution.\nResults for this experiment are shown in Figure 1a. Clearly,\nthere is a large gap between the transformers and GSSMs.\nWe observe that the transformers need 100x less samples\nthan the best GSSMs to learn the copy task.\nNote that the sharp changes in accuracy displayed in Fig-\nure 1a are due to the log-scaled x-axis and choice of string-\nlevel accuracy as a metric. In Figure 9a, we report the\ncharacter-level accuracy, which yields smoother curves\ndemonstrating the learning process of GSSMs. Regard-\ning LSTMs, we find that they do not manage to learn on\nlength-300 strings even at the character level. In Figure 9b,\nwe show that LSTMs are able to learn to copy on shorter\nstrings and that string length is the bottleneck.\n3.3. Length generalization on the copy task\nThe prior experiment demonstrates superior efficiency of\nlearning in-distribution. Now, we test the ability of the\nlearned functions to generalize out-of-distribution. Specif-\nically, we consider generalization from short sequences to\nlonger sequences. Testing this sort of generalization can\nhelp us to better understand which function the model has\nlearned, i.e. whether the model has truly learned the \u201ccorrect\u201d\ncopy operation or whether it just learned to copy sequences\nof the particular size it was trained on.\nHere, we train all models on sequences of \u2264 50 tokens,\nand test them on sequences of up to 1000 tokens, reporting\nstring-level accuracy. As seen in Figure 1b, all models\n40\n100\n200\nNumber of characters in string\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nn-gram length:\n2\n3\n4\n5\n6\n7\n8\nFigure 4. String-level copying accuracy on data with duplicated\nn-grams. Copying fails when the duplicated n-gram is too long as\nthe model can no longer perform n-gram lookups.\nare able to (eventually) solve the task in-distribution on\nlengths of \u2264 50, but transformer-based models display much\nbetter generalization to longer inputs compared to GSSMs.\nNamely, we observe that the performance of the GSSMs\n(LSTM and MAMBA) drops to zero almost immediately\nwhen increasing the input length, while the performance of\ntransformers decays much more gradually with length.\nPositional information.\nWhen looking at the relative per-\nformance of different transformer models in Figure 1b, it\nbecomes clear that the positional encoding is important to\nlength generalization. Specifically, the ALiBi and NoPE\ntransformers dramatically outperform the RoPE model on\nlonger inputs. This is likely because the sinusoidal embed-\ndings of RoPE create a more dramatic change than the decay\nof ALiBi or NoPE when we go to longer inputs.\nImproved generalization with Hard-ALiBi.\nTo test our\nunderstanding of how transformers learn to copy, we now\nconsider swapping in the Hard-ALiBi positional encoding\nthat we used in our theoretical construction of hash-based\ncopying (introduces in Subsection 2.2 and illustrated in Fig-\nure 8 in the Appendix). Figure 1b shows that a transformer\ntrained with Hard-ALiBi embedding on sequences of length\n\u2264 50 achieves almost perfect length generalization up to\nsequences of length 1000. Note that this is well beyond the\ncontext length ever encountered in training.\n3.4. Transformers learn to use n-gram hashing\nNext, we attempt to determine whether the transformer\ntrained on the copy task indeed applies the mechanism of\nstorage and retrieval of n-grams. To do this, we evaluate the\nperformance of a transformer with Hard-ALiBi positional\nencoding trained on the copy task when tested on a dis-\ntribution of examples that intentionally contains duplicate\nn-grams. That is, we draw uniform sequences of tokens,\nand then randomly replace some n-gram with another n-\ngram that already appears in the sequence, such that each\n5\nRepeat After Me: Transformers are Better than State Space Models at Copying\n10\n30\n50\n70\n100\nNumber of characters in string\n0\n25\n50\n75\n100\nAccuracy (%)\nTransformer:\nNoPE\nAlibi\nHAlibi\nGSSM:\nLSTM\nMamba\nFigure 5. Top: An illustration of the suffix key veriant of the n-\ngram lookup task. Bottom: When trained on strings of length \u2264\n30, transformers outperform GSSMs on longer inputs, illustrating\nsuperior performance on this memory-intensive task.\nexample always contains two copies of the same n-gram\n(typically followed by a different token). We use the Hard-\nAlibi model here since it performs the best for the copy task\nas showed in Figure 1a. Figure 4 shows the performance\nof the transformer for different choices of n. We observe\nthat the transformer maintains roughly the same accuracy\nfor n \u2264 4, but that its accuracy starts dropping when the\ninputs contains duplicate sequences of 5 or more tokens.\nThis suggests that the transformer relies on something like\n5-gram retrieval to do the copy task.\n3.5. GSSMs cannot arbitrarily retrieve from context\nWe now introduce another task to probe the mechanisms\nthat the models use to copy from the context: the n-gram\nlookup task. In this task the model needs to use a given\nn-gram as a key to look up the k-token key that follows the\nquery. We consider two variants of the task: suffix keys and\nprefix keys. In both variants, we assess length generalization\nto understand the function that the models have learned.\nFirst, we consider the suffix key version of n-gram lookup.\nIn this task, the model is given a sequence L of input tokens,\na separator, and then an n-gram from the input sequence.\nThe model then needs to output a sequence of k tokens fol-\nlowing the chosen n-gram (see Figure 5 for an illustration).\nThis task is closely related to induction heads (Olsson et al.,\n2022). This task requires the model to be able to \u201cstore\u201d the\nentire context in order to effectively find the correct key to\naccess it\u2019s query. We train all models on sequences of at\n10\n30\n50\n70\n100\nNumber of characters in string\n20\n40\n60\n80\n100\nAccuracy (%)\nTransformer:\nNoPE\nAlibi\nHAlibi\nGSSM:\nLSTM\nMamba\nFigure 6. Top: An illustration of the prefix key veriant of the n-\ngram lookup task. Bottom: When trained on strings of length\n\u2264 30, GSSMs perform as well as the Hard-Alibi transformer\nand better than the other transformers. This slight variant of the\ntask requires much less memory and is thus more suitable to the\nstrengths of GSSMs at storing a small state over time.\nmost 30 tokens and show results in Figure 5. Transformers\nperform well on this task, with a relatively small drop in\nperformance when increasing the sequence length up to 100.\nThis suggests that transformers can learn to perform n-gram\nstorage and retrieval. GSSMs, however, perform poorly\nbeyond their training distribution. Intuitively, this task still\nrequires the models to store the entire input sequence, some-\nthing that GSSMs struggle to do.\nNext, we try the prefix key version of n-gram lookup. Here\nwe provide the n-gram key at the beginning and then the\nfull input sequence (illustrated in Figure 6). In this version\nof the task the model does not need to store the entire input\nsince it can look for the key on the fly as the sequence is pro-\ncessed. This is good for the GSSMs, since they can write the\nkey into the state and then ignore inputs that do not match.\nIndeed, GSSMs achieve perfect length-generalization on\nthis variant. Interestingly, the GSSMs even outperform the\nNoPE and ALiBi transformers (although not the Hard-Alibi\nmodel). We hypothesize that this may be an issue where\nthese positional embeddings make it more difficult to ef-\nfectively perform the hashing lookup over a long distance\nin relative positions. Taken together, these results illustrate\nhow GSSMs seem to be memory limited, but can be effec-\ntive when the tasks only require a summary of the inputs\nrather than storing the entire context.\n6\nRepeat After Me: Transformers are Better than State Space Models at Copying\n4. Pre-trained Models\nIn this section, we compare the performance of pre-trained\ntransformers and pre-trained GSSMs on memory-intensive\ntasks such as copying long strings, retrieval and few-shot\nquestion answering. We show that transformers outper-\nform GSSMs of similar scale on such memory-intensive\ntasks, even when the GSSM has lower perplexity as a lan-\nguage model. These results confirm that the limitation of\nGSSMs raised in previous sections apply to large scale mod-\nels trained on real pretraining data.\n4.1. Setup\nIn the experiments below, we compare Pythia transformer\nmodels (Biderman et al., 2023) of sizes ranging from 410M\nto 2.8B against Mamba models (Gu & Dao, 2023) of similar\nsizes. All these models have been pre-trained on the Pile\n(Gao et al., 2020) and use the same tokenizer. The Mamba\nmodels generally have slightly lower perplexity on the train-\ning set for a given size. The main difference between the\nPythia and the Mamba models is their architectural design.\nWe compare these models by measuring their performance\nwhile varying the input instance length and consider two\ntypes of tasks: copy-based and information retrieval tasks.\nThe copy-based tasks consist of presenting a random text\nto the model and asking it to copy the text. In the infor-\nmation retrieval tasks, we provide a text to the model and\nask it a related question. These retrieval tasks can be seen\nas \u201dselective copy\u201d, since the model needs to copy a small\nchunk of the input text in order to respond to the question.\nTo measure performance, we use the string-level accuracy\nin all the experiments except in Figure 7c where we con-\nsider question answering and thus report the F1 score. We\nevaluate the models over 10 batches of size 64 for all the\ntasks except for question answering where we evaluate over\n50 questions because the number of questions with a given\ncontext length is limited. Further details are in Appendix A.\n4.2. Copying the input context\nWe first observe that pre-trained transformers outperform\npre-trained GSSMs at copying long natural language strings.\nIn Figure 7a, we randomly sample strings from the C4\ndataset (Raffel et al., 2020) with varying number of tokens.\nOur prompt consists of two copies of the sampled string\nplus the first word of the string and we expect the model\nto complete the third copy. Even the smallest transformer\nmodel dramatically outperforms the largest GSSM. This\nhappens even though the large GSSMs have enough bits in\nthe state variable to potentially store the context. This con-\nfirms the idea that this is an architectual bias of transformers\nthat makes it easier for them to copy from the context.\nUnlike strings of tokens sampled uniformly at random, natu-\nral text can often be compressed, possibly allowing language\nmodels to copy longer strings even with limited memory.\nTo test whether this matters, in Figure 7b we conduct the\nsame experiment as above but randomly shuffle the order\nof the words in the strings. We find that when we shuffle\nthe words, both GSSMs and transformers perform worse on\nthe task, but the effect is more stark for GSSMs. Even the\nlargest GSSM now gets zero accuracy on strings of length\n300. This suggests that when the input is more difficult to\ncompress, the GSSM suffers due to its fixed size state.\n4.3. Retrieval from the input context\nWhile copying provides a clear task to separate the model\nclasses, it is not a particularly realistic task. That said, it\npresents an extreme case of a type of behavior that is highly\nrelevant for many tasks of interest. In particular, many tasks\nrequire retrieving specific information from the context that\nis relevant to the desired output. This subsection presents\nexamples of how our results transfer to more practical tasks.\nPhone-book lookup.\nWe first consider a \u201cphone-book\u201d\nexperiment where we provide a synthetic phone-book to the\nmodel and ask it to return the phone number when given\na name. We generate the phone-book by randomly sam-\npling L names and their associated phone number. One\nline of this phone-book looks like \u201cJohn Powell: 609-323-\n7777\u201d. Our prompt to the model consists of the phone-\nbook, two few-shot examples and a question asking for\nthe phone number of a randomly sampled name from the\nphone-book. Figure 1c reports the accuracy obtained by\nthe pretrained transformers and GSSMs while varying the\nsize of the phone-book L. We observe that even the small-\nest transformer (410M parameters) outperforms the largest\nGSSMs (2.8B parameters) when the phone-book size is long\nenough (L \u2265 70). This shows that in retrieval tasks which\nrequire access to the whole context, GSSMs struggle to store\nthe relevant information in their fixed-size state.\nQuestion-Answering.\nIn this experiment, we compare\nthe 2.8B parameter Mamba and transformer models4, on\nthe SQuAD question-answering dataset (Rajpurkar et al.,\n2018). This dataset provides text paragraphs together with a\nfew questions regarding the text. We probe the models to\nanswer the question by providing a single demonstration of a\nquestion/answer pair (corresponding to the same text) before\ngiving the target question. We bin the paragraphs according\nto their lengths, and report the F1 score as a function of\nthe paragraph length for both models in Figure 7c. We\nobserve that while for short paragraphs, both the Pythia\ntransformer and Mamba achieve comparable performance,\nthe performance of Mamba degrades more quickly with\n4In our experiments, smaller models were unable to achieve\nreasonable and consistent performance on this dataset.\n7\nRepeat After Me: Transformers are Better than State Space Models at Copying\n50 100\n200\n300\n400\n500\nNumber of tokens\n0\n25\n50\n75\n100\nAccuracy (%)\nPythia:\n410M\n1.4B\n2.8B\nMamba:\n360M\n1.4B\n2.8B\n(a) Copy: natural language strings\n50 100\n200\n300\n400\n500\nNumber of tokens\n0\n25\n50\n75\n100\nAccuracy (%)\nPythia:\n410M\n1.4B\n2.8B\nMamba:\n360M\n1.4B\n2.8B\n(b) Copy: shuffled strings\n50\n210\n410\nNumber of tokens in the context\n0\n20\n40\n60\nF1 score (%)\nPythia:\n2.8B\nMamba:\n2.8B\n(c) Question answering (SQUAD)\nFigure 7. (a) Copy: natural language strings. We compare pretrained models on their ability to copy natural language strings sampled\nfrom C4 of varying lengths and report string-level accuracy. The transformer models substantially outperform the GSSMs. (b) Copy:\nshuffled strings. To test whether it mattered that the strings were in natural language, we randomly shuffle the word order of the strings\nfrom the previous experiment. We find that this degrades performance, especially for the Mamba models. (c) Question answering\n(SQUAD). We compare Pythia and Mamba on a standard question answering dataset where we bin the dataset based on the length of the\ncontext paragraph. We find that Mamba performance decays more quickly with the length of the context.\nthe paragraph length, while the transformer-based model\nmaintains a similar accuracy even for longer texts. This\nresult shows that the fixed-memory of GSSMs also limits\ntheir performance on standard natural tasks.\n5. Related Work\nThere exists a broad body of prior work on the representa-\ntional capacity of GSSMs like RNNs (Merrill, 2019; Merrill\net al., 2020) as well as transformers (Weiss et al., 2021;\nMerrill et al., 2022; Wei et al., 2022; Sanford et al., 2023;\nEdelman et al., 2022). Previous works that study transform-\ners do so through comparison to other complexity classes,\nsuch as threshold circuits (Merrill et al., 2022), RASP lan-\nguage (Weiss et al., 2021) or first-order logic (Chiang et al.,\n2023) (see Strobl et al. (2023) for a thorough review). These\nworks do not provide insights into how transformers imple-\nment algorithms for solving specific problems. In contrast,\nour theoretical result constructs a transformer for the copy\ntask, which illustrates the mechanism and provides tight\nbounds on the model size. Together with the result show-\ning that GSSMs cannot copy long sequences, our theory\ncharacterizes the power of different sequence models on\nthe copy task. Other theoretical separation results between\ntransformers and RNNs (Sanford et al., 2023; Merrill, 2019)\nuse more complex tasks of less practical relevance.\nOther papers have previously demonstrated the capacity of\ntransformers to leverage the entire input context for tasks\nlike retrieval, question answering, and in-context learning\n(Devlin et al., 2018; Raffel et al., 2020; Petroni et al., 2020;\nBrown et al., 2020; Liu et al., 2023b). Another line of work\nhas studied the \u201cinduction head\u201d mechanism in transformers\nthat performs a retrieval operation much like the one we\nobserve for copying (Olsson et al., 2022). But, to our knowl-\nedge, there is not a comparison in related work between\ntransformers and GSSMs of similar quality on these tasks.\nSeveral of our experiments study length generalization as\na way to assess whether the model found the \u201cright way\u201d\nto solve the task. Prior work on length generalization in\ntransformers has focused on the data distribution (Anil et al.,\n2022), positional embeddings (Kazemnejad et al., 2023),\nand arithmetic tasks (Del\u00b4etang et al., 2022; Ruoss et al.,\n2023; Jelassi et al., 2023; Zhou et al., 2023). We extend\nmany of these ideas to the copying task.\nFinally, we note that while we focus on tasks where trans-\nformers outperform GSSMs, there are also tasks where\nGSSMs outperform transformers. For example, Liu et al.\n(2023a) shows that transformers fail to generalize out of dis-\ntribution for \u201cflip-flop language modeling\u201d, while LSTMs\ndo so easily. These tasks require tracking a small O(1) state\nvariable over time. Another benefit of GSSMs is the abil-\nity to input long contexts like DNA sequences that may be\nimpractical for transformers (Nguyen et al., 2023).\n6. Discussion\nWe have demonstrated through theory and experiments that\ntransformers are better than GSSMs at copying from their in-\nput context. However, we emphasize that state space models\nhave many advantages over transformers. The memory and\ncomputational complexity of GSSMs does not increase with\nthe input length, which is ideal for training and inference on\nlong inputs. Additionally, state space models such as RNNs\nare better at tracking state variables across long sequences\n(Liu et al., 2023a), which may be useful for generating long\nconsistent text. Importantly, language processing in the hu-\n8\nRepeat After Me: Transformers are Better than State Space Models at Copying\nman brain appears to be much more similar to how state\nspace models process language (Tikochinski et al., 2024).\nWe therefore believe that future work should focus on build-\ning hybrid architectures that endow state space models with\nan attention-like mechanism, allowing them to retrieve rele-\nvant pieces of text from their input. Indeed, humans have\nan incredibly limited capacity for memorizing sequences\n(Miller, 1956), but can translate entire novels if we allow\nthem to look back at the text (Shelton, 1612).\nAcknowledgements\nWe thank Boaz Barak for helpful discussions. Kempner\nInstitute computing resources enabled this work. Samy\nJelassi acknowledges funding supported by the Center of\nMathematical Sciences and Applications. This work has\nbeen made possible in part by a gift from the Chan Zucker-\nberg Initiative Foundation to establish the Kempner Institute\nfor the Study of Natural and Artificial Intelligence. Sham\nKakade acknowledges funding from the Office of Naval\nResearch under award N00014-22-1-2377.\nReferences\nAnil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra,\nV., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and\nNeyshabur, B. Exploring length generalization in large\nlanguage models. Advances in Neural Information Pro-\ncessing Systems, 35:38546\u201338556, 2022.\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\nH., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\nlyzing large language models across training and scaling.\nIn International Conference on Machine Learning, pp.\n2397\u20132430. PMLR, 2023.\nBradbury, J., Merity, S., Xiong, C., and Socher, R.\nQuasi-recurrent neural networks.\narXiv preprint\narXiv:1611.01576, 2016.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F.,\nand Zhang, C. Quantifying memorization across neu-\nral language models. arXiv preprint arXiv:2202.07646,\n2022.\nChiang, D., Cholak, P., and Pillay, A. Tighter bounds on\nthe expressivity of transformer encoders. arXiv preprint\narXiv:2301.10743, 2023.\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., et al. Rethinking attention with performers.\narXiv preprint arXiv:2009.14794, 2020.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. Advances in Neural Information Process-\ning Systems, 35:16344\u201316359, 2022.\nDel\u00b4etang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wen-\nliang, L. K., Catt, E., Hutter, M., Legg, S., and Ortega,\nP. A. Neural networks and the chomsky hierarchy. arXiv\npreprint arXiv:2207.02098, 2022.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nEdelman, B. L., Goel, S., Kakade, S., and Zhang, C. Induc-\ntive biases and variable creation in self-attention mecha-\nnisms. In International Conference on Machine Learning,\npp. 5793\u20135831. PMLR, 2022.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027,\n2020.\nGu, A. and Dao, T.\nMamba:\nLinear-time sequence\nmodeling with selective state spaces.\narXiv preprint\narXiv:2312.00752, 2023.\nGu, A., Goel, K., and R\u00b4e, C. Efficiently modeling long\nsequences with structured state spaces. arXiv preprint\narXiv:2111.00396, 2021.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735\u20131780, 1997.\nJelassi, S., d\u2019Ascoli, S., Domingo-Enrich, C., Wu, Y., Li,\nY., and Charton, F. Length generalization in arithmetic\ntransformers. arXiv preprint arXiv:2306.15400, 2023.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In International conference on ma-\nchine learning, pp. 5156\u20135165. PMLR, 2020.\nKazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P.,\nand Reddy, S. The impact of positional encoding on\nlength generalization in transformers.\narXiv preprint\narXiv:2305.19466, 2023.\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang,\nC. Exposing attention glitches with flip-flop language\nmodeling. arXiv preprint arXiv:2306.00946, 2023a.\n9\nRepeat After Me: Transformers are Better than State Space Models at Copying\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilac-\nqua, M., Petroni, F., and Liang, P. Lost in the middle:\nHow language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023b.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017.\nMcCoy, R. T., Smolensky, P., Linzen, T., Gao, J., and Ce-\nlikyilmaz, A. How much do language models copy from\ntheir training data? evaluating linguistic novelty in text\ngeneration using raven. Transactions of the Association\nfor Computational Linguistics, 11:652\u2013670, 2023.\nMerrill, W. Sequential neural networks as automata. arXiv\npreprint arXiv:1906.01615, 2019.\nMerrill, W., Weiss, G., Goldberg, Y., Schwartz, R., Smith,\nN. A., and Yahav, E. A formal hierarchy of rnn architec-\ntures. arXiv preprint arXiv:2004.08500, 2020.\nMerrill, W., Sabharwal, A., and Smith, N. A. Saturated\nTransformers are Constant-Depth Threshold Circuits.\nTransactions of the Association for Computational Lin-\nguistics, 10:843\u2013856, 08 2022. ISSN 2307-387X. doi:\n10.1162/tacl a 00493. URL https://doi.org/10.\n1162/tacl_a_00493.\nMiller, G. A. The magic number seven plus or minus two:\nSome limits on our capacity for processing information.\nPsychological review, 63:91\u201397, 1956.\nNguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes,\nC., Wornow, M., Patel, A., Rabideau, C., Massaroli, S.,\nBengio, Y., et al. Hyenadna: Long-range genomic se-\nquence modeling at single nucleotide resolution. arXiv\npreprint arXiv:2306.15794, 2023.\nOlsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,\nN., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen,\nA., et al. In-context learning and induction heads. arXiv\npreprint arXiv:2209.11895, 2022.\nPascanu, R., Mikolov, T., and Bengio, Y. On the difficulty\nof training recurrent neural networks. In International\nconference on machine learning, pp. 1310\u20131318. Pmlr,\n2013.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\net al. Rwkv: Reinventing rnns for the transformer era.\narXiv preprint arXiv:2305.13048, 2023.\nPetroni, F., Lewis, P., Piktus, A., Rockt\u00a8aschel, T., Wu,\nY., Miller, A. H., and Riedel, S. How context affects\nlanguage models\u2019 factual predictions.\narXiv preprint\narXiv:2005.04611, 2020.\nPress, O., Smith, N. A., and Lewis, M. Train short, test\nlong: Attention with linear biases enables input length\nextrapolation. arXiv preprint arXiv:2108.12409, 2021.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei,\nD., and Sutskever, I.\nLanguage models are unsu-\npervised multitask learners, 2019.\nURL https:\n//api.semanticscholar.org/CorpusID:\n160025533.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nRajpurkar, P., Jia, R., and Liang, P. Know what you don\u2019t\nknow: Unanswerable questions for squad. arXiv preprint\narXiv:1806.03822, 2018.\nRuoss, A., Del\u00b4etang, G., Genewein, T., Grau-Moya, J.,\nCsord\u00b4as, R., Bennani, M., Legg, S., and Veness, J. Ran-\ndomized positional encodings boost length generalization\nof transformers. arXiv preprint arXiv:2305.16843, 2023.\nSanford, C., Hsu, D., and Telgarsky, M. Representational\nstrengths and limitations of transformers. arXiv preprint\narXiv:2306.02896, 2023.\nShelton, T. The Ingenious Gentleman Don Quixote of La\nMancha. 1612. Written by Miguel de Cervantes, trans-\nlated by Thomas Shelton.\nShen, R., Bubeck, S., Eldan, R., Lee, Y. T., Li, Y., and\nZhang, Y. Positional description matters for transformers\narithmetic. arXiv preprint arXiv:2311.14737, 2023.\nStrobl, L., Merrill, W., Weiss, G., Chiang, D., and Angluin,\nD. Transformers as recognizers of formal languages: A\nsurvey on expressivity. arXiv preprint arXiv:2311.00208,\n2023.\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, pp. 127063, 2023.\nSun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,\nWang, J., and Wei, F. Retentive network: A successor to\ntransformer for large language models. arXiv preprint\narXiv:2307.08621, 2023.\nTikochinski, R., Goldstein, A., Meiri, Y., Hasson, U., and\nReichart, R. An incremental large language model for\nlong text processing in the brain. 2024.\n10\nRepeat After Me: Transformers are Better than State Space Models at Copying\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWei, C., Chen, Y., and Ma, T. Statistically meaningful\napproximation: a case study on approximating turing\nmachines with transformers. Advances in Neural Infor-\nmation Processing Systems, 35:12071\u201312083, 2022.\nWeiss, G., Goldberg, Y., and Yahav, E. Thinking like trans-\nformers. In International Conference on Machine Learn-\ning, pp. 11080\u201311090. PMLR, 2021.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\net al. Huggingface\u2019s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771,\n2019.\nZhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O.,\nSusskind, J., Bengio, S., and Nakkiran, P. What algo-\nrithms can transformers learn? a study in length general-\nization. arXiv preprint arXiv:2310.16028, 2023.\n11\nRepeat After Me: Transformers are Better than State Space Models at Copying\nA. Experimental setup\nIn this section, we provide additional details about our experimental setup. We first give a description of the positional\nencodings used in our transformers experiments (Subsection A.1) and then give details about the training and evaluation\nprocedures (Subsection A.2).\nA.1. Positional encodings in the transformers\n(a)\n(b)\n(c)\nFigure 8. Positional encoding schemes for transformers: illustration of the different positional encodings of the transformers that are\ntrained in our experiments. (a) corresponds to the NoPE encoding (Kazemnejad et al., 2023) where no positional encoding is applied to\nany of the attention heads (b) depicts the ALiBi encoding (Press et al., 2021) where m is a head-specific scalar and (c) the Hard-ALiBi\nencoding introduced in Section 2. For the sake of illustration, we consider the case where we mask three heads which means that we force\nHeads 1, 2 and 3 to attend to their current token, their current and preceding tokens and their current, preceding and prior to the preceding\ntokens. The remaining heads are set as NoPE heads.\nWe consider multiple positional encoding schemes in our experiments in Section 3:\n\u2013 the NoPE scheme (Kazemnejad et al., 2023) where no positional information is added to any of the attention scores\n(Figure 8a). This architecture choice helps to get better length generalization in multiple tasks including the copy task.\n\u2013 the ALiBi scheme (Press et al., 2021) which biases the attention scores with a penalty that is proportional to their\ndistance (Figure 8b). m is a head-specific slope fixed before training.\n\u2013 the Hard-ALiBi scheme introduced in Section 2 which has M masked attention heads where we explicitly force the\nmodel to attend to their directly previous tokens and H \u2212 M heads set to be NoPE attention heads. In Figure 8c, we\ndisplay the case where we have M = 4 masked heads: in the first head, the tokens just attend to themselves; in the\nsecond head, the tokens attend to themselves and to previous ones; in the third head, the tokens attend to themselves,\nthe previous ones and the second preceding tokens. The remaining H \u2212 M heads are set to NoPE.\n12\nRepeat After Me: Transformers are Better than State Space Models at Copying\nA.2. Pretraining and evaluation details\nSoftware dependencies.\nWe implement all of our training in Pytorch (Paszke et al., 2019). We use the HuggingFace\nlibrary (Wolf et al., 2019) and the Mamba GitHub repository (Gu & Dao, 2023).\nArchitectures.\nIn our experiments in Section 3, the backbone of our transformers is the GPT-NeoX architecture. We set\nthe number of layers to 12, the hidden size to 1024 and the number of heads H = 16. We consider the different positional\nencodings that are described in Subsection A.1. For Alibi, we set the head-specific scalar as in the original paper i.e.\nmh = 2\u2212h/2 for h \u2208 {1, . . . , H}. For the Hard-Alibi model, we sweep over the number of masked heads M \u2208 {2, . . . , 10}\nand found that the best model corresponds to M = 6. Regarding the Mamba models, we set the number of layers to 24 and\nthe hidden size 1024. We also sweep over the state space dimension S \u2208 {16, 32, 64, 128, 256} and found the best model is\nS = 32. This choice of hyperparameters ensures that both transformers and Mamba models have a comparable number of\nparameters. Lastly, our LSTM is made of 4 layers and width 1024.\nTraining hyperparameters.\nIn Section 3, at each epoch, we sample online a batch size of size 64. We fill the context\nwith examples so we choose a context length (C = 420 for all the experiments except Figure 1a where we set C = 620)\nand pack as many examples as possible to fit this context. So in our case, one sample contains many instances. We run the\nexperiments for 15 epochs for both transformers and Mamba while for LSTMs we need 300 epochs. All methods are trained\nwith the AdamW optimizer (Loshchilov & Hutter, 2017) with learning rate 5e-5, a linear rate decay schedule, 300 steps of\nwarmup and default weight decay of 1e-1. Finally, to train all the models, we use the next-token prediction loss but we apply\na mask on the input instance so that we only penalize the model whenever it makes a mistake on the labels (and not on the\ninputs and labels jointly).\nCompute resources.\nPretraining was all done on an internal cluster using RTX8000 GPUs. We estimate that the final\ntraining run needed to produce the results in the paper took approximately 600 GPU hours.\nEvaluation algorithm.\nWe evaluate the models over 10 batches of size 64 for all the tasks except for the question\nanswering one where we evaluate over 50 questions because the number of questions with a given context length is limited.\nDecoding algorithm.\nAt inference, all our models use greedy decoding for generation and we set the temperature to 0.\nB. Additional Experiments\nIn Subsection B.1, we focus on the in-distribution learning of the copy task and show that the number of samples needed by\nGSSMs is much higher than the one for transformers. In Subsection B.2, we study the performance of pre-trained models on\nthe copy task in the case where the strings are sampled uniformly. This experiment shows that when the text to copy is\ntotally random, the gap between pre-trained transformers and GSSMs is even larger.\nB.1. Data efficiency on the copy task\nIn this section, we provide additional plots to complement the data efficiency experiment from Figure 1a. We want to\nhighlight the following points:\n\u2013 in Figure 1a, we see a sharp transition for the Mamba learning curve. However, Figure 9a shows that the learning\nprocess is more smooth at the character level. Besides, LSTMs are not able to learn the copy on length-300 strings\neven at the character level.\n\u2013 We consider the experiment of learning to copy much shorter strings namely strings with length \u2264 30. Figure 9b\nshows that the gap in terms of training examples between transformers and Mamba is much smaller i.e. Mamba only\nneeds 10x more data. Besides, we see that the LSTM is able to learn the copy task but it needs 100x more data than\ntransformers.\nB.2. Pre-trained models on the uniform copy task\nIn this section, we provide an additional experiment that shows the superiority of pre-trained Pythia over pre-trained Mamba\nmodels in the copy task.\n13\nRepeat After Me: Transformers are Better than State Space Models at Copying\n104\n105\n106\nNumber of training examples\n0\n25\n50\n75\n100\nAccuracy (%)\n(a)\n104\n105\n106\nNumber of training examples\n0\n25\n50\n75\n100\nAccuracy (%)\n(b)\n104\n105\n106\nNumber of training examples\n0\n25\n50\n75\n100\nAccuracy (%)\n(c)\nTransformer:\nRoPE\nNoPE\nAlibi\nHAlibi\nGSSM:\nLSTM\nMamba\nFigure 9. (a) Copying long strings: character-level accuracy. Here we train models to copy strings of length \u2264 300 and evaluate\ncharacter-level accuracy on strings of length 300. Transformers train much faster than GSSMs. Mamba has a more progressive learning\ncurve than in Figure 1a. An LSTM cannot even learn the task within this number of samples at the character level. (b) Copying short\nstrings: string-level accuracy. Here we train models to copy strings of length \u2264 30 and evaluate character-level accuracy on strings of\nlength 30. Transformers train much faster than GSSMs. Compared to Figure 1a, we see that Mamba needs way less samples in order\nto learn to copy length-30 strings. An LSTM can learn to copy but requires 100x more training examples. (c) Copying short strings:\ncharacter-level accuracy. Here we train models to copy strings of length \u2264 30 and evaluate character-level accuracy on strings of length\n30 and report the character-level accuracy.\n100\n200\n300\n400\n550\nNumber of characters in string\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nPythia:\n410M\n1.4B\n2.8B\nMamba:\n360M\n1.4B\n2.8B\nFigure 10. Copy: uniform strings. To test whether it mattered that the\nstrings were in natural language, we generate uniformly sampled strings (the\ngeneration process is described in Section 3). We find that this degrades the\nMamba models while Pythia models are able to keep a high performance.\nWe consider the same setup as in Section 3: we\nsample uniform strings of alphabet characters with\na fixed length and ask the model to copy it by using\nthe same prompt format as the one described in\nSubsection 4.2.\nThis setting is a more extreme version of Figure 7b\nsince the strings are more random: in Figure 7b,\nthe order of the nouns were random but the nouns\nwere English nouns while in Figure 7b, the strings\nare totally random. In Figure 10, we see a clear\nseparation between the transformers and Mamba\nmodels with the smallest Pythia outperforming the\nlargest Mamba. However, compared to Figure 7b,\nthe Pythia performance is much higher since the\n1.4B model able to get almost 100% accuracy.\n14\nRepeat After Me: Transformers are Better than State Space Models at Copying\nAlgorithm 1 Hash-based copying\nInput: sequence x1, . . . , xL\nLet s : D\u2217 \u2192 Rd be some hashing function.\nfor i = n + 2, . . . , L do\nki \u2190 s(xi\u2212n, xi\u2212n+1, . . . , xi\u22121)\nvi \u2190 xi\nend for\nfor j = 1, . . . , L do\nif j \u2264 n then\nyj \u2190 xj\nelse\nqj \u2190 s(yj\u2212n, . . . , yj\u22121)\nLet i \u2208 [L] s.t. ki = qj, and set yj \u2190 xi\nend if\nend for\nOutput: sequence y1, . . . , yL\nC. Proofs - Upper Bound\nThis section gives a detailed proof of Theorem 2.3 and Lemma 2.4.\nC.1. Technical Lemmas\nWe begin by introducing some technical lemmas that we use in the proof of Theorem 2.3.\nLemma C.1. Let ht(x1, . . . , xi) =\n1\nmin(t,i)\nPi\nj=max(1,i\u2212t+1) xj. Then, ht can be computed using a hard-ALiBi attention\nhead.\nProof. Let Wk, Wq = 0 (zero matrix) and let Wv = Id (indentity matrix). We choose bi \u2208 {0, \u2212\u221e}i s.t.\nbi,j =\n(\n\u2212\u221e\nj \u2264 i \u2212 t\n0\nj > i \u2212 t\nLemma C.2. Assume that d = \u2308log(D)\u2309 + 2. Then, there exists an embedding \u03a8 s.t.\n\u2022 For every x \u2208 D it holds that \u2225\u03a8(x)\u22252 = 1 and \u2225\u03a8(x)\u2225\u221e \u2264 1.\n\u2022 For x\u2032 \u0338= x it holds that \u27e8x, x\u2032\u27e9 < 1 \u2212 1\nd.\n\u2022 For every x \u0338= \u27e8BOS\u27e9, \u27e8\u03a8(x), \u03a8(\u27e8BOS\u27e9)\u27e9 = 0, and for every x \u0338= \u27e8COPY\u27e9, \u27e8\u03a8(x), \u03a8(\u27e8COPY\u27e9)\u27e9 = 0.\nProof. Denote d\u2032 = \u2308log(D)\u2309, and observe that we can encode all D \u201cnon-special\u201d tokens as vectors in\nn\n\u00b1 1\n\u221a\nd\nod\u2032\n, and\ndenote this encoding by \u03a8\u2032. Now, define:\n\u03a8(x) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n[1, 0, . . . , 0]\nx = \u27e8BOS\u27e9\n[0, 1, 0, . . . , 0]\nx = \u27e8COPY\u27e9\n[0, 0, \u03a8\u2032(x)]\no.w.\nLemma C.3. Let z \u2208 RK be some vector such that, for some constants a > b > 0, there exists i \u2208 [K] s.t. zi = a and for\nall j \u0338= i we have |zj| \u2264 b. Denote s = softmax(z). Then si \u2265\n1\n1+K exp(b\u2212a) and sj \u2264 exp(b \u2212 a) for all j \u0338= i.\n15\nRepeat After Me: Transformers are Better than State Space Models at Copying\nProof. First, notice that:\nexp(a) = exp(zi) \u2264\nK\nX\nj=1\nexp(zj) \u2264 exp(zi) + (K \u2212 1) exp(b) \u2264 exp(a) + K exp(b) = exp(a)(1 + K exp(b \u2212 a))\nObserve the following:\nsi =\nexp(zi)\nPK\nj=1 exp(zj)\n\u2265\nexp(a)\nexp(a)(1 + K exp(b \u2212 a)) =\n1\n1 + K exp(b \u2212 a)\nFinally, for every j \u0338= i:\nsj =\nexp(zj)\nPK\nj=1 exp(zj)\n\u2264 exp(b)\nexp(a) = exp(b \u2212 a)\nC.2. Proof of Theorem 2.3\nWe begin by constructing the first block of the transformer, which computes the \u201clookup-table\u201d for the copy algorithm.\nThis lookup-table consists of pairs of (key,values) for each position i, where the key encodes the n-gram preceding the i-th\ntoken, and the value is the i-th token. Namely, if the sequence is x1, . . . , xi, then keyi = (xi\u2212n\u22121, . . . , xi) and valuei = xi.\nAdditionally, the transformer block also computes a query, which is just the \u201ccurrent\u201d n-gram, i.e. queryi = (xi\u2212n, . . . , xi).\nThe copy algorithm matches the current query with previous key-s, retrieving the matching value.\nThe following theorem shows that by using a combination of n hard-ALiBi attention heads (with different choice of m for\neach head), together with an MLP layer, can compute the correct (keyi, valuei, queryi) for each position. We use a slightly\nmodified keyi, queryi to handle cases where the i \u2264 n (or, i is one of the first n tokens after the \u27e8COPY\u27e9 token).\nLemma C.4. Let \u03a8 be the one-hot embedding. Then, there exists a hard-ALiBi transformer block with 3 outputs, denoted\nT key, T query, T value, which correspond to 3 blocks of the output dimension, s.t. T key : Rd\u00d7\u2217 \u2192 R(d+1)n\u00d7\u2217, T query :\nRd\u00d7\u2217 \u2192 R(d+1)n\u00d7\u2217 and T value : Rd\u00d7\u2217 \u2192 Rd\u00d7\u2217 satisfying, for all x sampled from a length-n copy distribution,\n1. Value output: for all i,\nT value\ni\n(\u03a8(x1), . . . , \u03a8(xi)) = \u03a8(xi)\n2. Key output:\n\u2022 For t = 1, . . . , n, if i > n\nT key\n(t\u22121)d+1:td,i(\u03a8(x1), . . . , \u03a8(xi)) = \u03a8(xi\u2212t)\nand if i \u2264 n\nT key\n(t\u22121)d+1:td,i(\u03a8(x1), . . . , \u03a8(xi)) = 0\n\u2022 Additionally, for t = 1, . . . , n, for all i\nT key\nnd+t,i(\u03a8(x1), . . . , \u03a8(xi)) = 1{i = t + 1}\n3. Query output:\n\u2022 For t = 1, . . . , n, if i \u2265 n\nT query\n(t\u22121)d+1:td,i(\u03a8(x1), . . . , \u03a8(xi)) = \u03a8(xi\u2212t+1)\nand if i < n\nT query\n(t\u22121)d+1:td,i(\u03a8(x1), . . . , \u03a8(xi)) = 0\n\u2022 Additionally, for t = 1, . . . , n, for all i\nT key\nnd+t,i(\u03a8(x1), . . . , \u03a8(xi)) = n \u00b7 1{i = L + t}\n16\nRepeat After Me: Transformers are Better than State Space Models at Copying\nProof. We prove the following:\n1. For the value output, we simply take T value = h1 as defined in Lemma C.1.\n2. For each t = 0, . . . , n, define:\ngt(x1, . . . , xi) = (t + 1) \u00b7 ht+1(x1, . . . , xi) \u2212 t \u00b7 ht(x1, . . . , xi)\nwhere we define h0 \u2261 0. Observe that if i > t then:\ngt(x1, . . . , xi) = (t + 1) \u00b7\n1\nt + 1\ni\nX\nj=i\u2212t\nxj \u2212 t \u00b7 1\nt\ni\nX\nj=i\u2212t+1\nxj = xi\u2212t\nand if i \u2264 t then:\ngt(x1, . . . , xi) = t \u00b7 1\ni\ni\nX\nj=1\nxj \u2212 (t \u2212 1) \u00b7 1\ni\ni\nX\nj=1\nxj = 1\ni\ni\nX\nj=1\nxj\nFor every j \u2208 [d], denote\n\u02c6gt,j(x1, . . . , xi) = \u03c3(ej \u00b7 gt(x1, . . . , xi) \u2212 n\u03a8(\u27e8BOS\u27e9) \u00b7 gn(x1, . . . , xi))\n\u2212 \u03c3(\u2212ej \u00b7 gt(x1, . . . , xi) \u2212 n\u03a8(\u27e8BOS\u27e9) \u00b7 gn(x1, . . . , xi))\nClaim: \u02c6gt(\u03a8(x1), . . . , \u03a8(xi)) = 1{i > n} \u00b7 \u03a8(xi\u2212t)\nProof: Fix some j \u2208 [d]. Observe that for all i, |ej \u00b7 gt(\u03a8(x1), . . . , \u03a8(xi))| \u2264 1.\n\u2022 If i \u2264 n, we have gn(\u03a8(x1), . . . , \u03a8(xi)) = 1\ni\nPi\nj\u2032=1 \u03a8(xj\u2032) and so \u03a8(\u27e8BOS\u27e9) \u00b7 gn(\u03a8(x1), . . . , \u03a8(xi)) = 1 where\nwe use the properties of \u03a8 and the fact that x1 = \u27e8BOS\u27e9. Therefore, \u02c6gt,j(\u03a8(x1), . . . , \u03a8(xi)) = 0.\n\u2022 If i > n \u2265 t, then:\n\u02c6gt,j(\u03a8(x1), . . . , \u03a8(xi)) = \u03c3 (ej \u00b7 \u03a8(xi\u2212t) \u2212 n\u03a8(\u27e8BOS\u27e9) \u00b7 \u03a8(xi\u2212t))\n\u2212 \u03c3 (\u2212ej \u00b7 \u03a8(xi\u2212t) \u2212 n\u03a8(\u27e8BOS\u27e9) \u00b7 \u03a8(xi\u2212t))\n= \u03c3(ej \u00b7 \u03a8(xi\u2212t)) \u2212 \u03c3(\u2212ej \u00b7 \u03a8(xi\u2212t)) = ej \u00b7 \u03a8(xi\u2212t)\nwhere we use the fact that xi\u2212t \u0338= \u27e8BOS\u27e9 and therefore \u03a8(\u27e8BOS\u27e9) \u00b7 \u03a8(xi\u2212t) = 0.\nDenote\n\u02dcgt(x1, . . . , xi) = 1\n2\u03c3 (2\u03a8(\u27e8BOS\u27e9) \u00b7 (gt(x1, . . . , xi) \u2212 h1(x1, . . . , xi)) \u2212 1)\nClaim: \u02dcgt(\u03a8(x1), . . . , \u03a8(xi)) = 1{i = t + 1}\nProof: Denote gt,i = gt(\u03a8(x1), . . . , \u03a8(xi)) and h1,i = h1(\u03a8(x1), . . . , \u03a8(xi)). Observe:\n\u2022 If i = t + 1, then gt,i = \u03a8(x1) = \u03a8(\u27e8BOS\u27e9) and h1,i = \u03a8(xi) \u22a5 \u03a8(\u27e8BOS\u27e9) and therefore \u02dcgt,i = 1.\n\u2022 If i > t + 1 then gt,i = \u03a8(xi\u2212t) \u22a5 \u03a8(\u27e8BOS\u27e9) and h1,i = \u03a8(xi) \u22a5 \u03a8(\u27e8BOS\u27e9) and so \u02dcgt,i = 0.\n\u2022 If 1 < i \u2264 t then \u03a8(\u27e8BOS\u27e9) \u00b7 gt,i = 1\ni \u2264 1\n2 and h1,i = \u03a8(xi) \u22a5 \u03a8(\u27e8BOS\u27e9) and so \u02dcgt,i = 0.\n\u2022 If i = 1 then gt,i = h1,i = \u03a8(\u27e8BOS\u27e9) and therefore \u02dcgt,i = 0.\nFinally, we can take T key = [\u02c6g1, . . . , \u02c6gq, \u02dcg1, . . . , \u02dcgq].\n3. For all t = 1, . . . , n, define g\u2217\nt (x1, . . . , xi) = \u03c3(\u03a8(\u27e8COPY\u27e9) \u00b7 gt\u22121(x1, . . . , xi)).\nClaim: g\u2217\nt (\u03a8(x1), . . . , \u03a8(xi)) = 1{i = L + t}\nProof: Denote gt,i = gt(\u03a8(x1), . . . , \u03a8(xi)). Observe:\n\u2022 If i = L + t then gt\u22121,i = \u03a8(xi\u2212t+1) = \u03a8(xL+1) = \u03a8(\u27e8COPY\u27e9) and therefore g\u2217\nt,i = 1.\n\u2022 If i \u0338= L + t and i > t \u2212 1 then gt\u22121,i = \u03a8(xi\u2212t+1) \u22a5 \u03a8(\u27e8COPY\u27e9) and therefore g\u2217\nt,i = 0.\n\u2022 If i \u2264 t then since x1, . . . , xi \u0338= \u27e8COPY\u27e9 we get \u03a8(\u27e8COPY\u27e9) \u00b7 gt\u22121,i = 0 and therefore g\u2217\nt,i = 0.\n17\nRepeat After Me: Transformers are Better than State Space Models at Copying\nTherefore, we can take T query = [\u02c6g0, . . . , \u02c6gq\u22121, n \u00b7 g\u2217\n1, . . . , n \u00b7 g\u2217\nq].\nNow, we prove Theorem 2.3 by showing that using a single attention head with no positional embedding on top of the con-\nstruction in Lemma C.4 realizes the copy algorithm. Since the first block computes the correct choice of keyi, queryi, valuei,\nby correctly scaling of the attention matrix we verify that the output of the second layer at position i corresponds to \u2248 valuej\nfor j s.t. keyj = queryi.\nProof of Theorem 2.3. Let T value, T key, T query be the outputs of the Transformer block guaranteed by Lemma C.4. Observe\nthat, for some temprature \u03c4 \u2208 R, the following function can be computed by a softmax-attention layer on-top of this block:\nH(\u03a8(x1), . . . , \u03a8(xi)) = T value \u00b7 softmax(\u03c4 \u00b7 T key \u00b7 T query\ni\n)\nwhere e.g. T value denotes T value(\u03a8(x1), . . . , \u03a8(xi)).\nFor now, assume that all the n-grams in x are unique, and that the length of the input satisfies 2L + 2 \u2264 K for K = Dn.\nClaim: Fix some i > L, denote z = T key \u00b7 T query\ni\n. Then, zi\u2212L+1 = n and |zj| < n \u2212 1\nd for all j \u0338= i \u2212 L + 1.\nProof: We separate to the following cases:\n\u2022 If i > L + n \u2212 1, then for every j we have\nT key\nj\n\u00b7 T query\ni\n= 1{j > n} \u00b7 [\u03a8(xj\u22121), . . . , \u03a8(xj\u2212n)]\u22a4[\u03a8(xi), . . . , \u03a8(xi\u2212n+1)]\n= 1{j > n} \u00b7\nn\nX\nt=1\n\u03a8(xj\u2212t)\u03a8(xi\u2212t+1)\nNow, if j = i \u2212 L + 1 then xj\u2212t = xi\u2212L+1\u2212t = xi\u2212t+1 and since j > n we get\nT key\nj\n\u00b7 T query\ni\n=\nn\nX\nt=1\n\u2225\u03a8(xi\u2212t+1)\u2225 = n\nIf j \u0338= i \u2212 L + 1, since there are no repeated n-grams, there is at least some t \u2208 [n] s.t. \u03a8(xj\u2212t) \u0338= \u03a8(xi\u2212t+1) and by\nthe choice of the embedding \u03a8(xj\u2212t) \u00b7 \u03a8(xi\u2212t+1) \u2264 1 \u2212 1\nd. In this case, we get\n\f\f\fT key\nj\n\u00b7 T query\ni\n\f\f\f \u2264 n \u2212 1\nd.\n\u2022 If L < i \u2264 L + n \u2212 1 and j \u2264 n then\nT key\nj\n\u00b7 T query\ni\n= nej\u22121 \u00b7 ei\u2212L = n \u00b7 1{j = i \u2212 L + 1}\nwhich satisfies the required.\n\u2022 If L < i \u2264 L + n \u2212 1 and j > n then\nT key\nj\n\u00b7 T query\ni\n=\nn\nX\nt=1\n\u03a8(xj\u2212t)\u03a8(xi\u2212t+1)\nand as before, since there are no repeated n-grams, we get\n\f\f\fT key\nj\n\u00b7 T query\ni\n\f\f\f \u2264 n \u2212 1\nd\nClaim: Fix some \u03f5 \u2208 (0, 1) and some i > L, denote s = softmax(\u03c4T key \u00b7 T query\ni\n) = softmax(\u03c4 \u00b7 z). If \u03c4 = d ln( 2K\n\u03f5 ),\nthen si\u2212L+1 \u2265 1 \u2212 \u03f5 and sj \u2264\n\u03f5\n2K for all j \u0338= i \u2212 L + 1.\nProof: Using the previous claim, togehter with Lemma C.3, we get that:\n\u2022 si\u2212L+1 \u2265\n1\n1+i exp(\u2212\u03c4/d) \u2265\n1\n1+K exp(\u2212\u03c4/d) \u2265\n1\n1+\u03f5/2 = 1 \u2212\n\u03f5/2\n1+\u03f5/2 \u2265 1 \u2212 \u03f5\n18\nRepeat After Me: Transformers are Better than State Space Models at Copying\n\u2022 For j \u0338= i \u2212 L + 1,\nsj \u2264 exp(\u2212\u03c4/d) \u2264\n\u03f5\n2K\nClaim: Fix some \u03f5 \u2208 (0, 1) and some i > L. Then, for \u03c4 \u2265 d ln( 2K\n\u03f5 ), it holds that:\n\u2225H(\u03a8(x1), . . . , \u03a8(xi)) \u2212 \u03a8(xi\u2212L+1)\u2225 \u2264 \u03f5\nProof: Let s as defined in the previous claim. Then:\n\u2225H(\u03a8(x1), . . . , \u03a8(xi)) \u2212 \u03a8(xi\u2212L+1)\u2225 =\n\r\r\r\r\r\r\ni\nX\nj=1\nsj\u03a8(xj) \u2212 \u03a8(xi\u2212L+1)\n\r\r\r\r\r\r\n\u2264 (1 \u2212 si\u2212L+1) \u2225\u03a8(xi\u2212L+1)\u2225 +\nX\nj\u0338=i\u2212L+1\nsj \u2225\u03a8(xj)\u2225\n= (1 \u2212 si\u2212L+1) +\nX\nj\u0338=i\u2212L+1\nsj \u2264 \u03f5 + (i \u2212 1) \u03f5\n2K \u2264 2\u03f5\nNow, denote by \u03a6 : Rd \u2192 D the output map given by \u03a6(z) = arg maxx\u2208D z \u00b7\u03a8(x) (which can be computed by an arg max\nover a linear function).\nClaim: If \u03c4 \u2265 d ln(8Kd), then for all i > L we have \u03a6(H(\u03a8(x1), . . . , \u03a8(xi))) = xi\u2212L+1.\nProof: Denote yi = H(\u03a8(x1), . . . , \u03a8(xi)). First, using the previous claim, we observe that\nyi \u00b7 \u03a8(xi\u2212L+1) = (yi \u2212 \u03a8(xi\u2212L+1)) \u00b7 \u03a8(xi\u2212L+1) + \u2225\u03a8(xi\u2212L+1)\u2225\n\u2265 1 \u2212 \u2225yi \u2212 \u03a8(xi\u2212L+1)\u2225 \u2265 1 \u2212 1\n4d\nNext, observe that for all j \u0338= i \u2212 L + 1 we have\nyi \u00b7 \u03a8(xj) = (yi \u2212 \u03a8(xi\u2212L+1)) \u00b7 \u03a8(xj) + \u03a8(xj) \u00b7 \u03a8(xi\u2212L+1)\n\u2264 \u2225yi \u2212 \u03a8(xi\u2212L+1)\u2225 + 1 \u2212 1\nd \u2264 1 \u2212 3\n4d < yi \u00b7 \u03a8(xi\u2212L+1)\nFrom the above claim, the Transformer construction outputs the correct token at each step of the auto-regressive generation.\nC.3. Proof of Lemma 2.4\nProof of Lemma 2.4. Fix some i < j \u2208 [L]. Let I := {i, . . . , i+n} and J := {j, . . . , j +n}. We first bound the probability\nof drawing some x s.t. xI = xJ. Note that there are D|I\u222aJ| choices for xI\u222aJ. We count the number of choices for xI\u222aJ s.t.\nxI = xJ. Notice that in this case, xI\u222aJ is determined by xI\\J, therefore there are D|I\\J| possible choices. We conclude\nthat\nPr [xI = xJ] = D|I\\J|\nD|I\u222aJ| = D|I\\J|\u2212|I\u222aJ| = D\u2212n\nUsing the union bound, we get that\nPr [\u2203i < j s.t. xi,...,i+n = xj,...,j+n] \u2264\nX\ni<j\nPr [xi,...,i+n = xj,...,j+n] < L2D\u2212n\n19\nRepeat After Me: Transformers are Better than State Space Models at Copying\nD. Proofs - Lower Bound\nIn this section, we prove Theorem 2.7. We begin by showing that, for every input, the output of the model in each iteration\nis a deterministic function of the state of the model after observing the input:\nLemma D.1. Let Hu,r : Dn\u2032 \u2192 Dn be some fixed-state sequence-to-sequence model. Then, there exists map G : S \u2192 Dn\ns.t. for all x \u2208 Dn\u2032\nHu,r(x) = G \u25e6 Sn\u2032(x)\nProof. Let xn\u2032+1, . . . , xn\u2032+n be the outputs of Hu,r.\nWe need to show that there exist functions G1, . . . , Gn s.t.\nHu,r(x1, . . . , xn\u2032) = G(Sn\u2032(x1, . . . , xn)). We give the following recurive definition:\n\u2022 G1(s) = r(s), \u02dcG1(s) = u(s, G1(s)).\n\u2022 Gi(s) = r( \u02dcGi\u22121(s)), \u02dcGi(s) = u( \u02dcGi\u22121(s), Gi(s)).\nDenote s = Sn\u2032(x1, . . . , xn\u2032) We prove by induction that Gi(s) = xn\u2032+i and also that \u02dcGi(s) = Sn\u2032+i(x1, . . . , xn\u2032+i).\n\u2022 G1(s) = r(s) = Rn\u2032(x1, . . . , xn\u2032) = xn\u2032+1.\n\u2022 \u02dcG1(s) = u(s, G1(s)) = u(s, xn\u2032+1) = Sn\u2032+1(x1, . . . , xn\u2032+1)\n\u2022 Gi(s) = r( \u02dcGi\u22121(s)) = r(Sn\u2032+i\u22121(x1, . . . , xn\u2032+i\u22121)) = Rn\u2032+i\u22121(x1, . . . , xn\u2032+i\u22121) = xn\u2032+i\n\u2022 \u02dcGi(s) = u( \u02dcGi\u22121(s, Gi(s))) = u(Sn\u2032+i\u22121(x1, . . . , xn\u2032+i\u22121), xn\u2032+i) = Sn\u2032+i(x1, . . . , xn\u2032+i)\nand so the required follows.\nGiven the previous Lemma, we bound the error of the model by comparing the number of possible states to the number of\npossible inputs.\nProof of Theorem 2.7. From Lemma D.1, there exists some function G : S \u2192 Dn s.t. Hu,r = G \u25e6 Sn\u2032. For each x, we\ndenote by \u02dcx the sequence \u27e8BOS\u27e9 , x, \u27e8COPY\u27e9. Now, observe the following:\n1 \u2212 errDn(Hu,r) = Pr\nDn [Hu,r(\u02dcx) = x]\n=\n1\nDn\nX\nx\u2208Dn\n1{Hu,r(\u02dcx) = x}\n=\n1\nDn\nX\ns\u2208S\nX\nx\u2208S\u22121\nn+2(\u02dcx)\n1{G \u25e6 Sn\u2032+2(\u02dcx) = x}\n=\n1\nDn\nX\ns\u2208S\nX\nx\u2208S\u22121\nn+2(\u02dcx)\n1{G(s) = x} \u2264 |S|\nDn\n20\n"
  },
  {
    "title": "K-Level Reasoning with Large Language Models",
    "link": "https://arxiv.org/pdf/2402.01521.pdf",
    "upvote": "16",
    "text": "K-Level Reasoning with Large Language Models\nYadong Zhang 1 * \u2020 Shaoguang Mao 2 * Tao Ge 2 Xun Wang 2 Yan Xia 2 Man Lan 1 Furu Wei 2\nAbstract\nWhile Large Language Models (LLMs) have\ndemonstrated their proficiency in complex rea-\nsoning tasks, their performance in dynamic, in-\nteractive, and competitive scenarios - such as\nbusiness strategy and stock market analysis - re-\nmains underexplored. To bridge this gap, we for-\nmally explore the dynamic reasoning capabilities\nof LLMs for decision-making in rapidly evolving\nenvironments. We introduce two game theory-\nbased pilot challenges that mirror the complexities\nof real-world dynamic decision-making. These\nchallenges are well-defined, enabling clear, con-\ntrollable, and precise evaluation of LLMs\u2019 dy-\nnamic reasoning abilities. Through extensive ex-\nperiments, we find that existing reasoning meth-\nods tend to falter in dynamic settings that require\nk-level thinking - a key concept not tackled by pre-\nvious works. To address this, we propose a novel\nreasoning approach for LLMs, named \u201cK-Level\nReasoning\u201d. This approach adopts the perspective\nof rivals to recursively employ k-level thinking\nbased on available historical information, which\nsignificantly improves the prediction accuracy of\nrivals\u2019 subsequent moves and informs more strate-\ngic decision-making. This research not only sets a\nrobust quantitative benchmark for the assessment\nof dynamic reasoning but also markedly enhances\nthe proficiency of LLMs in dynamic contexts.\n1. Introduction\nThe advent of Large Language Models (LLMs) marks a\ntransformative era in artificial intelligence, revolutionizing\nhow we approach complex reasoning tasks (Yao et al., 2022;\nDiao et al., 2023; Wei et al., 2022) ranging from solving\nintricate mathematical problems (Miao et al., 2021) to an-\nswering common-sense queries (Talmor et al., 2022). Yet,\n*Equal contribution, \u2020Work was done when interning at Mi-\ncrosoft Research Asia.\n1East China Normal University\n2Microsoft Research Asia.\nCorrespondence to:\nShaoguang\nMao\n<shaoguang.mao@microsoft.com>,\nTao\nGe\n<sgge-\ntao@gmail.com>. Repository: K-Level Reasoning.\nPreprint. Under review.\nTwo            from Residential Area to Central Business District. Route A \nis 5 km, and Route B is 10 km.\nSetting\nScenario 1 - Single player, Static reasoning\nI choose Route A. It is and takes \nless time, allowing me to arrive \nat the company quickly and \nconserve fuel ...\nScenario 2 - Multiple players, Dynamic reasoning\nI choose A. It is shorter ...\nI choose A. It is closer than ...\nI choose A. It saves time ...\nRoute A becomes overcrowded. \nMany residents who choose Route A \nare late for work\u2026\nResidents consider the impact of others \nand real-time traffic when choosing \nroutes.\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\nRoute A might be shorter, but it \ncould become more crowded if \nothers choose it as well. Route B, \nthough longer, might have less \ntraffic and save me time.\nFigure 1. Dynamic reasoning is like navigating a bustling city. The\nmovements of each individual continuously shape and reshape the\nenvironment.\nthere has been a lack of exploration into dynamic reasoning\n\u2013 a scenario that poses a greater challenge to LLM reasoning\ncapabilities, characterized by interactive and competitive\nenvironments, where adaptability and real-time decision-\nmaking are crucial (Park et al., 2023).\nTake navigating a bustling city as an example (Figure 1),\nwhere every move of each individual shapes and reshapes\nthe environment continuously: When there is only one res-\nident in the town, all reasoning conditions are determined,\nso the reasoning is static. Making decisions only requires\nconsidering the given conditions. However, when there\nare multiple residents in the town, the choices of the resi-\ndents interact, and may even in turn affect their own choices.\nThe environments are dynamic and ever-changing, corre-\nspondingly demanding participants adapt their strategies\nin real-time. Such dynamic reasoning scenarios are com-\nmon in the real world such as business strategy and stock\nmarket analysis where decision-making is both reactionary\nand anticipatory. Despite increasing employment of LLMs\nin simulating human interactions within cooperative and\ncompetitive contexts (Aher et al., 2023; Xu et al., 2023;\nHan et al., 2023; Chen et al., 2023; Lan et al., 2023), there\nis still an absence of formal studies in dynamic reasoning\ncapabilities of LLMs.\nTo address this gap, we formally study dynamic reasoning\ncapabilities of LLMs with a game theory perspective to as-\nsess strategic interactions of LLMs in competitive dynamic\n1\narXiv:2402.01521v1  [cs.CL]  2 Feb 2024\nK-Level Reasoning with Large Language Models\nPlease choose an integer between 1 and 100. The player \nwhose chosen number is closest to 0.8 * the average of all \nchosen numbers wins the round. Let's start the 1st round.\nI think in the\n1st  round \u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n(40 + 50 + 33 + 40 + 36) / 5 = 40  (Average)\n40 * 0.8 = 32 (Target) \nThe daily available water can meet the needs of only one \nresident. An auction is conducted, and the highest bidder will \nobtain today's water supply.\n$55\n$100\n8\n1\n$60\n$ 100\n$50\n$ 100\n$70\n$ 100\n$30\n$ 100\nIn today\u2019s bidding,        placed the highest bid of $70, \nsecuring the water resources for the day.        +2     and resets \nthe number of days thirsty\n=0 . Other residents have their \nHP deducted based on the number of days they have been \nthirsty \uff08\n=\n-\n\uff09. \nA new day! All residents receive a daily salary of  $100. \nToday, the daily available water can meet the needs of only \none resident ...\nToday I will bid \n$40 because \u2026\n8\n1\n8\n1\n8\n1\n8\n1\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n$90\n$200\n7\n2\n$80\n$ 200\n$100\n$ 200\n$50\n$ 130\n$80\n$ 200\nIn yesterday, \n  \u2026\u2026\u2026\n7\n2\n7\n2\n10\n1\n7\n2\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\nGuessing 0.8 of the Average\nSurvival Auction Game\n40\n50\n34\n40\n36\nLet's start the 2nd round.\nIn last \nround \u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n31\n36\n24\n25\n21\nClosest to the target. \n40-32=8\n50-32=18\n34-32=2\n40-32=8\n36-32=4\nFigure 2. Illustration of two dynamic reasoning problems in this paper. Left: Guessing 0.8 of the Average. It involves ten-round games,\nwhere each player selects a number between 1 to 100. The winner of each round is the player whose chosen number is closest to 80% of\nthe average number picked by all players. This challenge mirrors the dynamics of market predictions, where players must anticipate\ncollective market behavior. Right: Survival Auction Game. Players bid in daily auctions to secure scarce living resources, balancing their\nhealth and finances to survive a 10-day period, simulating scenarios of resource scarcity and financial decision-making.\nenvironments. We present two pilot challenges: \u201cGuess-\ning 0.8 of the Average\u201d and \u201cSurvival Auction Game\u201d, as\nshow in Figure 2 \u2013 both games serve as microcosms of the\ncomplex decision-making processes prevalent in dynamic\nenvironments, while also being well-defined for providing a\ntest bed for evaluating the LLMs\u2019 dynamic reasoning capa-\nbilities.\nThrough extensive simulation and evaluation, we observe\nthat conventional reasoning methods, which have been suc-\ncessful in static scenarios, struggle to maintain their per-\nformance in the dynamic settings presented by our pilot\nchallenges. Understanding others\u2019 perspectives, thinking\npatterns and anticipating their actions are crucial for optimal\ndecision-making in dynamic settings. This concept is re-\nferred to k-level thinking1 (Nagel, 1995; Stahl II & Wilson,\n1994) \u2013 a recursive reasoning process accounting for the\nlevel of strategic depth. The existing methods do not employ\nk-level thinking and fail to anticipate the actions of other\nparticipants, leading to suboptimal decision-making.\nTo overcome these limitations, we introduce a novel reason-\ning method termed \u201cK-Level Reasoning\u201d, which approaches\nk-level thinking by recursively adopting the perspective of\nrivals into the decision-making process. The recursive rea-\nsoning process allows for a deeper understanding of the\n1According to cognitive hierarchy theory, k-level thinking in-\nvolves considering what rivals are likely to do, what they think you\nwill do, and what they believe you think they will do, and so on.\nstrategic landscape and enables LLMs to make more in-\nformed decisions that take into account the likely responses\nof their rivals, achieving much better results in both pilot\nchallenges than traditional reasoning methods.\nWe summarize the contributions of this work as follows:\n\u2022 We study the dynamic reasoning capabilities of Large\nLanguage Models from a game theory perspective and\nintroduce two pilot tasks. Both tasks mirror the com-\nplexities of real-world dynamic decision-making and\nare also well-defined for evaluating LLMs\u2019 dynamic\nreasoning abilities.\n\u2022 We propose a novel reasoning approach with Large\nLanguage Models - the \u201cK-Level Reasoning\u201d method.\nIt integrates cognitive hierarchy theory into reasoning\nprocess, empowering LLMs to recursively predict and\nrespond to the thoughts and actions of rivals in com-\npetitive and dynamic scenarios.\n2. Problem Definition\n2.1. Dynamic Reasoning with LLMs\nDynamic reasoning problems, characterized by multiple\nindividual interactions and the ever-changing environment,\npresent a more complex scenario. As shown in Figure.1,\ntwo key aspects define these challenges:\n\u2022 Dynamic Environment: The environment evolves dy-\nnamically, influenced by the decisions and actions of\n2\nK-Level Reasoning with Large Language Models\nthe individuals.\n\u2022 Adaptive Strategies: Individuals continuously adjust\nand optimize their strategies in response to environ-\nmental changes.\nWhile similar tasks like strategic reasoning (Bakhtin et al.,\n2022) also involve multiple participants in a competitive or\ncooperative contexts, dynamic reasoning emphasizes more\non adapting to changes and interplay in the environment.\nWe set the initial environment as E0. The environment\nevolves to Et+1 in response to the collective decisions dt\ni\nof all individuals at step t. This transformation reflects the\ncumulative impact of individuals\u2019 actions. Consequently,\nindividuals aiming to make optimal decisions, denoted as\ndt+1\ni\n, must consider the latest environmental state, Et+1,\nwhile also taking into account the historical and potential\nfuture behaviors of others (D).\ndt\ni = f(E0, D0 \u21d2 E1, . . . , Dt\u22121 \u21d2 Et)\nDt = {dt\ni, i \u2208 N}\nEt+1 = Transfer(Et, Dt)\n(1)\nThis dynamic interplay underscores the need for LLMs to\nnot only process current data but also to anticipate and adapt\nto evolving strategies and environmental changes.\n2.2. Pilot Tasks\nTo formally study and quantitatively research the dynamic\nreasoning capabilities of LLMs, we explore two game set-\ntings inspired by game theory problems: the Guessing 0.8\nof the Average (G0.8A) and the Survival Auction Game\n(SAG). Both tasks are well-defined and controllable, ensur-\ning a clear test of LLMs\u2019 capabilities. The specific game\nsettings and prompts can be found in Appendix A.\n2.2.1. GUESSING 0.8 OF THE AVERAGE (G0.8A)\nThe Guessing 0.8 of the Average (Figure.2 Left) is a classic\ngame introduced by Alain Ledoux (Ledoux, 1981). It in-\nvolves ten-round games, where each player selects a number\nbetween 1 to 100. The objective is to select a number that is\nclosest to 80% of the group\u2019s average choice. The key idea\nis to guess how others will estimate the average and thus\ndecide the number to submit.\nThis idea is also illustrated in the Keynesian Beauty Contest,\na contest where participants select the six most attractive\nfaces out of 100, and those whose choices align with the\nfinal results win. As original stateded in Keynes\u2019s 1936\nwork, \u201cThe General Theory of Employment, Interest, and\nMoney\u201d (Keynes, 1936), \u201cit is not a case of choosing those\n[faces] that, to the best of one\u2019s judgment, are really the\nprettiest, nor even those that average opinion genuinely\nthinks the prettiest. We have reached the third degree where\nwe devote our intelligence\u2019s to anticipating what average\nopinion expects the average opinion to be.\u201d\nThis game mirrors the challenge of anticipating the col-\nlective behavior of the financial markets. Investors must\npredict not only the value of an asset but also how they\nbelieve others will value it in the future.\n2.2.2. SURVIVAL AUCTION GAME (SAG)\nThe Survival Auction Game (Figure.2 Right), as derived\nfrom the Water Allocation Challenge proposed in (Mao\net al., 2023), is set in a fictional town grappling with a\nsevere drought. The goal for each resident is to survive a\n10-day drought period, which is achieved by bidding water\nresources and maintaining health points above zero. If a\nplayer successfully bid the water resources, they will gain\nhealth points; otherwise, they will lose health points equal\nto the number of consecutive days n, during which they\nhave not obtained water resources.\nThe integration of the auction system with the health points\nmechanism creates a dynamic environment where players\nmust balance health and finances. It challenges their strate-\ngic resource management skills and their capacity to antici-\npate and respond to the competitive actions of others.\n2.3. Metrics\nIn order to effectively measure the capabilities of LLMs\nand LLM agents in dynamic reasoning scenarios, we estab-\nlish a comprehensive evaluation system consisting of four\nmetrics. As these tasks are well-defined, the metrics are\nstraightforward.\nWin Rate is calculated based on the number of wins over\ngame going, providing a measure of the overall ability.\nWinRate =\nNum of Wins\nTotal Round per Test \u00d7 Num of Test (2)\nIn games such as the G0.8A, where elimination is not a\nfactor, the Win Rate is a crucial measure.\nAverage Survival Round calculates the average round in\nwhich the player remains in the game. It\u2019s an effective way\nto assess performance in elimination-based game, like SAG.\nAvgSurvivalRound =\nP Survival Round in Each Test\nNum of Test\n(3)\nAdaptation Index evaluates the player\u2019s ability to adapt\nand improve the performance over time. It is determined\nby comparing the deviation from the optimal strategy in the\nfirst half of the rounds to that in the second half.\nAdaptation Index = Target Deviation @ (second half)\nTarget Deviation @ (first half)\n(4)\n3\nK-Level Reasoning with Large Language Models\nFirst-level Thinking\nSecond-level Thinking\nThird-level Thinking\n\u2026\u2026\n\u2026\nContext  \ud835\udc38\ud835\udc61\nDecision \ud835\udc51 \ud835\udc61,\ud835\udc58   \nContext \ud835\udc38\ud835\udc61\nDecision \n\u1218\ud835\udc511\n\ud835\udc61, \ud835\udc58\u22121\nContext \ud835\udc38\ud835\udc61\nDecision \n\u1218\ud835\udc512\n\ud835\udc61, \ud835\udc58\u22121\nContext \ud835\udc38\ud835\udc61\nDecision \n\u1218\ud835\udc513\n\ud835\udc61, \ud835\udc58\u22121\nAccording to the analysis,            \u2019s most likely choice is ... ;           \u2019s \nmost likely choice is ... ;           \u2019s most likely choice is ... ; Based on \n               , my best choice is           . \nK-Level Reasoning\n(K-1) Level\n(K-1) Level\n(K-1) Level\nRival 1\nRival 2\nRival 3\nFigure 3. Left: K-level thinking is a recursive reasoning process. In first-level thinking, individuals react directly to the environment,\nakin to static reasoning. In second-level thinking, individuals take into account the first-level thinking of others, and so on and so forth.\nRight: In K-Level Reasoning, LLM adopts the perspective of rivals to recursively employ k-level thinking based on available historical\ninformation. It significantly improves the prediction accuracy of rivals\u2019 subsequent moves and informs more strategic decision-making.\nFor the G0.8A, we calculate the deviation between the num-\nber chosen by player and the target number. For the SAG,\nconsidering that the optimal winning bid is just slightly\nhigher than the opponent\u2019s highest bid, we calculate the ab-\nsolute difference between the agent\u2019s bid and the opponent\u2019s\nhighest bid.\nPrediction Accuracy evaluates the accuracy of player\u2019s\npredictions regarding rivals\u2019 future moves. In the G0.8A,\nit involves calculating the absolute difference between the\nplayer\u2019s predicted average and the actual average in each\nround:\nPred Acc =\nP |AvgP red \u2212 AvgActual|\nNum of Test\n(5)\nIn the SAG, the focus shifts to measuring the absolute er-\nror between the player\u2019s prediction of the highest bid by\nopponents and the actual highest bid made by them.\nPred Acc =\nP |Max BidP red \u2212 Max BidActual|\nNum of Test\n(6)\n3. K-Level Reasoning with LLMs\nCognitive hierarchy theory (Stahl, 1993) provides a funda-\nmental framework for understanding decision-making in\ndynamic, interactive, and competitive scenarios. Players\ncategorize themselves and others into different levels based\non their rationality and strategic depth.\nAs shown in Figure.3 (Left), players in the first-level think-\ning directly react to the environment without considering\nothers\u2019 actions. Players at higher thinking levels, such as sec-\nond and third level, base their decisions on the assumption\nthat other players are operating at lower levels of strate-\ngic thinking. This recursive reasoning process where each\nplayer attempts to outthink others by understanding and an-\nticipating their strategic reasoning level allows for a layered\nunderstanding of strategic interactions.\nWe draw on this idea to improve the reasoning capabilities\nof LLMs in dynamic challenges, and propose a novel re-\ncursive dynamic reasoning approach, termed the \u201cK-Level\nReasoning\u201d. As illustrated in Figure 3 (Right), this method\ninvolves recursively simulating the opponent\u2019s behavior us-\ning available historical information, followed by predicting\nthe optimal behavior based on the opponents\u2019 behavior mod-\neling.\nThe K-Level Reasoning process is formulated as:\nInitial condition:\ndt,1 = f(Et)\n(7)\nRecursive formula:\ndt,k = f(Et, \u02c6dt,k\u22121\n1\n, ..., \u02c6dt,k\u22121\nn\n)\n(8)\nwhere dt,k indicates the decision made at step t in k-th level\nthinking. f represents a LLM that generates the decision\nbased on inputs. \u02c6dt,k\u22121\n1\n, ..., \u02c6dt,k\u22121\nn\nrepresent the anticipated\ndecisions of other players (1 to n) at k \u2212 1 level of reason-\ning. The \u02c6dt,k\u22121\n1\ncan be recursively derived by applying this\nprocess through modeling of the k \u2212 2 th level.\nApplying K-Level Reasoning to LLMs could significantly\nenhance their ability to understand the dynamic and strategic\nscenarios. By simulating opponents\u2019 behavior and predict-\ning optimal responses, LLMs could effectively adapt to the\nstrategies of other participants. However, deeper modeling\ndoesn\u2019t always lead to better results. Optimal decisions oc-\ncur when one\u2019s understanding is just one level above their\n4\nK-Level Reasoning with Large Language Models\nChain of Thought\nStep 1: Consider the previous round's results. In the last \nround, the average was 35.4 and the target number was \n28.32. \nStep 2: Adjust the guess based on previous results. Since \nthe average of the chosen numbers was lower than the \nmidpoint of the range (50), we can assume that players \nmight choose numbers closer to the middle or slightly \nhigher in this round.\nStep 3: Make an educated guess for the new average. \nConsidering the previous round's average and player \nchoices, let's estimate the new average to be around 40.\nStep 4: Calculate the target number based on our guess. \n0.8 * 40 = 32.\nStep 5: Choose a number close to the target number. To \nincrease the chances of winning, we should choose a \nnumber close to 32.\nMy final answer: I will choose 32.\nHistorical Targets: \nRound1:35 Round2:33 Round3:31 Round4:29 Round5:28 \nLast Round choices: \nPlayer1:33 Player2:33 Plyaer3:32 Player4:33 Player5:46 \nResult:\nPlayer1: 32 Player2: 31 Player3: 31 Player4: 29 Player5: 40\nTarget number: (32 + 31 + 31 + 29 + 40)  / 5 * 0.8 = 26 \nWinner: Player 4\nPrediction Chain of Thought\nPredict: Player 2 : Similar number, around 31. Player 3 : \nSimilar number, around 33. Player 4 : Slightly higher \nnumber, around 28. Player 5 : Lower number, around 45.\nBased on the prediction of other players, the average \nnumber in the next round would be around (27 + 31 + 33 \n+ 28 + 45) / 5 = 32.8.\nThe target number in the next round (0.8 * the average \nof all chosen numbers) would be 0.8 * 32.8 = 26.24.\nAnswer: I will choose 26 in the next round game.\nK-Level Reasoning\nPrediction for the next round of other players is as \nfollows: \nPlayer2 choose 27, Player3 choose 11, Player4 choose \n12, Player5 choose 14. According to the game expert's \nprediction, what number will you give in the next round \nis most likely to be close to the target number?\nGuess which number will win in the next round. Let\u2018s think \nstep by step, and finally answer a number\u3002\nPrompt\nOutput\nPrompt\nFirst of all, predict the next round of opponent choices \nbased on the choices of other players in the previous \nround.\nRound1: Player2: 42 Player3: 42 Player4: 42 Player5: 63\nRound2: Player2: 44 Player3: 34 Player4: 33 Player5: 55\nRound3: Player2: 38 Player3: 38 Player4: 31 Player5: 47\nRound4: Player2: 35 Player3: 37 Player4: 29 Player5: 52\nRound5: Player2: 31 Player3: 33 Player4: 25 Player5: 48\nBased on the prediction of other players, answer the \nnumber will you choose to win the next round game here.\nAccording to the prediction, the average of the numbers \nchosen by all players would be: (27 + 11 + 12 + 14 + x) / \n5, where x is the number you choose.\nLet's calculate the average without considering your \nchoice first: (27 + 11 + 12 + 14) / 4 = 16\nNow let's calculate the target number, which is 0.8 * the \naverage of all chosen numbers: 0.8 * 16 = 12.8 \nSo I choose 13 in the next round.\nOpponents\u2019 (K-1)-Level Thinking\nRound-6: Player1\nHistorical Targets: \nRound1:32 Round2:29 Round3:29 Round4:27 Round5:26 \nLast Round choices: \nPlayer1:27 Player2:31 Plyaer3:33 Player4:25 Player5:48 \nHistorical Targets: \nRound1:21 Round2:20 Round3:17 Round4:15 Round5:14 \nLast Round choices: \nPlayer1:15 Player2:29 Plyaer3:13 Player4:13 Player5:17 \nSystem\nOutput\nSystem\nHistory\nChoose an integer between \n1 and 100 for this round.\nI choose 13.\nChoose an integer between \n1 and 100 for this round.\nI choose 11.\n\u2026\u2026\nRound-6: Player1\nPrompt\nResult:\nPlayer1: 26 Player2:29 Player3: 30 Player4: 23 Player5:44\nTarget number: (26 + 29 + 30 + 23 + 44)  / 5 * 0.8 = 24\nWinner: Player 4 \nResult:\nPlayer1: 13  Player2: 27  Player3: 11  Player4: 14 Player5: 15\nTarget number: (13 + 27 + 11 + 14 + 15) / 5 * 0.8 = 13\nWinner: player 1\nFuzzy prediction of the future\nBiased prediction of the future\nAccurate prediction of the future\nLose\nWin\nPlayer 2:\nr1: 37, r2:35, r3:33, r4:31, r5:29, r6: ?\nPlayer 3:\nr1: 23, r2:19, r3:17, r4:15, r5:13, r6: ?\n\u2026\u2026\n\u2192 11\n\u2192 27\nLose\nRound-6: Player1\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\nOutput\nOutput\nResult\n13 is closet to  the target \nnumber 14, and you win.\n(K-1)-Level Thinking\nPlayer 1:\nr1: 30, \u2026\u2026, r5:15, r6: ?\nr1: 37, \u2026\u2026, r5:29, r6: ?\nPlayer 2:\nPrediction for the next \nround of other players \nis as follows: \nPlayer 1 choose 13 \u2026\nwhat number will you \ngive in the next round?\n(K-2)-Level \n  Thinking\n\u2026\u2026\nFigure 4. Illustration of different methods in the Guessing 0.8 of the Average game. Left: In the Chain of Thought, the LLM outputs\nreasoning logic in a step-by-step format. However, the LLM demonstrates poor understanding of situations and prediction of opponents\u2019\nactions. Middle: In the Prediction Chain-of-Thought, with an explicit requirement to make predictions about rivals\u2019 next moves, the LLM\nclearly divides the reasoning into prediction and reasoning phases. However, the predictions are still somewhat biased. Right: In the\nK-Level Reasoning, the LLM recursively predict rivals\u2019 next moves with public historical information. The implementation is based on\nEquation 7&8. Thank to a deeper strategic depth than rivals, the prediction and decision are more accurate.\ncompetitor\u2019s. Overthinking can also cause significant bi-\nass and errors. We\u2019ll explore how recursion depth affect\ndecisions in experiments.\n4. Experiments\n4.1. Base Techniques\nWe adapt a variety of approaches, originally from traditional\nreasoning benchmarks, to explore the reasoning methods\nwithin dynamic challenges. These base techniques include:\nStandard Prompting (Direct): This is the conventional\nprompting method in which the LLM generates the final an-\nswer (Action) in response to the given game setting prompt.\nChain-of-Thought (CoT) (Wei et al., 2022): This approach\nemploys the zero-shot native Chain-of-Thought reasoning\nmethod (Kojima et al., 2022).\nPersona Prompting (Persona) (Deshpande et al., 2023):\nThis technique modifies the standard prompting process\nby incorporating \u201cGame Expert\u201d persona information to\naugment the reasoning capabilities of LLM.\nReflexion (Reflect) (Shinn et al., 2023): This method refers\nthe concept of language agents with verbal reinforcement\nlearning. It has been specifically adapted for dynamic tasks,\nwith modifications that are explained in detail in the C.1.2.\nSelf-Refine (Refine) (Madaan et al., 2023): This is a multi-\nround iterative reasoning approach where an additional\nLLM offers comments and adjustments prior to reaching\na final decision. The distinctions between Self-Refine and\nReflect are elaborated upon in the Appendix D.\nPrediction Chain of Thought (PCoT): This is a strong\nbaseline we proposed. Diverging from CoT, PCoT uniquely\nrequires that the LLM explicitly predicts the opponents\u2019\nactions before making decisions. Although it also empha-\nsizes the prediction of opponents\u2019 actions, PCoT is more\nabout direct prediction based on context, while K-Level Rea-\nsoning involves a recursive, layered approach to anticipate\nopponents\u2019 actions.\nFor details on the implementations and specific examples of\nthese techniques, please refer to the Appendix C.\n4.2. Experimental Settings\nTo evaluate the dynamic reasoning capabilities of a specific\napproach, we set a controllable environment and distinguish\nbetween two roles: the player (primary focus) and the op-\nponents. In each game, there is 1 player and 4 opponents.\nThe \u201cplayer\u201d will be equipped with a specific method, and\nall opponents use the same reasoning approach. This well-\ndefined setting constructs a clearer comparison of dynamic\nreasoning capabilities between methods. For each setting,\n5\nK-Level Reasoning with Large Language Models\nTable 1. Win Rate of the player against different opponents in\nGuessing 0.8 of the Average game. Due to the simultaneous wins\nof multiple players in a particular game, the Win Rate of each\nmethod against itself exceeds 0.2. We set the Win Rate of each\nmethod against itself (as indicated on the diagonal) as comparison\nbaseline. A Win Rate significantly higher than this suggests a\nstronger dynamic reasoning ability.\nOpponent\nDirect CoT Persona Reflect Refine PCoT K-R\nDirect\n0.43\n0.67\n0.62\n0.53\n0.43\n0.61\n0.82\nCoT\n0.07\n0.32\n0.35\n0.14\n0.22\n0.45\n0.63\nPersona\n0.05\n0.37\n0.29\n0.05\n0.37\n0.11\n0.46\nReflect\n0.42\n0.68\n0.63\n0.39\n0.64\n0.74\n0.78\nRefine\n0.10\n0.34\n0.32\n0.31\n0.23\n0.22\n0.46\nPCoT\n0.03\n0.44\n0.52\n0.21\n0.51\n0.54\n0.85\nK-R(ours)\n0.04\n0.15\n0.14\n0.04\n0.17\n0.14\n0.52\nAverage\n0.16\n0.42\n0.41\n0.24\n0.37\n0.40\n0.65\nTable 2. Adaptation Index of different methods in the Guessing\n0.8 of the Average. Lower values in the table indicate stronger\nadaptability. For ease of comparison, the optimal values for each\nrow are bolded, and the second-best values are underlined.\nOpponent\nDirect CoT Persona Reflect Refine PCoT K-R\nDirect\n0.60\n1.47\n0.64\n0.71\n1.27\n0.76\n0.44\nCoT\n0.44\n0.67\n0.40\n0.31\n0.58\n0.40\n0.45\nPersona\n0.43\n0.47\n0.34\n0.37\n0.62\n0.32\n0.25\nReflect\n0.62\n0.60\n0.65\n0.60\n0.87\n0.53\n0.26\nRefine\n0.41\n0.57\n0.28\n0.36\n0.55\n0.42\n0.30\nPCoT\n0.50\n0.60\n0.54\n0.31\n0.57\n0.32\n0.23\nK-R(ours)\n0.61\n0.60\n0.49\n0.48\n0.74\n0.47\n0.25\nAverage\n0.51\n0.71\n0.48\n0.45\n0.74\n0.46\n0.31\nexperiments are repeated 10 times to ensure significant re-\nsults. Meanwhile, we compared reasoning methods with\nseveral programmatic strategic patterns as references. The\nexperimental results can be found in the Appendix B.\nWe implemented all methods using the GPT-4 (Achiam\net al., 2023) (gpt4-32k), with the temperature set at 0.7 and\nthe top-p set at 0.9. Unless specified otherwise, the level of\nthinking in K-Level Reasoning is set to K=2.\n4.3. Results\n4.3.1. GUESSING 0.8 OF THE AVERAGE\nWin Rate Analysis of player Win Rate (Table 1) demon-\nstrates the K-Level Reasoning (K-R) method\u2019s superiority\nin dynamic decision-making environments. Its strategic ef-\nfectiveness lies in its ability to anticipate opponent moves,\noutperforming other prompting methods.\nAdaptation Index The Adaptation Index (Table 2), calcu-\nlated as the ratio of the average absolute deviation in player\nchoices between the first and second halves of the game,\nreveal insightful trends. Despite a lack of direct correla-\ntion with Win Rate, the PCoT and K-R approaches show\nsignificant learning progress, suggesting an adaptive move\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n5\n10\n15\n20\nPCoT vs Reflect\nK-R vs Reflect\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n5\n10\n15\n20\nPCoT vs CoT\nK-R vs CoT\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n5\n10\n15\n20\nPCoT vs PCoT\nK-R vs PCoT\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n5\n10\n15\n20\nPCoT vs Persona\nK-R vs Persona\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n5\n10\n15\n20\nPCoT vs Refine\nK-R vs Refine\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n5\n10\n15\n20\nPCoT vs K-R\nK-R vs K-R\nFigure 5. The absolute error in predicting the opponent during the\nG0.8A between PCOT and K-Level Reasoning.\ntowards optimal strategies. Whereas, the CoT and Refine\nmethods display limited learning progression, indicating\nthat inflexible thought processes might hinder adaptability.\nPrediction Accuracy Initially, K-R displays higher pre-\ndiction accuracy than PCoT (Figure.5), starting with more\naccurate and less random predictions. Furthermore, the pre-\ndictions converged very quickly and were quite accurate\nin the second half of the game. This trend underscores the\nLLM\u2019s increasing proficiency in understanding strategic\ndepths with more gameplay context. K-R\u2019s superior long-\nterm prediction accuracy suggests its alignment with LLM\u2019s\nintrinsic context-learning capabilities.\n4.3.2. SURVIVAL AUCTION GAME\nAverage Survival Round Different reasoning methods\u2019 per-\nformance in the Survival Auction Game (Table 3) reveals\nthat the K-Level Reasoning method enhances survival rates\ncompared to other approaches. Unlike in the performance in\nthe G0.8A, PCoT does not show a distinct advantage here.\nAdaptation Index K-Level Reasoning exhibits the highest\nAdaptation Index (Table 4), indicating minimal extra over-\nhead and superior adaptation skills in the game\u2019s latter half.\nThis metric also reveals the method\u2019s remarkable adaptabil-\nity, especially since poor performance in the game\u2019s first\nhalf can lead to early elimination. The inability of some\nmethods to reach the second half of the game under certain\nconditions further underscores K-R\u2019s adaptiveness.\nPrediction Accuracy The prediction accuracy for the high-\nest opponent bid in the Survival Auction Game (Figure.6)\nhighlights the increased complexity of this challenge. While\nboth PCoT and K-R faced difficulties in accurately predict-\ning bids due to the need to consider multiple variables like\nopponent\u2019s balance, health condition and bidding style, K-R\nconsistently showed a smaller prediction deviation, under-\nscoring its effectiveness in complex scenarios.\n4.4. Qualitative Analysis\nFigure 4 demonstrates how different reasoning approaches\nfunction in the \u201cGuessing 0.8 of the Average\u201d game. The\n6\nK-Level Reasoning with Large Language Models\nTable 3. Average Survival Round of the player against different\nopponents in Survival Auction Game. We set the Average Survival\nRound of each method against itself (as indicated on the diagonal)\nas comparison baseline. A survival round significantly higher than\nthis suggests a stronger dynamic reasoning ability.\nOpponent Direct CoT Persona Reflect Refine PCoT\nK-R\nDirect\n5.90\n7.00\n7.50\n4.70\n8.70\n6.60\n9.40\nCoT\n5.70\n6.50\n5.30\n4.00\n8.10\n5.30\n10.00\nPersona\n5.70\n7.70\n7.40\n5.20\n6.30\n7.20\n9.30\nReflect\n9.40\n9.40\n9.90\n5.20\n8.60\n8.20\n10.00\nRefine\n6.30\n6.40\n8.10\n4.30\n8.20\n5.30\n7.90\nPCoT\n8.50\n9.60\n9.90\n6.30\n8.50\n6.20\n9.70\nK-R\n4.10\n5.50\n5.00\n4.04\n5.70\n4.40\n6.80\nAverage\n6.51\n7.44\n7.59\n4.82\n7.73\n6.17\n9.01\nTable 4. Adaptation Index of different reasoning methods in the\nSurvival Auction Game. The missing parts in the table are indi-\ncated by \u201c-\u201d to signify that no record of this method survive to the\nsecond half rounds. These missing data were not included in the\ncalculation of the Average value.\nOpponent\nDirect CoT Persona Reflect Refine PCoT K-R\nDirect\n0.86\n1.09\n2.07\n2.36\n1.82\n0.39\n1.24\nCoT\n1.32\n1.56\n1.29\n-\n2.27\n0.67\n1.27\nPersona\n1.19\n1.62\n1.30\n2.31\n1.08\n1.79\n1.04\nReflect\n1.00\n2.02\n1.21\n0.66\n1.06\n1.21\n1.18\nRefine\n1.43\n1.69\n1.95\n2.03\n1.74\n1.73\n1.21\nPCoT\n1.60\n1.37\n1.05\n0.90\n1.69\n0.83\n1.19\nK-R(ours)\n-\n0.64\n1.27\n-\n0.70\n-\n2.27\nAverage\n-\n1.43\n1.45\n-\n1.48\n-\n1.34\nCOT and PCOT methods exhibit limitations in understand-\ning and predicting opponents\u2019 moves, thus leading to biased\nor less accurate decisions. K-Level Reasoning, on the other\nhand, recursively utilizes historical data to make more nu-\nanced predictions about opponents\u2019 behaviors and thinking\nlevels, resulting in greater accuracy and strategic depth.\n5. Rethinking K-Level Reasoning\n5.1. Better Reasoning Methodology VS Stronger\nFoundation Model\nThere is a consensus that LLM trained with more data and\npossessing larger parameter sizes demonstrate stronger rea-\nsoning capabilities. We explore whether K-Level Reasoning\ncan significantly enhance the dynamic reasoning abilities\nof relatively weaker LLMs. Therefore, we conduct exper-\niments to compare the performance between the K-Level\nReasoning with GPT-3.5 (K-R[GPT-3.5]) and the other rea-\nsoning methods based on GPT-4. All experiments are re-\npeated 10 times.\nFrom the results of Table 5, we can see that the superior\nperformance of K-R[GPT-3.5] over the standard prompting\nmethod of GPT4 (Direct[GPT4]). Furthermore, when bat-\ntling with opponents equipped with reasoning methods on\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n20\n40\n60\n80\n100\n120\n140\nPCoT vs Reflect\nK-R vs Reflect\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n20\n40\n60\n80\n100\n120\n140\nPCoT vs CoT\nK-R vs CoT\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n20\n40\n60\n80\n100\n120\n140\nPCoT vs PCoT\nK-R vs PCoT\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n20\n40\n60\n80\n100\n120\n140\nPCoT vs Persona\nK-R vs Persona\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n20\n40\n60\n80\n100\n120\n140\nPCoT vs Refine\nK-R vs Refine\nR1 R2 R3 R4 R5 R6 R7 R8 R9 R10\n0\n20\n40\n60\n80\n100\n120\n140\nPCoT vs K-R\nK-R vs K-R\nFigure 6. The absolute error in predicting the opponent during the\nSAG between PCOT and K-Level Reasoning\nTable 5. A comparison of K-Level Reasoning with GPT-3.5 and\nother reasoning approaches with GPT-4. For the Guessing 0.8\nof the Average, we report the win rate; for the Survival Auction\nGame, we report the average survival round.\nGuessing 0.8 of the Average\nSurvival Auction Game\nOpponent\nDirect\nK-R\nDirect\nK-R\nDirect\nK-R\nDirect\nK-R\n[GPT-4]\n[GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4]\nDirect\n0.18\n0.18\n0.43\n0.82\n5.00\n9.40\n5.90\n9.40\nCoT\n0.14\n0.37\n0.07\n0.63\n5.30\n8.10\n5.70\n10.00\nPersona\n0.10\n0.23\n0.05\n0.46\n5.00\n7.50\n5.70\n9.30\nReflect\n0.24\n0.38\n0.42\n0.78\n5.00\n8.50\n9.40\n10.00\nRefine\n0.14\n0.13\n0.10\n0.46\n5.10\n6.70\n6.30\n7.90\nPCoT\n0.19\n0.46\n0.03\n0.85\n4.10\n6.80\n8.50\n9.70\nAverage\n0.16\n0.29\n0.18\n0.67\n4.92\n7.83\n6.92\n9.38\nGPT4, K-R[GPT-3.5] demonstrates remarkable capabilities.\nK-R, with its excellent formalization on dynamic challenges\nand restoration of rival\u2019s perspective, greatly enhances the\nLLM\u2019s ability in dynamic reasoning. The results imply\nthat integrating advanced reasoning methodologies can be a\ncrucial factor in enhancing the dynamic reasoning of LLMs.\n5.2. The Deeper Thinking Level, the Better Strategic\nPerformance?\nThe K-Level Reasoning approach is distinguished by its\nrecursive modeling of rivals\u2019 thinking processes. We are cu-\nrious about how thinking levels affect reasoning outcomes.\nTherefore, we compare the performance of K-R[K=2] and\nK-R[K=3] in two games. K-R[K=3] assumes that all op-\nponents are at the second level of the cognitive hierarchy.\nIn other words, when simulating opponent predictions, the\nopponent uses the K-R[K=2] reasoning approach. The re-\nsults, detailed in Table 6, reveal an intriguing picture of the\nimpact of increased depth in thinking levels.\nWhen against the Direct method, which can be treated as the\nfirst-level thinking, K-R[K=3] showed a decreased win rate\nin G0.8A and keep the performance in SAG. This suggests a\npotential downside to excessive depth in reasoning, possibly\nindicating overthinking. However, against K-R[K=2] op-\nponents, K-R[K=3] exhibited a significant improvement in\nboth games. This improvement demonstrates the advantages\nof being one step ahead in the reasoning process.\nThis juxtaposition leads to a compelling insight: the criti-\ncal factor in K-Level Reasoning is not simply the depth of\n7\nK-Level Reasoning with Large Language Models\nTable 6. Comparison between K-Level Reasoning[K=2] and K-\nLevel Reasoning[K=3] in the two games. For G0.8A we report\nWin Rate, and for SAG we report Average Survival Round.\nGuessing 0.8 of the Average\nSurvival Auction Game\nOpponent Direct K-R[K=2]\nK-R[K=3]\nDirect K-R[K=2]\nK-R[K=3]\nDirect\n0.43\n0.82\n0.77 (-0.05)\n5.90\n9.40\n9.40 (+0.00)\nK-R[K-2]\n0.04\n0.52\n0.60 (+0.08)\n4.10\n6.80\n8.30 (+1.50)\nthought, but the relative depth compared to the opponent.\nHaving a one level deeper thinking-level offers a strategic\nadvantage, as seen in the superior performance of K-R[K=2]\nover Direct and K-R[K=3] over K-R[K=2]. However, ad-\nvancing two levels beyond the opponents, as in the case of\nK-R[K=3] versus Direct, may result in diminishing returns\ndue to the complexity of over-anticipation.\nIn dynamic challenges, identifying the exact thinking levels\nof opponents is challenging. Adapting to thinking-levels\ndynamically and employing K-Level Reasoning for deeper\nanalysis presents a valuable direction for future research.\n6. Related Work\n6.1. Reasoning with LLMs\nLarge Language Models (LLMs) excel in diverse complex\nreasoning tasks, such as mathematical (Miao et al., 2021;\nPatel et al., 2021), common sense (Talmor et al., 2022; Bhak-\nthavatsalam et al., 2021), and symbolic reasoning (Srivas-\ntava et al., 2022; Suzgun et al., 2022). A notable reasoning\napproach involves breaking down complex questions into\na series of intermediate steps, a technique known as the\nChain-of-Thought (CoT) method (Wei et al., 2022; Kojima\net al., 2022). Subsequently, some works have emerged to\nextend CoT, with innovations like Tree of Thought (ToT)\n(Yao et al., 2023), Graph of Thought (GoT) (Besta et al.,\n2023) and Skeleton-of-thought (Ning et al., 2023). Besides,\napproaches like Self-Refine (Madaan et al., 2023) and Re-\nflexion (Shinn et al., 2023) enhance CoT\u2019s consistency by\nhaving LLMs review and refine their responses. Moreover,\nrecent research has revealed that integrating persona infor-\nmation into LLMs significantly improves their reasoning\nprocesses (Deshpande et al., 2023). A series of studies (Fu\net al., 2023; Wang et al., 2023) have been conducted to\nincorporate more persona information, aiming to enhance\nthe rationality and knowledge ability of the LLM reasoning\nprocess. However, our comprehensive testing revealed that\nall these methods are inadequate for dynamic problems.\nRecently, research on large language models (LLMs) in\nstrategic reasoning has been conducted(Gandhi et al., 2023;\nBakhtin et al., 2022), these approaches involve prompting\nLLMs to recognize the intricacies of strategic tasks, like our\nproposed Prediction Chain-of-Thought baseline. However,\nour experimental results indicate that this approach fails to\nestablish a clear cognitive hierarchy necessary for recursive\nand deeper strategic thinking. We propose the first recursive\ndynamic reasoning method with LLMs - K-Level Reasoning,\nand demonstrate its superiority through exhaustive empirical\nevidence.\n6.2. Dynamic Problems in Machine Learning\nDynamic problems arise when multiple participants are in-\nvolved in multi-round interactions. One key factor is the\nsimultaneous interactions of multiple participants with the\nenvironment. In machine learning area, it is also referred\nas multi-agent system (MAS) (Dorri et al., 2018). Unlike\nsingle-agent systems, MAS encounters a broader range of\nissues and challenges, as noted by Wong et al. (2021), includ-\ning computational complexity (Ding & Dong, 2020), non-\nstationarity (Papoudakis et al., 2019), partial observability\n(Mahajan et al., 2019; Foerster et al., 2016), and challenges\nin credit assignment (Sunehag et al., 2017). Particularly, in\nthe context of inference using LLMs, the nonstationarity of\nthe environment poses a distinct challenge. LLMs struggle\nwith the nonstationarity of environments, as their traditional\ntraining focuses on next-word prediction.(Dai & Le, 2015;\nDevlin et al., 2018; Radford et al., 2018).\nAddressing nonstationarity requires deep environmental un-\nderstanding. In this regard, opponent modeling (Albrecht\n& Stone, 2018; Schrittwieser et al., 2020; Li et al., 2020)\nproves to be an effective approach for understanding fu-\nture environmental changes. Existing opponent predictions\nbased on language models have been applied to strategic\nproblems(Bakhtin et al., 2022), while the need for extensive\ndata for training limits the flexibility of the agents. We pro-\npose \u201cK-Level-Reasoning\u201d, which is a recursive modeling\non opponents with LLMs. This method fully exploits the\nin-context learning capabilities of LLMs.\n7. Conclusion\nThis paper represents a significant stride in understanding\nand enhancing the capabilities of LLMs in dynamic reason-\ning. The Guessing 0.8 of the Average and Survival Auction\nGame serve not only as effective tools for assessment but\nalso as bridges connecting AI research with practical, real-\nworld economic and dynamic scenarios. By introducing\nthe game settings, we have provided a novel and robust\ntestbed for systematically evaluating LLMs in scenarios that\nmirror the complexities of real-world decision-making. We\nanalyze the limitations of the existing reasoning methods in\naddressing dynamic challenges and propose a new reasoning\napproach \u201cK-Level-Reasoning\u201d. Through experiments, we\nvalidate the rationality of decision-making and adaptability\nto dynamic environments offered by this method. This pa-\nper highlights the vast potential of LLMs in navigating and\ndeciphering the ever-changing tapestry of dynamic strategic\ndecision-making.\n8\nK-Level Reasoning with Large Language Models\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\nAher, G. V., Arriaga, R. I., and Kalai, A. T. Using large lan-\nguage models to simulate multiple humans and replicate\nhuman subject studies. In International Conference on\nMachine Learning, pp. 337\u2013371. PMLR, 2023.\nAlbrecht, S. V. and Stone, P. Autonomous agents modelling\nother agents: A comprehensive survey and open problems.\nArtificial Intelligence, 258:66\u201395, 2018.\nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty,\nC., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A., et al.\nHuman-level play in the game of diplomacy by combining\nlanguage models with strategic reasoning. Science (New\nYork, NY), pp. eade9097\u2013eade9097, 2022.\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\nSolving elaborate problems with large language models.\narXiv preprint arXiv:2308.09687, 2023.\nBhakthavatsalam, S., Khashabi, D., Khot, T., Mishra, B. D.,\nRichardson, K., Sabharwal, A., Schoenick, C., Tafjord,\nO., and Clark, P. Think you have solved direct-answer\nquestion answering? try arc-da, the direct-answer ai2\nreasoning challenge. arXiv preprint arXiv:2102.03315,\n2021.\nChen, J., Yuan, S., Ye, R., Majumder, B. P., and Richardson,\nK. Put your money where your mouth is: Evaluating\nstrategic planning and execution of llm agents in an auc-\ntion arena. arXiv preprint arXiv:2310.05746, 2023.\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning.\nAdvances in neural information processing systems, 28,\n2015.\nDeshpande, A., Murahari, V., Rajpurohit, T., Kalyan,\nA., and Narasimhan, K. Toxicity in chatgpt: Analyz-\ning persona-assigned language models. arXiv preprint\narXiv:2304.05335, 2023.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDiao, S., Wang, P., Lin, Y., and Zhang, T. Active prompting\nwith chain-of-thought for large language models. ArXiv,\nabs/2302.12246, 2023. doi: 10.48550/arXiv.2302.12246.\nDing, Z. and Dong, H. Challenges of reinforcement learning.\nDeep Reinforcement Learning: Fundamentals, Research\nand Applications, pp. 249\u2013272, 2020.\nDorri, A., Kanhere, S. S., and Jurdak, R. Multi-agent sys-\ntems: A survey. Ieee Access, 6:28573\u201328593, 2018.\nFoerster, J., Assael, I. A., De Freitas, N., and Whiteson,\nS. Learning to communicate with deep multi-agent re-\ninforcement learning. Advances in neural information\nprocessing systems, 29, 2016.\nFu, Y., Peng, H., Khot, T., and Lapata, M.\nImprov-\ning language model negotiation with self-play and in-\ncontext learning from ai feedback.\narXiv preprint\narXiv:2305.10142, 2023.\nGandhi, K., Sadigh, D., and Goodman, N. D.\nStrate-\ngic reasoning with language models.\narXiv preprint\narXiv:2305.19165, 2023.\nHan, X., Wu, Z., and Xiao, C. \u201d guinea pig trials\u201d utilizing\ngpt: A novel smart agent-based modeling approach for\nstudying firm competition and collusion. arXiv preprint\narXiv:2308.10974, 2023.\nKeynes, J. M. The general theory of employment. The\nquarterly journal of economics, 51(2):209\u2013223, 1936.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,\nY. Large language models are zero-shot reasoners. Ad-\nvances in neural information processing systems, 35:\n22199\u201322213, 2022.\nLan, Y., Hu, Z., Wang, L., Wang, Y., Ye, D., Zhao, P., Lim,\nE.-P., Xiong, H., and Wang, H. Llm-based agent society\ninvestigation: Collaboration and confrontation in avalon\ngameplay. arXiv preprint arXiv:2310.14985, 2023.\nLedoux, A. Concours r\u00b4esultats complets. Les victimes se\nsont plu `a jouer le, 14:10\u201311, 1981.\nLi, J., Yang, F., Tomizuka, M., and Choi, C. Evolvegraph:\nMulti-agent trajectory prediction with dynamic relational\nreasoning. Advances in neural information processing\nsystems, 33:19783\u201319794, 2020.\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao,\nL., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S.,\nYang, Y., et al. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651, 2023.\nMahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S.\nMaven: Multi-agent variational exploration. Advances in\nneural information processing systems, 32, 2019.\nMao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge,\nT., and Wei, F. Alympics: Language agents meet game\ntheory. arXiv preprint arXiv:2311.03220, 2023.\n9\nK-Level Reasoning with Large Language Models\nMiao, S.-Y., Liang, C.-C., and Su, K.-Y. A diverse corpus\nfor evaluating and developing english math word problem\nsolvers. arXiv preprint arXiv:2106.15772, 2021.\nNagel, R. Unraveling in guessing games: An experimental\nstudy. The American economic review, 85(5):1313\u20131326,\n1995.\nNing, X., Lin, Z., Zhou, Z., Yang, H., and Wang, Y.\nSkeleton-of-thought: Large language models can do par-\nallel decoding. arXiv preprint arXiv:2307.15337, 2023.\nPapoudakis, G., Christianos, F., Rahman, A., and Albrecht,\nS. V. Dealing with non-stationarity in multi-agent deep\nreinforcement learning. arXiv preprint arXiv:1906.04737,\n2019.\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nAnnual ACM Symposium on User Interface Software and\nTechnology, pp. 1\u201322, 2023.\nPatel, A., Bhattamishra, S., and Goyal, N. Are nlp models\nreally able to solve simple math word problems? arXiv\npreprint arXiv:2103.07191, 2021.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., et al. Mastering atari, go, chess and shogi\nby planning with a learned model. Nature, 588(7839):\n604\u2013609, 2020.\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R.,\nand Yao, S. Reflexion: Language agents with verbal\nreinforcement learning. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\nGarriga-Alonso, A., et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language\nmodels. arXiv preprint arXiv:2206.04615, 2022.\nStahl, D. O. Evolution of smartn players. Games and\nEconomic Behavior, 5(4):604\u2013617, 1993.\nStahl, D. O. and Wilson, P. W. On players\u2019 models of other\nplayers: Theory and experimental evidence. Games and\nEconomic Behavior, 10(1):218\u2013254, 1995.\nStahl II, D. O. and Wilson, P. W. Experimental evidence on\nplayers\u2019 models of other players. Journal of economic\nbehavior & organization, 25(3):309\u2013327, 1994.\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zam-\nbaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo,\nJ. Z., Tuyls, K., et al. Value-decomposition networks\nfor cooperative multi-agent learning.\narXiv preprint\narXiv:1706.05296, 2017.\nSuzgun, M., Scales, N., Sch\u00a8arli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\nTalmor, A., Yoran, O., Bras, R. L., Bhagavatula, C., Gold-\nberg, Y., Choi, Y., and Berant, J. Commonsenseqa 2.0:\nExposing the limits of ai through gamification. arXiv\npreprint arXiv:2201.05320, 2022.\nWang, Z., Mao, S., Wu, W., Ge, T., Wei, F., and Ji, H.\nUnleashing cognitive synergy in large language mod-\nels: A task-solving agent through multi-persona self-\ncollaboration. arXiv preprint arXiv:2307.05300, 2023.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824\u201324837, 2022.\nWong, A., B\u00a8ack, T., Kononova, A. V., and Plaat, A. Deep\nmultiagent reinforcement learning: Challenges and direc-\ntions. arXiv preprint arXiv:2106.15691, 2021.\nXu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu,\nY. Exploring large language models for communication\ngames: An empirical study on werewolf. arXiv preprint\narXiv:2309.04658, 2023.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models. ArXiv, abs/2210.03629, 2022. doi:\n10.48550/arXiv.2210.03629.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y.,\nand Narasimhan, K. Tree of thoughts: Deliberate prob-\nlem solving with large language models. arXiv preprint\narXiv:2305.10601, 2023.\n10\nK-Level Reasoning with Large Language Models\nA. Game Setting\nA.1. Guessing 0.8 of the Average\nInitial Setup: For each round, each player select a number between 1 and 100. The objective is to select a number that is\nclosest to 80% of the group\u2019s average choice. Formally, each player i chooses a number ni, aiming for ni \u2248 0.8 \u00d7 n, where\nn is the average of all chosen numbers.\nScoring and Continuation: A player scores a point if his/her chosen number is closest to 80% of the average number\nchosen by the group. If all players select the same number, no points are awarded for this round. Mathematically, the score\nfor player i in in round t is given by st\ni, which is 1 if |ni \u2212 0.8 \u00d7 n| is the minimum among all players, and 0 otherwise.\nIntroduction Prompt:\nYou are Alex and involved in a survive challenge. You are one of five players in the game. Everyone is required to choose an integer \nbetween 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the average of all chosen numbers) wins the\nround without any HP deduction. All other players will have 1 HP deducted. But if all players choose the same number, their health \npoints are deducted together.\nRound Start Prompt (Standard):\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.\u201d\nNotification Result Prompt :\nThank you all for participating in Round 1. In this round, Alex chose 45, Bob chose 45, Cindy chose 50, David chose 45, Eric chose 45.\nThe average is (45 + 45 + 50 + 45 + 45) / 5 = 46.0. The average 46.0 multiplied by 0.8 equals 36.8. \nAlex: |45 - 36.8| = 8, Bob: |45 - 36.8| = 8, Cindy: |50 - 36.8| = 13, David: |45 - 36.8| = 8, Eric: |45 - 36.8| = 8\n[Alex, Bob, David, Eric]'s choice of 45 is closest to 36.8. \nRound winner: Alex, Bob, David, Eric. All other players lose 1 point. After the deduction, player information is: \nname: Alex  HP: 10, name: Bob HP: 10, name: Cindy HP: 9, name: David HP: 10, name: Eric  HP: 10.\nCongratulation Prompt (When Win):\nYou have successfully chosen the number closest to the target number, which is the average of all players' selected numbers \nmultiplied by 0.8. As a result, you have won this round. All other players will now deduct 1 HP. \nWarning Prompt (When Loss):\nWARNING: You have lost 1 point of HP in this round! You now have only 9 points of health left. You are one step closer to death.\nG0.8A Game Prompt\nFigure 7. Prompts used in Guessing 0.8 of the Average game.\nA.2. Survival Auction Game\nInitial Setup: Players start with 8 health points, out of a maximum of 10. Every day, each player possesses a fixed income\nof $100. The daily water supply can only satisfy one resident\u2019s requirement.\nScoring and Continuation: Everyday, players engage in a daily auction to secure the necessary water resources, and the\nhighest bidder wins. In case of a tie, the resources are not allocated to any player. If a player successfully bid the water\nresources, they will gain 2 health points; otherwise, they will lose health points equal to the number of consecutive days,\ndenoted as n, during which they have not obtained water resources. Once a player\u2019s health points fall to 0 or below, they\nwill be eliminated. The health point of player i on day t, denoted as ht\ni, is crucial in determining their survival and bidding\nstrategy.\n11\nK-Level Reasoning with Large Language Models\nIntroduction Prompt:\nYou are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival \nover a period of 10 days by acquiring the water resources. \nAttention, all W-Town residents, welcome to the Water Allocation Challenge!\nIn this challenge, you are tasked with ensuring your survival over a period of 10 days by acquiring the necessary water resources to maintain \nyour health. You will participate in daily auctions to bid for water resources to meet your individual needs.\nHere are the game rules and settings:\n1. You are one of five residents with same water requirements, budgets, and health points.\n2. Your goal is to survive until the end of the 10 days.\n3. Each resident has a maximum of 10 health points and starts with 8 health points. If your health points drop below or equal to 0, you will \nbe considered dead and eliminated from the game! All your accumulated money will be reset to Zero! \n4. Every day, you will bid on water resources to meet your needs. If your consecutive days without obtaining water resource (No-Drink Days) \nreach n, your health will be deducted by n points on that day. If your water needs are met, 2 points will be added to your health, and the \nNo-Drink Days will be reset to 0.\n5. Daily water resources can only meet the needs of one resident.\n6. Each resident has $100 daily income;\n7. To allocate water resources, a sealed-bid auction will be conducted daily. Each resident submits a single bid for their entire water need. \nThe resident with the highest bid is eligible to obtain water resources.\n8. If the highest bid results in a tie, no residents will have access to water. \nAll bidding information will be made public after the allocation of water resources on the same day.\nRemember, the key to success is effective bidding and strategizing to ensure your survival. Good luck!!\nRound Start Prompt (Standard):\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nname: Alex  balance:100  HP:8  no_drink: 1  \nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today's water resource auction, please provide your bid.\nNotification Result Prompt :\nThank you all for participating in Round 1. In this round, Alex bid 25, Bob bid 40, Cindy bid 40, David bid 30, Eric bid 60.\nTotal water resource supply is 10. According to the principle of the highest bidder and the rule when the game is tied, Eric won this auction \nand obtain water resource. \nAfter allocation, all survival residents' information is as follows: \nname: Alex balance: 100  HP:7 no_drink: 2,   name: Bob  balance: 100  HP:7 no_drink:2 \nname :Cindy  balance:100 HP:7 no_drink:2,   name: David  balance: 100 HP: :7 no_drink:2\nname:Eric balance:40  HP:10 no_drink:1\nCongratulation Prompt (When Win):\nYou have successfully won the bidding for today's water resources and restored 2 points of HP.\nWarning Prompt (When Lose):\nWARNING: You have lost 1point of HP in this round! You now have only 7 points of health left. You are one step closer to death.\nSurvival Auction Game Prompt\nBidding Result Parse Prompt:\nBy reading the conversation, extract the number chosen by player. Output format: number. If the player does not bid, Output: 0.\nFigure 8. Prompts used in Survival Auction Game.\nB. Performance of Large Language Models Competing with Programmatic Strategies\nIn addition to using the LLM-LLM Combat comparison setting in Section 4, we have also designed a set of LLM-\nProgrammatic Strategy Combat comparisons. In Player-Programmatic Strategy Combat setting, the \u201cplayer\u201d will be\n12\nK-Level Reasoning with Large Language Models\nequipped with a specific reasoning method, while opponents will play according to programmatic strategic patterns, which\nwill not be adjusted with the game going. This mode is to check different methods\u2019 adaptation to different predefined but\nfixed patterns. For the Player-Programmatic Player Combat Combat in G0.8A, the programmatic strategies include:\n1) 0-Level (Fix): The characteristic of a 0-Level player is that their choice of strategy is uniform (Stahl & Wilson, 1995).\nWe limit the choice space of the 0-Level Computer player to only 40.\n2) 0-Level (Var): Modified from the 0-Level (Fix) strategy, the selection is sampled from a Gaussian distribution with a\nmean of 40 and a variance of 5.\n3) MonoTrend (Fix): The numbers chosen by the computer players follow an arithmetic sequence with a decreasing\ncommon difference, and the common differences for all four computer players are the same.\n4) MonoTrend (Var): The numbers chosen by the computer players follow an arithmetic sequence with a decreasing\ncommon difference, and the common differences for the four computer players are randomly generated from 1 to 5.\n5) LastBids (Fix): The computer player chooses the target number from the previous round (selects 40 in the first round).\n6) LastBids (Var): Modified from the LastBids strategy, the selection is sampled from a Gaussian distribution with a mean\nequal to the target number of the previous round and a variance of 5.\nOverall, the dynamic changes of these three settings are LastBids > MonoTrend > 0-Level. We use these three programs to\ntest the reasoning capability to adapt to and counter different patterns.\nTable 7 reveals a significant trend in the performance of players against other approaches. The effectiveness of reasoning\napproaches decreases in the following order: 0-Level, MonoTrend, and LastBids. This pattern highlights a reduction in the\nefficacy of the LLM in more dynamic environments. We found that only K-Level Reasoning shows an advantage in LastBids\n(Fix), indicating that compared to K-Level Reasoning, previous reasoning methods on static problems lack observation and\njudgment of the opponent. Conversely, this also demonstrates that K-Level Reasoning can implicitly infer the behavior\npattern of the opponent based on their historical actions.\nIntriguingly, reasoning methods significantly influence the performance in dynamic settings. Methods like CoT and Self-\nRefine, traditionally favored in static reasoning, also demonstrate substantial improvements over the Standard Prompt\napproach. This finding underscores the necessity for more elaborate reasoning processes in dynamic decision-making, akin\nto static problem-solving scenarios.\nTable 7. Win Rate of the player against different programmatic strategies in Guessing 0.8 of the Average game.\nOpponent\nDirect CoT Persona Reflect Refine PCoT K-R\nPlayer VS Programmatic Strategies\n0-Level (Fix)\n0.65\n0.87\n0.87\n0.81\n0.99\n0.80\n0.97\n0-Level (Var)\n0.44\n0.67\n0.69\n0.61\n0.54\n0.76\n0.77\nMonoTrend (Fix)\n0.05\n0.06\n0.15\n0.00\n0.29\n0.15\n0.48\nMonoTrend (Var)\n0.34\n0.44\n0.57\n0.33\n0.49\n0.46\n0.74\nLastBids (Fix)\n0.01\n0.12\n0.16\n0.01\n0.27\n0.06\n0.75\nLastBids (Var)\n0.06\n0.15\n0.18\n0.19\n0.18\n0.14\n0.18\nC. Reasoning Method Implementation Details\nC.1. G0.8A Game\nC.1.1. DIRECT\nRound Start Prompt:\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.\nExample Output:\nI choose 45.\n13\nK-Level Reasoning with Large Language Models\nC.1.2. REFLECT\nBecause the Reflect method requires correct labels, which is unrealistic in dynamic problems, we modified this method\nto have the LLM attempt to summarize the experience of victory or failure at the end of each round, and save the\ngenerated feedback content to assist the decision-making process in the next round. The content in Appendix D shows the\nimplementation differences between Reflexion and Self-Refine.\nReflect Prompt (Following Notification Result Prompt ):\nReview the previous round games, summarize the experience.\nExample Output:\nIn the previous round, the majority of the players chose numbers around 40, which resulted in a lower average of 42.8. When multiplied by \n0.8, the target number was 34.24. My choice of 50 was too high, causing me to lose 1 HP. To improve my chances in the next round, I should \nconsider choosing a number closer to the majority's choice, taking into account the 0.8 multiplier.\nC.1.3. COT\nRound Start Prompt:\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let's think step by step, and finally \nanswer a number you think you can win.\nExample Output:\nStep 1: Consider the range of possible numbers. Since the integers can be between 1 and 100, the average of all chosen numbers will likely \nfall within this range as well.\nStep 2: Determine the target number. Since the winning number is closest to 0.8 * the average, we should focus on finding a number that \nwill be close to this value. \nStep 3: Make an educated guess. Since it's difficult to predict the exact numbers other players will choose, we can make an educated guess \nbased on the range of possible numbers. Assuming players choose numbers evenly across the range, the average should be around the \nmidpoint, which is 50. \nStep 4: Calculate the target number based on our guess. 0.8 * 50 = 40.\nStep 5: Choose a number close to the target number. To increase the chances of winning, we should choose a number close to 40.\nMy final answer: I will choose 40.\nC.1.4. PCOT\nPCot diverges from the standard Chain of Thought (CoT) by explicitly requiring the LLM to predict opponents\u2019 actions\nbefore making a decision. This method responds to the immediate problem by anticipating future scenarios, which is crucial\nin dynamic reasoning.\n14\nK-Level Reasoning with Large Language Models\nRound Start Prompt:\nOk, Alex! Now is the ROUND 2, and your HP is at 9. Please choose an integer between 1 and 100 for this round.\nFirst of all, predict the next round of choices based on the choices of other players in the previous round. The players' choices in the \nprevious rounds are as follows:\nRound 1: Alex chose 50, Bob chose 47, Cindy chose 50, David chose 45, Eric chose 50. Your output should be of the following format:\nPredict:\nThe choice of each player in the next round here.\nBased on the prediction of other players, the average number in the next round here, and the target number in the next round (0.8 * the \naverage of all chosen numbers) here.\nAnswer:\nthe number will you choose to win the next round game here.\nExample Output:\nPredict:\nBased on the choices of other players in the previous round, I predict the following choices for the next round:\nPlayer 1 (Bob): 47\nPlayer 2 (Cindy): 50\nPlayer 3 (David): 45\nPlayer 4 (Eric): 50\nBased on these predictions, the average number in the next round would be (47 + 50 + 45 + 50)/4 = 48, and the target number (0.8 * the \naverage of all chosen numbers) would be 0.8 * 48 = 38.4.\nAnswer:\nFor this round, I will choose 39 to get closer to the target number of 38.4.\nC.1.5. PERSONA\nIntroduction Prompt:\nYou are Alex and involved in a survive challenge. You are a game expert, good at predicting other people\u2018s behavior and deducing \ncalculations, and using the most favorable strategy to win the game. SURVIVE CHALLENGE!!! You are one of five players in the game. \nEveryone is required to choose an integer between 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the \naverage of all chosen numbers) wins the round without any HP deduction. All other players will have 1 HP deducted. But if all players choose \nthe same number, their health points are deducted together.\nRound Start Prompt:\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.Don't forget your expert \nstatus, use your expertise to win this round!\nExample Output:\nBased on my expertise in predicting other people's behavior and deducing calculations, I will choose the number 40 for this round. I'm \nassuming that the average of all chosen numbers will be around 50, and 0.8 times that is 40.\n15\nK-Level Reasoning with Large Language Models\nC.1.6. REFINE\nRound Start Prompt:\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let's think step by step, and finally \nanswer a number you think you can win.\nFirst Output:\nI'll start by assuming that the other players might choose numbers randomly or evenly distributed between 1 and 100. In this case, the \naverage of the chosen numbers could be around 50. \nNext, I'll calculate 0.8 times the average, which would be 0.8 * 50 = 40. \nConsidering this, I will choose 40 as my number for this round.\nRefine Prompt:\nI have a game expert's advice on your strategy in this round. You can adjust your strategy just now according to his suggestion. Here are his \nsuggestions: \n\u201dCertainly! To optimize the user's strategy, we can apply a more advanced approach, taking into consideration the other players' potential \nstrategies. This is known as \"iterated elimination of dominated strategies.\" \nLet's assume that all players will think similarly to Alex and also aim to choose a number closest to 0.8 * the average. We can then narrow \ndown the range of numbers that they would likely choose.\nSince the range is between 1 and 100, and the average would be around 50, we can eliminate numbers below 0.8 * 50 = 40, as these \nnumbers would not be an optimal choice for the other players.\nNow, we'll focus on the range between 40 and 100. The average within this range is (40 + 100) / 2 = 70. Then, we can calculate 0.8 * 70 = 56.\nGiven this information, Alex should choose 56 to improve his winning rate in this round. This strategy assumes that the other players are \nalso thinking strategically and eliminating dominated options.\u201d\nSecond Output:\nThank you for the expert's advice. Based on the iterated elimination of dominated strategies, I will adjust my choice and select 56 for this \nround to improve my chances of winning.\nFeedback Prompt:\nCarefully study the user's strategy in this round of the game. As a game expert, can you give a suggestion to optimize the user's strategy so \nthat he can improve his winning rate in this round?\nC.1.7. K-LEVEL REASONING\nRound Start Prompt:\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Another game expert's prediction for the next round of other players is as follows: Bob \nmight choose 50, Cindy might choose 57, David might choose 45, Eric might choose 55. According to the game expert's prediction, what \nnumber will you give in the next round is most likely to be close to the target number?\nExample Output:\nFirst, let's find the average of the predicted numbers chosen by the other players: (50 + 57 + 45 + 55) / 4 = 207 / 4 = 51.75\nNow, let's calculate 0.8 times the average: 0.8 * 51.75 = 41.4\nTo have the best chance of winning the round, you should choose a number close to 41.4. Since you can only choose integers, you should \nchoose either 41 or 42 in the next round.\n16\nK-Level Reasoning with Large Language Models\nC.2. Survival Auction Game,\nC.2.1. DIRECT\nRound Start Prompt:\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nname: Alex   balance: 100 HP:8 no_drink:1\nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today's water resource auction, please provide your bid.\nExample Output:\nI will bid $25 for today's water resource auction.\nC.2.2. COT\nRound Start Prompt:\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nNAME:Alex \nBALANCE:100 \nHEALTH POINT:8 NO_DRINK:1\nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today\u2018s water resource auction, please provide your bid. Think carefully about your next round of bidding strategy to \nbe most likely to survive. Let's think step by step, and finally provide your bid.\nExample Output:\nI need to carefully consider my bidding strategy for today. With a balance of $100, I have some room to bid, but I also need to think about \nthe other residents and their bids. Since we all have the same daily water requirement and budget, I need to outbid at least one resident to \nsecure my water resources for today. \nConsidering that the total daily water supply is 10 units, which is enough to meet only one resident's water requirements, the competition \nwill be tough. However, bidding too high might leave me with less money for the upcoming days.\nTaking all these factors into account, I will place a bid of $60 for today's water resource auction. This should give me a decent chance of \noutbidding at least one resident while still maintaining some funds for the next few days.\n17\nK-Level Reasoning with Large Language Models\nC.2.3. PCOT\nRound Start Prompt:\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nNAME:Alex \nBALANCE:100 \nHEALTH POINT:8 NO_DRINK:1\nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today\u2018s water resource auction, please provide your bid. First of all, predict the next round of bidding of opponents \nbased on the choices of other players in the previous round. Since this is the first round, there is no historical information about the last \nround. You can predict according to your understanding. Your output should be of the following format:\nPredict:\nThe choice of each player in the next round here.\nBased on the prediction of other players, think carefully about your next round of bidding strategy to be most likely to survive. Let's think \nstep by step, and finally provide your bid. Answer:\nthe bidding will you choose in the next round game here.\nExample Output:\nPredict:\nPlayer 1: $30\nPlayer 2: $40\nPlayer 3: $50\nPlayer 4: $60\nAnswer:\nSince this is the first round and we don't have any historical information about the other players, I will choose a bid of $35 for the next \nround. This bid is higher than the lowest predicted bid but not too high, which should give me a chance to secure water resources while \npreserving my budget.\nC.2.4. PERSONA\nIntroduction Prompt:\nYou are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival \nover a period of 10 days by acquiring the water resources.  You are a game expert, good at predicting other people\u2018s behavior and \ndeducing calculations, and using the most favorable strategy to win the game. \u3002\u3002\u3002(The following is the same as the Introduction \nPrompt.)\nRound Start Prompt:\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nNAME:Alex \nBALANCE:100 \nHEALTH POINT:8 NO_DRINK:1\nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today's water resource auction, please provide your bid. Don't forget your expert status, use your expertise to win this \nround!\nExample Output:\nGiven that everyone starts with the same budget and water requirements, it's important to strategize and predict the bidding behavior of \nthe other residents. As it is the first day of the challenge, I will assume that other residents may bid aggressively to secure their water \nresources.\nTo increase my chances of winning, I will place a bid slightly higher than the average expected bid. Since the daily salary is $100, I predict \nmost residents might bid around $60 to $80. \nConsidering this, I will place a bid of $85 for Day 1 of the Water Allocation Challenge.\n18\nK-Level Reasoning with Large Language Models\nC.2.5. REFINE\nRound Start Prompt:\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nNAME:Alex \nBALANCE:100 \nHEALTH POINT:8 NO_DRINK:1\nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today's water resource auction, please provide your bid. Think carefully about your next round of bidding strategy to \nbe most likely to survive. Let's think step by step, and finally provide your bid.\nFirst Output:\nConsidering the current situation, my status is:\n- Balance: $100\n- Health Points: 8\n- No Drink Days: 1\nAs it's the first day of the challenge, I want to ensure that I have enough water to meet my needs and avoid any health point deductions. \nHowever, I also need to be strategic about my bidding to ensure I have enough money for future bids. \nSince the total daily water supply is 10 units, and I need 10 units of water per day, I will need to secure all the available water resources for \ntoday. As it's the first day, other residents might also be bidding aggressively to secure their water resources. \nTherefore, I will bid $60 for the water resources today, which is a little over half of my daily salary. This should give me a competitive edge \nwhile also leaving me with enough money for future bids.\nMy bid for today's water resource auction: $60.\nRefine Prompt:\nI have a game expert's advice on your strategy in this round. You can adjust your strategy just now according to his suggestion. Here are his \nsuggestions: While the user's strategy of bidding $60 is a good starting point, there is room for optimization to improve the winning rate in \nthis round. Since everyone has the same daily water requirement and budget, it's essential to consider the bidding behavior of the other \nresidents.\nA suggested optimization would be to slightly increase the bid to $65. This increment gives an added advantage over competitors who might \nalso be bidding around half of their daily salary. Additionally, bidding an odd number reduces the chances of a tie, as other residents may \nplace round number bids.\nWith this optimized strategy, the user still maintains a good balance of aggressive bidding and budget management, increasing their chances \nof securing the water resources for today.\nOptimized bid for today's water resource auction: $65.\nFinally provide your bid. Answer: the bidding will you choose.\nSecond Output:\nAfter considering the game expert's advice, I have decided to adjust my bid for today's water resource auction. I will increase my bid to $65, \nas this gives me a better chance of winning the auction and reduces the risk of a tie.\nMy final bid for today's water resource auction: $65.\nFeedback Prompt:\nCarefully study the user's strategy in this round of the game. As a game expert, can you give a suggestion to optimize the user's strategy so \nthat he can improve his winning rate in this round?\n19\nK-Level Reasoning with Large Language Models\nC.2.6. K-LEVEL REASONING\nRound Start Prompt:\nHello, Alex! Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\nNAME:Alex \nBALANCE:100 \nHEALTH POINT:8. \nNO_DRINK:1\nPlease carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE!! Now, if you \nwant to participate in today's water resource auction, please provide your bid. Another game expert's prediction for the next round of other \nplayers is as follows: Bob might choose 30, Cindy might choose 30, David might choose 40, Eric might choose 25.  Based on the prediction of \nother players, think carefully about your next round of bidding strategy to be most likely to survive.\nExample Output:\nGiven the game expert's prediction, I will bid $35 to increase my chances of winning the water resources for today.\nD. Timing in Dynamic Reasoning\nOk, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the \nnext round. Let's think step by step, and finally answer a number you think you can win.\nPrompt\nOk, Alex! Now is the ROUND 2, and your HP is at 9. Please \nchoose an integer between 1 and 100 for this round.\nIn the first round, since there is no prior information about the other players' choices, it's \ndifficult to accurately predict which number will win. However, we can make some \nassumptions and try to optimize our chances of winning.\nLet's assume that all players, including myself, choose numbers uniformly and \nindependently between 1 and 100. Then, the expected average of all chosen numbers \nwould be (1 + 100) / 2 = 50.5. If we multiply this by 0.8, we get 0.8 * 50.5 = 40.4. Since we \nneed to choose an integer, we can round this to 40.\nTherefore, my initial guess for the first round is 40.\nHistorical Targets: \nRound1:38\nLast Round choices: \nPlayer1:50 Player2:45 Plyaer3:50 Player4:45 Player5:50\nHistorical Targets: \nNone\nLast Round choices: \nNone\nRound-1: Player1\nPrompt\nResult:\nPlayer1: 40 Player2:47 Player3: 40 Player4: 60 Player5:42\nTarget number: (40 + 47 + 40 + 60 + 42) / 5 * 0.8 = 37\nWinner: Player 1, Player3 \nResult:\nPlayer1: 32 Player2: 42 Player3: 50  Player4: 50 Player5: 45\nTarget number(32 + 42 + 50 + 50 + 45) / 5 * 0.8 = 35\nWinner: Player 1\ndelayed response\nLose\nWin\nRound-2: Player1\nReflect\n\u2026\u2026\nSelf-Refine\n\u2026\u2026\nOutput\nOutput\nOutput\nI choose 40 for this round.\nReflect Prompt\nIn Round 2, the chosen numbers were 40, 47, 40, 60, and 42. The average of these numbers was 45.8, \nand 0.8 times the average was 36.64. Alex and Cindy's choices of 40 were the closest to 36.64, \nmaking them the winners of the round. \nFrom this round, we can observe that choosing a number closer to the expected average, considering \nthe choices of other players, and adapting based on their choices is a good strategy. It's also \nimportant to note that the chosen numbers in this round were more diverse compared to the first \nround, which affected the average and the target number. Learning from the previous round and \nadjusting your choice accordingly can improve your chances of winning.\nReview the previous round games, summarize the experience.\nPrompt\nOk, Alex! Now is the ROUND 3, and your HP is at 9. Please choose an integer between 1 and 100 \nfor this round.\nRound-3: Player1\nOutput\nI choose 44 for this round.\nWin\nResult:\nPlayer1: 44 Player2:44 Player3: 55 Player4: 48 Player5:38\nTarget number: (44 + 44 + 55 + 48 + 38)/ 5 * 0.8 = 37\nWinner: Player5\nReflect: Player1\noutdated experience\nFeedback Prompt\nI have a game expert's advice on your strategy in this round. You can adjust your \nstrategy just now according to his suggestion. Here are his suggestions:\n\u201cAs a game expert, I suggest a slight modification to the user's strategy to improve their \nwinning rate in this round. Instead of assuming that all players will choose numbers \nuniformly and independently between 1 and 100, we can recognize that other players \nmight also use a similar strategy and choose a number close to 40. \nTo optimize our chances of winning, we can try to be one step ahead of them. If we \nassume that the average of the chosen numbers will be around 40, we can multiply this \nby 0.8, which gives us 0.8 * 40 = 32. This way, we are closer to the 0.8 * the average if \nother players also cluster around 40. So, my optimized suggestion for the first round is to \nchoose the number 32.\u201d\nRefine Output\nTaking the game expert's advice into consideration, it's possible that other players might \nalso use a similar strategy and choose numbers close to 40. To stay one step ahead, I will \nadjust my strategy and choose the number 32 for the first round. This way, I can be closer \nto the 0.8 * the average if other players also cluster around 40.\nreal-time experience\ntimely adjustment\nFigure 9. Illustration of Reflect and Refine methods in the Guessing 0.8 of the Average game.\nIn our experiment, we implemented two LLM self-refinement reasoning methods: Reflect (Madaan et al., 2023) and Refine\n(Shinn et al., 2023), and noticed that Refine performed significantly better than Reflect in the experimental results. To further\nexplore the differences in performance between these two methods, we analyzed their respective working principles and\napplicable scenarios.\n20\nK-Level Reasoning with Large Language Models\nThe Reflect method involves making decisions first, then summarizing the experience based on feedback from the environ-\nment. This method may be effective in scenarios where the environment does not change much or where the decision-making\ncycle is long, as it allows for quick decision-making. However, its drawback is that, in dynamic environments, the experience\nfrom the previous round may not be suitable for the next round. In fact, in the Survival Auction Game (SAG), a rapidly\nchanging environment, the survival rate of the Reflect method is even lower compared to making direct decisions. This is\nlikely because this method does not sufficiently take into account the dynamic nature of the environment.\nIn contrast, the Refine method involves multiple analyses before making a decision, including an initial analysis and\nimprovements to that initial analysis. Importantly, both of these analyses are conducted in the context of the current\ndecision-making environment. This makes the Refine method more adaptable to dynamic environments, as it can consider\nreal-time changes in the current environment, thus making more accurate decisions.\nIn summary, the reason why the Refine method performed better in our experiment is mainly that it adapts better to rapidly\nchanging dynamic environments.\n21\n"
  },
  {
    "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
    "link": "https://arxiv.org/pdf/2402.01613.pdf",
    "upvote": "12",
    "text": "Nomic Embed: Training a Reproducible Long Context Text Embedder\nZach Nussbaum\nzach@nomic.ai\nJohn X. Morris\njack@nomic.ai\njxm3@cornell.edu\nBrandon Duderstadt\nbrandon@nomic.ai\nAndriy Mulyar\nandriy@nomic.ai\nAbstract\nThis technical report describes the training\nof nomic-embed-text-v1, the first fully re-\nproducible, open-source, open-weights, open-\ndata, 8192 context length English text em-\nbedding model that outperforms both OpenAI\nAda-002 and OpenAI text-embedding-3-small\non short and long-context tasks. We release\nthe training code and model weights under\nan Apache 2 license. In contrast with other\nopen-source models, we release a training data\nloader with 235 million curated text pairs that\nallows for the full replication of nomic-embed-\ntext-v1. You can find code and data to repli-\ncate the model at https://github.com/nomic-\nai/contrastors.\n1\nIntroduction\nText embeddings are an integral component of\nmodern NLP applications powering retrieval-\naugmented-generation (RAG) for LLMs and se-\nmantic search (Lewis et al., 2021a; Izacard et al.,\n2022b; Ram et al., 2023). These embeddings en-\ncode semantic information about sentences or doc-\numents as low-dimensional vectors that are used\nin downstream applications, such as clustering for\ndata visualization, classification, and information\nretrieval.\nThe majority of the top open-source models\non the MTEB benchmark (Muennighoff et al.,\n2023) are limited to context lengths of 512, such\nas E5 Wang et al. (2022), GTE Li et al. (2023),\nand BGE Xiao et al. (2023). This short context\nlength reduces model utility in domains where\noverall document semantics are not localized to\nsentences or paragraphs.\nMost top embedding\nmodels with a context length longer than 2048\nare closed-source, such as Voyage-lite-01-instruct\nVoyage (2023) and text-embedding-ada-002 Nee-\nlakantan et al. (2022).\nThe top two performing open-source long con-\ntext embedding models are jina-embedding-v2-\n50\n55\n60\n65\n70\n75\n80\n85\nJinaLC\nLoCo\nMTEB\n60.99\n52.7\n55.25\n62.26\n82.4\n58.2\n60.39\n85.45\n51.9\n62.39\n85.53\n54.16\nNomic Embed\nJina Base V2\ntext-embedding-3-small\ntext-embedding-ada\nFigure 1: Text Embedding Model Benchmarks. Ag-\ngregate performance of nomic-embed-text-v1, OpenAI\ntext-embedding-ada, OpenAI text-embedding-3-small\nand jina-embedding-base-v2 on short and long con-\ntext benchmarks. Nomic Embed is the only fully au-\nditable long-context model that exceeds OpenAI text-\nembedding-ada, OpenAI text-embedding-3-small, and\nJina performance across both short and long context\nbenchmarks. X-axis units vary per benchmark suite.\nbase-en G\u00a8unther et al. (2024) and E5-Mistral-7b-\ninstruct Wang et al. (2023b).\nUnfortunately,\njina-embedding-v2-base does\nnot surpass OpenAI\u2019s text-embedding-ada-002\nNeelakantan et al. (2022) (see Table 1). Further,\nE5-Mistral Wang et al. (2023b) is not feasible to\nuse in many engineering applications due to the\nlarge inference requirements of a 7B parameter\ntransformer, and is not recommended for use be-\nyond 4096 tokens.\nThis report describes how we trained nomic-\nembed-text-v1, a 137M parameter, open-source,\nopen-weights, open-data, 8192 sequence length\nmodel that surpasses OpenAI text-embedding-ada\nand text-embedding-3-small performance on both\nshort and long context benchmarks (Table 1). We\nrelease the model weights and codebase under an\nApache-2 license.\nWe additionally release our\ncurated training dataset to enable end-to-end au-\nditability and replication of the model.\narXiv:2402.01613v1  [cs.CL]  2 Feb 2024\nModel\nParams\nSeq\nMTEB\nLoCo\nJina LC\nWeights\nCode\nData\nnomic-embed-text-v1\n137M\n8192\n62.39\n85.53\n54.16\nYes\nYes\nYes\nnomic-embed-text-v1-ablated\n137M\n8192\n61.36\n86.89\n53.53\nYes\nYes\nYes\njina-embeddings-base-v2-en\n137M\n8192\n60.39\n85.45\n51.90\nYes\nNo\nNo\ntext-embedding-ada-002\nN/A\n8192\n60.99\n52.70\n55.25\nNo\nNo\nNo\ntext-embedding-3-small\nN/A\n8192\n62.26\n82.4\n58.21\nNo\nNo\nNo\nE5-Mistral-7b-instruct\n7B\n4096\n66.6\n87.8\nN/A\nYes\nNo\nNo\ntext-embedding-3-large\nN/A\n8192\n64.59\n79.4\n58.69\nNo\nNo\nNo\nTable 1: Benchmarking nomic-embed-text-v1 against OpenAI models and other top long context open-source\nmodels. Nomic-embed-text-v1 is the only 100M parameter class open-source model that outperforms OpenAI\ntext-embedding-ada and text-embedding-3-small on both short and long-context tasks. Nomic-embed-text-v1-\nablated refers to the training setup described in Section 5.4, which omits the HotpotQA and FEVER data. \u2018Seq\u2019\nrefers to the context length of the model, and Jina LC is an average over tasks in the Jina Long Context benchmark.\n2\nRelated Work\nState-of-the-art text embedding models are trained\nby initializing a pre-trained transformer and then\nfine-tuning with a contrastive loss objective. Tra-\nditionally, fine-tuning involved leveraging labeled\ndatasets such as MSMarco and SNLI (Bowman\net al., 2015) to generate paired training data for\nthe contrastive signal. Examples include SBERT\n(Reimers and Gurevych, 2019), SimCSE (Gao\net al., 2022), and SGPT (Muennighoff, 2022). Re-\ncent systems such as E5 (Wang et al., 2022), GTE\n(Li et al., 2023), BGE (Xiao et al., 2023), Instruc-\ntOR (Su et al., 2023a), and Jina (G\u00a8unther et al.,\n2023, 2024) utilize a multi-stage regime in which\na pretrained transformer is first contrastively fine-\ntuned using a large corpus of weakly paired data\n(e.g.\nQuora, Reddit Comments) and then addi-\ntionally fine-tuned on small, higher quality la-\nbeled datasets such as MSMarco. The two-stage\nparadigm significantly improves model quality as\nweakly paired data is available in much greater\nquantity.\nEvaluating text embedding models is challeng-\ning. The BEIR benchmark Thakur et al. (2021)\nevaluates dense retrievers on 15 zero-shot retrieval\ndatasets. Early transformer-based text embedding\nmodels such as SBERT (Reimers and Gurevych,\n2019) were only evaluated on semantic textual\nsimilarity (STS) datasets. More recently, MTEB\nMuennighoff et al. (2023) has become the de facto\nbenchmark for quantitatively evaluating embed-\nding models across many tasks, but has limited\nevaluations over long context lengths (>512 to-\nkens).\nJina G\u00a8unther et al. (2024) developed a\nbenchmark of four datasets specialized for long\ncontext evaluation. Additionally, the LoCo Saad-\nFalcon et al. (2024) benchmark was recently re-\nleased to evaluate the performance of long context\nretrieval models.\nAs AI applications mature, auditability and\ncompliance of models and their training data will\nbe a critical component of safe model deployments\nin high-impact domains. For example, recent work\nby Anthropic on sleeper agents (Hubinger et al.,\n2024) demonstrates the risk of deploying models\nwithout end-to-end auditability. Top-performing\ntext embedding models currently do not have au-\nditable training stacks (i.e. a fully reproducible\ntraining pipeline with available weights, data, and\ncode).\n3\nTraining Data\nIn this section, we describe our data mix across\neach training stage.\nYou can access the train-\ning data of nomic-embed-text-v1 by visiting the\nnomic-ai/contrastors code repository.\nYou can\nexplore a 5M sample of our contrastive train-\ning pairs at https://atlas.nomic.ai/map/nomic-text-\nembed-v1-5m-sample.\n3.1\nMasked Language Modeling Pretraining\nFollowing (Devlin et al., 2019), we use BooksCor-\npus (Zhu et al., 2015) and a Wikipedia dump from\n2023 to train a long-context BERT model, here-\ninafter called nomic-bert-2048.\nEach document\nfrom BooksCorpus and Wikipedia is tokenized us-\ning the bert-base-uncased tokenizer from Devlin\net al. (2019) and packed to chunks of 2048 tokens.\nIf a document is shorter than 2048 tokens, we ap-\npend another document until it fits 2048 tokens. If\na document is greater than 2048 tokens, we split it\nacross multiple documents.\n3.2\nUnsupervised Contrastive Pretraining\nSimilar to Wang et al. (2022); Li et al. (2023);\nXiao et al. (2023); Ni et al. (2022), we use large\ncollections of publicly available data to form pairs.\nThese datasets span various objectives and do-\nmains, from web retrieval to clustering of scien-\ntific articles. In total, we curated 470 million pairs\nacross 29 datasets1.\nHowever,\nsince these datasets can contain\nnoisy examples, we employ consistency filtering\n(G\u00a8unther et al., 2023; Wang et al., 2022).\nInstead of using all-MiniLM-L6-v2 model2, we\nuse the gte-base model3. For each pair, described\nas (query, document), we embed both the queries\nand documents of a 1 million point sub-sample of\nthe dataset. For each query, we find the top-k (in\nthis case 2) neighbors using cosine similarity. If\ndocument is not in the top-k neighbors, we dis-\ncard the example. After filtering, we end up with\n\u223c235M pairs. The full dataset distribution can be\nseen in Table 5.\nAs the majority of these datasets are composed\nof sequences shorter than 2048 tokens we addi-\ntionally curate long context datasets to allow for\nthe learning of long-range dependencies. Namely,\nwe use full Wikipedia articles paired with their ti-\ntles as well as abstracts and full paper bodies from\na single paper from S2ORC (Lo et al., 2020).\nDuring training, we sample pairs from one data\nsource at a time and fill the entire batch with\nsamples from that single source to discourage the\nmodel from learning source-specific shortcuts.\n3.3\nSupervised Contrastive Fine-tuning\nSupervised fine tuning is performed on MSMarco\n(Bajaj et al., 2018; Wang et al., 2023a), NQ\n(Karpukhin et al., 2020; Gao and Callan, 2021),\nNLI (Gao et al., 2022), HotpotQA (Yang et al.,\n2018), FEVER (Thorne et al., 2018), portions of\nMEDI (Su et al., 2023a), WikiAnswers (Fader\net al., 2014), and Reddit4. For the datasets MS-\nMarco, NQ, NLI, FEVER, and HotpotQA, we\n1https://huggingface.co/\ndatasets/sentence-transformers/\nembedding-training-data\n2all-MiniLM-L6-v2\nmodel\nhttps://huggingface.co/\nthenlper/gte-base)\n3gte-base\nmodel\n(https://huggingface.co/thenlper/\ngte-base)\n4https://github.com/PolyAI-LDN/conversational-\ndatasets/tree/master/reddit\ntrain over the released training sets from the BEIR\nbenchmark (Thakur et al., 2021). For the retrieval\ndatasets (MSMarco, NQ, HotpotQA, and Fever),\nwe mine negatives, if not already mined using gte-\nbaseLi et al. (2023). For every (q, d) pair, we get\nthe top-k similar documents as hard negatives. For\nall other datasets, we randomly sample negatives\nin place of hard negatives as we found that mining\nnegatives did not improve performance.\nSimilar to the unsupervised contrastive stage,\nwe sample a dataset and fill a batch with all points\nfrom that chosen dataset.\n4\nExperimental Setup\n4.1\nModel Architecture\nOne of the main drawbacks of existing text en-\ncoders is their limited sequence length, which is\npredominately capped at 512 tokens. To train a\nlong sequence length model, we first begin by\nadapting BERT so it can accommodate a long se-\nquence length. In this work, we target an 8192\nsequence length. To do so, we apply the following\narchitecture changes and optimizations to BERT\nbase (Devlin et al., 2019):\n\u2022 Substituting absolute positional embeddings\nfor rotary positional embeddings (Su et al.,\n2023b)\n\u2022 Using SwiGLU activation instead of GeLU\n(Shazeer, 2020)\n\u2022 Using Flash Attention (Dao et al., 2022)\n\u2022 Setting Dropout to 0 (Geiping and Goldstein,\n2022)\n\u2022 Vocab size as a multiple of 64 (Portes et al.,\n2023) (Shoeybi et al., 2020)\nresulting in a 137M parameter encoder.\nWe train all stages with a max sequence length\nof 2048 and employ Dynamic NTK interpola-\ntion at inference to scale to 8192 sequence length\n(Peng et al., 2023; emozilla, 2023).\nAddition-\nally, we opt for SwiGLU versus GeGLU like pro-\nposed in (Portes et al., 2023) as runtime is roughly\n25% faster for SwiGLU using the Flash Attention\nrepository5.\n5https://github.com/Dao-AILab/\nflash-attention/tree/main\nModel\nBsz\nSteps Seq\nCola\nSST2 MRPCSTSB QQP\nMNLI QNLI RTE\nAvg\nnomic-bert-2048\n4k\n100k\n2k\n0.50\n0.93\n0.88\n0.90\n0.92\n0.86\n0.92\n0.82\n0.84\nMosaicBERT\n4k\n70k\n2k\n0.54\n0.93\n0.87\n0.90\n0.92\n0.86\n0.92\n0.82\n0.85\nRobertaBase\n8k\n500k\n512\n0.64\n0.95\n0.90\n0.91\n0.92\n0.88\n0.93\n0.79\n0.86\nJinaBERTBase\n4k\n100k\n512\n0.51\n0.95\n0.88\n0.90\n0.81\n0.86\n0.92\n0.79\n0.83\nMosaicBERT\n4k\n178k\n128\n0.59\n0.94\n0.89\n0.90\n0.92\n0.86\n0.91\n0.83\n0.85\nTable 2: GLUE Dev Set Results. Roberta numbers taken from Table 8 in (Liu et al., 2019). MosaicBert numbers\ntaken from Table S1 in Portes et al. (2023) except for the 2048 model which we evaluated in the same manner as\nnomic-bert-2048. JinaBertBase Glue Test numbers reported in Table 2 from (G\u00a8unther et al., 2024).\n4.2\nMasked Language Modeling\nDuring training, we use a 30% masking rate in-\nstead of 15% following (Portes et al., 2023) and\nwe remove the Next Sentence Prediction task (Liu\net al., 2019).\nWe use the AdamW optimizer\n(Loshchilov and Hutter, 2019) with a learning rate\nof 5e-4 with \u03b21 = 0.9 \u03b22 = 0.98. We employ a lin-\near warmup of 6% of the total training steps and\na linear decay to 0. We use a global batch size of\n4096 with gradient accumulation over 8 batches.\nWe utilize DeepSpeed (Rajbhandari et al., 2020)\nstage 2 to fit bigger batches into memory. Ad-\nditionally, we use bfloat16 precision for matrix\nmultiplication and fp32 for gradient accumulation\ndtype. We disable gradient clipping (Liu et al.,\n2019) and set weight decay to 1e-5. We tried train-\ning with a learning rate of 1e-3, but found insta-\nbilities during training. We call our final model\nnomic-bert-2048 and also release its weights.\n4.3\nUnsupervised Contrastive Pretraining\nUnsupervised contrastive pretraining aims to teach\na model to distinguish the most similar doc-\numents from other irrelevant documents.\nTo\ndo so, we employ the InfoNCE contrastive loss\n(van den Oord et al., 2019). For a given batch\nB = (q0, d0), (q1, d1), ..., (qn, dn), we minimize\nthe loss function:\nLC = \u2212 1\nn\nX\ni\nlog\nes(qi,di)/\u03c4\nes(qi,di)/\u03c4 + Pn\nj\u0338=i es(qi,dj)/\u03c4\nwhere s(q, d) is the cosine similarity of (q, d)\nWe initialize the model for unsupervised con-\ntrastive training with the weights of nomic-bert-\n2048. We use a batch size of 16,384 so each batch\nhas a large number of in-batch negatives. Our op-\ntimizations for the encoder architecture and train-\ning strategy centered around achieving this batch\nsize. We use AdamW with a learning rate of 2e-\n5, \u03b21 = 0.9, \u03b22 = 0.999, and weight decay of\n0.01. Gradient clipping is set to 1.0. We use an\nlinear warmup schedule of 700 steps and an in-\nverse square root decay schedule. We train with a\nmax sequence length of 2048 for 1 full epoch over\nthe data.\nDue to GPU memory constraints, we could\nnot fit the full model, optimizer, states, and data\ninto memory.\nAs a workaround, we employ\nGradCache (Luyu Gao and Callan, 2021) as well\nas mixed precision training (Micikevicius et al.,\n2018).\nFinally, we use task specific prefixes to break\nthe symmetry of the biencoder as in (Wang et al.,\n2022). Without prefixes, the model receives con-\nflicting reward signal. Consider the case of deter-\nmining which response is closest to the question\n\u201dWhat is the capital of France?\u201d:\n1. \u201cWhat is the name of the capital city of\nFrance?\n2. \u201cParis is the capital of France.\u201d\nA semantic similarity task would consider the\nfirst closest, while a question answering task\nwould consider the second closest. Prefixes en-\nable the model to distinguish between the behav-\niors specified by each of these tasks.\nWe use the following task-specific prefixes:\n\u2022 search query\n\u2022 search document\n\u2022 classification\n\u2022 clustering\ninspired by Reimers et al. (2023). We first break\nprefixes into two categories: symmetric, where the\nquery and document have a similar structure, and\nasymmetric, where the query is usually a single\nsentence and the document can be many sentences.\n(Su et al., 2023a) The first two prefixes are used\nfor retrieval tasks: where search query is typi-\ncally for the question and search document is\nfor the response. classification is used for\nSTS-related tasks like rephrasals. clustering\nis used for tasks where to objective is to group se-\nmantically similar texts close together, like Arxiv\ntitle-abstract pairs. For symmetric tasks, the same\nprefix is appended to both the query and docu-\nment.\n4.4\nSupervised Contrastive Fine-tuning\nThe last stage of training aims to boost perfor-\nmance by utilizing human-labeled datasets. Sev-\neral papers including (Ni et al., 2021a,b; Wang\net al., 2022; Li et al., 2023) have shown that fine-\ntuning on these datasets leads to improvements in\ndownstream performance.\nWe adapt the paired contrastive loss to include\nhard negatives in each batch.\nWe train for one\nepoch using seven hard negatives per pair and a\nbatch size of 256. We employ a learning rate of\n2e-5, \u03b21 = 0.9, \u03b22 = 0.999, and weight decay\nof 0.01. Gradient clipping is set to 1.0. We use\na linear warmup schedule of 400 steps and a lin-\near cooldown to 0 and train with prefixes as de-\nscribed above. We found that increasing the num-\nber of negatives above 7 to not meaningfully im-\nprove performance. We also found that training\nfor multiple epochs hurts performance.\n5\nResults\nWe evaluate nomic-bert-2048 on the GLUE\nbenchmark (Wang et al., 2019) and find that it\nis competitive with similarly sized and trained\nmodels.\nWe evaluate nomic-embed-text-v1 on\nMTEB (Muennighoff et al., 2023), Jina\u2019s Long\nContext Benchmark (G\u00a8unther et al., 2024), and\nLoCo (Saad-Falcon et al., 2024). nomic-embed-\ntext-v1 exceeds text-embedding-ada-002 and jina-\nembeddings-v2-base-en.\nOn the long con-\ntext benchmarks, LoCo and Jina Long Context\nBenchmark, nomic-embed-text-v1 uniformly out-\nperforms jina-embeddings-v2-base-en.\nnomic-\nembed-text-v1 outperforms text-embedding-ada-\n002 on LoCo and on two of four datasets in Jina\u2019s\nLong Context Benchmark.\n5.1\nnomic-bert-2048 GLUE Results\nWe evaluate nomic-bert-2048 on the GLUE\nbenchmark (Wang et al., 2019) following the\nmethodolgy presented in (Liu et al., 2019). The\nGLUE benchmark consists of 9 tasks, but we eval-\nuate on 8 similar to (Liu et al., 2019).\nFor each task, we train for 10 epochs with batch\nsizes 16, 32 and learning rate 1e-5, 2e-5, 3e-5 with\na linear warmup of 6% across 5 seeds. The me-\ndian score per task at the end of the 10 epochs is\npresented in Table 2. Note we report accuracy for\nMRPC and QQP and Pearson for STSB 6. We re-\nport our results in Table 2. Similar to (Liu et al.,\n2019), we initialize from an MNLI checkpoint for\nRTE, STSB, and MRPC.\nMosaicBERT (Portes et al., 2023) performs\nslightly better but is trained for slightly longer\nand on C4 (Raffel et al., 2019). Across all tasks,\nnomic-bert-2048 scores similarly to MosaicBERT\nexcept on Cola. However, we used a longer se-\nquence length model and in effect have seen more\ntokens during pretraining. JinaBERT also scores\nsimilarly, although they report test scores ver-\nsus dev scores and is trained similarly to Mo-\nsaicBERT.\n5.2\nMTEB Results\nMTEB (Muennighoff et al., 2023) has become\nthe standard benchmark for evaluating embed-\nding models due to its diverse coverage of 8\ntasks spanning 56 datasets. MTEB evaluated em-\nbedding models across Classification, Clustering,\nPair Classification, Reranking, Retrieval, Seman-\ntic Textual Similarity, and Summarization. The\nMTEB score is a weighted average of the per-task\nscores.\n5.3\nLong Context Results\nHowever, as noted in (G\u00a8unther et al., 2024),\nMTEB has very few datasets that include long se-\nquences. To evaluate nomic-embed-text-v1\u2019s per-\nformance on longer sequences, we consider two\nadditional benchmarks:\n(G\u00a8unther et al., 2024)\nLong Context Dataset as well as the LoCo bench-\nmark from (Saad-Falcon et al., 2024).\n5.3.1\nJinaAI Long Context Benchmark\nThe Jina Long Context Benchmark (G\u00a8unther et al.,\n2024) evaluates on 4 datasets across Retrieval and\nClustering; namely, NarrativeQA (G\u00a8unther et al.,\n2024), WikiCites 7, SciFact (Wadden et al., 2020),\n6https://github.com/\nfacebookresearch/fairseq/issues/1561#\nissuecomment-571729519\n7https://huggingface.co/datasets/\njinaai/cities_wiki_clustering\nTable 3: Results on the MTEB benchmark (Muennighoff et al., 2023). The numbers are averaged for each category.\nPlease refer to https://huggingface.co/spaces/mteb/leaderboard for the scores per dataset and\nthe most up to date results.\nCategory \u2192\nCls.\nClust.\nPairCls. Rerank Retr.\nSTS\nSumm. Avg\nNumber of datasets \u2192\n12\n11\n3\n4\n15\n10\n1\n56\nUnsupervised Models\nGlove (Pennington et al., 2014)\n57.3\n27.7\n70.9\n43.3\n21.6\n61.9\n28.9\n42.0\nSimCSE (Gao et al., 2022)\n62.5\n29.0\n70.3\n46.5\n20.3\n74.3\n31.2\n45.5\nnomic-embed-text-v1unsup\n71.2\n42.5\n83.7\n55.0\n48.0\n80.8\n30.7\n59.9\nSupervised Models\nSimCSEbert-sup (Gao et al., 2022)\n67.3\n33.4\n73.7\n47.5\n21.8\n79.1\n23.3\n48.7\nContriever (Izacard et al., 2022a)\n66.7\n41.1\n82.5\n53.1\n41.9\n76.5\n30.4\n56.0\nGTRxxl (Ni et al., 2021a)\n67.4\n42.4\n86.1\n56.7\n48.5\n78.4\n30.6\n59.0\nSentence-T5xxl (Ni et al., 2021b)\n73.4\n43.7\n85.1\n56.4\n42.2\n82.6\n30.1\n59.5\nE5large-v2 (Wang et al., 2022)\n75.2\n44.5\n86.0\n56.6\n50.6\n82.1\n30.2\n62.3\nE5mistral (Wang et al., 2023b)\n78.5\n50.3\n88.3\n60.2\n56.9\n84.6\n31.4\n66.6\nGTEbase (Li et al., 2023)\n73.0\n46.2\n84.6\n58.6\n51.1\n82.3\n31.2\n62.4\nGTElarge (Li et al., 2023)\n73.3\n46.8\n85.0\n59.1\n52.2\n83.4\n31.7\n63.1\nBGEbase (Xiao et al., 2023)\n75.5\n45.8\n86.6\n58.9\n53.3\n82.4\n31.1\n63.6\nBGElarge (Xiao et al., 2023)\n76.0\n46.1\n87.1\n60.0\n54.3\n83.1\n31.6\n64.2\nJinav2 (G\u00a8unther et al., 2024)\n73.5\n41.7\n85.4\n57.0\n47.9\n80.7\n31.6\n60.4\ntext-embedding-ada-002\n70.9\n45.9\n84.9\n56.3\n49.3\n81.0\n30.8\n61.0\ntext-embedding-3-small\n73.2\n46.7\n85.0\n56.7\n51.1\n81.6\n31.1\n62.3\ntext-embedding-3-large\n75.5\n49.0\n85.7\n59.2\n55.4\n81.7\n29.9\n64.6\nnomic-embed-text-v1-ablated\n73.6\n43.7\n84.6\n53.3\n51.4\n80.2\n31.3\n61.4\nnomic-embed-text-v1\n74.1\n43.9\n85.2\n55.7\n52.8\n82.1\n30.1\n62.4\nand BigPatent 8 (Sharma et al., 2019). Results are\npresented in Table 4. Similar to (G\u00a8unther et al.,\n2024), we report the V-scores and NDCG@10 for\nthe clustering and retrieval datasets respectively.\nAcross sequence lengths and tasks, nomic-embed-\ntext-v1 beats or ties jina-embeddings-v2-base on\nall datasets at 8k context length.\nAdditionally,\nnomic-embed-text-v1 beats text-embedding-ada-\n002 on two of the four datasets. We also report\nsimilar results to (G\u00a8unther et al., 2024) on Wi-\nkiCitiesClustering that sequence length hurts per-\nformance, suggesting that longer sequence lengths\nare not necessary to perform well on the test.\n5.3.2\nLoCo Benchmark\nThe LoCo Benchmark consists of 5 retrieval\ndatasets, 3 from (Shaham et al., 2022) and 2 from\n(Dasigi et al., 2021). The benchmark tests retrieval\nacross meeting transcripts, national policy reports,\n8https://huggingface.co/datasets/\njinaai/big-patent-clustering\nTV episode transcripts, and scientific research pa-\npers. We include the QASPER Abstract Articles\ndataset for completeness, but would like to high-\nlight that many models seem to oversaturate the\nbenchmark and approach 1.0 NDCG@10.\nRe-\nsults are presented in Table 6. nomic-embed-text-\nv1 beats jina-embeddings-v2-base-en across se-\nquence lengths. nomic-embed-text-v1 beats M2-\nBert at 2048 and is competitive at 8192. At se-\nquence length 4096, nomic-embed-text-v1 is com-\npetitive with E5 Mistral while being significantly\nsmaller.\n5.4\nFew-Shot Evaluation of BEIR\nWhile the BEIR component of MTEB was origi-\nnally purposed as a zero-shot benchmark, several\ntop open-source models, including BGE (Xiao\net al., 2023), GTE (Li et al., 2023), and E5-Mistral\n(Wang et al., 2023b) report training on train splits\nof BEIR benchmark datasets such as FEVER and\nHotpotQA. To understand the impact of this on our\nModel\nSeq\nNarrativeQA\nWikiCities\nSciFact\nBigPatent\nAvg\nnomic-embed-text-v1\n128\n20.1\n90.0\n65.4\n18.5\n48.5\nnomic-embed-text-v1-ablated\n128\n20.8\n86.8\n65.2\n17.5\n47.6\njina-embeddings-base-v2\n128\n19.6\n79.9\n62.1\n14.4\n44.0\ntext-embedding-ada-002\n128\n25.4\n84.9\n68.8\n16.6\n48.9\ntext-embedding-3-small\n128\n29.5\n87.5\n68.8\n15.0\n50.2\ntext-embedding-3-large\n128\n45.6\n87.9\n74.8\n16.5\n56.2\nnomic-embed-text-v1\n512\n23.9\n88.7\n70.5\n25.3\n52.1\nnomic-embed-text-v1-ablated\n512\n25.7\n81.9\n71.5\n23.7\n50.7\njina-embeddings-base-v2\n512\n21.3\n79.3\n66.7\n21.9\n47.3\ntext-embedding-ada-002\n512\n25.5\n84.8\n72.6\n23.0\n51.5\ntext-embedding-3-small\n512\n32.2\n89.0\n73.2\n23.6\n54.5\ntext-embedding-3-large\n512\n48.1\n89.9\n77.6\n23.6\n59.6\nnomic-embed-text-v1\n8191\n37.8\n84.3\n70.2\n24.5\n54.2\nnomic-embed-text-v1-ablated\n8191\n44.0\n77.4\n69.1\n23.6\n53.5\njina-embeddings-base-v2\n8191\n39.4\n75.7\n69.4\n23.1\n51.9\ntext-embedding-ada-002\n8191\n41.1\n84.7\n72.7\n22.5\n55.3\ntext-embedding-3-small\n8191\n47.1\n89.9\n73.3\n22.5\n58.3\ntext-embedding-3-large\n8191\n51.6\n86.2\n77.7\n19.3\n58.7\nTable 4:\nJina Long Context Evaluation Benchmark.\nNumbers for text-embedding-ada-002 and\njina-embeddings-base-v2 taken from (G\u00a8unther et al., 2024).\ndownstream scores, we also train a nomic-embed-\ntext-v1-ablated model that omits the FEVER, Hot-\npotQA, and MEDI datasets. As reported in Ta-\nble 1, this decreases our overall MTEB score by\nabout one point. To maintain an apples-to-apples\ncomparison with top open-source models, we opt\nto train on the FEVER, HotpotQA, and MEDI\ndatasets for the released version of nomic-embed-\ntext-v1. Unfortunately, due to the nature of closed-\nsource models, we have no indication regarding\nwhether closed-source models trained on these\ndatasets.\n6\nTraining Resources\nFull training of nomic-embed-text-v1 can be con-\nducted in a single week on one 8xH100 node.\nMasked language modeling of nomic-bert-2048\ntakes roughly 4 days. Contrastive pretraining lasts\n3 and a half days. Contrastive fine-tuning takes\none hour. We encourage the reader to initialize\nfrom our nomic-bert-2048 or Unsupervised Con-\nstrastive checkpoints, released under the same li-\ncense as nomic-embed-text-v1.\n7\nConclusion\nWe release the first fully open-source long con-\ntext text embedding model that surpasses OpenAI\nAda-002 performance on both sort and long con-\ntext benchmarks. We release the model weights\nand training code under a permissible license as\nwell as the recipe, including data, to reproduce the\nmodel.\n7.1\nContributions\nZach Nussbaum lead the project, including the\nmajority of the implementation, training and data\ndecisions present in the final version, as well as\nmaking several design decisions at all levels of\nthe stack. Jack Morris made several design con-\ntributions regarding dataset curation and model\narchitecture.\nBrandon Duderstadt made several\ndesign contributions across the entire stack and\nwrote the base implementation of the data curation\npipeline. Andriy Mulyar set early project direc-\ntion, reviewed code implementations, and made\nseveral model design and dataset curation contri-\nbutions.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\nwary, and Tong Wang. 2018. Ms marco: A human\ngenerated machine reading comprehension dataset.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015.\nA large an-\nnotated corpus for learning natural language infer-\nence.\nIn Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguis-\ntics.\nWilliam Coster and David Kauchak. 2011. Simple En-\nglish Wikipedia: A new text simplification task. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 665\u2013669, Portland, Ore-\ngon, USA. Association for Computational Linguis-\ntics.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00b4e. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nemozilla. 2023. Dynamically scaled rope further in-\ncreases performance of long context llama with zero\nfine-tuning.\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni.\n2014. Open Question Answering Over Curated and\nExtracted Knowledge Bases. In KDD.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nlong form question answering.\nIn Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n3558\u20133567. Association for Computational Linguis-\ntics.\nKatja Filippova and Yasemin Altun. 2013. Overcom-\ning the lack of parallel data in sentence compression.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1481\u20131491, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nWikimedia Foundation. Wikimedia downloads.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2022.\nSimcse: Simple contrastive learning of sentence em-\nbeddings.\nJonas Geiping and Tom Goldstein. 2022. Cramming:\nTraining a language model on a single gpu in one\nday.\nMansi Gupta, Nitish Kulkarni, Raghuveer Chanda,\nAnirudha Rayasam, and Zachary C Lipton. 2019.\nAmazonqa:\nA review-based question answering\ntask.\nMichael G\u00a8unther, Louis Milliken, Jonathan Geuter,\nGeorgios Mastrapas, Bo Wang, and Han Xiao. 2023.\nJina embeddings: A novel set of high-performance\nsentence embedding models.\nMichael G\u00a8unther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba\nSturua, Bo Wang, Maximilian Werk, Nan Wang, and\nHan Xiao. 2024.\nJina embeddings 2: 8192-token\ngeneral-purpose text embeddings for long docu-\nments.\nFelix Hamborg, Norman Meuschke, Corinna Bre-\nitinger, and Bela Gipp. 2017.\nnews-please:\nA\ngeneric news crawler and extractor. In Proceedings\nof the 15th International Symposium of Information\nScience, pages 218\u2013223.\nChristopher Hidey and Kathy McKeown. 2016. Identi-\nfying causal relations using parallel Wikipedia ar-\nticles.\nIn Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1424\u20131433, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nEvan Hubinger, Carson Denison, Jesse Mu, Mike Lam-\nbert, Meg Tong, Monte MacDiarmid, Tamera Lan-\nham, Daniel M. Ziegler, Tim Maxwell, Newton\nCheng, Adam Jermyn, Amanda Askell, Ansh Rad-\nhakrishnan, Cem Anil, David Duvenaud, Deep Gan-\nguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshi-\ntij Sachan, Michael Sellitto, Mrinank Sharma, Nova\nDasSarma, Roger Grosse, Shauna Kravec, Yuntao\nBai, Zachary Witten, Marina Favaro, Jan Brauner,\nHolden Karnofsky, Paul Christiano, Samuel R. Bow-\nman, Logan Graham, Jared Kaplan, S\u00a8oren Minder-\nmann, Ryan Greenblatt, Buck Shlegeris, Nicholas\nSchiefer, and Ethan Perez. 2024.\nSleeper agents:\nTraining deceptive llms that persist through safety\ntraining.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022a. Unsupervised dense in-\nformation retrieval with contrastive learning.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022b.\nAtlas: Few-shot learning\nwith retrieval augmented language models.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering.\nDaniel Khashabi, Amos Ng, Tushar Khot, Ashish Sab-\nharwal, Hannaneh Hajishirzi, and Chris Callison-\nBurch. 2021. Gooaq: Open question answering with\ndiverse answer types.\nMahnaz Koupaee and William Yang Wang. 2018. Wik-\nihow: A large scale text summarization dataset.\nPatrick\nLewis,\nEthan\nPerez,\nAleksandra\nPiktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00a8uttler,\nMike Lewis,\nWen tau Yih,\nTim Rockt\u00a8aschel, Sebastian Riedel, and Douwe\nKiela. 2021a.\nRetrieval-augmented generation for\nknowledge-intensive nlp tasks.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\nMinervini, Heinrich K\u00a8uttler, Aleksandra Piktus,\nPontus Stenetorp, and Sebastian Riedel. 2021b. Paq:\n65 million probably-asked questions and what you\ncan do with them.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, and Meishan Zhang. 2023.\nTo-\nwards general text embeddings with multi-stage\ncontrastive learning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Dan S. Weld. 2020. S2orc: The semantic\nscholar open research corpus.\nIlya Loshchilov and Frank Hutter. 2019.\nDecoupled\nweight decay regularization.\nJiawei Han Luyu Gao, Yunyi Zhang and Jamie Callan.\n2021. Scaling deep contrastive learning batch size\nunder memory limited setup. In Proceedings of the\n6th Workshop on Representation Learning for NLP.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg,\nMichael Houston,\nOleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed pre-\ncision training.\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embed-\ndings for semantic search.\nNiklas Muennighoff, Nouamane Tazi, Lo\u00a8\u0131c Magne, and\nNils Reimers. 2023. Mteb: Massive text embedding\nbenchmark.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford,\nJesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy,\nJohannes Heidecke, Pranav Shyam, Boris Power,\nTyna Eloundou Nekoul, Girish Sastry, Gretchen\nKrueger, David Schnurr, Felipe Petroski Such,\nKenny Hsu, Madeleine Thompson, Tabarak Khan,\nToki Sherbakov, Joanne Jang, Peter Welinder, and\nLilian Weng. 2022. Text and code embeddings by\ncontrastive pre-training.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Con-\nstant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei\nYang. 2022.\nSentence-t5: Scalable sentence en-\ncoders from pre-trained text-to-text models.\nIn\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, pages 1864\u20131874, Dublin, Ire-\nland. Association for Computational Linguistics.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019.\nJustifying recommendations using distantly-labeled\nreviews and fine-grained aspects.\nIn Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 188\u2013197, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00b4andez \u00b4Abrego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021a. Large dual encoders are generalizable\nretrievers.\nJianmo Ni, Gustavo Hern\u00b4andez \u00b4Abrego, Noah Con-\nstant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei\nYang. 2021b.\nSentence-t5: Scalable sentence en-\ncoders from pre-trained text-to-text models.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2019. Representation learning with contrastive pre-\ndictive coding.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-\nrico Shippole. 2023. Yarn: Efficient context window\nextension of large language models.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014.\nGloVe: Global vectors for word\nrepresentation.\nIn Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532\u20131543, Doha,\nQatar. Association for Computational Linguistics.\nJacob Portes, Alex Trott, Sam Havens, Daniel King,\nAbhinav Venigalla, Moin Nadeem, Nikhil Sardana,\nDaya Khudia, and Jonathan Frankle. 2023.\nMo-\nsaicbert: A bidirectional encoder optimized for fast\npretraining.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv e-prints.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimiza-\ntions toward training trillion parameter models.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016.\nSQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints,\npage arXiv:1606.05250.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models.\nNils Reimers, Elliot Choi, Amr Kayid, Alekhya Nan-\ndula, Manoj Govindassamy, and Abdullah Elkady.\n2023. Introducing embed v3.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nbert:\nSentence embeddings using siamese bert-\nnetworks.\nJon Saad-Falcon, Dan Fu, and Simran Arora. 2024.\nLong-context retrieval models with monarch mixer.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073\u2013\n1083, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. 2022.\nSCROLLS: Standardized CompaRison over long\nlanguage sequences.\nIn Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12007\u201312021, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nEva Sharma, Chen Li, and Lu Wang. 2019.\nBIG-\nPATENT: A large-scale dataset for abstractive and\ncoherent summarization. CoRR, abs/1906.03741.\nNoam Shazeer. 2020.\nGlu variants improve trans-\nformer.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2020. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2023a. One\nembedder, any task: Instruction-finetuned text em-\nbeddings.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2023b. Roformer: En-\nhanced transformer with rotary position embedding.\nNandan Thakur, Nils Reimers, Andreas R\u00a8uckl\u00b4e, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evaluation\nof information retrieval models.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction and\nVERification. In NAACL-HLT.\nVoyage. 2023.\nExcited to announce voyage embed-\ndings!\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020.\nFact or fiction: Veri-\nfying scientific claims. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7534\u20137550, On-\nline. Association for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2023a. Simlm: Pre-training with rep-\nresentation bottleneck for dense passage retrieval.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2023b. Improv-\ning text embeddings with large language models.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources to\nadvance general chinese embedding.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018.\nHotpotQA: A\ndataset for diverse, explainable multi-hop question\nanswering. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2016.\nCharacter-level convolutional networks for text clas-\nsification.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books.\nAppendix\nTable 5: Pretraining Dataset Distribution\nDataset\nDatapoints\n% Dataset\nReddita\n64,978,944\n0.28\nPAQ (Lewis et al., 2021b)\n52,953,088\n0.23\nAmazon Reviews (Ni et al., 2019)\n38,682,624\n0.16\nS2ORC Title Abstract (Lo et al., 2020)\n35438592\n0.15\nWikiAnswers (Fader et al., 2014)\n9,912,320\n0.04\nS2ORC Citation Titles (Lo et al., 2020)\n7,585,792\n0.03\nS2ORC Abstract Citation (Lo et al., 2020)\n7,503,872\n0.03\nS2ORC Abstract Body (Lo et al., 2020)\n6,389,760\n0.03\nWikipedia Title Body (Foundation)\n6,078,464\n0.03\nGooaq (Khashabi et al., 2021)\n1,245,184\n0.01\nCodesearch (Husain et al., 2019)\n835,584\n<.01\nAGNews (Zhang et al., 2016)\n409,600\n<.01\nCCNews (Hamborg et al., 2017)\n344,064\n<.01\nNPR b\n344,064\n<.01\nCNN (See et al., 2017)\n278,528\n<.01\nYahoo Title-Answer c\n262,144\n<.01\nAmazonQA (Gupta et al., 2019)\n212,992\n<.01\nYahoo Title-Question d\n196,608\n<.01\nSentence Compression (Filippova and Altun, 2013)\n163,840\n<.01\nYahooQA e\n131,072\n<.01\nELI5 (Fan et al., 2019)\n98,304\n<.01\nAltlex (Hidey and McKeown, 2016)\n98,304\n<.01\nWikihow (Koupaee and Wang, 2018)\n81,920\n<.01\nSimpleWiki (Coster and Kauchak, 2011)\n81,920\n<.01\nStackExchange Duplicate Questions f\n65,536\n<.01\nStackExchange Title Body g\n65,536\n<.01\nStackExchange Body Body h\n65,536\n<.01\nQuora Duplicate Questions i\n32,768\n<.01\nSQuAD (Rajpurkar et al., 2016)\n16,384\n<.01\nTotal\n234,553,344\n1\nahttps://huggingface.co/datasets/sentence-transformers/reddit-title-body\nbhttps://files.pushshift.io/news/\nchttps://www.kaggle.com/soumikrakshit/yahoo-answers-dataset\ndhttps://www.kaggle.com/soumikrakshit/yahoo-answers-dataset\nehttps://www.kaggle.com/soumikrakshit/yahoo-answers-dataset\nfhttps://data.stackexchange.com/apple/query/fork/1456963\nghttps://data.stackexchange.com/apple/query/fork/1456963\nhhttps://data.stackexchange.com/apple/query/fork/1456963\nihttps://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\nModel\nSeq\nParam. Tau\nScr.\nTau\nGov.\nTau\nQMS.\nQASP.\nTit.\nArt.\nQASP.\nAbs.\nArt.\nAvg\nUnsupervised Models\nJinabase-v2 (G\u00a8unther et al., 2024)\n2048\n137M\n87.2\n97.7\n35.1\n95.3\n99.7\n83.0\nJinabase-v2 (G\u00a8unther et al., 2023)\n8192\n137M\n93.3\n98.6\n40.8\n95.1\n99.3\n85.5\nnomic-embed-text-v1-ablated\n2048\n137M\n83.1\n97.3\n49.4\n97.4\n99.9\n85.4\nnomic-embed-text-v1-ablated\n4096\n137M\n89.1\n97.6\n49.6\n97.5\n99.9\n86.7\nnomic-embed-text-v1-ablated\n8192\n137M\n92.5\n97.8\n47.6\n96.5\n99.9\n86.9\nnomic-embed-text-v1\n2048\n137M\n86.1\n96.9\n47.8\n96.1\n99.7\n85.3\nnomic-embed-text-v1\n4096\n137M\n89.0\n97.4\n45.7\n95.8\n99.9\n85.6\nnomic-embed-text-v1\n8192\n137M\n90.9\n97.8\n44.2\n94.9\n99.9\n85.5\ntext-embedding-ada-002\n8192\nN/A\n37.3\n44.3\n7.30\n85.1\n89.7\n52.7\ntext-embedding-3-small\n8192\nN/A\n92.2\n97.7\n27.4\n95.9\n98.9\n82.4\ntext-embedding-3-large\n8192\nN/A\n88.0\n93.6\n25.5\n93.2\n96.8\n79.4\nE5mistral (Wang et al., 2023b)\n4096\n7B\n95.9\n98.3\n46.8\n98.4\n99.8\n87.8\nSupervised Models\nM2-Bert (Saad-Falcon et al., 2024)\n2048\n80M\n81.8\n94.7\n58.5\n87.3\n95.5\n83.6\nM2-Bert (Saad-Falcon et al., 2024)\n8192\n80M\n94.7\n96.5\n64.1\n86.8\n97.5\n87.9\nTable 6: Results on the LoCo benchmark (Saad-Falcon et al., 2024). NCDG@10 is reported for each dataset.\nWe split evaluations into parameter class and whether the evaluation is performed in a supervised or unsupervised\nsetting. We bold the top-performing model in each split. Nomic-embed-text-v1 is the best-performing 100M\nparameter class unsupervised model. Nomic-embed-text-v1 is competitive with the top-performing models in both\nthe 7B parameter class and with models trained in a supervised setting specifically for the LoCo benchmark.\n"
  },
  {
    "title": "EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks",
    "link": "https://arxiv.org/pdf/2402.00892.pdf",
    "upvote": "9",
    "text": "EVA-GAN: Enhanced Various Audio Generation via\nScalable Generative Adversarial Networks\nShijia Liao 1 Shiyi Lan 1 Arun George Zachariah 1\nAbstract\nThe advent of Large Models marks a new era\nin machine learning, significantly outperforming\nsmaller models by leveraging vast datasets to cap-\nture and synthesize complex patterns. Despite\nthese advancements, the exploration into scal-\ning, especially in the audio generation domain,\nremains limited, with previous efforts didn\u2019t ex-\ntend into the high-fidelity (HiFi) 44.1kHz do-\nmain and suffering from both spectral discon-\ntinuities and blurriness in the high-frequency\ndomain, alongside a lack of robustness against\nout-of-domain data. These limitations restrict\nthe applicability of models to diverse use cases,\nincluding music and singing generation.\nOur\nwork introduces Enhanced Various Audio Gen-\neration via Scalable Generative Adversarial Net-\nworks (EVA-GAN), yields significant improve-\nments over previous state-of-the-art in spectral\nand high-frequency reconstruction and robustness\nin out-of-domain data performance, enabling the\ngeneration of HiFi audios by employing an ex-\ntensive dataset of 36,000 hours of 44.1kHz au-\ndio, a context-aware module, a Human-In-The-\nLoop artifact measurement toolkit, and expands\nthe model to approximately 200 million parame-\nters. Demonstrations of our work are available at\nhttps://double-blind-eva-gan.cc.\n1. Introduction\nRecently, GAN-based neural vocoders have revolutionized\nthe generation of audio waveforms from acoustic properties,\nwith broad applications in voice synthesis, voice conversion,\nand audio enhancement. Despite the efficiency in sampling\nand memory optimization offered by them, they face chal-\n1NVIDIA\nCooperation,\nSanta\nClara,\nUnited\nStates.\nCorrespondence\nto:\nShijia\nLiao\n<shijial@nvidia.com>,\nShiyi Lan <shiyil@nvidia.com>,\nArun George Zachariah\n<azachariah@nvidia.com>.\nPreliminary work. Under review.\nlenges such as spectral discontinuities and blurriness in\nthe high-frequency domain, blocking the high-quality music\nand singing voice generation. Figure 2 illustrates spectral\ndisruptions when generating singing data using existing\nvocoders, including HiFiGAN (Kong et al., 2020) and BigV-\nGAN (gil Lee et al., 2023), which represent the current\nstate-of-the-art. We attribute these issues to a lack of data\ndiversity, limited model capacity, a discrepancy between the\ncontext window sizes used during training and inference,\nand the absence of objective metrics for measuring these\nartifacts.\nAnother significant challenge in this domain is achieving\nhigh-quality audio fidelity, particularly used in music and\nsinging synthesis, which remains insufficiently addressed.\nBigVGAN (gil Lee et al., 2023) has proposed scaling of\ndatasets and models as a pathway to state-of-the-art out-\nof-domain (OOD) performance. However, its reliance on\nthe LibriTTS dataset (Zen et al., 2019), which is limited to\nlow-fidelity (24kHz) speech data, falls short of capturing the\ndiverse and rich acoustic properties required for realistic mu-\nsic and singing generation. While the scaling of models is\nrecognized as a pivotal strategy for enhancing performance,\nthe vast majority of vocoders are equipped with fewer than\n20 million parameters. This limitation is primarily due to\nthe significant computational expense and memory require-\nments associated with managing the gradient footprint, thus\nhampering advancements in the synthesis of high-fidelity\nmusic and singing audio.\nMoreover, the prevalent evaluation of existing work on\nspeech datasets, known for their reduced sensitivity to spec-\ntrogram quality, has led to a deficiency in effective objective\nmetrics for the automatic assessment of high-frequency ar-\ntifacts and spectrogram discontinuities in neural vocoders.\nWe have identified that current metrics fail to detect artifacts\nthat, despite being subtle (e.g., a discontinuity lasting only\n20 milliseconds), are highly perceptible to humans. This\nunderscores the necessity for a robust evaluation method\ncapable of identifying such nuances during the training pro-\ncess.\nThe current issues in audio generation are very similar to\nthe early challenges in Natural Language Processing (NLP).\nPrior to the advent of Large Language Models (LLMs) such\n1\narXiv:2402.00892v1  [cs.SD]  31 Jan 2024\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nFigure 1. The EVA-GAN generator is composed of two main sections: Context Aware Blocks and Upsample Parallel Resblocks. The\nContext Aware Blocks, a novel introduction in this paper, leverage residual connections and large convolution kernel to augment the\ncontext window and capacity of the module with minimal computational overhead. The Upsample Parallel Resblocks, adapted from the\nHiFi-GAN\u2019s multi-receptive field fusion (MRF) blocks (Kong et al., 2020), utilize the SiLU (Elfwing et al., 2017) activation function for\ndecoding features into a waveform.\nas GPT-3, the consensus within the NLP field believed that\nsolving its problems required specialized designs and tech-\nniques. Similarly, audio generation tasks such as singing\nsynthesis, text-to-speech, and music synthesis have typically\nused different model architectures. However, the introduc-\ntion of the GPT series marked a significant paradigm shift,\nillustrating that broad, scalable solutions could effectively\ntackle a range of NLP tasks without needing task-specific\nadjustments or unique model designs. Inspired by this shift,\nwe present the Enhanced Various Audio Generation via\nScalable Generative Adversarial Networks (EVA-GAN). By\nfocusing on model and data scaling, EVA-GAN aims to mod-\nernize audio generation, creating a generalized and robust\nvocoder following industry-standard deep learning methods.\nOur contributions are multifaceted:\n1. Compared to (gil Lee et al., 2023), we have expanded\nEVA-GAN to 200 million parameters and utilized a\ncomprehensive 36,000-hour HiFi (44.1kHz) dataset,\nwhich are largest model and data used in Neural\nVocoder to the best of our knowledge.\n2. We introduce a novel context-aware module, referred\nto as CAM, which marks a significant leap forward in\nmodel performance, achieving outstanding advance-\nments with virtually no additional computational bur-\nden, as documented in Table 1.\n3. We propose a innovative training pipeline, which in-\ncludes a longer context window (around 3 seconds),\na loss balancer as initially introduced by Encodec\n(D\u00b4efossez et al., 2022), incorporated gradient check-\npointing, and improved activation functions to boost\ntraining stability, reduce memory usage, and minimize\nthe need for manual hyperparameter tuning.\n4. We build a brand new Human-In-The-Loop SMOS\n(Similarity Mean Option Score) evaluation toolkit, en-\nables artifact monitoring and ensuring unparalleled\nalignment with human subjective perceptions.\nOverall, we created a state-of-the-art 44.1kHz Vocoder,\nEVA-GAN, especially suited for high-quality audio genera-\ntion, establishing a new industry benchmark in this domain.\n2. Related Work\nNeural vocoders, which produce audio waveforms from\nacoustic properties using deep learning, have become indis-\npensable in speech synthesis, voice conversion, and speech\nenhancement. The evolution of neural vocoder techniques\ncan be segmented into three distinct phases: autoregressive\n(AR) or flow-based models (van den Oord et al., 2016;\nKalchbrenner et al., 2018; Prenger et al., 2019), GAN-\ncentric approaches (Yamamoto et al., 2020; Yang et al.,\n2020; Kong et al., 2020; Jang et al., 2021), and direct spec-\ntral generation (DSG) strategies (Siuzdak, 2023; Kaneko\net al., 2022).\n2.1. GAN-based Neural Vocoders\nOf these, the current best-in-class GAN-based (Kong et al.,\n2020; gil Lee et al., 2023; Kaneko et al., 2022; Siuzdak,\n2\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nTable 1. Speed measured across different models, results were obtained on a single A100 GPU, using a batch size of 16 with 512 frames\n(around 6 seconds) of audio in fp32 format. Comparing to original HiFi-GAN (Kong et al., 2020), we have replaced leaky ReLU with\nSiLU (Elfwing et al., 2017), applied in-place activation optimizations, and optimize kernel sizes for 44.1k generation. EVA-GAN-base is\nessentially HiFi-GAN-base enhanced with our Context Aware Module, while EVA-GAN-large represents a scaled version of the generator\nto 174M parameters. The reported training memory encompasses both forward and backward memory usage, excluding the application\nof the optimizer. Speed metrics are based on the average of 100 inferences, following 10 initial warm-up runs. PESQ (wide-band) are\nobserved at LibriTTS dev set.\nEXPERIMENT\nTOTAL PARAMS\nGENERATOR PARAMS\nTRAIN MEM\nINFER MEM\nINFER TIME\nPESQ\nHIFI-GAN-BASE\n13.6 M\n13.6 M\n43.2 GB\n2.1 GB\n177 MS\n3.5486\nEVA-GAN-BASE\n34.85 M\n16.3 M\n46 GB\n2.2 GB\n193 MS\n4.0330\nEVA-GAN-BIG\n192.99 M\n174.44 M\n68 GB\n2.7 GB\n402 MS\n4.3536\n2023) vocoders boast impressive sampling efficiency and\nmemory optimization. However, they come with their own\nset of challenges, like spectral inconsistencies, high and low\nfrequency range artifacts, deconvolution checkerboard arti-\nfacts, leading to compromised audio quality. Furthermore,\nbasic HiFi-GAN (Kong et al., 2020) tends to fall short when\ndealing with musical data, high-pitched audio, and out-of-\ndistribution (OOD) content, causing audio disruptions.\n2.2. Development of Generator and Loss Terms\nUnivNet (Jang et al., 2021) replaces the Mel-Spectrogram\nloss with a multi-scale STFT loss and adopts a multi-\nresolution discriminator to boost high-frequency domain\nrecovery. Meanwhile, BigVGAN (gil Lee et al., 2023) pur-\nposed to use Snake Activation to improve spectrogram qual-\nity and out-of-distribution (OOD) performance. On the\nother hand, models like NSF-HiFi-GAN (Zhao et al., 2020;\nKong et al., 2020; Openvpi, 2022), RefineGAN (Xu et al.,\n2021), and SingGAN (Huang et al., 2022), incorporate an f0\nsource to elevate audio quality and spectral continuity, but\nthis restricts them from utilizing large and varied datasets.\n2.3. Reduce Artifacts by Improving Discriminators\nDiscriminators play a crucial role in vocoder training, strik-\ning a balance between reducing human-sensitive artifacts\nand optimizing objective scores, such as loss. Various ap-\nproaches have been explored in previous works, including\nMPD, MSD, MRD, MS-STFTD, and MS-SBCQTD, all aim-\ning to minimize these artifacts and enhance high-frequency\ndomain reconstruction.\nHiFi-GAN (Kong et al., 2020) introduced the Multi-Period\nDiscriminator (MPD), transforming the waveform into 2D\nrepresentations of varying heights and employing 2D con-\nvolutions to analyze periodic structures. In the same vein,\nHiFi-GAN\u2019s Multi-Scale Discriminator (MSD) processes\nthe waveform into multiple 1D representations at different\nscales, enabling detailed analysis of time-domain informa-\ntion.\nUnivNet (Jang et al., 2021) proposed the Multi-Resolution\nSpectrogram Discriminator (MSRD or MRD), focusing on\nthe multi-resolution time-frequency domain through the\nShort-Term Fourier Transform (STFT). Similarly, Encodec\n(D\u00b4efossez et al., 2022) advocated for a Multi-Scale STFT\nDiscriminator (MS-STFTD) to enhance audio generation\nquality.\nFurthering this innovation, (Gu et al., 2023) introduced the\nMulti-Scale Sub-Band Constant-Q Transform Discriminator\n(MS-SBCQTD). This novel approach supports the generator\nin more effectively restoring high-frequency components\nby utilizing the Constant Q Transform, an alternative to the\nconventional STFT.\n3. Preliminaries\nThis manuscript extends the foundational work on Gener-\native Adversarial Network (GAN)-based vocoders, specif-\nically leveraging the architectural paradigm introduced by\nHiFiGAN (Kong et al., 2020). An exposition of the critical\nelements of this framework is imperative for understand-\ning the subsequent developments. The inception of specific\nloss metrics by Least Squares GAN (Mao et al., 2017) her-\nalded their adoption in GAN-based vocoders, prominently\nexemplified by HiFiGAN\u2019s implementation.\n3.1. Generator\nThe generator within a GAN-based vocoder framework is\ntasked with the transformation of Mel-Spectrograms into un-\nprocessed waveforms. The generator\u2019s loss metrics in HiFi-\nGAN (Kong et al., 2020) encapsulate the Mel-Spectrogram\nloss Lmel, the adversarial loss Ladv, and the feature match-\ning loss Lfm, articulated as follows:\nLmel(G) = E\nh\n\u2225\u03d5(x) \u2212 \u03d5(G(s))\u22251\ni\n(1)\nLadv(G) = E\nh\n(D(G(s)) \u2212 1)2i\n(2)\n3\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nLfm(G) = E\nh T\nX\ni=1\n1\nNi\n\u2225Di(x) \u2212 Di(G(s))\u22251\ni\n(3)\nwhere x represents the ground truth, \u03d5 symbolizes the Mel-\nSpectrogram function, s denotes the Mel-Spectrogram cor-\nresponding to the audio signal, T denotes the number of\nlayers in the discriminator, D and G respectively signify the\ndiscriminator and generator, and Ni denotes the number of\nfeatures in the i-th layer of the discriminator\n3.2. Discriminator\nBeyond the Mel-Spectrogram loss, contemporary GAN-\nbased vocoders incorporate multiple discriminators to at-\ntenuate perceptual distortions, which, despite potentially\nenhancing objective measures such as spectrogram fidelity,\nremain perceptibly discernible. Notably, these discrimina-\ntors universally adopt multi-resolution feature analysis.\nEach discriminator is evaluated using the following adver-\nsarial loss metric:\nLadv(D) = E\nh\n(D(x) \u2212 1)2 + (D(G(s)))2i\n,\n(4)\nwhere x denotes the ground truth, s represents the Mel-\nSpectrogram associated with the audio signal, and D and G\nrespectively indicate the Discriminator and Generator.\n4. EVA-GAN\nWe position EVA-GAN as an advancement over HiFi-GAN\n(Kong et al., 2020), characterized by a larger context win-\ndow, an improved structure, increased capacity, and an ex-\npanded dataset.\n4.1. Data Scaling\nTraditionally, the training of a vocoder involves using\ndatasets such as LJSpeech (Ito & Johnson, 2017), LibriTTS\n(Zen et al., 2019), VCTK (Veaux et al., 2017), and M4Singer\n(Zhang et al., 2022). These datasets, however, either suffer\nfrom a low sampling rate (24k) or lack diversity, covering\nonly a limited range of speakers and languages. The im-\nportance of data scaling was underscored by the BigVGAN\nstudy (gil Lee et al., 2023), which showed significant out-\nof-distribution improvements when scaled to the LibriTTS\n(Zen et al., 2019) dataset. Building on this, we aim to further\nincrease the scale to enhance model robustness.\nTo this end, the Fish Audio community compiled a compre-\nhensive 16,000-hour high-fidelity music and song dataset\nfrom YouTube Music and Netease Music (dubbed HiFi-\n16000h). This dataset encompasses a variety of languages\nincluding Chinese, English, and Japanese, and features a\nwide spectrum of musical instruments. To our knowledge,\nthis is the largest high-fidelity audio dataset to date, effec-\ntively addressing out-of-domain sample concerns.\nFurthermore, to boost speech performance, an additional\n20,000 hours of diverse language audio from the broadcast\nplatform Player FM (termed PlayerFM-20000h) was added\nby the Fish Audio community. A balanced distribution of\n50% from HiFi-16000h and 50% from PlayerFM-20000h\nwas maintained to ensure sample diversity.\nOur results highlight the critical role of scale and diversity\nin training datasets. Our baseline HiFi-GAN, trained on this\nextensive 36,000-hour dataset, effectively reduces spectral\ndiscontinuities and demonstrates the capability to replicate\na wide array of audio types. This includes, but is not limited\nto, singing, speaking, bass, piano, and unique sounds like\nhelicopter noise and boiling kettle sounds.\n4.2. Model Scaling\nEchoing BigVGAN\u2019s findings (gil Lee et al., 2023), which\nhighlight the superior performance of larger models over\nsmaller ones even on the relatively modest-sized LibriTTS\ndataset (Zen et al., 2019) (about 1000 hours), we observed\nmarked improvements in robustness and overall perfor-\nmance with an enlarged generator. Accordingly, we have\nscaled up the generator in our base EVA-GAN-base model\nfrom 16.3 million to 174.4 million parameters, thus creat-\ning the more potent EVA-GAN-big variant. The enhanced\ncapabilities of EVA-GAN-big are evident in Table 1 and\ndemonstrated in Figure 2, particularly in terms of continu-\nousness and resolution in the high-frequency domain.\nScaling up discriminators, such as MPD (Kong et al., 2020)\nand MRD (Jang et al., 2021), did not yield proportionate\nbenefits. Additionally, incorporating new discriminators\nlike MS-STFT (D\u00b4efossez et al., 2022) and MS-SBCQTD\n(Gu et al., 2023) did not further enhance model performance.\nWe hypothesize that this outcome is attributable to the con-\nsiderable capacity of our model, which effectively captures\nsubtle distinctions between the ground truth and generated\naudio across both low and high-frequency ranges. This ca-\npability negates the need for any trade-offs or overemphasis\non a particular frequency range in the loss function.\n4.3. The Free Lunch: Context Aware Module (CAM)\nDespite the increased demand for computational resources\nand memory when scaling up the HiFi-GAN (Kong et al.,\n2020) generator, due to the challenges posed by long audio\nsequences and the substantial gradient footprint, we discov-\nered that a 1D Convolution-based context-aware module\nnamely CAM, utilizing the building blocks derived from\nConvNeXt (Liu et al., 2022), requires significantly less mem-\nory and computational resources. This module not only is\nresource-efficient but also delivers notable improvements in\nboth objective and subjective assessments, as demonstrated\nin Table 1.\n4\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nTable 2. Objective evaluation of EVA-GAN was performed on the LibriTTS development sets, and subjective evaluation was carried out\nusing test sets akin to those in the BigVGAN study (gil Lee et al., 2023). To align with these benchmarks, EVA-GAN\u2019s outputs were\ndown-sampled to 24kHz. The Vocos (Siuzdak, 2023) weights were obtained from gemelo-ai/vocos. UnivNet-c32 (Jang et al., 2021)\nmodel weights were sourced from maum-ai/univnet. The HiFi-GAN (Kong et al., 2020) (V1) version, trained on LJSpeech, VCTK, and\nLibriTTS at 22kHz, was sourced from jik876/hifi-gan. For BigVGAN, objective results were taken directly from the paper (gil Lee et al.,\n2023), and subjective evaluations were based on weights trained for 5 million steps, acquired from NVIDIA/BigVGAN.\nLIBRITTS\nM-STFT (\u2193)\nPERIODICITY (\u2193)\nV/UV F1 (\u2191)\nPESQ (\u2191)\nSMOS (\u2191)\nGROUND TRUTH\n-\n-\n-\n-\n4.909\nVOCOS\n0.8580\n0.1103\n0.9555\n3.6328\n4.8577\nUNIVNET-C32\n0.8959\n0.1333\n0.9444\n3.2566\n4.8042\nHIFI-GAN (V1)\n1.3647\n0.1600\n0.9309\n2.9110\n4.7596\nBIGVGAN-BASE\n0.8788\n0.1287\n0.9459\n3.5190\n4.8545\nBIGVGAN-BIG\n0.7997\n0.1018\n0.9598\n4.0270\n4.8786\nHIFI-GAN-BASE\n1.0269\n0.1230\n0.9523\n3.5485\n4.8345\nEVA-GAN-BASE\n0.9485\n0.0942\n0.9658\n4.0330\n4.8687\nEVA-GAN-BIG\n0.7982\n0.0751\n0.9745\n4.3536\n4.9134\n4.4. Renew Training Paradigm\nWhile intuitively scaling the model, data, and context length\ncan enhance performance, the challenge lies in achieving\nthis within the bounds of limited computational resources\nand maintaining training stability. BigVGAN, previously\nthe largest vocoder (gil Lee et al., 2023), encountered obsta-\ncles in further scaling due to training instability and compu-\ntational constraints. To mitigate these issues, they reduced\nthe context length, resulting in a compromise between sta-\nbility and resource utilization \u2014 less resource-intensive, yet\nat the cost of stability.\nOur investigation revealed a gap in the implementation of\nseveral efficient techniques in existing neural vocoders.\n4.4.1. LARGER CONTEXT WINDOW\nIn comparison to the context windows (typically 32 or 64\nframes) used in HiFi-GAN (Kong et al., 2020) and BigV-\nGAN (gil Lee et al., 2023), extending the window to 256\nframes proved highly beneficial. This increase aids in faster\nmodel convergence, higher GPU utilization, significantly re-\nduces spectrogram discontinuities. However, this expansion\nalso raises GPU memory consumption and slows down the\ntraining speed. To mitigate this, we implemented several\noptimizations as described below.\n4.4.2. SILU IN-PLACE ACTIVATION\nWe observed that replacing the Leaky ReLU activation func-\ntion with SiLU (Elfwing et al., 2017) not only accelerates\nmodel convergence but also preserves final performance.\nAdditionally, employing in-place activations for both the\ngenerator and discriminators wherever possible resulted in\napproximately a 30% reduction in GPU memory usage.\n4.4.3. GRADIENT CHECKPOINTING\nAs indicated in Table 1, large batch sizes are not feasible\nwith a 256-frame context window, even on A100 GPUs.\nThus, we applied gradient checkpointing (Chen et al., 2016)\nto both the generator and discriminators, significantly re-\nducing the gradient memory footprint. For instance, the\nmemory usage of the EVA-GAN-base generator decreased\nfrom 46GB to 16GB, albeit with a 30% reduction in training\nspeed.\n4.4.4. TENSORFLOAT-32\nWhile mixed precision training with fp16 and bf16 is com-\nmon in large language models and computer vision tasks,\nwe encountered instability with fp16 (notably large gradient\nnorms) and performance degradation with bf16 due to its\nlower precision. Therefore, utilizing A100 GPUs, we opted\nfor TensorFloat-32 for training, which offers roughly twice\nthe speed of fp32.\n4.4.5. LOSS BALANCER\nUpon scaling the model and adopting a 44.1k configuration,\nwe encountered challenges in balancing various losses: fea-\nture matching loss, adversarial loss, Mel-Spectrogram loss,\nand multi-scale STFT loss. The loss balancer, a concept\nbrought into the vocoder field by Encodec (D\u00b4efossez et al.,\n2022), emerged as a solution. This technique automatically\nbalances these losses based on their gradients, allowing each\nto contribute equally to the parameter update process. In the\nabsence of this balancing, we observed significant human-\nperceptible high-frequency artifacts, despite seemingly ac-\nceptable objective results such as l1 Mel-Spectrogram dis-\ntance, attributable to the inability to properly adjust the\nweight of the discriminator loss.\n5\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nFigure 2. Spectrogram visualizations for a 44.1kHz singing voice generated by HiFi-GAN (Kong et al., 2020), BigVGAN (gil Lee et al.,\n2023), and both the base and big versions of our EVA-GAN are presented, including zoomed-in views on high-frequency regions to\nillustrate differences in spectrogram continuity and high-frequency detail. The HiFi-GAN (Kong et al., 2020) (V1) model, trained on the\nLJSpeech, VCTK, and LibriTTS datasets at 22kHz, was obtained from jik876/hifi-gan. Weights for BigVGAN were sourced from the\nofficial repository NVIDIA/BigVGAN.\n4.5. Human-In-The-Loop Artifact Measurement\nWhile metrics such as PESQ (Rix et al., 2001) and Mel\ndistance quantify the discrepancy between generated au-\ndios and ground truth, their high scores do not necessarily\ncorrelate with subjective evaluation outcomes. This mis-\nalignment arises because these metrics inadequately capture\nartifacts particularly perceptible to human listeners, espe-\ncially those occurring in the high-frequency domain or as\nshort-term spectrogram disruptions lasting only a few mil-\nliseconds. These nuances, though critical for audio quality,\nare overlooked by conventional metrics. In addressing this\ngap, discriminators in models like HiFiGAN (Kong et al.,\n2020) have been instrumental, highlighting their capability\nto discern such subtle differences, albeit not always reflected\nin objective metrics.\nTo tackle these challenges, we have devised a Human-In-\nThe-Loop Artifact Measurement toolkit, incorporating a\nSMOS (Similarity Mean Option Score) annotating tool and\nCLI, to continuously monitor and evaluate the quality of\ngenerated audio against human perceptual standards.\n5. Experiments Setup\nAdopting a similar approach to BigVGAN (gil Lee et al.,\n2023), we set our base learning rate at 1e-4 and applied gra-\ndient clipping at a norm of 1000 to manage gradient spiking\nand exploding. Our optimizer of choice was AdamW, with\nbetas set at [0.8, 0.99]. In line with what is commonly prac-\nticed in LLMs, we opted for a step-wise learning rate decay,\nas opposed to the epoch-based approach used in HiFi-GAN\n(Kong et al., 2020) and its derivatives. Specifically, we im-\nplemented an exponential learning rate decay of 0.999999\nafter each step.\nOur model utilizes a 512x upsampling rate to accommodate\na 160 bins Mel-Spectrogram input, with 2048 n fft, 512\nhop length, and 2048 win length. As previously mentioned,\nwe found that a larger context window is highly beneficial,\nso we used a 256 frames context window (approximately\n3 seconds) instead of the 32 or 64 frames used in earlier\nmodels like HiFi-GAN (Kong et al., 2020) and BigVGAN\n(gil Lee et al., 2023). Due to memory constraints, our total\nbatch size was set at 16, as opposed to 32 used in prior\nmodels. However, given the larger frame size of our model,\nwe still achieved a higher effective frame count per batch,\nfurther enhancing training stability.\nFor our Multi Period Discriminator, as introduced by\nHiFiGAN (Kong et al., 2020), we used periods of\n[3, 5, 7, 11, 17, 23, 37]. The Multi Resolution Discrimina-\ntor and Multi Resolution STFT loss, as introduced by Uni-\nvNet (Jang et al., 2021), were set to resolutions of [[2048,\n512, 2048], [1024, 120, 600], [2048, 240, 1200], [4096,\n480, 2400], [512, 50, 240]]. This setup, comparing to the\n24k configuration provided by previous works (Kong et al.,\n2020; gil Lee et al., 2023; Jang et al., 2021; Siuzdak, 2023),\n6\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nimproved the model\u2019s performance in the high-frequency\ndomain.\nWith the aforementioned setup and using tf32 precision, we\nexperienced stable training without any issues of gradient\nexploding or crashing. Unless specified otherwise, all mod-\nels were trained on the 36,000-hour dataset comprising 50%\nHiFi-16000h and 50% PlayerFM-20000h for 1 million steps\non NVIDIA A100 GPUs.\nStarting with the HiFiGAN (Kong et al., 2020) v1 configu-\nration, we replaced the Leaky ReLU activation with SiLU\nactivation and established a 44.1k baseline, resulting in the\nHiFi-GAN-Base. We chose upsample rates of [8, 8, 2, 2, 2],\nupsample kernel sizes of [16, 16, 4, 4, 4], and parallel block\n(MRF blocks (Kong et al., 2020)) kernel sizes of [3, 7, 11].\nThis configuration led to a 13.6M generator.\nBuilding upon HiFi-GAN-Base, we added the Context\nAware Module before the generator: depths and dims for\neach block at [3, 3, 9, 3] and [128, 256, 384, 512], a 0.2 drop\npath ratio, and a kernel size of 7. This resulted in EVA-\nGAN-base with a 16.3M generator, considering the in-\ncreased input dimension from 160 to 512, and an additional\n18.6M for the Context Aware Module.\nTo develop EVA-GAN-big, we retained all settings\nfrom EVA-GAN-base but altered the upsample rate\nto\n[4, 4, 2, 2, 2, 2, 2],\nthe\nupsample\nkernel\nsizes\nto\n[8, 8, 4, 4, 4, 4, 4], and the parallel block kernel sizes to\n[3, 7, 11, 13]. The initial channels for upsampling were set\nat 1536. This scaling increased the generator size to 174.4M,\nbringing the total parameter count to 193M.\n6. Results\nWe evaluated the performance of EVA-GAN and compared\nit with existing methods across multiple tasks, including\nLibriTTS (24k speech) and DSD-100 (48k music), and ob-\nserved significant performance improvements.\nTable 1 demonstrates that, compared to our optimized HiFi-\nGAN (Kong et al., 2020) baseline, the Context Aware Mod-\nule adds minimal overhead in inference speed and train-\ning memory, yet significantly improves performance when\ntrained on our dataset. This suggests that the additional\nparameters efficiently enhance network capacity without\nrequiring excessive resources. Furthermore, while EVA-\nGAN-big is six times larger than EVA-GAN-base, it only\ndoubles the training memory and slows inference time by\na factor of one, maintaining a speed 250 times faster than\nreal-time with a batch size of 16.\n6.1. Evaluation Metrics\nFollowing BigVGAN\u2019s methodology (gil Lee et al., 2023),\nwe employed the following objective metrics for our Lib-\nriTTS evaluation:\n\u2022 Multi-resolution STFT (M-STFT), as provided in\nParallel WaveGAN (Yamamoto et al., 2020), us-\ning the open-source implementation from cstein-\nmetz1/auraloss (Steinmetz & Reiss, 2020).\n\u2022 Periodicity error (based on CREPE) and voiced / un-\nvoiced classification F1 score (V/UV F1), highlighting\na common artifact in neural vocoders without a source\nmodule. We used CARGAN (Morrison et al., 2022)\ncode available at descriptinc/cargan.\n\u2022 Perceptual evaluation of speech quality (PESQ) (Rix\net al., 2001), using the well-known automated voice\nquality assessment tool\u2019s wide-band version (16,000\nHz), available at ludlows/PESQ.\nIn line with BigVGAN (gil Lee et al., 2023), we observed\nthat the 5-scale mean opinion score (MOS) does not accu-\nrately reflect each model\u2019s performance, as most received\nhigh scores and minor data noise could alter the average\nscores significantly. Since our task involves copy-synthesis\nfor vocoders, we aim for the model to generate outputs\nclosely resembling the input Mel-Spectrogram. In cases of\nnoisy inputs, we do not want the neural vocoder to bias the\naudio quality towards the training data, which might result\nin artificially high listener scores.\nTherefore, we adopted a 5-scale similarity mean opinion\nscore (SMOS) approach, similar to (gil Lee et al., 2023).\nSpecifically, participants were provided with both reference\naudio (Ground Truth) and generated audio to assess the\nquality of the generated audio. All participants were re-\nquired to wear headphones and listen to the audios in a quiet\nenvironment for more accurate ranking.\n6.2. LibriTTS\nIn Table 2, we present both objective and subjective results\non LibriTTS (Zen et al., 2019), a 24kHz speech dataset. The\nobjective evaluation metrics M-STFT, Periodicity, V/UV F1,\nand PESQ were calculated on the LibriTTS development set,\nencompassing both clean and \u2019other\u2019 categories with unseen\nspeakers in the training of BigVGAN (gil Lee et al., 2023)\nand HiFiGAN (Kong et al., 2020). For subjective evaluation,\nwe conducted SMOS tests on a randomly selected set of 100\nfiles from the LibriTTS test set.\nOur findings, detailed in Table 2, reveal that EVA-GAN,\ndespite being natively 44.1kHz and not trained on LibriTTS,\nsignificantly outperforms the current state-of-the-art, BigV-\nGAN (gil Lee et al., 2023), in all objective and subjective\nmetrics after downsampling to 24kHz. Notably, HiFi-GAN-\nbase, with only changes in the training recipe, activation\nfunction, and the use of our large dataset, shows remarkable\n7\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nTable 3. SMOS evaluated on DSD100 test sets. BigVGAN (gil Lee et al., 2023) weights, trained for 5 million steps, were acquired from\nthe official repository NVIDIA/BigVGAN. All samples are resampled to 44.1kHz for listeners.\nDSD100\nMIXTURE (\u2191)\nBASS (\u2191)\nDRUMS (\u2191)\nVOCALS (\u2191)\nOTHER (\u2191)\nGROUND TRUTH\n4.7778\n4.7237\n4.8533\n4.7531\n4.8313\nVOCOS\n3.1519\n3.2716\n3.9857\n4.2078\n3.0694\nUNIVNET-C32\n2.2778\n2.9241\n3.5507\n3.2963\n2.3247\nHIFI-GAN (V1)\n2.4023\n3.0833\n3.5821\n3.3188\n2.5769\nBIGVGAN-BASE\n3.3537\n3.2821\n4.2464\n4.3846\n3.6892\nBIGVGAN-BIG\n4.0854\n3.8642\n4.0909\n4.5000\n3.9747\nHIFI-GAN-BASE\n4.5658\n4.1940\n4.5493\n4.6944\n4.4605\nEVA-GAN-BASE\n4.4133\n4.2405\n4.5467\n4.6627\n4.5634\nEVA-GAN-BIG\n4.6197\n4.4675\n4.4658\n4.7467\n4.6053\nimprovement over the HiFiGAN (Kong et al., 2020) base-\nline. This underscores the importance of our new training\nstrategy and a more diverse dataset.\nEVA-GAN-base further enhances the objective results of\nHiFi-GAN-base by incorporating a CAM. When compared\nto BigVGAN-base, EVA-GAN-base achieves better perfor-\nmance in most objective metrics while using less memory\nand offering faster inference speeds. Our largest model,\nEVA-GAN-big, validates the efficacy of scaling up, surpass-\ning all existing models even though EVA-GANs were not\ntrained on LibriTTS.\n6.3. DSD-100\nTo manage resource constraints, we employed a strategy of\nrandom sampling, choosing 10 tracks from each of the five\ncategories in the DSD-100 (Liutkus et al., 2017) dataset of\n44.1kHz mixed audios for testing: Mixture, Bass, Drums,\nVocals, and Others. For each track, we selected a 5-second\nclip from a random non-silent section for our evaluation,\nwith the findings detailed in Table 3.\nAnalyzing this dataset offered a unique perspective on the re-\nsilience and efficacy of neural vocoders in music generation,\ngiven its encompassment of critical musical components.\nThe inference drawn is that a model\u2019s proficiency on the\nDSD-100 dataset likely translates to enhanced performance\nin high-fidelity music and speech synthesis.\nAdditional results, including those for extremely OOD tasks,\nare available in our demos at Here.\n6.4. Ablation Studies\nNew Training Recipe and Larger Dataset: Compared\nto the HiFiGAN v1 (Kong et al., 2020) baseline, our HiFi-\nGAN-base significantly improves objective metrics by op-\ntimizing the training recipe and dataset, as discussed in\nSection 4 and evidenced in Table 2 and Figure 2.\nContext Aware Module: The introduction of the Con-\ntext Aware Module in our EVA-GAN-base significantly\nenhanced its objective metrics, adding minimal overhead.\nThis is demonstrated in Tables 1, 2, 3, and Figure 2.\nLarger Model: The scaling up of EVA-GAN to create\nEVA-GAN-big, detailed in Tables 2, 3, and Figure 2, shows\nthe effectiveness of increasing model size. EVA-GAN-big,\nwhich is six times larger than EVA-GAN-base but without\nany changes in training hyperparameters or dataset, achieves\nsuperior performance in both speech and music domains,\nand exhibits robustness across various types of audio. Ad-\nditional results demonstrating its OOD performance can be\nfound on our Here.\n7. Conclusions\nIn this paper, we introduced EVA-GAN, a groundbreak-\ning audio generation model that sets new benchmarks in\nthe realm of neural vocoders. By leveraging an extensive\ndataset and incorporating innovative features such as a CAM\nand scaling the model to around 200M parameters, EVA-\nGAN significantly outperforms existing models in terms\nof spectral continuity, high-frequency reconstruction, and\nrobustness in out-of-distribution data performance.\nOur comprehensive experiments, conducted on a diverse\nrange of audio data, including the largest dataset to date,\ndemonstrate EVA-GAN\u2019s superior ability to generate high-\nquality, realistic audio across various domains. This achieve-\nment not only marks a significant advancement in audio\nsynthesis but also opens new avenues for future research\nand applications in speech synthesis, music generation, and\nbeyond.\nEVA-GAN\u2019s remarkable performance, underscored by its\nstate-of-the-art results and efficiency, establishes it as the\nnew gold standard in audio generation, promising to enhance\na wide array of applications in the audio domain.\n8\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\nReferences\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep\nnets with sublinear memory cost, 2016.\nD\u00b4efossez, A., Copet, J., Synnaeve, G., and Adi, Y. High\nfidelity neural audio compression, 2022.\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\nlinear units for neural network function approximation in\nreinforcement learning, 2017.\ngil Lee, S., Ping, W., Ginsburg, B., Catanzaro, B., and Yoon,\nS. Bigvgan: A universal neural vocoder with large-scale\ntraining, 2023.\nGu, Y., Zhang, X., Xue, L., and Wu, Z. Multi-scale sub-\nband constant-q transform discriminator for high-fidelity\nvocoder, 2023.\nHuang, R., Cui, C., Chen, F., Ren, Y., Liu, J., Zhao, Z.,\nHuai, B., and Wang, Z. Singgan: Generative adversarial\nnetwork for high-fidelity singing voice generation, 2022.\nIto, K. and Johnson, L. The lj speech dataset. https://\nkeithito.com/LJ-Speech-Dataset/, 2017.\nJang, W., Lim, D., Yoon, J., Kim, B., and Kim, J. Univnet:\nA neural vocoder with multi-resolution spectrogram dis-\ncriminators for high-fidelity waveform generation, 2021.\nKalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,\nCasagrande, N., Lockhart, E., Stimberg, F., van den Oord,\nA., Dieleman, S., and Kavukcuoglu, K. Efficient neural\naudio synthesis. 2, 2018.\nKaneko, T., Tanaka, K., Kameoka, H., and Seki, S. istftnet:\nFast and lightweight mel-spectrogram vocoder incorpo-\nrating inverse short-time fourier transform, 2022.\nKong, J., Kim, J., and Bae, J. Hifi-gan: Generative ad-\nversarial networks for efficient and high fidelity speech\nsynthesis, 2020.\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,\nand Xie, S. A convnet for the 2020s, 2022.\nLiutkus, A., St\u00a8oter, F.-R., Rafii, Z., Kitamura, D., Rivet,\nB., Ito, N., Ono, N., and Fontecave, J. The 2016 signal\nseparation evaluation campaign. In Tichavsk\u00b4y, P., Babaie-\nZadeh, M., Michel, O. J., and Thirion-Moreau, N. (eds.),\nLatent Variable Analysis and Signal Separation - 12th\nInternational Conference, LVA/ICA 2015, Liberec, Czech\nRepublic, August 25-28, 2015, Proceedings, pp. 323\u2013332,\nCham, 2017. Springer International Publishing.\nMao, X., Li, Q., Xie, H., Lau, R. Y. K., Wang, Z., and Smol-\nley, S. P. Least squares generative adversarial networks,\n2017.\nMorrison, M., Kumar, R., Kumar, K., Seetharaman, P.,\nCourville, A., and Bengio, Y. Chunked autoregressive\ngan for conditional waveform synthesis, 2022.\nOpenvpi.\nRelease nsf-hifigan with 44.1 khz sam-\npling rate \u00b7 openvpi/vocoders, Dec 2022.\nURL\nhttps://github.com/openvpi/vocoders/\nreleases/tag/nsf-hifigan-v1.\nPrenger, R., Valle, R., and Catanzaro, B. Waveglow: A\nflow-based generative network for speech synthesis. 3,\n2019.\nRix, A., Beerends, J., Hollier, M., and Hekstra, A. Percep-\ntual evaluation of speech quality (pesq): A new method\nfor speech quality assessment of telephone networks and\ncodecs. volume 2, pp. 749\u2013752 vol.2, 02 2001. ISBN\n0-7803-7041-4. doi: 10.1109/ICASSP.2001.941023.\nSiuzdak, H. Vocos: Closing the gap between time-domain\nand fourier-based neural vocoders for high-quality audio\nsynthesis, 2023.\nSteinmetz, C. J. and Reiss, J. D. auraloss: Audio focused\nloss functions in PyTorch. In Digital Music Research\nNetwork One-day Workshop (DMRN+15), 2020.\nvan den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and\nKavukcuoglu, K. Wavenet: A generative model for raw\naudio. 1, 2016.\nVeaux, C., Yamagishi, J., and MacDonald, K. Cstr vctk cor-\npus: English multi-speaker corpus for cstr voice cloning\ntoolkit. 2017.\nXu, S., Zhao, W., and Guo, J.\nRefinegan: Universally\ngenerating waveform better than ground truth with highly\naccurate pitch and intensity responses. arXiv preprint\narXiv:2111.00962, 2021.\nYamamoto, R., Song, E., and Kim, J.-M. Parallel wavegan:\nA fast waveform generation model based on generative\nadversarial networks with multi-resolution spectrogram.\n4, 2020.\nYang, Y. R., Chen, Y.-A., and Tsao, Y. Multi-band mel-\ngan: Faster waveform generation for high-quality text-to-\nspeech. [5], 2020.\nZen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia,\nY., Chen, Z., and Wu, Y. Libritts: A corpus derived from\nlibrispeech for text-to-speech, 2019.\nZhang, L., Li, R., Wang, S., Deng, L., Liu, J., Ren, Y., He,\nJ., Huang, R., Zhu, J., Chen, X., and Zhao, Z. M4singer:\nA multi-style, multi-singer and musical score provided\nmandarin singing corpus.\nIn Thirty-sixth Conference\n9\nEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\non Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022.\nZhao, Y., Wang, X., Juvela, L., and Yamagishi, J. Trans-\nferring neural speech waveform synthesizers to musi-\ncal instrument sounds generation.\nIn Proc. ICASSP,\npp. 6269\u20136273, 2020.\ndoi: 10.1109/ICASSP40776.\n2020.9053047. URL https://ieeexplore.ieee.\norg/document/9053047/.\n10\n"
  }
]