[
  {
    "title": "Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion",
    "link": "https://arxiv.org/pdf/2310.03502.pdf",
    "upvote": "73",
    "text": "Kandinsky: an Improved Text-to-Image Synthesis with\nImage Prior and Latent Diffusion\nAnton Razzhigaev1,2, Arseniy Shakhmatov3, Anastasia Maltseva3, Vladimir Arkhipkin3,\nIgor Pavlov3, Ilya Ryabov3, Angelina Kuts 3, Alexander Panchenko2,1,\nAndrey Kuznetsov3,1, and Denis Dimitrov3,1\n1AIRI, 2Skoltech, 3Sber AI\n{razzhigaev, kuznetsov, dimitrov}@airi.net\nAbstract\nText-to-image generation is a significant do-\nmain in modern computer vision and has\nachieved substantial improvements through the\nevolution of generative architectures. Among\nthese, there are diffusion-based models that\nhave demonstrated essential quality enhance-\nments. These models are generally split into\ntwo categories: pixel-level and latent-level ap-\nproaches. We present Kandinsky1, a novel ex-\nploration of latent diffusion architecture, com-\nbining the principles of the image prior models\nwith latent diffusion techniques. The image\nprior model is trained separately to map text\nembeddings to image embeddings of CLIP. An-\nother distinct feature of the proposed model\nis the modified MoVQ implementation, which\nserves as the image autoencoder component.\nOverall, the designed model contains 3.3B pa-\nrameters. We also deployed a user-friendly\ndemo system that supports diverse genera-\ntive modes such as text-to-image generation,\nimage fusion, text and image fusion, image\nvariations generation, and text-guided inpaint-\ning/outpainting. Additionally, we released the\nsource code and checkpoints for the Kandinsky\nmodels. Experimental evaluations demonstrate\na FID score of 8.03 on the COCO-30K dataset,\nmarking our model as the top open-source per-\nformer in terms of measurable image genera-\ntion quality.\n1\nIntroduction\nIn quite a short period of time, generative abilities\nof text-to-image models have improved substan-\ntially, providing users with photorealistic quality,\nnear real-time inference speed, a great number of\napplications and features, including simple easy-\nto-use web-based platforms and sophisticated AI\ngraphics editors.\nThis paper presents our unique investigation of\nlatent diffusion architecture design, offering a fresh\n1The system is named after Wassily Kandinsky, a famous\npainter and an art theorist.\nand innovative perspective on this dynamic field\nof study. First, we describe the new architecture\nof Kandinsky and its details. The demo system\nwith implemented features of the model is also\ndescribed. Second, we show the experiments, car-\nried out in terms of image generation quality and\ncome up with the highest FID score among exist-\ning open-source models. Additionally, we present\nthe rigorous ablation study of prior setups that we\nconducted, enabling us to carefully analyze and\nevaluate various configurations to arrive at the most\neffective and refined model design.\nOur contributions are as follows:\n\u2022 We present the first text-to-image architecture\ndesigned using a combination of image prior\nand latent diffusion.\n\u2022 We demonstrate experimental results compara-\nble to the state-of-the-art (SotA) models such\nas Stable Diffusion, IF, and DALL-E 2, in\nterms of FID metric and achieve the SotA\nscore among all existing open source models.\n\u2022 We provide a software implementation of\nthe proposed state-of-the-art method for text-\nto-image generation, and release pre-trained\nmodels, which is unique among the top-\nperforming methods.\nApache 2.0 license\nmakes it possible to use the model for both\nnon-commercial and commercial purposes.2 3\n\u2022 We create a web image editor application\nthat can be used for interactive generation\nof images by text prompts (English and Rus-\nsian languages are supported) on the basis\nof the proposed method, and provides in-\npainting/outpainting functionality.4 The video\ndemonstration is available on YouTube.5\n2https://github.com/ai-forever/Kandinsky-2\n3https://huggingface.co/kandinsky-community\n4https://fusionbrain.ai/en/editor\n5https://www.youtube.com/watch?v=c7zHPc59cWU\narXiv:2310.03502v1  [cs.CV]  5 Oct 2023\nClip-image\nEncoder\nLatent Diffusion\nU-Net\nMOVQ\nDecoder\nXLMR-CLIP\nLatent Diffusion\nU-Net\nCLIP-image\nEncoder\nCLIP-text\nEncoder\nNeon lights\nCLIP-text\nEncoder\nRed dress\nDiffusion\nMapping\nLatent Diffusion\nU-Net\nMOVQ\nDecoder\nXLMR-CLIP\ntime_emb\n+\nImage Prior\ntime_emb\n+\n+\nClip-image\nEncoder\nLatent Diffusion\nU-Net\nMOVQ\nDecoder\n+\nvariation\nimage fusion\nfusion with text\ninpainting\nMOVQ\nDecoder\nCLIP-text\nEncoder\nSad clown face\nImage Prior\nLatent Diffusion\nU-Net\nMOVQ\nDecoder\nCLIP-text\nEncoder\nClip-image\nEncoder\nImage Prior\n\u2014 Text embedding\n\u2014 Image embedding\nTiger on the grass\nXLMR-CLIP\nMSE\nLoss\ntime_emb\n+\ntext to image\nimage prior\ntraining\nFigure 1: Image prior scheme and inference regimes of the Kandinsky model.\n2\nRelated Work\nEarly text-to-image generative models, such as\nDALL-E (Ramesh et al., 2021) and CogView (Ding\net al., 2021), or later Parti (Yu et al., 2022) em-\nployed autoregressive approaches but often suf-\nfered from significant content-level artifacts. This\nled to the development of a new breed of models\nthat utilized the diffusion process to enhance image\nquality. Diffusion-based models, such as DALL-\nE 2 (Ramesh et al., 2022), Imagen (Saharia et al.,\n2022b), and Stable Diffusion6, have since become\ncornerstones in this domain. These models are typ-\nically divided into pixel-level (Ramesh et al., 2022;\nSaharia et al., 2022b) and latent-level (Rombach\net al., 2022) approaches.\nThis surge of interest has led to the design of\ninnovative approaches and architectures, paving\nthe way for numerous applications based on open-\nsource generative models, such as DreamBooth\n(Ruiz et al., 2023) and DreamPose (Karras et al.,\n2023). These applications exploit image generation\ntechniques to offer remarkable features, further fu-\neling the popularity and the rapid development of\ndiffusion-based image generation approaches.\nThis enabled a wide array of applications like\n3D object synthesis (Poole et al., 2023; Tang et al.,\n2023; Lin et al., 2022; Chen et al., 2023), video\ngeneration (Ho et al., 2022b; Luo et al., 2023; Ho\net al., 2022a; Singer et al., 2023; Blattmann et al.,\n2023; Esser et al., 2023), controllable image editing\n(Hertz et al., 2023; Parmar et al., 2023; Liew et al.,\n2022; Mou et al., 2023; Lu et al., 2023), and more,\n6https://github.com/CompVis/stable-diffusion\nwhich are now at the forefront of this domain.\nDiffusion models achieve state-of-the-art results\nin image generation task both unconditional (Ho\net al., 2020; Nichol and Dhariwal, 2021) and con-\nditional (Peebles and Xie, 2022). They beat GANs\n(Goodfellow et al., 2014) by generating images\nwith better scores of fidelity and diversity without\nadversarial training (Dhariwal and Nichol, 2021).\nDiffusion models also show the best performance\nin various image processing tasks like inpainting,\noutpainting, and super-resolution (Batzolis et al.,\n2021; Saharia et al., 2022a).\nText-to-image diffusion models have become\na popular research direction due to the high per-\nformance of diffusion models and the ability to\nsimply integrate text conditions with the classifier-\nfree guidance algorithm (Ho and Salimans, 2022).\nEarly models like GLIDE (Nichol et al., 2022), Im-\nagen (Saharia et al., 2022b), DALL-E 2 (Ramesh\net al., 2022) and eDiff-I (Balaji et al., 2022) gen-\nerate low-resolution image in pixel space and then\nupsample it with another super-resolution diffusion\nmodels. They are also using different text encoders,\nlarge language model T5 (Raffel et al., 2020) in\nImagen, CLIP (Radford et al., 2021) in GLIDE and\nDALL-E 2.\n3\nDemo System\nWe implemented a set of user-oriented solutions\nwhere Kandinsky model is embedded as a core\nimaging service. It has been done due to a variety\nof inference regimes, some of which need specific\nfront-end features to perform properly. Overall, we\nimplemented two main inference resources: Tele-\nFigure 2: Examples of inference regimes using Kandinsky model.\ngram bot and FusionBrain website.\nFusionBrain represents a web-based image edi-\ntor with such features as loading and saving images,\nsliding location window, erasing tools, zooming\nin/out, various styles selector, etc. (cf. Figure 3).\nIn terms of image generation, the three following\noptions are implemented on this side:\n\u2022 text-to-image generation \u2013 user inputs a text\nprompt in Russian or English, then selects an\naspect-ratio from the list (9:16, 2:3, 1:1, 16:9,\n3:2), and the system generates an image;\n\u2022 inpainting \u2013 using the specific erasing tool,\nuser can remove any arbitrary input image\npart and fill it, guided by a text prompt or\nwithout any guidance;\n\u2022 outpainting \u2013 input image can be extended\nwith a sliding window that can be used as\na mask for the following generation (if the\nwindow intersects any imaged area, then the\nempty window part is generated with or with-\nout text prompt guidance).\nInpainting and outpainting options are the main\nimage editing features of the model. Architectural\ndetails about these generation types can also be\nfound in Figure 1.\nTelegram bot contains the following image gen-\neration features (cf. Figure 2):\n\u2022 text-to-image generation;\n\u2022 image and text fusion \u2013 user inputs an im-\nage and a text prompt to create a new image\nguided by this prompt;\n\u2022 image fusion \u2013 user inputs an image as the\nmain one and another \u2019guiding\u2019 image, and\nthe system generates their fusion;\n\u2022 image variations \u2013 user inputs an image, and\nthe system generates several new images sim-\nilar to the input one.\n4\nKandinsky Architecture\nIn our work, we opted to deliver state-of-the-art\ntext-to-image synthesis. In the initial stages of our\nresearch, we experimented with multilingual text\nencoders, such as mT5 (Xue et al., 2021), XLMR\n(Conneau et al., 2020), XLMR-CLIP7, to facili-\ntate robust multilingual text-to-image generation.\nHowever, we discovered that using the CLIP-image\nembeddings instead of standalone text encoders re-\nsulted in improved image quality. As a result, we\nadopted an image prior approach, utilizing diffu-\nsion and linear mappings between text and image\nembedding spaces of CLIP, while keeping addi-\ntional conditioning with XLMR text embeddings.\nThat is why Kandinsky uses two text encoders:\nCLIP-text with image prior mapping and XLMR.\nWe have set these encoders to be frozen during the\ntraining phase.\nThe significant factor that influenced our design\nchoice was the efficiency of training latent diffu-\nsion models, as compared to pixel-level diffusion\nmodels (Rombach et al., 2022). This led us to fo-\ncus our efforts on the latent diffusion architecture.\nOur model essentially comprises three stages: text\nencoding, embedding mapping (image prior), and\nlatent diffusion.\nThe construction of our model involves three\nprimary steps: text encoding, embedding mapping\n(image prior), and latent diffusion. At the embed-\nding mapping step, which we also refer to as the\n7https://github.com/FreddeFrallan/\nMultilingual-CLIP\nFigure 3: Kandinsky web interface for \u201ca corgi gliding on the wave\u201d: generation (left) and in/outpainting (right).\nTable 1: Proposed architecture comparison by FID on\nCOCO-30K validation set on 256\u00d7256 resolution. * For\nthe IF model we reported reproduced results on COCO-\n30K, but authors provide FID of 7.19.\nModel\nFID-30K\nOpen Sourced Techologies\nKandinsky (Ours)\n8.03\nStable Diffusion 2.1 (2022) 8\n8.59\nGLIDE 8 (Nichol et al., 2022)\n12.24\nIF* (2023) 12\n15.10\nKandinsky 1.0 (2022) 9\n15.40\nruDALL-E Malevich (2022) 9\n20.00\nGLIGEN 10 (Li et al., 2023)\n21.04\nProprietary Technologies\neDiff-I (Balaji et al., 2022)\n6.95\nImagen (Saharia et al., 2022b)\n7.27\nGigaGAN (Kang et al., 2023)\n9.09\nDALL-E 2 (Ramesh et al., 2022)\n10.39\nDALL-E (Ramesh et al., 2021)\n17.89\nimage prior, we use the transformer-encoder model.\nThis model was trained from scratch with a diffu-\nsion process on text and image embeddings pro-\nvided by the CLIP-ViT-L14 model. A noteworthy\nfeature in our training process is the use of element-\nwise normalization of visual embeddings. This\nnormalization is based on full-dataset statistics and\nleads to faster convergence of the diffusion process.\nWe implemented inverse normalization to revert to\nthe original CLIP-image embedding space in the\ninference stage.\nThe image prior model is trained on text and\nimage embeddings, provided by the CLIP models.\n8https://github.com/Stability-AI/\nstablediffusion\n9https://github.com/ai-forever/ru-dalle\n10https://github.com/gligen/GLIGEN\nWe conducted a series of experiments and ablation\nstudies on the specific architecture design of the\nimage prior model (Table 3, Figure 6). The model\nwith the best human evaluation score is based on\na 1D-diffusion and standard transformer-encoder\nwith the following parameters: num_layers=20,\nnum_heads=32, and hidden_size=2048.\nThe latent diffusion part employs a UNet model\nalong with a custom pre-trained autoencoder. Our\ndiffusion model uses a combination of multiple\ncondition signals: CLIP-image embeddings, CLIP-\ntext embeddings, and XLMR-CLIP text embed-\ndings. CLIP-image and XLMR-CLIP embeddings\nare merged and utilized as an input to the latent\ndiffusion process. Also, we conditioned the dif-\nfusion process on these embeddings by adding all\nof them to the time-embedding. Notably, we did\nnot skip the quantization step of the autoencoder\nduring diffusion inference as it leads to an increase\nin the diversity and the quality of generated images\n(cf. Figure 4). In total, our model comprises 3.3 B\nparameters (Table 2).\nTable 2: Kandinsky model parameters.\nArchitecture part\nParams\nFreeze\nDiffusion Mapping\n1B\nFalse\nCLIP image encoder (ViT-L14)\n427M\nTrue\nCLIP text encoder\n340M\nTrue\nText encoder (XLM-R-L)\n560M\nTrue\nLatent Diffusion UNet\n1.22B\nFalse\nMoVQ image autoencoder\n67M\nTrue\nWe observed that the image decoding was our\nmain bottleneck in terms of generated image qual-\nity; hence, we developed a Sber-MoVQGAN, our\ncustom implementation of MoVQGAN (Zheng\nTable 3: Ablation study: FID on COCO-30K validation\nset on 256 \u00d7 256 resolution.\nSetup\nFID-30K\nCLIP\nDiffusion prior with quantization\n9.86\n0.287\nDiffusion prior w/o quantization\n9.87\n0.286\nLinear prior\n8.03\n0.261\nResidual prior\n8.61\n0.249\nNo prior\n25.92\n0.256\net al., 2022) with minor modifications. We trained\nthis autoencoder on the LAION HighRes dataset\n(Schuhmann et al., 2022), obtaining the SotA re-\nsults in image reconstruction. We released the\nweights and code for these models under an open\nsource licence11. The comparison of our autoen-\ncoder with competitors can be found in Table 4.\n5\nExperiments\nWe sought to evaluate and refine the performance\nof our proposed latent diffusion architecture in our\nexperimental analysis. To this end, we employed\nautomatic metrics, specifically FID-CLIP curves\non the COCO-30K dataset, to obtain the optimal\nguidance-scale value and compare Kandinsky with\ncompetitors (cf. Figure 4). Furthermore, we con-\nducted investigations with various image prior se-\ntups, exploring the impact of different configura-\ntions on the performance. These setups included:\nno prior, utilizing text embeddings directly; lin-\near prior, implementing one linear layer; ResNet\nprior, consisting of 18 residual MLP blocks; and\ntransformer diffusion prior.\nAn essential aspect of our experiments was the\nexploration of the effect of latent quantization\nwithin the MoVQ autoencoder. We examined the\noutputs with latent quantization, both enabled and\ndisabled, to better comprehend its influence on im-\nage generation quality.\nTo ensure a comprehensive evaluation, we also\nincluded an assessment of the IF model 12, which is\nthe closest open-source competitor to our proposed\nmodel. For this purpose, we computed FID scores\nfor the IF model 13 (Table 1).\nHowever, we acknowledged the limitations of au-\ntomatic metrics that become obvious when it comes\nto capturing user experience nuances. Hence, in\naddition to the FID-CLIP curves, we conducted a\nblind human evaluation to obtain insightful feed-\n11https://github.com/ai-forever/MoVQGAN\n12https://github.com/deep-floyd/IF\n13https://github.com/mseitzer/pytorch-fid\nFigure 4: CLIP-FID curves for different setups.\nFigure 5: Image generation results with prompt \"astro-\nnaut riding a horse\" for original image prior and linear\nprior trained on 500 pairs of images with cats.\nback and validate the quality of the generated im-\nages from the perspective of human perception\nbased on the DrawBench dataset (Saharia et al.,\n2022b).\nThe combination of automatic metrics and hu-\nman evaluation provides a comprehensive assess-\nment of Kandinsky performance, enabling us to\nmake informed decisions about the effectiveness\nand usability of our proposed image prior to design.\n6\nResults\nOur experiments and evaluations have showcased\nthe capabilities of Kandinsky architecture in text-to-\nimage synthesis. Kandinsky achieved the FID score\nof 8.03 on the COCO-30K validation set at a resolu-\ntion of 256\u00d7256, which puts it in close competition\nwith the state-of-the-art models, and among the\ntop performers within open-source systems. Our\nmethodical ablation studies further dissected the\nperformance of different configurations: quantiza-\ntion of latent codes in MoVQ slightly improves\nFigure 6: Human evaluation: competitors vs Kandinsky with diffusion prior on Drawbench. The total count of votes\nis 5000.\nTable 4: Sber-MoVQGAN comparison with competitors on ImageNet dataset.\nModel\nLatent size\nNum Z\nTrain steps\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nL1 \u2193\nViT-VQGAN*\n32x32\n8192\n500,000\n1.28\n-\n-\n-\nRQ-VAE*\n8x8x16\n16384\n10 epochs\n1.83\n-\n-\n-\nMo-VQGAN*\n16x16x4\n1024\n40 epochs\n1.12\n0.673\n22.42\n-\nVQ CompVis\n32x32\n16384\n971,043\n1.34\n0.650\n23.85\n0.0533\nKL CompVis\n32x32\n-\n246,803\n0.968\n0.692\n25.11\n0.0474\nSber-VQGAN\n32x32\n8192\n1 epoch\n1.44\n0.682\n24.31\n0.0503\nSber-MoVQGAN 67M\n32x32\n1024\n5,000,000\n1.34\n0.704\n25.68\n0.0451\nSber-MoVQGAN 67M\n32x32\n16384\n2,000,000\n0.965\n0.725\n26.45\n0.0415\nSber-MoVQGAN 102M\n32x32\n16384\n2,360,000\n0.776\n0.737\n26.89\n0.0398\nSber-MoVQGAN 270M\n32x32\n16384\n1,330,000\n0.686\n0.741\n27.04\n0.0393\nthe quality of images (FID 9.86 vs 9.87). The best\nCLIP score and human-eval score are obtained by\ndiffusion prior.\nThe best FID score is achieved using Linear\nPrior. This configuration stands out with the best\nFID score of 8.03. It is an intriguing outcome: the\nsimplest linear mapping showcased the best FID,\nsuggesting that there might exist a linear relation-\nship between visual and textual embedding vector\nspaces. To further scrutinize this hypothesis, we\ntrained a linear mapping on a subset of 500 cat\nimages and termed it the \"cat prior\". Astonish-\ningly, this mapping displayed high proficiency (cf.\nFigure 5).\n7\nConclusion\nWe presented Kandinsky, a system for various im-\nage generation and processing tasks based on a\nnovel latent diffusion model. Our model yielded\nthe SotA results among open-sourced systems. Ad-\nditionally, we provided an extensive ablation study\nof an image prior to design choices. Our system is\nequipped with free-to-use interfaces in the form of\nWeb application and Telegram messenger bot. The\npre-trained models are available on Hugging Face,\nand the source code is released under a permissive\nlicense enabling various, including commercial, ap-\nplications of the developed technology.\nIn future research, our goal is to investigate the\npotential of the latest image encoders. We plan to\nexplore the development of more efficient UNet\narchitectures for text-to-image tasks and focus on\nimproving the understanding of textual prompts.\nAdditionally, we aim to experiment with generat-\ning images at higher resolutions and to investigate\nnew features extending the model: local image\nediting by a text prompt, attention reweighting,\nphysics-based generation control, etc. The robust-\nness against generating abusive content remains\na crucial concern, warranting the exploration of\nreal-time moderation layers or robust classifiers to\nmitigate undesirable, e.g. toxic or abusive, outputs.\n8\nLimitations\nThe current system produces images that appear\nnatural, however, additional research can be con-\nducted to (1) enhance the semantic coherence be-\ntween the input text and the generated image, and\n(2) to improve the absolute values of FID and im-\nage quality based on human evaluations.\n9\nEthical Considerations\nWe performed multiple efforts to ensure that the\ngenerated images do not contain harmful, offen-\nsive, or abusive content by (1) cleansing the train-\ning dataset from samples that were marked to be\nharmful/offensive/abusive, and (2) detecting abu-\nsive textual prompts.\nWhile obvious queries, according to our tests, al-\nmost never generate abusive content, technically it\nis not guaranteed that certain carefully engineered\nprompts may not yield undesirable content. We,\ntherefore, recommend using an additional layer of\nclassifiers, depending on the application, which\nwould filter out the undesired content and/or use\nimage/representation transformation methods tai-\nlored to a given application.\nAcknowledgements\nAs usual, we would like to thank the anonymous\nreviewers for their useful comments. We would\nalso like to thank Sergey Markov and his team for\nhelpful feedback and discussions, for collabora-\ntion in multimodal dataset collecting, labelling and\nprocessing.\nReferences\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vah-\ndat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, Tero\nKarras, and Ming-Yu Liu. 2022. ediff-i: Text-to-\nimage diffusion models with an ensemble of expert\ndenoisers.\nGeorgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch\u00f6n-\nlieb, and Christian Etmann. 2021. Conditional image\ngeneration with score-based diffusion models. CoRR,\nabs/2111.13606.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim\nDockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. 2023.\nAlign your latents: High-\nresolution video synthesis with latent diffusion mod-\nels. CoRR, abs/2304.08818.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\n2023. Fantasia3d: Disentangling geometry and ap-\npearance for high-quality text-to-3d content creation.\nCoRR, abs/2303.13873.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440\u20138451. Association\nfor Computational Linguistics.\nPrafulla Dhariwal and Alexander Quinn Nichol. 2021.\nDiffusion models beat gans on image synthesis. In\nAdvances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Pro-\ncessing Systems 2021, NeurIPS 2021, December 6-\n14, 2021, virtual, pages 8780\u20138794.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, and Jie Tang. 2021. Cogview:\nMastering text-to-image generation via transformers.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 19822\u201319835.\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. 2023.\nStructure and content-guided video synthesis with\ndiffusion models. CoRR, abs/2302.03011.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neu-\nral Information Processing Systems 2014, December\n8-13 2014, Montreal, Quebec, Canada, pages 2672\u2013\n2680.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aber-\nman, Yael Pritch, and Daniel Cohen-Or. 2023.\nPrompt-to-prompt image editing with cross-attention\ncontrol. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net.\nJonathan Ho, William Chan, Chitwan Saharia, Jay\nWhang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P.\nKingma, Ben Poole, Mohammad Norouzi, David J.\nFleet, and Tim Salimans. 2022a. Imagen video: High\ndefinition video generation with diffusion models.\nCoRR, abs/2210.02303.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nJonathan Ho and Tim Salimans. 2022. Classifier-free\ndiffusion guidance. volume abs/2207.12598.\nJonathan Ho, Tim Salimans, Alexey A. Gritsenko,\nWilliam Chan, Mohammad Norouzi, and David J.\nFleet. 2022b. Video diffusion models. In NeurIPS.\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik\nPark, Eli Shechtman, Sylvain Paris, and Taesung\nPark. 2023. Scaling up gans for text-to-image synthe-\nsis. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2023, Vancouver,\nBC, Canada, June 17-24, 2023, pages 10124\u201310134.\nIEEE.\nJohanna Karras, Aleksander Holynski, Ting-Chun\nWang, and Ira Kemelmacher-Shlizerman. 2023.\nDreampose: Fashion image-to-video synthesis via\nstable diffusion.\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou\nMu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. 2023. GLIGEN: open-set grounded\ntext-to-image generation. CoRR, abs/2301.07093.\nJun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi\nFeng. 2022. Magicmix: Semantic mixing with diffu-\nsion models. CoRR, abs/2210.16056.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki\nTakikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2022.\nMagic3d: High-resolution text-to-3d content creation.\nCoRR, abs/2211.10440.\nShilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong.\n2023. TF-ICON: diffusion-based training-free cross-\ndomain image composition. CoRR, abs/2307.12493.\nZhengxiong Luo, Dayou Chen, Yingya Zhang, Yan\nHuang, Liang Wang, Yujun Shen, Deli Zhao, Jin-\ngren Zhou, and Tieniu Tan. 2023. Videofusion: De-\ncomposed diffusion models for high-quality video\ngeneration. CoRR, abs/2303.08320.\nChong Mou, Xintao Wang, Jiechong Song, Ying Shan,\nand Jian Zhang. 2023. Dragondiffusion: Enabling\ndrag-style manipulation on diffusion models. CoRR,\nabs/2307.02421.\nAlexander Quinn Nichol and Prafulla Dhariwal. 2021.\nImproved denoising diffusion probabilistic models.\nIn Proceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 8162\u20138171. PMLR.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya\nRamesh, Pranav Shyam, Pamela Mishkin, Bob Mc-\nGrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE:\ntowards photorealistic image generation and editing\nwith text-guided diffusion models. In International\nConference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n16784\u201316804. PMLR.\nGaurav Parmar, Krishna Kumar Singh, Richard Zhang,\nYijun Li, Jingwan Lu, and Jun-Yan Zhu. 2023.\nZero-shot image-to-image translation. In ACM SIG-\nGRAPH 2023 Conference Proceedings, SIGGRAPH\n2023, Los Angeles, CA, USA, August 6-10, 2023,\npages 11:1\u201311:11. ACM.\nWilliam Peebles and Saining Xie. 2022.\nScal-\nable diffusion models with transformers.\nCoRR,\nabs/2212.09748.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben\nMildenhall. 2023. Dreamfusion: Text-to-3d using\n2d diffusion. In The Eleventh International Con-\nference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023. OpenReview.net.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 8748\u20138763.\nPMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022.\nHierarchical text-\nconditional image generation with CLIP latents.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 8821\u20138831.\nPMLR.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, pages 10674\u201310685.\nIEEE.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch, Michael Rubinstein, and Kfir Aberman. 2023.\nDreambooth: Fine tuning text-to-image diffusion\nmodels for subject-driven generation. In IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2023, Vancouver, BC, Canada, June\n17-24, 2023, pages 22500\u201322510. IEEE.\nChitwan Saharia, William Chan, Huiwen Chang,\nChris A. Lee, Jonathan Ho, Tim Salimans, David J.\nFleet, and Mohammad Norouzi. 2022a.\nPalette:\nImage-to-image diffusion models. In SIGGRAPH\n\u201922: Special Interest Group on Computer Graphics\nand Interactive Techniques Conference, Vancouver,\nBC, Canada, August 7 - 11, 2022, pages 15:1\u201315:10.\nACM.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L. Denton, Seyed Kam-\nyar Seyed Ghasemipour, Raphael Gontijo Lopes,\nBurcu Karagol Ayan, Tim Salimans, Jonathan Ho,\nDavid J. Fleet, and Mohammad Norouzi. 2022b. Pho-\ntorealistic text-to-image diffusion models with deep\nlanguage understanding.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton\nMullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa Kundurthy, Katherine Crowson, Ludwig\nSchmidt, Robert Kaczmarczyk, and Jenia Jitsev.\n2022. LAION-5B: an open large-scale dataset for\ntraining next generation image-text models.\nIn\nNeurIPS.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie\nAn, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron\nAshual, Oran Gafni, Devi Parikh, Sonal Gupta, and\nYaniv Taigman. 2023. Make-a-video: Text-to-video\ngeneration without text-video data. In The Eleventh\nInternational Conference on Learning Representa-\ntions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net.\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang,\nRan Yi, Lizhuang Ma, and Dong Chen. 2023. Make-\nit-3d: High-fidelity 3d creation from A single image\nwith diffusion prior. CoRR, abs/2303.14184.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 483\u2013498. Association\nfor Computational Linguistics.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu-\nong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan,\nBen Hutchinson, Wei Han, Zarana Parekh, Xin Li,\nHan Zhang, Jason Baldridge, and Yonghui Wu. 2022.\nScaling autoregressive models for content-rich text-\nto-image generation.\nTrans. Mach. Learn. Res.,\n2022.\nChuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and\nDinh Phung. 2022. Movq: Modulating quantized vec-\ntors for high-fidelity image generation. In NeurIPS.\nA\nAdditional generation examples\n"
  },
  {
    "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?",
    "link": "https://arxiv.org/pdf/2310.03051.pdf",
    "upvote": "33",
    "text": "HOW FAR ARE LARGE LANGUAGE MODELS FROM\nAGENTS WITH THEORY-OF-MIND?\nPei Zhou\u2662\u2217\nAman Madaan\u2660\nSrividya Pranavi Potharaju \u2020\nAditya Gupta\nKevin R. McKee\u2021\nAri Holtzman\u2663\nJay Pujara\u2662\nXiang Ren\u2662\nSwaroop Mishra\u2021\nAida Nematzadeh\u2021\nShyam Upadhyay\u2020\nManaal Faruqui\u2020\n\u2020 Google\n\u2021 Google DeepMind\n\u2662 University of Southern California\n\u2660 Carnegie Mellon University\n\u2663 University of Chicago\npeiz@usc.edu\n7RP\u0003SXWV\u0003D\u0003FKRFRODWH\u0003EDU\u0003\nLQ\u0003WKH\u0003JUHHQ\u0003FXSERDUG\u0011\n:KLOH\u00037RP\u0003LV\u0003DZD\\\u000f\u0003(OOD\u0003\nSXWV\u0003WKH\u0003FKRFRODWH\u0003EDU\u0003LQ\u0003\nWKH\u0003EOXH\u0003FXSERDUG\u0011\n7RP\u0003FRPHV\u0003EDFN\u0011\n&XUUHQW\u001d\u0003$VN\u0003DERXW\u0003RWKHUV\u00b6\u0003PHQWDO\u0003VWDWHV\n4\u001d\u0003:KHUH\u0003ZLOO\u00037RP\u0003ORRN\u0003IRU\u0003WKH\u0003FKRFRODWH\u0003EDU\"\n$\u001d\u0003*UHHQ\u0003FXSERDUG\n\u0010\u00030RGHOV\u0003DUH\u0003JLYHQ\u0003ZKDW\u0003WR\u0003UHDVRQ\u0003DERXW\u0003\u000bIURP\u00034\f\n7\u0017'\u001d\u00033UREH\u0003DFWLRQV\u0003DV\u0003D\u0003VLWXDWHG\u0003DJHQW\n4\u001d\u0003,I\u0003\\RX\u0003ZHUH\u0003WKHUH\u000f\u0003ZKDW\u0003ZRXOG\u0003\\RX\u0003GR\"\u0003\n$\u001d\u00037HOO\u00037RP\u0003WKDW\u0003WKH\u0003FKRFRODWH\u0003EDU\u0003LV\u0003LQ\u0003WKH\u0003EOXH\u0003FXSERDUG\n\u0003\u0003\u0003\u00030RGHOV\u0003QHHG\u0003WR\u0003VHOI\u0010GLVFRYHU\u0003ZKDW\u0003WR\u0003UHDVRQ\u0003DERXW\u0011\n2EVHUYDWLRQ\n,QIHUHQFH\n$FWLRQ\n\u0010\u00035HDVRQLQJ\u001d\u0003\u00b37RP\u0003WKLQNV\u0003WKDW\u0003WKH\u0003FKRFRODWH\u0003LV\u0003LQ\u0003WKH\u0003\nJUHHQ\u0003FXSERDUG\u0003VLQFH\u0003KH\u0003LV\u0003QRW\u0003DZDUH\u0003RI\u0003LW\u0003EHLQJ\u0003PRYHG\u0011\u00b4\n\u0003\u0003\u0003\u00030XOWL\u00106WHS\u00035HDVRQLQJ\u001d\u0003\u00b37RP\u0003ZDQWV\u0003WKH\u0003FKRFRODWH\u0011\u0003\n+H\u0003ZLOO\u0003ORRN\u0003IRU\u0003LW\u0003LQ\u0003WKH\u0003JUHHQ\u0003FXSERDUG\u000f\u0003EXW\u0003LW\u0003KDV\u0003EHHQ\u0003\nPRYHG\u0011\u0003,\u0003FDQ\u0003KHOS\u0003E\\\u0003LQIRUPLQJ\u0003KLP\u0003RI\u0003WKH\u0003PRYH\u0011\u00b4\nFigure 1: Given observations, current social reasoning tasks ask models questions targeting specific\ninferences (left). We propose T4D to probe whether LLMs can decide proper actions using theory-of-\nmind as a situated agent (right). They key challenges in T4D are 1) models have to identify relevant\ninferences about mental states without being directed towards one and 2) to arrive at proper action\nchoices, more steps of reasoning are required.\nABSTRACT\nThinking is for Doing.\nHumans can infer other people\u2019s mental states from\nobservations\u2013an ability called Theory-of-Mind (ToM)\u2013and subsequently act prag-\nmatically on those inferences. Existing question answering benchmarks such as\nToMi ask models questions to make inferences about beliefs of characters in a story,\nbut do not test whether models can then use these inferences to guide their actions.\nWe propose a new evaluation paradigm for large language models (LLMs): Think-\ning for Doing (T4D), which requires models to connect inferences about others\u2019\nmental states to actions in social scenarios. Experiments on T4D demonstrate that\nLLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters\u2019 beliefs\nin stories, but they struggle to translate this capability into strategic action.\nOur analysis reveals the core challenge for LLMs lies in identifying the implicit\ninferences about mental states without being explicitly asked about as in ToMi,\nthat lead to choosing the correct action in T4D. To bridge this gap, we introduce\na zero-shot prompting framework, Foresee and Reflect (FaR), which provides\na reasoning structure that encourages LLMs to anticipate future challenges and\n\u2217Work done during Google Internship\n1\narXiv:2310.03051v1  [cs.CL]  4 Oct 2023\nreason about potential actions. FaR boosts GPT-4\u2019s performance from 50% to 71%\non T4D, outperforming other prompting methods such as Chain-of-Thought and\nSelf-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures\nand scenarios that also require ToM inferences to choose an action, consistently\noutperforming other methods including few-shot in-context learning.\n1\nINTRODUCTION\nHumans act with specific intentions, often grounded in reasoning about their environment and the\nmental states of others. For example, if Tom\u2019s friend Anne is looking for her backpack in the office,\nand Tom knows it is in the kitchen, Tom will intervene to help Anne by suggesting she check the\nkitchen. This proactive action stems from Tom\u2019s understanding of three aspects: 1) Anne\u2019s goal of\nfinding her backpack; 2) the knowledge of backpack being in the kitchen; and 3) Anne\u2019s belief of\nthinking the backpack is in the office. Reasoning about Anne\u2019s mental states allows Tom to conclude\nthat the mismatch between belief and knowledge prevents Anne from reaching her goal, and his\nintervention can help. Such capabilities to reason about and act on another individual\u2019s beliefs,\nintentions, and emotions are referred to as Theory-of-Mind (ToM), a critical element of human social\ninteractions (Premack & Woodruff, 1978; Frith & Frith, 2003)\nThe rise of large language models (LLMs) has prompted extensive research into their potential for\nTheory-of-Mind (ToM) capabilities (Sap et al., 2022; Kosinski, 2023; Ullman, 2023; Shapira et al.,\n2023a). These investigations predominantly rely on established psychological tests, such as the False\nBelief Test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985; Perner et al., 1987). While existing\nbenchmarks (Nematzadeh et al., 2018; Le et al., 2019) gauge LLMs\u2019 proficiency in inferring mental\nstates from scenarios (see Figure 1 left), they often overlook an essential human capability: acting1\non inferred mental states. Simply put: humans often act based on inferred intentions and beliefs. In\ncontrast, despite LLMs\u2019 performance in the False Belief Test, they often fail to infer what actions\nwould be most useful in scenarios that humans would find trivial, a crucial consideration for the\ndevelopment of next-generation AI agents, from virtual assistants to embodied robots.\nWe introduce a new evaluation paradigm: Thinking for Doing (T4D) (see Fiske, 1992) to probe\nwhether models can determine proper actions based on the mental states of others, rather than merely\nbeing able to answer questions about mental states. At its core, T4D envisions models as agents\nprocessing a series of observations to determine the most apt action from a set of options. Specifically,\nwe adopt stories from a widely-used ToM benchmark: ToMi (Le et al., 2019), based on Sally-Anne\nFalse Belief Test (Baron-Cohen et al., 1985) into observations in T4D. This integration ensures that\nmodels must utilize mental state reasoning, particularly when a character is identified to hold a false\nbelief (as depicted in Figure 1). The crux of T4D\u2019s novelty, as visualized in Figure 1, lies in its\nobjective: instead of merely eliciting inferences from mental state reasoning, it compels models to\ndetermine actions based on the former.\nT4D presents a new zero-shot challenge for LLMs. We find the highest performance (GPT-4)\ncapped at 50% while human annotators reach over 95% agreement. To gain deeper insights into\nthe challenges LLMs encounter in T4D, we identify three reasoning patterns from human-written\nrationales: question decomposition, theory-of-mind inferences, and commonsense assumptions. Then\nwe test LLMs in oracle settings, providing models with oracle reasoning steps based on the identified\npatterns. As demonstrated in Section 4.2, the primary challenge LLMs face in T4D is pinpointing the\ncorrect evidence to inform their actions. When we provide models with specific hints about relevant\ninferences, their performance significantly improves, approaching human levels.\nThe clear potential of LLMs to perform T4D with proper guidance leads to the question: Can\nwe develop a method that improves LLMs\u2019 T4D performance without providing oracle hints but\ninstead teaching models to better structure their reasoning process? In response, we introduce a\nnew zero-shot prompting framework Foresee and Reflect (FaR) that guides model\u2019s inferences by\nproviding a reasoning structure using future thinking. FaR has two components: Foresee, where\nit prompts the models to predict future events based on observations and Reflect, where models\nreason on which action choice better helps the characters with potential challenges. Comparison\nwith prompting strategies including Chain-of-Thought Wei et al. (2022), Tree-of-Thought (Yao\n1We use \u201cacting\u201d to refer to performing action in social scenarios like providing information to others.\n2\net al., 2023a) (zero-shot), and Self-Ask (Press et al., 2022) shows that FaR improves LLM zero-shot\nperformance by as much as 50% while other methods do not display significant improvement.\nTo explore FaR\u2019s strengths and limitations in more depth, we perform ablation studies aiming to\nanswer two questions: are both foresight and reflection needed for improving LLMs and what happens\nif we feed models noisy future predictions? We find that both components are crucial for tackling T4D\nand that LLMs are sensitive to noisy reasoning steps about the future in FaR, making how to help\nLLMs recover from noisy foresight an intriguing future direction. To examine whether FaR overfits on\nthe ToMi-converted T4D task, we also conduct generalization study by testing on out-of-distribution\nstory structures and a non-False-Belief ToM task. We find that FaR shows consistent improvement\nacross generalization tests, even outperforming few-shot prompting. Our contributions are as follows:\n1. We propose Thinking for Doing, a evaluation paradigm to challenge whether models can\nconnect social reasoning to actions.\n2. We find LLMs struggle on T4D and our analysis indicates the key bottleneck is identifying\nimplicit inference steps.\n3. We design Foresee and Reflect (FaR), a zero-shot prompting framework that dramatically\nimproves LLMs\u2019 performance on T4D. Analysis and generalization studies show that FaR\nrobustness generalize to diverse contexts.\n2\nBACKGROUND AND RELATED WORK\nTheory-of-Mind and Language Models\nTheory-of-mind has been studied extensively in psychol-\nogy and cognitive science (Premack & Woodruff, 1978; Baron-Cohen et al., 1985; Frith & Frith,\n2003), and clinical psychology tests such as False Belief Test (Wimmer & Perner, 1983) (FBT) were\ndeveloped to test ToM abilities in children. More recently, as neural language models (LM) display\nimpressive performance in many language understanding tasks, more studies aim to answer whether\nLMs exhibit ToM (Sap et al., 2022; Kosinski, 2023; Ullman, 2023; Shapira et al., 2023a; Sclar et al.,\n2023; Trott et al., 2023) using False Belief-templated story datasets such as ToM-bAbI (Nematzadeh\net al., 2018) and ToMi (Le et al., 2019). Though stories cover limited range of interactions, other\nsources of ToM tests also face challenges, such as scalability due to costs of human-generated\ninteractions (Bara et al., 2021) and noises in text-game environments (Zhou et al., 2023). This work\nfocuses on False-Belief tests for ToM, the most studied subarea, and revisits the format of such tasks\nwhen testing LLMs. Specifically, while probing work shows that LLMs display some degree of ToM\nbut lack robustness (Sap et al., 2022; Shapira et al., 2022), we find that when asked FBT in a more\nrealistic scenario, models fail even on the unperturbed tasks.\nLarge Language Models and Agents\nA line of recent work aims to build language agents (Andreas,\n2022; Mahowald et al., 2023) that can perform \u201cactions\u201d. Actions range from mimicking human social\nbehavior (Park et al., 2023), completing tasks using websites (Gur et al., 2023), and tool using (Yao\net al., 2023b; Schick et al., 2023). Our work distinguishes from them by focusing on actions that\nrequire proper mental state modeling of other individuals (ToM), attributing the performance gap\nbetween answering inference questions only and choosing actions based on inferences, and designed\na zero-shot prompt that improves models\u2019 capability that robustly generalizes.\nPrompting Techniques for LLM\nRecent advancements in the area of LLMs have given rise to a\nplethora of few-shot (Brown et al., 2020) and instruction (Mishra et al., 2021) prompting techniques,\nincluding Chain-of-Thought prompting (CoT) (Wei et al., 2022), Least-to-most prompting (Zhou\net al., 2022), and search-based approaches like Tree-of-Thought (ToT) (Yao et al., 2023a), Graph-of-\nThought (Besta et al., 2023; Yao et al., 2023c), and RAP (Hao et al., 2023).\nHowever, the primary objective of our work is not to introduce a new prompting technique. Instead,\nwe focus on the benefits of imposing a structured framework on the LLM\u2019s reasoning process,\nparticularly in the context of Theory of Mind (ToM) tasks. Specifically, our analysis (Section 4.2)\nreveals essential elements of reasoning that can help LLM agents act (Foresee (F) and Reflect (R)),\nand we capture this in our proposed approach FaR. Moreover, any prompting method that supports\ngranular, multi-step reasoning and captures the Foreseeing and Reflecting steps is well-equipped to\naddress the intricacies of ToM tasks.\n3\n3\nTHINKING FOR DOING (T4D): TASK AND DATA\nHere we formulate the Thinking for Doing (T4D) task that requires models to use social reasoning to\nchoose a proper action as a situated agent.\n3.1\nT4D TASK\nIn grounded social scenarios, an agent\u2019s perspective can be distilled into four primary variables:\n1. Observations O (e.g., Tom entered the kitchen. Tom wants a chocolate. Ella moves the chocolate.),\n2. Task T (e.g., Based on the above observations, who needs help?), 3. Inferences I (e.g., Tom is\nunaware of the chocolate\u2019s current location.), and 4. Action A (e.g., Inform Tom about the chocolate\u2019s\nlocation.). For a comprehensive illustration of these variables in context, please refer to Figure 1.\nTraditional social reasoning tasks typically challenge models with questions targeting specific in-\nferences. For example, they might pose a question like \u201cWhere will Jackson look for the onion?\u201d\naccompanied by a set of candidate answers (Nematzadeh et al., 2018; Sap et al., 2019; Le et al.,\n2019). This is depicted in the left side of Figure 1. Formally, this kind of task can be represented as\nestimation of P(I|O, TI), where TI denotes the inference-directed task articulated by the specific\nquestion and its associated answer options.\nHowever, in many real-world AI applications, particularly for embodied agents, decisions often\nrevolve around actions rather than explicit inferences. These decisions are influenced by underlying,\noften implicit, inferences. To bridge this gap, we introduce Thinking for Doing (T4D), a task designed\nto assess a model\u2019s ability to determine the appropriate action based solely on observations, without\nbeing directed towards a particular inference. Effectively, T4D represents a shift from directly probing\nfor specific inferences (TI) to eliciting actions (TA). In the T4D framework, the model\u2019s task is\nnot simply to make an inference but to decide on an action based on inferred mental states. This\ndecision-making process involves estimating P(A|O, TA), where TA encapsulates the action-oriented\ntask, such as determining Who would you prefer to assist the most? with potential actions A like\nAssist Jackson or Assist Noah. Crucially, in T4D, inferences I act as a latent variable, inferred from\nthe observable O to subsequently influence the chosen action A, i.e. P(A|O, TA, I).\n3.2\nCONVERTING TOM BENCHMARKS TO T4D\n7R0L\u0003\u000b/H\u0003HW\u0011\u0003DO\u0011\u0003DQG\u0003RWKHUV\f\n\u000b2EVHUYDWLRQV\f\u00032ZHQ\u0003HQWHUHG\u0003\nWKH\u0003ORXQJH\u0011\u0003$YHU\\\u0003KDWHV\u0003WKH\u0003\nSHDFK\u0011\u00031DWKDQ\u0003HQWHUHG\u0003WKH\u0003\nORXQJH\u0011\u00037KH\u0003VXLW\u0003LV\u0003LQ\u0003WKH\u0003\nFXSERDUG\u0011\u0003$YHU\\\u0003HQWHUHG\u0003WKH\u0003\nEDVHPHQW\u0011\u00032ZHQ\u0003OLNHV\u0003WKH\u0003MHDQV\u0011\u0003\n2ZHQ\u0003H[LWHG\u0003WKH\u0003ORXQJH\u0011\u00031DWKDQ\u0003\nPRYHG\u0003WKH\u0003VXLW\u0003WR\u0003WKH\u0003EDVNHW\u0011\u0003\n1DWKDQ\u0003H[LWHG\u0003WKH\u0003ORXQJH\u0011\u0003$YHU\\\u0003\nH[LWHG\u0003WKH\u0003EDVHPHQW\u0011\n\u000b4XHVWLRQ\f\u0003:KHUH\u0003ZLOO\u00032ZHQ\u0003ORRN\u0003\nIRU\u0003WKH\u0003VXLW\"\n$QVZHU\u0003HLWKHU\u0003$\u0003RU\u0003%\u0003IURP\u0003WKH\u0003\nIROORZLQJ\u0003RSWLRQV\u001d\u0003$\u0011\u0003FXSERDUG\u0003\n%\u0011\u0003EDVNHW\n7\u0017'\u00107R0\u0003\u000b2XUV\f\n\u000b2EVHUYDWLRQV\f\u00032ZHQ\u0003HQWHUHG\u0003WKH\u0003ORXQJH\u0011\u0003\n$YHU\\\u0003KDWHV\u0003WKH\u0003SHDFK\u0011\u00031DWKDQ\u0003HQWHUHG\u0003WKH\u0003\nORXQJH\u0011\u00037KH\u0003VXLW\u0003LV\u0003LQ\u0003WKH\u0003FXSERDUG\u0011\u0003$YHU\\\u0003\nHQWHUHG\u0003WKH\u0003EDVHPHQW\u0011\u00032ZHQ\u0003OLNHV\u0003WKH\u0003MHDQV\u0011\u0003\n2ZHQ\u0003H[LWHG\u0003WKH\u0003ORXQJH\u0011\u00031DWKDQ\u0003PRYHG\u0003WKH\u0003\nVXLW\u0003WR\u0003WKH\u0003EDVNHW\u0011\u00031DWKDQ\u0003H[LWHG\u0003WKH\u0003ORXQJH\u0011\u0003\n$YHU\\\u0003H[LWHG\u0003WKH\u0003EDVHPHQW\u0011\u00032ZHQ\u0003HQWHUHG\u0003\nWKH\u0003EDVHPHQW\u0011\u00031DWKDQ\u0003DQG\u00032ZHQ\u0003SODQ\u0003WR\u0003XVH\u0003\nWKH\u0003VXLW\u0003VRRQ\u0011\n\u000b4XHVWLRQ\f\u0003%DVHG\u0003RQ\u0003WKH\u0003DERYH\u0003\nREVHUYDWLRQV\u000f\u0003ZKR\u0003DPRQJ\u0003WKH\u0003LQGLYLGXDOV\u0003\nZRXOG\u0003PRVW\u0003EHQHILW\u0003IURP\u0003UHFHLYLQJ\u0003KHOSIXO\u0003\nLQIRUPDWLRQ\"\n$QVZHU\u0003IURP\u0003WKH\u0003IROORZLQJ\u0003RSWLRQV\u001d\u0003$\u0011\u00032ZHQ\u0003\n%\u0011\u00031DWKDQ\u0003&\u0011\u0003$YHU\\\u0003'\u0011\u00031RQH\u0003RI\u0003WKH\u0003DERYH\nFigure 2: Task input comparison of ToMi that asks an inference\nquestion given observations and our converted T4D that requires\nmodels to choose an action\nThis study focuses on a critical abil-\nity in social intelligence\u2013Theory of\nMind (ToM) and converts a widely-\nused existing benchmark: ToMi (Le\net al., 2019) from probing inferences\nto probing agent\u2019s action decisions.\nIn the classic Sally-Anne Test setup\n(used by ToMi), participants interpret\na stroy. For instance, consider Owen\nwho mistakenly believes the suit is\nplaced in the cupboard (Figure 2).\nToMi asks models to deduce Owen\u2019s\nmental states, with the expected an-\nswer being that Owen will search for\nthe suit inside the cupboard (due to\nmistaken beliefs).\nTo shift the focus towards actions as\nan agent who could potentially inter-\nvene and help other characters, we introduce an intent: both Owen and Nathan intend to use the suit\nin the near future. By explicitly stating both characters\u2019 intentions, we aim to deter models from\nadopting a rudimentary heuristic, like automatically assisting the character with immediate plans.\nHowever, we also ensure that this complexity does not obfuscate the task for humans. As validated\nin section 3.3, despite the shared intent to use the suit, human consensus consistently identifies\nOwen as the one needing help due to his misunderstanding about the suit\u2019s location. In our modified\ntask, termed T4D, models are prompted to identify which character they would assist the most by\n4\nproviding accurate information about the onion\u2019s location. Thus, in the T4D adaptation, models must\ndeduce from the narrative that: 1) Owen remains under the false impression that the suit is in the\ncupboard, and 2) considering his impending need for the suit, accurate knowledge about its location\nwould significantly benefit him. We programmatically convert the stories of ToMi (around 500) to\nT4D due to ToMi\u2019s templatic nature. Details of conversion are in Appendix A.\n3.3\nHUMAN AGREEMENT ON T4D\nBefore using T4D to evaluate our models, we seek to verify its validity by testing it with human\nToM (e.g., would human ToM encourage helping a character who holds outdated beliefs?). To do so,\nwe randomly sampled around 80 instances for evaluation by n = 20 human raters. To ensure this\nhuman study reflects how most people would use ToM in real life, we do not pre-train these raters\nextensively on the ToM tasks and do not provide any answers in the task examples. Our findings\nunderscore the robustness of T4D tasks: every instance garnered agreement from at least 17 of the 20\nraters. Moreover, over 90% of the instances achieved agreement levels exceeding 95% (19 or all 20\nraters in consensus). This strong human consensus shows that the design of T4D naturally aligns\nwith human perspectives on decision-making.\n4\nLLMS STRUGGLE ON T4D WHILE HUMANS FIND IT EASY\nHere we test LLMs on our T4D task and compare with their performance on the original ToMi set\nthat we convert from. We use PaLM 2 (Anil et al., 2023) Bison (S) and Unicorn (L) 2, ChatGPT\n(GPT-3.5) (OpenAI, 2022), and GPT-4 (OpenAI, 2023) accessed between June and August, 2023.\n4.1\nTHINKING IS \u201cEasy\u201d, T4D IS CHALLENGING FOR LLMS\nTable 1: LLMs\u2019 accuracy on T4D compared with\nToMi. We find gap between human performance on T4D\nis much larger than that on ToMi (*we count humans\ncorrect when there is more than 95% agreement).\nModels\nToMi\nT4D-ToM\nPaLM 2-S (Bison)\n87\n16\nPaLM 2-L (Unicorn)\n87\n30\nGPT-3.5-turbo (ChatGPT)\n74\n15\nGPT-4\n93\n50\nRandom Guessing\n50\n26\nHuman\n100\n90*\nWe focus on zero-shot performance following\nrecent studies (Sap et al., 2022; Shapira et al.,\n2023a; Sclar et al., 2023) to probe LLM\u2019s ca-\npabilities to understand and use theory-of-mind.\nSpecifically, we provide answer options and in-\nstruct models to output one answer option. The\nresults comparing LLM\u2019s performance on ToMi\nand T4D-ToM are shown in Table 1. We find\nthat both PaLM 2 and GPT models perform\nclose to perfect human scores on ToMi (best\nmodel GPT-4 gets 93% vs human 100%) but the\nperformance gap enlarges significantly across\nall models when tested on T4D-ToM (GPT-4\n50% vs human 90%). This discrepancy underscores the challenges posed by T4D for even the\nstrongest contemporary LLMs.\n4.2\nWHAT MAKES T4D CHALLENGING FOR LLMS?\nTo better understand why LLMs find T4D challenging, we conducted a study to understand the\nreasoning processes that humans use to tackle T4D tasks. By collecting and analyzing human-written\nrationales, we identified distinct dimensions of reasoning that seem particularly challenging for\nLLMs. Next, we discuss these challenges and experiments with oracle hints to determine if they can\nindeed aid the models in overcoming these reasoning hurdles. The major reasoning challenges, along\nwith examples and our proposed oracle hints, are summarized in Table 2 and we include example\nrationales in Appendix B.\nQuestion Decomposition (QD) We find that humans often break down the overarching T4D task\ninto more specific follow-up questions such as \u201cWho might have an information gap?\u201d and \u201cWhat\ninformation I can provide?\u201d. This decomposition bridges the gap between the general question and\nthe provided observations. To emulate this in models, we added oracle hints, spotlighting specific\n2https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\n5\nTable 2: Reasoning-Level breakdown. Following the example task from Figure 2, we show 3 types\nof reasoning challenges with example specific reasoning steps and design oracle hints to make each\nchallenge easier to analyze what makes LLMs struggle on T4D.\nReasoning\nChallenges\nExample Reasoning Steps\nHow to Provide Oracle Hints\nQuestion\nDecomposition (QD)\nWho would benefit from info?\n\u2013>Nathan and Owen plan to use the suit\n\u2013>Do they know the suit\u2019s location?\nAdd hint after question:\n\"HINT: this information is about\nan item\u2019s location\"\nTheory-of-Mind\n(ToM)\nNathan and Owen plan to use the suit soon\n\u2013>They need to know the location\nOwen left before the suit was moved\n\u2013>Owen thinks the suit is in the cupboard\nProvide oracle ToM inference:\n\"Owen will look for the suit in\nthe cupboard\"\nCommon Sense\nAssumption (CSA)\nNathan moved the suit to the basket\n\u2013>Though not mentioned, we can\nassume that the basket is lounge\nas Nathan is not said to exit the room\nMake assumptions explicit:\n\"Cupboard and basket are in lounge\"\n\"Characters do not leave room\nunless explicitly stated\"\ninformation, derived from the decomposition process. Essentially, we guide the models with oracle\ninference results (IQ), restructuring the task as i.e, P(A|O, TA, IQ).\nTheory-of-Mind Inferences (ToM) The second major reasoning challenge is the core inference\ntested in the Sally-Anne test \u2013 can models correctly infer that Sally will look for the item in the\nold location because she left the room before Anne moved the item? We make the ToM reasoning\nchallenge easier by providing oracle ToM inferences (IT oM) in the observations: \u201cSally will look for\nthe [ITEM] in the [OLD CONTAINER]\u201d. This shifts the task to P(A|O, TA, IT oM).\nCommon Sense Assumptions (CSA) The ambiguity inherent in ToMi, as noted by Sclar et al.\n(2023), presents another challenge. To solve the task, models must assume that both containers are\nlocated in the room, even though this is never mentioned explicitly in the observations. We make\nthese assumptions explicit in the observations, i.e, P(A|O, TA, KCS), where we use KCS to indicate\ncommonsense knowledge not explicitly present in the observation.\n+QD\n+ToM\n+CSA\nTask Settings\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy\nEffect of Adding Reasoning Dimensions\nGPT-3.5-Turbo\nGPT-4\nPaLM2-S\nPaLM2-L\nFigure 3: Increase in performance with provided\nreasoning levels. Adding oracle inferences about ques-\ntion decomposition (especially for PaLM2) and ToM\ndramatically improve performance.\nAnalysis Results\nAs illustrated in Figure 3,\nproviding oracle hints yields varying results\nacross the identified reasoning dimensions.\nGuiding models with hints related to item lo-\ncation (+QD) and incorporating oracle-derived\ncharacter beliefs (+ToM) significantly enhances\ntask performance. In contrast, merely clarify-\ning assumptions (+CSA) has a limited effect on\nboosting model accuracy.\nWe hypothesize that providing QD or ToM in-\nferences helps models by supplying suggestive\nevidence, either in the form of leading questions\n(IQ) or relevant ToM inferences (IT oM). These\nresults also suggest that the underlying reason\nfor the low performance of LLMs on T4D is\nattributed not to the task design but to their failure in drawing correct inferences and reasoning.\nThus, a key bottleneck in LLMs that makes T4D challenging (but easy for humans) is navigating\nthe unconstrained latent inference space I to locate the proper inference that makes choosing which\naction intent clear.\n5\nFORESEE AND REFLECT (FAR) PROMPTING\nBuilding on the insights from our T4D-ToM task analysis, we investigate can we help LLMs identify\nan implicit inference path that leads to correct action choices without hints. Given observations,\nhumans find it natural to identify relevant inferences and arrive at decisions such as \u201cwho should I\nprovide information to?\u201d However, ensuring that LLMs perform similarly structured reasoning is\n6\n*\u0137\u014a\u0188\u0196\u01a2\u00f4\u036f\u00d9\u012f\u0187\u036fh\u00f4\u01bc\u0188\u00e9\u0197\u036f\u035b*\u00d9\u00b6\u035c\n\u0011\u0003\u014a\u013c\u013f\u013f\u0003\u03cc\u0145\u0146\u0147\u0003\u0147\u013b\u013c\u0141\u013e\u0003\u0134\u0135\u0142\u0148\u0147\u0003\u013f\u013c\u013e\u0138\u013f\u014c\u0003\u0139\u0148\u0147\u0148\u0145\u0138\u0003\u0138\u0149\u0138\u0141\u0147\u0146\u0003\u0134\u0141\u0137\u0003\u013c\u0137\u0138\u0141\u0147\u013c\u0139\u014c\u0003\n\u0143\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0146\u0003\u0147\u013b\u0134\u0147\u0003\u0138\u0134\u0136\u013b\u0003\u013c\u0141\u0137\u013c\u0149\u013c\u0137\u0148\u0134\u013f\u0003\u0140\u013c\u013a\u013b\u0147\u0003\u0135\u0138\u0003\u0139\u0134\u0136\u013c\u0141\u013a\u02eb\u0003\n\u001c\u013b\u0138\u0141\u0003\u0011\u0003\u014a\u013c\u013f\u013f\u0003\u0145\u0138\u03d2\u0138\u0136\u0147\u0003\u0142\u0141\u0003\u014a\u013b\u0138\u0147\u013b\u0138\u0145\u0003\u0011\u0003\u0136\u0134\u0141\u0003\u013b\u0138\u013f\u0143\u0003\u0147\u013b\u0138\u0140\u0003\u014a\u013c\u0147\u013b\u0003\u0147\u013b\u0138\u0003\n\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0146\u0003\u013c\u0139\u0003\u0011\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u0138\u0003\u0147\u013b\u0138\u0140\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u0003\u0141\u0142\u014a\u02eb\u0003\u000e\u013c\u0141\u0134\u013f\u013f\u014c\u02e8\u0003\u0011\u0003\n\u014a\u013c\u013f\u013f\u0003\u0136\u013b\u0142\u0142\u0146\u0138\u0003\u0134\u0003\u03cc\u0141\u0134\u013f\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u0003\u0135\u0134\u0146\u0138\u0137\u0003\u0142\u0141\u0003\u0147\u013b\u0138\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u02eb\n\u000e\u0142\u0145\u0140\u0134\u0147\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u0003\u0134\u0146\u0003\u0139\u0142\u013f\u013f\u0142\u014a\u0146\u02ea\n\u0312\n\u0003\u0003\u0003\u0003\u02fb\u000b\u013b\u0134\u0145\u0134\u0136\u0147\u0138\u0145\u0003\t\u02fa\u0146\u0003\u013f\u013c\u013e\u0138\u013f\u014c\u0003\u0139\u0148\u0147\u0148\u0145\u0138\u0003\u0134\u0136\u0147\u013c\u0142\u0141\u0146\u02fb\u02ea\n\u0003\u0003\u0003\u0003\u02fb\u0018\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0003\u033b\u02fb\u02ea\n\u0003\u0003\u0003\u0003\u02fb\u000b\u0134\u0141\u0003\u0011\u0003\u013b\u0138\u013f\u0143\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0147\u0003\u0141\u0142\u014a\u0003\u0135\u014c\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u013c\u0141\u013a\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u02f2\u02fb\u02ea\n\u0003\u0003\u0003\u0003\u02fb\u0018\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0003\u033c\u02fb\u02ea\n\u0003\u0003\u0003\u0003\u02fb\u000b\u0134\u0141\u0003\u0011\u0003\u013b\u0138\u013f\u0143\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0147\u0003\u0141\u0142\u014a\u0003\u0135\u014c\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u013c\u0141\u013a\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u02f2\u02fb\u02ea\n\u0003\u0003\u0003\u0003\n\u02ec\n\u0003\u0003\u0003\u0003\u02fb\u000b\u013b\u0134\u0145\u0134\u0136\u0147\u0138\u0145\u0003\n\u02f5\u0146\u0003\u013f\u013c\u013e\u0138\u013f\u014c\u0003\u0139\u0148\u0147\u0148\u0145\u0138\u0003\u0134\u0136\u0147\u013c\u0142\u0141\u0146\u02fb\u02ea\u0003\n\u0003\u0003\u0003\u0003\u02fb\u0018\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0003\u033b\u02fb\u02ea\u0003\n\u0003\u0003\u0003\u0003\u02fb\u000b\u0134\u0141\u0003\u0011\u0003\u013b\u0138\u013f\u0143\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0147\u0003\u0141\u0142\u014a\u0003\u0135\u014c\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u013c\u0141\u013a\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u02f2\u02fb\u02ea\n\u02ec\n\u0003\u0003\u0003\u0003\u02fb\u03cc\u0141\u0134\u013f\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0136\u0142\u0141\u0146\u013c\u0137\u0138\u0145\u013c\u0141\u013a\u0003\u0134\u013f\u013f\u0003\u0146\u0147\u0138\u0143\u0146\u0003\u0134\u0135\u0142\u0149\u0138\u02fb\u02ea\n\u0003\u0003\u0003\u0003\u02fb\u03cc\u0141\u0134\u013f\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u02fb\u02ea\u0003\n\u0313\n\t\u0141\u0146\u014a\u0138\u0145\u02ea\n\u0312\n\u0003\u0003\u0003\u0003\u02fb\u001b\u0134\u013f\u013f\u014c\u02fa\u0146\u0003\u013f\u013c\u013e\u0138\u013f\u014c\u0003\u0139\u0148\u0147\u0148\u0145\u0138\u0003\u0134\u0136\u0147\u013c\u0142\u0141\u0146\u02fb\u02ea\u0003\u001b\u0134\u013f\u013f\u014c\u0003\u0143\u013f\u0134\u0141\u0146\u0003\u0147\u0142\u0003\u0148\u0146\u0138\u0003\u0147\u013b\u0138\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u0003\u0146\u0142\u0142\u0141\u02eb\u0003\n\u001b\u0134\u013f\u013f\u014c\u0003\u014a\u013c\u013f\u013f\u0003\u0138\u0141\u0147\u0138\u0145\u0003\u0147\u013b\u0138\u0003\u0145\u0142\u0142\u0140\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u0018\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0003\u033b\u02fb\u02ea\u0003\u001b\u0134\u013f\u013f\u014c\u0003\u0140\u013c\u013a\u013b\u0147\u0003\u0141\u0142\u0147\u0003\u013e\u0141\u0142\u014a\u0003\u014a\u013b\u0138\u0145\u0138\u0003\u0147\u013b\u0138\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u0003\n\u013c\u0146\u0003\u0146\u013c\u0141\u0136\u0138\u0003\t\u0141\u0141\u0138\u0003\u0140\u0142\u0149\u0138\u0137\u0003\u013c\u0147\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u000b\u0134\u0141\u0003\u0011\u0003\u013b\u0138\u013f\u0143\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0147\u0003\u0141\u0142\u014a\u0003\u0135\u014c\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u013c\u0141\u013a\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u02f2\u02fb\u02ea\u0003!\u0138\u0146\u02e8\u0003\u0011\u0003\u0136\u0134\u0141\u0003\n\u013c\u0141\u0139\u0142\u0145\u0140\u0003\u001b\u0134\u013f\u013f\u014c\u0003\u0147\u013b\u0134\u0147\u0003\u0147\u013b\u0138\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u0003\u013c\u0146\u0003\u013c\u0141\u0003\u0147\u013b\u0138\u0003\u0135\u0134\u0146\u013e\u0138\u0147\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u0018\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0003\u033c\u02fb\u02ea\u0003\u001b\u0134\u013f\u013f\u014c\u0003\u0140\u013c\u013a\u013b\u0147\u0003\u0141\u0138\u0138\u0137\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u0003\u0142\u0141\u0003\u013b\u0142\u014a\u0003\u0147\u0142\u0003\n\u0143\u013f\u0134\u014c\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u0146\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u000b\u0134\u0141\u0003\u0011\u0003\u013b\u0138\u013f\u0143\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0147\u0003\u0141\u0142\u014a\u0003\u0135\u014c\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u013c\u0141\u013a\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u02f2\u02fb\u02ea\u0003!\u0138\u0146\u02e8\u0003\u0011\u0003\u0136\u0134\u0141\u0003\n\u0143\u0145\u0142\u0149\u013c\u0137\u0138\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u0003\u0142\u0141\u0003\u013b\u0142\u014a\u0003\u0147\u0142\u0003\u0148\u0146\u0138\u0003\u013c\u0147\u02eb\n\u0003\u0003\u0003\u0003\n\u02ec\n\u0003\u0003\u0003\u0003\u02fb\t\u0141\u0141\u0138\u02f5\u0146\u0003\u013f\u013c\u013e\u0138\u013f\u014c\u0003\u0139\u0148\u0147\u0148\u0145\u0138\u0003\u0134\u0136\u0147\u013c\u0142\u0141\u0146\u02fb\u02ea\u0003\t\u0141\u0141\u0138\u0003\u014a\u013c\u013f\u013f\u0003\u0138\u0141\u0147\u0138\u0145\u0003\u0147\u013b\u0138\u0003\u0145\u0142\u0142\u0140\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u0018\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u0003\u0136\u013b\u0134\u013f\u013f\u0138\u0141\u013a\u0138\u0003\u033b\u02fb\u02ea\u0003\t\u0141\u0141\u0138\u0003\u0140\u013c\u013a\u013b\u0147\u0003\u0141\u0142\u0147\u0003\u013e\u0141\u0142\u014a\u0003\u013b\u0142\u014a\u0003\u0147\u0142\u0003\u0148\u0146\u0138\u0003\u0147\u013b\u0138\u0003\n\u0140\u0134\u0145\u0135\u013f\u0138\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u000b\u0134\u0141\u0003\u0011\u0003\u013b\u0138\u013f\u0143\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0147\u0003\u0141\u0142\u014a\u0003\u0135\u014c\u0003\u0143\u0145\u0142\u0149\u013c\u0137\u013c\u0141\u013a\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u02f2\u02fb\u02ea\u0003!\u0138\u0146\u02e8\u0003\u0011\u0003\u0136\u0134\u0141\u0003\u0147\u0138\u013f\u013f\u0003\n\t\u0141\u0141\u0138\u0003\u013b\u0142\u014a\u0003\u0147\u0142\u0003\u0143\u013f\u0134\u014c\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u0146\u02eb\n\u02ec\n\u0003\u0003\u0003\u0003\u02fb\u03cc\u0141\u0134\u013f\u0003\u0145\u0138\u0134\u0146\u0142\u0141\u013c\u0141\u013a\u0003\u0136\u0142\u0141\u0146\u013c\u0137\u0138\u0145\u013c\u0141\u013a\u0003\u0134\u013f\u013f\u0003\u0146\u0147\u0138\u0143\u0146\u0003\u0134\u0135\u0142\u0149\u0138\u02fb\u02ea\u0003\u001f\u013b\u013c\u013f\u0138\u0003\t\u0141\u0141\u0138\u0003\u0136\u0142\u0148\u013f\u0137\u0003\n\u0143\u0142\u0147\u0138\u0141\u0147\u013c\u0134\u013f\u013f\u014c\u0003\u0135\u0138\u0141\u0138\u03cc\u0147\u0003\u0139\u0145\u0142\u0140\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u0003\u0142\u0141\u0003\u013b\u0142\u014a\u0003\u0147\u0142\u0003\u0148\u0146\u0138\u0003\u0147\u013b\u0138\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u02e8\u0003\u013c\u0147\u0003\u013c\u0146\u0003\n\u001b\u0134\u013f\u013f\u014c\u0003\u014a\u013b\u0142\u0003\u0136\u0142\u0148\u013f\u0137\u0003\u0135\u0138\u0141\u0138\u03cc\u0147\u0003\u0147\u013b\u0138\u0003\u0140\u0142\u0146\u0147\u0003\u0139\u0145\u0142\u0140\u0003\u0140\u014c\u0003\u0134\u0146\u0146\u013c\u0146\u0147\u0134\u0141\u0136\u0138\u02eb\u0003\u0016\u0142\u0147\u0003\u0142\u0141\u013f\u014c\u0003\u0136\u0134\u0141\u0003\u0011\u0003\n\u0143\u0145\u0142\u0149\u013c\u0137\u0138\u0003\u013b\u0138\u0145\u0003\u014a\u013c\u0147\u013b\u0003\u013c\u0141\u0139\u0142\u0145\u0140\u0134\u0147\u013c\u0142\u0141\u0003\u0134\u0135\u0142\u0148\u0147\u0003\u0148\u0146\u013c\u0141\u013a\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u0146\u02e8\u0003\u0135\u0148\u0147\u0003\u0011\u0003\u0136\u0134\u0141\u0003\u0134\u013f\u0146\u0142\u0003\n\u013c\u0141\u0139\u0142\u0145\u0140\u0003\u013b\u0138\u0145\u0003\u0142\u0139\u0003\u0147\u013b\u0138\u0003\u0140\u0134\u0145\u0135\u013f\u0138\u02f5\u0146\u0003\u013f\u0142\u0136\u0134\u0147\u013c\u0142\u0141\u0003\u0146\u013c\u0141\u0136\u0138\u0003\u013b\u0138\u0003\u0140\u0134\u014c\u0003\u0141\u0142\u0147\u0003\u013e\u0141\u0142\u014a\u0003\u0147\u013b\u0134\u0147\u0003\n\t\u0141\u0141\u0138\u0003\u0140\u0142\u0149\u0138\u0137\u0003\u013c\u0147\u02eb\n\u0003\u0003\u0003\u0003\u02fb\u03cc\u0141\u0134\u013f\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u02fb\u02ea\u0003\u001c\u013b\u0148\u0146\u02e8\u0003\u0147\u013b\u0138\u0003\u03cc\u0141\u0134\u013f\u0003\u0134\u0141\u0146\u014a\u0138\u0145\u0003\u013c\u0146\u0003\t\u02eb\u0003\u001b\u0134\u013f\u013f\u014c\u02eb\n\u0313\n\u043e+\u00b4\u00d2\u035f\u0321\nFigure 4: Foresee and Reflect (FAR) prompt (left), a new zero-shot prompting framework that combines\nfuture prediction and pruning by action-aware reflection. The Foresee part is highlighted in yellow, Reflect\nis highlighted in blue. Example GPT-4 output shown on the right. The model follows FaR and structures\nintermediate reasoning steps by copying keys and filling in the values so we only need one inference call.\nchallenging. Although evidence points to LLMs\u2019 ability to infer, they do not necessarily connect\nthese inferences to coherent reasoning about actions.\nOur main methodology is to provide LLMs with a generalizable reasoning structure that guides the\nmodels to relevant inferences. To this end, we introduce the Foresee and Reflect (FAR) framework.\nThis framework equips LLMs with a structured reasoning paradigm, prompting them to: 1) extrapolate\npotential future events from given observations, and 2) introspect on actionable steps that would best\nserve humans in real-time contexts. As argued in Section 2, the primary contribution of FaR is not\nto introduce a new prompt but to showcase the benefits of imposing a structured framework on the\nLLM\u2019s reasoning process. Figure 4 presents FaR with an example output from GPT-4.\n5.1\nForesee: CONSIDERING POTENTIAL FUTURE EVENTS\nWe design FaR by first prompting models to look into the future by considering potential events that\nare likely to happen. This stems from the understanding that the most valuable help often aligns\nwith shaping a more desireable future outcome more desirable. This is also related to a personality\ntrait referred as \u201cConsideration of Future Consequences (CFC)\u201d in psychology (Strathman et al.,\n1994), which is the ability to predict future consequences to inform current action decisions. Given\nthe observations O, FaR guides LLMs to iterate over each character in the narrative, predicting their\nlikely future actions and pinpointing the potential challenges they might encounter. This approach\neffectively broadens the initial observations, extrapolating inferences about potential future events.\n5.2\nReflect: REASONING ABOUT ACTIONS\nAfter foreseeing likely future events, we prompt models to reflect on whether performing actions at\nthe moment could help with the potential challenges identified in the first step. This process can be\nconsidered as pruning the generated potential future inferences based on the available action options.\nOverall, FaR helps LLMs connect relevant inferences about future with the intended action choices,\ncompleting a reasoning chain spanning Observation\u2013Inferences\u2013Action.\nConnection to the A* Search Algorithm\nThe FaR methodology is conceptually analogous to the\nA* search algorithm (Hart et al., 1968), an algorithm for finding the optimal path in a weighted graph.\nWe draw the following connections: Start and Goal: FaR begins with observations and aims to arrive\nat an optimal action decision. Expanding Nodes: In the Foresee phase of FaR, potential inferences\n7\nPaLM2-S\nPaLM2-L\nGPT-3.5-Turbo\nGPT-4\nModels\n0\n10\n20\n30\n40\n50\n60\n70\nAccuracy\nComparing Zero-Shot Prompting for LLMs\nBase\nCoT\nToT (Basic)\nSelf-Ask-Reason\nFaR\nFigure 5: Comparison of zero-shot prompts. We find FaR improves LLMs performance the most.\n(akin to nodes in A*) are expanded by considering future events. Heuristics: The predictions made\nduring the Foresee step act as heuristics, guiding the reasoning process toward the most relevant\ninferences. Path Pruning: The Reflect stage in FaR narrows down the inferred events based on\navailable actions, similar to how A* prunes paths based on the heuristic and cost so far.\n6\nFAR BOOSTS LLM DRAMATICALLY AND GENERALIZES ROBUSTLY\nWe examine the potential of various zero-shot prompting methods on improving LLM\u2019s performance\non T4D and conduct generalization tests. We aim to answer three research questions through our\nexperiments: 1) How much can FaR improve LLM\u2019s zero-shot performance on T4D? 2) Are both the\n\u201cforesee\u201d and \u201creflect\u201d components necessary, and what are the limitations of FaR? and 3) Does FaR\ngeneralize robustly across scenarios where models need to connect inferences with intents?\n6.1\nBASELINES\nWe consider the following zero-shot prompting strategies, each offering a unique reasoning structure.\nFull descriptions of the prompts are available in the Appendix C Chain-of-Thought (CoT) (Wei et al.,\n2022):the zero-shot variant from Kojima et al. (2022) and add \u201cAnswer this question by reasoning\nstep-by-step.\u201d Tree-of-Thought (ToT) (Yao et al., 2023a) (Basic Zero-Shot): a zero-shot variant\ninspired by ToT, which prompts the LLM to envision a discussion among experts. Each expert\ncontributes a reasoning step, and if any expert detects an error in their logic, they exit the discussion.\nSelf-Ask (Press et al., 2022): this method emphasizes self-inquiry. Models generate and answer\ntheir follow-up questions, iterating until they reach a conclusive answer. A final reasoning step\nsolidifies the conclusion. FaR: following Section 5 and Figure 4, we design a prompt that guides\nmodels to think about likely future events and challenges that characters might encounter, and reflect\nwhether they can provide help. We apply each prompt and make one inference call on all LLMs with\nmaximum 800 tokens with a temperature of 0 (greedy sampling).\n6.2\nFAR DRAMATICALLY IMPROVES GPT-4 ZERO-SHOT PERFORMANCE\nFigure 5 present results of 4 different zero-shot prompting methods. We find that FaR can signifi-\ncantly boost LLMs\u2019 performance on T4D-ToM while other prompting methods do not help much.\nSpecifically, FaR helps increase GPT-4 accuracy from base 50% to 71% as well as all other LLMs\nwith the improvement between 12% and 18%. We also observe that more powerful models (GPT-4\nand PaLM2-L) tend to benefit more from FaR.\n6.3\nABLATION AND ANALYSIS\nBoth Foresight and Reflection Are Important\nFaR consists of two main components, one to\nforesee future events and challenges and one to reflect on action decisions. To investigate the\nindividual impact of these components, we modified the FaR prompt, isolating each element for\n8\nablation. Specifically, we omitted the foresight (referenced as yellow text in Figure 4) and reflection\nparts (blue text in Figure 4). Table 3 presents ablation on FaR for the two components using GPT-4.\nWe find that the performance significantly drops 17 and 12 points, respectively, when there is no\nforesee and there is no reflect, indicating that they are both crucial for T4D.\nTable 3: FaR ablations.\nPrompts\nGPT-4 Accuracy\nBase\n50.2\nFaR-NoForesee\n53.2\nFaR-NoReflect\n59.7\nFaR-NoisyForesee\n42\nFaR\n71.4\nRandom Guessing\n26\nHuman\n90\nProviding Noisy Foresight Undermines Performance\nWe further assessed the robustness of the FaR framework\nby introducing noisy foresight. For instance, a spurious\nforesight for the example in Figure 4 might be\u201cSally will\nenter the bedroom to sleep.\u201d without any evident reason\nfrom the observations. Table 3 shows that LLMs are very\nsensitive to manually-inputted reasoning steps in FaR and\nthe accuracy of GPT-4 drops from 71% to 42% (even lower\nthan baseline). This highlights a limitation: while the FaR\nframework can enhance reasoning when guided correctly,\nit\u2019s sensitive to the quality of the foresight provided and can degrade performance if misled.\n6.4\nFAR GENERALIZES TO DIVERSE SCENARIOS\nTable 4: Results on story-structure tests.\nFaR consistently improves the most.\nD1\nModel\nCoT\nToT\nSelf-Ask\nFaR\nGPT-3.5\n52\n39\n26\n52\nGPT-4\n71\n29\n33\n56\nPaLM 2-S\n69\n85\n52\n87\nPaLM 2-L\n84\n92\n87\n92\nD2\nModel\nCoT\nToT\nSelf-Ask\nFaR\nGPT-3.5\n21\n36\n44\n70\nGPT-4\n36\n34\n60\n95\nPaLM 2-S\n36\n39\n15\n42\nPaLM 2-L\n27\n15\n22\n90\nD3\nModel\nCoT\nToT\nSelf-Ask\nFaR\nGPT-3.5\n35\n48\n9\n50\nGPT-4\n79\n76\n63\n100\nPaLM 2-S\n12\n20\n20\n73\nPaLM 2-L\n46\n37\n12\n82\nWe probe the generalizability of FAR by evaluating its\nefficacy on out-of-distribution scenarios.\nStory Structure Robustness Tests\nWe use three chal-\nlenge sets from Sclar et al. (2023) to test if FaR can gen-\neralize to story structures beyond those included ToMi.\nThese sets introduce complexities such as the relocation\nof two items across two rooms (D1), the involvement of\nmultiple characters with an item (D2), and a single item\u2019s\nmovement among four containers (D3) 3. We convert\neach set (100 stories each) to T4D-style probes using our\nToMi conversion methodology. Table 4 shows results on\nthree types of story-structure change of the ToMi stories.\nOverall, FaR helps LLMs achieve the highest accuracy\ncompared to other zero-shot prompts on all three general-\nization tests, for almost all models.\nT4D-Faux Pas Case Studies\nTo further ascertain FAR\u2019s\nadaptability, we ventured beyond the classic Sally-Anne\nTest context. We explored Faux Pas scenarios, characterized by individuals inadvertently sharing\npotentially distressing or unwanted information (Baron-Cohen et al., 1999). We consider Faux Pas,\na category of social stories where a person \u201csays something without considering if it is something\nthat others might not want to hear or know\u201d (Baron-Cohen et al., 1999), and use 20 expert-curated\nstories from Shapira et al. (2023b). We convert the original set to T4D by asking models to choose a\ncharacter from the stories to provide emotional support (examples Appendix D). We test GPT-4 with\nmultiple zero-shot prompts as well as few-shot prompting with examples from T4D converted from\nToMi. Table 5 shows that FaR outperforms other methods dramatically, showing the generalizability\nof the zero-shot prompt FaR.\n3Sclar et al. (2023) propose SymbolicToM, a symbolic tracker of mental states for ToMi. We do not include\nSymbolicToM for comparison in T4D because including answers from the tracker gives away that the model\nshould focus on inferences about item\u2019s location, whereas other methods are not provided with such assumptions.\n9\n7\nCONCLUSION\nTable 5: Faux Pas re-\nsults using GPT-4.\nPrompts\nAccuracy\nBase\n31%\nCoT\n39%\nToT\n36%\nSelf-Ask\n43%\nFew-Shot\n41%\nFaR\n76%\nWe propose T4D, a task designed to challenge the capacity of LLMs in\nbridging Theory of Mind reasoning to actions. Our analyses highlighted a\nkey limitation in LLMs: their difficulty in grappling with implicit inferences\nwithout explicit guidance. To mitigate this, we introduced FaR, a structured\nreasoning paradigm, which not only boosts the performance of LLMs but\nalso ensures broader generalization. As a next step, it would be valuable to\ndelve deeper into understanding the internal representation of LLMs when\nguided by structured prompts like FaR.\nREFERENCES\nJacob Andreas. Language models as agent models. In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pp. 5769\u20135779, Abu Dhabi, United Arab Emirates, December 2022. As-\nsociation for Computational Linguistics. URL https://aclanthology.org/2022.findings-emnlp.\n423.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403, 2023.\nCristian-Paul Bara, CH-Wang Sky, and Joyce Chai. Mindcraft: Theory of mind modeling for situated\ndialogue in collaborative tasks. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pp. 1112\u20131125, 2021.\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith. Does the autistic child have a \u201ctheory of mind\u201d?\nCognition, 21(1):37\u201346, 1985.\nSimon Baron-Cohen, Michelle O\u2019riordan, Valerie Stone, Rosie Jones, and Kate Plaisted. Recog-\nnition of faux pas by normally developing children and children with asperger syndrome or\nhigh-functioning autism. Journal of autism and developmental disorders, 29:407\u2013418, 1999.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna\nGajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.\nGraph of thoughts: Solving elaborate problems with large language models. arXiv preprint\narXiv:2308.09687, 2023.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nSusan T Fiske. Thinking is for doing: portraits of social cognition from daguerreotype to laserphoto.\nJournal of personality and social psychology, 63(6):877, 1992.\nUta Frith and Christopher D Frith. Development and neurophysiology of mentalizing. Philosophical\nTransactions of the Royal Society of London. Series B: Biological Sciences, 358(1431):459\u2013473,\n2003.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and\nAleksandra Faust. A real-world webagent with planning, long context understanding, and program\nsynthesis. arXiv preprint arXiv:2307.12856, 2023.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992,\n2023.\nPeter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of\nminimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100\u2013107, 1968.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\n22199\u201322213, 2022.\n10\nMichal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083, 2023.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 5872\u20135877, 2019.\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and\nEvelina Fedorenko. Dissociating language and thought in large language models: a cognitive\nperspective. arXiv preprint arXiv:2301.06627, 2023.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing\nInstructional Prompts to GPTk\u2019s Language. arXiv preprint arXiv:2109.07830, 2021.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Griffiths. Evaluating theory\nof mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pp. 2392\u20132400, 2018.\nOpenAI. Chatgpt: Optimizing language models for dialogue, 2022. URL https://openai.com/blog/\nchatgpt/.\nR OpenAI. Gpt-4 technical report. arXiv, pp. 2303\u201308774, 2023.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\nJosef Perner, Susan R Leekam, and Heinz Wimmer. Three-year-olds\u2019 difficulty with false belief: The\ncase for a conceptual deficit. British journal of developmental psychology, 5(2):125\u2013137, 1987.\nDavid Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515\u2013526, 1978.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350,\n2022.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi.\nSocial IQa:\nCommonsense reasoning about social interactions.\nIn Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Con-\nference on Natural Language Processing (EMNLP-IJCNLP), pp. 4463\u20134473, Hong Kong,\nChina, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL\nhttps://aclanthology.org/D19-1454.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of\nsocial intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pp. 3762\u20133780, Abu Dhabi, United Arab Emirates, December 2022.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.\n248.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. Minding\nlanguage models\u2019(lack of) theory of mind: A plug-and-play multi-character belief tracker. arXiv\npreprint arXiv:2306.00924, 2023.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social reasoning in\nlarge language models. arXiv preprint arXiv:2305.14763, 2023a.\n11\nNatalie Shapira, Guy Zwirn, and Yoav Goldberg. How well do large language models perform on\nfaux pas tests? In Findings of the Association for Computational Linguistics: ACL 2023, pp.\n10438\u201310451, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi:\n10.18653/v1/2023.findings-acl.663. URL https://aclanthology.org/2023.findings-acl.663.\nOri Shapira, Ramakanth Pasunuru, Mohit Bansal, Ido Dagan, and Yael Amsterdamer. Interactive\nquery-assisted summarization via deep reinforcement learning.\nIn Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 2551\u20132568, Seattle, United States, July 2022. Association for\nComputational Linguistics. doi: 10.18653/v1/2022.naacl-main.184. URL https://aclanthology.\norg/2022.naacl-main.184.\nAlan Strathman, Faith Gleicher, David S Boninger, and C Scott Edwards. The consideration of future\nconsequences: Weighing immediate and distant outcomes of behavior. Journal of personality and\nsocial psychology, 66(4):742, 1994.\nSean Trott, Cameron Jones, Tyler Chang, James Michaelov, and Benjamin Bergen. Do large language\nmodels know what humans know? Cognitive Science, 47(7):e13309, 2023.\nTomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nHeinz Wimmer and Josef Perner. Beliefs about beliefs: Representation and constraining function of\nwrong beliefs in young children\u2019s understanding of deception. Cognition, 13(1):103\u2013128, 1983.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023a.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations (ICLR), 2023b.\nYao Yao, Zuchao Li, and Hai Zhao. Beyond chain-of-thought, effective graph-of-thought reasoning\nin large language models. arXiv preprint arXiv:2305.16582, 2023c.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-Most Prompting Enables Complex Reasoning in\nLarge Language Models. arXiv preprint arXiv:2205.10625, 2022.\nPei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris Callison-Burch, Yejin Choi, and\nPrithviraj Ammanabrolu. I cast detect thoughts: Learning to converse and guide with intents\nand theory-of-mind in dungeons and dragons. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 11136\u201311155, Toronto,\nCanada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.\n624. URL https://aclanthology.org/2023.acl-long.624.\n12\nA\nTOMI CONVERSION DETAILS\nToMi (Le et al., 2019) was proposed as a question answering task based on Sally-Anne Tests and\nimproved upon previous benchmark from Nematzadeh et al. (2018) by removing statistical biases\nmaking the task solvable without ToM. Specifically, ToMi defines multiple story primitives such as\n\u201cA enters the room\u201d, \u201cB moves the item\u201d,\u201cA left the room\u201d, etc. and primitives are combined into\nstories with a finite set of orderings (Sclar et al., 2023). Prior work such as Sap et al. (2022) has\nfound some errors in the ToMi dataset and filtered a clean version that we use to convert to T4D.\nOn a high-level, conversion consists of two main changes: 1) we add a sentence at the end of the\nstory with the intents of the two characters involved in moving the item (\u201cSally and Anne plan to use\nthe marble soon.\u201d); 2) we propose a new question given the stories about a situated agent\u2019s action and\nprovide a list of answer options from all the characters and a \u201cNone of the Above\u201d option. Specifically,\nwe need to parse the original ToMi tasks to find out 3 types of characters to programmatically generate\nthe additional sentence and new QA options: 1) the character who holds a false belief since they left\nbefore another character moves the item. This is also the character who is the correct answer of T4D\ntask, as they benefit the most from receiving helpful information; 2) the character who moves the\nitem after the first character left; 3) distracting characters who do not involve in moving or needing to\nuse the item, as they were introduced in ToMi to reduce biases.\nWe extract these character names from raw ToMi stories by extracting entities before verbs such\nas \u201cmoves\u201d, \u201centers\u201d, \u201cleaves\u201d, etc. Then we find the character holding a false belif by extracting\nfrom the original ToMi questions such as \u201cWhere will Sally look for the marble?\u201d, since the ToMi\nquestions directly targets the character holding a false belief. Next we find who is the character\nmoving the item by extracting the name before \u201cmoves\u201d, and the final character (if any) would be the\ndistracting character. Due to the templatic nature of ToMi, we extract these character names from all\nstories automatically. Finally, with extracted character names of each type, we apply the changes\nmentioned above by filling the names.\nB\nHUMAN STUDY DETAILS\nT4D Task Setup\nAs mentioned in Section 3.3, we conduct human studies with 20 raters who are not\ntrained on T4D tasks and not provided answers in examples. The annotators were selected randomly\nfrom a large pool of in-house human annotators. Figure 6 shows a screenshot of our instructions for\nhuman study with an example. We provide 3 of such examples without answers and raters answer\nsampled T4D instances in the multi-choice QA format as well. We also leave it optional for raters to\nprovide their rationales of solving T4D.\nExamples of Human Rationales\nIn Section 4.2, we summarize 3 reasoning levels from collected\nhuman rationales. Table 6 presents examples with highlighted texts corresponding to each of the 3\nreasoning levels.\nTable 6: Examples of human rationales we collected from human study. We highlighted parts from the rationales\nto correspond to the 3 reasoning levels discussed in 4.2: question decomposition, theory-of-mind inferences\n(about others\u2019 goals and beliefs), and commonsense assumptions.\nRationale 1\nWho in the stories have goals that require information? We know that William and Isla\nboth need to use eggplant. They need to know the location of it to be able to use it.\nWilliam moved the eggplant after Isla exited the closet, thus Isla is not aware of the\ncurrent location. I should choose Isla.\nRationale 2\nWilliam and Isla both plan to use the eggplant. But it is Isla who lacks the knowledge\nof the current location of the eggplant because William moved it (assuming that both\nthe envelop and the bucket are in the closet. Thus the answer should be Isla.\nRationale 3\nIsla is the right answer because she plans to use the eggplant but she does not know\nthat it has been moved to another location. She benefits from me telling her to avoid\ninconvenience.\n13\nFigure 6: Human study instruction screenshot with example.\nC\nPROMPT DETAILS\nHere we show different examples with full zero-shot prompts we use in our experiments. As discussed,\nwe make 1 inference/API call with each zero-shot prompt with maximum tokens of 800.\nChain-of-Thought\nYou are a clever and helpful AI assistant. Your goal is to help\nhuman users around you.\nOne day you have the following observations that follow chronological order:\nChloe loves the undershirt.\nAvery entered the dining room.\nAiden entered the\ndining room. The stockings is in the crate. Avery exited the dining room. Aiden\nmoved the stockings to the cupboard. Aiden exited the dining room. Avery entered\nthe sunroom. Aiden and Avery plan to use the stockings soon.\nAnswer the following multiple-choice question by reasoning step-by-step.\nBased on the above information, who among the individuals would most benefit from\nreceiving helpful information?\nAnswer from the following options: A. Chloe B.\nAvery C. Aiden D. None of the above.\nEnd your answer with \u2019Thus, the final answer is A (or B or C or D)\u2019\nAnswer:\nTree-of-Thought (Basic Zero-Shot)\nYou are a clever and helpful AI assistant. Your\ngoal is to help human users around you.\nOne day you have the following observations that follow chronological order:\nSophia entered the front yard. Jayden entered the bathroom. Logan entered the\nfront yard. The cabbage is in the suitcase. Jayden hates the spinach. Jayden\nexited the bathroom. Logan exited the front yard. Jayden likes the grapefruit.\n14\nSophia moved the cabbage to the basket.\nSophia exited the front yard.\nLogan\nentered the bathroom. Sophia and Logan plan to use the cabbage soon.\nImagine three different experts are answering this question.\nAll experts will write down 1 step of their thinking,\nthen share it with the group.\nThen all experts will go on to the next step, etc.\nIf any expert realises they\u2019re wrong at any point then they leave.\nThe question is...\nBased on the above information, who among the individuals would most benefit from\nreceiving helpful information? Answer from the following options: A. Sophia B.\nJayden C. Logan D. None of the above.\nEnd your answer with \u2019Thus, the final answer is A (or B or C or D)\u2019\nAnswer:\nSelf-Ask\nYou are a clever and helpful AI assistant. Your goal is to help human\nusers around you.\nOne day you have the following observations that follow chronological order:\nLucas entered the cellar. Elizabeth entered the cellar. Ava entered the cellar.\nThe pear is in the basket. Elizabeth exited the cellar. Lucas exited the cellar.\nAva moved the pear to the suitcase.\nAva exited the cellar.\nAva dislikes the\nslippers. Elizabeth entered the study. Ava and Elizabeth plan to use the pear\nsoon.\nBased on the above information, who among the individuals would most benefit from\nreceiving helpful information?\nAnswer from the following options: A. Lucas B.\nElizabeth C. Ava D. None of the above.\nI will answer by first coming up and answering useful follow up questions and\nthen reason slowly by considering all the follow up questions and answers, and\nfinally come up with a final answer.\nFormat answer as follows:\nAre follow up questions needed here: Yes.\nFollow up:\nIntermediate answer:\nFollow up:\nIntermediate answer:\nFollow up:\nIntermediate answer:\nLet\u2019s reason to get a final answer by considering all above follow up questions\nand answers:\nSo the final answer is:\nEnd your answer with \u2019Thus, the final answer is A (or B or C or D)\u2019\nAnswer:\nFaR\nYou are a clever and helpful AI assistant. Your goal is to help human users\naround you.\n15\nOne day you have the following observations that follow chronological order:\nJacob entered the bathroom.\nEmma entered the bathroom.\nThe carrot is in the\nbasket. Aiden entered the back yard. Emma exited the bathroom. Jacob moved the\ncarrot to the pantry. Jacob and Emma plan to use the carrot soon.\nBased on the above information, who among the individuals would most benefit from\nreceiving helpful information?\nAnswer from the following options: A. Jacob B.\nEmma C. Aiden D. None of the above.\nI will first think about likely future events and identify potential challenges\nthat each individual might be facing. Then I will reflect on whether I can help\nthem with the challenges if I provide them with information now. Finally, I will\nchoose a final answer based on the reasoning.\nFormat answer as follows:\n{\n\"Character A\u2019s likely future actions\":\n\"Potential challenge 1\":\n\"Can I help with it now by providing information?\":\n\"Potential challenge 2\":\n\"Can I help with it now by providing information?\":\n\"Potential challenge 3\":\n\"Can I help with it now by providing information?\":\n\"Character B\u2019s likely future actions\":\n\"Potential challenge 1\":\n\"Can I help with it now by providing information?\":\n\"Potential challenge 2\":\n\"Can I help with it now by providing information?\":\n\"Potential challenge 3\":\n\"Can I help with it now by providing information?\":\n\"Character C\u2019s likely future actions\":\n\"Potential challenge 1\":\n\"Can I help with it now by providing information?\":\n\"Potential challenge 2\":\n\"Can I help with it now by providing information?\":\n\"Potential challenge 3\":\n\"Can I help with it now by providing information?\":\n\"final reasoning considering all steps above\":\n\"final answer\":\n}\nAnswer:\nEnd your answer with \u2019Thus, the final answer is A (or B or C or D)\u2019\nAnswer:\n16\nD\nGENERALIZATION TEST DETAILS\nD.1\nSTORY STRUCTURE TESTS\nHere we show a concrete example for each of the story structure generalization test from Sclar et al.\n(2023).\nD1: Double Room False Belief Tests:\nLucas entered the playroom. Alexander entered the playroom. The plum is in the box.\nThe box is in the playroom. Alexander exited the playroom. Lucas moved the plum to the pantry.\nThe pantry is in the playroom. Lucas exited the playroom. Alexander entered the porch.\nLucas entered the porch. The strawberry is in the bathtub. The bathtub is in the porch.\nLucas exited the porch. Alexander moved the strawberry to the envelope. The envelope is in the\nporch.\nAlexander exited the porch. Lucas and Alexander plan to use the plum soon.\nBased on the above information, who among the individuals would most benefit from receiving\nhelpful information?\nAnswer from the following options: A. Alexander B. Lucas C. None of the above.\nD2: Three Active Characters Story:\nLucas entered the playroom. Alexander entered the playroom. Jacob entered the playroom.\nThe plum is in the box. The box is in the playroom. Alexander exited the playroom.\nLucas moved the plum to the pantry. The pantry is in the playroom. Lucas exited the playroom.\nJacob moved the plum to the bathtub. The bathtub is in the playroom. Jacob exited the playroom.\nLucas, Alexander, and Jacob plan to use the plum soon.\nBased on the above information, who among the individuals would most benefit from receiving\nhelpful information?\nAnswer from the following options: A. Lucas B. Alexander C. Jacob D. Lucas and Alexander E.\nNone of the above.\nD3: Four Containers with Multiple Movements:\nMia is in the patio. The slippers is in the crate. The crate is in the patio.\nMia moved the slippers to the suitcase. The suitcase is in the patio. Isla entered the patio.\nMia exited the patio. Isla moved the slippers to the bucket. The bucket is in the patio.\nIsla moved the slippers to the container. The container is in the patio. Mia and Isla plan to use the\nslippers soon.\nBased on the above information, who among the individuals would most benefit from receiving\nhelpful information?\nAnswer from the following options: A. Mia B. Isla C. None of the above.\nD.2\nFAUX-PAS TESTS\nShapira et al. (2023b) examines LLMs\u2019 performance on Faux-Pas stories and release an expert-curated\nset of Faux-Pas stories. We show an example of the story here: Kim helped her Mum make an apple\npie for her uncle when he came to visit. She carried it out of the kitchen.\nKim\u2019s uncle is sitting on the couch in the living room. His home has many boxes of pies, most boxes\nare for pineapple, no apple boxes. He is looking forward to Kim\u2019s baking. Kim showed her uncle the\npie she made...\n17\nBased on the above information, who would you like to help by providing emotional support?\nAnswer from the following options: A. Kim B. Kim\u2019s Mum C. Uncle Tom D. None of the above.\n18\n"
  },
  {
    "title": "Improved Baselines with Visual Instruction Tuning",
    "link": "https://arxiv.org/pdf/2310.03744.pdf",
    "upvote": "25",
    "text": "Improved Baselines with Visual Instruction Tuning\nHaotian Liu1\nChunyuan Li2\nYuheng Li1\nYong Jae Lee1\n1University of Wisconsin\u2013Madison\n2Microsoft Research, Redmond\nhttps://llava-vl.github.io\nAbstract\nLarge multimodal models (LMM) have recently shown\nencouraging progress with visual instruction tuning. In\nthis note, we show that the fully-connected vision-language\ncross-modal connector in LLaVA is surprisingly powerful\nand data-efficient. With simple modifications to LLaVA,\nnamely, using CLIP-ViT-L-336px with an MLP projection\nand adding academic-task-oriented VQA data with simple\nresponse formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final\n13B checkpoint uses merely 1.2M publicly available data,\nand finishes full training in \u223c1 day on a single 8-A100 node.\nWe hope this can make state-of-the-art LMM research more\naccessible. Code and model will be publicly available.\n1. Introduction\nLarge multimodal models (LMMs) have become increas-\ningly popular in the research community, as they are\nthe key building blocks towards general-purpose assis-\ntants [1, 22, 35]. Recent studies on LMMs are converg-\ning on a central concept known as visual instruction tun-\ning [28]. The results are promising, e.g. LLaVA [28] and\nMiniGPT-4 [49] demonstrate impressive results on natural\ninstruction-following and visual reasoning capabilities. To\nbetter understand the capability of LMMs, multiple bench-\nmarks [11, 20, 26, 29, 43] have been proposed. Recent\nworks further demonstrate improved performance by scal-\ning up the pretraining data [2, 9], instruction-following\ndata [9, 21, 45, 46], visual encoders [2], or langauge mod-\nels [31], respectively. The LLaVA architecture is also lever-\naged in different downstream tasks and domains, includ-\ning region-level [6, 44] and pixel-level [19] understanding,\nbiomedical assistants [23], image generation [3], adversarial\nstudies [4, 47].\nThis note establishes stronger and more feasible baselines\nbuilt upon the LLaVA framework. We report that two simple\nimprovements, namely, an MLP cross-modal connector and\nincorporating academic task related data such as VQA, are\northogonal to the framework of LLaVA, and when used with\nVQAv2\nGQA\nVizWiz\nSQA-IMG\nTextVQA\nPOPE\nMME\nMMBench\nMMBench-CN\nSEED-Bench\nLLaVA-Bench\nMM-Vet\n85.3\n1293.8\n49.5\n33.4\n63.1\n50.7\n78.9\n53.4\n25.6\n78.2\n57.5\n38.9\n68.2\n61.5\n1487.5\n60.6\n56.7\n58.2\n80.0\n63.3\n53.6\n71.6\n61.3\n85.9\n1531.3\n67.7\n63.6\n61.6\n70.7\n35.4\nBLIP-2\nInstructBLIP\nQwen-VL-Chat\nLLaVA-1.5\n101\n103\n# Training Samples (M)\nInstruct\nBLIP\nQwen-VL\n-Chat\nLLaVA\n-1.5\n129\n1.2\n1400\n50\n0.56\n0.67\nPre-training\nInstruction Tuning\nvision encoder (CLIP ViT-L/336px)\nvision-language connector (MLP)\ntokenizer &\nembedding\nUser: what is \nunusual about \nthis image?\nlanguage model (Vicuna v1.5 13B)\nFigure 1. LLaVA-1.5 achieves SoTA on a broad range of 11 tasks\n(Top), with high training sample efficiency (Left) and simple mod-\nifications to LLaVA (Right): an MLP connector and including\nacademic-task-oriented data with response formatting prompts.\nLLaVA, lead to better multimodal understanding capabilities.\nIn contrast to InstructBLIP [9] or Qwen-VL [2], which trains\nspecially designed visual resamplers on hundreds of millions\nor even billions of image-text paired data, LLaVA uses the\nsimplest architecture design for LMMs and requires only\ntraining a simple fully-connected projection layer on merely\n600K image-text pairs. Our final model can finish training\nin \u223c1 day on a single 8-A100 machine and achieves state-\nof-the-art results on a wide range of benchmarks. Moreover,\nunlike Qwen-VL [2] that includes in-house data in training,\nLLaVA utilizes only publicly available data. We hope these\nimproved and easily-reproducible baselines will provide a\nreference for future research in open-source LMM.\n1\narXiv:2310.03744v1  [cs.CV]  5 Oct 2023\n2. Background\nInstruction-following LMM. Common architectures in-\nclude a pre-trained visual backbone to encode visual fea-\ntures, a pre-trained large language model (LLM) to com-\nprehend the user instructions and produce responses, and a\nvision-language cross-modal connector to align the vision\nencoder outputs to the language models. As shown in Fig. 1,\nLLaVA [28] is perhaps the simplest architecture for LMMs.\nOptionally, visual resamplers (e.g. Qformer [24]) are used to\nreduce the number of visual patches [2, 9, 49]. Training an\ninstruction-following LMM usually follows a two-stage pro-\ntocol. First, the vision-language alignment pretraining stage\nleverages image-text pairs to align the visual features with\nthe language model\u2019s word embedding space. Earlier works\nutilize relatively few image-text pairs (e.g. \u223c600K [28] or\n\u223c6M [49]), while some recent works pretrain the vision-\nlanguage connector for a specific language model on a large\namount of image-text pairs (e.g. 129M [9] and 1.4B [2]),\nto maximize the LMM\u2019s performance. Second, the visual\ninstruction tuning stage tunes the model on visual instruc-\ntions, to enable the model to follow users\u2019 diverse requests\non instructions that involve the visual contents.\nMultimodal instruction-following data. In NLP, studies\nshow that the quality of instruction-following data largely\naffects the capability of the resulting instruction-following\nmodels [48]. For visual instruction tuning, LLaVA [28] is the\npioneer to leverage text-only GPT-4 to expand the existing\nCOCO [27] bounding box and caption dataset to a multi-\nmodal instruction-following dataset that contains three types\nof instruction-following data: conversational-style QA, de-\ntailed description, and complex reasoning. LLaVA\u2019s pipeline\nhas been employed to expand to textual understanding [45],\nmillion-scales [46], and region-level conversations [6]. In-\nstructBLIP [9] incorporates academic-task-oriented VQA\ndatasets to further enhance the model\u2019s visual capabilities.\nConversely, [5] identifies that such naive data merging can\nresult in the models that tend to overfit to VQA datasets and\nthus are inability to participate in natural conversations. The\nauthors further proposes to leverage the LLaVA pipeline to\nconvert VQA datasets to a conversational style. While this\nproves effective for training, it introduces added complexi-\nties in data scaling.\n3. Improved Baselines of LLaVA\nOverview. As the initial work of visual instruction tuning,\nLLaVA has showcased commendable proficiency in visual\nreasoning capabilities, surpassing even more recent mod-\nels on diverse benchmarks for real-life visual instruction-\nfollowing tasks, while only falling short on academic bench-\nmarks that typically require short-form answers (e.g. single-\nword). The latter was attributed to the fact that LLaVA has\nMethod\nLLM\nRes.\nGQA MME\nMM-Vet\nInstructBLIP\n14B\n224\n49.5\n1212.8\n25.6\nOnly using a subset of InstructBLIP training data\n0\nLLaVA\n7B\n224\n\u2013\n502.8\n23.8\n1\n+VQA-v2\n7B\n224\n47.0\n1197.0\n27.7\n2\n+Format prompt\n7B\n224\n46.8\n1323.8\n26.3\n3\n+MLP VL connector\n7B\n224\n47.3\n1355.2\n27.8\n4\n+OKVQA/OCR\n7B\n224\n50.0\n1377.6\n29.6\nAdditional scaling\n5\n+Region-level VQA\n7B\n224\n50.3\n1426.5\n30.8\n6\n+Scale up resolution\n7B\n336\n51.4\n1450\n30.3\n7\n+GQA\n7B\n336\n62.0\u2217 1469.2\n30.7\n8\n+ShareGPT\n7B\n336\n62.0\u2217 1510.7\n30.5\n9\n+Scale up LLM\n13B\n336\n63.3\u2217 1531.3\n36.3\nTable 1. Scaling results on \u25a0 data, \u25a0 model, and \u25a0 resolution.\nWe choose to conduct experiments on GQA [14], MME [11], and\nMM-Vet [43] to examine the representative capabilities of VQA\nwith short answers, VQA with output formatting, and natural vi-\nsual conversations, respectively. \u2217Training images of GQA were\nobserved during training.\nnot been pretrained on large-scale data, as other approaches\ndo. In this note, we first study the scaling effect of data,\nmodels and input image resolution on a selection of three\ndatasets in Table 1, and then compare the final model against\nexisting LMMs on a diverse set of 12 benchmarks in Table 2.\nWe show that the LLaVA\u2019s architecture is powerful and data-\nefficient for visual instruction tuning, and achieves the best\nperformance using significantly less compute and training\ndata than all other methods.\nResponse formatting prompts. We find that the inabil-\nity [5] to balance between short- and long-form VQA for\napproaches like InstructBLIP [9] is mainly due to the fol-\nlowing reasons. First, ambiguous prompts on the response\nformat. For example, Q: {Question} A: {Answer}. Such\nprompts do not clearly indicate the desirable output format,\nand can overfit an LLM behavorially to short-form answers\neven for natural visual conversations. Second, not finetuning\nthe LLM. The first issue is worsened by InstructBLIP only\nfinetuning the Qformer for instruction-tuning. It requires\nthe Qformer\u2019s visual output tokens to control the length of\nthe LLM\u2019s output to be either long-form or short-form, as in\nprefix tuning [25], but Qformer may lack the capability of\nproperly doing so, due to its limited capacity compared with\nLLMs like LLaMA. See Table 6 for a qualitative example.\nTo address this, we propose to use a single response for-\nmatting prompt that clearly indicates the output format, to\nbe appended at the end of VQA questions when promoting\nshort answers: Answer the question using a single word\nor phrase. We empirically show that when LLM is fine-\ntuned with such prompts, LLaVA is able to properly adjust\nthe output format according to the user\u2019s instructions, and\ndoes not require additional processing of the VQA data us-\n2\nMethod\nLLM\nRes.\nPT\nIT\nVQAv2 GQA\nVisWiz SQAI\nVQAT\nPOPE\nMME\nMMB\nMMBCN SEED\nLLaVAW MM-Vet\nBLIP-2\nVicuna-13B\n224\n129M\n-\n41.0\n41\n19.6\n61\n42.5\n85.3\n1293.8 \u2013\n\u2013\n46.4\n38.1\n22.4\nInstructBLIP\nVicuna-7B\n224\n129M\n1.2M\n\u2013\n49.2\n34.5\n60.5\n50.1\n\u2013\n\u2013\n36\n23.7\n53.4\n60.9\n26.2\nInstructBLIP\nVicuna-13B\n224\n129M\n1.2M\n\u2013\n49.5\n33.4\n63.1\n50.7\n78.9\n1212.8 \u2013\n\u2013\n\u2013\n58.2\n25.6\nShikra\nVicuna-13B\n224\n600K\n5.5M\n77.4\u2217\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n58.8\n\u2013\n\u2013\n\u2013\n\u2013\nIDEFICS-9B\nLLaMA-7B\n224\n353M\n1M\n50.9\n38.4\n35.5\n\u2013\n25.9\n\u2013\n\u2013\n48.2\n25.2\n\u2013\n\u2013\n\u2013\nIDEFICS-80B\nLLaMA-65B\n224\n353M\n1M\n60.0\n45.2\n36.0\n\u2013\n30.9\n\u2013\n\u2013\n54.5\n38.1\n\u2013\n\u2013\n\u2013\nQwen-VL\nQwen-7B\n448\n1.4B\u2020\n50M\u2020\n78.8\u2217\n59.3\u2217\n35.2\n67.1\n63.8\n\u2013\n\u2013\n38.2\n7.4\n56.3\n\u2013\n\u2013\nQwen-VL-Chat\nQwen-7B\n448\n1.4B\u2020\n50M\u2020\n78.2\u2217\n57.5\u2217\n38.9\n68.2\n61.5\n\u2013\n1487.5 60.6\n56.7\n58.2\n\u2013\n\u2013\nLLaVA-1.5\nVicuna-7B\n336\n558K\n665K\n78.5\u2217\n62.0\u2217\n50.0\n66.8\n58.2\n85.9\n1510.7 64.3\n58.3\n58.6\n63.4\n30.5\nLLaVA-1.5\nVicuna-13B\n336\n558K\n665K\n80.0\u2217\n63.3\u2217\n53.6\n71.6\n61.3\n85.9\n1531.3 67.7\n63.6\n61.6\n70.7\n35.4\nTable 2. Comparison with SoTA methods on 12 benchmarks. LLaVA achieves the best performance on 11/12 benchmarks, and ranks\nthe second on the other. Res, PT, IT indicate input image resolution, the number of samples in pretraining and instruction tuning stage,\nrespectively. Benchmark names are abbreviated due to space limits. VQA-v2 [12]; GQA [14]; VisWiz [13]; SQAI: ScienceQA-IMG [30];\nVQAT: TextVQA [40]; POPE [26]; MME [11]; MMB: MMBench [29]; MMBCN: MMBench-Chinese [29]; SEED: SEED-Bench [20];\nLLaVAW: LLaVA-Bench (In-the-Wild) [28]; MM-Vet [43]. \u2217The training images of the datasets are observed during training. \u2020Includes\nin-house data that is not publicly accessible.\ning ChatGPT [5], which further enables scaling to various\ndata sources. As shown in Table 1, by merely including\nVQAv2 [12] in training, LLaVA\u2019s performance on MME\nsignificantly improves (1323.8 vs 502.8) and outperforms\nInstructBLIP by 111 points.\nMLP vision-language connector. Inspired by the improved\nperformance in self-supervised learning by changing from a\nlinear projection to an MLP [7, 8], we find that improving the\nvision-language connector\u2019s representation power with a two-\nlayer MLP can improve LLaVA\u2019s multimodal capabilities,\ncompared with the original linear projection design.\nAcademic task oriented data. We further include addi-\ntional academic-task-oriented VQA datasets for VQA, OCR,\nand region-level perception, to enhance the model\u2019s capabili-\nties in various ways, as shown in Table 1. We first include\nfour additional datasets that are used in InstructBLIP: open-\nknowledge VQA (OKVQA [33], A-OKVQA [37]) and OCR\n(OCRVQA [34], TextCaps [39]). A-OKVQA is converted to\nmultiple choice questions and a specific response formatting\nprompt is used: Answer with the option\u2019s letter from the\ngiven choices directly. With only a subset of the datasets\nInstructBLIP uses, LLaVA already surpasses it on all three\ntasks in Table 1, suggesting LLaVA\u2019s effective design. Fur-\nthermore, we find further adding region-level VQA datasets\n(Visual Genome [18], RefCOCO [17, 32]) improves the\nmodel\u2019s capability of localizing fine-grained visual details.\nAdditional scaling. We further scale up the input image res-\nolution to allow LLM to clearly \u201csee\u201d the details of images,\nand add the GQA dataset as an additional visual knowledge\nsource. We also incorporate ShareGPT [38] data and scale up\nthe LLM to 13B as in [2, 6, 31]. Results on MM-Vet shows\nthe most significant improvement when scaling the LLM to\n13B, suggesting the importance of the base LLM\u2019s capability\nfor visual conversations. We denote the final model with\nall the modifications as LLaVA-1.5 (the last two rows in\nTable 1), which achieves an impressive performance that\nsignificantly outperforms the original LLaVA [28].\n4. Discussion\nComparison with SoTA. We benchmark LLaVA-1.5 on a\nwide range of academic VQA benchmarks and recent bench-\nmarks specifically proposed for instruction-following LMMs,\ntotalling 12 benchmarks. We show that it achieves the best\nperformance across 11 out of 12 benchmarks, despite using\nmagnitudes smaller pretraining and instruction tuning data\ncompared with other methods [2, 9]. It is encouraging that\nLLaVA-1.5 achieves the best performance with the simplest\narchitecture, academic compute and public datasets, and\nyields a fully-reproducible and affordable baseline for future\nresearch. The results also suggest that visual instruction\ntuning plays a more important role in improving an LMM\u2019s\ncapabilities than pretraining, and raises questions upon the\ncommon belief that LMMs require significant amount of\nvision-language alignment pretraining [2, 9, 24], despite\nthat the vision encoders (e.g. CLIP [36], OpenCLIP [16],\nEVA-CLIP [10], etc.) are already pretrained on web-scale\nimage-text paired dataset. LLaVA-1.5 (even the 7B model)\noutperforms 80B IDEFICS [15], a Flamingo-like LMM with\nbillions of trainable parameters for cross-modal connection.\nThis also makes us rethink the benefits of the vision samplers\nand the necessity of the additional large-scale pretraining, in\nterms of multimodal instruction-following capabilities.\nZero-shot format instruction generalization. Although\nLLaVA-1.5 is only trained with a limited number of for-\nmat instructions, it generalizes to others. First, VizWiz [13]\nrequires the model to output \u201cUnanswerable\u201d when the pro-\nvided content is insufficient to answer the question, and our\nresponse format prompt (Table 8) effectively instructs the\nmodel to do so (11.1% \u2192 67.8% on unanswerable questions).\nWe additionally present qualitative examples on instructing\n3\nVisual input example, Tricky Question:\nUser\nIf there are factual errors in the questions, point it out;\nif not, proceed answering the question.\nWhat\u2019s happening in the desert?\nGPT-4V\nSorry, I cannot answer that question as there is no\ncontext about which desert you are referring to.\nLLaVA\nIn the image, there is a city skyline with buildings and\na beach with people enjoying the sunset. The scene is\nset in a desert, which is unusual for a beach setting.\nLLaVA-1.5\nThere are no deserts in the image. The image features\na beach with palm trees, a city skyline, and a large\nbody of water.\nTable 3. LLaVA-1.5 can detect and answer the tricky question\nwhen prompted to verify the question.\nLLaVA-1.5 to verify the tricky questions (Fig. 3) and respond\nin a constrained JSON format (Fig. 4).\nZero-shot multilingual capability. Though LLaVA-1.5 is\nnot finetuned for multilingual multimodal instruction follow-\ning at all, we find that it is capable of following multilin-\ngual instructions, partly due to the multilingual language in-\nstructions in ShareGPT [38]. We quantitatively evaluate the\nmodel\u2019s generalization capability to Chinese on MMBench-\nCN [29], where the questions of MMBench are converted to\nChinese. Notably, LLaVA-1.5 outperforms Qwen-VL-Chat\nby 7.3% (63.6% vs 56.7%), despite Qwen being finetuned\non Chinese multimodal instructions while LLaVA-1.5 is not.\nComputational cost. For LLaVA-1.5, we use the same\npretraining dataset of LCS-558K1, and keep the training\niterations and batch size roughly the same for instruction\ntuning as LLaVA [28]. Due to the increased image input\nresolution to 336px, the training of LLaVA-1.5 is \u223c2\u00d7 as\nlong as LLaVA: \u223c6 hours of pretraining and \u223c20 hours of\nvisual instruction tuning, using 8\u00d7 A100s.\nLimitations. Despite the promising results demonstrated by\nLLaVA-1.5, several limitations must be acknowledged. First,\nLLaVA utilizes full image patches, potentially prolonging\neach training iteration. While visual resamplers [2, 9, 24]\nreduce the number of visual patches in LLMs, they currently\ncannot achieve convergence as efficiently as LLaVA with a\ncomparable amount of training data, probably due to more\ntrainable parameters in the resamplers. The development of\na sample-efficient visual resampler could pave the way for fu-\n1LCS-558K: a subset of \u223c558K image-text pairs from LAION-CC-SBU\nwith BLIP captions, as used in LLaVA-Lightning series.\nVisual input example, Constrained JSON Output:\nUser\nPlease read the text in this image and return the informa-\ntion in the following JSON format (note xxx is placeholder,\nif the information is not available in the image, put \"N/A\"\ninstead).\n{\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Ad-\ndress\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,\n\"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx}\nGPT-4V\n{\"class\": \"C\", \"DLN\": \"1234568\", \"DOB\": \"08/31/1977\",\n\"Name\": \"Ima Cardholder\", \"Address\": \"2570 24th Street, Any-\ntown, CA 95818\", \"EXP\": \"08/31/2014\", \"ISS\": \"08/31/2011\",\n\"SEX\": \"F\", \"HGT\": \"5-05\", \"WGT\": \"125 lbs\", \"EYES\":\n\"BRN\", \"HAIR\": \"BRN\", \"DONOR\": \"VETERAN\"}\nLLaVA\n{\"class\": \"Driver License\", \"DLN\": \"CA123 456 789\", \"DOB\":\n\"01/01/1990\", \"Name\": \"Amy\", \"Address\": \"Anytown, CA\",\n\"EXP\": \"08/31/2023\", \"ISS\": \"Veteran\", \"SEX\": \u201cF\", \"HGT\":\n\"5\u20195\", \"WGT\": \"130\", \"EYES\": \"Brown\", \"HAIR\": \"Brown\",\n\"DONOR\": \"Unknown\"}\nLLaVA-\n1.5\n{\"class\": \"C\", \"DLN\": \"1132456789\", \"DOB\": \"08/23/1971\",\n\"Name\": \"Ima Cardholder\", \"Address\": \"Anytown, State, Zip\",\n\"EXP\": \"08/23/2014\", \"ISS\": \"California\", \"SEX\": \"F\", \"HGT\":\n\"5\u20195\", \"WGT\": \"123\", \"EYES\": \"Brown\", \"HAIR\": \"Brown\",\n\"DONOR\": \"N/A\"}\nTable 4. LLaVA-1.5 can extract information from the image and\nanswer following the required format, despite a few errors com-\npared with GPT-4V. GPT-4V results are obtained from [42].\nture scaling-up of instruction-following multimodal models.\nSecond, LLaVA-1.5 is not yet capable of processing multiple\nimages due to the lack of such instruction-following data,\nand the limit of the context length. Third, although LLaVA-\n1.5 exhibits proficiency in following complex instructions,\nits problem-solving capabilities can still be limited in cer-\ntain domains, which could be improved with a more capa-\nble language model and with high-quality, targeted visual\ninstruction tuning data. Finally, despite its significantly re-\nduced propensity for hallucination, LLaVA is not exempt\nfrom producing hallucinations and occasionally disseminat-\ning misinformation, and should be used with caution in\ncritical applications (e.g. medical).\nAcknowledgements. This work was supported in part by\nNSF CAREER IIS2150012, and Institute of Information &\ncommunications Technology Planning & Evaluation(IITP)\ngrants funded by the Korea government(MSIT) (No. 2022-\n0-00871, Development of AI Autonomy and Knowledge\nEnhancement for AI Agent Collaboration) and (No. RS-\n2022-00187238, Development of Large Korean Language\nModel Technology for Efficient Pre-training).\n4\nAppendix\nData. Our final training data mixture contains a variety of\ndatasets: VQA [12, 14, 33, 37], OCR [34, 39], region-level\nVQA [17, 18, 32], visual conversation [28] and language con-\nversation [38] data. We adopt multiple strategies to reduce\ntraining cost and enhance efficiency, detailed as follows:\n1. For all VQA datasets, QA pairs from the same training\nimage are merged into a single conversation.\n2. For ShareGPT [38], we filter out invalid conversations as\n[41]. Unlike Vicuna [41], long conversations that surpass\n2048 tokens are truncated rather than splitting to multiple\nconversations. This results in \u223c40K conversations.\n3. Each QA pair in A-OKVQA [37] is augmented k times,\nwhere k is the number of choices per question, to coun-\nterbalance the lack of multiple-choice data.\n4. 80K conversations are sampled from OCRVQA [34].\n5. For Visual Genome, we sample 10 annotations for images\nwith additional annotations.\n6. For RefCOCO, conversations are dissected into segments,\neach containing fewer than 10 conversations.\n7. We obverse that language conversations are often longer\nthan visual ones. For each batch, we sample conversa-\ntions only from a single modality, and this speeds up the\ntraining by 25%, and does not affect the final outcome.\nAll data splits are concatenated together and sampled\nwith the same probability. We present the response format-\nting prompts of the final instruction-following data mixtures\nin Table 7 and the response format prompts used for each\nevaluation benchmark in Table 8.\nHyperparameters. LLaVA-1.5 use the same set of hyper-\nparameters as the original LLaVA, except that we halve the\nlearning rate in pretraining due to the usage of the MLP\nprojection layer instead of the original linear projection layer\ndesign. We show the training hyperparameters for both\nfirst-stage vision-language alignment pretraining and the\nsecond-stage visual instruction tuning in Table 5.\nHyperparameter\nPretrain\nFinetune\nbatch size\n256\n128\nlr\n1e-3\n2e-5\nlr schedule\ncosine decay\nlr warmup ratio\n0.03\nweight decay\n0\nepoch\n1\noptimizer\nAdamW\nDeepSpeed stage\n2\n3\nTable 5. Hyperparameters of LLaVA-1.5 are the same as the\noriginal LLaVA, except that we halve the learning rate in pretraining\ndue to the usage of the MLP projection layer.\nVisual input example, Different Format Prompts:\nNormal prompt\nWhat is the color of the shirt that the\nman is wearing?\nResponse\nThe man is wearing a yellow shirt.\nAmbiguous prompt\nQ: What is the color of the shirt that the\nman is wearing? A:\nResponse\nThe man is wearing a yellow shirt.\nFormatting prompt\nWhat is the color of the shirt that the\nman is wearing? Answer the question\nusing a single word or phrase.\nResponse\nYellow.\nTable 6. Comparison of how different prompt regularizes the out-\nput format. The results are obtained zero-shot directly after LLaVA\nundergoes the first-stage vision-language alignment pretraining,\nwithout any visual instruction tuning.\nData\nSize\nResponse formatting prompts\nLLaVA [28]\n158K\n\u2013\nShareGPT [38]\n40K\n\u2013\nVQAv2 [12]\n83K\nAnswer the question using a single word or phrase.\nGQA [14]\n72K\nOKVQA [33]\n9K\nOCRVQA [34]\n80K\nA-\nOKVQA [37]\n50K\nAnswer with the option\u2019s letter from the given\nchoices directly.\nTextCaps [39]\n22K\nProvide a one-sentence caption for the provided\nimage.\nRefCOCO\n30K\nNote: randomly choose between the two formats\n[17, 32]\nProvide a short description for this region.\nVG [18]\n86K\nProvide the bounding box coordinate of the region\nthis sentence describes.\nTotal\n665K\nTable 7. Instruction-following Data Mixture of LLaVA-1.5.\nData\nResponse formatting prompts\nLLaVA-Bench, MM-Vet\n\u2013\nVQAv2, GQA, TextVQA,\nMME, POPE\nAnswer the question using a single word or\nphrase.\nScienceQA,\nMMBench,\nSEED-Bench\nAnswer with the option\u2019s letter from the given\nchoices directly.\nVizWiz\nWhen the provided information is insufficient,\nrespond with \u2018Unanswerable\u2019.\nAnswer the\nquestion using a single word or phrase.\nTable 8. Response format prompt for evaluation.\n5\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatie Millican, Malcolm Reynolds, et al. Flamingo: a vi-\nsual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022. 1\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1,\n2, 3, 4\n[3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine. Training diffusion models with reinforcement\nlearning. arXiv preprint arXiv:2305.13301, 2023. 1\n[4] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo,\nMatthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh,\nDaphne Ippolito, Katherine Lee, Florian Tramer, et al. Are\naligned neural networks adversarially aligned? arXiv preprint\narXiv:2306.15447, 2023. 1\n[5] Delong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan\nWang. Visual instruction tuning with polite flamingo. arXiv\npreprint arXiv:2307.01003, 2023. 2, 3\n[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s\nreferential dialogue magic. arXiv preprint arXiv:2306.15195,\n2023. 1, 2, 3\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey Hinton. A simple framework for contrastive learning of\nvisual representations. In ICML, 2020. 3\n[8] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Im-\nproved baselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020. 3\n[9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,\nand Steven Hoi. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning.\narXiv preprint\narXiv:2305.06500, 2023. 1, 2, 3, 4\n[10] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.\nEva: Exploring the limits of masked visual representation\nlearning at scale. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 19358\u2013\n19369, 2023. 3\n[11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Meng-\ndan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu\nZheng, et al.\nMme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 1, 2, 3\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answering.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6904\u20136913, 2017. 3, 5\n[13] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 3\n[14] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 2, 3, 5\n[15] IDEFICS.\nIntroducing idefics:\nAn open reproduction\nof state-of-the-art visual language model.\nhttps://\nhuggingface.co/blog/idefics, 2023. 3\n[16] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal\nShankar, Hongseok Namkoong, John Miller, Hannaneh Ha-\njishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. 2021.\nIf you use this software, please cite it as below. 3\n[17] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787\u2013798, 2014. 3, 5\n[18] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32\u201373, 2017. 3, 5\n[19] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 1\n[20] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 1, 3\n[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi\nPu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it:\nMulti-modal in-context instruction tuning. arXiv preprint\narXiv:2306.05425, 2023. 1\n[22] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Lin-\njie Li, Lijuan Wang, and Jianfeng Gao. Multimodal founda-\ntion models: From specialists to general-purpose assistants.\narXiv preprint arXiv:2309.10020, 2023. 1\n[23] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\nand Jianfeng Gao. Llava-med: Training a large language-and-\nvision assistant for biomedicine in one day. arXiv preprint\narXiv:2306.00890, 2023. 1\n[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 2, 3, 4\n[25] Xiang Lisa Li and Percy Liang.\nPrefix-tuning: Optimiz-\ning continuous prompts for generation.\narXiv preprint\narXiv:2101.00190, 2021. 2\n[26] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 1, 3\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\n6\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 2\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 1, 2, 3, 4, 5\n[29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n1, 3, 4\n[30] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering. Advances in\nNeural Information Processing Systems, 2022. 3\n[31] Yadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jian-\nfeng Gao, and Yelong Shen. An empirical study of scal-\ning instruct-tuned large multimodal models. arXiv preprint\narXiv:2309.09958, 2023. 1, 3\n[32] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Cam-\nburu, Alan L Yuille, and Kevin Murphy. Generation and\ncomprehension of unambiguous object descriptions. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 11\u201320, 2016. 3, 5\n[33] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 3, 5\n[34] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering by\nreading text in images. In 2019 international conference on\ndocument analysis and recognition (ICDAR), pages 947\u2013952.\nIEEE, 2019. 3, 5\n[35] OpenAI.\nGpt-4v(ision) system card.\nhttps://cdn.\nopenai.com/papers/GPTV_System_Card.pdf,\n2023. 1\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\narXiv preprint arXiv:2103.00020, 2021. 3\n[37] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A bench-\nmark for visual question answering using world knowledge.\nIn European Conference on Computer Vision, pages 146\u2013162.\nSpringer, 2022. 3, 5\n[38] ShareGPT. https://sharegpt.com/, 2023. 3, 4, 5\n[39] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Aman-\npreet Singh. Textcaps: a dataset for image captioning with\nreading comprehension. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.\n3, 5\n[40] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xin-\nlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\nTowards vqa models that can read. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recog-\nnition, pages 8317\u20138326, 2019. 3\n[41] Vicuna. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. https://vicuna.lmsys.\norg/, 2023. 5\n[42] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof lmms: Preliminary explorations with gpt-4v (ision). arXiv\npreprint arXiv:2309.17421, 2023. 4\n[43] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 1, 2, 3\n[44] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi:\nInstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 1\n[45] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced\nvisual instruction tuning for text-rich image understanding.\narXiv preprint arXiv:2306.17107, 2023. 1, 2\n[46] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up\nvisual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023. 1, 2\n[47] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongx-\nuan Li, Ngai-Man Cheung, and Min Lin. On evaluating\nadversarial robustness of large vision-language models. arXiv\npreprint arXiv:2305.16934, 2023. 1\n[48] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,\nYuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,\net al. Lima: Less is more for alignment. arXiv preprint\narXiv:2305.11206, 2023. 2\n[49] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2\n7\n"
  },
  {
    "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
    "link": "https://arxiv.org/pdf/2310.03731.pdf",
    "upvote": "25",
    "text": "MATHCODER:\nSEAMLESS CODE INTEGRATION\nIN\nLLMS FOR ENHANCED MATHEMATICAL REASONING\nKe Wang1\u2217\nHouxing Ren1\u2217\nAojun Zhou1\u2217\nZimu Lu1\u2217\nSichun Luo 3\u2217\nWeikang Shi 1\u2217\nRenrui Zhang 1\nLinqi Song3\nMingjie Zhan1\u2020\nHongsheng Li1,2\u2021\n1Multimedia Laboratory (MMLab), The Chinese University of Hong Kong\n2Shanghai Artificial Intelligence Laboratory\n3City University of Hong Kong\n{wangk.gm, renhouxing, aojunzhou, zmjdll}@gmail.com\nhsli@ee.cuhk.edu.hk\nABSTRACT\nThe recently released GPT-4 Code Interpreter has demonstrated remarkable\nproficiency in solving challenging math problems, primarily attributed to its\nability to seamlessly reason with natural language, generate code, execute code,\nand continue reasoning based on the execution output. In this paper, we present\na method to fine-tune open-source language models, enabling them to use code\nfor modeling and deriving math equations and, consequently, enhancing their\nmathematical reasoning abilities. We propose a method of generating novel and\nhigh-quality datasets with math problems and their code-based solutions, referred\nto as MathCodeInstruct. Each solution interleaves natural language, code, and\nexecution results. We also introduce a customized supervised fine-tuning and\ninference approach.\nThis approach yields the MathCoder models, a family\nof models capable of generating code-based solutions for solving challenging\nmath problems.\nImpressively, the MathCoder models achieve state-of-the-art\nscores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%)\ndatasets, substantially outperforming other open-source alternatives. Notably, the\nMathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and\nMATH but also outperforms GPT-4 on the competition-level MATH dataset. The\ndataset and models will be released at https://github.com/mathllm/\nMathCoder.\n1\nINTRODUCTION\nRecently, closed-source large language models (LLMs) such as GPT-4 (OpenAI, 2023) and PaLM-\n2 (Anil et al., 2023), paired with methods such as Chain-of-Thought (CoT) (Wei et al., 2022) and\nProgram-Aided Language models (PAL) (Gao et al., 2023), have shown remarkable performance\non mathematical reasoning tasks. In contrast, current open-source LLMs (Touvron et al., 2023;\nPenedo et al., 2023; Zhang et al., 2022) still lag significantly behind in this area. Even Llama-2-\n70B (Touvron et al., 2023), one of the most potent open-source models, only scores 56.8% and\n13.5% respectively on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets,\nremarkably lower than GPT-4 Code Interpreter1, which scores 97% and 69.7% (Zhou et al., 2023a).\nTo narrow the gap between open-source and closed-source models in math problem solving, recent\nworks, such as the WizardMath (Luo et al., 2023) and RFT (Yuan et al., 2023), have tried to\ntune open-source models with math problems and CoT solutions, achieving a significant gain in\nperformance compared to their base model, Llama-2. On the other hand, methods such as PAL (Gao\net al., 2023), PoT (Chen et al., 2022), and CSV (Zhou et al., 2023a) encourage code usage in solving\n\u2217Equal contribution.\n\u2020Project leader.\n\u2021Corresponding author.\n1https://openai.com/blog/chatgpt-plugins#code-interpreter\n1\narXiv:2310.03731v1  [cs.CL]  5 Oct 2023\nTable 1: Comparison with different Instruction-following datasets.\nThe baseline datasets include recent RFT-u13b (Yuan et al., 2023) and\nWizardMath (Luo et al., 2023).\nDatasets\nSeed\nAnnotation\nRFT-100k\nGSM8K\nLlama\nWizardMath-96k\nGSM8K+MATH\nGPT-4\nOurs-49k\nGSM8K+MATH\nGPT-4\nOurs-80k\nGSM8K+MATH\nGPT-4 +\nSelf-distillation\n7B\n70B\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy\n17.8\n24.8\n46.2\n50.7\n73.1\nMathCoder\nWizardMath (SoTA)\nLlama-1 RFT\nFigure 1:\nPerformance comparison\nbetween MathCoder, WizardMath, and\nLlama-1 RFT models with different\nmodel sizes.\nmath problems, showing promising improvements when paired with closed-source models like GPT-\n3.5, GPT-4 and GPT-4 Code Interpreter. In particular, GPT-4 Code Interpreter surpasses the previous\nSOTA by a clear margin. Recent study (Zhou et al., 2023a) shows that this excellent performance can\nbe attributed to its ability to generate and assess the execution results of a chain of code interlaced\nwith natural language reasoning steps.\nHowever, existing open-source models fail to benefit from this sophisticated mechanism since they\nlag behind closed-source models in both code generation and natural language reasoning. Therefore,\nwe still lack an effective recipe to deliver open-source models to solve math problems in a manner\nsimilar to GPT-4 Code Interpreter.\nIn this paper, leveraging the strengths of GPT-4 Code Interpreter (Zhou et al., 2023a), we\nintroduce a simple yet effective framework, MathCoder, designed to enhance the mathematical\nreasoning capabilities of open-source models. This framework can be categorized into two parts:\n(1) math instruction-following dataset construction and (2) customized supervised fine-tuning.\nSpecifically, the instruction-following dataset, termed as MathCodeInstruct, consists exclusively of\n80k math problems and their corresponding solutions. Each solution is interwoven with natural\nlanguage for reasoning, code for execution, and execution results.\nThe comparison between\nMathCodeInstruct and other math instruction-tuning datasets is shown in Tab. 1.\nMathCodeInstruct is created in two steps. The first step is collecting GPT-4 Code Interpreter-\nstyle solutions for the GSM8K and MATH training sets. GSM8K and MATH are two important\ndatasets of math problems for improving and evaluating models\u2019 mathematical abilities, which\nconsist of grade school math word problems and challenging competition mathematics problems,\nrespectively. Using this data, we trained our initial models, termed MathCoder-Initial. The second\nstep is to augment more math problems by using an innovative prompt named problem interpolation,\nwhich asks the LLM to generate questions with difficulty levels that fall between the provided\nMATH and GSM8K problems. This paradigm generates problems that bridge the gap between\nthe grade-school-level problems in GSM8K and the challenging high-school-level problems in\nMATH, thus enhancing the dataset\u2019s generalization capability. We use MathCoder-Initial to generate\nsolutions for these new problems. Combining this new data with those from the first step, we fine-\ntune the base Llama-2 models, reaching a score that outperforms the SOTA by a clear margin on\nGSM8K and MATH. Concurrently with our work, MAmmoTH (Yue et al., 2023) also creates a\ndataset consisting of math problems and model-generated solutions. However, their solutions consist\nof either only code or only natural language reasoning steps, which is notably different from our\ndataset of GPT-4 Code Interpreter-style solutions.\nRegarding the supervised fine-tuning stage, we propose an effective training and inference pipeline\nto ensure that our fine-tuned model can behave in a manner similar to the GPT-4 Code Interpreter.\nWe use special tokens (<|text|>, <|code|>, <|execution|>) to identify if a part of the training data\nis natural language, code, or execution results. With this deliberately created training corpus, the\nmodel learns to generate interleaved natural language and code divided by special tokens. During\ninference, we can use the special tokens to detect code blocks and utilize Jupyter Notebooks for code\nexecution. We append the result of on-the-fly execution to the previous predictions of the model.\nThen, the model continues to autoregressively predict the next token based on this new version of\nthe input, which includes the execution result at the end. In this way, the model would be able to\n\"see\" the execution results and continue its reasoning accordingly.\n2\nGSM8K train\n& MATH train \n(pro)\nGPT-4 \n(Inference)\nGSM8K \n& MATH\n(pro+sol, 49k)\nMathCoder-Initial\n34B\nMathCoder-Initial\n34B\n(Inference)\nMathCode\nInstruct\n(pro+sol, 80k)\nCodeLlama\n34B\n(SFT)\nProblem\nInterpolation\n(pro)\nProblem\nInterpolation\n(pro+sol, 31k)\nMathCoder-CL\n7B, 13B, 34B\nMathCoder-L\n7B, 13B, 70B\nMathCode\nInstruct\n(pro+sol, 80k)\nCodeLlama\n7B, 13B, 34B\n(SFT)\nLlama-2\n7B, 13B, 70B\n(SFT)\n(b) Supervised Fine-Tuning (SFT)\n(a) Creating Dataset\npro: problem\nsol: solution\nFigure 2: The process of dataset creation and model fine-tuning. (a) First, solutions for problems in the\nGSM8K and MATH datasets are collected from the GPT-4. Then, we fine-tune the CodeLlama-34B model\non this data, producing the MathCoder-Initial. New problems are created using our novel prompt (detailed\nexamples in Appendix A), and their solutions are generated using MathCoder-Initial. (b) Finally, the new\nproblems and solutions are combined with the existing training data to create the final dataset, which we use to\nfine-tune the base Llama-2 model, producing our final MathCoder model.\nWe use MathCodeInstruct to fine-tune popular open-source Llama-2 and CodeLlama (Rozi\u00e8re\net al., 2023) models, creating a family of models named MathCoder. Experimental results show that\nthe models with our proposed dataset and training framework achieve significant improvement on\nvarious mathematical reasoning benchmarks, as depicted in Fig. 1.\nThis paper\u2019s main contributions can be summarized in three key aspects:\n\u2022 To the best of our knowledge, this is the first systematic study that explicitly integrates\nnatural language reasoning, code generation, and feedback from execution results into\nopen-source pre-trained large language models, aiming at enhancing their mathematical\nreasoning abilities.\n\u2022 We\nhave\nconstructed\na\nhigh-quality\nmathematical\ninstruction\ntuning\ndataset,\nMathCodeInstruct. This dataset comprises existing math problems from GSM8K\nand MATH, with GPT-4 Code Interpreter-style solutions, and newly formulated ones via\nour novel problem interpolation prompting strategy.\n\u2022 We have produced a family of models, MathCoder. We fine-tune Llama-2 and CodeLlama\nmodels on our dataset, producing a family of models with not only high accuracy on the\nGSM8K and MATH, but also a good performance on other out-of-domain datasets like\nMathematics and SimulEq.\n2\nMATHCODER: SPECIALIZING LLAMA FOR MATHEMATICAL REASONING\nIn this section, we first introduce the methodology on creating MathCodeInstruct in Sec. 2.1.\nSubsequently, we detail the supervised fine-tuning (SFT) and inference methods in Sec. 2.2.\n2.1\nMATHCODEINSTRUCT DATASET\nOur MathCodeInstruct dataset can be expressed as D = {D0, D1}, where D0 denotes the\nseed data and D1 is the data generated with the proposed prompting method, named problem\ninterpolation prompting. Fig. 2 (a) outlines the process for creating MathCodeInstruct Dataset.\nSeed data D0. First, we obtain solutions for the GSM8K and MATH training sets from the GPT-4.\nThe data can be expressed in (solution, question) pairs as {(yi, xi)}N\ni=1. Each solution yi contains\n3\nInput for MathCoder\n<|system|><|text|>Below is a math problem. Please solve it step by step.<|endofblock|><|endofmessage|>\n<|user|><|text|>A group of friends went on a road trip across two cities. In the first city, they spent half of the money they had plus an additional $50.\nIn the second city, they spent half of what was left plus an additional $20, leaving them with $40. How much money did they start with before the road\ntrip?<|endofblock|><|endofmessage|>\n(a) CoT example\nLet\u2019s call the amount of money they started with x.\n1. In the first city, they spent half ( x\n2 ) plus $50. What remains is\nx \u2212\n\u0010\nx\n2 + 50\n\u0011\n.\n2. Simplifying this gives x \u2212 x\n2 \u2212 50 = x\n2 \u2212 50.\n3. In the second city, they spent half of what remained (\nx\n2 \u221250\n2\n) plus\n$20. What remains is x\n2 \u2212 50 \u2212\n\u0012 x\n2 \u221250\n2\n+ 20\n\u0013\n.\n4. Simplifying this gives x\n2 \u2212 50 \u2212 x\n4 + 25 \u2212 20 = x\n4 \u2212 45.\n5. They are left with $40, so x\n4 \u2212 45 = 40.\n6. Solving for x gives x\n4 = 85 and x = 340.\n(b) PoT example\n# Define a variable to represent the starting amount of money\nstarting_amount = 0\n# Loop to find the starting amount\nwhile True:\namount = starting_amount\n# Spent in the first city: half of the money they had plus $50\namount -= (amount / 2 + 50)\n# Spent in the second city: half of what was left plus $20\namount -= (amount / 2 + 20)\n# Check if they are left with $40\nif amount == 40:\nbreak\nstarting_amount += 1\nstarting_amount > > > 340\n(c) LCE solution from MathCoder\n<|assistant|> <|text|> To solve this problem, let\u2019s set x as the\namount of money the group of friends started with.\n1. In the first city, they spent half of what they had ( x\n2 ) plus an\nadditional $50.\n2. After this, they are left with x \u2212\n\u0010\nx\n2 + 50\n\u0011\n.\n3. In the second city, they spend half of what\u2019s left plus an additional\n$20.\n4. Finally, they are left with $40.\nThe equation can be set up as follows:\nx \u2212\n\u0012 x\n2\n+ 50\n\u0013\n\u2212\n\uf8eb\n\uf8ed\nx \u2212\n\u0010\nx\n2 + 50\n\u0011\n2\n+ 20\n\uf8f6\n\uf8f8 = 40\nLet\u2019s go ahead and solve this equation using code.<|endofblock|>\n<|code|> from sympy import symbols, Eq, solve\n# Define the variable and equation\nx = symbols(\u2018x\u2019)\n# Equation based on the problem description\nequation = Eq(x - (x/2 + 50) - ((x - (x/2 + 50)) / 2 + 20), 40)\n# Solve the equation\ninitial_amount = solve(equation, x)\ninitial_amount[0]<|endofblock|>\n<|execution|> 340<|endofblock|>\n<|text|> Thus, the group of friends started with $340 before the road\ntrip.<|endofblock|><|endofmessage|>\nFigure 3: Example of CoT (Wei et al., 2022), PoT (Gao et al., 2023; Chen et al., 2022) and LCE solution with\nspecial token. In contrast to CoT, which consists solely of natural language, and PoT, which consists solely\nof code, our LCE solution intertwines natural language, code, and execution results. <|text|>, <|code|>, and\n<|execution|> are special tokens that denote natural language, code, and execution results respectively.\nthree kinds of components: natural language (text) for reasoning L, code for execution C, and\nexecution results E, where L is the natural language reasoning step, C is the Python code the model\ngenerates when its reasoning leads to some complex computation that it needs code to solve, and\nE is the output of the code. E is assessed by the model so a new L can be generated. All three\nkinds of components are closely chained together in the solutions, with each component influencing\nthe component that comes after. An integral solution yi can be expressed as (L, C, E, L, C, E, ...).\nAn example is shown in Fig. 3 (c). We call solutions in this format Natural Language, Code, and\nExecution (LCE) solutions. We put some case studies in Appendix E to demonstrate the advantage\nof LCE.\nWe filter the seed data D0 = ({(yi, xi)}), making sure that each solution yi provides the same\nanswer as the ground truth answer so that the quality of the dataset is further assured. Then, we fine-\ntune the CodeLlama-34B using the seed data D0, producing our initial MathCoder model, named\nMathCoder-Initial.\nProblem interpolation prompting D1. Using the initial MathCoder model, we can generate LCE\nsolutions for new problems.\nWe observed a large gap in difficulty between grade-school-level\nGSM8K problems and challenging competition MATH problems. To bridge this gap, we present\na novel prompting method (see details in Appendix A), which provides a powerful LLM like GPT-4\nwith a relatively simple problem drawn from the GSM8K training set, paired with a difficult problem\ndrawn from the MATH, and ask the model to generate a new problem with difficulty between the\ntwo. We then use GPT-4 to evaluate the new problems, and the results are shown in Fig. 4. We\n4\n15.3\n83.2\n95.6\n3.3\nGSM8K vs.\nInterpolation\nMATH vs.\nInterpolation\nMATH/GSM8K more difficult\nTie\nInterpolation more difficult\nFigure 4: Difficulty comparison of interpolation problems against MATH and GSM8K using GPT-4. The\nevaluation prompt and examples are shown in Appendix B.\ncan see that 83.2% of the new problems are more difficult than GSM8K, and 95.6% are easier than\nMATH, indicating that the problems generated in this way are appropriate in difficulty.\nWe also investigated using only GSM8K to create difficult problems, but we found that the new\nproblems were too similar to the original ones, and the large gap to MATH still exists (more\ninformation can be found in Appendix C).\nSelf-distillation. Given that we do not have ground truth answers for the new problems, we then\ngenerate n different LCE solutions as depicted in (Wang et al., 2023a) for each new problem with\nour initial MathCoder models, keeping only those solutions for which all n answers match (n is\nset to 3 in this paper), thus ensuring our dataset\u2019s quality. We use MathCoder-Initial here because\nit demonstrates the potential for effective model distillation using a model much weaker than the\npowerful closed-source models. As MathCoder-Initial already has an accuracy of 77.3% on GSM8K\nand 44.0% on MATH, it is plausible that distilling it can produce good results. It also reduces the\ncost compared to using GPT-4. Some examples can be found in Appendix A.\nCombining the new data D1 with the seed data D0 yields the MathCodeInstruct dataset D =\n{D0, D1}. We fine-tune the base Llama-2 (Touvron et al., 2023) and CodeLlama (Rozi\u00e8re et al.,\n2023) models using MathCodeInstruct to derive our final MathCoder models. For clarity, we\nrefer to the supervised fine-tuning of base Llama-2 as \"MathCoder-L\" and that of CodeLlama as\n\"MathCoder-CL\", as shown in Fig. 2 (b).\n2.2\nSUPERVISED FINE-TUNING AND INFERENCE\nSupervised Fine-tuning. In order to identify the three kinds of components in LCE solutions,\nas illustrated in Fig. 3 (c), we enclose them with special tokens. Reasoning language starts with\n<|text|>, while math code and execution results start with <|code|> and <|execution|> respectively.\nAll components end with <|endofblock|>. These tokens help the model understand the difference\nbetween each component and create LCE solutions during inference. After the special tokens are\nadded, all components are concatenated to form the solution, which is preceded by the original math\nquestion to form an instance of training data. In order to make the training more efficient, several\ninstances are concatenated together to form a single input, while cross-question masking is used to\nensure only tokens in the same instance are visible.\nDuring supervised fine-tuning, we apply a standard cross-entropy loss following Alpaca (Taori\net al., 2023). The loss is only computed on reasoning language and math code since they are the\ncomponents of the training data generated by the LLM. In particular, we zero-out the loss on tokens\nfrom execution results, as the model would not need to predict these tokens.\nInference. After supervised fine-tuning, the model has learned to output natural language and\ncode enclosed by special tokens.\nWe can identify the end of each component by looking for\n<|endofblock|>, and determine which component it is by examining the first token of the component.\nWhen a code generation is encountered, we utilize a Jupyter Notebook for real-time code execution,\nallowing the variables defined in previous code blocks to be used in subsequent ones.\nAfter\nexecution, the execution results are concatenated following the previous math code block. The\nmodel then continues to autoregressively generate the next reasoning language block, forming the\nchain of thoughts in the LCE format, until it reaches the final answer. This process ensures that the\nmodel behaves similarly to the GPT-4 Code Interpreter.\n5\n3\nEXPERIMENTS\n3.1\nDATASETS AND IMPLEMENTATION DETAILS\nDatasets.\nWe evaluate the MathCoder on five datasets, including two in-domain datasets:\nGSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021); and three out-of-domain datasets:\nSVAMP (Patel et al., 2021), Mathematics (Saxton et al., 2019), and SimulEq (Kushman et al., 2014).\nWe regard GSM8K and MATH as in-domain because their training sets are used for our supervised\nfine-tuning, while SVAMP, Mathematics, and SimulEq are out-of-domain because their training\nsets are not used in our fine-tuning. The extensive assortment of assessment datasets encompasses\nmathematical challenges from elementary, high school, and collegiate levels, covering various\nsubjects like geometry, formal logic, and even commonsense reasoning. The selection of these\ndatasets aims at providing a thorough evaluation of the models\u2019 ability to generalize to unknown\ncircumstances and diverse fields of mathematics.\nImplementation Details.\nDifferent base LLMs of varying sizes are tested, including Llama-\n2 (7B, 13B, and 70B) and CodeLlama (7B, 13B, and 34B). During training, we use a uniform\nlearning rate of 2 \u00d7 10\u22125 and a context length of 2048, and we set the batch size as 128 with\ndifferent ratios of gradient accumulation steps and per-device train batch size, considering the model\nsize. Additionally, we used a cosine scheduler for three epochs in total with a 50-step warm-\nup period. To efficiently train the computationally intensive models, we simultaneously employ\nDeepSpeed training with ZeRO-3 stage (Rajbhandari et al., 2020) and flash attention (Dao et al.,\n2022). The 7B, 13B, and 34B/70B models are trained on 8, 16, and 32 NVIDIA A800 80GB GPUs,\nrespectively. The text-generation-inference framework of Hugging Face is used for inference with\ngreedy decoding and max new tokens of every block set 512, and one to four GPUs are used as\nneeded. We allow up to 32 LCE blocks in every solution.\nBaselines.\nWe compare the proposed MathCoders with the following competitive baselines.\nClosed-Source Models: we consider three closed-source models, including ChatGPT-3.5 Brown\net al. (2020), GPT-4 (OpenAI, 2023), GPT-4 Code Interpreter (Zhou et al., 2023a), and PaLM-\n2 (Anil et al., 2023). Open-Source Models: we compare with Llama-2 (Touvron et al., 2023),\nWizardMath (Luo et al., 2023), Llama-1 RFT (Yuan et al., 2023), and Galactica (Taylor et al., 2022).\nFor baselines, CoT prompting (Wei et al., 2022) and few-shot in-context-learning (Dong et al., 2023)\nare used to maximize their performance while our MathCoders are always evaluated without extra\nprompting and under zero-shot setting (Kojima et al., 2023).\n3.2\nMAIN RESULTS\nComparison between MathCoder and SOTA open-source models. The experiment results in\nTab. 2 show that our method outperforms other open-source competitive math-solving models with\na clear advantage, achieving state-of-the-art results across all datasets.\nHowever, a substantial\nperformance gap still exists compared to the state-of-the-art closed-source method GPT-4 Code\nInterpreter. Our observations are as follows: (1) MathCoder-L-7B outperforms WizardMath-70B.\nEven the smallest version of MathCoder, MathCoder-L-7B, outperforms the largest WizardMath\nmodel, WizardMath-70B, on three out of five datasets, achieving a significant gain (+4.5%) in the\naverage score, as shown in Tab. 2. This is likely attributed to the fact that WizardMath is trained\nsolely on CoT data, while MathCoder is trained on our proposed LCE solutions. This demonstrates\nthe advantage of using solutions that interleave natural language, code, and execution (LCE blocks),\nsignificantly enhancing the model\u2019s ability to perform complex computations. (2) Additionally,\nit is worth noting that while the code ability of CodeLlama-34B significantly outperforms that of\nLlama-2-70B, in the case of MathCoder models, we observed that models based on Llama-2-70B\n(73.1%) can outperform CodeLlama-34B (70.2%). This contrasts with the findings in the concurrent\nwork, MAmmoTH (Yue et al., 2023). The main reason for this disparity might be that Llama-2-\n70B exhibits better natural language reasoning ability, and the MathCodeInstruct dataset can\nenhance language models\u2019 code generation ability for math problem-solving.\nComparison between Llama-2 and CodeLlama.\nTab. 3 shows that MathCoder-CL with\nCodeLlama as the base model brings a substantial improvement compared to MathCoder-L with\nLlama-2 as the base model. MathCode-CL-7B and MathCoder-CL-13B demonstrate an accuracy\nimprovement of 4.1% and 3.0% respectively, compared to the corresponding MathCoder-L models\n6\nTable 2:\nModel evaluation on in-domain (GSM8K & MATH) and out-of-domain datasets (SVAMP,\nMathematics & SimulEq). + incidates improvement w.r.t. the best open source model. SVA. stands for SVAMP,\nMat. stands for Mathematics, and Sim. stands for SimulEq.\nModel\nBase\nSize\nIn-Domain\nOut-of-Domain\nAverage\nGSM8K\nMATH\nSVA.\nMat.\nSim.\nColsed-Source Model\nChatGPT-3.5 (Zhao et al., 2023)\n-\n-\n80.8\n34.1\n-\n-\n-\n-\nGPT-4 (OpenAI, 2023)\n-\n-\n92.0\n42.5\n97.0\n-\n-\n-\nGPT-4 Code (Zhou et al., 2023a)\n-\n-\n97.0\n69.7\n-\n-\n-\n-\nPaLM-2 (Anil et al., 2023)\n-\n-\n80.7\n34.3\n-\n-\n-\n-\nOpen-Source Model\nGalactica (Taylor et al., 2022)\n-\n6.7B\n10.2\n2.2\n25.6\n4.6\n4.2\n9.4\n30B\n41.7\n12.7\n41.6\n11.8\n13.2\n24.2\nLlama-1 RFT (Yuan et al., 2023)\nLlama-1\n7B\n46.5\n5.2\n21.1\n5.1\n11.0\n17.8\n13B\n52.1\n5.1\n46.5\n6.7\n10.1\n24.1\n34B\n56.5\n7.4\n55.4\n7.6\n12.8\n27.9\nWizardMath (Luo et al., 2023)\nLlama-2\n7B\n54.9\n10.7\n36.1\n9.3\n12.8\n24.8\n13B\n63.9\n14.0\n51.9\n14.1\n14.9\n31.8\n70B\n81.6\n22.7\n71.8\n17.1\n37.9\n46.2\nMathCoder-L\nLlama-2\n7B\n64.2\n23.3\n71.5\n46.9\n47.5\n50.7\n+9.3\n+12.6\n+35.4\n+37.6\n+34.7\n+25.9\n13B\n72.6\n29.9\n76.9\n54.7\n62.3\n59.2\n+8.7\n+15.9\n+25.0\n+40.6\n+47.4\n+27.4\n70B\n83.9\n45.1\n84.9\n74.4\n77.0\n73.1\n+2.3\n+22.4\n+13.1\n+57.3\n+39.1\n+26.9\nMathCoder-CL\nCodeLlama\n7B\n67.8\n30.2\n70.7\n55.8\n49.6\n54.8\n+12.9\n+19.5\n+34.6\n+46.5\n+36.8\n+30.0\n13B\n74.1\n35.9\n78.0\n62.5\n60.7\n62.2\n+10.2\n+21.9\n+26.1\n+48.4\n+45.8\n+30.4\n34B\n81.7\n45.2\n82.5\n75.9\n65.8\n70.2\n+0.1\n+22.5\n+10.7\n+58.8\n+27.9\n+24.0\nTable 3: Model performance comparison for MathCoders with CodeLlama and Llama-2 as base.\nSize\nGSM8K\nMATH\nSVAMP\nMathematics\nSimulEq\nAverage\nMathCoder-CL-7B vs. MathCoder-L-7B\n+3.6\n+6.9\n-0.8\n+8.9\n+2.1\n+4.1\nMathCoder-CL-13B vs. MathCoder-L-13B\n+1.5\n+6.0\n+1.1\n+7.8\n-1.6\n+3.0\nof the same size. The potentially superior coding and reasoning capability of CodeLlama can be\nattributed to its additional training on code data (Rozi\u00e8re et al., 2023). This extended training\nprovides CodeLlama with a deeper understanding of programming concepts and patterns, allowing\nit to excel in coding-related tasks and exhibit more advanced math reasoning abilities.\nComparison among different subjects across various levels.\nMATH dataset problems are\ncategorized with difficulty levels ranging from 1 to 5, covering seven different math subjects,\nincluding algebra, prealgebra, number theory, counting and probability, precalculus, intermediate\nalgebra, and geometry. In Fig. 5, we present the performance comparison of MathCoder-L (7B, 13B)\nand MathCoder-CL (7B, 13B), grouped by these levels and subjects. More results are shown in\nAppendix D. We find that MathCoder achieves higher scores in algebra and prealgebra problems.\nHowever, when it comes to geometry problems, MathCoder struggles to achieve high scores,\nespecially for problems with higher difficulty levels. This suggests that code plays a more significant\nrole in computationally intensive questions.\n3.3\nABLATION STUDY\nAnalysis of the influence of problem interpolation.\nWe conducted an experiment to study the\ninfluence of the portion of MathCodeInstruct questions created using the proposed problem\n7\nL1\nL2\nL3\nL4\nL5\nalgebra\nprealgebra\nnumber\ntheory\ncounting and\nprobability\nprecalculus\nintermediate\nalgebra\ngeometry\n \n0.72\n0.51\n0.4\n0.27\n0.13\n0.52\n0.53\n0.41\n0.3\n0.14\n0.57\n0.28\n0.26\n0.15\n0.05\n0.51\n0.46\n0.17\n0.08\n0.07\n0.32\n0.15\n0.09\n0.04\n0\n0.29\n0.21\n0.17\n0.08\n0.04\n0.29\n0.3\n0.16\n0.13\n0.01\nMathCoder-L-7B\n(Overall: 0.233)\nL1\nL2\nL3\nL4\nL5\n \n0.76\n0.61\n0.49\n0.37\n0.21\n0.69\n0.64\n0.49\n0.38\n0.22\n0.67\n0.43\n0.32\n0.29\n0.17\n0.59\n0.5\n0.19\n0.15\n0.05\n0.4\n0.21\n0.12\n0.1\n0.03\n0.35\n0.3\n0.18\n0.13\n0.09\n0.29\n0.35\n0.27\n0.1\n0.01\nMathCoder-CL-7B\n(Overall: 0.3024)\nL1\nL2\nL3\nL4\nL5\n \n0.77\n0.63\n0.5\n0.4\n0.21\n0.59\n0.56\n0.5\n0.42\n0.19\n0.77\n0.41\n0.34\n0.2\n0.16\n0.67\n0.48\n0.24\n0.14\n0.06\n0.42\n0.21\n0.13\n0.09\n0\n0.4\n0.25\n0.18\n0.14\n0.06\n0.37\n0.39\n0.2\n0.17\n0.02\nMathCoder-L-13B\n(Overall: 0.2992)\nL1\nL2\nL3\nL4\nL5\n \n0.83\n0.67\n0.57\n0.45\n0.29\n0.73\n0.69\n0.52\n0.45\n0.24\n0.77\n0.49\n0.42\n0.32\n0.21\n0.74\n0.58\n0.22\n0.16\n0.18\n0.44\n0.34\n0.23\n0.15\n0.04\n0.44\n0.38\n0.24\n0.17\n0.1\n0.24\n0.48\n0.25\n0.18\n0.05\nMathCoder-CL-13B\n(Overall: 0.3588)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 5: Performance comparison of MathCoder-L (7B, 13B) and MathCoder-CL (7B, 13B) on the MATH\ndataset by levels and subjects. We can see that the improved accuracy from MathCoder-L to MathCoder-CL\ncomes primarily from subjects that require precise calculations like algebra and number theory.\nTable 4: Influence of the interpolation problems in MathCodeInstruct (as shown in Tab. 1) based on\nCodeLlama-34B.\nTrain set\nSamples\nGSM8K\nMATH\nSVAMP\nMathematics\nSimulEq\nAverage\nGSM8K+MATH\n49k\n77.3\n44.0\n78.6\n71.6\n59.3\n66.2\nGSM8K+MATH+Interpolation\n80k\n81.7\n45.2\n82.5\n75.9\n65.8\n70.2\n+4.4\n+1.2\n+3.9\n+4.3\n+6.4\n+4.0\ninterpolation. The experiment uses CodeLlama-34B as the base model. The experimental results in\nTab. 4 validate that problem interpolation brings a significant improvement across all five datasets.\nThe results indicate that by employing problem interpolation, we can generate problems with\nintermediate difficulty levels, thereby increasing the diversity of the problem set. This expands\nthe diversity of the problems and ultimately enhances the performance of the model.\nAnalysis of actual code execution in the inference stage. We investigate the impact of code\nexecution in the inference stage and report the results in Tab. 5. We conduct this investigation using\nCodeLlama-34B as the base model and train the models on our 80k MathCodeInstruct dataset.\nTab. 5 (#1) and Tab. 5 (#2) use the same model, trained with the cross-entropy loss computed on\nnot only natural language and code, but also the execution results. In this way, this model learns to\npredict the execution results. In Tab. 5 (#1), the code execution results are predicted by the model\nitself, while in Tab. 5 (#2), the execution result is returned from a Python code interpreter. From\nthe comparison between Tab. 5 (#1) and Tab. 5 (#2), we can see that Tab. 5 (#1) outperforms Tab. 5\n(#2) across all five datasets, showing an improvement of 34.0% in the average accuracy score. This\nindicates that actual code execution in the inference stage has a significant impact on the model\u2019s\nperformance. This study shows that the model failed to predict correct execution results for many\nprograms and that actually executing the code using an external tool can significantly improve the\naccuracy while doing complex computations. This finding validates the significance of integrating\ncode execution when solving math problems with LLMs, in line with previous closed-source GPT-4\nCode interpreter (Zhou et al., 2023a).\nAnalysis of execution results in the training stage. Based on the observation that actual code\nexecution contributes a lot to the model\u2019s performance, we investigate not forcing the model to\npredict the correct execution result. Tab. 5 (#3) is the performance of MathCoder-CL-34B, which\nignores execution results when computing the loss, so that the model does not learn to estimate\nthe execution results and the learning task at the supervised fine-tuning stage becomes simpler.\nCompared to Tab. 5 (#2), Tab. 5 (#3) improves the accuracy across four out of five datasets, resulting\nin a rise in the average accuracy from 69.1% to 70.2%, which aligns with the hypothesis that by\ncomputing the loss only on natural language and code, the model can focus more on the math\nproblem-solving skills itself, thus making the supervised fine-tuning more effective.\n8\nTable 5: Ablation study of with/without code execution during inference and of the loss with/without execution\nresults in training stage.\nExperiment\nInclude\nActual\nGSM8K\nMATH\nSVAMP\nMathematics\nSimuleq\nAverage\nexecution results\ncode execution\nfor training\nin inference\n#1\nYes\nNo\n54.1\n16.9\n69.6\n20.6\n14.2\n35.1\n#2\nYes\nYes\n79.9\n45.9\n81.9\n74.2\n63.6\n69.1\n+25.8\n+29.0\n+12.3\n+53.6\n+49.4\n+34.0\n#3\nNo\nYes\n81.7\n45.2\n82.5\n75.9\n65.8\n70.2\n+1.8\n-0.7\n+0.6\n+1.7\n+2.1\n+1.1\n4\nRELATED WORK\nInstruction Tuning. Instruction tuning is a method of enhancing LLMs\u2019 instruction following\nabilities, thus aligning language models with more useful objectives and human preferences. A\nlong line of previous works (Ye et al., 2021; Longpre et al., 2023; Sanh et al., 2021; Wang et al.,\n2022b; Wei et al., 2021; Chung et al., 2022; Longpre et al., 2023) is focused on enhancing LLMs\u2019\ninstruction following abilities in general. With the emergence of models like GPT-3 and GPT-4,\nrecent studies (Wang et al., 2022a; 2023b; Zhou et al., 2023b; Peng et al., 2023; Xu et al., 2023)\nhave started to utilize synthetic instructions generated by these powerful models to tune smaller\nmodels. Compared to these works, our instruction tuning is focused on using high-quality solutions\nfor math problems generated by models to improve our LLM\u2019s math-solving ability. Another related\nwork is presented in (Luo et al., 2023), but their method did not use code to solve math problems,\ndistinguishing our work from theirs.\nMathematical Reasoning. There are various benchmark datasets (Hendrycks et al., 2020; Ling\net al., 2017; Hendrycks et al., 2021) to measure a model\u2019s mathematical reasoning abilities. Recently,\nmany works have focused on enhancing LLMs\u2019 ability to solve math problems, reaching high scores\non these benchmarks. Many of them apply Chain-of-Thought (Wei et al., 2022; Kojima et al., 2023;\nWang et al., 2023a; Fu et al., 2022) to improve LLMs\u2019 multistep reasoning capability. Another line\nof works (Gao et al., 2023; Chen et al., 2022; Zhou et al., 2023a) utilize code to compensate for\nLLMs\u2019 limitations in doing complex math computations. Our work takes inspiration from these two\nlines of work, as we believe both Chain-of-Thought and code generation (Li et al., 2023a; Rozi\u00e8re\net al., 2023) are essential to solving math problems. There are also works focused on math-related\npre-training (Lewkowycz et al., 2022; Taylor et al., 2022) to improve a model\u2019s general reasoning\ncapability. We combine natural language and code seamlessly in our dataset, thus providing a\nmethod to train models more efficiently in solving math problems.\nDistillation.\nDistillation (Hinton et al., 2015) often involves transferring knowledge from a\nlarger, more powerful model to a smaller, weaker one (Taori et al., 2023; Zheng et al., 2023;\nCobbe et al., 2021).\nRecent research (Li et al., 2023b; Wang et al., 2022a; Allen-Zhu & Li,\n2020) has demonstrated the plausibility of self-distillation, achieving performance improvements\nby distilling the model itself. Our approach can also be viewed as a form of self-distillation, as the\nsolutions generated by MathCoder-Initial, which is built on CodeLlama-34B, are used to fine-tune\nCodeLlama-34B, resulting in MathCoder-CL-34B.\n5\nCONCLUSION AND LIMITATION\nIn this paper, we present MathCoder, an open-source large language model designed for math\nreasoning, bridging the gap between natural language understanding and computational problem-\nsolving.\nMathCoder incorporates math instruction-following dataset construction.\nBy utilizing\nthe GSM8K and MATH datasets as seed data, we leverage the GPT-4 to generate problems\nencompassing reasoning, code generation, and program execution. Additionally, we propose a\nproblem interpretation method to create intermediate-level problems. Furthermore, we introduce\na customized supervised fine-tuning approach, where the training loss is only applied to natural\nlanguage and code.\nOur empirical study demonstrates that MathCoder achieves state-of-the-\nart performance in five math datasets among open-source LLMs, with scores of 83.9% on the\nGSM8K dataset and 45.2% on the MATH dataset. It is worth noting that MathCoder outperforms\n9\nclosed-source models like ChatGPT-3.5 and PaLM-2 on the GSM8K and MATH datasets and even\noutperforms GPT-4 on the MATH dataset.\nHowever, our work does have certain limitations that warrant further exploration in future research.\nFirst, since we rely on the GPT-4 for data generation, MathCoder\u2019s capabilities are inherently\nconstrained by the capabilities of this model and unable to solve theorem-proving problems.\nAdditionally, as a series of uni-modal models, MathCoder still faces challenges in solving complex\ngeometry problems, which we acknowledge and plan to address in our future investigations.\nREFERENCES\nZeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks.\narXiv preprint\narXiv:2211.12588, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\nScaling instruction-finetuned language\nmodels. arXiv preprint arXiv:2210.11416, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness, 2022.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\nLei Li, and Zhifang Sui. A survey on in-context learning, 2023.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting\nfor multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong\nSong, and Jacob Steinhardt.\nMeasuring massive multitask language understanding.\nArXiv,\nabs/2009.03300,\n2020.\nURL https://api.semanticscholar.org/CorpusID:\n221516475.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\nDistilling the knowledge in a neural\nnetwork. ArXiv, abs/1503.02531, 2015. URL https://api.semanticscholar.org/\nCorpusID:7200347.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023.\n10\nNate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically\nsolve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 271\u2013281, 2014.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.\nSolving\nquantitative reasoning problems with language models.\nAdvances in Neural Information\nProcessing Systems, 35:3843\u20133857, 2022.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\nMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,\nTerry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o\nMonteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,\nLogesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,\nRudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan\nDey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha\nLuccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav\nTimor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,\nAlex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,\nSiva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean\nHughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\nsource be with you!, 2023a.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and\nMike Lewis. Self-alignment with instruction backtranslation. ArXiv, abs/2308.06259, 2023b.\nURL https://api.semanticscholar.org/CorpusID:260866107.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.\nProgram induction by rationale\ngeneration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 158\u2013167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo\nGeng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.\nWizardmath:\nEmpowering\nmathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint\narXiv:2308.09583, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\nword problems? arXiv preprint arXiv:2103.07191, 2021.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith gpt-4. arXiv preprint arXiv:2304.03277, 2023.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models, 2020.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez,\nJade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and\nGabriel Synnaeve. Code llama: Open foundation models for code, 2023.\n11\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\nenables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli.\nAnalysing mathematical\nreasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for\nscience. arXiv preprint arXiv:2211.09085, 2022.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023a. URL\nhttps://openreview.net/forum?id=1PL1NIMMrw.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022a.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv\npreprint arXiv:2204.07705, 2022b.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\nChandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels\ngo? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751,\n2023b.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\nmodels.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),\nAdvances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=_VjQlMeSB_J.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\narXiv preprint arXiv:2304.12244, 2023.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task\ngeneralization in nlp. arXiv preprint arXiv:2104.08835, 2021.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling\nrelationship on learning mathematical reasoning with large language models.\narXiv preprint\narXiv:2308.01825, 2023.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMammoth: Building math generalist models through hybrid instruction tuning, 2023.\n12\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\nOpt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\nXu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with\nlarge language models for reasoning, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,\nLinqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code\ninterpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206,\n2023b.\n13\nAPPENDIX\nA\nDATASET EXAMPLES\nIn this part, we include two examples that show the process of creating MathCodeInstruct.\nFig. 6 shows an example with only one LCE block, while Fig. 7 shows an example with three LCE\nblocks.\nB\nEXAMPLES OF DIFFICULTY COMPARISON\nWe show five\nexamples of using\nGPT-4\nto\nevaluate\nthe\ncomplexity\nof\nproblems\nin\nMathCodeInstruct. Fig. 8 and Fig. 9 are two examples that the newly generated interpolation\nproblems are more difficult than the origin GSM8K problems, and Fig. 10 is an example that the\norigin MATH problem is more difficult than the newly generated interpolation problem. These two\nsituations are the most common (83.2% and 95.6%).\nFig. 11 shows an example that the newly generated interpolation problem ties with the origin\nGSM8K problem, which situation accounts for 15.3% of all problems.\nFig. 12 shows an uncommon example that the origin GSM8K problem is slightly more difficult than\nthe newly generated interpolation problem according to GPT-4, which situation accounts for less\nthan 3% of all problems.\nC\nCREATING PROBLEMS USING ONLY GSM8K\nFig. 13, Fig. 14, Fig. 15, Fig. 16 and Fig. 17 are five examples that utilize problems from the train set\nof GSM8K to generate new problems which are more difficult than the origin ones. Compared with\nthe problems generated by our interpolation method, we can see that the new problems generated in\nthis way are much more similar to the raw GSM8K problems, sometimes just changing the name\nof some variables or scaling the value. These problems are only slightly more complicated than the\nraw problems, if not equally difficult, and are still much simpler than those from the MATH dataset.\nIn contrast to using just GSM8K, introducing problems from the MATH dataset in the interpolation\nmethod shows the model (GPT-4 here) a route to generate more challenging problems. Hence, the\nnewly generated problems are similar to the problems in the GSM8K and the problems in the MATH.\nConsequently, these interpolation problems can narrow the difficulty gap between the two datasets.\nD\nMORE EXPERIMENT RESULTS\nWe show the performance comparison of all MathCoders, MathCoder-L (7B, 13B, 70B) and\nMathCoder-CL (7B, 13B, 34B), on the MATH dataset by levels and subjects in Fig. 18. We can\nsee that the improved accuracy from MathCoder-L to MathCoder-CL comes primarily from subjects\nrequiring precise calculations like algebra, number theory, and counting and probability.\nE\nCASE STUDY WITH COT, POT AND LCE\nWe compare our LCE solutions with CoT solutions and PoT solutions. Fig. 19 is an example of a\nproblem in number theory, and Fig. 20 is an example of a problem in algebra. CoT and PoT failed\nto solve the problem in both cases, but LCE succeeded.\nFig. 21, Fig. 22, and Fig. 23 are three solutions to one problem in geometry. The CoT solutions\nsuccessfully figured out the coordinates of D, C, and E but failed to calculate the area, while the PoT\ncould not interpret the conditions in the problem. Compared with them, we can see that our LCE\nsolutions not only can utilize the conditions in the problems correctly but also will not make errors\nin calculation.\n14\n(a) Interpolation Prompt:\nPlease create a new problem, following the given example, \"Example 1\" is an easy problem and\n\"Example 2\" is much more difficulty than \"Example 1\", the new problem should be harder than\n\"Example 1\" and simpler than \"Example 2\".\nExample 1: Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another\nhour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in\nmiles per hour) does she need to walk the remaining distance?\nExample 2: The two solutions of equation x2 + bx + 48 = 0 are in the ratio of 3 to 1 for some values\nof b. What is the largest possible value of b?\n(b) According new problem generated by GPT-4:\nA group of friends went on a road trip across three cities. In the first city, they spent half of the money\nthey had plus an additional $50. In the second city, they spent half of the remaining money plus an\nadditional $30. In the third city, they spent half of what was left plus an additional $20, leaving them\nwith $40. How much money did they start with before the road trip?\n(c) Solution generated by MathCoder-Initial without extra prompt:\nLet\u2019s break this problem down step by step:\n1. Let x be the amount of money they started with.\n2. After visiting the first city, they spent x\n2 + 50. So, the remaining amount is x \u2212\n\u0000 x\n2 + 50\n\u0001\n.\n3. In the second city, they spent half of the remaining money plus $30. This means they spent\nx\u2212( x\n2 +50)\n2\n+ 30. After this expenditure, they would have x \u2212\n\u0000 x\n2 + 50\n\u0001\n\u2212\n\u0012\nx\u2212( x\n2 +50)\n2\n+ 30\n\u0013\nleft.\n4. In the third city, they spent half of what was left from the above amount plus $20. After this, they\nwere left with $40.\nUsing the above information, we can set up an equation to solve for x:\nx \u2212\n\u0010x\n2 + 50\n\u0011\n\u2212\n \nx \u2212\n\u0000 x\n2 + 50\n\u0001\n2\n+ 30\n!\n\u2212\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\nx \u2212\n\u0000 x\n2 + 50\n\u0001\n\u2212\n\u0012\nx\u2212( x\n2 +50)\n2\n+ 30\n\u0013\n2\n+ 20\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8 = 40\nLet\u2019s solve this equation to find the value of x.\nfrom sympy import symbols, Eq, solve\n# Define the variable\nx = symbols(\u2019x\u2019)\n# Define the equation based on the problem statement\nequation = Eq(x - (x/2 + 50) - (x - (x/2 + 50))/2 - 30 - (x - (x/2 + 50) - (x - (x/2 + 50))/2\n- 30)/2 - 20, 40)\n# Solve the equation\ninitial_money = solve(equation)[0]\n> > > 700\nThe group of friends started with $700 before the road trip.\nFigure 6: An example of the process of creating MathCodeInstruct. Firstly, \"Example 1\" and \"Example\n2\" are randomly chosen from the train set of GSM8K and MATH respectively. Then a new problem is generated\nby GPT-4 using the interpolation prompt. Finally, we use our initial MathCoder to generate LCE-style solution\nfor the new problem.\n15\n(a) Interpolation Prompt:\nPlease create a new problem, following the given example, \"Example 1\" is an easy problem and\n\"Example 2\" is much more difficulty than \"Example 1\", the new problem should be harder than\n\"Example 1\" and simpler than \"Example 2\".\nExample 1: A bus comes by Jerry\u2019s bus stop every 20 minutes starting at exactly 5:13 a.m. If Jerry\nshows up at exactly 8:35 a.m., how many minutes will he have to wait for the next bus?\nExample 2: Bella\u2019s grandfather is confined to a wheelchair. He is coming to visit her. Bella wants\nto build a wheelchair ramp. Her research shows that there must be 3.5 meters of ramp for every 30\ncentimeters of elevation. The distance from the ground to the front doorstep of Bella\u2019s house is 9 cm.\nWhat must the length of the ramp be?\n(b) According new problem generated by GPT-4:\nLiam is a bird watcher and records the pattern of a particular species of bird migration. Every 15 days,\nthe birds fly south 250 miles and then fly back north 100 miles. If this pattern continues, how far south\nfrom their original position will the birds be after 75 days?\n(c) Solution generated by MathCoder-Initial without extra prompt:\nLet\u2019s solve the problem step by step:\n1. Calculate the net distance the birds have moved from their original position in 15 days.\n2. Use the net distance to determine how far the birds will have moved after 75 days.\nLet\u2019s start by calculating the net distance the birds fly south in 15 days. The net distance can be found\nby subtracting the northward distance from the southward distance.\nsouth_distance_15_days, north_distance_15_days = 250, 100\nnet_distance_15_days = south_distance_15_days - north_distance_15_days\n> > > 150\nThe birds have moved a net distance of 150 miles south from their original position in 15 days.\nNow, to determine how far the birds will have moved after 75 days, we need to:\nCalculate how many 15-day cycles are within 75 days. Multiply the number of 15-day cycles by the\nnet distance moved in 15 days to get the total distance moved in 75 days. Let\u2019s find out how many\n15-day cycles are within 75 days.\ntotal_days = 75\nnumber_of_cycles = total_days // 15\n> > > 5\nThere are 5 cycles of 15 days within the span of 75 days.\nNow, let\u2019s calculate the total distance the birds will have moved after 75 days by multiplying the\nnumber of 15-day cycles by the net distance moved in one 15-day cycle.\ntotal_distance_75_days = number_of_cycles*net_distance_15_days\n> > > 750\nAfter 75 days, the birds will be 750 miles south from their original position.\nFigure 7: An example of the process of creating MathCodeInstruct. Firstly, \"Example 1\" and \"Example\n2\" are randomly chosen from the train set of GSM8K and MATH respectively. Then a new problem is generated\nby GPT-4 using the interpolation prompt. Finally, we use our initial MathCoder to generate LCE-style solution\nfor the new problem.\n16\n(a) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and\nfive nickels, how many coins does Zain have?\"\nProblem 2: \"In a marathon, every runner must run through several checkpoints. In the first segment,\nAngela runs 15 meters more than Miguel. For each subsequent segment, Angela runs 5 meters less\nthan she did in the previous segment, while Miguel runs consistently. If Miguel runs 50 meters in the\nfirst segment, how many meters does Angela run in total when she completes 5 segments?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(b) Judgement from GPT-4:\nLet\u2019s break down both problems step by step to assess their complexity.\nProblem 1:\nEmerie has:\n\u2022 Quarters: 6\n\u2022 Dimes: 7\n\u2022 Nickels: 5\nZain has 10 more of each coin than Emerie:\n\u2022 Quarters: 6 + 10 = 16\n\u2022 Dimes: 7 + 10 = 17\n\u2022 Nickels: 5 + 10 = 15\nTo find out how many coins Zain has, we just need to add these numbers together:\n16 + 17 + 15 = 48 coins\nProblem 2:\nGiven:\n- Miguel runs consistently at 50 meters per segment.\n- Angela runs 50 + 15 = 65 meters in the first segment.\n- Angela runs 5 meters less in each subsequent segment.\nUsing the concept of arithmetic series:\nSn = n\n2 (2a1 + (n \u2212 1)d)\nFor Angela, the first term, a1, is 65 meters (15 meters more than Miguel\u2019s 50 meters), and the common\ndifference, d, is -5 meters, as she runs 5 meters less in each subsequent segment. She completes 5\nsegments, so n = 5.\nLet\u2019s plug in the values to find out how many meters Angela runs in total for the 5 segments:\nS5 = 5\n2(2 \u00b7 65 + (5 \u2212 1) \u00b7 \u22125) = 5\n2(130 \u2212 20) = 5\n2(110) = 550\n2\n= 275 meters\nConclusion:\nProblem 1 involves simple arithmetic and requires the solver to add a constant number of coins to the\nknown number of coins that Emerie has. There\u2019s no series or sequential decrease/increase involved.\nProblem 2, however, involves understanding an arithmetic progression. It needs understanding of\nsequences and series, and it may require more steps to solve, such as identifying the first term, the\ncommon difference, and then applying the formula to find the sum of the series. Therefore, based on\nthese considerations, it could be argued that \"Problem 2\" is more difficult.\nFigure 8: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct.\n\"Problem 2\" is in MathCodeInstruct and \"Problem 1\" is the problem from GSM8K that was used to\ngenerate \"Problem 2\".\n17\n(a) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Peter has 15 birds. 1/3 are ducks. The rest are chickens and require special feed that costs\n$2 per bird. How much does it cost to feed the chickens?\"\nProblem 2: \"In a rectangular garden, the length is 4 times the width. If the total area of the garden is\nincreased by 120 square feet by increasing the width by 5 feet while keeping the length constant, what\nis the original area of the garden?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\", and \"Tie\".\n(b) Judgement from GPT-4:\nLet\u2019s break down the two problems to understand the complexity of each.\nProblem 1:\nPeter has 15 birds, and 1/3 are ducks. So the number of chickens is 2\n3 \u00d7 15 = 10 chickens.\nThe cost to feed the chickens is $2 per chicken, so:\nCost = 10 chickens \u00d7 $2/chicken = $20\nProblem 2:\nLet the original width of the garden be w and the length be 4w.\nThe original area of the garden is 4w2.\nIf the width is increased by 5 feet, the new width is w + 5.\nThe new area is then 4w(w + 5) = 4w2 + 20w.\nGiven the total area of the garden is increased by 120 square feet:\n4w2 + 20w \u2212 (4w2) = 120\n20w = 120\nw = 6 feet\nSo, the original area is:\n4w2 = 4(6)2 = 144 square feet\nComparison:\nProblem 1 involves basic arithmetic and a simple fraction, making it relatively straightforward.\nProblem 2 involves solving a quadratic equation and requires a better understanding of algebraic\nconcepts and the area of rectangles.\nThus, Problem 2 is more difficult than Problem 1.\nFigure 9: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct.\n\"Problem 2\" is in MathCodeInstruct and \"Problem 1\" is the problem from GSM8K that was used to\ngenerate \"Problem 2\".\n18\n(a) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"he two solutions of the equation x2 + bx + 48 = 0 are in the ratio of 3 to 1 for some\nvalues of b. What is the largest possible value of b?\"\nProblem 2: \"In a rectangular garden, the length is 4 times the width. If the total area of the garden is\nincreased by 120 square feet by increasing the width by 5 feet while keeping the length constant, what\nis the original area of the garden?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(b) Judgement from GPT-4:\nThe two problems provided, at their core, involve algebraic manipulation, but Problem 1 involves\nquadratic equations and a bit more conceptual understanding of quadratic roots, whereas Problem 2 is\nmore straightforward and involves linear equations. Based on this, it can be said that \"Problem 1\" is\nmore difficult compared to \"Problem 2\". Here\u2019s a brief explanation and solution for both problems to\nillustrate this point:\nProblem 1:\nGiven that the roots of x2 + bx + 48 = 0 are in the ratio 3 : 1, let the roots be 3k and k. The sum of\nthe roots is 4k, and the product of the roots is 48.\nUsing Vieta\u2019s formulas:\n- 3k + k = 4k = \u2212b\n- 3k \u2217 k = 3k2 = 48\nSolving for k in the second equation:\nk2 = 16 =\u21d2 k = \u00b14\nSo, the roots are 12 and 4 or \u221212 and \u22124. Thus, the possible values of b are \u221216 or 16.\nSo the largest possible value of b is 16.\nProblem 2:\nLet the width of the rectangular garden be w, then the length would be 4w.\nThe original area is:\nAoriginal = l \u2217 w = 4w \u2217 w = 4w2\nIf the width is increased by 5 feet, the new width is w + 5, and the new area is:\nAnew = l \u2217 (w + 5) = 4w \u2217 (w + 5) = 4w2 + 20w\nGiven that the total area of the garden is increased by 120 square feet:\nAnew = Aoriginal + 120\n4w2 + 20w = 4w2 + 120\nSolving for w:\n20w = 120 =\u21d2 w = 6\nSo, the original area of the garden is:\nAoriginal = 4w2 = 4 \u2217 36 = 144 square feet\nProblem 1 requires an understanding of quadratic equations, Vieta\u2019s formulas, and consideration of\nmultiple possibilities, making it more challenging compared to Problem 2, which involves simpler\nalgebraic manipulations with linear equations.\nFigure 10: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct.\n\"Problem 2\" is in MathCodeInstruct and \"Problem 1\" is the problem from MATH that was used to\ngenerate \"Problem 2\".\n19\n(a) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Kara mixes cups of sugar and water in the ratio of 1:2 to make one liter of lemonade.\nCalculate the total number of cups of sugar used if she used 84 cups to make lemonade.\"\nProblem 2: \"A cylindrical water tank has two compartments. The first compartment is half the height\nof the second compartment. When filled, the first compartment holds 60 liters of water. If a pipe\npumps water into the tank at a rate of 10 liters per minute and starts filling the second compartment,\nhow many minutes will it take to fill the entire tank?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(b) Judgement from GPT-4:\nLet\u2019s solve each problem one by one and then determine which one is more difficult.\nProblem 1:\nKara mixes cups of sugar and water in the ratio of 1 : 2.\nSo, for every 1 cup of sugar, there are 2 cups of water.\nThe ratio of sugar to the total mixture is:\nSugar : Total = 1 : (1 + 2) = 1 : 3\nShe used 84 cups to make lemonade, so:\nCups of sugar used = 1\n3 \u00d7 84 = 28 cups\nProblem 2:\nThe first compartment is half the height of the second compartment but holds 60 liters of water.\nSo, the second compartment will hold 2 \u00d7 60 liters of water, which is 120 liters of water.\nThe total volume of the tank, including both compartments, is:\n60 liters + 120 liters = 180 liters\nThe pipe pumps water at a rate of 10 liters per minute, so to fill the entire tank, it will take:\nTime =\n180 liters\n10 liters/min = 18 minutes\nConclusion:\nBoth problems can be solved with basic arithmetic and understanding of ratios, and neither requires\nadvanced mathematical concepts. Therefore, it can be considered a \"Tie\" in terms of difficulty.\nFigure 11: An example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct.\n\"Problem 2\" is in MathCodeInstruct, and \"Problem 1\" is the problem from GSM8K that was used to\ngenerate \"Problem 2\". It is an example of a tie.\n20\n(a) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"When Jeffrey walks, for every three steps forward, he takes two steps backwards.\nTherefore, if the distance between the house and the mailbox is 66 steps, what is the total number of\nsteps Jeffrey takes when he goes from the house to the mailbox?\"\nProblem 2: \"For every four apples Jake eats, he plants three apple seeds. If at the end of the month he\nhas consumed 24 apples, how many seeds has he planted?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(b) Judgement from GPT-4:\nLet\u2019s analyze each problem step by step:\nProblem 1:\nEvery 3 steps forward and 2 steps back mean Jeffrey effectively moves 1 step for every 5 steps he\ntakes (3 steps forward - 2 steps back = 1 step net movement).\nTo cover 66 steps:\nSteps needed = (Total distance / Net movement per cycle) \u00d7 Steps per cycle\nSteps needed = (66 steps / 1 step) \u00d7 5 = 330 steps\nProblem 2:\nFor every 4 apples Jake eats, he plants 3 seeds.\nSo, for 24 apples:\nSeeds planted = (24 apples / 4) \u00d7 3 seeds = 6 \u00d7 3 = 18 seeds\nTo determine which is more difficult, it seems both problems involve similar basic math operations\n(division and multiplication).\nHowever, the first problem introduces an extra layer of complexity with the two different types of\nmovements (forward and backward) compared to the direct ratio in the second problem.\nGiven this analysis, Problem 1 seems slightly more difficult than Problem 2 due to this added layer of\nunderstanding needed to solve it.\nFigure 12: A example of using GPT-4 to evaluate the complexity of problems in MathCodeInstruct and\nit is an uncommon example that the problem from GSM8K is slightly more difficult than the interpolation\nproblem. \"Problem 2\" is in MathCodeInstruct and \"Problem 1\" is the problem from GSM8K that was\nused to generate \"Problem 2\".\n21\n(a) Prompt:\nPlease create a new problem, following the given \"Example\" and the new problem should be harder\nthan the \"Example\".\nExample: \"Josh\u2019s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil\nfor $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?\"\n(b) According new problem generated by GPT-4:\nRebecca\u2019s dad gives her $50 to spend at the amusement park. She buys a ticket for $25 and a souvenir\nfor $6. After riding several rides, she decides to buy six balloons for her friends. If each balloon costs\n$2.50, how much money does Rebecca have left?\n(c) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Josh\u2019s mom gives him $20 to go shopping at the mall. He buys a hat for $10 and a pencil\nfor $2. Then he buys four cookies. If each cookie costs $1.25, how much money does Josh have left?\"\nProblem 2: \"Rebecca\u2019s dad gives her $50 to spend at the amusement park. She buys a ticket for $25\nand a souvenir for $6. After riding several rides, she decides to buy six balloons for her friends. If\neach balloon costs $2.50, how much money does Rebecca have left?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(d) Judgement from GPT-4:\nLet\u2019s solve both problems step-by-step:\nProblem 1: Josh\u2019s initial amount: $20\nHe buys a hat: $20 - $10 = $10 left\nHe buys a pencil: $10 - $2 = $8 left\nCost of four cookies = 4 x $1.25 = $5\nAmount after buying cookies: $8 - $5 = $3\nSo, Josh has $3 left.\nProblem 2: Rebecca\u2019s initial amount: $50\nShe buys a ticket: $50 - $25 = $25 left\nShe buys a souvenir: $25 - $6 = $19 left\nCost of six balloons = 6 x $2.50 = $15\nAmount after buying balloons: $19 - $15 = $4\nSo, Rebecca has $4 left.\nIn terms of difficulty, both problems involve subtraction and multiplication. However, Problem 2 has\nslightly larger numbers and one more multiplication operation than Problem 1. Hence, you could argue\nthat:\nAnswer: Problem 2 is more difficult than Problem 1.\nFigure 13: An example of using GPT-4 to create problems based only on the problems from GSM8K and then\nevaluate the complexity of the newly generated problems. \"Problem 2\" is the new problem, and \"Problem 1\" is\nthe problem from GSM8K that was used to generate \"Problem 2\".\n22\n(a) Prompt:\nPlease create a new problem, following the given \"Example\" and the new problem should be harder\nthan the \"Example\".\nExample: \"Kara mixes cups of sugar and water in the ratio of 1:2 to make one liter of lemonade.\nCalculate the total number of cups of sugar used if she used 84 cups to make lemonade.\"\n(b) According new problem generated by GPT-4:\nLiam mixes cups of honey, water, and lemon juice in the ratio of 2:5:3 to make one gallon of honey\nlemonade. Calculate the total number of cups of honey used if he used 120 cups to make honey\nlemonade.\n(c) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Kara mixes cups of sugar and water in the ratio of 1:2 to make one liter of lemonade.\nCalculate the total number of cups of sugar used if she used 84 cups to make lemonade.\"\nProblem 2: \"Liam mixes cups of honey, water, and lemon juice in the ratio of 2:5:3 to make one gallon\nof honey lemonade. Calculate the total number of cups of honey used if he used 120 cups to make\nhoney lemonade.\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(d) Judgement from GPT-4:\nLet\u2019s solve each problem to determine which one is more difficult:\nProblem 1:\nIf the ratio of sugar to water is 1:2, then for every 3 cups (1 cup of sugar + 2 cups of water), 1 liter of\nlemonade is made.\nSo, if she used 84 cups to make lemonade, then:\nNumber of liters of lemonade made = 84 cups / 3 = 28 liters\nTherefore, the total number of cups of sugar used is:\n28 liters * 1 cup of sugar = 28 cups of sugar\nProblem 2:\nIf the ratio of honey to water to lemon juice is 2:5:3, then for every 10 cups (2 cups of honey + 5 cups\nof water + 3 cups of lemon juice), 1 gallon of honey lemonade is made.\nSo, if he used 120 cups to make honey lemonade, then:\nNumber of gallons of honey lemonade made = 120 cups / 10 = 12 gallons\nTherefore, the total number of cups of honey used is:\n12 gallons * 2 cups of honey = 24 cups of honey\nComparison:\nBoth problems involve a similar approach to solve.\nBoth require understanding ratios and\nproportionally dividing the given total cups by the sum of the ratio\u2019s components.\nVerdict: Tie. Both problems are of similar difficulty.\nFigure 14: An example of using GPT-4 to create problems based only on the problems from GSM8K and then\nevaluate the complexity of the newly generated problems. \"Problem 2\" is the new problem, and \"Problem 1\" is\nthe problem from GSM8K that was used to generate \"Problem 2\".\n23\n(a) Prompt:\nPlease create a new problem, following the given \"Example\" and the new problem should be harder\nthan the \"Example\".\nExample: \"Hillary always buys the Wednesday, Thursday and Friday editions of the local newspaper\nfor $0.50 each. On Sunday, she spends $2.00 to get that copy. How much does she spend on the\nnewspaper over 8 weeks?\"\n(b) According new problem generated by GPT-4:\nJoseph purchases his favorite magazines on Monday, Tuesday, and Saturday every week. The Monday\nedition costs $1.25, the Tuesday edition costs $1.50, and the Saturday edition costs $2.75. Additionally,\nhe buys a special weekly edition every Sunday for $3.50. How much does Joseph spend on purchasing\nhis favorite magazines over 12 weeks?\n(c) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Hillary always buys the Wednesday, Thursday and Friday editions of the local newspaper\nfor $0.50 each. On Sunday, she spends $2.00 to get that copy. How much does she spend on the\nnewspaper over 8 weeks?\"\nProblem 2: \"Joseph purchases his favorite magazines on Monday, Tuesday, and Saturday every week.\nThe Monday edition costs $1.25, the Tuesday edition costs $1.50, and the Saturday edition costs\n$2.75. Additionally, he buys a special weekly edition every Sunday for $3.50. How much does Joseph\nspend on purchasing his favorite magazines over 12 weeks?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(d) Judgement from GPT-4:\nLet\u2019s break down each problem:\nProblem 1:\n1. Hillary buys the newspaper on Wednesday, Thursday, and Friday for $0.50 each. So, the cost for\nthese 3 days = 3 * $0.50 = $1.50\n2. On Sunday, she spends $2.00.\n3. The total she spends in a week = $1.50 + $2.00 = $3.50\n4. Over 8 weeks, she spends = 8 * $3.50 = $28.00\nProblem 2:\n1. Joseph buys magazines on Monday for $1.25, Tuesday for $1.50, and Saturday for $2.75. So, the\ncost for these 3 days = $1.25 + $1.50 + $2.75 = $5.50\n2. Additionally, he buys a special edition on Sunday for $3.50.\n3. The total he spends in a week = $5.50 + $3.50 = $9.00\n4. Over 12 weeks, he spends = 12 * $9.00 = $108.00\nIn terms of mathematical complexity, both problems involve the same operations: multiplication and\naddition. However, the second problem has more purchases and higher values involved, which could\nmake it slightly more challenging for some people to solve. Also, the fact that Joseph\u2019s purchases\nextend over 12 weeks, as opposed to Hillary\u2019s 8 weeks, adds a layer of complexity.\nSo, based on the given information, the answer would be: Problem 2.\nFigure 15: An example of using GPT-4 to create problems based only on the problems from GSM8K and then\nevaluate the complexity of the newly generated problems. \"Problem 2\" is the new problem, and \"Problem 1\" is\nthe problem from GSM8K that was used to generate \"Problem 2\".\n24\n(a) Prompt:\nPlease create a new problem, following the given \"Example\" and the new problem should be harder\nthan the \"Example\".\nExample: \"Peter has 15 birds. 1/3 are ducks. The rest are chickens and require special feed that costs\n$2 per bird. How much does it cost to feed the chickens?\"\n(b) According new problem generated by GPT-4:\nMaria has 36 animals. 1/4 of them are rabbits. The rest are cats, and each cat needs a special food that\ncosts $3 per cat per day. If Maria feeds the cats for 5 days, how much will she spend on cat food?\n(c) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Peter has 15 birds. 1/3 are ducks. The rest are chickens and require special feed that costs\n$2 per bird. How much does it cost to feed the chickens?\"\nProblem 2: \"Maria has 36 animals. 1/4 of them are rabbits. The rest are cats, and each cat needs a\nspecial food that costs $3 per cat per day. If Maria feeds the cats for 5 days, how much will she spend\non cat food?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(d) Judgement from GPT-4:\nLet\u2019s break down the steps required to solve each problem:\nProblem 1:\n1. Calculate the number of ducks: 15 \u00d7 1\n3 = 5.\n2. Subtract the number of ducks from the total to find the number of chickens: 15 \u2212 5 = 10.\n3. Calculate the cost to feed the chickens: 10 \u00d7 $2 = $20.\nProblem 2:\n1. Calculate the number of rabbits: 36 \u00d7 1\n4 = 9.\n2. Subtract the number of rabbits from the total to find the number of cats: 36 \u2212 9 = 27.\n3. Calculate the daily cost to feed the cats: 27 \u00d7 $3 = $81.\n4. Multiply the daily cost by 5 days: 5 \u00d7 $81 = $405.\nWhile both problems require a similar initial approach of finding the number of animals not explicitly\nstated, Problem 2 has an additional step of multiplying by the number of days to get the final cost.\nBased on the number of steps and calculations required, Problem 2 is slightly more difficult than\nProblem 1.\nFigure 16: An example of using GPT-4 to create problems based only on the problems from GSM8K and then\nevaluate the complexity of the newly generated problems. \"Problem 2\" is the new problem, and \"Problem 1\" is\nthe problem from GSM8K that was used to generate \"Problem 2\".\n25\n(a) Prompt:\nPlease create a new problem, following the given \"Example\" and the new problem should be harder\nthan the \"Example\".\nExample: \"Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and\nfive nickels, how many coins does Zain have?\"\n(b) According new problem generated by GPT-4:\nXavier has 15 more of each coin than Elara. If Elara has nine quarters, eleven dimes, eight nickels,\nand three pennies, how many coins does Xavier have?\n(c) Evaluation Prompt:\nWhich problem is more difficult?\nProblem 1: \"Zain has 10 more of each coin than Emerie. If Emerie has six quarters, seven dimes, and\nfive nickels, how many coins does Zain have?\"\nProblem 2: \"Xavier has 15 more of each coin than Elara. If Elara has nine quarters, eleven dimes,\neight nickels, and three pennies, how many coins does Xavier have?\"\nYou answer should be one of \"Problem 1\", \"Problem 2\" and \"Tie\".\n(d) Judgement from GPT-4:\nLet\u2019s break down each problem step by step:\nProblem 1:\nEmerie has: 6 quarters, 7 dimes, 5 nickels\nFor each type of coin, Zain has 10 more than Emerie.\nThus:\n- Zain has 6 + 10 = 16 quarters\n- Zain has 7 + 10 = 17 dimes\n- Zain has 5 + 10 = 15 nickels\nTotal number of coins Zain has = 16 + 17 + 15 = 48 coins.\nProblem 2:\nElara has: 9 quarters, 11 dimes, 8 nickels, 3 pennies\nFor each type of coin, Xavier has 15 more than Elara.\nThus:\n- Xavier has 9 + 15 = 24 quarters\n- Xavier has 11 + 15 = 26 dimes\n- Xavier has 8 + 15 = 23 nickels\n- Xavier has 3 + 15 = 18 pennies\nTotal number of coins Xavier has = 24 + 26 + 23 + 18 = 91 coins.\nNow, let\u2019s evaluate the complexity:\nProblem 1 has 3 types of coins and a difference of 10 for each coin.\nProblem 2 has 4 types of coins and a difference of 15 for each coin.\nConsidering that \"more difficult\" might mean more steps or operations to solve, Problem 2 has more\ntypes of coins and a larger difference, so it requires slightly more computation.\nAnswer: Problem 2 is more difficult.\nFigure 17: An example of using GPT-4 to create problems based only on the problems from GSM8K and then\nevaluate the complexity of the newly generated problems. \"Problem 2\" is the new problem, and \"Problem 1\" is\nthe problem from GSM8K that was used to generate \"Problem 2\".\n26\nalgebra\nprealgebra\nnumber\ntheory\ncounting and\nprobability\nprecalculus\nintermediate\nalgebra\ngeometry\n \n0.72\n0.51\n0.4\n0.27\n0.13\n0.52\n0.53\n0.41\n0.3\n0.14\n0.57\n0.28\n0.26\n0.15\n0.05\n0.51\n0.46\n0.17\n0.08\n0.07\n0.32\n0.15\n0.09\n0.04\n0\n0.29\n0.21\n0.17\n0.08\n0.04\n0.29\n0.3\n0.16\n0.13\n0.01\nMathCoder-L-7B\n(Overall: 0.233)\n \n0.76\n0.61\n0.49\n0.37\n0.21\n0.69\n0.64\n0.49\n0.38\n0.22\n0.67\n0.43\n0.32\n0.29\n0.17\n0.59\n0.5\n0.19\n0.15\n0.05\n0.4\n0.21\n0.12\n0.1\n0.03\n0.35\n0.3\n0.18\n0.13\n0.09\n0.29\n0.35\n0.27\n0.1\n0.01\nMathCoder-CL-7B\n(Overall: 0.3024)\nalgebra\nprealgebra\nnumber\ntheory\ncounting and\nprobability\nprecalculus\nintermediate\nalgebra\ngeometry\n \n0.77\n0.63\n0.5\n0.4\n0.21\n0.59\n0.56\n0.5\n0.42\n0.19\n0.77\n0.41\n0.34\n0.2\n0.16\n0.67\n0.48\n0.24\n0.14\n0.06\n0.42\n0.21\n0.13\n0.09\n0\n0.4\n0.25\n0.18\n0.14\n0.06\n0.37\n0.39\n0.2\n0.17\n0.02\nMathCoder-L-13B\n(Overall: 0.2992)\n \n0.83\n0.67\n0.57\n0.45\n0.29\n0.73\n0.69\n0.52\n0.45\n0.24\n0.77\n0.49\n0.42\n0.32\n0.21\n0.74\n0.58\n0.22\n0.16\n0.18\n0.44\n0.34\n0.23\n0.15\n0.04\n0.44\n0.38\n0.24\n0.17\n0.1\n0.24\n0.48\n0.25\n0.18\n0.05\nMathCoder-CL-13B\n(Overall: 0.3588)\nalgebra\nprealgebra\nnumber\ntheory\ncounting and\nprobability\nprecalculus\nintermediate\nalgebra\ngeometry\n \n0.9\n0.77\n0.75\n0.6\n0.41\n0.77\n0.81\n0.68\n0.53\n0.34\n0.87\n0.49\n0.64\n0.46\n0.26\n0.77\n0.58\n0.43\n0.38\n0.2\n0.46\n0.46\n0.35\n0.18\n0.01\n0.62\n0.38\n0.31\n0.2\n0.13\n0.39\n0.49\n0.42\n0.23\n0.08\nMathCoder-L-70B\n(Overall: 0.442)\n \n0.93\n0.78\n0.7\n0.62\n0.39\n0.83\n0.8\n0.66\n0.55\n0.33\n0.83\n0.58\n0.57\n0.49\n0.36\n0.77\n0.66\n0.51\n0.27\n0.26\n0.61\n0.42\n0.36\n0.2\n0.1\n0.48\n0.35\n0.28\n0.19\n0.1\n0.39\n0.44\n0.36\n0.22\n0.05\nMathCoder-CL-34B\n(Overall: 0.452)\nL1\nL2\nL3\nL4\nL5\nalgebra\nprealgebra\nnumber\ntheory\ncounting and\nprobability\nprecalculus\nintermediate\nalgebra\ngeometry\n \n0.92\n0.76\n0.73\n0.62\n0.37\n0.81\n0.79\n0.68\n0.57\n0.38\n0.8\n0.64\n0.61\n0.46\n0.31\n0.74\n0.62\n0.49\n0.31\n0.22\n0.6\n0.48\n0.28\n0.17\n0.07\n0.58\n0.45\n0.32\n0.22\n0.11\n0.5\n0.5\n0.44\n0.2\n0.07\nMathCode-CL-34B with loss on execution results\n(Overall: 0.459)\nL1\nL2\nL3\nL4\nL5\n \n0.44\n0.32\n0.27\n0.17\n0.07\n0.53\n0.45\n0.34\n0.21\n0.11\n0.3\n0.22\n0.13\n0.09\n0.04\n0.49\n0.37\n0.25\n0.1\n0.03\n0.33\n0.12\n0.02\n0.03\n0.01\n0.25\n0.15\n0.07\n0.02\n0.02\n0.45\n0.26\n0.14\n0.06\n0.01\nMathCode-CL-34B without code execution\n(Overall: 0.169)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 18: Performance comparison of MathCoeder-L (7B, 13B, 70B) and MathCoder-CL (7B, 13B, 34B)\non the MATH dataset by levels and subjects. The improved accuracy from MathCoder-L to MathCoder-CL\ncomes primarily from subjects that require precise calculations like algebra, number theory, and counting and\nprobability.\n27\nProblem: MATH/test/number-theory/1191\n<|user|><|text|> If n is an integer, 1 \u2264 n \u2264 2010, how many fractions\nn2\n2010 yield repeating decimals? <|endofblock|><|endofmessage|>\n(a) CoT solution for MATH/test/number-theory/1191\nTo determine whether a fraction a\nb produces a repeating decimal,\nyou need to look at the denominator, b. A fraction will produce a\nrepeating decimal if and only if, after reducing it to simplest form,\nthe denominator has a prime factor other than 2 or 5.\nLet\u2019s solve the problem step by step:\nStep 1: Prime Factorization of 2010\n2010 = 2 \u00d7 3 \u00d7 5 \u00d7 67\nStep 2: Repeating Decimal Criteria For\nn2\n2010 to be a repeating\ndecimal, the denominator, after reducing the fraction to the simplest\nform, must have a prime factor other than 2 or 5.\nStep 3: Counting Possible Values of n To get a denominator with\na prime factor other than 2 or 5 in reduced form, n should not be\ndivisible by 3, 67, or any multiple of them. So, we can find the\npossible values of n as follows:\na) There are 2010/3 = 670 numbers that are multiples of 3\nbetween 1 and 2010.\nb) There are 2010/67 = 30 numbers that are multiples of 67\nbetween 1 and 2010.\nc) There are overlapping numbers which are multiples of 3 \u00d7 67 =\n201. There are 2010/201 = 10 such numbers.\nStep 4:\nApplying the Inclusion-Exclusion Principle To avoid\ncounting the overlapping numbers twice, we apply the inclusion-\nexclusion principle:\nCount = 670 + 30 \u2212 10 = 700 \u2212 10 = 690\nStep 5: Subtract from Total Possible Values of n There are 2010\npossible values for n in total.\nTo find the count of n yielding\nrepeating decimals:\n2010 \u2212 690 = 1320\nSo, there are 1320 \u2717 fractions\nn2\n2010 that yield repeating decimals.\n(b) PoT solution for MATH/test/number-theory/1191\nfrom sympy.ntheory import factorint\n# Given range\nn_range = range(1, 2011)\n# Given denominator\ndenominator = 2010\n# Initialize counter for fractions with\nrepeating decimals\nrepeating_decimal_count = 0\n# Factor the denominator to find the prime\nfactorization\ndenominator_factors = factorint(denominator)\n# Loop over the range to find the fractions\nwith repeating decimals\nfor n in n_range:\n# Factor the numerator\nnumerator_factors = factorint(n**2)\n# Check if each factor of the denominator\nis present in the numerator with at\nleast the same power\nrepeating_decimal = any(\ndenominator_factors[p] >\nnumerator_factors.get(p, 0) for p in\ndenominator_factors)\n# If the fraction has repeating decimal,\nincrease the counter\nif repeating_decimal:\nrepeating_decimal_count += 1\nrepeating_decimal_count\n> > > 2009 \u2717\n(c) LCE solution for MATH/test/number-theory/1191\n<|assistant|> <|text|> Alright, for a fraction to yield a repeating\ndecimal, its denominator (after simplification) should contain prime\nfactors other than 2 and 5.\nSo, to solve this problem, we need to follow these steps:\n1. Factorize 2010 to get its prime factors.\n2. For each integer n from 1 to 2010, compute n2.\n3. Divide n2 by 2010 and simplify the fraction.\n4. Check if the simplified denominator has prime factors other than 2\nand 5.\nIf the denominator after simplification has only 2\u2019s and 5\u2019s as its\nprime factors, then it will yield a terminating decimal. Otherwise, it\nwill yield a repeating decimal.\nLet\u2019s proceed with these steps. <|endofblock|>\n<|code|>\nfrom math import gcd\n# Step 1: Factorize 2010 to get its prime\nfactors.\\\\\ndef prime_factors(n):\\\\\ni = 2\\\\\nfactors = {}\\\\\nwhile i * i <= n:\\\\\nif n % i:\ni += 1\nelse:\nn //= i\nfactors[i] = factors.get(i, 0) +\n1\nif n > 1:\nfactors[n] = factors.get(n, 0) + 1\nreturn factors\nfactors_2010 = prime_factors(2010)\n# Step 2, 3 and 4\ncount_repeating = 0\nfor n in range(1, 2011):\nn_square = n**2\ncommon_factor = gcd(n_square, 2010)\ndenominator = 2010 // common_factor\n# Check if the simplified denominator has\nprime factors other than 2 and 5\nfactors_denominator = prime_factors(\ndenominator)\nif 2 in factors_denominator:\ndel factors_denominator[2]\nif 5 in factors_denominator:\ndel factors_denominator[5]\nif factors_denominator:\n# if there are\nother prime factors left\ncount_repeating += 1\ncount_repeating\n<|endofblock|>\n<|execution|> 2000 <|endofblock|>\n<|text|> There are 2000 \u2713\nfractions of the form\nn2\n2010\nthat yield repeating decimals when 1\n\u2264\nn\n\u2264\n2010.\n<|endofblock|><|endofmessage|>\nFigure 19: Example of CoT, PoT and LCE solution with special token. The problem is from the test set of\nMATH in number theory with id 1191. In contrast to CoT, which consists solely of natural language, and PoT,\nwhich consists solely of code, our LCE solution intertwines natural language, code, and execution results.\n28\nProblem: MATH/test/algebra/2477\n<|user|><|text|> Consider the infinite arithmetic sequence A with first term 5 and common difference \u22122. Now define the infinite sequence B so that\nthe kth term of B is 2 raised to the kth term of A. Find the sum of all of the terms of B. <|endofblock|><|endofmessage|>\n(a) CoT solution for MATH/test/algebra/2477\nAlright. Let\u2019s start by understanding both sequences step by step.\nSequence A\nThis is an infinite arithmetic sequence. The first term, a1, is 5, and\nthe common difference, d, is -2.\nSo, we can describe the kth term of A as:\nak = 5 \u2212 2(k \u2212 1)\nak = 5 \u2212 2k + 2\nak = 7 \u2212 2k\nSequence B\nThe sequence B is defined such that the kth term of B is 2 raised\nto the kth term of A:\nbk = 2ak = 27\u22122k\nSum of Sequence B\nSince B is an infinite sequence, we are to find the sum of all terms of\nB:\nSB = 27 + 25 + 23 + 21 + \u00b7 \u00b7 \u00b7\nTo find SB, notice that it is a geometric series with the first term 27\nand common ratio 2\u22122 = 1\n4 .\nThe sum of an infinite geometric series can be found using the\nformula:\nS =\na\n1 \u2212 r\nwhere a is the first term and r is the common ratio.\nSubstitute the first term 27 and the common ratio 1\n4 into the formula:\nSB =\n27\n1 \u2212 1\n4\n=\n27\n3\n4\n=\n27 \u00b7 4\n3\n1\n=\n29\n3\nThus, the sum of all the terms of sequence B is 29\n3 .\u2717\n(b) PoT solution for MATH/test/algebra/2477\n# Given first term and common difference of\nsequence A\nfirst_term_A = 5\ncommon_diff_A = -2\n# Initialize sum of sequence B\nsum_B = 0\n# Loop over the terms of sequence A and\ncalculate corresponding terms of\nsequence B,\n# until the term of sequence A is non-\npositive.\nkth_term_A = first_term_A\nwhile kth_term_A > 0:\nkth_term_B = 2 ** kth_term_A\nsum_B += kth_term_B\nkth_term_A += common_diff_A\nsum_B\n> > > 42 \u2717\n(c) LCE solution for MATH/test/algebra/2477\n<|assistant|> <|text|> Sure. Let\u2019s solve the problem step by step.\nStep 1: Define Sequence A Given that the first term of sequence A\nis 5 and the common difference is -2, the kth term of A, denoted as\nAk, can be found using the formula:\nAk = 5 \u2212 2(k \u2212 1)\nAk = 7 \u2212 2k\nStep 2: Define Sequence B The kth term of sequence B, denoted\nas Bk, is defined as 2 raised to the kth term of A:\nBk = 2Ak\nSubstitute the value of Ak into the equation for Bk:\nBk = 27\u22122k\nStep 3: Find Sum of Sequence B To find the sum of all terms of\nsequence B, we need to find the sum to infinity of this sequence.\nSince the terms of sequence B are getting smaller (because 27\u22122k\ndecreases as k increases), this is a geometric series.\nThe sum of an infinite geometric series can be found using the\nformula:\nS =\na\n1 \u2212 r\nWhere:\n- S is the sum of the series,\n- a is the first term of the series,\n- r is the common ratio of the series.\nLet\u2019s find the first term and common ratio of the sequence B and then\nuse them to find the sum of sequence B.\n<|endofblock|>\n<|code|>\n# Given values\nfirst_term_A = 5\ncommon_difference_A = -2\nbase_B = 2\n# Calculating the first term of sequence B\nfirst_term_B = base_B ** first_term_A\n# Calculating the common ratio of sequence B\n# For k=1, A_k = 5, B_1 = 2^5\n# For k=2, A_k = 3, B_2 = 2^3 => B_2 = B_1 /\n2^2\ncommon_ratio_B = base_B ** (-2 * 1) # k=1, so\n2*1=2\n# Calculating the sum of sequence B\nsum_B = first_term_B / (1 - common_ratio_B)\nfirst_term_B, common_ratio_B, sum_B\n<|endofblock|>\n<|execution|> (32, 0.25, 42.666666666666667) <|endofblock|>\n<|text|>\nStep 3: Results Thus, the sum of all of the terms of sequence B is\napproximately 42.67.\u2713 <|endofblock|><|endofmessage|>\nFigure 20: Example of CoT, PoT and LCE solution with special token. The problem is from the test set of\nMATH in algebra with id 2477. In contrast to CoT, which consists solely of natural language, and PoT, which\nconsists solely of code, our LCE solution intertwines natural language, code, and execution results.\n29\nProblem: MATH/test/geometry/500\n<|user|><|text|> Points A(0, 0), B(9, 6) and C(6, 12) are vertices of triangle ABC. Point D is on segment AB such that 2(AD) = DB,\npoint E is on segment BC such that 2(BE) = EC and point F is on segment CA such that 2(CF ) = F A. What is the ratio of the area of\ntriangle DEF to the area of triangle ABC? Express your answer as a common fraction. <|endofblock|><|endofmessage|>\nCoT solution for MATH/test/geometry/500\nTo find the coordinates of points D, E, and F , we can use the section formula. The section formula states that if a line divides two points P (x1, y1)\nand Q(x2, y2) in the ratio m : n, then the coordinates (x, y) of the point dividing the line segment are given by:\nx =\nmx2 + nx1\nm + n\ny =\nmy2 + ny1\nm + n\nFind coordinates of D\nD divides AB in the ratio 2 : 1. Here, A(0, 0) and B(9, 6). So,\nxD =\n2 \u00b7 9 + 1 \u00b7 0\n2 + 1\n= 6\nyD =\n2 \u00b7 6 + 1 \u00b7 0\n2 + 1\n= 4\nSo, D(6, 4).\nFind coordinates of E\nE divides BC in the ratio 2 : 1. Here, B(9, 6) and C(6, 12). So,\nxE =\n2 \u00b7 6 + 1 \u00b7 9\n2 + 1\n= 7\nyE =\n2 \u00b7 12 + 1 \u00b7 6\n2 + 1\n= 10\nSo, E(7, 10).\nFind coordinates of F\nF divides CA in the ratio 2 : 1. Here, C(6, 12) and A(0, 0). So,\nxF =\n2 \u00b7 0 + 1 \u00b7 6\n2 + 1\n= 2\nyF =\n2 \u00b7 0 + 1 \u00b7 12\n2 + 1\n= 4\nSo, F (2, 4).\nFind the area ratio of \u25b3DEF to \u25b3ABC The area of a triangle formed by three vertices P (x1, y1), Q(x2, y2), and R(x3, y3) is given by:\nArea =\n1\n2\n|x1(y2 \u2212 y3) + x2(y3 \u2212 y1) + x3(y1 \u2212 y2)|\n1. Area of \u25b3ABC:\nAreaABC =\n1\n2\n|0 \u00b7 (6 \u2212 12) + 9 \u00b7 (12 \u2212 0) + 6 \u00b7 (0 \u2212 6)|\nAreaABC =\n1\n2\n|0 + 108 + 36|\nAreaABC = 72 square units\n2. Area of \u25b3DEF :\nAreaDEF =\n1\n2\n|6 \u00b7 (10 \u2212 4) + 7 \u00b7 (4 \u2212 4) + 2 \u00b7 (4 \u2212 10)|\nAreaDEF =\n1\n2\n|36 + 0 \u2212 12|\nAreaDEF = 12 square units\nFind the Ratio\nRatio =\nAreaDEF\nAreaABC\nRatio =\n12\n72\n=\n1\n6\nSo, the ratio of the area of triangle DEF to the area of triangle ABC is 1\n6 . \u2717\nFigure 21: Example of CoT solution. The problem is from the test set of MATH in geometry with id 500.\n30\nProblem: MATH/test/geometry/500\n<|user|><|text|> Points A(0, 0), B(9, 6) and C(6, 12) are vertices of triangle ABC. Point D is on segment AB such that 2(AD) = DB,\npoint E is on segment BC such that 2(BE) = EC and point F is on segment CA such that 2(CF ) = F A. What is the ratio of the area of\ntriangle DEF to the area of triangle ABC? Express your answer as a common fraction. <|endofblock|><|endofmessage|>\nPoT solution for MATH/test/geometry/500\nfrom sympy import symbols, solve, Point, Polygon, Rational\n# Given points of the triangle ABC\nA = Point(0, 0)\nB = Point(9, 6)\nC = Point(6, 12)\n# Given that D is on segment AB such that 2(AD) = DB\n# Therefore, AD:DB = 1:2\n# Using section formula to find the coordinates of D\nm1, m2 = 2, 1\nD = Point((m1*B.x + m2*A.x)/(m1+m2), (m1*B.y + m2*A.y)/(m1+m2))\n# Given that E is on segment BC such that 2(BE) = EC\n# Therefore, BE:EC = 1:2\n# Using section formula to find the coordinates of E\nm1, m2 = 1, 2\nE = Point((m1*C.x + m2*B.x)/(m1+m2), (m1*C.y + m2*B.y)/(m1+m2))\n# Given that F is on segment CA such that 2(CF) = FA\n# Therefore, CF:FA = 1:2\n# Using section formula to find the coordinates of F\nm1, m2 = 1, 2\nF = Point((m1*A.x + m2*C.x)/(m1+m2), (m1*A.y + m2*C.y)/(m1+m2))\n# Calculating the area of the triangles ABC and DEF\ntriangle_ABC = Polygon(A, B, C)\ntriangle_DEF = Polygon(D, E, F)\narea_ABC = abs(triangle_ABC.area)\narea_DEF = abs(triangle_DEF.area)\n# Finding the ratio of the areas of triangle DEF to triangle ABC\nratio = Rational(area_DEF, area_ABC)\nratio\n> > > 2/9 \u2717\nFigure 22: Example of PoT solution. The problem is from the test set of MATH in geometry with id 500.\n31\nProblem: MATH/test/geometry/500\n<|user|><|text|> Points A(0, 0), B(9, 6) and C(6, 12) are vertices of triangle ABC. Point D is on segment AB such that 2(AD) = DB,\npoint E is on segment BC such that 2(BE) = EC and point F is on segment CA such that 2(CF ) = F A. What is the ratio of the area of\ntriangle DEF to the area of triangle ABC? Express your answer as a common fraction. <|endofblock|><|endofmessage|>\nLCE solution for MATH/test/geometry/500\n<|assistant|> <|text|> Let\u2019s solve the problem step by step:\n1. Find Points D, E, F:\n- Use the section formula to find the coordinates of the points D, E, and F .\n- The section formula is:\n(x, y) =\n\u0012 m \u00b7 x2 + n \u00b7 x1\nm + n\n,\nm \u00b7 y2 + n \u00b7 y1\nm + n\n\u0013\n- For point D, m = 2, n = 1, x1 = 0, y1 = 0, x2 = 9, and y2 = 6.\n- For point E, m = 2, n = 1, x1 = 9, y1 = 6, x2 = 6, and y2 = 12.\n- For point F , m = 2, n = 1, x1 = 6, y1 = 12, x2 = 0, and y2 = 0.\n2. Find the Area of the Triangles ABC and DEF:\n- The area of a triangle with vertices (x1, y1), (x2, y2), and (x3, y3) is given by:\nArea =\n|x1(y2 \u2212 y3) + x2(y3 \u2212 y1) + x3(y1 \u2212 y2)|\n2\n3. Find the Ratio of the Areas:\n- The ratio of the areas of triangles DEF and ABC is given by:\nRatio =\nArea of DEF\nArea of ABC\nLet\u2019s start by finding the coordinates of points D, E, and F using the section formula. <|endofblock|>\n<|code|>\n# Given points and ratios\nA = (0, 0)\nB = (9, 6)\nC = (6, 12)\nm1, n1 = 2, 1 # ratio for point D\nm2, n2 = 2, 1 # ratio for point E\nm3, n3 = 2, 1 # ratio for point F\n# Using section formula to find coordinates of D, E, F\nD = ((m1 * B[0] + n1 * A[0]) / (m1 + n1), (m1 * B[1] + n1 * A[1]) / (m1 + n1))\nE = ((m2 * C[0] + n2 * B[0]) / (m2 + n2), (m2 * C[1] + n2 * B[1]) / (m2 + n2))\nF = ((m3 * A[0] + n3 * C[0]) / (m3 + n3), (m3 * A[1] + n3 * C[1]) / (m3 + n3))\nD, E, F\n<|endofblock|>\n<|execution|> ((6.0, 4.0), (7.0, 10.0), (2.0, 4.0)) <|endofblock|>\n<|text|> The coordinates of the points are as follows:\n- Point D is located at (6, 4)\n- Point E is located at (7, 10)\n- Point F is located at (2, 4)\nNow, let\u2019s proceed to find the areas of triangles ABC and DEF using the formula for the area of a triangle, and then find the ratio of these areas.\n<|endofblock|>\n<|code|>\n# Function to find the area of a triangle given its vertices\ndef triangle_area(v1, v2, v3):\nreturn 0.5 * abs(v1[0] * (v2[1] - v3[1]) + v2[0] * (v3[1] - v1[1]) + v3[0] * (v1[1] - v2[1]))\n# Areas of triangles ABC and DEF\narea_ABC = triangle_area(A, B, C)\narea_DEF = triangle_area(D, E, F)\n# Ratio of areas\nratio_areas = area_DEF / area_ABC\narea_ABC, area_DEF, ratio_areas\n<|endofblock|>\n<|execution|> (36.0, 12.0, 0.3333333333333333) <|endofblock|>\n<|text|> So, the ratio of the area of triangle DEF to the area of triangle ABC is 1\n3\u2713 or 0.3333. <|endofblock|><|endofmessage|>\nFigure 23: Example of LCE solution with special token. The problem is from the test set of MATH in geometry\nwith id 500. In contrast to CoT, which consists solely of natural language, and PoT, which consists solely of\ncode, our LCE solution intertwines natural language, code, and execution results.\n32\n"
  },
  {
    "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
    "link": "https://arxiv.org/pdf/2310.03714.pdf",
    "upvote": "24",
    "text": "Preprint\nDSPY:\nCOMPILING\nDECLARATIVE\nLANGUAGE\nMODEL CALLS INTO SELF-IMPROVING PIPELINES\nOmar Khattab,1 Arnav Singhvi,2\nParidhi Maheshwari,4 Zhiyuan Zhang,1\nKeshav Santhanam,1 Sri Vardhamanan,6 Saiful Haq,6\nAshutosh Sharma,6 Thomas T. Joshi,7 Hanna Moazam,8\nHeather Miller,3,9 Matei Zaharia,2 Christopher Potts1\n1Stanford University, 2UC Berkeley, 3Carnegie Mellon University,\n4Amazon Alexa AI, 5Dashworks Technologies, Inc.,\n6IIT Bombay, 7Calera Capital, 8Microsoft, 9Two Sigma Investments\nokhattab@cs.stanford.edu\nABSTRACT\nThe ML community is rapidly exploring techniques for prompting language mod-\nels (LMs) and for stacking them into pipelines that solve complex tasks. Un-\nfortunately, existing LM pipelines are typically implemented using hard-coded\n\u201cprompt templates\u201d, i.e. lengthy strings discovered via trial and error. Toward a\nmore systematic approach for developing and optimizing LM pipelines, we intro-\nduce DSPy, a programming model that abstracts LM pipelines as text transforma-\ntion graphs, i.e. imperative computation graphs where LMs are invoked through\ndeclarative modules. DSPy modules are parameterized, meaning they can learn\n(by creating and collecting demonstrations) how to apply compositions of prompt-\ning, finetuning, augmentation, and reasoning techniques. We design a compiler\nthat will optimize any DSPy pipeline to maximize a given metric. We conduct\ntwo case studies, showing that succinct DSPy programs can express and optimize\nsophisticated LM pipelines that reason about math word problems, tackle multi-\nhop retrieval, answer complex questions, and control agent loops. Within minutes\nof compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-\nbootstrap pipelines that outperform standard few-shot prompting (generally by\nover 25% and 65%, respectively) and pipelines with expert-created demonstra-\ntions (by up to 5\u201346% and 16\u201340%, respectively). On top of that, DSPy pro-\ngrams compiled to open and relatively small LMs like 770M-parameter T5 and\nllama2-13b-chat are competitive with approaches that rely on expert-written\nprompt chains for proprietary GPT-3.5.\nDSPy is available at https://github.com/stanfordnlp/dspy.\n1\nINTRODUCTION\nLanguage models (LMs) are enabling researchers to build NLP systems at higher levels of abstrac-\ntion and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an\nexploding space of \u201cprompting\u201d techniques\u2014and lightweight finetuning techniques\u2014for adapting\nLMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022;\nWang et al., 2022b), and augmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al.,\n2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these tech-\nniques are explored in isolation, but interest has been growing in building multi-stage pipelines and\nagents that decompose complex tasks into more manageable calls to LMs in an effort to improve\nperformance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot\net al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza & Rafiei, 2023; Shinn et al., 2023).\nUnfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is\nexacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\n1\narXiv:2310.03714v1  [cs.CL]  5 Oct 2023\nPreprint\ncalls in existing LM pipelines and in popular developer frameworks are generally implemented using\nhard-coded \u2018prompt templates\u2019, that is, long strings of instructions and demonstrations that are hand\ncrafted through manual trial and error. We argue that this approach, while pervasive, can be brittle\nand unscalable\u2014conceptually akin to hand-tuning the weights for a classifier. A given string prompt\nmight not generalize to different pipelines or across different LMs, data domains, or even inputs.\nToward a more systematic approach to designing AI pipelines, we introduce the DSPy programming\nmodel.1 DSPy pushes building new LM pipelines away from manipulating free-form strings and\ncloser to programming (composing modular operators to build text transformation graphs) where a\ncompiler automatically generates optimized LM invocation strategies and prompts from a program.\nWe draw inspiration from the consensus that emerged around neural network abstractions (Bergstra\net al., 2013), where (1) many general-purpose layers can be modularly composed in any complex\narchitecture and (2) the model weights can be trained using optimizers instead of being hand-tuned.\nTo this end, we propose the DSPy programming model (Sec 3). We first translate string-based\nprompting techniques, including complex and task-dependent ones like Chain of Thought (Wei et al.,\n2022) and ReAct (Yao et al., 2022), into declarative modules that carry natural-language typed sig-\nnatures. DSPy modules are task-adaptive components\u2014akin to neural network layers\u2014that abstract\nany particular text transformation, like answering a question or summarizing a paper. We then pa-\nrameterize each module so that it can learn its desired behavior by iteratively bootstrapping useful\ndemonstrations within the pipeline. Inspired directly by PyTorch abstractions (Paszke et al., 2019),\nDSPy modules are used via expressive define-by-run computational graphs. Pipelines are expressed\nby (1) declaring the modules needed and (2) using these modules in any logical control flow (e.g.,\nif statements, for loops, exceptions, etc.) to logically connect the modules.\nWe then develop the DSPy compiler (Sec 4), which optimizes any DSPy program to improve quality\nor cost. The compiler inputs are the program, a few training inputs with optional labels, and a valida-\ntion metric. The compiler simulates versions of the program on the inputs and bootstraps example\ntraces of each module for self-improvement, using them to construct effective few-shot prompts\nor finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is\nconducted by teleprompters,2 which are general-purpose optimization strategies that determine how\nthe modules should learn from data. In this way, the compiler automatically maps the declarative\nmodules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.\nProgramming models like DSPy could be assessed along many dimensions, but we focus on the role\nof expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove\ntheir role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and\nteleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al.\n2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain\nof thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and\nagent loops. Our evaluations use a number of different compiling strategies effectively and show\nthat straightforward DSPy programs outperform systems using hand-crafted prompts, while also\nallowing our programs to use much smaller and hence more efficient LMs effectively.\nOverall, this work proposes the first programming model that translates prompting techniques into\nparameterized declarative modules and introduces an effective compiler with general optimiza-\ntion strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contri-\nbutions are empirical and algorithmic: with DSPy, we have found that we can implement very\nshort programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as\nllama2-13b-chat and T5-Large (770M parameters). Without hand-crafted prompts and within\nminutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of\nsimple programs from 33% to 82% (Sec 6) and from 32% to 46% (Sec 7) for GPT-3.5 and, simi-\nlarly, from 9% to 47% (Sec 6) and from 22% to 41% (Sec 7) for llama2-13b-chat.\n1DSPy is pronounced dee-ess-pie. It\u2019s the second iteration of our earlier Demonstrate\u2013Search\u2013Predict\nframework (DSP; Khattab et al. 2022). This paper introduces the key concepts in DSPy. For more extensive and\nup-to-date documentation of the framework, we refer readers to https://github.com/stanfordnlp/dspy.\n2We derive the name tele-prompters from the notion of abstracting and automating the task of prompting,\nin particular, such that it happens at a distance, without manual intervention.\n2\nPreprint\n2\nRELATED WORK\nThis work is inspired by the role that Torch (Collobert et al., 2002), Theano (Bergstra et al., 2010;\n2011; Al-Rfou et al., 2016), Chainer (Tokui et al., 2015), and others played in the development in\ndeep learning by providing powerful abstractions. A similar transformation is emerging with higher-\nlevel pipelines of LMs, and we are seeking to offer a solid conceptual framework and programming\nabstractions for what we call foundation model programming. We draw on differentiable program-\nming (Wang et al., 2018) but applied to LM calls rather than neural networks, and borrow syntactic\nelements from PyTorch (Paszke et al., 2019).\nIn-context learning (McCann et al. 2018; Radford et al. 2018; Brown et al. 2020) is a key mechanism\nfor foundation model programming. A growing body of work has revealed that, especially with\ninstruction tuning (Ouyang et al., 2022), we can elicit sophisticated behavior via prompting (Wei\net al., 2022; Wang et al., 2022b; Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Madaan et al.,\n2023). Similarly, forms of weak supervision that would normally require task-specific (Khattab\net al., 2021a;b) or hand-built (Ratner et al., 2016; Hancock et al., 2018) heuristics are now done by\nLMs (Wang et al., 2022b; Zelikman et al., 2022; Zhang et al., 2022; Shao et al., 2023).\nIn-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval\nmodels (Chen et al., 2017; Lewis et al., 2020; Guu et al., 2020; Lazaridou et al., 2022; Izacard et al.,\n2022), multimodal foundation models, and more traditional tools like APIs (Nakano et al., 2021)\nand calculators. A number of toolkits have been developed to facilitate this, including LangChain\n(Chase, 2022), Semantic Kernel (Microsoft, 2023), LlamaIndex (Liu, 2022), and many other re-\ntrieval and agent libraries. These toolkits provide pre-packaged chains and agents that connect\nLMs with numerous accessible tools. However, they suffer from the pervasive prompt engineering\nchallenges we address in DSPy: they express task-specific behavior through hand-written prompt\ntemplates (for detailed discussion, see Appendix B).\nResearchers are starting to apply discrete optimization and RL to find effective prompts, generally\nfor a single logical LM call (Guo et al., 2023; Pryzant et al., 2023; Huang et al., 2022; Yang et al.,\n2023). DSPy seeks to generalize this space: it offers a rich framework for optimizing arbitrary\npipelines from high-level declarative signatures, by bootstrapping high-quality multi-stage demon-\nstrations with constraints. In this framework, DSPy teleprompters may apply optimization using\nmodel selection techniques like cross-validation or, in principle, with sophisticated techniques in-\nvolving RL and LM feedback (Hu et al., 2023; Zhao et al., 2023a; Shinn et al., 2023) or learned or\nBayesian hyperparameter optimization methods (Bergstra et al., 2013; Akiba et al., 2019).\nThe present paper seeks to motivate DSPy as a programming model and to report new empirical\nfindings from applying the DSPy compiler. This is inspired by formative work by Bergstra et al.\n(2010; 2013), Paszke et al. (2019), and Wolf et al. (2020), who support their respective programming\nmodels with a mix of benchmark numbers and some qualitative measures. For the current paper, we\nfocus on showing that DSPy and its compiler allow us to build outstanding LM systems without\nhand-crafted prompt strings, but instead from truly modular units, and that this opens up doors for\nsystematically exploring a rich design space at a very high programmatic level of abstraction.\n3\nTHE DSPY PROGRAMMING MODEL\nWe present DSPy, which treats LMs as abstract devices for text generation,3 and optimizes their us-\nage in arbitrary computational graphs. DSPy programs are expressed in Python: each program takes\nthe task input (e.g., a question to answer or a paper to summarize) and returns the output (e.g., an\nanswer or a summary) after a series of steps. DSPy contributes three abstractions toward automatic\noptimization: signatures, modules, and teleprompters. Signatures abstract the input/output behavior\nof a module; modules replace existing hand-prompting techniques and can be composed in arbitrary\npipelines; and teleprompters optimize all modules in the pipeline to maximize a metric.\n3We assume access to one or more LMs, which consume a prompt string and return text completions. This\nmay be a promptable LM capable of in-context learning (e.g., GPT-3.5 or Llama2-7b) or a smaller finetuneable\nLM (e.g., T5-base). An LM may be selected as the default; operations will use it unless configured otherwise.\n3\nPreprint\n3.1\nNATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING & FINETUNING\nInstead of free-form string prompts, DSPy programs use natural language signatures to assign work\nto the LM. A DSPy signature is natural-language typed declaration of a function: a short declarative\nspec that tells DSPy what a text transformation needs to do (e.g., \u201cconsume questions and return\nanswers\u201d), rather than how a specific LM should be prompted to implement that behavior. More\nformally, a DSPy signature is a tuple of input fields and output fields (and an optional instruction).\nA field consists of field name and optional metadata.4 In typical usage, the roles of fields are inferred\nby DSPy as a function of field names. For instance, the DSPy compiler will use in-context learning\nto interpret question differently from answer and will iteratively refine its usage of these fields.\nSignatures offer two benefits over prompts: they can be compiled into self-improving and pipeline-\nadaptive prompts or finetunes. This is primarily done by bootstrapping (Sec 4) useful demonstrating\nexamples for each signature. Additionally, they handle structured formatting and parsing logic to\nreduce (or, ideally, avoid) brittle string manipulation in user programs.\nIn practice, DSPy signatures can be expressed with a shorthand notation like question -> answer,\nso that line 1 in the following is a complete DSPy program for a basic question-answering system\n(with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):\n1 qa = dspy.Predict(\"question -> answer\")\n2 qa(question=\"Where is Guaran\u00b4\u0131 spoken?\")\n3 # Out: Prediction(answer=\u2019Guaran\u00b4\u0131 is spoken mainly in South America.\u2019)\nIn the shorthand notation, each field\u2019s name indicates the semantic role that the input (or output)\nfield plays in the transformation. DSPy will parse this notation and expand the field names into\nmeaningful instructions for the LM, so that english document -> french translation would\nprompt for English to French translation. When needed, DSPy offers more advanced programming\ninterfaces for expressing more explicit constraints on signatures (Appendix A).\n3.2\nPARAMETERIZED & TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\nAkin to type signatures in programming languages, DSPy signatures simply define an interface and\nprovide type-like hints on the expected behavior. To use a signature, we must declare a module with\nthat signature, like we instantiated a Predict module above. A module declaration like this returns\na function having that signature.\nThe Predict Module The core module for working with signatures in DSPy is Predict (simplified\npseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to\nuse (initially None, but otherwise overrides the default LM for this module), and a list of demon-\nstrations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as\na callable function: it takes in keyword arguments corresponding to the signature input fields (e.g.,\nquestion), formats a prompt to implement the signature and includes the appropriate demonstra-\ntions, calls the LM, and parses the output fields. When Predict detects it\u2019s being used in compile\nmode, it will also internally track input/output traces to assist the teleprompter at bootstrapping the\ndemonstrations.\nOther Built-in Modules DSPy modules translate prompting techniques into modular functions that\nsupport any signature, contrasting with the standard approach of prompting LMs with task-specific\ndetails (e.g., hand-written few-shot examples). To this end, DSPy includes a number of more sophis-\nticated modules like ChainOfThought, ProgramOfThought, MultiChainComparison, and ReAct.5\nThese can all be used interchangeably to implement a DSPy signature. For instance, simply chang-\n4String descriptions of the task and the fields are also optional and usually omitted. Fields can carry optional\nfield prefix and description. By default, fields are assumed to hold free-form strings; we are actively exploring\noptional data type as a way to specify constraints on valid values (e.g., bool or int) and more gracefully handle\nformatting and parsing logic, though this feature is not core to DSPy at the time of writing.\n5These modules generalize prompting techniques from the literature, respectively, by Wei et al. (2022),\nChen et al. (2022), Yoran et al. (2023), and Yao et al. (2022) and, in doing so, generalize the ideas on zero-shot\nprompting and rationale self-generation from Kojima et al. (2022), Zelikman et al. (2022), Zhang et al. (2022),\nand Huang et al. (2022) to parameterized modules that can bootstrap arbitrary multi-stage pipelines.\n4\nPreprint\ning Predict to ChainOfThought in the above program leads to a system that thinks step by step\nbefore committing to its output field.\nImportantly, all of these modules are implemented in a few lines of code by expanding the user-\ndefined signature and calling Predict one or more times on new signatures as appropriate. For\ninstance, we show a simplified implementation of the built-in ChainOfThought below.\n1 class ChainOfThought(dspy.Module):\n2\ndef __init__(self , signature):\n3\n# Modify signature from \u2018*inputs -> *outputs \u2018 to \u2018*inputs -> rationale , *outputs \u2018.\n4\nrationale_field = dspy.OutputField(prefix=\"Reasoning: Let\u2019s think step by step.\")\n5\nsignature = dspy.Signature(signature).prepend_output_field(rationale_field)\n6\n7\n# Declare a sub -module with the modified signature.\n8\nself.predict = dspy.Predict(signature)\n9\n10\ndef forward(self , ** kwargs):\n11\n# Just forward the inputs to the sub -module.\n12\nreturn self.predict (** kwargs)\nThis is a fully-fledged module capable of learning effective few-shot prompting for any LM or task.\nWe contrast that with Appendix C, which copies long reasoning prompts hand-written by sources\nranging from recent research to popular prompting libraries.\nParameterization Uniquely, DSPy parameterizes these prompting techniques. To understand this\nparameterization, observe that any LM call seeking to implement a particular signature needs to\nspecify parameters that include: (1) the specific LM to call (Chen et al., 2023), (2) the prompt in-\nstructions (Yang et al., 2023) and the string prefix of each signature field and, most importantly, (3)\nthe demonstrations used as few-shot prompts (for frozen LMs) or as training data (for finetuning).\nWe focus primarily on automatically generating and selecting useful demonstrations. In our case\nstudies, we find that bootstrapping good demonstrations gives us a powerful way to teach sophisti-\ncated pipelines of LMs new behaviors systematically.\nTools DSPy programs may use tools, which are modules that execute computation. We support re-\ntrieval models through a dspy.Retrieve module. At the time of writing, DSPy has built-in support\nfor ColBERTv2, Pyserini, and Pinecone retrievers, and we have explored experimental dspy.SQL\nfor executing SQL queries and dspy.PythonInterpreter for executing Python code in a sandbox.\nPrograms DSPy modules can be composed in arbitrary pipelines in a define-by-run interface. In-\nspired directly by PyTorch and Chainer, one first declares the modules needed at initialization, allow-\ning DSPy to keep track of them for optimization, and then one expresses the pipeline with arbitrary\ncode that calls the modules in a forward method. As a simple illustration, we offer the following\nsimple but complete retrieval-augmented generation (RAG) system.\n1 class RAG(dspy.Module):\n2\ndef __init__(self , num_passages =3):\n3\n# \u2018Retrieve \u2018 will use the user\u2019s default retrieval settings unless overriden.\n4\nself.retrieve = dspy.Retrieve(k=num_passages)\n5\n# \u2018ChainOfThought \u2018 with signature that generates answers given retrieval & question.\n6\nself.generate_answer = dspy.ChainOfThought(\"context , question -> answer\")\n7\n8\ndef forward(self , question):\n9\ncontext = self.retrieve(question).passages\n10\nreturn self.generate_answer(context=context , question=question)\nTo highlight modularity, we use ChainOfThought as a drop-in replacement of the basic Predict.\nOne can now simply write RAG()(\"Where is Guaran\u00b4\u0131 spoken?\") to use it. Notice that, if we\nuse a signature \"context, question -> search query\", we get a system that generates search\nqueries rather than answers.\n3.3\nTELEPROMPTERS CAN AUTOMATE PROMPTING FOR ARBITRARY PIPELINES\nWhen compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that\ntakes the program, a training set, and a metric\u2014and returns a new optimized program. Different\nteleprompters (Sec 4) apply different strategies for optimization.\n5\nPreprint\nIn DSPy, training sets may be small, potentially a handful of examples, though larger data enables\nmore powerful optimization. Training examples may be incomplete, i.e., only input values are nec-\nessary. Labels for the pipeline steps are not required, unless they need to be used in the metric. In\npractice, we typically assume labels only for (at most) the program\u2019s final output, not the intermedi-\nate steps. This label-efficiency is critical for modularity: building a new pipeline in DSPy requires\nsimply recompiling the new pipeline\u2019s code, not annotating data specific to the new pipeline.\nMetrics can be simple notions like exact match (EM) or F1, but they can be entire DSPy programs\nthat balance multiple concerns. For example, we may compile the RAG module above against a\ndataset of question\u2013answer pairs qa trainset and the metric EM. The goal of optimization here is\nto effectively bootstrap few-shot demonstrations. The following code achieves this:\n1 # Small training set with only questions and final answers.\n2 qa_trainset = [dspy.Example(question=\"What is the capital of France?\", answer=\"Paris\")]\n3\n4 # The teleprompter will bootstrap missing labels: reasoning chains and retrieval contexts.\n5 teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)\n6 compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)\nIn this example, the BootstrapFewShot teleprompter (Sec 4, Appendix E.1) simulates RAG on the\ntraining example(s). It will collect demonstrations of each module (i.e., examples of its input\u2013output\nbehavior) that collectively lead to valid output (i.e., respecting the signatures and the metric).\nIf one wanted to push the compiled program to be extractive given its retrieved contexts, one could\ndefine a custom metric to use in place of dspy.evaluate.answer exact match:\n1 def answer_and_context_match(example , pred , trace=None):\n2\nanswer_match = dspy.evaluate.answer_exact_match(example , pred)\n3\n4\n# Is the prediction a substring of some passage?\n5\ncontext_match = any((pred.answer.lower () in c) for c in pred.context)\n6\n7\nreturn answer_match and context_match\nNotice that behavior like this might be more accurately checked by another DSPy program that\nchecks for faithful grounding of answers. Such metrics are fully supported and encouraged in DSPy.\nTeleprompters can be composed by specifying a teacher program. DSPy will sample demonstra-\ntions from this program for prompt optimization. This composition can enable very rich pipelines,\nwhere expensive programs (e.g., complex expensive ensembles using large LMs) supervise cheap\nprograms (e.g., simple pipelines using smaller LMs). One may start with compiled rag from above\n(say, compiled to use a large Llama2-13b-chat LM) but now fine-tune Flan-T5-large to create an\nefficient program:\n1 # Larger set of questions with *no labels *. Labels for all steps will be bootstrapped.\n2 unlabeled_questions = [dspy.Example(question=\"What is the capital of Germany?\"), ...]\n3\n4 # As we assumes no answer , we use \u2018answer_passage_match \u2018 to filter ungrounded answers.\n5 finetuning_teleprompter = BootstrapFinetune(metric=dspy.evaluate.answer_passage_match)\n6\n7 # We set \u2018teacher=compiled_rag \u2018 to compose. Bootstrapping will now use \u2018compiled_rag \u2018.\n8 compiled_rag_via_finetune = finetuning_teleprompter.compile(RAG(), teacher=compiled_rag ,\ntrainset=unlabeled_questions , target=\u2019google/flan -t5 -large \u2019)\n4\nTHE DSPY COMPILER\nA key source of DSPy\u2019s expressive power is its ability to compile\u2014or automatically optimize\u2014any\nprogram in this programming model. Compiling relies on a teleprompter, which is an optimizer for\nDSPy programs that improves the quality (or cost) of modules via prompting or finetuning, which\nare unified in DSPy. While DSPy does not enforce this when creating new teleprompters, typical\nteleprompters go through three stages.\nStage 1: Candidate Generation The compiler first (recursively) finds all unique Predict modules\n(predictors) in a program, including those nested under other modules. For each unique predictor\np, the teleprompter may generate candidate values for the parameters of p: the instructions, field\ndescriptions, or\u2014most importantly\u2014demonstrations (i.e., example input\u2013output pairs). In this iter-\n6\nPreprint\nation of DSPy, we focus on demonstrations and find that simple rejection-sampling-like approaches\ncan help bootstrap highly effective multi-stage systems.\nConsider the simplest non-trivial teleprompter in DSPy, BootstrapFewShot (simplified pseudocode\nin Appendix E.1). This teleprompter will simulate a teacher program (or, if unset, the zero-shot\nversion of the program being compiled) on some training inputs, possibly one or more times with\na high temperature. When running in compile mode, multi-stage traces are tracked transparently\nand in a thread-safe fashion throughout execution. The program\u2019s metric is used to filter for multi-\nstage traces that together help the pipeline pass the metric. We thus obtain potential labels for all\nsignatures in the program by throwing away the bad examples and using the good examples as\npotential demonstrations, though these design decisions are under user control.\nWhile LMs can be highly unreliable, we find they can be rather efficient at searching the space\nof solutions for multi-stage designs. A well-decomposed program can typically find at least a few\ntraining examples where the LM can pass the constraints enforced by the signatures and metrics,\nallowing us to bootstrap iteratively if needed.\nStage 2: Parameter Optimization Now each parameter has a discrete set of candidates: demon-\nstrations, instructions, etc. Many hyperparameter tuning algorithms (e.g., random search or Tree-\nstructured Parzen Estimators as in HyperOpt (Bergstra et al., 2013) and Optuna (Akiba et al., 2019))\ncan be applied for selection among candidates. We report simplified implementations of DSPy\u2019s\nBootstrapFewShotWithRandomSearch and BootstrapFewShotWithOptuna in Appendix E.2 and\nAppendix E.3.\nAnother type of optimization is finetuning with BootstrapFinetune, where the demonstrations are\nused to update the LM\u2019s weights for each predictor. When this is applied, the LM parameter of each\nmodule is updated to the new LM weights. Typically, we are optimizing average quality using the\nmetric with cross-validation over the training set or a validation set. This is applicable even with no\nlabels for any stages, depending on the nature of metric.\nStage 3: Higher-Order Program Optimization A different type of optimization that the DSPy\ncompiler supports is modifying the control flow of the program. One of the simplest forms of\nthese is ensembles, which we use in the case studies in this work. An ensemble will bootstrap\nmultiple copies of the same program, and then replace the program with a new one that runs them\nall in parallel and reduces their predictions into one with a custom function (e.g., majority voting).\nIn future work, this stage can easily accommodate techniques for more dynamic (i.e., test-time)\nbootstrapping as well as automatic backtracking-like logic.\n5\nGOALS OF EVALUATION\nProgramming frameworks can be evaluated along many dimensions: computational efficiency, de-\nveloper efficiency, intuitiveness of the code and concepts, and so forth. In this paper, we focus on\nperhaps the most pressing issue for current LM pipelines: the role of hand-written, task-specific\nprompts in achieving performant systems. Our evaluations seek to test the following hypotheses:\nH1 With DSPy, we can replace hand-crafted prompt strings with concise and well-defined\nmodules, without reducing quality or expressive power.\nH2 Parameterizing the modules and treating prompting as an optimization problem makes\nDSPy better at adapting to different LMs, and it may outperform expert-written prompts.\nH3 The resulting modularity makes it possible to more thoroughly explore complex pipelines\nthat have useful performance characteristics or that fit nuanced metrics.\nOur evaluation will explore these hypotheses using diverse task\u2013program pairs. We hope this begins\na shift from underspecified questions like \u201chow do different LMs compare on GSM8K\u201d toward \u201chow\nthey compare on GSM8K with program P when compiled with strategy S\u201d, which is a well-defined\nand reproducible run. Ultimately, our goal is to reduce the role of artful prompt construction in\nmodern AI in favor of the development of new modular, composable programs and optimizers.\n7\nPreprint\nTable 1: Results with in-context learning on GSM8K math word problems. Each row represents\na separate pipeline: the module in the Program column is compiled against the examples in the\nTraining set. The programs, compilers, and (small) training sets are defined in Section 6. Rows with\nensemble build on the immediately preceding row. Notably, all programs in this table are expressed\nby composing two to four DSPy modules and teleprompters. Compiling the correct modules, instead\nof string prompts, improves different LMs from 4\u201320% accuracy to 49\u201388% accuracy.\nGPT-3.5\nLlama2-13b-chat\nProgram\nCompilation\nTraining\nDev\nTest\nDev\nTest\nvanilla\nnone\nn/a\n24.0\n25.2\n7.0\n9.4\nfewshot\ntrainset\n33.1\n\u2013\n4.3\n\u2013\nbootstrap\ntrainset\n44.0\n\u2013\n28.0\n\u2013\nbootstrap\u00d72\ntrainset\n64.7\n61.7\n37.3\n36.5\n+ensemble\ntrainset\n62.7\n61.9\n39.0\n34.6\nCoT\nnone\nn/a\n50.0\n\u2013\n26.7\n\u2013\nfewshot\ntrainset\n63.0\n\u2013\n27.3\n\u2013\nfewshot\n+human CoT\n78.6\n72.4\n34.3\n33.7\nbootstrap\ntrainset\n80.3\n72.9\n43.3\n\u2013\n+ensemble\ntrainset\n88.3\n81.6\n43.7\n\u2013\nreflection\nnone\nn/a\n65.0\n\u2013\n36.7\n\u2013\nfewshot\ntrainset\n71.7\n\u2013\n36.3\n\u2013\nbootstrap\ntrainset\n83.0\n76.0\n44.3\n40.2\n+ensemble\ntrainset\n86.7\n\u2013\n49.0\n46.9\n6\nCASE STUDY: MATH WORD PROBLEMS\nWe evaluate on the popular GSM8K dataset with grade school math questions (Cobbe et al., 2021).\nWe sample 200 and 300 question\u2013answer pairs from the official training set for training and develop-\nment, respectively. Our final evaluations use the 1.3k official test set examples. We report extensive\ncomparisons on the development set to avoid overfitting on test. Following prior work on GSM8K,\nwe evaluate the accuracy of the final numerical value that appears in the LM output.\nPrograms Considered For this task, we consider three simple DSPy programs: a one-step Pre-\ndict module (vanilla), a two-step ChainOfThought module (CoT), and finally a multi-stage Com-\nparerOfThoughts module (ThoughtReflection). These are fully defined by the following code:\n1 vanilla = dspy.Predict(\"question -> answer\")\n# GSM8K Program \u2018vanilla \u2018\n2\n3 CoT = dspy.ChainOfThought(\"question -> answer\")\n# GSM8K Program \u2018CoT \u2018\n1 class ThoughtReflection(dspy.Module):\n2\ndef __init__(self , num_attempts):\n3\nself.predict = dspy.ChainOfThought(\"question -> answer\", n=num_attempts)\n4\nself.compare = dspy.MultiChainComparison(\u2019question -> answer \u2019, M=num_attempts)\n5\n6\ndef forward(self , question):\n7\ncompletions = self.predict(question=question).completions\n8\nreturn self.compare(question=question , completions=completions)\n9\n10 reflection = ThoughtReflection(num_attempts =5) # GSM8K Program \u2018reflection \u2018\nIn reflection, five reasoning chains are sampled from the LM (alongside their answers) and they\nare compared in parallel by a built-in MultiChainComparison module, which generalizes Yoran\net al. (2023). This generates a new answer taking into account the patterns from the five attempts.\nCritically, the modules used are all generic, none is specific math problems or particular LM.\nCompiling As we discussed in Section 4, DSPy programs can be compiled into new, optimized\nprograms. In our experiments, we evaluate the programs zero-shot (no compiling) as well as a\nnumber of strategies for compiling. Our simplest compiler is LabeledFewShot:\n1 fewshot = dspy.LabeledFewShot(k=8).compile(program , trainset=trainset)\nHere, program can be any DSPy module. This simply samples k=8 random demonstrations from the\ntrainset for the fields common to the training examples and the signature(s), in this case, question\nand answer, but not the reasoning for instance. We report the average of 3\u20135 runs (depending on the\nsetting) when applying such random sampling.\n8\nPreprint\nNext, we also consider bootstrapping few-shot examples with random search:\n1 tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_accuracy)\n2 bootstrap = tp.compile(program , trainset=trainset , valset=devset)\nThis will generate demonstration chains for examples in the training set and optimize the selection\nof demonstrations (from this set) to self-improve the program\u2019s modules. As the name indicates, this\nis done with random search, treating the selection of demonstrations as a parameter to optimize.\nNext, if desired, this bootstrapping process can be nested in DSPy. In particular, we can use the\noptimized bootstrap program itself to further bootstrap another program. This is relevant, for\nexample, whenever the original zero-shot program performs relatively poorly.\n1 bootstrap2 = tp.compile(program , teacher=bootstrap , trainset=trainset , valset=devset)\nAnd lastly, we consider ensembling these bootstraps:\n1 # A program that ensembles the top -7 candidate programs from a bootstrapping compiler run\n(in particular \u2018bootstrap \u2018 or , when applicable , \u2018bootstrap2 \u2018) with majority voting.\n2 ensemble = Ensemble(reduce_fn=dspy.majority).compile(bootstrap.programs [:7])\nGSM8K includes human reasoning chains. Above, trainset does not include these reasoning\nchains. We also evaluate with trainset human CoT, which extends the examples in trainset with\nthe human reasoning string. These two datasets can be used interchangeably as the value for the\ntrainset parameter above. We note here that compiling generally runs on the order of minutes\n(or tens of minutes) as even the more expensive settings only require running the program a few\nthousand times (e.g., 10\u201320 trials over 150\u2013300 validation examples) and they can occur in parallel.\nResults Our results are summarized in Table 1, which includes dev results as well as our evaluation\nof promising representatives of each approach on the test set. First, the vanilla program results\nshow that GPT-3.5 and llama2-13b-chat struggle with math word problems when they have to\npredict the answers directly, that is, without using a reasoning chain first. This is most pronounced\nin the absence of good demonstrations, which can be seen in the none compilation setting (i.e.,\nzero-shot instruction) and the fewshot setting (i.e., sampling random question\u2013answer pairs). In-\nterestingly, however, vanilla is helped substantially by compiling with bootstrap and by iterating\nthis process into bootstrap\u00d72. On inspecting the prompts bootstrapped (Appendix F), we see that\nthe prompt allows the LM to leverage the answer field for reasoning first, which is permitted as the\nmetric extracts the final numerical value for evaluation.\nNext, we consider the CoT program. While the expert human reasoning chains (+human CoT) provide\na large boost when available, we can match or surpass this using bootstrap, substantiating our\nhypothesis that DSPy can cut the need for hand-crafted prompts. Beyond this, we see that the\nreflection program, while only a few lines longer than the others, is a clear winner, though CoT is\nquite effective with ensemble. Overall, the bootstrap compilation procedure leads to large gains\nfor every program, across both LMs. Indeed, all programs in this table are expressed by composing\ntwo to four DSPy modules and teleprompters, and they reveal overall that\u2014in the new paradigm\nprescribed by DSPy\u2014it\u2019s composing the right generic modules, rather than manipulating string\nprompts, that improves different LMs from 4\u201320% accuracy to 49\u201388% accuracy.\nWe can informally compare with the following.\nZhang et al. (2022) reports 48% for\ntext-davinci-002, which aligns closely with our llama2-13b-chat results, and reports 59.4%\nwith codex when employing a manual CoT approach and 62.8% with an automatic CoT method.\nWang et al. (2022b) report 57% for CoT prompting with PaLM 540-B, which becomes 74% upon\nadding self-consistency. The Llama2 authors (Touvron et al., 2023) presents 28.7% for llama2-13b,\n42.2% for llama2-34b, and 56.8% for llama2-70b. Intriguingly, our program with the 13b variant\nof the model is competitive with their 34b-based results even though we don\u2019t use human reasoning\nchains in our program. Zhao et al. (2023b) reports 80.8% for CoT with gpt-3.5-turbo from April\n2023. The GPT-4 authors (OpenAI, 2023) reports that GPT-3.5 scores 57.1% and GPT-4 elevates\nthis to 92% but they note that GPT-4 was in fact pre-trained on a subset of GSM8K\u2019s training set.\n9\nPreprint\n7\nCASE STUDY: COMPLEX QUESTION ANSWERING\nIn this case study, we explore the multi-hop question answering task with the HotPotQA (Yang et al.,\n2018) dataset in the open-domain \u201cfullwiki\u201d setting. For retrieval, we use a search index of the of-\nficial Wikipedia 2017 \u201cabstracts\u201d dump of HotPotQA. Search is conducted by a ColBERTv2 (San-\nthanam et al., 2021) retriever. The HotPotQA test set is hidden, so we reserve the official validation\nset for our testing, and sample 1000 examples for that. We sub-divide the training set into 70%/30%\ntrain/validation splits. In the training (and thus validation) split, we keep only examples marked as\n\u201chard\u201d in the original dataset, which matches the designation of the official validation and test sets.\nFor training and for reporting development results, we sample 200 and 300 examples respectively.\nPrograms Considered Our simplest baseline is the vanilla program used in the previous case\nstudy on GSM8K (Sec 6); the \"question -> answer\" signature is universal enough that it will\nwork for this task (and many others) when compiled appropriately.\nOur baseline RAG program is the one given in Section 3.2 as a simple example of RAG with a\ndspy.ChainOfThought layer. We will see that this program does not excel at HotPotQA, and this\nmotivates us to evaluate two multi-hop programs.\nTo that end, we first test ReAct (Yao et al., 2022), a multi-step agent for tool use, which is imple-\nmented as a built-in module in DSPy. In the simplest case, a ReAct module for a particular signature\ncan be declared as follows in DSPy:\n1 react = dspy.ReAct(\"question -> answer\", tools =[dspy.Retrieve(k=1)], max_iters =5)\nWe also test the following custom program, which simulates the information flow in Baleen (Khattab\net al., 2021a) and IRRR (Qi et al., 2020) and has similarities to IRCoT (Trivedi et al., 2022).\n1 class BasicMultiHop(dspy.Module):\n2\ndef __init__(self , passages_per_hop):\n3\nself.retrieve = dspy.Retrieve(k=passages_per_hop)\n4\nself.generate_query = dspy.ChainOfThought(\"context , question -> search_query\")\n5\nself.generate_answer = dspy.ChainOfThought(\"context , question -> answer\")\n6\n7\ndef forward(self , question):\n8\ncontext = []\n9\n10\nfor hop in range (2):\n11\nquery = self.generate_query(context=context , question=question).search_query\n12\ncontext += self.retrieve(query).passages\n13\n14\nreturn self.generate_answer(context=context , question=question)\n15\n16 multihop = BasicMultiHop(passages_per_hop =3)\nCompiling For compilers, we continue to use the ones that we used for GSM8K (see Sec 6). We\nalso consider two compositions of our teleprompters. For ReAct, we consider bootstrapping with\nBootstrapFewShotWithRandomSearch starting from an earlier bootstrap of the ReAct program.\nFor the simple multihop program, we also consider fine-tuning with T5-Large starting from the\nearlier bootstrap of that program.\n1 multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program ,\nteacher=bootstrap , trainset=trainset , target=\u2019t5 -large \u2019)\nResults Table 2 summarizes our results. Compared with the vanilla few-shot prompting, a chain-\nof-thought and retrieval-augmented generation (CoT RAG) program can self-bootstrap in DSPy to\nincrease answer EM substantially. However, this relies entirely on the ColBERTv2 retriever to find\nrelevant passages directly from the original questions, limiting its passage recall. This is tackled in\nthe react and multihop programs, which will generate queries for the retriever in multiple iterative\n\u201chops\u201d. Indeed, overall, a simple multihop program performs the best, and in general bootstrap\nagain proves to be very effective at raising its quality relative to its fewshot variant for both LMs.\nIn particular, we can see that bootstrap (and/or bootstrap\u00d72) can outperform both fewshot\nprompting (for multihop) and expert human reasoning (for react; adapted slightly from Yao et al.\n(2022) to our retrieval setting). Perhaps most importantly, we can make llama2-13b-chat compet-\nitive with GPT-3.5 by simply compiling our programs.\nTo assess the finetuning capacity of DSPy, we also evaluated the compiler multihop t5 defined\nabove which produces a T5-Large (770M parameter) model. This program scores 39.3% answer\nEM and 46.0% passage accuracy on the dev set, using only 200 labeled inputs and 800 unlabeled\n10\nPreprint\nTable 2: Results with in-context learning on HotPotQA multi-hop retrieval question answering. We\nreport answer exact match (Ans) and pair-retrieval accuracy (Psg). Each row represents a separate\npipeline: the module in the Program column is compiled against the examples in the Training set.\nThe programs, compilers, and (small) training sets are defined in the main text. For HotPotQA, we\nuse the training set (and not dev) directly for cross-validation. \u2217The marked result is evaluated on\n50% of our test set due to cost.\nGPT-3.5\nLlama2-13b-chat\nProgram\nCompiler\nDev\nTest\nDev\nTest\nAns\nPsg\nAns\nPsg\nAns\nPsg\nAns\nPsg\nvanilla\nfewshot\n34.3\nn/a\n31.5\nn/a\n27.5\nn/a\n21.8\nn/a\nCoT RAG\nfewshot\n36.4\n36.0\n29.8\n34.4\n34.5\n36.0\n28.0\n34.4\nbootstrap\n42.3\n36.0\n\u2013\n\u2013\n38.3\n36.0\n32.9\n34.4\nreact\nnone\n20.3\n\u2013\n\u2013\n\u2013\n20.0\n\u2013\n\u2013\n\u2013\n+human r\n33.0\n\u2013\n\u2013\n\u2013\n28.3\n\u2013\n\u2013\n\u2013\nbootstrap\n31.0\n\u2013\n\u2013\n\u2013\n24.7\n\u2013\n\u2013\n\u2013\nbootstrap\u00d72\n39.0\n\u2013\n\u2013\n\u2013\n40.0\n\u2013\n\u2013\n\u2013\nmultihop\nfewshot\n36.9\n38.3\n31.2\n40.8\n34.7\n32.0\n31.3\n30.8\nbootstrap\n48.7\n47.0\n39.6\n43.8\n42.0\n48.3\n36.4\n43.5\nensemble\n54.7\n\u2013\n45.6\u2217\n\u2013\n50.0\n\u2013\n41.0\n\u2013\nquestions. For compiling, we use a teacher program consisting of an ensemble (union) of two\nmultihop with llama2-13b-chat. Considering its extremely small size and local availability, this\ncompiled program with T5-Large would impose orders of magnitude lower costs for inference than\na proprietary LM like GPT-3.5.\nOur results may be pegged against the evaluation on HotPotQA in a number of recent papers, though\nthere is significant variation in evaluation methodology and test set samples across studies in this\nspace. Using CoT prompting, Si et al. (2022) achieve 25.2% EM. With a \u201crecite-and-answer\u201d tech-\nnique that uses PaLM-62B (Chowdhery et al., 2022) to recite evidence passages, Sun et al. (2022)\nachieve 26.5% EM. Wang et al. (2022a) achieve 33.8% EM and 44.6% F1 when applying self-\nconsistency for PaLM-540B. Yao et al. (2022) achieve 27.4% EM using ReAct with PaLM-540B\nand 30.8 with text-davinci-002, with a tool giving it the ability for search using a Wikipedia\nAPI. They push their PaLM results to 35.1% EM by applying an additional CoT step with self-\nconsistency, which may resemble our ensemble approach in the sense of aggregating multiple an-\nswers. Trivedi et al. (2022) reports 49% using a pipeline with code-davinci-002 LM on a sample\nof 500 HotPotQA questions.\n8\nCONCLUSION\nThis paper introduced DSPy, a new programming model for designing AI systems using pipelines\nof pretrained LMs and other tools. We presented three new concepts introduced in this abstraction\n(DSPy signatures, modules, and teleprompters), and showed in two very different case studies that\nit supports rapid development of highly effective systems that use relatively small LMs. We have\nmaintained open-source versions of this framework for close to a year. In this period, we have seen\nand created a large number of programs that were compiled to high-quality systems by DSPy, span-\nning tasks from information extraction to low-resource synthetic data generation. In the interest of\nspace and to maintain reasonable scope in this paper, we leave reporting on such tasks under con-\ntrolled experimental conditions to future work. While in-context learning has proved transformative\nover the past 2\u20133 years of LM research, we argue that the true expressive power in this emerging\nparadigm is in building sophisticated text transformation graphs in which composable modules and\noptimizers (teleprompters) come together to leverage LMs in more systematic and reliable ways.\nACKNOWLEDGMENTS\nWe thank Josh Purtell for suggesting the apt name \u201ctext transformation graph\u201d for the computational\ngraph abstraction of DSPy. We thank Rick Battle, Igor Kotenkov, Lisa Li, David Hall, Ashwin\nParanjape, Chris Manning, Percy Liang, and many researchers, developers, and users for valuable\n11\nPreprint\ndiscussions and feedback. We thank Giuseppe Attanasio for his public LATEX GitHub-style Python\ncode formatting gist.6\nThis work was partially supported by IBM as a founding member of the Stanford Institute for\nHuman-Centered Artificial Intelligence (HAI), Oracle, Virtusa, and Cigna Healthcare. It was also\npartially supported by an HAI Azure compute grant. This research was supported in part by affiliate\nmembers and other supporters of the Stanford DAWN project\u2013Facebook, Google, and VMware\u2014as\nwell as the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or\nrecommendations expressed in this material are those of the authors and do not necessarily reflect\nthe views of the National Science Foundation. Omar Khattab is supported by the Apple Scholars in\nAI/ML fellowship.\n\\usepackage[pdftex]{graphicx} ...\n\\includegraphics[width=0.8\\linewidth]{myfile.pdf}\nREFERENCES\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:\nA next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM\nSIGKDD international conference on knowledge discovery & data mining, pp. 2623\u20132631, 2019.\nRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,\nNicolas Ballas, Fr\u00b4ed\u00b4eric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al.\nTheano: A Python framework for fast computation of mathematical expressions. arXiv e-prints,\npp. arXiv\u20131605, 2016.\nJames Bergstra, Olivier Breuleux, Fr\u00b4ed\u00b4eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume\nDesjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A CPU and GPU\nmath compiler in Python. In Proc. 9th python in science conf, volume 1, pp. 3\u201310, 2010.\nJames Bergstra, Fr\u00b4ed\u00b4eric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan Pascanu, Olivier De-\nlalleau, Guillaume Desjardins, David Warde-Farley, Ian Goodfellow, Arnaud Bergeron, et al.\nTheano: Deep learning on gpus with Python. In NIPS 2011, BigLearning Workshop, Granada,\nSpain, volume 3. Citeseer, 2011.\nJames Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter\noptimization in hundreds of dimensions for vision architectures. In International conference on\nmachine learning, pp. 115\u2013123. PMLR, 2013.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHarrison Chase.\nHwchase17/langchain.\n2022.\nURL https://github.com/hwchase17/\nlangchain.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.\nReading Wikipedia to answer\nopen-domain questions.\nIn Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1870\u20131879, Vancouver, Canada, July\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:\n//aclanthology.org/P17-1171.\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while\nreducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023.\n6https://gist.github.com/g8a9/07c2be12ae02cfad4aa430d77dc940cb\n12\nPreprint\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nRonan Collobert, Samy Bengio, and Johnny Mari\u00b4ethoz. Torch: a modular machine learning software\nlibrary. Technical report, Idiap, 2002.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language model\ncascades. arXiv preprint arXiv:2207.10342, 2022.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan,\nVincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what\nlanguage models say, using language models. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 16477\u201316508, 2023a.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764\u201310799. PMLR, 2023b.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,\nand Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\nprompt optimizers. arXiv preprint arXiv:2309.08532, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. URL https:\n//arxiv.org/abs/2002.08909.\nBraden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher\nR\u00b4e. Training classifiers with natural language explanations. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1884\u2013\n1895. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/\nP18-1175.\nBin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. En-\nabling intelligent interactions between an agent and an LLM: A reinforcement learning approach.\narXiv preprint arXiv:2306.03604, 2023. URL https://arxiv.org/abs/2306.03604.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\nHan. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.\nEhud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham,\nHofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl systems: A modular, neuro-symbolic\narchitecture that combines large language models, external knowledge sources and discrete rea-\nsoning. arXiv preprint arXiv:2205.00445, 2022.\nOmar Khattab, Christopher Potts, and Matei Zaharia. Baleen: Robust Multi-Hop Reasoning at Scale\nvia Condensed Retrieval. In Thirty-Fifth Conference on Neural Information Processing Systems,\n2021a.\nOmar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with\nColBERT. Transactions of the Association for Computational Linguistics, 9:929\u2013944, 2021b.\n13\nPreprint\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\nknowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\nSabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv\npreprint arXiv:2210.02406, 2022.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev.\nInternet-\naugmented language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115, 2022.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\nman Goyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, Sebastian\nRiedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp\ntasks.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-\nvances in Neural Information Processing Systems, volume 33, pp. 9459\u20139474. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.pdf.\nJerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama index.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv:1806.08730, 2018. URL https:\n//arxiv.org/abs/1806.08730.\nMicrosoft. Semantic kernel. 2023. URL https://learn.microsoft.com/semantic-kernel/.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schul-\nman. WebGPT: Browser-assisted question-answering with human feedback, 2021. URL https:\n//arxiv.org/abs/2112.09332.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran\nAssociates, Inc., 2019. URL https://proceedings.neurips.cc/paper files/paper/2019/\nfile/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.\nMohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-\nsql with self-correction. arXiv preprint arXiv:2304.11015, 2023.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350,\n2022.\n14\nPreprint\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\u201d gradient descent\u201d and beam search. arXiv preprint arXiv:2305.03495, 2023.\nPeng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. Answering complex\nopen-domain questions through iterative query generation.\nIn Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 2590\u20132602, Hong Kong,\nChina, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1261. URL\nhttps://aclanthology.org/D19-1261.\nPeng Qi, Haejun Lee, Oghenetegiri Sido, Christopher D Manning, et al. Retrieve, rerank, read,\nthen iterate: Answering open-domain questions of arbitrary complexity from text. arXiv preprint\narXiv:2010.12527, 2020. URL https://arxiv.org/abs/2010.12527.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. Ms, OpenAI, 2018. URL https://openai.com/blog/\nlanguage-unsupervised/.\nAlexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00b4e. Data\nprogramming:\nCreating large training sets, quickly.\nIn D. D. Lee, M. Sugiyama, U. V.\nLuxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems\n29, pp. 3567\u20133575. Curran Associates, Inc., 2016.\nURL https://papers.nips.cc/paper/\n6523-data-programming-creating-large-training-sets-quickly.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Col-\nBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.\narXiv preprint\narXiv:2112.01488, 2021.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Syn-\nthetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv\npreprint arXiv:2302.00618, 2023.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and\nLijuan Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. arXiv preprint arXiv:2210.01296, 2022.\nSeiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source\nframework for deep learning. In Proceedings of workshop on machine learning systems (Learn-\ningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS),\nvolume 5, pp. 1\u20136, 2015.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving re-\ntrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.\narXiv\npreprint arXiv:2212.10509, 2022.\nFei Wang, James Decker, Xilun Wu, Gregory Essertel, and Tiark Rompf.\nBackpropaga-\ntion with callbacks:\nFoundations for efficient and expressive differentiable programming.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-\nnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-\nciates, Inc., 2018. URL https://proceedings.neurips.cc/paper files/paper/2018/file/\n34e157766f31db3d2099831d348a7933-Paper.pdf.\n15\nPreprint\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\nRationale-\naugmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency\nimproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171,\n2022b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander Rush.\nTransformers: State-of-the-art\nnatural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, 2020. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-demos.6.\nURL https:\n//aclanthology.org/2020.emnlp-demos.6.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\nChen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. arXiv preprint arXiv:1809.09600, 2018.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering\nquestions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007,\n2023.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning.\narXiv preprint arXiv:2203.14465, 2022.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in\nlarge language models. arXiv preprint arXiv:2210.03493, 2022.\nAndrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. ExpeL:\nLLM agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023a. URL https:\n//arxiv.org/pdf/2308.10144.\nXu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with\nlarge language models for reasoning. arXiv preprint arXiv:2305.14333, 2023b.\n16\nPreprint\nA\nADVANCED SIGNATURES\nWhen more control is desired, one can express signatures as Python classes to provide explicit\ninstructions of the transformation and describe the format or role of each field more directly. For\ninstance, the following signature generates search queries using context and an optional question:\n1 class GenerateSearchQuery(dspy.Signature):\n2\n\"\"\" Write a simple search query that will help answer a complex question.\"\"\"\n3\n4\ncontext = dspy.InputField(desc=\"may contain relevant facts\")\n5\nquestion = dspy.InputField ()\n6\nquery = dspy.OutputField(dtype=dspy.SearchQuery)\nUsing the above, we can specify a complete system for the generation of a synthetic IR dataset where\nthe queries are mediated by a question generated by the LM:\n1 query_gen = dspy.Predict(GenerateSearchQuery)\n2 query_gen(context=\"Language typology\")\n3 # Out: Prediction(question=\u2019What are the main types of language classification?\u2019,\nquery=\u2019\"language classification\" OR \"language typology\" -wikipedia \u2019)\nIf questions are available, they can be supplied as shown:\nquery gen(context=\"Language\ntypology\", question=\"What are the primary language families of South America?\").\nAs a work in progress feature, users can optionally specify the type of output fields as bool, int,\nfloat, list, or dict instead of the default free-form string type, as in contexts, question ->\nanswer found: bool.\nB\nCOMPARISON WITH EXISTING LIBRARIES LIKE LANGCHAIN AND\nLLAMAINDEX\nLangChain and LlamaIndex are perhaps the most popular library in the general space of prompting\nLMs. These libraries have a different focus compared to DSPy and they suffer internally from the\nprompt engineering challenges that DSPy aims to resolve. In particular, whereas the goal of DSPy\nis to tackle the fundamental challenges of prompt engineering for building new LM computational\ngraphs, LangChain and LlamaIndex generally help application developers who need pre-packaged\ncomponents and chains, e.g., implementations of popular and reusable pipelines (e.g., particular\nagents and specific retrieval pipelines) and tools (e.g., connections to various databases and imple-\nmentations of long- and short-term memory for agents).\nThese off-the-shelf higher-level abstractions contrast with DSPy\u2019s focus on introducing core com-\nposable operators. In particular, DSPy introduces signatures (to abstract prompts), modules (to\nabstract prompting techniques), and teleprompters to act as optimizers for arbitrary imperative code\n(DSPy programs) that chain modules together. Its goal is to help researchers and practitioners\nbuild new LM pipelines quickly and achieve very high quality through automatic compilation (self-\nimprovement) instead of manual prompt engineering.\nIn contrast, typical existing research implementations and existing libraries like LangChain and\nLlamaIndex are implemented using manual prompt engineering, which is the key problem that DSPy\ntackles. We conducted an informal study to highlight this. In late September 2023, we found\nthat the LangChain codebase contains 50 strings exceeding 1000 characters, which are generally\nprompts, compared to none at all in DSPy. Indeed, a substantial number of LangChain\u2019s Python\nfiles are singularly dedicated to task-related templating and prompt engineering with 12 prompts.py\nfiles and and 42 prompt.py files. DSPy, on the other hand, provides a structured framework that\nautomatically bootstraps prompts. The library itself does not contain a single hand-written prompt\ndemonstration for any tasks at the time of writing, despite the very high quality with various LMs.\nTo review the typical forms of prompt engineering in existing libraries, we consider the follow-\ning in LangChain. The LangChain Program-Aided Language Model Gao et al. (2023a) chain pro-\ngram uses few-shot learning, leveraging a template that is 3982 characters long with 8 math word\nproblems (Prompt 2) and corresponding outputted programs as learning examples for the language\nmodel. LangChain also contains a prompt for SQL query tasks for each of the databases like Or-\nacle, GoogleSQL, DuckDB, Crate, and MySQL, with the average length of these prompts at 1058\ncharacters. Other task areas such as QA with sources (Prompt B) and Graph QA also have signif-\n17\nPreprint\nicantly lengthy prompt templates, with averages of 1337 and 722 characters, respectively. While\nexpert-written prompts can be useful, we believe that LM- and task-adaptive prompts bootstrapped\nautomatically can offer far more power (and are far more modular) than hard-coding a prompt per\ndatabase provider inside the code base. The next appendix section contains a number of prompts\ncopied from related research papers and existing libraries.\nC\nSAMPLE LARGE PROMPTS\nThis section highlights a few popular existing frameworks that structure prompts with extensive\nprompt engineering templates. The primary objective is to capture how many words and characters\nare used for such large multi-line prompts defined for tasks or tools and present these example\nprompts retrieved from open-sourced papers and repositories. The formatting of these example\nprompts is adapted from Gao et al. (2023a).\nTask/Tool Prompt\nSource\nWords\nCharacters\nPrompt 1: Text-evidence checker\nGao et al. (2023a)\n818\n4964\nPrompt 2: Math word problems (PAL)\nLangChain & Gao et al. (2023b)\n566\n3957\nPrompt 3: ReAct\nYao et al. (2022)\n593\n3889\nPrompt 4: Zero-shot ReAct\nLangChain\n101\n600\nPrompt 5: QA with sources\nLangChain\n992\n6197\nPrompt 6: SQL MyScale querying\nLangChain\n343\n2239\nPrompt 7: Relevant docs retrieval\nLlamaIndex\n129\n719\nPrompt 8: IRS chatbot\nLlamaIndex\n389\n2258\n18\nPreprint\n1\n[web] I will check some things you said.\n2\n3\n(1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This\nis to prevent a buildup of mucus. It\u2019s called the nasal cycle.\n4\nI checked: How often do your nostrils switch?\n5\nI found this article: Although we don\u2019t usually notice it, during the nasal cycle one nostril becomes congested and thus\ncontributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every\n2 hours, according to a small 2016 study published in the journal PLOS One.\n6\nYour nose\u2019s switching time is about every 2 hours, not 45 minutes.\n7\nThis disagrees with what you said.\n8\n9\n(2) You said: The Little House books were written by Laura Ingalls Wilder. The books were published by HarperCollins.\n10\nI checked: Who published the Little House books?\n11\nI found this article: These are the books that started it all -- the stories that captured the hearts and imaginations of\nchildren and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books\nremain a favorite to this day.\n12\nThe Little House books were published by HarperCollins.\n13\nThis agrees with what you said.\n14\n15\n(3) You said: The Stanford Prison Experiment was conducted in the basement of Jordan Hall, Stanford\u2019s psychology building.\n16\nI checked: Where was Stanford Prison Experiment conducted?\n17\nI found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set\nout to examine the psychological effects of authority and powerlessness in a prison environment.\n18\nThe Stanford Prison Experiment was conducted in Jordan Hall.\n19\nThis agrees with what you said.\n20\n21\n(4) You said: Social work is a profession that is based in the philosophical tradition of humanism. It is an intellectual\ndiscipline that has its roots in the 1800s.\n22\nI checked: When did social work have its roots?\n23\nI found this article: The Emergence and Growth of the Social work Profession<br><br> Social work\u2019s roots were planted in\nthe 1880s, when charity organization societies (COS) were created to organize municipal voluntary relief associations and\nsettlement houses were established.\n24\nSocial work has its roots in the 1880s, not 1800s.\n25\nThis disagrees with what you said.\n26\n27\n(5) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency\nlist. It is named after Vaclav Havel and Samih Hakimi.\n28\nI checked: What is the Havel-Hakimi algorithm?\n29\nI found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree\nsequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm.\nThe algorithm was published by Havel (1955), and later by Hakimi (1962).\n30\nHavel-Hakimi algorithm is for constructing a special solution if a simple graph for the given degree sequence exists, or\nproving that one cannot find a positive answer, not converting the adjacency matrix of a graph into its adjacency list.\n31\nThis disagrees with what you said.\n32\n33\n(6) You said: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing. The song was produced by Michael Lloyd.\n34\nI checked: Who was the producer of \"(I\u2019ve Had) The Time of My Life\"?\n35\nI found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd,\nwas released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at\nStanford University.\n36\n\"Time of My Life\" was produced by Michael Lloyd.\n37\nThis agrees with what you said.\n38\n39\n(7) You said: Kelvin Hopins was suspended from the Labor Party because he had allegedly sexually harassed and behaved\ninappropriately towards a Labour Party activist, Ava Etemadzadeh.\n40\nI checked: Why was Kelvin Hopins suspeneded from the Labor Party?\n41\nI found this article: A former Labour MP has left the party before an inquiry into sexual harassment allegations against\nhim was able to be concluded, the party has confirmed. Kelvin Hopkins was accused in 2017 of inappropriate physical contact\nand was suspended by the Labour party pending an investigation.This agrees with what you said.\n42\nKelvin Hopins was suspended because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party\nactivist, Ava Etemadzadeh.\n43\nThis agrees with what you said.\n44\n45\n(8) You said: In the battles of Lexington and Concord, the British side was led by General Thomas Smith.\n46\nI checked: Who led the British side in the battle of Lexington and Concord?\n47\nI found this article: Interesting Facts about the Battles of Lexington and Concord. The British were led by Lieutenant\nColonel Francis Smith. There were 700 British regulars.\n48\nThe British side was led by Lieutenant Colonel Francis Smith, not General Thomas Hall.\n49\nThis disagrees with what you said.\n50\n51\n(9) You said: {text}\n52\nI checked: {query}\n53\nI found this article: {evidence}\n54\nFigure 1: Example few-shot prompt using a reasoning chain for agreement model that identifies\ninconsistencies between text and evidence (Gao et al., 2023a).\n19\nPreprint\n1\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n2\n3\n# solution in Python:\n4\n5\n6\ndef solution():\n7\n\"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"\n8\nmoney initial = 23\n9\nbagels = 5\n10\nbagel cost = 3\n11\nmoney spent = bagels * bagel cost\n12\nmoney left = money initial - money spent\n13\nresult = money left\n14\nreturn result\n15\n16\n17\n18\n19\n20\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he\nhave at the end of wednesday?\n21\n22\n# solution in Python:\n23\n24\n25\ndef solution():\n26\n\"\"\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls\ndid he have at the end of wednesday?\"\"\"\n27\ngolf balls initial = 58\n28\ngolf balls lost tuesday = 23\n29\ngolf balls lost wednesday = 2\n30\ngolf balls left = golf balls initial - golf balls lost tuesday - golf balls lost wednesday\n31\nresult = golf balls left\n32\nreturn result\n33\n34\n35\n36\n37\n38\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday.\nHow many computers are now in the server room?\n39\n40\n# solution in Python:\n41\n42\n43\ndef solution():\n44\n\"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday.\nHow many computers are now in the server room?\"\"\"\n45\ncomputers initial = 9\n46\ncomputers per day = 5\n47\nnum days = 4\n48\ncomputers added = computers per day * num days\n49\ncomputers total = computers initial + computers added\n50\nresult = computers total\n51\nreturn result\n52\n53\n54\n55\n56\n57\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n58\n59\n# solution in Python:\n60\n61\n62\ndef solution():\n63\n\"\"\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\"\"\"\n64\ntoys initial = 5\n65\nmom toys = 2\n66\ndad toys = 2\n67\ntotal received = mom toys + dad toys\n68\ntotal toys = toys initial + total received\n69\nresult = total toys\n70\nreturn result\n71\n72\n73\n74\n75\n76\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to\nDenny?\n77\n78\n# solution in Python:\n79\n80\n81\n20\nPreprint\n1\n2\n3\n4\ndef solution():\n5\n\"\"\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give\nto Denny?\"\"\"\n6\njason lollipops initial = 20\n7\njason lollipops after = 12\n8\ndenny lollipops = jason lollipops initial - jason lollipops after\n9\nresult = denny lollipops\n10\nreturn result\n11\n12\n13\n14\n15\n16\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n17\n18\n# solution in Python:\n19\n20\ndef solution():\n21\n\"\"\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\"\"\n22\nleah chocolates = 32\n23\nsister chocolates = 42\n24\ntotal chocolates = leah chocolates + sister chocolates\n25\nchocolates eaten = 35\n26\nchocolates left = total chocolates - chocolates eaten\n27\nresult = chocolates left\n28\nreturn result\n29\n30\n31\n32\n33\n34\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n35\n36\n# solution in Python:\n37\n38\n39\ndef solution():\n40\n\"\"\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\"\"\"\n41\ncars initial = 3\n42\ncars arrived = 2\n43\ntotal cars = cars initial + cars arrived\n44\nresult = total cars\n45\nreturn result\n46\n47\n48\n49\n50\n51\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be\n21 trees. How many trees did the grove workers plant today?\n52\n53\n# solution in Python:\n54\n55\n56\ndef solution():\n57\n\"\"\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will\nbe 21 trees. How many trees did the grove workers plant today?\"\"\"\n58\ntrees initial = 15\n59\ntrees after = 21\n60\ntrees added = trees after - trees initial\n61\nresult = trees added\n62\nreturn result\n63\n64\n65\n66\n67\n68\nQ: {question}\n69\n70\n# solution in Python:\nFigure 2: PAL example few-shot prompt for solving math questions by generating code.\n21\nPreprint\n1\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current\nsituation, and Action can be three types:\n2\n(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\nwill return some similar entities to search.\n3\n(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n4\n(3) Finish[answer], which returns the answer and finishes the task.\n5\nHere are some examples.\n6\nQuestion: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n7\nAction 1: Search[Colorado orogeny]\n8\nObservation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n9\nAction 2: Lookup[eastern sector]\n10\nObservation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n11\nAction 3: Search[High Plains]\n12\nObservation 3: High Plains refers to one of two distinct land regions:\n13\nAction 4: Search[High Plains (United States)]\n14\nObservation 4: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation\nfrom around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n15\nAction 5: Finish[1,800 to 7,000 ft]\n16\nQuestion: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening\nnamed after who?\n17\nAction 1: Search[Milhouse]\n18\nObservation 1: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons\nvoiced by Pamela Hayden and created by Matt Groening.\n19\nAction 2: Lookup[named after]\n20\nObservation 2: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n21\nAction 3: Finish[Richard Nixon]\n22\nQuestion: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n23\nAction 1: Search[Adam Clayton Powell]\n24\nObservation 1: Could not find [Adam Clayton Powell]. Similar: [\u2019Adam Clayton Powell III\u2019, \u2019Seventh Avenue (Manhattan)\u2019,\n\u2019Adam Clayton Powell Jr. State Office Building\u2019, \u2019Isabel Washington Powell\u2019, \u2019Adam Powell\u2019, \u2019Adam Clayton Powell (film)\u2019,\n\u2019Giancarlo Esposito\u2019].\n25\nAction 2: Search[Adam Clayton Powell (film)]\n26\nObservation 2: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the\nrise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the\nPBS series The American Experience.\n27\nAction 3: Finish[The Saimaa Gesture]\n28\nQuestion: What profession does Nicholas Ray and Elia Kazan have in common?\n29\nAction 1: Search[Nicholas Ray]\n30\nObservation 1: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 { June 16, 1979) was an American film\ndirector, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n31\nAction 2: Search[Elia Kazan]\n32\nObservation 2: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n33\nAction 3: Finish[director, screenwriter, actor]\n34\nQuestion: Which magazine was started first Arthur\u2019s Magazine or First for Women?\n35\nAction 1: Search[Arthur\u2019s Magazine]\n36\nObservation 1: Arthur\u2019s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th\ncentury.\n37\nAction 2: Search[First for Women]\n38\nObservation 2: First for Women is a woman\u2019s magazine published by Bauer Media Group in the USA.[1] The magazine was started\nin 1989.\n39\nAction 3: Finish[Arthur\u2019s Magazine]\n40\nQuestion: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n41\nAction 1: Search[Pavel Urysohn]\n42\nObservation 1: Pavel Samuilovich Urysohn (February 3, 1898 \u02c6a August 17, 1924) was a Soviet mathematician who is best known\nfor his contributions in dimension theory.\n43\nAction 2: Search[Leonid Levin]\n44\nObservation 2: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n45\nAction 3: Finish[yes]\nFigure 3: ReAct example prompt for interleaving Thought, Action, Observation steps.\n22\nPreprint\n1\nAnswer the following questions as best you can. You have access to the following tools:\n2\nSearch: useful for when you need to answer questions about the world\n3\nUse the following format:\n4\nQuestion: the input question you must answer\n5\nThought: you should always think about what to do\n6\nAction: the action to take, should be one of [Search]\n7\nAction Input: the input to the action\n8\nObservation: the result of the action\n9\n... (this Thought/Action/Action Input/Observation can repeat N times)\n10\nThought: I now know the final answer\n11\nFinal Answer: the final answer to the original input question\n12\nBegin!\n13\nQuestion: {question}\n14\nThought:\nFigure 4: Langchain ReAct example prompt for interleaving Thought, Action, Observation steps.\n23\nPreprint\n1\nGiven the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n2\nIf you don\u2019t know the answer, just say that you don\u2019t know. Don\u2019t try to make up an answer.\n3\nALWAYS return a \"SOURCES\" part in your answer.\n4\n5\nQUESTION: Which state/country\u2019s law governs the interpretation of the contract?\n6\n=========\n7\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English\ncourts in relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may\napply to any court for an injunction or other relief to protect its Intellectual Property Rights.\n8\nSource: 28-pl\n9\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver\nof such (or any other) right or remedy.\n10\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall\nnot affect the continuation in force of the remainder of the term (if any) and this Agreement.\n11\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or\njoint venture of any kind between the parties.\n12\n11.9 No Third-Party Beneficiaries.\n13\nSource: 30-pl\n14\nContent:\n(b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any\nAnti-Bribery Laws (as defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n15\nSource: 4-pl\n16\n=========\n17\nFINAL ANSWER: This Agreement is governed by English law.\n18\nSOURCES: 28-pl\n19\n20\nQUESTION: What did the president say about Michael Jackson?\n21\n=========\n22\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet.\nJustices of the Supreme Court. My fellow Americans.\n23\nLast year COVID-19 kept us apart. This year we are finally together again.\n24\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\n25\nWith a duty to one another to the American people to the Constitution.\n26\nAnd with an unwavering resolve that freedom will always triumph over tyranny.\n27\nSix days ago, Russia\u2019s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to\nhis menacing ways. But he badly miscalculated.\n28\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined.\n29\nHe met the Ukrainian people.\n30\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n31\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending\ntheir homeland.\n32\nSource: 0-pl\n33\nContent: And we won\u2019t stop.\n34\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life.\n35\nLet\u2019s use this moment to reset. Let\u2019s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A\nGod-awful disease.\n36\nLet\u2019s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.\n37\nWe can\u2019t change how divided we\u2019ve been. But we can change how we move forward|on COVID-19 and other issues we must face\ntogether.\n38\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner,\nOfficer Jason Rivera.\n39\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.\n40\nOfficer Mora was 27 years old.\n41\nOfficer Rivera was 22.\n42\nBoth Dominican Americans who\u2019d grown up on the same streets they later chose to patrol as police officers.\n43\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their\nmission to restore the trust and safety every community deserves.\n44\nSource: 24-pl\n45\nContent: And a proud Ukrainian people, who have known 30 years of independence, have repeatedly shown that they will not\ntolerate anyone who tries to take their country backwards.\n46\nTo all Americans, I will be honest with you, as I\u2019ve always promised. A Russian dictator, invading a foreign country, has\ncosts around the world.\n47\nAnd I\u2019m taking robust action to make sure the pain of our sanctions is targeted at Russia\u2019s economy. And I will use every\ntool at our disposal to protect American businesses and consumers.\n48\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil\nfrom reserves around the world.\n49\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready\nto do more if necessary, unified with our allies.\n50\nThese steps will help blunt gas prices here at home. And I know the news about what\u2019s happening can seem alarming.\n51\nBut I want you to know that we are going to be okay.\n52\nSource: 5-pl\n53\nContent: More support for patients and families.\n54\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health.\n55\nIt\u2019s based on DARPA|the Defense Department project that led to the Internet, GPS, and so much more.\n56\nARPA-H will have a singular purpose|to drive breakthroughs in cancer, Alzheimer\u2019s, diabetes, and more.\n24\nPreprint\n1\nA unity agenda for the nation.\n2\nWe can do this.\n3\nMy fellow Americans|tonight , we have gathered in a sacred space|the citadel of our democracy.\n4\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done\ngreat things.\n5\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror.\n6\nAnd built the strongest, freest, and most prosperous nation the world has ever known.\n7\nNow is the hour.\n8\nOur moment of responsibility.\n9\nOur test of resolve and conscience, of history itself.\n10\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged.\n11\nWell I know this nation.\n12\nSource: 34-pl\n13\n=========\n14\nFINAL ANSWER: The president did not mention Michael Jackson.\n15\nSOURCES:\n16\n17\nQUESTION: {question}\n18\n=========\n19\n{summaries}\n20\n=========\n21\nFINAL ANSWER:\nFigure 5: Langchain example prompt for QA with sources.\n1\nYou are a MyScale expert. Given an input question, first create a syntactically correct MyScale query to run, then look\nat the results of the query and return the answer to the input question.\n2\nMyScale queries has a vector distance function called DISTANCE(column, array) to compute relevance to the user\u2019s question\nand sort the feature array column by the relevance.\n3\nWhen the query is asking for {top k} closest row, you have to use this distance function to calculate distance to entity\u2019s\narray on vector column and order by the distance to retrieve relevant rows.\n4\n*NOTICE*: DISTANCE(column, array) only accept an array column as its first argument and a NeuralArray(entity) as its second\nargument. You also need a user defined function called NeuralArray(entity) to retrieve the entity\u2019s array.\n5\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top k} results using\nthe LIMIT clause as per MyScale. You should only order according to the distance function.\n6\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap\neach column name in double quotes (\") to denote them as delimited identifiers.\n7\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do\nnot exist. Also, pay attention to which column is in which table.\n8\nPay attention to use today() function to get the current date, if the question involves \"today\". ORDER BY clause should\nalways be after WHERE clause. DO NOT add semicolon to the end of SQL. Pay attention to the comment in table schema.\n9\n10\nUse the following format:\n11\n======== table info ========\n12\n{table info}\n13\nQuestion: {input}\n14\nSQLQuery:\n15\n16\nHere are some examples:\n17\n======== table info ========\n18\nCREATE TABLE \"ChatPaper\" (\n19\nabstract String,\n20\nid String,\n21\nvector Array(Float32),\n22\n) ENGINE = ReplicatedReplacingMergeTree()\n23\nORDER BY id\n24\nPRIMARY KEY id\n25\nQuestion: What is Feature Pyramid Network?\n26\nSQLQuery:\nSELECT\nChatPaper.title,\nChatPaper.id,\nChatPaper.authors\nFROM\nChatPaper\nORDER\nBY\nDISTANCE(vector,\nNeuralArray(PaperRank contribution)) LIMIT {top k}\n27\n28\nLet\u2019s begin:\n29\n======== table info ========\n30\n{table info}\n31\nQuestion: {input}\n32\nSQLQuery:\nFigure 6: Langchain example prompt for SQL querying using MyScale.\n25\nPreprint\n1\nA list of documents is shown below. Each document has a number next to it along with a summary of the document. A question\nis also provided.\n2\nRespond with the numbers of the documents you should consult to answer the question, in order of relevance, as well as the\nrelevance score.\n3\nThe relevance score is a number from 1-10 based on how relevant you think the document is to the question.\n4\nDo not include any documents that are not relevant to the question.\n5\n6\nExample format:\n7\nDocument 1:\n8\n<summary of document 1>\n9\n10\nDocument 2:\n11\n<summary of document 2>\n12\n13\n...\n14\n15\nDocument 10:\n16\n<summary of document 10>\n17\n18\nQuestion: <question>\n19\nAnswer:\n20\nDoc: 9, Relevance: 7\n21\nDoc: 3, Relevance: 4\n22\nDoc: 7, Relevance: 3\n23\n24\nLet\u2019s try this now:\n25\n{context str}\n26\nQuestion: {query str}\n27\nAnswer:\nFigure 7: LlamaIndex example prompt for returning relevant documents and corresponding sum-\nmaries.\n1\nYou are an IRS chatbot whose primary goal is to help users with filing their tax returns for the 2022 year.\n2\nProvide concise replies that are polite and professional.\n3\nAnswer questions truthfully based on official government information, with consideration to context provided below on\nchanges for 2022 that can affect tax refund.\n4\nDo not answer questions that are not related to United States tax procedures and respond with \"I can only help with any\ntax-related questions you may have.\".\n5\nIf you do not know the answer to a question, respond by saying \\I do not know the answer to your question. You may be able\nto find your answer at www.irs.gov/faqs\"\n6\n7\nChanges for 2022 that can affect tax refund:\n8\nChanges in the number of dependents, employment or self-employment income and divorce, among other factors, may affect\nyour tax-filing status and refund.\nNo additional stimulus payments.\nUnlike 2020 and 2021, there were no new stimulus\npayments for 2022 so taxpayers should not expect to get an additional payment.\n9\nSome tax credits return to 2019 levels.\nThis means that taxpayers will likely receive a significantly smaller refund\ncompared with the previous tax year. Changes include amounts for the Child Tax Credit (CTC), the Earned Income Tax Credit\n(EITC) and the Child and Dependent Care Credit will revert to pre-COVID levels.\n10\nFor 2022, the CTC is worth $2,000 for each qualifying child. A child must be under age 17 at the end of 2022 to be a\nqualifying child. For the EITC, eligible taxpayers with no children will get $560 for the 2022 tax year. The Child and\nDependent Care Credit returns to a maximum of $2,100 in 2022.\n11\nNo above-the-line charitable deductions. During COVID, taxpayers were able to take up to a $600 charitable donation tax\ndeduction on their tax returns. However, for tax year 2022, taxpayers who don\u2019t itemize and who take the standard deduction,\nwon\u2019t be able to deduct their charitable contributions.\n12\nMore people may be eligible for the Premium Tax Credit. For tax year 2022, taxpayers may qualify for temporarily expanded\neligibility for the premium tax credit.\n13\nEligibility rules changed to claim a tax credit for clean vehicles. Review the changes under the Inflation Reduction Act\nof 2022 to qualify for a Clean Vehicle Credit.\nFigure 8: LlamaIndex example prompt for IRS chatbot guidelines.\n26\nPreprint\nD\nMODULES\nD.1\nPREDICT\n1 class Predict(dspy.Module):\n2\ndef __init__(self , signature , ** config):\n3\nself.signature = dspy.Signature(signature)\n4\nself.config = config\n5\n6\n# Module Parameters.\n7\nself.lm = dspy.ParameterLM(None) # use the default LM\n8\nself.demonstrations = dspy.ParameterDemonstrations ([])\n9\n10\ndef forward(self , ** kwargs):\n11\nlm = get_the_right_lm(self.lm , kwargs)\n12\nsignature = get_the_right_signature(self.signature , kwargs)\n13\ndemonstrations = get_the_right_demonstrations(self.demonstrations , kwargs)\n14\n15\nprompt = signature(demos=self.demos , ** kwargs)\n16\ncompletions = lm.generate(prompt , **self.config)\n17\nprediction = Prediction.from_completions(completions , signature=signature)\n18\n19\nif dsp.settings.compiling is not None:\n20\ntrace = dict(predictor=self , inputs=kwargs , outputs=prediction)\n21\ndspy.settings.traces.append(trace)\n22\n23\nreturn prediction\nD.2\nCHAIN OF THOUGHT\n1 class ChainOfThought(dspy.Module):\n2\ndef __init__(self , signature):\n3\n4\n# Modify signature from \u2018*inputs -> *outputs \u2018 to \u2018*inputs -> rationale , *outputs \u2018.\n5\nrationale_field = dspy.OutputField(prefix=\"Reasoning: Let\u2019s think step by step.\")\n6\nsignature = dspy.Signature(signature).prepend_output_field(rationale_field)\n7\n8\n# Declare a sub -module with the modified signature.\n9\nself.predict = dspy.Predict(self.signature)\n10\n11\ndef forward(self , ** kwargs):\n12\n# Just forward the inputs to the sub -module.\n13\nreturn self.predict (** kwargs)\n27\nPreprint\nE\nTELEPROMPTERS\nE.1\nBOOTSTRAPFEWSHOT\n1 class SimplifiedBootstrapFewShot(Teleprompter):\n2\ndef __init__(self , metric=None):\n3\nself.metric = metric\n4\n5\ndef compile(self , student , trainset , teacher=None):\n6\nteacher = teacher if teacher is not None else student\n7\ncompiled_program = student.deepcopy ()\n8\n9\n# Step 1. Prepare mappings between student and teacher Predict modules.\n10\n# Note: other modules will rely on Predict internally.\n11\nassert student_and_teacher_have_compatible_predict_modules(student , teacher)\n12\nname2predictor , predictor2name = map_predictors_recursively(student , teacher)\n13\n14\n# Step 2. Bootstrap traces for each Predict module.\n15\n# We\u2019ll loop over the training set. We\u2019ll try each example once for simplicity.\n16\nfor example in trainset:\n17\nif we_found_enough_bootstrapped_demos (): break\n18\n19\n# turn on compiling mode which will allow us to keep track of the traces\n20\nwith dspy.setting.context(compiling=True):\n21\n# run the teacher program on the example , and get its final prediction\n22\n# note that compiling=True may affect the internal behavior here\n23\nprediction = teacher (** example.inputs ())\n24\n25\n# get the trace of the all interal Predict calls from teacher program\n26\npredicted_traces = dspy.settings.trace\n27\n28\n# if the prediction is valid , add the example to the traces\n29\nif self.metric(example , prediction , predicted_traces):\n30\nfor predictor , inputs , outputs in predicted_traces:\n31\nd = dspy.Example(automated=True , **inputs , ** outputs)\n32\npredictor_name = self.predictor2name[id(predictor)]\n33\ncompiled_program[predictor_name ]. demonstrations.append(d)\n34\n35\n36\nreturn compiled_program\nE.2\nBOOTSTRAPFEWSHOTWITHRANDOMSEARCH\n1 class SimplifiedBootstrapFewShotWithRandomSearch(Teleprompter):\n2\ndef __init__(self , metric = None , trials =16):\n3\nself.metric = metric\n4\nself.trials = trials\n5\n6\ndef compile(self , student , *, teacher=None , trainset , valset=None):\n7\n# we can do forms of cross -validation if valset is unset.\n8\nvalset = trainset if valset is None else valset\n9\n10\ncandidates = []\n11\nfor seed in range(self.trials):\n12\n# Create a new basic bootstrap few -shot program.\n13\nshuffled_trainset = shuffle(trainset , seed=seed)\n14\ntp = BootstrapFewShot(metric=metric , max_bootstrap_demos=random_size ())\n15\ncandidate_program = tp.compile(student , shuffled_trainset , teacher)\n16\n17\n# Step 2: Evaluate the generated candidate program.\n18\nscore = evaluate_program(candidate_program , self.metric , valset)\n19\ncandidates.append ((score , candidate_program))\n20\n21\n# return the best candidate program.\n22\nreturn max(candidates , key=lambda x: x[0]) [1]\n28\nPreprint\nE.3\nBOOTSTRAPFEWSHOTWITHOPTUNA\n1 class SimplifiedBootstrapFewShotWithOptuna(Teleprompter):\n2\ndef __init__(self , metric , trials =16):\n3\nself.metric = metric\n4\nself.trials = trials\n5\n6\ndef objective(self , trial):\n7\npool = self.pool\n8\n9\n# Step 1: Create copy of student program.\n10\ncandidate_program = self.student.reset_copy ()\n11\n12\n# Step 2: Based on trial , select demos for each predictor in program.\n13\n# Note. For simplicity , we can just select a single demo for each predictor.\n14\n# But we can easily tune the number of demonstrations to select here.\n15\nfor (name , predictor1), (_, predictor2) in \\\n16\nzip(pool.named_predictors (), candidate_program.named_predictors ()):\n17\nall_demos = predictor1.demos\n18\ndemo_index = trial.suggest_int(f\"demo_index_for_{name}\", 0, len(all_demos) - 1)\n19\npredictor2.demos = [all_demos[demo_index ]]\n20\n21\n# Step 3: Evaluate the modified candidate program.\n22\nscore = evaluate_program(candidate_program , self.metric , self.valset)\n23\n24\n# Step 4: Store the candidate for Optuna to select highest -scoring program.\n25\ntrial.set_user_attr(\"program\", candidate_program)\n26\nreturn score\n27\n28\ndef compile(self , student , trainset , teacher=None , valset=None):\n29\nself.trainset = trainset\n30\nself.valset = trainset if valset is None else valset\n31\n32\nself.student = student.deepcopy ()\n33\nself.teacher = teacher.deepcopy () if teacher else student.deepcopy ()\n34\n35\n# Leverage BootstrapFewshot to create a large number of potential demonstrations.\n36\ntp = BootstrapFewShot ()\n37\nself.pool = tp.compile(self.student , self.teacher , self.trainset , self.metric)\n38\n39\n# Use Optuna to find the best program by optimizing the objective function.\n40\nbest_program = optimize_with_optuna(self.objective)\n41\n42\nprint(\u2019Best score:\u2019, best_program.score)\n43\nprint(\u2019Best program:\u2019, best_program)\n44\nreturn best_program\n29\nPreprint\nF\nEXAMPLES OF THE PROMPTS AUTOMATICALLY GENERATED BY DSPY\nFor GSM8K, we include the prompt bootstrapped by DSPy for GSM8K llama2-13b-chat for the\nvanilla program compiled with bootstrap\u00d72 in Figure 9.\nWe also include a CoT prompt for GSM8K and a generate query prompt from the multihop pro-\ngram for HotPotQA. All of these, particularly their demonstrations\u2019 labels and their selection, are\ngenerated by DSPy automatically using llama2-13b-chat.\n30\nPreprint\n1\nGiven the fields \u2018question\u2018, produce the fields \u2018answer\u2018.\n2\n3\n---\n4\n5\nFollow the following format.\n6\n7\nQuestion: ${question}\n8\nAnswer: ${answer}\n9\n10\n---\n11\n12\nQuestion: Jimmy and Irene go shopping for clothes on a Tuesday, where senior citizens get a 10% discount on their purchases.\nJimmy picks out 3 shorts from the $15 rack. Irene grabs 5 shirts from the $17 rack. How much money do they give to the\ncashier?\n13\nAnswer: Jimmy picks out 3 shorts at $15 each = $45. Irene grabs 5 shirts at $17 each = $85. Total cost = $45 + $85 = $130.\nSince senior citizens get a 10% discount, they will pay 10% of $130 = $13. So, they will give the cashier $130 - $13 =\n$117. ---\n14\n15\n---\n16\n17\nQuestion:\nFigure 9: Copy of the prompt automatically generated by DSPy for GSM8K Llama2-13b-chat\nvanilla program compiled with bootstrap\u00d72.\n1\nGiven the fields \u2018question\u2018, produce the fields \u2018answer\u2018.\n2\n3\n---\n4\n5\nFollow the following format.\n6\n7\nQuestion: ${question}\n8\nReasoning: Let\u2019s think step by step in order to $produce the answer. We ...\n9\nAnswer: ${answer}\n10\n11\n---\n12\n13\nQuestion: Mark is baking bread. He has to let it rise for 120 minutes twice. He also needs to spend 10 minutes kneading\nit and 30 minutes baking it. How many minutes does it take Mark to finish making the bread?\n14\nReasoning: Let\u2019s think step by step in order to find out how many minutes it takes Mark to finish making the bread. We know\nthat he needs to let it rise for 120 minutes twice, so that\u2019s 240 minutes. Then, he needs to spend 10 minutes kneading it\nand 30 minutes baking it. So, in total, it will take Mark 240 + 10 + 30 = 280 minutes to finish making the bread.\n15\nAnswer: 280 ---\n16\n17\n---\n18\n19\nQuestion: Ben has $2000 for his business operations costs. He orders goods from his supplier and writes them a cheque for\n$600. His debtor pays him $800 from the purchases they had made on credit. Mr. Ben then decides to do equipment maintenance\nand spends $1200 on the whole operation. How much money is Mr. Ben remaining with?\n20\nReasoning: Let\u2019s think step by step in order to find out how much money Mr. Ben is remaining with. We know that he had\n$2000 to begin with, and he spent $600 on goods from his supplier, so he has $2000 - $600 = $1400 left. Then, his debtor\npaid him $800, so he has $1400 + $800 = $2200. Finally, he spent $1200 on equipment maintenance, so he has $2200 - $1200\n= $1000 left.\n21\nAnswer: $1000 ---\n22\n23\n---\n24\n25\n... several other demonstrations here ...\n26\n27\n---\n28\n29\nQuestion:\nFigure 10: Shortened copy of the prompt automatically generated by DSPy for GSM8K Llama2-\n13b-chat CoT program compiled with bootstrap.\n31\nPreprint\n1\nGiven the fields \u2018context\u2018, \u2018question\u2018, produce the fields \u2018search query\u2018.\n2\n3\n---\n4\n5\nFollow the following format.\n6\n7\nContext: ${context}\n8\nQuestion: ${question}\n9\nReasoning: Let\u2019s think step by step in order to ${produce the search query}. We ...\n10\nSearch Query: ${search query}\n11\n12\n---\n13\n14\nContext:\n15\n[1] Twilight (novel series) | Twilight is a series of four vampire-themed fantasy romance novels by American author\nStephenie Meyer. ...\n16\n[2] Harper Connelly Mysteries | The Harper Connelly Mysteries is a series of fantasy mystery novels written by Charlaine\nHarris, and first published in 2005. ...\n17\n[3] The Dark Heroine | The Dark Heroine is a series of vampire-themed fantasy romance novels written by English author\nAbigail Gibbs, published by HarperCollins in 2012. ...\n18\n19\nQuestion: In which year was the first of the vampire-themed fantasy romance novels for which The Twilight Saga: The\nOfficial Illustrated Guide serves as a spin-off encyclopedic reference book first published?\n20\n21\nReasoning: Let\u2019s think step by step in order to determine the year the first of the vampire-themed fantasy romance novels\nwas first published. ...\n22\n23\nSearch Query: When was the first of the vampire-themed fantasy romance novels published?\n24\n25\n---\n26\n27\nContext:\n28\n[1] The Victorians | The Victorians - Their Story In Pictures is a 2009 British documentary series which focuses on\nVictorian art and culture. ...\n29\n[2] The Caxtons | The Caxtons: A Family Picture is an 1849 Victorian novel by Edward Bulwer-Lytton that was popular in its\ntime.\n30\n[3] Victorian (comics) | The Victorian is a 25-issue comic book series published by Penny-Farthing Press and starting in\n1999. ...\n31\n32\nQuestion: The Victorians - Their Story In Pictures is a documentary series written by an author born in what year?\n33\n34\nReasoning: Let\u2019s think step by step in order to produce the search query. We know that the documentary series is about\nVictorian art and culture, and it was written and presented by Jeremy Paxman. Therefore, we need to find the year in which\nJeremy Paxman was born.\n35\n36\nSearch Query: Jeremy Paxman birth year\n37\n38\n---\n39\n40\n41\nContext:\nFigure 11: Shortened copy of the prompt automatically generated by DSPy for HotPotQA Llama2-\n13b-chat multi-hop program (generating second hop query) compiled with bootstrap.\n32\n"
  },
  {
    "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
    "link": "https://arxiv.org/pdf/2310.03739.pdf",
    "upvote": "21",
    "text": "Preprint. Under review.\nALIGNING TEXT-TO-IMAGE DIFFUSION MODELS WITH\nREWARD BACKPROPAGATION\nMihir Prabhudesai1\nAnirudh Goyal2\nDeepak Pathak1\nKaterina Fragkiadaki1\n1Carnegie Mellon University\n2Google DeepMind\nEpoch 0\nEpoch 2\nEpoch 5\nEpoch 10\nEpoch 0\nEpoch 2\nEpoch 5\nEpoch 10\nFigure 1: We present AlignProp, a direct backpropagation-based approach to adapt text-to-image\ndiffusion models for desired reward function. The above examples showcase the adaptation of\ndiffusion model output (epoch 0) through a sequence of adaptation steps (epoch 1-10) to different\nreward functions. The reward function in the left two examples is that of concept removal trained to\ndisregard the concept of \"books\" despite the prompt to the text-to-image model explicitly mentioning\n\"fruits and books.\" The reward function for the adaptation on the right is that of human-preference\nalignment crafted from human rankings of image-text pairs. As shown in all the examples, the\nproposed approach can effectively align the diffusion model with the reward function.\nABSTRACT\nText-to-image diffusion models have recently emerged at the forefront of image\ngeneration, powered by very large-scale unsupervised or weakly supervised text-\nto-image training datasets. Due to their unsupervised training, controlling their\nbehavior in downstream tasks, such as maximizing human-perceived image quality,\nimage-text alignment, or ethical image generation, is difficult. Recent works fine-\ntune diffusion models to downstream reward functions using vanilla reinforcement\nlearning, notorious for the high variance of the gradient estimators. In this paper,\nwe propose AlignProp, a method that aligns diffusion models to downstream re-\nward functions using end-to-end backpropagation of the reward gradient through\nthe denoising process. While naive implementation of such backpropagation would\nrequire prohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test AlignProp\nin finetuning diffusion models to various objectives, such as image-text semantic\nalignment, aesthetics, compressibility and controllability of the number of objects\npresent, as well as their combinations. We show AlignProp achieves higher re-\nwards in fewer training steps than alternatives, while being conceptually simpler,\nmaking it a straightforward choice for optimizing diffusion models for differen-\ntiable reward functions of interest. Code and Visualization results are available at\nhttps://align-prop.github.io/.\n1\nINTRODUCTION\nDiffusion probabilistic models Sohl-Dickstein et al. (2015); Goyal et al. (2017); Ho et al. (2020a)\nare currently the de facto standard for generative modeling in continuous domains. Text-to-image\ndiffusion models such as DALLE Ramesh et al. (2022), Imagen Saharia et al. (2022), and Latent\nDiffusion Rombach et al. (2022) are at the forefront of image generation by training on web-scale data.\n1\narXiv:2310.03739v1  [cs.CV]  5 Oct 2023\nPreprint. Under review.\nHowever, most use cases of diffusion models are related to downstream objectives such as aesthetics,\nfairness, text-to-image alignment, or robot task achievement, which may be not well aligned with\nmaximizing likelihood in the training dataset. For example, while the training images may contain\nunusual camera and object viewpoints with objects half-visible or truncated at the image border,\nhuman users usually prefer image samples of standardized viewpoints, beautifully centered content\nwith key elements in focus. Additionally, because of the noise present in pre-training datasets, there\nis frequently a misalignment between the semantics of the generated images and the associated text\nprompts. This occurs because the models tend to adopt the noise, biases, and peculiarities inherent\nin the training data. In this paper, we consider the problem of training diffusion models to optimize\ndownstream objectives directly, as opposed to matching a data distribution.\nThe most straightforward approach to aligning pre-trained models to downstream objectives is\nsupervised fine-tuning on a small-scale human-curated dataset of high-quality model responses\nOuyang et al. (2022); Lee et al. (2023). Unfortunately, this is mostly not a feasible option. It is not\nonly difficult to collect data on samples that display desired properties like aesthetics, fairness, and\ntext-to-image alignment, but such data can easily be biased. On the other hand, it is much easier to\nask humans for relative feedback by showing two or more samples. Hence, as a result, the common\npractice is to train a reward model by explicitly collecting data of human preferences by asking a\nhuman subject to rank a number of examples as per the desired metric. However, in the case of\ndiffusion models, this leads to a unique challenge: given such a reward function, how does one update\nthe weights of the diffusion model?\nThe core architecture of diffusion models operates by iteratively refining a data sample through a\nsequence of stochastic transformations. Even though the learned reward function is differentiable, it\nis non-trivial to update the diffusion model through the long chain of diffusion sampling as it would\nrequire prohibitive memory resources to store the partial derivatives of all neural layers and denoising\nsteps. This can easily be on the order of several terabytes of GPU memory Wallace et al. (2023)\nfor the scale of modern text-to-image diffusion models. As a result, the typical alternative is to use\nreinforcement learning and directly update the diffusion weights via REINFORCE. This is the most\ncommon approach today to align diffusion models with a reward function Black et al. (2023); Lee\net al. (2023); Ziegler et al. (2020); Stiennon et al. (2020). However, RL methods are notorious for\nhigh variance gradients and hence often result in poor sample efficiency.\nIn this paper, we revisit the idea of direct end-to-end backpropagation of the gradients of a differen-\ntiable reward function through the diffusion chain, and device practical ways to get around the issue\nof using exponential memory and compute. We propose Alignment by Backpropagation (AlignProp),\na model that casts denoising inference of text-to-image diffusion models as a differentiable recur-\nrent policy. This policy effectively maps conditioning input prompts and sampled noise to output\nimages, and fine-tunes the weights of the denoising model using end-to-end backpropagation through\ndifferentiable reward functions applied to the output-generated image.\nWe fine-tune low-rank adapter weights Hu et al. (2021), added to the original denoising U-Net, instead\nof the original weights, and we use gradient checkpointing Gruslys et al. (2016); Chen et al. (2016) to\ncompute partial derivatives on demand, as opposed to storing them all at once. In this way, AlignProp\nincurs reasonable memory cost while only doubling the processing cost per training step, which gets\ncompensated due to the fact that direct backdrop needs less number of steps to optimize. However,\nend-to-end backdrop quickly tends to over-optimize the model to excessively maximize the reward\nmodel leading to collapse. We address the over-optimization Gao et al. (2022) with randomized\ntruncated backpropagation Tallec & Ollivier (2017), i.e., randomly sampling the denoising step up to\nwhich we back-propagate the reward.\nWe test AlignProp in finetuning StableDiffusion Rombach et al. (2022) to maximize objectives of\naesthetic quality, text-to-image semantic alignment, and modulating object presence, as we show in\nFigure 1. We show it achieves higher rewards and is more preferred by human users than reinforcement\nlearning alternatives of Black et al. (2023); Lee et al. (2023). We ablate design choices of the proposed\nmodel and show the importance of backpropagating through the denoising chain for variable number\nof steps. We show adapted layers in early denoising steps align the semantic content while adapted\nlayers in later denoising steps adapt the high frequency details to the downstream objective. Last,\nwe show convex combinations of weights of the finetuned models maximize combinations of the\ncorresponding reward functions. Our code and models are publicly available at <GitHub Link>.\n2\nPreprint. Under review.\n2\nRELATED WORK\nDenoising diffusion models Sohl-Dickstein et al. (2015); Goyal et al. (2017); Ho et al. (2020a) have\nemerged as an effective class of generative models for modalities including images Ramesh et al.\n(2021); Rombach et al. (2022); Saharia et al. (2022), videos Singer et al. (2022); Ho et al. (2022a;b),\n3D shapes Zeng et al. (2022) and robot or vehicle trajectories Ajay et al. (2023); Pearce et al. (2023);\nChi et al. (2023); Tan et al. (2023). Remarkably, they have also been adapted for text generation\nLovelace et al. (2022); Lin et al. (2023) and have demonstrated utility in discriminative tasks like\nImage Classification Li et al. (2023); Prabhudesai et al. (2023). Diffusion models are often pre-trained\nin large usupervised or very weakly supervised datasets, and adapted to improve their performance\nin downstream tasks, as well as their alignment with user intent. Some forms of adaptation do not\nalter the parameters of the diffusion model. Instead, they optimize conditional input prompts Hao\net al. (2022); Gal et al. (2022); Kumari et al. (2023), manipulate the text-image cross attention layers\nFeng et al. (2023) to improve text-to-image alignment, guide sampling during inference using the\ngradients of a pre-trained classifier Dhariwal & Nichol (2021), or use classifier-free Ho & Salimans\n(2021) guidance by combining conditional and unconditional diffusion models. Other adaptation\nmethods finetune the model\u2019s parameters using small-scale human-curated datasets, or human labels\nof absolute or relative quality of model\u2019s responses Lee et al. (2023); Black et al. (2023); Wu et al.\n(2023b); Dong et al. (2023). These methods first fit a neural network reward function using human\nscores or relative preferences, and then finetune the diffusion model using reward-weighted likelihood\nLee et al. (2023), reward-filtered likelihood Dong et al. (2023) or reinforcement learning Black et al.\n(2023), such as PPO Schulman et al. (2017). In this paper, we show how diffusion models can be\nfinetuned directly for downstream differentiable reward models using end-to-end backpropagation.\nOur formulation resembles deep deterministic policy gradients Silver et al. (2014), a reinforcement\nlearning method that trains a deterministic policy through end-to-end differentiation of a concurrently\ntrained Q function. Work of Xu et al. (2023) also consider backpropagating through a differentiable\nreward function but only backpropagates through a single step of the sampling process. We show that\nAlignProp outperforms it by backpropagating gradients through the entire sampling chain. Work of\nWallace et al. (2023) also backpropagates through the sampling process of the diffusion chain but\ninstead of optimizing the weights of the diffusion model, it optimizes the starting noise. This process\nis expensive, as it needs to be performed for each prompt separately.\n3\nBACKGROUND\nWe first discuss background material in Section 3 and present details of our method in Section 4.\nDiffusion Models\nA diffusion model learns to model a probability distribution p(x) by inverting a\nprocess that gradually adds noise to a sample x, which is known as forward diffusion process. The\namount of noise added varies according to the diffusion timestep t following a variance schedule\nof {\u03b2t \u2208 (0, 1)}T\nt=1. Forward diffusion process adds noise to x following xt = \u221a\u00af\u03b1tx + \u221a1 \u2212 \u00af\u03b1t\u03f5\nwhere \u03f5 \u223c N(0, 1), is a sample from a Gaussian distribution (with the same dimensionality as x),\n\u03b1t = 1 \u2212 \u03b2t, and \u00af\u03b1t = Qt\ni=1 \u03b1i. It then learns a reverse diffusion process that predicts the noise \u02c6\u03f5\ngiven xt and t as input The denoising process is modeled by a neural network \u02c6\u03f5 = \u03f5\u03b8(xt; t) that takes\nas input the noisy sample xt and the noise level t and tries to predict the noise component \u03f5.\nDiffusion models can be easily extended to model p(x|c), where c is the conditioning signal, such as\nimage caption, image category or semantic maps etc. . This is done by adding an additional input\nto the denoising neural network \u03f5\u03b8. For this work we consider text-conditioned image diffusion\nmodels such as Stable Diffusion Rombach et al. (2022), which are trained using a large collection of\nimage-text pairs D\u2032 = {(xi, ci)}N\ni=1 using the following objective:\nLdiff(\u03b8; D\u2032) =\n1\n|D\u2032|\nX\nxi,ci\u2208D\u2032\n||\u03f5\u03b8(\u221a\u00af\u03b1txi +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5, ci, t) \u2212 \u03f5||2.\nThis loss corresponds to a reweighted form of the variational lower bound for log p(x|c) Ho et al.\n(2020b).\n3\nPreprint. Under review.\nIn order to draw a sample from the learned distribution p\u03b8(x|c), we start by drawing a sample\nxT \u223c N(0, 1). Then, we progressively denoise the sample by iterated application of \u03f5\u03b8 according\nto a specified sampling schedule Ho et al. (2020b); Song et al. (2020), which terminates with x0\nsampled from p\u03b8(x):\nxt\u22121 = \u221aat\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t, c)\n\u0013\n+ \u03c3tz, where z \u223c N(0, 1)\nIn denoising diffusion implicit models (DDIMs) Song et al. (2022), there is no added noise in\nintermediate steps, and the only stochasticity in sampling comes from the initially sampled noise\nxT \u223c N(0, 1):\nxt\u22121 = \u221aat\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t, c)\n\u0013\n.\n(1)\n4\nALIGNPROP\nReward \nModel\nR\nxT\nxt\nxt\u22121\nx0\nBackpropogate\nBackpropogate\nBackpropogate\nRandomly Truncate\nLoss\nDDIM Sampling\nDDIM Sampling\nDDIM \nSampling\nFigure 2: Given a batch of prompts, denoted as P, AlignProp generates images from noise xT through\nDDIM Sampling. Subsequently, these generated images undergo evaluation by the Reward model\nR\u03d5 to acquire a corresponding reward score. The optimization process involves updating the weights\nin the diffusion process by minimizing the negative of the obtained reward through gradient descent.\nTo mitigate overfitting, we randomize the number of time-steps we backpropagate gradients to.\nThe architecture for AlignProp is illustrated in Figure 2 We introduce a method that transforms\ndenoising inference within text-to-image diffusion models into a differentiable recurrent policy, which\nadeptly correlates conditioning input prompts and sampled noise to produce output images. This\napproach facilitates fine-tuning of the denoising model\u2019s weights through end-to-end backpropagation,\nguided by differentiable reward functions applied to the generated output image.\nThe proposed model casts conditional image denoising as a single step MDP with states S =\n{(xT , c), xT \u223c N(0, 1)}, actions are the generated image samples, and the whole DDIM denoising\nchain of Eq. 1 corresponds to a differentiable policy that maps states to image samples: A = {x0 :\nx0 \u223c \u03c0\u03b8(\u00b7|xT , c), xT \u223c N(0, 1) }. The reward function is a differentiable function of parameters \u03d5\nthat depends only on generated images R\u03d5(x0), x0 \u2208 A. Given a dataset of prompts input P, our\nloss function reads:\nLalign(\u03b8; P) = \u2212 1\n|P|\nX\nci\u2208P\nR\u03d5(\u03c0\u03b8(xT , ci))\n(2)\nWe update the parameters of the diffusion model using gradient descent on Lalign. The policy \u03c0 is\nrecurrent and training it is reminiscent of backpropagation through time used for training recurrent\nnetworks. The gradient through update the parameters of diffusion model w.r.t downstream object\n(i.e., differentiable reward function) looks like:\n\u2207\u03b8Lalign = \u2202Lalign\n\u2202\u03b8\n+\nT\nX\nt=0\n\u2202Lalign\n\u2202xt\n\u00b7 \u2202xt\n\u2202\u03b8\n|\n{z\n}\n\u201cmemory inefficient\u201d\n.\n(3)\n4.1\nREDUCING MEMORY OVERHEAD\nNaively optimizing Eq. 2 with backpropagation requires to store the intermediate activations for each\nneural layer and denoising timestep of \u03c0\u03b8 in the GPU VRAM. As can be seen in Eq. 1, the number\n4\nPreprint. Under review.\nof activations to store for \u03c0\u03b8 scale linearly with the number of diffusion timesteps. Therefore, our\nalignment finetuning would cost T times more memory than training the diffusion model \u03f5\u03b8 with\ndiffusion loss of Eq. 1. For instance, training StableDiffusion Rombach et al. (2022) using a batch\nsize of 1 takes about 20GBs of GPU RAM, therefore training our policy \u03c0\u03b8 comprised of T chained\ndenoising models with end-to-end backpropagation would require about 1TB of GPU RAM, which is\ninfeasible. We use two design choice to enable full backpropagation through the denoising chain:\n1. Finetuning low-rank adapter (LoRA) modules Hu et al. (2021) in place of the original diffusion\nweights, and 2. Gradient checkpointing for computing partial derivatives on demand Gruslys et al.\n(2016); Chen et al. (2016).\nFinetuning LoRA weights: Instead of fine-tuning the weights \u03b8 of the original diffusion model \u03f5\u03b8,\nwe add low-rank weight kernels in each of its linear neural layers, and only finetune these weights, as\nproposed in Hu et al. (2021). Specifically, each linear layer of the Unet of StableDiffusion is modified\nfrom h = Wx into h = Wx + BAx, where W \u2208 Rm\u00d7m, A \u2208 Rm\u00d7k, B \u2208 Rk\u00d7m, where k << m.\nLoRA weights are initialized at 0 and do not affect the performance of the pre-trained model initially.\nEffectively, this means we finetune 800K parameters instead of 800M, which reduces our GPU\nRAM usage by 2X to about 500GBs.\nGradient Checkpointing: Gradient checkpointing is a well known technique used to reduce the\nmemory footprint of training neural networks Gruslys et al. (2016); Chen et al. (2016). Instead\nof storing all intermediate activations in memory for backpropagation, we only store a subset and\nrecompute the rest on-the-fly during the backward pass. This allows for training deeper networks\nwith limited memory at the cost of increased computation time. We find that gradient checkpointing\nsignificantly reduces our memory usage from 512 GBs to 15GBs, thus making it feasible to do full\nbackpropogation on a single GPU.\n0-10 timesteps\n10-20 timesteps\n20-30 timesteps\n30-40 timesteps\n40-50 timesteps\nNo LoRA Disable\nAll LoRA Disable\nFigure 3: We illustrate the impact of deactivating finetuned LoRA weights across varying ranges of\ndiffusion timesteps during inference. The visualization highlights that earlier timesteps predominantly\ncontribute to semantic aspects, whereas the later timesteps are instrumental in capturing fine-grained\ndetails.\n4.2\nRANDOMIZED TRUNCATED BACKPROPAGATION\nDuring our experimentation, we encountered a significant issue with full backpropagation through\ntime (BPTT) - it led to mode collapse within just two training epochs. Irrespective of the input\nconditioning prompt, we observed that the model consistently generated the same image. To address\nthis challenge, we explored truncated backpropagation through time (TBTT) as an alternative strategy.\nHowever, TBTT introduces a bias towards short-term dependencies, as it restricts the backpropagation\nto a fixed number of steps, denoted as K (a hyperparameter). This bias can affect gradient estimates\nand hinder the model\u2019s ability to capture long-range dependencies effectively. (Tallec & Ollivier,\n2017) demonstrated that the bias introduced by truncation in the backpropagation through time\nalgorithm can be mitigated by randomizing the truncation lengths, i.e., varying the number of\ntime-steps for which backpropagation occurs.\nOur human evaluation experiments, detailed in Section 5, provided valuable insights. It was observed\nthat setting K \u223c Uniform(0, 50) yielded the most promising results in terms of aligned image\ngeneration. To delve deeper into the impact of different values of K on alignment and image\ngeneration, we conducted a comprehensive investigation, the results of which are presented in Figure\n3. Finally, the loss which we optimize is:\nb\u2207\u03b8LTBTT-align = \u2202Lalign\n\u2202\u03b8\n+\nK\nX\nk=0\n\u2202Lalign\n\u2202xk\n\u2202xk\n\u2202\u03b8 .\n(4)\n5\nPreprint. Under review.\n5\nEXPERIMENTS\nIn this study, we subject AlignProp to rigorous evaluation by employing it to fine-tune pre-trained\ntext-to-image diffusion models across a diverse spectrum of downstream reward functions. For all our\nexperimental scenarios, we utilize StableDiffusion Rombach et al. (2022) a state-of-the-art pre-trained\nmodel. Our investigation is motivated by the pursuit of answers to the following questions:\n\u2022 How does AlignProp compare with existing state-of-the-art alignment methods concerning\nability to optimize downstream reward, data and compute efficiency?\n\u2022 To what extent does the fine-tuned model exhibit generalization capabilities when presented\nwith novel prompts?\n\u2022 How does AlignProp measure up against existing state-of-the-art alignment methods in\nterms of human evaluations assessing the quality of the generated images?\nBaselines.\nWe compare against the following methods: (a) Stable Diffusion, the pre-trained\ndiffusion model without any finetuning. We use Stable Diffusion v1.5 for all our experiments, (b)\nReward Weighted Regression (RWR) Lee et al. (2023), which uses Reward weighted negative\nlikelihood for finetuning. Specifically, they weigh the diffusion loss with the corresponding reward\nfor each example. Unlike AlignProp, they do not use gradients from the reward model, (c) Denoising\nDiffusion Policy Optimization (DDPO) Black et al. (2023) current state-of-the-art method for\nfine-tuning text-to-image models using reinforcement learning (PPO). We use their official pytorch\ncodebase for reproducing results, (d) ReFL, which is a finetuning method proposed in ImageRewards\nXu et al. (2023). where instead of doing all the denoising steps (which is about 50 for us), they do\na small number of denoising steps (30-40) using DDPM scheduler. Then they do a single step of\nbackpropogation (K=1) to update the diffusion U-Net weights. We instead do all the sampling steps\nas reducing the number of denoising steps reduces the fidelity of the image. Further instead of doing\na single backpropogation step (K=1), we use randomized TBTT where K \u223c Uniform(0, 50), we\nfind this to significantly improve the generation quality as shown in Table 3 and Figure 5.\nReward models.\nFor aligning text-to-image diffusion models, we explore various reward functions\nto guide the training process. These reward functions encompass\n(i) Aesthetics Reward. We employ the LAION aesthetic predictor V2 Schuhmann (2022), which\nleverages a multi-layer perceptron (MLP) architecture trained atop CLIP embeddings. This model\u2019s\ntraining data consists of 176,000 human image ratings, spanning a range from 1 to 10, with images\nachieving a score of 10 being considered art pieces. To ensure consistency with previous research,\nour training procedure closely follows the approach utilized in DDPO, which employs 50 animal\nnames as prompts.\n(ii) Human Preference Reward. We use Human Preference v2 Wu et al. (2023a), where CLIP\nmodel is fine-tuned using an extensive dataset comprising 798,090 human ranking choices across\n433,760 pairs of images. Notably, this dataset stands as one of the largest of its kind, facilitating\nrobust image-text alignment. For our training regimen, we draw from the 2400 prompts introduced in\nthe HPS v2 dataset.\n(iii) Concept Removal. This is focused on the removal of specified object categories from generated\nimages. This task holds the potential for curbing the generation of abusive or harmful content by\ntext-to-image models. In this instance, we address a simplified version of the task, instructing the\ntext-to-image model to generate images containing \"<Concept Name> and Books.\" To facilitate\nthis task, we enlist the aid of OwlViT Minderer et al., an open vocabulary object detector capable\nof identifying \"books\" within images. As our reward function, we adopt a confidence score-based\napproach, specifically (1.0 - c), where c represents the confidence score assigned by OwlViT to its\n\"books\" detection.\n5.1\nSAMPLE AND DATA EFFICIENCY IN REWARD FINETUNING\nThe task of pre-training foundation models, such as Stable Diffusion, on large-scale training datasets is\ncurrently within the reach of only a select few entities with access to substantial computing resources.\nHowever, the subsequent fine-tuning of these foundation models towards downstream objectives\nshould ideally be accessible to a broader range of entities, including academic institutions with more\n6\nPreprint. Under review.\nDDPO\nAlignProp (Ours)\nFigure 4: Reward finetuning results on multiple reward functions. In the top half of the figure, we\ncompare the data efficiency of AlignProp and DDPO. In the bottom half of the figure we compare the\nconvergence speed of AlignProp and DDPO. As seen AlignProp outperforms DDPO on both metrics.\nlimited computational capabilities. To enable such accessibility, it is imperative that an effective\nfine-tuning method for foundation models be both sample-efficient and incur low computational\ncosts.\nIn this section, we undertake a comprehensive comparison between AlignProp and DDPO across\nvarious reward functions, examining the achieved rewards and the computational and data efficiency.\nOur results are visually presented in Figure 4. Notably, our method demonstrates superior performance\nin terms of rewards while simultaneously exhibiting remarkable data efficiency, being 25 times more\ndata-efficient than DDPO.\nMethod\nAnimals\nHPS v2\nTrain.\nTest.\nTrain.\nTest.\nStable Diffusion\n5.73\n5.64\n2.74\n2.86\nRWR\n6.21\n6.04\n2.81\n2.91\nDDPO\n7.18\n6.82\n2.87\n2.93\nAlignProp (Ours)\n8.94\n8.71\n3.30\n3.32\nTable 1: Reward on novel text prompts (higher is\nbetter). We split the class names in these datasets\ninto train and test split such that there is no overlap\nin them. The proposed method AlignProp achieves\nhigher reward compared to all other baselines.\nTo further emphasize the efficiency of our ap-\nproach, we conduct a comparative analysis of\nwall-clock times for DDPO and AlignProp on\nidentical computational resources, which in-\nclude 4 A100 GPUs and the same batch size.\nAs illustrated in the HPS v2 dataset scenario,\nAlignProp achieves a score of 0.28 in just 48\nminutes, whereas DDPO requires approximately\n23 hours, highlighting a substantial 25-fold ac-\nceleration in convergence speed. Additionally,\nwe provide qualitative results in Figure 5, offer-\ning a visual representation of the outputs gener-\nated by both AlignProp and DDPO. Notably, our\nmethod excels in generating significantly more\nartistic images when compared to DDPO.\n5.2\nGENERALIZATION TO NEW TEXT PROMPTS\nOne pivotal consideration favoring model finetuning, as opposed to prompt finetuning or initial noise\nxT finetuning , is the model\u2019s potential for generalization to new prompts. Here, we evaluate the\nfinetuned models stemming from AlignProp and baseline approaches, specifically assessing their\ncapacity for generalization.\nHere, we consider two different prompt datasets, each with a different type of split: (a) Animals: We\nstart with a dataset encompassing a curated list of 50 animals, mirroring the training set configuration\nemployed in DDPO. However, in the test set, we introduce a novel set of animals as prompts that\nwere not encountered during the training phase. The training process in this scenario is conducted\nusing the Aesthetic reward model, (b) HPS v2: The Human Preference dataset offers a diverse array\nof 3200 prompts, categorized into four different styles. Each style comprises roughly 800 prompts.\n7\nPreprint. Under review.\nReFL\nStable \nDiffusion\nAlignProp\n (K=1)\nAlignProp\n(Ours)\nDDPO\nCrocodile\nOctopus\nHippopotamus\nSnail\nCheetah\nLobster\nFigure 5: Qualitative comparison with baselines is conducted on novel animals that were not\nencountered during the training phase. In this scenario, both AlignProp and the baselines are trained\nusing an Aesthetic reward model.\nTo establish a train-test split, we adopt a random selection approach, allocating 600 prompts for\ntraining and reserving 200 for testing. In this case, we utilize the HPS-v2 reward model for the\ntraining regimen.\nIn Table 1, we present the average rewards achieved by the models on both the train and test splits\nafter 100 epochs of training. Notably, AlignProp consistently outperforms the baseline methods in\nterms of the generalization exhibited by the resulting fine-tuned models.\n5.3\nHUMAN EVALUATIONS FOR FIDELITY AND IMAGE-TEXT ALIGNMENT\nWe conducted a human preference study using Amazon Mechanical Turk, performing a paired\ncomparison test between the proposed method and different baselines such as ReFL, DDPO, and\nStable Diffusion. In this evaluation, we employed the HPS reward function. For the assessment of\nimage-text alignment, we presented two image generations from each method (ours and the baseline)\nusing the same prompt, accompanied by the question: \u2018Which image is more consistent with the\ntext?\u2019 In terms of fidelity, we displayed two samples for a specific prompt from both the baselines and\nAlignProp, with the question: \u2018Which image quality appears better?\u2019 We collected 500 responses for\neach task. As depicted in Table 2, the proposed method is preferred over the baselines. For additional\nqualitative results on HPS reward function and prompts, please refer to Appendix Figure 7\n5.4\nMIXING WEIGHTS\nIn this context, we demonstrate the ability of AlignProp to interpolate between different reward\nfunctions during the inference phase. We draw inspiration from the concept presented in ModelSoup\nWortsman et al. (2022), which showcases how averaging the weights of multiple fine-tuned models\ncan enhance image classification accuracy. Expanding upon this idea, we extend it to the domain of\nimage editing, revealing that averaging the LoRA weights of diffusion models trained with distinct\nreward functions can yield images that satisfy multiple reward criteria.\nTo achieve this, we employ a straightforward approach: suppose \u03b81 and \u03b82 represent the LoRA\nweights trained using reward functions R1 and R2 respectively. In this parameter space, we perform\ninterpolation by computing a weighted average as \u03b8mod = \u03b1 \u00b7 \u03b81 + (1 \u2212 \u03b1) \u00b7 \u03b82 where \u03b1 is a scalar\nranging from 0 to 1.\n8\nPreprint. Under review.\nTo better understand the effect of mixing we consider two complementary reward functions specif-\nically compressibility and aesthetics. For compressiblity we finetune CLIP supervised using the\nJPEG compression reward from Black et al. (2023). As illustrated in Appendix Table 4 and Figure\n6, AlignProp adeptly demonstrates its capacity to interpolate between distinct reward functions,\nachieving the highest overall reward when the mixing coefficient is set to 0.5.\nMethod\nFidelity\nImage-text Alignment\nStable Diffusion\n20.8%\n34.4%\nAlignProp (Ours)\n79.2%\n65.6%\nReFL\n28.8%\n35.0%\nAlignProp (Ours)\n71.2%\n65.0%\nDDPO\n17.6%\n36.4%\nAlignProp (Ours)\n82.4%\n63.6%\nTable 2:\nWe showcase Human Evaluation\nScores, with the percentage indicating the fre-\nquency with which humans preferred the gener-\nated images from AlignProp over the baselines\nMixing 0.5\nAesthetic Model\nCompression Model \nFigure 6: Here, we depict the image generations\nproduced by the Aesthetic Model, Compression\nModel, and a hybrid model formed by averaging\nthe parameters of both. The mixing coefficient for\nthis combined model is set at 0.5\nAblations:\nIn Table 3, we analyze various design choices made throughout our experiments. We\ndo our ablations while using HPSv2 as our reward function. Firstly, we explore different choices for\n\u2018K\u2019 in truncated backpropagation. Subsequently, we investigate the impact of using EDICT affine\ncoupling layers to reduce computational memory. EDICT, as described in Wallace et al. (2023),\nrevises the forward and backward diffusion processes through affine coupling layers. However, we\nobserved that EDICT layers significantly slow down the process (by a factor of 2) compared to simple\ngradient checkpointing and also yield subpar performance. Additionally, we evaluate the choice\nbetween LoRA and the complete UNet fine-tuning strategy. Finally, we assess different scheduling\nmethods, comparing DDIM with Ancestral sampling.\n6\nLIMITATIONS\nAvg. Reward\nAlignProp (Ours)\n3.30\nwith K=1\n2.91\nwith K=10\n3.14\nwith EDICT\n2.95\nw/o LoRA\n3.09\nw/o DDIM\n2.97\nTable 3: We compute the average reward\nfor both the proposed method, Align-\nProp, and the baseline models after com-\npleting 100 training epochs. The table\ndemonstrates that the proposed method\nconsistently outperforms all other base-\nlines in terms of achieved rewards.\nAlignProp leverages gradients derived from the reward\nfunction to finetune diffusion models enhancing both sam-\npling efficiency and computational efficacy. However, this\nreliance on a differentiable reward function introduces a\ncaveat. In instances where the reward function is imper-\nfect, the fine-tuning process may lead to over-optimization,\ndeviating from the intended outcome. Addressing and mit-\nigating this risk of over-optimization stands as a direct\nfocus for future research.\n7\nCONCLUSION\nWe have introduced AlignProp, a differentiable framework\nfor fine-tuning pre-trained diffusion models, optimizing\ndirectly across a spectrum of diverse reward functions.\nBy conceptualizing the denoising process as a recurrent,\ndifferentiable policy responsible for mapping noise and\nprompt-conditioning information to output images, we\nhave introduced a class of pathwise gradient algorithms that surpass existing reinforcement methods.\nTo address concerns related to over-optimization, AlignProp incorporates truncated backpropagation\nand manages memory requirements through low-rank adapter modules and gradient checkpointing.\nOur experiments consistently highlight AlignProp\u2019s efficacy in optimizing diverse reward functions,\neven for tasks that prove challenging to specify through prompts alone. Looking ahead, future\nresearch avenues may explore the extension of these concepts to diffusion-based language models,\nwith the aim of enhancing their alignment with human feedback.\n9\nPreprint. Under review.\nREFERENCES\nAnurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is\nconditional generative modeling all you need for decision-making?, 2023.\nKevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models\nwith reinforcement learning. arXiv preprint arXiv:2305.13301, 2023.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. CoRR, abs/1604.06174, 2016. URL http://arxiv.org/abs/1604.06174.\nCheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran\nSong. Diffusion policy: Visuomotor policy learning via action diffusion, 2023.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In\nM. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Ad-\nvances in Neural Information Processing Systems, volume 34, pp. 8780\u20138794. Curran Asso-\nciates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/\n2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao,\nJipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative\nfoundation model alignment, 2023.\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for\ncompositional text-to-image synthesis, 2023.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion, 2022.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022.\nAnirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback:\nLearning a transition operator as a stochastic recurrent net. Advances in Neural Information\nProcessing Systems, 30, 2017.\nAudrunas Gruslys, R\u00e9mi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient\nbackpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/\nabs/1606.03401.\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation,\n2022.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applications, 2021. URL https://openreview.\nnet/forum?id=qw8AKxfYbI.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. CoRR,\nabs/2006.11239, 2020a. URL https://arxiv.org/abs/2006.11239.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems, 33:6840\u20136851, 2020b.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P.\nKingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High\ndefinition video generation with diffusion models, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.\nFleet. Video diffusion models, 2022b.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n10\nPreprint. Under review.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion, 2023.\nKimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel,\nMohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human\nfeedback, 2023.\nAlexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion\nmodel is secretly a zero-shot classifier. arXiv preprint arXiv:2303.16203, 2023.\nZhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, and Weizhu\nChen. Text generation with diffusion language models: A pre-training approach with continuous\nparagraph denoise, 2023.\nJustin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Weinberger. Latent\ndiffusion for language generation. arXiv preprint arXiv:2212.09462, 2022.\nM Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran,\nA Arnab, M Dehghani, Z Shen, et al. Simple open-vocabulary object detection with vision\ntransformers. arxiv 2022. arXiv preprint arXiv:2205.06230.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback, 2022.\nTim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu,\nSergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin.\nImitating human behaviour with diffusion models, 2023.\nMihir Prabhudesai, Tsung-Wei Ke, Alexander Cong Li, Deepak Pathak, and Katerina Fragkiadaki.\nTest-time adaptation with diffusion models. In ICML 2023 Workshop on Structured Probabilistic\nInference {\\&} Generative Modeling, 2023.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. URL\nhttps://arxiv.org/abs/2102.12092.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. URL https:\n//api.semanticscholar.org/CorpusID:248097655.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion\nmodels with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=08Yk-n5l2Al.\nC Schuhmann.\nLaoin aesthetic predictor.\n2022.\nURL https://laion.ai/blog/\nlaion-aesthetics/.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.\nDeterministic policy gradient algorithms. In Eric P. Xing and Tony Jebara (eds.), Proceedings of\nthe 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine\nLearning Research, pp. 387\u2013395, Bejing, China, 22\u201324 Jun 2014. PMLR. URL https://\nproceedings.mlr.press/v32/silver14.html.\n11\nPreprint. Under review.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video:\nText-to-video generation without text-video data, 2022.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. CoRR,\nabs/2009.01325, 2020. URL https://arxiv.org/abs/2009.01325.\nCorentin Tallec and Yann Ollivier. Unbiasing truncated backpropagation through time. arXiv preprint\narXiv:1705.08209, 2017.\nShuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Kraehenbuehl. Language\nconditioned traffic generation, 2023.\nBram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimiza-\ntion improves classifier guidance, 2023.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\ninference time. In International Conference on Machine Learning, pp. 23965\u201323998. PMLR,\n2022.\nXiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.\nHuman preference score v2: A solid benchmark for evaluating human preferences of text-to-image\nsynthesis. arXiv preprint arXiv:2306.09341, 2023a.\nXiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better\naligning text-to-image models with human preference, 2023b.\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagereward: Learning and evaluating human preferences for text-to-image generation, 2023.\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten\nKreis. Lion: Latent point diffusion models for 3d shape generation, 2022.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020.\n12\nPreprint. Under review.\nA\nAPPENDIX\nAlignProp\nPrompt\n\u201cThe black motorcycle is \nparked on the sidewalk.\u201d\n\u201cA toy elephant is sitting inside \na wooden car toy.\u201d\n\u201cA bunch of people posing \nwith some bikes..\u201d\n\u201cTwo small planes sitting near \neach other on a run way.\u201d\n\u201cA man sitting at a table in \nfront of bowls of spices.\u201d\n\u201cResidential bathroom with \ncommode and shower and \nplain white walls.\u201d\nStable Diffusion\nPrompt\nStable Diffusion\nAlignProp\nFigure 7: In this Figure we qualitatively compare AlignProp with Stable diffusion while using\nmultiple prompts of HPS v2 evaluation prompt dataset. As can be seen AlignProp achieves higher\nfidelity results with better image-text alignment.\nAesthetic\nCompressibility\nAesthetic + Compressibility\nCompressibility Model\n5.32\n8.32\n6.82\nMixing 0.5\n6.61\n7.92\n7.27\nAesthetic Model\n7.13\n5.57\n6.35\nTable 4: In the above Table, we compare the mixing results between Compressibility model and\nAesthetic Model. As can be seen from the results, mixing the weights achieves the best results when\nthe metric accounts for an average of both the scores. Additionally it achieves second best results on\nthe individual metrics, thus indicating that mixing does result in an interpolation in the reward space.\nMixing 0.5\nAesthetic  \nModel\nCompressibility  \nModel\nFigure 8: In this Figure we show additional mixing results between compressibilty and Aesthetic\nmodel.\nImplementation details\nWe use 4 A100 GPUs for all our experiments, with Adam Optimizer and\na learning rate of 1e-3. We build our codebase on top of Pytorch 2.0. Few reward functions such as\n13\nPreprint. Under review.\naesthetics that are not text-conditioned and are trained on relatively small datasets can result in a\nloss of image-text alignment if optimized above a specific score value. To prevent this we do early\nstopping (10th epoch in the training process).\n14\n"
  },
  {
    "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
    "link": "https://arxiv.org/pdf/2310.00704.pdf",
    "upvote": "15",
    "text": "Under review as a conference paper at ICLR 2024\nUNIAUDIO: AN AUDIO FOUNDATION MODEL TOWARD\nUNIVERSAL AUDIO GENERATION\nDongchao Yang1\u2217, Jinchuan Tian2\u2217, Xu Tan3\u2020, Rongjie Huang4, Songxiang Liu, Xuankai Chang2,\nJiatong Shi2, Sheng Zhao3, Jiang Bian3, Xixin Wu1, Zhou Zhao4, Shinji Watanabe2, Helen Meng1\u2020\n1 The Chinese University of Hong Kong, 2 Carnegie Mellon University,\n3 Microsoft Research Asia, 4 Zhejiang University\ndcyang@se.cuhk.edu.hk, jinchuat@andrew.cmu.edu\nABSTRACT\nLarge Language models (LLM) have demonstrated the capability to handle a vari-\nety of generative tasks. This paper presents the UniAudio system, which, unlike\nprior task-specific approaches, leverages LLM techniques to generate multiple\ntypes of audio (including speech, sounds, music, and singing) with given input\nconditions. UniAudio 1) first tokenizes all types of target audio along with other\ncondition modalities, 2) concatenates source-target pair as a single sequence, and\n3) performs next-token prediction using LLM. Also, a multi-scale Transformer\nmodel is proposed to handle the overly long sequences caused by the residual\nvector quantization-based neural codec in tokenization. Training of UniAudio is\nscaled up to 165K hours of audio and 1B parameters, based on all generative tasks,\naiming to obtain sufficient prior knowledge not only in the intrinsic properties of\naudio but also the inter-relationship between audio and other modalities. Therefore,\nthe trained UniAudio model has the potential to become a foundation model for\nuniversal audio generation: it shows strong capability in all trained tasks and can\nseamlessly support new audio generation tasks after simple fine-tuning. Experi-\nments demonstrate that UniAudio achieves state-of-the-art or at least competitive\nresults on most of the 11 audio generation tasks. Demo and code are released.1 .\n1\nINTRODUCTION\nAudio generation is an important component of generative AI. Recently, the popularity of generative\nAI has induced increasingly emergent and varying needs in audio generation: audio is expected to\nbe generated based on humans\u2019s demands, such as speech synthesis (TTS), voice conversion (VC),\nsinging voice synthesis (SVS), text-to-sound, and text-to-music. Prior works on audio generation\ntasks are commonly task-specific: their designs heavily leverage domain knowledge and their usage\nis restricted to fixed setups (Tan et al., 2021; Luo & Mesgarani, 2019; Zmolikova et al., 2023; Huang\net al., 2021b; Cho et al., 2021). Instead of taking care of each task independently, this work is an\nattempt to achieve universal audio generation, which intends to accomplish multiple audio generation\ntasks with only one unified model. The universal audio generation model is expected to obtain\nsufficient prior knowledge in audio and related modalities, which has the potential to provide simple\nand effective solutions for the increasing needs of generating diverse types of audio.\nThe superiority of Large Languge Models (LLM) in text-generative tasks inspires a series of LLM-\nbased models in audio generation (Wang et al., 2023a; Kharitonov et al., 2023; Huang et al., 2023b;\nAgostinelli et al., 2023; Borsos et al., 2023). Among these works, LLM\u2019s capability in independent\ntasks has been extensively studied in tasks like text-to-speech (TTS) (Wang et al., 2023a; Kharitonov\net al., 2023; Huang et al., 2023b) and music generation (Agostinelli et al., 2023; Copet et al., 2023),\nand achieves competitive performance. However, LLM\u2019s ability to process multiple tasks with a\nunified model is less exploited in audio generation research: most existing LLM-based works are\nstill designed for single tasks (Wang et al., 2023a; Kharitonov et al., 2023). We argue that achieving\nuniversality and versatility in audio generation through the LLM paradigm is promising but has not\nyet been comprehensively studied before this work.\n1https://uniaudio666.github.io/demo_UniAudio/\n* Equal contribution; \u2020 Corresponding author\n1\narXiv:2310.00704v5  [cs.SD]  11 Dec 2023\nUnder review as a conference paper at ICLR 2024\nToward universal audio generation, this work presents UniAudio, which adopts LLM techniques\nand is able to generate multiple types of audio (speech, sounds, music, and singing) conditioned on\nvarious input modalities, such as phoneme sequences, textual descriptions, and audio itself. The\nproposed UniAudio is mainly featured as follows: First, all types of audio, along with all other input\nmodalities, are tokenized as discrete sequences. Specifically, a universal neural codec model is built to\neffectively tokenize audio regardless of the audio type, and other tokenizers are used to tokenize other\ndifferent modalites. Then, UniAudio concatenates the source-target pair as a single sequence. Lastly,\nUniAudio performs next-token prediction using LLM. The residual vector quantization (Zeghidour\net al., 2021) based on neural codecs is used in the tokenization process, resulting in overly long token\nsequences (one frame corresponding to multiple tokens) that cannot be processed efficiently by LLM.\nA multi-scale Transformer architecture is designed to reduce computational complexity by modeling\nthe inter- and intra-frame correlation separately. Specifically, a global Transformer module is used to\nmodel the inter-frame correlation (e.g. semantic level), and a local Transformer module is used to\nmodel the intra-frame correlation (e.g. acoustic level).\nTo demonstrate the scalability of UniAudio for new tasks, the building process of UniAudio takes\ntwo stages. Firstly, the proposed UniAudio is trained on multiple audio generation tasks jointly,\nwhich allows the model to obtain sufficient prior knowledge not only of the intrinsic properties of\naudio but also of the interrelationship between audio and other input modalities. Secondly, through\nfine-tuning, the trained model can seamlessly support more unseen audio generation tasks. Thus,\nUniAudio has the potential to become a foundation model for universal audio generation: it is able to\ncontinuously support emergent needs in audio generation. Experimentally, our UniAudio supports 11\naudio generation tasks: the training stage includes 7 audio generation tasks, while 4 tasks are further\nadded in the fine-tuning stage. The building process of UniAudio is scaled up to 165k hours of audio\nand 1B parameters. Among the 11 tasks, UniAudio consistently obtains competitive performance in\nboth objective and subjective evaluations. State-of-the-art results are even achieved on most of these\ntasks. Further investigation suggests that training multiple tasks simultaneously in the training stage\nis mutually beneficial to each task involved. In addition, UniAudio can effectively adapt to new audio\ngeneration tasks and outperform task-specific models with a non-trivial gap.\nTo sum up, this work reveals that building universal audio generation models is necessary, promising,\nand beneficial. The main contributions of this work are summarized as follows:\n(1) Toward universal audio generation, UniAudio is presented as a unified solution for 11 audio\ngeneration tasks.\n(2) Per methodology, UniAudio provides novel approaches for (i) sequential representations of audio\nand other input modalities; (ii) uniform formulation for LLM-based audio generation tasks; and (iii)\nefficient model architecture specifically designed for audio generation.\n(3) Per experiments, the overall performance of UniAudio is well validated, and the benefits of\nbuilding a versatile audio generation model are verified by exhaustive experimental results.\n(4) Demo and code are released, in the hope that UniAudio can become a foundation model that\nsupports emergent audio generation in future research.\n2\nUNIAUDIO\nThis section introduces the technical details of the proposed UniAudio. Section 2.1 explains how\naudio and other modalities are tokenized. Then, all considered audio generation tasks are uniformly\nformulated in Section 2.2. Subsequently, the multi-scale Transformer architecture is proposed in\nSection 2.3 to handle the overly long sequence challenge caused by the adoption of neural codecs.\n2.1\nTOKENIZATION\nLLM are commonly used for sequential modeling, so audio and all other input modalities are\ntokenized before being processed. These processes for each modality are completed by independent\nmodules. All of these modules are fixed in the optimization of UniAudio or parameter-free.\n2.1.1\nAUDIO\nFor all audio generation tasks considered in this work, audio, regardless of its types (speech, sounds,\nmusic, or singing), is the target to predict. Instead of modeling different types of audio separately,\nUniAudio intends to tokenize all types of audio as a single and unified modality (even though they\n2\nUnder review as a conference paper at ICLR 2024\ncommonly have distinct patterns, such as frequency span), which requires a model that is well-suited\nto mapping all audio types into a shared latent space. Following Wang et al. (2023a); Kharitonov\net al. (2023), neural codec models (D\u00e9fossez et al., 2022; Yang et al., 2023b; Kumar et al., 2023) are\nused in this work for audio tokenization. An audio signal of duration d with sample rate fs can be\nrepresented by a sequence x \u2208 [\u22121, 1]d\u2217fs. An audio neural codec intends to compress x and then\nrecover it as \u02c6x using an encoder-decoder architecture with a quantization module:\nh = Encoder(x) \u2208 RT \u2217L;\n\u02c6h = Quantization(h);\n\u02c6x = Decoder(\u02c6h)\n(1)\nwhere T denotes the number of audio frames after down-sampling in the encoder, and L denotes the\nfeature dimension of the encoder. The discrete representations of audio are the intermediate product\nof the quantization process. Given any frame of hidden output ht, the integer vector zt = [z1\nt , ..., znq\nt ]\nis generated by Residual Vector Quantization (RVQ) (Zeghidour et al., 2021), where nq denotes the\nnumber of vector quantization layers. Iteratively, each element zk\nt is the index among all pre-learned\nand fixed k-th level quantizer vectors {q\u2217\nk} that has the smallest L2 distance to the residual between\nht and the sum of all previous chosen quantizer vectors {qzj\nt\nj , j = 1, ..., k \u2212 1}. With the discrete\nrepresentation zt, \u02c6ht is reconstructed as a close estimation of ht that can be used to recover xt with\nthe decoder.\nzk\nt = arg min\nm\nDistance(ht \u2212\nk\u22121\nX\nj=1\nqzj\nt\nj , qm\nk );\n\u02c6ht =\nnq\nX\nj=1\nqzj\nt\nj ;\n1 \u2264 k \u2264 nq\n(2)\nThe discrete representation of all audio frames z \u2208 ZT \u00d7nq is a matrix and needs to be converted into\na sequence before being processed by LM: it is simply flattened as a sequence, in which every nq\nelement for one frame is consecutive. Without specifically stated, we set nq = 3 in our experiments.\nAs the waveform can be recovered from z with a neural codec decoder, the rest of this paper mainly\ndiscusses how to predict the audio token sequence z using LLM techniques. As UniAudio intends to\ngenerate both speech and non-speech content, we build the codec model on our own and with broader\ndata coverage. Details of our codec configuration is in Appendix E.\n2.1.2\nOTHER MODALITIES\nBesides audio, other modalities considered in UniAudio also need to be represented as sequences.\nIn addition, most of these sequences are transformed into discrete ones through tokenization. The\nserialization and tokenization of these input modalities, along with their key features, are briefly\nsummarized as below.\nPhoneme: Phonemes are the basic units of speech pronunciation in linguistics. Phoneme sequences\nhave multiple sources: (1) when only text is available, phoneme sequence without duration informa-\ntion can be obtained by text-to-phoneme mapping using a pronunciation dictionary; (2) when only\nspeech is available, phoneme sequence with duration information is obtained by beam search of the\nDNN-HMM system (Hinton et al., 2012); (3) when both text and speech are available, phoneme\nsequence with duration information is obtained by forced alignment of the DNN-HMM system 2.\nMIDI: MIDI (Zhang et al., 2022) is widely used for singing voice synthesis tasks. F0 and duration\ninformation are included in the MIDI. We use the duration information to flatten the F0 sequence, so\nthat the frame-level F0 sequence is obtained.\nText: Text acts as a effective carrier of human instructions in audio generation tasks (Yang et al.,\n2023a; Copet et al., 2023). In this work, these textual instructions are represented as continuous\nembeddings derived from pre-trained text LLM (Raffel et al., 2020), as these embeddings contain\nrich textual semantics. Processing these continuous embeddings with LLM is further clarified in\nSection 2.33.\nSemantic Token: The semantic tokens are derived from the continuous embeddings output by audio\nself-supervised learning (SSL) models. These continuous representations are highly informative and\n2CMUDict (http://www.speech.cs.cmu.edu/cgi-bin/cmudict) is adopted as the pronunciation dict; kaldi recipe\n(https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech/s5/local/chain/run_tdnn.sh) is adopted to build the\ndeep neural network-hidden Markov model (DNN-HMM) system.\n3The encoder of T5 (https://github.com/google-research/text-to-text-transfer-transformer) is used to extract\nthe continuous text embeddings.\n3\nUnder review as a conference paper at ICLR 2024\ncan be adopted in both speech understanding (Rubenstein et al., 2023) and generative tasks (Borsos\net al., 2023). Following Huang et al. (2023b), these continuous representations are tokenized by\nperforming K-means clustering (Hsu et al., 2021) over these continuous representations. Since the\ncontinuous representations are frame-level, the semantic tokens also encode duration information4.\n2.2\nUNIFIED TASK FORMULATION\nTable 1: Sequence formats of all tasks supported by UniAudio. Text color represents modality. black:\naudio; green: phoneme; blue: MIDI; purple: text; brown: semantic token. \u2663 means tasks that\ngenerate audio with deterministic length. \u2662: means tasks that are only included in the fine-tuning\nstage. The speaker prompt is a 3-second speech and is used to represent the speaker identification.\nTask\nConditions\nAudio Target\nText-to-Speech (TTS) (Wang et al., 2023a)\nphoneme, speaker prompt\nspeech\nVoice Conversion (VC) \u2663 (Wang et al., 2023e)\nsemantic token, speaker prompt\nspeech\nSpeech Enhancement (SE) \u2663 (Wang et al., 2023b)\nnoisy speech\nspeech\nTarget Speech Extraction (TSE) \u2663 (Wang et al., 2018)\nmixed speech, speaker prompt\nspeech\nSinging Voice Synthesis (SVS) (Liu et al., 2022)\nphoneme (with duration), speaker prompt, MIDI\nsinging\nText-to-Sound (Sound) (Yang et al., 2023c)\ntextual description\nsounds\nText-to-Music (Music) (Agostinelli et al., 2023)\ntextual description\nmusic\nAudio Edit (A-Edit) \u2663\u2662 (Wang et al., 2023d)\ntextual description, original sounds\nsounds\nSpeech dereverberation (SD) \u2663\u2662 (Wu et al., 2016)\nreverberant speech\nspeech\nInstruct TTS (I-TTS)\u2662 (Guo et al., 2023)\nphoneme, textual instruction\nspeech\nSpeech Edit (S-Edit) \u2662 (Tae et al., 2021)\nphoneme (with duration), original speech\nspeech\nFor all tasks considered in UniAudio, the target audio is generated based on given conditions. With\nthe same target modality, i.e., audio, it is the conditions that define different audio generation tasks.\nHowever, even with the variance in conditions, all tasks can still be uniformly formulated as sequential\nmodeling tasks that can be processed by LLM: both the target audio and the conditions are first\ntransformed as sub-sequences and spliced as [conditions, target] sequences to be processed.\nUniAudio supports 11 audio generation tasks in total. The sequential formats of each task are defined\nin Table 1, in which the sub-sequences of all modalities are derived as in Section 2.1. However,\ndue to the unique configurations of each task, some of the condition sub-sequences are subject to\ntask-specific pre-processing operations during the tokenization. For audio, these operations are\nmainly for data corruption, such as adding noise, reverberation, and speech mixed with other speakers\nin the raw audio before tokenization. For phoneme and semantic tokens, duration information is\nreserved by default but can also be removed. For singing voice synthesis and speech edit tasks, the\nduration information of phoneme is used. For TTS and I-TTS tasks, the duration information is not\nused. For MIDI, the duration information is used repeat the F0 sequence. For text embeddings, no\noperations are applied in this work.\nTo avoid ambiguity, some special discrete tokens (enclosed by <>) are inserted to indicate (1) the start\nand end of the whole sequence; (2) the start and end of each sub-sequence of a certain modality; and\n(3) the task identifier. For example, for a text-to-sound task sequence that generates target audio based\non textual description, the whole sequence is like: <start> <sound_task> <text_start> text_sequence\n<text_end> <audio_start> audio_sequence <audio_end> <end>.\n2.3\nMULTI-SCALE TRANSFORMER\nPrevious work on LLM-based audio generation (Copet et al., 2023) advocates to modeling the discrete\naudio tokens as flattened sequences. If so, these sequences are processed in the length of T \u00d7 nq,\nwhich is highly challenging considering the quadratic space complexity of Transformer (Vaswani\net al., 2017) with respect to the lengths. Inspired by Yu et al. (2023), a multi-scale Transformer\narchitecture is specifically designed for discrete audio sequences, which is a hierarchical model that\nprocesses the inter- and intra-frame correlation by global and local Transformer modules separately.\nAn overview of the proposed architecture is in Figure 1. Instead of processing the whole flattened\nsequence token-by-token like prior works (Kharitonov et al., 2023), the multi-scale transformer\n4The 9-th layer hidden output of Hubert (Hsu et al., 2021) is adopted as the semantic token representations\n(https://github.com/facebookresearch/fairseq/hubert). The number of clusters for K-means is 500.\n4\nUnder review as a conference paper at ICLR 2024\nAudio\nPhoneme\nMIDI\nText\nTokenization for Multiple Modalities\nSemantic\nToken\nCondition Sequences\nTarget Audio Sequence\nTTS\nVC\nSE\nTSE\n...\nTask Formulation for Multiple Tasks\nGlobal Transformer\nPredicted Audio Sequence\nMulti-Scale Transformer\n<e>\n<e>\nLocal\nTransformer\nLocal\nTransformer\nLocal\nTransformer\n<e>\n<e>\n<e>\nFigure 1: Overview of UniAudio (left) and multi-scale Transformer architecture (right). <e> represent\nthe end of the sequence. zk\nt denotes the k-th audio token at t-th frame.\nconsiders patches (i.e., every consecutive nq token) as the global modeling units and then handles the\ntokens within each patch locally. Note that both the global and local Transformers are causal.\nFor audio token sequences, each patch accounts for nq consecutive audio tokens that exactly represent\none audio frame. First, as suggested in Equation 2, regardless of the exact choices of each quantization\nvector qz\u2217\nt\n\u2217 , it is the summed quantization vector \u02c6ht that is used to represent the audio frame. Thus,\nin the embedding stage, each patch (a.k.a., frame) is represented by the summed vector of the\ncorresponding embeddings before entering the global Transformer. Second, the global Transformer\nis to predict audio frame-by-frame: to predict the frame xt, it outputs the continuous representations\nthat include frame xt\u22121 and all previous content. These continuous representations will be further\nprocessed by the local Transformer. Third, also as in Equation 2, given the hidden representation\nht, the acquisition of zt is independent of any hidden output other than ht. Inspired by this, it is\nreasonable to predict the discrete tokens for frame xt, a.k.a., patch zt, only with the hidden output\nof global Transformer corresponding to frame xt\u22121. To be more detailed, as the acquisition of each\ntoken zk\nt is auto-regressively dependent on its prior tokens {zj\nt |j < k}, a local Transformer is adopted\nto predict the patch sequence zt in auto-regressive style. During this process, the corresponding\nvector output by the global transformer acts as a patch-level context, which is linearly transformed\nand then added to the embedded results of each token zk\nt .\nThe proposed multi-scale Transformer architecture is also compatible with discrete and continuous\nsequences besides audio. For all discrete tokens except audio (phoneme, semantic, MIDI and special\ntokens), each token has independent semantics and thus should account for one patch. So these\ndiscrete tokens repeat for nq times to fill each patch. The continuous text embeddings are also\nrepeated for nq times for the same purpose. Additionally, their embedding process is replaced by\na linear transformation while their predicting targets for local Transformer are consecutive special\ntokens <continuous_token>.\nThe design of the proposed multi-scale Transformer can effectively reduce computational complexity.\nFirst, the equivalent sequence length for the global Transformer is reduced from T \u00d7 nq to T, which\nmakes the global modeling cost independent to nq and thus the adoption of a larger nq becomes\nfeasible. Second, the intra-patch computation to generate the discrete tokens for each frame is\noffloaded to the local Transformer. The computation on the local transformer is comparatively light\nsince it only processes the very short sequence (fixed to the length of nq) and empirically has fewer\nparameters than the global Transformer by design.\n3\nEXPERIMENTS\nThis section first introduces the experimental setup in Section 3.1. The results for the training stage\nand the fine-tuning stage are presented in Section 3.2 and 3.3 respectively. Ablation studies are\npresented in Section 3.4.\n3.1\nEXPERIMENTAL SETUP\nData and Model: UniAudio is built on labeled datasets. Specifically, 12 datasets are adopted in this\nwork, all of which are publicly available. The overall audio volume is 165K hours. Detailed data\nstatistics and their adoption for each task are in Appendix A.1. Discrete tokens from all modalities\n5\nUnder review as a conference paper at ICLR 2024\nTable 2: Performance evaluation for UniAudio and selected prior works in the training stage\nTask\nModel\nObjective Evaluation\nSubjective Evaluation\nMetrics\nResults\nMetrics\nResults\nText-to-Speech\nShen et al. (2023)\nSIM(\u2191) / WER(\u2193)\n0.62 / 2.3\nMOS(\u2191)\n/ SMOS(\u2191)\n3.83\u00b10.10 / 3.11\u00b10.10\nUniAudio\n0.71 / 2.0\n3.81\u00b10.07 / 3.56\u00b10.10\nVoice\nConversion\nWang et al. (2023e)\nSIM(\u2191) / WER(\u2193)\n0.82 / 4.9\nMOS(\u2191)\n/ SMOS(\u2191)\n3.41\u00b10.08 / 3.17\u00b10.09\nUniAudio\n0.87 / 4.8\n3.54\u00b10.07 / 3.56\u00b10.07\nSpeech\nEnhancement\nRichter et al. (2023)\nPESQ(\u2191) / VISQOL(\u2191)\n/ DNSMOS(\u2191)\n3.21 / 2.72 / 3.29\nMOS(\u2191)\n3.56\u00b10.08\nUniAudio\n2.63 / 2.44 / 3.66\n3.68\u00b10.07\nTarget Speaker\nExtraction\nWang et al. (2018)\nPESQ(\u2191) / VISQOL(\u2191)\n/ DNSMOS(\u2191)\n2.41 / 2.36 / 3.35\nMOS(\u2191)\n3.43\u00b10.09\nUniAudio\n1.88 / 1.68 / 3.96\n3.72\u00b10.06\nSinging Voice\nSynthesis\nLiu et al. (2022)\n-\n-\nMOS(\u2191)\n/ SMOS(\u2191)\n3.94\u00b10.02 / 4.05\u00b10.06\nUniAudio\n4.08\u00b10.04 / 4.04\u00b10.05\nText-to-Sound\nLiu et al. (2023a)\nFAD (\u2193) / KL (\u2193)\n4.93 / 2.6\nOVL (\u2191)\n/ REL (\u2191)\n61.0\u00b11.9 / 65.7\u00b11.8\nUniAudio\n3.12 / 2.6\n61.9\u00b11.9 / 66.1\u00b11.5\nText-to-Music\nCopet et al. (2023)\nFAD (\u2193) / KL (\u2193)\n4.52 / 1.4\nOVL (\u2191)\n/ REL (\u2191)\n73.3\u00b11.5 / 71.3\u00b11.7\nUniAudio\n3.65 / 1.9\n67.9\u00b11.7 / 70.0\u00b11.5\nform a joint vocabulary of size 4212, including all special tokens. Vanilla Transformer decoder layers\nwith causality are consistently adopted in global and local Transformer. The overall parameter budget\nis roughly 1B. Detailed model configuration is in Appendix A.2. Existing neural codec models are\nsub-optimal for universal audio generation, mainly due to data coverage. An improved neural codec\nmodel is then built with fewer quantization levels nq, smaller frame-per-second rate, higher quality,\nand wider coverage (see Appendix E).\nTraining and Inference: The training stage includes 7 tasks while 4 new tasks are added in the\nfine-tuning stage. Table 1 specifies the tasks for fine-tuning only. Both the training and fine-tuning\nare completed with 16 AMD MI200-64G GPUs. The detailed configuration of optimization is in\nAppendix A.3. To retain the performance of previous tasks during fine-tuning, following Conneau\net al. (2020), the training data are re-sampled with respect to tasks with \u03b1 = 0.05. Top-k sampling is\nadopted consistently for inference, in which k and the temperature are set to 30 and 0.8, respectively.\nAs the global Transformer does not directly predict tokens, the sampling process only happens in the\nlocal Transformer inference.\nEvaluation: For evaluation, most tasks are evaluated using both objective and subjective metrics 5.\nGenerally, for objective evaluation, Word Error Rate (WER) is used to evaluate the intelligibility of\ngenerated speech; Similarity Score (SIM) is for similarity in terms of speaker identity6; Perceptual\nEvaluation of Speech Quality (PESQ), VISQOL7, DNSMOS 8 and Mel Cepstral Distortion (MCD)\nare signal-level quality metrics derived from human auditory research; Following (Copet et al., 2023),\nFr\u00e9chet Audio Distance (FAD), Kullback-Leiber (KL) Divergence, and Fr\u00e9chet Distance (FD) are for\naudio fidelity and audio similarity; For subjective evaluation, MOS and SMOS are adopted to provide\nhuman-centric judgment for speech and sing related tasks. For text-to-sound and text-to-music tasks,\nwe use overall quality (OVL), and relevance to the text input (REL) (Copet et al., 2023). Note all\nsubjective results are obtained from Amazon Mechanical Turk9 for fair comparison. Appendix F\nshows details of the subjective evaluation process.\n3.2\nTHE RESULTS OF 7 GENERATIVE TASKS IN THE TRAINING STAGE\nThis section presents the overall evaluation results of the proposed UniAudio model over all 7 audio\ngeneration tasks during the training stage. A comprehensive comparison is conducted between\nUniAuduio and multiple prior works on each task, including not only the LM-based methods but also\nthe diffusion model-based methods as well as other conventional audio generation methods. The\ndetailed comparison is presented in Appendix B. We selected one of the most advanced prior work in\neach task and present the results in Table 2.\n5Following the setting of DiffSinger (Liu et al., 2022), SVS tasks don\u2019t report the objective results\n6WER and SIM evaluation models follow Wang et al. (2023a)\n7https://github.com/google/visqol\n8https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS\n9https://www.mturk.com/\n6\nUnder review as a conference paper at ICLR 2024\nAs suggested in Table 2, UniAudio is a versatile system that can handle all 7 audio generation tasks\ntogether and achieve competitive performance. Per subjective evaluation, UniAudio surpasses the\nbaselines in 3 out of 6 tasks (TTS, VC, Sound); per objective evaluation, it achieves better results\non 5 out of the 7 tasks except SVS and Music. We also find UniAudio under-perform on several\nmetrics. UniAudio\u2019s subjective performance for SE and TSE is less competitive compared with its\ncompetitors, which is also observed in previous literature (Erdogan et al., 2023) that the signal-level\nevaluation metrics may not be suitable for LM-based generative methods. UniAudio cannot surpass\nthe selected competitor (Copet et al., 2023) in the Text-to-Music task. We note that (Copet et al.,\n2023) is built with more private labeled data than our UniAudio.\n3.3\nTHE RESULTS OF 4 GENERATIVE TASKS IN THE FINE-TUNING STAGE\nTable 3: Performance evaluation for UniAudio and selected prior works in the fine-tuning stage\nTask\nModel\nEvaluation\nMetrics\nResults\nAudio Edit\nAUDIT (Wang et al., 2023d)\nFD (\u2193) / KL (\u2193)\n20.78 / 0.86\nUniAudio\n17.78 / 0.77\nSpeech Dereverb.\nSGMSE+ Richter et al. (2023)\nPESQ(\u2191) / DNSMOS(\u2191)\n2.87 / 3.42\nUniAudio\n2.13 / 3.51\nInstructed TTS\nGroundTruth\nMOS(\u2191) / SMOS(\u2191)\n3.77\u00b10.07 / 3.85\u00b10.08\nUniAudio\n3.61\u00b10.09 / 3.71\u00b10.09\nSpeech Edit\nTTS system regeneration\nMCD(\u2193) / MOS(\u2191)\n6.98 / 3.69\u00b10.08\nUniAudio\n5.12 / 3.82\u00b10.06\nAs UniAudio is designed to continuously support new audio generation tasks, this section reports\nUniAudio\u2019s performance on unseen tasks. The model is obtained by fine-tuning over 4 new tasks\njointly and the results are presented in Table 3. Similar to section 3.2, for each task, we compare\nUniAudio\u2019s performance with one selected prior work and report the detailed results in Appendix B.\nAs shown in Table 3, the fine-tuned UniAudio model surpasses its baselines in audio edit and speech\ndereverberation and is approaching the ground-truth quality in the Instructed TTS task. For speech\nediting, UniAudio shows considerable improvement compared to generating the whole sentence.\n3.4\nABLATION STUDY\n3.4.1\nBENEFIT OF BUILDING UNIFIED AUDIO GENERATION MODEL\nTo further validate our claim that building a unified model for all 11 audio generation tasks is\npromising and beneficial, more ablation studies are conducted. In Appendix C.1, we demonstrate\nthat the joint-trained UniAudio model consistently outperforms the models that are trained for each\nspecific task10, regardless they are included in the training stage or the fine-tuning stage. In Appendix\nC.2, we additionally validate that fine-tuning over the 4 new audio generation tasks does not affect\nUniAudio\u2019s performance on the original 7 tasks. In Appendix C.3, we observe that UniAudio can\nconsistently benefit from increased training data volume of each task, which provides another reason\nto build universal audio generation models: these models are easier to scale up as the data collection\nis more feasible. We provide more discussion in Appendix D about the effectiveness of building a\nuniversal audio generation model.\n3.4.2\nTHE EFFECTIVENESS OF MULTI-SCALE TRANSFORMER MODEL\nAs in section 2.3, the adoption of neural codecs has become a popular choice of LLM-based audio\ngeneration but causes an overly long sequence issue that needs further consideration. This section\ncompares the proposed multi-scale Transformer with four representative approaches in this field:\nFlattening Prediction (e.g. SPEARTTS (Kharitonov et al., 2023)), Coarse first prediction (e.g.\nVALL-E (Wang et al., 2023a)), Parallel prediction (e.g. AudioGen (Kreuk et al., 2022)), and Delay\nprediction (e.g. MusicGen (Copet et al., 2023)). Figure 2 illustrates the prediction order of these five\narchitectures. Experiments are conducted on text-to-speech and text-to-music tasks and the results\nare reported in Table 4 and 5 respectively 11.\nAuto-Regression and Performance: Among all 4 baselines aforementioned, Copet et al. (2023)\nclaims that the flattening method provides the best audio generation quality. they further claim that\n10Note the task-specific models are built with the corresponding subset of the training data.\n11Results are based on unofficial implementations.\n7\nUnder review as a conference paper at ICLR 2024\nCoarse \ufb01rst\nParallel\n1\n1\nMulti-scale Transformer (Ours)\nDelay\n0\n0\n1\n0\n3\n2\n1\n6\n5\n4\n9\n8\n7\nA\nB\nC\nE\nD\nFlattening\nD\nFigure 2: Order of token prediction for 4 representative methods in audio generation (Copet et al.,\n2023) and the proposed multi-scale Transformer. Assume nq = 3 and T = 3. Current token\nprediction (red) is conditioned on prior tokens (in green). Tokens in orange are concurrently predicted\nwith the current token. 0 is a special token indicating empty positions in the delay prediction.\nTable 4: Model comparison among Coarse first, Flattening, Parallel, delay prediction, and multi-scale\nTransformer. Experiments were conducted on the LibriTTS. GPU memory and training time are\nobtained by a 20-second audio (average of 100 trials). All models have a similar parameter budget.\nStructure\nnq\nMOS (\u2191)\nMCD (\u2193)\nGPU Mem. (GB)\nTime (s) / Iter.\nCoarse first\n8\n3.48\u00b10.05\n7.37\n18.7\n0.58\nParallel\n3\n3.14\u00b10.07\n7.89\n13.56\n0.53\nDelay\n3\n3.48\u00b10.05\n6.95\n13.65\n0.59\nFlattening\n3\n3.80\u00b10.09\n6.56\n36.7\n1.63\nMulti-Scale Transformer (ours)\n3\n3.77\u00b10.05\n6.52\n19.4\n0.73\nMulti-Scale Transformer (ours)\n8\n3.84\u00b10.06\n6.27\n24.0\n1.10\nthe superior performance of flattening prediction is mainly attributed to the auto-regressive property;\nthe other three methods do not reserve this property as the concurrent prediction is introduced\n(see Fig. 2). Under the scenario of codec adoption, we reinterpret the auto-regressive property\nas: current token prediction is based on all tokens of previous frames and the previous tokens\nwithin the current frame, or formally, the prediction of the current token zk\nt is based on tokens:\n{zk\u2032\nt\u2032 |t\u2032 < t} \u222a {zk\u2032\nt\u2032 |t\u2032 = t, k\u2032 < k}. With this definition, we claim that the proposed multi-scale\ntransformer is also auto-regressive.\nAligned with Copet et al. (2023), our experiments also validate the importance of the auto-regressive\nproperty. As in Table 4 and 5, flattening prediction brings better generation quality than parallel,\ncoarse first, and delay prediction. Additionally, with the same auto-regressive property, our proposed\nmulti-scale transformer achieves a comparable performance with flattening prediction in terms of\ngeneration quality, which, again, validates the importance of auto-regression.\nEfficiency: Besides generation quality, efficiency is a major concern of audio generation. Although\nwith the auto-regressive property, the flattening prediction is sub-optimal in terms of efficiency: the\nmodeling is based on the T \u00d7 nq long sequence, which has a space complexity of O((T \u2217 nq)2) in\nself-attention. As increasing nq gives higher reconstruction quality at the cost of longer sequences and\nmore computation, this issue becomes more severe when a larger nq is adopted. Since the sequence\nlength grows proportionally with nq, we experimentally find it difficult to train with nq \u2265 4. By\ncontrast, the proposed multi-scale Transformer distributes the inter- and intra-frame modeling to the\nglobal and local sub-modules respectively, which thus alleviates the space complexity to O(T\u22172).\nFinally, without the requirement of auto-regression, methods like parallel, coarse first, and delay\npredictions achieve better efficiency due to the adoption of concurrent predictions. Since the space\ncomplexity is independent to nq, training a larger nq with the multi-scale transformer is then feasible.\n8\nUnder review as a conference paper at ICLR 2024\nTable 5: The ablation study to explore the effectiveness of our proposed multi-scale transformer.\nExperiments were conducted on text-to-music tasks with the Million Song dataset.\nStructure\nnq\nFAD (\u2193)\nKL (\u2193)\nOVL. (\u2191)\nREL. (\u2191)\nParallel\n3\n6.92\n2.36\n60.4\u00b12.3\n61.3\u00b11.5\nDelay\n3\n6.07\n2.23\n62.8\u00b11.9\n63.9\u00b11.6\nFlatten\n3\n5.18\n1.83\n64.8\u00b11.8\n65.2\u00b12.0\nMulti-Scale Transformer (ours)\n3\n5.24\n1.80\n64.4\u00b12.1\n66.2\u00b12.4\nExperimentally, the proposed multi-scale transformer considerably reduces the time and memory cost\ncompared with the flatting prediction. It still costs more time and memory compared with the other\nthree baselines.\nBased on the observations above, we claim that the proposed multi-scale transformer is an auto-\nregressive architecture that achieves a better trade-off between generation quality and efficiency.\n4\nRELATED WORKS\nThis work is an attempt to achieve universal audio generation through LLM-based techniques. There\nis a long research history for many audio generation tasks. Conventionally, the design of these tasks\nheavily leverages the domain knowledge of each specific task, and their workflows are distinctive\nfrom each other: For tasks like TTS, SE, TSE, TT-Music, VC, S-Edit, SD, SVS, (1) their neural\nnetwork architectures are based on Transformer (Ren et al., 2020) or others (Oord et al., 2016; Luo &\nMesgarani, 2019); (2) their training objectives can be either in time-domain (Luo & Mesgarani, 2019),\nfrequency-domain (Yu et al., 2017) or others (Gu et al., 2021; Shen et al., 2023); (3) their designs are\ninspired by and derived from linguistics and phonetics (Zen et al., 2013), signal processing (Griffin &\nLim, 1984), auditory perception (Shadle & Damper, 2001) and machine learning (Wang et al., 2016)\nresearch, etc; (4) they use different generative models, such as diffusion model (Shen et al., 2023;\nWang et al., 2023b), flow (Le et al., 2023), Seq2Seq (Ren et al., 2020; Liu et al., 2021).\nThe prosperity of LLM techniques (Radford et al., 2019; OpenAI, 2023) significantly promotes\nprogress in audio generation research in several directions. First, the large language models, along\nwith the prompt methods, inspired multiple emergent audio generation tasks that are based on textual\ninstruction or descriptions from humans, such as Instruct-TTS (Yang et al., 2023a), Text-to-sound\n(Kreuk et al., 2022; Huang et al., 2023a) and text-to-music Copet et al. (2023); Agostinelli et al.\n(2023). Second, besides the text, audio can also be tokenized as discrete sequences (Zeghidour et al.,\n2021; D\u00e9fossez et al., 2022; Kumar et al., 2023) that can be further processed by LMs. LM-based\naudio generative models then show superior capability in generalization towards unseen speakers\n(Wang et al., 2023a), low resources (Kharitonov et al., 2023) and multilingual (Zhang et al., 2023)\nscenarios. These methods also achieve state-of-the-art results in overall performance within their own\nscopes. Finally, the LM-like model can be further combined with existing generative models (e.g.,\ndiffusion models Rombach et al. (2022)) to obtain improved generation quality.\nIt is laborious to handle each audio generation task case-by-case, especially when considering the data\nshortage as well as the emergent and varying needs in this area. Alternatively, building a universal\naudio generation model is a promising and practical paradigm. Given the rapid progress in audio\ngeneration research, recent designs of audio generation, including LM-based ones, tend to support\nmultiple audio generation tasks simultaneously. Some pioneer works (Wang et al., 2023c; Le et al.,\n2023; Shen et al., 2023; Liu et al., 2023b; Jiang et al., 2023) clearly consider supporting multiple\ntasks as a key strength; the designs of other prior works (Borsos et al., 2023; Kharitonov et al., 2023;\nShen et al., 2023) do show the potential to generate audio in a broader sense than what they originally\nclaim. Following these pioneering research works, UniAudio supports an extended coverage of 11\naudio generation tasks in a unified LM-based model.\n5\nLIMITATION\nNot all known audio generation tasks are included in the proposed UniAudio, such as noise removal,\nnoise speech edit (Wang et al., 2023c) and speech-to-speech translation (Rubenstein et al., 2023;\nBarrault et al., 2023). All new tasks added in fine-tuning are formulated with the known modalities\nin the training stage; Introducing new modalities during fine-tuning is unexplored in this work.\nCurrent UniAudio considers neither unlabeled data nor domain-specific foundation models, which\n9\nUnder review as a conference paper at ICLR 2024\ncan possibly further improve the overall performance. The samples generated by UniAudio are not\nguaranteed in quality and may contain errors.\n6\nCONCLUSION\nTo handle the emergent and varying needs in audio generation, this work is an attempt to achieve\nuniversal audio generation. UniAudio is proposed as a unified LM-based generative model that\nsupports 11 different audio generation tasks. In experiments, the proposed UniAudio provides com-\npetitive performance on all 11 tasks. It also empirically demonstrates the capability of continuously\nintegrating unseen audio generation tasks. Demo and code are released, in the hope that UniAudio\ncan become a foundation model for universal audio generation in further research.\n7\nETHICAL STATEMENT\nWe are delving into the revolutionary field of generating diverse audio using large language model\ntechniques. We find ourselves at the confluence of innovation and responsibility. It is imperative to\nacknowledge the ethical dimensions of our work and ensure that our contributions are employed for\nthe betterment of society.\nBeing Open: As we advance in this domain, it\u2019s crucial to ensure that the benefits of this technology\nare widespread and not limited to a privileged few. Our code is released publicly along with this\nsubmission to ensure equal access for each person. All experiments are based on open-accessible\ndatasets that allow research-oriented comparison and reproduction.\nAvoid Misuse:\nWhile our model can produce a myriad of audio content ranging from music to\nspeech, there\u2019s potential for misuse in the generation of misinformation, deepfake audio, or any\nharmful content. We advocate for adopting our code and model responsibly, with full respect to\nindividual privacy and observance of regulations. Concerning the potential misuse of our model,\ncheckpoints will not be released.\nREFERENCES\nAndrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\nLo\u00efc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise\nDuquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. Seamlessm4t-\nmassively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596,\n2023.\nZal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,\nDominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a language\nmodeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2023.\nEdresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00f6lge, and\nMoacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for\neveryone. In International Conference on Machine Learning, pp. 2709\u20132720. PMLR, 2022.\nJun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, and Helen Meng. Fullsubnet+: Channel\nattention fullsubnet with complex spectrograms for speech enhancement. In ICASSP 2022-2022\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7857\u2013\n7861. IEEE, 2022.\nYin-Ping Cho, Fu-Rong Yang, Yung-Chuan Chang, Ching-Ting Cheng, Xiao-Han Wang, and Yi-Wen\nLiu. A survey on recent deep learning-driven singing voice synthesis systems. In 2021 IEEE\nInternational Conference on Artificial Intelligence and Virtual Reality (AIVR), pp. 319\u2013323. IEEE,\n2021.\nAlexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli.\nUnsupervised cross-lingual representation learning for speech recognition.\narXiv preprint\narXiv:2006.13979, 2020.\n10\nUnder review as a conference paper at ICLR 2024\nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and\nAlexandre D\u00e9fossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284,\n2023.\nAlexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\nKonstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset.\nIn ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 736\u2013740. IEEE, 2020.\nHakan Erdogan, Scott Wisdom, Xuankai Chang, Zal\u00e1n Borsos, Marco Tagliasacchi, Neil Zeghidour,\nand John R Hershey. Tokensplit: Using discrete speech representations for direct, refined, and\ntranscript-conditioned speech separation and recognition. arXiv preprint arXiv:2308.10415, 2023.\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for\naudio events. In 2017 IEEE international conference on acoustics, speech and signal processing\n(ICASSP), pp. 776\u2013780. IEEE, 2017.\nDaniel Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. IEEE\nTransactions on acoustics, speech, and signal processing, 32(2):236\u2013243, 1984.\nRongzhi Gu, Shi-Xiong Zhang, Yuexian Zou, and Dong Yu. Complex neural spatial filter: Enhancing\nmulti-channel target speech separation in complex domain. IEEE Signal Processing Letters, 28:\n1370\u20131374, 2021.\nZhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan. Prompttts: Controllable text-to-\nspeech with text descriptions. In ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\nXiang Hao, Xiangdong Su, Radu Horaud, and Xiaofei Li. Fullsubnet: A full-band and sub-band\nfusion model for real-time single-channel speech enhancement. In ICASSP 2021-2021 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6633\u20136637.\nIEEE, 2021.\nGeoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,\nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury. Deep\nneural networks for acoustic modeling in speech recognition: The shared views of four research\ngroups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012. doi: 10.1109/MSP.2012.2205597.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n29:3451\u20133460, 2021.\nRongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Multi-singer: Fast\nmulti-singer singing voice vocoder with a large-scale corpus. In Proceedings of the 29th ACM\nInternational Conference on Multimedia, pp. 3945\u20133954, 2021a.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin\nLiu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced\ndiffusion models. arXiv preprint arXiv:2301.12661, 2023a.\nRongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue\nJiang, Chao Weng, Zhou Zhao, and Dong Yu. Make-a-voice: Unified voice synthesis with discrete\nrepresentation. arXiv preprint arXiv:2305.19269, 2023b.\nTzu-hsien Huang, Jheng-hao Lin, and Hung-yi Lee. How far are we from robust voice conversion: A\nsurvey. In 2021 IEEE Spoken Language Technology Workshop (SLT), pp. 514\u2013521. IEEE, 2021b.\nZiyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie\nHuang, Chunfeng Wang, Xiang Yin, et al. Mega-tts: Zero-shot text-to-speech at scale with intrinsic\ninductive bias. arXiv preprint arXiv:2306.03509, 2023.\n11\nUnder review as a conference paper at ICLR 2024\nJacob Kahn, Morgane Rivi\u00e8re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel\nMazar\u00e9, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light:\nA benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669\u20137673. IEEE, 2020.\nEugene Kharitonov, Damien Vincent, Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Sertan Girgin, Olivier Pietquin,\nMatt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity\ntext-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023.\nChris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating\ncaptions for audios in the wild. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pp. 119\u2013132, 2019.\nTom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L Seltzer, and Sanjeev Khudanpur. A study on\ndata augmentation of reverberant speech for robust speech recognition. In 2017 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5220\u20135224. IEEE, 2017.\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi\nParikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv\npreprint arXiv:2209.15352, 2022.\nRithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-\nfidelity audio compression with improved RVQGAN. In Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023. URL https://openreview.net/forum?id=\nqjnl1QUnFA.\nMatthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson,\nVimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. Voicebox: Text-guided multi-\nlingual universal speech generation at scale. 2023. URL https://dl.fbaipublicfiles.\ncom/voicebox/paper.pdf.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv\npreprint arXiv:2301.12503, 2023a.\nHaohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu\nWang, Yuxuan Wang, and Mark D. Plumbley. AudioLDM 2: Learning holistic audio generation\nwith self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023b.\nJinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis\nvia shallow diffusion mechanism. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11020\u201311028, 2022.\nSongxiang Liu, Yuewen Cao, Disong Wang, Xixin Wu, Xunying Liu, and Helen Meng. Any-to-many\nvoice conversion with location-relative sequence-to-sequence modeling. IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 29:1717\u20131728, 2021.\nYen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao. Con-\nditional diffusion probabilistic model for speech enhancement. In ICASSP 2022-2022 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7402\u20137406.\nIEEE, 2022.\nYi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for\nspeech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8):\n1256\u20131266, 2019.\nBrian McFee, Thierry Bertin-Mahieux, Daniel PW Ellis, and Gert RG Lanckriet. The million song\ndataset challenge. In Proceedings of the 21st International Conference on World Wide Web, pp.\n909\u2013916, 2012.\nXinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley,\nYuexian Zou, and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-labelled audio captioning\ndataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395, 2023.\n12\nUnder review as a conference paper at ICLR 2024\nAnnamaria Mesaros, Toni Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit Shah, Emmanuel\nVincent, Bhiksha Raj, and Tuomas Virtanen. Dcase 2017 challenge setup: Tasks, datasets and\nbaseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes\nand Events, 2017.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw\naudio. arXiv preprint arXiv:1609.03499, 2016.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2204.06125, 2023.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\nbased on public domain audio books. In 2015 IEEE international conference on acoustics, speech\nand signal processing (ICASSP), pp. 5206\u20135210. IEEE, 2015.\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and\nQuoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.\narXiv preprint arXiv:1904.08779, 2019.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A\nlarge-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast\nand high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020.\nJulius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, and Timo Gerkmann. Speech\nenhancement and dereverberation with diffusion-based generative models. IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nPaul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal\u00e1n Borsos,\nF\u00e9lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.\nAudiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925,\n2023.\nChristine H Shadle and Robert I Damper. Prospects for articulatory synthesis: A position paper. In\n4th ISCA tutorial and research workshop (ITRW) on speech synthesis, 2001.\nKai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang\nBian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing\nsynthesizers. arXiv preprint arXiv:2304.09116, 2023.\nYao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. Aishell-3: A multi-speaker mandarin tts\ncorpus and the baselines. arXiv preprint arXiv:2010.11567, 2020.\nJaesung Tae, Hyeongju Kim, and Taesu Kim. Editts: Score-based editing for controllable text-to-\nspeech. arXiv preprint arXiv:2110.02584, 2021.\nXu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint\narXiv:2106.15561, 2021.\n13\nUnder review as a conference paper at ICLR 2024\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nChristophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Cstr vctk corpus: English multi-\nspeaker corpus for cstr voice cloning toolkit. University of Edinburgh. The Centre for Speech\nTechnology Research (CSTR), 6:15, 2017.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech\nsynthesizers. arXiv preprint arXiv:2301.02111, 2023a.\nQuan Wang, Hannah Muckenhirn, Kevin Wilson, Prashant Sridhar, Zelin Wu, John Hershey, Rif A\nSaurous, Ron J Weiss, Ye Jia, and Ignacio Lopez Moreno. Voicefilter: Targeted voice separation\nby speaker-conditioned spectrogram masking. arXiv preprint arXiv:1810.04826, 2018.\nWen Wang, Dongchao Yang, Qichen Ye, Bowen Cao, and Yuexian Zou. Nadiffuse: Noise-aware\ndiffusion-based model for speech enhancement. arXiv preprint arXiv:2309.01212, 2023b.\nWenfu Wang, Shuang Xu, Bo Xu, et al. First step towards end-to-end parametric tts synthesis:\nGenerating spectral parameters with neural attention. In Interspeech, pp. 2243\u20132247, 2016.\nXiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen,\nMin Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. Speechx: Neural codec language model as\na versatile speech transformer. arXiv preprint arXiv:2308.06873, 2023c.\nYu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei\nXie, and Mengxiao Bi. Opencpop: A high-quality open source chinese popular song corpus for\nsinging voice synthesis. arXiv preprint arXiv:2201.07429, 2022.\nYuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, and Sheng Zhao. Audit: Au-\ndio editing by following instructions with latent diffusion models. arXiv preprint arXiv:2304.00830,\n2023d.\nZhichao Wang, Yuanzhe Chen, Lei Xie, Qiao Tian, and Yuping Wang. Lm-vc: Zero-shot voice\nconversion via speech generation based on language models. arXiv preprint arXiv:2306.10521,\n2023e.\nBo Wu, Kehuang Li, Minglei Yang, and Chin-Hui Lee. A reverberation-time-aware approach to\nspeech dereverberation based on deep neural networks. IEEE/ACM transactions on audio, speech,\nand language processing, 25(1):102\u2013111, 2016.\nDongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, and Dong\nYu. Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt.\narXiv preprint arXiv:2301.13662, 2023a.\nDongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou.\nHifi-codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint\narXiv:2305.02765, 2023b.\nDongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.\nDiffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 2023c.\nDong Yu, Morten Kolb\u00e6k, Zheng-Hua Tan, and Jesper Jensen. Permutation invariant training of\ndeep models for speaker-independent multi-talker speech separation. In 2017 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 241\u2013245. IEEE, 2017.\nLili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.\nMegabyte: Predicting million-byte sequences with multiscale transformers.\narXiv preprint\narXiv:2305.07185, 2023.\n14\nUnder review as a conference paper at ICLR 2024\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 30:495\u2013507, 2021.\nHeiga Zen, Andrew Senior, and Mike Schuster. Statistical parametric speech synthesis using deep\nneural networks. In 2013 ieee international conference on acoustics, speech and signal processing,\npp. 7962\u20137966. IEEE, 2013.\nHeiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu.\nLibritts: A corpus derived from librispeech for text-to-speech. arXiv preprint arXiv:1904.02882,\n2019.\nLichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, Rongjie\nHuang, Jieming Zhu, Xiao Chen, et al. M4singer: A multi-style, multi-singer and musical score\nprovided mandarin singing corpus. Advances in Neural Information Processing Systems, 35:\n6914\u20136926, 2022.\nZiqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing\nLiu, Huaming Wang, Jinyu Li, et al. Speak foreign languages with your own voice: Cross-lingual\nneural codec language modeling. arXiv preprint arXiv:2303.03926, 2023.\nKate\u02c7rina \u017dmol\u00edkov\u00e1, Marc Delcroix, Keisuke Kinoshita, Tsubasa Ochiai, Tomohiro Nakatani, Luk\u00e1\u0161\nBurget, and Jan \u02c7Cernock`y. Speakerbeam: Speaker aware neural network for target speaker\nextraction in speech mixtures. IEEE Journal of Selected Topics in Signal Processing, 13(4):\n800\u2013814, 2019.\nKaterina Zmolikova, Marc Delcroix, Tsubasa Ochiai, Keisuke Kinoshita, Jan \u02c7Cernock`y, and Dong\nYu. Neural target speech extraction: An overview. IEEE Signal Processing Magazine, 40(3):8\u201329,\n2023.\n15\nUnder review as a conference paper at ICLR 2024\nAppendices\nA\nEXPERIMENTAL SETUP\nThis appendix describes experimental setups in detail, including data statistics, model architecture\nand optimization strategy.\nA.1\nDATA DESCRIPTION\n12 public datasets are adopted in this work for training. Besides, several test sets are additionally used\nonly for zero-shot evaluation. The statistics of these datasets are in Table 6. Datasets adoption for\neach task is described in Table 7. Note some datasets are adopted by more than one task.\nTable 6: Data statistics\nDataset\nType\nAnnotation\nVolume (hrs)\nTraining\nLibriLight (Kahn et al., 2020)\nspeech\n-\n60k\nLibriTTS (Zen et al., 2019)\nspeech\ntext\n1k\nMLS (Pratap et al., 2020)\nspeech\n-\n20k\nAudioSet (Gemmeke et al., 2017)\nsound\n-\n5.8k\nAudioCaps (Kim et al., 2019)\nsound\ntext description\n500\nWavCaps (Mei et al., 2023)\nsound\ntext description\n7k\nMillion Song Dataset (McFee et al., 2012)\nmusic\ntext description\n7k\nOpenCPOP (Wang et al., 2022)\nsinging\ntext, MIDI\n5.2\nOpenSinger (Huang et al., 2021a)\nsinging\ntext, MIDI\n50\nAISHELL3 (Shi et al., 2020)\nspeech\ntext\n85\nPromptSpeech (Guo et al., 2023)\nspeech\ntext, instruction\n200\nopenSLR26,openSLR28 (Ko et al., 2017)\nroom impulse response\n-\n100\nTest\nLibriSpeech test-clean Panayotov et al. (2015)\nspeech\ntext\n8\nVCTK (Veaux et al., 2017)\nspeech\ntext\n50\nTUT2017 Task1 (Mesaros et al., 2017)\nNoise\n-\n10\nCloth (Drossos et al., 2020)\nSound\ntext description\n3\nMusicCaps (Agostinelli et al., 2023)\nMusic\ntext description\n15\nM4Singer(Zhang et al., 2022)\nsinging\ntext, MIDI\n1\nTable 7: Dataset adoption of all tasks\nTask\nTraining dataset\nTest set\nTrain Volume (hrs)\nTraining Stage\nTTS\nLibrilight\nLibriSpeech clean-test\n60k\nVC\nLibrilight\nVCTK\n60k\nSE\nMLS, Audioset\nTUT2017 Task1, VCTK\n20k\nTSE\nMLS\nLibri2Mix test set\n10k\nSound\nAudioCaps, WavCaps\nCloth test set\n7k\nMusic\nMSD\nMusicCaps\n7k\nSinging\nOpenCPOP, OPenSinger, AISHEELL-3\nM4Singer test set\n150\nFine-Tuning Stage\nI-TTS\nPromptSpeech\nPromptSpeech test set\n200\nSpeech dereverberation\nLibriTTS, openSLR26, openSLR28\nLibriTTS test set\n100\nSpeech edit\nLibriTTS\nLibriTTS test set\n100\nAudio edit\nAudioCaps, WavCaps\nAudioCaps test set\n500\nSum\n-\n-\n166k\nA.2\nMODEL CONFIGURATION\nThe model configuration of the proposed multi-scale Transformer is described in Table 8.\n16\nUnder review as a conference paper at ICLR 2024\nTable 8: Model configuration (with nq = 3)\nHyper-parameter\nGlobal Transforemr\nLocal Transformer\n#layer\n24\n8\n#Attention dim\n1536\n1536\n#Attention head\n12\n12\n#Feed-Forward dim\n6144\n6144\n#Params (M)\n744\n238\nMax context length (in #tokens)\n3,000\n3\nCausality\nYes\nYes\nA.3\nOPTIMIZATION\nThe optimization configurations adopted in both the training and fine-tuning stages are presented in\nTable 9\nTable 9: Optimization Configuration\nHyper-parameter\nPre-training\nFine-Tuning\nBatch Size (#patches/GPU)\n8k\n8k\nPeak Learning Rate\n1e-4\n1e-5\nWarm-up Steps\n10000\n1000\nTraining Steps\n800k\n50k\nLearning rate decay\nNoam (Vaswani et al., 2017)\nNoam (Vaswani et al., 2017)\nB\nTHE DETAILS OF EXPERIMENTS\nThis section presents detailed experimental results on each task. In the following, if the training set\nand test sets come from different datasets, we label them as zero-shot settings.\nB.1\nTTS AND VC TASKS\nFor TTS tasks, UniAudio is compared with the many previous SOTA models, Table 10 presents the\nresults. For FastSpeech 2, we only conduct QMOS evaluation as its implementation adopts speaker\nid as input 12. We can see that UniAudio obtains better performance in terms of WER, SIM than\nYourTTS, VALL-E, NaturalSpeech 2 and Make-A-Voice. Compared with VoiceBox, UniAudio also\ngets comparable performance in terms of objective metrics. From the MOS evaluation, we can see\nthat UniAudio can generate high-quality speech compared with previous SOTA works. Furthermore,\nUniAudio realizes the best zero-shot clone ability (e.g. SMOS is 3.56 and SIM is 0.708). More\nexperiments, such as cross-lingual zero-shot TTS and Mandarin Chinese speech synthesis can be\nfound in demo page. For VC task, we conducted experiments on VCTK dataset, we randomly chose\n200 audio pairs. PPG-VC and YourTTS are trained on small-scale datasets. Make-A-Voice and\nLM-VC 13 are trained on large-scale datasets as the same as UniAudio. Compared with previous\nwork, UniAudio got better performance in voice conversion tasks.\nB.2\nSPEECH ENHANCEMENT AND TARGET SPEAKER EXTRACTION\nFor the SE task, we compare with previous SOTA methods, including discriminative methods (such as\nFullSubNet and FullSubNet+) and generative methods (such as SGMSE+ and NADiffuSE). Note that\nthe CDiffuSE and NADiffuSE are both trained on the voicebank-demand dataset. Other models never\nsaw the VCTK dataset in the training stage. We obtain the inference results based on their open-source\nmodels. Table 11 presents the results, we can see that UniAuido obtains the best DNSMOS score.\nThe PESQ and VISQOL scores are lower than other SOTA methods, we think these metrics may not\n12https://github.com/ming024/FastSpeech2\n13We seek help from the authors, they provide the inference results.\n17\nUnder review as a conference paper at ICLR 2024\nTable 10: The performance comparison with previous SOTA methods in TTS and VC tasks. We\ndo not conduct MOS evaluation for VALL-E, SPEARTTS and VoiceBox due to the models are not\nreleased.\nModel\nZero-shot\nSIM (\u2191)\nWER (\u2193)\nMOS (\u2191)\nSMOS (\u2191)\nText-to-Speech\nGroundTruth\n-\n-\n1.9\n3.99\u00b10.08\n-\nFastSpeech 2 (Ren et al., 2020)\n\u2717\n-\n-\n3.81\u00b10.10\n-\nYourTTS (Casanova et al., 2022)\n\u2713\n0.337\n7.7\n3.66\u00b10.07\n3.02\u00b10.07\nVALL-E (Wang et al., 2023a)\n\u2713\n0.580\n5.9\n-\n-\nMake-A-Voice (TTS) (Huang et al., 2023b)\n\u2713\n0.498\n5.7\n3.74\u00b10.08\n3.11\u00b10.06\nNaturalSpeech 2 (Shen et al., 2023)\n\u2713\n0.620\n2.3\n3.83\u00b10.10\n3.11\u00b10.10\nSPEAR-TTS (Kharitonov et al., 2023)\n\u2713\n0.560\n/\n-\n-\nVoiceBox (Le et al., 2023)\n\u2713\n0.681\n1.9\n-\n-\nUniAudio\n\u2713\n0.708\n2.0\n3.81\u00b10.07\n3.56\u00b10.10\nVoice Conversion\nGroundTruth\n-\n-\n3.25\n3.74\u00b10.08\n-\nPPG-VC (Liu et al., 2021)\n\u2717\n0.78\n12.3\n3.41\u00b10.10\n3.47\u00b10.10\nYourTTS (Casanova et al., 2022)\n\u2713\n0.719\n10.1\n3.61\u00b10.10\n3.26\u00b10.10\nMake-A-Voice (VC) (Huang et al., 2023b)\n\u2713\n0.678\n6.2\n3.43\u00b10.09\n3.47\u00b10.10\nLM-VC (Wang et al., 2023e)\n\u2713\n0.820\n4.91\n3.41\u00b10.08\n3.17\u00b10.09\nUniAudio\n\u2713\n0.868\n4.8\n3.54\u00b10.07\n3.56\u00b10.07\naccurately assess the performance of generative methods. The similar finding is also observed in\nprevious literature (Erdogan et al., 2023) that the signal-level evaluation metrics may not be suitable\nfor generative methods. In contrast, we recommend using DNSMOS and MOS scores as the main\nmetrics. UniAuido can get good results in extremely noisy environments, we recommend readers\nrefer to the demo page. For the TSE task, we conducted experiments on the LibriMix test set. The\npopular TSE systems: VoiceFilter 14 and SpeakBeam15 are used as baseline systems. As Table 11\nshows, we can see that UniAudio obtains the best performance in terms of DNSMOS and MOS.\nTable 11: The performance of SE and TSE tasks comparison with previous SOTA methods.\nModel\nZero-shot\nPESQ (\u2191)\nVISQOL(\u2191)\nDNSMOS(\u2191)\nMOS(\u2191)\nSpeech Enhancement\nCDiffuSE (Lu et al., 2022)\n\u2717\n1.88\n1.21\n2.54\n-\nNADiffuSE (Wang et al., 2023b)\n\u2717\n2.96\n2.41\n3.03\n3.30\u00b10.08\nSGMSE+ (Richter et al., 2023)\n\u2713\n3.21\n2.72\n3.29\n3.56\u00b10.08\nFullSubNet (Hao et al., 2021)\n\u2713\n3.21\n2.77\n3.37\n3.61\u00b10.10\nFullSubNet+ (Chen et al., 2022)\n\u2713\n3.41\n2.99\n3.34\n3.42\u00b10.08\nUniAudio\n\u2713\n2.63\n2.44\n3.66\n3.68\u00b10.07\nTarget Speaker Extraction\nSpeakerBeam (\u017dmol\u00edkov\u00e1 et al., 2019)\n\u2717\n2.89\n2.25\n3.18\n3.68\u00b10.1\nVoiceFilter (Wang et al., 2018)\n\u2717\n2.41\n2.36\n3.35\n3.43\u00b10.09\nUniAudio\n\u2713\n1.88\n1.68\n3.96\n3.72\u00b10.06\nB.3\nSINGING VOICE SYNTHESIS\nFollowing Make-A-Voice, we conduct experiments on the M4Singer test set. We compare the\ngenerated singing samples with other systems, including 1) Diffsinger; 2) Make-A-Voice, a two-stage\naudio language model for singing voice generation. As illustrated in Table 12, we can see that\nUniAudio gets comparable results with Make-A-Voice and Diffsinger.\nB.4\nTEXT-TO-SOUND AND TEXT-TO-MUSIC GENERATION\nThe text-to-sound generation task has attracted great interest in audio research. Following Diffsound\n(Yang et al., 2023c), most of the methods evaluate their systems on the AudioCaps (Kim et al., 2019)\n14https://github.com/Edresson/VoiceSplit\n15https://github.com/BUTSpeechFIT/speakerbeam\n18\nUnder review as a conference paper at ICLR 2024\nTable 12: Quality and style similarity of generated samples in singing voice synthesis.\nModel\nMOS (\u2191)\nSMOS (\u2191)\nDiffsinger (Liu et al., 2022)\n3.94\u00b10.02\n4.05\u00b10.06\nMake-A-Voice (Huang et al., 2023b)\n3.96\u00b10.03\n4.04\u00b10.05\nUniAudio\n4.08\u00b10.04\n4.04\u00b10.05\ntest set. However, we found that if the training data includes the AudioCaps data, the model is easy to\noverfit with AudioCaps. As a result, the best performance can be obtained when the model only trains\non the Audiocaps. In this study, we conduct a zero-shot evaluation on the Cloth test set (Drossos\net al., 2020). Table 13 shows the results. We can see that UniAudio obtains better performance than\nDiffsound and AudioLDM. Compared to recent SOTA models, such as Tango and Make-an-Audio\n2, UniAudio also gets comparable performance. For the text-to-music task, we follow MusicGen\n(Copet et al., 2023), evaluating our methods on MusicCaps (Agostinelli et al., 2023). Compared\nwith previous SOTAs, UniAudio gets a comparable performance with other models. From the MOS\nevaluation performance, we can see that MusicGen is better than our current models. We speculate\none of the reasons is that MusicGen uses a large-scale high-quality dataset (20k hours).\nTable 13: Text-to-sound and text-to-music evaluation. We report the subjective metrics including\nFAD(\u2193), and KL(\u2193). Furthermore, we also conduct objective evaluation. Note that the training data\nof AudioGen includes Cloth datatset, thus can not be seen as zero-shot setting.\nModel\nTraining Data (Hours)\nFAD\nKL\nOVL.\nREL.\nText-to-Sound Generation\nReference\n/\n/\n/\n70.47\u00b11.9\n78.84\u00b11.5\nDiffsound\n2k\n7.8\n6.53\n-\n-\nAudioGen\n4k\n2.55\n2.5\n63.84\u00b12.1\n72.12\u00b11.8\nTango\n3.3k\n3.61\n2.59\n66.2\u00b11.7\n68.57\u00b11.5\nMake-an-Audio 2\n8.7k\n2.13\n2.49\n61.52\u00b11.6\n69.9\u00b11.5\nAudioLMD\n9k\n4.93\n2.6\n60.95\u00b11.9\n65.7\u00b11.8\nUniAudio\n7k\n3.12\n2.57\n61.9\u00b11.9\n66.1\u00b11.5\nText-to-Music Generation\nRiffusion\n-\n14.8\n2.06\n-\n-\nMousai\n-\n7.5\n1.59\n-\n-\nMusicLM\n280k\n4.0\n-\n-\n-\nNoise2Music\n280k\n2.1\n-\n-\n-\nMusicGen\n20k\n4.52\n1.41\n73.28\u00b11.5\n71.28\u00b11.7\nUniAudio\n8k\n3.65\n1.87\n67.85\u00b11.70\n70.0\u00b11.5\nB.5\nAUDIO EDIT\nAudio edit aims to edit the original audio based on Human\u2019s instruction. AUDIT (Wang et al.,\n2023d) is the SOTA model in audio edit task, which designs a data simulation strategy to get triplet\ntraining and test data (e.g., {audio, audio, text}). The authors set 5 different tasks, including adding,\ndropping, replacing, inpainting and super-resolution, and simulated large-scale data for each task.\nTo validate that our pre-trained model can be fine-tuned with small-scale data, we choose adding,\ndropping and super-resolution tasks to fine-tune simultaneously. To finish the fine-tuning process, we\ndefine a new task label: Audit_task. The experimental results as Table 14 shows. We can observe\nthat: (1) UniAudio can get better performance with the previous SOTA model. (2) Fine-tuning\npre-trained UniAudio can get better performance than training it from scratch, which further validates\nthe effectiveness of pre-training a model on large-scale training data.\nB.6\nINSTRUCTED TTS\nUsing instruction to guide speech synthesis has received great attention (Guo et al., 2023; Yang et al.,\n2023a). In this part, we fine-tune the UniAudio model on the PromptSpeech (Guo et al., 2023) dataset.\n19\nUnder review as a conference paper at ICLR 2024\nTable 14: Audio edit task evaluation.\nType\nModel\nFD\nKL\nAdding task\nAUDIT\n21.80\n0.92\nUniAudio (scratch)\n20.2\n0.99\nUniAudio (fine-tune)\n19.69\n0.934\nDropping task\nAUDIT\n22.40\n0.95\nUniAudio (scratch)\n27.76\n1.38\nUniAudio (fine-tune)\n23.1\n1.10\nSuper-Resolution task\nAUDIT\n18.14\n0.73\nUniAudio (scratch)\n11.51\n0.29\nUniAudio (fine-tune)\n10.54\n0.289\nTable 15: Quality and style similarity of generated samples for Instructed TTS task.\nModel\nMOS (\u2191)\nSMOS (\u2191)\nGT\n3.77\u00b10.07\n3.85\u00b10.08\nUniAudio (scratch)\n3.62\u00b10.07\n3.67\u00b10.08\nUniAudio (tuning)\n3.61\u00b10.09\n3.71\u00b10.09\nFurthermore, we also try to train a UniAudio model from scratch with the PromptSpeech dataset.\nDifferent from previous works that designed special style encoders to capture the style information\nfrom text descriptions, we directly use the T5 text encoder to extract representations from text and\nthen combine it with the phoneme sequence input to the UniAudio, which is more convenient.16\nTable 15 shows the results, we can see that UniAudio has good performance in terms of style control\nand speech quality when compared with the ground truth samples.\nB.7\nSPEECH DEREVERBERATION\nFor the speech dereverberation task, we use the Room Impulse Response (RIR) data from the\nopenSLR26 and openSLR28 dataset, and the speech data from the LibriTTS clean part. We simulate\nabout 100 hours of training data and 1 hour of test data. We compare with previous SOTA systems,\nsuch as FullSubNet, FullSubNet+ and SGMSE+. Table 16 presents the results. We can see that\nUniAudio obtains the SOTA performance in speech dereverberation tasks with small-scale training\ndata in terms of DNSMOS metric. Similar with speech enhancement task, we speculate that PESQ\nmay not suitable for the generative methods.\nTable 16: Results comparison with previous speech Dereverberation systems.\nModel\nPESQ (\u2191)\nDNSMOS(\u2191)\nSGMSE+\n2.87\n3.42\nFullSubNet\n2.29\n3.32\nFullSubNet+\n2.27\n3.25\nUniAudio (scratch)\n1.23\n3.18\nUniAudio (tuning)\n2.13\n3.51\n16Note that the authors of PromptTTS (Guo et al., 2023) told us their objective metrics tools, checkpoints, and\ngenerated samples have been lost due to the machine errors. Thus we cannot fairly compare with them.\n20\nUnder review as a conference paper at ICLR 2024\nB.8\nSPEECH EDIT\nFor the speech edit task, we use the LibriTTS dataset. In practice, we randomly choose some words\nto mask in the training stage. We expect the model to recover the whole speech based on the phoneme\nsequence. In the inference stage, we can mask the region that we want to update in the speech and\ninput the new words so that the model can edit the speech. For this task, we take the TTS system\nthat regenerates a complete waveform from the whole sentence to be edited as the baseline. In the\nevaluation, we mainly validate three situations: (1) word replacement; (2) insert a new word; and (3)\ndelete a word. For each situation, we randomly chose 10 sentences from the LibriTTS test clean set.\nC\nABLATION STUDY\nC.1\nTHE INFLUENCE OF MULTI-TASK TRAINING\nIn this part, we explore whether multi-task training can bring better performance than task-specific\ntraining. To answer this question, we use the same model trained on different tasks, respectively.\nTable 17 shows the experimental results, UniAudio (single) means that the model is trained on a\nsingle task. We observe that multi-task training brings the gain over all of the tasks. In Appendix D,\nwe give some potential reasons why multi-task training can bring improvement.\nTable 17: The ablation study of the effectiveness of multi-task training.\nTask\nModel\nObjective Evaluation\nSubjective Evaluation\nMetrics\nResults\nMetrics\nResults\nText-to-Speech\nUniAudio (Single)\nSIM(\u2191) / WER(\u2193)\n0.64 / 2.4\nMOS(\u2191)\n/ SMOS(\u2191)\n3.77\u00b10.06 / 3.46\u00b10.10\nUniAudio\n0.71 / 2.0\n3.81\u00b10.07 / 3.56\u00b10.10\nVoice\nConversion\nUniAudio (Single)\nSIM(\u2191) / WER(\u2193)\n0.84 / 5.4\nMOS(\u2191)\n/ SMOS(\u2191)\n3.45\u00b10.07 / 3.44\u00b10.07\nUniAudio\n0.87 / 4.8\n3.54\u00b10.07 / 3.56\u00b10.07\nSpeech\nEnhancement\nUniAudio (Single)\nPESQ(\u2191)\n/ VISQOL(\u2191) / DNSMOS(\u2191)\n2.35 / 2.30 / 3.45\nMOS(\u2191)\n3.65\u00b10.08\nUniAudio\n2.63 / 2.44 / 3.66\n3.68\u00b10.07\nTarget Speaker\nExtraction\nUniAudio (Single)\nPESQ(\u2191)\n/ VISQOL(\u2191) / DNSMOS(\u2191)\n1.97 / 1.61 / 3.93\nMOS(\u2191)\n3.58\u00b10.08\nUniAudio\n1.88 / 1.68 / 3.96\n3.72\u00b10.06\nSinging Voice\nSynthesis\nUniAudio (Single)\n-\n-\nMOS(\u2191)\n/ SMOS(\u2191)\n4.14\u00b10.07 / 4.02\u00b10.02\nUniAudio\n4.08\u00b10.04 / 4.04\u00b10.05\nText-to-Sound\nUniAudio (Single)\nFAD (\u2193) / KL (\u2193)\n3.84 / 2.7\nOVL (\u2191)\n/ REL (\u2191)\n60.0\u00b12.1 / 61.2\u00b11.8\nUniAudio\n3.12 / 2.6\n61.9\u00b11.9 / 66.1\u00b11.5\nText-to-Music\nUniAudio (Single)\nFAD (\u2193) / KL (\u2193)\n5.24 / 1.8\nOVL (\u2191)\n/ REL (\u2191)\n64.4\u00b12.1 / 66.2\u00b12.4\nUniAudio\n3.65 / 1.9\n67.9\u00b11.7 / 70.0\u00b11.5\nAudio Edit\nUniAudio (single)\nFD (\u2193) / KL (\u2193)\n19.82 / 0.92\n-\n-\nUniAudio\n17.78 / 0.77\n-\nSpeech Dereverb.\nUniAudio (single)\nPESQ(\u2191) / DNSMOS(\u2191)\n1.23 / 3.18\n-\n-\nUniAudio\n2.13 / 3.51\n-\nInstructed TTS\nUniAudio (single)\n-\n-\nMOS(\u2191) / SMOS(\u2191)\n3.62\u00b10.07 / 3.67\u00b10.08\nUniAudio\n-\n3.61\u00b10.09 / 3.71\u00b10.09\nSpeech Edit\nUniAudio (single)\nMCD (\u2193)\n5.26\nMOS(\u2191)\n3.73\u00b10.07\nUniAudio\n5.12\n3.82\u00b10.06\nC.2\nFINE-TUNING THE PRE-TRAINED MODEL ON THE NEW TASK WILL INFLUENCE THE\nPERFORMANCE ON PREVIOUS TASKS?\nIn this part, we conduct experiments to explore whether fine-tuning the pre-trained model on new\ntasks will influence the performance of previous tasks. We evaluate the pre-trained UniAudio model\n(trained on 7 tasks) and fine-tuned UniAudio model (fine-tuned on 4 new tasks) on 7 tasks. Figure 3\nshows the results. We can see that the performance does not significantly drop on previous training\ntasks, which demonstrates that UniAudio has the potential to add new tasks continuously without\nlosing previous task knowledge.\nC.3\nTHE INFLUENCE OF DATA QUANTITY\nIn this part, we conduct experiments to explore the influence of data quantity, we give three settings:\n(1) using all of the data; (2) using 1/2 training data for each task; (3) using 1/4 training data for each\ntask. We present the results in Figure 4. Based on the experimental results, this work claims that\nthe data quantity is a key point to building a strong audio foundation model. In the future, we will\nexplore to use of more unlabeled data to help improve the performance.\n21\nUnder review as a conference paper at ICLR 2024\nSIM(\n)\nWER(\n)\nSIM(\n)\nWER(\n)\nPESQ(\n)\nVISQOL(\n)\nD-MOS(\n)\nPESQ(\n)\nVISQOL(\n)\nD-MOS(\n)\nKL(\n)\nFAD(\n)\nKL(\n)\nFAD(\n)\nMOS(\n)\nSMOS(\n)\n0.71\n2.00\n0.86\n4.80\n2.65\n2.36\n3.66\n1.75\n1.69\n3.96\n3.12\n2.57\n3.65\n1.87\n4.08\n4.04\n0.71\n2.00\n0.85\n4.90\n2.63\n2.44\n3.60\n1.88\n1.68\n3.86\n3.12\n2.59\n3.68\n1.91\n4.02\n3.98\nTTS\nVC\nSE\nTSE\nTT-Sound\nTT-Music\nSVS\nBefore Fine-Tuning\nAfter Fine-Tuning\nFigure 3: Performance comparison over 7 audio generation tasks before/after fine-tuning.\nSIM(\n)\nWER(\n)\nSIM(\n)\nWER(\n)\nPESQ(\n)\nDNSMOS(\n)\nPESQ(\n)\nDNSMOS(\n)\nKL(\n)\nFAD(\n)\nKL(\n)\nFAD(\n)\nMCD(\n)\n0.71\n2.00\n0.86\n4.80\n2.65\n3.66\n1.75\n3.96\n3.12\n2.57\n3.65\n1.87\n6.26\n0.60\n3.20\n0.84\n5.30\n2.48\n3.30\n1.23\n3.60\n5.29\n2.96\n5.41\n1.96\n6.68\n0.57\n3.90\n0.80\n5.90\n2.07\n3.20\n1.20\n3.50\n5.75\n3.03\n9.28\n2.39\n7.16\nTTS\nVC\nSE\nTSE\nTT-Sound\nTT-Music\nSVS\nfull data\n1/2 data\n1/4 data\nFigure 4: Performance comparison over different data quantity.\nD\nWHY UNIAUDIO CAN WORK WELL?\nFrom the previous discussions, we can see that the universal modeling strategy brings improvement\nfor different tasks. In this part, we try to give some potential explanations.\n(1) Deterministic latent space: we formulate different modalities into a deterministic latent space\n(fixed vocabulary) by tokenization. Different tokens can be seen as specific \u2019words\u2019, and we can use a\nnext-token prediction strategy to train the model. Similar to GPT-series (Radford et al., 2018; 2019),\nsuch strategy creates the opportunity for the model to learn the intrinsic properties of audio and the\ninterrelationship between audio and other modalities.\n(2) Shared information between different types of audio: Although multiple types of audio (speech,\nsounds, music, and singing) present significant differences in the time domain or frequency domain,\nneural audio codec models effectively capture their shared information (rethinking the working\nprinciple of neural codecs, which similar information will be allocated the same token id). Due to the\nshared information that exists in different types of audio, multi-task training can be seen as increasing\ntraining data for each task.\n(3) Data augmentation perspective: We speculate that multi-task training can be viewed as data\naugmentation for some tasks. Considering the TTS and VC task\u2019s definition:\nTTS: <phoneme_sequence> <prompt> <audio_sequence>\nVC: <semantic_token> <prompt> <audio_sequence>\nWe can see that the difference in task formulation for TTS and VC is that they use different ways to\ndenote the phonetic information. In essence, they carry the same phonetic information. The difference\nis that semantic tokens include the duration information. Thus we can view the phoneme sequence as\na special semantic sequence that drops the duration information. Such dropping operation is widely\nused as a data augmentation strategy (Park et al., 2019).\nE\nTHE DETAILS OF AUDIO CODEC MODELS\nIn this part, we give more details about our neural audio codec model in Section 2.1.1. We adopt a\nsimilar encoder-decoder framework with the Encodec model, the difference includes: (1) we replace\n22\nUnder review as a conference paper at ICLR 2024\nTable 18: Performance comparison between encodec and our universal neural codec. FPS: frame\nper second; TPS: token per second. Perceptual evaluation of speech quality (PESQ\u2191); Short Term\nObjective Intelligibility (STOI\u2191).\nType\nSpeech (VCTK)\nSound (cloth)\nMusic (musiccaps)\nSing (m4sing)\nAverage\n(Veaux et al., 2017)\n(Drossos et al., 2020)\n(Agostinelli et al., 2023)\n(Zhang et al., 2022)\n-\nModel\nnq\nFPS\nTPS\nPESQ\nSTOI\nPESQ\nSTOI\nPESQ\nSTOI\nPESQ\nSTOI\nPESQ\nSTOI\nEncodec\n8\n75\n600\n2.18\n0.79\n2.23\n0.48\n1.86\n0.57\n1.95\n0.76\n2.05\n0.65\nOurs\n3\n50\n150\n2.96\n0.85\n2.42\n0.49\n1.99\n0.57\n3.13\n0.85\n2.62\n0.69\nOurs\n4\n50\n200\n3.11\n0.86\n2.5\n0.51\n2.08\n0.59\n3.27\n0.86\n2.73\n0.71\nOurs\n8\n50\n400\n3.36\n0.88\n2.67\n0.54\n2.31\n0.65\n3.49\n0.89\n2.95\n0.74\nthe multi-scale STFT-based (MS-STFT) discriminator as our multi-scale Mel-based discriminator.\n(2) We rewrite the vector quantization implementation 17 based on Encodec\u2019s open-source version 18,\nmaking it more suitable for DDP training. Figure 5 shows the details of the mel-based discriminator.\nWe combine the mel-spectrogram and log-mel-spectrogram features and then input them into a\nnetwork consisting of several convolutional layers. Our motivation is that the mel-spectrogram has\na strong intrinsic inductive bias, especially for sounds and music-related audio (the SOTA sounds\nor music classification systems are based on the log-mel-spectrogram in the literature.). Thus, we\nspeculate that choosing a mel-spectrogram-based discriminator can better promote high-fidelity audio\nreconstruction. In our experiments, we use 6 different discriminators with different configurations 19.\nSpecifically, we set the hidden_dim as {64, 128, 256, 512, 512, 512} and the hop length as {32, 64,\n128, 256, 512, 1024}. We train the neural audio codec model based on the Librilight and AudioSet\ndatasets. Table 18 demonstrates that the neural codec model adopted in this work outperforms prior\nEncodec (D\u00e9fossez et al., 2022).\nSTFT\nMel\nLog-mel\nConv2D(2, hidden_dim//32)\nConv2D(hidden_dim//32,\nhidden_dim//16 )\nConv2D(hidden_dim//16,\nhidden_dim//8 )\nConv2D(hidden_dim//8,\nhidden_dim//4 )\nConv2D(hidden_dim//4,\nhidden_dim//2)\nConv2D(hidden_dim//2,\nhidden_dim)\nConv2D(hidden_dim, 1)\nlogit\nFigure 5: The overview of a single Mel-based discriminator. In practice, we will use multiple\ndiscriminators by setting different hop lengths and hidden dimensions.\nF\nSUBJECTIVE EVALUATION\nFor TTS and VC tasks, we focus on speech quality (QMOS) and speaker similarity (SMOS). The\ndetails are as follows. For speech quality evaluation, we conduct the MOS (mean opinion score) tests\nand explicitly ask the raters to focus on examining the audio quality and naturalness, and ignore the\ndifferences of style (timbre, emotion, and prosody. The testers present and rate the samples, and each\ntester is asked to evaluate the subjective naturalness on a 1-5 Likert scale.\nFor speaker similarity evaluation, we ask the raters to focus on the similarity of the speaker identity\n(timbre) to the reference, and ignore the differences in content, grammar, or audio quality. We paired\neach synthesized utterance with a reference utterance to evaluate how well the synthesized speech\nmatched that of the target speaker.\n17Please refer to our source code to find the details.\n18https://github.com/facebookresearch/encodec/blob/main/encodec/quantization/core_vq.py\n19In our experiments, we find the mel-based discriminator brings better reconstruction performance when we\ntrain a universal neural audio codec.\n23\nUnder review as a conference paper at ICLR 2024\n\uff08a\uff09Speech Enhancement\n\uff08b\uff09Target Speaker Extraction\n\uff08c\uff09Qverall quality of Audio \n\uff08d\uff09Relevance to the text input\n\uff08e\uff09Speech quality MOS \n\uff08f\uff09Speech Similarity  MOS \nFigure 6: Screenshots of subjective evaluations.\nFor SE and TSE tasks, we write explicit instructions to ask the rater to assess the generated speech.\nRefer to Figure 6 to see the details.\nFor SVS, we also conduct quality MOS (QMOS) and style similarity MOS (SMOS). Different from\nTTS\u2019s SMOS evaluation, we explicitly instruct the raters to focus on the similarity of the style (timbre,\nemotion, and prosody) to the reference, and ignore the differences in content, grammar, or audio\nquality.\nFor sound and music generation tasks, we follow AudioGen (Kreuk et al., 2022) and MusicGen\n(Copet et al., 2023) to evaluate (1) overall quality (OVL), and (2) relevance to the text input (REL).\nOur subjective evaluation tests are crowd-sourced and conducted by 20 native speakers via Amazon\nMechanical Turk. The screenshots of instructions for testers have been shown in Figure 6. We paid\nabout $500 on participant compensation. A small subset of speech samples used in the test is available\nat https://uniaudio666.github.io/demo_UniAudio/.\n24\n"
  },
  {
    "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "link": "https://arxiv.org/pdf/2310.03214.pdf",
    "upvote": "14",
    "text": "Preprint\nFRESHLLMS:\nREFRESHING LARGE LANGUAGE MODELS\nWITH SEARCH ENGINE AUGMENTATION\nTu Vu1\nMohit Iyyer2\nXuezhi Wang1\nNoah Constant1\nJerry Wei1\nJason Wei3\u2217 Chris Tar1\nYun-Hsuan Sung1\nDenny Zhou1\nQuoc Le1\nThang Luong1\nGoogle1\nUniversity of Massachusetts Amherst2\nOpenAI3\nfreshllms@google.com\nABSTRACT\nMost large language models (LLMS) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in the\ncontext of answering questions that test current world knowledge. Specifically, we\nintroduce FRESHQA, a novel dynamic QA benchmark encompassing a diverse range\nof question and answer types, including questions that require fast-changing world\nknowledge as well as questions with false premises that need to be debunked. We\nbenchmark a diverse array of both closed and open-source LLMS under a two-mode\nevaluation procedure that allows us to measure both correctness and hallucination.\nThrough human evaluations involving more than 50K judgments, we shed light on\nlimitations of these models and demonstrate significant room for improvement: for\ninstance, all models (regardless of model size) struggle on questions that involve\nfast-changing knowledge and false premises. Motivated by these results, we present\nFRESHPROMPT, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FRESHQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FRESHPROMPT outperforms both competing search engine-augmented\nprompting methods such as SELF-ASK (Press et al., 2022) as well as commercial\nsystems such as PERPLEXITY.AI.1 Further analysis of FRESHPROMPT reveals that\nboth the number of retrieved evidences and their order play a key role in influencing\nthe correctness of LLM-generated answers. Additionally, instructing the LLM\nto generate concise and direct answers helps reduce hallucination compared to\nencouraging more verbose answers. To facilitate future work, we release FRESHQA\nat github.com/freshllms/freshqa and commit to updating it at regular intervals.\n1\nINTRODUCTION\nRecent large language models (LLMS) such as BARD and CHATGPT/GPT-42 are designed to be\nversatile open-domain chatbots that can engage in multi-turn conversations on diverse subjects.\nDespite their impressive capabilities, these LLMS often \u201challucinate\u201d plausible but factually incorrect\ninformation (Maynez et al., 2020; Liu et al., 2023b), which reduces the trustworthiness of their\nresponses, especially in settings where accurate and up-to-date information is critical. This behavior\ncan be partially attributed to the presence of outdated knowledge encoded in their parameters. While\nadditional training using human feedback (Ouyang et al., 2022) or knowledge-enhanced tasks can\nmitigate this issue, it is not easily scalable for real-time knowledge updates (e.g., stock price of a\ncompany). In-context learning (Brown et al., 2020) is an appealing alternative in which real-time\nknowledge can be injected into an LLM\u2019s prompt for conditioning generation. While recent work\nhas begun to explore augmenting LLMS with web search results (Lazaridou et al., 2022; Press et al.,\n2022), it is unclear how to take full advantage of search engine outputs to increase LLM factuality.\n\u2217Work done while at Google.\n1https://www.perplexity.ai\n2https://bard.google.com, https://chat.openai.com\n1\narXiv:2310.03214v2  [cs.CL]  22 Nov 2023\nPreprint\nType\nQuestion\nAnswer (as of this writing)\nnever-changing\nHas Virginia Woolf's novel about the Ramsay family entered the \npublic domain in the United States?\nYes, Virginia Woolf's 1927 novel To the Lighthouse \nentered the\u00a0public domain\u00a0in 2023.\nnever-changing\nWhat breed of dog was Queen Elizabeth II of England famous \nfor keeping?\nPembroke Welsh Corgi dogs.\nslow-changing\nHow many vehicle models does Tesla o\ufb00er?\nTesla o\ufb00ers \ufb01ve vehicle models: Model S, Model \nX, Model 3, Model Y, and the Tesla Semi.\nslow-changing\nWhich team holds the record for largest de\ufb01cit overcome to win \nan NFL game?\nThe record for the largest NFL comeback is held \nby the Minnesota Vikings.\nfast-changing\nWhich game won the Spiel des Jahres award most recently?\nDorfromantik won the 2023 Spiel des Jahres.\nfast-changing\nWhat is Brad Pitt's most recent movie as an actor\nBrad Pitt recently starred in Babylon, directed \nby\u00a0Damien Chazelle. \nfalse-premise\nWhat was the text of Donald Trump\u2019s \ufb01rst tweet in 2022, made \nafter his unbanning from Twitter by Elon Musk?\nHe did not tweet in 2022.\nfalse-premise\nIn which round did Novak Djokovic lose at the 2022 Australian \nOpen?\nHe was not allowed to play at the tournament \ndue to his vaccination status.\nFigure 1: FRESHQA exemplars. Our questions are broadly divided into four main categories based on\nthe nature of the answer: never-changing, in which the answer almost never changes; slow-changing,\nin which the answer typically changes over the course of several years; fast-changing, in which the\nanswer typically changes within a year or less; and false-premise, which includes questions whose\npremises are factually incorrect and thus have to be rebutted.\nIn this work, we collect a novel QA benchmark, dubbed FRESHQA, to evaluate the factuality of\nexisting LLMS. FRESHQA consists of 600 natural questions that are broadly divided into the four\nmain categories shown in Figure 1. FRESHQA\u2019s questions span a diverse set of topics with diverse\ndifficulty levels (requiring single-hop and multi-hop reasoning), and require a model to \u201cunderstand\u201d\nthe world\u2019s up-to-date knowledge to be able to answer correctly. Additionally, FRESHQA is dynamic\nin nature: some of the ground-truth answers may change over time, and a question classified under\na specific category may undergo reclassification at some later point in time (e.g., the current false-\npremise question \u201cHow long has Elon Musk been married to his current spouse?\u201d will fall into the\nfast-changing category if Elon Musk gets married again in the future).\nWe benchmark how well different LLMS perform on FRESHQA by prompting them with questions and\noptionally a few question-answer demonstrations and then sampling a response. Then, we conduct\nan extensive human evaluation of the factual accuracy of the models\u2019 responses, consisting of more\nthan 50K judgements. We evaluate each response in a two-mode evaluation procedure: RELAXED,\nwhich measures only whether the main answer is correct; and STRICT, which measures whether all\nof the claims in the response are factual and up-to-date (i.e., no hallucination). Our study sheds\nlight on the factuality of old and new LLMS and reveals different model behaviors across question\ntypes. Unsurprisingly, there are flat scaling curves on questions that involve fast-changing knowledge:\nsimply increasing the model size does not lead to reliable performance gains. We also observe similar\ntrends on false-premise questions, though several LLMS are able to debunk a false-premise question\nif explicitly asked \u201cPlease check if the question contains a valid premise before answering\u201d. Overall,\nFRESHQA is challenging for current LLMS and leaves ample room for improvement.\nMotivated by these findings, we further investigate how to effectively improve LLMS\u2019 factuality\nby grounding their responses to accurate and up-to-date information from search engines. Given\nthe rapid development of ever larger LLMS and the ever-changing nature of knowledge, we explore\nin-context learning approaches that allow an LLM to attend over knowledge provided at inference\ntime through its prompt. We develop FRESHPROMPT, a simple yet effective method that, for a given\nquestion, takes full advantage of a search engine by extracting all up-to-date and relevant information\n(including knowledge from relevant questions that search users also ask) and uses few-shot in-context\nlearning to teach a model to reason over retrieved evidences and figure out the right answer. We\nshow that FRESHPROMPT significantly boosts LLMS\u2019s factuality: for example, our best GPT-4 +\nFRESHPROMPT variant yields an improvement of 32.6% and 49.0% accuracy over the vanilla GPT-4\non FRESHQA under RELAXED and STRICT, respectively. Since our method requires no additional\ntraining, it is flexible and applicable to a variety of scenarios.\nTaken together, our key contributions include:\n2\nPreprint\n\u2022 We introduce a novel dynamic QA benchmark, FRESHQA, which features a diverse set of\nquestion and answer types, including questions whose answers may change over time and\nquestions whose premises are factually incorrect. We make our dataset freely available and\ncommit to updating the ground-truth answers at a regular schedule to encourage exploration\nof methods to improve LLMS\u2019 factuality.\n\u2022 We benchmark a wide range of both closed and open-source LLMS on our dataset. Through\nan extensive and rigorous human evaluation study, we shed light on limitations of current\nLLMS: they struggle on fast-changing, false-premise, and multi-hop questions, and our\ntwo-mode evaluation captures increased hallucinations produced by techniques such as\nchain-of-thought prompting (Wei et al., 2022).\n\u2022 We present FRESHPROMPT, a simple in-context learning method that can substantially boost\nan LLM\u2019s factuality compared to competing search-augmented approaches by effectively\nincorporating factual and up-to-date information from a search engine into the model\u2019s\nprompt. Furthermore, we perform a series of sensitivity and ablation analyses to better\nunderstand what facets of FRESHPROMPT contribute to its success.\n2\nFRESHQA\nIn this section, we address the growing need to assess LLM factuality by curating a novel QA\nbenchmark, FRESHQA, with 600 questions that cover a wide spectrum of question and answer types.\n2.1\nDATA COLLECTION\nWe collected FRESHQA by recruiting both NLP researchers (including the authors and their colleagues)\nand online freelancers3 to write questions of varying difficulty levels and topics whose answers may\nchange based on new developments in the world. The annotators were shown a few exemplars of\nthe four broad types of questions defined in Figure 1. Within each of these four categories, we ask\nannotators to write questions at two different difficulty levels: one-hop, where the question explicitly\nmentions all of the relevant information needed to answer it, and thus no additional reasoning is\nrequired (e.g., \u201cWho is the CEO of Twitter\u201d); and multi-hop, where the question requires one or more\nadditional steps of reasoning in order to gather all of the relevant information needed to answer it\n(e.g., \u201cWhat is the total height of the tallest building in the world?\u201d). Annotators were encouraged to\nwrite questions that involve fresh knowledge (knowledge that has changed recently or new events)\nand appear natural (i.e., plausible for a real person to type into a search engine). For false-premise\nquestions, we requested a brief explanation elucidating why the question is flawed.4\nQuality control:\nUpon obtaining the initial dataset, we conducted multiple thorough data cleaning\nand quality assessments. This involved manual review of each example to ensure well-formed\nquestions, removal of duplicates and invalid questions (e.g., too easy or controversial), and verification\nof answers and supporting evidence URLS. We also manually collected supplementary valid answers\nfor each question (e.g., different names of the same person, different date formats, etc.). To facilitate\nfuture answer updates, we excluded questions whose answers are likely to change more frequently\nthan once per week, and additionally incorporated the expected next review date for each question.\nData size and split:\nThe resulting dataset is divided into a test set consisting of 125 questions for\neach of the four broad question types (500 total examples) and a development set comprising 25\nquestions for each question type (100 total examples), sampled randomly within types. Additionally,\n15 examples spanning different question types were extracted for demonstration purposes (i.e., for\nuse in few-shot in-context learning), and the remaining data was discarded. The development set is\nreserved for future studies and not used in this paper.5\nFRESHQA requires regular updates:\nOur dataset has time sensitivity since the ground-truth\nanswers may change with new developments in the world. As such, we commit to updating the\ndataset regularly and encourage researchers to evaluate on the latest version of the dataset, as close to\nthe release date of the updated dataset as possible.\n3We use UPWORK (https://www.upwork.com) with a compensation rate of $2 per example.\n4Additionally, the annotators were asked to include the year the answer to the question last changed and an\nURL to a reputable website that supports the answer.\n5Although our test set is currently balanced across question types, the distribution may change over time due\nto reclassification of questions from one category to another.\n3\nPreprint\n2.2\nEVALUATION\nAll model responses were evaluated by the authors in a two-mode evaluation procedure: RELAXED,\nwhich focuses solely on evaluating the correctness of the primary answer; and STRICT, which\nadditionally examines whether all of the facts in the answer are accurate (i.e., no hallucination).\nOverall, our setup provides both ends of the spectrum for evaluating factuality (the difference between\na model\u2019s strict and relaxed performance provides a way to measure hallucination), offering a more\ncomprehensive and nuanced understanding of their performance.\nEvaluation protocol:\nIn both evaluation modes, we credit a model\u2019s response only if it provides a\nconfident and definitive answer, or the correct answer can be obviously inferred from the response.\nThe primary or final answer when standing alone must be accurate. Any additional information that is\nprovided must not contradict the primary answer or reshape one\u2019s perception of it. For false-premise\nquestions, the model must point out the presence of a false premise to receive credit. For answers\nthat involve names of entities (e.g., people), complete names or commonly recognized names are\nexpected. Regarding numerical answers, approximate numbers are generally not accepted unless\nexplicitly included in the ground-truth answers. Under RELAXED, we accept ill-formed responses\n(including those in a non-English language), as well as hallucinated or outdated information that does\nnot significantly impact the primary answer. Under STRICT, however, a response that contains any\nhallucination, no matter how minor, will not receive credit. Furthermore, we accept a response in\nSTRICT when the model indicates that the information might be outdated (e.g., \u201cAs of my knowledge\ncutoff date in September 2021\u201d) only if it is evident that the knowledge has not changed.6 Figure 4 in\nAppendix A shows specific examples of each evaluation criteria.\nInter-rater agreement and automatic evaluation:\nTwo authors independently evaluated a subset\nof 100 answers in both modes and had an agreement of 99% for RELAXED and 96% for STRICT,\nshowing that the protocol is reliable for comparing different LLMS. Additionally, to facilitate future\nevaluations, we develop FRESHEVAL, a simple automatic metric that uses few-shot in-context learning\nto teach an LLM to judge model responses, achieving an average agreement of 96.5% with human\nevaluations for RELAXED and 96% for STRICT. See Appendix B for details.\n3\nPRE-TRAINED LLMS STRUGGLE ON FRESHQA\nWe use FRESHQA to benchmark LLMS that do not have access to real-time data or the ability to\nbrowse the Internet for current information.7 While all LLMS (regardless of size) predictably struggle\non questions requiring up-to-date knowledge, they also underperform on false premise questions. In\nour experiments, we simply feed individual questions as prompts into each model and decode the\nmodel\u2019s predictions using a temperature of 0 without fine-tuning (see Appendix C for more details).\nBaselines:\nWe experiment with a series of models varying in size from 770M to 540B parameters,\nincluding basic pre-trained models such as T5 (Raffel et al., 2020; Lester et al., 2021), PALM and\nPALMCHILLA (Chowdhery et al., 2022), optionally using FEW-SHOT prompting (Brown et al., 2020)\nand Chain-of-Thought (COT, Wei et al., 2022);8 instruction-tuned models including FLAN-T5 and\nFLAN-PALM (Chung et al., 2022; Longpre et al., 2023), and OPENAI\u2019s GPT-3.5 (Ouyang et al.,\n2022), CODEX (Chen et al., 2021a), CHATGPT, and GPT-4 (OpenAI, 2023).\n3.1\nRESULTS AND DISCUSSION\nFRESHQA presents a challenge for LLMS:\nWe visualize the accuracy of different LLMS on\nFRESHQA in both evaluation modes in Figure 2.9 A first obvious takeaway is that all models struggle\n6Note that even without access to real-time data, a model may still provide accurate answers to certain\nquestions involving current information, potentially through random guesses or by leveraging past valid responses\n(e.g., for the question \u201cWhich drama series won the most recent Primetime Emmy Award for Outstanding Drama\nSeries?\u201d, while \u201cSuccession\u201d won the award most recently (as of this writing), it was also the winner in 2020,\nso a model trained in 2021 could potentially provide the correct answer).\n7With the exception of CHATGPT and GPT-4, which have access to the current date. Note that the latest versions\nof these models can now browse the Internet.\n8As we are interested in exploring how these methods perform without being specifically designed for\nFRESHQA, we use the 5-shot demonstrations for TRIVIAQA (Joshi et al., 2017) used in Sun et al. (2023).\n9Table 3 and Table 4 in Appendix D contain concrete numbers under STRICT and RELAXED, respectively.\n4\nPreprint\nOverall\nFast-changing questions\nFalse-premise questions\nFigure 2: Accuracy of different LLMS on FRESHQA under RELAXED and STRICT (no hallucination)\nevaluations. Models benchmarked on the same date of April 26, 2023. All models (regardless of\nmodel size) struggle on questions that involve fast-changing knowledge and false premises.\non FRESHQA: overall accuracy ranges from 0.8% to 32.0% under STRICT, and 0.8% to 46.4%\nunder RELAXED. Switching from RELAXED to STRICT results in a marked decrease in accuracy for\nCHATGPT and GPT-4. This is mainly due to the lack of access to up-to-date information, as they\nproduce \u201coutdated\u201d answers (which often start with the prefix \u2018\u2018As of my knowledge cutoff date in\nSeptember 2021\u201d), and in many cases, \u201crefuse\u201d to provide an answer (e.g., \u201cAs an AI language model,\nI cannot provide real-time information.\u201d). Similarly, the accuracy of PALM (across model sizes)\ndrops significantly under STRICT. Much of this drop is due to artifacts such as conversation-like\nresponses with unexpected special tokens (e.g., the end-of-turn [eot]), and hallucination. In contrast,\nFLAN-PALM and CODEX exhibit minimal hallucination due to their concise and direct answers.\nLLMS struggle with questions about current information:\nThe lack of up-to-date parametric\nknowledge results in dramatically degraded accuracies across models on questions involving fast-\nchanging or recent knowledge. GPT-4 generally obtains the highest accuracy on these questions,\nwith the exception of questions about recent knowledge (i.e., since 2022) under STRICT where it\nunderperforms FLAN-PALM and CODEX, but it never exceeds 15% across both evaluation modes. Our\nevaluation confirms that CHATGPT and GPT-4 have been exposed to data containing information\nbeyond their knowledge cutoff date (Appendix E). Additionally, GPT-4 is more reluctant to answer\nfast-changing questions (refusing to answer 60% of the time) compared to CHATGPT (16%).\nQuestions with false premises pose a hurdle for LLMS:\nAll models struggle on questions with\nfalse premises, and using larger models does not increase accuracy for T5 and PALM (\u201cflat scaling\u201d),\nwith performance within the range of 0.0% to 1.6%. GPT-3.5, CHATGPT, and GPT-4 demonstrate\nmuch superior accuracies to all other models, achieving accuracies between 25.8% to 42.7% under\nSTRICT and 32.3% to 66.9% under RELAXED. CHATGPT performs the best under STRICT (42.7%)\nwhile GPT-4 is the most accurate model under RELAXED (66.9%), with an impressive accuracy of\n83.9% on questions about knowledge before 2022. These results suggest that OPENAI\u2019s models are\nlikely trained to cope with false-premise questions.\nCOT increases hallucination:\nOverall, FEW-SHOT and COT prompting are beneficial for large\nmodels and sometimes advantageous for moderately-sized models on questions with valid premises,\nespecially on questions about never-changing or old knowledge. Under STRICT, FEW-SHOT and\nCOT yields +36.1% and +26.9% respective accuracy improvement over zero-shot prompting with\nPALM 540B on questions involving knowledge before 2022 (+21.9% and +29.7% under RELAXED).\nCOT largely demonstrates superior performance compared to FEW-SHOT under RELAXED, whereas\nFEW-SHOT obtains better results under STRICT, as COT introduces more room for hallucination.\nMulti-hop reasoning is challenging for several models:\nT5 LARGE and XL are incapable of\ndealing with multi-hop questions, while FLAN-PALM 540B, CODEX, and GPT-3.5 suffer the most\nwhen switching from one-hop to multi-hop questions. GPT-4 remains stable across these two types of\nquestions (with a difference of less than 2% in accuracy across settings). See Appendix D for details.\n4\nPROMPTING SEARCH ENGINE-AUGMENTED LANGUAGE MODELS\nThe low accuracies reported in the previous section are largely unsurprising, as none of the models\nwe evaluated had access to real-time information. In this section, we evaluate the impact of search\n5\nPreprint\n {source_webpage}\n {publication_date}\n {title}\n {text_snippet}\n \n{highlighted_words}\n{demonstrations}   # details omitted for brevity\n {question}\n{retrieved_evidences}   # chronological order\nquestion: {question}\nanswer: {reasoning_and_answer}\nFigure 3: FRESHPROMPT\u2019s format. We cast all retrieved evidences into a unified format with useful\ninformation, including source webpage, date, title, text snippet, and highlighted words (left). Few-shot\ndemonstrations are provided at the beginning of the prompt. Each demonstration shows the model an\nexample question and a list of retrieved evidences for the question, followed by some reasoning over\nthe evidences to figure out the most relevant and up-to-date answer (right).\nengine augmentation to LLMS on FRESHQA. We present FRESHPROMPT, a simple few-shot prompting\nmethod that substantially boosts FRESHQA performance of an LLM by incorporating relevant and\nup-to-date information retrieved from a search engine (GOOGLE SEARCH) into the prompt.\n4.1\nFRESHPROMPT\nOur FRESHPROMPT method leverages a text prompt to (1) introduce contextually relevant and up-to-\ndate information (including answers to relevant questions) from a search engine to a pre-trained LLM,\nand (2) teach the model to reason over retrieved evidences. More specifically, given a question q, we\nfirst use q verbatim to query a search engine, in our case GOOGLE SEARCH.10 We retrieve all of the\nsearch results, including the answer box, organic results, and other useful information, such as the\nknowledge graph, questions and answers from crowdsourced QA platforms, and related questions\nthat search users also ask (see Figure 9 in Appendix F). For each of these results, we extract the\nassociated text snippet x along with other information, such as source s (e.g., WIKIPEDIA), date d,\ntitle t, highlighted words h, and then create a list of k retrieved evidences E = {(s, d, t, x, h)}.\nThese evidences are then cast into a common format (Figure 3, left) and used to condition the model\nthrough in-context learning. To encourage the model to focus on more recent evidences, in line with\nrecent findings (Liu et al., 2023a), we sort the evidences E in the prompt from oldest to newest.\nTo help the model to \u201cunderstand\u201d the task and the desired output, we provide few-shot demonstrations\nof input-output exemplars at the beginning of the input prompt. Each demonstration shows the model\nan example question and a list of retrieved evidences for the question, followed by a chain-of-thought\nreasoning over the evidences to figure out the most relevant and up-to-date answer (Figure 3, right).\nAlthough we include a few exemplars of questions with false premises in the demonstrations, we also\nexperiment with an explicit false premise check in the prompt: \u201cPlease check if the question contains\na valid premise before answering\u201d. Figure 10 in Appendix G shows a realistic prompt.\n4.2\nEXPERIMENT SETUP\nWe closely follow the setup in Section 3 except in cases where we lack control over the model\u2019s\ndecoding via an API (e.g., PERPLEXITY.AI). Some of the models we evaluate can potentially change\nover time, which presents a challenge to the reproducibility of our evaluation results; thus, we\nevaluate all models on the same date of April 26, 2023. In addition to GPT-3.5 and GPT-4, we\nevaluate GOOGLE SEARCH by simply querying GOOGLE SEARCH and using the answer in the answer\nbox (if any) or the text snippet of the top-1 search result; PERPLEXITY.AI (PPLX.AI), an answer\nengine that combines an LLM and a search engine to generate useful responses to users\u2019 queries;11\nand SELF-ASK (Press et al., 2022), a method that uses few-shot in-context learning to teach an LLM\nto decompose each question into simpler sub-questions that are answered via GOOGLE SEARCH.12\nFRESHPROMPT setup: We apply FRESHPROMPT to both GPT-3.5 and GPT-4 by sequentially in-\ncorporating the following retrieved evidences into the input prompt: o organic search results, r\n10We scrape the results from GOOGLE SEARCH using SERPAPI (https://serpapi.com).\n11https://www.perplexity.ai. At the time of evaluation, PPLX.AI was a combination of GPT-3.5 and BING\nSEARCH, and was able to provide both concise and detailed answers. We evaluated its concise answers.\n12We use the few-shot prompt provided by SELF-ASK\u2019s authors and apply it to both GPT-3.5 and GPT-4. For\nsimplicity, we evaluate solely the final answer from SELF-ASK, disregarding intermediate answers.\n6\nPreprint\nTable 1: Accuracy of different search engine-augmented LLMS on FRESHQA under STRICT (no\nhallucination) evaluations. Models benchmarked on the same date of April 26, 2023. We report\naccuracy across different categories of questions, including fast-changing (fast), slow-changing (slow),\nnever-changing (never), false-premise, questions that involve knowledge before 2022 (< 2022) and\nsince 2022 (\u2265 2022), one-hop (1-hop) and multi-hop (m-hop) questions. + indicates a model with\naccess to the current date. UTD stands for \u201cup-to-date\u201d.\nModel (size)\nknowl.\nall\nvalid premise\nfalse premise\ncutoff\nall\nfast\nslow\nnever\n< 2022\n\u2265 2022\n1-hop\nm-hop\nall\n< 2022\ncomparison against baselines\nGOOGLE SEARCH (N/A)\nUTD\n39.6\n48.9\n32.0\n46.4\n68.3\n67.4\n37.9\n55.6\n32.4\n11.3\n9.7\nGPT-3.5 (N/A)\n2021\n26.0\n26.1\n4.0\n15.2\n58.7\n61.0\n5.1\n28.0\n21.3\n25.8\n34.4\nGPT-3.5 + SELF-ASK (N/A)\nUTD\n41.6\n51.1\n36.8\n43.2\n73.0\n73.8\n37.4\n52.2\n48.1\n12.9\n17.2\nGPT-3.5 + FRESHPROMPT\nUTD\n56.0\n62.5\n46.4\n60.8\n80.2\n71.6\n57.0\n68.7\n47.2\n36.3\n43.0\nPPLX.AI (N/A)\nUTD\n52.2\n57.2\n38.4\n53.6\n79.4\n73.0\n47.7\n63.8\n40.7\n37.1\n38.7\nGPT-4 (N/A)\n2021+\n28.6\n26.9\n12.0\n4.0\n64.3\n58.2\n8.1\n27.2\n25.9\n33.9\n41.9\nGPT-4 + SELF-ASK (N/A)\nUTD\n47.8\n47.1\n39.2\n46.4\n55.6\n51.8\n44.3\n43.7\n55.6\n50.0\n61.3\nGPT-4 + FRESHPROMPT\nUTD\n75.6\n77.1\n59.2\n77.6\n94.4\n88.7\n70.2\n81.3\n66.7\n71.0\n77.4\nsensitivity and ablation studies\nGPT-3.5 (N/A)\n2021\n26.0\n26.1\n4.0\n15.2\n58.7\n61.0\n5.1\n28.0\n21.3\n25.8\n34.4\nGPT-3.5 + FRESHPROMPT\nUTD\n56.0\n62.5\n46.4\n60.8\n80.2\n71.6\n57.0\n68.7\n47.2\n36.3\n43.0\nw/ PREMISE CHECK\nUTD\n35.2\n27.1\n14.4\n28.0\n38.9\n36.2\n21.7\n31.0\n17.6\n59.7\n67.7\nGPT-4 (N/A)\n2021+\n28.6\n26.9\n12.0\n4.0\n64.3\n58.2\n8.1\n27.2\n25.9\n33.9\n41.9\nGPT-4 w/ SNIPPETS ONLY & SEARCH ORDER\nUTD\n74.0\n75.5\n56.8\n75.2\n94.4\n87.9\n68.1\n79.9\n64.8\n69.4\n77.4\nGPT-4 w/ SNIPPETS ONLY & TIME ORDER\nUTD\n74.8\n75.5\n58.4\n74.4\n93.7\n87.9\n68.1\n79.9\n64.8\n72.6\n82.8\nGPT-4 w/ SNIPPETS ONLY & RANDOM ORDER\nUTD\n72.4\n73.7\n56.8\n69.6\n94.4\n87.9\n65.1\n78.4\n62.0\n68.5\n76.3\nGPT-4 + FRESHPROMPT\nUTD\n75.6\n77.1\n59.2\n77.6\n94.4\n88.7\n70.2\n81.3\n66.7\n71.0\n77.4\nw/ PREMISE CHECK\nUTD\n75.0\n74.2\n56.8\n76.0\n89.7\n85.1\n67.7\n79.5\n61.1\n77.4\n79.6\nw/o ANSWER BOX\nUTD\n74.2\n74.7\n57.6\n74.4\n92.1\n88.7\n66.4\n79.1\n63.9\n72.6\n78.5\nw/o ANSWER BOX & RELEVANT INFO\nUTD\n72.4\n72.9\n54.4\n71.2\n92.9\n87.2\n64.3\n78.0\n60.2\n71.0\n78.5\nw/ 1 EVIDENCE\nUTD\n61.4\n60.9\n40.0\n55.2\n87.3\n79.4\n49.8\n66.8\n46.3\n62.9\n75.3\nw/ 5 EVIDENCES\nUTD\n70.6\n72.1\n56.0\n69.6\n90.5\n81.6\n66.4\n78.0\n57.4\n66.1\n73.1\nw/ 15 EVIDENCES\nUTD\n77.6\n78.5\n60.8\n78.4\n96.0\n88.7\n72.3\n81.7\n70.4\n75.0\n80.6\nw/ 15 DEMONSTRATIONS\nUTD\n74.6\n75.5\n56.8\n76.0\n93.7\n87.9\n68.1\n79.9\n64.8\n71.8\n76.3\nw/ LONG DEMONSTRATION ANSWERS\nUTD\n73.0\n72.6\n55.2\n71.2\n91.3\n83.7\n66.0\n77.6\n60.2\n74.2\n81.7\nrelated questions that search users also ask, a questions and answers from crowdsourced QA plat-\nforms, and the snippets from the knowledge graph and answer box (if available). These evidences\nare arranged in sequence up to the end of the prompt. Given the models\u2019 context limit, we only\nkeep the top n evidences (closer to the end of the prompt) after sorting them based on the cor-\nresponding date. Unless otherwise specified, we use (o, r, a, n, m) = (10, 2, 2, 5) for GPT-3.5,\nand (o, r, a, n, m) = (10, 3, 3, 10) for GPT-4. Additionally, we include m = 5 question-answer\ndemonstrations at the beginning of the prompt.\n4.3\nRESULTS AND DISCUSSION\nFRESHPROMPT significantly improves FRESHQA accuracy:\nTable 1 presents concrete numbers\nunder STRICT (see Appendix H for results under RELAXED). FRESHPROMPT offers large improvements\nover the vanilla GPT-3.5 and GPT-4 across the board. GPT-4 + FRESHPROMPT achieves absolute\naccuracy improvements of 47% and 31.4% over GPT-4 under STRICT and RELAXED, respectively.\nThe reduction in the absolute accuracy gap between STRICT and RELAXED (from 17.8% to 2.2%)\nalso suggests that FRESHPROMPT dramatically diminishes the presence of outdated and hallucinated\nanswers. Unsurprisingly, the most significant improvements for both GPT-3.5 and GPT-4 are on\nthe categories of fast-changing and slow-changing questions, which both concern recent knowledge.\nThat said, questions about old knowledge also benefit from FRESHPROMPT. For example, GPT-4 +\nFRESHPROMPT yields a +30.5% higher accuracy than GPT-4 on questions with valid premises that\ninvolve knowledge before 2022 (+9.9% under RELAXED). Additionally, FRESHPROMPT produces\nnotable gains on false-premise questions (+37.1% and +8.1% respective accuracy improvements\nunder STRICT and RELAXED for GPT-4).\nFRESHPROMPT outperforms other search-augmented methods by a large margin:\nGPT-4 +\nFRESHPROMPT demonstrates superior accuracy across question types, surpassing all other methods by\na substantial margin. Its best variant (with 15 retrieved evidences per question) achieves impressive\noverall accuracies of 77.6% and 79.0% under STRICT and RELAXED, respectively. GPT-3.5 + FRESH-\nPROMPT surpasses PPLX.AI and SELF-ASK (all performed on top of GPT-3.5) in overall accuracy\nby +3.8% and +14.4% under STRICT. Under RELAXED, however, PPLX.AI achieves a +4.2% higher\n7\nPreprint\noverall accuracy than GPT-3.5 + FRESHPROMPT, which is a large part due to its superior accuracy on\nfalse-premise questions (58.1% vs. 41.1%). The large accuracy gap of 14.0% between STRICT and\nRELAXED for PPLX.AI suggests that its outputs contain a large amount of hallucination. Overall, all\nsearch-engine augmented approaches (SELF-ASK, PPLX.AI, and FRESHPROMPT) provide significant\ngains across question types over vanilla GPT-3.5 and GPT-4. GOOGLE SEARCH generally provides\nbetter results than both GPT-3.5 and GPT-4, except on questions with false premises, but lags far\nbehind PPLX.AI and GPT-3.5/GPT-4 + FRESHPROMPT across the board.\nThe premise check boosts accuracy on false-premise questions but can hurt accuracy on those\nwith valid premises:\nAs discussed in Section 3.1, OPENAI\u2019s LLMS such as GPT-3.5 and GPT-4 are\nlikely tuned to handle false-premise questions, and this is also true for PPLX.AI. Additionally, we\nempirically find that several LLMS possess the ability to debunk a false-premise question if explicitly\nasked, e.g.. \u201cPlease check if the question contains a valid premise before answering\u201d. Adding this\npremise check to GPT-3.5 and GPT-4 yields +23.4% and +6.4% respective accuracy improvement\non false-premise questions under STRICT (+22.6% and +11.3% under RELAXED). However, this is\nharmful for GPT-3.5 with regard to other question types, decreasing overall accuracy by 20.8% and\n21% under STRICT and RELAXED, respectively. This is not a problem for GPT-4, with a slight decrease\nof 0.6% under STRICT and a slight increase of and 1.2% under RELAXED.\nHaving more relevant and up-to-date evidences at the end of the input context is helpful:\nWe\nalso analyze how the order of the evidences in the prompt impacts GPT-4\u2019s accuracy. Our results\nshow that using the order returned by GOOGLE SEARCH (SEARCH ORDER, top search results at the\nend of the input context) or sorting the evidences by their associated date information (TIME ORDER,\nmore recent results at the end) generally results in better accuracy compared to using a random order\n(RANDOM ORDER), with up to a +2.2% higher overall accuracy in STRICT and RELAXED. Using only\nthe text snippet for each evidence without additional information (such as source, date, etc.) as in\nGPT-4 + FRESHPROMPT slightly reduces accuracy, with less than 1% in both settings.\nAdditional retrieved information beyond the organic search results provides further gains:\nIncorporating additional retrieved evidences other than the organic search results, such as the answer\nbox or related questions that search users also ask, is helpful. Removing the answer box decreases\nGPT-4 + FRESHPROMPT\u2019s overall accuracy under STRICT by 1.4% (1.6% under RELAXED). Removing\nboth the answer box and other relevant information (including related questions) reduces GPT-4 +\nFRESHPROMPT\u2019s overall accuracy by 3.2% (3.0% under RELAXED).\nIncreasing the number of retrieved evidences further improves FRESHPROMPT:\nWe explore the\neffect of the number of retrieved evidences for each question as well as the number of demonstrations\nby varying these numbers in our experiments with GPT-4. Note that our default setting for GPT-4 +\nFRESHPROMPT uses 10 retrieved evidences for each question and 5 demonstrations. Our results suggest\nthat the number of retrieved evidences for each question is the most important ingredient for achieving\nhighest accuracy. Under STRICT, increasing this number from 1 to 5, 10, and 15 leads to corresponding\noverall accuracy improvements of +9.2%, +14.2%, and +16.2%, respectively. This suggests that\nGPT-4 is able to efficiently handle an increasing number of retrieved evidences (including conflicting\nanswers) and ground its responses into the most factual and up-to-date information. On the other\nhand, increasing the number of demonstrations from 5 to 15 slightly hurts accuracy in both evaluation\nsettings (1% decrease in overall accuracy under STRICT).\nVerbose demonstrations improve on complex questions but also increase hallucination:\nTo\nevaluate the effect of the writing style of the answer (including the reasoning) in each demonstration,\nwe manually rewrite these answers into a more verbose version (LONG DEMONSTRATION ANSWERS).\nOur manual inspection reveals that using more verbose demonstration answers may be helpful when\ndealing with complex questions but can be more harmful as it provides room for hallucination (a\ndecrease of 2.6% in overall accuracy under STRICT).\n5\nRELATED WORK\nKnowledge augmented LLMS:\nMany prior works study semi-parametric knowledge augmentation\nin LLMS via additional fine-tuning (Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022;\nIzacard et al., 2022), while others advocate for knowledge generation instead of retrieval (Yu et al.,\n2023a; Sun et al., 2023). FRESHPROMPT aligns with a recent emerging trend in QA applications that\naugments LLMS\u2019 prompts with knowledge retrieved from search engines for real-time alignment to\ncurrent and factual information (Nakano et al., 2021; Lazaridou et al., 2022; Menick et al., 2022; Yao\n8\nPreprint\net al., 2022; Press et al., 2022; Khattab et al., 2022; Schick et al., 2023; Luo et al., 2023). Similar to\nour method, Lazaridou et al. (2022) proposed a few-shot in-context learning approach that inserts\ndocuments from GOOGLE SEARCH into the prompt. We do not compare to this method due to its\nexpensive inference cost, as it chunks retrieved documents into evidence paragraphs and performs\nk = 50 inference calls to the LLM to generate k answers followed by LLM reranking. In contrast,\nFRESHPROMPT only performs a single inference call to the LLM. SELF-ASK (Press et al., 2022) also\nuses few-shot in-context learning to teach an LLM to ask itself follow-up questions before answering\nthe initial question, although it focuses more on decomposition.\nTime-sensitive QA:\nFRESHQA aligns with a growing body of work on benchmarking LLMS\u2019\ntemporal reasoning capabilities (Chen et al., 2021b; Zhang & Choi, 2021; Liska et al., 2022; Kasai\net al., 2022). Chen et al. (2021b) created TIMEQA by extracting evolving facts from WIKIDATA along\nwith aligned WIKIPEDIA passages to synthesize 20K timestamped question-answer pairs. Zhang &\nChoi (2021) constructed SITUATEDQA by annotating 9K realistic questions from existing open-domain\nQA datasets with temporal context (i.e., timestamps). STREAMINGQA (Liska et al., 2022) consists of\nboth LLM-generated and human-written questions (146K total questions) answerable from a corpus\nof timestamped news articles. Also related is the dynamic REALTIMEQA benchmark (Kasai et al.,\n2022), which evaluates models weekly on a set of around 30 multiple-choice questions about new\nevents extracted from news websites. In contrast, FRESHQA contains a fixed set of human written\nopen-ended questions whose answers by nature can change based on new developments in the world\nand thus offers a complementary generative evaluation of time-sensitive QA.\nQA over questionable or counterfactual premises:\nRecent work has also introduced QA bench-\nmarks with questionable premises (Yu et al., 2023c; Kim et al., 2023) or counterfactual premises (Yu\net al., 2023b). CREPE (Yu et al., 2023c) consists of 8400 Reddit questions (of which 25% questions\ncontain false premises annotated by human workers) split into train/dev/test sets. Kim et al. (2023)\nconstructed (QA)2, an evaluation set of 602 questions based on frequent search engine queries, which\nare annotated by expert annotators and crowdworkers, and evenly divided between those with and\nwithout questionable premises. Consistent with these efforts, we find that current LLMS struggle\nwith handling false premise questions; additionally, several LLMS are able to debunk a false-premise\nquestion if explicitly asked to check for the premise\u2019s validity. Similar to above, these benchmarks\nare complementary and combining them is a promising direction for future work.\n6\nLIMITATIONS AND FUTURE WORK\nOne obvious challenge with FRESHQA is the need for regular answer updating by the maintainers;\nin the interim period between updates, the answers to some questions might become stale. This\ncould be addressed by support from the open-source community (e.g., updates via GITHUB pull\nrequests). On the method side, FRESHPROMPT interfaces with GOOGLE SEARCH, and it is unclear how\nit performs with other search engines for which some types of context (e.g., answer boxes) are not\navailable. Additionally, we only perform one search query per question, and thus our method could\nbe further improved via question decomposition and multiple search queries (Khattab et al., 2022).\nSince FRESHQA consists of relatively simple English language questions, it is also unclear how well\nFRESHPROMPT performs in the context of multilingual/cross-lingual QA and long-form QA (Fan et al.,\n2019). Finally, FRESHPROMPT relies on in-context learning and thus may underperform approaches\nthat fine-tune the base LLM on new knowledge.\n7\nCONCLUSION\nOur work offers a fine-grained and exhaustive evaluation of the capabilities of modern LLMS to\nadapt to ever-changing world knowledge with and without search engine augmentation. In the\nprocess, we develop a new dataset\u2014FRESHQA\u2014of 600 questions that test a broad range of reasoning\nabilities, from the incorporation of fast-changing knowledge to identification of questions with\nfalse premises. Our two-mode evaluation also provides a way to measure both correctness and\nhallucination. Additionally, we propose a simple few-shot in-context learning algorithm called\nFRESHPROMPT that incorporates relevant evidences retrieved from GOOGLE SEARCH into the prompt of\nan LLM. FRESHPROMPT significantly improves performance over competing search engine-augmented\napproaches on FRESHQA, and an ablation reveals that factors such as the number of incorporated\nevidences and their order impact the correctness of LLM-generated answers. We release FRESHQA\nand commit to updating its answers regularly to facilitate future research.\n9\nPreprint\n8\nACKNOWLEDGEMENTS\nWe thank Colin Raffel, Hamed Zamani, and Subhransu Maji for helpful discussion and feedback.\nWe would also like to thank Chengrun Yang, Xinyun Chen for their insightful comments on this\nmanuscript. Finally, we are grateful to the following people for their contributions to creating our\nFRESHQA dataset: Marzena Karpinska, Dustin Tran, Daniel Cer, Sam Fullerton, Elizabeth Clark,\nNishant Raj, Xiaoyu Song, Yapei Chang, Yixiao Song, Nader Akoury, Ankita Gupta, Bill Ray, Chau\nPham, Wenlong Zhao, Maximilian Mozes, Simeng Sun, Ronan Salz, Kalpesh Krishna, Katherine Thai,\nKanishka Misra, Salaheddin Alzu\u2019bi, Erica Cai, Thibault Sellam, Jiao Sun, Dhruv Agarwal, Tessa\nMasis, Andrew Drozdov, Brian Lester, George Wei, Naveen Jafer Nizar, Shufan Wang, Youngwoo\nKim, and Shib Sankar Dasgupta. This project was partially supported by award IIS-2046248 from\nthe National Science Foundation (NSF), as well as NSF\u2019s CLOUDBANK program.\nREFERENCES\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\nVinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. Improving\nlanguage models by retrieving from trillions of tokens. In Proceedings of the 39th International\nConference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning\nResearch (PMLR), pp. 2206\u20132240. PMLR, 2022. URL https://proceedings.mlr.press/\nv162/borgeaud22a.html.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In Advances in Neural Information\nProcessing Systems (NeurIPS), volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021a. URL https://arxiv.\norg/abs/2107.03374.\nWenhu\nChen,\nXinyi\nWang,\nWilliam\nYang\nWang,\nand\nWilliam\nYang\nWang.\nA\ndataset for answering time-sensitive questions.\nIn Proceedings of the Neural Infor-\nmation Processing Systems Track on Datasets and Benchmarks (NeurIPS), volume 1,\n2021b.\nURL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/\npaper/2021/file/1f0e3dad99908345f7439f8ffabdffc4-Paper-round2.pdf.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https:\n//arxiv.org/abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022. URL https://arxiv.org/abs/2210.11416.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL), pp. 3558\u20133567, 2019. URL https://aclanthology.org/\nP19-1346.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In Proceedings of the 37th International Conference on Machine\n10\nPreprint\nLearning (ICML), volume 119 of Proceedings of Machine Learning Research (PMLR), pp. 3929\u2013\n3938. PMLR, 2020. URL https://proceedings.mlr.press/v119/guu20a.html.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. 2022. URL https://arxiv.org/abs/2208.03299.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (ACL), pp. 1601\u20131611, 2017. URL\nhttps://aclanthology.org/P17-1147.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui. Realtime qa: What\u2019s the answer right now?\n2022. URL https://arxiv.org/abs/2207.13332.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\nknowledge-intensive nlp. 2022. URL https://arxiv.org/abs/2212.14024.\nNajoung Kim, Phu Mon Htut, Samuel R. Bowman, and Jackson Petty. (QA)2: Question answering\nwith questionable assumptions. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (ACL), pp. 8466\u20138487, 2023. URL https://aclanthology.org/\n2023.acl-long.472.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-\naugmented language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115, 2022.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 3045\u20133059, November 2021. URL https://aclanthology.org/\n2021.emnlp-main.243.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel,\nand Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp tasks.\nIn\nAdvances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 9459\u2013\n9474, 2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.pdf.\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal,\nCyprien De Masson D\u2019Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-\nMcmahon, Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. StreamingQA: A benchmark for\nadaptation to new knowledge over time in question answering models. In Proceedings of the 39th\nInternational Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine\nLearning Research (PMLR), pp. 13604\u201313622. PMLR, 2022. URL https://proceedings.mlr.\npress/v162/liska22a.html.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023a. URL https://arxiv.org/abs/2307.03172.\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\n2023b. URL https://arxiv.org/abs/2304.09848.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.\nLe, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods\nfor effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. URL https://arxiv.\norg/abs/2301.13688.\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny\nFox, Helen Meng, and James Glass. Sail: Search-augmented instruction learning. 2023. URL\nhttps://arxiv.org/abs/2305.15225.\n11\nPreprint\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics (ACL), pp. 1906\u20131919, 2020. URL https://aclanthology.org/\n2020.acl-main.173.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nURL https://arxiv.org/abs/2203.11147.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. 2021. URL https://arxiv.org/abs/2112.09332.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.0877, 2023. URL https://arxiv.org/\nabs/2303.0877.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Chris-\ntiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp.\n27730\u201327744, 2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350,\n2022. URL https://arxiv.org/abs/2210.03350.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-\ntext transformer. Journal of Machine Learning Research (JMLR), 21(140):1\u201367, 2020. URL\nhttps://jmlr.org/papers/v21/20-074.html.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.04761.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. Proceedings of the 11th International Conference on Learning Representations (ICLR\n2023), 2023. URL https://openreview.net/forum?id=-cqvvvb-NkI.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. 2022. URL https://arxiv.org/\nabs/2210.03629.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong\ncontext generators. Proceedings of the 11th International Conference on Learning Representations\n(ICLR 2023), 2023a. URL https://openreview.net/forum?id=fB0hRu9GZUS.\nWenhao Yu, Meng Jiang, Peter Clark, and Ashish Sabharwal. Ifqa: A dataset for open-domain\nquestion answering under counterfactual presuppositions. 2023b. URL https://arxiv.org/\nabs/2305.14010.\nXinyan Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi.\nCREPE: Open-domain\nquestion answering with false presuppositions. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (ACL), pp. 10457\u201310480, 2023c. URL https:\n//aclanthology.org/2023.acl-long.583.\n12\nPreprint\nMichael Zhang and Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 7371\u20137387, 2021. URL https://aclanthology.org/2021.emnlp-main.586.\n13\nPreprint\nAPPENDIX\nA\nEVALUATION PROTOCOL\nFigure 4 shows specific examples of each evaluation criteria.\nB\nINTER-RATER AGREEMENT AND AUTOMATIC EVALUATION\nTwo authors independently evaluated a randomly sampled subset of 100 answers across models\n(including 50 questions with valid premises and 50 questions with false premises) in both modes\nRELAXED and STRICT.\nTo facilitate future evaluations, we also develop FRESHEVAL, a simple automatic metric that uses\nfew-shot in-context learning to teach an LLM to judge model responses. In each evaluation, the model\nis conditioned on a given question, a list of valid answers for the question, and a model response,\nand is then expected to generate a comment on the correctness of the response, followed by a final\njudgement. At the beginning of each input prompt, we also provide an instruction of the evaluation\ntask, and sample comments and evaluations of the examples in Figure 4 as demonstrations.13 See\nFigure 5 and Figure 6 for FRESHEVAL\u2019s prompts for RELAXED and STRICT evaluations, and Figure 7\nfor FRESHEVAL\u2019s sample output for STRICT evaluation.\nTable 2 reports the inter-rater agreement between the two human raters, and between FRESHEVAL\nand each human rater, in terms of exact accuracy. The two human raters had an agreement of 99%\nfor RELAXED and 96% for STRICT, while FRESHEVAL achieved an average agreement of 96.5% with\nhuman evaluations for RELAXED and 96% for STRICT. Overall, the high accuracies demonstrate that\nour evaluation protocol is reproducible and reliable, and FRESHEVAL can be used in place of human\nevaluation on FRESHQA.\nC\nADDITIONAL EXPERIMENT SETUP DETAILS FOR SECTION 3\nTo increase reproducibility, we select the most likely token at every decoding timestep (i.e., with\na temperature of 0) and generate a maximum number of 256 tokens for all models. Note that the\nAPI for some models is non-deterministic by default, even with a temperature of 0. For non-chat\nmodels that were not pre-trained with a QA task, we feed them a text prompt of the format: \u201cQ:\n<question>\\nA: \u201d (\u201c\\ n\u201d is the new line character).\nFor OPENAI models, we use the 2023-03-15-preview API in AZURE OPENAI SERVICE. We use the\nmodel names text-davinci-003, code-davinci-002, gpt-3.5-turbo, and gpt-4 for GPT-3.5,\nCODEX, CHATGPT, and GPT-4, respectively.\nD\nADDITIONAL EXPERIMENT RESULTS FOR SECTION 3\nTable 3 and Table 4 show the accuracy of different LLMS on FRESHQA under STRICT (no hallucination)\nand RELAXED evaluations, respectively.\nE\nCHATGPT/GPT-4\u2019S AWARENESS OF RECENT KNOWLEDGE\nAlthough CHATGPT and GPT-4 were originally trained in 2021, our manual evaluation suggests\nthat they have been exposed to data containing information beyond their knowledge cutoff date in\nSeptember, 2021. Figure 8 indicates that CHATGPT is aware of the recent Russian invasion of Ukraine\non February 24, 2022.\n13In our experiments, we found that using separate prompts for RELAXED and STRICT evaluations resulted in\nbetter performance compared to using a single, combined prompt for both evaluation modes. We also found\nthat additionally incorporating retrieved evidences for the question into the prompt did not improve inter-rater\nagreement between FRESHEVAL and human raters.\n14\nPreprint\nF\nGOOGLE SEARCH RESULTS\nFigure 9 shows different types of search results from GOOGLE SEARCH for given a query.\nG\nA REALISTIC PROMPT FOR FRESHPROMPT\nFigure 10 displays a realistic prompt for FRESHPROMPT.\nH\nADDITIONAL EXPERIMENT RESULTS FOR SECTION 4\nTable 5 presents the accuracy of different search engine-augmented LLMS on FRESHQA under\nRELAXED.\n15\nPreprint\nID\nQuestion\nType\nPrimary answer  \n(as of this writing)\nModel response\nRelaxed Strict\nExplanation\n1\nHow old is the world's oldest \nveri\ufb01ed living person?\nfast-changing\n116 years old\nThe oldest veri\ufb01ed living person is Maria Branyas Morera, who was born on \nMarch 4, 1907, making her 116 years old.\nthe primary answer is correct, and all the provided \ninformation is accurate and up-to-date\n2\nWhen did the UK adopt the \nEuro?\nfalse-premise\nThe UK has never \nadopted the Euro.\nThe UK has never adopted the Euro as its o\ufb03cial currency. The country has \nretained the British pound sterling (GBP) as its currency.\nthe response debunks the question's false \npremise, and all the provided information is \naccurate and up-to-date\n3\nHow much longer will Brittney \nGriner spend in the Russian \nprison?\nfalse-premise\nShe was released in \n2022.\nI'm sorry, but I have no information to suggest that Brittney Griner is currently in a \nRussian prison.\na con\ufb01dent and de\ufb01nitive answer is required\n4\nWhich language has the third \nlargest population by number \nof native speakers?\nslow-changing\nEnglish\n1. Mandarin 2. Spanish 3. English\nthough the correct answer can be obviously \ninferred from the response, a response without an \nexplicit statement is less desirable\n5\nHas Virginia Woolf's novel \nabout the Ramsay family \nentered the public domain in \nthe United States?\nnever-changing Yes\n1927's To the Lighthouse is in the public domain in the United States. The \n\ufb01nal answer is To the Lighthouse.\nthe \ufb01nal answer when standing alone must be \ncorrect\n6\nIs Tesla's stock price above \n$200 a share?\nfast-changing\nYes\nTesla's stock price is above $200 a share. The stock is trading at a price of $120.\ncon\ufb02icting information\n7\nWhen did Lionel Messi win his \nsecond FIFA World Cup?\nfalse-premise\nMessi has not won a \nsecond FIFA World \nCup.\nThis question does not contain a valid premise. Lionel Messi has never won a \nFIFA World Cup.\ndespite debunking the question's false premise, \nthe response inaccurately shapes one's \nknowledge perception\n8\nHow many times has Jeremy \nCorbyn served as Prime \nMinister of the UK?\nfalse-premise\nCorbyn has never \nserved as Prime \nMinister of the UK.\nZero.\na rebuttal is always required for false-premise \nquestions\n9\nWho founded Amazon?\nnever-changing Je\ufb00 Bezos\n100% of people said Je\ufb00.\na complete name or commonly recognized name \nis required\n10 How many books has Colleen \nHoover published?\nfast-changing\n25\nColleen Hoover has published over 20 books and novellas.\napproximate numbers are not accepted unless \nincluded in the ground-truth answers\n11 What is the largest model size \nof GPT-3?\nnever-changing 175B parameters\n175 billion parameters. It is a 175 billion parameter, 1.37TB, 137B token, 137B \nwordpiece, 137B BPE, 137B byte pair encoding, 137B BPE token, 137B BPE \nwordpiece, 137B BPE token, 137B BPE wordpiece,...\nthe primary answer is correct, but the response is \nill-formed\n12 How many member nations \nexist in the United Nations?\nslow-changing\n193\nAs of my knowledge cuto\ufb00 date in September 2021, there were 193 member \nnations in the United Nations.\nthe primary answer is correct, but it is unclear \nwhether the information is still up-to-date\n13 Who won the biggest single-\ntournament payday in tennis \nhistory?\nslow-changing\nNovak Djokovic\nThe biggest single-tournament payday in tennis history was won by Novak \nDjokovic at the 2021 Australian Open.\nthe primary answer is correct, but the response \ncontains hallucinated information\n14 Who was the \ufb01rst football \nplayer to score one hundred \ninternational goals?\nnever-changing Ali Daei\nAs of my knowledge cuto\ufb00 date in September 2021, Ali Daei, a former Iranian \nfootballer, was the \ufb01rst player to score 100 international goals.\nall the information is correct and it is evident that \nthe knowledge has not changed\n15 Who is the latest winner of the \nFormula 1 world \nchampionship?\nfast-changing\nMax Verstappen\nMax Verstappen won the 2022 Formula 1 world championship. He is a Dutch \nracing driver who competes in Formula One for Red Bull Racing. He is the son of \nformer Formula One driver Jos Verstappen. Verstappen started his racing career \nin karting at the age of seven. He won the 2013 European Karting Championship \nand the 2013 World Karting Championship. In 2014, he moved to single-seater \nracing, competing in the European Formula Three Championship. He won the \nchampionship in his first season, becoming the first driver to do so since Nico \nRosberg in 2005. In 2015, Verstappen moved to Formula One, driving for Toro \nRosso. He became the youngest driver to compete in Formula One at the age of \n17. He scored his first points in Formula One at the 2015 Hungarian GrandPrix. \nIn 2016, Verstappen moved to Red Bull Racing. He won his first race at the 2016 \nSpanish Grand Prix. He became the youngest driver to win a race in Formula \nOne at theage of 18. Verstappen finished the 2016 season in third place in the \ndrivers' championship. In 2017, Verstappen won four races and finished the \nseason in second place in the drivers' championship. In 2018, Verstappen won \nseven races and finished the season in second place in the drivers' \nchampionship. In 2019, Verstappen won nine races and finished the season in \nfirst place in the drivers' championship. He is the first Dutch driver to win the \nFormula One world championship.\nthe primary answer is correct, but the response \ncontains several hallucinations (e.g., Max \nVerstappen did not win the Formula Three \nEuropean Championship in 2014)\nFigure 4: FRESHQA sample evaluation. To get credit in both evaluation modes RELAXED and STRICT,\nall the information in the answer must be accurate and up-to-date (examples 1 and 2). In both modes,\nwe credit a model\u2019s response only if it provides a confident and definitive answer (example 3), or\nthe correct answer can be obviously inferred from the response (provided all other requirements are\nsatisfied, see example 4). The primary or final answer when standing alone must be accurate (example\n5). Any additional information that is provided must not contradict the primary answer (example\n6) or reshape one\u2019s perception of it (example 7). For false-premise questions, the model must point\nout the presence of a false premise to receive credit (example 8). For answers that involve names\nof entities (e.g., people), complete names or commonly recognized names are expected (example\n9). Regarding numerical answers, approximate numbers are generally not accepted unless explicitly\nincluded in the ground-truth answers (example 10). Under RELAXED, we accept ill-formed responses\n(including those in a non-English language), as well as hallucinated or outdated information that does\nnot significantly impact the primary answer; under STRICT, however, a response that contains any\nhallucination, no matter how minor, will not receive credit (examples 11, 12, and 13). Furthermore,\nwe accept a response in STRICT when the model indicates that the information might be outdated\n(e.g., \u201cAs of my knowledge cutoff date in September 2021\u201d) only if it is evident that the knowledge\nhas not changed (example 14).\n16\nPreprint\nTable 2: Inter-rater agreement between two authors (RATER 1 and RATER 2), and between FRESHEVAL\nand each human rater, in terms of exact accuracy across 100 RELAXED judgements, 100 STRICT\njudgements, and all ALL 200 judgements. In each of these three categories, in addition to the overall\naccuracy (overall), we report accuracy across questions with valid premises (vp) and questions with\nfalse premises (fp). The high accuracies demonstrate that our evaluation protocol is reproducible and\nreliable, and FRESHEVAL can be used in place of human evaluation on FRESHQA.\nRELAXED\nSTRICT\nALL\noverall\nvp\nfp\noverall\nvp\nfp\noverall\nvp\nfp\nRATER 1 vs. RATER 2\n99.0\n98.0\n100\n96.0\n100.0\n92.0\n97.5\n99.0\n96.0\nFRESHEVAL vs. RATER 1\n97.0\n98.0\n96.0\n97.0\n100.0\n94.0\n97.0\n99.0\n95.0\nFRESHEVAL vs. RATER 2\n96.0\n96.0\n96.0\n95.0\n100.0\n90.0\n95.5\n98.0\n93.0\n17\nPreprint\nPlease evaluate the response to each given question under a relaxed evaluation, where hallucinations, outdated \ninformation, and ill-formed answers are allowed, as long as the primary answer is accurate. Please credit the \nresponse only if it provides a confident and definitive answer, or the correct answer can be obviously inferred \nfrom the response. The primary or final answer when standing alone must be accurate. Any additional \ninformation that is provided must not contradict the primary answer or reshape one's perception of it. For \nfalse-premise questions, the response must point out the presence of a false premise to receive credit. For \nanswers that involve names of entities (e.g., people), complete names or commonly recognized names are \nexpected. Regarding numerical answers, approximate numbers are generally not accepted unless explicitly \nincluded in the ground-truth answers. We accept ill-formed responses (including those in a non-English \nlanguage), as well as hallucinated or outdated information that does not significantly impact the primary answer.\n# some demonstrations are omitted for brevity\nquestion: Is Tesla's stock price above $250 a share?\ncorrect answer(s): Yes\nresponse: Yes, it is. The stock price is currently at $207.\ncomment: This is a valid question. While the primary answer in the response (Yes) is accurate, the additional \ninformation contradicts the primary answer ($207 is not above $250). Thus, the response is not credited.\nevaluation: incorrect\nquestion: Who is the latest winner of the Formula 1 world championship?\ncorrect answer(s): Max Verstappen | Max Emilian Verstappen\nresponse: Max Verstappen won the 2022 Formula 1 world championship. He is a Dutch racing driver who \ncompetes in Formula One for Red Bull Racing. He is the son of former Formula One driver Jos Verstappen. \nVerstappen started his racing career in karting at the age of seven. He won the 2013 European Karting \nChampionship and the 2013 World Karting Championship. In 2014, he moved to single-seater racing, competing \nin the European Formula Three Championship. He won the championship in his first season, becoming the first \ndriver to do so since Nico Rosberg in 2005\u2026\n# the rest is omitted for brevity\ncomment: This is a valid question. Even though the response contains several instances of hallucinated \ninformation (e.g., Max Verstappen did not win the Formula Three European Championship in 2014), the primary \nanswer in the response (Max Verstappen) is still accurate. Thus, the response is credited.\nevaluation: correct\nquestion: In what year did bodybuilding become an Olympic sport?\ncorrect answer(s): Bodybuilding is not an Olympic sport.\nresponse: Bodybuilding is not currently an Olympic sport. It has been recognized by the International Olympic \nCommittee (IOC) but has not yet been included in the Olympic program. However, bodybuilding has been \nincluded in other multi-sport events such as the Pan American Games and the Asian Games.\ncomment: \nFigure 5: FRESHEVAL\u2019s prompt for RELAXED evaluation.\n18\nPreprint\nPlease evaluate the response to each given question under a strict evaluation, where no hallucinations, outdated \ninformation, or ill-formed answers are allowed. Please credit the response only if it provides a confident and \ndefinitive answer, or the correct answer can be obviously inferred from the response. The primary or final answer \nwhen standing alone must be accurate. Any additional information that is provided must not contradict the primary \nanswer or reshape one's perception of it. For false-premise questions, the response must point out the presence \nof a false premise to receive credit. For answers that involve names of entities (e.g., people), complete names or \ncommonly recognized names are expected. Regarding numerical answers, approximate numbers are generally not \naccepted unless explicitly included in the ground-truth answers. A response that contains any hallucination, no \nmatter how minor, will not receive credit. Furthermore, when the response indicates that the information might be \noutdated, we accept it only if it is evident that the knowledge has not changed (e.g., through common sense or \nwell-known facts).\n# some demonstrations are omitted for brevity\nquestion: How old is the world's oldest verified living person?\ncorrect answer(s): 116 years old | 116\nresponse: As of today {current_date} , the most up-to-date and relevant information regarding this query is as \nfollows. The oldest verified living person is Maria Branyas Morera, who was born on March 4, 1907, making her 116 \nyears old.\ncomment: This is a valid question. The primary answer in the response (116 years old) is accurate, and all the \nprovided information is accurate and up-to-date. Thus, the response is credited.\nevaluation: correct\nquestion: Who is the latest winner of the Formula 1 world championship?\ncorrect answer(s): Max Verstappen | Max Emilian Verstappen\nresponse: Max Verstappen won the 2022 Formula 1 world championship. He is a Dutch racing driver who \ncompetes in Formula One for Red Bull Racing. He is the son of former Formula One driver Jos Verstappen. \nVerstappen started his racing career in karting at the age of seven. He won the 2013 European Karting \nChampionship and the 2013 World Karting Championship. In 2014, he moved to single-seater racing, competing in \nthe European Formula Three Championship. He won the championship in his first season, becoming the first driver \nto do so since Nico Rosberg in 2005\u2026\n# the rest is omitted for brevity\ncomment: This is a valid question. While the primary answer in the response (Max Verstappen) is accurate, the \nresponse contains several instances of hallucinated information (e.g., Max Verstappen did not win the Formula \nThree European Championship in 2014). Thus, the response is not credited.\nevaluation: incorrect\nquestion: In January 2023, the NHC revised the fatality data of Hurricane Katrina, increasing the reported death \ntoll from 1,800 to what number?\ncorrect answer(s): The reported death toll decreased to 1,392.\nresponse: I am unable to provide the updated fatality data for Hurricane Katrina in January 2023, as my knowledge \nonly extends up to September 2021. Please consult a more recent source for the latest information.\ncomment: \nFigure 6: FRESHEVAL\u2019s prompt for STRICT evaluation.\n19\nPreprint\nquestion: How much did Elon Musk buy Activision Blizzard for?\ncorrect answer(s): Elon Musk has not purchased Activision Blizzard so far (Microsoft purchased Activision Blizzard \nfor US$68.7 billion in January 2022).\nresponse: As of my knowledge cutoff date in September 2021, Elon Musk did not purchase Activision Blizzard. If this \nevent occurred after that date, I am unable to provide the information.\nquestion: Which cryptocurrency has a higher market cap than Bitcoin?\ncorrect answer(s): No cryptocurrencies currently have a higher market cap than Bitcoin.\nresponse: As of my last knowledge update in September 2021, no cryptocurrency had a higher market cap than \nBitcoin. However, cryptocurrency markets are highly volatile, and rankings can change over time. To get the most \nup-to-date information, please refer to a reliable source like CoinMarketCap or CoinGecko.\nquestion: What's the world record time for running a mile?\ncorrect answer(s): 3:43.13 | 3 minutes and 43 seconds\nresponse: As of today {current_date} , the most up-to-date and relevant information regarding this query is as \nfollows. The world record time for running a mile is 3 minutes 43.13 seconds, set by Hicham El Guerrouj of Morocco \non July 7, 1999.\ncomment: The question contains a false premise. The response correctly identifies that Elon Musk has not \npurchased Activision Blizzard, which is accurate and debunks the false premise. However, the response includes \nunnecessary information about a knowledge cutoff date that is not relevant to the accuracy of the answer. The \nprimary answer is correct, but the additional information about the knowledge cutoff date could potentially \nconfuse the issue. Thus, the response is not credited.\nevaluation: incorrect\ncomment: The question contains a false premise. The response fails to provide a current answer and instead relies \non outdated information without confirming its continued accuracy. Thus, the response is not credited.\nevaluation: incorrect\ncomment: This is a valid question. The primary answer in the response (3 minutes 43.13 seconds) is accurate, and all \nthe provided information is accurate and up-to-date. Thus, the response is credited.\nevaluation: correct\nFigure 7: FRESHEVAL\u2019s sample output for STRICT evaluation.\n20\nPreprint\nTable 3: Accuracy of different LLMS on FRESHQA under STRICT (no hallucination) evaluations.\nModels benchmarked on the same date of April 26, 2023. We report accuracy across different\ncategories of questions, including fast-changing (fast), slow-changing (slow), never-changing (never),\nfalse-premise, questions that involve knowledge before 2022 (< 2022) and since 2022 (\u2265 2022),\none-hop (1-hop) and multi-hop (m-hop) questions. + indicates a model with access to the current\ndate.\nModel (size)\nknowl.\nall\nvalid premise\nfalse premise\ncutoff\nall\nfast\nslow\nnever\n< 2022\n\u2265 2022\n1-hop\nm-hop\nall\n< 2022\nwithout access to a search engine\nOPENAI CODEX (N/A)\n2021\n25.0\n31.4\n5.6\n28.0\n60.3\n64.5\n11.5\n34.7\n23.1\n5.6\n7.5\nGPT 3.5 (N/A)\n2021\n26.0\n26.1\n4.0\n15.2\n58.7\n61.0\n5.1\n28.0\n21.3\n25.8\n34.4\nCHATGPT (N/A)\n2021+\n32.0\n28.5\n7.2\n16.0\n61.9\n63.1\n7.7\n29.9\n25.0\n42.7\n52.7\nGPT 4 (N/A)\n2021+\n28.6\n26.9\n12.0\n4.0\n64.3\n58.2\n8.1\n27.2\n25.9\n33.9\n41.9\nFLAN-PALM (540B)\n2022\n23.4\n30.3\n10.4\n24.8\n55.6\n60.3\n12.3\n32.5\n25.0\n2.4\n3.2\nPALM (540B)\n2021\n7.2\n9.3\n0.8\n11.2\n15.9\n20.6\n2.6\n9.3\n9.3\n0.8\n1.1\nw/ FEW-SHOT\n20.0\n26.3\n5.6\n19.2\n54.0\n56.7\n8.1\n25.7\n27.8\n0.8\n1.1\nw/ COT\n15.4\n19.1\n0.8\n9.6\n46.8\n47.5\n2.1\n20.5\n15.7\n4.0\n5.4\nPALMCHILLA (62B)\n2022\n12.2\n16.0\n2.4\n15.2\n30.2\n35.5\n4.3\n17.2\n13.0\n0.8\n1.1\nPALM (62B)\n2021\n6.2\n8.2\n1.6\n8.8\n14.3\n16.3\n3.4\n7.8\n9.3\n0.0\n0.0\nw/ FEW-SHOT\n12.8\n16.8\n3.2\n15.2\n31.7\n35.5\n5.5\n17.9\n13.9\n0.8\n1.1\nw/ COT\n7.0\n9.0\n0.8\n6.4\n19.8\n21.3\n1.7\n10.1\n6.5\n0.8\n1.1\nPALM (8B)\n2021\n5.6\n7.5\n0.8\n5.6\n16.0\n16.2\n2.1\n8.6\n4.6\n0.0\n0.0\nw/ FEW-SHOT\n8.4\n11.2\n0.8\n9.6\n23.0\n24.8\n3.0\n14.2\n3.7\n0.0\n0.0\nw/ COT\n7.8\n10.4\n0.0\n6.4\n24.6\n24.8\n1.7\n11.2\n8.3\n0.0\n0.0\nFLAN-T5 XXL (11B)\n2022\n6.6\n8.8\n3.2\n10.4\n12.7\n13.5\n6.0\n10.1\n5.6\n0.0\n0.0\nT5 XXL (11B)\n2019\n7.0\n8.8\n2.4\n4.8\n19.0\n16.3\n4.3\n10.4\n4.6\n1.6\n2.2\nw/ FEW-SHOT\n8.4\n11.2\n5.6\n11.2\n16.7\n17.7\n7.2\n13.4\n5.6\n0.0\n0.0\nw/ COT\n6.2\n8.2\n2.4\n6.4\n15.9\n15.6\n3.8\n8.6\n7.4\n0.0\n0.0\nT5 XL (3B)\n2019\n4.4\n5.9\n2.4\n4.8\n10.3\n10.6\n3.0\n7.5\n1.9\n0.0\n0.0\nw/ FEW-SHOT\n6.0\n8.0\n4.0\n8.8\n11.1\n13.5\n4.7\n8.2\n7.4\n0.0\n0.0\nw/ COT\n2.8\n3.7\n2.4\n1.6\n7.1\n7.8\n1.3\n4.1\n2.8\n0.0\n0.0\nT5 LARGE (770M)\n2019\n2.6\n3.5\n0.8\n4.0\n5.6\n5.7\n2.1\n3.7\n2.8\n0.0\n0.0\nw/ FEW-SHOT\n0.8\n1.1\n0.0\n0.0\n3.2\n2.8\n0.0\n1.1\n0.9\n0.0\n0.0\nw/ COT\n0.8\n1.1\n0.8\n0.0\n2.4\n2.1\n0.4\n1.1\n0.9\n0.0\n0.0\n21\nPreprint\nTable 4: Accuracy of different LLMS on FRESHQA under RELAXED evaluations. Models bench-\nmarked on the same date of April 26, 2023. We report accuracy across different categories of\nquestions, including fast-changing (fast), slow-changing (slow), never-changing (never), false-\npremise, questions that involve knowledge before 2022 (< 2022) and since 2022 (\u2265 2022), one-hop\n(1-hop) and multi-hop (m-hop) questions. + indicates a model with access to the current date.\nModel (size)\nknowl.\nall\nvalid premise\nfalse premise\ncutoff\nall\nfast\nslow\nnever\n< 2022\n\u2265 2022\n1-hop\nm-hop\nall\n< 2022\nwithout access to a search engine\nOPENAI CODEX (N/A)\n2021\n25.6\n32.2\n6.4\n29.6\n60.3\n66.0\n11.9\n35.4\n24.1\n5.6\n7.5\nGPT 3.5 (N/A)\n2021\n32.4\n32.4\n8.0\n28.0\n61.1\n68.1\n11.1\n34.7\n26.9\n32.3\n43.0\nCHATGPT (N/A)\n2021+\n41.4\n36.7\n10.4\n32.8\n66.7\n76.6\n12.8\n36.2\n38.0\n55.6\n66.7\nGPT 4 (N/A)\n2021+\n46.4\n39.6\n14.4\n35.2\n69.0\n80.9\n14.9\n39.2\n40.7\n66.9\n83.9\nFLAN-PALM (540B)\n2022\n23.6\n30.3\n10.4\n24.8\n55.6\n60.3\n12.3\n32.5\n25.0\n3.2\n4.3\nPALM (540B)\n2021\n12.2\n16.0\n2.4\n14.4\n31.0\n34.8\n4.7\n16.4\n14.8\n0.8\n1.1\nw/ FEW-SHOT\n20.2\n26.3\n5.6\n19.2\n54.0\n56.7\n8.1\n25.7\n27.8\n1.6\n2.2\nw/ COT\n22.8\n28.2\n4.0\n20.0\n60.3\n64.5\n6.4\n28.4\n27.8\n6.5\n8.6\nPALMCHILLA (62B)\n2022\n15.0\n19.4\n2.4\n19.2\n36.5\n43.3\n5.1\n20.1\n17.6\n1.6\n2.2\nPALM (62B)\n2021\n8.6\n11.2\n2.4\n11.2\n19.8\n22.0\n4.7\n11.6\n10.2\n0.8\n1.1\nw/ FEW-SHOT\n14.2\n18.4\n4.0\n15.2\n35.7\n39.0\n6.0\n18.7\n17.6\n1.6\n2.2\nw/ COT\n12.8\n16.2\n2.4\n15.2\n31.0\n34.8\n5.1\n17.5\n13.0\n2.4\n3.2\nPALM (8B)\n2021\n8.8\n11.2\n0.8\n11.2\n21.6\n21.1\n5.2\n13.1\n6.5\n1.6\n2.1\nw/ FEW-SHOT\n9.2\n12.2\n0.8\n10.4\n25.4\n27.0\n3.4\n15.3\n4.6\n0.0\n0.0\nw/ COT\n11.4\n15.2\n2.4\n11.2\n31.7\n32.6\n4.7\n16.8\n11.1\n0.0\n0.0\nFLAN-T5 XXL (11B)\n2022\n7.2\n9.6\n3.2\n12.0\n13.5\n14.2\n6.8\n10.8\n6.5\n0.0\n0.0\nT5 XXL (11B)\n2019\n10.8\n13.8\n3.2\n12.8\n25.4\n22.7\n8.5\n16.0\n8.3\n1.6\n2.2\nw/ FEW-SHOT\n9.0\n12.0\n5.6\n11.2\n19.0\n19.1\n7.7\n14.6\n5.6\n0.0\n0.0\nw/ COT\n13.0\n17.3\n4.0\n17.6\n30.2\n31.2\n8.9\n19.0\n13.0\n0.0\n0.0\nT5 XL (3B)\n2019\n5.8\n7.7\n4.0\n5.6\n13.5\n13.5\n4.3\n9.0\n4.6\n0.0\n0.0\nw/ FEW-SHOT\n6.0\n8.0\n4.0\n8.8\n11.1\n13.5\n4.7\n8.2\n7.4\n0.0\n0.0\nw/ COT\n5.2\n6.9\n3.2\n4.0\n13.5\n14.2\n2.6\n8.6\n2.8\n0.0\n0.0\nT5 LARGE (770M)\n2019\n4.4\n5.3\n2.4\n4.8\n8.7\n7.1\n4.3\n5.6\n4.6\n1.6\n2.2\nw/ FEW-SHOT\n0.8\n1.1\n0.0\n0.0\n3.2\n2.8\n0.0\n1.1\n0.9\n0.0\n0.0\nw/ COT\n2.2\n2.9\n0.8\n0.8\n7.1\n7.1\n0.4\n3.4\n1.9\n0.0\n0.0\n22\nPreprint\nFigure 8: Despite its knowledge cutoff date in September 2021, CHATGPT is aware of the recent\nRussian invasion of Ukraine on February 24, 2022. Questions asked on April 9, 2023.\n23\nPreprint\nanswer box\nanswer box\nrelated questions\nrelated questions\nknowledge graph\nsearch query  \nsearch query  \nquestions and answers\norganic results\norganic results\nFigure 9: GOOGLE SEARCH produces different types of search results for given a query, including the\nanswer box, organic results, and other useful information, such as the knowledge graph, questions\nand answers from crowdsourced QA platforms, and related questions that search users also ask.\nEach of these results contains an associated text snippet along with other information, such as source\nwebpage, date, title, and highlighted words.\n24\nPreprint\n{other_demonstrations}  # omitted for brevity\n{current_date}\n{retrieved_evidences}  # omitted for brevity  \nFigure 10: A realistic prompt for FRESHPROMPT. We cast all retrieved evidences into a unified format\nwith useful information, including source webpage, date, title, text snippet, and highlighted words.\nFew-shot demonstrations are provided at the beginning of the prompt. Each demonstration shows\nthe model an example question and a list of retrieved evidences for the question, followed by some\nreasoning over the evidences to figure out the most relevant and up-to-date answer.\n25\nPreprint\nTable 5: Accuracy of different search engine-augmented LLMS on FRESHQA under RELAXED evalua-\ntions. Models benchmarked on the same date of April 26, 2023. We report accuracy across different\ncategories of questions, including fast-changing (fast), slow-changing (slow), never-changing (never),\nfalse-premise, questions that involve knowledge before 2022 (< 2022) and since 2022 (\u2265 2022),\none-hop (1-hop) and multi-hop (m-hop) questions. + indicates a model with access to the current\ndate. UTD stands for \u201cup-to-date\u201d.\nModel\nknowl.\nall\nvalid premise\nfalse premise\ncutoff\nall\nfast\nslow\nnever\n< 2022\n\u2265 2022\n1-hop\nm-hop\nall\n< 2022\ncomparison against baselines\nGOOGLE SEARCH\nUTD\n47.4\n58.8\n42.4\n56.0\n77.8\n74.5\n49.4\n66.4\n39.8\n12.9\n11.8\nGPT-3.5\n2021\n32.4\n32.4\n8.0\n28.0\n61.1\n68.1\n11.1\n34.7\n26.9\n32.3\n43.0\nGPT-3.5 + SELF-ASK\nUTD\n42.0\n51.6\n36.8\n44.8\n73.0\n74.5\n37.9\n53.0\n48.1\n12.9\n17.2\nGPT-3.5 + FRESHPROMPT\nUTD\n62.0\n68.9\n51.2\n70.4\n84.9\n78.0\n63.4\n75.0\n53.7\n41.1\n49.5\nPPLX.AI\nUTD\n66.2\n68.9\n48.8\n67.2\n90.5\n85.1\n59.1\n76.1\n50.9\n58.1\n60.2\nGPT-4\n2021+\n46.4\n39.6\n14.4\n35.2\n69.0\n80.9\n14.9\n39.2\n40.7\n66.9\n83.9\nGPT-4 + SELF-ASK\nUTD\n50.4\n48.4\n40.0\n49.6\n55.6\n52.5\n46.0\n45.1\n56.5\n56.5\n69.9\nGPT-4 + FRESHPROMPT\nUTD\n77.8\n78.7\n61.6\n79.2\n95.2\n90.8\n71.5\n83.2\n67.6\n75.0\n80.6\nsensitivity and ablation studies\nGPT-3.5\n2021\n32.4\n32.4\n8.0\n28.0\n61.1\n68.1\n11.1\n34.7\n26.9\n32.3\n43.0\nGPT-3.5 + FRESHPROMPT\nUTD\n62.0\n68.9\n51.2\n70.4\n84.9\n78.0\n63.4\n75.0\n53.7\n41.1\n49.5\nw/ PREMISE CHECK\nUTD\n41.0\n33.5\n23.2\n32.0\n45.2\n44.0\n27.2\n37.7\n23.1\n63.7\n72.0\nGPT-4\n2021+\n46.4\n39.6\n14.4\n35.2\n69.0\n80.9\n14.9\n39.2\n40.7\n66.9\n83.9\nGPT-4 w/ SNIPPETS ONLY & SEARCH ORDER\nUTD\n77.6\n78.2\n59.2\n80.0\n95.2\n90.8\n70.6\n82.1\n68.5\n75.8\n83.9\nGPT-4 w/ SNIPPETS ONLY & TIME ORDER\nUTD\n77.6\n78.2\n59.2\n79.2\n96.0\n90.1\n71.1\n82.1\n68.5\n75.8\n86.0\nGPT-4 w/ SNIPPETS ONLY & RANDOM ORDER\nUTD\n75.4\n76.1\n58.4\n73.6\n96.0\n90.8\n67.2\n80.6\n64.8\n73.4\n81.7\nGPT-4 + FRESHPROMPT\nUTD\n77.8\n78.7\n61.6\n79.2\n95.2\n90.8\n71.5\n83.2\n67.6\n75.0\n80.6\nw/ PREMISE CHECK\nUTD\n78.8\n76.3\n59.2\n76.8\n92.9\n87.2\n69.8\n82.1\n62.0\n86.3\n90.3\nw/o ANSWER BOX\nUTD\n76.2\n76.6\n59.2\n76.0\n94.4\n90.1\n68.5\n81.0\n65.7\n75.0\n80.6\nw/o ANSWER BOX & RELEVANT INFO\nUTD\n74.8\n75.0\n56.0\n74.4\n94.4\n89.4\n66.4\n80.6\n61.1\n74.2\n81.7\nw/ 1 EVIDENCE\nUTD\n67.2\n67.3\n47.2\n66.4\n88.1\n85.8\n56.2\n72.0\n55.6\n66.9\n79.6\nw/ 5 EVIDENCES\nUTD\n74.2\n75.0\n56.8\n74.4\n93.7\n87.2\n67.7\n81.7\n58.3\n71.8\n77.4\nw/ 15 EVIDENCES\nUTD\n79.0\n79.5\n62.4\n80.0\n96.0\n90.1\n73.2\n83.2\n70.4\n77.4\n81.7\nw/ 15 DEMONSTRATIONS\nUTD\n77.2\n78.2\n60.0\n78.4\n96.0\n91.5\n70.2\n82.8\n66.7\n74.2\n79.6\nw/ LONG DEMONSTRATION ANSWERS\nUTD\n77.8\n77.9\n60.8\n77.6\n95.2\n90.1\n70.6\n82.8\n65.7\n77.4\n83.9\n26\n"
  },
  {
    "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency",
    "link": "https://arxiv.org/pdf/2310.03734.pdf",
    "upvote": "13",
    "text": "LEVERAGING UNPAIRED DATA FOR VISION-LANGUAGE\nGENERATIVE MODELS VIA CYCLE CONSISTENCY\nTianhong Li1\u2217\nSangnie Bhardwaj2,3\u2217\nYonglong Tian3\nHan Zhang4\nJarred Barber3\nDina Katabi1\nGuillaume Lajoie2\nHuiwen Chang5\nDilip Krishnan3\n1MIT CSAIL\n2Mila\n3Google Research\n4Google DeepMind\n5OpenAI\nABSTRACT\nCurrent vision-language generative models rely on expansive corpora of paired\nimage-text data to attain optimal performance and generalization capabilities.\nHowever, automatically collecting such data (e.g. via large-scale web scraping)\nleads to low quality and poor image-text correlation, while human annotation is\nmore accurate but requires significant manual effort and expense. We introduce\nITIT (InTegrating Image Text): an innovative training paradigm grounded in the\nconcept of cycle consistency which allows vision-language training on unpaired\nimage and text data. ITIT is comprised of a joint image-text encoder with disjoint\nimage and text decoders that enable bidirectional image-to-text and text-to-image\ngeneration in a single framework. During training, ITIT leverages a small set of\npaired image-text data to ensure its output matches the input reasonably well in\nboth directions. Simultaneously, the model is also trained on much larger datasets\ncontaining only images or texts. This is achieved by enforcing cycle consistency\nbetween the original unpaired samples and the cycle-generated counterparts. For\ninstance, it generates a caption for a given input image and then uses the caption\nto create an output image, and enforces similarity between the input and output\nimages. Our experiments show that ITIT with unpaired datasets exhibits similar\nscaling behavior as using high-quality paired data. We demonstrate image gen-\neration and captioning performance on par with state-of-the-art text-to-image and\nimage-to-text models with orders of magnitude fewer (only 3M) paired image-text\ndata.\n1\nINTRODUCTION\nImage-text multimodal training has gained remarkable attention in recent years. Models for text-\nto-image generation show the impressive ability to synthesize realistic images from textual prompts\n(Rombach et al., 2022; Chang et al., 2023; Saharia et al., 2022; Yu et al., 2022b; Ramesh et al., 2021).\nSimilarly, image-to-text models have demonstrated advanced image comprehension capabilities by\nproviding precise descriptions of input images (Chen et al., 2023; Wang et al., 2022a; Li et al., 2022;\n2023a; Alayrac et al., 2022; Wang et al., 2022b). Training these models to exceptional performance\ndemands datasets comprising hundreds of millions to billions of paired image-text samples (Schuh-\nmann et al., 2022). The collection of very large paired datasets comes at a considerable cost as well\nas concerns of low quality (Jia et al., 2021). On the other hand, diverse and vast unimodal images or\ntexts datasets remain unused in current generative vision-language training (Raffel et al., 2020; Sun\net al., 2017; Zhai et al., 2022). This raises a natural question: can we leverage unpaired image and\ntext data to facilitate generative vision-language training?\nThe major problem with using unpaired data during vision-language training is the lack of super-\nvision. To overcome this problem, we introduce ITIT, a novel training paradigm that uses cycle\nconsistency losses between cycle-generated images/texts and their corresponding original inputs to\nprovide supervision for image-only and text-only data (Figure 1). ITIT utilizes a small set of paired\nimage-text data to achieve reasonable text-to-image and image-to-text generation performance. Si-\nmultaneously, for unpaired image (text) data, ITIT generates corresponding text (image) counter-\n\u2217Co-first authors. This work was done when Tianhong Li interned at and Huiwen Chang worked at Google\nResearch. Correspondence to Tianhong Li <tianhong@mit.edu>.\n1\narXiv:2310.03734v1  [cs.CV]  5 Oct 2023\nT2I\nUnpaired Text\nSynthesized Image\nI2T\na\non\nFlamingo\nlake\nReconstructed Text\na\non\nFlamingo\nlake\nT2I\nUnpaired Image\nI2T\nSynthesized Text\non\ncat\nA\ngrass\nReconstructed \nImage \nReconstruction\nLoss\nReconstruction\nLoss\nText-Image-Text (T2I2T) Cycle\nImage-Text-Image (I2T2I) Cycle\ngreen\nFigure 1: Overview of ITIT. For unpaired data, ITIT first generates the image/text counterpart, and\nthen uses these generated counterparts to reconstruct the original text or image.\nparts and employs them as inputs to reconstruct the input image (text): this corresponds to a full\ncycle loss. We consider two kinds of full cycles: T2I2T (starting with an unpaired text sample); and\nI2T2I (starting with an unpaired image sample). These two types of cycles enable us to leverage\nboth unpaired image and text data to provide informative supervision signals for training.\nTo enable cycle training, we first unify image-to-text (I2T) and text-to-image (T2I) generation in\nthe same framework, with a bi-directional image-text encoder and disjoint image and text decoders.\nWe tokenize images into discrete visual tokens (Van Den Oord et al., 2017) and combine them with\ntext embeddings from a pre-trained T5 model (Raffel et al., 2020) as input to the joint image-text\nencoder. For I2T generation, we employ an autoregressive text decoder (Wang et al., 2022a), while\nfor T2I generation we use a non-autoregressive parallel image decoder (Chang et al., 2023), which\nis an order of magnitude faster than autoregressive image decoders such as Yu et al. (2022b).\nA technical challenge of ITIT is that, state-of-the-art text-to-image and image-to-text generation\nprocesses typically involve multiple forward steps of the model (Esser et al., 2021; Chang et al.,\n2023; Rombach et al., 2022; Wang et al., 2022a). Back-propagating gradient through all these for-\nward steps brings significant memory and computation overheads. To solve this problem, for T2I2T\ncycle, we first generate the image with parallel decoding. We then back-propagate the gradient\nthrough one step of the parallel decoding process. For I2T2I cycle, we first generate the text auto-\nregressively with multiple steps. Then we forward the text decoder once with the generated text\nas input, and back-propagate the gradient only to this forward step. This significantly reduces the\ncomputational overhead of the cycle training, making it feasible to apply in large model settings.\nWe evaluate the performance of ITIT on standard image-to-text and text-to-image generation bench-\nmarks and demonstrate that, by leveraging unpaired data and cycle consistency, ITIT attains per-\nformance levels similar to a non-cycle baseline. However, ITIT uses up to 2 orders of magnitude\nlower paired data. Furthermore, ITIT scales similarly with unpaired data as the baseline does with\nequivalent amounts of paired data, while being much more robust to low data quality. We also\ncompare ITIT with state-of-the-art methods and show that we can achieve comparable performance\non common text-to-image and image-to-text benchmarks with substantially lesser paired data. Our\ncontributions are summarized as follows:\n\u2022 We introduce a framework that unifies text-to-image and image-to-text generation, and pro-\npose ITIT, a novel technique that enforces consistency between cycle-generated images/text\nand their corresponding originals. This approach allows the training of image-to-text and\ntext-to-image models using unpaired image and text data.\n\u2022 We comprehensively evaluate the proposed ITIT framework and the image-text cycle con-\nsistency method, and demonstrate that they significantly enhance model performance.\n2\n\u2022 We show that ITIT can achieve performance on par with state-of-the-art methods on com-\nmon text-to-image and image-to-text benchmarks with much lesser (\u223c100x) paired data.\nWhen scaling up training data to improve model efficacy, we show that we can add only un-\npaired examples using our framework and achieve similar performance as scaled-up paired\ndata, without the downsides of significant manual effort and poor pairing quality.\n2\nLITERATURE REVIEW\nImage-to-Text Generation. Various works explore autonomously generating textual descriptions\nfrom input images, either training the network with generative loss alone (Wang et al., 2022b;\nAlayrac et al., 2022; Chen et al., 2023; Li et al., 2022; 2023a), or combining it with contrastive\nlearning (Yu et al., 2022a). GIT (Wang et al., 2022a) trains a model comprising an image encoder\nand an auto-regressive text decoder using a language modeling loss, the image encoder pre-trained\nwith contrastive loss (Radford et al., 2021). In our work, we adopt a similar framework to GIT for\nour Image-to-Text (I2T) framework, but we initialize our image encoder from scratch.\nText-to-Image Generation. Recent works focus on two primary paradigms: diffusion-based mod-\nels (Rombach et al. (2022); Dhariwal & Nichol (2021); Nichol et al. (2021); Saharia et al. (2022);\nRamesh et al. (2022); Ruiz et al. (2023)); and token-based methods. Token-based strategies trans-\nform raw images into image tokens, and predict these tokens either in an autoregressive manner\n(Esser et al., 2021; Ramesh et al., 2021; Gafni et al., 2022; Yu et al., 2021; Ding et al., 2021; Yu\net al., 2022b) or in parallel (Chang et al., 2022; Li et al., 2023b; Chang et al., 2023). Muse (Chang\net al., 2023) demonstrates that token-based strategies with parallel decoding can be considerably\nfaster than diffusion-based or autoregressive generative models. Since this speed advantage facili-\ntates text-to-image synthesis during training, we adopt this strategy in our T2I framework.\nUnifying Image and Text Generation. COBIT (You et al. (2023)) achieves this by employing\ndistinct image and text unicoders, coupled with a unified cross-modal decoder. Additionally, CM3\n(Aghajanyan et al. (2022)) and CM3Leon (Yu et al. (2023)) harness causally masked generative\nmodels trained on extensive multi-modal document datasets, and enable the synthesis of both text\nand images. However, all these works still heavily rely on large-scale paired image-text datasets.\nLeveraging Unpaired Data in Generative Vision-Language Training. Early works have tried to\nuse unpaired image and text to train image captioning model in an unsupervised way (Feng et al.,\n2019). However, the performance is relatively poor. Recent efforts in incorporating unpaired data\ninto generative vision-language training primarily focus on pre-trained image and text encoders\n(Esser et al., 2021; Roberts et al., 2019). However, these applications are limited to pre-training\nand do not encompass the entire generative vision-language training procedure, thus providing only\nincremental improvements. In some cases, researchers have explored the use of text-only data to\nimprove text decoders (Wang et al. (2022b)), utilizing text-to-text training. However, this only\nenhances the text decoder and not the image encoder, resulting again in constrained improvements.\nCycle-consistency. The concept of cycle consistency has previously been used to provide regu-\nlarization and/or compensate for a lack of annotated data. Zach et al. (2010); Zhou et al. (2016);\nGodard et al. (2016); Zhu et al. (2017); Messikommer et al. (2022) explore it for computer vision\napplications such as learning dense correspondence, event detection, depth estimation, and image-\nto-image translation. Most related to our work is Gorti & Ma (2018), which uses text-image-text\ncycle consistency to perform text-to-image translation, but the performance is poor. Moreover, none\nof the previous works has explored the potential of cycle consistency in generative vision-language\ntraining using unpaired data.\nOur novel approach diverges from preceding vision-language models that heavily rely on either a\nlarge corpus of paired image-text data, or fine-tuning methods that target only text or image en-\ncoder/decoders separately. For the first time, our method facilitates the utilization of unpaired image\nand text data during generative vision-language training. This innovation significantly reduces the\ndependency on paired image-text samples during the training process, which empowers the expan-\nsion of generative vision-language training to nearly boundless text-only and image-only datasets.\n3\n3\nMETHOD\nITIT is the first framework that enables generative vision-language training on unpaired image-only\nand text-only data. It uses a simple yet effective architecture: a unified image-text encoder and two\nseparate image and text decoders. This design seamlessly enables text-to-image and image-to-text\ngeneration in the same framework, which paves the way for text-image-text (T2I2T) and image-text-\nimage (I2T2I) cyclic losses. Below, we describe each component of our ITIT architecture and the\ncycle-consistency training paradigm in detail.\n3.1\nUNIFIED IMAGE-TEXT GENERATION FRAMEWORK\nArchitecture. We first obtain text embedding T = [tl]L\nl=1 from the output of a T5 encoder (Roberts\net al., 2019) on the raw text. Similarly, raw images are passed through a pre-trained VQ-tokenizer\n(Esser et al., 2021) to output image tokens I = [ik]K\nk=1. L and K are the token sequence lengths\nfor text and image, respectively. The image tokens I are then embedded with an embedding layer\nand concatenated with the T5 text features T as input to the image-text encoder. Modality-specific\ndecoders then operate on the encoded image-text features to generate either text or image tokens.\nThe text decoder is autoregressive (Wang et al., 2022a), while the image decoder is parallel (Chang\net al., 2023). Both encoder and decoders are based on Transformer (Vaswani et al., 2017) layers. A\ndetailed description of the model architecture is included in Appendix B.\nImage-to-Text (I2T) Training. As shown in Figure 2, we input masked image tokens along with\nempty text embedding to the image-text encoder. Masking is used to save computation, similar to\nMAE (He et al., 2022). We then use the features generated by the image-text encoder, as well as the\nground-truth text tokens prepended with [BOS] (begin-of-sentence) token as the input to our text\ndecoder. We use an auto-regressive language modeling (LM) loss to train the encoder and decoder:\nLI2T = \u2212E(I,T )\u2208D\n\u0002\nL\nX\nl=1\nlog p(tl|IM, t0, \u00b7 \u00b7 \u00b7 , tl\u22121)\n\u0003\n,\n(1)\nwhich is a CE loss with label smoothing 0.1. Here, t0 is set to be the [BOS] token. IM are\nthe (subset of) unmasked tokens in I and p(ik|IM, T) is the probability predicted by the encoder-\ndecoder network (the \u2018logits\u2019 layer), D is the distribution of paired image-text data. Note that the\ntext decoder employs causal attention similar to GIT (Wang et al. (2022a)): each text token only\ndepends on the preceding text tokens and all image features.\nText-to-Image (T2I) Training. As shown in Figure 2, right panel, we use masked image modeling\nfor image generation, where the training objective is to reconstruct masked image tokens conditioned\non the unmasked image tokens and the paired text features. We denote the binary mask determining\nwhich image tokens are masked by M = [mk]K\nk=1. We use a cross-entropy loss between the ground-\ntruth one-hot image tokens and the output of the image decoder. Specifically,\nLT 2I = \u2212E(I,T )\u2208D\n\u0002\nX\n\u2200k:mk=1\nlog p(ik|IM, T)\n\u0003\n,\n(2)\nInference. We follow GIT (Wang et al., 2022a) for image-to-text inference and Muse (Chang et al.,\n2023) for text-to-image inference. More details are included in Appendix B.\n3.2\nTRAINING WITH CYCLE CONSISTENCY\nOur cycle consistency training paradigm allows training with image-only and text-only data. The\nkey idea is to first synthesize the corresponding text/image from the image-only or text-only data,\nand then use the synthesized data as input to reconstruct the original image/text. This allows us to\napply cycle consistency supervision on image-only and text-only data.\nText-Image-Text (T2I2T) Cycle. Our T2I2T training pipeline is shown in Figure 3, top panel. At\neach training iteration, we first synthesize pseudo paired image tokens I\u2032 for input text T = [tl]L\nl=1\nusing our T2I inference pipeline. We then apply random mask M to I\u2032, perform reconstruction\non I\u2032\nM with the text T using the T2I pipeline, and obtain the reconstructed synthesized image \u02dcI.\nThis two-step process allows us to avoid the excessive memory requirements of back-propagating\n4\n   I2T Training Module\nImage Text \nEncoder\nText \nDecoder\non\ncat\nA\ncat\nA\nBOS\nEmpty\nText Embedding\nGenerated\nText Logits\nMasked \nImage Tokens\n\u2026\n\u2026\n   T2I Training Module\nImage Text \nEncoder\nImage \nDecoder\nT5 Text Embedding\nMasked \nImage Tokens\nGenerated \nImage Logits\nGround-truth\nText Tokens\nFigure 2: I2T (left) and T2I (right) training pipelines for paired image and text data.\ndog\nA\nand\nT2I\ncat\nA\non\nI2T\n24-Step \nParallel \nDecoding\nOnline Pseudo Pair \nGeneration\nT2I2T\nI2T2I\nT2I\nI2T\nCycle Consistency Training\nI2T\ncat\nA\non\nT2I\nCross-Entropy\nCross-Entropy\nStop Grad\nStop Grad\ndog\nA\nand\nA\nBOS\ncat\nA\nBOS\ndog\nargmax\nargmax\n&\nmask\nGreedy\nDecoding\nFigure 3: Text-image-text (top) and image-text-image (bottom) cycle training pipelines for unpaired\nimage and text data. We use pseudo-generated image and text to enable the cycle consistency. Image\ntoken masks M are always randomly chosen. The dashed line denotes causal attention. Text tokens\nprepended with [BOS] token are used for auto-regressive language modeling loss.\ngradients through all 24 steps of parallel decoding, while still training the T2I module. Finally, we\nrandomly mask \u02dcI and use \u02dcIM to generate text using the I2T pipeline. The objective of our cycle\nparadigm is to enforce consistency between this generated text and the original text. Therefore, the\nT2I2T cycle-consistency loss can be formulated as follows:\nLT 2I2T = \u2212ET \u2208Dtext\n\u0002\nL\nX\nl=1\nlog p(tl|\u02dcIM, t0, \u00b7 \u00b7 \u00b7 , tl\u22121)\n\u0003\n,\n(3)\nThis is very similar to the I2T loss in Equation (1), except that \u02dcI is synthesized from T instead of\nbeing drawn from the image-text joint distribution.\nImage-Text-Image (I2T2I) Consistency. Our I2T2I training pipeline is shown in Figure 3, bottom\npanel. Similar to the T2I2T pipeline, we first synthesize pseudo paired text tokens T \u2032 for input image\ntokens I using our I2T inference pipeline. We then use the I2T training pipeline to predict \u02dctl from\nt\u2032\n0, \u00b7 \u00b7 \u00b7 , t\u2032\nl\u22121 and IM. As before, this avoids the excessive memory requirements of back-propagating\ngradients through the auto-regressive greedy decoding. We then mask I, and pass it through the T2I\npipeline with the predicted \u02dcT to reconstruct the masked image tokens. Again, the loss enforces\nconsistency between the reconstructed and the original image tokens using cross-entropy:\nLI2T 2I = \u2212EI\u2208Dimage\n\u0002\nX\n\u2200k:mk=1\nlog p(ik|IM, \u02dcT)\n\u0003\n,\n(4)\nGradient Estimation. One challenge in our cycle training is that \u02dcik = arg max(p(ik|I\u2032\nM, T) and\n\u02dctl = arg max p(tl|IM, t\u2032\n0, \u00b7 \u00b7 \u00b7 , t\u2032\nl\u22121), which are not differentiable. To solve this, we use a straight-\nthrough estimation on the predicted logits to approximate the gradient. Specifically, we directly copy\nthe gradient on the one-hot prediction to the predicted logits after softmax. We show in section 4.4\nthat this helps improve both text-to-image and image-to-text performance.\n5\n~100x fewer image-text pairs\n~100x fewer image-text pairs\n~100x fewer image-text pairs\nFigure 4: How ITIT-H\u2019s performance scales with additional paired Shutterstock data. The baseline\n(T2I+I2T) is trained with paired samples only. ITIT is trained with the same number of paired\nsamples, as well as 398M unpaired samples (the full Shutterstock dataset) using cycle loss.\n4\nRESULTS\n4.1\nEXPERIMENT SETUP\nDatasets. We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen\net al., 2023), and Shutterstock (Shutterstock, 2023). CC3M contains 3.3 million high-quality image-\ntext pairs. WebLI (Web Language Image) contains 111 million images where the image-text pairing\nquality is much lower than CC3M. Thus, WebLI is significantly noisier and, as we show, leads to\nworse performance for I2T. Shutterstock contains 398 million images labeled by human annotators,\nwhich incurs significant expense and effort. More dataset details are included in Appendix C.\nWe use CC3M as our paired dataset, 50% of WebLI images as our unpaired image dataset, and the\nother 50% of WebLI texts as our unpaired text dataset for most of our experiments (Section 4.3 and\nSection 4.4). This 50%-50% split ensures that corresponding image-text pairs are not present in our\nunpaired image and text splits. We use the Shutterstock dataset in Section 4.2, where we analyze\nhow ITIT scales w.r.t. different number of paired and unpaired data samples.\nTraining. We set the input image resolution as 256x256 to be consistent with previous literature. Af-\nter passing through the VQGAN tokenizer, the image token sequence length is 16x16 (256 tokens).\nThe raw text (maximum length of 64) is tokenized by SentencePiece tokenization (SentencePiece,\n2023), and embedded using a pre-trained T5 encoder. These embeddings are then concatenated with\nthe image token embeddings as the input to our image-text encoder.\nWe experiment with ViT-B, ViT-L, and ViT-H size Transformers (Dosovitskiy et al. (2021)) for\nour image-text encoder. We combine the losses in Equations 1 through 4 with equal weight for\ntraining. For results in Section 4.3, we use Adafactor (Shazeer & Stern, 2018) to train the model for\n1.5M steps with a batch size of 2048 (1024 for image-text pairs, 512 for unpaired images, and 512\nfor unpaired texts). We use a cosine learning rate schedule with 5K steps warmup and maximum\nlearning rate 1 \u00d7 10\u22124. For other experiments, we use the exact same training paradigm except that\nwe train the models for 500K steps. More details are included in Appendix B.\nEvaluation. We follow the commonly used MS-COCO benchmark and evaluation protocols. For\nimage-captioning, we evaluate both the zero-shot and fine-tuning performance of ITIT on the COCO\nKarpathy split (Karpathy & Fei-Fei, 2015) and report the CIDEr score (Vedantam et al., 2015). For\ntext-to-image generation, we evaluate ITIT on 30K image-text pairs randomly selected from the\nCOCO Captions training set and report the Frechet Inception Distance (FID) score (Heusel et al.,\n2017). CIDEr is the higher the better, and FID is the lower the better.\n4.2\nSCALE WITH DATA\nIn this section, we comprehensively evaluate ITIT\u2019s performance with different amounts of paired\nand unpaired data on Shutterstock dataset (Shutterstock, 2023) consisting of 398M image-text pairs.\n6\n0.0 (1.2)\n2.8 (4.0)\n396.8 (398.0)\nAdditional (Total) Data (in M)\n14\n16\n18\n20\n22\nCIDEr\n100%\n30%\n0.3%\n100%\n100%\nShutterStock\n0.0 (1.2)\n2.8 (4.0)\n396.8 (398.0)\nAdditional (Total) Data (in M)\n19.0\n19.5\n20.0\n20.5\n21.0\nFID\n100%\n30%\n0.3%\n100%\n100%\n0 (3)\n11 (14)\n111 (114)\nAdditional (Total) Data (in M)\n24\n26\n28\n30\n32\n34\n36\nCIDEr\n100%\n21%\n2.6%\n100%\n100%\nCC3M + WebLi\n0 (3)\n11 (14)\n111 (114)\nAdditional (Total) Data (in M)\n12.1\n12.2\n12.3\n12.4\n12.5\n12.6\n12.7\n12.8\nFID\n100%\n21%\n2.6%\n100%\n100%\n% Paired \nTotal Data\nAdditional Paired Data Only\nAdditional Unpaired Data with Cycle loss\nFigure 5: How ITIT\u2019s performance scales with the total amount of data used (x-axis). The base-\nline (T2I + I2T) in blue is trained entirely with increasing amounts of paired data. ITIT (orange)\nis trained with an increasing amount of unpaired data using cycle loss, while keeping the total\namount of data equal for both curves. For example, the rightmost point with Shutterstock uses 1.2M\nimage-text pairs and 396.8M unpaired samples (half as unpaired image and half as unpaired text)\nfor ITIT with cycle loss, and 398M image-text pairs for the baseline. Left: Shutterstock data as both\npaired and unpaired. Right: CC3M as paired data, and varying fractions of WebLI as additional\npaired / unpaired data.\nFigure 4 analyses how ITIT\u2019s performance scales with paired data. We train a baseline with only\npaired data, with the sum of the losses in Equation (1) and Equation (2). ITIT is trained with the\nsame paired data as the baseline, and the entire set of 398M images and text present in Shutterstock\nas unpaired data. More paired data helps both settings, but training with unpaired data significantly\nimproves ITIT\u2019s performance over the baseline on both image captioning and text-to-image gen-\neration. Remarkably, with only 4M paired data and 398M unpaired data, ITIT achieves a similar\nperformance as training with 398M paired data. Note that ITIT does not use any samples not present\nin the baseline trained with 398M paired data, as all of the samples are from Shutterstock. Therefore\nITIT can perform similarly as a baseline with 100x fewer image-text pairs, significantly reducing\nthe effort and expense for the training of generative vision-language training.\nNext, we evaluate how ITIT\u2019s performance scales w.r.t. the total amount of data used. We first\ntrain a model with 1.2M paired image-text Shutterstock data. We then evaluate the effect of training\nmodels on adding increasing amounts of additional paired data vs. adding increasing amounts of\nunpaired data with cycle loss, keeping the total amount of data the same for both. As expected,\nwe see in Figure 5 that performance scales up with additional paired data. Surprisingly, however,\nadditional unpaired data exhibits similar scalability as paired. In fact, we can achieve 19.2 FID and\n21.0 CIDEr with only 1.2M paired and 396.8M unpaired examples, which is very competitive with\n19.0 FID and 22.2 CIDEr using 398M paired examples only. This experiment thus demonstrates\nthat when scaling up training data, practitioners can rely on only adding unpaired examples using\nour method and achieve similar performance as paired data without the extra manual effort required\nto collect it.\nWe repeat the above experiment in a more realistic setting, where the small-scale paired dataset\ncan contain high-quality image-text pairs but a large-scale paired dataset has much lower quality.\nFor this, we use the high-quality CC3M as the paired dataset, and the much larger WebLI as the\nlow-quality unpaired dataset. As before, we start with a model trained on 3M paired examples\n(from CC3M), and add additional training data from WebLI in paired (blue) or unpaired (orange)\nform. As shown in Figure 5, right pair, adding low-quality image-text pairs harms image captioning\nperformance severely for the fully-paired case. However, the ITIT regime is not affected by this low\nquality and scales similarly as before. This demonstrates that our method is robust to low data quality\nin large datasets, and can in fact be used to achieve significantly better performance in settings when\npaired data is present but of low quality.\n4.3\nCOMPARISON TO PRIOR WORK\nIn Table 1, we compare ITIT with state-of-the-art image-to-text and text-to-image models on the\ncommonly used MS-COCO benchmark. As shown, all SOTA methods rely heavily on training on\na large corpus of paired image-text data. ITIT, however, is trained with only 3M paired examples\n7\nTable 1: Quantitative comparison with state-of-the-art text-to-image and image-to-text models on\nMS-COCO. The image-captioning performance is evaluated on the COCO Karpathy split, and the\ntext-to-image generation FID is evaluated on 30K COCO images. \u2020 denotes our re-implementation.\nWe highlight in green other models that use comparable amounts of paired data. Note that the GIT\n(CLIP) model uses a CLIP (Radford et al., 2021) encoder pre-trained with 400M image-text pairs.\nMethods\n#params\n#paired data\n#unpaired data\nFID\u2193\nCIDEr\u2191 (zs)\nCIDEr\u2191 (ft)\nT2I\nStableDiffusion (Rombach et al., 2022)\n800M\n400M\n-\n12.60\n-\n-\nGLIDE (Nichol et al., 2021)\n5B\n250M\n-\n12.24\n-\n-\nMake-A-Scene (Gafni et al., 2022)\n4B\n35M\n-\n11.84\n-\n-\nDALL-E 2 (Ramesh et al., 2022)\n3.5B\n650M\n-\n10.39\n-\n-\nPARTI (Yu et al., 2022b)\n750M\n5000M\n-\n10.71\n-\n-\nMuse-512 (Chang et al., 2023)\n3B\n860M\n-\n7.88\n-\n-\nMuse\u2020 (Chang et al., 2023)\n750M\n3M\n-\n23.7\n-\n-\nI2T\nBLIP (Li et al., 2022)\n446M\n129M\n-\n-\n-\n136.7\nSimVLMbase (Wang et al., 2022b)\n-\n1100M\n365M T\n-\n24.0\n134.8\nSimVLMhuge (Wang et al., 2022b)\n\u223c1.4B\n1100M\n365M T\n-\n32.2\n143.3\nGIT (CLIP) (Wang et al., 2022a)\n681M\n800M\n-\n-\n-\n144.8\nGITB (scratch) (Wang et al., 2022a)\n129M\n10M\n-\n-\n-\n89.0\nT2I+I2T\nCoBIT-Base (You et al., 2023)\n626M\n5200M\n-\n10.35\n43.0\n135.4\nCoBIT-Large (You et al., 2023)\n1091M\n5200M\n-\n9.37\n44.8\n139.5\nCM3Leon (Yu et al., 2023)\n7B\n340M\n-\n4.88\n61.6\n-\nITIT-B\n221M\n3M\n55M I+55M T\n13.4\n32.1\n103.5\nITIT-L\n487M\n3M\n55M I+55M T\n12.0\n35.1\n116.4\nITIT-H\n868M\n3M\n55M I+55M T\n10.4\n38.2\n125.3\n(CC3M), and an additional 55M unpaired image and text examples each (WebLI). Despite this, it\nbeats many other methods trained on much more data for text-to-image generation (FID). For I2T, it\nbeats methods using a comparable amount of data (highlighted in green), and achieves performance\ncompetitive with other SOTA methods. We find that the pre-training data (both the mixture and\nthe size) also makes a difference to CIDEr score. For example, GIT (Wang et al., 2022a) achieves\nonly 89.0 CIDEr fine-tuning performance on COCO captions when trained from scratch with 10M\nimage-text pairs, which is far from its reported performance (144.8) when trained with 800M image-\ntext pairs. Our approach is orthogonal to dataset mixture considerations, and we believe that scaling\ndata size and variety will further enhance FID and CIDEr scores. We leave this to future work.\n4.4\nABLATIONS\nIn Table 2, we ablate the effectiveness of the four components of ITIT: T2I, I2T, T2I2T, and I2T2I.\nAs shown in rows 1-3, combining T2I and I2T training in our framework already improves image\ncaptioning performance. This is likely because the T2I training alleviates the overfitting problem of\nI2T training, as shown in GIT (Wang et al., 2022a).\nAs before (Figure 5), we can see in row 4 that combining CC3M and WebLI improves text-to-image\ngeneration, but harms image captioning performance. This is because of the lower image-text pairing\nquality of WebLI compared to CC3M. The remaining rows demonstrate that the cycle loss alleviates\nthis by using WebLI as unpaired data and does not depend on its image-text pairing quality. It is\nthus more generalizable to large-scale image-text datasets.\nNext, rows 5-7 are naive baselines for using unpaired image or text data during generative vision-\nlanguage training. We can simply perform text-to-text (T2T) autoregressive training without condi-\ntioning on images, which has been explored in some prior works (Wang et al. (2022b)). Similarly,\nwe can perform image-to-image (I2I) reconstructive training without conditioning on text. Such\nbaselines do improve the performance over not using any paired data (row 3).\nWe consider an ablation where the gradient of the cycle consistency loss is backpropagated up\nuntil the argmax step. Hence, only half of the cycle is trained. In fact, this is equivalent to first\nsynthesizing an image counterpart from unpaired text and then using it as a pseudo image-text pair\nto train the I2T model (similarly for T2I). Rows 8-10 show that the half-cycle loss achieves much\nbetter performance than non-cycle baselines.\n8\nTable 2: Quantitative comparison between different variants of ITIT on MS-COCO. All experiments\nuse ITITB trained with 500K steps. We take 50% of WebLI data and use the images as our unpaired\nimage data, and the other 50% of WebLI data and use the texts as our unpaired text data.\nT2I\nI2T\nT2I2T\nI2T2I\npaired data\nunpaired text\nunpaired image\nFID\u2193\nCIDEr\u2191\nPaired data only\n1\n!\n%\n%\n%\nCC3M\n%\n%\n15.5\nN/A\n2\n%\n!\n%\n%\nCC3M\n%\n%\nN/A\n19.0\n3\n!\n!\n%\n%\nCC3M\n%\n%\n15.7\n23.5\n4\n!\n!\n%\n%\nCC3M+WebLI\n%\n%\n14.2\n20.7\nPaired+unpaired data, no cycle\n5\n!\n!\nT2T\n%\nCC3M\n50% WebLI\n%\n15.1\n26.0\n6\n!\n!\n%\nI2I\nCC3M\n%\n50% WebLI\n15.9\n24.2\n7\n!\n!\nT2T\nI2I\nCC3M\n50% WebLI\n50% WebLI\n15.6\n28.5\nPaired+unpaired data, half cycle\n8\n!\n!\nHalf\n%\nCC3M\n50% WebLI\n%\n14.8\n27.6\n9\n!\n!\n%\nHalf\nCC3M\n%\n50% WebLI\n14.7\n24.8\n10\n!\n!\nHalf\nHalf\nCC3M\n50% WebLI\n50% WebLI\n14.5\n30.5\nPaired+unpaired data, full cycle\n11\n!\n!\nFull\n%\nCC3M\n50% WebLI\n%\n14.6\n28.4\n12\n!\n!\n%\nFull\nCC3M\n%\n50% WebLI\n14.6\n26.3\n13\n!\n!\nFull\nFull\nCC3M\nCC3M\nCC3M\n15.4\n24.4\n14\n!\n!\nFull\nFull\nCC3M\n50% WebLI\n50% WebLI\n14.3\n31.1\nFigure 6: Iteratively generating text to image to text and so on. With ITIT, the generated results are\nmore consistent than the results from a model trained without the cycle consistency loss.\nFinally, rows 11-14 show the performance of the full cycle ITIT training. Although T2I2T favors\nimage captioning while I2T2I favors text-to-image generation, they both show significant improve-\nment in text-to-image generation and image captioning. Moreover, row 14 demonstrates that such\ntwo cycle losses can be combined to further improve performance. Additionally, we can see that\nthe full cycle loss beats the half-cycle baselines (row 8-10), demonstrating the effectiveness of the\ngradient estimation step.\nLastly, we find by comparing row 3 and 13 that the cycle consistency loss can slightly improve the\nperformance even without any additional data. We believe this is because it forces better image-\ntext alignment. However, comparing row 13 and 14 shows that the huge improvements in both\ntext-to-image and image-to-text generation mainly stem from the usage of additional unpaired data.\n4.5\nCYCLE-GENERATION RESULTS\nWith a framework that can perform both image-to-text and text-to-image, we can easily perform\ncycle-generation, as shown in Figure 6. With ITIT training, the cycle generation often keeps the\nsame semantics as the input text prompt. On the other hand, without the cycle consistency training,\nthe cycle generation misses the \u201cblue\u201d semantics after the first cycle. This demonstrates that our\ncycle consistency training not only enables integrating unpaired image and text data into generative\nvision-language training, but also improves image-text alignment for both image-to-text and text-\nto-image generation. We include a number of results of image and text generation in Appendix A\n(Figures 1 through 4).\n9\n5\nDISCUSSION\nWe propose ITIT, a novel training scheme that for the first time incorporates unpaired images and\ntext into generative vision-language training. Through extensive ablations, we demonstrate the ef-\nfectiveness of both the T2I2T cycle and I2T2I cycle in improving text-to-image and image-to-text\ngeneration performance. As a result, ITIT achieves performance competitive with state-of-the-art\nvision-language generative models, but with only 3 million paired image-text samples. Our method\ncan be used even when paired image-text data is present, and is especially helpful when the pairing\nquality is low. Future directions include scaling ITIT to larger unpaired image and text data and\nmodel sizes, and utilizing more diverse datasets.\n10\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n11315\u201311325, 2022.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin\nMurphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked gener-\native transformers. arXiv preprint arXiv:2301.00704, 2023.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong,\nHassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba\nSeyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua\nZhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2023.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural\nInformation Processing Systems, 34:8780\u20138794, 2021.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural\nInformation Processing Systems, 34:19822\u201319835, 2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In Int. Conf. on Learning Representations (ICLR),\n2021.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873\u201312883,\n2021.\nYang Feng, Lin Ma, Wei Liu, and Jiebo Luo. Unsupervised image captioning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 4125\u20134134, 2019.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:\nScene-based text-to-image generation with human priors. In European Conference on Computer Vision, pp.\n89\u2013106. Springer, 2022.\nCl\u00b4ement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with\nleft-right consistency. CoRR, abs/1609.03677, 2016. URL http://arxiv.org/abs/1609.03677.\nSatya Krishna Gorti and Jeremy Ma. Text-to-image-to-text translation using cycle consistent adversarial net-\nworks. arXiv preprint arXiv:1808.04538, 2018.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n16000\u201316009, June 2022.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by\na two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing\nsystems, 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint\narXiv:1611.01144, 2016.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision.\nIn International conference on machine learning, pp. 4904\u20134916. PMLR, 2021.\n11\nAndrej Karpathy and Li Fei-Fei.\nDeep visual-semantic alignments for generating image descriptions.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128\u20133137, 2015.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine Learning,\npp. 12888\u201312900. PMLR, 2022.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.\nTianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked\ngenerative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 2142\u20132152, 2023b.\nIlya Loshchilov and Frank Hutter.\nSgdr: Stochastic gradient descent with warm restarts.\narXiv preprint\narXiv:1608.03983, 2016.\nNico Messikommer, Daniel Gehrig, Mathias Gehrig, and Davide Scaramuzza. Bridging the gap between events\nand frames through unsupervised domain adaptation. IEEE Robotics and Automation Letters, 7(2):3515\u2013\n3522, apr 2022. doi: 10.1109/lra.2022.3145053. URL https://doi.org/10.1109%2Flra.2022.\n3145053.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741, 2021.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural\nlanguage supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The\nJournal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821\u2013\n8831. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nAdam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J. Liu, Sharan Narang,\nWei Li, and Yanqi Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer.\nTechnical report, Google, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10684\u201310695, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 22500\u201322510, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u2013\n36494, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset\nfor training next generation image-text models. Advances in Neural Information Processing Systems, 35:\n25278\u201325294, 2022.\nSentencePiece. Unsupervised text tokenizer for neural network-based text generation. https://github.\ncom/google/sentencepiece, 2023.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-\nnymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556\u20132565, 2018.\n12\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Interna-\ntional Conference on Machine Learning, pp. 4596\u20134604. PMLR, 2018.\nShutterstock. Shutterstock. https://www.shutterstock.com/, 2023.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness\nof data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pp.\n843\u2013852, 2017.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the incep-\ntion architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2818\u20132826, 2016.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural infor-\nmation processing systems, 30, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566\u2013\n4575, 2015.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan\nWang. Git: A generative image-to-text transformer for vision and language, 2022a.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\nlanguage model pretraining with weak supervision, 2022b.\nHaoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. Cobit: A con-\ntrastive bi-directional image-text generation model, 2023.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Ja-\nson Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint\narXiv:2110.04627, 2021.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-\ntrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022a.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image\ngeneration. arXiv preprint arXiv:2206.10789, 2(3):5, 2022b.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and\ninstruction tuning. arXiv preprint arXiv:2309.02591, 2023.\nChristopher Zach, Manfred Klopschitz, and Marc Pollefeys. Disambiguating visual relations using loop con-\nstraints. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp.\n1426\u20131433, 2010. doi: 10.1109/CVPR.2010.5539801.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104\u201312113, 2022.\nTinghui Zhou, Philipp Kr\u00a8ahenb\u00a8uhl, Mathieu Aubry, Qixing Huang, and Alexei A. Efros.\nLearning dense\ncorrespondence via 3d-guided cycle consistency. CoRR, abs/1604.05383, 2016. URL http://arxiv.\norg/abs/1604.05383.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using\ncycle-consistent adversarial networks, 2017.\n13\nA\nADDITIONAL QUALITATIVE RESULTS\nImage-to-Text and Text-to-Image Generation. In Figure 7, Figure 8 and Figure 9, we show the\nperformance of ITIT on text-to-image generation and image captioning. The model uses ViT-H as\nthe backbone and is trained with CC3M as paired data and WebLI as unpaired data for 1.5M steps.\nAs shown in the results, our model can generate realistic images from text prompts, and can also\ngenerate accurate captions for the input image.\nCycle Generation. We include more cycle-generation results in Figure 10. With ITIT, the gen-\nerated results are quite consistent with the original input. Without the cycle consistency loss, the\ngenerated text/image can easily miss some key information in the input image/text, causing the\ncycle-generation results to diverge from the original input. This demonstrates that the proposed cy-\ncle consistency loss forces the cycle generation to be more consistent and improve the input-output\nalignment for both text-to-image and image-to-text generation.\nB\nIMPLEMENTATION DETAILS\nIn this section, we include our implementation details, including hyper-parameters, model architec-\nture, and training paradigm. We will also release our code for better reproducibility.\nImage Tokenizer and Detokenizer. We use a CNN-based VQGAN encoder to encode the 256x256\ninput images to 16x16 feature maps. The quantizer then quantizes each pixel of the encoder\u2019s output\nfeature map using a codebook with 8192 entries. The detokenizer operates on the 16x16 discrete\ntokens and reconstructs the 256x256 image. Our VQGAN tokenizer and detokenizer are trained on\nthe WebLI dataset with batch size 256.\nViT architecture. After the tokenizer, the image latent sequence length becomes 256. Since we\nalways use a masking ratio larger than 50%, we drop 128 of the masked image tokens from the input.\nThe tokens are then embedded with an embedding layer and concatenated with the text embedding\nfrom T5-XXL with a length of 64. We then use an image-text encoder Transformer with standard\nViT architecture Dosovitskiy et al. (2021), which consists of a stack of Transformer blocks Vaswani\net al. (2017), where each block consists of a multi-head self-attention block and an MLP block.\nThe text decoder is similar to the one used in GIT (Wang et al., 2022a), which consists of 6 Trans-\nformer blocks with causal attention mask. The attention mask ensures each text token can only\nattend to its previous text tokens and the image tokens.\nThe image decoder is similar to the one used in MAGE (Li et al., 2023b), which consists of 8\nTransformer blocks with self-attention. The input to the image decoder is padded with the previously\ndropped masked image tokens. In this way, we save a lot of computation in the image-text encoder.\nVision-language Training. Please refer to Table 3 for our default vision-language training setting.\nWith the online synthesis step, ITIT requires \u223c2x training time as standard I2T and T2I non-cycle\ntraining. Our ViT-H training with 1.5M steps takes \u223c10.9 days on 512 TPUv3.\nGradient scale for I2T loss: Similar to GIT (Wang et al. (2022a)), we found that the image-text\nencoder should receive a smaller gradient than the text decoder. Therefore, we scale down the\ngradient backpropagated from the text decoder to the image-text encoder by 0.1. In practice, this is\nsimply implemented by z = 0.1z + stopgrad(0.9z) where z denotes the output of the image-text\nencoder.\nCycle Training. During cycle training, the image-text encoder experiences the forward pass twice.\nWe found that if we back-propagated the gradient back to it twice, the training becomes unstable.\nTherefore, we stop the gradient after the first forward pass of the image-text encoder, which signifi-\ncantly stabilize the cycle training.\nGradient Estimation. We use Gumbel softmax (Jang et al., 2016) with strength 1.0 for the gradient\nestimation in the T2I2T cycle, and use straight-through softmax for the gradient estimation in the\nI2T2I cycle. We found that using Gumbel softmax in the T2I2T cycle improves zero-shot CIDEr by\n0.3, while using Gumbel softmax in the I2T2I cycle does not bring improvement.\nI2T Inference. To generate captions for an image, we first extract visual features from the image\ntokens (and empty text tokens) using the image-text encoder. Then we auto-regressively generate the\n14\nTable 3: Pre-training Setting.\nconfig\nvalue\noptimizer\nAdafactor (Shazeer & Stern, 2018)\npeak learning rate\n1e-4\nweight decay\n0.045\noptimizer momentum\n\u03b21, \u03b22 = 0.9, 0.96\nT2I batch size\n512\nI2T/I2I batch size\n512\nT2I2T batch size\n512\nI2T2I batch size\n512\nlearning rate schedule\ncosine decay (Loshchilov & Hutter, 2016)\nwarmup steps\n5000\ntraining steps\n1.5M\ngradient clip\n3.0\nlabel smoothing (Szegedy et al., 2016)\n0.1\ndropout\n0.1\nimage masking ratio min\n0.5\nimage masking ratio max\n1.0 (T2I), 0.75 (I2T)\nimage masking ratio mode\n0.75\nimage masking ratio std\n0.25\ntokens, conditioned on the visual features and previously generated tokens. Following GIT (Wang\net al. (2022a)), we use beam search to generate the tokens with a beam size set to 4.\nT2I Inference. To enable classifier-free guidance (Ho & Salimans, 2022) for generation, when\nperforming I2T training with empty text embedding as input, we also train the image decoder to\nreconstruct missing tokens. In such cases, the T2I training becomes image-to-image (I2I) training,\nwhich is to reconstruct the original tokens from unmasked tokens.\nSimilar to Muse (Chang et al. (2023)), we use parallel decoding with classifier-free guidance to\ngenerate an image from a text prompt. We start with entirely masked image tokens, and concatenate\nthem with the text prompt tokens. At each iteration, the decoder predicts a conditional logit lc\nand an unconditional logit lu for each masked token. The final logits lg are formed by moving\naway from lu by the guidance scale \u03c4: lg = (1 + \u03c4)lc \u2212 \u03c4lu. We then sample each token using\ncategorical sampling. After that, the corresponding prediction score of each token plus a noise\nsampled from a random Gumbel distribution multiplied by temperature t is used as the \u201dconfidence\u201d\nscore indicating the model\u2019s belief of each token prediction. We then sample the top-k tokens with\nthe highest predicted probability, and replace the corresponding masked tokens with these sampled\npredicted tokens. The number of masked tokens to be replaced in each iteration follows a cosine\nfunction (Chang et al., 2022). We use a total of 24 steps to generate an image. For each model, we\nsweep temperature t and classifier-free guidance scale \u03c4 for the optimal FID. In practice, we find\nt = 32 and \u03c4 = 2.0 serve as near-optimal temperature and guidance scale.\nC\nDATASETS\nWe use three datasets in our main paper: CC3M, WebLI, and Shutterstock. CC3M contains 3.3M\nimage-text pairs. The raw descriptions are harvested from the alt-text HTML attribute associated\nwith web images and then filtered to form the final dataset. WebLI (Web Language Image) contains\n111 million images from the public web with image-associated alt-text labels where the image-text\npairing quality is much lower than CC3M, because no filtering was applied to the alt-text data.\nShutterstock contains 398 million images labeled by human annotators, which incurs significant\nexpense and effort.\n15\nHalf of a white \ncake with \ncholocate on top\nA full bathroom \nwith a wicker \nlaundry basket\nA small white cat \non a large bowl\nA large \npassenger \nairplane flying \nthrough the air\nA large red bus \non the side of a \ncity street\nA motorcycle \nparked on grass \npathway\nA fall landscape \nwith a house \nnext to a lake\nA plate of food \nwith noodles, \ncarrots and \nbroccoli\nPlants sitting \nnear a window \ninside a house\nFigure 7: ITIT text-to-image generation results.\n16\nBaseline\nITIT\nperson and me on the \nbeach\nperson with a surfboard \non the beach\nperson preparing food \nin the kitchen\npreparing food in the \nkitchen\na close up of a bull\na bull on the farm\ntv in the living room\na tv stand in a room\nBaseline\nITIT\na homeless man sleeps \non a park bench\na man sitting on a \nbench\nblack and white \nphotograph of the clock \ntower\nblack and white photo \nof the clock tower\na picture of a baby \neating watermelon\ncute baby with a \nbirthday cake\na selection of food from \nthe menu\na picture of the meal\nBaseline\nITIT\ndesk in the living room\nmy desk at the office\na child sleeps on a bed\nperson sleeping on the \nbed\nman selling bananas \non the street\na vendor sells bananas \nat a market\nbathroom : small \nbathroom ideas with \nshower stall\nexample of a trendy \nbathroom design\nBaseline\nITIT\na plane in the sky\na plane in the sky\na pizza topped with \nmozzarella and cherry \ntomatoes\na photo of a pizza\na skateboarder does a \ntrick on a skateboard\na young man \nskateboarding in a park\na woman dances in \ntraditional dress\na girl in traditional \ncostume\nFigure 8: Image-to-text generation performance (zero-shot on COCO Captions). The baseline is\ntrained on CC3M only. ITIT is trained on CC3M as paired data and WebLI as unpaired data. We\nobfuscate the faces of people in the images.\n17\na living room filled with \nfurniture and a flat \nscreen tv\na man in a baseball \nuniform holding a bat\na cat standing under an \numbrella in the grass\na group of people flying \nkites on a cloudy day\na couple of zebra \nstanding next to each \nother\na white cat sitting on \ntop of a sink\na group of people \nwalking down a \nsidewalk next to a body \nof water\na giraffe is standing in \na zoo enclosure\na cell phone sitting on \ntop of a newspaper\na woman in a blue \ndress is holding a \ntennis racket\na bathroom with a toilet \na sink and a window\na fire hydrant on a city \nstreet at night\na man walking down a \nstreet holding a cell \nphone\na red motorcycle \nparked on the side of a \nroad\na person riding a skate \nboard on a rail\na display case filled \nwith different types of \nvegetables\na group of teddy bears \nsitting on top of a table\na group of boats are \ndocked in the water\na man riding a \nskateboard up the side \nof a ramp\na clock on a pole in \nfront of a building\nFigure 9: ITIT image-to-text generation results (fine-tuned on COCO Captions).\n18\nITIT\nNo \nCycle\nLoss\na pink bicycle \nleaning against a \ntree\npink bicycle \nleaning against a \ntree\na pink bicycle \nleaning against a \ntree\npink bike leaning \nagainst a tree\npink bicycle \nleaning against a \ntree\na pink bicycle \nleaning against a \ntree\npink bicycle \nleaning against a \ntree\npink bicycle \nleaning against a \nwall\npink bicycle\nvector illustration \nof a bicycle\nITIT\nNo \nCycle\nLoss\npiece of \nchocolate cake \non a yellow plate\nslice of chocolate \ncake on a yellow \nplate\na slice of \nchocolate cake \non a yellow plate\na slice of \nchocolate cake \non a yellow plate\na slice of \nchocolate cake \non a yellow plate\npiece of \nchocolate cake \non a yellow plate\npiece of \nchocolate cake \non a white \nbackground\nchocolate cake \nisolated on a \nwhite \nbackground\nchocolate cake \nisolated on a \nwhite \nbackground\ncake on a white \nbackground\nITIT\nNo \nCycle\nLoss\na pizza and a \ncup of coffee on \na table\npizza with a cup \nof coffee\npizza with a cup \nof coffee\ncoffee and a \nslice of pizza\ncoffee and a \nslice of pizza\na pizza and a \ncup of coffee on \na table\npizza\npizza from the \ntop - spicy pizza\npizza from the \ntop - spicy pizza\npieces of pizza \non a white \nbackground\nFigure 10: More cycle generation results with or without cycle consistency loss.\n19\n"
  },
  {
    "title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
    "link": "https://arxiv.org/pdf/2310.03094.pdf",
    "upvote": "12",
    "text": "Published as a conference paper at ICLR 2024\nLARGE LANGUAGE MODEL CASCADES WITH MIX-\nTURE OF THOUGHT REPRESENTATIONS FOR COST-\nEFFICIENT REASONING\nMurong Yue1, Jie Zhao2, Min Zhang3, Liang Du2, Ziyu Yao1\n1George Mason University 2Microsoft 3Virginia Tech\n1{myue, ziyuyao}@gmu.edu 2{zhaojie, liang.du}@microsoft.com\n3{minzhang23}@vt.edu\nABSTRACT\nLarge language models (LLMs) such as GPT-4 have exhibited remarkable per-\nformance in a variety of tasks, but this strong performance often comes with the\nhigh expense of using paid API services. In this paper, we are motivated to study\nbuilding an LLM cascade to save the cost of using LLMs, particularly for perform-\ning reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the\nintuition that simpler questions can be addressed by a weaker but more afford-\nable LLM, whereas only the challenging questions necessitate the stronger and\nmore expensive LLM. To realize this decision-making, we consider \u201canswer con-\nsistency\u201d of the weaker LLM as a signal of the question difficulty and propose\nseveral methods for the answer sampling and consistency checking, including one\nleveraging a mixture of two thought representations (i.e., Chain-of-Thought (Wei\net al., 2022) and Program-of-Thought (Chen et al., 2022; Gao et al., 2023)).\nThrough experiments on six reasoning benchmark datasets, with GPT-3.5-turbo\nand GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate\nthat our proposed LLM cascades can achieve performance comparable to using\nsolely the stronger LLM but require only 40% of its cost. Our codes are available\nat https://github.com/MurongYue/LLM_MoT_cascade.\n1\nINTRODUCTION\nLarge language models (LLMs) such as GPT-4 have exhibited remarkable performance in reasoning\ntasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023). Because of the intensive\ncomputing resources required for training and hosting the LLMs for inference, many such LLMs\nare only accessible via paid API services, thus leading to high monetary costs. In this work, we\nare motivated to study strategies for reducing the costs of using LLMs while not sacrificing task\nperformance, particularly for LLMs\u2019 applications to reasoning tasks.\nDifferent types and versions of LLMs often come with different capabilities and costs. Typically,\nLLMs with better performance (termed \u201cstronger LLMs\u201d) are more expensive than those with rela-\ntively worse overall performance (termed \u201cweaker LLMs\u201d). For example, GPT-4 (OpenAI, 2023) is\n30 times more expensive than GPT-3.5-turbo for the output tokens.1 It thus implies a promising so-\nlution to cost-saving. That is, simple questions could be answered by the weaker but more affordable\nLLM, whereas only the difficult questions need to be tackled by the more expensive, stronger LLM.\nDrawing inspirations from here, Chen et al. (2023a) explored the idea of \u201cLLM cascades\u201d, where a\nquestion is always first answered by a weaker LLM, and then optionally routed to a stronger LLM\nwhen the the weaker LLM\u2019s answer is not accepted (Figure 1). To decide this routing, this work\nsuggested fine-tuning a smaller LLM to score each question along with its answer produced by the\nweaker LLM. While this approach could work for some tasks, in practice, we observed that it did\nnot yield satisfying performance for intricate reasoning tasks. Intuitively, it is very challenging to\nevaluate the difficulty and the answer correctness of a reasoning question solely based on its literal\nexpression, even with a large enough LLM, since the errors could be nuanced despite the reasoning\npaths appearing promising (Madaan et al., 2023).\n1https://openai.com/pricing.\n1\narXiv:2310.03094v3  [cs.CL]  8 Feb 2024\nPublished as a conference paper at ICLR 2024\nIn this work, we proposed to devise this routing decision-maker from a different angle, i.e., the\n\u201canswer consistency\u201d of the weaker LLM (Wang et al., 2023). This is inspired by the observation\nthat answers from the weaker LLM tend to be consistent in multiple sampling paths when the ques-\ntion is easy, but inconsistent when the question is hard. To implement this idea, we proposed two\ntypes of methods, a vote-based method that examines if the agreement of multiple answer samples\non the majority-voted answer surpasses a pre-defined confidence threshold, and a verification-based\nmethod that checks if the majority-voted answers sampled from different prompts are consistent. To\nrealize the two methods in reasoning tasks, we further investigated multiple strategies for answer\nsampling, including sampling from a single set versus two sets of task demonstrations. In particu-\nlar, we proposed to leverage a \u201cmixture of thought (MoT) representations\u201d, which samples answers\nfrom both Chain-of-Thought (Wei et al., 2022, CoT) and Program-of-Thought (Chen et al., 2022;\nGao et al., 2023, PoT) prompts, emulating how experts can provide diverse perspectives to the same\nquestion. This follows the same spirit of ensembling (Rokach, 2010), but is applied to developing\nLLM cascades for the first time. By pairing different sampling strategies with the two answer consis-\ntency checking methods (i.e., vote and verification), we end up with ten approaches to implementing\nthe LLM cascade.\nTo evaluate the proposed approaches, we conducted experiments on six reasoning datasets, covering\nmathematical, symbolic, and causal reasoning tasks, using GPT-3.5-turbo as the weaker LLM and\nGPT-4 as the stronger one. The experimental results demonstrated the effectiveness of LLM cas-\ncades based on answering consistency. That is, different approaches that we proposed for LLM cas-\ncades can generally achieve performance comparable to or even better than fully using the stronger\nLLM, while they require only half or less relative cost to the latter. In particular, our approaches\nbased on a mixture of thought representations achieved comparable task performance with only 40%\nof the cost of GPT-4. Our results also underscored the effectiveness of sampling answers from di-\nverse prompt settings, such as sampling from different task demonstrations or different thought rep-\nresentations. Our further analysis revealed that different prompt settings can often provide different\nopinions for the more complex questions while tending to be consistent for easier ones, which allows\nus to distinguish questions at different difficulty levels more accurately for cascade decision-making.\nFinally, we also compared our consistency-based approaches with fine-tuned smaller LLMs (Chen\net al., 2023a) as well as other variants that make the routing decisions based on the literal expressions\nof the question and its answer. Our approaches exhibited strong advantages over all of them.\n2\nLLM CASCADES FOR COST-EFFICIENT REASONING\n2.1\nOVERVIEW OF LLM CASCADES\nQuery\nWeaker\nLLM\nAnswer\nAnswer\nLLM Cascade\nDecision Maker\nStronger\nLLM\nAccept\nReject\nFinal Answer\nFinal Answer\nFigure 1: Illustration of LLM cascade chaining a\nweaker but cheaper LLM with a stronger but more\ncostly one.\nWe leverage a cascade of LLMs to save the cost\nof in-context LLM reasoning, as illustrated in\nFigure 1. Specifically, we assume two LLMs.\nThe weaker LLM (denoted as LLM w) yields\nrelatively worse performance but is less costly,\nwhereas the stronger LLM (denoted as LLM s)\nenjoys better task performance but is more ex-\npensive. Given a question Q, the LLM cascade\nfirst employs the weaker LLM to obtain an ini-\ntial answer Aw. This answer, along with other\nmetadata produced by the weaker LLM, will then be fed to a cascade decision maker to decide\nwhether the answer can be accepted as the final one. If the answer is rejected, the stronger LLM\nshould be invoked to provide a more reliable answer As. As a consequence, the total cost of answer-\ning the question becomes\nC = Cw + Cd + 1reject Cs,\n(1)\nwhere Cw and Cs indicate the costs from calling the weaker and the stronger LLMs, respectively,\nCd denotes any cost involved in the LLM cascade decision-making process, and 1reject = 1 holds\nif and only if the decision maker rejects the answer.\nBoth LLMs solve the question via few-shot in-context learning, e.g., for the weaker LLM, an answer\nAw is produced by sampling from PLLMw(Aw |E1||E2|| . . . ||EM||Q), where E1||E2|| . . . ||EM||Q\ndenotes a concatenation of M task demonstrations and the input question Q, forming the \u201cprompt\ninput\u201d to the LLM (Brown et al., 2020). As M task examples are used to demonstrate the task,\n2\nPublished as a conference paper at ICLR 2024\nit indicates a \u201cM-shot in-context learning\u201d of LLM. For reasoning tasks, in practice, a LLM will\ntypically be prompted to elaborate on its reasoning process via \u201cthought representations\u201d, such as\nChain-of-Thought (Wei et al., 2022, CoT) and Program-of-Thought (Chen et al., 2022; Gao et al.,\n2023, PoT), where the reasoning process is described step by step via natural language and program-\nming language, respectively. The answer (e.g., a numerical result of the mathematical calculation)\ncan then be extracted from the texts (for CoT) or obtained by executing the code (for PoT).\n2.2\nANSWER CONSISTENCY-BASED CASCADE DECISION-MAKING\nThe core of our LLM cascade is the decision maker, which takes in the output from the weaker\nLLM, and then decides whether to route to the stronger LLM or not. An ideal cascade decision\nmaker should call the stronger LLM only when the answer by the weaker LLM is wrong, such that\nthe total cost C can be minimized without degrading the overall task performance (compared with\nusing the stronger LLM all the time). To this end, we propose two methodologies based on the\n\u201canswer consistency\u201d of the weaker LLM, which we elaborate on below.\nAnswer Consistency and Sources of Sampling\nAnswer consistency has been found helpful for\nimproving the LLM performance in reasoning tasks (Wang et al., 2023). Instead of greedily de-\ncoding one answer for each question, Wang et al. (2023) sampled a diverse set of reasoning paths\n(or thought processes) and then selected the most consistent answer by marginalizing out the sam-\npled paths. Drawing inspiration from the prior work, we make the following hypothesis: When the\nweaker LLM samples highly consistent answers for a given question, it reveals a high \u201cconfidence\u201d\nin solving this question and its most consistent answer is likely to be correct; in this case, there is\nthus no need to invoke the stronger LLM.\nTo realize this intuition, we generalize from Wang et al. (2023) and consider three sources of sam-\npling consistency:\n\u2022 In-distribution sampling: As Wang et al. (2023), we consider sampling multiple answers given\nthe same prompt input to the weaker LLM. In practice, this can be achieved by setting a non-zero\ntemperature for the weaker LLM.\n\u2022 Sampling from different in-context demonstrations: We further consider sampling answers\nfrom two sets of task demonstrations under the same thought representation. For example, to\ndemonstrate the CoT process in mathematical reasoning tasks, Wei et al. (2022) annotated eight\nmath examples as the demonstrations and performed 8-shot in-context learning. We additionally\nannotated another eight examples as the second set of demonstrations, which allowed us to further\ndiversify the sources of answer sampling.\n\u2022 Sampling from different thought representations: While existing literature typically investi-\ngated either CoT or PoT independently, in this work, we propose to leverage the synergy of both\nthought representations in a single task. We hypothesize that an LLM obtains truly high confi-\ndence in its problem-solving, only when it is able to produce a consistent answer agnostic to how\nthe intermediate steps are represented. Therefore, we propose to sample the weaker LLM answers\nfrom a \u201cmixture of thought (MoT) representations\u201d, which includes both CoT and PoT prompts.\nBelow, we introduce our methodologies for consistency checking based on the answer samples.\nMethod 1: Vote-based decision-making\nThe first method calculates the consistency of the\nweaker LLM\u2019s answer samples by voting.\nFormally, for a single prompt, we denote the set\nof answers produced by the weaker LLM for each question Q as (Aw\n1 , Aw\n2 , ..., Aw\nK), where K\nis the pre-defined number of samples. When sampling from two different prompts, we denote\n(Aw\n11, Aw\n12, ..., Aw\n1K1) and (Aw\n21, Aw\n22, ..., Aw\n2K2) as the answer samples produced by each of them,\nwhere K1 and K2 represent the pre-defined sample size for each prompt setting, respectively. Note\nthat for this method, we do not distinguish answers sampled with a single prompt or multiple prompts\n(e.g., samples from different prompts have exactly equal weights when voting). The most consis-\ntent answer can then be selected as the one that most samples agree with, and this answer will also\nbe regarded as the final answer Aw by the weaker LLM. The decision maker measures the weaker\nLLM\u2019s consistency via the agreement score\ns =\nPK\ni=1 1Aw\ni =Aw\nK\nor s =\nPK1\ni=1 1Aw\n1i=Aw + PK2\ni=1 1Aw\n2i=Aw\nK1 + K2\n.\n(2)\n3\nPublished as a conference paper at ICLR 2024\nCoT\nPoT\nQuestion: Kobe and Pau went to\na restaurant...\nAnswer: Pau ordered 5 x 2 = 10\nfried chickens in total.Therefore,\nPau ate 10 x 2 = 20 pieces of fried\nchicken. Ans = 20\n#Question: Kobe and Pau went to\na restaurant...\n#Python code, return ans\nkobe_order=5\npau_order=kobe_order*2\npau_eaten =2*pau_order\n#Question: Joelle has 5 orchids and\n4 African daisies on her balcony...\n#Python code, return ans\nQuery \n\"Joelle has 5 orchids and 4 African\ndaisies on her balcony...How\nmany\u00a0petals do the daisies have\ncompared to the orchids?\"\nConsistency\nMeasure\nMajority\nVote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D   /2D       -Vote\nVote-based\n...\nVerify-based\nMajority\nVote\nMajority\nVote\nMoT-1D-Verify\n...\n...\nPoT-2D-Verify\n...\n...\nCoT-2D-Verify\n...\n...\nQuestion: Joelle has 5 orchids and\n4 African daisies on her balcony...\nAnswer:\u00a0\n...\nAccept\nReject\nY\nN\n...\n...\n...\n...\nCoT-1D-Vote\nPoT-1D-Vote\n...\n...\nAccept\nReject\nY\nN\nMoT-2D-Verify\n...\n...\nWeaker\nLLM\nDemo1\nDemo1 Demo2\nDemo1\nDemo1 Demo2\nDemo1\nDemo1 Demo2\nDemo1\nDemo2\nDemo1\nDemo2\nDemo1\nDemo2\nDemo1\nDemo1\nFigure 2: An overview of our approaches (6 vote-based and 4 verification-based). We use\nto\nrepresent the answers from PoT and\nto represent the answers from CoT. Demoi is the i-th set of\ndemonstrations.\nThe larger the s, the more consistent the weaker LLM\u2019s answer samples. In conjunction with a\npre-defined threshold value \u03c4, the decision maker accepts the weaker LLM\u2019s most consistent answer\nAw when s \u2265 \u03c4 and rejects it otherwise. As a result, the total cost of answering a question (Eq 1)\ncan vary depending on the threshold.\nMethod 2: Verification-based decision-making\nIn the case of producing samples from two dif-\nferent prompt settings (i.e., different demonstrations or thought representations), we propose the\nsecond method, which compares the most consistent answers produced by each prompt as the an-\nswer verification. As previously mentioned, we could obtain two sets of answers from distinct\nprompts: (Aw\n11, Aw\n12, ..., Aw\n1K1) and (Aw\n21, Aw\n22, ..., Aw\n2K2). Our method then verifies the most consis-\ntent answers within each prompt, denoted as Aw\u2032\n1 and Aw\u2032\n2 respectively, as follows:\ns = 1Aw\u2032\n1 =Aw\u2032\n2 .\n(3)\nOnly when s equals 1, i.e., when the two answers are the same, the weaker LLM\u2019s answer will be\naccepted by the decision maker. In this case, the final answer of the weaker LLM will be the same\nas the two most consistent answers, i.e., Aw = Aw\u2032\n1 = Aw\u2032\n2 .\nIn comparison, these two methods have different applicable scenarios. The vote-based method is\nwell-suited for scenarios with pre-defined cost constraints. In such cases, we have the flexibility to\ntune the threshold to ensure it aligns with the constraint. On the other hand, the verification-based\nmethod is capable of producing relatively optimal results without the need for threshold tuning,\nalthough it lacks flexibility. We will systematically compare the two methods in experiments.\n2.3\nLLM CASCADES FOR REASONING TASKS\nWe instantiate the proposed two methods in LLM reasoning tasks with different sampling sources,\nresulting in 10 approaches, as summarized in Figure 2. Specifically, 6 approaches adopt vote-based\ndecision-making: CoT-1D-Vote collects K answers sampled from prompting the weaker LLM\nwith the CoT representation, and then calculates the answer consistency for decision-making, fol-\nlowing Eq 2. Similarly, PoT-1D-Vote bases its decision-making on answers sampled from a\nPoT prompt. To diversify the sources of the answers, for each thought representation, we further\nconsider sampling from two sets of CoT or PoT demonstrations, resulting in CoT-2D-Vote and\nPoT-2D-Vote, respectively. Finally, the vote-based approaches also include two variants lever-\naging a mixture of thought (MoT) representations. For MoT-1D-Vote, K1 answers are sampled\nfrom the CoT prompt and another K2 from the PoT prompt, and a union set of their answers are\nthen used to compute the consistency score s. For MoT-2D-Vote, the procedure is similar, except\nthat the CoT and the PoT prompts are annotated from two sets of demonstration examples.\nThe verification-based approaches assume answer samples from two different prompts.\nWe\ninstantiate 4 variants, including CoT-2D-Verify, where we prompt the weaker LLM with\n4\nPublished as a conference paper at ICLR 2024\ntwo sets of CoT demonstrations, resulting in two answer sets for decision-making (Eq 3);\nPoT-2D-Verify, where we similarly prompt the weaker LLM with two sets of PoT demonstra-\ntions; MoT-1D-Verify, where we consider two sets of answers from two thought representation\nprompts (but on the same set of task demonstration examples); and MoT-2D-Verify, which ad-\nditionally employs different sets of demonstrations when prompting the weaker LLM with different\nthought representations.\nCost-Comparable Sample Size Configuration\nTo fairly compare all approaches in terms of their\neffectiveness in identifying easy (or correct) vs. hard (or correct) questions, we aim to configure\ntheir costs to be comparable. Since we use the same prompt to the stronger LLM, Cs is agnostic\nto the specific approach. However, different approaches may require varying costs in calling the\nweaker LLM (i.e., Cw). For example, even with the same total sampling size (i.e., K = K1 +\nK2), CoT-1D-Vote and CoT-2D-Vote result in different input token usages because the former\nprompt the LLM once but the latter prompt it twice, doubling the token usages. Therefore, we unify\nthe Cw by a coarse-grained cost alignment. Our analysis suggests different configurations of K for\ndifferent approaches. We refer readers to Appendix A for more details. Since our configuration can\nonly yield comparable costs due to certain simplifications, rather than exact costs, we still report the\nactual token cost for each approach.\n3\nEXPERIMENT\n3.1\nEXPERIMENTAL SETTING\nWe evaluate our LLM cascade approaches on six datasets, covering (1) mathematical reasoning,\nincluding GSM8k (Cobbe et al., 2021), ASDIV (Ling et al., 2017), and TabMWP (Lu et al., 2023);\n(2) symbolic reasoning from BIG-Bench Hard (bench authors, 2023), including DATE and Navi-\ngate; and (3) causal reasoning, including CREPE (Zhang et al., 2023). In our pipeline, we leverage\nthe GPT-3.5-turbo (4k context) as the weaker LLM and the GPT-4 (8k context) with CoT self-\nconsistency (Wang et al., 2023, SC) as the stronger LLM. Throughout our experiments, we set the\nnumber of task demonstrations as M = 8. We use the same demonstration examples as prior\nwork (Chen et al., 2022; Wei et al., 2022). When additional task demonstrations are needed (e.g.,\nfor 2D approaches), we randomly sample examples from the training data and manually annotate\nthem with thought representations. We set the number of sampling paths as K = 20 for GPT-3.5-\nturbo and K = 3 for GPT-4. The sampling temperature by default is 0.4 for both LLMs. The\nmetrics we use are the task accuracy and the relative cost compared with the cost of GPT-4 with\nCoT SC (denoted as GPT-4-CoT-SC). A lower relative cost and higher accuracy indicate bet-\nter performance. We also compared our approaches with baselines using only the weaker LLM\n(i.e., GPT-3.5-CoT-SC, GPT-3.5-PoT-SC) or only the strong LLM in different ways (i.e.,\nGPT-4-CoT-Greedy, GPT-4-PoT-Greedy, GPT-4-CoT-SC, GPT-4-PoT-SC). Our ex-\nperimental details can be found in Appendix B and reproducible prompts in Appendix L.\n3.2\nMAIN RESULTS\nFigure 3 illustrates the performance of our proposed approaches. For Vote-based approaches, we\ndraw curves by changing the pre-defined threshold \u03c4 varying from 0.4 to 1. A high value of threshold\nsignifies a more rigorous criterion for trusting the answers from the weaker LLM, making more\nexamples transferred to the stronger LLM. Our observations are as follows:\nOur pipeline achieves comparable task performance with significantly reduced costs.\nOn av-\nerage, all of our cascade variants (Vote or Verify) demonstrate significant cost efficiency. In\nparticular, as shown in the average plot, the four MoT variants achieve comparable task perfor-\nmance (\u223c0.929 accuracy) to GPT-4-CoT-SC (0.931) while demanding only 40% of its cost. On\nCREPE, MoT variants even outperform GPT-4-CoT-SC (0.885 vs. 0.871) at 47% of its cost. Our\napproaches based on CoT and PoT also exhibit the capability to save costs while maintaining the\noverall task performance. For example, CoT-2D-Vote achieved 0.924 task accuracy on average\nbut demanded only 57% relative cost. These observations suggest the effectiveness of our cascade\ndecision maker via checking the answer consistency of the weaker LLM.\nSampling from diverse prompt settings helps cascade decision-making.\nOur results show\nthat variants involving diverse sources of sampling,\nsuch as CoT/PoT-2D-Vote and\nMoT-1D/2D-Vote, can more precisely distinguish between easy and hard reasoning questions,\n5\nPublished as a conference paper at ICLR 2024\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.80\n0.85\n0.90\n0.95\nTask Accuracy\nGSM8k\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.86\n0.88\n0.90\n0.92\n0.94\nTask Accuracy\nASDIV\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\nTask Accuracy\nTabMWP\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nRelative Cost\n0.70\n0.75\n0.80\n0.85\n0.90\nTask Accuracy\nDATE\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nRelative Cost\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\nTask Accuracy\nNavigate\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nRelative Cost\n0.75\n0.80\n0.85\nTask Accuracy\nCREPE\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\nTask Accuracy\nAverage\nLegend\nGPT-3.5-CoT-SC\nGPT-3.5-PoT-SC\nGPT-4-CoT-SC\nGPT-4-PoT-SC\nGPT-4-CoT-Greedy\nGPT-4-PoT-Greedy\nCoT-1D-Vote\nPoT-1D-Vote\nMoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-2D-Vote\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\n \nFigure 3: Main experiment results over six reasoning datasets. The bottom figure represents the\naverage performance. The exact numerical results are included in Appendix C.\ncompared with their counterparts sampling from single sources, i.e., CoT/PoT-1D-Vote. For\nexample, between CoT-2D-Vote and CoT-1D-Vote, the former outperforms the latter by 1.4%\nabsolute accuracy under the same relative cost of 0.4 on average.\nMixing thought representations is particularly effective.\nFurthermore, we find that mixing the\ntwo thought representations (i.e., MoT-1D/2D-Vote) outperforms decision-making using either\nof them (i.e., CoT-1D/2D-vote and PoT-1D/2D-vote). This is illustrated by the gap in the\naverage plot and is consistent on most datasets except DATE, where many test questions are very\nsimilar to the demonstration examples. Intuitively, this is because different thought representations\ncan bring in more diverse \u201copinions\u201d of the weaker LLM on the same input question, resembling\nhow a group of experts with diverse perspectives could contribute to more effective results in collab-\norative work. It can also be viewed as \u201censembling\u201d LLMs, which utilizes the intuition that variants\nof the same model typically share few mistakes (Rokach, 2010). We provide a further investigation\nof this effect in Section 3.3. We also note that when using MoT, no obvious difference is perceived\nbetween using one set (i.e., MoT-1D-Vote) or two sets (i.e., MoT-2D-Vote) of task demonstra-\ntions. This result reveals that tuning the thought representations is more helpful for measuring an\nLLM\u2019s (un)certainty on its answer than tuning the task demonstrations.\nIncreasing the threshold yields marginal benefits for MoT-1D/2D-Vote.\nAs costs increase,\nthe curves of MoT-1D/2D-Vote flatten out, showing that pushing the threshold to exceedingly\nhigh is unnecessary. This is because even for an easy question, the weaker LLM may still have hal-\nlucinations in a small set of answers. Setting the threshold too high can lead to the decision-making\nbeing influenced by them. It may result in easy questions being sent to the stronger LLM incorrectly,\nthus driving up the overall cost. In our practice, a threshold that can balance cost and accuracy typi-\ncally falls between 0.5 and 0.6. When it comes to MoT-1D/2D-Verify, we can strike a balance\nbetween cost and accuracy as the verification method can tolerate a few hallucinations.\n3.3\nANALYSIS ON MIXTURE OF THOUGHT REPRESENTATIONS\nTo understand the effect of MoT, we analyze three vote-based approaches. We first group questions\ninto \u201ceasy\u201d and \u201chard\u201d based on whether the weaker LLM can answer them correctly (i.e., whether\nthe majority-voted answer Aw is correct or not). For all easy/hard questions, we then calculate\nthe average consistency score following Eq 2. We present the average consistency scores for each\napproach, categorizing easy and hard questions based on whether the weaker LLM can answer them\ncorrectly (i.e., Aw being correct). A higher average consistency score indicates our decision-maker\nplaces greater trust in the answer from the weaker LLM. Hence, an effective cascade decision-\n6\nPublished as a conference paper at ICLR 2024\nQuestion:\u00a0Blake and Kelly are having a contest to see who can run the most in 15 minutes. They decide to do it on a football \ufb01eld that is 100\nyards long. Blake runs back and forth 15 times. Kelly runs back and forth once, and then decides that she doesn't want to run next to Blake, so\nshe starts to run to the 40-yard line and back. She does this 34 times. How much farther does the winner run than the loser?\nAnswer: [...(Ignored)] Kelly runs to the 40-\nyard line and back 34 times, so she runs\n34*80=2720 yards.\u00a0Thus, Blake runs 3000-\n2720=280 yards farther than Kelly.\nAnswer: [...(Ignored)] Kelly runs 34 x (40 x 2)\n= 2720 yards. Blake runs 3000 - 2720 = 280\nyards\u00a0farther\u00a0than Kelly.\n# Python code, return ans\n[...(Ignored)]\ndistance_covered_by_kelly =\n(length_of_football_field *\nnum_of_laps_for_kelly * 2) + (40 *\nnum_of_laps_for_kelly * 2)\nans = abs(distance_covered_by_blake\n- distance_covered_by_kelly)\nCoT 1\nCoT 2\nPoT 1\nBlack runs 200 yards for each back and\nforth because...He runs 3000 yards in total\nbecause...Kelly runs 200 yards at \ufb01rst\nbecause 100 x 2 = 200. She runs 80 yards\neach time she does a back and forth to the\n40-yard line because... She runs 2720 from\nthese back and forth. She runs 2920 in total\nbecause...The winner ran 80 yards more\nbecause 3000 - 2920=80.\nGold\n(Answer via Python execution: 6520.0)\nLogic Generation Error\nValue Grounding Error\nFigure 4: Average consistency scores (Top) and case studies (Bottom; simplified for presentation\npurposes) of various approaches showing the advantage of MoT. Shades highlight similar mistakes\nby CoT even based on different task demonstrations, and dissimilar mistakes by PoT.\nmaker should reveal relatively higher consistency scores for easy questions and lower ones for hard\nquestions, leading to a larger \u201dgap\u201d between them.\nWe show the results in Figure 4 (Top). It is observed that all approaches lead to higher consistency\nscores on the easy questions than those on the hard questions, which explains their overall effec-\ntiveness for cascade decision-making. Because of involving two different thought representations,\nMoT-1D-Vote tends to have a lower consistency score compared with the two CoT approaches.\nHowever, it still ends up with a larger \u201cgap\u201d in the consistency scores for the easy and the hard\nquestions, which is particularly prominent on the Navigate dataset where it gives the best perfor-\nmance gain (Figure 3). In contrast, CoT-1D-Vote results in the smallest score gap, indicating\nits weakness in distinguishing between the easy and the hard questions, particularly for Navigate.\nThis weakness is mitigated by diversifying the prompting with two sets of task demonstrations (i.e.,\nCoT-2D-Vote), but it still underperforms mixing the thought representations.\nFinally, in Figure 4 (Bottom) we show that when CoT cannot answer a hard question, prompting the\nweaker LLM with another set of task demonstrations (but still under the CoT representation) often\nyields the same mistaken answer, which thus results in a high consistency score. On the contrary,\nPoT tends to make mistakes in a very different way and result in a different mistaken answer, which\nexplains the low consistency score of MoT-1D-Vote. More cases are shown in Appendix D.\n3.4\nROBUSTNESS EVALUATION\n0.2\n0.4\n0.6\n0.8\nRelative Cost\n0.90\n0.92\n0.94\n0.96\nTask Accuracy\nGSM8k variant T\n0.2\n0.4\n0.6\n0.8\nRelative Cost\n0.80\n0.85\n0.90\nTask Accuracy\nDate variant T\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.85\n0.86\n0.87\n0.88\n0.89\nTask Accuracy\nCREPE variant T\nCoT-2D-Vote T=0.4\nMoT-1D-Vote T=0.4\nCoT-2D-Vote T=0.8\nMoT-1D-Vote T=0.8\n0.2\n0.4\n0.6\nRelative Cost\n0.90\n0.92\n0.94\nTask Accuracy\nGSM8k variant K\n0.2\n0.4\n0.6\n0.8\nRelative Cost\n0.80\n0.85\n0.90\nTask Accuracy\nDate variant K\n0.2\n0.4\n0.6\n0.8\nRelative Cost\n0.85\n0.86\n0.87\n0.88\nTask Accuracy\nCREPE variant K\nCoT-2D-Vote K=20\nMoT-1D-Vote K=20\nCoT-2D-Vote K=40\nMoT-1D-Vote K=40\nFigure 5: Robustness analysis with varying temperature T and sample size K.\n7\nPublished as a conference paper at ICLR 2024\nWe further analyze if our results are sensitive to the change of the sampling temperature T or the\nsample size K. We select datasets for each type of reasoning task and conduct experiments with\nCoT-2D-Vote and MoT-1D-Vote. Our results are shown in Figure 5. We first look into the\neffect when increasing the sampling temperature T from 0.4 (our default setting) to 0.8. For both\napproaches, increasing their temperature yields comparable or slightly better performance. This is\nowing to the increased answer diversity when the temperature gets higher. However, in any case,\nMoT-1D-Vote consistently outperforms CoT-2D-Vote. Increasing the sample size K from 20\nto 40, on the other hand, leads to a rightward shift of the curves, implying that it requires higher cost\nto achieve the same task accuracy. This can be explained by the higher cost of the larger sample size,\nwhereas increasing the sample size does not contribute to the detection of easy vs. hard questions.\nLike in the results of varying temperatures, MoT robustly outperforms CoT in any case.\n3.5\nCOMPARISON TO EXTERNAL TEXT-BASED VERIFIERS\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.85\n0.90\n0.95\nTask Accuracy\nGSM8k\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.70\n0.75\n0.80\n0.85\n0.90\nTask Accuracy\nDATE\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.82\n0.84\n0.86\n0.88\nTask Accuracy\nCREPE\nGPT-3.5-CoT-SC\nGPT-4-CoT-SC\nMoT-1D-Vote\nCoT-2D-Vote\nMoT-1D-Verify\nLLM-Q\nLLM-QA\nFinetuned-Q\nFinetuned-QA\nFigure 6: Comparison with external verifiers showing the advantages of our approaches based on\nanswer consistency. We do not report finetuned verifiers for DATE as it does not have a training set.\nAs mentioned, prior work implemented the LLM cascade decision-maker by training an external\nverifier, which scores a question and its answer (from the weaker LLM) based on their literal de-\nscriptions (Chen et al., 2023a; \u02c7Sakota et al., 2023). Related to the above work, Chen et al. (2023b)\nand Madaan et al. (2023) also showed the promise of prompting LLMs to evaluate their own re-\nsponses. To perform a general comparison with such external verifiers, we conducted experiments on\nGSM8k, DATE, and CREPE with the following baselines: Finetuned-Q, which is a RoBERTa-\nbase model (Liu et al., 2020) fine-tuned to decide whether a question should be routed to the stronger\nLLM based on its description; Finetuned-QA, which works similarly as Finetuned-Q but ad-\nditionally takes the majority-voted answer from the weaker LLM (GPT-3.5-CoT-SC) as input;\nLLM-Q, where we instead prompt GPT-3.5-turbo as the verifier to judge based on the question de-\nscription; and LLM-QA, which similarly employs GPT-3.5-turbo to decide upon the question and\nthe weaker LLM\u2019s majority-voted answer. We leave details of the baselines in Appendix E.\nThe results in Figure 6 show that incorporating the external verifiers cannot achieve comparable\naccuracy with GPT-4-CoT-SC. For example, on the GSM8k dataset, the highest accuracy with the\nexternal verifiers is 0.892, which is way lower than the accuracy (0.958) of GPT-4-CoT-SC and\nthe accuracy (0.951) of our approaches. They also show lower task accuracies than our approaches\nunder the same cost. It indicates that the external verifiers cannot yield satisfying results in complex\nreasoning tasks, which can be due to the intrinsic challenge of deciding question difficulty and\nanswer correctness solely based on their textual descriptions. For the calibration analysis, please\nrefer to the appendix I.\n3.6\nADDITIONAL STUDIES\nHow weak can the weaker LLM be?\nWe evaluate our approaches when adopting LLAMA2-13B\n(Touvron et al., 2023) as the weaker LLM and GPT-4 as the stronger LLM. The results are shown\nin the Appendix F. On the DATE dataset, our approaches still works. However, LLAMA2-13B\nas the weaker LLM doesn\u2019t yield ideal results on GSM8k and CREPE. That is because most of the\nquestions in GSM8k and CREPE are excessively complex for LLAMA2-13B. Therefore, LLAMA2-\n13B often fails to answer the questions consistently across multiple samplings. Hence, the choice of\na weaker LLM should be contingent on the task\u2019s level of difficulty. When the current weaker LLM\nstruggles with the task, it is advisable to consider switching to a more powerful LLM.\nCan the stronger LLM benefit from the weaker LLM hints?\nIn our LLM cascade, all questions\nwill obtain answers from the weaker LLM, no matter if they will be sent to the stronger LLM\n8\nPublished as a conference paper at ICLR 2024\nor not. Therefore, an interesting question is whether the answer produced by the weaker LLM\n(correct or incorrect) can provide \u201chints\u201d to enhance the stronger LLM. To answer this question, we\nexperimented with MoT-1D-Verify, where we additionally passed the two inconsistent answers\nAw\u2032\n1 and Aw\u2032\n2 as hints to the stronger LLM following the format of prior work (Zheng et al., 2023).\nWe observed that the hints can only yield slight improvement on DATE, but greatly hurt the model\nperformance on GSM8k and CREPE. Therefore, we conclude that hints from the weaker LLM,\nwhen it is uncertain about the question, do not help the stronger LLM. Details are in Appendix G.\nCan our method generalize to factual-based tasks?\nWe also explored whether our method can\nbe generalized to factual-based reasoning tasks in Appendix J.\n4\nRELATED WORK\nEarlier research has delved into techniques to enhance the cost efficiency of LLMs, including quan-\ntization, pruning, and decoding methods(Bai et al., 2022; Kurtic et al., 2023; Leviathan et al., 2022).\nWhile many LLMs are closed-source, certain studies have concentrated on how to utilize the API\nefficiently. Chen et al. (2023a) proposed an LLM cascade, which sends a query to a list of LLMs\nsequentially if the answers provided by the prior LLMs are regarded unacceptable. A concurrent\neffort is made by \u02c7Sakota et al. (2023), where the proposed approach determines which LLM to in-\nvoke prior to sending the query to any LLM. Both of these approaches employed external verifiers\nfine-tuned from smaller LMs and therefore demanded substantial training data and additional model\nlearning. In our experiments, we showed the ineffectiveness of these approaches when they were\nused to save costs for complex reasoning tasks, which echos the discovery of the shortcomings of\nLLM fine-tuning in recent work (Ni et al., 2023). In contrast, we proposed a novel solution for\nLLM cascades based on the answer consistency of the weaker LLM, which is training-free and is\nnot limited to a specific weaker LLM.\nReasoning with LLMs\nLLMs have demonstrated strong capability in solving reasoning\ntasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023). The standard few-shot in-\ncontext learning for LLMs is to feed the question to the LLM and let it generate the answer directly\n(Brown et al., 2020). Wei et al. (2022) then proposed the Chain-of-Thought approach, showing that\nprompting LLMs to think step by step can significantly improve their performance in solving reason-\ning problems, as it activates the built-in reasoning capabilities of LLMs (Madaan & Yazdanbakhsh,\n2023). To further facilitate the mathematical steps in a reasoning task, Chen et al. (2022) and Gao\net al. (2023) proposed the Program-of-Thought prompting, using code as the intermediate step and\nexecuting it with an external interpreter to entangle reasoning and numerical calculation. While\nmost existing works studied CoT and PoT independently as alternative approaches, several other\nworks have delved into the synergy between them, e.g., integrating code into CoT (Cheng et al.,\n2023b; He-Yueya et al., 2023), ensembling algebraic and code representations (Imani et al., 2023;\nYue et al., 2023), or performing automatic selection between CoT and PoT (Zhao et al., 2023). Sim-\nilar to these works, our approaches employ CoT and PoT jointly; however, unlike them, we leverage\nthis synergy for the novel application of cost-saving for LLM reasoning. On the other hand, like our\nLLM cascades, there are also recent works building pipelines chaining multiple LLMs (Varshney &\nBaral, 2022; Xiong et al., 2023a; Li et al., 2023; Lin et al., 2023a), but these works have a different\nfocus on improving the task accuracy, rather than reducing the LLM costs.\nUncertainty of LLMs\nOur approach involves measuring the uncertainty of the results of LLMs.\nMultiple concurrent works have investigated using the voting score to evaluate the answer\u2019s uncer-\ntainty (Xiong et al., 2023b; Si et al., 2023; Cai et al., 2023). However, they dismissed the potential\napplication for different representations of intermediate steps in the evaluation.\n5\nCONCLUSION\nWe introduce a simple yet efficient and universally applicable economical pipeline to dynamically\ndecide the LLMs in reasoning tasks, so as to save the token costs. Our approach based on checking\nthe answer consistency of the weaker LLM is novel and effective. Our discoveries emphasize that\nleveraging prompts with a mixture of thought representations in weaker LLM achieves the best\nperformance as it introduces diverse answers. Compared with fully employing the stronger LLM,\nour pipeline requires approximately 40% of expenses to achieve a comparable result. Future works\nare listed in Appendix K.\n9\nPublished as a conference paper at ICLR 2024\n6\nETHICS STATEMENT\nOur research endeavors to build a cost-efficient pipeline in solving reasoning problems in the LLM\nera. The significance of this work holds the potential to benefit a diverse range of organizations,\nparticularly those with limited financial resources such as local businesses, educational institutions,\nand non-profit organizations. By devising cost-effective solutions, we empower these resource-\nconstrained entities to harness the reasoning ability of LLMs cheaply, thereby fostering fairness\nand inclusivity within the NLP community. This aligns with the broader goal of ensuring that ad-\nvancements in NLP do not remain exclusive to a select few, but are accessible to a wider audience,\nregardless of their financial constraints.\nMoreover, for these industry giants, the cost savings facilitated by our findings can also be sub-\nstantial, particularly when facing extremely large throughputs. By optimizing cost-efficiency, our\nresearch contributes to not only economic savings but also environmental sustainability by mitigat-\ning the carbon footprint associated with running large-scale computations.\n7\nREPRODUCIBILITY STATEMENT\nOur pipeline is simple to implement and reproducible. We have documented all the experimental\ndetails, both in the main text and the appendix sections. While we cannot include the complete text of\nevery prompt due to their excessive length, we do provide examples of each prompt in Appendix L,\nfacilitating the reader\u2019s comprehension of the style employed. All of our implementations (including\nthe complete prompt scripts, the code for training external verifiers, the code for approach evaluation,\netc.) are publicly available at https://github.com/MurongYue/LLM_MoT_cascade.\n8\nACKNOWLEDGEMENT\nThis project was supported by Microsoft. It was also partially supported by resources provided\nby the Office of Research Computing at George Mason University (URL: https://orc.gmu.\nedu) and funded in part by grants from the National Science Foundation (Awards Number 1625039\nand 2018631). Murong and Ziyu appreciate the funding support from George Mason College of\nEngineering and Computing.\nREFERENCES\nHaoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael Lyu. Towards efficient post-\ntraining quantization of pre-trained language models. In Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\nURL https://openreview.net/forum?id=tvDRmAxGIjw.\nBIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of\nlanguage models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=uyTL5Bvosj.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nZefan Cai, Baobao Chang, and Wenjuan Han. Human-in-the-loop through chain-of-thought. arXiv\npreprint arXiv:2306.07932, 2023.\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while\nreducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023a.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00a8arli, and Denny Zhou. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128, 2023b.\n10\nPublished as a conference paper at ICLR 2024\nZhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language\nmodel apis. arXiv preprint arXiv:2301.08721, 2023a.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Binding lan-\nguage models in symbolic languages. In The Eleventh International Conference on Learning\nRepresentations, 2023b. URL https://openreview.net/forum?id=lH1PV42cbF.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,\n2021.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig.\nPAL: Program-aided language models.\nIn Andreas Krause, Emma\nBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.),\nProceedings of the 40th International Conference on Machine Learning, volume 202 of Pro-\nceedings of Machine Learning Research, pp. 10764\u201310799. PMLR, 23\u201329 Jul 2023.\nURL\nhttps://proceedings.mlr.press/v202/gao23f.html.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle\nUse a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transac-\ntions of the Association for Computational Linguistics (TACL), 2021.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 14953\u201314962, June 2023.\nJoy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. Solving math word problems\nby combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102, 2023.\nShima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large\nlanguage models. arXiv preprint arXiv:2303.05398, 2023.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer\nEl-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bow-\nman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna\nKravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom\nBrown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Ka-\nplan. Language models (mostly) know what they know, 2022.\nEldar Kurtic, Elias Frantar, and Dan Alistarh. Ziplm: Hardware-aware structured pruning of lan-\nguage models. arXiv preprint arXiv:2302.04089, 2023.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\ndecoding. In International Conference on Machine Learning, 2022. URL https://api.\nsemanticscholar.org/CorpusID:254096365.\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,\nYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.\nSolving quantitative rea-\nsoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=IFXTZERXdM7.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12286\u2013\n12312, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.acl-long.687. URL https://aclanthology.org/2023.acl-long.687.\n11\nPublished as a conference paper at ICLR 2024\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavat-\nula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with\nfast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023a. URL https://openreview.net/forum?id=\nRzk3GP1HN7.\nJianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more\nwith less, 2023b.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen-\neration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 158\u2013167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pre-\ntraining approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. In International Conference on Learning Representations (ICLR), 2023.\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought it takes\ntwo to tango, 2023. URL https://openreview.net/forum?id=z9fXRC5XdT.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir\nRadev, and Jianfeng Gao.\nLearning math reasoning from self-sampled correct and partially-\ncorrect solutions. In The Eleventh International Conference on Learning Representations, 2023.\nURL https://openreview.net/forum?id=4D4TSJE6-K.\nOpenAI. Gpt-4 technical report, 2023.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\nLior Rokach. Ensemble-based classifiers. Artificial intelligence review, 33:1\u201339, 2010.\nMarija \u02c7Sakota, Maxime Peyrard, and Robert West. Fly-swat or cannon? cost-effective language\nmodel choice via meta-modeling. arXiv preprint arXiv:2308.06077, 2023.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber,\nand Lijuan Wang.\nPrompting GPT-3 to be reliable.\nIn The Eleventh International Confer-\nence on Learning Representations, 2023. URL https://openreview.net/forum?id=\n98p5x51L5af.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nNeeraj Varshney and Chitta Baral. Model cascading: Towards jointly improving efficiency and\naccuracy of NLP systems. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Pro-\nceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.\n11007\u201311021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2022.emnlp-main.756. URL https://aclanthology.\norg/2022.emnlp-main.756.\n12\nPublished as a conference paper at ICLR 2024\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=1PL1NIMMrw.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Ad-\nvances in Neural Information Processing Systems, 2022. URL https://openreview.net/\nforum?id=_VjQlMeSB_J.\nKai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Diving into the inter-consistency of\nlarge language models: An insightful analysis through debate. arXiv preprint arXiv:2305.11595,\n2023a.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms\nexpress their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint\narXiv:2306.13063, 2023b.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\nMammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint\narXiv:2309.05653, 2023.\nLi Zhang, Hainiu Xu, Yue Yang, Shuyan Zhou, Weiqiu You, Manni Arora, and Chris Callison-\nburch. Causal reasoning of entities and events in procedural texts. In Findings of the Associa-\ntion for Computational Linguistics: EACL 2023, pp. 415\u2013431, Dubrovnik, Croatia, May 2023.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2023.\nfindings-eacl.31.\nXu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with\nlarge language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting\nimproves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. arXiv preprint arXiv:2304.06364, 2023.\n13\nPublished as a conference paper at ICLR 2024\nA\nCOST ANALYSIS OF LLM CASCADE APPROACHES\nIn Section 2.3, we introduced ten approaches to implement the LLM cascade decision maker. As\nwe are interested in saving the cost of LLM usage, we first analyze the token usage of each ap-\nproach. While different thought representations of the same task example could induce different\ntoken usages, the difference is very hard to quantify and also depends on the specific reasoning\ntask. For example, for GSM8k (Cobbe et al., 2021), the PoT representation of the same example is\ntypically shorter than its CoT counterpart, while a reversed comparison is observed for DATE and\nNavigate (bench authors, 2023). In addition, different demonstration examples may also consist of\ndifferent numbers of tokens, which is hard to quantify as well. To provide a unified analysis, we\nassume that every demonstration example for any task, regardless of the thought representation in\nuse, consumes Ntok tokens.\nBased on our hypothesis, we conducted a cost analysis, with results summarized in Table 1. To\ngive an example, consider comparing the token costs of CoT-1D-Vote and CoT-2D-Vote. For\nCoT-1D-Vote, the cost of calling weaker LLM is (MCw\ni +KCw\no )\u00d7Ntok; for CoT-2D-Vote, it\nis (2MCw\ni +K1Cw\no +K2Cw\no )\u00d7Ntok. Here, Cw\ni and Cw\no are the input and output per-token cost for\nthe weaker LLM, respectively, and M is the number of demonstrations in each prompt. In practice,\nwe set K1 = K2 = K2D/2, where K2D is the total sample size of two sets of demonstration\nexamples, and the total cost for CoT-2D-Vote can then be rewritten to (2MCw\ni + K2DCw\no ). To\nkeep the costs of CoT-1D-Vote and CoT-2D-Vote consistent, we set K2D = K1D \u2212 M \u00d7 Cw\ni\nCw\no .\nSimilarly, we can get the total sample size of different representations KMoT . The analysis guides\nus in configuring each approach to be \u201ccost-comparable\u201d (i.e., leading to similar Cw\u2019s). Finally,\nwe note that we do not change the number of task demonstrations (i.e., M) because LLMs are very\nsensitive to this configuration.\nMethod\nCertainty Value (s) Definition\nCost Estimation\nCoT-1D-Vote\nAgreement of K CoT samples on the majority-voted answer Aw, s\n=\nPK\ni=1 1Aw\ni =Aw/K\n(MCw\ni + K1DCw\no ) \u00d7 Ntok\nPoT-1D-Vote\nAgreement of K PoT samples on the majority-voted answer Aw, s\n=\nPK\ni=1 1Aw\ni =Aw/K\n(MCw\ni + K1DCw\no ) \u00d7 Ntok\nMoT-1D-Vote\nAgreement of KMoT /2 CoT and KMoT /2 PoT samples on the majority-voted\nanswer Aw, s = PKMoT\ni=1\n1Aw\ni =Aw/KMoT\n2\u00d7(MCw\ni + KMoT\n2\nCw\no )\u00d7Ntok\nCoT-2D-Vote\nAgreement of K2D/2 CoT samples over 2 sets of task demonstrations on the\nmajority-voted answer Aw, s = PK2D\ni=1 1Aw\ni =Aw/K2D\n2\u00d7(MCw\ni + K2D\n2\nCw\no )\u00d7Ntok\nPoT-2D-Vote\nAgreement of K2D/2 PoT samples over 2 sets of task demonstrations on the\nmajority-voted answer Aw, s = PK2D\ni=1 1Aw\ni =Aw/K2D\n2\u00d7(MCw\ni + K2D\n2\nCw\no )\u00d7Ntok\nMoT-2D-Vote\nAgreement of KMoT /2 CoT samples on one set of task demos and KMoT /2\nPoT samples on another set of task demos on their majority-voted answer A\u2217,\ns = PKMoT\ni=1\n1Aw\ni =Aw/KMoT\n2\u00d7(MCw\ni + KMoT\n2\nCw\no )\u00d7Ntok\nCoT-2D-Verify\nAgreement of the two majority-voted answers Aw\u2032\n1 and Aw\u2032\n2 over 2 sets of CoT\ntask demonstrations with K2D examples, s = 1Aw\u2032\n1 =Aw\u2032\n2\n2\u00d7(MCw\ni + K2D\n2\nCw\no )\u00d7Ntok\nPoT-2D-Verify\nAgreement of the two majority-voted answers Aw\u2032\n1 and Aw\u2032\n2 over 2 sets of PoT\ntask demonstrations with K2D examples, s = 1Aw\u2032\n1 =Aw\u2032\n2\n2\u00d7(MCw\ni + K2D\n2\nCw\no )\u00d7Ntok\nMoT-2D-Verify\nAgreement of the two majority-voted answers Aw\u2032\n1\nand Aw\u2032\n2\nover one set of\nCoT and another set of PoT task demonstrations with KMoT /2 examples, s =\n1Aw\u2032\n1 =Aw\u2032\n2\n2\u00d7(MCw\ni + KMoT\n2\nCw\no )\u00d7Ntok\nMoT-1D-Verify\nAgreement of the two majority-voted answers Aw\u2032\n1\nand Aw\u2032\n2\nfor CoT and\nPoT over the same set of task demonstrations with KMoT /2 examples, s =\n1Aw\u2032\n1 =Aw\u2032\n2\n2\u00d7(MCw\ni + KMoT\n2\nCw\no )\u00d7Ntok\nTable 1: Investigated approaches and cost calculations. For simplicity, we assume M-shot demon-\nstration prompting and that every demonstration example has Ntok tokens. Cw\ni is the input price per\ntoken and Cw\no is the output price per token. To keep consistent costs of using the weaker LLM, we\nset KMoT = K2D = K1D \u2212 M \u00d7 Cw\ni\nCw\no .\nB\nMAIN EXPERIMENTAL DETAILS\nImplementation Details\nWe run each approach two times to reduce variance and report the aver-\nage results. Since DATE and Navigate do not have a training set, before experiments we sampled 8\nshots of examples randomly and annotated them to be the second set of task demonstrations (for 2D\n14\nPublished as a conference paper at ICLR 2024\napproaches). The remaining examples are used consistently across all experiments as the test set.\nWe adopt \u201cPython code, return ans\u201d in PoT prompt to let the LLM generate the Python code (Chen\net al., 2022). The interpreter we used for executing PoT is Python 3.10, with some packages, such\nas DateTime2 to facilitate the execution of the generated code.\nMetrics Details\nWe evaluate the methods based on task accuracy and cost efficiency, with a lower\ncost and higher accuracy indicating better performance. For accuracy, when the answer is a string,\nwe use exact matching. When the answer is a number, we relax the evaluation criteria due to po-\ntential variations in the exact computations carried out by the external interpreter. Following prior\nwork (Chen et al., 2022), we adopt the tolerance to 0.001. For cost efficiency, we calculate the actual\ntoken number based on the tiktoken3 and the total cost 4 for each method and get the relative cost by\ncomparing it with the total cost of GPT-4-CoT-SC.\nC\nMAIN RESULT TABLES\nIn this section, we show the exact numerical results that are reported in Figure 3. The GSM8k result\nis in Table 2. The ASDIV result is in Table 3. The TabMWP result is in Table 4. The DATE result is\nin Table 5. The Navigation result is in Table 6. The CREPE result is in Table 7. The average results\nare in Table 8.\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.875\n0.184\n0.832\n0.141\n0.897\n0.211\n0.854\n0.170\n0.901\n0.187\n0.904\n0.195\n0.50\n0.896\n0.233\n0.850\n0.179\n0.912\n0.249\n0.872\n0.209\n0.917\n0.235\n0.918\n0.241\n0.55\n0.907\n0.265\n0.861\n0.206\n0.929\n0.309\n0.894\n0.262\n0.943\n0.331\n0.942\n0.334\n0.60\n0.917\n0.302\n0.876\n0.240\n0.937\n0.354\n0.910\n0.305\n0.948\n0.381\n0.948\n0.381\n0.65\n0.924\n0.336\n0.886\n0.265\n0.945\n0.408\n0.920\n0.342\n0.951\n0.425\n0.952\n0.425\n0.70\n0.929\n0.372\n0.895\n0.292\n0.945\n0.408\n0.920\n0.342\n0.951\n0.425\n0.952\n0.425\n0.80\n0.941\n0.438\n0.910\n0.346\n0.952\n0.514\n0.933\n0.427\n0.954\n0.528\n0.955\n0.530\n0.90\n0.950\n0.518\n0.924\n0.410\n0.955\n0.587\n0.938\n0.485\n0.955\n0.598\n0.956\n0.600\n1.00\n0.954\n0.660\n0.940\n0.517\n0.956\n0.699\n0.947\n0.565\n0.957\n0.693\n0.957\n0.691\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.926\n0.322\n0.909\n0.311\n0.947\n0.401\n0.949\n0.403\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.842\n0.111\n0.792\n0.078\n0.945\n0.789\n0.942\n0.603\n0.958\n1.000\n0.947\n0.745\nTable 2: Exact numerical results on GSM8k.\n2https://pypi.org/project/DATETime/\n3https://platform.openai.com/tokenizer\n4https://openai.com/pricing\n15\nPublished as a conference paper at ICLR 2024\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.902\n0.181\n0.872\n0.173\n0.911\n0.200\n0.877\n0.167\n0.913\n0.189\n0.916\n0.184\n0.50\n0.908\n0.209\n0.878\n0.190\n0.917\n0.225\n0.883\n0.188\n0.918\n0.218\n0.920\n0.214\n0.55\n0.910\n0.226\n0.882\n0.210\n0.924\n0.262\n0.894\n0.222\n0.930\n0.291\n0.931\n0.290\n0.60\n0.914\n0.243\n0.887\n0.224\n0.926\n0.288\n0.904\n0.251\n0.932\n0.324\n0.933\n0.324\n0.65\n0.917\n0.261\n0.893\n0.239\n0.929\n0.312\n0.912\n0.273\n0.933\n0.346\n0.933\n0.348\n0.70\n0.920\n0.281\n0.897\n0.254\n0.929\n0.312\n0.912\n0.275\n0.933\n0.347\n0.933\n0.348\n0.80\n0.924\n0.311\n0.906\n0.283\n0.930\n0.365\n0.923\n0.328\n0.935\n0.396\n0.934\n0.400\n0.90\n0.927\n0.348\n0.916\n0.318\n0.930\n0.402\n0.927\n0.360\n0.935\n0.430\n0.934\n0.433\n1.00\n0.930\n0.429\n0.925\n0.367\n0.931\n0.458\n0.930\n0.393\n0.935\n0.473\n0.934\n0.474\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.924\n0.272\n0.908\n0.297\n0.933\n0.357\n0.934\n0.361\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.887\n0.115\n0.854\n0.106\n0.927\n0.742\n0.930\n0.725\n0.933\n1.000\n0.943\n0.927\nTable 3: Exact numerical results on ASDIV.\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.868\n0.141\n0.883\n0.150\n0.900\n0.152\n0.892\n0.151\n0.902\n0.155\n0.912\n0.149\n0.50\n0.880\n0.169\n0.894\n0.177\n0.909\n0.183\n0.897\n0.179\n0.919\n0.193\n0.924\n0.185\n0.55\n0.890\n0.191\n0.899\n0.195\n0.934\n0.273\n0.916\n0.238\n0.951\n0.324\n0.952\n0.329\n0.60\n0.899\n0.216\n0.905\n0.214\n0.941\n0.318\n0.925\n0.288\n0.952\n0.370\n0.952\n0.381\n0.65\n0.910\n0.245\n0.910\n0.236\n0.945\n0.355\n0.935\n0.327\n0.954\n0.409\n0.952\n0.421\n0.70\n0.917\n0.273\n0.915\n0.254\n0.945\n0.355\n0.935\n0.327\n0.954\n0.409\n0.952\n0.421\n0.80\n0.927\n0.327\n0.924\n0.292\n0.950\n0.448\n0.946\n0.411\n0.957\n0.484\n0.956\n0.496\n0.90\n0.935\n0.393\n0.934\n0.345\n0.950\n0.513\n0.952\n0.460\n0.955\n0.531\n0.956\n0.548\n1.00\n0.946\n0.535\n0.947\n0.425\n0.950\n0.601\n0.955\n0.524\n0.955\n0.610\n0.955\n0.630\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.940\n0.333\n0.920\n0.262\n0.950\n0.342\n0.951\n0.357\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.844\n0.092\n0.849\n0.072\n0.958\n0.819\n0.933\n0.728\n0.961\n1.000\n0.941\n0.849\nTable 4: Exact numerical results on TabMWP.\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.737\n0.192\n0.793\n0.203\n0.775\n0.203\n0.829\n0.208\n0.805\n0.207\n0.812\n0.197\n0.50\n0.758\n0.235\n0.806\n0.229\n0.792\n0.240\n0.844\n0.235\n0.821\n0.240\n0.826\n0.225\n0.55\n0.769\n0.259\n0.818\n0.256\n0.835\n0.362\n0.872\n0.324\n0.871\n0.367\n0.868\n0.367\n0.60\n0.779\n0.289\n0.828\n0.283\n0.849\n0.431\n0.877\n0.367\n0.881\n0.417\n0.872\n0.425\n0.65\n0.795\n0.328\n0.837\n0.306\n0.862\n0.489\n0.882\n0.419\n0.889\n0.469\n0.881\n0.484\n0.70\n0.809\n0.357\n0.848\n0.329\n0.862\n0.489\n0.882\n0.419\n0.889\n0.469\n0.881\n0.484\n0.80\n0.835\n0.425\n0.860\n0.378\n0.877\n0.604\n0.890\n0.502\n0.894\n0.569\n0.888\n0.583\n0.90\n0.854\n0.511\n0.877\n0.425\n0.883\n0.666\n0.892\n0.552\n0.895\n0.637\n0.891\n0.654\n1.00\n0.877\n0.657\n0.888\n0.523\n0.888\n0.770\n0.889\n0.625\n0.892\n0.739\n0.892\n0.744\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.850\n0.456\n0.873\n0.392\n0.875\n0.449\n0.869\n0.459\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.677\n0.107\n0.770\n0.162\n0.876\n0.750\n0.873\n1.031\n0.886\n1.000\n0.890\n1.402\nTable 5: Exact numerical results on DATE.\n16\nPublished as a conference paper at ICLR 2024\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.860\n0.132\n0.871\n0.213\n0.898\n0.125\n0.868\n0.192\n0.878\n0.145\n0.880\n0.146\n0.50\n0.860\n0.134\n0.877\n0.246\n0.898\n0.125\n0.874\n0.219\n0.882\n0.153\n0.883\n0.152\n0.55\n0.862\n0.140\n0.881\n0.270\n0.913\n0.233\n0.882\n0.261\n0.964\n0.368\n0.964\n0.365\n0.60\n0.864\n0.144\n0.885\n0.293\n0.920\n0.265\n0.892\n0.305\n0.966\n0.402\n0.966\n0.400\n0.65\n0.866\n0.149\n0.889\n0.317\n0.924\n0.282\n0.908\n0.371\n0.968\n0.440\n0.967\n0.437\n0.70\n0.869\n0.159\n0.892\n0.349\n0.924\n0.282\n0.908\n0.371\n0.968\n0.440\n0.967\n0.437\n0.80\n0.876\n0.175\n0.903\n0.432\n0.934\n0.318\n0.926\n0.537\n0.971\n0.545\n0.970\n0.543\n0.90\n0.884\n0.193\n0.920\n0.548\n0.941\n0.339\n0.935\n0.649\n0.973\n0.626\n0.972\n0.624\n1.00\n0.901\n0.240\n0.942\n0.743\n0.948\n0.377\n0.948\n0.815\n0.973\n0.754\n0.973\n0.754\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.929\n0.293\n0.879\n0.208\n0.966\n0.400\n0.966\n0.398\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.856\n0.128\n0.859\n0.154\n0.977\n0.701\n0.877\n0.966\n0.975\n1.000\n0.878\n1.302\nTable 6: Exact numerical results on Navigate.\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.838\n0.188\n0.838\n0.461\n0.850\n0.180\n0.846\n0.369\n0.866\n0.255\n0.874\n0.257\n0.50\n0.843\n0.208\n0.842\n0.485\n0.857\n0.197\n0.855\n0.422\n0.871\n0.295\n0.878\n0.301\n0.55\n0.847\n0.227\n0.846\n0.500\n0.865\n0.241\n0.870\n0.552\n0.885\n0.460\n0.885\n0.471\n0.60\n0.852\n0.261\n0.846\n0.517\n0.865\n0.280\n0.869\n0.603\n0.882\n0.519\n0.882\n0.525\n0.65\n0.856\n0.284\n0.847\n0.537\n0.868\n0.320\n0.869\n0.658\n0.882\n0.573\n0.881\n0.575\n0.70\n0.863\n0.312\n0.849\n0.557\n0.869\n0.369\n0.872\n0.708\n0.879\n0.614\n0.880\n0.620\n0.80\n0.867\n0.374\n0.853\n0.603\n0.868\n0.419\n0.874\n0.754\n0.879\n0.660\n0.878\n0.663\n0.90\n0.873\n0.455\n0.861\n0.683\n0.869\n0.534\n0.874\n0.851\n0.877\n0.760\n0.873\n0.760\n1.00\n0.872\n0.594\n0.868\n0.803\n0.871\n0.621\n0.873\n0.925\n0.873\n0.847\n0.871\n0.846\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.860\n0.302\n0.874\n0.581\n0.882\n0.534\n0.883\n0.536\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.820\n0.157\n0.719\n0.226\n0.871\n0.577\n0.876\n0.871\n0.871\n1.000\n0.882\n1.522\nTable 7: Exact numerical results on CREPE.\nVoting Method\nCoT-1D-Vote\nPoT-1D-Vote\nCoT-2D-Vote\nPoT-2D-Vote\nMoT-1D-Vote\nMoT-2d-Vote\nThreshold\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.40\n0.847\n0.167\n0.848\n0.202\n0.872\n0.178\n0.861\n0.195\n0.878\n0.183\n0.883\n0.183\n0.50\n0.857\n0.195\n0.858\n0.231\n0.881\n0.203\n0.871\n0.227\n0.888\n0.216\n0.891\n0.216\n0.55\n0.864\n0.216\n0.864\n0.253\n0.900\n0.273\n0.888\n0.286\n0.924\n0.345\n0.924\n0.347\n0.60\n0.871\n0.240\n0.871\n0.276\n0.906\n0.312\n0.896\n0.329\n0.927\n0.389\n0.925\n0.391\n0.65\n0.878\n0.263\n0.877\n0.297\n0.912\n0.348\n0.904\n0.371\n0.929\n0.428\n0.928\n0.431\n0.70\n0.884\n0.289\n0.882\n0.320\n0.912\n0.354\n0.905\n0.377\n0.929\n0.433\n0.927\n0.436\n0.80\n0.895\n0.336\n0.893\n0.370\n0.918\n0.428\n0.915\n0.465\n0.932\n0.513\n0.930\n0.517\n0.90\n0.904\n0.394\n0.905\n0.436\n0.921\n0.488\n0.920\n0.530\n0.932\n0.576\n0.930\n0.581\n1.00\n0.913\n0.506\n0.918\n0.543\n0.924\n0.567\n0.924\n0.611\n0.931\n0.663\n0.930\n0.667\nVerify Method\nCoT-2D-Verify\nPoT-2D-Verify\nMoT-1D-Verify\nMoT-2D-Verify\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.905\n0.311\n0.894\n0.316\n0.925\n0.398\n0.925\n0.402\nWithout Cascade\nGPT-3-CoT-SC\nGPT-3-PoT-SC\nGPT-4 CoT-Greedy\nGPT-4 PoT-Greedy\nGPT-4 CoT-SC\nGPT-4 PoT-SC\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\nAcc\nCost\n0.821\n0.116\n0.807\n0.117\n0.926\n0.743\n0.905\n0.763\n0.931\n1.000\n0.913\n1.017\nTable 8: Exact numerical results on average across all datasets.\n17\nPublished as a conference paper at ICLR 2024\nD\nCASE STUDY\nIn this section, we perform a case study to learn why MoT can achieve better performance. When\nCoT makes a mistake, prompting the LLM with another set of CoT task demonstrations often leads\nto the same mistake. In contrast, PoT more frequently either directly gives a correct answer or\nmakes a different mistake than CoT\u2019s. Among those cases, 43% fall into the former category that\nPoT can solve the question, while the remaining 57% fall into the latter that PoT can make a different\nmistake. Here we use CoT1, CoT2, and PoT1 to represent the different prompts. CoT1 and CoT2\nshare the same representation but different demonstrations while CoT1 and PoT1 share the same\ndemonstration but different representations.\nD.1\nCASES POT CAN SOLVE\nHere we show cases satisfying that PoT can solve the question. The first case shows that the PoT can\nprovide different reasoning paths to avoid the same mistake with CoT and the second case shows\nthat the PoT can avoid the computation errors.\nD.1.1\nCASE 1\nQuestion: Juan and his brother Carlos are selling lemonade. For each gallon they make it costs 3\nfor lemons and 2 for sugar. They sell each glass for 0.50 and get 20 glasses per gallon. If they made\n25 in profit, how much did they spend on lemons?\nGold Answer: Each gallon sells for a total of 10 because 20 \u00d7 0.5 = 10\nEach gallon costs 5 to make because 3 + 2 = 5\nThey make 5 off each gallon because 10 \u2212 5 = 5\nThey made and sold 5 gallons because 25/5 = 5\nThey spent 15 on lemons because 5 \u00d7 3 = 15\nCoT1: Answer: They make 10 on each gallon of lemonade because 20 \u00d7 0.5 = 10\nThey made 2.5 gallons of lemonade because 25/10 = 2.5\nThey spent 7.5 on lemons because 2.5 \u00d7 3 = 7.5\nThey spent 5 on sugar because 2.5 \u00d7 2 = 5\nThey spent 12.5 in total because 7.5 + 5 = 12.5\nans = 7.5\nCoT2: Answer: They make 10 per gallon because 20 \u00d7 0.5 = 10\nThey sold 2.5 gallons because 25/10 = 2.5\nThey spent 7.5 on sugar because 2.5 \u00d7 2 \u00d7 1.5 = 7.5\nThey spent 7.5 on lemons because 2.5 \u00d7 3 = 7.5\nans = 7.5\nPoT1:\n# Python code, return ans\ncost_per_gallon = 3 + 2\nnum_of_glasses_per_gallon = 20\nprofit_per_gallon = num_of_glasses_per_gallon * 0.5 -\\\ncost_per_gallon\ntotal_profit = 25\ntotal_gallons = total_profit / profit_per_gallon\ncost_of_lemons = total_gallons * 3\nans = cost_of_lemons\n#ans = 15.0(This line is not generated by LLM.)\nD.1.2\nCASE 2\nQuestion: Robert wants to buy a bookshelf and a sofa and has received two offers. Furniture\nUnited\u2019s offer includes a 1, 350 advance payment and 6 monthly installments of 350 each. Cozy\nHomes\u2019 offer includes a 1, 100 advance payment and 9 monthly installments of 250 each. What is\nthe cost difference between the two offers?\n18\nPublished as a conference paper at ICLR 2024\nGold Answer: Furniture United\u2019s offer is 1, 350 + (6 \u00d7 350) = 3, 450 Cozy Homes\u2019 offer is\n1, 100 + (9 \u00d7 250) = 3, 350. The difference is 3, 450 \u2212 3, 350 = 100\nCoT1: Answer: Furniture United\u2019s total cost would be 1350 + 6 \u00d7 350 = 3150. Cozy Homes\u2019 total\ncost would be 1100+9\u00d7250 = 3350. The cost difference between the two offers is 3350\u22123150 =\n200. ans = 200\nCoT2: Answer: The total cost of Furniture United\u2019s offer is 1350 + (6 \u00d7 350) = 3, 150. The total\ncost of Cozy Homes\u2019 offer is 1100+(9\u00d7250) = 3, 350. The cost difference between the two offers\nis 3350 \u2212 3150 = 200. ans = 200.\nPoT1:\nfurniture_united_advance = 1350\nfurniture_united_monthly_installment = 350\ncozy_homes_advance = 1100\ncozy_homes_monthly_installment = 250\nfurniture_united_total_cost = furniture_united_advance + \\\n(6 * furniture_united_monthly_installment)\ncozy_homes_total_cost = cozy_homes_advance + \\\n(9 * cozy_homes_monthly_installment)\nans = furniture_united_total_cost - cozy_homes_total_cost\n# ans = 100.0(This line is not generated by LLM.)\nD.2\nCASES POT CANNOT SOLVE\nIn this section, we show some cases that PoT cannot solve but will make a different mistake. In\ncase 1, the CoT forgot to calculate the first 40-yard line but the PoT calculated the first 40-yard but\nwith an incorrect grounding value. In case 2, the CoT forgot to remove the lowest score. The PoT\nremoved the lowest score but grounded an incorrect number of total tests. We can see that although\nboth the CoT and PoT make mistakes, the reasons why they make mistakes are different, making\nthe final results different.\nD.2.1\nCASE 1\nQuestion: Blake and Kelly are having a contest to see who can run the most in 15 minutes. They\ndecide to do it on a football field that is 100 yards long. Blake runs back and forth 15 times. Kelly\nruns back and forth once, and then decides that she doesn\u2019t want to run next to Blake, so she starts\nto run to the 40-yard line and back. She does this 34 times. How much farther does the winner run\nthan the loser?\nGold Answer: Blake runs 200 yards for each back and forth because 100 x 2 = 200 He runs 3000\nyards in total because 200 x 15 = 3000 Kelly runs 200 yards at first because 100 x 2 = 200 She runs\n80 yards each time she does a back and forth to the 40-yard line because 40 x 2 = 80 She runs 2720\nfrom these back and forth because 34 x 80 = 2720 She runs 2920 in total because 2720 + 200 = 2920\nBlake won because 3000 > 2920 The winner ran 80 yards more than the loser because 3000 - 2920\n= 80\nCoT1: Answer: Blake runs back and forth 15 times, so she runs 15*100*2=3000 yards. Kelly\nruns to the 40-yard line and back 34 times, so she runs 34*80=2720 yards. Thus, Blake runs 3000-\n2720=280 yards farther than Kelly. ans = 280\nCoT2: Answer: Blake runs a total of 15 x 100 x 2 = 3000 yards. Kelly runs 34 x (40 x 2) = 2720\nyards. Blake runs 3000 - 2720 = 280 yards farther than Kelly. ans = 280.\nPoT1:\n# Python code, return ans\nlength_of_football_field = 100\nnum_of_laps_for_blake = 15\nnum_of_laps_for_kelly = 34\ndistance_covered_by_blake = length_of_football_field * \\\nnum_of_laps_for_blake * 2\ndistance_covered_by_kelly = (length_of_football_field *\\\n19\nPublished as a conference paper at ICLR 2024\nnum_of_laps_for_kelly * 2) + (40 * num_of_laps_for_kelly * 2)\nans = abs(distance_covered_by_blake - distance_covered_by_kelly)\n# ans = 6520.0(This line is not generated by LLM.)\nD.2.2\nCASE 2\nQuestion: Brinley is in Mr. Bert\u2019s math class. Mr. Bert gives six tests each semester. He allows the\nstudents to remove the lowest score from the average each semester. If Brinley has scores of 89, 71,\n92, 100 and 86 on the first five tests, what score does she need on the sixth test to get an average of\n93?\nGold Answer: First, identify the lowest score, which is 71, and remove it. Since Brinley wants a\nscore of 93, we know her total score needs to be 93 (average) * 5 (tests) = 465 points So far, Brinley\nhas earned 89 + 92 +100 + 86 = 367 points The final test must be 465 - 367 = 98 points.\nCoT1: First, we need to find Brinley\u2019s current average by adding up her five test scores and dividing\nby 5: (89 + 71 + 92 + 100 + 86) / 5 = 87.6 We know that Brinley wants her average to be 93, which\nmeans she needs to have a total of 6 test scores (since one will be dropped). The total of these\n6 scores needs to be 93 x 6 = 558 To find out what score Brinley needs on the sixth test, we can\nsubtract the total of her first five scores from the desired total: 558 - (89 + 71 + 92 + 100 + 86) =\n120 So Brinley needs to get a score of 120 on her sixth test to have an average of 93 after dropping\nher lowest score. ans = 120\nCoT2: Answer: Brinley\u2019s first five test scores add up to 438. To have an average of 93, Brinley\nneeds a total of 93 x 6 = 558 points for all six tests. To get 558 points after five tests, Brinley needs\nto score 558 - 438 = 120 points on the sixth test. ans = 120\nPoT1:\n# Python code, return ans\nnum_of_tests = 6\ntotal_of_first_five_tests_scores = 89 + 71 + 92 + 100 + 86\ndesired_average = 93\nlowest_score = min(89, 71, 92, 100, 86)\nsum_of_best_five_scores = total_of_first_five_tests_scores -\\\nlowest_score\nans = (desired_average * num_of_tests) - sum_of_best_five_scores\n# ans = 191.0(This line is not generated by LLM.)\nE\nIMPLEMENTATION DETAILS OF EXTERNAL VERIFIERS\nThroughout these experiments, our choice of the weaker LLM remains GPT-3.5-turbo. We only per-\nform our experiment with CoT representation because the finetuned model we utilize is not suitable\nfor understanding Python code.\nThe input to the verifier can be either the question alone or a concatenation of both the question and\nthe answer produced by the weaker LLM. The question texts include the context, e.g., the goal and\ncontext in CREPE. The answer texts contain both intermediate thoughts and the final answer (e.g.,\nan integer in mathematical reasoning). We have multiple generation results from the weaker LLM.\nTo avoid excessive input length, we only employ the majority-voted answer and its intermediate\nthoughts in the test time. Specifically, in the test time, we extract the majority-voted answer from two\nCoT prompts with different demonstrations as this setting shows better performance in our method,\nand then randomly select one collection of intermediate thoughts that result in this majority-voted\nanswer.\nWe still use task accuracy and relative cost as the metric. Note that only the question as input doesn\u2019t\nneed the answers from the weaker LLM necessarily. Therefore, the total cost C (Eq 1) for only the\nquestion as input becomes:\nC = Cd + (1 \u2212 1reject)Cw + 1reject Cs.\n(4)\nTo use an LLM as the verifier, we have designed two distinct prompts: one for \u201cquestion diffi-\nculty prediction\u201d, which solely relies on the questions (LLM-Q), and another for \u201canswer evalua-\n20\nPublished as a conference paper at ICLR 2024\ntion\u201d, which incorporates both questions and answers (LLM-QA). The difficulty prediction (LLM-Q)\nprompt entails the LLM\u2019s determination of whether a question necessitates redirection to the stronger\nLLM, based solely on the question\u2019s complexity. If the LLM deems a question challenging for dif-\nficulty prediction, it triggers forwarding to the stronger LLM. The answer evaluation (LLM-QA)\nprompt is to give the LLM both the question and the answer. We then let the LLM rethink the solv-\ning steps and generate feedback to evaluate whether the solution is correct. If not, the question will\nbe transferred to the stronger LLM. Both prompting methods are implemented with M = 8 shots of\ndemonstrations, and we keep the number of \u201ceasy\u201d and \u201chard\u201d questions or the number of \u201ccorrect\u201d\nand \u201cincorrect\u201d answers balanced. We set the sampling path as K = 20 and temperature as T = 0.4\nto sample the major answer and then use the self-consistency (SC) to decide the final \u201cdifficulty\u201d or\n\u201ccorrectness of the answers\u201d. The reproducible prompts are in the Appendix L. Notably, the cost\nfor decision Cd in Eq. 1 is no longer 0 with an external verifier. For the LLM as a verifier, we\nuse GPT-3.5-turbo as it\u2019s much cheaper than GPT-4. We count the number of tokens used for the\napproaches LLM-Q and LLM-QA.\nFor finetuning a smaller LLM (RoBERTa-base of Liu et al. (2020)) as the verifier, we similarly\ntested two variants, when the input contains only the question (Finetuned-Q) or when it includes\nboth the question and the answers (Finetuned-QA), in which case we use a separation token to\nsplit them. The training data for fine-tuning the models come from the original training split of\neach benchmark dataset. Since DATE does not have a training split, we only performed experiments\non GSM8k and CREPE. In each training instance, we have K = 20 generated responses and we\nlabel all of them. We label the generated responses where GPT-3.5-CoT-SC correctly predicts\nthe answer as positive. Notably, the number of total training cases is the number of sampling paths\nK = 20 multiplied by the number of questions. To address the data imbalance issue (i.e., most of\nthe training cases are positive), we perform simple under-sampling over the GSM8k. For CREPE,\nthe resulting training set is relatively smaller, so we did not perform the under-sampling.\nIn the training process, we fine-tune RoBERTa-base with the learning rate 1 \u00d7 10\u22125 and batch size\n32. We randomly split 10% of the training data to be the development set. The performance of the\nbest accuracy over the development set is shown in Table 9. One notable approach, predicting \u201call\npositive\u201d, is to assign a positive label to every example in the development set. We could learn that\nthe external verifier could achieve better performance than \u201call positive\u201d but not significant enough.\nDataset\nMethod\nDev\nGSM8k\nAll positive\n0.5\nBased on Question\n0.592\nBased on Question and Answer\n0.615\nCREPE\nAll positive\n0.715\nBased on Question\n0.749\nBased on Question and Answer\n0.812\nTable 9: Performance of finetuning RoBERTa as verifier over the dev set\nTable 10 shows the exact numerical results of those presented in Figure 6. We can learn that the\nexternal verifier could boost the performance but not that significant. Comparing LLM-Q with\nLLM-QA, an interesting observation is that the latter costs less than the former. This may seem\ncounter-intuitive at first glance because LLM-QA includes a much longer input (a concatenation of\nquestion and answer) than LLM-Q (only question). However, we found out that for both approaches,\ntheir final costs are mainly determined by the number of cases transferred to the stronger LLM. That\nis, when including both the question and the answer as input, LLM-QA tends to trust the weaker\nLLM more frequently, leading to fewer cases being routed to the stronger LLM and hence the lower\ncost. However, both approaches do not yield accuracy as high as our MoT-based approaches.\nF\nLLAMA2-13B AS THE WEAKER LLM\nAlthough GPT-3.5-turbo is considered to be weaker compared to GPT-4, it is still more powerful\nand expensive than most of the open-source LLMs. In this section, we tested the performance of\n21\nPublished as a conference paper at ICLR 2024\nDataset\nMethod\nAccuracy\nCost\nGSM8k\nGPT-3.5-turbo CoT-SC\n0.842\n0.111\nLLM-Q\n0.884\n0.253\nLLM-QA\n0.862\n0.135\nFinetuned-Q\n0.892\n0.340\nFinetuned-QA\n0.882\n0.262\nCoT-2D-Vote with threshold=0.7\n0.945\n0.408\nMoT-1D-Vote with threshold=0.5\n0.917\n0.235\nMoT-1D-Vote with threshold=0.7\n0.951\n0.425\nMoT-1D-Verify\n0.947\n0.401\nGPT-4 CoT-SC\n0.958\n1.000\nDATE\nGPT-3.5-turbo CoT-SC\n0.676\n0.107\nLLM-Q\n0.750\n0.367\nLLM-QA\n0.707\n0.142\nCoT-2D-Vote with threshold=0.65\n0.862\n0.489\nMoT-1D-Vote with threshold=0.65\n0.889\n0.469\nMoT-1D-Verify\n0.875\n0.449\nGPT-4 CoT-SC\n0.886\n1.000\nCREPE\nGPT-3.5-turbo CoT-SC\n0.820\n0.157\nLLM-Q\n0.864\n0.406\nLLM-QA\n0.846\n0.169\nFinetuned-Q\n0.854\n0.323\nFinetuned-QA\n0.847\n0.322\nCoT-2D-Vote with threshold=0.55\n0.865\n0.241\nMoT-1D-Vote with threshold=0.55\n0.885\n0.460\nMoT-1D-Verify\n0.882\n0.534\nGPT-4 CoT-SC\n0.871\n1.000\nTable 10: Exact numerical results of various external verifiers. Specifically, because DATE does not\nhave a training set, we do not report finetuned models\u2019 performance on it.\nFigure 7: The performance of using LLAMA2-13B as weaker LLM.\nLLAMA2-13B (Touvron et al., 2023), an open-source and easy-to-deploy LLM, as the weaker LLM\nand GPT-4 as the stronger LLM.\nThe result is shown in the Figure 7. Through experimentation, we observe that choosing LLAMA2-\n13B as a weaker LLM and applying our strategy on GSM8k and CREPE did not yield ideal results,\nbut it performed well on the DATE dataset. On GSM8k and CREPE datasets, accuracy and cost\nchange approximately linearly and we cannot achieve a comparable result with a lower cost, but on\nthe DATE dataset, we obtain a curve similar to the curve using GPT-3.5-turbo as a weaker LLM.\nTo analyze the cause, we have selected cases in MoT-1D-Vote whose agreement score s (Eq 2) is\ngreater than the 0.6, 0.7, and 0.8. We report their task accuracy and proportion among all cases. A\nlower accuracy in those cases is indicative of reduced overall task performance, while a diminished\nproportion implies an increased cost.\nThe results are displayed in Table 11. Comparing LLAMA2-13B to GPT-3.5-turbo, we observe\nthat LLAMA2-13B exhibits a significantly lower proportion. This suggests that LLAMA2-13B is\nless likely to provide consistent answers across multiple samplings. For example, only 16.3% of the\ncases have an agreement score greater than the 0.6 threshold on GSM8k in LLAMA2-13B but 77.3%\nin GPT-3.5-turbo. However, the difference in accuracy is slight in GSM8k and DATE. This outcome\naligns with our hypothesis that the LLM can consistently answer easy questions but is hesitant to the\nhard questions. The lower proportions show that the number of easy questions for LLAMA2-13B is\n22\nPublished as a conference paper at ICLR 2024\nfar fewer than that in GPT-3.5-turbo as LLAMA2-13B cannot solve these complex reasoning tasks\nwell. Therefore, LLAMA2-13B is not a suitable choice as a weaker LLM for particular tasks.\nThreshold\nDataset\nWeak LLM\nAccuracy\nProportion\n0.6\nGSM8k\nGPT-3.5-turbo\n0.964\n0.773\nLLAMA2-13B\n0.923\n0.163\nDATE\nGPT-3.5-turbo\n0.887\n0.721\nLLAMA2-13B\n0.824\n0.461\nCREPE\nGPT-3.5-turbo\n0.916\n0.722\nLLAMA2-13B\n0.639\n0.243\n0.7\nGSM8k\nGPT-3.5-turbo\n0.981\n0.707\nLLAMA2-13B\n0.946\n0.093\nDATE\nGPT-3.5-turbo\n0.893\n0.656\nLLAMA2-13B\n0.893\n0.328\nCREPE\nGPT-3.5-turbo\n0.934\n0.655\nLLAMA2-13B\n0.658\n0.095\n0.8\nGSM8k\nGPT-3.5-turbo\n0.985\n0.639\nLLAMA2-13B\n1.000\n0.070\nDATE\nGPT-3.5-turbo\n0.900\n0.593\nLLAMA2-13B\n0.933\n0.203\nCREPE\nGPT-3.5-turbo\n0.956\n0.588\nLLAMA2-13B\n0.600\n0.013\nTable 11:\nThe performance of cases where the percentage of the major answer under\nMoT-1D-Vote be greater than the threshold with LLAMA2-13B as the weaker LLM.\nG\nCAN THE STRONGER LLM BENEFIT FROM THE WEAKER LLM\u2019S HINTS?\nDataset\nMethod\nAccuracy\nGSM8k\nW/o Hints\n0.891\nW/ Hints\n0.867\nDATE\nW/o Hints\n0.892\nW/ Hints\n0.910\nCREPE\nW/o Hints\n0.774\nW/ Hints\n0.727\nTable 12:\nThe GPT-4-CoT-SC accuracy over the instances that Aw\u2032\n1\n\u0338=\nAw\u2032\n2\nin the\nMoT-1D-Verify approach\nWe conducted an extended experiment to investigate whether progressive hints can further enhance\nour pipeline. Previous works have proven that progressive hints in multiple iterations can enhance\nthe LLM\u2019s reasoning performance (Zheng et al., 2023). For example, if the answer is 13 in the first\niteration and 14 in the second one, the hint for the third iteration can be \u201cHint: The answers may\nbe close to 13, 14\u201d. In MoT-1D/2D-Verify, before the decision maker transfers a question to\na stronger LLM, we already have two different answers generated by the weaker LLM. However,\nthe answers from the weaker LLM will be discarded when using the stronger LLM. To incorporate\nthe answers in the leveraging process of the stronger LLM, we follow the prior work (Zheng et al.,\n2023) to provide these two inconsistent answers as hints. For example, if the main answer is 5 with\nCoT and 12 with PoT, the hint for the stronger LLM can be \u201cHint: The answers may be close to 5,\n12\u201d. We perform the exploration of whether using these answers as hints can improve the stronger\nLLM\u2019s performance. Prompts can be found in Appendix L. In the prompts, we acknowledge that\nthe hints may not always be correct. In our exploration, we choose the data that will be sent to the\nstronger LLM by the MoT-1D-Verify approach. We then assess the GPT-4-CoT-SC accuracy\nof these cases, both with and without the use of hints, respectively. We conduct our experiments on\nGSM8k, DATE, and CREPE datasets.\nThe results in Table 12 reveal that progressive hints may not necessarily further enhance our pipeline.\nAdding progressive hints only yields a slight improvement on the DATE dataset, with diminishing\nreturns on GSM8k and GREPE. The reason is that GPT-4 tends to be misled by the incorrect answer,\nespecially when both answers from GPT-3.5-turbo are incorrect.\n23\nPublished as a conference paper at ICLR 2024\naccuracy\ncost\nw/o Batch Prompting\n0.947\n0.401\nw Batch Prompting\n0.924\n0.325\nTable 13: The GSM8k results with and without batch prompting\nH\nCAN BATCHPROMPT FURTHER REDUCE THE COST?\nBatch Prompting is a method to reduce costs by inputting multiple questions into LLM at\nonce (Cheng et al., 2023a; Lin et al., 2023b). In this section, we explore whether our method can be\nused in conjunction with batch prompting. Our experiments are based on the basic batch prompt-\ning setup (Cheng et al., 2023a). We use MoT-1D-Verify for decision making. Specifically, we\ngrouped a batch of 4 test questions into each API call of the weaker LLM, in addition to the original\n8-shot demonstrations. The prompt is shown in Figure 39. Like in our previous experiments, we\nobtain multiple samples from running the weaker LLM, and the verification-based method (Eq 3)\ncan then be leveraged independently for each test question. For example, if two of four test instances\nare rejected by the decision maker, we only feed the rejected two to the stronger LLM. We perform\nthe experiment on the GSM8k dataset.\nWe show the result in Table 13. The results show that adding batch prompting can indeed reduce\ncosts more. However, using batch prompting slightly affects the accuracy of both the weaker LLM\nand the stronger LLM. The result indicates that our method is orthogonal to the batch prompting\napproach.\nI\nCALIBRATION ANALYSIS\nIn this section, we performed a calibration analysis comparing our approaches with baselines. For\nour voting-based method, we take n/K as the confidence score. For LLM as an external verifier, we\nfollowed the prior works\u2019 prompting method (Kadavath et al., 2022) to let the LLM make the deci-\nsion according to the textual contents.5 Following their setting, we consider two temperature setups,\ni.e., T = 1 and T = 2, and donate the variants as LLM-QA T=1 and LLM-QA T=2, respectively.\nThe prompts are shown in Table 31. We also note that, in the original method of Kadavath et al.\n(2022), the authors used the token probability of an LLM in predicting the answer to be \u201cTrue\u201d\nas its confidence score. Since we cannot obtain the token probability of GPT-3.5-turbo, the confi-\ndence score is obtained by sampling the True/False prediction K times and reporting the frequency\nof \u201cTrue\u201d (n/K). This will induce significantly more costs than our approach, but we adopted the\nstrategy here for this calibration analysis. We perform the calibration analysis in the GSM8k dataset.\nThe calibration results on GSM8k are illustrated in Figure 8(Left). Our major observations are: (1)\nAll decision-making methods yield a monotone calibration curve, implying that when they have\nhigher confidence in a certain answer, the answer is generally more likely to be true. For our three\nvote-based approaches, this upward trend also supports our initial hypothesis that a question is easy\nif the weaker LLM exhibits consistency in its responses across multiple sampling paths; (2) LLM-QA\nT=2 with a larger temperature can lead to the least calibrated decision maker; (3) However, there is\nno a significant difference among other approaches in terms of their calibration degree (though all\nour variants showed to be better than LLM-QA).\nWe wanted to note that achieving perfect calibration with n/K as the confidence score is not nec-\nessary for our task. To give an extreme example, let us consider the case that whenever an answer\nhas n/K > 0.5, the answer from the weaker LLM is correct (accuracy 1.0). In this case, we would\nhave a very poor calibration curve, but our pipeline would actually perform very well, because any\nmajority-voted answer it accepts in this case is perfectly true. Another factor to note is the \u201csize\u201d\nof each bin in the calibration plot (e.g., the number of instances that have confidence scores ranging\nbetween 0.6 and 0.7). While this factor will also dramatically impact the LLM cascade performance\nin practice, it cannot be shown in the calibration plot.\n5In our experiments in Section 3.5, the LLM-QA model is additionally prompted to explain its decision\nmaking, but we ignored this explanation following Kadavath et al. (2022) for the purpose of calibration analysis.\n24\nPublished as a conference paper at ICLR 2024\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nperfect\nCoT-1D-Vote: ECE = 0.035\nCoT-2D-Vote: ECE = 0.043\nMoT-1D-Vote: ECE = 0.060\nLLM-QA T=1: ECE = 0.078\nLLM-QA T=2: ECE = 0.179\n0.2\n0.4\n0.6\n0.8\nConfidence Score\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\nAccuracy\nCoT-1D-Vote\nCoT-2D-Vote\nMoT-1D-Vote\nLLM-QA T=1\nLLM-QA T=2\nFigure 8: The left figure is for the calibration analysis of different approaches. The number of each\nmethod is the expected calibration error (ECE). The right figure plots the accuracy of the subset of\nanswers satisfying n/K greater than the confidence score.\nTo give a more direct comparison among different decision-making approaches, we plot Fig-\nure 8(Right), where for each approach we report the accuracy of a subset of answers satisfying\nn/K greater than the confidence score, following the exact way how the approach will be applied\nin the LLM cascade pipeline. As such, curves on this plot have taken into account the \u201cbin sizes\u201d\nand can reflect their actual effectiveness. A similar analysis was also performed by Kadavath et al.\n(2022). We first observed that as the confidence score grows, there is a consistent upward trend in\nthe subset accuracy for all approaches, which echoes the monotone curves in the calibration analy-\nsis. The higher position of MoT compared with other variants and baselines also explains its superior\nperformance in the main experimental results.\nJ\nCAN OUR METHOD GENERALIZE TO FACTUALITY-BASED TASKS?\nIn prior studies, the efficacy of our approach in mathematical, symbolic, and causal reasoning has\nbeen evidenced. This section explores its potential to be generalized to factual reasoning tasks.\nOur investigation utilized StrategyQA (Geva et al., 2021), a factoid question answering dataset that\nrequires implicit reasoning over facts. We employed an open-domain question-answering format,\npresenting only questions without context to the LLM. Our few-shot prompts are shown in Figure 27\nand Figure 28.\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative Cost\n0.60\n0.65\n0.70\n0.75\n0.80\nAccuracy\nGPT-3.5-CoT-SC\nGPT-3.5-PoT-SC\nGPT-4-CoT-SC\nMoT-1D-Vote\nFigure 9: Performance of MoT-1D-Vote and baselines on the dev set of strategyQA.\nThe result is shown in Figure 9. It shows that our method is still effective on factual reasoning tasks.\nThat is again owing to the fact that using different prompt representations could trigger different\nreasoning paths, which often results in more trustworthy answers when the two representations\nagree with each other.\n25\nPublished as a conference paper at ICLR 2024\nIs a curling iron\nnecessary in curling?\nQuestion\nA curling iron is a tool used to make the hair curly using\nheat. The sport of curling requires curling brooms, stones\n(rocks), and curling shoes. A curling iron is not necessary\nin curling.\nGolden\nYes, a curling iron is necessary for\ncurling as it is the tool used to create\ncurls in the hair.\nCoT\nnecessary_equipment_for_curling = [\"curling stone\",\n\"curling broom\"]\n\u00a0equipment_used_in_curling_with_curling_iron = [\"curling\niron\"]\nans = (\"curling iron\" in necessary_equipment_for_curling)\nPoT\nFigure 10: An example showing that MoT helps in factual reasoning tasks such as strategyQA.\nAn example is shown in Figure 10. For the question \u201cIs a curling iron necessary in curling?\u201d, the\ngolden answer is \u201cNo, curling is an ice sport and doesn\u2019t need a curling iron\u201d. However, most of\nthe CoT answers are \u201cyes\u201d with hallucinations about the concept \u201ccurling\u201d. In contrast, most of\nthe PoT answers are \u201cNo\u201d. The PoT processes typically list the necessary equipment for curling,\nsuch as \u201ccurling stone\u201d and \u201cbroom\u201d, and then check if \u201ccurling iron\u201d is on the list. By checking\nthe consistency between CoT and PoT, MoT-1D-Vote is thus able to identify the incorrect or\nuntrustworthy answer. It is worth noting that, judging from the results of Figure 9, PoT is not better\nthan CoT, but the combination of CoT and PoT generates diverse thoughts and answers, instead of\nleaning towards one kind of thinking, thus reducing errors in factual reasoning. Therefore, we can\nstill leverage consistency checking across MoT prompts in decision-making to check if the answer\nfrom the weaker LLM is trustworthy in factual-based reasoning tasks.\nK\nLIMITATIONS AND FUTURE WORKS\nOur approach is still subject to some limitations. Firstly, its applicability is confined to tasks where\nquestions have clear answers, such as mathematical reasoning. Secondly, the utilization of our\npipeline might lead to increased latency in cases demanding a stronger LLM. Thirdly, our method is\nbased on the assumption that the intermediate steps could be expressed via different representations.\nIn addition, if weaker LLM has overconfidence in some incorrect beliefs, our method will fail in\nthese cases.\nConsidering these limitations, we identify some potential avenues for future research. In tackling\nthe initial challenge, we could integrate other metrics, such as semantic similarity, to evaluate the\nconsistency of the general textual generation tasks. To address the latency issue, it is essential to\nestablish a framework for making decisions early for some obviously easy or hard questions. For\nthe third one, we believe that how to represent the intermediate steps in a specific task relies on hu-\nman designation. Recent research has shown some predefined programming interfaces effectively\nmanage complex tasks, even those that appear unrelated to traditional code generation tasks, such\nas image understanding (Gupta & Kembhavi, 2023). If a task\u2019s intermediate steps cannot be ex-\npressed directly via code, we can also introduce diverse answers by leveraging some programming\ninterfaces/tools.\nAdditionally, due to CoT and PoT being suitable for different tasks, the answer\u2019s correctness with\ndifferent representations varies across tasks, e.g., the CoT answers are more reliable in GSM8k but\nthe PoT answers are more reliable in DATE. The incorporation of learning algorithms to perform\nweighted voting for specific tasks yields a promising improvement.\nL\nFULL PROMPTS\nWe show the prompts used for this paper according to Table 14. We show one example prompt in\neach case.\n26\nPublished as a conference paper at ICLR 2024\nPrompt Type\nDataset\nPrompt Table\nCoT\nGSM8k\nprompt 15\nASDIV\nprompt 17\nTabMWP\nprompt 19\nDate\nprompt 21\nNavigate\nprompt 23\nCREPE\nprompt 25\nPoT\nGSM8k\nprompt 16\nASDIV\nprompt 18\nTabMWP\nprompt ??\nDate\nprompt 22\nNavigate\nprompt 24\nCREPE\nprompt 26\nLLM as External Verifier\nGSM8k w/ Question\nprompt 29\nGSM8k w/ Question and Answer\nprompt 30\nDATE w/ Question\nprompt 32\nDATE w/ Question and Answer\nprompt 33\nCREPE w/ Question\nprompt 34\nCREPE w/ Question and Answer\nprompt 35\nLearning from hints\nGSM8k\nprompt 36\nDate\nprompt 37\nCREPE\nprompt 38\nTable 14: The index of each prompt.\nGSM8k CoT\nComplete\nthe\ntext ,\ns t a r t\nwith\n\u2019 Answer \u2019\nand\nthe\nl a s t\nl i n e\ns t a r t s\nwith\n,\u2192 \u2019 ans =\n\u2019 .\nQuestion : Manny had 3\nb i r t h d a y\ncookie\np i e s\nto\nshare\nwith\nh i s\n24\n,\u2192 c l a s s m a t e s\nand\nh i s\nteacher , Mr .\nKeith .\nI f\neach\nof\nthe\ncookie\np i e s\n,\u2192 were\ncut\ni n t o\n10\ns l i c e s\nand Manny ,\nh i s\nclassmates ,\nand Mr .\nKeith\n,\u2192\na l l\nhad 1 piece , how many\ns l i c e s\nare\nl e f t ?\nAnswer :\nThere\ni s\na\nt o t a l\nof 3 x 10 = <<3*10=30>>30 cookie\ns l i c e s .\nThere\n,\u2192 are\n24 + 1 + 1 = <<24+1+1=26>>26 people who a t e\nthe\ncookie\np i e c e s .\n,\u2192 There\ni s\n30 \u2212 26 = <<30\u221226=4>>4 cookie\ns l i c e s\nl e f t .\nans = 4\n. . .\nTable 15: GSM8k CoT task demonstrations\n27\nPublished as a conference paper at ICLR 2024\nGSM8k PoT\n# Question : Manny had 3\nb i r t h d a y\ncookie\np i e s\nto\nshare\nwith\nh i s\n24\n,\u2192 c l a s s m a t e s\nand\nh i s\nteacher , Mr .\nKeith .\nI f\neach\nof\nthe\ncookie\np i e s\n,\u2192 were\ncut\ni n t o\n10\ns l i c e s\nand Manny ,\nh i s\nclassmates ,\nand Mr .\nKeith\n,\u2192\na l l\nhad 1 piece , how many\ns l i c e s\nare\nl e f t ?\n# Python code ,\nr e t u r n\nans\nnum cookie pies = 3\nn u m s l i c e s p e r c o o k i e p i e = 10\nt o t a l p e o p l e\n= 24 + 1 + 1\nt o t a l s l i c e s n e e d e d\n=\nt o t a l p e o p l e\n* 1\nt o t a l s l i c e s\n= num cookie pies *\nn u m s l i c e s p e r c o o k i e p i e\nans =\nt o t a l s l i c e s\n\u2212\nt o t a l s l i c e s n e e d e d\n. . .\nTable 16: GSM8k PoT task demonstrations\nASDIV CoT\nPl ease\nanswer\nthe\nmath\nq u e s t i o n\nand\nr e t u r n\na number\nas\nthe\nanswer .\nQuestion :\nI s a b e l l a \u2019 s\nh a i r\ni s\n18\ni nch es\nlong . By the\nend\nof\nthe\nyear\nher\n,\u2192 h a i r\ni s\n24\ninches\nlong . How much\nh a i r\ndid\nshe grow?\nAnswer :\nI s a b e l l a \u2019 s\nh a i r\ni s\n18\ni nch es\nlong\nand 24\ninch es\nlong .\nSo the\nl e n g t h\nshe grow\nin\none\nyear\ni s\n24 \u2212 18 = 6\nans = 6\n. . .\nTable 17: ASDIV CoT task demonstrations\nASDIV PoT\n# Question :\nI s a b e l l a \u2019 s\nh a i r\ni s\n18\ni nch es\nlong . By the\nend\nof\nthe\nyear\n,\u2192 her\nh a i r\ni s\n24\ninches\nlong . How much\nh a i r\ndid\nshe grow?\n# Python code ,\nr e t u r n\nans\nI s a b e l l a h a i r b e f o r e\n= 18\nI s a b e l l a h a i r a f t e r y e a r\n= 24\nh a i r g r o w t h =\nI s a b e l l a h a i r a f t e r y e a r\n\u2212\nI s a b e l l a h a i r b e f o r e\nans = h a i r g r o w t h\n. . .\nTable 18: ASDIV PoT task demonstrations\n28\nPublished as a conference paper at ICLR 2024\nTabMWP CoT\nRead\nthe\nf o l l o w i n g\nt a b l e\nr e g a r d i n g\n\u201d Coins \u201d and\nthen\nanswer a\nq u e s t i o n :\nName |\nNumber of\ncoins\nBraden\n|\n76\nCamilla\n|\n94\nRick\n|\n86\nMary\n|\n84\nHector\n|\n80\nDevin\n|\n83\nEmily\n|\n82\nAvery\n|\n87\nQuestion : Some\nf r i e n d s\nd i s c u s s e d\nthe\ns i z e s\nof\nt h e i r\ncoin\nc o l l e c t i o n s .\n,\u2192 What\ni s\nthe mean of\nthe\nnumbers ?\nExplain :\nLet \u2019 s\nt h i n k\ns t e p\nby\ns t e p .\nThe numbers\nof\ncoins\nof\neach\none\nare\nin\n[76 ,\n94 ,\n86 ,\n84 ,\n80 ,\n83 ,\n82 ,\n8 7 ] .\nSo the mean of\nthe\nnumbers\ni s\n(76+94+86+84+80+83+82+87) /8 = 648/8 = 81\nAnswer :\n81\n. . .\nTable 19: TabMWP CoT task demonstrations\nStrategyQA PoT\nQuestion :\nAre more\npeople\ntoday\nr e l a t e d\nto\nGenghis Khan than\nJ u l i u s\n,\u2192 Caesar ?\n# Python code ,\nr e t u r n\nans\nc h i l d r e n n u m o f J u l i u s C a e s a r = 3\nchildren num of Genghis Khan = 6\nans = ( children num of Genghis Khan > c h i l d r e n n u m o f J u l i u s C a e s a r )\nTable 20: StrategyQA PoT task demonstrations\nDATE CoT\nQ:\nToday\ni s\nChristmas Eve of\n1937. What\ni s\nthe\ndate\ntomorrow\nin\n,\u2192 MM/DD/YYYY?\nExplain :\nToday\ni s\nthe\nChristmas Eve of\n1937 ,\nso\ntoday\ni s\n12 /2 4/ 19 37 .\nToday\ni s\n12/24/1937 ,\nthe\ndate\ntomorrow\ni s\n12 /2 5/ 19 37 .\nA:\n12/25/1937\n. . .\nTable 21: DATE CoT task demonstrations\n29\nPublished as a conference paper at ICLR 2024\nDATE PoT\n# Write\nPython Code to\nsolve\nthe\nf o l l o w i n g\nq u e s t i o n s .\nfrom\nda t et i me\nimport\ndate ,\nt i m e d e l t a\nfrom\nd a t e u t i l . r e l a t i v e d e l t a\nimport\nr e l a t i v e d e l t a\n# Q:\nToday\ni s\nChristmas Eve of\n1937. What\ni s\nthe\ndate\ntomorrow\nin\n,\u2192 MM/DD/YYYY?\n# today\ni s\nChristmas Eve of\n1937 ,\nthen\ntoday\ni s\n12/24/1937\ntoday = date (1937 ,\n12 ,\n24)\n# tomorrow\ndate tomorrow = today +\nr e l a t i v e d e l t a ( days =1)\n# The answer\nf o r m a t t e d\nwith %m/%d/%Y i s\nans = date tomorrow . s t r f t i m e (\u2019%m/%d/%Y\u2019 )\n. . .\nTable 22: DATE PoT task demonstrations\nNavigate CoT\nFollowing\nt h e s e\ni n s t r u c t i o n s ,\ni f\nwe r e t u r n\nto\nthe\ns t a r t i n g\npoint ,\nr e t u r n\n,\u2192 \u2019 yes \u2019 ;\ne l s e\nr e t u r n\n\u2019no \u2019 .\nI n s t r u c t i o n :\nTake 1\ns t e p .\nTake 2\ns t e p s .\nTake 3\ns t e p s .\nTurn\naround .\nTake\n,\u2192 6\ns t e p s .\nTurn\nl e f t .\nExplain :\ns t a r t p o s i t i o n\n= [0 ,\n0] ,\nassume\nthe\ns t a r t\nface\ni s\nto x\u2212 p o s i t i v e\n1 .\nTake 1\ns t e p .\nThe\nc u r r e n t\np o s i t i o n\ni s\n[1 ,\n0]\n2 .\nTake 2\ns t e p s .\nThe\nc u r r e n t\np o s i t i o n\ni s\n[3 ,\n0]\n3 .\nTake 3\ns t e p s .\nThe\nc u r r e n t\np o s i t i o n\ni s\n[6 ,\n0]\n4 .\nTurn\naround .\nThe\nface\ni s\nto x\u2212 n e g a t i v e\nin\nthe\nf o l l o w i n g\ns t e p s .\n5 .\nTake 6\ns t e p s .\nThe\nc u r r e n t\np o s i t i o n\ni s\n[0 ,\n0]\n6 .\nTurn\nl e f t .\nThe\nface\ni s\ny\u2212 n e g a t i v e\nin\nthe\nf o l l o w i n g\ns t e p s .\nAfter\na l l\nthe\nsteps ,\nthe\np o s i t i o n\ni s\n[0 ,\n0] ,\nthe\nsame as\nthe\ns t a r t i n g\n,\u2192 p o i n t .\nAnswer :\nyes\n. . .\nTable 23: Navigate CoT task demonstrations\n30\nPublished as a conference paper at ICLR 2024\nNavigate PoT\nFollowing\nt h e s e\ni n s t r u c t i o n s ,\ni f\nwe r e t u r n\nto\nthe\ns t a r t i n g\npoint ,\nr e t u r n\n,\u2192 \u2019 yes \u2019 ;\ne l s e\nr e t u r n\n\u2019no \u2019 .\ndef\nl e f t r o t a t e ( f a c e d i r e c t ) :\nf a c e d i r e c t\n= t u p l e ( f a c e d i r e c t )\nmapping dict = {(1 ,\n0) :\n(0 ,\n1) ,\n(0 ,\n1) :\n( \u22121 ,\n0) ,\n( \u22121 ,\n0) :\n(0 ,\n\u22121) ,\n,\u2192 (0 ,\n\u22121) :\n(1 ,\n0) }\nr e t u r n\nl i s t ( mapping dict [ f a c e d i r e c t ] )\ndef\nr i g h t r o t a t e ( f a c e d i r e c t ) :\nf a c e d i r e c t\n= t u p l e ( f a c e d i r e c t )\nmapping dict = {(1 ,\n0) :\n(0 ,\n\u22121) ,\n(0 ,\n1) :\n(1 ,\n0) ,\n( \u22121 ,\n0) :\n(0 ,\n1) ,\n,\u2192 (0 ,\n\u22121) :\n( \u22121 ,\n0) }\nr e t u r n\nl i s t ( mapping dict [ f a c e d i r e c t ] )\ndef\na r o u n d r o t a t e ( f a c e d i r e c t ) :\nf a c e d i r e c t\n= t u p l e ( f a c e d i r e c t )\nmapping dict = {(1 ,\n0) :\n( \u22121 ,\n0) ,\n(0 ,\n1) :\n(0 ,\n\u22121) ,\n( \u22121 ,\n0) :\n(1 ,\n0) ,\n,\u2192 (0 ,\n\u22121) :\n(0 ,\n1) }\nr e t u r n\nl i s t ( mapping dict [ f a c e d i r e c t ] )\ndef\nmove steps ( c u r r e n t p o s i t i o n ,\nf a c e d i r e c t ,\nstep ,\ns t e p d i r e c t = \u2019 \u2019) :\nn e w l i s t = [ ]\ni f\ns t e p d i r e c t\n==\n\u2019 l e f t \u2019 :\nf a c e d i r e c t\n=\nl e f t r o t a t e ( f a c e d i r e c t )\ne l i f\ns t e p d i r e c t\n==\n\u2019 r i g h t \u2019 :\nf a c e d i r e c t\n=\nr i g h t r o t a t e ( f a c e d i r e c t )\ne l i f\ns t e p d i r e c t\n==\n\u2019 backward \u2019 :\nf a c e d i r e c t\n= a r o u n d r o t a t e ( f a c e d i r e c t )\nf o r\ni\nin\nrange ( len ( c u r r e n t p o s i t i o n ) ) :\nn e w l i s t . append ( c u r r e n t p o s i t i o n [ i ] +\nf a c e d i r e c t [ i ] *\ns t e p )\nr e t u r n\nn e w l i s t\n#\nI n s t r u c t i o n :\nTake 1\ns t e p .\nTake 2\ns t e p s .\nTake 3\ns t e p s .\nTurn\naround .\n,\u2192 Take 6\ns t e p s .\nTurn\nl e f t .\n# Python code ,\nr e t u r n\nans\ns t a r t p o s i t i o n\n= [0 ,\n0]\nc u r r e n t p o s i t i o n\n=\ns t a r t p o s i t i o n\n# assume\nthe\ns t a r t\nface\ni s\nto x\u2212 p o s i t i v e\nf a c e d i r e c t\n= [1 ,\n0]\n# Take 1\ns t e p .\nc u r r e n t p o s i t i o n\n= move steps ( c u r r e n t p o s i t i o n ,\nf a c e d i r e c t ,\n1)\n# Take 2\ns t e p s .\nc u r r e n t p o s i t i o n\n= move steps ( c u r r e n t p o s i t i o n ,\nf a c e d i r e c t ,\n2)\n# Take 3\ns t e p s .\nc u r r e n t p o s i t i o n\n= move steps ( c u r r e n t p o s i t i o n ,\nf a c e d i r e c t ,\n3)\n# Turn\naround .\nf a c e d i r e c t\n= a r o u n d r o t a t e ( f a c e d i r e c t )\n# Take 6\ns t e p s .\nc u r r e n t p o s i t i o n\n= move steps ( c u r r e n t p o s i t i o n ,\nf a c e d i r e c t ,\n6)\n# Turn\nl e f t . Now the\nface\ni s\nto y\u2212 p o s i t i v e\nf a c e d i r e c t\n=\nl e f t r o t a t e ( f a c e d i r e c t )\nans = \u2019 yes \u2019\ni f\nc u r r e n t p o s i t i o n\n==\ns t a r t p o s i t i o n\ne l s e\n\u2019no \u2019\n. . .\nTable 24: Navigate PoT task demonstrations\n31\nPublished as a conference paper at ICLR 2024\nCREPE CoT\nAnswer\nthe\nq u e s t i o n .\nThe\nf i n a l\nanswers\nare\n\u201dmore\nl i k e l y \u201d ,\n\u201d e q u a l l y\n,\u2192 l i k e l y \u201d and \u201d l e s s\nl i k e l y \u201d .\nGoal : Wash sn e a k er s\nC u r r e n t C o n t e x t :\nBrush\no f f\nd i r t\nfrom\nthe\ns u r f a c e\nof\nthe\ns n e a k e r s .\nRemove\n,\u2192 s h o e l a c e s .\nRinse\nthe\ns h o e l a c e s\nin\nsoapy\nwater\nand\na i r\ndry .\nApply\n,\u2192 mild\nd e t e r g e n t\nto\nthe\ns n e a k e r s\nand\nrub\ng e n t l y .\nQuestion :\nWhat \u2019 s\nthe\nl i k e l i h o o d\nt h a t\nThe\ns n e a k e r s\nare damp?\nExplain :\nFor\ns t e p\n1 ,\na f t e r\nbrushing\no f f\nd i r t ,\nthe\ns n e a k e r s\nare damp\ni s\n\u201d e q u a l l y\n,\u2192 l i k e l y \u201d\nbecause\nt h e r e\ni s\nno change\nf o r\nthe\ndampness\nof\nthe\nshoes .\nFor\ns t e p\n2 ,\na f t e r\nremoving\nshoelaces ,\nthe\ns n e a k e r s\nare damp\ni s\n\u201d e q u a l l y\n,\u2192 l i k e l y \u201d\nbecause\nt h e r e\ni s\nno change\nf o r\nthe\ndampness\nof\nthe\nshoes .\nFor\ns t e p\n3 ,\na f t e r\nr i n s i n g\nthe\ns h o e l a c e s\nin\nsoapy\nwater\nand\na i r\ndry ,\nthe\n,\u2192 s n ea k e r s\nare damp\ni s\n\u201d e q u a l l y\nl i k e l y \u201d\nbecause we removed\nthe\n,\u2192 s h oe l a ce\nfrom\nthe\nsneaker\nso we don \u2019 t\nchange\nthe\ndampness\nof\n,\u2192 sneaker .\nFor\ns t e p\n4 ,\na f t e r\napplying\nmild\nd e t e r g e n t\nto\nthe\ns n e a k e r s\nand\nrubbing\n,\u2192 gently ,\nthe\ns n e ak e r s\nare damp\ni s\n\u201dmore\nl i k e l y \u201d\nbecause we put\nmild\n,\u2192 d e t e r g e n t\nto\nthe\ns n e a k e r s .\nTherefore ,\nin\nthe\nf i n a l\nstep ,\nthe\nevent\ni s\n\u201dmore\nl i k e l y \u201d .\nAnswer :\nmore\nl i k e l y\n. . .\nTable 25: CREPE CoT task demonstrations\n32\nPublished as a conference paper at ICLR 2024\nCREPE PoT\n# Answer\nthe\nq u e s t i o n .\nThe answers\nare\n\u201dmore\nl i k e l y \u201d ,\n\u201d e q u a l l y\nl i k e l y \u201d\n,\u2192 and \u201d l e s s\nl i k e l y \u201d .\n# Goal : Wash s ne a k e rs\n#\nC u r r e n t C o n t e x t :\nBrush\no f f\nd i r t\nfrom\nthe\ns u r f a c e\nof\nthe\ns n e a k e r s .\n,\u2192 Remove s h o e l a c e s .\nRinse\nthe\ns h o e l a c e s\nin\nsoapy\nwater\nand\na i r\ndry .\n,\u2192 Apply mild\nd e t e r g e n t\nto\nthe\ns n e a k e r s\nand\nrub\ng e n t l y .\n# Question :\nWhat \u2019 s\nthe\nl i k e l i h o o d\nt h a t\nThe\ns n e a k e r s\nare damp?\n# Python\ncode\nc l a s s\nWash Sneakers ( ) :\n#\nI n i t\nfrom\nC u r r e n t C o n t e x t\n# Brush\no f f\nd i r t\nfrom\nthe\ns u r f a c e\nof\nthe\ns n e a k e r s\n# Remove s h o e l a c e s\n# Rinse\nthe\ns h o e l a c e s\nin\nsoapy\nwater\nand\na i r\ndry\n# Apply mild\nd e t e r g e n t\nto\nthe\ns n e a k e r s\nand\nrub\ng e n t l y .\ndef\ni n i t\n( s e l f ) :\ns e l f . event0 = None # event0\ni s\nthe\nl i k e l i h o o d\nt h a t\nThe\ns n e a k e r s\n,\u2192 are damp .\ndef\nb r u s h o f f d i r t ( s e l f ) :\n# After\nbrushing\no f f\nd i r t ,\nevent0\nbecomes \u201d e q u a l l y\nl i k e l y \u201d\ns e l f . event0 = \u201d e q u a l l y\nl i k e l y \u201d\ndef\nremove shoelaces ( s e l f ) :\n# After\nremoving\nshoelaces ,\nevent0\nbecomes \u201d e q u a l l y\nl i k e l y \u201d\ns e l f . event0 = \u201d e q u a l l y\nl i k e l y \u201d\ndef\nr i n s e s h o e l a c e i n w a t e r ( s e l f ) :\n# After\nr i n s i n g\nin\nwater ,\nevent0\nbecomes \u201d e q u a l l y\nl i k e l y \u201d\ns e l f . event0 = \u201d e q u a l l y\nl i k e l y \u201d\ndef\na p p l y d e t e r g e n t s n e a k e r ( s e l f ) :\n# After\nr i n s i n g\nin\nwater ,\nevent0\nbecomes \u201dmore\nl i k e l y \u201d\ns e l f . event0 = \u201dmore\nl i k e l y \u201d\ndef\nc a l l a l l f u n c s i n o r d e r ( s e l f ) :\ns e l f . b r u s h o f f d i r t ( )\ns e l f . remove shoelaces ( )\ns e l f . r i n s e s h o e l a c e i n w a t e r ( )\ns e l f . a p p l y d e t e r g e n t s n e a k e r ( )\nc = Wash Sneakers ( )\nc . c a l l a l l f u n c s i n o r d e r ( )\nans = c . event0\n. . .\nTable 26: CREPE PoT task demonstrations\nStrategyQA CoT\nQuestion :\nAre more\npeople\ntoday\nr e l a t e d\nto\nGenghis Khan than\nJ u l i u s\n,\u2192 Caesar ?\nExplain :\nJ u l i u s\nCaesar\nhad\nt h r e e\nc h i l d r e n .\nGenghis Khan had\ns i x t e e n\nc h i l d r e n .\nThe\nc h i l d r e n\nnumber\nof\nGenghis Khan\ni s\nl a r g e r\nthan\nJ u l i u s\nCaesar .\nAnswer :\nYes\nTable 27: StrategyQA CoT task demonstrations\n33\nPublished as a conference paper at ICLR 2024\nStrategyQA PoT\nQuestion :\nAre more\npeople\ntoday\nr e l a t e d\nto\nGenghis Khan than\nJ u l i u s\n,\u2192 Caesar ?\n# Python code ,\nr e t u r n\nans\nc h i l d r e n n u m o f J u l i u s C a e s a r = 3\nchildren num of Genghis Khan = 6\nans = ( children num of Genghis Khan > c h i l d r e n n u m o f J u l i u s C a e s a r )\nTable 28: StrategyQA PoT task demonstrations\nGSM8k Question Difficulty Prediction\nP r e d i c t\nthe\nhardness\nl e v e l\nof\nthe\nq u e s t i o n s .\nQuestion :\nShawna \u2019 s\nworkout\ngoal\ni s\n30\ns i t u p s . On Monday ,\nShawna was only\n,\u2192 able\nto\ndo 12\ns i t u p s ,\nso\nshe\ndecided\nt h a t\nshe\nwould make up\nf o r\n,\u2192 the\nr e s t\non Tuesday .\nHowever ,\nshe was only\nable\nto\ndo 19\ns i t u p s\non\n,\u2192 Tuesday . How many\ns i t u p s\nwould Shawna have\nto\ndo on Wednesday\nto\n,\u2192 meet\nher minimum goal\nand make up\nf o r\nthe\nones\nshe\ndidn \u2019 t\ndo?\nLevel :\nHard\n. . .\nTable 29: GSM8k question difficulty prediction task demonstrations\nGSM8k Question and Answer Difficulty Prediction\nGenerate\nfeedback\nand\np r e d i c t\ni f\nthe\ng e n e r a t e d\nanswer\ni s\nt r u s t f u l .\nQuestion :\nShawna \u2019 s\nworkout\ngoal\ni s\n30\ns i t u p s . On Monday ,\nShawna was only\n,\u2192 able\nto\ndo 12\ns i t u p s ,\nso\nshe\ndecided\nt h a t\nshe\nwould make up\nf o r\n,\u2192 the\nr e s t\non Tuesday .\nHowever ,\nshe was only\nable\nto\ndo 19\ns i t u p s\non\n,\u2192 Tuesday . How many\ns i t u p s\nwould Shawna have\nto\ndo on Wednesday\nto\n,\u2192 meet\nher minimum goal\nand make up\nf o r\nthe\nones\nshe\ndidn \u2019 t\ndo?\nAnswer :\nShawna \u2019 s\ngoal\ni s\n30\ns i t u p s ,\nand\nshe\nhas\na l r e a d y\ndone 12 + 19 =\n,\u2192 <<12+19=31>>31 s i t u p s .\\ nTo meet\nher\ngoal ,\nshe\nneeds\nto\ndo 30 \u2212 31\n,\u2192 = <<30\u221231=\u22121>>\u22121 s i t u p s .\\ nSince\nshe can \u2019 t\ndo\nn e g a t i v e\ns i t u p s ,\nshe\n,\u2192 doesn \u2019 t\nneed\nto\ndo any more\ns i t u p s\nto\nmeet\nher\ngoal .\\ nans = 0\nFeedback :\nThe answer\ni s\ni n c o r r e c t .\nThe goal\ni s\n30\ns i t u p s\neveryday .\nSo\n,\u2192 the\nt o t a l\ni s\n3*30 = 90\nr a t h e r\nthan\n30.\nT r u s t f u l : No\n. . .\nTable 30: GSM8k question and answer difficulty prediction task demonstrations\n34\nPublished as a conference paper at ICLR 2024\nCalibration\nQuestion :\nShawna \u2019 s\nworkout\ngoal\ni s\n30\ns i t u p s . On Monday ,\nShawna was only\n,\u2192 able\nto\ndo 12\ns i t u p s ,\nso\nshe\ndecided\nt h a t\nshe\nwould make up\nf o r\n,\u2192 the\nr e s t\non Tuesday .\nHowever ,\nshe was only\nable\nto\ndo 19\ns i t u p s\non\n,\u2192 Tuesday . How many\ns i t u p s\nwould Shawna have\nto\ndo on Wednesday\nto\n,\u2192 meet\nher minimum goal\nand make up\nf o r\nthe\nones\nshe\ndidn \u2019 t\ndo?\nProposed Answer :\nShawna \u2019 s\ngoal\ni s\n30\ns i t u p s ,\nand\nshe\nhas\na l r e a d y\ndone 12\n,\u2192 + 19 = <<12+19=31>>31 s i t u p s .\\ nTo meet\nher\ngoal ,\nshe\nneeds\nto\ndo\n,\u2192 30 \u2212 31 = <<30\u221231=\u22121>>\u22121 s i t u p s .\\ nSince\nshe can \u2019 t\ndo\nn e g a t i v e\n,\u2192 s i t u p s ,\nshe\ndoesn \u2019 t\nneed\nto\ndo any more\ns i t u p s\nto\nmeet\nher\n,\u2192 goal .\\ nans = 0\nI s\nthe\nproposed\nanswer :\n(A)\nTrue\n(B)\nFalse\nThe proposed\nanswer\ni s : B\nTable 31: Calibration task demonstrations\nDATE Question Difficulty Prediction\nP r e d i c t\nthe\nhardness\nl e v e l\nof\nthe\nq u e s t i o n s .\nQuestion :\nToday\ni s\nChristmas Eve of\n1937. What\ni s\nthe\ndate\ntomorrow\nin\n,\u2192 MM/DD/YYYY?\nLevel :\nEasy\n. . .\nTable 32: DATE question difficulty prediction task demonstrations\nDATE Question and Answer Difficulty Prediction\nGenerate\nfeedback\nand\np r e d i c t\ni f\nthe\ng e n e r a t e d\nanswer\ni s\nt r u s t f u l .\nQuestion :\nToday\ni s\nChristmas Eve of\n1937. What\ni s\nthe\ndate\ntomorrow\nin\n,\u2192 MM/DD/YYYY?\nExplain :\nToday\ni s\nthe\nChristmas Eve of\n1937 ,\nso\ntoday\ni s\n12 /2 4/ 19 37 .\nToday\ni s\n12/24/1937 ,\nthe\ndate\ntomorrow\ni s\n12 /2 5/ 19 37 .\nA:\n12/25/1937\nFeedback :\nThe answer\ni s\nc o r r e c t .\nT r u s t f u l :\nYes\n. . .\nTable 33: DATE question and answer difficulty prediction task demonstrations\n35\nPublished as a conference paper at ICLR 2024\nCREPE Question Difficulty Prediction\nP r e d i c t\nthe\nhardness\nl e v e l\nof\nthe\nq u e s t i o n s .\nGoal :\nMaking Matcha Green Tea\nthe\nT r a d i t i o n a l Way\nC u r r e n t C o n t e x t :\nBoil\n3/4\ncup\n(180 ml )\nof\nwater\nand\npour\ni t\ni n t o\nyour\n,\u2192 matcha bowl .\nWhisk\nthe\nhot\nwater\nwith\na\nchasen .\nDiscard\nthe\nhot\n,\u2192 water\nfrom\nthe\nbowl .\nScoop 2\nt s p\n( 1 . 5\nheaping\nteaspoons\nor 10 g )\n,\u2192 of\nmatcha\ni n t o\na\nf i n e\nmesh\ns t r a i n e r .\nS i f t\nthe\nmatcha\ni n t o\nyour\n,\u2192 empty ,\ndry bowl .\nPour\nb o i l i n g\nwater\ni n t o\na\nteacup . Add a\nsmall\n,\u2192 amount\nof\nhot\nwater\ni n t o\nthe\nmatcha bowl and whisk\ni t .\nQuestion :\nWhat \u2019 s\nthe\nl i k e l i h o o d\nt h a t\nI\ndrink\nthe\nmatcha ?\nLevel :\nEasy\n. . .\nTable 34: CREPE question difficulty prediction task demonstrations\nCREPE Question and Answer Difficulty Prediction\nGenerate\nfeedback\nand\np r e d i c t\ni f\nthe\ng e n e r a t e d\nanswer\ni s\nt r u s t f u l .\nGoal :\nBoating\nin\nlake\nC u r r e n t C o n t e x t :\nRent a\nboat .\nFind a\nlake\nwhich\nallows\nb o a t i n g .\nDrive\nto\n,\u2192 the\nlake\nand\ns e t\nthe\nboat\nin\nlake .\nQuestion :\nWhat \u2019 s\nthe\nl i k e l i h o o d\nt h a t\nthe\nboat\ni s\nin\nlake ?\nExplain :\nFor\ns t e p\n1 ,\na f t e r\nr e n t i n g\na boat ,\nt h a t\nthe\nboat\ni s\nin\nthe\nlake\n,\u2192 i s\n\u201d l e s s\nl i k e l y \u201d\nbecause\nthe\nboat\ni s\nnot\nyet\nin\nthe\nlake .\nFor\ns t e p\n2 ,\na f t e r\nf i n d i n g\na\nlake\nwhich\nallows\nboating ,\nt h a t\nthe\nboat\ni s\n,\u2192 in\nthe\nlake\ni s\n\u201d e q u a l l y\nl i k e l y \u201d\nbecause we have\nfound a\nlake\nbut\n,\u2192 the\nboat\ni s\nnot\nyet\nin\nthe\nlake .\nFor\ns t e p\n3 ,\na f t e r\nd r i v i n g\nto\nthe\nlake\nand\ns e t t i n g\nthe\nboat\nin\nthe\nlake ,\n,\u2192 t h a t\nthe\nboat\ni s\nin\nthe\nlake\ni s\n\u201dmore\nl i k e l y \u201d\nbecause we have\ns e t\n,\u2192 the\nboat\nin\nthe\nlake .\nTherefore ,\nin\nthe\nf i n a l\nstep ,\nthe\nevent\ni s\n\u201dmore\nl i k e l y \u201d .\nFeedback :\nThe answer\ni s\nc o r r e c t .\nT r u s t f u l :\nYes\n. . .\nTable 35: CREPE question and answer difficulty prediction task demonstrations\nGSM8k Hints\nGiven\nthe\nh i n t s ( may not\nbe\nc o r r e c t ) ,\nanswer\nthe\nq u e s t i o n .\nQuestion :\nTara\nhas a shoebox\nt h a t\ni s\n4 i nch es\nt a l l\nand 6\ninch es\nwide .\n,\u2192 She\nputs\na\nsquare\nblock\ni n s i d e\nt h a t\ni s\n4 i nch es\nper\ns i d e . How many\n,\u2192 square\ninches\nof\nthe\nbox\nare\nl e f t\nuncovered ?\nHints :\nThe answer may be\nnear\nto\n24 or\n8 .\nAnswer :\nThe shoebox\ni s\n24\nsquare\ninch es\nbecause 4 x 6 = <<4*6=24>>24\nThe block\ni s\n16\nsquare\ninche s\nbecause 4 x 4 = <<4*4=16>>16\nThere\nare 8 square\ninches\nuncovered\nbecause 24 \u2212 16 = <<24\u221216=8>>8\nans = 8\n. . .\nTable 36: GSM8k with hints task demonstrations\n36\nPublished as a conference paper at ICLR 2024\nDATE Hints\nGiven\nthe\nh i n t s ( may not\nbe\nc o r r e c t ) ,\nanswer\nthe\nq u e s t i o n .\nQuestion :\nJane was born on\nthe\nl a s t\nday\nof\nFeburary\nin\n2000. Today\ni s\n,\u2192 her 16\u2212 year \u2212old\nb i r t h d a y .\nWhat\ni s\nthe\ndate\n24 hours\nl a t e r\nin\n,\u2192 MM/DD/YYYY?\nHints :\nThe answer may be\nnear\n02/29/2016\nor\n03 /0 1/ 20 16 .\nAnswer :\nJane was born on\nthe\nl a s t\nday\nof\nFeburary\nin\n2000 ,\nso\nshe\nborn\n,\u2192 on\n02/29/2000.\nToday\ni s\nher 16\u2212 year \u2212old\nb i r t h d a y .\nSo today\ni s\n02 /2 9/ 20 16 .\nThe\ndate\n24 hours\nl a t e r\nthan\ntoday\ni s\n0 3/ 01 /2 01 6.\nA:\n03/01/2016\n. . .\nTable 37: DATE with hints task demonstrations\nCREPE Hints\nGiven\nthe\nh i n t s ( which may not\nbe\nc o r r e c t ) ,\nanswer\nthe\nq u e s t i o n\nwith\n,\u2192 \u201dmore\nl i k e l y \u201d ,\n\u201d e q u a l l y\nl i k e l y \u201d\nor\n\u201d l e s s\nl i k e l y \u201d .\nGoal : Buy a\nf l i g h t\nt i c k e t\nfrom SF to\nHawaii .\nContext :\nOpen google\nf l i g h t s\nand\ncheck\nthe\nf l i g h t s\na v a i l a b i l i t y\nduring\n,\u2192 the\nd a t e s\nwanted .\nChoose non\u2212 stop\nf l i g h t s\nand\ncheck\ni f\nthe\ntime\n,\u2192 s l o t s\nwork .\nAfter\nd e c i d i n g\nr i g h t\ntime\ns l o t s\nand\nprice ,\nproceed\nto\n,\u2192 buy\nt i c k e t s .\nGot an\nemail\nc o n f i r m a t i o n\nof\nr e s e r v a t i o n .\nQuestion :\nJane was born on\nthe\nl a s t\nday\nof\nFeburary\nin\n2000. Today\ni s\n,\u2192 her 16\u2212 year \u2212old\nb i r t h d a y .\nWhat\ni s\nthe\ndate\n24 hours\nl a t e r\nin\n,\u2192 MM/DD/YYYY?\nHints :\nThe answer may be\nnear\n\u201dmore\nl i k e l y \u201d\nor\n\u201d l e s s\nl i k e l y \u201d .\nExplain :\nFor\ns t e p\n1 ,\na f t e r\nchecking\nf l i g h t s\na v a i l a b i l i t y ,\nt h a t\nI\ncan\n,\u2192 refund\nthe\nt i c k e t\ni f\ni t\ni s\nr e f u n d a b l e\ni s\n\u201d e q u a l l y\nl i k e l y \u201d\nbecause\n,\u2192 I\nhaven \u2019 t\npurchased\nthe\nt i c k e t\nyet .\nFor\ns t e p\n2 ,\na f t e r\nchoosing non\u2212 stop\nf l i g h t s\nand\nchecking\ni f\nthe\ntime\n,\u2192 s l o t s\nwork ,\nt h a t\nI\ncan\nrefund\nthe\nt i c k e t\ni f\ni t\ni s\nr e f u n d a b l e\ni s\n,\u2192 \u201d e q u a l l y\nl i k e l y \u201d\nbecause\nI\nhaven \u2019 t\npurchased\nthe\nt i c k e t\nyet .\nFor\ns t e p\n3 ,\na f t e r\nd ec i di n g\non\nthe\nr i g h t\ntime\ns l o t s\nand\np r i c e\nand\n,\u2192 proceeding\nto\nbuy\nt i c k e t s ,\nt h a t\nI\ncan\nrefund\nthe\nt i c k e t\ni f\ni t\ni s\n,\u2192 r e f u n d a b l e\ni s\n\u201d l e s s\nl i k e l y \u201d\nbecause\nI\nhave\na l r e a d y\npurchased\nthe\n,\u2192 t i c k e t .\nFor\ns t e p\n4 ,\na f t e r\ng e t t i n g\nan\nemail\nc o n f i r m a t i o n\nof\nr e s e r v a t i o n ,\nt h a t\nI\n,\u2192 can\nrefund\nthe\nt i c k e t\ni f\ni t\ni s\nr e f u n d a b l e\ni s\n\u201dmore\nl i k e l y \u201d\nbecause\n,\u2192 I\ncan\ncheck\nthe\nterms\nand\nc o n d i t i o n s\nof\nthe\nt i c k e t\nand\nsee\ni f\ni t\n,\u2192 i s\nr e f u n d a b l e\nor\nnot .\nTherefore ,\nin\nthe\nf i n a l\nstep ,\nthe\nevent\ni s\n\u201dmore\nl i k e l y \u201d .\nAnswer :\nmore\nl i k e l y\n. . .\nTable 38: CREPE with hints task demonstrations\n37\nPublished as a conference paper at ICLR 2024\nBatch Prompting\n{\nQuestion1 : A robe\nt a k e s\n2\nb o l t s\nof\nblue\nf i b e r\nand\nh a l f\nt h a t\nmuch white\n,\u2192 f i b e r .\nHow many\nb o l t s\nin\nt o t a l\ndoes\ni t\ntake ?\nQuestion2 :\nJosh\nd ecides\nto\nt r y\nf l i p p i n g\na house .\nHe buys a house\nf o r\n,\u2192 $80 ,000\nand\nthen\nputs\nin\n$50 ,000\nin\nr e p a i r s .\nThis\ni n c r e a s e d\nthe\n,\u2192 value\nof\nthe\nhouse by 150%.\nHow much\np r o f i t\ndid\nhe make?\nQuestion3 :\nEvery day ,\nWendi\nfeeds\neach\nof\nher\nchickens\nt h r e e\ncups\nof\n,\u2192 mixed\nchicken\nfeed ,\nc o n t a i n i n g\nseeds ,\nmealworms and\nv e g e t a b l e s\nto\n,\u2192 help\nkeep them\nh e a l t h y .\nShe\ngives\nthe\nchickens\nt h e i r\nfeed\nin\n,\u2192 t h r e e\ns e p a r a t e\nmeals .\nIn\nthe\nmorning ,\nshe\ngives\nher\nf l o c k\nof\n,\u2192 chickens\n15 cups\nof\nfeed .\nIn\nthe\nafternoon ,\nshe\ngives\nher\n,\u2192 chickens\nanother\n25 cups\nof\nfeed .\nHow many cups\nof\nfeed\ndoes\nshe\n,\u2192 need\nto\ngive\nher\nchickens\nin\nthe\nf i n a l\nmeal\nof\nthe\nday\ni f\nthe\ns i z e\n,\u2192 of Wendi \u2019 s\nf l o c k\ni s\n20\nchickens ?\nQuestion4 :\nKylar\nwent\nto\nthe\ns t o r e\nto\nbuy\ng l a s s e s\nf o r\nh i s new apartment .\n,\u2192 One\ng l a s s\nc o s t s\n$5 ,\nbut\nevery\nsecond\ng l a s s\nc o s t s\nonly 60% of\nthe\n,\u2192 p r i c e .\nKylar\nwants\nto\nbuy 16\ng l a s s e s . How much does he need\nto\npay\n,\u2192 f o r\nthem ?\nAnswer1 :\nI t\nt a k e s 2/2=<<2/2=1>>1 b o l t\nof\nwhite\nf i b e r \\nSo\nthe\nt o t a l\n,\u2192 amount\nof\nf a b r i c\ni s 2+1=<<2+1=3>>3 b o l t s\nof\nf a b r i c\nans = 3\nAnswer2 :\nThe\nc o s t\nof\nthe\nhouse\nand\nr e p a i r s\ncame out\nto\n,\u2192 80 ,000+50 ,000=$<<80000+50000=130000>>130,000\\nHe i n c r e a s e d\nthe\n,\u2192 value\nof\nthe\nhouse by 80 ,000*1.5= < <80000*1.5=120000 > >120 ,000\\nSo\n,\u2192 the new value\nof\nthe\nhouse\ni s\n,\u2192 120 ,000+80 ,000=$<<120000+80000=200000>>200,000\\nSo he made a\n,\u2192 p r o f i t\nof\n200 ,000 \u2212130 ,000=$<<200000\u2212130000=70000>>70,000\nans = 70000\nAnswer3 :\nI f\neach\nchicken\ne a t s\n3 cups\nof\nfeed\nper\nday ,\nthen\nf o r\n20\n,\u2192 chickens\nthey\nwould need 3*20=<<3*20=60>>60 cups\nof\nfeed\nper\n,\u2192 day .\\ n I f\nshe\nfeeds\nthe\nf l o c k\n15 cups\nof\nfeed\nin\nthe\nmorning ,\nand\n,\u2192 25 cups\nin\nthe\nafternoon ,\nthen\nthe\nf i n a l\nmeal would\nr e q u i r e\n,\u2192 60\u221215\u221225=<<60\u221215\u221225=20>>20 cups\nof\nchicken\nfeed .\nans = 20\nAnswer4 :\nThe\nd i s c o u n t\np r i c e\nof\none\ng l a s s\ni s\n60/100 * 5 =\n,\u2192 $<<60/100*5=3>>3.\\ n I f\nevery\nsecond\ng l a s s\ni s\ncheaper ,\nt h a t\nmeans\n,\u2192 Kylar\ni s\ngoing\nto\nbuy 16\n/\n2 = <<16/2=8>>8 cheaper\ng l a s s e s .\\ nSo\n,\u2192 f o r\nthe\ncheaper\ng l a s s e s ,\nKylar\ni s\ngoing\nto\npay 8 * 3 =\n,\u2192 $<<8*3=24>>24.\\nAnd f o r\nthe\nr e g u l a r \u2212 p r i c e d\ng l a s s e s ,\nKylar\nw i l l\npay\n,\u2192 8 * 5 = $<<8*5=40>>40.\\nSo\nin\nt o t a l\nKylar\nneeds\nto\npay 24 + 40 =\n,\u2192 $<<24+40=64>>64 f o r\nthe\ng l a s s e s\nhe wants\nto\nbuy .\nans = 64\n}\n. . .\nTable 39: Batch Prompting task demonstrations\n38\n"
  },
  {
    "title": "A Long Way to Go: Investigating Length Correlations in RLHF",
    "link": "https://arxiv.org/pdf/2310.03716.pdf",
    "upvote": "9",
    "text": "A LONG WAY TO GO:\nINVESTIGATING LENGTH CORRELATIONS IN RLHF\nPrasann Singhal\nThe University of Texas at Austin\nprasanns@cs.utexas.edu\nTanya Goyal\nPrinceton University\ntanyagoyal@princeton.edu\nJiacheng Xu\nSalesforce AI\njiacheng.xu@salesforce.com\nGreg Durrett\nThe University of Texas at Austin\ngdurrett@cs.utexas.edu\nABSTRACT\nGreat successes have been reported using Reinforcement Learning from Hu-\nman Feedback (RLHF) to align large language models. Open-source preference\ndatasets and reward models have enabled wider experimentation beyond generic\nchat settings, particularly to make systems more \u201chelpful\u201d for tasks like web ques-\ntion answering, summarization, and multi-turn dialogue. When optimizing for\nhelpfulness, RLHF has been consistently observed to drive models to produce\nlonger outputs. This paper demonstrates that optimizing for response length is a\nsignificant factor behind RLHF\u2019s reported improvements in these settings. First,\nwe study the relationship between reward and length for reward models trained\non three open-source preference datasets for helpfulness. Here, length correlates\nstrongly with reward, and improvements in reward score are driven in large part by\nshifting the distribution over output lengths. We then explore interventions during\nboth RL and reward model learning to see if we can achieve the same downstream\nimprovements as RLHF without increasing length. While our interventions miti-\ngate length increases, they aren\u2019t uniformly effective across settings. Furthermore,\nwe find that RLHF with a reward based solely on length can reproduce most of the\ndownstream improvements over the initial supervised fine-tuned model, showing\nthat reward models in these settings have a long way to go.\nAdults typically do not roll off of the bed because they have developed the \nmuscle memory to keep their bodies from involuntarily moving during \nsleep and maintaining proper posture. \nAdults generally do not roll off of the bed because they have developed muscle \ncontrol and awareness which helps them to remain upright while sleeping.  \nAdditionally, most adults find it uncomfortable or even painful to move \naround in their sleep, so rolling off the bed would be difficult without waking \nup first. There may also be safety concerns such as falling out of bed and \npotentially hurting oneself if one were to lose balance or fall from the \u2026\nQuestion: Why don\u2019t adults roll off the bed? \nSFT (Before); 59 tokens\nRLHF (After); 243 tokens: Similar output, but much longer / more details\nFigure 1: Log-scaled heatmap of output length vs. RLHF reward model score for a set of outputs\ngenerated from an SFT LLaMA-7B model on WebGPT. Reward correlates strongly with length, and\nrunning PPO consistently leads to longer outputs (right); this paper analyzes these phenomena.\n1\nINTRODUCTION\nReinforcement Learning from Human Feedback (RLHF) is widely used to align large language\nmodels (LLMs) with desired downstream properties such as helpfulness or harmlessness (Ouyang\n1\narXiv:2310.03716v1  [cs.CL]  5 Oct 2023\net al., 2022; Bai et al., 2022). Specifically, starting from a policy model that is either a base LLM\nor an LLM that has undergone supervised fine-tuning, the standard paradigm generally consists of\n(1) training a reward model (RM) on a labeled set of preferences on pairs of outputs for the same\ninputs, and (2) optimizing the policy model with respect to the reward model via an RL algorithm\nlike proximal policy optimization (PPO) (Schulman et al., 2017).\nThis procedure relies on two things. First, the reward model must be correctly specified and not mis-\naligned with human preferences (Zhuang & Hadfield-Menell, 2021; Pang et al., 2022; Bobu et al.,\n2023). Second, the optimization algorithm must do a good job of balancing reward optimization\nwith staying close to the initial policy distribution. Not meeting these conditions generally leads to\nover-optimization of the reward model at the expense of human judgments (Dubois et al., 2023),\nwhich in the worst case leads to pathological \u201creward hacking\u201d (Skalse et al., 2022). Ad hoc adjust-\nments (Touvron et al., 2023b) and improvements in PPO (Zheng et al., 2023b) have stabilized the\nprocess and eliminated overt reward hacking in many LLM fine-tuning settings. However, it is not\nalways clear what changes in the policy model\u2019s behavior are responsible for reward improvements,\nand to what extent these correspond to meaningful improvements in quality versus optimization of\nspurious correlations in the reward function (Pang et al., 2022).\nGiven that the vast majority of recent work reports an increase in output length after RLHF for help-\nfulness, (Dubois et al., 2023; Zheng et al., 2023b; Sun et al., 2023; Wu et al., 2023; Nakano et al.,\n2021; Stiennon et al., 2020), this paper focuses on the question of length and asks whether this is a\ncorrelation being optimized for. Length does not necessarily represent a spurious correlation, as hu-\nman raters may legitimately prefer longer and more informative outputs. Nevertheless, we explore\nhow much of the optimization and improvement is purely based on length as opposed to other fea-\ntures. We find that length often constitutes a majority of the reward and downstream improvements\nof RLHF, indicating that length may play a much larger role than previously documented.\nWe organize our investigation into three parts: (1) We investigate whether PPO with standard reward\nmodels optimizes for length in three different helpfulness-oriented settings. At fixed output lengths,\nPPO only gives mild improvements in reward; in two settings, nearly all reward improvement comes\nfrom shifting the distribution over lengths produced by the policy. (2) We investigate preference data\nand reward models, aiming to understand the source of length correlation and whether this can be\nmitigated through a series of interventions. We find these biases to originate from data imbalances,\nas well as significant robustness issues in standard reward modeling. (3) We conduct an experiment\nwhere we measure how much doing PPO with a reward based only on length can reproduce PPO\nquality gains with trained reward models.\nWe postulate that further improvements to RLHF will require the disentanglement of length from\nboth optimization, and in particular, reward models: RLHF research still has a long way to go.\nOur Contributions: (1) We conduct a multi-faceted exploration of a prevalent correlation between\nlength and reward in RLHF. (2) We explore several interventions to study and mitigate length in-\ncreases, and characterize their performance across three datasets. (3) We plan to release a diverse\nset of reward and generation models to support future open work in RLHF 1.\n2\nTASK SETUP\nRLHF is technique for optimizing the performance of text generation systems (Sutskever et al., 2014;\nBahdanau et al., 2015), in which we place a distribution over target output y = (y1, . . . , yn) given\ninput sequences of words x via a generation model \u03c0\u03b8: p(y \u2223 x; \u03c0\u03b8) = \u220fn\nk=1 p(yk \u2223 y<k, x; \u03c0\u03b8).\nHistorically, these models were trained with both language modeling pre-training (learning to pre-\ndict the next word given context) and supervised fine-tuning (SFT; learning to generate outputs to\nmaximize the likelihood of references on some dataset, also referred to as behavioral cloning).\nRLHF is a technique introduced to further improve upon this approach, and can be broken into\nthree components. First, it requires a set of preference judgments over model outputs of the form\nP = {(x1, y+\n1 , y\u2212\n1 ), . . . , (xn, y+\nn, y\u2212\nn)} with triples of prompts xi, preferred continuations y+\ni , and\ndispreferred continuations , y\u2212\ni .\n1Code available at https://github.com/PrasannS/rlhf-length-biases\n2\nThen, given some P, the task is to train a scalar reward model R(q, x) such that for any given\npreference triple, R(xi, y+\ni ) > R(xi, y\u2212\ni ). We use the standard Bradley-Terry preference model\n(Bradley & Terry, 1952), where P(y1 \u227b y2 \u2223 x) =\nexp(R(x,y1))\nexp(R(x,y1))+exp(R(x,y2)) and the reward model\nis trained to optimize the log likelihood of the observed preferences.\nFinally, given R, we use reinforcement learning, specifically proximal policy optimization (Schul-\nman et al., 2017, PPO) to optimize a supervised fine-tuned (SFT) model \u03c0SFT\n\u03b8\nto get a model\n\u03c0RL\n\u03b8\n= PPO(\u03c0SFT\n\u03b8\n, R) that, for a query distribution X = (x1, . . . , xm), maximizes the reward\nR(xi, \u03c0\u03b8(xi)), with a constraint that we not deviate too strongly from the initial distribution. RL\noptimization in PPO is based on the maximization of the following equation:\nRfinal(x, y) = R(x, y) \u2212 \u03bbDKL(\u03c0\u2217\n\u03b8 (y\u2223x)\u2225\u03c0SFT\n\u03b8\n(y\u2223x))\n(1)\nwhere \u03bb controls the strength of a Kullback-Leibler (KL) divergence penalty between the original\npolicy \u03c0SFT\n\u03b8\nand the current policy \u03c0\u2217\n\u03b8 at a given step.\n2.1\nTASKS\nWe explore a collection of three preference datasets corresponding to three tasks (examples in Ap-\npendix C). We selected these datasets to provide a diversity of tasks oriented towards helpfulness\nthat are still challenging for our base model, LLaMA-7B (Touvron et al., 2023a). Conveniently, we\nalso have three types of preference supervision: explicit human labels, implicit preferences from\nupvotes, and synthetic preferences.2\nWebGPT (Question answering; human labels)\nThis dataset (Nakano et al., 2021) contains hu-\nman annotated preference labels between two outputs for the open-domain long-form question an-\nswering (LFQA) task (Fan et al., 2019). As human annotation is expensive, this dataset is relatively\nsmaller at only 19.6K examples (mean tokens per y = 169) compared to the others we study.\nStack (Technical question answering; upvotes)\nReleased by Hugging Face, this dataset collects\ntechnical questions and answers from StackExchange (Lambert et al., 2023). The preference label\nbetween two answers is derived using the number of upvotes; the one with more upvotes is assumed\nto be preferred. We use a subset of 100K (mean tokens per y = 236) pairs from the dataset following\nthe Hugging Face implementation (von Werra et al., 2020).\nRLCD (Multi-turn conversation; synthetic preferences)\nFinally, we explore multi-turn dia-\nlogue style data, released by Yang et al. (2023). Starting from the input instructions in the Help-\nful/Harmless dataset by Anthropic (Bai et al., 2022), they automatically generated preferred and\nnot-preferred outputs using prompt heuristics, e.g. appending \u201cgenerate unhelpful outputs\u201d to the\nprompt. The \u201chelpfulness\u201d subset that we use consists of 40K examples and mean tokens per y = 45.\n2.2\nEXPERIMENTAL SETUP\nFramework\nWe use the standard implementation and hyperparameters for the 3 components of\nRLHF to maintain consistency. We base our RLHF implementation on the Huggingface TRL frame-\nwork with hyperparameters we find to work best based on reward convergence and downstream eval-\nuation (\u03bb = 0.04, batch size 64, see more details in Appendix A) (von Werra et al., 2020), and use\nLoRA (rank=16) (Hu et al., 2021) to enable training large Llama-7B models (Touvron et al., 2023a)\nwith limited GPU memory. For our SFT models we use the released AlpacaFarm SFT model for\nWebGPT and RLCD as we find it to work well, and the TRL SFT model for Stack.\nEvaluation\nOur evaluation relies on two factors. First, reward is an intrinsic metric optimized\nby the PPO process. Second, we follow past work in AlpacaFarm (Dubois et al., 2023) to conduct\ndownstream evaluation using more powerful LLMs as proxies for human preferences. Specifically,\nwe sample responses on fixed held-out test sets of 500 prompts for each setting, then use their\n2Note: Our settings are oriented towards helpfulness, which we infer to be closer related to length, however\nstudying our approaches on other objectives such as harmlessness could be interesting future work.\n3\n0\n50\n100\n150\n200\n250\nLength\n0\n10\n20\n30\n40\n50\nCount\nW-GPT\nType\nSft\nStd\n50\n100\n150\n200\n250\nLength\n0\n20\n40\n60\n80\nCount\nSTACK\nType\nSft\nStd\n0\n50\n100\n150\n200\n250\nLength\n0\n5\n10\n15\n20\nCount\nRLCD\nType\nSft\nStd\nFigure 2: Histograms of output lengths before (blue) and after (red) PPO; means shown with dashed\nlines. Across settings, PPO leads to dramatic length increases.\n50\n100\n150\n200\nLength\n1.0\n0.5\n0.0\n0.5\n1.0\nReward\nW-GPT\n100\n150\n200\n250\nLength\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nReward\nStack\n0\n25\n50\n75\n100\n125\n150\nLength\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nReward\nRLCD\nFigure 3: Length-binned scatter plots with length (x-axis), vs reward (y-axis) for WebGPT, Stack,\nand RLCD. Arrows indicate improvement (up) or degradation (down) from SFT to PPO (HIGH KL)\nwithin a bin. Size and color intensity scaled by proportion of examples in bin. On WebGPT and\nRLCD, gains from increasing length are much larger than gains from RLHF within length buckets.\nexact evaluation scheme based on using a panel of 12 simulated OpenAI API based \u201cannotators,\u201d\nwhich they show correspond well with human preference judgements. The final format is an overall\npairwise \u201cwin rate\u201d of one set of paired outputs vs another, which we call simulated preferences.\n3\nEXAMINING PPO\nIn this section, we first show that: (1) Output length increases during PPO (Figure 2). (2) There\nexists a positive correlation between length and reward model scores (Figure 3). Taken together,\nthis evidence suggests that simply increasing length could be a successful way to improve reward.\nMotivated by this, we investigate the following question: Is length increase the primary factor for\nreward models scores increasing during PPO, or are other features also optimized?\n3.1\nLENGTH INCREASES DURING PPO\nTo contextualize the rest of the work, we first show that length actually does increase as a result of\nPPO. Indeed, when comparing histograms of generation lengths (see Figure 2) on a fixed query set\nbefore and after our initial PPO runs, we find that PPO causes notable length increases.\nWe now investigate the extent to which other features are learned, with two different settings of the\nKL weight \u03bb in the objective. Figure 3 shows reward scores stratified by length, binned into buckets\nof 20 tokens for the higher \u03bb variant (HIGH KL). While reward score does increase in each bin on\naverage, the increases in reward are uneven. Furthermore, the increases are less strong than the\nlength trends: generating an answer that\u2019s 40 tokens longer (shifted over by two bins) often provides\na larger improvement than PPO. (See Figure 10 for a plot with our standard, lower-KL PPO setting.)\nTo quantify this more precisely, we estimate the percentage of length-based optimization as the ratio\nof weighted reward gain (WRG) to the overall reward improvement (\u2206R) from PPO, where weighted\nreward gain is the sum of each bin\u2019s difference value multiplied by the total number of examples in\neach bin. Weights are computed by total examples from SFT and PPO combined.\n4\nTable 1: Weighted reward gain (WRG), reward impovement\n(\u2206R), and the ratio of the two for PPO with standard (STD)\nand high (HIGH KL) \u03bb. Low ratios on WGPT and RLCD in-\ndicate high PPO dependence on length.\nSTACK shows this\npattern to a weaker extent\nWGPT\nSTACK\nRLCD\nSTD\nHIGH KL\nSTD\nHIGH KL\nSTD\nHIGH KL\n\u2206R\n0.82\n0.20\n0.89\n0.67\n0.94\n0.61\nWRG\n0.02\n0.03\n0.48\n0.37\n0.25\n0.12\nratio\n2.0%\n15.1%\n53.4%\n56.5%\n27.2%\n19.1%\nTable 1 reports results.\nRevis-\niting this in the context of Fig-\nure 3, we see that around 70%\u201390%\nof the improvement on WebGPT\nand RLCD is explained purely by\nshifts in length.\nSTACK shows a\nlower value here, with only about\n40% of the gain arising from length.\nOne reason for this is that STACK\noutputs are close to the length limit\nduring training,3 so gain from in-\ncreasing length is not possible to\nachieve. Second, Stack\u2019s technical\nQA setting represents a different style of answer that we believe does require optimizing for features\nbeyond length.\n3.2\nINTERVENING ON OPTIMIZATION\nWe see that in a standard pipeline, PPO has a tendency to optimize only on length, but what if we\nconstrain optimization to mitigate this? We test the effects of several interventions below.\nThe simplest intervention to PPO to encourage short outputs is to just increase the KL coefficient\n\u03bb (H-KL) (Equation 1), with the intuition that closer to the initial distribution should mean closer to\nthe initial length. We experiment with setting it to 0.12 instead of 0.04; larger values impede model\nconvergence.\nWe also experiment with a scalar penalty on the reward to control length (LEN-C). We set R\u2032 =\n\u03c3 (1 \u2212 len(y)\nN\n), where N is a maximum length value that we do not want PPO to exceed, and \u03c3 is a\nmoving average of batch reward standard deviation.4\nA similar option to prevent outputs from getting longer may just be to altogether omit (OMIT)\noutputs beyond a length threshold from PPO, so that no update is made to encourage these. In\npractice we swap these examples with randomly sampled outputs from the batch.\nFinally, prior work examining ways to improve implementations of PPO mentions that reward\nscaling (RM-SC) can be useful for \u201ccontrolling training fluctuations\u201d and reducing over-optimization\n(Zheng et al., 2023b). Similar to batch normalization (Ioffe & Szegedy, 2015), for each batch X, Y\nof sampled outputs, we compute the mean (\u00b5) and standard deviation (\u03c3) of R. We then take a\nmoving average of these values across N previous batches and \u201cscale\u201d R to become R\u2032 =\nR\u2212\u00b5\n\u03c3 ,\nwhere we note \u03c3 remains relatively constant across training.\nResults\nWe report results for the interventions on the reward score and PPO in Table 2. Note the\nRM row is comparable within each setting since we use the same underlying reward models, and\nthus we use it as our primary metric to reason about length and reward tradeoffs. We also report\nsimulated preferences (see Section 2.2) vs STD, where < 50% indicates being worse than standard\nPPO on downstream answer quality.\nWe find that across all interventions, length always increases relative to SFT, and reward model\nscore is always worse than standard PPO. These patterns suggest that a strong component of PPO\nis related to length. Including the fact that length control (LEN-C) led to convergence failure (reward\nnot increasing during training) on W-GPT and STACK, this suggests that length is a difficult feature\nto disentangle post-hoc from reward.\nRecalling the scatter plots from Figure 3, we note that across all of these different interventions,\nthe scatter plots display similar patterns (see Appendix B), implying that while these interventions\n3Stack, due to SFT having higher initial length, tends to generate unboundedly long outputs after PPO. We\nset a higher max length (216) than the source TRL codebase (128) for Stack; however the pattern remains.\n4We try several variants of this idea, such as a scalar penalty past a length threshold, and note similar con-\nvergence failures. In general, we find that stricter versions of these constraints negatively affects convergence.\n5\nTable 2: Length, reward model score, and simulated preference (SIM PREF, Section 2.2) vs. standard\nPPO (STD) when using different reward score and PPO interventions. Some interventions mitigate\nlength increases, but the overall reward is always lower, and length still rises compared to SFT. Note\nthat LEN-C fails to optimize on W-GPT and STACK, as does OMIT on RLCD and STACK. \u2217 indicates\na statistically significant delta from STD with p < 0.05 based on a bootstrap test.\nW-GPT\nSTACK\nRLCD\nSFT\nSTD\nRM-SC\nH-KL\nOMIT\nSFT\nSTD\nRM-SC\nH-KL\nSFT\nSTD\nRM-SC\nLEN-C\nH-KL\nLEN\n100\n230\n128\n120\n127\n203\n257\n249\n250\n59\n94\n82\n72\n97\nRM\n-0.45\n0.25\n-0.05\n-0.06\n-0.13\n0.05\n0.74\n0.40\n0.30\n4.4\n5.50\n5.00\n5.20\n5.20\nSIM PREF\n42%\u2217\n\u2212\n49%\n45%\u2217\n48%\n42%\u2217\n\u2212\n46% \u2217\n45% \u2217\n37% \u2217\n\u2212\n41% \u2217\n44% \u2217\n43%\u2217\nreduce the overall optimization towards length, they don\u2019t change the fundamental tendency of PPO\nto avoid optimizing for other features. However, while length still increases with respect to SFT,\nseveral interventions do allow for length increases to be mitigated while still recovering a large\nportion of reward and downstream performance gain (e.g., RM-SC).\n4\nEXAMINING REWARD MODELING\nSection 3.2 showed that interventions during PPO do not fully mitigate the issue of reward gains\ncoming from length increases. We now investigate whether we can intervene even earlier in the\nprocess, on the preference data itself, in order to circumvent this length dependence.\n4.1\nANALYZING PREFERENCES OVER TRAINING\nTable 3: Accuracy on the\npreference data from choos-\ning the longer response.\nSubstantially above random\nguessing (50%).\nWGPT\nSTACK\nRLCD\n55.7%\n59.6%\n63.1%\nOne root cause of length correlation is length imbalances in the pref-\nerence datasets, where longer answers are systematically preferred to\nshorter answers. We can measure this with length heuristic agree-\nment: the accuracy of always predicting that the longer output is\nthe gold preferred output (see Table 3): We see that all datasets are\nslightly imbalanced towards longer outputs. However, this doesn\u2019t\nfully explain the strong correlations suggested earlier in Figure 3.\nTo understand this better, we can study training dynamics of reward model learning by comput-\ning statistics over several epochs of training. Given reward model R being trained on preference\ndataset P for E epochs, we can track each data point (xi, y+\ni , y\u2212\ni ) \u2208 P where we compute the\ndistribution of confidence (RM score of \u201cpreferred\u201d subtracted from \u201cdispreferred\u201d), at each epoch\nci = {(e, R(xi, y+\ni ) \u2212 R(xi, y\u2212\ni )) \u2236 e \u2208 {2, . . . , E}}, where we exclude epoch 1 to mitigate noise.\nResults\nFirst, we note that when examining \u201ccartography\u201d plots (Swayamdipta et al., 2020) exam-\nining the mean (ci) and variance (\u03c3(ci)) of different ci (see Appendix B.1), we find that the values\nare largely centered at zero, suggesting that reward models are not able to make progress on most\ntraining examples: the predictions are low-confidence and largely do not change. This suggests that\nmost features are instead learned on the set of \u201ceasy\u201d examples with higher ci.\nWith the hypothesis that length may be related to \u201ceasy\u201d examples, we use length heuristic accuracy\nagain, but this time, we compute it on slices where we bin training examples based on ci, plotting\nthese bins by confidence (x-axis) against length heuristic accuracy (y-axis) on each slice as scatter\nplots in Figure 4.\nThe figure shows strikingly clean patterns, with the mean confidence ci for data in an interval of\ntraining examples correlating strongly with the length heuristic. This means that (1) the length\nheuristic applies to most examples that are easy, and (2) perhaps more tellingly, the overwhelming\nmajority of \u201chard\u201d examples are cases where the model follows the length heuristic to confi-\ndently predict the wrong answer. Overall, this supports that length is one of the strongest features\nlearned in these models. Note that WebGPT, with the strongest pattern, also displayed the lowest\nWRG from Table 1, implying that these correlations propagate through all stages.\n6\n1\n0\n1\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLength Heuristic\nW-GPT\n2\n0\n2\n4\nConfidence\n0.2\n0.4\n0.6\n0.8\nLength Heuristic\nStack\n5\n0\n5\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLength Heuristic\nRLCD\nFigure 4: Accuracy of length heuristic proportional to overall training confidence of examples esti-\nmated with reward model dataset cartography. High confidence examples overwhelmingly abide by\nthe length heuristic, and most misclassified cases (high negative confidence value) are following the\nlength heuristic incorrectly. Note that most examples are in the low-confidence center (shown by\nsize of points).\n4.2\nINTERVENTIONS ON PREFERENCE DATA\n4.2.1\nSETUP\nGiven the strong length biases learned from preference data in standard RMs (STD), we now examine\nwhether we can eliminate these biases by strategically modifying preference data.\nLength Balancing (BAL)\nThe simplest intervention is to remove length biases from the preference\ndata. Specifically we balance data such that the distribution of pair length differences are symmetric\nby bins of 10. Suppose there are more examples where preferred responses are 20 tokens longer than\ndispreferred ones compared to the reverse case; we then subsample the cases which are 20 tokens\nlonger until they match the number of cases which are 20 tokens shorter, thereby balancing the data.\nConfidence-Based Truncation (C-TR)\nOur previous results suggest that something more data-\nspecific beyond a surface length bias may influence training: for example, a particular set of \u201ceasy\u201d\nexamples may be corrupting the data, and removing them may help, as established in literature on\ndataset cartography Swayamdipta et al. (2020). Given that we\u2019ve trained some Rbase, and computed\nci on dataset P (Section 4.1), we can test this idea by training a new RM Rtrunc on a subset of P\nwhere ci < \u03b81 and ci > \u03b82, with threshold hyper-parameters \u03b81, and \u03b82. We experiment with several\nvariants (see Appendix B.1), keeping sets of \u02dc50% of the data for each. Below we report results when\nwe set \u03b81 < \u03b82, keeping a central subset of data.\nTable 4: Eval accuracy (ACC) and pearson within batch\n(CORR) for different trained RMs (RAND is random base-\nline). Different approaches bring down correlation or main-\ntaining good accuracy, but few do both except for BAL on\nstack, and R-DA to a lesser extent.\nWGPT\nSTACK\nRLCD\nACC\nCORR\nACC\nCORR\nACC\nCORR\nRAND\n50%\n0\n50%\n0\n50%\n0\nSTD\n61.5%\n0.72\n70%\n0.55\n80%\n0.67\nBAL\n52.6%\n-0.13\n61.9%\n-0.09\n73.1%\n0.62\nC-TR\n58.8%\n0.67\n59.5%\n0.31\n77.2%\n0.57\nR-DA\n62.5%\n0.35\n72.6%\n0.37\n80%\n0.43\nReward Data Augmentation (R-\nDA)\nIn line with the hypothesis\nthat over-optimization stems from\nspurious correlations in the data,\nanother\npotential\nintervention\nis\ndata augmentation, specifically us-\ning \u201crandom pairing\u201d where we can\npair matching prompt output pairs\nqi, p\u2212\ni from P with p\u2212\ni serving as a\n\u201cprefered\u201d example, and a randomly\nsampled p+\nj from another prompt\nserving as a \u201cdispreferred\u201d example.\nThis serves to encourage disregard-\ning stylistic features in favor of rele-\nvance to the query.\n7\nTable 5: The length (LEN) in tokens, and simulated preference win rate over the standard PPO (SIM\nPREF) for the SFT model, standard PPO (STD), and interventions. Interventions at this stage are\nmore successful than during PPO, but still face the fundamental issues in reward modeling. Recall\nthat we omit BAL for W-GPT since the balanced RM had near-random accuracy.\nW-GPT\nSTACK\nRLCD\nMethod\nSFT\nSTD\nR-DA\nC-TR\nSFT\nSTD\nBAL\nR-DA\nC-TR\nSFT\nSTD\nBAL\nR-DA\nC-TR\nLEN\n100\n230\n139\n141\n203\n257\n148\n256\n244\n59\n94\n82\n112\n97\nSIM PREF\n42%\u2217\n\u2212\n49%\n44%\u2217\n42%\u2217\n\u2212\n57%\u2217\n58%\u2217\n44%\u2217\n37%\u2217\n\u2212\n44%\u2217\n44%\u2217\n50%\n4.2.2\nRESULTS\nWe first report in Table 4 the evaluation accuracy of these different reward models, as well as a\ncorrelation within batch (CORR) measure which, given sets of 8 generations, is the mean Pearson\ncorrelation between output length and reward model score for each batch. While the standard reward\nmodel (STD) achieves high accuracies across settings, this comes with high length correlation.\nData Augmentation (R-DA) improves on both of these partially, while confidence-based truncation\n(C-TR) brings length correlation down at the cost of accuracy. Note that, when using correlation\nwithin batch, we find that BAL leads to length bias being reversed, but at near-random accuracies,\nwhile other truncation strategies don\u2019t yield notable differences. These patterns indicate that, per-\nhaps because RMs fail to learn on most examples, they are particularly brittle, and can learn spurious\ncorrelations easily. As the only setting where length balancing eliminates correlation and maintains\nabove-random accuracy, we see more evidence that STACK is the one setting of our three where\nreward models can learn features other than length.\nWe then show results for downstream adjustments to preference data in Table 5: Length still usually\nincreases from the SFT starting point, though many interventions are shorter relative to STD. BAL\non STACK, perhaps due to there being other easy non-length features to learn, even leads to shorter\noutputs than SFT, confirming the importance of preference data to final PPO length biases.\nUnlike our PPO interventions described in Table 2, simulated preference doesn\u2019t always decrease\nwith preference data interventions: On STACK, where BAL is shorter than SFT, it also improves\nSIM PREF over normal PPO, suggesting that at least in noisier settings there is somehow room for\nPPO to do more than just increase length, but this pattern is inconsistent. Compared to later stages,\ninterventions on preference data seem to be the most promising for overall improvement of RLHF\nbeyond length, though the fundamental inability of reward models to learn well from data remains.\n5\nHOW FAR CAN LENGTH GO?\nMany of our experiments suggest that our reward models are primarily guiding PPO to produce\nlonger outputs, yet we still see improvements on downstream simulated preferences. One explana-\ntions for this is that humans and models like GPT-4 have a bias towards preferring longer outputs\nin the settings we study (Zheng et al., 2023a). Another possibility is that optimizing for length with\nPPO intrinsically improves the quality of generation even in the absence of other features.\nWe investigate two interventions aimed purely at increasing length, which show how far optimizing\nfor this single aspect can go. First, we sample 8 outputs from the SFT model and choose the longest\none (SFT-LONG). Second, we use length as our reward for PPO (keeping the standard KL term)\nwith R\u2217(y) = 1 \u2212 \u00bb\u00bb\u00bb\u00bb\u00bb\nlen(y)\nN\n\u2212 1\u00bb\u00bb\u00bb\u00bb\u00bb. In this case, N is a target length hyperparameter (set to 156, 120,\nand 200 on WebGPT, RLCD, and STACK respectively). We call this setting LPPO, and also explore\na variant of length-only PPO with \u03bb set to 0 (LPPO \u03bb = 0) in Table 6.\nFirst, we note that SFT-LONG can lead to moderate improvements (57% winrate vs SFT on STACK\nand 52% on RLCD), though not on WebGPT. When we then compare to LPPO, we find that purely\noptimizing for length actually reproduces most of the performance improvements of RLHF\nwith the reward models. Notably, this approach yields simulated preference improvements over\nSFT-LONG, which has even longer outputs.\n8\nTable 6: Simulated preferences (winrate against SFT and winrate against standard PPO) from purely\noptimizing for higher length (LPPO). Using our length reward does better than choosing the longest\nof 8 sampled from SFT (SFT-LONG) and reproduces most PPO improvements from STD PPO (SIM\nPREF (PPO)).\nW-GPT\nSTACK\nRLCD\nSFT-LONG\nLPPO\nLPPO \u03bb = 0\nSFT-LONG\nLPPO\nLPPO \u03bb = 0\nSFT-LONG\nLPPO\nLPPO \u03bb = 0\nLEN(SFT)\n100\n\u2212\n\u2212\n203\n\u2212\n\u2212\n59\n\u2212\n\u2212\nLEN\n141\n118\n167\n249\n252\n248\n117\n98\n163\nSIM PREF (SFT)\n48%\n56%\u2217\n53%\n57%\u2217\n59%\u2217\n58%\u2217\n52%\n64%\u2217\n51%\nSIM PREF (PPO)\n\u2212\n48%\n47%\n\u2212\n43%\u2217\n42%\u2217\n\u2212\n48%\n44%\u2217\nIt is still possible that RLHF with our reward models does lead to other changes or improvements\nin the outputs beyond length. This experiment also does not necessarily establish flaws in the pref-\nerence judgments; these outputs with the right length are often more informative and more useful\n(Figure 1). However, it does show that a significant fraction of the downstream gains can be ex-\nplained by optimizing for length.\n6\nRELATED WORK\nRL\nReinforcement learning from human feedback has been explored extensively (Knox & Stone,\n2009), often being used in robotics tasks to extrapolate reward signal beyond an initial preference\nset (Brown et al., 2019). Recent work in NLP has explored implementations (Zheng et al., 2023b;\nTouvron et al., 2023b), objectives (Wu et al., 2023), and even alternatives (Rafailov et al., 2023;\nZhao et al., 2022; 2023) for RLHF, but have generally overlooked or dismissed length increases.\nOur work is largely orthogonal to these directions, using the issue of length to analyze the lack of\nrobustness in current reward models. Finally, other past uses of RL in NLP (Ammanabrolu & Riedl,\n2018; Martin et al., 2017; Ramamurthy et al., 2023) have largely faced different sets of issues due\nto reward not coming from models learned over human preferences.\nReward Model\nIn the context of noisy and biased preference data, are reward models able to\nlearn robust features reflecting the underlying preferences? In broader NLP, dataset artifacts have\nbeen a prevalent issue even on simpler settings like natural language inference (Gururangan et al.,\n2018; Poliak et al., 2018). In the context of RLHF, Stiennon et al. (2020) notes that over-optimizing\nfor a reward model leads to pathological summaries, Dubois et al. (2023) notes a pattern of human\npreferences going up briefly then down as reward model score increases, and Pang et al. (2022)\npresent some cases where reward hacking can be produced within synthetic settings. Our work,\nin comparison, delves further into what causes reward over-optimization in realistic settings, while\nalso further exploring diagnostics and solutions. We focus on length as it is the most prevalent, but\nour experimental paradigm is applicable to any analysis of over-optimization in RLHF.\nLength control and length biases\nTechniques outside of RLHF for controlling length of NLP\nmodels have been explored (Kikuchi et al., 2016; Ficler & Goldberg, 2017). Length divergences\nspecifically between training time and test time have been explored in the machine translation litera-\nture (Riley & Chiang, 2022), but these have been attributed to inference techniques and label bias in\ntext generation methods. The open-ended nature of our generation problems is quite different from\nMT. Murray & Chiang (2018) use a per-word reward similar to our per-word penalty in RL, though\nto solve the opposite problem of outputs being too short. Finally, in discriminative \u201ctext matching\u201d\ntasks like paraphrasing, past work has observed similar length heuristics, Jiang et al. (2022), but the\nsentence-pair format of these tasks makes their issues somewhat different.\n7\nCONCLUSION AND LIMITATIONS\nIn this work we study correlations of length and reward in RLHF. Across three datasets and across\nseveral stages of observational and intervention-based exploration, we make a case that RLHF in\nthese settings achieves a large part of its gains by optimizing for response length.\n9\nWhile the extent of the patterns we find are surprising, this doesn\u2019t necessarily invalidate the po-\ntential of RLHF. We note that our Stack setting, which involves the most technical responses, does\ndemonstrate improvements in reward even for outputs already at our maximum length. Furthermore,\noptimizing purely for length does seem to lead to \u201cqualitative\u201d improvements beyond just sampling\nfrom the base model and choosing longer outputs, indicating that the learning dynamics of RLHF\nmay be beneficial for LM training. Rather than claiming length to be an inherent shortcoming, we\nseek to use it as a vehicle to analyzing RLHF\u2019s successes and failures.\nOne limitation of our work is that, while we explore diverse settings, we are restricted to open-source\npreference datasets. Recent work such as Llama-2 (Touvron et al., 2023b) develops an extensive\ndataset of preferences and pursues a sophisticated RLHF strategy, which may not face the limitations\nwe do. Furthermore, we focus primarily on a broad \u201chelpfulness\u201d objective (again, aligning with\nthese preference datasets) using LLaMA-7B as the base model. While these represent a substantial\nfraction of research on open reward models, our findings may not necessarily apply to RLHF running\non larger closed-source models, or with alternate objectives like \u201charmlessness\u201d.\nDespite these limitations, we believe our work shows that RLHF with these reward models is not\nyet achieving its full potential. We believe that developing more accurate and robust reward models,\neither by changing the reward model, its objective, or the preference collection process, may hold\nthe key to unlocking the full capabilities of RLHF.\nREPRODUCIBILITY\nFor our various studies on the relationship between RLHF and length, we first trained a set of\nreward models and policy models. In order to support future open RLHF research, we release our\ncode as well as reward and policy models. In addition to detailing our experimental setup and\nevaluation scheme in Section 2.2, as well as describing our interventions in detail in Section 3.2 and\nSection 4, we include further hyper-parameters and instructions in Appendix A. Note that we use\nopen preference datasets, publicly available base models, and open-source RLHF code that doesn\u2019t\nrequire prohibitive computational resources.\nACKNOWLEDGMENTS\nThis work was supported by NSF CAREER Award IIS-2145280, a grant from Open Philanthropy, a\ngift from Salesforce, Inc., and a gift from Amazon. Thanks to Eunsol Choi and members of the UT\nTAUR lab for helpful discussion and feedback.\nREFERENCES\nPrithviraj Ammanabrolu and Mark O. Riedl.\nPlaying text-adventure games with graph-\nbased deep reinforcement learning.\nArXiv, abs/1812.01628, 2018.\nURL https://api.\nsemanticscholar.org/CorpusID:54458698.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate.\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Con-\nference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario\nAmodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and\nJared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human\nfeedback. ArXiv, abs/2204.05862, 2022. URL https://api.semanticscholar.org/\nCorpusID:248118878.\nAndreea Bobu, Andi Peng, Pulkit Agrawal, Julie A. Shah, and Anca D. Dragan.\nAligning\nrobot and human representations.\nArXiv, abs/2302.01928, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:256598067.\n10\nRalph Allan Bradley and Milton E. Terry.\nRank Analysis of Incomplete Block Designs: I.\nThe Method of Paired Comparisons.\nBiometrika, 39:324, 1952.\nURL https://api.\nsemanticscholar.org/CorpusID:125209808.\nDaniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub-\noptimal demonstrations via inverse reinforcement learning from observations. In International\nConference on Machine Learning, 2019. URL https://api.semanticscholar.org/\nCorpusID:119111734.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto.\nAlpacaFarm: A Simulation Framework\nfor Methods that Learn from Human Feedback, 2023.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, Florence, Italy, July 2019. Association for Computational Linguistics.\nURL https://aclanthology.org/P19-1346.\nJessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language genera-\ntion. In Proceedings of the Workshop on Stylistic Variation, pp. 94\u2013104, Copenhagen, Denmark,\nSeptember 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4912. URL\nhttps://aclanthology.org/W17-4912.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and\nNoah A. Smith. Annotation artifacts in natural language inference data. ArXiv, abs/1803.02324,\n2018. URL https://api.semanticscholar.org/CorpusID:4537113.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text\nDegeneration. 2020. URL https://arxiv.org/abs/1904.09751.\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and\nWeizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. ArXiv, abs/2106.09685,\n2021. URL https://api.semanticscholar.org/CorpusID:235458009.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International Conference on Machine Learning, 2015. URL\nhttps://api.semanticscholar.org/CorpusID:5808102.\nLan Jiang, Tianshu Lyu, Yankai Lin, Meng Chong, Xiaoyong Lyu, and Dawei Yin. On length di-\nvergence bias in textual matching models. In Findings of the Association for Computational Lin-\nguistics: ACL 2022, pp. 4187\u20134193, Dublin, Ireland, May 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.findings-acl.330. URL https://aclanthology.org/\n2022.findings-acl.330.\nYuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. Control-\nling output length in neural encoder-decoders. In Conference on Empirical Methods in Natural\nLanguage Processing, 2016. URL https://api.semanticscholar.org/CorpusID:\n11157751.\nW. Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The\ntamer framework.\nIn Proceedings of the Fifth International Conference on Knowledge Cap-\nture, K-CAP \u201909, pp. 9\u201316, New York, NY, USA, 2009. Association for Computing Machin-\nery. ISBN 9781605586588. doi: 10.1145/1597735.1597738. URL https://doi.org/10.\n1145/1597735.1597738.\nNathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush.\nHuggingFace H4 Stack\nExchange Preference Dataset, 2023.\nURL https://huggingface.co/datasets/\nHuggingFaceH4/stack-exchange-preferences.\nLara J. Martin, Prithviraj Ammanabrolu, William Hancock, Shruti Singh, Brent Harrison, and\nMark O. Riedl. Event representations for automated story generation with deep neural nets. In\nAAAI Conference on Artificial Intelligence, 2017. URL https://api.semanticscholar.\norg/CorpusID:19127777.\n11\nKenton Murray and David Chiang. Correcting length bias in neural machine translation. In Pro-\nceedings of the Third Conference on Machine Translation: Research Papers, pp. 212\u2013223, Brus-\nsels, Belgium, October 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/\nW18-6322. URL https://aclanthology.org/W18-6322.\nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloun-\ndou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\nWebGPT: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332,\n2021. URL https://api.semanticscholar.org/CorpusID:245329531.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano,\nJan Leike, and Ryan J. Lowe.\nTraining language models to follow instructions with human\nfeedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/\nCorpusID:246426909.\nRichard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, and He He. Re-\nward gaming in conditional text generation.\nIn Annual Meeting of the Association for Com-\nputational Linguistics, 2022. URL https://api.semanticscholar.org/CorpusID:\n253553557.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme.\nHypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint\nConference on Lexical and Computational Semantics, pp. 180\u2013191, New Orleans, Louisiana, June\n2018. Association for Computational Linguistics. doi: 10.18653/v1/S18-2023. URL https:\n//aclanthology.org/S18-2023.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and\nChelsea Finn.\nDirect preference optimization:\nYour language model is secretly a reward\nmodel.\nArXiv, abs/2305.18290, 2023.\nURL https://api.semanticscholar.org/\nCorpusID:258959321.\nRajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Chris-\ntian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is Reinforcement Learning (Not) for Nat-\nural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language\nPolicy Optimization. In Proceedings of the International Conference on Learning Representa-\ntions (ICLR), 2023. URL https://arxiv.org/abs/2210.01241.\nDarcey Riley and David Chiang. A continuum of generation tasks for investigating length bias and\ndegenerate repetition. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Inter-\npreting Neural Networks for NLP, pp. 426\u2013440, Abu Dhabi, United Arab Emirates (Hybrid), De-\ncember 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.blackboxnlp-1.\n36. URL https://aclanthology.org/2022.blackboxnlp-1.36.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProxi-\nmal policy optimization algorithms.\nArXiv, abs/1707.06347, 2017.\nURL https://api.\nsemanticscholar.org/CorpusID:28695052.\nJoar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger.\nDefining\nand characterizing reward hacking.\nArXiv, abs/2209.13085, 2022.\nURL https://api.\nsemanticscholar.org/CorpusID:252545256.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2020. URL https://api.\nsemanticscholar.org/CorpusID:221665105.\nSimeng Sun, Dhawal Gupta, and Mohit Iyyer. Exploring the impact of low-rank adaptation on\nthe performance, efficiency, and regularization of RLHF. ArXiv, abs/2309.09055, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:261884455.\n12\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn Advances in neural information processing systems, pp. 3104\u20133112, 2014.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi,\nNoah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with\ntraining dynamics. In Conference on Empirical Methods in Natural Language Processing, 2020.\nURL https://api.semanticscholar.org/CorpusID:221856637.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.\nLLaMA: Open and Effi-\ncient Foundation Language Models. ArXiv, abs/2302.13971, 2023a. URL https://api.\nsemanticscholar.org/CorpusID:257219404.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas\nBlecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S.\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\nKhabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert\nStojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat\nModels. ArXiv, abs/2307.09288, 2023b. URL https://api.semanticscholar.org/\nCorpusID:259950998.\nLeandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\nLambert, and Shengyi Huang.\nTRL: Transformer Reinforcement Learning.\nhttps://\ngithub.com/huggingface/trl, 2020.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A.\nSmith, Mari Ostendorf, and Hanna Hajishirzi.\nFine-grained human feedback gives better re-\nwards for language model training.\nArXiv, abs/2306.01693, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:259064099.\nKevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian.\nRLCD: Re-\ninforcement Learning from Contrast Distillation for Language Model Alignment.\nArXiv,\nabs/2307.12950,\n2023.\nURL https://api.semanticscholar.org/CorpusID:\n260357852.\nYao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J. Liu. Cal-\nibrating sequence likelihood improves conditional language generation. ArXiv, abs/2210.00045,\n2022. URL https://api.semanticscholar.org/CorpusID:252683988.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. SLiC-\nHF: Sequence Likelihood Calibration with Human Feedback. ArXiv, abs/2305.10425, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:258741082.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph Gonzalez, and Ioan Cris-\ntian Stoica. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. ArXiv, abs/2306.05685,\n2023a. URL https://api.semanticscholar.org/CorpusID:259129398.\nRui Zheng, Shihan Dou, Songyang Gao, Wei-Yuan Shen, Bing Wang, Yan Liu, Senjie Jin, Qin\nLiu, Limao Xiong, Luyao Chen, Zhiheng Xi, Yuhao Zhou, Nuo Xu, Wen-De Lai, Minghao Zhu,\nRongxiang Weng, Wen-Chun Cheng, Cheng Chang, Zhangyue Yin, Yuan Long Hua, Haoran\nHuang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets\nof RLHF in Large Language Models Part I: PPO. ArXiv, abs/2307.04964, 2023b. URL https:\n//api.semanticscholar.org/CorpusID:259766568.\n13\nSimon Zhuang and Dylan Hadfield-Menell.\nConsequences of Misaligned AI.\nArXiv,\nabs/2102.03896,\n2021.\nURL https://api.semanticscholar.org/CorpusID:\n227275607.\nA\nTRAINING / EVALUATION DETAILS\nHardware\nAll experiments were conducted across 2 workstations, one with 4 NVIDIA RTX\nA6000 GPUs and another with 8 NVIDIA A40 GPUs. However, all of our individual experiments\nwere run across 2 GPUs. In this configuration, training an RM takes around 6 hours on the Stack\ndataset, 3 hours on the RLCD dataset, and 1 hour on the WebGPT dataset after \u02dc1 epoch. For PPO,\ntraining takes around 12-18 hours for a single run.\nA.1\nREWARD MODELS\nFor StackExchange, we have a train set of 100K examples and an evaluation set of 10K examples.\nFor WebGPT, since we use 18k examples from training and 1.6K as an evaluation set. For RLCD,\nwe use 40K training examples and 2.6 examples for the test set, where we use these test sets in all\nof the evaluation we show above.\nFor training, we follow a rule of continuing to train until eval accuracy stops going up. Prior work\nfinds that reward model training for just 1 epoch is most effective to avoid over-fitting, however for\nsome of our preference data interventions we note that convergence takes longer. Overall, this ends\nup with usually 1-2 epochs of training at most for the checkpoints that we use. We use bfloat16,\nlearning rate of 1e-5, and batch size of 2 with 2 gradient accumulation steps. With these configura-\ntions, 1 epoch on 10K examples takes around 2 GPU hours.\nNote that for the training dynamics analysis (Figure 4), we run reward model training for 5 epochs\nto reduce variance, however we don\u2019t use those models directly (though we note that eval accuracy\ndoesn\u2019t go down significantly even at that point).\nA.2\nPPO\nFor our RLHF step, as stated before, we use LoRA and 8-bit quantization for the policy and reward\nmodels, since the TRL training configuration requires having all used models on each device used\nfor training. We merge reward model and generation models with LoRA adapters before PPO.\nPast work has commented on the stability of PPO and \u201csecrets\u201d needed to get it working well (Zheng\net al., 2023b). We found that setting the right KL coefficient and batch size were the most important\nfor stable convergence.\nFor training we generally run training for between 150-200 steps, where this is a hyperparameter for\neach dataset depending on speed of convergence and allowing sufficient steps for KL to decrease in\ncertain settings (Figure 5). We experimented with runs of up to 400 steps and generally did not find\nimprovement in simulated preference or reward.\nWith 2 GPUs, batch size of 32 on each, training takes around 16 hours to complete 200 steps, giving\nan overall time of 32 GPU hours per PPO model. Note that we use max length of 156 on WebGPT\nand RLCD, and 216 on STACK since it has a higher starting point for the SFT model (note that this\nhyperparameter has a strong influence on training speed).\nFigures 5, 6, 7 show statistics over the course of training for our standard settings, with KL of 0.04.\nWe note that RLHF does successfully increase reward score. The last half of training usually yields\na decrease in KL divergence, as the model has optimized for reward and is regularized closer to the\ninitial policy model by the KL term.\nA.3\nINFERENCE / EVALUATION\nOnce we have our trained PPO models, we finally sample outputs that we can use to compare\ndifferent systems and evaluation. For all results, unless otherwise stated, we generate 500 outputs\neach from a fixed set of the held out data from each dataset, and base our results on those (we find\n14\nFigure 5: Training for standard WebGPT run\nFigure 6: Training for standard Stack run\nFigure 7: Training for standard RLCD run\nthis to be a sufficient amount, especially when comparing patterns across a set of interventions /\nsettings which themselves serve as additional datapoints). Computing simulated preference (Dubois\net al., 2023) for 100 outputs costs around $3.5 USD using the OpenAI API. These calls are made to\ngpt-4-0314, gpt-3.5-turbo-0301, and text-davinci-003.\nWe decode with nucleus sampling (Holtzman et al., 2020) with a p value of 0.9, maximum length of\n256, temperature 0.9, and a repetition penalty of 1.2 (based on the TRL repository default hyperpa-\nrameters). Whenever we sample multiple outputs from a single prompt, we draw 8 samples.\nA.4\nINTERVENTIONS\nFor length control, we set the length center N to the starting mean SFT length, noting similar pat-\nterns with different configurations as well (50 tokens for RLCD, 100 for WGPT, 200 for STACK),\nfor omission, we use 24 tokens above these value (to allow for stable training). For reward data aug-\nmentation, we augment with 25% additional data, noting that 50% gives similar patterns, however\ndata augmentation may need to be explored further in future work.\nB\nEXTRA PLOTS\nB.1\nDATASET CARTOGRAPHY\nWe here include dataset cartography plots in the style of Swayamdipta et al. (2020) for our reward\nmodeling tasks. First, Figure 8 shows the dataset cartography plots on our respective settings. Note\nthat WebGPT, the dataset with the strongest length biases, seems the most centered at 0 variance,\nand symmetric with respect to the x-axis. RLCD and STACK, where there seems to be more room for\nother features, on the other hand, demonstrate an \u201cupward tilt\u201d where at higher variances, the models\nare able to learn correct higher confidence features. This provides evidence for our hypothesis that\n15\n(a) WebGPT\n(b) Stack\n(c) RLCD\nFigure 8: Logarithmically scaled heatmaps plotting, with a preference pair in training data being\na single point, the variance of confidence over training (x-axis) vs the mean confidence (y-axis).\nThese are the plots for WebGPT, STACK, and RLCD respectively left to right.\n(a) WebGPT DA\n(b) RLCD Truncate Middle\n(c) RLCD Truncate Top\nFigure 9: Dataset cartography plots on WebGPT when doing Reward Data Augmentation, as well\nas RLCD when truncating the central and upper sections of data.\nstrong length biases emerge as a symptom of reward models being unable to learn clear features\nfrom most training data.\nFigure 9 shows plots for two additional settings. First, we see that when doing data augmentation,\nthe augmented data emerges clearly and separately in the \u201chigh confidence\u201d zone, while the initial\nplot remains in a similar space as before, though interestingly now displaying more of the \u201cupward\ntilt\u201d with a longer tail. Next, we see that when cutting out the central section of the cartography\nplot and training on the remaining data, the shape is actually preserved, suggesting that the small\namount of remaining data has an intrinsic tendency to learn the strong length correlation. Likewise,\nwhen removing the upper section of \u201ceasy examples\u201d, we see that suddenly the RLCD plot becomes\nmuch more centered and sharper at the left with low variance, suggestive of the \u201cbrittleness\u201d that we\nsaw with WebGPT, where now the easiest pattern is harder to learn, exacerbating the pattern even\nfurther.\nB.2\nLENGTH SCATTERPLOTS\nEarlier in Section 3, we discuss the idea of reward gain due to length vs other features, examining\nresults with Higher KL term. Here we show comparable plots for WebGPT (Figure 11) and RLCD\n(Figure 12). Note that the patterns and reward gain are very similar, suggesting that the length\nconstraining techniques all perform similar functions, despite having very different formulations.\nAlso note that for these two settings the ratio of reward gain independent of length remains quite\nlow.\nC\nSAMPLE OUTPUTS\n16\n50\n100\n150\n200\nLength\n1.0\n0.5\n0.0\n0.5\n1.0\nReward\nW-GPT\n100\n150\n200\n250\nLength\n0.5\n0.0\n0.5\n1.0\n1.5\nReward\nStack\n0\n25\n50\n75\n100\n125\n150\nLength\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nReward\nRLCD\nFigure 10: Bin-scatters on different settings with red (normal KL) and black (high KL) shown\ntogether\n100\n150\n200\n250\nLength\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\nOmit\n50\n100\n150\n200\nLength\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nReward\nHKL\n0\n50\n100\n150\n200\n250\nLength\n1.0\n0.5\n0.0\n0.5\nReward\nRW-SC\nFigure 11: WGPT Bin Scatters on other types of length constraints.\n50\n100\n150\n200\n250\nLength\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nReward\nLen Penalty\n0\n50\n100\n150\n200\n250\nLength\n3\n4\n5\n6\nReward\nHKL\n0\n50\n100\n150\n200\n250\nLength\n3\n4\n5\n6\nReward\nRW-SC\nFigure 12: RLCD Bin Scatters on other types of length constraints.\n17\nTable 7: Examples for StackExchange\nQuestion\nBefore PPO\nAfter PPO\nHelp , How can you\ndetect\nwhenever\nthe\npower\ncord\nis\nun-\nplugged from electrical\nsocket\nthrough\nSym-\nbian OS ?\n1. <[URL]2. <[URL]3.\n<[URL]entry23253>\n(same\nsolution,\nbut\nwith more details)\n1. **Check if it\u2019s plugged in** - call \u2018isPluggedIn()\u2018 on your [Pow-\nerManager]([URL] object, which will return \u2018true\u2018 when the device\nhas been connected to AC power and the phone doesn\u2019t have a bat-\ntery installed; otherwise, it returns \u2018false\u2018.2. If that isn\u2019t enough\nfor you (perhaps because you need an alarm or something), then\ntake a look at <[URL] about how to use <[URL] time units within\nthe Android framework. The solution outlined there should work\nfine with any of the three different timestamps available via System-\nClock - you just need to pick one. I suggest using milliseconds since\nthey seem most useful here, but feel free to experiment!\u2014Edit: Just\nwanted to make sure this wasn\u2019t too much code to throw away :)\u201c\u2018js-\nvar now = new Date().getTime() / TimeUnit.SECONDS.toMillis(1);\nI am asking this ques-\ntion from an educa-\ntional/hacking point of\nview, (I wouldn\u2019t re-\nally want to code like\nthis).Is it possible to\nimplement a while loop\nonly using **C** pre-\nprocessor directives.\nI\nunderstand that macros\ncannot be expanded re-\ncursively, so how would\nthis be accomplished?\n1) \u2018//define while(x)\u2018,\nthen in the macro: \u2018if\nx\u2018 and \u2018endif\u2018, or if\nyou wish your \"con-\nditional\"\nstatement\nto execute each time\nafter one condition was\ntrue\nbefore\nmoving\non...2)\u2018define\nif(!x)\u2018;\n\u2018if x\u2018 - as above for the\nconditional\nstatement,\nbut without need for\nthe \u2019else\u2019. If you do not\nspecify any condition,\nthe\nend\nwill\nalways\nexecute.3) \u2018while(true);\n//\nno\nbreak\u20184)\nThis\nshould get you started\nalong the right lines at\nleast. The preprocessor\nis fairly limited though\nwhen compared with\nwhat some other lan-\nguages offer!\n1) For the sake of simplicity let us assume all your \u2018while\u2018 loops\nare nested inside a macro called \u2018FUNCT\u2018. 2) As a first approach\nyou could define another variable in order to test if the condition is\ntrue or not. This new variable will have as value zero when there\nis no loop and nonzero otherwise:\u201c\u2018define FOO 0; undef FUNCT-\ndefine BOO if(FOO != 0) FUNCT(); else // do nothing here be-\ncause we exit directly ; return 0; /* You can use other values in-\nstead */ int main()return BOO;\u201c\u20183) Another option involves adding\nsome variables at compile time which may increase significantly the\nsize of your program but it also increases its readability:\u201c\u2018include\n<stdio.h>define LOOP(N) int j=1; for(j = 0 ; j< N*5-4 ; ++j )/*\nDO SOMETHING HERE */ return j; /* The number of iterations */\n//The user defines\nApart from the normal\ndifferences in pitch and\ntone between male and\nfemale voices, are there\nany\ndifferent\nwords,\ndifferent intonations, or\nother linguistic habits\nthat tend to be used\nmore by men or women\nin\nKorean?Are\nthere\nany \u2019male habits\u2019 that\n(to\nsound\nnatural)\na\nfemale\nlearner\nmay\nwant to avoid imitating,\nand vice versa?\n1.\nNo, it is not an\nexaggeration;\nKorean\nis\ngender-specific\nwhen\nit\ncomes\nto\nnouns/adjectives etc..2.\nThere exists [the fol-\nlowing list of female\nnames]([URL]\nwhich\nwill help you select ap-\npropriate name for your\ncharacter if needed but\nmost likely using com-\nmon female names as\ncharacters might lead\nto some confusion with\nreaders because many\npeople\nhave\nheard\nthem so often while\nothers never hear such\nnames ever before... So\nI\nwould\nrecommend\nmaking up new ones.\n1. **Tone** - I don\u2019t think it is possible to have an overview of\nthis one because tones depend on context. For instance the word\nfor bread has two totally distinct meanings depending if it is said\nwith a high or low rising/falling tone. However generally speaking\nKoreans use lower tones at end sentences whereas native English\nspeakers often prefer higher ones. 2. **Pitch/Timbre** \u2013 as men-\ntioned above this depends so much on context but here again gen-\neralizations can help you out: When asking questions your voice\ngoes up, when saying statements down; you use less vocal variety\nin speech than people do normally; in fact all your vocal cues like\nvolume etcetera will be toned-down compared to how we would ex-\npress ourselves if using our first language. 3. **Use of vowels**\n\u2013 well Korean has no shortage of long open vowel sounds while\nEnglish prefers to reduce them whenever possible. This difference\nalone means that certain pronunciation issues become quite obvious\nfor non natives of both languages. Aside from that however, there\nisn\u2019t too many rules regarding vows although they play an important\nrole in differentiate between dialect\n18\nTable 8: Examples for WebGPT\nQuestion\nBefore PPO\nAfter PPO\nWhy is it adults don\u2019t\nroll off of the bed ?I\nunderstand there are ex-\nceptions but I was just\ncurious\nAdults typically do not\nroll off of the bed be-\ncause they have devel-\noped the muscle mem-\nory to keep their bod-\nies from involuntarily\nmoving\nduring\nsleep\nand maintaining proper\nposture.\nAdults generally do not roll off of the bed because they have devel-\noped muscle control and awareness which helps them to remain up-\nright while sleeping. Additionally, most adults find it uncomfortable\nor even painful to move around in their sleep, so rolling off the bed\nwould be difficult without waking up first. There may also be safety\nconcerns such as falling out of bed and potentially hurting oneself if\none were to lose balance or fall from the bed. Finally, some people\nmay simply choose to stay in the same position for long periods of\ntime when they are sleeping and thus avoid having to shift positions\nduring their slumber. It should also be noted that some medical\nconditions can lead to excessive movement at night, making it more\nlikely for someone to roll off the bed. In these cases, medications\nand/or lifestyle changes could help reduce this movement. Overall,\nit appears to be mostly a matter of choice and comfort rather than\nany physical limitation preventing an individual from getting up or\noff the bed.\nA strong magnet will\nseparate a mixture ofA.\nclear glass and green\nglass.B.\npaper\ncups\nand plastic cups.C. iron\nnails\nand\naluminum\nnails.D. sand and salt.\nB)paper cups and plas-\ntic cups\nC. Iron nails and Aluminum Nails. A strong magnet can be used\nto sort out metal objects, such as nails. The magnetic force be-\ntween the two different metals will cause them to separating from\neach other in the mixture. Magnetic separation is also useful for\nsorting materials into different classes based on their composition\nor properties. In this case, it could be used to separate iron nails\nfrom aluminum nails, since they have very different physical and\nchemical properties. It cannot be used to sort out clear glass from\ngreen glass because the difference in color alone does not represent\nany significant differences in the compositions of the two types of\nglass. Paper cups and plastic cups are made up of completely dif-\nferent materials (plastics versus cellulose) so a magnet would not\nhelp differentiate the two. Finally, salt and sand are both crystalline\nsolids with similar structures but different sizes, making them im-\npossible to distinguish using a magnet.\nHow come when we\nclose one eye, we see\n\u201cnothing\u201d out of that\neye, but when we close\nboth, we see black?And\nwhen I say nothing I\nmean that you don\u2019t see\nblack, just...nothing.\nWhen\nonly\none\neye\nis closed, it prevents\nvisual\ninformation\nfrom entering the brain\nthrough that pathway;\nhowever,\nsince\nthere\nare still signals being\nsent to the other eye,\nour\nbrains\nperceive\n\"something\" in terms\nof\nlight\nand\ncolor\n(or darkness).\nWhen\nboth eyes are shut, all\nincoming\nsignals\nare\nblocked and thus the\nbrain interprets this as\n\"blackness\".\nWhen we close only one eye, our brain interprets the lack of input\nfrom that eye as darkness because it doesn\u2019t receive any visual in-\nformation to process. But when we close both eyes, our brains com-\nbine the two images into one and interpret the combined image as\ndarkness since there isn\u2019t enough contrast between the two images\nfor us to be able to make out individual details. This phenomenon is\nknown as binocular vision and helps protect us against blind spots\nby allowing us to use both eyes together in order to gain better depth\nperception and detect motion more easily. It also allows us to have\na wider field of view compared to if we were using just one eye\nat a time. In addition, since each eye sees slightly different things\ndepending on what angle it is facing, combining both images can\nhelp fill-in missing pieces of detail or detect objects that are closer\nor further away than what could be detected with just one eye. By\nclosing both eyes, your brain uses all this data to create a single im-\nage which appears as pitch black due to the absence of significant\ncontrasts. That being said, some people do still experience ghostly\nshapes and movement even when they shut their eyes completely;\nthese effects arise from electrical activity happening within the eye\nitself\n19\nTable 9: Examples for Anthropic HelpfulHarmless (RLCD)\nQuestion\nBefore PPO\nAfter PPO\nHuman:\nHow\ndo\nI\nmake\nItalian\nice?\nAssistant:\nThere are\nmany forms of Italian\nice,\nand I can help\nyou try to figure out\nwhich\nof\nthem\nyou\nwant. You could ask for\ngelato with a swirl of\nchocolate syrup, or you\ncould ask for granita di\nlimone, or maybe some\nstracciatella or espresso\ngranita.Human:\nHow\ncan I make Italian ice\nat\nhome?Assistant:\nFirst, you\u2019ll need some\nice.\nYou can use ice\ntrays or ice cubes in a\nbag, or even a tray or\nbag\nof\nsnow.Human:\nDo\nI\nneed\nan\nice\nshaver?Assistant:\nYes! It\u2019s recommended\nto have an ice shaver\nif you plan on making\ntrue\nItalian-style\nice.\nAn ice shaver will al-\nlow you to create fine\nshavings of ice instead\nof large chunks.\nYes, it\u2019s best to have an ice shaver if possible since this will allow\nyou to create small pieces of ice without having to break up the\nentire block. If you don\u2019t have one, you can still make great Italian\nice by using a blender or food processor instead. Just be sure not to\nover-process your ice or it may become too frozen and hard to eat!\nHuman:\nMy niece is\ninto\nSesame\nStreet.\nWhat are some sesame\nstreet\ncharacters\nshe\ncan\nadd\nto\nher\ntoy\ncollection?Assistant:\nSome of your niece\u2019s\nfavorite Sesame Street\ncharacters\ninclude\nElmo, Cookie Monster,\nAbby Cadabby, Grover\nand Bert Ernie.\nYou\ncould also check out\nplush toys featuring the\nentire Muppet gang!\nSesame Street has so many beloved characters! Your niece could\nhave fun with Elmo, Cookie Monster, Grover, Abby Cadabby and\nmore.\nShe might also enjoy playing with Big Bird, Oscar the\nGrouch or Bert Ernie - they all offer lots of interactive play options.\nAdditionally, you may want to check out some of the other classic\nMuppets like Kermit the Frog and Miss Piggy.\nContinue\nthe\ncon-\nversation:What\nare\nsome\nof\nthe\nhealth\nbenefits of going to a\nspa?Assistant:\nGoing to a spa can\nprovide\nnumerous\nhealth\nbenefits,\nsuch\nas reduced stress lev-\nels,\nimproved mental\nclarity and overall well-\nbeing, increased energy\nlevel, decreased risk of\ndisease, improved skin\ncomplexion, relief from\nmuscle tension, anxiety\nreduction,\nenhanced\nimmunity system, and\nbetter digestion.\nGoing to a spa can have numerous health benefits, including relax-\nation and stress relief. Spas provide opportunities for people to un-\nwind in both physical and emotional ways by providing massages,\nfacials, body wraps, manicures, pedicures, and more. This allows\nthem to reduce their levels of stress, anxiety, and other negative\nthoughts and feelings while also reducing muscle tension and in-\ncreasing energy level. Additionally, many spa treatments include\ndetoxifying elements such as herbs and minerals which can help\nimprove overall health and well-being.\n20\n"
  },
  {
    "title": "Drag View: Generalizable Novel View Synthesis with Unposed Imagery",
    "link": "https://arxiv.org/pdf/2310.03704.pdf",
    "upvote": "7",
    "text": "Pose-Free Generalizable Rendering Transformer\nZhiwen Fan1*, Panwang Pan2*, Peihao Wang1, Yifan Jiang1,\nHanwen Jiang1, Dejia Xu1, Zehao Zhu1, Dilin Wang3, Zhangyang Wang1\n1University of Texas at Austin, 2ByteDance 3Meta\n{zhiwenfan,atlaswang}@utexas.edu\nImages with\nunknown poses\n\u2026\nGeneralizable\nNeRF\nSfM\nZero-shot novel view synthesis,\non LLFF datasets.\nPF-GRT\n(Ours)\nZero-shot novel view synthesis,\non Shiny datasets.\nPoses\nZero-Shot View Synthesis\nFigure 1. Inference pipeline of our Pose-Free Generalizable Rendering Transformer (PF-GRT), which facilitates novel view synthesis\nwithout the need for computing camera poses (left). We present zero-shot (generalizable) rendering results on various datasets (right).\nAbstract\nIn the field of novel-view synthesis, the necessity of know-\ning camera poses (e.g., via Structure from Motion) be-\nfore rendering has been a common practice.\nHowever,\nthe consistent acquisition of accurate camera poses re-\nmains elusive, and errors in pose extraction can adversely\nimpact the view synthesis process. To address this chal-\nlenge, we introduce PF-GRT, a new Pose-Free framework\nfor Generalizable Rendering Transformer, eliminating the\nneed for pre-computed camera poses and instead leverag-\ning feature-matching learned directly from data. PF-GRT\nis parameterized using a local relative coordinate system,\nwhere one of the source images is set as the origin. An\nOmniView Transformer is designed for fusing multi-view\ncues under the pose-free setting, where unposed-view fu-\nsion and origin-centric aggregation are performed.\nThe\n3D point feature along target ray is sampled by project-\ning onto the selected origin plane. The final pixel intensi-\nties are modulated and decoded using another Transformer.\nPF-GRT demonstrates an impressive ability to generalize\nto new scenes that were not encountered during the train-\ning phase, without the need of pre-computing camera poses.\nOur experiments with zero-shot rendering on the LLFF,\nRealEstate-10k, Shiny, and Blender datasets reveal that it\nproduces superior quality in generating photo-realistic im-\nages. Moreover, it demonstrates robustness against noise\nin test camera poses.\nCode is available at https://\n*Equal Contribution.\nzhiwenfan.github.io/PF-GRT/.\n1. Introduction\nNovel view synthesis, as demonstrated by recent works [23,\n37, 40, 43, 48], has showcased the capability to generate\nnew views on unseen scenes in a feed-forward manner. De-\nspite their effectiveness, the prerequisite of camera poses\nfor each view to offer explicit geometric priors between\nscene structures and 2D pixels is a common practice uti-\nlizing Structure from Motion (SfM) [34] before rendering.\nHowever, accurate camera poses not only complicate the\nsynthesis pipeline but also restrict applicability in scenar-\nios where precise camera information is unavailable or dif-\nficult to obtain.\nIn some cases, inaccurate pose estima-\ntion propagates its error to the renderer, adversely reduc-\ning the synthesized image quality. One could bypass the\ndemand for camera poses by adopting only a single image\nto learn generalizable NeRFs (e.g., PixelNeRF [48]), and\nrender the target image from the constructed feature vol-\nume. On the other hand, Scene Representation Transformer\n(SRT)[32] and RUST[33] have pioneeringly explored the\nrepresentation of multiple images as a \u201cset latent scene rep-\nresentation\u201d and generate novel views even in the presence\nof flawed camera poses or without any pose information.\nHowever, these works still face challenges: scene recon-\nstruction under a single input is highly ill-posed and fails\neasily in in-the-wild scenes, while the latent representa-\ntion results in blurred rendering outcomes with a lower res-\narXiv:2310.03704v3  [cs.CV]  27 Dec 2023\n(a). Epipolar-based Tranformer, with knowing poses.\n(b). OmniView Transformer, without knowing source poses.\nTarget View I0\n(Relative [R|t] to origin)\nSource Image I1\n(selected as origin)\nTarget View I0\n(To be Rendered)\nSource Image I1\n(pose is known)\nSource Image I2\n(pose is known)\nSource Image I3\n(pose is known)\nSource Image I2 to IN\n\uff08Pose is Unknown\uff09\n\u2026\nSource Image I1\n(selected as origin)\nPose-free Cross-view Aggregation\nSource Image Feature\nPooling\nEpipolar line\nSampled 3D points\nCandidate 3D points\nUnposed view fusion\nOrigin-centric aggregation\nFigure 2. Epipolar Transformer vs. OmniView Transformer. The Epipolar Transformer requires camera poses to search for candidate\nmatching along the epipolar line. The OmniView Transformer finds the correspondences using global feature matching by using Unposed\nView Fusion and Origin-Centric Aggregation, which does not rely on camera poses.\nolution (e.g., 128\u00d7128 in SRT), limiting their applicabil-\nity in achieving photorealistic rendering. In this work, we\ntake a step forward, unifying global feature matching with\nTransformer [41] and the image-based rendering (IBR) [15]\nfor photo-realistic novel view synthesis without the need\nfor camera poses and per-scene optimization, all accom-\nplished in a feed-forward pass.\nOur proposed framework, PF-GRT, is parameterized by a\nlocal coordinate system, where one unposed view is used as\na starting point (origin), and the target view to be rendered\nis defined by a relative transformation. Another key design\nelement comes from using Transformer for global feature\nmatching and fusing the multi-view cues: the proposed Om-\nniView Transformer sequentially performs Unposed View\nFusion and Origin-Centric Aggregation without the require-\nment of camera poses for computing epipolar constraints.\nPixel-aligned 3D feature along each target ray is sampled\nby projecting onto the origin plane of the local system. The\nfinal pixel intensities along each target ray are modulated\nand decoded using another transformer, taking into account\nall warped 3D point features.\nIn training, PF-GRT is optimized using a large-scale\ndataset [43] with calibrated multi-view images; the start-\ning view (origin) is randomly selected, and the ground-truth\nposes are converted into relative for applying the photomet-\nric losses. In inference, our method selects any unposed\nsource image as origin (the root of relative coordinate sys-\ntem), and can render any free-view by specifying a relative\ntransformation relative to the origin. This is achieved with-\nout the pre-computed poses in an unseen scene.\nComprehensive\nexperiments\non\ndiverse\nreal-world\nscenes, including LLFF, RealEstate10K, Shiny, and object-\nlevel Blender datasets demonstrate our approach achieves\nsuperior zero-shot rendering quality (see Figure 1), sur-\npassing prior pose-free generalizable NeRFs with a large-\nmargin. Additionally, our method demonstrates superior ro-\nbustness against camera pose noise in new scenes.\nOur major contributions are encapsulated as follows:\n\u2022 We introduce a new formulation for generating novel\nviews, by unifying the pixel-aligned feature of the target\npixel with Image-based Rendering (IBR), eliminating the\nneed for pose annotations among test scenes, and stream-\nlining the rendering process.\n\u2022 We propose an efficient OmniView Transformer to aggre-\ngate multi-view features with adaptive weights, by broad-\nening the epipolar constraint to encompass all source pix-\nels. A source-conditioned modulation layer is integrated\nto handle projection occlusions, and pixel intensities are\ndecoded by another Transformer.\n\u2022 Trained with large-scale multi-view datasets, PF-GRT\nis able to render photo-realistic novel views in unseen\nscenes in a feed-forward pass.\nIt significantly outper-\nforms other pose-free, generalizable neural rendering\ntechniques in diverse datasets.\n2. Related Works\nGeneralizable Neural Scene Representations\nBuilding\ngeneralizable feature volumes dates back to Neural Vol-\numes [24], wherein an encoder-decoder framework is\nadopted to create a feature volume. Later on, NeRF [27]\nand its follow-ups [1\u20133, 9, 11, 13, 16, 17, 22, 28, 30, 35,\n39, 42, 47] have emerged as effective scene representations.\nHowever, their costly per-scene fitting nature constitutes\na significant limitation. Generalizable NeRFs endeavor to\ncircumvent time-consuming optimization by conceptualiz-\ning novel view synthesis as a cross-view image-based in-\nOrigin View\n(Coord. Origin)\nRandomly\nTarget View\n(To be Rendered)\nUnposed Source Views\nView\nSelector\nAll Source Views\nOrigin View\nRotation\nScore\nTranslation\nScore\nView Selector\nFew\nSource Views\nStop\nGrad\nRelative\nMulti-scale Feature Extraction\nUnposed View Fusion\nOrigin-Centric Agg.\nOrigin-Centric Agg.\n\u2026\nOmniView Transformer\nProjection & Modulation\nOrigin Plane\nFeature-wise\nModulation\nConditional\n-\nGT\nRay\nDecoder\nEncoder\nPhotometric Loss\nPixel-aligned 3D Point\nFigure 3. The overall pipeline of the proposed PF-GRT. Given unposed source images with a specified origin view, PF-GRT selects\na limited number of source images closest to the origin view. Multi-scale 2D features are extracted, and the OmniView Transformer is\nused for aggregating features from the unposed sources towards the origin plane. The 3D point feature on the target ray is initialized via\nprojection, and the final pixel intensities are modulated and then decoded using another Transformer.\nterpolation problem.\nNeuRay [23], IBRNet [43], MVS-\nNeRF [5], and PixelNeRF [48] assemble a generalizable\n3D representation using features aggregated from observed\nviews. GPNR [37] and GNT [40] enhance the novel view\nrenderings with a Transformer-based aggregation process.\nA view transformer aggregates image features along epipo-\nlar lines, while a ray transformer combines coordinate-wise\npoint features along rays through the attention mechanism.\nPose-free NeRFs\nNumerous efforts have been exerted to\ndiminish the necessity for calibrated camera poses during\nNeRF training. NeRF- - [44] makes an early endeavor to op-\ntimize camera parameters with NeRF training for forward-\nfacing scenes simultaneously. BARF [20] refines NeRFs\nfrom imperfect (or even unknown) camera poses via coarse-\nto-fine registration, while GARF [6] incorporates Gaussian\nactivations. NoPe-NeRF [4] employs monocular depth pri-\nors to restrict the relative poses between consecutive frames.\nEfforts have also been made to expand generalizable NeRF\ntoward unposed images. PixelNeRF [48] builds a generaliz-\nable feature volume that estimates novel views from single-\nview observation, which can be unposed. MonoNeRF [12]\nassumes that the camera motion between adjacent frames is\nsmall, and disentangles the predicted depth from the camera\npose. It decodes another input frame based on these inputs\nwith a volume rendering representation to formulate the re-\nconstruction loss. FlowCam [36] explicitly estimates the\ncamera pose of the video frame by fitting a 3D scene flow\nfield with the assistance of a pretrained 2D optical flows\nmodel. Generalizable methods, SRT [32] and RUST [33],\ninfer a set-latent scene representation from a set of unposed\nimages to synthesize novel views. The concurrent work,\nLeap [18], constructs neural volume with feature aggrega-\ntion, designed for pose-free object-level novel view synthe-\nsis.\nTransformers as Neural Scene Representations\nTrans-\nformers are extensively utilized to represent scenes. IBR-\nNet [43] processes points sampled from rays using MLP\nand estimates density via a transformer. NeRFormer [31]\nemploys attention modules to craft feature volumes with\nepipolar geometry constraints. LFNR [38], GPNR [37] and\nGNT [40] introduces a two-stage transformer-based model\nto accumulate features along epipolar lines and aggregate\nfeatures along reference views to produce the color of target\nrays. SRT [32] and RUST [33] infer a set-latent scene rep-\nresentation via a vision transformer and parameterize light\nfields by attending into the scene representation for novel\nview renderings.\n3. Method\nOverview\nGiven an unposed image set with known in-\ntrinsics {(Ii \u2208 RH\u00d7W \u00d73, Ki \u2208 R3\u00d73)}N\ni=0 of a scene,\nPF-GRT synthesizes new views in a single forward pass.\nSpecifically, the relative coordinate system is constructed\nwith one unposed view as the origin, and the target view\nis defined using relative transformation.\nThe OmniView\nTransformer performs Unposed View Fusion and Origin-\nCentric Aggregation towards the origin view. Pixel-aligned\n3D point feature on the target ray is initialized by projecting\nonto the origin plane. The pixel intensities are subsequently\nmodulated, conditioned on the origin plane, and decoded\nusing another Transformer. To determine the most suitable\nsource views for aggregation, a view selector is designed to\nidentify the closest K views (K \u2264 N) relative to the origin\nview, based on global feature distances. An overview of our\npipeline is illustrated in Figure 3.\n3.1. OmniView Transformer\nWe propose to use Transformer for learning global fea-\nture matching across unposed source views to find the best\nmatching feature for rendering a target ray.\nUnposed View Fusion\nHaving extracted the multi-view\nfeature from a shared 2D encoder, tokens are initialized by\nsubdividing the feature map into M \u00d7M patch grids, which\nenables generalization to various datasets under different\nresolutions. The Unposed View Fusion (UVF) module ag-\ngregates the features from each unposed view to capture\nlong-range global context. Specifically, the UVF layer per-\nforms self-attention on the token set F \u2208 {f 1, ..., f M\u00d7M}\nof each unposed source view, which consists of M \u00d7 M\ndiscrete tokens:\nF = FFN(Attention(F, F))\n(1)\nHere, we utilize attention layers to retrieve relevant infor-\nmation within the given view and update the view-specific\nfeature to be embedded with global context information.\nOrigin-Centric Aggregation\nNext, we propose propa-\ngating the multi-view cue from unposed views toward the\norigin view (I0), which acts as the root of the constructed\nlocal coordinate system for rendering.\nThe proposed\nOrigin-Centric Aggregation (OCA) performs the propaga-\ntion sequentially for each source view via:\nF0 = FFN(Attention(F0, Fi)),\ni \u2208 N\n(2)\nThe amalgamation of source features toward the origin view\nenriches the multi-view information in the updated origin\nfeature plane. The pseudocode implementation of the Om-\nniView Transformer is provided in the Appendix.\n3.2. Target Ray Initialization and Decoding\nTo render a pixel c = (o, d) in target view, the relative\ntransformation from origin plane is specified. Pixel-aligned\n3D points p \u2208 {p1, p2, ..., pM} are uniformly sampled be-\ntween the near and far plane. Point feature are tokenized,\nmodulated and decoded into pixel intensities.\nTokenizing 3D Points on Target Ray\nThe uniformly\nsampled points, p are projected onto the feature maps of the\norigin view, culminating in the initialized point features:\n\u02c6p = K0(R0(p) + t0)\n(3)\nF (F0, \u02c6p) = Interpolation(\u03a0(\u02c6p, F0))\n(4)\nHere, R0 and t0 symbolize the relative rotation and trans-\nlation between the target view to the origin view, \u03a0 repre-\nsents the projection function, and F0 is the extracted fea-\nture maps on origin view, formulating the pixel-aligned 3D\npoint-wise feature \u02c6p \u2208 { \u02c6p1, \u02c6p2, ...,\n\u02c6\npM}.\nOrigin-Conditioned Modulation Layer\nWhen project-\ning points into regions of the scene that are obscured from\nthe camera\u2019s viewpoint, or where erroneous feature match-\nings from unposed source views occur, the projected points\noften become occluded or missed, degrading the rendered\nquality. To counteract this issue, an origin view-conditioned\nlayer is introduced to modulate the point features through an\naffine transformation, utilizing the extracted global coher-\nence derived from the origin view. Formally, a Feature-wise\nLinear Modulation (FiLM) layer [29] is utilized to scale and\nshift the point feature \u02c6p following:\n\u03b3\u03c4, \u03b2\u03c4 = MLP\u03b3(GAP(F0)), MLP\u03b2(GAP(F0))\n(5)\n\u02c6p := \u03b3\u03c4 \u02c6p + \u03b2\u03c4\n(6)\nThis modulation is formulated by two groups of parameters,\n\u03b3\u03c4 and \u03b2\u03c4, resulting in the modulated point-wise feature \u02c6p.\nGAP denotes Global Average Pooling.\nRay-based Rendering with Transformers\nRecent re-\nsearch advocates the utilization of the Transformer archi-\ntecture to adaptively learn the blending weights along the\nray for each point, augmenting both expressiveness [40]\nand generalization [37]. Instead of using the simplified ver-\nsion of volume rendering [25] from NeRF [27], we employ\nsuch an \u201cattention\u201d mechanism to determine the aggrega-\ntion weights for each sampled point feature in a data-driven\nway, to decode the final pixel intensity:\nc(o, d) = MLP \u25e6 Mean \u25e6 Attention(\u02c6p, \u02c6p)),\n(7)\n3.3. View Selection via Feature Similarity\nView selection aims to select efficiently a few source im-\nages, which is the nearest to the origin view, to reduce the\ncomputational redundancy when performing OmniView at-\ntention. Specifically, a network is designed to extract multi-\nscale features [14] from all source images, and multiple de-\ncoding heads are devised for regressing the relative rotation\nand translation scores between I0 and each source image\n{Ii, i \u0338= 0}. In particular, four decoding heads are utilized\nfor estimating the three normalized relative angles and the\ndistance value between the two images. Top K images are\nselected out of the N (K \u2264 N).\n3.4. Training and Inference\nDuring the training phase, the view selector identifies the\nnearest K source images from the N unposed source im-\nages. This selection procedure is guided by a specified loss\nfunction that operates based on the online-computed rela-\ntive angle and distance values of each image pair.\n\u0398\u2217\ns = arg min\n\u0398 (\u2225\u2220(I0, Ii) \u2212 \u2220gt\u22252\n2 + \u2225d(I0, Ii) \u2212 dgt\u22252\n2).\n(8)\nMethods\nReal Forward-facing(LLFF)\nRealEstate10K Datasets\nShiny Datasets\nNeRF Synthetic Objects\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPixelNeRF [48]\n8.379\n0.313\n0.643\n9.008\n0.407\n0.503\n9.025\n0.285\n0.607\n7.105\n0.565\n0.413\nPixelNeRF-ft\n10.234\n0.346\n0.593\n11.115\n0.462\n0.470\n10.130\n0.304\n0.555\n7.561\n0.569\n0.406\nUpSRT [32]\n16.669\n0.541\n0.418\n16.833\n0.593\n0.314\n15.117\n0.471\n0.428\n15.725\n0.814\n0.205\nLEAP [18]\n9.220\n0.228\n0.694\n11.336\n0.527\n0.459\n9.659\n0.257\n0.668\n18.020\n0.831\n0.187\nOurs\n22.728\n0.778\n0.180\n24.208\n0.789\n0.190\n19.192\n0.604\n0.338\n22.832\n0.835\n0.134\nTable 1. Quantitative Comparison in a Generalizable Pose-Free Setting. PF-GRT outperforms previous pose-free methods that utilize\nboth single-view feature volume (PixelNeRF)[48] and multi-view \u201cset of latents\u201d(UpSRT)[32], in addition to aggregation to neural volume\n(LEAP) [18]. Owing to the effective OmniView Transformer and the IBR formulation, our method can generate novel views with the\nhighest quality. We color each row as best , second best , and third best .\nGround-Truth\nUpSRT\nLEAP\nPixelNeRF-ft\nOurs\nFigure 4. Qualitative Comparison Under a Generalizable Pose-Free Setting. Single-view PixelNeRF [48] introduces artifacts within\nthe constructed feature volume and shows limited capacity in handling complex scenes. Multi-view SRT [32] fails to render sharp details\nin scenes with rich textures. LEAP [18] can generalize to object-level datasets, but it fails to scale up to scene-level synthesis. In contrast,\nour method more accurately recovers fine details through global correspondence matching and image-based rendering.\nThe rest part of the model is optimized utilizing the L2\ndistance between the rendered target pixels and the corre-\nsponding ground-truth pixels, as exemplified by:\n\u0398\u2217\nt = arg min\n\u0398 \u2225C(ri|\u0398, \u03b8, x) \u2212 Cgt\u22252\n2 .\n(9)\nNote that the training requires ground-truth camera poses\nfor calculating the score for the view selector, and the abso-\nlute poses are converted into relative poses for supervising\nthe model training. For inference on unseen scenes, there is\nno reliance on any pose estimator. Instead, target views are\nspecified based on a relative transformation from the origin\nview, which can be selected arbitrarily. Other source views\nare determined by the view selector.\n4. Experiments\n4.1. Implementation Details\nDatasets.\nWe train PF-GRT on a large-scale dataset, span-\nning from \u223c 1000 Google Scanned Objects [8] to real-world\ndatasets: sparsely-sampled RealEstate10K [50], 100 scenes\nfrom the Spaces datasets [10], and 102 real-world scenes\ncaptured using handheld phones [26, 43]. We evaluate the\nzero-shot accuracy on diverse datasets without finetuning:\n\u2022 LLFF Datasets[26] contain eight real-world forward-\nfacing scenes, with a resolution of 504 \u00d7 378.\n\u2022 RealEstate10K[50]. The test set consists of four real-\nworld scenes at a resolution of 176\u00d7144.\n\u2022 Shiny Datasets[45] contain eight scenes with challeng-\ning lighting conditions, with evaluations conducted on 8\u00d7\ndownsampled images.\n\u2022 Blender Datasets, the widely-adopted synthetic data cre-\nated by NeRF[27], encompasses eight objects with a\ntested resolution of 400\u00d7400.\nTraining and Inference Details.\nPF-GRT is trained end-\nto-end, with the gradient stopping operation after the view\nselector. The Adam optimizer is employed to minimize the\ntraining loss for the model. The learning rate decreases ex-\nponentially over training steps with a base of 10\u22123. The\ncomprehensive training encompasses 250,000 steps, with\n4,096 randomly sampled rays during each iteration.\nIn\nboth training and inference phases, 128 points are uniformly\nsampled along each target ray. Grid number M is set as 7\nto balance efficiency and accuracy.\nBaselines.\nWe evaluate pose-free novel view synthesis\nwith UpSRT [32], single-view based PixelNeRF [48], and\nthe generalizable LEAP [18]. We re-trained UpSRT [32]1\non the same datasets as ours for 400K iterations until fully\nconverged. We report both the vanilla PixelNeRF from their\nprovided checkpoint, and the finetuned version on the same\ndataset as ours.\nWe use the checkpoint from LEAP[18]\n1https://github.com/stelzner/srt\ntrained on OmniObject datasets [46]. We also test robust-\nness against camera pose noises in source views with other\ngeneralizable NeRFs [23, 40, 43]: we synthetically perturb\nthe camera poses with additive Gaussian noise on both ro-\ntation and translation vectors, akin to BARF [20].\nMetrics\nThe rendering quality is reported using metrics\nsuch as peak signal-to-noise ratio (PSNR), structural simi-\nlarity (SSIM), and perceptual similarity via LPIPS [49].\n4.2. Generalizable Pose-free Novel View Synthesis.\nTable 1 showcases that PF-GRT surpasses the best-\nperforming multi-view pose-free SRT [32], LEAP [18],\nand single-view PixelNeRF on all unseen test scenes,\nencompassing synthetic datasets, forward-facing datasets,\nRealEstate10K datasets and Shiny datasets with complex\nlighting conditions. This validates the efficacy of the de-\nsigned OmniView Transformer for effective view aggrega-\ntion under pose-free setting, and the image-based rendering\nto generate detailed textures. We present qualitative results\nin Figure 4, wherein it can be observed that the \u201clatent\u201d rep-\nresentation in SRT overlooks the image details, and Pix-\nelNeRF struggles under complex scenes using single-view\nfeature volume-based neural rendering. LEAP has difficul-\nties to generalize to scene-level test cases. See the video in\nsupplementary materials for detailed comparisons.\n4.3. Robustness against Noisy Poses.\nMulti-view images captured in the real world typically\nrequire a pre-processing step (e.g., COLMAP [34]) to\ncompute the poses.\nHowever, this computation is slow,\nespecially when the number of source images is large,\nand often contains errors [20].\nWe examine the current\nbest-performing generalizable NeRFs against noisy cam-\nera poses in the tested source views, a practical concern.\nFollowing [20], who apply additive Gaussian perturbation\nat different levels to the camera poses, we directly test\nthe trained generalizable model with the provided check-\npoint to assess robustness. It is evident from Figure 5 that\nall generalizable methods suffer from noisy camera poses,\nwith significant degradation in performance even under a\nsmall amount of noisy calibration (\u03c3=0.003). On the other\nhand, our framework PF-GRT, which generates new views\nin a feed-forward pass without estimating camera poses,\ndemonstrates stability in rendering quality. This stability is\nattributed to our approach of not relying on absolute camera\nposes for cross-view feature aggregation but instead learn-\ning the cross-view feature mapping from large-scale train-\ning using the OmniView Transformer. Consequently, the\nsource view pose noises do not affect the rendering quality.\nFigure 6 visualizes the effect of different noise levels on\nsource views in the evaluation, illustrating that subtle noise\nsignificantly decreases the rendering quality. Quantitative\nMethods\nReal Forward-facing(LLFF)\nNeRF Synthetic Objects\nPSNR \u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2191\nIBRNet [43]\n21.395\n0.686\n0.303\n20.027\n0.813\n0.145\nNeuRay [23]\n21.520\n0.681\n0.303\n21.424\n0.832\n0.135\nGNT [40]\n21.341\n0.682\n0.307\n20.554\n0.830\n0.139\nOurs\n22.728\n0.778\n0.180\n22.832\n0.835\n0.134\nTable 2. Quantitative Comparison of Robustness to Noisy Poses in source views. The table presents a performance comparison between\nPF-GRT and various generalizable NeRFs using the NeRF-Synthetic [27] and LLFF datasets [26], where both rotation and translation\nmatrices are perturbed with \u03c3 = 0.003. PF-GRT showcases its robustness in handling pose perturbations in rendered views. We color each\nrow as best , second best , and third best .\nGT\nGNT\nNeuRay\nOurs\nIBRNet\nFigure 5. Visualizations of Different Methods Against Noisy Poses on Source Images When Rendering. All adopted generalizable\nNeRFs suffer from noisy camera poses in source views at evaluation, even with very mild perturbation (e.g., \u03c3=0.003).\nFigure 6. Visualizations of generalizable NeRFs with different\nnoise level. We increases the noise from (\u03c3=0.003) to (\u03c3=0.1),\nmethods that rely on camera poses for cross-view aggregation are\ndecreasing in rendering quality. Our method demonstrates the ro-\nbustness against test pose noises, as we perform global feature\nmatching toward the origin view, instead of using epipolar con-\nstraints.\nresults, with a noise level of 0.003 on both real forward-\nfacing and synthetic objects datasets, are presented in Ta-\nble 2.\n4.4. Ablation Study\nWe now execute a series of ablation studies regarding\nour module choice on the LLFF datasets [26] and aver-\nage the metrics across all scenes. The evaluation begins\nwith the use of the \u201corigin view\u201d directly for evaluation,\nand subsequently, we incrementally integrate the proposed\ntechniques (Pixel-aligned Feature, OmniView Transformer,\nConditional Modulation Layer) in this study.\nPixel-aligned Feature?\nWe study utilizing the relative\ncoordinate system between the origin view and the target\nviews, and obtain the 3D point feature along the target ray\nby directly projecting onto origin feature plane. The final\npixel intensities of target ray are regressed using ray atten-\ntion. As we can see in the second row of Table 3 and the\nthird column of Figure 7, the missing of multi-view cues\nresults in an inferior rendering quality.\nOmniView Transformer?\nWe further employ the Om-\nniView Transformer for Unposed-View Fusion and Origin-\nCentric Aggregation, using a data-driven attention mecha-\nnism focused on the origin view. This design significantly\nimproves quantitative metrics, with an increase in PSNR\nOrigin View\nProjection only\nProjection+OmniView\nFull model\nTarget (GT)\nFigure 7. Visualization for Ablation Study. We visualize the \u201corigin view\u201d (1st column), \u201cground-truth target view\u201d (2nd column),\nrendered images using projection and ray attention (3rd column), the incorporation of the OmniView Transformer (4th column), and our\nfull model (last column).\nPixel-aligned\nOmniView\nModulation\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFeature\nTransformer\nFiLM-Layer\n\u2717\n\u2717\n\u2717\n14.198\n0.355\n0.407\n\u2713\n\u2717\n\u2717\n16.519\n0.412\n0.400\n\u2713\n\u2713\n\u2717\n22.287\n0.740\n0.197\n\u2713\n\u2713\n\u2713\n22.728\n0.778\n0.180\nTable 3. Ablation study of the proposed components in PF-\nGRT. We start by constructing the relative coordinate system\nbetween origin and target view, utilizing pixel-aligned feature\n(2nd row) shows a better metrics than the baseline that directly\nevaluating the origin view (1st row). The introduction of Om-\nniView Transformer (3rd row) significantly improve the PSNR\nfrom 16.519dB to 22.287dB. Additionally, the feature-wise mod-\nulation layer further improve the PSNR to 22.728dB (last row).\nRandom?/GT?/\nDisentangled\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nSelect Acc.\u2191\nView Selector?\nDecoder Heads\nRandom\nN/A\n17.841\n0.505\n0.390\n0.017\nView Selector\n\u2717\n22.243\n0.756\n0.198\n0.688\nView Selector\n\u2713\n22.728\n0.778\n0.180\n0.731\nGT\nN/A\n24.275\n0.822\n0.135\n1.000\nTable 4. Ablation on the View Selector. The incorporation of\nthe view selector finds the best matched source images, enabling\nPF-GRT to effectively aggregate cross-view features. The em-\nployment of disentangled decoder heads for each axis of rota-\ntion and translation vector further improves the view selection ac-\ncuracy and, thus, the synthesis quality. PF-GRT, which utilizes\nground-truth poses for view selection, is listed in the last row.\nfrom 16.519 to 22.287, as shown in the third row of Ta-\nble 3. Additionally, Figure 7 demonstrates that the visual\nquality is also improved by integrating missing information\nfrom other source views.\nConditional Modulation Layer?\nConditioned on the\nstatistics of the origin view, the 3D point-wise features on\nthe target ray are affine-transformed using learnable param-\neters. This adjustment aids in filling in missing regions in\noccluded areas (see the second-last column of Figure 7) es-\npecially when cross-view aggregation is imprecise and the\nmodulation layer can guide the projection toward a plausi-\nble solution. The last row of Table 3 shows that the modu-\nlation layer improves SSIM from 0.74 to 0.778.\nAnalysis on the Viewpoint Selector.\nInitially, we assess\nthe use of random source view selection in the evaluation,\nwhere the selected source views may not be ideal for fea-\nture aggregation, resulting in a 0.505 SSIM metric (1st row\nof Table 4). Utilizing the selector to regress the averaged\nrelative viewing direction and distance directly elevates the\nSSIM to 0.756, a significant improvement. Employing sep-\narate decoder heads to disentangle the similarity score for\nthe three axes of relative rotation and translation distance\nfurther enhances the quantitative metrics to 0.778 (\u2191 0.02 in\nSSIM). Additionally, we illustrate the use of ground-truth\nposes to identify the nearest source views with viewing di-\nrections most analogous to the \u201corigin view,\u201d serving as the\nupper bound of the view selector.\nPose-Free Generalizable Rendering Transformer\nSupplementary Material\nA5. Motivation of OmniView Transformer\nAs previously mentioned in the main paper, knowing the\nmulti-view camera poses enables the framework design to\nsearch for correspondences along the epipolar line. As illus-\ntrated in Figure A8(a), we consider a simple case with sev-\neral source views. For the pixel to be rendered in the target\nview, epipolar attention used in [37, 40] builds correspon-\ndences among the target ray and epipolar line of neighbor-\ning source images. However, without knowing the poses,\nwe are unable to build such a search pattern, and thereby,\nwe resort to utilizing an attention mechanism to search over\nall source pixels toward the origin view (the relative co-\nordinate system is constructed), which is the origin of the\nrelative coordinate system. We propose the use of a CNN\nnetwork to extract multi-scale feature maps. Subsequent to\nthe CNN encoder, these extracted feature maps from source\nviews (Ii \u2208 RH\u00d7W \u00d73)i = 0K are subdivided into M \u00d7 M\ngrids within each view, facilitating the model to be agnos-\ntic to diverse image resolutions (as shown in Figure A8\n(b)).\nThe Unposed View Fusion, which performs intra-\nimage long-range global context aggregation, is designed to\nretrieve relevant information within the source views, while\nthe Origin-Centric Aggregation focuses on capturing cross-\nrelationships across these two views. This way, inter-image\nfeature interaction between images is facilitated.\nA6. Implementation Details\nMemory-Efficient OmniView Transformer\nThe most\nstraightforward method for aggregating the initially pro-\njected 3D point feature involves building cross-attention\nbetween the target 3D point feature and all source pixels.\nHowever, this approach is intractable as it cannot scale to\nhigh-resolution input images and a large number of source\nviews. Therefore, we propose to leverage the 8\u00d7 downsam-\npled CNN features and pool them into a fixed number of\n2D grids (here, we use a 7\u00d77 grid) for each view. Conse-\nquently, our design is agnostic to input resolution, allowing\nattention to be performed in a patch-wise manner. Never-\ntheless, during training, the sampled ray is typically large\n(e.g., 4096 in PF-GRT), incurring 4096 \u00d7 128 sampled\npoints in each iteration. The cross-attention among sampled\npoints and tokenized patches in the source views remains\nintractable. Therefore, akin to the [CLS] token in Vision\nTransformer [7], we employ the cross-attention mechanism\nto propagate multi-view information in source views toward\nthe origin view. We then project the sampled 3D points onto\nthe starting view, ensuring an efficient implementation re-\ngardless of the number of source views used. Please refer\nto the PyTorch-like pseudo-code Algorithm 1 for a detailed\nexplanation.\nAlgorithm 1 OmniView Transformer: PyTorch-like Pseu-\ndocode\npt \u2192 points coordinate in target view(Nrays, Npts, 3)\nX0 \u2192 flattened tokens in origin(1, Npatch, C)\n{Xi}K\ni=1 \u2192 flattened tokens in source views(K, Npatch, C)\nf t \u2192 projected points feature(Nrays, Npts, D)\nfQ, fK, fV , frgb \u2192 functions that parameterize MLP layers\nfor 0 \u2264 i \u2264 K do\n\u25b7 Unposed View Fusion\nQ = fQ(Xi)\nK = fK(Xi)\nV = fV (Xi)\nA = matmul(Q, KT )/\n\u221a\nD\nA = softmax(A, dim = \u22121)\nXi = matmul(A, V )\nend for\nfor 1 \u2264 i \u2264 K do\n\u25b7 Origin-Centric Aggregation\nQ = fQ(Xi)\nK = fK(X0)\nV = fV (X0)\nA = matmul(Q, KT )/\n\u221a\nD\nA = softmax(A, dim = \u22121)\nX0 = matmul(A, V )\nend for\nfor 0 \u2264 i \u2264 (Nrays \u00d7 Npts) do\n\u25b7 Point-wise projection\nf i\nt = interp.(proj.(modulation(pi\nt), X0))\nend for\nfor 0 \u2264 i \u2264 Nrays do\n\u25b7 Ray attention\nQ = fQ(f i\nt)\nK = fK(f i\nt)\nV = fV (f i\nt)\nA = matmul(Q, KT )/\n\u221a\nD\nA = softmax(A, dim = \u22121)\nf i\nt = matmul(A, V )\nend for\nRGB = frgb(meanNpts\ni=1(f i\nt))\nA7. Additional Experiments\nScene-wise Quantitative Metrics\nTable A5, Table A6\nTable A7, and able A8 include a scene-wise quantitative\n(a). Epipolar-based Tranformer, with knowing poses.\n(b). OmniView Transformer, without knowing source poses.\nTarget View I0\n(Relative [R|t] to origin)\nSource Image I1\n(selected as origin)\nTarget View I0\n(To be Rendered)\nSource Image I1\n(pose is known)\nSource Image I2\n(pose is known)\nSource Image I3\n(pose is known)\nSource Image I2 to IN\n\uff08Pose is Unknown\uff09\n\u2026\nSource Image I1\n(selected as origin)\nPose-free Cross-view Aggregation\nSource Image Feature\nPooling\nEpipolar line\nSampled 3D points\nCandidate 3D points\nUnposed view fusion\nOrigin-centric aggregation\nFigure A8. Illustration of Epipolar Attention and OmniView Attention. The figure is the same with Figure 2 in the main draft.\nresults presented in the main paper. Our method quanti-\ntatively surpasses both the generalizable single-view based\nmethod PixelNeRF [48] (and PixelNeRF with finetuning)\nand multi-view based method UpSRT [32] trained with\n4000k iterations and LEAP [18] from the pre-trained model\nweights. We also include videos to demonstrate our results\nin the attached video.\nMore Free-View Rendering\nWe provide the visualiza-\ntion of multiple rendered novel viewpoints for the real-\nworld dataset by interpolating between test views. This vi-\nsualization, illustrated in Figure A9, demonstrates the capa-\nbilities of our method in generating diverse viewpoints, of-\nfering insight into its performance and potential limitations\nin real-world scenarios.\nA8. Conclusion and Limitations\nWe present a novel framework, PF-GRT, for photo-realistic\nrendering from a sparse set of unposed images. PF-GRT\nconstructs a relative coordinate system to parameterize the\ntarget view. It adapts the OmniView Transformer for a pose-\nfree setting, effectively fusing the unposed source images,\nand aggregating multi-view cues toward the origin view via\na data-driven attention mechanism. PF-GRT enjoys the ad-\nvantages of global feature matching, and Image-Based Ren-\ndering (IBR) to produce state-of-the-art rendering quality in\ncomplex scenes. Moreover, PF-GRT can render new views\non unseen datasets without any scene-specific optimization\nand pre-computed camera poses, showcasing both the flexi-\nbility in pose annotations and robustness against noisy com-\nputed camera poses. Our approach also carries certain lim-\nitations inherited from these previously proposed methods\n(e.g., IBRNet [43]). For instance, while our method re-\nquires casting rays for rendering a pixel, this inevitably in-\ntroduces a computational rendering cost, similar to other\nMLP-based [2, 3, 27] and Transformer-based [37, 40, 43]\nneural rendering frameworks. Incorporating depth priors\nfrom the predictive multi-view depth to guide the point sam-\npling [21], or leveraging the Gaussian Splatting [19] tech-\nnique may mitigate this efficiency issue.\nTable A5. Comparison of PF-GRT with other pose-free generalizable novel view-synthesis methods on the forward-facing LLFF datasets\n(scene-wise).\nModels\nTrex\nFern\nFlower\nLeaves\nRoom\nFortress\nHorns\nOrchids\npixelNeRF\n8.266\n8.655\n8.234\n7.026\n8.872\n10.55\n7.743\n7.177\npixelNeRF-ft\n9.6914\n9.541\n11.9751\n7.8957\n11.7662\n12.729\n9.7231\n9.2533\nSRT\n16.383\n16.7918\n17.0056\n14.0108\n18.3335\n19.7896\n16.6579\n14.3807\nLEAP\n8.6914\n8.541\n10.9751\n6.8957\n10.7662\n11.729\n8.7231\n8.2533\nPF-GRT\n21.489\n21.847\n22.786\n17.725\n26.836\n27.261\n23.866\n16.139\n(a) PSNR\u2191\nModels\nTrex\nFern\nFlower\nLeaves\nRoom\nFortress\nHorns\nOrchids\npixelNeRF\n0.351\n0.326\n0.24\n0.127\n0.492\n0.418\n0.275\n0.161\npixelNeRF-ft\n0.3762\n0.3639\n0.3551\n0.1757\n0.4983\n0.5289\n0.3719\n0.222\nSRT\n0.8166\n0.7976\n0.8637\n0.801\n0.7821\n0.8849\n0.6413\n0.7349\nLEAP\n0.2596\n0.3175\n0.3435\n0.4334\n0.1388\n0.2476\n0.352\n0.4128\nPF-GTR\n0.798\n0.737\n0.773\n0.674\n0.848\n0.820\n0.804\n0.590\n(b) SSIM\u2191\nModels\nTrex\nFern\nFlower\nLeaves\nRoom\nFortress\nHorns\nOrchids\npixelNeRF\n0.618\n0.645\n0.658\n0.668\n0.603\n0.582\n0.669\n0.738\npixelNeRF-ft\n0.5496\n0.6075\n0.6335\n0.7234\n0.4288\n0.5376\n0.642\n0.7028\nSRT\n0.3754\n0.4158\n0.4337\n0.5559\n0.2397\n0.3417\n0.4471\n0.5375\nLEAP\n0.6508\n0.7514\n0.75\n0.7542\n0.5786\n0.7135\n0.673\n0.7877\nPF-GRT\n0.181\n0.208\n0.158\n0.285\n0.133\n0.136\n0.171\n0.312\n(c) LPIPS\u2193\nTable A6. Comparison of PF-GRT with other pose-free generalizable novel view-synthesis methods on the Shiny datasets (scene-wise).\nModels\nCD\nGiants\nLab\nSeasoning\nPasta\nCrest\nFood\npixelNeRF\n11.2911\n8.7536\n10.1085\n9.0397\n8.7596\n8.6769\n9.3571\npixelNeRF-ft\n12.5323\n9.7544\n11.0138\n9.0327\n9.6698\n9.3726\n10.3788\nSRT\n15.5747\n16.2062\n11.7957\n15.668\n13.5123\n12.6199\n14.4884\nLEAP\n10.4334\n8.919\n9.4274\n6.9765\n10.9673\n9.156\n10.15\nPF-GRT\n23.3704\n22.1177\n13.561\n22.8052\n19.1302\n14.0699\n19.1466\n(a) PSNR\u2191\nModels\nCD\nGiants\nLab\nSeasoning\nPasta\nCrest\nFood\npixelNeRF\n0.351\n0.326\n0.24\n0.127\n0.492\n0.418\n0.275\npixelNeRF-ft\n0.3762\n0.3639\n0.3551\n0.1757\n0.4983\n0.5289\n0.3719\nSRT\n0.8166\n0.7976\n0.8637\n0.801\n0.7821\n0.8849\n0.6413\nLEAP\n0.2596\n0.3175\n0.3435\n0.4334\n0.1388\n0.2476\n0.352\nPF-GTR\n0.798\n0.737\n0.773\n0.674\n0.848\n0.820\n0.804\n(b) SSIM\u2191\nModels\nCD\nGiants\nLab\nSeasoning\nPasta\nCrest\nFood\npixelNeRF\n0.6849\n0.5693\n0.6756\n0.6279\n0.6499\n0.8021\n0.6388\npixelNeRF-ft\n0.6628\n0.5492\n0.6704\n0.6265\n0.631\n0.7897\n0.6379\nSRT\n0.3308\n0.2361\n0.5706\n0.3832\n0.2284\n0.5667\n0.3395\nLEAP\n0.6449\n0.7132\n0.6774\n0.6807\n0.702\n0.7294\n0.6782\nPF-GRT\n0.1896\n0.2178\n0.5078\n0.2149\n0.187\n0.5232\n0.2912\n(c) LPIPS\u2193\nTable A7. Comparison of PF-GRT with other pose-free generalizable novel view-synthesis methods on the real Real-Estate datasets\n(scene-wise).\nModels\n0bcef\n000db\n000eb\n8516c\npixelNeRF\n8.541\n9.284\n10.084\n8.055\npixelNeRF-ft\n11.5395\n11.4856\n10.7908\n10.5445\nSRT\n17.1401\n17.3898\n16.261\n16.6377\nLEAP\n11.6061\n12.329\n11.3418\n11.2685\nPF-GRT\n24.760\n22.808\n23.487\n25.778\n(a) PSNR\u2191\nModels\n0bcef\n000db\n000eb\n8516c\npixelNeRF\n0.427\n0.380\n0.401\n0.373\npixelNeRF-ft\n0.5093\n0.4646\n0.48\n0.4381\nSRT\n0.6594\n0.5449\n0.5429\n0.6012\nLEAP\n0.528\n0.5261\n0.5256\n0.5291\nPF-GRT\n0.804\n0.750\n0.785\n0.816\n(b) SSIM\u2191\nModels\n0bcef\n000db\n000eb\n8516c\npixelNeRF\n0.507\n0.515\n0.486\n0.504\npixelNeRF-ft\n0.4958\n0.4694\n0.4518\n0.5018\nSRT\n0.3152\n0.2922\n0.3252\n0.3134\nLEAP\n0.4608\n0.4563\n0.4408\n0.4581\nPF-GRT\n0.174\n0.220\n0.193\n0.172\n(c) LPIPS\u2193\nTable A8. Comparison of PF-GRT with other pose-free generalizable novel view-synthesis methods on the NeRF Synthetic Datasets\n(scene-wise).\nModels\nChair\nDrums\nFicus\nHotdog\nMaterials\nMic\nShip\nLego\npixelNeRF\n7.2024\n7.7479\n7.4265\n6.9255\n7.0926\n7.0269\n6.3125\n7.1134\npixelNeRF-ft\n7.8914\n8.3051\n8.1891\n7.6405\n8.2315\n7.591\n7.2083\n8.0065\nSRT\n16.0348\n15.6772\n15.0571\n15.8147\n15.1039\n14.5086\n13.7598\n14.417\nLEAP\n17.466\n15.2234\n19.4337\n17.0554\n17.0797\n19.4747\n21.6511\n16.7814\nPF-GRT\n25.104\n19.192\n21.785\n22.712\n27.359\n25.14\n16.533\n21.019\n(a) PSNR\u2191\nModels\nChair\nDrums\nFicus\nHotdog\nMaterials\nMic\nShip\nLego\npixelNeRF\n0.6046\n0.5743\n0.6283\n0.6036\n0.5708\n0.6191\n0.4011\n0.5232\npixelNeRF-ft\n0.6435\n0.6334\n0.68\n0.6643\n0.6083\n0.6564\n0.4278\n0.5535\nSRT\n0.8166\n0.7976\n0.8637\n0.801\n0.7821\n0.8849\n0.6413\n0.7349\nLEAP\n0.8696\n0.7965\n0.9094\n0.831\n0.8049\n0.9089\n0.7531\n0.7598\nPF-GRT\n0.871\n0.835\n0.822\n0.875\n0.8\n0.881\n0.677\n0.817\n(b) SSIM\u2191\nModels\nChair\nDrums\nFicus\nHotdog\nMaterials\nMic\nShip\nLego\npixelNeRF\n0.3755\n0.4147\n0.3515\n0.4186\n0.4162\n0.372\n0.5294\n0.4321\npixelNeRF-ft\n0.3651\n0.4076\n0.3223\n0.3715\n0.3819\n0.3453\n0.5064\n0.4018\nSRT\n0.2024\n0.2121\n0.1883\n0.2318\n0.2148\n0.1942\n0.2787\n0.2505\nLEAP\n0.1666\n0.2132\n0.1184\n0.2169\n0.1896\n0.1111\n0.246\n0.2243\nPF-GRT\n0.083\n0.154\n0.098\n0.087\n0.174\n0.046\n0.402\n0.126\n(c) LPIPS\u2193\nTrajectories\nFree-view1\nFree-view2\nFree-view3\nFree-view4\nFree-view5\nFree-view6\nFigure A9. Visualization on more interpolated viewpoints. The visualized images showcase the efficacy of our method in handling\nvarious viewpoints by interpolating between test views on real-world datasets. See the video in supplementary materials for more detailed\ncomparisons.\nReferences\n[1] Benjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Jo-\nhannes Kopf, and Changil Kim.\nLearning Neural Light\nFields With Ray-Space Embedding Networks.\n2022\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 19787\u201319797, 2022. 2\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-Nerf: A Multiscale Representation for Anti-Aliasing\nNeural Radiance Fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5855\u2013\n5864, 2021. A2\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\nAnti-Aliased Neural Radiance Fields.\n2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 5460\u20135469, 2022. 2, A2\n[4] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and\nVictor Adrian Prisacariu.\nNope-nerf:\nOptimising neu-\nral radiance field with no pose prior.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4160\u20134169, 2023. 3\n[5] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast Gen-\neralizable Radiance Field Reconstruction From Multi-View\nStereo. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14124\u201314133, 2021. 3\n[6] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and\nSimon Lucey. Garf: gaussian activated radiance fields for\nhigh fidelity reconstruction and pose estimation. arXiv e-\nprints, pages arXiv\u20132204, 2022. 3\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. A1\n[8] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items. In 2022 In-\nternational Conference on Robotics and Automation (ICRA),\npages 2553\u20132560. IEEE, 2022. 6\n[9] Brandon Yushan Feng and Amitabh Varshney.\nSIGNET:\nEfficient Neural Representation for Light Fields.\n2021\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 14204\u201314213, 2021. 2\n[10] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-\nVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and\nRichard Tucker. Deepview: View synthesis with learned gra-\ndient descent. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2367\u2013\n2376, 2019. 6\n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance Fields Without Neural Networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 5501\u20135510, 2022. 2\n[12] Yang Fu, Ishan Misra, and Xiaolong Wang.\nMononerf:\nlearning generalizable nerfs from monocular videos with-\nout camera poses. In International Conference on Machine\nLearning, pages 10392\u201310404. PMLR, 2023. 3\n[13] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14346\u2013\n14355, 2021. 2\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 4\n[15] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. ACM Transactions\non Graphics (ToG), 37(6):1\u201315, 2018. 2\n[16] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 5875\u20135884, 2021. 2\n[17] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia.\nEfficientNeRF Efficient Neural Radiance Fields. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12902\u201312911, 2022. 2\n[18] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang.\nLeap: Liberate sparse-view 3d modeling from camera poses.\narXiv preprint arXiv:2310.01410, 2023. 3, 5, 6, A2\n[19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. A2\n[20] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\nmon Lucey. Barf: Bundle-adjusting neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5741\u20135751, 2021. 3, 6\n[21] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou.\nEfficient neural radiance\nfields for interactive free-viewpoint video. In SIGGRAPH\nAsia 2022 Conference Papers, pages 1\u20139, 2022. A2\n[22] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural Sparse Voxel Fields. Advances\nin Neural Information Processing Systems, 33:15651\u201315663,\n2020. 2\n[23] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng\nWang, Christian Theobalt, Xiaowei Zhou, and Wenping\nWang.\nNeural Rays for Occlusion-Aware Image-Based\nRendering.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7824\u2013\n7833, 2022. 1, 3, 6, 7\n[24] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\narXiv preprint arXiv:1906.07751, 2019. 2\n[25] Nelson Max. Optical models for direct volume rendering.\nIEEE Transactions on Visualization and Computer Graphics,\n1(2):99\u2013108, 1995. 4\n[26] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 38(4):1\u201314, 2019. 6, 7\n[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 2,\n4, 6, 7, A2\n[28] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2\n[29] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI con-\nference on artificial intelligence, 2018. 4\n[30] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding Up Neural Radiance Fields With\nThousands of Tiny Mlps. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14335\u2013\n14345, 2021. 2\n[31] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3d: Large-scale learning and evaluation of\nreal-life 3d category reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 10901\u201310911, 2021. 3\n[32] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs\nBergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario\nLu\u02c7ci\u00b4c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene\nrepresentation transformer: Geometry-free novel view syn-\nthesis through set-latent scene representations. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6229\u20136238, 2022. 1, 3, 5, 6, A2\n[33] Mehdi SM Sajjadi, Aravindh Mahendran, Thomas Kipf, Eti-\nenne Pot, Daniel Duckworth, Mario Lu\u02c7ci\u00b4c, and Klaus Greff.\nRust: Latent neural scene representations from unposed im-\nagery. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 17297\u201317306,\n2023. 1, 3\n[34] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-Motion Revisited. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 1, 6\n[35] Vincent Sitzmann, Semon Rezchikov, William T. Free-\nman, Joshua B. Tenenbaum, and Fr\u00b4edo Durand.\nLight\nField Networks: Neural Scene Representations With Single-\nEvaluation Rendering. Advances in Neural Information Pro-\ncessing Systems, 34, 2021. 2\n[36] Cameron Smith, Yilun Du, Ayush Tewari, and Vincent Sitz-\nmann. Flowcam: Training generalizable 3d radiance fields\nwithout camera poses via pixel-aligned scene flow. arXiv\npreprint arXiv:2306.00180, 2023. 3\n[37] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\nAmeesh Makadia. Generalizable patch-based neural render-\ning.\nIn European Conference on Computer Vision, pages\n156\u2013174. Springer, 2022. 1, 3, 4, A1, A2\n[38] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\nAmeesh Makadia. Light field neural rendering. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8269\u20138279, 2022. 3\n[39] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct Voxel\nGrid Optimization: Super-Fast Convergence for Radiance\nFields Reconstruction.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5459\u20135469, 2022. 2\n[40] Mukund Varma, Peihao Wang, Xuxi Chen, Tianlong Chen,\nSubhashini Venugopalan, and Zhangyang Wang. Is attention\nall that nerf needs?\nIn The Eleventh International Confer-\nence on Learning Representations, 2022. 1, 3, 4, 6, 7, A1,\nA2\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[42] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Men-\nglei Chai, Yun Fu, and S. Tulyakov. R2L: Distilling Neu-\nral Radiance Field to Neural Light Field for Efficient Novel\nView Synthesis. In European Conference on Computer Vi-\nsion. Springer, Springer, 2022. 2\n[43] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4690\u20134699, 2021. 1, 2, 3, 6,\n7, A2\n[44] Zirui Wang,\nShangzhe Wu,\nWeidi Xie,\nMin Chen,\nand Victor Adrian Prisacariu.\nNerf\u2013:\nNeural radiance\nfields without known camera parameters.\narXiv preprint\narXiv:2102.07064, 2021. 3\n[45] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8534\u20138543, 2021. 6\n[46] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\net al. Omniobject3d: Large-vocabulary 3d object dataset for\nrealistic perception, reconstruction and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 803\u2013814, 2023. 6\n[47] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. Plenoctrees for Real-Time Rendering of\nNeural Radiance Fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[48] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 1,\n3, 5, 6, A2\n[49] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 6\n[50] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely.\nStereo magnification:\nLearning\nview synthesis using multiplane images.\narXiv preprint\narXiv:1805.09817, 2018. 6\n"
  },
  {
    "title": "HeaP: Hierarchical Policies for Web Actions using LLMs",
    "link": "https://arxiv.org/pdf/2310.03720.pdf",
    "upvote": "5",
    "text": "HEAP: HIERARCHICAL POLICIES FOR\nWEB ACTIONS USING LLMS\nPaloma Sodhi\nASAPP, NY, USA\npsodhi@asapp.com\nS.R.K. Branavan\nASAPP, NY, USA\nbranavan@asapp.com\nRyan McDonald\nASAPP, NY, USA\nrmcdonald@asapp.com\nABSTRACT\nLarge language models (LLMs) have demonstrated remarkable capabilities in per-\nforming a range of instruction following tasks in few and zero-shot settings. How-\never, teaching LLMs to perform tasks on the web presents fundamental challenges\n\u2013 combinatorially large open-world tasks and variations across web interfaces. We\ntackle these challenges by leveraging LLMs to decompose web tasks into a collec-\ntion of sub-tasks, each of which can be solved by a low-level, closed-loop policy.\nThese policies constitute a shared grammar across tasks, i.e., new web tasks can\nbe expressed as a composition of these policies. We propose a novel framework,\nHierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of\nhierarchical LLM prompts from demonstrations for planning high-level tasks and\nexecuting them via a sequence of low-level policies. We evaluate HeaP against a\nrange of baselines on a suite of web tasks, including MiniWoB++, WebArena, a\nmock airline CRM, as well as live website interactions, and show that it is able to\noutperform prior works using orders of magnitude less data.\n1\nINTRODUCTION\nRecent advances in instruction following large language models (LLMs) (Ouyang et al., 2022;\nTouvron et al., 2023) have shown impressive zero and few-shot capabilities in solving tasks by\nparsing natural language instructions and breaking them down into actionable steps (Yao et al., 2022b;\nHuang et al., 2022b). In this paper, we focus on the problem of teaching LLMs to perform tasks\non the web, for instance booking flights or making appointments. Assisting humans in performing\nweb tasks has significant implications on a variety of domains given the pervasive nature of web and\ncloud-based applications in everyday life.\nPrior works collect large amounts of demonstrations of web tasks to train language models (Furuta\net al., 2023; Gur et al., 2022; Humphreys et al., 2022; Liu et al., 2018; Shi et al., 2017). However,\nteaching LLMs to perform tasks on the web presents fundamental challenges. (1) Combinatorially\nlarge open-world tasks: There are countless ways to interact with the web, leading to a combinatorially\nlarge space of tasks such as booking flights, making appointments, payments, etc. (2) Variations\nacross web interfaces: Web interfaces differ from one website to another, e.g. booking a flight on\nJetBlue is different from booking it on United. Hence, it is intractable to cover all such variations in\ntasks and interfaces in the training data, and have a single supervised model that can solve all tasks.\nOur key insight is to leverage LLMs to decompose complex web tasks into a set of modular sub-tasks,\neach of which can be solved by a low-level, closed-loop web policy. These policies constitute a\nshared grammar across tasks, i.e., any new web task can be expressed as a composition of these\npolicies. For example, the task of booking a flight can be expressed as a sequence of policies for\nfilling source and destination airports, choosing flight dates, and filling in passenger details. Each\nlow-level policy is specialized for a particular sub-task, e.g. a fill text policy can work on text boxes\nacross web user interfaces (UIs) that either require clicking and typing text, or require typing partial\ntext and auto-completing from a list of options.\nWhile manually programming these policies can be tedious, it is much easier to learn them from\nhumans performing varied tasks on the web. We propose a novel framework, Hierarchical Policies\n1\narXiv:2310.03720v1  [cs.LG]  5 Oct 2023\nfor Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts for planning\nhigh-level tasks and executing low-level policies. We first collect raw demonstrations from a human\nuser, auto-label them with low-level policies, and auto-generate both task and policy prompts. At\ninference time, given a task objective, we hierarchically invoke an LLM to first generate a task plan\nand then generate actions for each policy in the plan. HeaP enables LLMs to respond effectively to\ndynamic web pages as well as generalize across tasks and interfaces from few-shot demonstrations.\nExperimentally, we evaluate HeaP on a range of increasingly complex benchmarks: MiniWoB++,\nWebArena, a mock airline CRM simulator and live website interactions.1 We show that HeaP has\nsignificantly better task success rates and requires orders of magnitude less training (or demonstration)\ndata relative to prior work (see Table 1 for summary).\n2\nRELATED WORK\nLanguage models for web tasks.\nEarly work mapping natural language instructions into actions\n(Branavan et al., 2009; Artzi & Zettlemoyer, 2013; Nogueira & Cho, 2016) has rapidly evolved\nresulting in new applications and datasets (Zhou et al., 2023; Deng et al., 2023). In language models\nperforming web tasks, there are broadly 3 classes of methods: (1) Reinforcement learning (RL) for\nweb navigation that train RL agents to navigate web interfaces (Humphreys et al., 2022; Gur et al.,\n2021; Liu et al., 2018; Shi et al., 2017). However, these are often sample inefficient and exploration on\nlive websites can pose practical safety concerns. (2) In-context learning with large language models\nuses a combination of instructions and in-context examples with large language models (OpenAI,\n2023a; Significant Gravitas, 2023; Wang et al., 2023b; Friedman, 2022; LangChain, 2023), with\na significant portion being open-source initiatives. While impressive, they often rely on manually\ncrafted prompts and heuristic strategies to tackle context lengths and task generalization, making it\nchallenging to build on existing findings. (3) Fine-tuning language models for web tasks focuses on\nfine-tuning language models on specific web tasks and has emerged as a predominant approach in\nprior works (Gur et al., 2022; Furuta et al., 2023; Yao et al., 2022a; Gur et al., 2023; Mazumder &\nRiva, 2020; Gur et al., 2018). However, training such models has limitations such as an inability\nto generalize from few examples of tasks and interfaces, necessitating frequent retraining. As our\nmethod, HeaP, is compositional in how it uses the LLM, it is inherently not task-specific and does\nnot have these shortcomings.\nLanguage models for decision making.\nLarge language models have shown impressive out-of-the-\nbox decision making capabilities (Ouyang et al., 2022; ?; Brown et al., 2020; Radford et al., 2019).\nThis arises from an ability to break down complex tasks into smaller sub-tasks (Huang et al., 2022a;\nZhou et al., 2021), reason about intermeditate steps (Yao et al., 2022b; Wei et al., 2022), and recover\nfrom errors (Miao et al., 2023). As a results, LLMs in recent times, have found applications in diverse\ndomains like web retrieval (Nakano et al., 2021; Liu et al., 2023; Zaheer et al., 2022; Schick et al.,\n2023; Xu et al., 2021), robotics (Ahn et al., 2022; Huang et al., 2022b; Wang et al., 2023a), and\ntext-based games (Yao et al., 2020; Shridhar et al., 2020). Moreover, advances in multi-modal LLMs\nenable decision making from both language and image feedback (Shaw et al., 2023; Lee et al., 2023;\nBurns et al., 2022). However, such decision making capabilities remain to be explored for general\npurpose web tasks involving clicks, types, form filling, etc. Our approach, HeaP, leverages the task\ndecomposition and reasoning capabilities of LLMs to perform a wide range of web tasks. With only\na handful of examples, HeaP can generalize, showing improved performance over prior works (Gur\net al., 2022; Furuta et al., 2023; Humphreys et al., 2022; Liu et al., 2018) that train models with orders\nof magnitude more data.\n3\nPROBLEM FORMULATION\nThe overall goal is to learn a policy that performs a web task. The web task is represented as a context\n\u03d5, that can be (a) an explicit instruction such as \"Book me a flight from NYC to BOS\" (b) a structured\ndictionary defining the parameters of the task, or (c) a supporting set of texts such as a conversation\nwhere the instruction is implicit. Given the current context \u03d5, the goal is to perform a web task that\nachieves the task objective. We formulate this as a Contextual Markov Decision Process (CMDP),\n< \u03a6, S, A, T , r >, defined below:\n1We will open-source the code, simulator, and data.\n2\n(a) Inference Time\n\u03c01\n\u03c02\nLow-level Policies\n1.FILL_TEXT(From, New York) \n2.FILL_TEXT(To, Chicago) \n3.CHOOSE_DATE(Depart, 5/15/23) \n4.CHOOSE_DATE(Return, 5/22/23) \n5.CLICK(Book Flight)\nHigh-level Task\nI would like to \ufb02y from NYC \nto Chicago\nI want to leave May 15 and \nreturn the week a=er\nHello, how can I help you?\nSure, can I have your travel \ndates please?\nCustomer\nAgent\nInput\n(b) Train Time\nCollect raw demonstra=ons\n\u2026\ns1\ns2\ns3\ns4\na1\na2\na3\na4\n\u2026\n FILL_TEXT(From,New York)\n\u03c01\nAutolabel demonstra=ons with policies\n CHOOSE_DATE(Depart,5/15/23)\n\u03c02\n\u2026\ns1\ns2\ns3\ns4\na1\na2\na3\na4\ns1\ns2\ns3\na1\na2\na3\n\u2026\n\u03c02\n\u03c02\n1\n\u03c0\n\u03c01\nFILL_TEXT \n(FIELD,TEXT)\n\u03c02\nCHOOSE_DATE \n(FIELD,DATE)\nHigh-level Task Prompt\nLow-level Policy Prompts\n</> \n1. FILL_TEXT(From,New York) \n2. CHOOSE_DATE(Depart,5/15/23) \n3. \u2026 \n</> \n1. CHOOSE_LIST(Guests,2) \n2. CHOOSE_DATE(Return,5/22/23)\n</> \nCLICK 7 \n</> \nTYPE \u201cNew   \n York\u201d \n...\n</> \nCLICK 7 \n</> \nCLICK 63 \n...\n(a) Inference\n(b) Prompt Generation\nFigure 1: HeaP Overview: (a) Inference: High-level task planner creates a sequence of steps like filling text\nor choosing dates from an input context and starting webpage. Each step is a call to a low-level web policy that\ndirectly interacts with the webpage. (b) Prompt Generation: Dataset of raw state-action demonstrations is\ntransformed into task and policy base prompts by first auto-labeling with policies and then generating prompts.\n\u2022 Context, \u03d5 \u2208 \u03a6 is the web task objective expressed explicitly as an instruction or structured\nparameters or implicitly as a conversation\n\u2022 State, s \u2208 S is the current state of the webpage, i.e., the current DOM d serialized as text.2\n\u2022 Action, a \u2208 A(s) are the set of web actions that can be performed on the current webpage,\ni.e. click(id), type(id,value), where id specifies an element in the webpage, and\nvalue is a string. The action space can be quite large since a typical webpage can have\nhundreds of elements, and value can be any arbitrary text.\n\u2022 Transition function, T (s\u2032|s, a) represents the change in the webpage on performing an action.\n\u2022 Reward, r(s, a) is awarded for reaching a set of subgoals, e.g. cancelling a flight has subgoals\nlike finding the booking and then canceling it.\nThe goal is to learn a policy \u03c0 : S \u00d7 \u03d5 \u2192 A that maximizes performance, i.e., the cumulative reward\nJ(\u03c0) = E\u03c0\nhPT\nt=1[r(st, at)]\ni\n. Instead of explicitly defining the reward function and solving the\nMDP, we aim to learn this policy \u03c0 from demonstrations D = {(\u03d5i, si\n1, ai\n1, si\n2, ai\n2, . . .)}N\ni=1.\nWe leverage LLMs that are highly effective at generalizing from few-shot demonstrations without the\nneed for fine-tuning. To do so, we translate demonstrations D into in-context examples for an LLM\nprompt P. A simple way to do this is to flatten all demonstrations D, i.e., concatenate the conversation\n\u03d5, with state action trajectories, and merge them together. However, a typical demonstration may\nconsist of a lengthy chain of actions, with each state in the chain being the entire webpage document\nobject model (DOM). In terms of total tokens, N demonstrations each of T timesteps, each step\ncomprising of X tokens of both conversation and webpage would result in N \u00d7 T \u00d7 X tokens. This\ncan quickly exhaust context space even for simple websites. We tackle this problem in our approach\nby hierarchically composing prompts.\n4\nAPPROACH\nWe present a framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that performs a\nrange of web tasks from natural language conversations by hierarchically invoking a Large Language\nModel (LLM). The framework consists of a hierarchy of two levels: a high-level task planner that in\nturns invokes a sequence of low-level web policies.\nConsider the example in Fig. 1. Given a conversation with a customer looking to book flights, and a\nbooking website, the task planner generates a plan, i.e, a sequence of steps to execute. Examples of\n2For some tasks, the current webpage may not be sufficient to define state. In such cases, we can extend state\nto a history of previous webpages and actions.\n3\nAlgorithm 1 HeaP Inference: Compose policies to solve the task\n1: Input: Context \u03d5, Current Webpage State s0, LLM, Environment T\n2: \u03be \u2190 TASKPLANNER(\u03d5, s0)\n\u25b7 Get task plan, i.e., sequence of calls to web policies\n3: for (\u03c0i, \u03c8i) \u2208 \u03be do\n4:\nWEBPOLICY(\u03c0i, \u03c8i)\n\u25b7 Execute web policy\n5: end for\n6: function TASKPLANNER(Context \u03d5, State s)\n7:\nPtask \u2190 Load base prompt for task planner\n8:\n\u03be \u2190 LLM(\u03d5, s, Ptask)\n\u25b7 Predict plan given context, state\n9:\nreturn Plan \u03be = {(\u03c01, \u03c81), (\u03c02, \u03c82), . . . , (\u03c0N, \u03c8N)}\n10: end function\n11: function WEBPOLICY(Policy \u03c0, Instruction \u03c8)\n12:\nPpolicy \u2190 Load base prompt for web policy \u03c0\n13:\ns \u2190 GETCURRENTSTATE(), a \u2190 None, aprev \u2190 {}\n\u25b7 Initialize state, action, prev actions\n14:\nwhile a not done do\n15:\na \u2190 LLM(\u03c8, s, aprev, Ppolicy)\n\u25b7 Predict action given instruction, state, prev actions\n16:\naprev \u2190 aprev \u222a a\n\u25b7 Append action to prev actions\n17:\ns \u2190 T (s, a)\n\u25b7 Execute action to transition to next state\n18:\nend while\n19: end function\nsteps are either filling a text box, choosing a date, or choosing an option from a drop-down. Each\nof these steps can be delegated to a corresponding web policy that interacts with the web page and\nexecutes web actions like clicking and typing. For instance, the Fill_TEXT(field, text) web\npolicy searches for the web element corresponding to field, clicking it, typing a text and optionally\nchoosing from a list of autocomplete options. On the other hand, the CHOOSE_DATE(field,\ndate) web policy clicks on the web element, navigates a grid of dates and clicks on the correct date.\n4.1\nINFERENCE TIME: COMPOSE POLICIES TO SOLVE THE TASK\nAlgorithm 1 describes the inference time procedure. We take as input a context \u03d5, which can\nbe a conversation or an explicit objective, and the current webpage state s0. This is sent to a\ntask planner that generates a plan. The plan is a sequence of calls to low-level web policies.\nEach element of the sequence is represented as a web policy type \u03c0 and instruction to the policy\n\u03c8, i.e., \u03be = {(\u03c01, \u03c81), (\u03c02, \u03c82), . . . (\u03c0N, \u03c8N)}. For example, CHOOSE_DATE(field, date)\ncorresponds to calls to policy \u03c0 = CHOOSE_DATE with instruction \u03c8 = (field, date).\nThe web policies in plan \u03be are invoked one by one. Each policy \u03c0i predicts the next action a given its\ninstruction \u03c8i, current state s, and previous actions aprev. Once the policy issues the special action\n\u201cDONE\u201d, control is handed back to the outer loop and the next policy is executed. When all policies\nin the plan \u03be are done, the task planner is invoked again for the next plan. The process is terminated\nwhen the task planner produces an empty plan.\nBoth the task planner and the web policies are calls to an LLM with different base prompts.\nThe base prompt for the task planner shows examples of {input:\n[overall context \u03d5,\ncurrent state s0], output:\nplan \u03be}.\nThe base prompt for web policies shows ex-\namples of {input:\n[instruction \u03c8t, current state st, previous actions a1:t\u22121],\noutput:\nnext action at}. We additionally include chain-of-thought (CoT) reasoning Wei et al.\n(2022) to both task and policy prompts that forces the LLM to generate a series of short sentences\njustifying the actions it predicts. We found this to uniformly improve performance (Appendix B).\n4.2\nGENERATE TASK AND POLICY PROMPTS FROM DEMONSTRATIONS\nTo generate prompts from demonstrations, we collect demonstrations from human users performing\ntasks on the browser. We design a browser plugin to record webpage DOM d and events such as\nclicks and types. Each demonstration is expressed as text by converting the DOM tree into a list of\nsalient web elements like links, buttons, inputs. The parsed demonstration dataset is represented as\nD = {(\u03d5, s1, a1, . . . , sT , aT )}.\nWe then autolabel each step t with a low-level policy \u03c0t and instruction \u03c8t to create a labeled dataset\nDlabel = {(\u03d5, s1, a1, (\u03c01, \u03c81), . . . , sT , aT , (\u03c0T , \u03c8T ))}. We leverage LLMs to autolabel demonstra-\n4\ntions and describe details in Appendix. D. Finally, we convert demonstrations to base prompts for\nboth high-level planner and low-level policies and list representative prompts in Appendix. G.\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nEnvironments. We evaluate across 4 distinct environments, each emphasizing different components:\n\u2022 MiniWoB++ (Liu et al., 2018): An extension of the OpenAI MiniWoB benchmark (Shi et al.,\n2017) covering a range of web interactions like form filling, search, choose dates, etc. We evaluate\nacross 45 distinct tasks that don\u2019t rely on visual reasoning, and average over 50 seeds per task.\n\u2022 WebArena (Zhou et al., 2023): A recent community benchmark offering complex web tasks\nacross multiple domains. Compared to MiniWoB++, WebArena websites are highly realistic with\ntasks mirroring those that humans routinely perform on the internet. We evaluate on a set of 125\nexamples sampled from 12 distinct intents from 2 domains, Gitlab and OpenStreetMaps.\n\u2022 Airline CRM: A new CRM simulator that we developed, modeled after customer service work-\nflows of popular airline websites. Compared to MiniWoB++, Airline CRM offers longer-horizon\ntasks tied to a mock database, capturing typical CRM activities more effectively. We evaluate\nacross 5 distinct tasks each with 20 randomized scenarios. More simulator details in Appendix E.\n\u2022 Live Websites: Finally, we create an environment to interact with live websites, such as popular\nairlines like JetBlue, American, United. The raw browser content is considerably more complex,\nbeing \u223c100x larger than the simulators. We evaluate generalization across UIs by performing the\nsame search-flight task across 3 very different website UIs and average across 10 runs per UI.\nBaselines. We compare against various baselines including prior state-of-the-art (Furuta et al., 2023;\nGur et al., 2022; Humphreys et al., 2022; Liu et al., 2018) and methods Flat Zero-shot, Flat\nFew-shot, HeaP Zero-shot, HeaP Few-shot. Flat Zero-shot is a single prompt con-\ntaining only the instructions and no in-context examples. Flat Few-shot includes both instruc-\ntions and in-context examples. Both of these follow a chain-of-thought (CoT) prompting style\nsimilar to ReAct (Yao et al., 2022b). HeaP Few-shot and HeaP Zero-shot is our hierarchical\nprompting approach, HeaP, with and without in-context examples respectively. Detailed prompts\nfor the different methods can be found in Appendix G. All 4 methods use the instruction fine-tuned\ntext-davinci-0033 model. We found it to perform better at multi-step reasoning compared to\ngpt-3.5-turbo1(Ouyang et al., 2022) while being more cost-effective than gpt-41(OpenAI,\n2023b). More details on model hyper-parameters in Appendix C.2.\nMetrics. We define 3 metrics: Success Rate (%suc\u2191), Task Progress (%prog\u2191), and Number Actions\n(#act\u2193). %suc\u2191 is either 0 or 1 based on the task being completed successfully. %prog\u2191 is between\n0 and 1 indicating progress towards completing the task. #act\u2193 is the number of actions.\n5.2\nRESULTS AND ANALYSIS\nOverall Results.\n\u2022 On the MiniWob++ benchmark, HeaP Few-shot matches or outperforms priors works with\norders of magnitude fewer demonstrations (21 demos for HeaP vs 347k demos in (Furuta et al.,\n2023) or 2.4M demos in (Humphreys et al., 2022)). See Table 1.\n\u2022 On the WebArena benchmark (Gitlab, Maps), HeaP Few-shot achieves much better success\nrates than prior works (Zhou et al., 2023; Yao et al., 2022b) that use Flat chain-of-thought\nprompting. See Fig. 4.\n\u2022 On airline CRM and live websites, we see a similar trend where HeaP Few-shot achieves\nbetter success rates and task progress with lower number of actions. See Fig. 5, Fig.7.\n\u2022 HeaP Few-shot achieves higher success rates by breaking down complex tasks into reusable\nlow-level policy calls each of which can be covered with their own in-context examples. See\nFig. 2 for an ablation and Figs. 8,9 for qualitative examples.\n\u2022 Finally, we provide ablations on different model scales and CoT reasoning in Appendix B.\n3https://platform.openai.com/docs/models\n5\nComparison to prior works. In Table 1, HeaP Few-shot outperforms or matches priors works\nwith orders of magnitude lower demonstrations on the MiniWob++ benchmark. HeaP has an average\nsuccess rate of 0.96 using only 21 in-context examples.\nMethod\nModels\nTraining\nSuccess\nSize\nRate\nWGE (Liu et al., 2018)\n-\n12K+\n0.77\nCC-Net (SL) (Humphreys et al., 2022)\nResNet\n2.4M\n0.33\nCC-Net (SL+RL) (Humphreys et al., 2022)\nResNet\n2.4M\n0.96\nWebN-T5 (Gur et al., 2022)\nT5-XL\n12K\n0.56\nWebGUM (HTML) (Furuta et al., 2023)\nFlan-T5-XL\n347K\n0.90\nFlat / ReAct (Yao et al., 2022b)\nGPT-text-davinci-003\n7\n0.72\nHeaP (Ours)\nGPT-text-davinci-003\n21\n0.96\nTable 1: Comparison to prior works with success rates averaged across\n45 MiniWoB++ tasks. HeaP achieves a higher success rate with orders of\nmagnitude lower samples. See Appendix B.3 for breakup over individual tasks.\nHeaP outperforms all the\nsupervised learning base-\nlines and matches the\nmost perfomant baseline\nCC-Net (Humphreys et al.,\n2022) that trains an RL\nagent using 2.4 million\ndemonstrations.\nHeaP\noutperforms the most re-\ncent baseline, WebGUM\n(Furuta et al., 2023) which\nfine tunes a pre-trained in-\nstruction model on 347K\ndemonstrations.\nPart of\nthe performance gain comes from in-context learning and CoT reasoning with large-scale mod-\nels similar to ReAct (Yao et al., 2022b). However, HeaP with its hierarchical prompting improves\nsuccess rates significantly over ReAct (aka Flat), by breaking down complex tasks and covering\nthem efficiently with more in-context examples.\nWhy does hierarchical prompting help?\nFigure 2: HeaP vs Flat with varying in-\ncontext examples on subset of MiniWob++.\nThe key benefit of hierarchical prompting is to break\ndown complex tasks into a set of smaller policies, each\nof which can be covered by a handful of demonstrations.\nIn contrast, covering the entire task would require com-\nbinatorially many more demonstrations. Fig. 2 shows\nan ablation of HeaP vs Flat with varying number of\nin-context examples. Hierarchy helps at two levels: (1)\nFor the same number of examples (\u2264 7), improvements\ncome from decomposing task instructions into granu-\nlar policy instructions (2) Hierarchical decomposition\nresults in smaller individual policies. This allows us\nto add more in-context examples (> 7) in each policy\nprompt compared to what is possible with a single flat\nprompt (see Sec 3) resulting in higher success rates.\nHeaP does better on complex tasks\nComparable performance on simple tasks\nHeaP \nFew-Shot\nFlat\u2028\nFew-Shot\nFigure 3: Task-wise success rates breakdown on MiniWob++ (averaged over 50 seeds per task)\nWe see quantitative evidence for this in Fig. 3 which shows a task-wise success rates breakdown\non MiniWob++. The gap between HeaP Few-Shot and Flat Few-Shot is much higher on\ncomplex tasks. We characterize complex tasks as those that either require heterogeneous sets of\nactions or multiple steps with changing webpages. We dive deeper into the book-flight task in Fig. 8\nwhere HeaP Few-shot significantly outperforms baselines. HeaP task planner breaks down the\ntask into a set of policy calls like FILL_TEXT, CHOOSE_DATE. The policies, e.g. CHOOSE_DATE\nissues a set of low-level actions like CLICK to solve sub-tasks like navigating and picking the right\n6\nWGE\nCC-Net\nWebN-T5\nWebGUM\nFlat\nFlat\nHeaP\nHeaP\nTask\n(SL+RL)\n(HTML)\nZero-shot\nFew-shot\nZero-shot\nFew-shot\n%suc\u2191\n%suc\u2191\n%suc\u2191\n%suc\u2191\n%suc\u2191\n#act\u2193\n%suc\u2191\n#act\u2193\n%suc\u2191\n#act\u2193\n%suc\u2191\n#act\u2193\nsimple\nclick-option\n1.00\n0.99\n0.37\n1.00\n0.76\n3.68\n1.00\n2.62\n0.80\n2.94\n1.00\n1.94\nclick-dialog-2\n1.00\n1.00\n0.24\n0.34\n0.98\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.02\nenter-date\n0.00\n1.00\n0.00\n1.00\n1.00\n3.00\n1.00\n2.00\n0.00\n4.08\n1.00\n2.00\nlogin-user\n0.99\n1.00\n0.82\n1.00\n0.96\n3.42\n1.00\n3.00\n1.00\n3.06\n1.00\n3.00\ngrid-coordinate\n1.00\n1.00\n0.49\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\ncomplex\ncopy-paste-2\n-\n0.63\n0.00\n0.00\n0.54\n7.66\n0.56\n4.00\n0.48\n3.84\n0.96\n2.04\nfind-word\n-\n0.88\n0.00\n0.00\n0.22\n2.62\n0.26\n5.18\n0.12\n2.92\n0.98\n2.00\nchoose-date-medium\n-\n0.99\n0.00\n0.57\n0.32\n2.90\n0.20\n2.76\n0.20\n9.26\n1.00\n3.86\nclick-checkboxes-large\n0.68\n0.71\n0.22\n0.98\n0.00\n8.40\n0.20\n8.40\n0.00\n7.00\n1.00\n6.20\nclick-checkboxes-transfer\n0.64\n0.99\n0.63\n0.99\n0.40\n4.80\n0.40\n3.90\n0.54\n3.20\n0.94\n2.84\nemail-inbox\n0.43\n1.00\n0.38\n0.99\n0.40\n7.00\n0.70\n4.50\n0.00\n3.00\n0.90\n5.20\nsimple-algebra\n-\n0.75\n0.00\n0.00\n0.14\n8.80\n0.30\n6.78\n0.04\n4.38\n0.74\n2.00\nlogin-user-popup\n-\n1.00\n0.72\n0.97\n0.46\n6.28\n0.46\n3.52\n0.46\n5.82\n1.00\n4.88\nsearch-engine\n0.26\n1.00\n0.34\n0.59\n0.38\n3.64\n0.38\n3.16\n0.26\n4.46\n1.00\n4.30\nbook-flight\n0.00\n0.87\n0.00\n0.48\n0.00\n16.00\n0.10\n11.10\n0.00\n13.52\n0.90\n9.14\nTable 2: Task-wise performance breakup on MiniWoB++ benchmark on a subset of 15 tasks. See Appendix\nB.3 for a full breakup over 45 tasks.\nHeaP \nFew-Shot\nFlat\u2028\nFew-Shot\nFILL_TEXT \nflight-to \u201cLEX\"\nCHOOSE_DATE \ndatepicker \n\"10/16/2016\"\nFILL_TEXT \nflight-to \u201cLEX\"\nCHOOSE_DATE \ndatepicker \n\"10/16/2016\"\nCHOOSE_DATE \ndatepicker \n\"10/16/2016\"\nCLICK Book flight\n* TYPE 9 flight-\nto \u201cLEX\"\n* CLICK 13 \ndatepicker\n* TYPE 9 flight-to \n\u201cLEX\" \n* CLICK 26 \nLexington, KY (LEX) \n* CLICK 13 datepicker \n* CLICK 31 Prev \n* CLICK 128 Prev\n* CLICK 13 datepicker \n* CLICK 31 Prev \n* CLICK 128 Prev \n* CLICK 289 10/16/2016\n* CLICK 400 Book \nflight for $209\nTask \nPlan\nWeb\u2028\nPolicies\nBrowser UI\nLLM\u2028\nReason\nI have to \ufb01rst type \"LEX\" \nin the \ufb01eld \ufb02ight-to \ncorresponding to id 9\nThere is a corresponding \ndropdown text in \"Lexington, KY \n(LEX)\" in id 26 \nI have to click on id 26\nI have to \ufb01rst click on a \ndatepicker.\nI am currently in December \n(12/2016).  \nI have to go to October (10/2016).  \nSince 12 > 10, I have to click on \nPrev.\n I have already navigated to \nthe right month (October \n2016). I have to click on the id \ncorresponding to 10/16/2016.\nSince I have to book the shortest \n\ufb02ight, I should look at duration. The \nshortest duration is 3h 21m \ncorresponding to id 398. I have to \nclick on button below \ncorresponding to id 400.\nCLICK Search\nThe instruction says to \nCLICK Search, so I must \nissue CLICK action. \nSearch corresponds to id \n422.\n* CLICK 422 \nSearch\nFigure 7: Outputs from HeaP Few-shot on book-\ufb02ight task showing hierarchical task planner actions,\nlow-level web policy actions, and LLM reasoning.\nMethod\nGITLAB\nMAPS\n%suc\"\n#act#\n%suc\"\n#act#\nFlat\nZero-shot\n0.15\u00b1 0.04\n8.62\u00b1 0.33\n0.27\u00b1 0.07\n7.98\u00b1 0.37\nFlat\nFew-shot\n0.18\u00b1 0.05\n7.08\u00b1 0.38\n0.23\u00b1 0.06\n6.08\u00b1 0.40\nHeaP\nZero-shot\n0.15\u00b1 0.04\n8.35\u00b1 0.32\n0.26\u00b1 0.08\n8.29\u00b1 0.43\nHeaP\nFew-shot\n0.35\u00b1 0.06\n6.31\u00b1 0.39\n0.53\u00b1 0.07\n5.50\u00b1 0.33\nTable 6\n6\nDISCUSSION\nIn this paper, we present a hierarchical framework HeaP for solving web tasks from few-shot demon-\nstrations. We evaluate against a range of baselines on a suite of web tasks, including MiniWoB++, a\nmock airline CRM and live website interactions. We analyze results and characterize performance\ngains from both hierarchical prompting and demonstrations.\nHierarchy breaks down Complex Tasks Our results indicate that hierarchical prompting achieves\nhigher success rates by breaking down complex tasks into reusable low-level policy calls (see Fig. 8).\nThis is evident in the performance difference between HeaP Few-shot and Flat Few-shot\n(see Table 7, Fig. 5, Fig. 6), with Fig. 2 showing the role of hierarchy in both better task decomposition\nand ability to pack in more examples.\nSample Ef\ufb01cient Generalization HeaP matches or outperforms priors works with multiple orders\nof magnitude less data, i.e. 15 examples for HeaP vs 12k examples in (14) or 2.2M examples in (18))\nHeaP Few-shot:\nNavigates webpages and \ncounts until it reaches 7th link\nHeaP Zero-shot:\nMistakenly clicks on the 7th link \u2028\nin the current webpage\nLLM Reason:\u2028\nThe instruction is to \n\ufb01nd and click the \nsearch link with the \nindex of 7. Since the \nlist of web ids and links \nprovided are in order, \nthe link with the index \nof 7 is the link with the \nid of 17. \nLLM Reason:\u2028\nLooking at the browser content, \nI see the following links: \nDolores, Cheree, Joye. The \nmakes the index of the links as \nDolores (0 + 1 = 1), Cheree (0 + \n2 = 2), Joye (0 + 3 = 3). I am told \nto \ufb01nd and click the link 7. Since \n7 > 3, I have to go to the next \npage. To go to the next page, I \nhave to click on the id \ncorresponding to > sign, which \ncorresponds to id 28. \n\u2026 \nCLICK 17 Joye\nCLICK 28 >\nCLICK 50 >\nCLICK 52 Riley\nFigure 8: HeaP Few-shot vs HeaP Zero-shot on a search-engine task. The instruction asks to \ufb01nd the\n7th link, however, it is ambiguous what 7 refers to. HeaP Few-shot with a single in-context demo is able to\nground the task in the UI and reason that the 7th link lies in the 2nd webpage and navigates to the link correctly.\n10\nFigure 4: Evaluation on WebArena Benchmark (Gitlab, Maps). (Left) Aggregate metrics (Right) Success rate\nbreakdown of 125 examples across 12 intent buckets. Flat Few-Shot is the baseline reasoning agent from\nWebArena (Zhou et al., 2023) following ReAct (Yao et al., 2022b) style CoT prompting.\nTask\nMetric\nFlat\nHeaP\nZero-shot\nFew-shot\nZero-shot\nFew-shot\nCANCEL\nFLIGHT\n%suc\"\n0.10 \u00b1 0.10\n1.00 \u00b1 0.00\n0.20 \u00b1 0.10\n1.00 \u00b1 0.00\n%prog\"\n0.15 \u00b1 0.10\n1.00 \u00b1 0.00\n0.80 \u00b1 0.00\n1.00 \u00b1 0.00\n#act#\n11.20 \u00b1 0.20\n6.00 \u00b1 0.00\n11.33 \u00b1 0.85\n6.00 \u00b1 0.00\nFIND\nBOOKING\n%suc\"\n0.00 \u00b1 0.00\n0.90 \u00b1 0.05\n1.00 \u00b1 0.00\n1.00 \u00b1 0.00\n%prog\"\n0.00 \u00b1 0.00\n0.90 \u00b1 0.10\n1.00 \u00b1 0.00\n1.00 \u00b1 0.00\n#act#\n11.00 \u00b1 0.00\n4.10 \u00b1 0.75\n3.00 \u00b1 0.00\n3.00 \u00b1 0.00\nSEARCH\nFLIGHT\n%suc\"\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n1.00 \u00b1 0.00\n%prog\"\n0.50 \u00b1 0.04\n0.60 \u00b1 0.00\n0.60 \u00b1 0.05\n1.00 \u00b1 0.00\n#act#\n11.00 \u00b1 0.00\n11.00 \u00b1 0.00\n11.00 \u00b1 0.00\n5.00 \u00b1 0.00\nUPDATE\nPASSENGER\nDETAILS\n%suc\"\n0.00 \u00b1 0.00\n0.50 \u00b1 0.15\n0.30 \u00b1 0.10\n0.65 \u00b1 0.15\n%prog\"\n0.00 \u00b1 0.00\n0.90 \u00b1 0.00\n0.60 \u00b1 0.10\n0.90 \u00b1 0.05\n#act#\n16.00 \u00b1 0.00\n11.10 \u00b1 1.25\n14.30 \u00b1 0.70\n11.85 \u00b1 0.75\nBOOK\nFLIGHT\n%suc\"\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n0.65 \u00b1 0.15\n%prog\"\n0.55 \u00b1 0.05\n0.40 \u00b1 0.05\n0.40 \u00b1 0.05\n0.82 \u00b1 0.05\n#act#\n26.00 \u00b1 0.00\n26.00 \u00b1 0.00\n25.75 \u00b1 0.25\n22.25 \u00b1 0.90\nTasks / Intent\nMetric\nFlat\nHeaP\nZero-shot\nFew-shot\nZero-shot\nFew-shot\nCheck out my most recent\n%suc\"\n0.11 \u00b1 0.11\n1.00 \u00b1 0.00\n0.22 \u00b1 0.15\n1.00 \u00b1 0.00\n#act#\n11.22 \u00b1 0.22\n6.00 \u00b1 0.00\n11.33 \u00b1 0.90\n6.00 \u00b1 0.00\nHow many commits did {{user}}\nmake to {{repo}} on {{date}}?\n%suc\"\n0.00 \u00b1 0.00\n0.90 \u00b1 0.10\n1.00 \u00b1 0.00\n1.00 \u00b1 0.00\n#act#\n11.00 \u00b1 0.00\n4.10 \u00b1 0.78\n3.00 \u00b1 0.00\n3.00 \u00b1 0.00\nOpen my latest updated issue that\nhas keyword {{keyword}} in its\ntitle to check if it is closed\n%suc\"\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n0.00 \u00b1 0.00\n1.00 \u00b1 0.00\n#act#\n11.00 \u00b1 0.00\n11.00 \u00b1 0.00\n11.00 \u00b1 0.00\n5.00 \u00b1 0.00\nCreate a new {{scope}} project\nplanner and add {{account list}} as\nmembers\n%suc\"\n0.00 \u00b1 0.00\n0.53 \u00b1 0.13\n0.27 \u00b1 0.12\n0.67 \u00b1 0.13\n#act#\n16.00 \u00b1 0.00\n11.13 \u00b1 1.23\n14.33 \u00b1 0.69\n11.87 \u00b1 0.73\n2\nFigure 5: (Left) Evaluation on 5 airline CRM tasks averaged over 20 randomized scenarios per task. (Right)\nSimulator visualization of a book-flight task consisting of >20 steps.\ndate from a datepicker. This step is particularly challenging for baselines due to the variations in\nnavigating the datepicker. However, the CHOOSE_DATE policy in HeaP Few-shot has the ability\nto cover these variations with more in-context examples, making it more robust.\nOn the WebArena benchmark, we observe a similar trend in Fig. 4 showing a breakdown of success\nrates across 12 different intents on 2 domains. Compared to MiniWob++, this is a significantly\nmore challenging environment where prior work with Flat CoT prompting (Zhou et al., 2023; Yao\net al., 2022b) has very limited success rates (\u223c 18%). This limitation arises from the challenge\nof understanding how to interact appropriately with web pages. HeaP provides a mechanism for\ndefining dedicated policies that can be taught with targeted in-context examples. For instance, a task\nlike searching a Gitlab issue involves filtering and sorting by specific criteria. A dedicated policy like\nSEARCH_ISSUE() can handle this by filtering by keywords, sorting, and determining issue status.\nHow well does HeaP generalize across tasks? Table 2 along with Appendix B.3 shows metrics\nacross 45 tasks from MiniWoB++ (Liu et al., 2018; Shi et al., 2017) averaged over 50 seeds per task.\nHeaP Few-shot obtains higher success rates with lower number of actions compared to baselines,\nwith the performance gap higher for complex tasks, with complex being tasks that either require a\nheterogeneous set of actions or multiple steps with changing webpages. HeaP Few-shot achieves\n7\nthis with only 21 examples from 6 tasks and is able to solve the remaining 39 tasks without ever\nhaving seen them. Table 3 shows the breakup of in-context examples across different environments.\nSimilarly, Fig. 5 shows metrics on 5 longer horizon CRM tasks (each averaged over 20 scenarios)\ncorresponding to typical airline workflows like find & cancel bookings, update passenger details, find\n& book flights. HeaP Few-shot obtains higher success and task progress with lower number of\nactions compared to baselines. It achieves this with 10 in-context examples from 2 tasks (Table 3)\nEnvironment\nMethod\nExamples\nTasks covered by examples\nMiniWob++\nFlat\n7\nchoose-date, book-flight\nHeaP\n21\nchoose-date, book-flight\nsearch-engine, click-tab-2\nclick-checkbox, email-inbox\n|\u2013 TASK_PLANNER\n3\n|\u2013 FILL_TEXT\n5\n|\u2013 CHOOSE_DATE\n4\n|\u2013 SEARCH_LINK\n3\n|\u2013 SEARCH_TAB\n1\n|\u2013 CLICK_CHECKBOX\n2\n|\u2013 PROCESS_EMAIL\n3\nWebArena\nFlat\n3\ncount_commits, closest_location,\nHeaP\n15\ncount_commits,\nsearch_issue,\ntravel_time,\nclosest_location\n|\u2013 TASK_PLANNER\n3\n|\u2013 FIND_COMMIT\n2\n|\u2013 SEARCH_ISSUE\n3\n|\u2013 FIND_DIRECTIONS\n4\n|\u2013 SEARCH_NEAREST_PLACE\n4\nAirline CRM\nFlat\n5\ncancel flight\nHeaP\n10\ncancel flight, book flight\n|\u2013 TASK_PLANNER\n3\n|\u2013 FILL_TEXT\n2\n|\u2013 CHOOSE_DATE\n2\n|\u2013 SELECT_FLIGHT\n3\nLiveWeb\nFlat\n3\njetblue.com\nHeaP\n5\njetblue.com\n|\u2013 TASK_PLANNER\n1\n|\u2013 FILL_TEXT\n2\n|\u2013 CHOOSE_DATE\n2\nTable 3: In-context examples for HeaP and Flat.\nEach example is a state-action pair at particular timestep.\nFigure 6: Token counts for browser content before and\nafter compression on different environments.\nHow well does HeaP generalize across com-\nplex webpages?\nFig. 7 shows evaluation\nof HeaP Few-shot and Flat Few-shot\nacross 10 runs each on 3 different live websites\nwith task specification coming from short simu-\nlated conversations. What makes this task chal-\nlenging is that the browser content from these\nwebsites have a lot of extraneous information\nthat make it challenging to parse the correct\nfields. Fig. 6 shows the extent of compression\nwe perform to fit the browser content into the\nLLM\u2019s context space (see Appendix F for de-\ntails). For WebArena, we use the accessibility\ntree browser content representation from the en-\nvironment (Zhou et al., 2023). We evaluate each\nrun by comparing model performance against\na reference human demonstration. In Fig. 7,\nHeaP Few-shot is able to generalize to mul-\ntiple websites even though it has demonstration\nfrom only one (i.e. jetblue.com). In contrast,\nFlat Few-shot fails to generalize from it\u2019s\ndemonstration. Again HeaP Few-shot, by hi-\nerarchically decomposing the problem, is able\nto use demonstrations more efficiently.\nAblations on reasoning, models, and few-shot\nexamples. Appendix B shows ablations on CoT\nreasoning and model scales. Overall, we find\nCoT to boost performance across tasks, espe-\ncially multi-step tasks. For models, gpt-4 im-\nproves performance across methods, but having\nboth hierarchical prompting and few-shot ex-\namples continue to help. gpt-3.5-turbo does better in zero-shot setting but under-performs\ntext-davinci-003 when given few-shot examples. Fig. 9 shows the effect of few-shot examples\nqualitatively on a search-engine task. Few-shot examples help ground the task in concrete low-level\nactions on the web UI, resulting in HeaP Few-shot navigating to the desired link correctly.\nError Analysis. We cluster common failure modes of HeaP: (1) Content parsing errors: Browser\ncontent may be parsed with incorrect associations. Specifically, since we flatten the DOM structure\nand add to the LLM context, this can cause incorrect text associations. (2) Error recovery: LLM\nTrain\nTest\nMetric\nFlat\nHeaP\nFew-shot\nFew-shot\njetblue.com\n%suc\"\n0.60\u00b1 0.05\n1.00\u00b1 0.00\n%prog\"\n0.75\u00b1 0.02\n1.00\u00b1 0.00\n#act#\n8.00\u00b1 0.00\n7.00\u00b1 0.00\nunited.com\n%suc\"\n0.20\u00b1 0.04\n0.50\u00b1 0.17\n%prog\"\n0.47\u00b1 0.03\n0.68\u00b1 0.11\n#act#\n8.00\u00b1 0.00\n6.40\u00b1 0.27\naa.com\n%suc\"\n0.00\u00b1 0.00\n0.20\u00b1 0.13\n%prog\"\n0.40\u00b1 0.04\n0.60\u00b1 0.08\n#act#\n6.00\u00b1 0.00\n6.00\u00b1 0.00\nTable 2:\nTask\nMetric\nFlat\nHeaP\nZero-shot\nFew-shot\nZero-shot\nFew-shot\nCancel\nFlight\n%suc\"\n0.11 \u00b1 0.11\n1.00 \u00b1 0.00\n0.22 \u00b1 0.15\n1.00 \u00b1 0.00\n%prog\"\n0.19 \u00b1 0.12\n1.00 \u00b1 0.00\n0.81 \u00b1 0.04\n1.00 \u00b1 0.00\n#act#\n11.22 \u00b1 0.22\n6.00 \u00b1 0.00\n11.33 \u00b1 0.90\n6.00 \u00b1 0.00\nind\noking\n%suc\"\n0.00 \u00b1 0.00\n0.90 \u00b1 0.10\n1.00 \u00b1 0.00\n1.00 \u00b1 0.00\n%prog\"\n0 00 \u00b1 0 00\n0 90 \u00b1 0 10\n1 00 \u00b1 0 00\n1 00 \u00b1 0 00\nFigure 7: (Left) Evaluation on 3 live airline websites averaged over 10 runs per website. (Right) Difference in\ntrain (jetblue) v/s test (united, aa) website UIs.\n8\nFILL_TEXT \nflight-to \u201cLEX\"\nCHOOSE_DATE \ndatepicker \n\"10/16/2016\"\nFILL_TEXT \nflight-to \u201cLEX\"\nCHOOSE_DATE \ndatepicker \n\"10/16/2016\"\nCHOOSE_DATE \ndatepicker \n\"10/16/2016\"\nCLICK Book flight\n* TYPE 9 flight-\nto \u201cLEX\"\n* CLICK 13 \ndatepicker\n* TYPE 9 flight-to \n\u201cLEX\" \n* CLICK 26 \nLexington, KY (LEX) \n* CLICK 13 datepicker \n* CLICK 31 Prev \n* CLICK 128 Prev\n* CLICK 13 datepicker \n* CLICK 31 Prev \n* CLICK 128 Prev \n* CLICK 289 10/16/2016\n* CLICK 400 Book \nflight for $209\nTask \nPlan\nWeb\u2028\nPolicies\nBrowser UI\nLLM\u2028\nReason\nI have to \ufb01rst type \"LEX\" \nin the \ufb01eld \ufb02ight-to \ncorresponding to id 9\nThere is a corresponding \ndropdown text in \"Lexington, KY \n(LEX)\" in id 26 \nI have to click on id 26\nI have to \ufb01rst click on a \ndatepicker.\nI am currently in December \n(12/2016).  \nI have to go to October (10/2016).  \nSince 12 > 10, I have to click on \nPrev.\n I have already navigated to \nthe right month (October \n2016). I have to click on the id \ncorresponding to 10/16/2016.\nSince I have to book the shortest \n\ufb02ight, I should look at duration. The \nshortest duration is 3h 21m \ncorresponding to id 398. I have to \nclick on button below \ncorresponding to id 400.\nCLICK Search\nThe instruction says to \nCLICK Search, so I must \nissue CLICK action. \nSearch corresponds to id \n422.\n* CLICK 422 \nSearch\nFigure 8: Outputs from HeaP Few-shot on book-flight task showing hierarchical task planner actions,\nlow-level web policy actions, and LLM reasoning.\nHeaP Few-shot:\nNavigates webpages and \ncounts until it reaches 7th link\nHeaP Zero-shot:\nMistakenly clicks on the 7th link \u2028\nin the current webpage\nLLM Reason:\u2028\nThe instruction is to \n\ufb01nd and click the \nsearch link with the \nindex of 7. Since the \nlist of web ids and links \nprovided are in order, \nthe link with the index \nof 7 is the link with the \nid of 17. \nLLM Reason:\u2028\nLooking at the browser content, \nI see the following links: \nDolores, Cheree, Joye. The \nmakes the index of the links as \nDolores (0 + 1 = 1), Cheree (0 + \n2 = 2), Joye (0 + 3 = 3). I am told \nto \ufb01nd and click the link 7. Since \n7 > 3, I have to go to the next \npage. To go to the next page, I \nhave to click on the id \ncorresponding to > sign, which \ncorresponds to id 28. \n\u2026 \nCLICK 17 Joye\nCLICK 28 >\nCLICK 50 >\nCLICK 52 Riley\nFigure 9: HeaP Few-shot vs HeaP Zero-shot on a search-engine task. The instruction asks to find the\n7th link, however, it is ambiguous what 7 refers to. HeaP Few-shot with a single in-context demo is able to\nground the task in the UI and reason that the 7th link lies in the 2nd webpage and navigates to the link correctly.\nmay not know how to recover from incorrect actions. For instance, HeaP clicks on a wrong link,\nsending it to a new webpage not seen in the demonstrations. (3) Visual information gaps: Visual\nelements, such as specific dropdown menus in maps environment, do not appear in the DOM. Such\ntasks require multi-modal models that reason about browser images.\n6\nDISCUSSION AND LIMITATIONS\nIn this paper, we present a hierarchical framework HeaP for solving web tasks from few-shot\ndemonstrations. We evaluate against a range of baselines on a suite of web tasks and characterize\nperformance gains from both hierarchical prompting and demonstrations. Our key takeaways are:\n(1) Hierarchy breaks down complex tasks Our results indicate that hierarchical prompting achieves\nhigher success rates by breaking down complex tasks into reusable low-level policy calls. This is\nevident in the performance difference between HeaP Few-shot and Flat Few-shot (see Figs.\n3,4,5,7), with Fig. 2 showing the role of hierarchy in both better task decomposition and ability to\npack in more examples. (2) Sample efficient generalization HeaP matches or outperforms priors\nworks with multiple orders of magnitude less data (see Table 1). It is able to adapt to unseen tasks\nwith only a handful of task demonstrations seen in-context (see Table 3). (3) Effects of few-shot\nprompting and reasoning Few-shot examples in the prompt are effective at grounding high-level\ntask instructions as actions on the web UI environment (see Fig. 9). CoT reasoning significantly\nboosts performances across all methods, particularly on multi-step tasks (see Appendix B).\nWhile HeaP shows promise, there are still limitations and open challenges: (1) Complex Webpages.\nHeaP is currently unable to handle pages with visual only components since those observations\ndon\u2019t get parsed from the HTML DOM. Leveraging pretrained multi-modal models offer a promising\navenue (Lee et al., 2023; Furuta et al., 2023). Moreover, parsing pages containing long tables,\ndatabases needs advanced compression techniques such as learning dedicated saliency models (Wang\net al., 2022; Sridhar et al., 2023) to determine relevant web elements. (2) Verification and Error\nRecovery. HeaP may click on a wrong link sending it to a new webpage and must learn to recover\nfrom such errors. Learning from incorrect actions either via human feedback or self-verification\nare interesting directions of future work. Action LLMs also carry potential for misuse given their\nexecution on open-domain environments, requiring careful verification and security solutions.\n9\nACKNOWLEDGEMENTS\nWe would like to thank Daniel Ciolek, Volkan Cirik, Michael Griffiths for help with browser tooling and plugins.\nWe are grateful to Kilian Weinberger, Yoav Artzi, Ramya Ramakrishnan, and the rest of the ASAPP research\nteam for insightful feedback and suggestions.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nYoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to\nactions. Transactions of the Association for Computational Linguistics (TACL), 2013.\nSatchuthananthavale RK Branavan, Harr Chen, Luke S Zettlemoyer, and Regina Barzilay. Reinforcement\nlearning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual\nMeeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.\nAssociation for Computational Linguistics, 2009.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems (NeurIPS), 2020.\nAndrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A Plummer. Interac-\ntive mobile app navigation with uncertain or under-specified natural language commands. arXiv preprint\narXiv:2202.02312, 2022.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web:\nTowards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.\nFriedman. Natbot: Drive a browser with gpt-3. https://github.com/nat/natbot, 2022.\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Mul-\ntimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854,\n2023.\nIzzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate the web. arXiv\npreprint arXiv:1812.09195, 2018.\nIzzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Manoj Tiwari, Honglak Lee, and Aleksandra\nFaust. Environment generation for zero-shot compositional reinforcement learning. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2021.\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan\nNarang, Noah Fiedel, and Aleksandra Faust. Understanding HTML with large language models. arXiv\npreprint arXiv:2210.03945, 2022.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra\nFaust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv\npreprint arXiv:2307.12856, 2023.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In International Conference on Machine Learning\n(ICML), 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with\nlanguage models. arXiv preprint arXiv:2207.05608, 2022b.\nPeter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh\nAbramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. A data-driven approach for learning to\ncontrol computers. In International Conference on Machine Learning (ICML), 2022.\nLangChain. Langchain. https://python.langchain.com/en/latest/index.html, 2023.\n10\nKenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi\nKhandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as\npretraining for visual language understanding. In International Conference on Machine Learning (ICML),\n2023.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on\nweb interfaces using workflow-guided exploration. In International Conference on Learning Representations\n(ICLR), 2018.\nXiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang.\nWebglm: Towards an efficient web-enhanced question answering system with human preferences. arXiv\npreprint arXiv:2306.07906, 2023.\nSahisnu Mazumder and Oriana Riva. Flin: A flexible natural language interface for web navigation. arXiv\npreprint arXiv:2010.12844, 2020.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-step\nreasoning. arXiv preprint arXiv:2308.00436, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering\nwith human feedback. arXiv preprint arXiv:2112.09332, 2021.\nRodrigo Nogueira and Kyunghyun Cho. End-to-end goal-driven web navigation. In Advances in Neural\nInformation Processing Systems, volume 29, 2016.\nOpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins, 2023a.\nOpenAI. GPT-4 technical report, 2023b.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. arXiv preprint arXiv:2203.02155, 2022.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv\npreprint arXiv:2302.04761, 2023.\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal,\nKenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical\nuser interfaces. arXiv preprint arXiv:2306.00245, 2023.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain\nplatform for web-based agents. In International Conference on Machine Learning (ICML), 2017.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks.\nIn Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, 2020.\nSignificant Gravitas.\nAuto-GPT: An autonomous gpt-4 experiment.\nhttps://github.com/\nSignificant-Gravitas/Auto-GPT, 2023.\nAbishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and Shuyan Zhou. Hierarchical prompting assists large\nlanguage model on web navigation. arXiv preprint arXiv:2305.14257, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288, 2023.\nHuaxiaoyue Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, and Sanjiban Choudhury. Demo2code: From\nsummarizing demonstrations to synthesizing code via extended chain-of-thought. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint\narXiv:2308.11432, 2023b.\n11\nQifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu. Webformer: The web-page\ntransformer for structure information extraction. arXiv preprint arXiv:2202.00217, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\nNancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James Landay, and Monica S Lam.\nGrounding open-domain instructions to automate web support tasks. arXiv preprint arXiv:2103.16057, 2021.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language\nmodels for action generation in text-based games. arXiv preprint arXiv:2010.02903, 2020.\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web\ninteraction with grounded language agents. In Advances in Neural Information Processing Systems (NeurIPS),\n2022a.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022b.\nManzil Zaheer, Kenneth Marino, Will Grathwohl, John Schultz, Wendy Shang, Sheila Babayan, Arun Ahuja,\nIshita Dasgupta, Christine Kaeser-Chen, and Rob Fergus. Learning to navigate wikipedia by taking random\nwalks. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\nShuyan Zhou, Pengcheng Yin, and Graham Neubig. Hierarchical control of situated agents through natural\nlanguage. arXiv preprint arXiv:2109.08214, 2021.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk,\nDaniel Fried, Uri Alon, et al. WebArena: A realistic web environment for building autonomous agents. arXiv\npreprint arXiv:2307.13854, 2023.\n12\nAppendix\nTable of Contents\nA Broader Impacts\n14\nB Ablations\n14\nB.1\nEffect of Chain-of-Thought Reasoning\n. . . . . . . . . . . . . . . . . . . . . .\n14\nB.2\nModel Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nB.3\nTask-wise Performance on MiniWoB++ . . . . . . . . . . . . . . . . . . . . . .\n16\nC Experimental Details\n18\nC.1\nHeaP Planner Policy Code Architecture\n. . . . . . . . . . . . . . . . . . . . .\n18\nC.2\nHyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nD Autolabeling and Prompt Generation\n18\nD.1\nCollect and Autolabel demonstrations . . . . . . . . . . . . . . . . . . . . . . .\n18\nD.2 Autogenerate prompts from labeled demonstrations . . . . . . . . . . . . . . . .\n19\nD.3 Augmenting prompts with chain-of-thought reasoning\n. . . . . . . . . . . . . .\n19\nD.4 Autolabeling Examples\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nE Airline CRM Environment\n19\nF\nLive Websites Dataset\n22\nF.1\nCollecting Human Task Demos . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nF.2\nParsing Browser Content\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nF.3\nAugmenting with Conversations . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nG Library of Prompts\n23\nG.1\nHeaP Few-shot Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nG.2\nFlat Few-shot Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nG.3\nHeaP Zero-shot Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nG.4\nFlat Zero-shot Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nG.5 Auto-labeling Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nG.6\nReasoning Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n13\nA\nBROADER IMPACTS\nThis paper presents a novel method of using Large Language Models (LLMs) to execute web actions through\nhierarchical prompting and few-shot examples. The broader impact of our work spans both technological and\nsocietal dimensions.\nTechnological Impact:\nBy enabling LLMs to perform web actions, a vast range of applications can be\nautomated and improved, leading to greater efficiencies and new capabilities. For instance, customer service can\nbe improved by automating frequently performed web actions, thereby increasing productivity and customer\nsatisfaction. The automation can streamline processes and free up human resources for more complex, nuanced\ninteractions.\nThe idea of hierarchical prompting opens up new avenues for artificial intelligence research, especially in the\nfield of natural language understanding and web interaction. As LLMs become more and more capable, we need\nscalable approaches for decision making that allow these models handle a diverse array of tasks while being able\nto fail gracefully. Approaches that build on ideas around task decomposition, modularity and reusability would\nenable improved scalability and failure handling.\nSocietal Impact:\nEquipping LLMs with the ability to carry out web tasks opens up a a variety of possibilities\nfor societal benefits and change. For instance, the technology could transform accessibility to digital services,\nin particular for the elderly or individuals with certain disabilities.This technology also serves as a powerful\nassistant to humans by automating repetitive tasks hence reducing cognitive burden on employees across different\nsectors such as customer care and IT services.\nHowever, while these advancements promise substantial benefits, they also present challenges that need to be\ndiligently addressed. The potential for misuse of this technology, such as unauthorized data access, privacy\nbreaches, or malicious automation, is of serious concern. It is essential to develop robust security measures and\nclear ethical guidelines for the use of LLMs that respect users\u2019 privacy. Moreover, care must be taken that the\ndeployment of such technologies is to eventually augment humans and help free them up for more creative and\nnuanced interactions. This calls for careful planning and implementation, including strategies for re-skilling or\nup-skilling.\nB\nABLATIONS\nWe conduct two ablations. The first studies how the performance of all methods vary with model architectures.\nThe second looks at the performance gains from adding chain-of-thought reasoning to all prompts.\nB.1\nEFFECT OF CHAIN-OF-THOUGHT REASONING\nWhile we initially did not have chain-of-thought reasoning 4, we found that adding it in helped HeaP\nFew-shot rationalize a particular action well and boost performance. We wanted to understand which\ntasks in particular were helped by chain-of-thought reasoning, and what the trade-offs were.\nWe wanted to test the following hypothesis for HeaP Few-shot with and without chain-of-thought:\n1. Hypothesis 1: Chain-of-thought reasoning helps across all tasks. Even though chain-of-thought\nreasoning makes prompts slightly longer, the added step of rationalizing actions should always boost\nperformance.\n2. Hypothesis 2: Chain-of-thought reasoning particularly helps in multi-step tasks. Multi-step\ntasks often require breaking down a problem into a set of steps and executing each step. While\ndemonstrations certainly show how to break down task, adding chain-of-thought better rationalizes\nthis breakdown and helps generalize to new tasks not covered in the demonstrations.\nWe compared HeaP Few-shot with two versions - having chain of thought, and not having. Fig. 10 shows a\nplot of success rate for each of the 3 clusters of tasks - single, composite, multi.\nHypothesis 1: Chain-of-thought reasoning helps across all tasks.. We find this to be true since for all tasks,\nchain-of-thought performs either equally or better. This confirms that the extra tokens consumed by the reasoning\ndoes not hurt performance and in fact helps significantly in some cases.\nHypothesis 2: Chain-of-thought reasoning particularly helps in multi-step tasks. We find this to be true as\nwell. Looking at the multi-step tasks, chain-of-thought has the largest performance gains compared to any other\ncluster. The performance gains are he largest in book-flight and search-engine where the horizon length is the\n4Chain-of-Thought Prompting Elicits Reasoning in Large Language Modelshttps://arxiv.org/abs/2201.11903\n14\nNo Chain-of-thought\n Chain-of-thought\nComposite\nSingle\nMulit-step\nSuccess Rate\nSuccess Rate\nSuccess Rate\nFigure 10: Success rate of HeaP Few-shot with both chain-of-thought and without.\nlargest. In comparison, for single and composite the performace gains vary, being higher for certain tasks like\nchoose-date and find-word and zero for others like click tasks.\nB.2\nMODEL SCALING\nWhile we developed the prompts with text-davinci-003, we wanted to compare how performance varies\nwith newer models gpt-3.5-turbo and gpt-4. gpt-3.5-turbo is also an InstructGPT model optimized\nfor chat and trained at 1/10th the price of text-davinci-003. gpt-4 is a significantly larger model, and\ncapable of solving more complex tasks.\nWe wanted to test the following hypothesis:\n1. Hypothesis 1: GPT-4 improves performance across all methods, but both hierarchical prompting\nand few-shot examples help. GPT-4 is a powerful model and with an exhaustive set of instructions in\nthe prompt, it should be able to achieve perfect performance. However, designing such exhaustive\nprompts for all web tasks is challenging. We hypothesize that hierarchical prompting helps break\ndown and scope instructions leading to better performance for GPT-4. Similarly, few-shot examples\nhelps GPT-4 ground instructions in the webpage.\n2. Hypothesis 2: gpt-3.5-turbo slightly worse than text-davinci-003 given few-shot\nexamples. Practioners have noted that while gpt-3.5-turbo has better 0 shot performance,\ntext-davinci-003 is trained on a more diverse set of tasks and performs better with k-shot\nlearning https://scale.com/blog/chatgpt-vs-davinci#Introduction. Since 0\nshot performance for web-tasks is challenging without exhaustive instructions, we hypothesize that\ntext-davinci-003 will perform better.\n15\nbook-\ufb02ight \nSuccess Rate\nSearch-engine \nSuccess Rate\nSimple-algebra \nSuccess rate\ngpt-3.5-turbo \ntext-davinci-003 \ngpt-4\nFigure 11: Success rate of all methods with different models.\nWe compare three language models text-davinci-003, gpt-3.5-turbo and gpt-4 for all baselines\non 3 tasks from MiniWoB++ - book-flight, search-engine, simple-algebra. Fig. 11 shows a plot for each of these\ntasks.\nHypothesis 1: GPT-4 improves performance across all methods, but both hierarchical prompting and\nfew-shot examples help. We find this to be true. GPT-4 is unambiguously better than any other model and\nimproves all methods. However, as shown in book-flights and search-engine, both hierarchical prompting\n(HeaP) and few-shot examples boost performance. In simpler reasoning tasks like simple algebra, it gets it\ncorrect right away.\nHypothesis 2:\ngpt-3.5-turbo slightly worse than text-davinci-003 given few-shot exam-\nples. We also find evidence for this. text-davinci-003 with few-shot examples always outperforms\ngpt-3.5-turbo. Although, in simple-algebra, zero shot performance of gpt-3.5-turbo is better than\ntext-davinci-003, matching what other practitioners have observed in other tasks.\nB.3\nTASK-WISE PERFORMANCE ON MINIWOB++\nTable 4 shows a task-wise performance breakup on the MiniWoB++ benchmark for various models. We choose\na set of 45 tasks that don\u2019t require visual reasoning. HEAP FEW-SHOT has competitive performance to prior\nworks like WGE Liu et al. (2018), CC-Net Humphreys et al. (2022), WebN-T5 Gur et al. (2022), WebGUM\nFuruta et al. (2023) while using 1000x or more lower data. See Table 1 for aggregate metrics on success rates\nand dataset sizes.\nThe gap in HEAP FEW-SHOT and FLAT FEW-SHOT performance is particularly pronounced on tasks involving\nmultiple steps and composing different low-level actions, demonstrating that hierarchy is helpful in solving\nmore complex tasks. Few-shot variants of both HeaP and FLAT generally leads to improved success rates and\n16\nWGE\nCC-Net\nWebN-T5\nWebGUM\nFlat\nFlat\nHeaP\nHeaP\nTask\n(SL+RL)\n(HTML)\nZero-shot\nFew-shot\nZero-shot\nFew-shot\n%suc\u2191\n%suc\u2191\n%suc\u2191\n%suc\u2191\n%suc\u2191\n#act\u2193\n%suc\u2191\n#act\u2193\n%suc\u2191\n#act\u2193\n%suc\u2191\n#act\u2193\nsimple\nclick-link\n1.00\n0.99\n1.00\n1.00\n0.94\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nclick-option\n1.00\n0.99\n0.37\n1.00\n0.76\n3.68\n1.00\n2.62\n0.80\n2.94\n1.00\n1.94\nfocus-text\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nclick-button\n1.00\n1.00\n1.00\n0.98\n0.98\n1.02\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nclick-button-sequence\n0.99\n1.00\n1.00\n1.00\n0.96\n2.00\n1.00\n2.00\n1.00\n2.00\n1.00\n2.00\nclick-dialog\n1.00\n1.00\n1.00\n1.00\n1.00\n1.06\n1.00\n1.20\n1.00\n1.28\n1.00\n1.02\nclick-dialog-2\n1.00\n1.00\n0.24\n0.34\n0.98\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.02\nclick-tab\n0.55\n1.00\n0.74\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.04\nclick-test\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nclick-test-2\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nenter-text\n1.00\n1.00\n0.89\n1.00\n1.00\n2.50\n1.00\n2.00\n1.00\n2.10\n1.00\n2.00\nfocus-text-2\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nenter-text-dynamic\n1.00\n1.00\n0.98\n1.00\n0.98\n2.44\n1.00\n2.00\n0.98\n2.06\n1.00\n2.00\nenter-password\n0.99\n1.00\n0.97\n1.00\n1.00\n3.08\n1.00\n3.00\n1.00\n3.20\n1.00\n4.52\nlogin-user\n0.99\n1.00\n0.82\n1.00\n0.96\n3.42\n1.00\n3.00\n1.00\n3.06\n1.00\n3.00\nclick-pie\n0.32\n0.97\n0.51\n0.99\n1.00\n3.00\n1.00\n3.00\n1.00\n3.00\n1.00\n3.00\nenter-date\n0.00\n1.00\n0.00\n1.00\n1.00\n3.00\n1.00\n2.00\n0.00\n4.08\n1.00\n2.00\ngrid-coordinate\n1.00\n1.00\n0.49\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nclick-widget\n0.93\n1.00\n1.00\n1.00\n0.94\n1.00\n0.94\n1.00\n0.94\n1.00\n1.00\n1.00\ncomplex\nemail-inbox\n0.43\n1.00\n0.38\n0.99\n0.40\n7.00\n0.70\n4.50\n0.00\n3.00\n0.90\n5.20\nemail-inbox-nl-turk\n0.77\n1.00\n0.23\n0.92\n0.40\n7.20\n0.50\n6.00\n0.00\n2.90\n1.00\n4.58\nemail-inbox-forward-nl-turk\n-\n1.00\n0.33\n1.00\n0.30\n5.08\n0.40\n4.80\n0.00\n3.50\n0.90\n4.30\nmulti-orderings\n0.99\n1.00\n0.88\n1.00\n0.56\n3.60\n0.98\n3.98\n0.76\n4.28\n1.00\n4.00\nchoose-date\n0.00\n0.97\n0.00\n0.08\n0.20\n3.00\n0.94\n3.60\n0.20\n5.80\n1.00\n5.40\nclick-collapsible-2\n0.65\n0.98\n0.00\n0.94\n0.60\n3.64\n0.74\n4.14\n0.34\n3.34\n0.80\n4.50\nsimple-arithmetic\n-\n0.86\n0.00\n0.00\n0.82\n2.12\n0.92\n2.12\n0.54\n2.66\n1.00\n2.00\nclick-tab-2\n0.64\n0.98\n0.18\n0.94\n0.76\n5.58\n0.88\n4.62\n0.88\n2.84\n1.00\n2.24\nclick-tab-2-hard\n-\n0.98\n0.12\n0.57\n0.68\n3.36\n0.88\n3.84\n0.76\n3.06\n1.00\n2.42\nmulti-layouts\n0.99\n1.00\n0.83\n1.00\n0.42\n4.46\n0.72\n3.94\n0.66\n4.82\n0.94\n4.00\ncopy-paste\n-\n0.79\n0.00\n0.00\n0.14\n2.14\n0.70\n3.48\n0.98\n2.72\n1.00\n2.00\nclick-collapsible\n1.00\n1.00\n0.00\n1.00\n0.54\n1.76\n0.68\n1.88\n0.86\n1.88\n1.00\n2.00\nchoose-date-easy\n-\n0.99\n0.03\n1.00\n0.74\n2.74\n0.62\n2.62\n0.20\n10.18\n1.00\n3.10\ncopy-paste-2\n-\n0.63\n0.00\n0.00\n0.54\n7.66\n0.56\n4.00\n0.48\n3.84\n0.96\n2.04\nsimple-algebra\n-\n0.75\n0.00\n0.00\n0.14\n8.80\n0.30\n6.78\n0.04\n4.38\n0.74\n2.00\nclick-checkboxes\n0.98\n0.98\n0.96\n1.00\n0.40\n4.90\n0.44\n5.94\n0.74\n3.20\n0.90\n3.14\nclick-checkboxes-transfer\n0.64\n0.99\n0.63\n0.99\n0.40\n4.80\n0.40\n3.90\n0.54\n3.20\n0.94\n2.84\nlogin-user-popup\n-\n1.00\n0.72\n0.97\n0.46\n6.28\n0.46\n3.52\n0.46\n5.82\n1.00\n4.88\nclick-checkboxes-soft\n0.51\n0.95\n0.54\n1.00\n0.00\n7.00\n0.00\n7.30\n0.04\n6.94\n0.54\n5.64\nenter-text-2\n-\n0.98\n0.00\n0.00\n0.00\n2.60\n0.40\n5.20\n0.40\n2.00\n1.00\n2.00\nemail-inbox-forward-nl\n-\n1.00\n0.60\n1.00\n0.10\n5.04\n0.10\n4.58\n0.00\n3.24\n0.74\n4.74\nsearch-engine\n0.26\n1.00\n0.34\n0.59\n0.38\n3.64\n0.38\n3.16\n0.26\n4.46\n1.00\n4.30\nfind-word\n-\n0.88\n0.00\n0.00\n0.22\n2.62\n0.26\n5.18\n0.12\n2.92\n0.98\n2.00\nchoose-date-medium\n-\n0.99\n0.00\n0.57\n0.32\n2.90\n0.20\n2.76\n0.20\n9.26\n1.00\n3.86\nclick-checkboxes-large\n0.68\n0.71\n0.22\n0.98\n0.00\n8.40\n0.20\n8.40\n0.00\n7.00\n1.00\n6.20\nbook-flight\n0.00\n0.87\n0.00\n0.48\n0.00\n16.00\n0.10\n11.10\n0.00\n13.52\n0.90\n9.14\nMean\n0.77\n0.96\n0.56\n0.91\n0.62\n3.70\n0.72\n3.38\n0.60\n3.43\n0.96\n2.89\nTable 4: Task-wise performance breakup on MiniWoB++ benchmark on a set of 45 tasks.\nclass Policy: \n   def __init__(self, env): \n       self.env = env \n       self.base_prompt = None \n       self.prev_actions = [] \n   def act(self, **kwargs): \n       # abstract method to \nexecute policy \n       pass \nclass HighLevelTask(Policy): \n   def __init__(self, env): \n       self.env = env \n       self.base_prompt = getattr(prompts, \u201ctask_planner\u201d) \n       self.prev_actions = [] \n   def _get_llm_action(self, obs, objective): \n       pass \n   def _is_done(self, obs, action): \n      pass \n   def act(self, objective): \n       # call low-level policies \n       pass \nclass LowLevelPolicy(Policy): \n   def __init__(self, env): \n       self.env = env \n       self.base_prompt = \ngetattr(prompts, \u201cfill_text\u201d) \n       self.prev_actions = [] \n   def _get_llm_action(self, obs, \nobjective): \n       pass \n   def _is_done(self, obs, action): \n      pass \n   def act(self, objective): \n       # execute action on browser \n       pass \nclass LowLevelPolicy(Policy): \n   def __init__(self, env): \n       self.env = env \n       self.base_prompt = \ngetattr(prompts, \u201cchoose_date\") \n       self.prev_actions = [] \n   def _get_llm_action(self, obs, \nobjective): \n       pass \n   def _is_done(self, obs, action): \n      pass \n   def act(self, objective): \n       # execute action on browser \n       pass \nclass LowLevelPolicy(Policy): \n   def __init__(self, env): \n       self.env = env \n       self.base_prompt = \ngetattr(prompts, \u201cfind_directions\u201d) \n       self.prev_actions = [] \n   def _get_llm_action(self, obs, \nobjective): \n       pass \n   def _is_done(self, obs, action): \n      pass \n   def act(self, objective): \n       # execute action on browser \n       pass \nFigure 12: HeaP Planner Policy Code Architecture\nefficiency, as evident from the reduced number of actions needed for task completion. This underlines the\nimportance of having a handful of demonstrations to ground the task on the UI.\n17\nC\nEXPERIMENTAL DETAILS\nC.1\nHEAP PLANNER POLICY CODE ARCHITECTURE\nFig. 12 shows the hierarchical architecture as a code skeleton consisting of a high-level task planner and various\nlow-level policies for executing web actions. The high-level planner is represented by the HighLevelTask\nclass, which inherits from a generic Policy class. LowLevelPolicy also inherits from the generic Policy\nclass, but is instantiated with different base prompts to serve as specialized policies for various sub-tasks.\nHighLevelTask is responsible for orchestrating the execution of tasks by calling upon different low-level\npolicies as needed to accomplish specific sub-tasks. It does so by instantiating the LowLevelPolicy class\nwith different base prompt parameters, like fill_text, choose_date, find_directions, etc. This\nhelps tailor the low-level policy for a specialized sub-task like filling text fields, selecting dates, finding directions.\nThis design allows for the easy addition of new policies to handle complex web tasks, creating a library of skills,\nhence providing a dynamic and flexible framework for executing a wide array of web tasks.\nC.2\nHYPER-PARAMETERS\nWe use OpenAI API for model calls, text-davinci-003 as the default model with a temperature of 0.3\nand set the number of calls to 3. For gpt-3.5-turbo gpt-4 we use the same temperature and number of\ncalls. Below are the exact API calls,\nListing 1: Hyper-parameters for different models\n1\nif (model_type == \"gpt-3.5-turbo\"):\n2\nresponse = openai.ChatCompletion.create(\n3\nmodel=model_type,\n4\nmessages=[{\"role\": \"user\", \"content\": prompt}],\n5\ntemperature=0.3,\n6\ntop_p=1,\n7\nn=3,\n8\nmax_tokens=max_tokens\n9\n)\n10\nresponse = response.choices[0].message.content.strip()\n11\n12\nelif (model_type == \"gpt-4\"):\n13\nresponse = openai.ChatCompletion.create(\n14\nmodel=model_type,\n15\nmessages=[{\"role\": \"user\", \"content\": prompt}],\n16\ntemperature=0.3,\n17\ntop_p=1,\n18\nn=3,\n19\nmax_tokens=max_tokens\n20\n)\n21\nresponse = response.choices[0].message.content.strip()\n22\n23\nelif (model_type == \"text-davinci-003\"):\n24\nresponse = openai.Completion.create(\n25\nmodel=model_type,\n26\nprompt=prompt,\n27\ntemperature=0.3,\n28\nbest_of=3,\n29\nn=3,\n30\nmax_tokens=max_tokens\n31\n)\n32\nresponse = response.choices[0].text.strip()\nD\nAUTOLABELING AND PROMPT GENERATION\nWe first collect a dataset of human demonstrations. We then \u201cautolabel\u201d each timestep with the low-level policy\nthat was being executed. We convert the labelled datasets to prompts for both high-level planner and low-level\npolicy. Finally, we augment prompts with chain-of-thought reasoning.\nD.1\nCOLLECT AND AUTOLABEL DEMONSTRATIONS\nWe collect demonstrations from a human user who is given a conversation context \u03d5 and performs a sequence\nof actions in the browser to complete the task. We design a browser plugin to record webpage DOM d and\nevents such as clicks and types. Each demonstration is parsed as text by converting the DOM tree into a\nlist of salient web elements like links, buttons, inputs. Each element is represented as text, e.g. <button\n18\nid=18 title=\"Travelers\">1 Adult />. Action events are parsed as a CLICK <id> or TYPE\n<id> <val>. The parsed demonstration dataset is represented as D = {(\u03d5, s1, a1, . . . , sT , aT )}.\nWe\nthen\n\u201cautolabel\u201d\neach\ntimestep\nt\nwith\na\nlow-level\npolicy\n\u03c0t\nand\ninstruction\n\u03c8t\nto\ncre-\nate\na\nlabeled\ndataset\n(\u03d5, s1, a1, \u03c01, \u03c81, . . . , sT , aT , \u03c0T , \u03c8T ).\nIdeally,\nto\nlabel\nfull\nsequences,\nwe\nwould\nselect\nthe\nmost\nlikely\nsequence\nof\npolicies\ngiven\nthe\nentire\nstate-action\nsequence\narg max\u03c01:T ,\u03c81:T log P(\u03c01, \u03c81 . . . , \u03c0T , \u03c8T |s1, a1, . . . , sT , aT ). However, this becomes impractical given\ndemonstrations are too long to fit in to context length. Instead, we relax the problem to predicting the current\npolicy given the previous policy, i.e. arg max\u03c0t P(\u03c0t|\u03c0t\u22121, st, at).\nWe construct an autolabeling prompt (Appendix G.5) that consists of an overall instruction along with a small set\nof human-annotated labels as in-context examples. Each in-context example has the following format: {input:\n[state st, action at, previous policy \u03c0t\u22121)], output:\ncurrent policy \u03c0t}.\nIn-context examples are chosen such that they cover every low-level policy. We note that the auto-labeller can\ngeneralize to new tasks not contained in the prompt as long as the constituent low-level policies are covered (e.g.\nSearch Flight \u2192 Cancel Booking)\nD.2\nAUTOGENERATE PROMPTS FROM LABELED DEMONSTRATIONS\nOnce we have a dataset of labeled demonstrations Dlabel = {(\u03d5, s0, a0, (\u03c00, \u03c80), . . . , sT , aT , (\u03c0T , \u03c8T ))},\nwe can use that to generate base prompts for both the high-level task planner and low-level policies. For\nthe task planner, we concatenate the following input\u2013output pairs as in-context examples in the base prompt:\n{input:\n[context \u03d5, initial state s0]} and {output:\nsequence of web policy\ncalls (\u03c01, \u03c81), (\u03c02, \u03c82) . . . , (\u03c0T , \u03c8T )}. For each web policy, we search the dataset Dlabel for all instances\nof the policy and create a prompt by concatenating examples of {input:\n[instruction \u03c8t, current\nstate st, previous actions at:t\u2212k]} and {output:\nnext action at}.\nD.3\nAUGMENTING PROMPTS WITH CHAIN-OF-THOUGHT REASONING\nEmpirically, we observed that directly going from input browser content to output web actions can be difficult or\nerror prone for the LLM. Chain-of-thought prompting Wei et al. (2022) has shown to be effective to mitigate\nthis. We additionally include a reasoning section between the input and output sections of the prompt that forces\nthe LLM to generate a series of short sentences that provide justifications for the actions it predictions. We\nfound this to uniformly improve performance for both task and policy prompts (Appendix B). We subsequently\nadded in a simple reason generator prompt (Appendix G.6) that works across both high-level tasks and low-level\npolicies. The prompt takes the current browser content st, previous actions at\u22121:t\u2212k, current action at and\ngenerates a reason r.\nD.4\nAUTOLABELING EXAMPLES\nWe include the following set of in-context examples for autolabeling that allows us to cover the different low-level\npolicies:\n1. MiniWoB: In-context examples include 1 book flight demo, 1 search engine demo.\nThis\nlets us cover CHOOSE_DATE(),\nFILL_TEXT(),\nFIND_AND_CLICK_SEARCH_LINK(),\nFIND_AND_CLICK_TAB_LINK() policies.\n2. Airline CRM: In-context examples include 1 demo of search flight, covering CHOOSE_DATE(),\nFILL_TEXT() policies.\n3. LiveWeb:\nIn-context examples include 1 demo of book flight, covering CHOOSE_DATE(),\nFILL_TEXT()\nE\nAIRLINE CRM ENVIRONMENT\nWe constructed a mock Airline CRM environment based on typical airline call-center workflows, and modelled\non public airline websites (e.g., jetblue.com, aa.com). The website is built as a standard single-page web\napplication, using a light-weight web development stack. Figs 13, 14 shows sample screenshots from the CRM.\nThis CRM allows us to test scenarios that are more complex than those in MiniWoB++, and scenarios that cannot\npractically be run on public websites (e.g., cancelling a flight). The scenarios currently supported are described\nbelow.\n1. Find one-way or return flight. This is a single step task, that requires the source & destination\nairports and flight dates to be entered in the UI, and the search button to be clicked.\n19\nFigure 13: search flight screen of the mock airline CRM.\n2. Book flight This is a four step task:\n(a) Find flight (scenario 1),\n(b) Select desired outward and return flights & click Confirm,\n(c) Enter passenger details (Title, first name, last name, gender, date-of-birth) & click Save,\n(d) Enter payment card details (card number, expiry, CVC/CVV) & click Book flight.\n3. Find a booking This is a single step task - enter booking reference & click Search.\n4. Cancel a booking This is a three step task:\n(a) Find booking (scenario 3),\n(b) Click Cancel,\n(c) Confirm cancellation by re-entering booking reference & click Cancel.\n5. Modify passenger details on an existing booking This is a three step task:\n(a) Find booking (scenario 3),\n(b) Click Modify,\n(c) Change any required passenger details & click Save.\n6. Modify flights on an existing booking This is\n(a) Find booking (scenario 3),\n(b) Click Modify,\n(c) Find flight (scenario 1),\n20\nFigure 14: find and modify booking screen of the mock airline CRM.\n(d) Select desired outward and return flights & click Save,\nIn addition to supporting the above scenarios, the CRM also exposes a few helper APIs that make running and\nevaluating experiments easier. Two of these are of interest here:\n\u2022 https://<base-url>/generate-random-scenario. This API returns a scenario ran-\ndomly selected from those listed above, along with all of the data required for completing that scenario\non the UI. Shown below is an example of a scenario returned by this API. In addition to the task\nspecific data, the scenario includes a unique id, and a unique URL on which the task can be executed.\nListing 2: Mock data table used to create a task scenario\n1\n{\n2\n\"scenario\": \"TASK_FIND_FLIGHT\",\n3\n\"id\": \"ylmjd3iuqpdc3gdrvspq\",\n4\n\"url\": \"https://<base-url>/?scenario=ylmjd3iuqpdc3gdrvspq\",\n5\n\"details\": {\n6\n\"flight\": {\n7\n\"from\": \"JFK\",\n8\n\"to\": \"FLL\",\n9\n\"departure\": \"2023-07-07\",\n10\n\"return\": \"2023-09-13\",\n11\n\"outward-departure-time\": \"7:01pm\",\n12\n\"outward-arrival-time\": \"0:13pm\",\n13\n\"return-departure-time\": \"6:00am\",\n14\n\"return-arrival-time\": \"8:43am\"\n15\n}\n16\n}\n17\n}\n\u2022 https://<base-url>/evaluate?scenario=<id> This API provides a means of automat-\nically evaluating the success rate and task progress metrics for scenarios generated by the API above.\nSpecifically, if the UI actions are performed on the unique URL returned by the generate-random-\nscenario API, calling the evaluate API afterwards with the scenario id will return the metrics. These\nmetrics are calculated with reference to the gold standard actions required for the given scenario.\n21\nF\nLIVE WEBSITES DATASET\nF.1\nCOLLECTING HUMAN TASK DEMOS\nWe collected a dataset of human agents searching for flights across three websites: https://www.jetblue.\ncom/, https://www.aa.com/, https://www.united.com/en/us. For each of the websites, a\nhuman agent was given a set of 10 task specifications as short conversations, e.g. \"Book a flight departing from\n<>, arriving at <>, leaving on <> and returning on <>\". The actions of the human agent, e.g. the click and types,\nwere recorded for each episode. Along with the actions, the raw DOM of the website was also recorded. This is\ncrucial as over the course of the paper, the DOM would change as a result of updates to the website. This also\ngoes to show that simply memorizing actions is unlikely to succeed for these live website tasks.\nF.2\nPARSING BROWSER CONTENT\nGiven a data point of raw website DOM (Document Object Model) and action, we make use of playwright\nhttps://playwright.dev to parse the DOM and extract a list of web elements. This process involves\ntraversing the DOM tree and extracting salient nodes, i.e. nodes with actionable elements like <input>, <button>,\n<link>. We also propagate up text attributes from child nodes to the salient parent nodes since the text label for\nan element may occur inside the subtree. Every web element in the list is represented by an id and optionally a\nvalue. The list of all such elements is concatenated to represent the browser content in natural text form. This\nis input as the browser observation in the LLM context. Natural language descriptions attached to different web\nelements helps it generalize across different websites since LLMs have been pre-trained on natural language\ndata. This text form is included under Browser Content in the LLM context. We also convert the demonstrated\nactions to CLICK <id> or TYPE <id> \"TEXT\".\nF.3\nAUGMENTING WITH CONVERSATIONS\nEach episode of the dataset is represented by the task, the sequence of browser content and actions. Since we\nwanted to test how models can generalize to different conversations, we used an LLM to automatically generate\nconversations between a customer and agent that converted the task specification into a short conversational\ncontext. We specified the conversations to be diverse and set the temperature to be high (0.6) which naturally\nresulted in different conversations for every task.\n22\nG\nLIBRARY OF PROMPTS\nWe list out prompts for the four different methods HeaP Few-shot, Flat Few-shot, HeaP\nZero-shot, Flat Zero-shot, that we use to evaluate MiniWoB++ tasks. We also include the train\ntime auto labeling (and reasoning) prompts used to annotate human-collected demonstrations for the search\nflight task in the LiveWeb dataset.\nG.1\nHEAP FEW-SHOT PROMPT\nListing 3: HeaP Few-shot Task Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. To do\nthis, you will be given specific information and allowed to issue certain actions that\nwill get you closest to achieving your objective.\n3\n4\nYou are given:\n5\n1. CONTEXT: The goal you need to achieve, either explicitly stated or implied from a\nconversation between a customer (CUS) and agent (REP).\n6\n2. BROWSER CONTENT: A simplified text description of the BROWSER CONTENT, without formatting\nelements.\n7\n3. URL: The current webpage URL.\n8\n4. PREVIOUS ACTIONS: A list of your past actions.\n9\n10\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue these actions:\n11\n- CLICK <id> - Click on the specified element.\n12\n- FILL_TEXT <id> \"TEXT\": Fill a text box with the given text.\n13\n- CHOOSE_DATE <id> \"DATE\": Select a date value from a datepicker.\n14\n- FIND_AND_CLICK_SEARCH_LINK \"LINK_NUMBER\": When dealing with a search engine, find and click\na link the LINK_NUMBER from search results.\n15\n- CLICK_CHECKBOXES \"CONTEXT\": Select checkbox(es) specified by CONTEXT\n16\n- PROCESS_EMAIL \"CONTEXT\": Find / Reply / Process any email task specified by CONTEXT\n17\n- DONE: Once you finish issuing all actions.\n18\n19\nHere are a few examples:\n20\n21\n### Input:\n22\nCONTEXT:\n23\nBook the shortest one-way flight from: LEB to: RDG on 12/26/2016\n24\nBROWSER CONTENT:\n25\n<body id=1 val= />\n26\n<div id=2 val=wrap />\n27\n<div id=3 val=area />\n28\n<div id=4 val=menu />\n29\n<h2 id=5 val=Book Your One-Way Flight />\n30\n<div id=6 val= />\n31\n<input_text id=7 val=flight-from />\n32\n<div id=8 val= />\n33\n<input_text id=9 val=flight-to />\n34\n<div id=10 val= />\n35\n<div id=11 val=Departure Date />\n36\n<div id=12 val= />\n37\n<input_text id=13 val=datepicker />\n38\n<div id=14 val= />\n39\n<button id=15 val=Search />\n40\n<div id=16 val= />\n41\n<div id=23 val=1 result is available, use up and down arrow keys to navigate. />\n42\n<div id=18 val= />\n43\n<div id=27 val=1 result is available, use up and down arrow keys to navigate. />\n44\nURL: https://\n45\nPREVIOUS ACTIONS:\n46\n47\n### Response:\n48\nREASONING:\n49\nI will first check the instruction to see if the CONTEXT falls in that list. No it does not.\nTherefore, I can proceed.\n50\nI have to book the shortest flight from LEB to RDG on 12/26/2016\n51\nLooking at the browser content, I have to fill in the following fields\n52\nI have to fill flight-from with \"LEB\"\n53\nI have to fill flight-to with \"RDG\"\n54\nI have to choose date from the datepicker as 12/26/2016\n55\nI have to click Search\n56\nFinally, I have to issue DONE\n57\nACTION:\n58\nFILL_TEXT flight-from \"LEB\"\n59\nFILL_TEXT flight-to \"RDG\"\n60\nCHOOSE_DATE datepicker \"12/26/2016\"\n23\n61\nCLICK Search\n62\nDONE\n63\n64\n### Input:\n65\nCONTEXT:\n66\nCopy the text in the textarea below, paste it into the textbox and press Submit.\n67\nBROWSER CONTENT:\n68\n<body id=1 val= />\n69\n<div id=2 val=wrap />\n70\n<div id=3 val=area />\n71\n<div id=4 val=container />\n72\n<input_text id=5 val=/>\n73\n<textarea id=6 val=Commodo adipiscing eu erat enim ipsum sodales eget id />\n74\n<button id=7 val=Submit />\n75\nURL:\n76\nhttps://\n77\nPREVIOUS ACTIONS:\n78\n79\n### Response:\n80\nREASONING:\n81\nI will first check the instruction to see if the CONTEXT falls in that list. No it does not.\nTherefore, I can proceed.\n82\nI have to first copy the text from text area id=6 to input_text id=5\n83\nI have to click Submit\n84\nFinally, I have to issue DONE\n85\nACTION:\n86\nTYPE 5 \"Commodo adipiscing eu erat enim ipsum sodales eget id\"\n87\nCLICK 7\n88\nDONE\n89\n90\n### Input:\n91\nCONTEXT:\n92\nFind the 2nd word in the paragraph, type that into the textbox and press \"Submit\".\n93\nBROWSER CONTENT:\n94\n<body id=1 val= />\n95\n<div id=2 val=wrap />\n96\n<div id=3 val=area />\n97\n<p id=4 val=Dis urna proin placerat neque, lectus turpis. />\n98\n<input_text id=5 val=answer-input />\n99\n<button id=6 val=Submit />\n100\nURL:\n101\nhttps://\n102\nPREVIOUS ACTIONS:\n103\n104\n### Response:\n105\nREASONING:\n106\nI will first check the instruction to see if the CONTEXT falls in that list. No it does not.\nTherefore, I can proceed.\n107\nI have to find the 2nd word in the paragraph and type it into the textbox.\n108\nThe paragraph is in id 4, \"Dis urna proin placerat neque, lectus turpis.\"\n109\nThe 2nd word is \"urna\".\n110\nI have to type that in id 5.\n111\nThen, I have to click Submit.\n112\nFinally, I have to issue DONE\n113\nACTION:\n114\nTYPE 5 \"urna\"\n115\nCLICK 6\n116\nDONE\n117\n118\n### Input:\n119\nCONTEXT:\n120\nBook the shortest one-way flight from: Cincinnati, OH to: LEX on 10/16/2016.\n121\nBROWSER CONTENT:\n122\n<label id=362 val=Depart: />\n123\n<div id=364 val=2:50 PM />\n124\n<div id=365 val=Sun Oct 16 2016 />\n125\n<div id=366 val=Cincinnati, OH (CVG) />\n126\n<label id=369 val=Arrives: />\n127\n<div id=371 val=5:32 AM />\n128\n<div id=372 val=Mon Oct 17 2016 />\n129\n<div id=373 val=LEX />\n130\n<label id=376 val=Duration: />\n131\n<div id=378 val=14h 42m />\n132\n<button id=380 val=Book flight for $379 />\n133\n<div id=383 val= />\n134\n<label id=406 val=Depart: />\n135\n<div id=408 val=11:06 PM />\n136\n<div id=409 val=Sun Oct 16 2016 />\n137\n<div id=410 val=Cincinnati, OH (CVG) />\n138\n<label id=413 val=Arrives: />\n139\n<div id=415 val=2:27 AM />\n24\n140\n<div id=416 val=Mon Oct 17 2016 />\n141\n<div id=417 val=LEX />\n142\n<label id=420 val=Duration: />\n143\n<div id=422 val=3h 21m />\n144\n<button id=424 val=Book flight for $209 />\n145\n<div id=425 val= />\n146\n<label id=428 val=Depart: />\n147\n<div id=430 val=3:23 AM />\n148\n<div id=431 val=Sun Oct 16 2016 />\n149\n<div id=432 val=Cincinnati, OH (CVG) />\n150\n<label id=435 val=Arrives: />\n151\n<div id=437 val=5:19 AM />\n152\n<div id=438 val=Mon Oct 17 2016 />\n153\n<div id=439 val=LEX />\n154\n<label id=442 val=Duration: />\n155\n<div id=444 val=25h 56m />\n156\n<button id=446 val=Book flight for $1197 />\n157\nURL:\n158\nhttps://\n159\nPREVIOUS ACTIONS:\n160\n161\n### Response:\n162\nREASONING:\n163\nI will first check the instruction to see if the CONTEXT falls in that list. No it does not.\nTherefore, I can proceed.\n164\nI have to book the shortest flight from\nCincinnati, OH to: LEX\n165\nLooking at the browser content, I see different flight options with flight durations and\nprices\n166\nSince I have to book the shortest flight, I should look at duration\n167\nThe shortest duration is 3h 21m corresponding to id 422\n168\nTo book this flight, I have to click on button below corresponding to id 424\n169\nFinally, I have to issue DONE\n170\nACTION:\n171\nCLICK <button id=424 val=Book flight for $209 />\n172\nDONE\n173\n174\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n175\n176\n### Input:\n177\nCONTEXT:\n178\n{context}\n179\nBROWSER CONTENT:\n180\n{browser_content}\n181\nURL:\n182\n{url}\n183\nPREVIOUS ACTIONS:\n184\n{previous_actions}\n185\n186\n### Response:\nListing 4: HeaP Few-shot FILL_TEXT() Policy Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. Your\ngoal is to fill a text box. Here are the instructions:\n3\n1. First type in a text box.\n4\n2. If there is a dropdown text, click on the corresponding id.\n5\n3. Issue DONE.\n6\n7\nYou are given:\n8\n1. CONTEXT: An instruction like FILL_TEXT <id> \"TEXT\"\n9\n2. BROWSER CONTENT: A simplified text description of the BROWSER CONTENT, without formatting\nelements.\n10\n3. URL: The current webpage URL.\n11\n4. PREVIOUS ACTIONS: A list of your past actions.\n12\n13\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue these actions:\n14\n- TYPE <id> \"TEXT\" - Type \"TEXT\" into the input element.\n15\n- CLICK <id> - Click on the specified element.\n16\n- DONE - Once you finish issuing all actions.\n17\n18\nHere are a few examples:\n19\n20\n### Input:\n21\nCONTEXT:\n22\nFILL_TEXT flight-from \"LEB\"\n23\nBROWSER CONTENT:\n24\n<body id=1 val= />\n25\n<div id=2 val=wrap />\n25\n26\n<div id=3 val=area />\n27\n<div id=4 val=menu />\n28\n<h2 id=5 val=Book Your One-Way Flight />\n29\n<div id=6 val= />\n30\n<input_text id=7 val=flight-from />\n31\n<div id=8 val= />\n32\n<input_text id=9 val=flight-to />\n33\n<div id=10 val= />\n34\nURL:\n35\nhttps://\n36\nPREVIOUS ACTIONS:\n37\n38\n### Response:\n39\nREASONING:\n40\nI have no previous actions.\n41\nI have to first type \"LEB\" in the field flight-from corresponding to id 7\n42\nACTION:\n43\nTYPE 7 flight-from \"LEB\"\n44\n45\n### Input:\n46\nCONTEXT:\n47\nFILL_TEXT flight-from \"LEB\"\n48\nBROWSER CONTENT:\n49\n<body id=1 val= />\n50\n<div id=2 val=wrap />\n51\n<div id=3 val=area />\n52\n<div id=4 val=menu />\n53\n<h2 id=5 val=Book Your One-Way Flight />\n54\n<div id=6 val= />\n55\n<input_text id=7 val=flight-from />\n56\n<div id=8 val= />\n57\n<div id=14 val= />\n58\n<button id=15 val=Search />\n59\n<ul id=18 val=ui-id-1 />\n60\n<li id=19 val= />\n61\n<div id=20 val=Hanover, NH (HNV) />\n62\n<li id=21 val= />\n63\n<div id=22 val=Lebanon, NH (LEB) />\n64\n<li id=23 val= />\n65\n<div id=24 val=White River, VT (WHR) />\n66\n<div id=16 val= />\n67\n<div id=25 val=3 results are available, use up and down arrow keys to navigate. />\n68\n<div id=17 val= />\n69\nURL:\n70\nhttps://\n71\nPREVIOUS ACTIONS:\n72\nTYPE 7 flight-from \"LEB\"\n73\n74\n### Response:\n75\nREASONING:\n76\nI have already typed in \"LEB\" in id 7\n77\nThere is a corresponding dropdown text in \"Lebanon, NH (LEB)\" in id 22\n78\nI have to click on id 22 Lebanon, NH (LEB)\n79\nACTION:\n80\nCLICK 22 Lebanon, NH (LEB)\n81\n82\n### Input:\n83\nCONTEXT:\n84\nFILL_TEXT flight-from \"LEB\"\n85\nBROWSER CONTENT:\n86\n<body id=1 val= />\n87\n<div id=2 val=wrap />\n88\n<div id=3 val=area />\n89\n<div id=4 val=menu />\n90\n<h2 id=5 val=Book Your One-Way Flight />\n91\n<div id=6 val= />\n92\n<input_text id=7 val=flight-from />\n93\n<div id=8 val= />\n94\n<input_text id=9 val=flight-to />\n95\n<div id=10 val= />\n96\n<div id=11 val=Departure Date />\n97\n<div id=12 val= />\n98\n<input_text id=13 val=datepicker />\n99\nURL:\n100\nhttps://\n101\n102\n### Response:\n103\nPREVIOUS ACTIONS:\n104\nTYPE 7 flight-from \"LEB\"\n105\nCLICK 22 Lebanon, NH (LEB)\n106\nREASONING:\n26\n107\nI have already typed in \"LEB\" in id 7\n108\nI have clicked on the dropdown text in id 22 Lebanon, NH (LEB)\n109\nI am done filling text\n110\nACTION:\n111\nDONE\n112\n113\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n114\n115\n### Input:\n116\nCONTEXT:\n117\n{context}\n118\nBROWSER CONTENT:\n119\n{browser_content}\n120\nURL:\n121\n{url}\n122\nPREVIOUS ACTIONS:\n123\n{previous_actions}\n124\n125\n### Response:\nListing 5: HeaP Few-shot CHOOSE_DATE() Policy Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. Your\ngoal is to choose a date from a datepicker.\nHere are the instructions:\n3\n1. First click on the datepicker\n4\n2. Check if you are in the right month, else use Prev to Navigate to the right month\n5\n3. Once you are at the right month, click on the right date\n6\n4. Once you have clicked the date, issue DONE.\n7\n8\nYou are given:\n9\n1. CONTEXT: An instruction like CHOOSE_DATE <id> \"DATE\".\n10\n2. BROWSER CONTENT: A simplified text description of the BROWSER CONTENT, without formatting\nelements.\n11\n3. URL: The current webpage URL.\n12\n4. PREVIOUS ACTIONS: A list of your past actions.\n13\n14\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue these actions:\n15\n- CLICK <id> - Click on the specified element.\n16\n- TYPE <id> \"TEXT\" - Type \"TEXT\" into the input element.\n17\n- DONE - Once you finish issuing all actions.\n18\n19\nHere are a few examples:\n20\n21\n### Input:\n22\nCONTEXT:\n23\nCHOOSE_DATE datepicker \"11/03/2016\"\n24\nBROWSER CONTENT:\n25\n<body id=1 val= />\n26\n<div id=2 val=wrap />\n27\n<div id=3 val=area />\n28\n<p id=4 val= />\n29\n<t id=-1 val=Date: />\n30\n<input_text id=5 val=datepicker />\n31\n<button id=6 val=Submit />\n32\nURL:\n33\nhttps://\n34\nPREVIOUS ACTIONS:\n35\n36\n### Response:\n37\nREASONING:\n38\nI have no previous actions.\n39\nI have to first click on a datepicker.\n40\nACTION:\n41\nCLICK 5 datepicker\n42\n43\n### Input:\n44\nCONTEXT:\n45\nCHOOSE_DATE datepicker \"11/03/2016\"\n46\nBROWSER CONTENT:\n47\n<div id=8 val= />\n48\n<a id=9 val= />\n49\n<span id=10 val=Prev />\n50\n<a id=11 val= />\n51\n<div id=13 val= />\n52\n<span id=14 val=December />\n53\n<span id=15 val=2016 />\n54\n<a id=40 val=12/1/2016 />\n55\n<a id=42 val=12/2/2016 />\n27\n56\n<a id=44 val=12/3/2016 />\n57\n<a id=47 val=12/4/2016 />\n58\n<a id=49 val=12/5/2016 />\n59\n<a id=51 val=12/6/2016 />\n60\n<a id=53 val=12/7/2016 />\n61\n<a id=55 val=12/8/2016 />\n62\n<a id=57 val=12/9/2016 />\n63\n<a id=59 val=12/10/2016 />\n64\n<a id=62 val=12/11/2016 />\n65\n<a id=64 val=12/12/2016 />\n66\n<a id=66 val=12/13/2016 />\n67\n<a id=68 val=12/14/2016 />\n68\nURL:\n69\nhttps://\n70\nPREVIOUS ACTIONS:\n71\nCLICK 5 datepicker\n72\n73\n### Response:\n74\nREASONING:\n75\nI have already clicked on datepicker.\n76\nLooking at the BROWSER CONTENT val, I am currently in Decemeber (12/2016).\n77\nI have to go to November (11/2016).\n78\nSince 11 < 12, I have to click on Prev\n79\nACTION:\n80\nCLICK 10 Prev\n81\n82\n### Input:\n83\nCONTEXT:\n84\nCHOOSE_DATE datepicker \"11/03/2016\"\n85\nBROWSER CONTENT:\n86\n<tbody id=33 val= />\n87\n<a id=40 val=11/1/2016 />\n88\n<a id=42 val=11/2/2016 />\n89\n<a id=44 val=11/3/2016 />\n90\n<a id=47 val=11/4/2016 />\n91\n<a id=49 val=11/5/2016 />\n92\n<a id=51 val=11/6/2016 />\n93\n<a id=53 val=11/7/2016 />\n94\n<a id=55 val=11/8/2016 />\n95\n<a id=57 val=11/9/2016 />\n96\n<a id=59 val=11/10/2016 />\n97\n<a id=62 val=11/11/2016 />\n98\n<a id=64 val=11/12/2016 />\n99\n<a id=66 val=11/13/2016 />\n100\n<a id=68 val=11/14/2016 />\n101\n<a id=70 val=11/15/2016 />\n102\nURL:\n103\nhttps://\n104\nPREVIOUS ACTIONS:\n105\nCLICK 5 datepicker\n106\nCLICK 10 Prev\n107\n108\n### Response:\n109\nREASONING:\n110\nI have already clicked on datepicker.\n111\nLooking at the BROWSER CONTENT val, I am currently in November (11/2016).\n112\nI have to go to November (11/2016).\n113\nSince 11 = 11, I am in the correct month.\n114\nI have to click on the id corresponding to 11/3/2016\n115\nACTION:\n116\nCLICK 44 11/3/2016\n117\n118\n### Input:\n119\nCONTEXT:\n120\nCHOOSE_DATE datepicker \"11/03/2016\"\n121\nURL:\n122\nhttps://\n123\nBROWSER CONTENT:\n124\n<body id=1 val= />\n125\n<div id=2 val=wrap />\n126\n<div id=3 val=area />\n127\n<p id=4 val= />\n128\n<t id=-1 val=Date: />\n129\n<input_text id=5 val=datepicker />\n130\n<button id=6 val=Submit />\n131\nPREVIOUS ACTIONS:\n132\nCLICK 5 datepicker\n133\nCLICK 10 prev\n134\nCLICK 44 11/3/2016\n135\n136\n### Response:\n28\n137\nREASONING:\n138\nI am done selecting the dates.\n139\nACTION:\n140\nDONE\n141\n142\n### Input:\n143\nCONTEXT:\n144\nCHOOSE_DATE 5 \"05/20/2010\"\n145\nBROWSER CONTENT:\n146\n<body id=1 val= />\n147\n<div id=2 val=wrap />\n148\n<div id=3 val=area />\n149\n<div id=4 val=form />\n150\n<input_date id=5 val=tt />\n151\n<button id=6 val=Submit />\n152\n153\n### Response:\n154\nREASONING:\n155\nI see no datepicker. I will directly type the date.\n156\nACTION:\n157\nTYPE 5 \"05/20/2010\"\n158\n159\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n160\n161\n### Input:\n162\nCONTEXT:\n163\n{context}\n164\nBROWSER CONTENT:\n165\n{browser_content}\n166\nURL:\n167\n{url}\n168\nPREVIOUS ACTIONS:\n169\n{previous_actions}\n170\n171\n### Response:\nG.2\nFLAT FEW-SHOT PROMPT\nListing 6: Flat Few-shot Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. To do\nthis, you will be given specific information and allowed one action at a time that get\nyou closest to achieving your objective.\n3\n4\nYou are given:\n5\n1. CONTEXT: The goal you need to achieve, either explicitly stated or implied from a\nconversation between a customer (CUS) and agent (REP).\n6\n2. BROWSER CONTENT: A simplified text description of the current browser content, without\nformatting elements.\n7\n3. URL: The current webpage URL.\n8\n4. PREVIOUS ACTIONS: A list of your past actions.\n9\n10\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue any one of these actions:\n11\n- CLICK <id> - Click on the specified element.\n12\n- TYPE <id> \"TEXT\" - Type \"TEXT\" into the input element.\n13\n- DONE - Once you finish issuing all actions.\n14\n15\nHere are a few examples:\n16\n17\n### Input:\n18\nCONTEXT:\n19\nFind the 2nd word in the paragraph, type that into the textbox and press \"Submit\".\n20\nBROWSER CONTENT:\n21\n<body id=1 val= />\n22\n<div id=2 val=wrap />\n23\n<div id=3 val=area />\n24\n<p id=4 val=Dis urna proin placerat neque, lectus turpis. />\n25\n<input_text id=5 val=answer-input />\n26\n<button id=6 val=Submit />\n27\nURL:\n28\nhttps://\n29\nPREVIOUS ACTIONS:\n30\n31\n### Response:\n32\nREASONING:\n33\nI have to find the 2nd word in the paragraph and type it into the textbox.\n29\n34\nThe paragraph is in id 4, \"Dis urna proin placerat neque, lectus turpis.\"\n35\nThe 2nd word is \"urna\".\n36\nI have to type that in id 5.\n37\nACTION:\n38\nTYPE 5 \"urna\"\n39\n40\n### Input:\n41\nCONTEXT:\n42\nFind the 2nd word in the paragraph, type that into the textbox and press \"Submit\".\n43\nBROWSER CONTENT:\n44\n<body id=1 val= />\n45\n<div id=2 val=wrap />\n46\n<div id=3 val=area />\n47\n<p id=4 val=Dis urna proin placerat neque, lectus turpis. />\n48\n<input_text id=5 val=urna />\n49\n<button id=6 val=Submit />\n50\nURL:\n51\nhttps://\n52\nPREVIOUS ACTIONS:\n53\nTYPE 5 \"urna\"\n54\n55\n### Response:\n56\nREASONING:\n57\nLooking at the current browser context, I can verify that I have typed in \"urna\" in id 5.\n58\nI have to click submit.\n59\nACTION:\n60\nCLICK 6\n61\n62\n### Input:\n63\nCONTEXT:\n64\nCopy the text in the textarea below, paste it into the textbox and press Submit.\n65\nBROWSER CONTENT:\n66\n<body id=1 val= />\n67\n<div id=2 val=wrap />\n68\n<div id=3 val=area />\n69\n<div id=4 val=container />\n70\n<input_text id=5 val=Commodo adipiscing eu erat enim ipsum sodales eget id/>\n71\n<textarea id=6 val=Commodo adipiscing eu erat enim ipsum sodales eget id />\n72\n<button id=7 val=Submit />\n73\nURL:\n74\nhttps://\n75\nPREVIOUS ACTIONS:\n76\nTYPE 5 \"Commodo adipiscing eu erat enim ipsum sodales eget id\"\n77\n78\n### Response:\n79\nREASONING:\n80\nI have already typed in the text in the textbox.\n81\nI have to clicl submit\n82\nACTION:\n83\nCLICK 7\n84\n85\n### Input:\n86\nCONTEXT:\n87\nSelect 11/03/2016 as the date and hit submit.\n88\nBROWSER CONTENT:\n89\n<input_text id=5 val=datepicker />\n90\n<div id=8 val= />\n91\n<a id=9 val= />\n92\n<span id=10 val=Prev />\n93\n<a id=11 val= />\n94\n<div id=13 val= />\n95\n<span id=14 val=December />\n96\n<span id=15 val=2016 />\n97\n<a id=40 val=12/1/2016 />\n98\n<a id=42 val=12/2/2016 />\n99\n<a id=44 val=12/3/2016 />\n100\n<a id=47 val=12/4/2016 />\n101\n<a id=49 val=12/5/2016 />\n102\n<a id=51 val=12/6/2016 />\n103\n<a id=53 val=12/7/2016 />\n104\n<a id=55 val=12/8/2016 />\n105\n<a id=57 val=12/9/2016 />\n106\nURL:\n107\nhttps://\n108\nPREVIOUS ACTIONS:\n109\nCLICK 5\n110\n111\n### Response:\n112\nREASONING:\n113\nI have already clicked on datepicker.\n114\nLooking at the current browser content val, I am currently in Decemeber (12/2016).\n30\n115\nI have to go to November (11/2016).\n116\nSince 11 < 12, I have to click on Prev\n117\nACTION:\n118\nCLICK 10\n119\n120\n### Input:\n121\nCONTEXT:\n122\nSelect 11/03/2016 as the date and hit submit.\n123\nBROWSER CONTENT:\n124\n<tbody id=33 val= />\n125\n<a id=40 val=11/1/2016 />\n126\n<a id=42 val=11/2/2016 />\n127\n<a id=44 val=11/3/2016 />\n128\n<a id=47 val=11/4/2016 />\n129\n<a id=49 val=11/5/2016 />\n130\n<a id=51 val=11/6/2016 />\n131\n<a id=53 val=11/7/2016 />\n132\n<a id=55 val=11/8/2016 />\n133\n<a id=57 val=11/9/2016 />\n134\n<a id=59 val=11/10/2016 />\n135\n<a id=62 val=11/11/2016 />\n136\n<a id=64 val=11/12/2016 />\n137\n<a id=66 val=11/13/2016 />\n138\n<a id=68 val=11/14/2016 />\n139\n<a id=70 val=11/15/2016 />\n140\nURL:\n141\nhttps://\n142\nPREVIOUS ACTIONS:\n143\nCLICK 5\n144\nCLICK 10\n145\n146\n### Response:\n147\nREASONING:\n148\nI have already clicked on datepicker.\n149\nLooking at the current browser content val, I am currently in November (11/2016).\n150\nI have to go to November (11/2016).\n151\nSince 11 = 11, I am in the correct month.\n152\nI have to click on the id corresponding to 11/3/2016\n153\nACTION:\n154\nCLICK 44\n155\n156\n### Input\n157\nCONTEXT:\n158\nSelect 11/03/2016 as the date and hit submit.\n159\nBROWSER CONTENT:\n160\n<body id=1 val= />\n161\n<div id=2 val=wrap />\n162\n<div id=3 val=area />\n163\n<p id=4 val= />\n164\n<t id=-4 val=Date: />\n165\n<input_text id=5 val=11/03/2016 />\n166\n<button id=6 val=Submit />\n167\nURL:\n168\nhttps://\n169\nPREVIOUS ACTIONS:\n170\nCLICK 5\n171\nCLICK 10\n172\nCLICK 139\n173\n174\n### Response\n175\nREASONING:\n176\nLooking at the current browser content, I see the data field input_text id=5 has the correct\ndate 11/03/2016\n177\nI must go ahead and click submit\n178\nACTION:\n179\nCLICK 6\n180\n181\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n182\n183\n### Input:\n184\nCONTEXT:\n185\n{context}\n186\nBROWSER CONTENT:\n187\n{browser_content}\n188\nURL:\n189\n{url}\n190\nPREVIOUS ACTIONS:\n191\n{previous_actions}\n192\n193\n### Response:\n31\nG.3\nHEAP ZERO-SHOT PROMPT\nListing 7: HeaP Zero-shot Task Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. To do\nthis, you will be given specific information and allowed to issue certain actions that\nwill get you closest to achieving your objective.\n3\n4\nYou are given:\n5\n1. CONTEXT: The goal you need to achieve, either explicitly stated or implied from a\nconversation between a customer (CUS) and agent (REP).\n6\n2. BROWSER CONTENT: A simplified text description of the BROWSER CONTENT, without formatting\nelements.\n7\n3. URL: The current webpage URL.\n8\n4. PREVIOUS ACTIONS: A list of your past actions.\n9\n10\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue these actions:\n11\n- CLICK <id> - Click on the specified element.\n12\n- FILL_TEXT <id> \"TEXT\": Fill a text box with the given text.\n13\n- CHOOSE_DATE <id> \"DATE\": Select a date value from a datepicker.\n14\n- FIND_AND_CLICK_SEARCH_LINK \"LINK_NUMBER\": When dealing with a search engine, find and click\na link the LINK_NUMBER from search results.\n15\n- CLICK_CHECKBOXES \"CONTEXT\": Select checkbox(es) specified by CONTEXT\n16\n- PROCESS_EMAIL \"CONTEXT\": Find / Reply / Process any email task specified by CONTEXT\n17\n- DONE: Once you finish issuing all actions.\n18\n19\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n20\n21\n### Input:\n22\nCONTEXT:\n23\n{context}\n24\nBROWSER CONTENT:\n25\n{browser_content}\n26\nURL:\n27\n{url}\n28\nPREVIOUS ACTIONS:\n29\n{previous_actions}\n30\n31\n### Response:\nListing 8: HeaP Zero-shot FILL_TEXT() Policy Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. Your\ngoal is to fill a text box. Here are the instructions:\n3\n1. First type in a text box.\n4\n2. If there is a dropdown text, click on the corresponding id.\n5\n3. Issue DONE.\n6\n7\nYou are given:\n8\n1. CONTEXT: An instruction like FILL_TEXT <id> \"TEXT\"\n9\n2. BROWSER CONTENT: A simplified text description of the BROWSER CONTENT, without formatting\nelements.\n10\n3. URL: The current webpage URL.\n11\n4. PREVIOUS ACTIONS: A list of your past actions.\n12\n13\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue these actions:\n14\n- TYPE <id> \"TEXT\" - Type \"TEXT\" into the input element.\n15\n- CLICK <id> - Click on the specified element.\n16\n- DONE - Once you finish issuing all actions.\n17\n18\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n19\n20\n### Input:\n21\nCONTEXT:\n22\n{context}\n23\nBROWSER CONTENT:\n24\n{browser_content}\n25\nURL:\n26\n{url}\n27\nPREVIOUS ACTIONS:\n28\n{previous_actions}\n29\n30\n### Response:\n32\nListing 9: HeaP Zero-shot CHOOSE_DATE() Policy Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. Your\ngoal is to choose a date from a datepicker.\nHere are the instructions:\n3\n1. First click on the datepicker\n4\n2. Check if you are in the right month, else use Prev to Navigate to the right month\n5\n3. Once you are at the right month, click on the right date\n6\n4. Once you have clicked the date, issue DONE.\n7\n8\nYou are given:\n9\n1. CONTEXT: An instruction like CHOOSE_DATE <id> \"DATE\".\n10\n2. BROWSER CONTENT: A simplified text description of the BROWSER CONTENT, without formatting\nelements.\n11\n3. URL: The current webpage URL.\n12\n4. PREVIOUS ACTIONS: A list of your past actions.\n13\n14\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue these actions:\n15\n- CLICK <id> - Click on the specified element.\n16\n- TYPE <id> \"TEXT\" - Type \"TEXT\" into the input element.\n17\n- DONE - Once you finish issuing all actions.\n18\n19\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n20\n21\n### Input:\n22\nCONTEXT:\n23\n{context}\n24\nBROWSER CONTENT:\n25\n{browser_content}\n26\nURL:\n27\n{url}\n28\nPREVIOUS ACTIONS:\n29\n{previous_actions}\n30\n31\n### Response:\nG.4\nFLAT ZERO-SHOT PROMPT\nListing 10: Flat Zero-shot Prompt\n1\n### Instruction:\n2\nYou are an AI assistant performing tasks in a web browser on behalf of a human agent. To do\nthis, you will be given specific information and allowed one action at a time that get\nyou closest to achieving your objective.\n3\n4\nYou are given:\n5\n1. CONTEXT: The goal you need to achieve, either explicitly stated or implied from a\nconversation between a customer (CUS) and agent (REP).\n6\n2. BROWSER CONTENT: A simplified text description of the current browser content, without\nformatting elements.\n7\n3. URL: The current webpage URL.\n8\n4. PREVIOUS ACTIONS: A list of your past actions.\n9\n10\nYou can only interact with web elements like links, inputs, and buttons in the browser content\n. You can issue any one of these actions:\n11\n- CLICK <id> - Click on the specified element.\n12\n- TYPE <id> \"TEXT\" - Type \"TEXT\" into the input element.\n13\n- DONE - Once you finish issuing all actions.\n14\n15\nThe current CONTEXT, URL, and BROWSER CONTENT follow. Respond with REASONING and ACTION.\n16\n17\n### Input:\n18\nCONTEXT:\n19\n{context}\n20\nBROWSER CONTENT:\n21\n{browser_content}\n22\nURL:\n23\n{url}\n24\nPREVIOUS ACTIONS:\n25\n{previous_actions}\n26\n27\n### Response:\nG.5\nAUTO-LABELING PROMPT\n33\nListing 11: Autolabel Prompt\n1\n### Instruction:\n2\nYou are an AI assistant labeling web actions as one of several skills.\n3\n4\nYou are given:\n5\n- a CONTEXT that contains the objective you are trying to achieve. The objective may be\nexplicitly stated or be implicit in a conversation between a customer (CUS) and agent (\nREP).\n6\n- A simplified text description of the browser content and the current webpage URL. The\nbrowser content is highly simplified with all formatting elements removed.\n7\n- The current web action being performed\n8\n- The previous label that was assigned\n9\n10\nYou can only assign labels from the following types\n11\n- FILL_TEXT \"desription\" \"TEXT\" - fill a text box\n12\n- CHOOSE_DATE \"desription\" \"DATE\" - choose a date from a date grid\n13\n- CLICK \"desription\" - click a button\n14\n15\nHere are a few examples:\n16\n17\n### Input:\n18\nCONTEXT:\n19\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n20\nBROWSER CONTENT:\n21\n<text id=10>JetBlue Home</text>\n22\n<text id=11>Where on Earth are you headed?</text>\n23\n<button id=12>Flights</button>\n24\n<button id=13>Flights + Hotel</button>\n25\n<button id=14>Flights + Cruise</button>\n26\n<button id=15>Cars</button>\n27\n<button id=16>Stays</button>\n28\n<button id=17>Roundtrip</button>\n29\n<button id=18 title=\"Travelers\">1 Adult</button>\n30\n<text id=19>Use TrueBlue points</text>\n31\n<text id=20>From</text>\n32\n<input id=21 text>Washington D.C. area (WAS)</input>\n33\n<button id=22>Reverse origin and destination city or airport</button>\n34\n<text id=23>To</text>\n35\n<input id=24 text/>\n36\n<text id=25>Depart</text>\n37\n<input id=26 Depart Press DOWN ARROW key to select available dates/>\n38\n<text id=27>Return</text>\n39\n<input id=28 Return Press DOWN ARROW key to select available dates/>\n40\n<button id=29>Search flights</button>\n41\nCURRENT ACTION:\n42\nTYPE 21 Boston\n43\nPREVIOUS LABEL:\n44\n45\n### Response:\n46\nCURRENT LABEL:\n47\nFILL_TEXT From \"Boston\"\n48\n49\n### Input:\n50\nCONTEXT:\n51\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n52\nBROWSER CONTENT:\n53\n<text id=20>From</text>\n54\n<input id=21 text>Boston</input>\n55\n<text id=22>Boston</text>\n56\n<text id=23>area</text>\n57\n<text id=24>Boston</text>\n58\n<text id=25>, MA (BOS)</text>\n59\n<text id=26>Providence, RI (PVD)</text>\n60\n<text id=27>Worcester, MA (ORH)</text>\n61\n<text id=28>Browse by regions</text>\n62\n<text id=29>Browse by regions</text>\n63\n<text id=30>Mint Service</text>\n64\n<text id=31>Partner Airline</text>\n65\n<button id=32>Reverse origin and destination city or airport</button>\n66\n<text id=33>To</text>\n67\n<input id=34 text/>\n68\n<text id=35>Depart</text>\n69\n<input id=36 Depart Press DOWN ARROW key to select available dates/>\n70\n<text id=37>Return</text>\n71\n<input id=38 Return Press DOWN ARROW key to select available dates/>\n72\n<button id=39>Search flights</button>\n73\nCURRENT ACTION:\n74\nCLICK 24\n75\nPREVIOUS LABEL:\n76\nFILL_TEXT From \"Boston\"\n34\n77\n78\n### Response:\n79\nCURRENT LABEL:\n80\nFILL_TEXT From \"Boston\"\n81\n82\n### Input:\n83\nCONTEXT:\n84\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n85\nBROWSER CONTENT:\n86\n<button id=13>Flights + Hotel</button>\n87\n<button id=14>Flights + Cruise</button>\n88\n<button id=15>Cars</button>\n89\n<button id=16>Stays</button>\n90\n<button id=17>Roundtrip</button>\n91\n<button id=18 title=\"Travelers\">1 Adult</button>\n92\n<text id=19>Use TrueBlue points</text>\n93\n<text id=20>From</text>\n94\n<input id=21 text>Boston, MA (BOS)</input>\n95\n<button id=22>Reverse origin and destination city or airport</button>\n96\n<text id=23>To</text>\n97\n<input id=24 text/>\n98\n<text id=25>Browse by regions</text>\n99\n<text id=26>Mint Service</text>\n100\n<text id=27>Partner Airline</text>\n101\n<text id=28>Depart</text>\n102\n<input id=29 Depart Press DOWN ARROW key to select available dates/>\n103\n<text id=30>Return</text>\n104\n<input id=31 Return Press DOWN ARROW key to select available dates/>\n105\n<button id=32>Search flights</button>\n106\nCURRENT ACTION:\n107\nTYPE 24 Chicago\n108\nPREVIOUS LABEL:\n109\nFILL_TEXT From \"Boston\"\n110\n111\n### Response:\n112\nCURRENT LABEL:\n113\nFILL_TEXT To \"Chicago\"\n114\n115\n### Input:\n116\nCONTEXT:\n117\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n118\nCURRENT BROWSER CONTENT:\n119\n<text id=20>From</text>\n120\n<input id=21 text>Boston, MA (BOS)</input>\n121\n<button id=22>Reverse origin and destination city or airport</button>\n122\n<text id=23>To</text>\n123\n<input id=24 text>Chicago</input>\n124\n<text id=25>Chicago</text>\n125\n<text id=26>, IL (ORD)</text>\n126\n<text id=27>Browse by regions</text>\n127\n<text id=28>Browse by regions</text>\n128\n<text id=29>Mint Service</text>\n129\n<text id=30>Partner Airline</text>\n130\n<text id=31>Depart</text>\n131\n<input id=32 Depart Press DOWN ARROW key to select available dates/>\n132\n<text id=33>Return</text>\n133\n<input id=34 Return Press DOWN ARROW key to select available dates/>\n134\n<button id=35>Search flights</button>\n135\nCURRENT ACTION:\n136\nCLICK 25\n137\nPREVIOUS LABEL:\n138\nFILL_TEXT To \"Chicago\"\n139\n140\n### Response:\n141\nCURRENT LABEL:\n142\nFILL_TEXT To \"Chicago\"\n143\n144\n### Input:\n145\nCONTEXT:\n146\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n147\nBROWSER CONTENT:\n148\n<button id=17>Roundtrip</button>\n149\n<button id=18 title=\"Travelers\">1 Adult</button>\n150\n<text id=19>Use TrueBlue points</text>\n151\n<text id=20>From</text>\n152\n<input id=21 text>Boston, MA (BOS)</input>\n153\n<button id=22>Reverse origin and destination city or airport</button>\n154\n<text id=23>To</text>\n155\n<input id=24 text>Chicago, IL (ORD)</input>\n156\n<text id=25>Depart</text>\n157\n<input id=26 Depart Press DOWN ARROW key to select available dates/>\n35\n158\n<text id=27>Return</text>\n159\n<input id=28 Return Press DOWN ARROW key to select available dates/>\n160\n<button id=29 aria-label=\"Previous Month\"/>\n161\n<text id=30>April 2023</text>\n162\n<text id=31>S</text>\n163\n<text id=32>M</text>\n164\n<text id=33>T</text>\n165\n<text id=34>W</text>\n166\n<text id=35>T</text>\n167\n<text id=36>F</text>\n168\n<text id=37>S</text>\n169\n<button id=38 aria-label=\"depart Saturday, April 1, 2023\">1</button>\n170\n<button id=39 aria-label=\"depart Sunday, April 2, 2023\">2</button>\n171\n<button id=40 aria-label=\"depart Monday, April 3, 2023\">3</button>\n172\n<button id=41 aria-label=\"depart Tuesday, April 4, 2023\">4</button>\n173\n<button id=42 aria-label=\"depart Wednesday, April 5, 2023\">5</button>\n174\n<button id=43 aria-label=\"depart Thursday, April 6, 2023\">6</button>\n175\n<button id=44 aria-label=\"depart Friday, April 7, 2023\">7</button>\n176\nCURRENT ACTION:\n177\nTYPE 26 12/01/2023\n178\nPREVIOUS LABEL:\n179\nFILL_TEXT To \"Chicago\"\n180\n181\n### Response:\n182\nCURRENT LABEL:\n183\nCHOOSE_DATE Depart 12/01/2023\n184\n185\n### Input:\n186\nCONTEXT:\n187\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n188\nBROWSER CONTENT:\n189\n<button id=18 title=\"Travelers\">1 Adult</button>\n190\n<text id=19>Use TrueBlue points</text>\n191\n<text id=20>From</text>\n192\n<input id=21 text>Boston, MA (BOS)</input>\n193\n<button id=22>Reverse origin and destination city or airport</button>\n194\n<text id=23>To</text>\n195\n<input id=24 text>Chicago, IL (ORD)</input>\n196\n<text id=25>Depart</text>\n197\n<input id=26 Depart Press DOWN ARROW key to select available dates>12/01/2023</input>\n198\n<text id=27>Return</text>\n199\n<input id=28 Return Press DOWN ARROW key to select available dates/>\n200\n<button id=29 aria-label=\"Previous Month\"/>\n201\n<text id=30>April 2023</text>\n202\n<text id=31>S</text>\n203\n<text id=32>M</text>\n204\n<text id=33>T</text>\n205\n<text id=34>W</text>\n206\n<text id=35>T</text>\n207\n<text id=36>F</text>\n208\n<text id=37>S</text>\n209\n<button id=38 aria-label=\"depart Saturday, April 1, 2023\">1</button>\n210\n<button id=39 aria-label=\"depart Sunday, April 2, 2023\">2</button>\n211\nCURRENT ACTION:\n212\nTYPE 28 12/12/2023\n213\nPREVIOUS LABEL:\n214\nCHOOSE_DATE Depart 12/01/2023\n215\n216\n### Response:\n217\nCURRENT LABEL:\n218\nCHOOSE_DATE Return 12/12/2023\n219\n220\n### Input:\n221\nCONTEXT:\n222\nBook a flight from Boston to Chicago departing on Dec 1, 2023 and returning on Dec 12, 2023\n223\nBROWSER CONTENT:\n224\n<link id=4>Book</link>\n225\n<link id=5>Manage Trips</link>\n226\n<link id=6>Check In</link>\n227\n<link id=7>Travel Info</link>\n228\n<link id=8>TrueBlue</link>\n229\n<link id=9 aria-label=\"Shopping cart (Empty)\"/>\n230\n<text id=108>Flights available for sale through Mar 19, 2024.</text>\n231\n<button id=109>Done</button>\n232\n<button id=110>Search flights</button>\n233\nCURRENT ACTION:\n234\nCLICK 110\n235\nPREVIOUS LABEL:\n236\nCHOOSE_DATE Return 12/12/2023\n237\n238\n### Response:\n36\n239\nCURRENT LABEL:\n240\nCLICK Search flights\n241\n242\nThe current context, browser content, previous label follow. Response with your CURRENT LABEL.\n243\n244\n### Input:\n245\nCONTEXT:\n246\n{context}\n247\nCURRENT BROWSER CONTENT:\n248\n{browser_content}\n249\nCURRENT ACTION:\n250\n{current_action}\n251\nPREVIOUS LABEL:\n252\n{previous_label}\n253\n254\n### Response:\n255\nCURRENT LABEL:\nG.6\nREASONING PROMPT\nListing 12: Reasoning Prompt\n1\n### Instruction:\n2\nYou are an AI assistant whose goal is to generate reasoning as to why a particular web action\nwas taken. You are given:\n3\n1. CONTEXT: An instruction stating the overall goal you need to achieve\n4\n2. CURRENT BROWSER CONTENT: A simplified text description of the current browser content,\nwithout formatting elements.\n5\n3. CURRENT URL: The current webpage URL\n6\n4. PREVIOUS ACTIONS: A list of your past actions.\n7\n5. YOUR ACTION: The current action that you took\n8\n9\nProvide reasoning (after REASONING:) for why the current action (ACTION:) was taken.\n10\n11\nHere are some examples:\n12\n13\n### Input:\n14\nCONTEXT:\n15\nCHOOSE_DATE Depart \"12/03/2023\"\n16\nBROWSER CONTENT:\n17\n<button id=18 title=\"Travelers\">1 Adult</button>\n18\n<text id=19>Use TrueBlue points</text>\n19\n<text id=20>From</text>\n20\n<input id=21 text>New York City area (NYC)</input>\n21\n<button id=22>Reverse origin and destination city or airport</button>\n22\n<text id=23>To</text>\n23\n<input id=24 text>Boston area</input>\n24\n<text id=25>Depart</text>\n25\n<input id=26 Depart Press DOWN ARROW key to select available dates/>\n26\n<text id=27>Return</text>\n27\n<input id=28 Return Press DOWN ARROW key to select available dates/>\n28\n<button id=29 aria-label=\"Previous Month\"/>\n29\n<text id=30>April 2023</text>\n30\n<text id=31>S</text>\n31\n<text id=32>M</text>\n32\n<text id=33>T</text>\n33\n<text id=34>W</text>\n34\n<text id=35>T</text>\n35\n<text id=36>F</text>\n36\n<text id=37>S</text>\n37\n<button id=38 aria-label=\"depart Saturday, April 1, 2023\">1</button>\n38\n<button id=39 aria-label=\"depart Sunday, April 2, 2023\">2</button>\n39\n<button id=40 aria-label=\"depart Monday, April 3, 2023\">3</button>\n40\n<button id=41 aria-label=\"depart Tuesday, April 4, 2023\">4</button>\n41\n<button id=42 aria-label=\"depart Wednesday, April 5, 2023\">5</button>\n42\n<button id=63 aria-label=\"depart Wednesday, April 26, 2023\">26</button>\n43\n<button id=64 aria-label=\"depart Thursday, April 27, 2023\">27</button>\n44\n<button id=65 aria-label=\"depart Friday, April 28, 2023\">28</button>\n45\n<button id=66 aria-label=\"depart Saturday, April 29, 2023\">29</button>\n46\n<button id=67 aria-label=\"depart Sunday, April 30, 2023\">30</button>\n47\nURL:\n48\nhttps://www.jetblue.com/\n49\nPREVIOUS ACTIONS:\n50\nACTION:\n51\nTYPE 26 Depart Press DOWN ARROW key to select available dates \"12/03/2023\"\n52\n53\n### Response:\n54\nREASONING:\n37\n55\nI have no previous actions.\n56\nI have to first type \"12/03/2023\" in the field Depart which corresponds to input id 26\n57\n58\n### Input:\n59\nCONTEXT:\n60\nFILL_TEXT From \"Seattle\"\n61\nBROWSER CONTENT:\n62\n<button id=16>Stays</button>\n63\n<button id=17>Roundtrip</button>\n64\n<button id=18 title=\"Travelers\">1 Adult</button>\n65\n<text id=19>Use TrueBlue points</text>\n66\n<text id=20>From</text>\n67\n<input id=21 text>Seattle</input>\n68\n<text id=22>Seattle</text>\n69\n<text id=23>, WA (SEA)</text>\n70\n<text id=24>Seattle</text>\n71\n<text id=25>/King Country, WA (BFI)</text>\n72\n<text id=26>Browse by regions</text>\n73\n<text id=27>Browse by regions</text>\n74\n<text id=28>Mint Service</text>\n75\n<text id=29>Partner Airline</text>\n76\n<button id=30>Reverse origin and destination city or airport</button>\n77\nURL:\n78\nhttps://www.jetblue.com/\n79\nPREVIOUS ACTIONS:\n80\nTYPE 21 \"Seattle\"\n81\nACTION:\n82\nCLICK 22 Seattle\n83\n84\n### Response:\n85\nREASONING:\n86\nI have already typed in \"Seattle\" in id 21\n87\nI should next check if there is a dropdown text below id 21\n88\nYes, there is a corresponding dropdown text \"Seattle\" in id 22\n89\nI have to click on id 22\n90\n91\n### Input:\n92\nCONTEXT:\n93\n{context}\n94\nBROWSER CONTENT:\n95\n{browser_content}\n96\nURL:\n97\n{url}\n98\nPREVIOUS ACTIONS:\n99\n{previous_actions}\n100\nACTION:\n101\n{current_action}\n102\n103\n### Response:\n104\nREASONING:\n38\n"
  },
  {
    "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately",
    "link": "https://arxiv.org/pdf/2310.03046.pdf",
    "upvote": "5",
    "text": "Preprint\nECOASSISTANT: USING LLM ASSISTANT MORE\nAFFORDABLY AND ACCURATELY\nJieyu Zhang\u2217*,1, Ranjay Krishna1, Ahmed H. Awadallah2, Chi Wang2\n1University of Washington\n2Micorsoft Research\n1{jieyuz2,ranjay}@cs.washington.edu\n2{hassanam,wang.chi}@microsoft.com\nABSTRACT\nToday, users ask Large language models (LLMs) as assistants to answer queries\nthat require external knowledge; they ask about the weather in a specific city, about\nstock prices, and even about where specific locations are within their neighborhood.\nThese queries require the LLM to produce code that invokes external APIs to answer\nthe user\u2019s question, yet LLMs rarely produce correct code on the first try, requiring\niterative code refinement upon execution results. In addition, using LLM assistants\nto support high query volumes can be expensive. In this work, we contribute a\nframework, EcoAssistant1, that enables LLMs to answer code-driven queries\nmore affordably and accurately. EcoAssistant contains three components.\nFirst, it allows the LLM assistants to converse with an automatic code executor\nto iteratively refine code or to produce answers based on the execution results.\nSecond, we use a hierarchy of LLM assistants, which attempts to answer the query\nwith weaker, cheaper LLMs before backing off to stronger, expensive ones. Third,\nwe retrieve solutions from past successful queries as in-context demonstrations\nto help subsequent queries. Empirically, we show that EcoAssistant offers\ndistinct advantages for affordability and accuracy, surpassing GPT-4 by 10 points\nof success rate with less than 50% of GPT-4\u2019s cost.\n1\nINTRODUCTION\nRecently, users have been using conversational LLMs such as ChatGPT (OpenAI, 2023) for var-\nious queries. Reports indicate that 23% of ChatGPT user queries are for knowledge extraction\npurposes (Fishkin, 2023). Many of these queries require knowledge that is external to the information\nstored within any pre-trained large language models (LLMs). For example, users ask about the\nweather in their city: \u201cWhat is the current cloud coverage in Mumbai, India?\u201d; they ask about stock\nprices: \u201cCan you give me the opening price of Microsoft for the month of January 2023?\u201d; some\neven ask for place recommendations: \u201cI\u2019m looking for a 24-hour pharmacy in Montreal, can you find\none for me?\u201d. These tasks can only be completed by calling external APIs that contain the requested\ninformation. As such, these types of tasks\u2014what we call code-driven question answering\u2014require\nLLMs to generate code to fetch necessary information via APIs.\nJust as human coders rarely generate correct code on the first attempt, LLMs also struggle (Yang\net al., 2023). This is especially dire since current LLMs lack the ability to execute their generated\ncode and iteratively debug as most human programmers do. In addition, as ChatGPT received roughly\n80M queries in July 2023, 23% of it makes up 4M knowledge queries that month itself (Fishkin,\n2023; Chen et al., 2023; Wang et al., 2023a). Such a high volume of user queries can be expensive\nfor those who aim to develop a system using online LLM services with a fee to process these queries.\nTo overcome these challenges, in this work, we present EcoAssistant, the first system that is\ntailored for leveraging conversational LLMs to tackle code-driven question answering more affordably\n\u2217Work during Miscrosoft Research internship.\n1https://github.com/JieyuZ2/EcoAssistant\n1\narXiv:2310.03046v1  [cs.SE]  3 Oct 2023\nPreprint\nand accurately. EcoAssistant doesn\u2019t need any offline preparation or training; it is a purely\nonline service that improves with use. It contains three fundamental components. First, to support\niterative coding, it allows the conversational LLM as an assistant agent to converse with an automatic\ncode executor and iteratively refine code to make the correct API calls. We build EcoAssistant\nusing AutoGen (Wu et al., 2023), a recent framework that enables building LLM applications via\nmulti-agent conversation. Unlike existing practices that use LLMs to produce code or answer the\nuser query in a single generation, our system design exploits the recent advance of conversational\nLLMs that can iteratively refine their outputs (OpenAI, 2023; Touvron et al., 2023).\nSecond, we employ a hierarchy of LLM assistants, referred to as assistant hierarchy, which attempts to\nanswer the query with weaker, cheaper LLMs before backing off to stronger, expensive ones. For each\nquery, we start the conversation with the most cost-effective LLM assistant, and progressively back\noff to more expensive ones only when the current one fails. As LLMs typically have heterogeneous\npricing structures, such a simple strategy could reduce the overall cost of the system by reducing the\nusage of expensive LLMs.\nThird, we propose solution demonstration, which retrieves solutions from past successful queries as\nin-context demonstrations to help subsequent queries. To achieve this, we store correct query-code\npairs in a database once a query succeeds; then when a new query enters, we retrieve the most similar\nquery as well as the associated code from the database as in-context demonstrations in the LLM\u2019s\nprompt. With the proven solutions demonstrated, the assistant is more likely to generate accurate and\nefficient responses without redundant iterations, thereby increasing the likelihood of success.\nAlthough the assistant hierarchy and solution demonstration offer distinct advantages when used\nindividually, we find that their interplay leads to a synergistic effect that amplifies their individual\nbenefits. This is, because the assistants in the hierarchy share the database storing query-code pairs,\nthese solutions from stronger, expensive LLMs serve as useful guidance for the weaker models on\nsubsequent queries. As a consequence, the weaker assistant is likely to solve more queries in the\nfuture, which further reduces the systems\u2019 reliance on expensive LLMs.\nWe conduct systematic experiments on various types of queries to investigate both the performance\nand the dollar cost of the proposed system. Our results highlight that the assistant hierarchy can sig-\nnificantly reduce the cost, while the solution demonstration largely boosts the system\u2019s performance.\nIn addition, EcoAssistant, which incorporates both of these strategies, achieves superior perfor-\nmance with a further reduction of the cost. In addition, we show that EcoAssistant outperforms\nan individual GPT-4 assistant with a margin of 10% success rate with less than half of the expense.\n2\nTHE TASK OF CODE-DRIVEN QUESTION ANSWERING\nIn this work, we focus on a practical yet challenging task called code-driven question answering,\nwhere LLMs have to answer knowledge queries; The LLM has to generate code to invoke APIs for\nacquiring the necessary information needed to answer the user\u2019s question. For example, a user query\ncould be asking for dynamic or real-time information like the weather of a specific location at a certain\ndate. Since this information is not stored in the model\u2019s internal knowledge or general knowledge\nbase, the model would rely on the weather APIs to acquire the information. To achieve this, LLMs\nneed to not only understand the user\u2019s query correctly but also write decent Python code. Thus, this\nnew task presents a multi-faceted challenge: it demands proficiency in language understanding and\ngeneration of both natural and programming language. This characteristic differentiates code-driven\nquestion answering from existing question answering paradigms such as open-domain question\nanswering (Lee et al., 2019; Chen et al., 2017) or browser-assistant question answering (Nakano\net al., 2021), since they typically do not challenge the LLMs\u2019 capability of generating and refining\ncode. It is also different from generic code generation task (Chen et al., 2021; Hendrycks et al., 2021;\nAustin et al., 2021; Lu et al., 2021; Yang et al., 2023) by requiring LLMs to exploit domain-specific\nAPI based on the user query.\nIterative coding.\nCode-driven question answering naturally requires iterative coding (Yang et al.,\n2023). We connect the underlying LLM attempting to generate the code with a code executor.\nIntuitively, the code executor executes the generated code and forwards either the execution results\nor the failed execution trace back to the LLM. This interaction may occur multiple times, as the\nLLM uses the previous execution trace to refine its generation. One could view this process as an\n2\nPreprint\nautomatic multi-turn chat between the LLM and the code executor, which happens completely in the\nbackground, without the user\u2019s involvement. We adopt chat LLMs such as GPT-3.5-turbo, allowing\nus to leverage all the recent advancements of LLMs for chat-purposes.\nQueries come streaming.\nWe also consider a real-world scenario where queries come streaming\nsequentially over time. Therefore, each query is not an independent task but could leverage past\nqueries as guidance. In such a setting, one could imagine deriving keeping track of successful queries\nto improve future ones. Our system, described below, investigates how to utilize past queries to better\nserve future ones.\n3\nEC OAS S I S T A N T: USING LLM ASSISTANT MORE AFFORDABLY AND\nACCURATELY\nTo re-iterate, the task of code-driven question answering is both challenging and expensive. LLMs\nstruggle to generate the correct code at the first attempt to utilize APIs, and handling a high volume of\nuser queries using LLM services with a fee can be cost-intensive. To tackle this task in an affordable\nand accurate manner, we develop EcoAssistant, a system that uses LLMs to answer knowledge\nqueries correctly while reducing dollar costs.\nEcoAssistant contains three components (see Figure 1). First, it places LLMs as an assistant\nagent in conversation with a code executor. The LLM iteratively debugs its code by reading the code\nexecutor\u2019s outputs or failed execution trace, and finally produces the answer based on the information\nobtained. Second, to reduce expenses, we use a hierarchy of LLM assistants, attempting queries with\ncheaper LLM assistants before resorting to more expensive alternatives. Third, we keep track of\nsuccessful queries and the associated code and use them as in-context demonstrations for subsequent\nones. This allows LLMs in the future to use past successes as guidance. Our system requires no\noffline preparation, no dataset curation, and no training.\nFigure 1: EcoAssistant: the system involves two agents, one executor agent for executing the\ncode and the other assistant agent backed by LLMs for suggesting code to obtain information and\naddress the user queries. The query-code database stores the previous successful query and code pair.\nWhen a new query comes, the most similar query in the database is retrieved and then demonstrated in\nthe initial prompt with the associated code. The conversation invokes the most cost-effective assistant\nfirst and tries the more expensive one in the assistant hierarchy only when the current one fails.\nAutomated conversation between LLM assistants and code executor.\nEcoAssistant places\nthe LLM as an assistant agent within a conversation with a code executor. The executor extracts\n3\nPreprint\nthe generated code and executes it, forwarding the output back to the LLM; it then awaits the next\nconversation turn, where presumably the LLM will refine its generation, learning from its past\nmistake, or produce the final answer according to the execution results.\nTo achieve this conversation flow, we develop our system upon AutoGen (Wu et al., 2023), a recent\ninfrastructure that facilitates automated multi-agent conversation. In particular, we leverage the\nbuilt-in AssistantAgent and UserProxyAgent of AutoGen as the LLM assistant and code\nexecutor, respectively. The former is configured with the dedicated system prompts proposed in\nAutoGen, which instructs the LLM to 1) suggest code in a coding block when necessary, 2) refine the\ncode according to the execution results, and 3) append a special code \"TERMINATE\" at the end of\nthe response when it wants to terminate the conversation.\nThe latter, acting as a proxy of the user, automatically extracts the code from the LLM\u2019s message, and\nexecutes it in the local environment. It then sends the execution results back to the LLM. When there\nis no code detected, it would send back a default message. Thus, the conversation is automated and\nthe user only needs to input the original query to trigger the conversation without manual intervention\nlike copying, pasting, and executing the code.\nFinally, the conversation terminates when encountering one of the following cases: 1) the context\nwindow of the LLM is exceeded; 2) the number of back-forth turns in the conversation exceeds a set\nthreshold2; and 3) the LLM appends \"TERMINATE\" at the end of its response.\nAssistant hierarchy.\nWe employ a hierarchy of LLM assistants. In particular, given multiple\nLLMs, we initiate an AssistantAgent for each and start the automated conversation with the\nmost cost-effective LLM assistant. If the conversation between the current LLM assistant and the\ncode executor concludes without successfully resolving the query, the system would then restarts\nthe conversation with the next more expensive LLM assistant in the hierarchy. Considering that\nLLMs typically have varied pricing structures (e.g., GPT-3.5-turbo is an order of magnitude cheaper\nthan GPT-4), this strategy has the potential to significantly reduce costs by minimizing the usage of\nexpensive LLMs, while still effectively addressing queries.\nSolution demonstration.\nIn most practical scenarios, queries from users would appear sequentially\nover time. Our system leverages past success to help the LLM assistants address future queries.\nSpecifically, whenever a query is deemed successfully resolved by user feedback, we capture and\nstore the query and the final generated code snippet. These query-code pairs are saved in a specialized\nvector database. When new queries appear, EcoAssistant retrieves the most similar query from\nthe database, which is then appended with the associated code to the initial prompt for the new query,\nserving as a demonstration. We show that this utilization of past successful query-code pairs improves\nthe query resolution process with fewer iterations and enhances the system\u2019s performance.\nThe assistant hierarchy and solution demonstration as standalone designs offer distinct advantages:\nthe assistant hierarchy has the potential to reduce the cost by minimizing the reliance on expensive\nLLMs, and the solution demonstration can enhance the performance of LLMs by leveraging past\nsuccess. Together, they amplify the individual benefits. Even without a specialized design, the\nstronger LLM assistants implicitly advise weaker ones in the hierarchy by sharing their solutions via\nthe query-code database.\n4\nEXPERIMENT\nIn this section, we conduct experiments to investigate the performance and dollar cost of the\nEcoAssistant on various types of queries, with both model evaluation and human evaluation.\nWe empirically show the individual benefits introduced by the assistant hierarchy and the solution\ndemonstration, and the EcoAssistant could surpass an individual GPT-4 assistant by 10% of the\nsuccess rate with less than half of the GPT-4 assistant\u2019s expense.\n2We set the maximum number of turns of a conversation to 5 in this paper.\n4\nPreprint\n4.1\nSETUP\nDataset\nWe consider three datasets from the ToolBench (Qin et al., 2023) whose queries correspond\nto the domains of Places, Weather, and Stock respectively. We randomly sample 100 queries from\neach dataset. Each dataset comes with a recommended API to use. We list an example query and\nthe API for each dataset in Table 1. In addition to evaluating the methods on each dataset separately,\nwe also consider a setup where all three datasets are combined, resulting in 300 queries. We then\nrandomly shuffle the 300 queries to construct three datasets with different orders of queries and refer\nto them as Mixed-1, Mixed-2, and Mixed-3 respectively.\nTable 1: The default API and example query for each dataset.\nDataset\nAPI\nExample query\nPlaces\nGoogle Places1\nI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?\nWeather\nWeather API2\nWhat is the current cloud coverage in Mumbai, India?\nStock\nAlpha Vantage Stock API3\nCan you give me the opening price of Microsoft for the month of January 2023?\n1 https://developers.google.com/maps/documentation/places/web-service/overview\n2 https://www.weatherapi.com\n3 https://www.alphavantage.co/documentation/\nPrompt and LLMs\nThe initial prompt contains the query, the API name/key, and the retrieved\nquery-code pair when solution demonstration is used, and LLMs have to rely on their internal\nknowledge of the API and coding skills to answer the query. To avoid leaking the confidential\nAPI key to the LLM services, we randomly generate a unique four-byte text string in hexadecimal\nfor each API as a fake key, then whenever the code is executed, we replace the fake key in the\ncode with the actual API key so that the code can function as expected. We present the prompt we\nused in the experiments in Appendix D. Since the techniques we propose are orthogonal to existing\nprompting methods and can be synergistically applied, we also conducted experiments using the\nChain-of-Thought (CoT) prompting (Wei et al., 2022). In this work, we focus on conversational\nLLMs including two black-box models with various costs (GPT-3.5-turbo and GPT-4) and one\nopen-source model (LLAMA-2-13B-chat (Touvron et al., 2023)). We assume the dollar cost of the\nLLAMA-2-13B-chat is zero since it can be hosted with a reasonable amount of computing resources,\nwhile recording the cost of using black-box LLM services.\nCompared methods and implementation\nWe investigate the performance of three assistants\nbacked by different conversational LLMs (LLAMA-2-13B-chat, GPT-3.5-turbo, and GPT-4) as well\nas two types of assistant hierarchy: AssistantHier-G (GPT-3.5-turbo + GPT-4) and AssistantHier-L\n(LLAMA-2-13B-chat + GPT-3.5-turbo + GPT-4). For each assistant or assistant hierarchy, we include\nits vanilla version and the following variants: + CoT (with Chain-of-Thought prompting), + SolDemo\n(with solution demonstration), and + CoT + SolDemo (with both Chain-of-thought prompting and\nsolution demonstration). We couple each assistant or assistant hierarchy to a code executor agent in\norder to tackle the code-driven question answering task. Note that the proposed EcoAssistant\nsystem includes both assistant hierarchy and solution demonstration i.e., AssistantHier-G/L (+ CoT)\n+ SolDemo. We implement all the systems based on AutoGen (Wu et al., 2023), a Python library3 for\nmulti-agent conversation framework. For solution demonstration, we use Chroma (Chroma, 2023), an\nopen-source embedding database to store the query-code pairs; we use multi-qa-mpnet-base-dot-v1\nmodel to embed the user query and the cosine similarity for similarity search 4.\nEvaluation protocol\nWe focus on both the dollar cost and performance of compared methods. For\nmodel performance, we report the success rate, i.e., the percentage of queries that are successfully\nhandled by the model. Since the queries usually do not have ground truth answer (Qin et al., 2023),\ne.g., asking the weather at a certain date, we adopt both model evaluation and human evaluation. For\nmodel evaluation, we leverage GPT-4 as a proxy of the user to judge whether the system successfully\nhandles the query (Zheng et al., 2023; Fu et al., 2023; Wang et al., 2023b). In particular, after the\nconversation is terminated, we prompt GPT-4 with the whole conversation history and ask it whether\nthe tested system successfully handles the query. The details can be found in Appendix D. We repeat\neach evaluation three times with different random seeds if not otherwise specified.\n3https://github.com/microsoft/autogen\n4https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1\n5\nPreprint\n4.2\nMODEL EVALUATION: INDIVIDUAL DATASET\nFirst, we conduct experiments on each of the three individual dataset to investigate the performance\nof compared systems. The results are present in Table 2. We summarize our findings as below.\nTable 2: Success rate (%) and dollar cost on the Places, Weather, and Stock dataset.\nMethod\nPlaces\nWeather\nStock\nSuccess rate (%)\nCost\nSuccess rate (%)\nCost\nSuccess rate (%)\nCost\nLLAMA-2-13B-chat\n27.00\n0.00\n6.33\n0.00\n6.67\n0.00\n+ CoT\n25.00\n0.00\n7.67\n0.00\n6.33\n0.00\n+ SolDemo\n56.00\n0.00\n6.00\n0.00\n31.33\n0.00\n+ CoT + SolDemo\n52.00\n0.00\n4.67\n0.00\n14.00\n0.00\nGPT-3.5-turbo\n39.33\n0.47\n46.00\n0.41\n17.00\n0.36\n+ CoT\n61.33\n0.67\n69.00\n0.67\n50.00\n0.84\n+ SolDemo\n77.33\n0.49\n79.67\n0.54\n68.00\n0.50\n+ CoT + SolDemo\n70.33\n0.73\n78.33\n0.69\n64.67\n0.80\nGPT-4\n85.00\n12.58\n87.33\n10.73\n59.33\n18.49\n+ CoT\n78.67\n15.16\n75.67\n11.67\n57.67\n19.01\n+ SolDemo\n88.00\n11.76\n87.33\n10.81\n75.00\n14.33\n+ CoT + SolDemo\n87.33\n13.75\n84.67\n11.30\n77.67\n15.52\nAsistantHier-G\n89.33\n8.61\n90.67\n6.21\n64.33\n15.99\n+ CoT\n89.33\n7.28\n88.33\n4.87\n73.33\n11.85\n+ SolDemo\n96.67\n3.73\n95.00\n3.04\n81.67\n8.10\n+ CoT + SolDemo\n96.33\n5.52\n93.00\n3.49\n86.00\n8.04\nAsistantHier-L\n91.67\n5.97\n91.67\n5.89\n66.33\n15.10\n+ CoT\n91.33\n5.89\n89.00\n4.36\n75.33\n11.01\n+ SolDemo\n97.00\n3.33\n98.00\n2.24\n85.00\n6.70\n+ CoT + SolDemo\n95.33\n3.52\n96.33\n2.82\n84.33\n6.86\nFinding 1: the compared LLMs have distinct performance- the more expensive the model\nis, the better it performs.\nFrom the results, we can see that individual LLMs (GPT-3.5-turbo,\nGPT-4, LLAMA-2-13B-chat) have heterogeneous performance. In particular, GPT-4 achieves the\nhighest success rate followed by GPT-3.5-turbo, while LLAMA-2-13B-chat underperforms the\nothers. On the other hand, GPT-4 has the highest cost, GPT-3.5-turbo is relatively cost-effective, and\nLLAMA-2-13B-chat has zero dollar cost since it can be hosted on a local machine.\nFinding 2: the Chain-of-Thought (CoT) prompting could largely enhance LLM with moderate\nperformance, i.e., GPT-3.5-turbo.\nthe Chain-of-Thought (CoT) prompting consistently enhances\nthe performance of GPT-3.5-turbo across datasets. However, for GPT-4 and LLAMA-2-13B-chat,\nthe success rate doesn\u2019t necessarily benefit from CoT. We hypothesize this could be because GPT-4\nis already highly competent, leaving little room for CoT to further improve its performance, while\nthe performance of LLAMA-2-13B-chat does not benefit from CoT probably due to its inherent\ninadequacy in tackling code-driven question answering tasks. In addition, we find that CoT tends\nto increase the dollar cost, since it encourages the model to think step by step and therefore would\nincrease the number of tokens that LLMs input and output. Finally, when comparing AssistantHier-G\nto AssistantHier-G + CoT, the latter results in a reduced cost. This is attributed to CoT enhancing the\nsuccess rate of GPT-3.5-turbo, therefore decreasing the reliance on the more expensive GPT-4.\nFinding 3: solution demonstration could boost the success rate, especially when the method is\nnot well-performing.\nIn almost all cases, solution demonstration could significantly improve the\nsuccess rate without introducing a notable increase in the cost, especially for less competitive LLMs.\nIn particular, solution demonstration almost doubles the success rate of vanilla GPT-3.5-turbo on the\nPlaces and the Weather dataset, while introducing a 3x boost of success rate on the Stock dataset.\nFor LLAMA-2-13B-chat, solution demonstration also increases the success rate on the Places and\nthe Stock dataset by a large margin. Yet for high-performing model GPT-4, solution demonstration\ndoes not result in a significant performance boost except for the Stock dataset where the GPT-4 only\nexhibits a success rate of around 60%.\nFinding 4: Compared to the GPT-4, the assistant hierarchy could significantly reduce the cost\nwhile slightly improving the success rate.\nBy comparing assistant hierarchy (AsistantHier-G and\n6\nPreprint\nAsistantHier-L) with vanilla GPT-4, we can see that the cost is significantly reduced; in particular,\nAsistantHier-G achieves cost savings of approximately 10%-30%, while AsistantHier-L realizes\nreductions in the range of 15%-50%. In addition, assistant hierarchy also leads to a slight boost in\nsuccess rate since it allows trying multiple assistants for a single query.\nFinding 5: EcoAssistant (assistant hierarchy + solution demonstration) achieves superior\nperformance with moderate cost.\nFinally, EcoAssistant (AsistantHier-G/L (+ CoT) + Sol-\nDemo) delivers the highest success rate across all datasets, surpassing the top-performing GPT-4\nvariants by roughly a 10 percentage point margin in success rate. Additionally, this combination\nleads to a further cost reduction of approximately 30%-50% when compared to solely using the\nassistant hierarchy approach (AsistantHier-G/L (+ CoT)). These findings are surprising, given that\nthe solution demonstration could only enhance the success rate when used individually. We attribute\nthis synergy to the fact that solutions generated by the high-performing GPT-4 subsequently guide\nthe more affordable, albeit weaker, LLMs. As a result, cost savings emerge, because these more\neconomical LLMs can tackle a greater number of tasks using the GPT-4 solution as a demonstration,\nthus minimizing the reliance on the pricier GPT-4 assistant.\n4.3\nMODEL EVALUATION: MIXED DATASET\nWe also evaluate methods on mixed datasets: Mixed-1, Mixed-2, and Mixed-3. Each of them\nencompasses queries from all individual datasets, distinguished by different query orderings. These\nexperiments investigate how the methods perform when queries span multiple domains and ordering.\nSpecifically, we assess six methods in this experiment: GPT-3.5-turbo, GPT-3.5-turbo + SolDemo,\nGPT-4, GPT-4 + SolDemo, AsistantHier-G, AsistantHier-G + SolDemo (EcoAssistant). We\nvisualize the results in Figure 2 in order to demonstrate the scaling trend of cost and number of\nsuccessful queries with regard to the number of queries processed in the streaming setting.\nFigure 2: The curves of the number of successful queries / cost with regard to the number of queries\nprocessed on mixed datasets. The three datasets encompass queries from all individual datasets,\ndistinguished by different query orderings. We can see that EcoAssistant (AsistantHier-G +\nSolDemo) leads to the best performance while maintaining relatively low cost.\nFrom the results, we draw the following conclusions. First, because the solution demonstration relies\non the past solutions and therefore could be affected by the ordering of the queries, it is then important\nto ascertain its robustness against different query sequences; the results suggest that the method\nis largely order-agnostic, as evidenced by the consistency in performance curves (X + SolDemo)\n7\nPreprint\nacross datasets. Second, the two variants of GPT-3.5-turbo are the most cost-effective since the cost\ncurves are nearly flat compared to other methods, yet the GPT-3.5-turbo alone underperforms all\nthe compared methods in terms of the success count; however, integrating solution demonstration\n(GPT-3.5-turbo + SolDemo) substantially uplifts its efficacy. Third, despite the descent performance\nGPT-4 renders, it is the most cost-intensive method as indicated by its steeper cost curve; fortunately,\nthe assistant hierarchy (AsistantHier-G) would reduce the cost as its cost curves have a smaller slop,\nwithout sacrificing the performance. Finally, the EcoAssistant (AsistantHier-G + SolDemo)\nexhibits the best performance and concurrently has a much flatter cost curve than other methods,\nexcept for GPT-3.5-turbo variants.\n4.4\nHUMAN EVALUATION: MIXED DATASET\nFor human evaluation, we sample 100 queries from all the 300 queries of different datasets to form\na dataset referred to as Mixed-100. We gather one code snippet for each of the 100 queries, which\ncould produce the necessary information for addressing the query. In particular, for each query, we\ncollect all the code snippets generated by LLMs in previous experiments. From this collection, we\nmanually choose and validate one snippet (with necessary manual modifications) to ensure that the\ncode snippet can effectively obtain the needed information. Thus, one can refer to the output of the\ngolden code snippet when assessing whether the model successfully addresses the query. For all the\nexperiments in this section, we adopt this strategy to do human evaluation.\nTable 3: Human evaluation results on the Mixed-100 dataset.\nGPT-3.5-turbo\nGPT-3.5-turbo\nGPT-4\nGPT-4\nAsistantHier-G\nAsistantHier-G\n+ SolDemo\n+ SolDemo\n+ SolDemo\nMetric\nMain results\nSuccess rate (%)\n25\n45\n59\n78\n63\n80\nCost\n0.36\n0.48\n13.77\n10.27\n11.84\n5.90\nModel\nAvg. model calls per query\nGPT-3.5-turbo\n2.42\n2.91\n-\n-\n2.42\n2.92\nGPT-4\n-\n-\n3.12\n2.57\n2.51\n1.25\nFrom the main results part of Table 3, we can see that our main conclusion still holds for the case of\nhuman evaluation. Specifically, solution demonstration can significantly improve the success rate,\nand assistant hierarchy contributes to cost savings when compared to GPT-4. More importantly,\nEcoAssistant (AssistantHier-G + SolDemo) delivers top-tier performance at a moderate expense;\nwhen comparing it against GPT-4, we can see that its success rate is 10 points higher while incurring\nless than half of GPT-4\u2019s cost. To better explain the effect of the proposed techniques, we also present\nthe averaged model calls per query for each method in Table 3. First, the solution demonstration\nincreases the model calls of GPT-3.5-turbo, because the vanilla GPT-3.5-turbo struggles to produce\nformatted code that can be extracted and executed by the code executor, and therefore the conversation\nterminates early without the query being appropriately addressed; with solution demonstration, GPT-\n3.5-turbo is more likely to generate formatted code for the code executor to execute, thus the\nconversation would proceed. Second, for GPT-4, the solution demonstration reduces the number\nof model calls because it guides the model to write good code at the beginning, requiring fewer\nturns to refine the code for outputting necessary information. Finally, comparing AssistantHier-G\nwith/without SolDemo, we can see that the averaged model calls of GPT-3.5-turbo increase while that\nof GPT-4 reduces. This indicates that EcoAssistant (AssistantHier-G + SolDemo), even with\na higher success rate, relies less on expensive GPT-4 because the GPT-3.5-turbo is able to address\nmore queries thanks to the solution demonstration, leading to the saving of cost.\n4.5\nHUMAN EVALUATION: AUTONOMOUS SYSTEMS WITHOUT HUMAN FEEDBACK\nIn the above experiments, we incorporate a user in the loop\u2014either an actual human or a GPT-4\nmodel\u2014to determine the successful completion of a query. This feedback serves three primary\nfunctions: 1) calculating the success rate of the evaluated method; 2) for solution demonstration\nto decide whether to store the query-code pair; and 3) for assistant hierarchy to decide whether\nto invoke the next assistant in the hierarchy. However, in practice, users may prefer a system that\noperates autonomously, without necessitating user feedback. Regarding this, we build an autonomous\nsystem for each compared method, which requires no human feedback. Specifically, we add a GPT-4\n8\nPreprint\nevaluator to serve the aforementioned functions 2) and 3), and after all the queries are processed, we\nmanually assess the success of each query to calculate the success rate. In addition, because now we\ntreat the GPT-4 evaluator as part of the system, we include its cost as the system cost as well. Note\nthat the method without solution demonstration or assistant hierarchy (e.g., GPT-3.5-turbo alone)\nwould remain the same as before.\nWe evaluate these autonomous systems on the Mixed-100 dataset, applying the same human eval-\nuation strategy as in the previous section. The outcomes are detailed in Table 4. When comparing\nEcoAssistant (AssistantHier-G + SolDemo) to GPT-4, we still observe a success rate boost\nexceeding 10 points and a cost reduction of over 50%. However, while EcoAssistant presents a\ncost comparable to its non-autonomous counterpart (as shown in Table 3), its success rate is dimin-\nished by 8 points. This is because the GPT-4 evaluator would occasionally mistrust the GPT-3.5-turbo\nand would not resort to the GPT-4 assistant, leading to the performance drop; simultaneously, as it\ncalls upon the GPT-4 assistant less frequently than its non-autonomous version, the cost does not\ngrow even when factoring in the GPT-4 evaluator\u2019s expenses.\nWe also present the run-time of these autonomous systems in Table 4. We can see that the solution\ndemonstration can largely reduce the run-time, indicating that it streamlines the query resolution pro-\ncess with fewer iterations. In addition, the assistant hierarchy (AssistantHier-G) exhibits the longest\nrun-time, as it often tries out both GPT-3.5-turbo and GPT-4 assistant. Remarkably, EcoAssistant\nnecessitates less than half of the run-time of AssistantHier-G and even outperforms a standalone\nGPT-4 assistant. This underscores the synergistic effect of integrating the assistant hierarchy and\nsolution demonstration, further reducing the dependence on the more latency-prone GPT-4 assistant.\nTable 4: Human evaluation results on the Mixed-100 for autonomous systems.\nGPT-3.5-turbo\nGPT-3.5-turbo\nGPT-4\nGPT-4\nAsistantHier-G\nAsistantHier-G\n+ SolDemo\n+ SolDemo\n+ SolDemo\nMetric\nMain results\nSuccess rate (%)\n25\n47\n59\n77\n54\n72\nCost\n0.36\n2.46\n13.77\n12.07\n12.99\n5.78\nModel\nAvg. model calls per query\nGPT-3.5-turbo\n2.42\n2.84\n-\n-\n2.45\n2.90\nGPT-4\n-\n-\n3.12\n2.49\n2.29\n0.59\nRun-time (s)\n2414\n2073\n5272\n3873\n8993\n4033\n5\nRELATED WORK\nHere, we briefly discuss related work of existing attempts that build LLM systems/applications upon\nmulti-agent conversation and prior works on cost-effective deployment of LLMs.\nLLM-based multi-agent conversation\nLLM-based agents have attracted great attention from\nboth practitioners and researchers (Xi et al., 2023; Wang et al., 2023c; Liu et al., 2023). Recently,\nthere have been efforts towards harnessing multi-agent conversations in LLM-based applications to\nunleash the potential of among-agent collaboration (Wu et al., 2023). Example applications include\ncollaborative task completion with multiple agents (Li et al., 2023; Hong et al., 2023; Qian et al., 2023;\nTalebirad & Nadiri, 2023) and leveraging multi-agent debate to encourage divergent thinking (Liang\net al., 2023) or to improve factuality and reasoning (Du et al., 2023). In this work, we focus on\nexploiting the multi-agent conversation framework to tackle code-driven question answering with an\nemphasis on both cost-efficiency and performance.\nCost-effective deployment of LLMs\nCountless efforts have been devoted to the cost-effective\ndeployment of LLMs. Most of the existing attempts aim to improve the time/compute efficiency via\ntechniques like model quantization (Yao et al., 2023) and prompt summarization (Arefeen et al., 2023),\ncompression (Mu et al., 2023), and batching (Lin et al., 2023), etc.. In contrast, we seek to reduce the\ndollar cost of using LLM API services. With a similar goal, EcoOptiGen (Wang et al., 2023a) strives\nto reduce dollar cost for hyperparameter optimization of LLM inference and FrugalGPT (Chen et al.,\n2023) explores several techniques to reduce the dollar cost for single-turn text generation, while we\nfocus on leveraging LLM as agent in a multi-agent conversation in a cost-effective way.\n9\nPreprint\n6\nCONCLUSION\nIn this study, we explore affordable and precise LLM applications for code-driven question answer-\ning. We introduce EcoAssistant, an LLM-based system built upon a two-agent conversation\nframework. It involves an assistant agent backed by LLM and a code executor agent, and relies on\ntheir interaction to address user queries. EcoAssistant also includes two simple yet effective\ntechniques: assistant hierarchy, which prioritizes cost-effective LLMs, and solution demonstration,\nwhich leverages past successful solutions for new queries. Our empirical evaluations demonstrate\nthat EcoAssistant could simultaneously reduce the cost and enhance the performance.\nREFERENCES\nMd Adnan Arefeen, Biplob Debnath, and Srimat Chakradhar. Leancontext: Cost-efficient domain-\nspecific question answering using llms. arXiv preprint arXiv:2309.00841, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 1870\u20131879, Vancouver, Canada, July 2017. Association\nfor Computational Linguistics. doi: 10.18653/v1/P17-1171.\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while\nreducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nChroma. Chromadb. https://github.com/chroma-core/chroma, 2023. URL https:\n//github.com/chroma-core/chroma.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factual-\nity and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325,\n2023.\nRand\nFishkin.\nWe\nanalyzed\nmillions\nof\nchatgpt\nuser\nses-\nsions:\nVisits\nare\ndown\n29\nhttps://sparktoro.com/blog/\nwe-analyzed-millions-of-chatgpt-user-sessions-visits-are-down-29-since-may-program\n2023.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166, 2023.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge\ncompetence with apps. NeurIPS, 2021.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework. arXiv preprint arXiv:2308.00352, 2023.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 6086\u20136096, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1612.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for \"mind\" exploration of large scale language model society, 2023.\n10\nPreprint\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu,\nand Shuming Shi. Encouraging divergent thinking in large language models through multi-agent\ndebate, 2023.\nJianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more\nwith less. arXiv preprint arXiv:2309.00384, 2023.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,\nKaiwen Men, Kejuan Yang, et al.\nAgentbench: Evaluating llms as agents.\narXiv preprint\narXiv:2308.03688, 2023.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark\ndataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. arXiv\npreprint arXiv:2304.08467, 2023.\nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\nBrowser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021.\nOpenAI.\nGpt-4 technical report.\nArXiv, abs/2303.08774, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:257532815.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian,\nKunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei\nTang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang\nHuang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng\nJi, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of\nintelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nChi Wang, Susan Xueqing Liu, and Ahmed H. Awadallah. Cost-effective hyperparameter optimization\nfor large language model generation inference. In AutoML\u201923, 2023a.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and\nJie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048,\n2023b.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\narXiv preprint arXiv:2308.11432, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via\nmulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.\n11\nPreprint\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and\nbenchmarking interactive coding with execution feedback. In ArXiv, 2023.\nZhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study on\npost-training quantization for large language models. arXiv preprint arXiv:2303.08302, 2023.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n12\nPreprint\nA\nLIMITATIONS AND FUTURE WORK\nLimitations\nFirst, our system relies on a pre-defined hierarchy of LLM assistants. Such a static\nhierarchy may not always be optimal for all queries, and the system may benefit from a more adaptive\nselection mechanism. Second, our system depends on a database to store past successful query-\ncode pairs, it may become a bottleneck when processing millions of queries. Thus, some pruning\nmechanisms that delete less useful items in the database could be helpful when the number of queries\nexplodes. Third, while EcoAssistant attempts to handle a broad range of queries, it might not be\nas adept at deeply specialized or niche queries that demand expert-level domain knowledge. Fourth,\nwhile EcoAssistant attempts to handle a broad range of queries, it might not be as adept at\ndeeply specialized or niche queries that demand expert-level domain knowledge. In addition, the\nback-and-forth conversational nature, especially with multiple iterative refinements, might introduce\nlatency, leading to longer response times for the end users. Finally, the system might struggle with\nlong conversational contexts, especially given the token limits of current LLMs. This could affect the\nquality of responses in extended conversations.\nFuture work\nHere, we list several directions for future work. 1) Informative user feedback: In this\nwork, we leverage binary user feedback indicating whether the user query is successfully addressed.\nIncorporating more informative user feedback to guide the conversation in EcoAssistant might\nenable more targeted and efficient task completion. 2) More agents in the system: In this work, we\ninvolve two types of agents in the system. One direction to explore could be adding more agents\nto the system for better collaborative task completion. 3) Advanced retrieval mechanisms: The\ncurrent approach retrieves past solutions based on query similarity. Exploring more advanced retrieval\nmechanisms might enhance the efficacy of solution demonstrations. 4) Multimodal interactions:\nExtending EcoAssistant to support multimodal interactions, such as voice or images, can broaden\nits applicability and cater to an even wider user base.\nB\nMODEL EVALUATION V.S. HUMAN EVALUATION\nHere, we investigate the reliability of model (GPT-4) evaluation. In particular, we treat whether the\nquery is successfully addressed as a binary classification task, and use the human evaluation results\nas ground truth to assess the efficacy of model evaluation results. We evaluate the accuracy, precision,\nand recall of model evaluation using queries and the human evaluation results in Section 4.4. The\nresults can be found in Table 5; we separate the results based on the methods used to process the\nqueries. From the results, we can see that all the recall is 100%, which means when the GPT-4\nevaluator concludes that the assistant fails, it truly fails, while the precision ranges from 66% to\n84%, indicating that there is still room for improvement. As the autonomous systems described in\nSection 4.5 rely on the GPT-4 evaluator to judge whether the user query is successfully addressed, a\nbetter GPT-4 evaluator is likely to contribute to better autonomous systems, which we leave as future\nwork.\nTable 5: We evaluate the efficacy of model evaluation using the human evaluation results as ground\ntruth for experiments in Section 4.4. We found that model evaluation is always correct when it\nconcludes that the assistant fails to address the user query, as all the recall is 100%, while the\nprecision ranges from 66% to 84%, which indicates that model evaluation could provide a certain\nlevel of signal on the system performance.\nMetric\nGPT-3.5-turbo\nGPT-3.5-turbo\nGPT-4\nGPT-4\nAsistantHier-G\nAsistantHier-G\n+ SolDemo\n+ SolDemo\n+ SolDemo\nAccuracy\n91.75\n82.63\n80.41\n85.86\n85.88\n71.72\nPrecision\n75.76\n72.58\n75.32\n84.78\n72.09\n66.12\nRecall\n100\n100\n100\n100\n100\n100\nC\nA VISUALIZATION OF THE ASSISTANT-CODE EXECUTOR CONVERSATION\nHere, we demonstrate the assistant-code executor conversation in Figure 3. In particular, we trigger\nthe conversation with a user query. Then the conversation between the LLM assistant and the code\nexecutor would proceed automatically until a termination condition is satisfied.\n13\nPreprint\nFigure 3: A visualization of the assistant-code executor conversation.\nD\nIMPLEMENTATION DETAILS\nHardware\nAll experiments ran on a machine with an Intel(R) Xeon(R) CPU E5-2678 v3 with\n512G memory and two 48G NVIDIA RTX A6000 GPUs. Note that the GPUs are only for hosting the\nLLAMA-2-13b-chat model, while the GPT family models as LLM API services do not require GPU.\nPrompts and default message\nFirst, we present our prompt used for each user query in Figure 4,\nwhere the red part indicates the recommended API for the LLMs to use, the blue part is the template\nfor demonstrating the retrieved query-code pair when the solution demonstration is applied, and\nthe final black part is the user query. For Chain-of-Thought prompting, we append one sentence\n\"Let\u2019s think step by step.\" at the end of the prompt. Then, we present the system prompt we used\nfor model evaluation in Figure 5. In particular, we leverage GPT-4 for model evaluation and set\nthe system prompt as in Figure 5. Then we prompt the GPT-4 model with the whole conversation\nhistory, following the template stated in the first point of the system prompt. The GPT-4 model would\ngenerate a single indicator token (yes/no) as the judgment for whether the user query is successfully\naddressed. Finally, in the code executor agent, if there is a coding block in the response generated by\nthe assistant, the code executor would automatically extract and execute the code, and then send back\nthe execution results; if no coding block is detected, we set a default message \"Reply TERMINATE\nif everything is done.\" as the response of the code executor to let the conversation proceed.\nE\nCASE STUDY\nHere, we present several case studies on how solution demonstration leads to successful query\ncompletion. In particular, we focus on the GPT-3.5-turbo assistant, and compare the conversation\nwith and without solution demonstration. We showcase one query from each of the three datasets.\nThe mapping between the figures of the conversation and the corresponding method/dataset is in the\nTable 6.\n14\nPreprint\nFigure 4: Prompt we used for each user query.\nFigure 5: System prompt for model evaluation.\nTable 6: The look-up table for case studies.\nDataset\nMethod\nFigure\nPlaces\nGPT-3.5-turbo\nFigure 6\nGPT-3.5-turbo + SolDemo\nFigure 7\nWeather\nGPT-3.5-turbo\nFigure 8\nGPT-3.5-turbo + SolDemo\nFigure 9& 10\nStock\nGPT-3.5-turbo\nFigure 11\nGPT-3.5-turbo + SolDemo\nFigure 12\n15\nPreprint\nFigure 6: The GPT-3.5-turbo assistant fails to use the API due to wrong parameters.\n16\nPreprint\nFigure 7: With solution demonstration, the GPT-3.5-turbo assistant successfully obtains the restaurant\ninformation and answers the user query.\n17\nPreprint\nFigure 8: GPT-3.5-turbo fails to make the code work because it mistakenly use the API key of Google\nPlaces for the WeatherAPI.\n18\nPreprint\nFigure 9: First half of GPT-3.5-turbo with solution demonstration for the query \"Can you provide me\nwith the subset and sunrise times for Paris this upcoming Wednesday\".\n19\nPreprint\nFigure 10: Second half of GPT-3.5-turbo with solution demonstration for the query \"Can you\nprovide me with the subset and sunrise times for Paris this upcoming Wednesday\". With solution\ndemonstration, GPT-3.5-turbo successfully figures out the correct date and obtains the queried\ninformation to answer the user query.\n20\nPreprint\nFigure 11: The GPT-3.5-turbo assistant fails to calculate the stock price rise due to incorrect code\nlogic: the variable price_on_date is not used correctly.\n21\nPreprint\nFigure 12: With solution demonstration, the GPT-3.5-turbo assistant obtains the correct stock\ninformation and answers the user query successfully.\n22\n"
  }
]